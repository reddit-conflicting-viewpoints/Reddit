post_id,comment_id,parent_id,comment,up_vote_count,controversiality,total_awards_received,is_locked,is_collapsed,is_submitter,created_utc
t75c0b,hzfp3ui,t3_t75c0b,"The parity bit indicates if the number is odd. 1 = odd, 0 = even. You then add this bit, so your two options are:

* Even + 0
* Odd + 1

In both cases the result is an even number. So a correctly transmitted message will always be even.",33,0,0,False,False,False,1646473818.0
t75c0b,hzfsekj,t3_t75c0b,"FuZzy is correct, the parity bit is set or cleared to Ensure the entire message is even, this can also be interpreted as saying that the parity bit being set implies that the rest of the message is odd.

As for why we pick odd or even its somewhat arbitrary but one good thing about the agreed upon system is that a zero message has a zero parity bit making the whole packet zeros (good for simplicity / consistency / compression etc)",11,0,0,False,False,False,1646476497.0
t75c0b,hzgca19,t3_t75c0b,"The why a certain standard is chosen is typically rooted in 1960s/70s era reasons, more modern chips (reflected in eg. RS232) can have either odd or even parity, but ‘back in the day’ when you have just XOR ports to check for parity, an extra port to invert is ludicrously expensive.",3,0,0,False,False,False,1646489973.0
t75c0b,hzfu9r7,t3_t75c0b,"The ""compute parity bit"" step on the right is a little misleading. Think of it as counting the 1 bits (including the parity bit), and that count can then be tested for whether it's even.

If calculations on the left and right were truly the same as the shared name suggests, then I would have phrased the conditional as a comparison to the received parity bit rather than a test for even.",3,0,0,False,False,False,1646478009.0
t75c0b,hzg0l62,t3_t75c0b,“Compute parity bit” should instead say “compute parity of transmission”,1,0,0,False,False,False,1646482959.0
t75c0b,hzhrdng,t3_t75c0b,"Whether a protocol uses even or odd parity is a design choice.

The reason your diagram is showing even parity is because the protocol it's describing mandates that, and the protocol was designed that way. There's usually not a strong reason one way or the other.

A lot of old parallel protocols use even parity because XORing everything together results in even parity.

Read [the wiki page for more](https://en.wikipedia.org/wiki/Parity_bit).",1,0,0,False,False,False,1646512509.0
t75c0b,hzgior0,t1_hzgca19,"Indeed it comes down to details. 

Remains despite advances, to reject some false messages in particular. Examples: broken pin in a code plug or false strobe on an asynchronous parallel bus. In the examples to reject, there is the no-plug / false-strobe ""data resting message"", and a 1-bit corrupted legal message.",2,0,0,False,False,False,1646493133.0
t75c0b,hzhrf9p,t1_hzhrdng,"**[Parity bit](https://en.wikipedia.org/wiki/Parity_bit)** 
 
 >A parity bit, or check bit, is a bit added to a string of binary code. Parity bits are a simple form of error detecting code. Parity bits are generally applied to the smallest units of a communication protocol, typically 8-bit octets (bytes), although they can also be applied separately to an entire message string of bits. The parity bit ensures that the total number of 1-bits in the string is even or odd.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1646512528.0
t75c0b,hzgt3id,t1_hzgior0,"We’re actually returning to more error correction, both 10 and 25G Ethernet over copper and also PCIe4 and DDR5, the channels are so noisy they’re basically a spectrum of frequencies and advanced parity and error correction algorithms are used to pick out the correct bits from the streams.

We’re heading back into the days of analog signal analysis to troubleshoot.",3,0,0,False,False,False,1646497745.0
t7d7ws,hzgz8k5,t3_t7d7ws,I read your excerpt as saying that they are equivalent terms.,3,0,0,False,False,False,1646500330.0
t7d7ws,hzh7ofk,t3_t7d7ws,"Period and rate are two ways of measuring the same thing, and they are reciprocal to each other. Period measures the amount of time between two ticks, the rate measures how many ticks will fit into one second.

1/period = rate and 1/rate = period.",1,0,0,False,False,False,1646503894.0
t7d7ws,hzgzr5w,t1_hzgz8k5,It does say that. But that basically means they said x is also called y. Then they defined x in terms of y and vice versa. That confuses me.,1,0,0,False,False,True,1646500546.0
t7d7ws,hzhgw01,t1_hzh7ofk,You misunderstand. I'm not asking for the difference between period and rate. But between the clock cycle and clock period,1,0,0,False,False,True,1646507857.0
t7d7ws,hzhkaw7,t1_hzhgw01,"Clock cycle is a period. Clock rate is 1/(clock cycle).

Clock cycle is how long it takes in time to perform one clock/tick of processing.

Clock rate is how many times it can do a clock cycle per second.

They didn't misunderstand you, it is you who is misunderstanding.",1,0,0,False,False,False,1646509327.0
t7d7ws,hzhps5i,t1_hzhgw01,A clock cycle is when the system clock changes its value. Clock period is how long that takes. The inverse of the period is the frequency.,1,0,0,False,False,False,1646511794.0
t7d7ws,hzhmyza,t1_hzhkaw7,"so clock cycle is the exact same thing as clock period? If so why give separate definitions?

You also said ""Clock cycle is how long it takes in time to perform one clock/tick of processing."" In the textbook clock is the same as clock cycle, so you just said ""Clock cycle is how long it takes in time to perform one clock cycle of processing"". You see my confusion?

Also I get clock rate, no need to mention that.",1,0,0,False,False,True,1646510529.0
t7d7ws,hzhq2iq,t1_hzhps5i,that makes sense thank you. But could you relate it to the textbook definition? Why did they say the clock cycle is the “The time for one clock period” then.,1,0,0,False,False,True,1646511924.0
t7d7ws,hzhqv7r,t1_hzhq2iq,"I’m not sure, I think the writer didn’t word that section very well. I’ve never heard of anyone using the terms clock cycle and period interchangeably either, because they’re different things. You seem to understand the difference, so that’s all that matters.",1,0,0,False,False,False,1646512284.0
t7d7ws,hzhratj,t1_hzhqv7r,ok thank you. I was just unsure because it's a popular textbook,1,0,0,False,False,True,1646512474.0
t7hgpl,hzhoojw,t3_t7hgpl,"Can you solve the problem on paper with sample input 1? The input says that the yard is 5x5, and there is one tree at (2,4). What is the largest square that can be made without intersecting a tree or going out of bounds?

Edit: I realize that the answer is given. The point of solving it yourself is to think through how you would arrive at such a solution algorithmically. What if there was another tree at (3,3)?

This can be a bit tricky as you can’t just say “from the top-left, what is the largest square that can be made until I encounter a tree”, because there could be a single tree at 1,1 or maybe 4,4. That’s the real challenge and frankly some of the fun too. Good luck!",1,0,0,False,False,False,1646511304.0
t7hgpl,hzhr5s1,t3_t7hgpl,"If u know dynamic programming, you can get the harder marks. If you don’t just brute force it.",1,0,0,False,False,False,1646512413.0
t7hgpl,hzhrm74,t1_hzhr5s1,What? This doesn't look like a dynamic programming problem?,1,0,0,False,False,False,1646512614.0
t7hgpl,hzhs4lb,t1_hzhrm74,look up leetcode 221. it’s basically the same problem.,1,0,0,False,False,False,1646512840.0
t7hgpl,hzhsitv,t1_hzhs4lb,"No they're not, the constraints are different. The approach that works there won't work here.",1,0,0,False,False,False,1646513014.0
t7hgpl,hzhsvqa,t1_hzhsitv,True. The number of trees is very low in this one.,1,0,0,False,False,False,1646513174.0
t7934c,hzg6ld6,t3_t7934c,"You have to find your own purpose! It's a self discovery journey, no one can come here and tell you what you'll able to do with the information. Find your own path.",12,0,0,False,False,False,1646486837.0
t7934c,hzg5z4i,t3_t7934c,"It’ll make you better at the theory of programming which could potentially get you a good job, particularly if you back it up with a degree.",2,0,0,False,False,False,1646486476.0
t7934c,hzgbena,t3_t7934c,"Generally studying computer science isn't as much about programming as the theory behind it...like a music degree, you aren't taking guitar class.  But like a music degree, you learn the theory and then picking up programming languages is easier.  And you're exposed to different ways people write software...functional programming, object oriented programming, etc.  You don't need a computer science degree to write software like you don't need a music degree to write music, but it looks good on paper and is the foundation you're meant to create upon.  Algorithms, data structures, performance analysis, are all used constantly, so it's not like you're learning calculus to write accounting software.  All in my opinion of course.",1,0,0,False,False,False,1646489515.0
t7934c,hzgjds6,t3_t7934c,"Most college degrees (and as a result, the courses that make up a degree) are there to teach whatever ""underlying theories"" there are to a subject (so far). So, what's does ""theory"" mean?

* Computer Science teaches how to build a CPU, how compilers work, how graphics work, but they don't tell you \*how\* to build or \*what\* to build.
* Music teaches musical theory and provides some tonal/sonic training, but it doesn't tell you how to make an instrument and use it or what kind of music to create.
* Mechanical Engineering teaches thermodynamics (heat flows) and fluid dynamics, but it doesn't tell you \*what\* to build or what materials to use.

For some people going theory->applied or practice is easier. For others, they have an easier time going applied/practice->theory. The latter is usually more work though.

One more analogy that can make things more obvious is that you can learn about how to build an engine, how gears work, and the materials that make up the moving parts of a car. But none of that teaches you how to be a good/great driver. And the reverse is somewhat true as well.

And again, being ""well rounded"" in the subject with both theory and practice is good for many, but generally isn't true for everyone.",1,0,0,False,False,False,1646493454.0
t7934c,hzgjkga,t3_t7934c,"You can start programming today and your journey is probably going to be very enjoyable, even profitable. The work you put in a CS degree is equivalent of a tall, fast vehicle with which you can travel faster while at the same time see farther and better.",1,0,0,False,False,False,1646493540.0
t7934c,hzgphwd,t3_t7934c,"A computer is a machine that can do simple yet mundane tasks such as math very quickly. These tasks are extremely tedious for a human, and can be prone to errors. By combining simple mathematical operations with a little bit of human creativity, we can create functions and applicatons that take do many different things - anything as simple as a function that determines whether a number is prime to a program that can tell you, with a certain degree of confidence, the best move in a game of chess. We can create an application of any level of complexity by combining these functions and other applications. 

I might be biased, but I think everyone should know basic scripting and programming fundamentals.",1,0,0,False,False,False,1646496199.0
t7934c,hzhjmr7,t3_t7934c,"I liked the use of the word ""purpose"". This is what I feel everyone should ask to self before learning anything.",1,0,0,False,False,False,1646509026.0
t7934c,hzg9hpi,t1_hzg6ld6,Exactly. Finding yourself is key to finding purpose and it often takes time.,2,0,0,False,False,False,1646488486.0
t7iah6,hzhsxv2,t3_t7iah6,Depends on what you think is fun. They’re all in heavy use and beneficial,1,0,0,False,False,False,1646513200.0
t6qcd8,hzcvare,t3_t6qcd8,"I believe that is generally the point of chapter problems in some texts. You may understand what you are reading but more intimate understanding usually requires putting what you learn to work. It's not expected that you know the answers off the top of your head nor be able to find the answers be directly in the text. It's expected that you will spend some time on it and may need to look elsewhere. 

In general, when you are stuck, reviewing the questions and then sleeping on it can do wonders.",46,0,0,False,False,False,1646423461.0
t6qcd8,hzd91e2,t3_t6qcd8,"I've seen problems included in textbooks that were actually later used in subsequent proofs (math textbooks). At a certain point, they stop treating you like a noob and start treating you like a potential peer, and handing you things that once upon a time stumped professionals. They expect you to try your hand, maybe succeed, and if you fail ask your professor.",13,0,0,False,False,False,1646428716.0
t6qcd8,hzd1lqw,t3_t6qcd8,"I think you will need to learn strategies to solve problems like this. If you think about a career in the field, you get paid to solve problems that don't have the answers available! The game is to solve puzzles by taking what you do know and seeing what else you might need to get to the solution. As another post said, you look at other sources. But for that to work, you have to figure out what to ask!

The good news is that you can learn the skill by doing it. Practice gives you patterns that you recognize. 

Don't give up. All of us are stupid until we put in the effort. And there is always a new problem that makes you feel stupid until you crack that one too! Enjoy the journey.",7,0,0,False,False,False,1646425885.0
t6qcd8,hzdwcj2,t3_t6qcd8,"Roughly:  All it takes to feel you’ve understood something is not to notice anything you can’t make sense of.  Thus, a shallow reading or otherwise limited, or even incorrect, perspective will give the sense of having “understood” something.

This is a large part of *why* we have questions: to force us to see how deeply we’ve understood.  What connections have we made?  What implications have we appreciated?  What subtleties do we have a grasp of?

I believe, in fact, that a major feature of students that gain heavy understanding is: they *ask themselves questions * as they learn.  They try to frame what they’ve learned multiple ways and see if it still makes sense.

That you are having trouble with questions doesn’t mean you are incapable of deeper understanding, but it likely means that you don’t, yet, have said deep understanding.  And this is a reason to engage with such questions more — not only to test what you know, but to learn how to learn.",3,0,0,False,False,False,1646438107.0
t6qcd8,hzetphh,t3_t6qcd8,">I fully understand what the text is saying and it makes sense to me
 
As someone who teaches computer science as part of their job, my experience is that this is not the case if students aren't able to nail the exercises around it. I can give a bunch of explanations and definitions to students, have them parrot them back to me, even in their own words - but they don't actually 'get it'. Programming (not computer science) is a practical discipline - the explanations, the theory, they are watering the dirt, not growing the plant. If you can't do the exercises, you don't understand the material well enough (and you might not be *ready* to understand the material well enough).
 
The issue of solutions not being found in the text is the point - there's no value in just memorising solutions that are already there, if that was all programming was AI could do it. It's about being able to tweak, combine, refine your 'toolkit' of solutions in new ways, because none of the problems you'll encounter in the 'real world' will have ready made plug and play solutions to them either.
 
For your specific questions:
 
>Are you stupid
 
No, you are inexperienced and lack problem solving skills. That's fixable - this is something the 10,000 hour rule applies to very well. Keep at it, and far in excess of the mandatory hours your course tells you to do. There are loads of problems to solve on any number of websites, you should be doing this every minute you can bring yourself to spend until you get comfortable with it, then your education (and presumably later career) will be much easier and have a lot more downtime).
 
>Are our courses too shallow
 
No, a course that teaches you every problem and chapter is worthless, because what the hell would you do when xyz thing comes out 6 months after you graduate? Go back to school to learn it?",3,0,0,False,False,False,1646453360.0
t6qcd8,hzdgkf5,t3_t6qcd8,"You are enrolled in a degree which is about mathematical problem-solving, it is expected that you do not know the answers to problems and it is expected that you will not find the answers to problems on your own.

As you said ""10-20% are quite normal"". I imagine those are the ones you solve intuitively (since you state the others are solved ""unintuitively""). The first 10% of those will usually be the ones who weed out the students who are not meant for a particular field. The 20-50% range are meant to challenge the average student. Most of us fall within this category, and most of us struggle at one point of another, have to review the material several times, maybe go to bed a day or two or even a week feeling frustrated with a particular problem or theory.

Beyond that, we encounter diminishing returns. 60-70% are for above-average students to intuit. They will stuggle nonetheless, but get there without help or not as much help as others. Beyond that, you will be considered anything from brilliant to gifted.

If you breeze through the most complex problems, you're a genius, somebody like Gauss, who frustrated his math teacher immensely because he was a child prodigy beyond anyone's level (or Terence Tao in contemporary measure, if you prefer).

So you'll guess it's perfectly normal to feel the way you do. To me, it's exhilarating not knowing something about a subject I'm studying. It means all the learning is ahead of me.

All the best.",2,0,0,False,False,False,1646431593.0
t6qcd8,hze99rs,t3_t6qcd8,"Textbooks are often riddled with ""trick questions"" which the author hoped would impart some useful practical lesson but which actually waste the reader's time and give them an unrealistic view of the difficulty of the problem. In programming textbooks, in particular, you tend to find that there is a mixture of ""easy refresher"" problems, ""dig a little deeper"" problems, and ""lulz you'll never get this"" problems. Often the latter give the distinct impression of an author trying to overstate the difficulty of the subject matter, for whatever reason.

I encourage you to keep at it, but only to a point. If a problem seems truly absurd, then there is a real chance the author is genuinely wasting your time and you might be better off finding a different exercise somewhere else (or making one yourself) which covers the same concept. On the other hand, I have encountered one or two textbooks which are arranged in such a way that ""hints"" to seemingly impossible exercises are hidden throughout the book in short unassuming paragraphs (in at least one case I think the author was having fun with this on purpose -- I won't name names).

Finally, I encourage you to understand that there is *nothing* in all of computer science or programming that is any harder to understand than the things you do in your daily life. It is a steep up-front learning curve to feel comfortable with writing programs in general, but once you have been doing it for awhile you look back on your earlier ignorance and chuckle a little (as with all complex things). The concepts in question are as fundamental as counting, language, and logic, which are things we all do (despite many centuries of attempts to characterize these things as being the purview of a subset of people). Don't be fooled by the mystique around the subject matter. These subjects (which seem very artificial at first glance) are piggybacking on really basic human stuff we all do every day. So don't ever feel defeated by a textbook problem. Sometimes they are there just to get you used to digging a really deep and uncertain hole with the tools at your disposal (which is impossible to impart in a way that isn't practical, and is a frequent part of the endeavor at any scale large or small). 

Sometimes I get real stuck on a textbook problem or some detail of a personal project. I take a shower, or go for a bike ride, or do some small side-project for a couple hours, or whatever, and let my mind chew on it. Don't underestimate the intelligence of the people who write your textbooks. They know a thing or two about that feeling, and it is important to get used to it and cherish it. Because when I finish my shower or my bike ride or whatever and get back to my project, or solve the problem, then it is always very rewarding. Not just because I accomplished something hard (laudable but not for its own sake), but because I have increased my understanding and knowledge; my little personal skill-tree is expanded in what feels like a literal way. Not all fields offer this in such regular measure, but it is important to recognize that there are ups and downs to it. I find that it is always worthwhile.

So stick with it! You'll get it.",2,0,0,False,False,False,1646443825.0
t6qcd8,hzf7v5q,t3_t6qcd8,Operating systems was also supposed to be the “hardest” CS class. It’s a 400 senior level class,1,0,0,False,False,False,1646461333.0
t6qcd8,hzfr4fm,t3_t6qcd8,"No.

You're not stupid.

Programming is a language.

Like english, spanish, etc.

It takes time to comprehend.

You may be 'tested' by others, to challenge your 'vocab' but ,

give yourself a break.

You speak broken - \*\* programming language here  \*\* -

I don't make fun of the people that don't speak english the way I do.

It's stupid. They speak a language I don't.

Probably better and more descriptively than I do english.

And; They have their whole life.

Now, they speak mine??

Kudos to that!! Shyt,  I barely dew.. lol

&#x200B;

Programming is a burrito shop.. Quick mart.. Gas Station.. Airport..

It's patience for a message you want to receive, anyone that makes you feel other wise; honestly, has some soul searching to do.

&#x200B;

My message, is return zero. Hopefully. 

Stay wid-it.",1,0,0,False,False,False,1646475449.0
t6qcd8,hzdupad,t1_hzcvare,In grad school I never started homework the day it was assigned. But I always read the problems and thought about a plan of attack at least briefly. It was usually possible to tell which questions would give the most trouble and I usually would think about those in my downtime. Definitely worked for me.,13,0,0,False,False,False,1646437406.0
t6qcd8,hzf7yzc,t1_hzd1lqw,A computer science degree is too teach problem solving NOT coding. So well said!,2,0,0,False,False,False,1646461402.0
t6qcd8,hze164w,t1_hzdupad,I usually read the questions and skim through the chapter before a full read through.,6,0,0,False,False,False,1646440195.0
t7fwog,hzhqp03,t3_t7fwog,"You could take the image file and the modified image file, get their output as a hex dump, and then diff the two files. It might be easiest to start with a simple modification to the image to better see how it affects the data. 

There might be a some audio engineers or someone else that can save you the trouble of investigating it yourself, but in the meantime, that should get you started.

Edit: Interestingly, if you are able to get a diff in this way, then you could probably isolate the difference in values and generate a standalone image that just shows the effect visually.",1,0,0,False,False,False,1646512206.0
t7fwog,hzhrski,t1_hzhqp03,I think actually hex editing is another method to produce glitches in image files for glitch art. I'll play around with this and see what I get. Thanks for the idea!,2,0,0,False,False,True,1646512692.0
t5xsgh,hz8s9ij,t3_t5xsgh,"It depends on what you consider ""Computer Science.""

If you think CS is all about programming, then yes, one does not need to be especially strong in math.

If you think of CS as the academic subject that is taught at the university level -- algorithmic complexity analysis, computability, finite state automata, etc. -- then there is a fair amount of math.

Many people who think CS is about programming are vastly disappointed when they major in it in college.",84,0,0,False,False,False,1646347614.0
t5xsgh,hz965l5,t3_t5xsgh,"I don't understand your distinction between thinking mathematically and thinking logically or algorithmically. logic and algorithms ARE math. 

designing and evaluating algorithms  is its own branch (or several branches) of math. logical arithmetic is one of the foundations of computers theoretically and practically.",17,0,0,False,False,False,1646353654.0
t5xsgh,hz9shq6,t3_t5xsgh,"This question has been asked many times. Most people’s definition of ‘math’ is not broad enough. They don’t realize how huge of a space math really covers.

Look up discrete math, set theory, and predicate logic. If you can program, you are using these constantly.",11,0,0,False,False,False,1646364413.0
t5xsgh,hz8wtmk,t3_t5xsgh,"Computer science is a formal education in mathematics.

This follows logically from the definition of computer science being the study of computation, which is performed through the medium of algorithms. In fact, it is often said that computer science is essentially the study of algorithms, which are mathematical processes for problem-solving (just to be sure, anyone can google this in reputable sources, but I'm sure it's all well-known).

This must be evident to anyone who has studied discrete math. The basic notions of set and function theory therein are the logical foundations for computer science and it is no coincidence that computers operate in a, you guessed it, *discrete* state.

What I believe to be the confounding element in this conversation is that computer science operates through abstraction from low level to high level logic and therefore exempts most computer users, including computer scientists, from confronting the mathematical foundations every time they have to perform a task through a computer. Moreover, when studying processors, we find that A/L units operate with very basic mathematical tasks, hence the impression of simplicity is enhanced.

Additionally, programming languages are acrobatic exercises in semantics and advanced linguistics, so having a padronance of a more linguistically-oriented logic may well aid you in computing variables, but what is guiding your brain in achieving your results in without a doubt a good level of algorithmic thinking (which is just math in the form of step-by-step instructions which eventually terminate upon meeting certain criteria).

Deep mathematical thinking was required to build everything we use today, from operating systems to beautiful videogame graphics. So we study them in computer science because we need a handful of highly mathematically skilled people to keep furthering the science of computation.",9,0,0,False,False,False,1646349568.0
t5xsgh,hz7zr3s,t3_t5xsgh,"Maybe I'm the only one, but I do not agree. I'm constantly noticing that CS and math are pretty much the same. Or at the very least, CS is a very specific branch of math.

 I mean, sure, when you go on developing web apps, or mobile apps you are not going to use much math. But still, is that computer science? Is development computer science? I think it is too reductive to affirm that.

CS involves so many things. From IA, to Web development. Maybe it's because we call some things in different ways, but trust me, many structures and theories we use are pure math. 

The algorithms are pure math. You have probabilistic algorithms, and you have to prove they are correct. Many algorithms use algebraic structures to be proven and implemented. The whole graph theory is pure algebra. Even most of the design patterns in Software Engineering can be represented as algebraic structures.

So no, I really think the opposite. In many, many ways, math and CS are the one and same. I recognise it is maybe not the most common feeling.

I want to be clear, I like this kind of discussions. They are very formative, and they always bring new perspectives",43,0,0,False,False,False,1646335912.0
t5xsgh,hz85x2h,t3_t5xsgh,"IMHO, early application areas of cs (30-40 years ago) and current areas (e.g. machine learning) more focuses on math. But if we speak about web development, I totally agree with you.",4,0,0,False,False,False,1646338340.0
t5xsgh,hz8klh4,t3_t5xsgh,"> I find that thinking logically and algorithmically is far more important than thinking mathematically in CS.

True, I tend to think of it more like stats and infinite series than algebra. 

> people told me that CS was all about math

I'd also contend that there is a significant divide between the layperson, average programmer, and a real computer scientist/developer that is working on low level optimization. There are colloquial ""people"" out there that have difficulty conceptualizing ideas such as graph traversal, exponentiation, or logarithms, for whom these concepts may be too abstract. The average person is still extremely math averse.",4,0,0,False,False,False,1646344347.0
t5xsgh,hz8so70,t3_t5xsgh,CS is math. Once you take some CS theory courses you’ll see that.,3,0,0,False,False,False,1646347789.0
t5xsgh,hz9h0sy,t3_t5xsgh,"OP I don't know what kind of programming you've been exposed to or where you've worked. But let me tell you as someone without the formal education working closely with CE, EE and IoT, I am struggling to compete. The types of stuff I get to work on compared to what my colleagues, if I can even call them that, work on... I might as well be doing plumbing. No offense to the web guys, desktop or GUI or enterprise devs out there, but it's not the same league or ballpark of programming at all. If you're not solving real world problems or real time problems or doing things that directly affect external reality or crushing some wild ass theoretical research stats etc. I hate to break it but you're not really doing computer science far as I can tell. What you are doing is something anyone can learn, possibly even a machine some day. I, a philosophy major drop-out, working as a software developer for 2 years, am living proof.",3,0,0,False,False,False,1646358705.0
t5xsgh,hz8fcg4,t3_t5xsgh,"My math is bad on paper, because I don't get motivated to solve problems with no meaning, and hence my grades suffered on math classes on every level.

On the other hand, if I'm designing something or optimizing am algorithm, I transform to something else, and math becomes much easier.

My math teacher *advised* me to pivot to social sciences, but I have B.Sc. and M.Sc. in CS, and Ph.D. in Software Engineering (because the university didn't have a CS Ph.D. program).",3,0,0,False,False,False,1646342079.0
t5xsgh,hz9duk8,t3_t5xsgh,"not really, but i kinda understand what you mean. writing a function is pretty much just the same thing as solving a math problem, only the formatting and workflow is different. also there's almost always a way to get direct feedback and to check against many different results, which engages other parts of your mind as well i think. i find it much easier to spend hours coding something than working on classic algebra or calc problems or st. even though what i'm doing could be nearly identical. i would think CS falls more into the applied branches of mathematics, as opposed to endless theorems and hypotheticals.",2,0,0,False,False,False,1646357253.0
t5xsgh,hza1mfe,t3_t5xsgh,"Advanced cs is all about math, especially when you get to actual olympiads like USACO - graph theory, matrices, and linear algebra all come into play.",2,0,0,False,False,False,1646369057.0
t5xsgh,hz7q3kr,t3_t5xsgh,"Completely, math is important in niche markets in cs but for general development you don’t really need it much. It’s useful to know it either way but it isn’t integral to being a good dev.

But you do need it extensively in certain things eg. Cryptography, game dev and some other cyber security areas, and it’s honestly easier than writing algorithms",7,0,0,False,False,False,1646332173.0
t5xsgh,hz7ort4,t3_t5xsgh,"Yup, completely agree with this, and I have a buddy who does as well. We're not bad at math, but we just don't like it and it doesn't fit us. We like and are much better at CS than math.",1,0,0,False,False,False,1646331667.0
t5xsgh,hz8mffl,t3_t5xsgh,"I also never really enjoyed my analysis, linear algebra or probability classes. I was good enough at them to get good grades but found the whole ordeal rather unpleasant.

Of course, logic (the kind that underlies formal methods) is also a kind of maths, and highly necessary for whatever I do now. I do rather like this. I was not really aware that this is a proper part of maths until university, and you might not, either. High school math and actual math have a strenuous relationship.",1,0,0,False,False,False,1646345163.0
t5xsgh,hz8z0wz,t3_t5xsgh,"Depends on what you want to do, I personally like computer graphics and audio programming, and math can get really intensive, I'm thankful of the math courses I'm doing and I don't even get the best grades, but being capable of understanding important topics and solving in paper some problems before programming is a 10/10 experience for me.",1,0,0,False,False,False,1646350523.0
t5xsgh,hz9lmx3,t3_t5xsgh,"Thinking mathematically is part of thinking logically, and thinking algorithmically is part of thinking mathematically.",1,0,0,False,False,False,1646360879.0
t5xsgh,hzahf3k,t3_t5xsgh,">only improper formatting

Me too. I would get 90/100 for bad writing. Thus I prefer the keyboard. xD

&#x200B;

edit: I love math and I love solving the problems. I also love studying purely theoretical stuff, but only out of curiousity. I love whiteboards (better hand writing).",1,0,0,False,False,False,1646379016.0
t5xsgh,hzaw49i,t3_t5xsgh,"When talking strictly about end user programming (for example web development). In a sense, it is math, but you just need rudimentarily level of math to be a web dev. You just need basic algebra and you are good to go.

Using CSS and styling, using loops and conditions and other more advanced patterns also, along with technical knowledge of your platform and tools available (web, mobile...) is not something that requires some deep understanding of math. 

It is more important to have a great abstract thinking skills, creative skills along side logic and solving problems skills to be a good developer.",1,0,0,False,False,False,1646390917.0
t5xsgh,hzb6q97,t3_t5xsgh,"Like other have said, studying C's isn't really for getting a job in Software engineering, maybe a trade school that teaches software engineering would be better suited, if you don't want to do the other parts of a CS degree, I know I don't, which is why I switched from a general C's degree, to a Data-Science degree.",1,0,0,False,False,False,1646398167.0
t5xsgh,hzbktqt,t3_t5xsgh,"Get the degree, forget math, I barely use it in my day to day.

Engineers do most of the math then tell us what they want it to to in most cases.",1,0,0,False,False,False,1646405047.0
t5xsgh,hzbll2f,t3_t5xsgh,"… It depends what you consider math.

That ability to think logically and algorithmically matters much more in formal proof based mathematics than it does in say multi variable calculus. A lot of the people who were good at algebra and calculus drop out of the math major in junior year when the formal stuff, and a lot of people who maybe were kind of bored by that stuff tend to switch into the math major around that time,",1,0,0,False,False,False,1646405374.0
t5xsgh,hzbny35,t3_t5xsgh,"Where do I start? I was a high school math and physics teacher for some years and my students always asked me the same: ""why am I learning this algebra and calculus? I'm not going to need it in real life."" Well, let me tell you... We don't teach math in a way to learn concepts, but in a way that you develop certain abilities like logical thinking, spatial reasoning, problem solving, critical thinking, etc.

The problems that you're solving, and as you stated that you did something with astronomy, require SUPER HIGH LEVEL MATH. Do you see it? No! Because somebody else, spent time doing it and simplifying in a way that you can get it. But, if you find a problem and need to solve it... Maybe you need to come back to that math and figure it out where the thing went wrong. 

For me CS is a science, not just programming. It requires all the abilities that you need in math, or physics, or chemistry. Math is embedded deeply in programming, just for you is not clear. Let me give you an example: What is the smallest distance between two points? You're going to answer a line, the answer is right, but not completely. It's a line following the euclidean metric, but if you're in a space that's not euclidean... Well, the answer changes and so all the math behind it. That's how you can do astronomy! 

Anyway, I hope this answers your question. Have a good day!",1,0,0,False,False,False,1646406380.0
t5xsgh,hzbwufl,t3_t5xsgh,"I definitely disagree. CS *is* math. The only way they're separated is if you consider CS to simply be software development, but that is a gross misrepresentation of the field (and still not really unrelated to math).",1,0,0,False,False,False,1646410009.0
t5xsgh,hzc0k56,t3_t5xsgh,"Depends on the problems you're solving. Doing CRUD work, and relatively simple things; yeah. Probably not hard to implement or solve problems. 
Coming up with algorithms for chess, pentominoes, and advanced games; yeah. Fair amount of math involved.",1,0,0,False,False,False,1646411473.0
t5xsgh,hzccrhd,t3_t5xsgh,"Computer science is so much more than math, human computer interaction for example, design, these are incredibly big areas of computer science, there are a lot of computer science majors who don’t do any programming at all once they get out and work.",1,0,0,False,False,False,1646416269.0
t5xsgh,hzfxa7z,t3_t5xsgh,"So I disagree with the framing of this question. Ok, to be fair, it is depending on how philosophical you want to get. Computer science IS math. Isn’t it a science? I don’t really think so. We don’t do much modeling reality in computer science (in my experience). What you probably think of as math is actually arithmetic. At some point math stops being about solving equations (or even working with numbers really) and becomes: here are a couple things we agree on, usually by definition, prove this new thing follows as a result of accepting the first things. Take a look at an automata textbook, doesn’t even try to hide its stripes, it’s very clearly math but also maybe some of the most fundamental knowledge of comp sci.",1,0,0,False,False,False,1646480471.0
t5xsgh,hz7tpvb,t3_t5xsgh,"Unless you wanna work in machine learning, I don't think math is that crucial. Certainly not for web dev, which is what many CS majors are gonna end up in.",-4,0,0,False,False,False,1646333554.0
t5xsgh,hz950su,t1_hz8s9ij,"Yes, but then why is it all grouped together? I understand that CS is established on advanced mathematics on a rudimentary level, but I find that most graduates pursue a job in programming. Software development requires little math (assuming the application of that software is not in a specifically mathematical context), so why is any field pertaining to this so full of math? As stated in my post, the way you work on mathematics and the way you work on computation is vastly different, requiring completely different skills, despite them being so ""closely related.""",-3,1,0,False,False,True,1646353150.0
t5xsgh,hzgzckd,t1_hz8s9ij,Hey I sucks at math bad but I still got through CS in college,1,0,0,False,False,False,1646500376.0
t5xsgh,hz9qwa7,t1_hz8s9ij,"I feel like it also depends on what you consider ""math"" lol",0,0,0,False,False,False,1646363641.0
t5xsgh,hz96ii8,t1_hz965l5,"When you solve pure math questions, you think one way, when you solve algorithms, you think a different way.",-13,0,0,False,True,True,1646353813.0
t5xsgh,hzave12,t1_hz9shq6,I'm gonna go overboard and say that even many social science degrees should have math heavy programs. Being able to make deductions is a skill whose lack in masses fucks countries. (see Turkey),2,0,0,False,False,False,1646390325.0
t5xsgh,hzbi2v5,t1_hz9shq6,"Relevant xkcd

https://xkcd.com/435/",1,0,0,False,False,False,1646403826.0
t5xsgh,hz8s80b,t1_hz7zr3s,"Imagine if all engineers were required to get Physics degrees and do side projects building bridges to get engineering jobs. That's where we are with CS education. So until CS and software engineering split off, computer science will both be all math and no math.",13,0,0,False,False,False,1646347596.0
t5xsgh,hz9470s,t1_hz7zr3s,"I feel like CS was, as you said, reduced to software development in most contexts. Companies who hire devs enjoy seeing a CS degree, so the two are heavily associated. However, I am also referring to the actual process of problem-solving with respect to computational questions. I find that basic mathematics is important; for example, when determining complexity to an application you require the basics of discrete mathematics. But, I find that most work is not math-heavy. I also feel like in some cases, the application of advanced mathematics may be ill-advised for comprehension purposes, but those are a few a dozen. I have worked with AI and found it to be moderately heavy in math, so you definitely have a point there, but I do not find algorithms to contain heavy mathematics. That may just be me, though.",-6,0,0,False,True,True,1646352784.0
t5xsgh,hz9s9iw,t1_hz9h0sy,"I have worked with advanced concepts. My personal projects involve stuff that has to do with space, like radiotelescopes, exoplanet observation, and satellite simulation. I get that it has some math, but not nearly the amount you are expected to learn. It's all just algorithms, which require a wildly different thought process. I think that CS is highly inclusive and expandable; I worked with AI that generates code in the past, and I noticed hard limits on what it can accomplish. Sure, basic UI has been simplified a great deal, but it still requires balance, organization, and expertise. I talked about competing in computing competitions and scoring higher than math sweats, or even matching the scores of experts in the field. Look, I understand that from an academic perspective, math is crucial to CS. But, I also know that learning the math required to solve complex problems is incredibly easy when actually applying it. The thought process is completely different, because the type of question is completely different.

Also, don't put yourself down because of the people you work with. We're all trying, and as long as you do what you love, you will have the passion and persistence to improve in the field. From my experience, the best way to express this would be through personal projects.",1,0,0,False,False,True,1646364302.0
t5xsgh,hz95kap,t1_hz8fcg4,Interesting. Would you say that you ever struggled with math or just with the pointlessness of solving nothing problems?,2,0,0,False,False,True,1646353389.0
t5xsgh,hza55t1,t1_hza1mfe,I did the USACO.,1,0,0,False,False,True,1646371025.0
t5xsgh,hz8dshb,t1_hz7q3kr,"Math is important for lots of computer science, not just niches. Of course most CS graduates end up in developer jobs (and probably had that in mind in the first place) which aren't really computer science. So the actual computer science is niche among computer science graduates.",11,0,0,False,False,False,1646341438.0
t5xsgh,hz7qrxs,t1_hz7q3kr,"I guess that's personal preference, because I actually really like writing algorithms! I worked with lots of math when making physics simulators for fun, but that was pretty much all.",1,0,0,False,False,True,1646332428.0
t5xsgh,hz9dq5u,t1_hz950su,"Programming is often the raw implementation of high level CS concepts/ 

They're bundled together because often, like you said, CS graduates come out and become great SWEs. However, schools still want to teach the CS / math side of things because not everyone goes onto becoming a SWE, some go to grad school and maybe even pursue a PhD where pretty much, it becomes all math (although I've heard they still do programming as well, again programming is the implementation).

&#x200B;

I'd argue that math and computational skills are closely related. They're not exactly the same but definitely related; definitely not vastly different. A similarity I can point out now is that many math, CS, and programming problems are solved using algorithms, ie a set path to solving a problem. Here's 1 example: Row Echelon Form (REF) from Intro Lin Alg, you can totally write your matrix on a paper and solve that way by hand (through an algorithm). Or you can equivalently write a program in pretty much any language of your choice to manipulate a 2d array (a matrix) to do the same thing. These processes are slightly different but at its core is an algorithm.

I'd argue to say that vastly different would be art, where the solution to the problem can take many forms and the skillset in that department is recognizing the different ways you can arrive at that solution and how those ways mean different things. CS Math, etc is always works because it can be proved to work. Another example: in programming we might import / include our favorite unit testing framework and write a test, whereas in math we'd write a formal proof; in art? english? you write an essay.

&#x200B;

Atleast that's sorta my take on it. Like I said I think the main reason is that they still have to teach you the raw CS stuff because they never know who wants to pursue masters and phds. They teach the broadest spectrum of content (obviously with some core lower level classes and choice in higher level classes) so that you can be the most well rounded candidate.",15,0,0,False,False,False,1646357197.0
t5xsgh,hzauudb,t1_hz950su,"This is like saying accounting is not math. All the foundation comes from math, doesn't matter if you are using basic algebra or double integration. In time you'll be able to tell apart the approach of self-taught devs and devs with formal background, and yes that involves learning math, a lot. SWEs use math all the time including, but not limited to these subjects that I am writing in a heartbeat from the top of my head. They just may not know it (they do if they are good): Algorithms, formal logic, computation theory, set theory, discrete math, probability, graph theory, discrete maths, series...

Just because it does not involve calculus doesn't mean it is not math. I mean... [Everything is set theory](https://www.youtube.com/watch?v=fDncO6zkosM)",2,0,0,False,False,False,1646389871.0
t5xsgh,hzahkcx,t1_hz950su,Maybe CS isn’t for you and you should go major in software engineering if you don’t like math.,1,0,0,False,False,False,1646379126.0
t5xsgh,hzbi3wm,t1_hz950su,That’s why university usually offer 2 majors: computer science and software engineer.,1,0,0,False,False,False,1646403839.0
t5xsgh,hz9dxfn,t1_hz96ii8,Do you mind elaborating? I don’t understand your distinction as well. Computational math and conceptual math still fall under the same category.,6,0,0,False,False,False,1646357291.0
t5xsgh,hzble2b,t1_hzbi2v5,"It’s a great one, but no not relevant here. This is saying that math abstracts everything so is at the base of everything.

I’m saying programming is a direct application of discrete math.",1,0,0,False,False,False,1646405290.0
t5xsgh,hzao862,t1_hz9470s,"Maybe you are expecting Math to be a bunch of equations and formulas, and thus you are not realizing that CS is mostly Math and Information Theory (there is a bit of Physics , Linguistics and Psychology here and there too). Algorithms are Math, the logical operations used for flow control in a program are Math, variables are a fundamental mathematical concept, and every operation you do to them is a mathematical one. Sets, arrays, matrices, data structures... all Math. Just not elementary school Math. Also... maybe it is because English is not my native language, but I don't really get the concept of ""heavy mathematics"".",5,0,0,False,False,False,1646384355.0
t5xsgh,hzbgne7,t1_hz9470s,"I unedrstand that point of view, but still, are algorithms cs? Sure, you use them, but one of the first algorithms i can think of is the Euclidian algorithm to find the mcd. I would not say that is CS, that's just an algorithm.

Algorithms go way back in time, when the notion of CS was unthinkable.  Algorithms alone are just one way to solve problems. You can use some algorithms in CS, but I don't think you could use the ""prepare the coffee"" algorithm with a normal computer. As computer scientists we have to understand algorithms, create new ones, and more importantly **prove** they are correct. And the proof, I'm sure you know, is always mathematical.

Now, it is clear we will not use always very complex objects in math (such as tensors, groups, ...), not explicitly at least. But we have to understand the science behind our own algorithms. That does the true difference between a CS and a ""simple"" developer.

This is my point of view, I really think a solid foundation of math is crucial to understand CS, and really, they have so much in common that I'm truly sad that in highschool (I'm from Italy, so maybe that changes in other countries) there is no education regarding CS in parallel to math",1,0,0,False,False,False,1646403186.0
t5xsgh,hz9uhq8,t1_hz9s9iw,"I feel like my math capabilities is the true barrier to entry for advancement in the field. I can solve some of the same problems these guys are solving, but I couldn't solve them on paper and that counts for a lot by my assessment. I think my saving grace is I did really well in the more advanced logic courses in the philosophy program. I basically dropped out a few courses short of a degree to accept the job offer. Been a pleasure to learn things like Karnaugh maps and the Quine-Mccluskey method. I am taking math courses presently to be more competitive, but I don't see how I could work on robotics and IoT stuff without it. Maybe we're not comparing apples to apples here, but I think even some more advanced finance or dare I say scalability issues in web / site reliability could present a similar barrier to entry. Maybe you're speaking more to the courses like Calculus 3 or Advanced Linear Algebra not being required?",1,0,0,False,False,False,1646365384.0
t5xsgh,hzaa3tm,t1_hz95kap,"When I don't see a direct utility of solving a problem, the method of solving it doesn't interest me much, hence I can't learn. However, when math is a way to solve a concrete problem at hand, I'm much more willing to read about the steps I don't understand or do the research required to understand how it all works.

Another thing is, I strongly need to know how everything works together. This need is present since I'm aware of myself. When I can't understand how math works, or what it does, it becomes harder for me to learn & understand.

Most mathematicians and math teachers don't think about that aspect & frankly, they don't care (believe me, I asked), and it makes my life much harder.",1,0,0,False,False,False,1646374030.0
t5xsgh,hzdvei4,t1_hza55t1,"What div - silver and bronze are fairly non mathy, but a lot of plat and some gold are mathy",1,0,0,False,False,False,1646437704.0
t5xsgh,hzaux97,t1_hz9dq5u,Schools want to teach maths / CS because it is not (at least should not be) their function to give everyone a trade.,3,0,0,False,False,False,1646389939.0
t5xsgh,hzbb6ok,t1_hzahkcx,"It's not that I don't like math, I just don't find it the same. Also, SE is also filled with math; not as much as CS, but still.",1,0,0,False,False,True,1646400549.0
t5xsgh,hz9tz2v,t1_hz9dxfn,"I would also like to add that I occasionally work with random concepts in math, and I find it to be slightly similar to the way computing works.

I will often just explore random concepts in math, thinking and reading about them, then coming up with ideas. It's similar, but it's not the same.",1,0,0,False,False,True,1646365132.0
t5xsgh,hz9ta8v,t1_hz9dxfn,"Sure. But, this is the way I solve problems, so it is unlikely to remain uniform with everyone's preference.

When working with mathematics problems I use a combination of established theory and pattern recognition, applying them both.

When working with computational problems, however, I take on a more logical and practical approach.

I may not know how to solve a math question immediately, but I do know how to solve a computational problem immediately (or after thinking about it for a bit). I know the general path I will take to solve the math problem, but I have already composed and executed all the rough code with the exact returns I expect to see with a computational problem. The rigorous and applicable approach to computing is, at least for me, impossible with math.

With math, I would sail out an approach I think MAY work, and go from there.

With CS, I already know exactly how I will do a given question. There is no writing code and thinking along the way.",-2,0,0,False,False,True,1646364803.0
t5xsgh,hzbe2jk,t1_hzao862,"Sorry, I should have clarified. I am referring to the type of math you learn in third-fourth year university.

I find that CS just has basic mathematical relations which compound into a higher degree of complexity. I realized from reading this that it is really mathematical, but it certainly does not feel that way. Are you sure that sets, arrays, matrices, and data structures are not elementary math? I guess not. But I find it a lot easier to work with these concepts in software than I do on paper.",1,0,0,False,False,True,1646401972.0
t5xsgh,hzav8iy,t1_hzao862,"Probably they refer to calculus by ""heavy mathematics"" Integration is a hard to grasp concept. That doesn't mean CS is not heavy, but... different.",1,0,0,False,False,False,1646390197.0
t5xsgh,hzavj2p,t1_hzaa3tm,This is a problem with your country's education system and how they are greate at making students hate maths from elementary school.,2,0,0,False,False,False,1646390439.0
t5xsgh,hzbe2tz,t1_hzbb6ok,"Software engineering doesn't have math. Where are you getting that from? Maybe at most you would deal with some mild Big O analysis but nothing strenuous. 

Software engineering is the art of designing and structuring large programs (see: design patterns).",2,0,0,False,False,False,1646401975.0
t5xsgh,hzawjwe,t1_hz9ta8v,"I'd say the distinction is you know programming better than math, or rather you aren't doing hard enough programming tasks.",6,0,0,False,False,False,1646391265.0
t5xsgh,hzblkmz,t1_hzbe2jk,"Nothing to be sorry about! 

Regarding those concepts being elementary math, you are actually right. I recall learning about these things not much later than the basic operations. I taught a few CS courses in different topics and realized that most people (including myself, retrospectively) struggle with advanced Math because the fundaments (e.g. sets theory)  are somehow known but not clearly understood. I guess the problem is that we expect elementary school kids to understand abstract things way too early.",1,0,0,False,False,False,1646405369.0
t5xsgh,hzazutl,t1_hzavj2p,"I don't *hate* mathematics per se. It just didn't make sense. Not the mechanics, just what I'm working on.

When I did some research on that issue, I've found that there's a fundamental view difference between me and other people. I'm interested in the whole picture, other people just wanted to go through the hurdle and find the solution.

This is not limited to maths for me. My brain works like that. I need to be able to see the whole process and innately understand it. This is why social sciences also feel too boring for me. The mechanics are too intangible. ""When you do this to masses, they react like that, most of the time."" So, Why? ""We don't know, and frankly we don't care"". OK, I'll see myself out, then.

What I like about computers, that I can think about every level, and dig in a tangible way if I want to. Also it's a creative medium, a kind of art for me, but that's a discussion for another time.",3,0,0,False,False,False,1646393801.0
t5xsgh,hzbea4a,t1_hzbe2tz,"If you take it in university, the timetable is the exact same amount of math as CS. Maybe a little less.",1,0,0,False,False,True,1646402071.0
t5xsgh,hzbc6ui,t1_hzawjwe,"I have solved very advanced programming problems. I work on impressive personal projects and write olympiads (like how I wrote in the post). That is why I am so confused by this. I am definitely more familiar with computation, but I also wanted to point out that the way you solve problems is different. What I have concluded from this post is that people establish CS as the field you study in school, not as the application of logic. I don't solve problems with pure math. I don't bust out a notebook and start jotting down formulas and solving that way. If I need a visual aid, it looks like abstract elements connecting together, not as formulas. I understand your position with the assumption that I am simply not doing difficult questions, but I am.",1,0,0,False,False,True,1646401059.0
t5xsgh,hzbflc7,t1_hzbea4a,"Oh I think you mean the software engineering specialization. That's not software engineering, that's just a computer science major track with a focus on design patterns and the software design process in your junior and senior years. 

Actual software engineering doesn't use mathematics to any big extent.",3,0,0,False,False,False,1646402697.0
t5xsgh,hzcp7sj,t1_hzbc6ui,"I don't really see how I don't need to think about the problem and I do very advanced programming problems can simultaneously be true, especially olympiad like questions which as a person who did both CS/Math competitions are very similar.",1,0,0,False,False,False,1646421139.0
t5xsgh,hzcdath,t1_hzbflc7,What subjects come under software engineering specifically?,1,0,0,False,False,False,1646416483.0
t5xsgh,hzdu18a,t1_hzcdath,"Usually things like software engineering (usually this is the actual course name), database systems, operating systems, cloud computing, or any other CS classes that focus on big-picture design philosophies.",1,0,0,False,False,False,1646437119.0
t5xsgh,hzeufqi,t1_hzdu18a,And where does web development fit in that? Is it also like one branch of SE?,1,0,0,False,False,False,1646453735.0
t63qk4,hz9q5c1,t3_t63qk4,"The difference is that your Java skills can be learned on your own time fairly trivially. Java isn't hard to pick up. Programming languages are the easiest part of programming after typing.

Finite state automata? That underpins many systems that you take for granted. Most distributed systems, for instance, such as DropBox or Google Drive. But, also on a smaller scale. You use finite state machines to design complex embedded systems running on a tiny microcontroller. Depending on what you're working on, control theory can come into play.

Oh, compilers too. Do you want to learn how compilers work? What about writing your own? Learning the theory behind this absolutely will make you a much stronger programmer, and finite state automata is the foundation of compiler theory. Crack open the Dragon Book and take a look for yourself.

As for linear algebra? It's absolutely mandatory for gameplay programming, graphics programming, audio, machine learning, and statistics, to name a few.

Logic, Boolean algebra, and discrete math? That stuff is used literally every day. You can't write software without either of those things. Not much more needs to be said here. Even if you're just slinging Java

Set theory? It underpins all of database theory. You wanna work with SQL databases? Good luck writing performant SQL queries that don't bog your servers down without deeply understanding set theory. Inner joins? Left joins? What even are those without set theory and relational algebra?

Just today, I zeroed in on how to create a custom memory-efficient *and* fast composite data structure involving a bidirectional hashmap (you don't see those every day), all because I remembered my time complexity and algorithm analysis theory from school. O(1) instead of O(n) for the win.

The more theory you know, the harder problems you can work on.

Every time you ask, ""when will I use this"", you fail yourself. Adding more tools to your tool belt makes you a stronger engineer.

Computer science is one of very few college degrees where the vast bulk of the course material is directly applicable to nearly all aspects of the job.

If all you want to do is, say, frontend web development, or basic CRUD apps that people outsource to places like India, then skip the theory.

If you want to do more interesting stuff, learn as much theory as you can stomach.",38,0,0,False,False,False,1646363282.0
t63qk4,hz9h6ad,t3_t63qk4,"I have benefited from knowing some amount of theory, mostly when it can be applied to algorithms in a single step. For example, organizing code into a state machine means that the program can never be in an undefined state, which in general is either more difficult or impossible to prove.

I personally think that knowing how something works theoretically helps you use it correctly. For example for matrix algorithms I won't have to just memorize the library's specifications, I'll have a little bit of intuition for what runtime and correctness properties it will have. Too much knowledge is unnecessary, but a moderate amount can decrease mental load.

Some people will say that knowing the theory will improve your ability to program because it generally improves your ability to think abstractly. However, I've seen plenty of good programmers that learned specifically to program. You can learn the abstraction needed for software construction directly, by learning various kinds of software construction methods and their properties (a subject I think is undertaught in many universities).

TLDR If you don't like theory for its own sake, you can treat it as abstraction training, a mnemonic device for previously-implemented capabilities, or in some cases directly as tools of software construction. There will be some things you still have to just push through because they're part of the curriculum.",4,0,0,False,False,False,1646358776.0
t63qk4,hzczf48,t3_t63qk4,"If you want to study after undergrad or go into research, they will be very important. Otherwise they are just useful  pieces of information that might help.

I'm still an undergraduate so I can't speak much on after graduating but I tutor at a local school and they are all working on a project that suits them. One of the students is working on some optimization project for a game but he didn't quite understand why his code was running slowly. I open the code up and he has so many nested for loops, so I ask him why. He says he's running through all the possible permutations of the variables he's optimizing. Obviously that's a horrible idea, since you can show quite quickly that the number of permutations is huge. He asks if there is a better way to go about it, so I showed him how to phrase it as a calculus problem and use that to find solutions. 

A lot of it is going to be very domain specific, but if you can spot a place where to apply some theoretical stuff you know to find a better solution, or at least show a current solution is bad.",1,0,0,False,False,False,1646425043.0
t63qk4,hzhaecz,t3_t63qk4,"When you first start, all the difficulty is usually in the applied stuff. Frankly, lots of high level concepts in early computer science education aren't that hard, the difficult part is translating them into running code, which may be challenging since you aren't aware of the syntax or the tools at your disposal.

Once you've become half-decent at programming though, and start tackling meatier applications, understanding the high level stuff becomes exponentially more important. You can't understand everything about microprocessors, networking, etc., but knowing how these systems interact with each other helps you to diagnose issues and, more importantly, having mathematical models to analyze these systems.",1,0,0,False,False,False,1646505059.0
t63qk4,hzc7hfp,t3_t63qk4,Yes.,0,0,0,False,False,False,1646414197.0
t63qk4,hzap9g4,t1_hz9q5c1,"Thank you for this reply. 

While attending university I was always willing to study all those ""useless"" concepts that don't directly affect your programming skills. I've always understood the importance of knowing everything in depth. The more you know, the more you can understand, the better you can reason about things, the more overall control you have. 

But during my whole lifetime I've always been surrounded by programmers complaining: ""Why should I know how a TCP frame is composed? I will never use it. Why should I care about the fetch-decode-execute cycle? Why do I have to know how to solve integrals? It's useless to study algorithm complexity, nobody will ever pay me for that, nor will for writing a binary search tree. Why all that geometry? It's just a waste of time"". I've read countless threads stating that you can learn programming alone and that having a degree is useless.

This is the first time I actually read somebody explaining the importance of this all.

Thank you!!",6,0,0,False,False,False,1646385221.0
t63qk4,hzajahr,t1_hz9q5c1,you know your stuff,1,0,0,False,False,False,1646380420.0
t65n1n,hzad6p7,t3_t65n1n,I'd like to know as well. I'm getting destroyed in comp org. My grades are fine but I just feel it's hard to grasp,1,0,0,False,False,False,1646376071.0
t65n1n,hzapofi,t3_t65n1n,"For architecture I suggest you to start from the basis with the book ""But how do it know?"" by J Clark Scott. There are a lot of youtube videos about it like this playlist : [https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl\_eE6yzA](https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl_eE6yzA)   in which you can understand how computer really works. You'll see how ram memory is made and organized and how CPU is made in order to execute instructions. 

If you wanto to get a quick view try this: [https://youtu.be/1BidIYu6Cls](https://youtu.be/1BidIYu6Cls) 

I think with this you have the basis and you can go deeper",1,0,0,False,False,False,1646385567.0
t65n1n,hzb2n0e,t1_hzad6p7,Honestly geeks4geeks has a good practice section. They also go over all of the different data structures and algorithms so you know where to start from.,0,0,0,False,False,False,1646395714.0
t5phed,hz6fbbs,t3_t5phed,"This statement is treating all of software engineering as if it's a single technical problem.

Engineers today spend relatively little time in the ""brood over tricky algorithms, invent clever optimizations"" stage. The ""find and eliminate bugs"" stage remains central, alas. That won't be solved with ""provably correct"" code because in practice a huge proportion of bugs are really bugs in specifications as much as anything else. And figuring out the right specifications is only partly technical—this aspect of engineering is often more a matter of business, design, and diplomacy.

Frankly, the very first sentence (""programming is harder than truck-driving"") already makes me skeptical. Truck driving is greatly underestimated, and for the same reasons: people focus too much on computer-friendly parts of the job.",36,0,0,False,False,False,1646312907.0
t5phed,hz6f34m,t3_t5phed,[Mandatory reference ](https://www.commitstrip.com/en/2016/08/25/a-very-comprehensive-and-precise-spec/),27,0,0,False,False,False,1646312784.0
t5phed,hz6b18v,t3_t5phed,"How will we specify a problem for the computer to solve? Perhaps via some representation that can be understood by both humans and machines. Maybe we call these representations programming languages?

Jokes aside, if a machine is trained on flawed data, it’ll produce flawed results. It absolutely can make mistakes as it lacks any empathy or awareness of its surroundings (see racist Twitter bot).

A neural network might be able to estimate faster than we can, but it won’t be able to further our understanding in any meaningful way, because it’s not intelligent. In the recent application of DNNs to [Density Functional Theory](https://arxiv.org/abs/1809.02723) you’ll see we’re able to make much better predictions thanks to machine learning, but we’re no closer to reaching a deeper understanding of the underlying laws of nature.

Most of the software-building people I’ve worked with do a lot more than data entry. I’m hopeful we’ll be able to continue to augment human intelligence by eliminating drudgery, by automatically optimising more and more of our systems, and by surfacing data and insight to enable better decision making.

Creativity, communication, understanding, and real thinking require a true artificial intelligence, which has been ten years away for at least thirty years. Until then, nature has provided us with a spectacular solution: a well-fed team of motivated people.",43,0,0,False,False,False,1646310498.0
t5phed,hz6hf5q,t3_t5phed,"AI is far from taking over anything remotely close to programming.

You could have it produce boilerplate code, akin to copy pasting snippets of code from the docs or StackOverflow, but too much of most projects require original thoughts which is impossible for our current ML models to do easily.",12,0,0,False,False,False,1646314003.0
t5phed,hz6bfq1,t3_t5phed,That would be a revolution of incalculable consequences and I disagree with the timeline even if my only argument is that I heard it in various form over the last 45 years.,10,0,0,False,False,False,1646310735.0
t5phed,hz6gt0s,t3_t5phed,"Yes AI never makes mistakes, it would never crash an airline into the ground.",7,0,0,False,False,False,1646313690.0
t5phed,hz6kpsv,t3_t5phed,"Well, I rarely have to write assembly code and if I have to do that, it is part of larger, more abstract code. The compiler is already good enough to understand what I want. My code is just an explanation of the problem that I want to solve.  But in a way, a compiler is already parsing my nonsense and creating something useful out of it.  


Perhaps the way we write code is going to change in the future but I don't see an AI listening to ordinary humans and then finding solutions.The thing with AI is, we don't really have that artificial intelligence. There are networks that are great at finding patterns, and there are scientists who know how to apply and interpret statistics. It's nothing that just works without anyone with some expertise behind that.  


With that said, learning to code is a way to train a wide range of skillsets, such as problem formulation, math, abstraction and handling frustration. Even if writing code as today is not what we are going to do in 10, 20 or 30 years, these skills are going to help to switch to a different level of problem solving.",6,0,0,False,False,False,1646315612.0
t5phed,hz69rc1,t3_t5phed,"I don’t agree with his timeline but I do agree with the vision. The devil comes in the part about specifying the problem you want to solve in a way the AI can understand. For complex projects that part may be as difficult, if not more difficult, than just doing the work itself.

However as AI improves in terms of how we communicate and specify things, and the AI itself becomes more able to understand subtle language through better NLP, we will likely get to a point where you can describe your project in fairly simple natural terms, as one would to other humans, and the Al will be able to comprehend that and write the software. 

I do think when this shift happens it will happen quickly, there is just a huge amount of money to be saved by corporations if they can use an AI for the brunt of their development work. The incentives are huge.",3,0,0,False,False,False,1646309729.0
t5phed,hz6wd1z,t3_t5phed,"The same as human drivers, you need to solve some liability issues. Also, the elephant in the room is application support.

If a user finds a bug in production, let's say a miscalculation in an income statement, this usually happen during the closing process, so you have a few days to fix it. Usually, a developer debugs the code, propose a fix, the user test it and the fix is deployed to production. Sometimes the solution is developed by a 3rd party support contractor and an SLA is involved.

If the code was written by AI and the developer can't understand it nor even debug it. If the tables and logs can't be audited by the government or by an external company, how can we be sure that the underlying code is not moving money to another shady account, or committing fraud?

What if (for some random reason), the AI can't comply with the SLA and the bug is not fixed in time? If income statement AI is a third-party solution? will the AI company pay the fines to the government? will this be regulated by the support agreement? how can the government audit that and prove fraud if nobody understands the code? How can we assess that risk?",1,0,0,False,False,False,1646320676.0
t5phed,hz6z5m6,t3_t5phed,"He's both right and wrong. Repetitive, relatively low skill programming tasks are becoming easier with AI or GUI-based tools. Github copilot, for instance, basically writes your SQL queries for you. Software like EditorX can make your front end web app and MS Power Platform can do full stack. ERP companies already have a lot of their release configuration done by non-programmers. Keep in mind that none of this applies to academic computer science.

However, all of this has been happening for a minute now and it already has cut into the market for low skill programming jobs. He underestimates how obtuse the problems can be in industry sometimes, and ""out of the box"" solutions don't always cut it, especially for really innovative projects. Somebody still needs to make design specs, implement, and do verification.

Realistically, I think many people are overbetting on the growth of the tech sector for employment prospects and career progression; it seems like everyone with a pulse ""lurns 2 code"" these days, but a lot of the jobs that employed so many people during the first and second dot com bubbles (data cleaning, basic web dev, DBAs, on prem networking installations, I could go on...) are already disappearing and emerging fields like ML engineering aren't going to create more jobs than they replace, so all the mediocre ""software engineers"" hoping to make fat stacks making react apps for startups are going to be disappointed. That said, there will be niche tasks for skilled programmers for the foreseeable future.",1,0,0,False,False,False,1646321793.0
t5phed,hz7f2ly,t3_t5phed,"I laugh at the thought that the systems built by AI will be bug free. The AI is not always going to know something it hasn't seen before is a bug, just like every other programmer. The fact that we will still have to specify the requirements to the AI means there will be mistakes/misinterpretations that result in bugs (exactly as we have today). Additionally, the AI systems will be flawed from the beginning because they were built by people. We could certainly have the AIs iterate on better versions of themselves but I think that will be asymptotic, the harder the problems get, the longer it will take to solve them.  


I believe it will certainly be revolutionary compared to what folks are dealing with today but I think the reality will fall far short of this prediction.",1,0,0,False,False,False,1646327998.0
t5phed,hz7rg4w,t3_t5phed,"Computers are already ""self programming"" compared to the '70s or '80s. All the libraries you can just import into Python to do machine learning from you are practically cheating to my mind.

You still have to define the logic, of course.",0,0,0,False,False,False,1646332683.0
t5phed,hzachpe,t1_hz6fbbs,"I think the hard part of programming is innovation. I think that will be the toughest hurdle. Creating solutions can be tricky. Take graphics for example, had we tried to brute force our way with earlier techniques, it just wouldn't have worked. We needed to come up with estimations of reality. I don't think an AI will have an easy time of deciding the balance.",1,0,0,False,False,False,1646375604.0
t5phed,hz6l1op,t1_hz6f34m,Nice one. I wrote a lengthy reply and this comic tells the same story in four panels.,4,0,0,False,False,False,1646315771.0
t5phed,hz82msa,t1_hz6f34m,Lol. Yes THIS. Basically human coders will never go away. They will just move to higher and higher level languages with more and more complex compiler/AI running underneath to map the intentions into actual operations. Data scientists fulfill this role today in AI.,3,0,0,False,False,False,1646337048.0
t5phed,hz8ho53,t1_hz6b18v,"Most of the qualities you describe on your last paragraph there were considered first barriers to playing high level chess, then to high level Go. Basically whenever a task is not yet achievable by an AI it’s considered to require intangibles of “creativity” or “understanding”, then once it’s been solved by a computer it immediately becomes written off as “not real intelligence” or “not real AI”.",-6,0,0,False,True,False,1646343099.0
t5phed,hz7p7sq,t1_hz6bfq1,"I agree. It's a pretty big leap to go from ""most basic algorithms will be replaced by machine learning in the near future"" to ""programming will be replaced by maching learning in the near future"". I feel like the former might have some validity in his argument but the latter definitely does not.",3,0,0,False,False,False,1646331839.0
t5phed,hz7jl2e,t1_hz69rc1,"No it will never happen. We can’t even prove correctness in most software much less generate a solution from a specification. 

We cannot even agree on what constitutes a specification.

Most software exists to serve people and the form of the solution is based on human behavior and understanding which is a moving target.

Are you going to tell me you are going to auto program a good text editor?  A sales website for computer systems? A digital audio workstation? A new kind of social network? 

This is nonsense. I work in e-commerce with lots and lots of personalization instrumentation and behavior analytics. When people are involved there is no specifying a solution up front. 

Programming is art, not science. You cannot automate art.",1,0,0,False,False,False,1646329702.0
t5phed,hzbg37u,t1_hz6z5m6,"> so all the mediocre ""software engineers"" hoping to make fat stacks making react apps for startups are going to be disappointed.

This seems to line up with what I've read elsewhere on this topic, that the tech profession is transitioning into a pyramid-esque structure where only the most experienced programmers will end up cornering a substantial share of the pie, while entry-level devs will be left in a state barely better than a gig economy. As someone who just started learning programming this past year, I'd love to know which sub-fields and specialties in your opinion stand a good chance of being 'future-proof'?",1,0,0,False,False,False,1646402927.0
t5phed,hz906ul,t1_hz7jl2e,"Proving correctness is obviously not a requirement for writing software, as you said we can’t do it now for most cases. Hence the downing of two Boeing airlines recently. 

You are looking at this from too narrow a perspective. Maybe your job will never be automated, especially of as you said your work is a moving target that even your managers cannot agree on.

But there is a lot of programming that is more static, take for example writing parts of the C++ standard library. There are many many algorithms in that standard that are well described and don’t change over time, they are easy to specify and yes an AI in the near future will be able to write those algorithms. 

Or as I just mentioned above software for vehicles or industrial systems, where we already are legally required to provide very clear specifications. 

There is a difference between an artist painting a watercolour and a painter coming to your house to decorate your bedrooms. Just because we can’t automate the first doesn’t mean the second won’t happen, and will still save a huge amount of development time across.",1,0,0,False,False,False,1646351030.0
t5phed,hz9lnfc,t1_hz906ul,"And how will you specify to the level of detail required those algorithms efficiently? By the time you have done it you’ve coded it.

Sorry, I don’t buy any of it. The amount of software you can generate is proportionally about the same as the amount you can prove correct.

The building blocks get bigger, sure. But above a certain size they stop being generally useful. We still build houses with bricks. Why?",1,0,0,False,False,False,1646360888.0
t5phed,hz9vrj6,t1_hz9lnfc,"The first point is not true, for example.. take [std::partial_sort](https://en.cppreference.com/w/cpp/algorithm/partial_sort). This is an algorithm takes a range and takes a middle point, and modifies the range such that all elements less than the middle are before it and all elements larger are after it. That's the only specification.

It's not the hardest to code, but still would take a human developer quite a lot of time to do efficiently and well. Certainly a lot more time that it takes to specify it's behaviour in natural language. And many developers would not even be able to do it well, it requires a lot of experience in the language.

Here is the actual implementation:

    namespace impl {
    template<typename RandomIt, typename Compare = std::less<typename std::iterator_traits<RandomIt>::value_type>>
    void sift_down(RandomIt begin, RandomIt end, const Compare &comp = {}) { // sift down element at 'begin'
      const auto length = static_cast<size_t>(end - begin);
      size_t current = 0;
      size_t next = 2;
      while (next < length) {
        if (comp(*(begin + next), *(begin + (next - 1))))
          --next;
        if (!comp(*(begin + current), *(begin + next)))
          return;
        std::iter_swap(begin + current, begin + next);
        current = next;
        next = 2 * current + 2;
      }
      --next;
      if (next < length && comp(*(begin + current), *(begin + next)))
        std::iter_swap(begin + current, begin + next);
    }

    template<typename RandomIt, typename Compare = std::less<typename std::iterator_traits<RandomIt>::value_type>>
    void heap_select(RandomIt begin, RandomIt middle, RandomIt end, const Compare &comp = {}) {
      std::make_heap(begin, middle, comp);
      for (auto i = middle; i != end; ++i)
        if (comp(*i, *begin)) {
          std::iter_swap(begin, i);
          sift_down(begin, middle, comp);
        }
    }
    } // namespace impl

    template<typename RandomIt, typename Compare = std::less<typename std::iterator_traits<RandomIt>::value_type>>
    void partial_sort(RandomIt begin, RandomIt middle, RandomIt end, Compare comp = {}) {
      impl::heap_select(begin, middle, end, comp);
      std::sort_heap(begin, middle, comp);
    }

Which do you think takes longer, the specification or coding the algorithm? and there are tons of examples like that.

I don't understand your point about the amount of software you can generate being provably correct, it's not true at all, proving software correctness is exponentially harder than writing the software. That's why we don't do it in 99% of use cases.",1,0,0,False,False,False,1646366007.0
t5phed,hzacvhj,t1_hz9vrj6,"Which do you think is harder.  Writing a program that understands your specification and generates code to fulfill it?  Or writing the program directly?

You're living in fantasy land here.",1,0,0,False,False,False,1646375858.0
t5phed,hzanfg3,t1_hzacvhj,"lol do you understand ML/AI at all? Is obviously harder to write the ML, there are hundreds of people working on it as we speak in many companies, most notably at Google. It’s not easy, it may not even be done within the next couple of decades. The point is once they do figure it out that same ML or similar can be used repeatedly, that’s the point of AI/ML. Obviously building that system for the first time is hard and may take decades of work, but then you also save dacades and more into the future once you have tools like this",1,0,0,False,False,False,1646383694.0
t5phed,hzdh9ip,t1_hzanfg3,"Yes, that's my job. 

And no, I don't buy your science fiction.",1,0,0,False,False,False,1646431870.0
t678u0,hze73yv,t3_t678u0,"I agree with Linus. Stepping up to a better C standard will certainly improve the code.

Also, if memory serves, there have been attempts at writing a kernel in C++, all of which I believe never really showed any substantial improvement. Best to keep critical software (such as a kernel) in as simple a language as necessary, but no simpler.",2,0,0,False,False,False,1646442846.0
t5hd3a,hz4vj8x,t3_t5hd3a,"The term used is usually ""buffer"".",63,0,0,False,False,False,1646276529.0
t5hd3a,hz6ru3b,t3_t5hd3a,"Honestly, I didn't think of buffer before looking in this post. Thank you for posting this question",3,0,0,False,False,False,1646318820.0
t5hd3a,hz4wlgt,t1_hz4vj8x,HOLY SHIT THANK YOU,23,0,0,False,False,True,1646277046.0
t5hd3a,hz6fus0,t1_hz4vj8x,"I was browsing my feed, didn't saw the sub and was thinking: Wow, OP apparently puts a lot of thought before posting to scoial media. Wonder what's that called. I was totally blindsided by it being about computer memory instead of human memory.",3,0,0,False,False,False,1646313195.0
t5hd3a,hz5wql0,t1_hz4wlgt,Good luck on your cross-word puzzle.,38,0,0,False,False,False,1646299857.0
t5yyn2,hz8101v,t3_t5yyn2,"Inb4 the “it’s just somebody else’s computer” guys show up.

The cloud is a way to rent hardware and software services from a provider so you don’t have to physically host your own code, data and applications or operate your own data center. Amazon realized around 2005 that the way they were running their internal IT operations could be useful to others and so they launched AWS, creating the public cloud paradigm.

The reason it gets confusing is the obvious question: “But weren’t data centres and application and web hosting around long before 2005? What makes the cloud any different?”

And yes, you could rent rack space and various hosted services before AWS. What makes the cloud different isn’t one thing, but a combination of properties which create a completely different value proposition:

- Fast on demand spinup and shutdown of services with per-second pricing; allowing a very agile workflow with much shorter iterations, prototyping, and dramatically reduced cost of change.

- Reduced lock-in through on-demand pricing, absence of long term binding licenses, and vendor agnostic open standards.

- Elasticity over manual scaling

- A focus on reliable, scalable, secure low level building blocks (as opposed to high level application/platform services like traditional web hosting, mail hosting, etc. or infrastructure level bare-metal servers, database servers, firewalls, NAS servers, etc.) all offering APIs, ready to be composed into reliable, scalable, secure applications by developers.

- Multitenant hyperscale pools of dynamically reassigned resources with guarantees on security and tenant encapsulation; this is in stark contrast to traditional data center thinking and required inventing or utilizing then-new technologies like hardware hypervisor virtualization.

- Self-service management through a web console of all resources from networking to the color of a BI dashboard. User friendly abstractions and interfaces allow non-ops people (in particular, software developers) to provision the infrastructure they need in a self service fashion without going through an IT Ops department (or vendor management, finance, security, etc). The ability to run a successful startup with no dedicated IT Ops department saves several times more on salaries than most startups spend on cloud infrastructure in the first place.

Put all these together and you get a developer oriented, radically agile, innovation-and-experimentation friendly, low cost, reliable, scalable toolkit that has since spawned literally dozens of billion dollar unicorn companies.

So when cynical neckbeards like to snarkily dismiss the cloud with a “the cloud is just somebody else’s computer” they’re really talking about the old world of colocation data centers and leaving out the part where AWS disrupted that entire industry and changed the world.",1,0,0,False,False,False,1646336406.0
t5yyn2,hzcgb3n,t1_hz8101v,"Somebody else’s computer

Sorry I’m late there was a lot of traffic",1,0,0,False,False,False,1646417672.0
t4vup0,hz153ah,t3_t4vup0,If you're interested in the evolution of algorithms and computing read 'Rage inside the Machine' by Robert Elliot Smith,11,0,0,False,False,False,1646214619.0
t4vup0,hz1s56z,t3_t4vup0,"To think this was designed even before Queen Victoria ascended the throne. I hope we eventually raise enough money to build the much larger (steam powered!) *Analytical Engine*, that really would be a Steam Punk revival. With advances in CAD CAM it might be economically feasible. Trouble is, the detailed design was only partially finished, but I think we have enough to go on.",8,0,0,False,False,False,1646229478.0
t4vup0,hz4ap85,t3_t4vup0,"Is it really proper to say this was the predecessor of the modern computer, rather than an early attempt at computer design? I didn’t think there was a continuous intellectual history from Babbage to the first ‘modern’ computers.",3,0,0,False,False,False,1646266371.0
t4vup0,hz14gpc,t3_t4vup0,I watched a really awesome presentation of this on YouTube the other day!,2,0,0,False,False,False,1646214116.0
t4vup0,hz1jx9q,t3_t4vup0,The carry part is really beautiful,1,0,0,False,False,False,1646225248.0
t4vup0,hz3l3av,t3_t4vup0,Wow it looks very interesting.  Thanks for sharing!,1,0,0,False,False,False,1646255126.0
t4vup0,hz4dt9y,t3_t4vup0,"Can I use instagram on it? No? Then it's not a computer!

&#x200B;

Just kidding! I saw a model of it at the Computer History Museum in California, it blew my mind in person!",1,0,0,False,False,False,1646267906.0
t5hxts,hz6bq86,t3_t5hxts,Can it create an inventory of military equipment thats part of a long convoy using video footage?,1,0,0,False,False,False,1646310908.0
t5p78w,hz667jo,t3_t5p78w,"Well... It really depends on what you're searching for, and how. I've heard that some search engines are better at searching for certain things, but I have no empirical data to verify that.

It helps if you include the site keyword in searches for Google, but I'm not sure if you can include a list of sites. I tend to use the time feature in Google often, especially when it comes to tech related searches. It's often better to find info that's new and not years old.",0,0,0,False,False,False,1646307360.0
t4g4on,hyzelip,t3_t4g4on,Theoretically could you make a GPU like this? It would be cool to see one made as just a project,4,0,0,False,False,False,1646180245.0
t4g4on,hyzrjqu,t3_t4g4on,That kid is definitely the next Shane from StuffMadeHere on yt.,6,0,0,False,False,False,1646186014.0
t4g4on,hyzi3h9,t3_t4g4on,I love that YouTube channel of your looking at the one I think your looking at. He explains everything so nicely so you can understand how everything works,1,0,0,False,False,False,1646181787.0
t4g4on,hz14b69,t3_t4g4on,"I wanna buy this, where to get one?",1,0,0,False,False,False,1646213991.0
t4g4on,hyzovnr,t1_hyzelip,https://youtu.be/OW1EmG7b4DU,4,0,0,False,False,False,1646184822.0
t4g4on,hz165r1,t1_hyzelip,https://www.youtube.com/watch?v=l7rce6IQDWs,2,0,0,False,False,False,1646215478.0
t4g4on,hyzvhg3,t1_hyzrjqu,"He looks fun, i just subscribed to his channel, I'll show it to Nicolai",2,0,0,False,False,True,1646187778.0
t4g4on,hz04pe7,t1_hyzi3h9,"Yes, Ben Eater is genius, he deserves all credit",2,0,0,False,False,True,1646191741.0
t55e0p,hz32iov,t3_t55e0p,"Computations will be most efficient if you use native types internally. If you want to avoid rounding errors when adding and subtracting, then something like BigDecimal would be the way to go (see also [GMP](https://gmplib.org/) for very efficient handling of arbitrary-precision numbers).

Intel processors do have instructions for operating in base-10, [but they aren't supported in 64-bit mode](https://stackoverflow.com/questions/33182491/why-bcd-instructions-were-removed-in-amd64). 

It would make sense to store numbers in base 10 if you expect to do frequent conversions between the internal representation and base 10 text input/output. Note that there has been [a lot of recent work](https://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html) optimizing binary-decimal conversions. 

So, assuming you want to store decimal values in memory, whether you use one digit per byte, two digits per byte, or 9 digits per 32-bit word, you'll have to implement your own carry-borrow logic to support addition and subtraction. [Here's a discussion of it](http://www.numberworld.org/y-cruncher/internals/addition.html) by the guy who wrote the software that has been used in every record-breaking computation of pi since 2010.

To do efficient multiplication, in order of increasing performance and code complexity, see [Karatsuba](https://en.wikipedia.org/wiki/Karatsuba_algorithm), [Toom-Cook](https://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication), and [Schönhage–Strassen](https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm).

For division, see [Knuth's Algorithm D](https://skanthak.homepage.t-online.de/division.html#:~:text=Note%3A%20Knuth's%20Algorithm%20D%20builds,not%20supported%20in%20ANSI%20C!) and [Newton-Raphson](https://en.wikipedia.org/wiki/Division_algorithm#Newton%E2%80%93Raphson_division).",2,0,0,False,False,False,1646247918.0
t55e0p,hz32jw8,t1_hz32iov,"It seems that your comment contains 1 or more links that are hard to tap for mobile users. 
I will extend those so they're easier for our sausage fingers to click!


[Here is link number 1 - Previous text ""GMP""](https://gmplib.org/)



----
^Please ^PM ^[\/u\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20hz32jw8)",1,0,0,False,False,False,1646247931.0
t55e0p,hz714e4,t1_hz32iov,Thank you!,1,0,0,False,False,True,1646322580.0
t53mm3,hz2c3ac,t3_t53mm3,"Here is another, slightly more recent paper that I think pairs nicely with this [Quantum Computing Through Quaternions](http://www.ejtp.com/articles/ejtpv5i19p1.pdf)",1,0,0,False,False,True,1646237868.0
t4wi72,hz1x1zt,t3_t4wi72,tell the students to come up with a good board game of their own.,1,0,0,False,False,False,1646231735.0
t4wi72,hz17n3s,t3_t4wi72,Have you considered Go?,1,0,0,False,False,False,1646216684.0
t4wi72,hz31gu2,t3_t4wi72,Perhaps Gomoku? It uses the Go board but the rules are much simpler: the player needs to be able to get 5 pieces in a row to win. Kind of similar to the games you've mentioned.,1,0,0,False,False,False,1646247517.0
t4wi72,hz3663a,t1_hz1x1zt,"Really? How does that teach OOP. IMHO, the teacher should give as many instructions as possible and just leave the OOP part on students. Coming up with a board game would be unnecessarily hard and nit serve much purpose.",5,0,0,False,False,False,1646249322.0
t4sxsp,hz2c4dn,t3_t4sxsp,"What textbook is this? It seems like it's mixing up two's complement numbers with floating point numbers.

\> normalised numbers must begin with 01 for positive numbers and 10 for negative numbers

Maybe that's the way this textbook's floating point number format works, but that's not how IEEE-754 works.",1,0,0,False,False,False,1646237880.0
t4vrpq,hz1bfvk,t3_t4vrpq,"I found very inspirational and clear the book ""But how do it know?"" by J Clark Scott. It explains how a computer work from basic principle at hardware level. It makes you understand how  CPU and RAM are really done and how they really work. Even if you are a software guy understanding this stuff make you a better programmer. Here you can understand what is a low level language. 

There are quite a lot of youtube video about this book for example look this play list 

[https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl\_eE6yzA](https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl_eE6yzA)

Or if you want to get a quick view about what you can learn try this one: 

[https://youtu.be/1BidIYu6Cls](https://youtu.be/1BidIYu6Cls)",1,0,0,False,False,False,1646219719.0
t4bslx,hyxkw7b,t3_t4bslx,https://craftinginterpreters.com/ will be your friend,20,0,0,False,False,False,1646154416.0
t4bslx,hyxheg5,t3_t4bslx,"Did you try a web search on ""how to build a compiler?""",45,0,0,False,False,False,1646153070.0
t4bslx,hyxeuhi,t3_t4bslx,"are you sure you want a compiler, or an interpreter, when I started building my own ""compiler"", i realised I wanted to build an interpreter instead lol",30,0,0,False,False,False,1646152065.0
t4bslx,hyydjgz,t3_t4bslx,"Right, so step one is to google it",8,0,0,False,False,False,1646165435.0
t4bslx,hyxjc76,t3_t4bslx,Read crafting interpreters,4,0,0,False,False,False,1646153814.0
t4bslx,hyzgkbw,t3_t4bslx,Come over to r/programminglanguages :D,3,0,0,False,False,False,1646181112.0
t4bslx,hyy7qgc,t3_t4bslx,"First steps, you need to be proficient in a language, preferable in a lower level, such as C/C++ or Rust. Then decide what kind of compiler, if you want to also write your own language, you need to write a parser, lexer, types etc. Then, if you are going to do some memory management you need to decide the strategy, etc. All this depends on how good you understand programming, preferably lower level and not just python and JavaScript.",2,0,0,False,False,False,1646163193.0
t4bslx,hyyarg9,t3_t4bslx,"[https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html)

[https://www.youtube.com/watch?v=-J\_xL4IGhJA&list=PLE18841CABEA24090](https://www.youtube.com/watch?v=-J_xL4IGhJA&list=PLE18841CABEA24090)

Later portions of the book and accompanying course cover this explicitly. It's an absolute classic. Have fun!",2,0,0,False,False,False,1646164355.0
t4bslx,hz13l1i,t3_t4bslx,"I very much enjoyed ""Writing an Interpreter in Go"" from Thorsten Ball https://interpreterbook.com/. It is well written and easy to start with. But be aware of the rabbit hole it awaits for you :D",2,0,0,False,False,False,1646213397.0
t4bslx,hyy124v,t3_t4bslx,"[This is not my area, but I have enjoyed The Dragon Book at various times.](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools)

I was told by a CS instructor that I worked with, and that used this book  for his compiler design course, that the final was to demonstrate that you compiler can compile itself. I never tried such a thing, but simply offer this as a possible source for you. I like the book.",3,0,0,False,False,False,1646160618.0
t4bslx,hyxptdl,t3_t4bslx,"I'd recommend GNU bison/Yacc and I'd look a little bit into grammars/context-free grammars maybe some automata.

It's not quite from scratch though.",2,0,0,False,False,False,1646156313.0
t4bslx,hyxj2q0,t3_t4bslx,"If you are serious about compiler construction just dive in, grab a copy of The Dragon Book and Engineering a Compiler.

You may find you are uninterested, though, some people aren't sure about what's involved and lose interest while playing with simple grammars.",2,0,0,False,False,False,1646153714.0
t4bslx,hyxk02o,t3_t4bslx,"There's a free online university-level course listed in OSSU under [""Advanced Programming""](https://github.com/ossu/computer-science#advanced-programming).

Not sure if you're asking for something easier.",1,0,0,False,False,False,1646154072.0
t4bslx,hyyahq1,t3_t4bslx,This is a good online course from Cornell: https://www.cs.cornell.edu/courses/cs6120/2020fa/self-guided/,1,0,0,False,False,False,1646164252.0
t4bslx,hz06e8c,t3_t4bslx,I build compiler at work. But it’s for deep learning.,1,0,0,False,False,False,1646192500.0
t4bslx,hyy9dch,t1_hyxkw7b,"Best answer.

Also note that it shows you how to make a compiler and also an interpreter.",3,0,0,False,False,False,1646163824.0
t4bslx,hyxjwca,t1_hyxeuhi,I wrote an interpreter for a made-up dialect of python in college. It was actually one of my favorite projects that semester. I would second this route OP,9,0,0,False,False,False,1646154031.0
t4bslx,hyxl7t5,t1_hyxeuhi,"Interesting. When I started doing research into programming language design, I thought what I wanted to make was an interpreter but I quickly learned that compilers were more fascinating to me personally.",2,0,0,False,False,False,1646154542.0
t4oaxq,hyzt4mk,t3_t4oaxq,"it depends on context of the user. if i don't expect anyone to use/see my code i might include elements like that, but it is good practice to make the code understandable.

also minimal code cache is important on integrated devices",1,0,0,False,False,False,1646186729.0
t4oaxq,hyzt6hs,t3_t4oaxq,"The main exceptions I can think of are:

1. Certain types of one liners make sense when you've got older/simpler compilers and you're close to the hardware. Operating Systems development and embedded work falls into this category.
2. Heavy syntactic sugar (usually newest language features) come in handy for performance when it comes to super common libraries for high performance (e.g. C++).
3. HDL languages for hardware design can use some of these constructs as well.

But that's about all I can think of. Other people may have more to contribute.",1,0,0,False,False,False,1646186752.0
t4oaxq,hz1bsso,t3_t4oaxq,"It's a bit like writing English. You develop a certain taste for a style. There seems to be subjective guidelines on what makes something more readable, but at the end of the day, there is a collective understanding of what appears better than the other.

One-liners are mostly better, except when one tries to nest logic within the one line.",1,0,0,False,False,False,1646219988.0
t3ic7t,hysi94g,t3_t3ic7t,"I took Calc 1 and passed and was excited!

I took Calc 2 and then I took Calc 2 again and was less excited.

I took Calc 3 and then I took Calc 3 again and then I took Calc 3 again and graduated.",91,0,0,False,False,False,1646064359.0
t3ic7t,hysh1lq,t3_t3ic7t,"I struggled with calculus as well, and it was extra frustrating because I always felt like I understood what was being taught conceptually, but when it came to demonstrating that understanding by solving a problem, I just couldn't do it *quickly* enough.  I think that came down to practice.

It sucks because it's much easier to just write down a calculus problem than it is to think of a real-world, not totally contrived application of a calculus concept that you can interact with to drive the concept home, so that's what calc homework usually actually looks like.  Maybe what you can do is, for each concept you encounter, think of *one* way it can be applied.  Do all the homework and practice problems to ensure you'll be able to do them on a test.  But maybe also try to think of a real-world application yourself and experiment with writing programs that use the math you learned.  That way you'll understand not only how to recognize, in a ""vacuum"", when a problem requires a certain technique or concept - but also you'll know the motivation for that concept and will understand it more deeply.

Ultimately it all comes down to the amount of time you're willing to spend drilling the thing you're trying to learn.  For me, Calc was a paradigm shift because it was the first math class I ever really had to study for.  The bottom line is that if you want to do well, you have to practice consistently and start doing so early.",16,0,0,False,False,False,1646063881.0
t3ic7t,hyslipb,t3_t3ic7t,"PatrickJMT on YouTube is literally the reason I finally passed. Many of my grad-student instructors (except for one) were not very good at teaching or explaining. Hell, there were a few I could barely understand due to heavy accents and rough English.",14,0,0,False,False,False,1646065658.0
t3ic7t,hyseztm,t3_t3ic7t,I got bodied by calc 1. Then did much better in calc 2.,55,0,0,False,False,False,1646063060.0
t3ic7t,hysm83c,t3_t3ic7t,I barely passed my pre-calc and calc 1 (integral) and I failed my cal 2 (differential). I retook it on my 3rd year in college and still barely passed it around 70-80%. Just suck through the math series. It will be hard but you'll be able to understand math memes in the future. I think that is the upside lol.,9,0,0,False,False,False,1646065939.0
t3ic7t,hyshfz7,t3_t3ic7t,"Work in practice questions frequently as this type of information cannot be absorbed in large chunks. Re-doing assignment questions is not that helpful unless you truly don't understand yet. 

Get access to the previous exams on record and also find similar problems in your text.

Keep in mind that in midterm exams it is very easy to go from an A to a C by getting a few small errors or sometimes even getting a single question wrong.",7,0,0,False,False,False,1646064037.0
t3ic7t,hysp5bl,t3_t3ic7t,"Calc 1 felt like too many things too fast, also ""learning things in a vacuum"" is an appropriate description as any. Calc 2 is a bit better Imo. There is a more focused scope, and the problems have application; often physics like you mentioned.",6,0,0,False,False,False,1646067065.0
t3ic7t,hytarya,t3_t3ic7t,"It's sad to see how a thing as beautiful as calculus is taught this way. Their entire focus is on the mechanics; how to integrate this, how to differentiate that, which trigonometric substitution simplifies this question etc etc. It's so frustrating to see everyone teach calculus this way. No one teaches how calculus helps solving problems and where does it come up in problems no one suspects them to come up in. Instead of making us find ways to apply calculus on a problem, they give us a dumb function to integrate and tell us  to remember all those rules. Ok I get that one must know what these rules are but, what's often overlooked is why are these rules the way they are. You won't be integrating complex functions by hand when you are doing deep learning. Rather, the knowledge of how and why calculus works would come more handy.",4,0,0,False,False,False,1646075317.0
t3ic7t,hyshftm,t3_t3ic7t,"As someone who did so poorly in highschool mathematics i feel your struggle. In the begining of my degree pursuit, which is computer engineering,  i had to start at the very begining (elementary algebra) which did not have credits towards my degree. Now i am on differential equations, having taken algebra, trig, cal 1, cal 2. I can honestly say that surprisingly math has become my best subject thanks to some great professors. Now that my required math courses are almost done,  i can now see how useful mathematics such as calculus can be in any stem profession.",3,0,0,False,False,False,1646064035.0
t3ic7t,hysmobt,t3_t3ic7t,"In my experience so far, calculus has mainly been of use when doing ML/AI. Things like back propagation require calculus. A lot of 3D graphics require linear algebra as well. I haven’t experienced much use of calculus in CS though.",3,0,0,False,False,False,1646066115.0
t3ic7t,hyszie0,t3_t3ic7t,"I failed HS Algebra 4 times in 2018. I was in college. I was so embarrassed that I would hide my Pre-Calculus textbook in the library because I was so embarassed. I never let that stand in my way of pursuing CS.

I would spend hours on topics that were trivial to others who had a good mathematical aptitude. I ended up acing all math classes after putting in this effort.

I would later find out that I am just not so good at sitting down and thinking about... ANYTHING (I have ADHD. Diagnosed at the END of my degree haha). Things are all better, I still use and love calculus concepts to this day. Calc is HARD. Series and Sequences form the base of a lot of analytical stuff such as big O notation and Analysis, so do well there.

Whenever I feel a disconnect on any subject, I google and read some of the applications to help make the concept more concrete or attach it to some real world phenomenon. Good luck!!

&#x200B;

edit : Pre-algebra, intermediate algebra, slopes and parabolas basically",3,0,0,False,False,False,1646071022.0
t3ic7t,hysk7ed,t3_t3ic7t,"I recommend turning to other resources to supplement your learning in addition to or instead of rewatching the lectures. Khan Academy is good, YT videos, etc",2,0,0,False,False,False,1646065144.0
t3ic7t,hysqgeo,t3_t3ic7t,"Hey OP, as a returning student who failed out of Calc III and retaking it this semester, I'm realizing a lot of the problems I've had with learning it in the past. I think the most important thing to do for these math classes is just repetition and understanding of the problems and  less of the concepts. A lot of math teachers will write out proofs and a bunch of other nonintuitive things, but when it comes down to it, the problems you're given for homework and quizzes are what you're going to be tested on. If you understand how to do them you will most likely do well. I do all of the homework problems and if I don't completely get the answer on my own the first time through, I redo it later (I have a list of all homework problems and I circle ones that I need to practice). More than anything though, I think these classes are when I realized that I should have paid a lot more attention in previous courses as it all builds and the amount of time it takes to really do and understands the problems drastically increased at this point. I recommend using chegg for problems you struggle with (always attempt the problem first), ask questions in lecture if you don't understand and read ahead if possible so you know what questions you really need to ask your teacher (I wouldn't try to 100% understand from text, again it's a lot of proofs and nonintuitive stuff I would skip, read the example problems etc). I think just becoming more proficient at studying was another thing needed for success at these more difficult courses. I don't play addicting games like League of Legends or online FPS because I know they collapse my attention span (it's crazy like if I play them on the weekend or especially during the week it affects my ability to study so much). I hope this helps, but it sounds like you are dedicated to your studies and will succeed. Just put in the work!",2,0,0,False,False,False,1646067567.0
t3ic7t,hyst8bx,t3_t3ic7t,"A lot of schools have equivalent CS degrees in which you only take CS classes and no/little math. Sometimes businesses classes instead.

In short, yes, people can struggle with math while in a CS degree",2,0,0,False,False,False,1646068643.0
t3ic7t,hysxtl2,t3_t3ic7t,Pay a maths student to tutor you.,2,0,0,False,False,False,1646070385.0
t3ic7t,hyt35uw,t3_t3ic7t,"Went back to get my degree at 35, so I was almost 2 decades out from math education.  Much of the calculus I was able to get - but remembering my trigonometry formulas was just not happening.

Of all the classes that were a problem for me, it took me too long in linear algebra to get some basic concepts due to a bad book and a professor who just did examples directly from the text - but throwing in ""you need to remember trig because these matrices on the exam have trig identities you need to simplify in almost all of them to get the points for your answer"" didn't help.",2,0,0,False,False,False,1646072420.0
t3ic7t,hyt4xh2,t3_t3ic7t,I used to call that portion of the program the weeder portion. It weeds out anyone not an expert in math. I only passed from paying a private tutor for 2 semesters. I failed the first time trying to do it on my own.,2,0,0,False,False,False,1646073089.0
t3ic7t,hytp74z,t3_t3ic7t,"For me I’ve just had horrible professors, when I use something like khan academy I almost instantly understand everything.",2,0,0,False,False,False,1646080901.0
t3ic7t,hytqeau,t3_t3ic7t,"I'm a CS grad student and I still find programming much easier than math. I got trashcanned by linear programming as an undergrad, and I'm getting trashcanned by non-linear optimization right now. Also got somewhat trashcanned by multivariable calc :\^). For my thinking, computers are easy, because things(generally) must be expressed in concrete terms to evaluate them in a programming language, whereas sometimes with mathematical objects I struggle with having the vision and creativity to understand their relationships.",2,0,0,False,False,False,1646081364.0
t3ic7t,hytvidm,t3_t3ic7t,"I know the course work may seem like it exists in a vacuum but actually calculus is some of the most important pieces of math EVER. 

It has wide ranging applications in almost every scientific field, if you can use your data to make a graph then calculus has some kind of insight on it for you. 

Maybe read up on some of the cool stuff that applied calculus is used for and there goes your motivation. 

Also remember that learning calculus is mostly about learning to *think* properly, so focus on the proofs (for me that was my favorite part). 

If all else fails, push forward. As you advance you are gonna see less math.",2,0,0,False,False,False,1646083331.0
t3ic7t,hysf80f,t3_t3ic7t,I avoided taking Calc lol,3,0,0,False,False,False,1646063153.0
t3ic7t,hyslm6m,t3_t3ic7t,"Fuck calc. Fuck calc II. And most likely fuck your lecture videos. Khan Academy / Organic chemistry tutor on YouTube is the best route. Symbolab/Mathway/Photomath for help with your problems. Utilizing Chegg is another great resource for extremely particular questions that you can’t find examples of online. Using these resources with good intent is really beneficial. 

In short, yes. Trig, Calc, & Calc II we’re awful. Grasping their core concepts isn’t anywhere near as important as grasping concepts like truth tables and other CS specified math topics.",0,0,0,False,False,False,1646065696.0
t3ic7t,hyspmpj,t3_t3ic7t,"Yes. This, THIS THIS THIS
I'm in my first year of college and I've absolutely loved every bit of it until I came across calculus. I went into it eager to learn it and to tackle it but I was wrong. It's been kicking my ass relentlessly and I dread it. To make it worse, my class is full online (my mistake) so there's never any live lectures",1,0,0,False,False,False,1646067250.0
t3ic7t,hysqgfb,t3_t3ic7t,Struggled with calc too. Gilbert Strangs MIT OCW video lectures helped me a lot.,1,0,0,False,False,False,1646067568.0
t3ic7t,hyt4yt6,t3_t3ic7t,"I'm about to get my PhD in computer science. I suck at math. I barely made it through, especially trigonometry. It got a little better later, but I always struggled. I live in fear that someone will find out that I actually don't know math lol. That being said, someone's gonna sign my dissertation and I'm getting job offers to really good places so I guess it's ok.",1,0,0,False,False,False,1646073103.0
t3ic7t,hyte7mu,t3_t3ic7t,"Thank you for all the responses, I'm just gonna focus all my study time into calculus to pass it lol. My programming and elective classes are all a breeze. I just got path this hellscape of math.",1,0,0,False,False,True,1646076638.0
t3ic7t,hytlqob,t3_t3ic7t,"It was actually interesting for me albeit very difficult because I suck at math. 

Cal 1 and 2 wrecked me. I passed them but I lived in the help centers.",1,0,0,False,False,False,1646079558.0
t3ic7t,hytp02u,t3_t3ic7t,"What got me was discrete mathematics. Professor was really secretive, joking aside it really did kick my ass.",1,0,0,False,False,False,1646080825.0
t3ic7t,hytrqf5,t3_t3ic7t,Professor Leonard on YT is a godsend when it comes to calc,1,0,0,False,False,False,1646081879.0
t3ic7t,hyttttf,t3_t3ic7t,"Calc 1 was pretty easy for me. Calc 2 was a struggle at times. 
Right now, I am struggling in my design and analysis of algorithms class. All I can say is don’t forget all the stuff you are learning in these math classes i.e.  college algebra, Calculus, linear algebra, discrete structures...Don’t make the same mistake I did.",1,0,0,False,False,False,1646082685.0
t3ic7t,hyu2g8x,t3_t3ic7t,At least your lectures arr recorded 😩,1,0,0,False,False,False,1646086052.0
t3ic7t,hyu6p9k,t3_t3ic7t,Calc 2 the dream killer ☠,1,0,0,False,False,False,1646087808.0
t3ic7t,hyu6r48,t3_t3ic7t,"I failed Calculus II during my second semester of CS back in 2015 (only course I failed in college). Took it again over that summer in an accelerated 5 week semester. Passed with an A! 

I would recommend finding a tutor (if you're not seeing on already). There's no shame in it. That's what helped me tremendously the second go 'round.  

Hang in there, pal! You've got this!",1,0,0,False,False,False,1646087829.0
t3ic7t,hyu8kcv,t3_t3ic7t,I struggled more with probability theory,1,0,0,False,False,False,1646088595.0
t3ic7t,hyuktxo,t3_t3ic7t,You’ll be fine brother just keep studying . Once you learn that calculus is really just algebra and converting one thing to another it becomes much much easier.,1,0,0,False,False,False,1646094156.0
t3ic7t,hyuma8b,t3_t3ic7t,"Program state is position.

Function calls are velocity.

Multiple threads is acceleration.",1,0,0,False,False,False,1646094851.0
t3ic7t,hyuqkb8,t3_t3ic7t,"I did totally fine with calculus in high school and first level of college.  It was in the calculus 2 in college where I started to not ""get it"" but we had a horrible professor who was on the brink of retirement and simply did not give a fuck if anyone learned anything or not.  I ended up dropping his class immediately after writing the midterm exam.  When I redid the course as a night course the next semester, we had an amazing passionate professor and I aced the course because he was able to explain things well, relate the concepts to the real world, and most importantly he cared that his students were actually learning. 

Guess the point I'm saying is the professor can make a huge difference in how well the students are able to learn, so maybe you've just had bad luck with who your teachers are?",1,0,0,False,False,False,1646096880.0
t3ic7t,hyuvbjj,t3_t3ic7t,"When you first start calc it's really confusing, try to understand it is my best advice. 

Also you need more matrix algebra than calc. Since graphic cards use a lot of matri. Algebra for their operations.",1,0,0,False,False,False,1646099125.0
t3ic7t,hyvk475,t3_t3ic7t,"Get an engineering oriented maths book like Advanced Engineering Mathematics by Kreyszig, it's full of real world applications and helped me quite a bit as a programmer.",1,0,0,False,False,False,1646111171.0
t3ic7t,hyvt1m1,t3_t3ic7t,"I love calculus. Calc 3D was one of my favorite courses. It almost never comes up when software engineering though. So don't let that deter you. If you'd like some free tutoring lmk, I've got time",1,0,0,False,False,False,1646116736.0
t3ic7t,hyvxtab,t3_t3ic7t,">Did anyone ever struggle with Calculus?

One of those famous subjects known for being a struggle for a lot of people?

Nah. Aced it. Got into Google. EZ.",1,0,0,False,False,False,1646120194.0
t3ic7t,hyw1ej1,t3_t3ic7t,"It's been a real thing. Out of 55 students in our class, only 13 had enough preparation to attend the final exam. Out of them, only 5 passed. Two inhumans out of those 5 got A+. The rest of us studied all night and simply didn't go to the exam hall. One of those who attended and failed was a medalist in Math Olympiad! We later had to retake the course with our juniors and passed eventually.",1,0,0,False,False,False,1646122992.0
t3ic7t,hywbswt,t3_t3ic7t,"In my uni you needed a very strong math background to get into cs. For the most part students passed calc 1 no problem.

In calc 2 they fucked up.

My kryptonite were however physics 2 & electronics. Did those fuck up my gpa? Yes. But did i pass them? Also yes. 

If you like most of the classes just force pass the specific ones you don't and you'll be fine.",1,0,0,False,False,False,1646131685.0
t3ic7t,hywey8l,t3_t3ic7t,"Exact opposite, I loved calc 3-5 but hated my programming classes",1,0,0,False,False,False,1646134169.0
t3ic7t,hywki7c,t3_t3ic7t,Chegg and WolframAlpha got me through some tough times getting my undergrad degree,1,0,0,False,False,False,1646137999.0
t3ic7t,hywpiek,t3_t3ic7t,"I failed Algebra 3 times, calc 2 twice, calc 3 twice and proofs 3 times. 

That's just the math classes I failed during my CS degree :')",1,0,0,False,False,False,1646140843.0
t3ic7t,hyxmi38,t3_t3ic7t,"If anyone needs materials for preparation for Calculus, I highly recommend the free and peer-reviewed textbooks available from Rice, https://openstax.org/subjects/math

Others already paid for the books, and there are a few to choose from.

Algebra -

https://openstax.org/details/books/elementary-algebra-2e

https://openstax.org/details/books/algebra-and-trigonometry-2e

https://openstax.org/details/books/precalculus-2e

Calculus - 

https://openstax.org/details/books/calculus-volume-1

https://openstax.org/details/books/calculus-volume-2

https://openstax.org/details/books/calculus-volume-3",1,0,0,False,False,False,1646155038.0
t3ic7t,hysfys9,t3_t3ic7t,Yes because I was lazy and choose to not take them seriously. But hey they're hard,0,0,0,False,False,False,1646063451.0
t3ic7t,hyukayr,t3_t3ic7t,Calculus sucks ass but if you grind it out it’s really not the worst thing in the world,0,0,0,False,False,False,1646093909.0
t3ic7t,hywhibi,t1_hysi94g,I think the number just means how many tries ya need to pass!,7,0,0,False,False,False,1646136035.0
t3ic7t,hyv5pvy,t1_hysi94g,"Same.

I didn't do nearly as well in college as I would have liked, but I got through it without changing majors.",4,0,0,False,False,False,1646103922.0
t3ic7t,hyvp6i9,t1_hysi94g,"Is this Single Var, Multi Vars, Diff Eq?  
Or Single variable broken into two classes?  
(I’m curious of some places make CS students work ODEs as part of a holdover from Electrical Engineering or something.)",2,0,0,False,False,False,1646114210.0
t3ic7t,hyszlv6,t1_hysh1lq,"Didn't read this before my comment, but exactly. It's just an hours into game type of thing",2,0,0,False,False,False,1646071058.0
t3ic7t,hyuyq7p,t1_hyslipb,I'll check him out thank you,1,0,0,False,False,False,1646100672.0
t3ic7t,hysf938,t1_hyseztm,"haha thanks at least I can hope to have the same experience as you with calc 2, doing calc homework now :C",7,0,0,False,False,True,1646063165.0
t3ic7t,hysxgp6,t1_hyseztm,"Same. Never learned to study in high school, got wrecked in calc 1. Learned from my mistakes and calc 2 was a breeze.",6,0,0,False,False,False,1646070250.0
t3ic7t,hysguj2,t1_hyseztm,"This is very relatable. Calc 1 was a whole new world for me and I scraped by with a C. Then Calc 2, 3, and 4 (what my school calls Linear Algebra / Diff Eq) were all As.",4,0,0,False,False,False,1646063802.0
t3ic7t,hyt10dm,t1_hyseztm,"exact opposite for me. Breezed through cal1, bodied by cal2. almost had to take cal2 twice. fuck that class.",5,0,0,False,False,False,1646071592.0
t3ic7t,hysir41,t1_hyseztm,Calc 2 I got a 0 a straight 0.,6,0,0,False,False,False,1646064563.0
t3ic7t,hz1mxwr,t1_hyt35uw,Just use the complex exponential expressions for sine and cosine. They are like one formula for sine and one for cosine and from them you can derive basically every trig formula so you only have to memorise two formulas instead of 20.,1,0,0,False,False,False,1646226872.0
t3ic7t,hz99svm,t1_hytqeau,yea i think it just comes with time that you think of the mathematics concurrently with the programming. It has its applications but sometimes its hard to see where and when math is the answer.,1,0,0,False,False,True,1646355316.0
t3ic7t,hytapj5,t1_hysf80f,For a CS Degree?,3,0,0,False,False,False,1646075290.0
t3ic7t,hyvxxbr,t1_hyuma8b,Multiple threads as acceleration sounds like a recipe for disaster.,1,0,0,False,False,False,1646120276.0
t3ic7t,hyw0255,t1_hyvxtab,"Nice dude, I struggle at the beginning of pre-calc then I picked up steam and ended with a B+. I'm hoping the same happens for Calc,",1,0,0,False,False,True,1646121921.0
t3ic7t,hywik2d,t1_hywey8l,Do you work as a programmer now? I like the programming because it's like a word puzzle in essence. Just figure out what words do what you them to do. Makes sense because I always excelled in learning foreign languages so I guess the language center of my brain is over-developed,1,0,0,False,False,True,1646136750.0
t3ic7t,hywogwn,t1_hywki7c,"Do you usually use wolfram alpha for finding exact values of limits? Like just calculate with 0.1, 0.01, 0.001 until you see it resolve to something? I don't know if that's applicable for a lot of stuff with proofs",1,0,0,False,False,True,1646140275.0
t3ic7t,hyxo8jy,t1_hyvp6i9,"We used the Stewart Calculus textbook. My 3 courses covered Functions and Models, Limits and Derivatives, Differentiation Rules, Applications of Differentiation, Integrals, Applications of Integration, Techniques of Integration, Parametric Equations and Polar Coordinates, Infinite Sequences and Series, Vectors and the Geometry of Space, Vector Functions, Multiple Integrals, Vector Calculus, and the subsection of Linear Equations.

The only calculus course I didn't take was Diff Eq (Calc 4). I was also required to take Probability and Statistics and Linear Algebra, so I have a math minor.

I should also note, I failed out of Trig in high school, took an Algebra refresher when I went back to college 10 years later, then taught myself most of Trig and tested out of it into Calc 1.",2,0,0,False,False,False,1646155703.0
t3ic7t,hysfp03,t1_hysf938,"I was never thrilled with how my teacher explained things but there are some amazing, short videos on YouTube from khan acedemy and the organic chemistry tutor that helped so much.",7,0,0,False,False,False,1646063343.0
t3ic7t,hysh7gh,t1_hysf938,"Definitely start checking out videos from Khan Academy and doing their calculus practice (along with your assigned problems), Organic Chemistry Tutor (also teaches calculus), 3Blue1Brown Essence of Calculus, etc. They will all help a lot and give you a more fundamental understanding of what's going on.",3,0,0,False,False,False,1646063945.0
t3ic7t,hytalu9,t1_hysf938,"I hope you do as well, Calc 2 was brutal, luckily Calc3/Multivariate and Vector calc went smoothly",3,0,0,False,False,False,1646075250.0
t3ic7t,hytfg0u,t1_hysf938,"I had the opposite experience; my calc 2 class was orders of magnitude more difficult than 1 and 3. And while it's true that you might not use it in the role you're going for now, an understanding of calculus is fundamental to a lot of things. Most people don't do the same thing forever. It's likely that you will change roles or career paths at some point and this might give options you wouldn't have otherwise.",2,0,0,False,False,False,1646077110.0
t3ic7t,hyskryv,t1_hysguj2,"Calc 2 (integral calculus), was such a a bitch for me.",8,0,0,False,False,False,1646065371.0
t3ic7t,hyv206h,t1_hyt10dm,I remember my teacher of day one pretty much saying “you will probably struggle with one and be perfect fine with the other.” Calc was just such a huge jump in logic in my opinion,2,0,0,False,False,False,1646102189.0
t3ic7t,hyut5uq,t1_hysir41,are you a cabbage or something,5,0,0,False,False,False,1646098111.0
t3ic7t,hytf18c,t1_hytapj5,"Yeah, it was there as one of the elective options, but because I have to do Linear Algebra and also CS theory, I haven’t done Maths in years so I don’t want to risk overextending myself. From what I understand (and I stand corrected), Calculus is more for electrical field or computer engineering which deals a lot with electrical principles.",1,0,0,False,False,False,1646076952.0
t3ic7t,hyw151n,t1_hyw0255,"I'm joking.

I haven't gotten to our Calc classes yet. I expect to struggle.",1,0,0,False,False,False,1646122783.0
t3ic7t,hywjcyp,t1_hywik2d,"I work as an MLOps engineer.

I was never interested in coding, I like applied math but a math degree on its own isn’t that marketable.

I guess some people find this to be fun and intellectually rewarding. I really don’t.",1,0,0,False,False,False,1646137277.0
t3ic7t,hyyjrn6,t1_hyxo8jy,"Cool.  The calc numbering just varies in terms of what it means.

Congrats on coming back from a weaker start re: maths!

Actually one of the nice things about taking calc: gets you comfortable with algebra.  
In practice I suspect one almost always develops real comfort with an area only after a couple courses / some time with elements that build on it.",1,0,0,False,False,False,1646167813.0
t3ic7t,hysqjjf,t1_hyskryv,"Oh yea, definitely hard classes. But I kicked into gear",3,0,0,False,False,False,1646067602.0
t3ic7t,hyuveyc,t1_hyut5uq,No I'm good at math usually. I wrote so much shit the teacher maybe gave me negative note. Tbh I didn't attend the classes. There were at 8:30 and I have to take 1h45 public transport.,1,0,0,False,False,False,1646099167.0
t3ic7t,hyuwv0h,t1_hytf18c,"We also had an ""elective math"" (calculus) but so far everyone in our school that was ever a CS took it. 
I guess it isn't ""elective""",2,0,0,False,False,False,1646099823.0
t3ic7t,hyxlblz,t1_hyw151n,I demand a name change to calc2nub when the time is appropriate.,1,0,0,False,False,False,1646154583.0
t3ic7t,hyv86nj,t1_hyuveyc,"""I didn't attend class"" 
""I got a 0""

Surprising",1,0,0,False,False,False,1646105093.0
t3ic7t,hyvv8ok,t1_hyuwv0h,Yeah either that or everyone’s just such a smarty pants🙂,1,0,0,False,False,False,1646118293.0
t3ic7t,hywf27n,t1_hyv86nj,"Yeah when I was in highschool,I slept 50-60% of time and still be in top 5. I thought college would be same. I didn't attend most of my programming class but still got 80+%. Man I was a lazy student.",3,0,0,False,False,False,1646134255.0
t3yzfh,hyvelig,t3_t3yzfh,That’s not how writing software works.,11,0,0,False,False,False,1646108218.0
t3yzfh,hyvgzvq,t3_t3yzfh,That's not where the cost of development lies.,9,0,0,False,False,False,1646109457.0
t3yzfh,hyvhz4w,t3_t3yzfh,"I don't understand why you think this would be a good idea.

What problem do you think this would solve exactly?",5,0,0,False,False,False,1646109984.0
t3yzfh,hyw0s3v,t3_t3yzfh,"From a high level, very basic and easy perspective, look at the usage of libraries. There's no way to write a .NET program that uses EFCore and then just transpile that to Java. There's an equivalent (sorta) ecosystem there, that likely solved the problem in a different way. 

It's often the case however that you could write a C library that is then called by a different language or technology. You can outsource the heavy lifting of Python to a C file directly. You could also just straight up build microservices, and develop a protocol for the interaction between any other tech stack and it. That's how the Internet works, today. 

Modularity is a design feature for a system, and a well designed one can integrate multiple different technologies. 

All that being said, remember that languages are tools. Some of them are better at specific tasks than others. You wouldn't write math heavy code in COBOL, and you wouldn't write a web front-end framework in C. 

Can you implement bubble sort in any language? Sure! But, that doesn't really have anything to do with systems development.",6,0,0,False,False,False,1646122492.0
t3yzfh,hyvotcd,t3_t3yzfh,"Because transpilers don’t work. Basically you’re compiling in one language and then decompiling in another. Ever seen decompiled source code? It’s unreadable unless it was very simple C-code or assembler, even if you have the symbols. Python, Java and any other language produces just utterly unreadable code because in many cases you’re also decompiling parts of an interpreter or virtual machine and all the components and intricacies of higher languages (the weird pointer counters and garbage collection and precompilation optimizations etc etc)",5,0,0,False,False,False,1646113980.0
t3yzfh,hyvnal6,t3_t3yzfh,"Some companies do model to code transformations, but not really source to source transformation. If they need to have an universal platform, then they have JVM which can run various language (java, kotlin, scala, python, groovy, etc.) on top of it.",2,0,0,False,False,False,1646113037.0
t3yzfh,hyvfvr7,t3_t3yzfh,Try and implement one and see.,1,0,0,False,False,False,1646108872.0
t3yzfh,hyw91b7,t3_t3yzfh,Because it suuuuuuuuuuuuuuuuuuuuuuuuucks,1,0,0,False,False,False,1646129368.0
t3yzfh,hyxrtuo,t3_t3yzfh,"Generally compilers are transpilers, and companies use transpiling all the time. That isn't really the same as what you're suggesting, re: transpiling for the purpose of translation of logic between high level languages and dependencies. The reason that is not very commonplace is that the task represents a set of extremely difficult problems to provide solutions for. We do use tools like this somewhat often, they just are not very good at translating ""an application."" They can be very good at translating functions and blocks, and we fill in the gaps as we go slowly using the tools to translate more and more of the logic until we have a program that is generally working. Then you have a program translated to the language you want, but your translating has left you with something that isn't idiomatic in the language it was translated to, so you go to work making it a ""good"" program in the new language. Eg, you need developers fluent in both languages and the tools to even be doing the work at all, and at the end the developers still had to write the program, just, sort of assisted.

There are usually tools available in languages you may know that you can try out on a program. The last one I used was https://github.com/immunant/c2rust while playing with the Doom source.",1,0,0,False,False,False,1646157083.0
t3yzfh,hyvpv81,t1_hyvelig,Could you elaborate?,1,0,0,False,False,True,1646114644.0
t3yzfh,hyvq2ro,t1_hyvgzvq,I see. Where then?,2,0,0,False,False,True,1646114776.0
t3yzfh,hyvq1jw,t1_hyvhz4w,I am still learning CS. This was a question that just popped into my head,2,0,0,False,False,True,1646114754.0
t3yzfh,hyxditk,t1_hyw0s3v,Thanks! Well explained!,1,0,0,False,False,True,1646151536.0
t3yzfh,hyvq73b,t1_hyvotcd,Thanks for actually explaining!,1,0,0,False,False,True,1646114854.0
t3yzfh,hyx2fv2,t1_hyvfvr7,"Honestly, this is the best answer. If OP is interested in transpilers, why not try to implement one. As long as you're honest with yourself and have realistic expectations. Working on things you find interesting is a fantastic way to learn.",2,0,0,False,False,False,1646146952.0
t3yzfh,hyvpxt2,t1_hyvfvr7,"Jeez, Why so salty?! Not living up to your username right now",1,0,0,False,False,True,1646114690.0
t3yzfh,hywe07w,t1_hyvpv81,Software is not written in a programming language really. It is written in frameworks and libraries. There is no 1 to 1 conversion between multiple stacks.,3,0,0,False,False,False,1646133448.0
t3yzfh,hyvqj4f,t1_hyvq2ro,The language is the easy part of solving problems. The hard part is decomposing the problem into an architecture that solves the problem efficiently and can be maintained.,6,0,0,False,False,False,1646115072.0
t3yzfh,hyw008y,t1_hyvq1jw,"Yeah transpilers do exist, in fact the first C++ compiler (cfront) actually compiled C++ down to C code.

But the whole point of software development is to be able to maintain that software. Transpiler output isn't human readable imo, thus it isn't easily maintained.",3,0,0,False,False,False,1646121881.0
t3yzfh,hyzqig4,t1_hyvq1jw,"That’s cool. Asking questions is how you learn. My opinion on this is similar to what others have said about the cost of development not lying in software engineers learning languages. It’s about solving the problem. But more importantly, these languages usually have frameworks that are super effective and built specifically to do certain tasks. Hiring a whole team to build a transpiler would be loads more work and also result in less effective code, which brings up an interesting point….

Translating anything from one language to another will result in information loss. Even if it’s English to Spanish, some of the original information will be lost. In this example, information would be lost during transpilation and less effective code. Hope this helps.",1,0,0,False,False,False,1646185554.0
t3yzfh,hyxqv0i,t1_hyvpxt2,"It's not like that.

You'll bump into the problems in doing things that way when you try it that way. Often times the way people do things has reasons that you just haven't had the experience to encounter. You don't really appreciate the complexity of a project until you're neck deep in it, and can't really judge the complexity or scope of an approach until you've given it an honest try or gotten neck deep in something that was done that way.

The answer to ""Why not do it like this?"" is not always some specific well known quantity like in physics or electrical engineering. Often as not it's ""I don't know if anyone's tried. Give it a shot and let us know how it goes.""",2,0,0,False,False,False,1646156711.0
t3yzfh,hyxht5e,t1_hyvqj4f,"> can be maintained.

This has been the critical issue for my most recent jobs. My 2 most recent jobs have been working on state data systems where the data has to be kept *forever*. One was modernizing a mainframe system that handled vehicle registrations. Originally implemented in the 1970s, the mainframe can't handle newer regulations and laws that politicians want to pass. Due to the age of the COBOL programmers and how brittle the big ball of mud & lavaflows has become, it takes about a year to make *any* change.",1,0,0,False,False,False,1646153229.0
t3nltj,hytf9rp,t3_t3nltj,"My take on it is that the cpu is the nerd and the gpu is the jock. The cpu is good at math, English, chem, bio, etc. yet the cpu is an average athlete. The gpu is average in academics, but can throw ball far af. 

In other words, the cpu is designed to handle a breadth of tasks at a steady pace. Whereas the gpu is designed to do one task at a much faster rate. The cpu has several cores working on multiple different more complicated tasks, and the gpu has exponentially more cores working on the same task.

Someone feel free to correct me if I’m wrong. Haven’t spent too much time on architecture outside of an assembly course.",7,0,0,False,False,False,1646077043.0
t3nltj,hytfc83,t3_t3nltj,[This is a good entry level summary](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/),2,0,0,False,False,False,1646077070.0
t3nltj,hythh6r,t3_t3nltj,"The short version is this:

The cpu have to handle anything, so it has a lot of complex internal structures to handle any type of calculation. Also CPUs calculation has to happen in order, so it is not as beneficial to have many, many cores going at the same time since the calculation has to be done in specific order. We have multi core cpus now so they can handle different tasks logically, but each task still works in its own sequence. 

GPUs deal with graphics which are made from millions and millions of little pixels, each for them have to be processed separately and independently from the others, so you can do them all at the same time, that means the GPU is designed to be made out of many little cores that are very simple. 


So the CPU is like an novelist writing a story, it has be well thought out, and each step following the next, while the GPU is like a printing press, it can make many copies of the same text quickly, but it can’t come up with anything creative. 



Nowadays we are starting to see an overlap between the two because programmers are getting better at dividing tasks in a way that enables parallel processsing, while GPUs are handling more complex calculation, so now we have processor cores that can kinda do both like the ARM processors.",2,0,0,False,False,False,1646077895.0
t3nltj,hyu2ylr,t3_t3nltj,"Being no expert, the biggest difference to me is the amount of cores and their specialisation. A CPU will commonly have 1-16 cores, whereas in GPUs it's in the thousands, and they can have clusters specialised for certain operations (like tensor cores for AI imagine upscaling on modern GTX cards). A CPU's cores are more generalised to run a few but diverse instructions fast, compared to graphics cards, where you need to be able to run many types (light ray tracing, material functions, mesh deformations, anti-aliasing, rasterization) of operations as possible in parallel. If you'd run the same graphics processing workload on a CPU, it would run at unplayable frames per second, and I'd wager that has to do with lack of parallelism and context switching on the CPU",1,0,0,False,False,False,1646086261.0
t3nltj,hyv79ue,t1_hytf9rp,"Adding to this, a CPU is generally faster than a GPU, but only operates on a small amount of data at one time. A CPU has around 4-32 cores, each acting independently (e.g. one might be adding while another divides). They can quickly change tasks dynamically (i.e. conditional branching).

A GPU is slower and more restricted, but operates on thousands of data simultaneously. They have hundreds or thousands of cores, each executing the same task on different data. So a GPU is great for bulk operations, common in graphics, e.g. project each vertex from a 1000-polygon model from 3D space to a 2D screen, or darken each of a million pixels by 10%.",3,0,0,False,False,False,1646104659.0
t42bo4,hyw36v7,t3_t42bo4,"Please use more helpful titles for your posts.

I like Computerphile.",11,0,0,False,False,False,1646124463.0
t42bo4,hywedxh,t3_t42bo4,"This is a playlist taken from the book ""but how do it know?"" [https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl\_eE6yzA](https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl_eE6yzA) 

You can find how the main logic gates are made by transistors and how to build latches, decoder, adder, multiplexer etc so how to build a CPU and a memory RAM from logic gates. I think it could be a good start.",3,0,0,False,False,False,1646133736.0
t42bo4,hywfz6q,t3_t42bo4,Onur mutlu lectures,2,0,0,False,False,False,1646134938.0
t42bo4,hyx7n1q,t3_t42bo4,"If you need a university level course, look at OSSU's [Core systems](https://github.com/ossu/computer-science#core-systems) or [Advanced systems](https://github.com/ossu/computer-science#advanced-systems).",2,0,0,False,False,False,1646149136.0
t42bo4,hyw8zwp,t1_hyw36v7,Ok thanks im kinda new to reddit so that's why,3,0,0,False,False,True,1646129334.0
t2leeq,hymqa6b,t3_t2leeq,"Hardware caches are just memory. Software caching exists too in programs.

In either case, ""locality"" means that a recently accessed memory address (hardware) or variable (software) will likely be accessed again. Temporal locality means ""recently accessed"" and spatial locality is nearby in terms of addressing (e.g. index i+1 vs index i in an array, or data on the same virtual memory page, or member variables in the same class).

In hardware, you'd put a cache between the CPU core and the entire available computer memory. In software, you might cache recent variables used, recent web pages, or recent database query results. In either case, you're caching a subset of the full data available on the assumption that either recently used data (temporal locality) or nearby data (spatial locality) will be used again soon (called a ""cache hit"").

See if these set of [lecture notes](https://snir.cs.illinois.edu/PDF/Temporal%20and%20Spatial%20Locality.pdf) found via web search helps.",61,0,0,False,False,False,1645958968.0
t2leeq,hyn14tj,t3_t2leeq,"I have given this analogy before but is one of the best ones.

Think you are a construction worker. You obviously need tools. When you are doing some construction you generally work on a small part of the building at a time and probably for some of those tasks some tools like hammers are required more often than another one.

Here tools are the data/instruction, and your hands is the processor that process the data and the construction framework to build the outcome.

1) You can carry two smaller tools directly in your hands or one large tool. But you can work the fastest as you dont extra time to reach the tools themselves. So your hands also can hold some tools (data) in the palm of your hands using your fingers. So here your palm and fingers are the cache where you keep your most used tool and you can use those tools as fast as your hands (processor) work.

2) Now your hands can only hold so many tools. But you might be working on something at the 50th floor. You cant expect to come down everytime you want to change to a less frequently used tool.
So you use your jacket pockets and straps.

Now you can carry many more tools but there is a penalty wrt time when you want to change tools. Because you will have to empty your hand into the jacket and get another tool. So your Max speed reduces to how fast you can swap.

3) Expanding it further. Even the jacket can only hold so much. So maybe you keep a stash of additional tools in the same floor you are currently working on or maybe on the elevator you use to ho up. This will be slower than a jacket but can hold more tools and will save you a trip to the ground.

4) Even that may not be enough if you want power tools. Then youd NEED to go to the ground and fetch it. Same thing, now the whole ground is your area (memory) so it is huge but going up and down will be quiet slow.

5) Finally maybe you missed to bring something entirely to the construction site. So here you'd need to drive/ fly to a home or a shop to get what you need. At this point the time taken is huuuge. But you got the whole earth as your storage. 

Now if you see the numbered points.

1= Describes local level 1 CPU cache that is exclusive to the cores.

2= level 2 describes level 2/3 shared CPU cache that is shared among the cores.

3= Describes level 4 cache or RAM

4= Describes disks like hdd/ssd

5= Describes the internet

I hope this helped.",32,0,1,False,False,False,1645966950.0
t2leeq,hyne80a,t3_t2leeq,Fancy way of saying “it’s closer”,5,0,0,False,False,False,1645973787.0
t2leeq,hymzfrj,t3_t2leeq,"Making memory really fast is expensive. If you want to make RAM chips people can afford, you can not make them too fast. As a rough approximation, reading a byte from main memory takes \~100 nanoseconds. Keep in mind that modern CPUs execute at several instructions per nanosecond. However, reading adjacent bytes from main memory all at once also just takes 100 ns, as the data is transferred ""in bulk"".

So, it would be really bad to have to wait 100 ns every time you access main memory. But making it faster would make it much more expensive.

The solution is to add much smaller but much faster memory into the CPU. Since these are very small (1/1000th of main memory), you can make them really fast. This allows you to keep often used data in the cache. Also, most programs, when accessing some memory location, will access the adjacent ones. So since reading 64 acjacent bytes from memory is as cheap as reading one byte, it makes sense to just read all bytes and keep them around since the program is likely going to read them anyway in the very near future.

Another key latency reduction mechanism is that the cache is part of the CPU chip, and not a seperate chip somewhere else on the motherboard. It is literally ""nearer"" to the CPU parts doing the work.  


So, cache is faster because it's a much faster chip in itself, and because it exploits the fact that programs usually access data in the future that is ""next to"" data already accessed in the recent past. This is what spatial and temporal locality refers to: Over short times, data accesses remain in the same area.",2,0,0,False,False,False,1645965868.0
t2leeq,hymvj03,t3_t2leeq,"Locality means localness, caches are very small and very near the cpu so hitting the cache up for data is hundreds or thousands of times faster than hitting ram (and millions of times faster than hitting the harddisk) on the other hand becase they are so small you cant fit everything, if your data is large enough and you access it randomly then inevitably you will get RAM or even Disk latency.",1,0,0,False,False,False,1645963096.0
t2leeq,hymzoek,t3_t2leeq,"> Why are caches so much faster in transferring data

Data can be accessed 50x faster from the L1 cache as compared to normal memory. The speed of data access is different for L1, L2, and L3 caches but it is surely faster than accessing data from normal memory. Data access is very fast from the cache because it is intrinsically fast and is located close to the CPU core as compared to normal memory(DRAM).

The locality is the property of data that determines the likelihood of a particular set of data to be accessed by the CPU. The locality is utilized by the cache prefetcher to prefetch the data that will be required by the CPU. Prefetching is the act of storing data in the cache before it was required by the CPU. Prefetching saves CPU time that would have been wasted if the CPU had to wait for some data to be present in the cache.

The data to be prefetched is determined by the locality of the data also referred to as locality of reference. There are mainly 4 types of locality of reference:
- Temporal Locality
- Spatial Locality
- Equidistant Locality
- Branch Locality

According to the temporal locality, if a memory address is accessed at a particular time, it is likely that it will be referenced again in the future at regular time intervals or maybe not at regular intervals. If the temporal locality of a memory address is high, then it is worth investing in storing that data in fast memory to reduce the latency.

According to spatial locality, if a particular memory address is accessed at a particular time, it is likely that its nearby locations will be accessed. For example, if the first and second elements of an array have been accessed then it is likely that more elements will be accessed from that array so it is worth investing in storing that array in fast memory to reduce the CPU latency.",1,0,0,False,False,False,1645966024.0
t2leeq,hynunr4,t3_t2leeq,"Want to try explaining this in ELI5 fashion.  

There are different kinds of memory, you have cache which is like a note card you keep in your hands at all times. It's small, but easily accessible, and quick to read. 

You have RAM, which is like a small notebook that is in your backpack. It's a lot larger, but it is less accessible, and much more time consuming to reach. 

Locality is where you are keeping data that is more closely related. If you're in math class, you're going to want your note card to be filled with math notes and not English notes. The same principle here.  

In this case, spatial is talking about memory location. You're accessing your math notebook a lot so you add some more math notes that you haven't used yet, but they're in the same region as your new math notes.  

Temporal is talking about time, in this case what was the most recent note you've had to use.  The math formula the professor just put on the board is very likely to be relevant very quickly, so we keep that on our note card. 


This is how the cache operates. We have a small but very fast and accessible piece of memory that is filled with more carefully selected notes.",1,0,0,False,False,False,1645980572.0
t2leeq,hyp76b7,t3_t2leeq,"There's lots of tradeoffs that go into building memory. In particular, space cost and speed. Caching is how hardware manufacturers make the most of this. One of these tradeoffs is distance from the cpu. The electric signals move at finite speed. If you need to read memory through a bus, then you need to wait for the circuitry to find and send data across the bus. The kind of memory cells used also makes a difference.

There's two kinds of memory cells that can traditionally be used to make RAM: dynamic RAM (dram) and static RAM (sram).

Dynamic RAM is based on a capacitor, which can hold an electric charge. This is exactly what you want memory to do, so you just need to add some circuitry to read and write the charge in the capacitor. Hence, easy to make and takes less space. So when you read DRAM you're actually taking an analog reading of the voltage and converting this to a digital signal. This requires more prep work. But capacitors leak, especially at the nanoscale. You have to periodically refresh the DRAM cells by reading and then rewriting the stored bits. This can delay cpu reads (i think??) 

Static RAM is based on latches, which use logic gates that loop their output back as input to store bits. This stores the data in a digital form and does not require special refreshing. This makes reads and writes faster. But now you need a bunch of transistors for what dram can do with one capacitor per bit stored.

So L1 caches will probably be made of SRAM. But your main memory will be DRAM. The L1 will be much closer to the processor, both physically and in terms of what the circuits have to calculate to fetch the data. Main memory will be DRAM. It will probably be  a removable component. The processor has to access it through the memory bus. This is much slower.",1,0,0,False,False,False,1645999637.0
t2leeq,hyql9u4,t3_t2leeq,"Why would Amazon build fulfillment centers around the county? Couldn't they just ship directly from Seattle or the factory in China every time someone order something?

The answer is basically the same, each hardware pieces (CPU, RAM, disk) is like its own location. Copying/moving data between them takes time/latency that can be measured.",1,0,0,False,False,False,1646022383.0
t2leeq,hymq5s0,t3_t2leeq,"I can tell you that sentence means ‘the cache is closer in space and time than other kinds of storage’.

As to what that means and why that’s important, I dunno. Probably something to do with not having to jump around so much in memory? Is that even a thing any more? SSDs don’t have a ‘reader arm’ that physically needs to move. But many machines are still HDD.

Obviously e.g. a browser cache is stored locally, so retrieving it should be way faster than getting it from its source over the net.",-4,0,0,False,False,False,1645958871.0
t2leeq,hynnz7l,t1_hymqa6b,Thank you so much!!,4,0,0,False,False,True,1645977916.0
t2leeq,hyno0xv,t1_hyn14tj,Thanks!!,1,0,0,False,False,True,1645977936.0
t2leeq,hyof6f1,t1_hyn14tj,what a fantastic explanation.,1,0,0,False,False,False,1645988479.0
t2n8jw,hynooj3,t3_t2n8jw,"The standard uses only a single bit for sign followed by eleven bits for exponent bias(assuming 64 bit).

Exponent bias is sometimes referred to as offset. The standard allows for more positive numbers and lexicographical order. Negative numbers usually have exponent bias starting with 1.",2,0,0,False,False,False,1645978202.0
t2n8jw,hyqui0a,t3_t2n8jw,"\> a normalised floating point number can only start with 01 or 10 depending on whether it’s positive or negative

That is not the case. Here's how [single-precision floating point numbers](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) are stored. The first bit is always the sign bit (0 for positive, 1 for negative), then 8 exponent bits, and then 23 fraction bits. The fraction bits of the number will be shifted (or normalized) so that the most significant set bit is hidden. ""110000..."" will be stored as ""10000..."" and ""0001010101..."" will be stored as ""010101..."".",2,0,0,False,False,False,1646027548.0
t2n8jw,hynr9if,t1_hynooj3,"Hmm right ok. So I understand the fact that like negative numbers start with a 1 usually and like that normalisation is for maximising precision and range, but why can they not start with 11? Because when they’re normalised don’t all negatives have to start with 10?",1,0,0,False,False,True,1645979231.0
t2n8jw,hyo8voa,t1_hynr9if,"Those would represent de-normalized values outside the set of normalized numbers. It can vary per implementation but those are often used for signed zero values, subnormal numbers, representations of infinity(+,-), etc.

Sometimes the leading bit is not used as it's implied based on the exp. Sign is based on leading exp bit xor value therefore 00, 11 would not be used.",1,0,0,False,False,False,1645986041.0
t25gm6,hyjwq2m,t3_t25gm6,"The object doesn't need to be stored on the heap. It can be stored basically anywhere.  
 
But yes, it's accurate to say that if an object is passed to a function by reference then the function can change the original object.",26,0,0,False,False,False,1645905167.0
t25gm6,hykuh4f,t3_t25gm6,"Compilers are free to implement references using any mechanism they like (including by using advanced optimization to completely optimize entire function calls away)

But yes, one theoretically logical way to to think of it is that the object is put on the stack and a pointer to the object is passed into the function (which is OFTEN how a compiler will actually implement that type of code)

Keep in mind that what you typed was a pass by reference, and it's not really consistent to talk about what the compiler may or may not  do when talking about the logic of some code, as that is compiler specific and only really useful when doing micro optimizing on a fixed code base on a specific platform and a specific architecture etc, etc.

Better to use the vernacular of the programming language: A reference to the object was passed in and that reference was modified (implementation is thankfully not important when understanding the meaning in any programming language)

So No, we cannot say that.",3,0,0,False,False,False,1645919624.0
t25gm6,hykx1wf,t3_t25gm6,"When passing a variable to a function by reference, that variable usually resides on the stack.
Note that when passing object references (basically pointers), where the heap is often involved, that is still pass by value (i. e. passing the reference by value), despite the potentially confusing terms.",1,0,0,False,False,False,1645920784.0
t25gm6,hymmucn,t3_t25gm6,"The practical application of this is the buffer overflow.  C and C++ were extremely vulnerable to these types of things for the exact reasons we are discussing here.  You can overflow the data stack or the heap.  

Thats why they teach this in CS.  If you didn't know the differences in passing by value vs ref then you could accidentally crash stuff (or on purpose.)  

And it was easy too.. you just pass pointers and then swap a short int with a float and you'd immediately crash the kernel.  You could also use the data stack to buffer bad data and abuse the trustef kernel access during runtime and you could essentially write anything you wanted.",1,0,0,False,False,False,1645956229.0
t25gm6,hykikpa,t3_t25gm6,only if you have the perimeter set to '&mut',-1,0,0,False,False,False,1645914442.0
t25gm6,hyjxiki,t1_hyjwq2m,"Thanks. Does that make the function ""impure"" then, even though it may appear to be following the rules of a pure function?",3,0,0,False,False,True,1645905499.0
t25gm6,hymg6g4,t1_hykuh4f,"I learned what he is asking about in college.  Some of the CS theory is deprecated but his fundamentals are still solid.  Pass by reference hasn't changed.  It's just really ""how it works"" anymore but the main takeaways with pass by reference/value and pure functions is absolutely still relevant to develop good programming habits.  


Pass by reference vs value is important to know even if its older theory.",2,0,0,False,False,False,1645951103.0
t25gm6,hykxzfl,t1_hykx1wf,"It depends on the computer language. In Java for example, which has primitives like **integer** and **boolean**, those variable types are on the stack, but if it's an object then the stack only holds a name + pointer to the object on the heap. In Python, I believe that everything is an object, which suggests that the only thing that is on the stack is the variable name and a pointer.",1,0,0,False,False,True,1645921210.0
t25gm6,hyjybmz,t1_hyjxiki,"Mutating state in this way is a violation of the rules of pure functions. It would not ""appear to be pure""",19,0,0,False,False,False,1645905841.0
t25gm6,hyk4cb3,t1_hyjxiki,"Depends on how you define ""pure"". One way to define it is that it doesn't modify anything other than its arguments.",0,0,0,False,False,False,1645908390.0
t25gm6,hymjxad,t1_hymg6g4,"I certainly would not say it isn't important, I'm just saying that one should use the term pass by reference when they talk to other programmers rather than talking about what a compiler may or may not do in relation to that command",1,0,0,False,False,False,1645953973.0
t25gm6,hykdm76,t1_hyk4cb3,"I think most implementations of functional paradigms consider argument modification a no-no. If you need to modify an arg, you return it as a new object instead.",11,0,0,False,False,False,1645912315.0
t25gm6,hyleq4s,t1_hyk4cb3,"There is only one definition of pure, and it requires that there be no possible side effects to the call. That means you never, ever change anything that has a reference outside the function. That's a hard rule with no exceptions - anyone using an 'alternative definition' doesn't have a different opinion, they're just wrong.
 
You can take those arguments and store a modified version of them, but the original argument absolutely must stay the same. What you might be thinking of is swapping out the reference to something else - which is fairly pointless and a bad programming practice (e.g. ""myArg = myArg.someOperationReturningACopy()""), but won't modify anything outside the function and so is ok from a functional programming perspective.",4,0,0,False,False,False,1645928974.0
t2pdve,hynd4cw,t3_t2pdve,"The number of people that deliver the book to your doorstep may increase the number of books that can be delivered at once but it can't change the rate at which you can read them. Likewise, the number of books that can be delivered from a certain publisher can't change the rate at which the books are produced. That's the simplest analogy that I can think of.",5,0,0,False,False,False,1645973289.0
t2pdve,hynbv0o,t3_t2pdve,"Update: [https://www.chegg.com/homework-help/questions-and-answers/consider-figure-shows-time-download-file-also-known-page-load-time-plt-webserver-decreases-q45543120](https://www.chegg.com/homework-help/questions-and-answers/consider-figure-shows-time-download-file-also-known-page-load-time-plt-webserver-decreases-q45543120)

Found someone who could explain but unable to view it :(",1,0,0,False,False,True,1645972700.0
t2pdve,hyqmo92,t3_t2pdve,"Let's say that digital signals travel at the speed of light. And imagine that for any network connection, it travels in a straight line from point A to point B.

Latency would be the time it takes for that signal to travel from point A to point B. If that distance is 100 miles west or 100 miles east, the latency would be the same. In other words, this is related to the length the signal has to travel.

Bandwidth corresponds to the width of the pipe. A ""narrow"" pipe could maybe send 1 million bits per second, while a ""fat"" pipe could send 100 million bits per second.

Note that changing bandwidth will not make the pipe shorter or the speed of light faster (i.e. bandwidth doesn't affect latency). Similarly, making a pipe longer won't make it wider or narrower (i.e. latency doesn't affect bandwidth). This is a bit of an oversimplifcation, but hopefully that makes the concepts clearer.

Now, assume we have four different pipes

* Short, narrow pipe (100 miles, 1 Mbps)
* Short, fat pipe (100 miles, 100 MBps)
* Long, narrow pipe (1000 miles, 1 Mbps)
* Long, fat pipe (1000 miles, 100 MBps)

Let's also assume that we have a web page that's got some text, some images, and some large videos. On the fat pipes, the text and images will load quickly and the videos will play without hiccups or pauses. On the narrow pipes, the videos might have problems, but the text and images could still be fine (depending on size). In those cases, the fat pipe wins over the narrow pipe.

Now, imagine you've got a not-best-optimized commerce site. You could have a web page that first loads your account, then loads all your orders, then for each order loads the items in that order. Each of those web page API calls (or DB like queries) requires a ""round trip"" to get the information, so latency can dominate the load time instead of bandwidth. In those cases, the short pipe would load faster than the long pipe.",1,0,0,False,False,False,1646023115.0
t2pdve,hys4l6j,t3_t2pdve,"At least one of the reasons is that a single web page isn't a single resource that the browser downloads in one go. A typical page rather consists of a number of resources: HTML, style sheets, images, scripts, content dynamically loaded by those scripts. The HTML page includes references to the other resources such as style sheets, JavaScript code, possibly images, etc. The browser has to load at least a part of the HTML before it can even begin to request for the JavaScript files, style sheets, and any other resources the HTML refers to.

If some content -- say an image -- is not referenced to by the HTML but is loaded by a script instead, that adds another layer of indirection: the HTML has to be loaded first, then the script that the HTML points to, and then the image that the script loads. Each of these would be a separate request from the browser to the web server.

Moreover, even if the browser has the URLs for all other resources such as images after loading the initial HTML and can start loading multiple ones in parallel, browsers probably limit the number of simultaneous connections they open to the same domain. (This at least used to be the case. I don't know how recent versions of browsers behave.) If a page had a lot of images, for example, some of those images would need to finish loading before the browser would request for the next one.

Some scripts might also only load after some other script has finished executing or some resource has been loaded. Executing the scripts could also take non-negligible time sometimes.

So, let's say you have, for one reason or another, ten requests that the browser makes in sequence, the next one only started after finishing the previous one. Let's also assume you have, say, a 50 ms round-trip time to the server, i.e. there's a latency of 25 ms before the first byte of the request transmitted by your computer reaches the server, and another 25 ms on the way back. This is (at least theoretically) independent of the throughput of the connection.

Even assuming infinite throughput (bandwidth) and no other delays, both of which are unrealistic, those ten requests in sequence would take a minimum of 10 * 50 ms = 500 ms just from the latencies of getting the *first byte* of every resource request and response through (plus the initial latency of the TCP three-way handshake).

You can take a look at how this unfolds in practice with the web developer tools in Firefox or Chrome. Open the ""network"" tab, open or reload a page (perhaps one that you don't have in browser cache already), and take a look at the chart of the resource loading timeline.

Edit: clarified a bit",1,0,0,False,False,False,1646058569.0
t2pdve,hyr6srm,t1_hynd4cw,"Thanks for the analogy. What is the ""rate at which you can read them""? Is it the processing time? Are you saying PLT is affected by not just transmission time and propagation time but also processing time?",1,0,0,False,False,True,1646035978.0
t2pdve,hyr6ee6,t1_hyqmo92,Thanks for the analogy. Assume I have 0.5Mb of data to load from the web server. The increase in bandwidth will decrease the transmission time 0.5Mb/1Mbps vs 0.5 Mb/100Mbps. Why does page load time not decrease with bandwidth?,1,0,0,False,False,True,1646035676.0
t2pdve,hyv2dbs,t1_hys4l6j,">Thanks for the elaboration. To clarify, to load a page, browser would require to request for multiple resources, and the multiple RTTs due to back n forth of request and response will much out weight the size of each data coming back from the server, hence an increase in b/w will make no difference to the PLT? (Correct me if im wrong! I assume the data to server is also very small since its just a get request?)",1,0,0,False,False,True,1646102365.0
t2pdve,hysvj1u,t1_hyr6ee6,"If your pipe is really short (say 0.01 microseconds), then the data load dominates the total page load time. But if your pipe is really long (say 5s latency), then any back and forth communication (say authentication, SSL handshakes, etc.) would dominate the total page load time.",1,0,0,False,False,False,1646069523.0
t2pdve,hywxgnn,t1_hyv2dbs,"That's the gist of it. I'm not really an expert on this but ""multiple resources, each with a fixed minimum cost regardless of bandwidth"" should be at least most of the answer.

Bandwidth (and the size of the resources) can still be a significant factor if the total size of the resources is large and the bandwidth is small. The chart you linked to also indicates that. But yes, multiple back and forths due to multiple small resources would explain why increasing bandwidth gives little benefit beyond a certain point.

If you take a look at the request timing for example in Chrome developer tools, you can see an overall picture of the loading times of various resource in the waterfall chart. You can also mouseover the timespan of an individual resource in the chart to get a breakdown of what the time was spent on.

For some resources I get the on a random local website I just tried, some larger individual resources such as a half megabyte javascript bundle and some fonts had most of their loading times spent on downloading. Most other resources had the time either dominated by time to first byte or more evenly distributed.

Some requests also spent some time in the queue before being sent. Some time was also spent setting up an SSL connection, but that would probably only happen on the first page load to the site. There was also a gap with nothing happening in the network between the last resource whose loading was initiated by the HTML page and the first resource that was loaded by a script, so script execution time also seemed to add some time between the requests.",1,0,0,False,False,False,1646144750.0
t2kkbb,hymkykm,t3_t2kkbb,"You're confusing the amount of memory allocation (10 bits, or 1 byte and 2 bits as you correctly put it) with the number of possible values that can be stored in that amount of memory (2\^10 = 1024 different possible stored values, only one of which can be stored in that amount of memory at any given time).",35,0,0,False,False,False,1645954765.0
t2kkbb,hymm5re,t3_t2kkbb,"1 bit can store 2 possible values

8 bits (1 byte) can store 2\^8 possible values

10 bits (1 byte and 2 bits) can store 2\^10 possible values

1024 bits (1024/8 bytes = 128 bytes) can store 2\^1024 possible values

8192 bits (8192/8 bytes = 1024 bytes) can store 2\^8192 possible values

&#x200B;

I hope that clears things up.",11,0,0,False,False,False,1645955699.0
t2kkbb,hymkpq2,t3_t2kkbb,"Using 1 bit you can store two numbers 0 & 1, not 1 & 2. Each bit can be either 0 or 1. Decimal 2 would need two bits 10.",5,0,0,False,False,False,1645954578.0
t2kkbb,hymowmg,t3_t2kkbb,"You are getting confused between counting in base-2 and actual memory allocation. You are also counting binary values incorrectly. A binary digit is only ever 0 or 1. To get the decimal value “4”, you would need to have 3 bits, since the maximum you can have with 2 bits is “11”, which is of course in decimal is “3”.
Secondly, memory allocation is different to binary values. One byte is made up 8 bits, which can hold one of 2^8 possible values. In binary this can range from 00000000 to 11111111, which in decimal is represented as 0 to 255 (or in hex 00 to FF). Remember that one byte can only hold one value.
Side note: to get a number larger than 1 byte from memory, these need to be stored in separate memory locations as separate bytes. Different processors have different ways of doing this (little-endian and big-endian) which determine which byte to store first.

Edit: So 2^10 in terms of memory allocation simply means 2^10 = 1024 bytes",3,0,0,False,False,False,1645957873.0
t2kkbb,hymue7s,t3_t2kkbb,"2\^10 is 1024 the same way 1+1 is 2. It's not ""1024 bytes"" the same way 1+1 is not ""2 feet"".

You are basically asking us why ""1+1 is 2 feet but 2+2 is 4 miles? How does that make sense?"".

The answer, of course, it that it does not. You can't simply add units to numbers and hope that it makes sense.

""2\^10 *bytes*"" is ""1024 *bytes*"". ""2\^10 *bits*"" is ""1024 *bits*"". ""2\^10"" is just 1024. One byte is 8 bits, or 2\^3 bits. So 1024 bits is 128 bytes, roughly.",3,0,0,False,False,False,1645962229.0
t2kkbb,hymkhim,t3_t2kkbb,I get the storing numbers part but how is 2\^8 = 8 bits. Could you please elaborate?,1,0,0,False,False,False,1645954404.0
t2kkbb,hymlqez,t3_t2kkbb,"I may be wrong buy let me see what I can do to help you

So a Bit in Essence stand for Binary Digit ie it uses 2numbers 0 and 1 to represent data.

So 2^1 =1 says that it can represent a combination of 0s and 1s to represent a single Binary information

2^2 = 4 says that we can represent 4combination of numbers using 0s and 1s

Similarly 2^3 = 8 says that we can represent a combination of 3binary digital.
I.e it maybe be 000 ,001, 010 and so on upto 111
The 111 is equivalent to 7 in decimal number system if I'm correct

So 2^10 would be able to represent numbers from 0 to 1023 in Binary number system so basically we got 10 places to add different combination of 0s and 1s to represent numbers 

__ __ __  __  __  __  __  __  __  __

Coming back to the byte part it's not that 2^8 bits is a byte rather a collection of 8bits is a byte 
So it would be 2^3 =8

I maybe wrong on a few things but hope this helped 😅",1,0,0,False,False,False,1645955366.0
t2kkbb,hympw4v,t3_t2kkbb,"base 2 counting means if there is a non 0 value in a position, it indicates a value of x^y, where x is 2 and y is the position of the 1, where position counting starts from 0.

so 0001 in binary is 0^3 + 0^2 + 0^1 + 2^0 which = 1

10 is 0^0 + 2^1 which = 2

1000 is 2^3 + 0^2 + 0^1 + 0^0 which = 8

1010 is 2^3 + 0^2 + 2^1 + 0^0 which = 10

and so on",1,0,0,False,False,False,1645958654.0
t2kkbb,hymrl67,t3_t2kkbb,"Lets start with the decimal numbers.
There are 10 digits as you know.
0

1

2

3

4

5

6

7

8

9

If you want to get the next number what do we do? We are out of digits at this point. We use the same digits! We just add a count of how many times we ""ran out of digits"".
We went through 0-9 once. Here while counting from 0-9 we did not run out of digits right all digits from 0-9 are unique. We can assume this as we ran out of all the digit 0 times when counting from 0-9.

Once we hit 9 there are no more new symbols. So we add 1 to the right as start over again. So this time it is-
1...0

1...1

1...2

...

...

1...9

And we ran out of digits again (0-9). What do we do? What comes after 1? Thats right! 2!

Counting in binary is THE SAME.
Only difference you are limited tp two digits or symbols.

0

1

We are already out of digits. So what would be equivalent of 2 here? There is no symbol for two in binary! We do the same thing. Using 0 and 1 we ""ran out of all the symbols/digits. So for the first 0 and 1? Number of times we ran out of the symbols=0 times.
What comes after 0? 1! Is it in the binary numbers? Yes! So use that!.

0

1 (all symbols used up)

count how many times we ran out of symbols and repeat for all the symbols

1... 0 = 2

1... 1 = 3

Now after 1 there should be 2, but there is no 2 in binary! But there IS a representation of 2! That is 10 (just see previously )
So for the number 4, we need 2 for how many times we ran out the symbols followed by 0.
I.e.

10... 0 = 4
10... 1 = 5 (ran out of symbols 3 times till here)

For six? How many times we ran out? 3.
So

11... 0 = 6
11... 1 = 7

And it goes on.

Now that you see that we count the same way regardless of the base.

What you are getting confused have to do with powers.

What is 10²? = 100. How many digits does 100 have? 3 digits right? But 100 is a lot bigger number than 3!

When you write 2^8 what you mean is, 2x2x2x2x2x2x2x2. Use a calculator and find out what that number is. That is its value.
But how many BITS does it take? I.e. how many digits do you need to write 2^8? Thats right, 8 digits. So 2^8 gives you 256 (value) that can be represented with the help of at least 8 bits.

Same thing for 2^10.

The value of 2^10 is 1024. And you would need atleast 10 binary digits to represent it.
1111111111 (10 digits) in binary = 1024 in decimal.",1,0,0,False,False,False,1645960020.0
t2kkbb,hyn3b35,t3_t2kkbb,"2^10 means you have 10 bits. With 10 bits (which IS a byte and two bits), you can store 1024 different numbers. So it's not 1024 bytes, its 1024 possible values",1,0,0,False,False,False,1645968243.0
t2kkbb,hynh7hs,t3_t2kkbb,"2^10 isn't 1024 bytes.

There are two things to keep in mind. 1) The number of total bits and 2) The largest number those bits could represent.

For example 2 bits is just two bits. Either 00, 01, 10, or 11. There are 4 possible ways you can write down 2 bits. But, you only have 2.

It's completely made up how we choose to represent those bits. For example,
00 could represent 0
01 could represent 1
10 could represent 2
11 could represent 3

But, it's made up because it could also represent something different.

00 could represent 1
01 could represent 2
10 could represent 3
11 could represent 4

But, then you have no 0. So, this is less common. Further it could be something completely arbitrary.

00 could represent RED
01 could represent BANANA
10 could represent PUTIN
11 could represent YO MOMMA (cause she so big)",1,0,0,False,False,False,1645975100.0
t2kkbb,hypz9fx,t1_hymkykm,"e.g. the (unsigned) binary number 1111111111 is 1023 in decimal but it has 10 digits. equates to 1024 values when u include 0. 

(also 2 bits is 0-3 not 1-4 as stated, although 0-3 is 4 different values)",4,0,0,False,False,False,1646011982.0
t2kkbb,hyn1qx4,t1_hymkhim,"The thing is it's technically not. 2^8 is the possible amount of unique combinations given 8 bits.

So for unsigned integers [0,255].",2,0,0,False,False,False,1645967323.0
t2kkbb,hyqmot3,t1_hypz9fx,"Good point. It's important to remember that the largest positive integer we can store is actually `2**bits - 1` because the smallest number is, of course, zero. And that if we want a signed integer, we're automatically restricted to half as many possibilities on either side of zero because we need one of those bits for the sign.",3,0,0,False,False,False,1646023123.0
t2kkbb,hyr9hg5,t1_hyqmot3,"well a signed integer would have the same amount of possible values, just half is below zero. a 10 bit signed integer is from -512 to 511, which is 1024 different values. (tbh i think getting onto twos compliment binary is a bit much for this question)",2,0,0,False,False,False,1646038055.0
t1jz33,hygmuwb,t3_t1jz33,"It is irreversible because you only know the result of operations, but there can be up to infinite number of inputs that can generate that output. Also in sha256 you use shift operations so you also lose information, so every time you shift 3 you have 3 unknown bits that now you have to guess. To clarify what I said, if you only know the result of addition there is infinite number of inputs that generate that number (3+7=10, 5+5=10...), that is also true for modulo operations. About your question about brute force being faster, it's just because of the fact that when reverse guessing you would need much more computation ( since you need to trace the steps back ) than just brute forcing it.

Edit: I really recommend this video. He even wrote scripts that visually show how every operation that is used work, from basic (XOR, SHIFT, ROTATE) to more complex combinations of operations (Sigma for example).

[https://www.youtube.com/watch?v=f9EbD6iY9zI](https://www.youtube.com/watch?v=f9EbD6iY9zI)",38,0,0,False,False,False,1645840521.0
t1jz33,hygm219,t3_t1jz33,"Maybe I’m misunderstanding your question, but if you’re asking about taking a SHA256 hash and getting the original input as a string, the short answer is that it is not possible. The SHA algorithm is a one-way hashing function. It is not a lossless compression algorithm. There is information lost when converting the input to the corresponding hash output.",10,0,0,False,False,False,1645840138.0
t1jz33,hyhkupz,t3_t1jz33,"At a technical level, being ""expensive to reverse"" isn't really what's going on. Hash functions, by their very nature, are non-invertible functions, since there are an infinite number of possible inputs and a finite number of possible outputs. Instead, the cryptographic security of a hash algorithm is (generally) determined by three types of resistance it should have:

* **Pre-image Resistance:** Given some desired hash value `h`, it is computationally expensive to find some message `m` such that `hash(m) = h`.
* **Second Pre-image Resistance:** Given some message `m1`, it is computationally expensive to find some message `m2` such that `m2 ≠ m1` and `hash(m2) = hash(m1)`
* **Collision Resistance:** It is computationally expensive to find *any* pair of messages `m1` and `m2` such that `m1 ≠ m2` and `hash(m1) = hash(m2)`.

As to how hash algorithms achieve this, mostly they just do a lot of processing of the data to make it infeasible to predict how changes to the input message affects the resulting hash. This is often achieved by performing ""rounds"" of processing the input data that accumulate changes with each round. This is also how most encryption algorithms work, just with fixed-size inputs instead of the variable-sized inputs of hash algorithms.",9,0,0,False,False,False,1645858876.0
t1jz33,hyipsv5,t3_t1jz33,"The key missing piece of information lies in the prevention of 'step by step reversal' to walk back to a collision. This is not immediately possible, due to a key feature of the compression mechanism. You can read up on SHA's compression mechanism via the link in the SHA wikipedia page https://en.wikipedia.org/wiki/One-way_compression_function#Davies%E2%80%93Meyer.
 
The key statement is at the beginning:
 
>feeds each block of the message (m i) as the key to a block cipher. It feeds the previous hash value (H i-1) as the plaintext to be encrypted
 
What this means is the idea of trying to walk back step by step **can't occur in isolation from previous steps**, because what the algorithm does changes in response to what the algorithm was given **one step prior to this step**. If you had some algorithm 'BAD' that was just adding 1 to each character, i.e. 'test' becomes 'uftu' in one step, and 'vguv' in the next step, then knowing this  BAD algorithm you could reverse the step 'vguv' to 'uftu' easily, then to 'test' just as easily, no matter how many steps. 
 
But consider if BAD didn't add 1, but instead it added the number of the first character of the previous step. So step 1, there is no previous step, so it's just 1. 'test' becomes 'uftu' as before. But then 'uftu' isn't advanced 1 step, it's advanced 't' steps, i.e. 20 steps. 'uftu' thus becomes 'ozno' instead.
 
Now consider trying to go backwards. You know the end result is 'ozno'. You don't however know what the algorithm did to get there - did it advance by 1, 2, 3? This method is simple enough that it is fairly easy to bruteforce and see what it was, but you can't just reverse the step in one quick decision, and you have to do that bruteforcing for every step. Making the bruteforcing take long enough is the key to the collision-resistant feature of SHA.

Further to this, in SHA it's not repeated steps but the next chunk of the data. The analogy mostly holds true, but it's made more difficult in SHA because you can't bruteforce it as easily when there isn't such a direct relationship between steps.",2,0,0,False,False,False,1645887293.0
t1jz33,hyh4gu0,t3_t1jz33,"I don't know about SHA 256, but i can tell you about RSA encryption. RSA basically uses modular arithmetic to convert an existing message to an encrypted form.

In RSA you select 2 distinct primes p and q. And n=pq. 
Select a number e such that greatest common divisor of e and (p-1)(q-1) is 1. 

This (e,n) is your public key. This you share with everyone including potential hackers.

Select a d such that de-1 is divisible by      (p-1)(q-1).

(This might seem complicated because it is but feel free to ask)

This (d,n) will be your private key. This you share only with the reciever.

The entire magic here lies in n. Out of the two numbers you as a hacker know (e,n) you know that one of them is a product of two primes. The key in decrypting the message is finding the primes. Once you find them you can also find the private key and the message is decrypted.

But finding the two primes is the most computationally expensive task. Modern RSA uses 50 digit primes multiplying them gives you much larger number n. There is no algorithm to find the primes in a time which is useful even for the most powerful computers.

It is like finding a needle in a haystack.

There is a lecture on YouTube from MIT where they explain this.",2,0,0,False,False,False,1645849105.0
t1jz33,hyiczuk,t3_t1jz33,"There may or may not be a simple, cheap function that reverses a hash function such as SHA256, but nobody knows one. They're usually treated as not reversible, because they're deliberately designed to be not reversible and it's a really painfully hard problem to solve. Nobody has managed. Nobody knows for sure whether it's really reversible or not, but as a human thing, everyone who has tried has failed.",1,0,0,False,False,False,1645880418.0
t1jz33,hyjr2k9,t3_t1jz33,"The step where you throw away the input is pretty tricky to reverse.

Think of a hash less as a converted version of its input and more as the accumulated side effects of running the input through the hash function.",1,0,0,False,False,False,1645902859.0
t1jz33,hymjm4l,t3_t1jz33,"SHA-2 was intentionally designed to be one way.  You can hashing algorythms that aren't intentionally meant to be obtuse.  SHA:256 is used mostly as part of trust based validation for digital cert signage, etc.

Secure CA structures can use SHA 256 cert hierarchies becsuse the values are always unique to the time they were run.  

All I care about is that the SHA hashes match.  It's like buying a godaddy cert.  The CA creates the hash for the cert and then you get it.  It is an easy way to eatablish trust and authenticity because the data can't be forged.

If a malware tries to masquerade as a legit program for example, I check the hash values and if there isn't a match, no dice.

You can see what the SHA 256 hash does.  The hash output is created by transforming two data elements and merging them into one output.  This is the compression element.  It also encrypts by block cipher.  

So the hash is not only merging data values, it then also encrypting in blocks.",1,0,0,False,False,False,1645953731.0
t1jz33,hygprmh,t1_hygmuwb,Thanks for the youtube video -- it was exactly what I'm looking for!,8,0,0,False,False,True,1645841900.0
t1jz33,hygmetz,t1_hygm219,"Right -- but you could get an input string which results in the same output. (a collision)

Based on reading your comment -- I assume that finding an input string which generates the same output has no relevance to anything?",3,0,0,False,False,True,1645840306.0
t1jz33,hyhsjwo,t1_hyh4gu0,The only popular encryption algorithm that uses prime numbers is RSA,1,0,0,False,False,False,1645864543.0
t1jz33,hymhfp2,t1_hyh4gu0,"The RSA algorithm isn't hash based, it uses key pairs",1,0,0,False,False,False,1645952061.0
t1jz33,hygn88s,t1_hygmetz,"Depending on the reason for wanting to produce a collision, this could be a success. If you want to find some input that produces a target output, then a collision means you’ve succeeded. You may not know if it was a collision or the original input, but that may be irrelevant.",4,0,0,False,False,False,1645840697.0
t1jz33,hygrjok,t1_hygmetz,Some corollary reading: https://crackstation.net/hashing-security.htm,3,0,0,False,False,False,1645842744.0
t1jz33,hyhxl2m,t1_hyhsjwo,I'm not really familiar with encryptions. Truth be told I'm only familiar with RSA and diffie Hellman exchange.,2,0,0,False,False,False,1645868564.0
t1jz33,hyhz1gn,t1_hyhsjwo,I have edited my comment,2,0,0,False,False,False,1645869741.0
t26lo8,hykfwth,t3_t26lo8,"I really like mit's material. This lecture is a bit old now but i really like the tutor:

https://youtu.be/whjt_N9uYFI",2,0,0,False,False,False,1645913302.0
t26lo8,hyk41wk,t3_t26lo8,"this was okay, but really a shallow dive... is there a better video?

https://www.youtube.com/watch?v=6aDHWSNKlVw",0,0,0,False,False,True,1645908272.0
t26lo8,hyk3zxm,t3_t26lo8,I don't want to ask for help solving a specific equation here because it won't help me understand what's happening...,-2,0,0,False,False,True,1645908250.0
t26lo8,hylxma3,t3_t26lo8,Try fixing your computer.,-2,0,0,False,False,False,1645938736.0
t26lo8,hym2b7k,t1_hykfwth,Thanks,1,0,0,False,False,True,1645941491.0
t26lo8,hym2aqg,t1_hylxma3,Theory. Not hardware,1,0,0,False,False,True,1645941482.0
t26lo8,hym37s3,t1_hym2aqg,Well you can always restart your computer to find good sourcres,0,0,0,False,False,False,1645942058.0
t26lo8,hym3brb,t1_hym37s3,"i can google things, doesnt mean the results will be high quality, that's what i did and i struggled",2,0,0,False,False,True,1645942128.0
t26lo8,hym3t1n,t1_hym3brb,;/,1,0,0,False,False,False,1645942432.0
t1fiti,hyfwtuq,t3_t1fiti,"I have done this using the unweighted condition w1 \* w2 \* w3 \* … \* wn > 1. It does work, however it must also account for slippage to make sure it is truly a profitable opportunity, ie for each trade amountOut = price \* amountIn \* (1 - slippage), where slippage is like 0.003. I found that w1 \* w2 \* w3 \* … \* wn > 1 + Sum(slippage\_i) works for this, where slippage\_i is the slippage of the ith trade in the sequence. Then the optimal amount to arbitrage is the amount such that after the arb trades occur, the condition w1 \* w2 \* w3 \* … \* wn = 1 + Sum(slippage\_i) is met. Note that I didn't formally derive the truth of this, but figured out it worked empirically. Thats just to say the Sum(slippage\_i) might not be the exact optimal value to add to 1, but I found it to be close enough, where variations to this number up and down resulted in smaller returns.

BTW it was profitable to do this in the crypto markets for a time for me, but ultimately was beaten by those who could get their arbitrage transaction to be executed faster than ours. Arbitrage is very competitive and big / institutional players have a large advantage.",6,0,0,False,False,False,1645828852.0
t1fiti,hyh92oe,t3_t1fiti,Log transform keeps the numbers smaller. The product is basically exponential in the length of the path.,2,0,0,False,False,False,1645851567.0
t1fiti,hz0egeu,t3_t1fiti,"This doesn’t work outside of academic theory. Bots pay top dollar to run in eg, the NY11 data center.

Since they are in the same building as the exchange, that nickel difference has already been traded hands a few times before your feed loads.Assuming you find a “glitch” the order will never fill. 

Consider a graph with edge weights equal to volume * price and multiplying that by a second graph of correlation weights (hint encode graphs as matrices)

This gives you a mechanism to compare longer term trends and negate the HFT advantage.

https://en.wikipedia.org/wiki/Adjacency_matrix",1,0,0,False,False,False,1646196412.0
t1fiti,hygk7la,t1_hyfwtuq,"Thank you for your input. I figured that method should work it makes a lot more sense to me. We will see how it goes. I am hoping that I can at the least better plan my orders using this information to maximize my throughput and profitability. I am curious to try and find ""week spots"" in the market that let me place limit orders close to mid market price in the hopes that I will be able to capture profit when the price moves.",3,0,0,False,False,True,1645839264.0
t1fiti,hyhskv5,t1_hyh92oe,"Ah thank you so much for this. I hadn't even thought about how the potential exponential growth along a path can lead to overflow issues. Although, I wonder how much this is an issue considering that under ideal conditions all loops in a network of financial markets should have a product that equals 1.",1,0,0,False,False,True,1645864562.0
t1fiti,hyk5jnz,t1_hyhskv5,"Products in (0,1) can get exponentially small which can also be a problem, this shows up for instance in the Viterbi path algorithm on Markov chains.

In general multiplying numbers is slower than adding them.",1,0,0,False,False,False,1645908899.0
t1vfy6,hz0d6w5,t3_t1vfy6,"Indexes are typically some form of b-tree structure. The naïve implementation would be an alphabetical list of values with each letter representing a leaf in the tree (eg h-e-l-l-o-w-o-r-l-d)

Then you can binary search the first letter to find the h node. Next binary search it’s children for e and continue.

This finds a value in O(log n) steps worst case. So having 10 vs 100000000 values in a table is only 7x slower compared to 10^6 more data

https://en.m.wikipedia.org/wiki/B-tree",1,0,0,False,False,False,1646195768.0
t1i3op,hyg8jm9,t3_t1i3op,"I can't speak as to the difficulty of programming video codecs, but you're correct as to the size of movie files. It goes back to the CD-R days. Similar reason as to why many newer movies are released in 2.35GB or 4.7GB -- so one or two movies will fit nicely on a DVD-R.",8,0,0,False,False,False,1645833875.0
t1i3op,hyh5noa,t3_t1i3op,"If you mean coming up with the algos to do it, then very, top 5% programmer minimum, if you mean using existing algos and libraries, anyone can do that just fine",5,0,0,False,False,False,1645849727.0
t1i3op,hyiklgj,t3_t1i3op,"To come up with the theory behind it, not a single person but a group of people took decades to develop the theories ever since the early 1900s. 

They span your average intelligence like everything else, some are really intelligent, others aren’t as intelligent but they all  did work really hard, scientists but also business people who all need accountants and janitors and waitresses to bring their food in restaurants. It’s not something you do on your own.

For lossy compression you find in JPEG, MP3 and others, it’s a lot of psychophysics to know what the human mind does and doesn’t perceive. For lossless compression like FLAC its basically signal and information theory (Shannon).",4,0,0,False,False,False,1645884718.0
t1i3op,hyiunaa,t3_t1i3op,[How does JPEG work?](https://youtu.be/Kv1Hiv3ox8I) Downsizing a picture is wild enough. I can’t imagine a movie.,3,0,0,False,False,False,1645889488.0
t1i3op,hyh3sio,t3_t1i3op,"You can always target a file with lossy compression down to any size, though the fidelity will take some serious hits the smaller size you go.

The article you read was likely aiming for the 80-minute CD standard as shown in this [wiki article](https://en.wikipedia.org/wiki/CD-ROM):

>The playing time of a standard CD is 74 minutes, or 4,440 seconds, contained in 333,000 blocks or sectors. Therefore, the net capacity of a Mode-1 CD-ROM is 650 MB (650 × 220). For 80 minute CDs, the capacity is 703 MB.

Of course, these days most algorithms will target DVD sizes, Blu-Ray sizes, or certain bitrates for streaming.

Beyond that, there's a couple of questions you're likely conflating:

* The theoretical underpinnings are typically found in the subjects of Information Theory and Source Coding.
* The more practical side (usually programming oriented) will be called Data Compression. Data compression comes in both lossless (i.e. for data that needs to be reproduced exactly, like finance data and MS Word docs) and lossy (e.g. for audio/photos/video).
* There are tons of old algorithms (Huffman, Lempel-Ziv, JPEG, MPEG, MP3) as well as lots of new ones (zstd, H.265, VP9).

It's up to you whether you want to start from the theoretical (which gets pretty abstract) or start with existing/current algorithms/codebases. All of these are not difficult at all to understand and there are lots of explanations to be found online. However, coming up with new mathematical theories to come up with the basis of new compression (e.g. zstd) does either require some brilliance and/or expertise.",2,0,0,False,False,False,1645848763.0
t1i3op,hymqhob,t3_t1i3op,"You cant shrink data without transforming it.  That is a matter of physics.  Binary already is the smallest form info can be broken down into and it also only involves 2 phase states, 0 or 1.  You can't get anything smaller than that.  The total ""size"" of whatever it is that is being stored is dependant on what it constitutes in the real world.

Shrinking = Removing stuff. 

Data compression deals with conversion and reconstitution and there is always loss involved in that process.  It just isnt always significant to be confirmed via the naked eyes.

How data can be compressed depends on what ""kind"" of data it is.  Because again, physics.  There is a limit to how many times you can fold paper in half, for example.  Getting a stronger machine (the codec) doesnt mean you can transform paper to occupy less space than it itself occupies.  So folding a paper in half doesn't ""shrink"" it in rhe sense where it takes up less space.  You can alter the shape of somerthing but not its topology.  

Multimedia codecs aim at both the conversion of data and then compressing it.  Both procedures as lossful.

There is no such thing as lossless conversion and when referring to compression algorhythms, people are referring to the ability to ""pack data""  more efficiently so it can be accessed elsewhere or sent via different methods.  If the ""unpacking of the data"" has not altered a single bit period, it can be considered 100% lossless.",1,0,0,False,False,False,1645959133.0
t1i3op,hygpkku,t1_hyg8jm9,"Happy cake day, brother!",1,0,0,False,False,False,1645841810.0
t1i3op,hyguasx,t1_hygpkku,"Hey, didn't even know =)",3,0,0,False,False,False,1645844063.0
t0sxp4,hyc40u3,t3_t0sxp4,"I seem to recall something like 

> Please enter your name: lynxbucker

> Hello, lynxbuckler!",151,0,0,False,False,False,1645760413.0
t0sxp4,hybzuzt,t3_t0sxp4,"IsPalindrome, reverse the char in a string. IsUnique string",82,0,0,False,False,False,1645758481.0
t0sxp4,hybyz9q,t3_t0sxp4,Fibonacci for sure,128,0,0,False,False,False,1645758076.0
t0sxp4,hyc6otx,t3_t0sxp4,"If you are programming in a new (to you) GUI, this collection of 7 tasks is helpful.

https://eugenkiss.github.io/7guis/tasks/",25,0,0,False,False,False,1645761691.0
t0sxp4,hyclmax,t3_t0sxp4,Random number Guessing game.,23,0,0,False,False,False,1645770020.0
t0sxp4,hyc7mv0,t3_t0sxp4,Farenheit to Celsius calculator,44,0,0,False,False,False,1645762156.0
t0sxp4,hyckfgn,t3_t0sxp4,"Enter a number: 17

Enter another number: 12

17 + 12 = 29",42,0,0,False,False,False,1645769254.0
t0sxp4,hyc1eah,t3_t0sxp4,"~~Fractal~~ Factorial calculation - first recursive function

Edit: Sorry I meant factorial, n!",12,0,0,False,False,False,1645759196.0
t0sxp4,hycjf1a,t3_t0sxp4,"Depends on whether you mean someone who is learning how to program, or someone experienced who is just learning a new language. The someone experienced thing isn't very interesting, you can just skip to the end. The path someone actually new takes is way more interesting:
 
1. Hello World  
1. Long string outputs (Border of * around a message, etc)
1. Set a variable and output it (hello $yourName)  
1. Basic arithmetic (5 + 7 = 12)  
1. User input (please tell me your name => hello $yourName)  
1. Here could be branching or could be looping. I prefer branching.",10,0,0,False,False,False,1645768617.0
t0sxp4,hyc0ij5,t3_t0sxp4,BMI calculator.,24,0,0,False,False,False,1645758783.0
t0sxp4,hycj4qy,t3_t0sxp4,Calculator,7,0,0,False,False,False,1645768438.0
t0sxp4,hye4zj5,t3_t0sxp4,"- Reading a text document
- Reading a csv
- Converting between data types and cleaning/summarizing

Those are typically some of my first attempts but I'm a data scientist so that's what's important to me in any new language. After that I'd probably feel comfortable enough moving on to testing some modeling packages and seeing how they work.",6,0,0,False,False,False,1645804132.0
t0sxp4,hyc0961,t3_t0sxp4,Bubble Sort,14,0,0,False,False,False,1645758662.0
t0sxp4,hycxdvp,t3_t0sxp4,sieve of eratosthenes,3,0,0,False,False,False,1645778554.0
t0sxp4,hydfn9a,t3_t0sxp4,First x prime numbers,3,0,0,False,False,False,1645792261.0
t0sxp4,hyd1k3e,t3_t0sxp4,Tower of Hanoi,5,0,0,False,False,False,1645781909.0
t0sxp4,hycyyso,t3_t0sxp4,"For functional factorial seems to be the equivalent

Surprised no-one's mentioned so far",2,0,0,False,False,False,1645779828.0
t0sxp4,hycyzqw,t3_t0sxp4,Game of Life,2,0,0,False,False,False,1645779848.0
t0sxp4,hydueu9,t3_t0sxp4,Printing christmas tree,2,0,0,False,False,False,1645799741.0
t0sxp4,hye2n22,t3_t0sxp4,"Conway's game of life must be in the top 10, also tic tac toe",2,0,0,False,False,False,1645803197.0
t0sxp4,hyf2bwr,t3_t0sxp4,Tictactoe,2,0,0,False,False,False,1645816783.0
t0sxp4,hygbpqd,t3_t0sxp4,phishing site,2,0,0,False,False,False,1645835311.0
t0sxp4,hydi1s9,t3_t0sxp4,"print(my girlfriend is hot)

Was my second program. Showed that one to my girlfriend 😉",2,0,0,False,False,False,1645793652.0
t0sxp4,hycb4kr,t3_t0sxp4,there are some others but theyre not even close to hello world imo,1,0,0,False,False,False,1645763927.0
t0sxp4,hyf6zzo,t3_t0sxp4,"Some thing you can build when learning programming

Sorting  
\- Bubble Sort  
\- Selection Sort  
\- Insertion Sort  
\- Heap Sort  
\- Merge Sort  
\- QuickSort

Clustering  
\- k-means algorithm

List Search  
\- Linear Search  
\- Binary Search

Graph Search  
\-Breadth-first search  
\- Depth-first search  
\- Bellman-Ford Algorithm  
\- Bijkstra's Algorithm   
\- A\* Algorithm

Math  
\- Euclidean Algorithm  
\- Primality Test

Generate  
\- Prime Numbers  
\- Fibbonacci Sequence",1,0,0,False,False,False,1645818607.0
t0sxp4,hycaino,t3_t0sxp4,"Array print for loop
Read input and print some txt
Calculator",1,0,0,False,False,False,1645763616.0
t0sxp4,hyd7qer,t3_t0sxp4,"roguelike

.

.

.

Context: on the first semester of my CS studies, there was a teacher in one of the groups, who insisted that the only way to get the highest grade in his group is to create roguelike project. First. CS. Semester",1,0,0,False,False,False,1645786862.0
t0sxp4,hyd8v21,t3_t0sxp4,"Enter a number: 5
Enter another number: 2
Enter the operator: *

5*2 = 10

Would you like to do another calculation (Y/N)? N",1,0,0,False,False,False,1645787712.0
t0sxp4,hydk2dg,t3_t0sxp4,MadLibs ? For me at least ~,1,0,0,False,False,False,1645794758.0
t0sxp4,hye3lah,t3_t0sxp4,Any sort of basic calculation in general,1,0,0,False,False,False,1645803577.0
t0sxp4,hyebvnr,t3_t0sxp4,"My ""do I know basic data types/strucutres, loops, and conditionals"" test when I'm learning a new language is to write a prime number sieve.",1,0,0,False,False,False,1645806793.0
t0sxp4,hyecras,t3_t0sxp4,Smoke on the Water,1,0,0,False,False,False,1645807127.0
t0sxp4,hyeo87o,t3_t0sxp4,"Something like enter name, enter income, output “{name} has an income of ${income}”",1,0,0,False,False,False,1645811456.0
t0sxp4,hyeuenu,t3_t0sxp4,"Something like
hello world
name Input then read name back
Input two numbers then add and print result
Those two again but using separate functions

Idk I think my first “big” project was a text based tic tac toe game.",1,0,0,False,False,False,1645813765.0
t0sxp4,hyewh0i,t3_t0sxp4,"A common trope for web frameworks is to build a TODO application that demonstrates CRUD operations and or MVC capabilities.

https://todomvc.com

A lot of applications can be modeled as glorified todo applications, so it’s useful to see how the frameworks work.",1,0,0,False,False,False,1645814544.0
t0sxp4,hyf1jc5,t3_t0sxp4,"Printing patterns with #

#
##
###",1,0,0,False,False,False,1645816472.0
t0sxp4,hyf9bh9,t3_t0sxp4,Rock Paper Scissors game,1,0,0,False,False,False,1645819511.0
t0sxp4,hyfow85,t3_t0sxp4,Ping Pong for socket programming is also a classic,1,0,0,False,False,False,1645825642.0
t0sxp4,hyi42ov,t3_t0sxp4,When I was a kid I tried to create a text based adventure game and failed miserably because the program didn't run in a loop and instead called functions from functions until it crashed.,1,0,0,False,False,False,1645873922.0
t0sxp4,hycegva,t3_t0sxp4,"console application that plays music, usually star wars or some gameboy game theme song",-5,0,0,False,True,False,1645765724.0
t0sxp4,hycwbj5,t3_t0sxp4,Matrix/Linear Algebra calculator,-1,0,0,False,False,False,1645777717.0
t0sxp4,hyc45fv,t1_hyc40u3,"Oh true! I always assumed that was just advanced Hello world though, but good one!",25,0,0,False,False,True,1645760473.0
t0sxp4,hyec1v2,t1_hyc40u3,"When I started doing learning programming, that was my first program. I didn't know about ""Hello World!""",1,0,0,False,False,False,1645806859.0
t0sxp4,hycmgp1,t1_hybzuzt,I was about to say something about Turing machines being a little advanced to be in the first 10 programs until I realized you weren't talking about making one of those 😂,12,0,0,False,False,False,1645770581.0
t0sxp4,hyd3y8l,t1_hybzuzt,"Lmao, we weren't allowed to use a string, because it would be too easy. Took me a while to start dividing by 10",2,0,0,False,False,False,1645783841.0
t0sxp4,hybz1en,t1_hybyz9q,Different sorting algorithms also,37,0,0,False,False,False,1645758103.0
t0sxp4,hyciima,t1_hyc6otx,GUI programming isn't even in the first 2 years for most programmers...,18,0,0,False,False,False,1645768060.0
t0sxp4,hydiyf3,t1_hyclmax,"If you implement the ""number is higher/lower"" functionality this also can be used to introduce the idea of binary search (assuming a teacher is explaining it) without needing trees or anything fancy.",11,0,0,False,False,False,1645794161.0
t0sxp4,hycxuvd,t1_hyc7mv0,My first project,4,0,0,False,False,False,1645778935.0
t0sxp4,hycybb4,t1_hyc1eah,"Fractals involve graphics. Just drawing anything on the screen may prove a hard challenge depending on the environment.

An easier first recursive function could be to print out all files on the hard drive starting from C:\\ (or / if you're a penguin hugger). 

Or the sum of the first n numbers in the fibonacci sequence.",2,0,0,False,False,False,1645779299.0
t0sxp4,hydu4dk,t1_hycjf1a,I think branching comes first because you need to understand the basics of conditional programming before you use loops,1,0,0,False,False,False,1645799614.0
t0sxp4,hyfkun0,t1_hyc0ij5,One of the first things I made.,2,0,0,False,False,False,1645824055.0
t0sxp4,hyeio9r,t1_hye4zj5,Very underrated answer,1,0,0,False,False,True,1645809365.0
t0sxp4,hycf6ms,t1_hyc0961,[deleted],-9,0,0,False,True,False,1645766129.0
t0sxp4,hyf9b47,t1_hydi1s9,"print(""my girlfriend is hot"")

Sorry my python learner ass couldn’t help it",1,0,0,False,False,False,1645819507.0
t0sxp4,hygl5lu,t1_hyecras,How’s this only have one upvote?,1,0,0,False,False,False,1645839712.0
t0sxp4,hye42ya,t1_hyc45fv,"I mean... aren't all programs just an advanced Hello World if you look at it that way?

In Hello World you test can you get something to output.  In this example you test can you get some input.  Different but simple and accomplishes a purpose.",10,0,0,False,False,False,1645803773.0
t0sxp4,hydohk8,t1_hycmgp1,Yeah lol Just basic leetcode like challenges u usually get asked in a starter course,1,0,0,False,False,False,1645797008.0
t0sxp4,hydojhm,t1_hyd3y8l,Do it in place with no extra data structures,2,0,0,False,False,False,1645797033.0
t0sxp4,hycnc8r,t1_hyciima,"Should be, tis a nice way to feel like you're actually creating something of substance even if it's simple",7,0,0,False,False,False,1645771166.0
t0sxp4,hycy0zz,t1_hyciima,Frontend developers would disagree.,1,0,0,False,False,False,1645779067.0
t0sxp4,hydy3u7,t1_hydu4dk,"In C, sure - but many languages have abstracted it through implicit iterators such that it's much easier to loop (with no condition listed) than it is to branch.",1,0,0,False,False,False,1645801341.0
t0sxp4,hyeb7l4,t1_hycf6ms,"It's not meant to be efficient, we all know it isn't, it's meant to be easy to implement as an exercise. 

Then you move onto something more efficient but harder to implement like Shell Sort.",5,0,0,False,False,False,1645806540.0
t0sxp4,hyfl465,t1_hyf9b47,"That's a good catch though 👍🏼

No need to apologize for that one",1,0,0,False,False,False,1645824159.0
t0sxp4,hyw7cfm,t1_hye42ya,"The HelloWorld Paradox - by u/dasonk.

If everything is made up of something smaller, does anything actually exist?

More discussion on similar ideas in [this](https://youtu.be/fXW-QjBsruE) VSauce video.",1,0,0,False,False,False,1646127941.0
t0sxp4,hycy6rk,t1_hycnc8r,Adding a gui is a whole new layer of stuff to learn beyond the logic. If you're just getting in to programming I dont see how that's a good thing rather than the more basic terminal programs people usually make,6,0,0,False,False,False,1645779196.0
t0sxp4,hycsso7,t1_hycnc8r,"I've seen mixed results. It works wonders for good programmers (as in the top 10-20% of a class), but it confuses the fuck out of people who aren't excelling and you completely lose anyone who is struggling.",9,0,0,False,False,False,1645775042.0
t0sxp4,hycyc3c,t1_hycy0zz,"HTML isn't GUI programming, no matter how much front end 'developers' would like to think it is.",-3,0,0,False,False,False,1645779317.0
t0sxp4,hycyhf7,t1_hycy6rk,"Fair, I suppose I'm more thinking about ways to keep people engaged as they are first learning. Having something visual and more tangible like a GUI alongside learning logic has helped people I know who've started to code in the past",2,0,0,False,False,False,1645779438.0
t0sxp4,hycyr2y,t1_hycyc3c,"Frontend devs write more javascript than HTML with today's tooling. 

Though conceptually HTML isn't so different from e.g. XAML which is the markup language used for desktop applications developed with the WPF library in C#. Both are markup languages.

My point is that it's entirely possible to approach programming from a gui perspective, and it's a path that many takes.",2,0,0,False,False,False,1645779658.0
t0sxp4,hydlydg,t1_hycyc3c,"Markup is a great way to separate presentation and logic, and html is a pretty reasonable choice for the presentation layer since so many languages have html templating support.",1,0,0,False,False,False,1645795755.0
t0sxp4,hyczu2m,t1_hycyhf7,Like get why someone might like a gui but have you tried making a gui in say haskell? It suuucks,2,0,0,False,False,False,1645780522.0
t0z4bp,hyd6b8e,t3_t0z4bp,"Computers don't think, they just compute things.",16,0,0,False,False,False,1645785739.0
t0z4bp,hydc6aw,t3_t0z4bp,"Try nandgame.com. it's a ""game"" where you build a computer from the first logic gate to basic assembly.

It's pretty short imo, but that way you learn how things work. You need just a minimal boolean logic knowledge, everything else, you'll learn about there.

Then, there's the OS layer, a bit more complex. But step by step",9,0,0,False,False,False,1645790069.0
t0z4bp,hyda5zf,t3_t0z4bp,"You’re on the right track with those subjects (you’ve described the core of a bachelor’s degree in computer science with a helping of EE). For me, the two specific topics that made it click were:

1. How we go from discrete transistors to logic gates (and the realization that you can make all the other gates out of NAND gates)

2. The detailed workings of a simple CPU architecture. In our case, we studied the x86 with Tanenbaum’s SCO book, but you might want to study RISC since it’s both simpler and more modern

After those, you can move on to the other subjects with a lot more confidence in how things work on the lowest levels; it will teach you the real physical meaning of concepts like assembly, instruction sets, registers, interrupts, pointers, 32-bit versus 64-bit architecture, why bitwise operations are so efficient, why floating point math is a damn miracle, and the weird way in which data in memory, program code, disk access, and I/O devices are actually all kind of the same thing.

Understanding these concepts will make all the other pieces of the puzzle come together so much more easily, especially if you’re like me that you can’t easily retain knowledge until you understand it in depth.",3,0,0,False,False,False,1645788673.0
t0z4bp,hyddi7q,t3_t0z4bp,"If you are looking for intros, these two can help a ton:  
[But How Do It Know?](https://www.goodreads.com/book/show/18276352-but-how-do-it-know---the-basic-principles-of-computers-for-everyone)  
[Computer Science Illuminated](https://www.amazon.com/Computer-Science-Illuminated-Advantage-Access/dp/1284155617)",2,0,0,False,False,False,1645790945.0
t0z4bp,hyf4g5r,t3_t0z4bp,"XINU (XINU Is Not UNIX) is an embedded systems OS that is used to teach some of concepts you mentioned.

Operating System Design: The XINU Approach, Douglas Comer is the text that we’re using in my OS class.",2,0,0,False,False,False,1645817613.0
t0z4bp,hydghh2,t3_t0z4bp,"Overall you already stated most of the relevent courses, one important subject you missed is operating systems.

The rest really depend on what you rather know more about, no one can know literally everything, so courses like advanced arch would help for more modern mechanisms if thats what interest you and courses like semiconductors would help if youre interested at the really low level.",1,0,0,False,False,False,1645792759.0
t0z4bp,hydqymq,t3_t0z4bp,"Turing machines

Then understand there is nothing more to it.",1,0,0,False,False,False,1645798184.0
t0z4bp,hydxemn,t3_t0z4bp,[Intel’s Introduction to Computers](https://www.intel.com/content/www/us/en/education/k12/the-journey-inside/explore-the-curriculum/intro-to-computers.html),1,0,0,False,False,False,1645801044.0
t0z4bp,hyews4a,t3_t0z4bp,"but computers dont *think*, they just mindlessly follow a list of instructions.",1,0,0,False,False,False,1645814660.0
t0z4bp,hygenwk,t3_t0z4bp,please specify your background so people know if its worth the hasstle.,1,0,0,False,False,False,1645836666.0
t0z4bp,hz1e5th,t3_t0z4bp,"Computer executes instructions provided by programs. If you want understand it the best book is ""But how do it know?"" by J Clark Scott. There are even videos about it like this playlist: 

[https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl\_eE6yzA](https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl_eE6yzA)

You can understand how CPU and RAM are made and how they really work. You'll get a complete understanding of how computer acts. You make yourself a better programmer knowing hoe computer behaves. It doesn't need any particular knowledge.

If you want a quick view try this : [https://youtu.be/1BidIYu6Cls](https://youtu.be/1BidIYu6Cls)",1,0,0,False,False,False,1646221682.0
t0z4bp,hyd90lr,t1_hyd6b8e,Yeah that's what I meant TT,1,0,0,False,False,True,1645787828.0
t0z4bp,hydnyn5,t1_hydc6aw,That sounds really cool. Gamifying studies will help a lot of people. I wish they did this more and I hope it's effective at helping people study,3,0,0,False,False,True,1645796757.0
t0z4bp,hydno5u,t1_hyda5zf,"I want to be a good ""hacker"" that's why I'm learning all of this. Idk if I'll dig any deeper but I really find these subjects fascinating",2,0,0,False,False,True,1645796614.0
t0z4bp,hydnq4e,t1_hyddi7q,I'll check them out. Thanks,1,0,0,False,False,True,1645796641.0
t0z4bp,hyhgfuq,t1_hyf4g5r,How good is it. What concepts does it teach?,1,0,0,False,False,True,1645855922.0
t0z4bp,hydil4b,t1_hydghh2,I'm from a cyber security background. Felt like learning all of this will give me a solid foundation to study the rest of the security concepts,1,0,0,False,False,True,1645793956.0
t0z4bp,hyggrq5,t1_hygenwk,I did reply to some of them. I'm from a cyber security background,1,0,0,False,False,True,1645837650.0
t0z4bp,hyev86g,t1_hyd90lr,if you want to learn about how computer do calculations then start studying computer architecture.,3,0,0,False,False,False,1645814073.0
t0z4bp,hygh7fz,t1_hyggrq5,"Ok great, I recommend the book ""Computer Organisation and Design RISC V Edition"". There is a revision from 2020 or so. Read through the first 4 Chapters and you can build a cpu from logic.
If you want to solidify your knowledge write one in VHDL or build a minecraft computer or build one from breadboard logic gates. All months long side projects with a ton of fun.",2,0,0,False,False,False,1645837853.0
t0z4bp,hyhg794,t1_hyev86g,"Yes, that's the plan. I wanted to know which computer architecture books go from complete beginner to expert.",1,0,0,False,False,True,1645855770.0
t0z4bp,hyhfc9w,t1_hygh7fz,That sounds so fucking cool and fun. I was planning to do these things after I finished studying computer architecture and all. Thanks a lot,1,0,0,False,False,True,1645855228.0
t0z4bp,hyhx53c,t1_hyhfc9w,"It is cool :) I wrote a CPU on an FPGA as my bachelor thesis. I wrote a VGA Interface for it too. It was complete enough to be a target for the GCC, so I was able to run C code on it. In the end I wrote chess, which two players can play against each other. I wanted to write a simple operating system but the time didnt allow it. If you know *exactly* how cpus work, there is no barrier of understanding you cant penetrate in CS.",2,0,0,False,False,False,1645868208.0
t14tkh,hyf6gx2,t3_t14tkh,"In software, you just can't. It's not a matter of ""we don't know how"", it's a matter of ""by definition it cannot be done"". The basis of software is taking a set of inputs and turning them into outputs, deterministically. In other words, the same input always equals the same output. Any circumvention of this would be something to do with how the software is represented (i.e., hardware/firmware).  
  
It's similar to imagining a mathematical equation/algorithm that gives a different result with the exact same values plugged in. It just by definition doesn't work.",5,0,0,False,False,False,1645818404.0
t14tkh,hydwrax,t3_t14tkh,Isn't there hardware in most CPUs now that does this so the problem becomes moot anyhow?,5,0,0,False,False,False,1645800770.0
t14tkh,hyfw4tx,t3_t14tkh,It would make encryption somewhat more secure and some simulations possibly more realistic.,2,0,0,False,False,False,1645828565.0
t14tkh,hyfouo9,t3_t14tkh,"I tried googling trn but  couldn't find anything that seemed right, what does it mean?",1,0,0,False,False,False,1645825625.0
t14tkh,hyhe8sn,t3_t14tkh,There is probably a proof that you cannot create true random number using software.,1,0,0,False,False,False,1645854565.0
t14tkh,hyf8k4f,t1_hyf6gx2,"That is true ig. But, i was thinking for about the implications, would anything even change?",-1,0,0,False,False,True,1645819212.0
t14tkh,hydxd7h,t1_hydwrax,exactly what I think.,0,0,0,False,False,True,1645801028.0
t14tkh,hyh9poc,t1_hyfw4tx,ohh okay!,1,0,0,False,False,True,1645851927.0
t14tkh,hyfp40d,t1_hyfouo9,True random numbers,2,0,0,False,False,False,1645825727.0
t14tkh,hyhkb4b,t1_hyhe8sn,"probably, I would read that",1,0,0,False,False,True,1645858489.0
t14tkh,hyfp284,t1_hyf8k4f,I mean at that point mathematical logic would be broken so literally anything goes.,5,0,0,False,False,False,1645825707.0
t14tkh,hyfpkun,t1_hyfp40d,"Ahh thank you, makes sense.",1,0,0,False,False,False,1645825912.0
t14tkh,hyh9oep,t1_hyfp284,hahahhaha,1,0,0,False,False,True,1645851908.0
szvhcw,hy6ba1n,t3_szvhcw,"Aight, now make it run doom",39,0,0,False,False,False,1645663338.0
szvhcw,hy628gc,t3_szvhcw,"We are documenting our progress on https://www.instagram.com/electrickids_club/

And https://www.youtube.com/channel/UCf9S7qTQquE_ed9PCWIfsbg/videos",12,0,0,False,False,True,1645659352.0
szvhcw,hy62lfa,t3_szvhcw,Ben Eater 8bit computer /r/beneater,7,0,0,False,False,True,1645659507.0
szvhcw,hy6b2x5,t3_szvhcw,Very cool!,3,0,0,False,False,False,1645663247.0
szvhcw,hy6qbvb,t3_szvhcw,"This just reminded me that I built the clock module with the intention of eventually completing the entire computer some time ago and I completely forgot about it. 

Ben is such a gifted communicator though. This is such a fun project.",2,0,0,False,False,False,1645670057.0
szvhcw,hyagewn,t3_szvhcw,I’m so amazed how organized this looks lmao my circuits look like Frankenstein,2,0,0,False,False,False,1645735526.0
szvhcw,hy6l7bs,t3_szvhcw,Where are the instructions?,1,0,0,False,False,False,1645667769.0
szvhcw,hy7n0jy,t3_szvhcw,Doing this would be fun. Wish I could get these projects instead of printing Hello World in python from my school!,1,0,0,False,False,False,1645686710.0
szvhcw,hy6bjok,t1_hy6ba1n,"Hahaha, to be sincere i have no idea how, but the other benEater PC explain the video part",7,0,0,False,False,True,1645663457.0
szvhcw,hy6bd25,t1_hy6b2x5,"I want everyone to make their own, it is an amazing project, a lot of learning",4,0,0,False,False,True,1645663375.0
szvhcw,hyaqymf,t1_hyagewn,"Thanks, we want to hang it on the wall when finish, so it should be nice",1,0,0,False,False,True,1645739404.0
szvhcw,hy6lfms,t1_hy6l7bs,"BenEater.com

r/benEater

https://www.youtube.com/watch?v=HyznrdDSSGM&list=PLowKtXNTBypGqImE405J2565dvjafglHU&index=2",6,0,0,False,False,True,1645667870.0
szvhcw,hy8nvoa,t1_hy6l7bs,"You can look in the book ""digital computer electronics"" for more information. Ben essentially follows the book in recreating the SAP-1 computer and then makes some modifications for the jump instructions, and some extra instructions, which is briefly explained in the SAP-2 computer.",6,0,0,False,False,False,1645710675.0
szvhcw,hy6bka8,t1_hy6bd25,Teach him how to etch a circuit board and solder next.,3,0,0,False,False,False,1645663464.0
szvhcw,hy77nil,t1_hy6lfms,Thanks so much!,2,0,0,False,False,False,1645678174.0
szvhcw,hy8u8bf,t1_hy8nvoa,Nice! Thanks! Off to find a copy :),1,0,0,False,False,False,1645713501.0
szp1uf,hy4x738,t3_szp1uf,"Just guessing it's 1 dimensional vs 2 dimensional, i.e. an x axis or an x and y axis.",5,0,0,False,False,False,1645643624.0
szp1uf,hy51j50,t3_szp1uf,"I guess two dimensional, although I can’t see a reason why that would matter, since the first step in solving TSP usually involves creating a table of the distance between each two possible points.

Perhaps 2D means bi-directional in this context?",2,0,0,False,False,False,1645645267.0
szp1uf,hy5934l,t3_szp1uf,"Traveling Salesmen may be defined as a search problem to find the shortest path (searching the space of all possible paths) from a source point to a destination point, making sure to stop at a list (of unknown length) of points (chosen at random, any of all possible points) before arriving at the destination point. Got it? Good.

1D means that the space of “all possible points” is all the points on a real number line. The space of “all possible paths” is all the line segments that connect exactly two points, where no two line segments connect the same two points, embedded on the real number line. 

In the 2D case, the points lie on a flat map, but the space of all possible paths becomes much difficult to define. Here, paths that might be optimal when searched can have some pretty whacky curvature, and so standard algorithms take a long time (or forever) to check that those curves are a solution to the problem. This is why the problem in 2D has ongoing research associated with it - it’s the minimal case where we clearly don’t have the answer, but we know very well how to define the problem.

In 3D, the space of points is like the space of points theoretically accessible to you via spacecraft, and the space of paths is a generalization of the problem in 2D.

In 4D, the problem is not well formed and has provably no solutions - the set of all points will include a singularity, so if we want to stop there, then the set of all paths that go to the singularity, then leave and arrive at the final destination, doesn’t exist, since there are no paths that leave the singularity point.

In 5, 6D, you have the set of all points that can be described with exactly 5, 6 linear coordinates. The paths are a generalization of the problem in 3D.

Any higher than 6 and the points are easy to define, but the paths are obviously very difficult to define. It’s not even clear for most cases whether there are obstacles to the definition as there are in 4D, so I won’t say any more.",2,0,0,False,False,False,1645648110.0
szp1uf,hy54l3i,t1_hy51j50,..... if it was 1 dimension you could simply start with the left most x axis and move right...,4,0,0,False,False,False,1645646434.0
szp1uf,hy5cdhv,t1_hy5934l,"Thank you for the explanation. Helped a lot. If I may ask one question, I know and have implemented 1D TSP Hamiltonian. Yet how should I start to change it to 2D case? Do you have any suggestions or any web pages or any resource may be helpful. Thank you in advance!",2,0,0,False,False,True,1645649316.0
szp1uf,hy634ui,t1_hy5934l,It’s funny how it’s always the 4d case that acts up and behaves differently from the others.,1,0,0,False,False,False,1645659736.0
szp1uf,hya2i5m,t1_hy5934l,"Why would 4D be any different than 3D, 5D, 6D? A point in 4D space is just (a,b,c,d), where a,b,c,d \in R. No problem to define a metric, for example: d(x,y) = sqrt((x_1 - y_1)^2 + (x_2 - y_2) + ...). No Singularity there. Then fill the matrix with distances from every point to every other and solve it like the 2D case. Same goes for any metric and any Dimensionality. The 1D case is only special because we know a more efficiant algorithm to solve than in higher dimensions. And in 0D the TSP is trivial.",1,0,0,False,False,False,1645730335.0
szp1uf,hy56si8,t1_hy54l3i,"So if I make just a distance matrix (N x N) with N x N entries, it is considered 1D and if I include x and y coordinates in each entry (2N x 2N) makes it 2D?",0,0,0,False,False,True,1645647259.0
szp1uf,hy5x2u7,t1_hy5cdhv,"It heavily depends on your implementation details. 

Your numerical approximation of the 1D Hamiltonian must amount to a list of lines. 

The 2D Hamiltonian might be a list of curves, or a list of lines with angles attached. The curves could be quadratic, cubic, quartic, exponential, etc. They could also be built of more exotic polynomial expansions, given that they satisfy some differential constraints. Constructing 2D Hamiltonians is probably the subject of a course like classical mechanics. 

Anyway, once you’ve constructed a formula, you’re gonna store some data representing it that will have a space complexity of O(N^2) in 2D, rather than the O(N) array in 1D you likely have now.",1,0,0,False,False,False,1645657178.0
szp1uf,hybwuja,t1_hya2i5m,"It only matters if someone tells you that the topological dimension is 4, and you have to start assuming topological conditions before finding coordinates to flow along / metrics that make sense. 

I can’t prove this myself or talk about it formally - I may be misinterpreting some information from a recent Roger Penrose talk at the RI where he was talking about research into topological properties of (1,3) space time.",1,0,0,False,False,False,1645757103.0
szp1uf,hy58ohu,t1_hy56si8,"You don't need a distance matrix at all.  You can just sort the list of entries by their x coordinate and you have your shortest path. 

If you're given a start point you simply move left (or right,  doesn't matter) and loop around to the front once you reach the end.

Edit: for 1D",3,0,0,False,False,False,1645647961.0
szp1uf,hy5c77s,t1_hy56si8,"2D is much more difficult (famously NP hard).

There are no proven shortcuts. The only known way to cimprehensively solve it is a brute force search of the every possible route in O(n!) time.

There are algorithms that aproximate the answer. Nearest neighbor uses an N x N matrix and always move to the closest unvisited node",3,0,0,False,False,False,1645649255.0
szp1uf,hy5cn7w,t1_hy58ohu,I actually have implemented the 1D case. Yet I'm having difficulties on understanding how to convert it to the 2D case.,1,0,0,False,False,True,1645649411.0
szp1uf,hy5d47t,t1_hy5c77s,If you have could you please provide some information on the 2D case? May be a webpage or a PDF about the Hamiltonian/cost function or the brute-force implementation?,1,0,0,False,False,True,1645649581.0
szuw76,hzcbbnm,t3_szuw76,"mmm I can just leave you a link of an informative channel ... try this playlist : [https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl\_eE6yzA](https://youtube.com/playlist?list=PLnAxReCloSeTJc8ZGogzjtCtXl_eE6yzA) 

&#x200B;

or this if you want a quick view on what is about: [https://youtu.be/1BidIYu6Cls](https://youtu.be/1BidIYu6Cls)",1,0,0,False,False,False,1646415706.0
szda4g,hy40f5g,t3_szda4g,">Most importantly what list of problems do I cover ..., do I solve problems from each topics alone at first and then combine them later for instance?

I have read many successes following this guide on leetcode:[The DEFINITIVE way on how to leetcode properly](https://www.reddit.com/r/cscareerquestions/comments/sgktuv/the_definitive_way_on_how_to_leetcode_properly/?utm_source=share&utm_medium=web2x&context=3)

There are also a lot of debate/praises in that discussion, so take it with a grain of salt, although some of the things outlined in that post might be a helpful structure for someone looking to find effective use of their leetcode grind.

As far as study plans, on a 2-3 month schedule, with the steps you've outlined, and on top of that, you're writing a thesis, don't you think it's a path to burn-out? Interview prep, bachelor's thesis, your sanity, and everything else in your life, one of those things is bound to give. Although, I don't personally know you so that statement is probably useless.

But nevertheless, I would jump straight to leetcode, aiming for at least 2-3 problems a day (depending on difficulty). Then every couple of weeks, when you're comfortable on a particular topic at a particular difficulty, increase your daily problems. I used to do at most 10 problems a day, or two hours a day, whichever comes first.",5,0,1,False,False,False,1645630989.0
szda4g,hy4tvxw,t1_hy40f5g,"I concur. I also do not personally know you, but I would also advise you adjust your schedule to focus on interview prep, leetcode and your thesis - those are surely your priorities - have a life with the remainder. Everything else you can learn to do on the job. With the dedication you're displaying, you won't need my kind regards... :)",3,0,0,False,False,False,1645642356.0
szvufg,hy671f6,t3_szvufg,"The following are written in different languages, but have exactly the same meaning:

  * I am using Reddit
  * Używam Reddita
  * 私はRedditを使用しています

As you may notice: different syntax, different words, different symbols, but still the same meaning.

And [computer languages](https://en.wikipedia.org/wiki/Computer_language) are exactly the same - a way to express some information.",2,0,0,False,False,False,1645661443.0
szvufg,hy6m4lh,t3_szvufg,"You are pretty close in your understanding. Different languages work better for different cases. Have you ever downloaded some software off the internet and there was a zip file with a bunch of files? You might have an EXE and a TXT file and some other helper files. All those files are different formats but can interact with each other.

Basically you could have one file that just calls the other files to run based on what you need. 

In the case of stuxnet, a lot of the virus ran on windows to spread, but the end target was some specific piece of hardware that was running the centrifuges. So you would need different languages to interface with that. So you switch to that language/file once you found the target.",1,0,0,False,False,False,1645668178.0
szvufg,hy7wwst,t3_szvufg,"Virus is not different from any other complex application. Most applications will require at least two languages. For example web application will have UI written in Html5 and Backend in Python. 

Similarly, Virus will have :

1. Core payload9s) written in C++ , ASM 
2. (Web) Html5 to initiate a browser attack vector . E.g.: [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-30563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-30563)
3. (Windows) Powershell to initiate install sequence
4. iOS : Swift / Obj-C to manage interaction with iOS APIs",1,0,0,False,False,False,1645693549.0
szvufg,hyb9wal,t3_szvufg,"There are a few ways of writing a single piece of software in multiple languages.

One of them is to use a [foreign function interface](https://en.wikipedia.org/wiki/Foreign_function_interface). These are tools provided by languages to directly run the environment of another language within the same process. For example, JNI, the Java Native Interface, can be used to call into C++. The program file contains both compiled java code and compiled C++ code, and the active process will have separate sections of memory for the two to be loaded into, separate sections of memory for them to execute within, but since it is the same process they can directly access each other's memory. Communication between the two components can be done either through the FFI (e.g. C++ creates a return value and gives it to Java), or indirectly through some shared memory (e.g. C++ modifies an object that was created and continues to be managed by Java).

You can also have multiple binaries which launch multiple different processes, each one built out of a single language, and then use some inter-process messaging to communicate to one another. There are [tons](https://en.wikipedia.org/wiki/Inter-process_communication) of IPC mechanisms, and at a simple level the programs could just do different things and 'interact' via side effects of different parts of the system being compromised. This is an extremely common practice in modern software construction, but I'm not sure how common it is in virus deployment.",1,0,0,False,False,False,1645746962.0
szvufg,hy6cm7u,t3_szvufg,[deleted],0,0,0,False,False,False,1645663938.0
szvufg,hy67rgv,t1_hy671f6,"Right, but if I said “I am using Reddit. I’m on the computer science subreddit. I’m talking to someone.” And said each sentence in a different language, it wouldn’t make sense to a person. If I’m talking to you in Russian, then intermingle English words and Chinese words into the sentence, you would have no idea what I mean. Now I know there’s obviously a way this is possible, I just can’t wrap my head around the technical aspect. I’m looking for the technical answer not just the general concept",1,0,0,False,False,True,1645661761.0
szvufg,hy6qsxz,t1_hy6m4lh,"Yeah, I think the PLCs use something called Ladder logic? I’d never heard of that language I figured they’d use like C++ or something but it says ladder logic on google",1,0,0,False,False,True,1645670270.0
szvufg,hy6d1wj,t1_hy6cm7u,[deleted],2,0,0,False,False,False,1645664136.0
szvufg,hy68k4c,t1_hy67rgv,"Well, computer language is a tool. Programming languages fall under _construction_ category.

Now imagine you are making a table.  
Does table top needs to be made with the same tool as the legs?  
Do those tools even need to be by the same manufacturer?  
Do they even need all be electric or all manual?

---

>And said each sentence in a different language, it wouldn’t make sense to a person. If I’m talking to you in Russian, then intermingle English words and Chinese words into the sentence, you would have no idea what I mean.

Yes, but when I'm replacing ""_for example_"" with ""_e.g._"" it's understood despite the latter being an abbreviation of Latin ""_exempli gratia_""",1,0,0,False,False,False,1645662112.0
szvufg,hy6n1g1,t1_hy67rgv,"In your example all of the languages are being used in one conversation. Think of it more like a different language for each conversation based on topic.

In real life: Reddit uses the languages HTML for the text you are reading, it uses CSS for styling the text and that little box around the OP text and the orange in the arrow. When you upvote this comment and the numbers go up, that is being tracked on the back end using Python. Each language has a use case, and it would be impossible to build a whole (modern) site using only HTML.",1,0,0,False,False,False,1645668583.0
szvufg,hy9ykjm,t1_hy67rgv,"In that instance one could either translate each of those sentences into the language that you want as you encounter them, or you can translate it all to an intermediary language beforehand, and then translate that. Consider the analogy of the Bible, the Old Testament is in Hebrew and Aramaic and the New Testament is in Greek, it was all translated into Latin later on, and that Latin translation was used as the basis for translating the text into English, French, German, Chinese etc. 

Depending on the languages used in the programme, each language could either be directly compiled to machine code, or the entire programme could be compiled into assembly language, which the computer can then interpret into machine code in real time.",1,0,0,False,False,False,1645728856.0
szvufg,hy6npqz,t1_hy6d1wj,That above comment is irrelevant to the question.,1,0,0,False,False,False,1645668884.0
szvufg,hy692ls,t1_hy68k4c,So in the end who puts them all together? The table has to be put together and that’s the person. Who is the person?,1,0,0,False,False,True,1645662344.0
szvufg,hy6ajpr,t1_hy692ls,"Who is the person? Well, the person. A programmer.

I've put threaded holes in the table top using one tool, then screws onto legs using another, so I can put the whole table together (probably I will use yet another tool to help myself with that task too).

And of course I needed to be aware of each part during production, because had I put screws both onto table top and legs, there would be no holes for screws to put into.",2,0,0,False,False,False,1645663005.0
szvufg,hy6bdyx,t1_hy6ajpr,"By person I meant programming language or like interpreter. There’s a person using all these tools to create a table. In these worms, what is the thing that is using all these tools? If all the tools are programming languages, there’s got to be something interpreting (I guess that’s the right word?) these languages and executing them. What thing is able to understand what each language does and how does it execute each one?

Just like how there’s a person using every tool. The person knows how to use each one and is carrying out the operation those tools are designed to do. 

There’s a universal thing that is able to carry out all these operations with the different tools (languages). What is it?",1,0,0,False,False,True,1645663386.0
szvufg,hy6drz9,t1_hy6bdyx,"Carpenter is using tools to make table. Table is designed for human family of four. They have no need for the carpenter's tools to use their table.

In our analogy, _carpenter_ is _programmer_, _table_ is a _program_ and the
_human family of four_ is _CPU (and the rest of necessary components) of given
architecture_

CPU read bits. Simple binary on and off to control electricity inside of it.",1,0,0,False,False,False,1645664457.0
szsqjo,hy5m56s,t3_szsqjo,"Let's take the following string as an example: `AAAAAAAAABBBBBBCCCD`  
We can compress it without losing any data into `9A6B3C1D` - undeniably it's shorter than the original.  
But now you want to see the original, so you need to decompress data; you need to reverse the process and that takes time.

&nbsp;

To break it into steps:

&nbsp; Without compression

  1. Read original string

&nbsp; With compression

  1. Read compressed data
  2. Build the original (decompression)
  3. Read original string",14,0,0,False,False,False,1645652895.0
szsqjo,hy5nhbg,t3_szsqjo,"It is not _always_ slower to read compressed data, it depends on a few factors (i.e. CPU/GPU speed vs disk speed, and level of compression).

If your disk is very slow but your CPU is very fast, it can be faster to load a small compressed file and decompress it in RAM.",3,0,0,False,False,False,1645653391.0
szsqjo,hy8fhoi,t3_szsqjo,"For the trivial example of y=x^2 sure one would ""store"" the equation and not the value pairs anyway. That's an obvious choice of the programmer of the agent if you ask me. Whoever makes such intelligent agents will not make inefficient software choices.

To add to the already good summary by others which say that decompression as a ""software"" step is always an overhead step hence takes time; you might want to consider for what purpose you read the data. You read the data to perform operations on. Now the operations usually are themselves only defined in the context of uncompressed representation of the data. 

Eg. U store a list of emails in a file. 
You want to search for your email in the file. The operation needed is a string comparison. When one of the emails in list is equal to your email we have found it. Now how does this string comparison operation make any sense for the compressed list which is basically a blob. 
In theory you could try and solve this problem (hint homomorphic encryption) but unless you are talking about security it's not worth it, and most likely will be even slower. 

Another Eg. Think. 
3 + 2 has meaning. And we get 5.
Compressed(3) + compressed(2) 
Makes little sense.
 Do we get compressed(5) ? Or 5? Which one and why?How do we even define such fancy + operation?
It's a can of worms worth investigating for cyber security encryption schemes. Not really unzipping files or sorts.",1,0,0,False,False,False,1645706414.0
szsqjo,hy5wrzm,t1_hy5m56s,"Is it always necessary to decompress it to the original string to read it?

For example if you wanted to decompress a relationship like y = x^2 wouldn’t you have to list all the possible (y, x) pairs? This doesn’t appear to be the fastest way to make use of the data.",2,0,0,False,False,True,1645657054.0
szsqjo,hz0hfo3,t1_hy5nhbg,"False. It’s always more steps to decode an encoded value

The exception is null encoded which reduces to a non encoded value",1,0,0,False,False,False,1646198012.0
szsqjo,hy5zu1w,t1_hy5wrzm,"It depend on your definition of read, but in most cases to proccess data or in other words preform some function that depend on the data you would need to decompress it.

After compression your data turns into a different representation, while it has the same information of course, you cant easily perform functions on that representation. You could argue to give support of some instructions in the hardwere to support some basic functions on compressed data but that wont work too well as even if the hardwere could use some tricks to get attributes you need out of the representation complicated instructions are better avoided for very niche use case.",3,0,0,False,False,False,1645658333.0
szsqjo,hy60yzr,t1_hy5wrzm,"Is it necessary? Well, yes, if you want to read it.  
While the example I gave is as meaningless compressed as uncompressed and so simple you can make sense out of it without breaking a sweat, if it was some real text it would be necessary. 

For example:

    eNp1VtmK5TYQfR+Yf6gPMP6HkAUGkmFCSN51Zd3bFWzJo8XM5+dUlWT7PgQauluWaztL+feUw0a8l7bRktaUqXAlt4U6kU+xBF9DbZncwjsXz/FFYeU60x8pP5hWh/dDpe+NC6WFE4UX/n2G9mKHW3+FhSpHz0uLlR6riwvCv7I7eHEjyMhIIbse6ntzm4Qq1RVy3retuDjT3xJ6xfOaMn4mauvqNp/yHlBipc29oqMYvDypmT2HIm0cbpW3olyf6au+RQu/IpfCG/3bSkXlBT0HCVIrLmcuDbmbRyI6uLow0z+4w4+2Ylh6QuEHuVjlCR9uw/2zl839YDkIOZWZfgoxuEgxRWLczwsiaIKZvoz/9aU1lOYWR08Ekc41fMdnz7yhMI542jzqkMAr66wkch8rpezZ4r4wlmv86HYlXuQ30746L/OeqACjlKPLAd2+WqAtybRm+jlj+BK4yIHn2hbkjvz46DNEHZ5xlefPnz5/+pYTHhf3Yp2fQYSZR6ChoNaw7TixaV+tnmz4iqvSsk2+j2c0+OBHiBgTkEWtFBp4GPiJM3J2ZeqRdWJvJZeAhycynZy0BqmiFHdjiA4atG1lR2SwI9CeaoiVB1mPsNLKr4bu0Ux1k7YlyLudQ0T9OEHtFTWBH5ER7tfK6EBn0F/VvCYPlNRKH1MPJTmGVhoQ0kQdFtDWbddY8keKvpUJpJd8Z5NXCT5tW1rS1OW9quAHcEXJoFJfeEOfoNlJI5HzQBEsMCb/iS7eMknlowzaW75Ek1vNxmuAKqgpTbr6jktKvTCnfbgrmLV0CqnZvIDnxR29oi8t7KV4Q2GmXxJc4CSNUZ6jOUx4PpUa+RyykvwcXcu70N/wVlcRkhhy79wwk4FQDSGVz2QWiImKltDlzYqQZ0gHV47wwV7ZIBRAQteAGjljuOuCvd7ufjiaO/gIObvuRAoDgn6gthyyWLTR7iL3AHbFVHK6lLoCrnaG3XOo3C7BDcqOdHtYVxAlCAtGDrGQAxYXXVVKiUN8iXqsvQihVZbdWwemvOijrjrLfxOPzBEO61sWQygeis/KPiWS2tOoajhiR2+S0ANdWxm3+oKf6beGeMbxQzbadI8vV5TvYN10o1tJi/xtXDU6amvZtzG3U8QGSPeoiZ4ctb6u/KHfsePEs6XmaymZiDAVsY2l8eD6Jd2TsW9bcDqX8vhtG8qme76jcPQ+jJFGg1sFnSfGL8ZV3QoW557ytmAVeQXmOuxxw4+Zvt24I6IdPOhe35dKn9QpLttKZ+WmdJv+3bdULQIaDPMD26xm9/YJM9pp696qQ/ddjYBJOlKLUKpc7NrbejA2owhriFKlbtmFnqDO/zkjqCvG0i20r1hTwHCX7lq2A2xJKn8vFdh+fltnNo9b6ye4uPkMeczD+IJNeX5ydKMZnwYYusdnXVUSWw2yLfvp6YBecRV2WuYFHisADd/ojn59aJxxh4fM/wFRLMzl

Would you guess what it is? If you want to use that data in any sensible way other than storage, then you need to decompress it first.

^(BTW, it's 5 paragraphs of lorem ipsum I compressed using ) ^[this](http://www.unit-conversion.info/texttools/compress/)",2,0,0,False,False,False,1645658814.0
szsqjo,hz0h1bf,t1_hy5wrzm,"No, there’s several formats that support partial deserializarion (eg protobufs, Avro, bond, …)

Basically you go through and iterative process. First you read the base object structure. It will have fields that **point to** compressed values. Suppose you want that specific field value. In that case you’d decompress that segment.

The result could be another object full of pointers that recursively uses the same methodology",1,0,0,False,False,False,1646197791.0
szsqjo,hy63dkp,t1_hy5zu1w,"I should clarify the context of my question. I was thinking of how intelligence can be defined as the ability to compress data, so then I was wondering if an extremely intelligent agent would decompress the data to read it or just read it directly. In other words, if decompression is only useful if the agent is not intelligent enough to use the compressed data directly.",1,0,0,False,False,True,1645659839.0
szsqjo,hy7fvxs,t1_hy63dkp,"I think compressing data is different from being able to understand data, compressed or uncompressed. You could say both use different kinds of intelligence.

If you want to follow this line of thought, pick one: More intelligent computers can compress more efficiently (although I think there is a limit, no?) Or omnisciently-intelligent computers don't need to work with inefficient uncompressed data, b/c they are smart enough to understand compressed data.",1,0,0,False,False,False,1645682459.0
szsqjo,hy8sug9,t1_hy63dkp,"In terms of computer science, involving the concept of intelligence usually leads to little more than hand-waving. The combination of computing and intelligence sounds interesting, and it *is* a valid area of research in theory of artificial intelligence and cognitive science. In practice, though, it's often really hard to bring the informal and somewhat elusive concept of intelligence together with the formal concepts and logic of computer science so that one could actually think or talk about the two in any kinds of substantial terms.

While it *might* be that an ""extremely intelligent"" agent might be able to find a way to process or perform computation on compressed data without decompressing it first, it's rather hard to say anything substantial about that.

If you want to look at that from a CS perspective, remember that computer science, including the properties of data compression, is exceedingly formal at its core.

So, let's try to approach your original question from the formal end.

First off, it might be mathematically impossible to directly do processing on data compressed using a particular compression algorithm, or fundamentally impossible to do so efficiently even if there were some (possibly inefficient) way of doing so. In CS, it's generally useful to distinguish between what's fundamentally impossible, and what's something we just don't know how to do. Our knowledge of where that border lies is also limited, mathematically and scientifically. Formally studying the limits of what's fundamentally possible is [a valid and interesting area of research](https://en.wikipedia.org/wiki/Theory_of_computation), but there's a lot we don't know.

Generally, if you take a random compression algorithm, we probably don't know how to do some particular kind of processing on compressed data, even if it were fundamentally possible to do so. Moreover, there's a good chance nobody knows if it's even fundamentally *possible* to do that. A general hunch would probably be that, for some arbitrary type of compression, it might well be impossible, or at least that doing *arbitrary* computation on the compressed data would be impossible even if some select individual operations might be possible. There's a good chance nobody has actually proven any of that for some particular compression method, though.

If you wanted to have compression that allows the compressed data to be processed without decompressing it first, the ""easiest"" way of doing that would probably be to design the compression method with that in mind from the get go, so that you could show that the possibility of performing certain computational operations on the data would be maintained despite the compression. It might be that you couldn't maintain the possibility of doing *all* kinds of computation on the data, and so you might still need to decompress first if the processing you need to do weren't within those constraints, but at least you might be able to maintain (and prove) the possibility of doing *some* kind of processing.

It's also quite possible that some operations, while perhaps possible to do on the compressed data without decompressing the entire data first, might be inefficient or slow to do, and so you might not always gain much compared to decompressing first.

The constraint of maintaining the possibility of computation would also quite possibly limit the compression techniques you can use, so you'd probably have to settle for a worse compression ratio than what you might be able to achieve using other state-of-the-art compression algorithms.

A compression method that maintains the possibility of doing computation directly on the compressed data might be called homomorphic compression. (There's a somewhat related concept of homomorphic encryption which allows for doing computation on encrypted data without having to decrypt it first.)

I don't know if homomorphic compression is a widely studied concept, but at least you might now have a name for it.",1,0,0,False,False,False,1645712908.0
sys7fr,hy0fg08,t3_sys7fr,"Solve easy problems.
Solve slightly less easy problems.
Solve slightly harder problems.
Repeat.",17,0,0,False,False,False,1645562846.0
sys7fr,hxzgrg9,t3_sys7fr,"NO. Intellicence is ~~learned~~ trained (ok, there are environemental and genetic factors).

Intelligence (and creativity, which, depending on definition might be the same) is, in my experience a ~~learned~~ trained ability. And I mean in all subcathegories (individual talents, such as math, language, music, art etc.).

Quite frankly, with young people/students, intelligence is just actually deep thinking about a problem. It's hard work, thus most people avoid it. But it gets easier with training, and bam! suddenly you are intelligent.

Sure, there are predispositions, genetic (dyslexia/dyscalcula, AD(H)S, depressions etc. with their disadvantages and advantages), but these can be overcome with hard work (lots of people with dylexia speak multiple languages - and often better than their peers, because they had to put in extra effort).

So, to learn problem solving, first identify what you want to learn - different disciplines have different approaches. Examples (my experience, feel free to correcft!): Engineering is all about calculation; physics is about models; math is about proofs and consequences; medicine is about information and studies; chemistry is about gut feelings (i was told so, not sure?);

and computer science is about complexity management (That is to cut a problem into smaller black-boxed units, thus reducing a large problem into smaller less complex problems. All the way down to +1, -1 and ==0.). And then are the different approaches for programming: a data scientist looks at aproblem differenlty than a JS coder.

Then just find easier problems and try to solve them by thinking about it. Will be hard and need concentration, but that's how one advances. Then gradually try harder problems.

There are (free) programming-challanges on some onine sites. Try these. Or if you want try reading some CS books, and try to follow along.",57,0,0,False,False,False,1645549648.0
sys7fr,hy0x7kh,t3_sys7fr,"i honestly firmly believe even the stupidest of us can learn high level concepts. Its all about principles. I think our educational system is genuinely trash. Garbage. And its because of ignornace.

Principles",5,0,0,False,False,False,1645569583.0
sys7fr,hy0vrk1,t3_sys7fr,A skill that can be learned.,3,0,0,False,False,False,1645569015.0
sys7fr,hy2kamm,t3_sys7fr,Leaving this here for you. http://www.aaronsw.com/weblog/dweck,3,0,0,False,False,False,1645597836.0
sys7fr,hxzeqrf,t3_sys7fr,It’s a skill that one can build with practice and patience!,5,0,0,False,False,False,1645548871.0
sys7fr,hy0vwmd,t3_sys7fr,I learned it in my youth playing video games like The Incredible Machine and others,2,0,0,False,False,False,1645569070.0
sys7fr,hxzk3lu,t3_sys7fr,Honestly by being wrong alot. Just be persistent and take problems you don't understand. It may take a while but you will figure it out,6,0,0,False,False,False,1645550933.0
sys7fr,hxzgs4i,t3_sys7fr,By doing/solving problems and learning from how others solved them.,2,0,0,False,False,False,1645549655.0
sys7fr,hxzhlgh,t3_sys7fr,"There is some genetic preposition that might give you some boost. But it's basically the same thing as with sports: 

Training beats talent!",0,1,0,False,False,False,1645549967.0
sys7fr,hxzix50,t3_sys7fr,"I've recently been tutoring new Computer Science students. For these students, it's a daunting task. The tricky part is that you're actually having to learn multiple things at the same time. It's not just about learning what code is or how to code. 

The two major concepts are problem solving and writing in a programming language. They're completely different skills, that are often presented as one thing. You have to learn how to break a problem down into steps, and then learn how to map those steps to a given language. Computers do not ""think"" like people do. 

This is why a lot of people point towards writing a problem down on paper in pseudocode first. Solve the problem first, then figure out how to implement it. The first part is the hard part of learning. The second part is the frustrating part of learning. Especially if you aren't using an IDE to yell at you for that missing ;, you might be left with absolutely no clue how to figure out what the problem is. 

While breaking down a problem into smaller pieces might come easier to some, it's definitely a skill that is learned. You start with the basics, and then build up different techniques and applications of those techniques. You read about different tools, or different ways to solve a problem. 

Computer Science is not a field that lends itself well to rote memorization. No one cares that you memorized the time complexity for 35 sort algorithms. What matters is that you understand how and (more importantly) when to apply them. 

At my university, the Calculus track is perhaps a little slower paced than others. But it culminates in a Calculus 3 that uses every single technique from Calculus 1 and 2 in a new and different application: sequences and series. You begin to get a picture of math as not some esoteric thing but something with real applications to other problems. 

As you get comfortable with the implementation basics and can at least put together a program that compiles and executes, it becomes easier to learn new techniques while learning a new syntax. Skills are transferrable. 

TL;DR: We start out learning how to problem solve concurrently with implementing this solution in something completely foreign. This makes things even harder.",1,1,0,False,False,False,1645550478.0
sys7fr,hy1b322,t3_sys7fr,"Well first of all there are different kinds of problems. These problems require different skills, methods and techniques to handle them. For example the categories in the cynefin framework. A baker solves the problem of baking bread every day. Is this ability innate to him? I think not.

The kind of problems you are thinking of are certainly what is described in the cynefin framework as complex problems. However, it is important to remember that these discrete categories are only a simplification. A problem can consist of any number of other problems that belong to different categories. The essential factor that makes a problem challenging is complexity. Here it is necessary to understand what constitutes complexity, e.g. a high number of variables, the relation of these variables, dynamics, or multi-targetness.
We humans have different capacities to handle complexity mentally. However, there are ways and means to simplify this. For example divide and conquer, visualizations or mnemonics.

Therefore, from my perspective, the ability to solve problems is always dependent on the individual ability of the person to handle complexity. This is influenced by a multitude of factors. Therefore, the ability to solve problems is not characterized by a few discrete factors but by a potentially infinite number of factors that have made the individual the person he or she is.",1,0,0,False,False,False,1645575415.0
sys7fr,hy1fbm1,t3_sys7fr,"Problem solving is like a language.  Start with it as a young child, and you’ll be better with it than most who did not.  A strong curiosity at any age can make up for a lack of experience as a kid.

The things that massively inhibit this are typical authoritarian behaviors:
* hostility towards questioning assumptions
* hostility towards mistakes
* not encouraging people to research
* not having toys to play with

Great ways to learn include:
* taking things apart, and trying to put them back together
* trying to build kits and diy plans
* teaching yourself to assemble a computer, like a raspberry pi
* teaching yourself to program or script to solve specific problems or provide helpful functions
* learning how to repair cars, build with wood, forge metal, or any other complex skill that entices you
* minecraft building, redstone logic gates, etc count for this too.

But if you can find nothing that motivates you to try to figure things out on your own, you’ll be at a huge disadvantage.",1,0,0,False,False,False,1645577371.0
sys7fr,hy1jhf8,t3_sys7fr,"its partially natural i think, yea

other than that it can be improved just by doing

the though process is similar to math although the application is different but its like a procedural process w input and output & consistent modifications based on a formula that can be tested for accuracy",1,0,0,False,False,False,1645579255.0
sys7fr,hy23w8t,t3_sys7fr,I studied Physics and Math in college. Solving those types of problems are really helpful for tackling programming problems.,1,0,0,False,False,False,1645588686.0
sys7fr,hy2b3td,t3_sys7fr,"It's both. 

Your intelligence is fixed at birth. For example, Isaac Newton or John von Neumann. You can't ""train"" to be as smart as those guys. They were born that way.

However, problem solving is absolutely a skill that can be trained. Will you get as good as von Neumann? No, but you can still become very good. Even with innate talent, you still need study and practice.

How does one train? Well, writing software is a pretty solid way. So is learning math and physics.

There are no shortcuts.",1,0,0,False,False,False,1645592299.0
sys7fr,hy2nhj1,t3_sys7fr,"As with most things, a little of both. 

You know what's most important? Actually being interested in it.",1,0,0,False,False,False,1645600118.0
sys7fr,hy2xx2s,t3_sys7fr," I would suggest reading at least the introductory sections in Paul Zeitz's ""The Art and Craft of Problem Solving"".

 He lays multiple factors that affect your problem solving skills; some of them are about experience, some are psychological, some are general techniques to apply.

 And he states at the beginning of the book that he believes this skill can be improved upon. If you have the time, please do read the book past its introduction and strategies. The problems presented there are almost always interesting. The book made me discover some stuff on my own that I would not have attempted otherwise because of how daunting the questions look at first glance.",1,0,0,False,False,False,1645608511.0
sys7fr,hy33rcd,t3_sys7fr,I created problems first then I tried to deal with the fallout.,1,0,0,False,False,False,1645613391.0
sys7fr,hy3e5s3,t3_sys7fr,"I think some people can have a natural tendency towards the curiosity that leads to strong problem solving skills, but you can absolutely improve those skills. A lot of people tell me I’m a savvy problem solver. I think I’m just stubborn and will bang my head against a challenge until I find a feasible solution. I don’t think I’m much, if at all, smarter than people around me. I just look at a thing and go, there’s got to be a better way and I’m going to find it if it exists. 

There’s also different kinds of problem solving. I work in accounting at the moment. I have a colleague that can spend hours untangling a single account. I don’t have patience in that area. I’m like, you tell me what the problem was and I’ll find a tech based solution or procedure change to ensure that doesn’t happen again. 

We are both valuable to our department.",1,0,0,False,False,False,1645620644.0
sys7fr,hy3sgxa,t3_sys7fr,"The problem with school is that it makes the “smart” people “feel smart and confident” and the “not-so-smart/average” people feel the opposite. You can learn anything if you apply yourself to it. You may not be a so-called genius, but the more you learn, the better you become and that applies to anything or anyone.",1,0,0,False,False,False,1645627677.0
sys7fr,hy3wtpa,t3_sys7fr,"To repeat what other have said with an anecdote: I used to be so bad at math. I did okay in algebra, geometry, and pre-calc. When I started my calculus class in college I was getting my butt kicked. I knew how serious it was for me to pass and make a good grade so I used to stay up late every night completing calculus problems in the textbook and finding more problems to solve on the internet. I was the only person in that class to score 100/100 on the first calc test. The professor thought I was rain man or something.

You just have to practice solving problems. It’s important to remember that there are different types of problems and that you have to take an approach that applies to that problem.

“Everybody is a genius. But if you judge a fish by its ability to climb a tree, it will live its whole life believing that it is stupid.” -Albert Einstein",1,0,0,False,False,False,1645629516.0
sys7fr,hyc2inx,t3_sys7fr,For the people who are REALLY good (like the best in the world) they usually started at quite a young age. Most people can become very good at any age with consistent practice.,1,0,0,False,False,False,1645759714.0
sys7fr,hxzhzs3,t3_sys7fr,"It’s learned, we have a whole subject on problem solving, there’s a whole process to follow making charts and basically listing all the possibilities(and I mean all) and literally testing each one",-2,0,0,False,False,False,1645550121.0
sys7fr,hxzvs42,t3_sys7fr,"I think the curiosity might be ingrained in certain people, but the process is learned. Experience and repetition, along with trial and error, helps you hone your skills. It becomes ingrained in you after a while.",-2,0,0,False,False,False,1645555368.0
sys7fr,hxziab2,t3_sys7fr,Problem solving skills are gained. However how far you will go is innate.,-4,0,0,False,False,False,1645550237.0
sys7fr,hxzwkgg,t3_sys7fr,"It's learned. IMHO, one of the primary skill in problem solving is to distill noise and narrow it down to the core problem/root cause. Then solve it in phases starting from the root cause. 

As you gain experience, then you'll learn to trade off. Should this be solved faster or should the root cause be taken care of first or we need to solve what's better for the optics or according to stakeholder's need and wants or etc, etc",-2,0,0,False,False,False,1645555666.0
sys7fr,hxzwvmx,t3_sys7fr,"In my experience repetition and trying your hardest will grant you the most success. I used to have serious imposter syndrome and would procrastinate even starting things because i was afraid they would be hard and I wouldn’t know how to do them.

I realized later on that when starting a new project theres a good chance I won’t know how to do it, and that that’s a hump I have to get over every time I’m doing something new. This isn’t a bad thing, because I feel learning something new often helps me understand a concept that I may need in the future, so when I start that future task I feel much more confident.

Basically, practice. When you fall, just make sure you get back up because that’s just how it is. The best engineers probably don’t think up an idea and have it work the first try, and you probably won’t either. You just can’t give up.

Small tip; when beginning a problem think ahead. If you just start programming without considering what could go wrong because of how you’re currently implementing things, you will have to do a ton of backtracking. This is why pseudo code and flow charts can be very useful.",-3,0,0,False,False,False,1645555780.0
sys7fr,hxzxzpg,t3_sys7fr,"100% investiture of effort. Some people will be better at aspects of it quickly, but the brain can go further than any of us understand. 

The way we measure it might trend toward a specific type of intelligence. But that is usually mass appeal over actual problem solving.

Could rant for a while about this, but multiple time collage failure now CTO here, Einstein said it best “I don’t thing I’m smarter than anyone, I just spend more time on the problem” not exact quote. 

Keep thinking about something you especially if other people don’t understand why you are confused!",-3,0,0,False,False,False,1645556195.0
sys7fr,hy2uzri,t3_sys7fr,"No it’s definitely not in the DNA. 

I’d say it’s a mixture between experience and luck (luck bc the right idea popped up in your head) I like to think of the brain as a random idea generator, and everything around it is me. So implementing testing and tweaking the idea is the part which you can influence. You can even influence the quality and number of your ideas. But not really the idea that comes out of your Brain. All the parts you can influence are improved by training. 

So I’d say you learn creativity by having fun. 

The more fun you have the more time you’ll spent in the subject and the more you’ll train.",0,0,0,False,False,False,1645606074.0
sys7fr,hxzjrlu,t1_hxzgrg9,"Sorry to quibble with you over this, because I agree with almost everything you wrote, but intelligence isn't learned.

>Intelligence is different from learning. Learning refers to the act of retaining facts and information or abilities and being able to recall them for future use, while intelligence is the cognitive ability of someone to perform these and other processes. 

It's the [capacity to learn](https://en.wikipedia.org/wiki/Intelligence). While you can improve this capacity with practice, much (not ALL - *much*) of our intellectual capacity is [baked-in at birth](https://www.nature.com/articles/nrn2793):

>More than 100 years of empirical research provide conclusive evidence that a general factor of intelligence (also known as g, general cognitive ability, mental ability and IQ (intelligence quotient)) exists, despite some claims to the contrary. Intelligence can be reliably measured, is stable in rank-order across the lifespan, and is predictive of many important life outcomes, including educational and occupational success, health and longevity.

>Intelligence shows high heritability in quantitative genetic studies; this heritability increases across the lifespan to mid-adulthood and partly overlaps with genetic variance that influences brain structure.

That doesn't mean that a lot of people's intelligence differences aren't also due to shortcomings in nutrition, access to quality care, childhood stimulation, and decent schools.",23,0,0,False,False,False,1645550804.0
sys7fr,hxzstbe,t1_hxzgrg9,"If you're going to redefine the meaning of intelligence to be that of knowledge, you should just use the latter.",6,0,0,False,False,False,1645554245.0
sys7fr,hy188pt,t1_hxzgrg9,">Sure, there are predispositions, genetic (dyslexia/dyscalcula, AD(H)S,   
depressions etc. with their disadvantages and advantages),

I can throw in my own experience. Since ADHD makes you think in a pretty weird way (sometimes it feels like an NFA) ADHD CAN help in computer Science. It was way easier for me than for my classmates to come up with problem solving ideas. But i had problems to learn concepts like Interfaces, Classes and such. To sum it up: i have an advantage in understanding and solving problems, but a huge disadvantage in digging very deep into code and errors. But in my opinion its fantastic, that we live in a world where different kind of thinkers can work together.",1,0,0,False,False,False,1645574172.0
sys7fr,hy2nny6,t1_hy0vwmd,And to this day he still thinks mice power his computer,2,0,0,False,False,False,1645600251.0
sys7fr,hxztrpz,t1_hxzk3lu,"The persistence part is of utmost importance.

You will be wrong alot, and the learning happens when you do not give up.",2,0,0,False,False,False,1645554606.0
sys7fr,hxzt1q2,t1_hxzhlgh,"Well at the very top level, both are necessary, neither one alone is sufficient. This applies to both problem solving and sports.",2,0,0,False,False,False,1645554333.0
sys7fr,hy0nlx0,t1_hxzjrlu,I agree I mande a mistake with the word choice. Should have written trained instead of learing.,4,0,0,False,False,False,1645565892.0
sys7fr,hy07f1w,t1_hxzt1q2,"Maybe. But what be a World Champion in problem solving like? How would you measure it? It's also related to the domain I guess. 

I'm talking about decent problem solving capabilites.",2,0,0,False,False,False,1645559779.0
sys7fr,hy2wjv2,t1_hy0nlx0,Either you have really fat fingers or your autocorrect is going crazy.,2,0,0,False,False,False,1645607359.0
szg2tp,hy3doza,t3_szg2tp,"**Important note**: The point is not that the ""calculus"" of OWL is used to solve problems. It is about expressing the semantic description of the problem holistically by means of the ontology.",1,0,0,False,False,True,1645620376.0
szg2tp,hyetzz4,t1_hy3doza,Those are just semantic games you are good enough,1,0,0,False,False,False,1645813614.0
sywkhe,hy0kcj7,t3_sywkhe,"You train on your training set, so the results there wont be consistent of the real distribution.

You evaluate your model on your validation set - a set that does not change the gradient but will approximate better the results on the real distribution.

That model reached the best validation accuracy on epoch 5 so thats the best model for the real distribution.",2,0,0,False,False,False,1645564681.0
sywkhe,hy08sxk,t3_sywkhe,"As log says, val loss didn't imrove since the fifth epoch and it's the main criterion for model estimation. 

As an alternative version, how could it be – model saving settings, but it's less likely",2,0,0,False,False,False,1645560307.0
sywkhe,hy0qq3i,t3_sywkhe,"As others are saying, whatever program you're using seems to weigh `val_loss` more so it goes with that.

Here's a good discussion and answers: https://datascience.stackexchange.com/questions/46523/is-a-large-number-of-epochs-good-or-bad-idea-in-cnn


My basic understanding and explanation:
Less epochs can be a good thing to simply not over-fit your data. If you have a training set, you don't want the training to become extremely accurate to the point it basically memorizes the dataset. You want your cnn to recognize patterns. The more you run it, the more details it will gather.",1,0,0,False,False,False,1645567063.0
sy225y,hxx6ytz,t3_sy225y,"I actually like the idea of understanding some linear algebra concepts via graph theory, but unfortunately I don't think this article gets the job done. It's kind of rambling and doesn't help with any of the parts of linear algebra that are hardest to understand.",7,0,0,False,False,False,1645501480.0
sy225y,hxwi73q,t3_sy225y,"As a mathematician, this is an insane read lol. Feels like trying to explain the english language through tax forms!",19,0,0,False,False,False,1645489874.0
sy225y,hxyvdm5,t3_sy225y,"I've never heard of matrix multiplication being referred to as a ""dot product"" before.  I thought a dot product returns a scalar value.

Edit: With all due respect to the author, this honestly reads like it was written by someone who doesn't know linear algebra but discovered last weekend that adjacency matrices exist and that doing operations on the matrix results in transformations to the graph.  (Which, is true and cool, but is a lot more limited in scope than ""explaining linear algebra through graph theory."")  There's a lot more to linear algebra than just matrix math, and there's a lot more to even matrix math than what's covered in this writeup --- there are graph eigenvalues, for instance!",4,0,0,False,False,False,1645541008.0
sy225y,hxzw8hd,t3_sy225y,"This article is unfinished, right? You need to show how the shortest path algorithm is based on matrix-vector multiplication, with a generalized multiply & reduce operator. Ditto all-pairs-shortest-path is matrix-matrix multiplication.

Or the other way around, in your view :-)",1,0,0,False,False,False,1645555540.0
sy225y,hxx8n16,t1_hxwi73q,"I feel like this article really stopped just short of being useful for anyone. 

What's discussed here isn't all that useful to those who want to learn about linear algebra nor those who want to learn about graphs. There's hints of some elegant complexity hiding right behind the surface towards the end, but it climaxes at a simple one-sentence observation, and then we're back to the mundane ""here's a transpose"". Before you know it, it's over and I was just left with the feeling ""this was it?"".

There's a beautiful abstraction on adjacency matrices that their multiplications represent a very compact form of summing up all hops between two vertices:

    M_i^T * M_j = sum_k [i -> k] + [k -> j]

where `M_ik = [i -> k]`is the weight of the `i -> k` edge.

It's fun too, because one of the surprisingly useful operators is to compute the fix-point, and you can do this simply via an inverse operation - (I - M)^(-1) = M^0 + M^1 + M^2 + ...

This result can generalize to linear algebras that aren't event numerical. In fact, it's even more powerful since we're able to find other computable fixed points using suitable domains to calculate fun things like reachability, connectedness, min/max paths, etc. 

--------------

My favorite exposition of this in my PLT courses is how to write a (really inefficient) parser for an arbitrary grammar composed of rules of terminals and non-terminals with the underlying algebra that `a + b` means alternation (match either the subterm `a` or `b`) and `ab` AKA `a * b` means concatenation (match the subterm `a` followed by the subterm `b`).

If you forget about LL/LR/LALR grammar *generators* for a moment, it's actually incredibly easy (but also inefficient) to execute a parse trace for any arbitrary grammar that follows these simple (but expressive) laws of linearity.

let's start with the following grammar

    X -> '(' A
    A -> X ')' | A A | '(' ')'


This contains 2 non-terminals (aka variables) - `X, A` as well as 2 terminals (aka letters) - `(, )`. Non-terminals are ""names"" of rules and are upper-case, terminals are letters to actually match during a parse and are quoted.

Now, let's construct a suitable algebra on top of this grammar language. We're interested in outputting a parse tree. For example, if the input is `(())`, the grammar (which could admit many parses) may output the tree


    A = X ')' = X['(' A] ')' = X['(' A['(' ')']] ')'


So, our underlying algebra will have to manipulate these (sets of) parse trees. Here's a semiring that can do the job:

`ParseTreeSemiring<G>` = (set = `Set<ParseTree>`, `+` = set union, `*` = `TreeMult<G>`)

where `G` is the underlying grammar that generates this semiring. The objects in this algebra are sets of ParseTrees, because you can generate multiple valid parses for the same string on any given grammar `G`.

Now, before we get to the magical `TreeMult` operation, let's define a corollary operator, this time operating on the terms of a grammar G - I call it the `left-of` operator that takes in bigrams (two terms concatenated together). Here's what it does

let's say we have the same grammar as before


    X -> '(' A
    A -> X ')' | A A | '(' ')'


`left-of(xy)` = all non-terminals where xy appears as a subterm

For example, left-of('(' ')') = {A}, left-of('(' A) = {X}, and left-of('(' '(') = 0.

What is 0? It's the identity of set-union - the empty set. In addition, whatever we define the TreeMult operation to be, 0 / empty-set must also be its anihilator so that 0 * x = 0.

Now, we can define `TreeMult(T, T')` on singular trees `T` and `T'` as the set of trees rooted at A - (A, T, T') - such that A is in `left-of(T.root, T'.root)`. To lift TreeMult onto sets of trees, do a point-wise union.

Finally, to find the set of all valid parse trees: 

1. Lift this semiring through the usual semiring extension into matrices
2. For a string of length n, create an matrix n+1 by n+1 matrix with all 0s except just the first upper off-diagonal filled with the trivial parse tree of just the letter at the i-th position. Call this T, and move it into a singleton set {T}
3. compute the power series until you get a fixed point:

 {T} + {T} * {T} + {T} * {T} * {T} + ...

AKA

 {T} + {T}^2 + {T}^3 + ...

Note that you're guaranteed convergence within sqrt(n) rounds

4. Look at the 0,n^th element of the final matrix, and you'll find a set of all possible parse trees!

Note - there's no inverse, no multiplicative commutativity, and no identities here! We're pretty handicapped from an algebraic perspective, and yet we can still solve this reachability problem (reaching a feasible parse tree) through simple lin-alg operations.

---------------------

Why does this work? 

The multiplication here has a very specific meaning - `T' = left-of(ab)` is an operation that tells us all possible rules that could produce a trace of `T = ab`. That's hard to conceptualize, until you reverse the process and realize that it's just saying that two trees T' & T are related iff T is derived (using the rules of the grammar) from T'. The `left-of` operation induces a graph structure that connects two trees together iff one can be derived from the other. 

In other words, it's the (reverse) parse derivation graph, and the matrix form is just the adjacency-derivation matrix of this graph.

Combined with the semantics of matrix multiplication on adjacency matrices, and we can reduce parsing down to a simple problem of repeatedly multiplying an adjacency matrix using a very exotic semiring.

In fact, many of these (linear) language (and anything you can pose as a reachability graph) problems can be straight-forwardedly reduced to matrix multiplication (or finding an eigen-state) as long as you can find a suitable semi-ring. Though, be warned, many of these are ugly af.",15,0,0,False,False,False,1645502324.0
sy225y,hxwo3uq,t1_hxwi73q,"I find intuitive to grasp graph theory concepts through linear algebra, but the opposite feels like an aberration",3,0,0,False,False,False,1645492617.0
sy225y,hxzwg7d,t1_hxx8n16,Brilliant. I just entered a much shorter reply making a similar point. Though the grammar stuff is new to me. Thanks for spelling all that out.,1,0,0,False,False,False,1645555622.0
syffqo,hxxlxyk,t3_syffqo,"Most popular 50 years from now? Definitely not. But will a lot of the popular ones from now still be around? Probably, even if only because these languages will need to be maintained in legacy systems",12,0,0,False,False,False,1645509857.0
syffqo,hxycbbx,t3_syffqo,"C is 50 and people still use it. COBOL is 63 and banks still use it.

A lot of languages we use now (Java, Python, C++) are over 30 years old. Many others are getting closer to 30 (PHP, JavaScript, Ruby).

So it's not surprising if we use a language for 30+ years. Considering JavaScript is still the only official browser language I could easily see it being the dominant language. It already has taken over and seems to be the most used. I don't see that changing anytime soon.",5,0,0,False,False,False,1645530490.0
syffqo,hxxnhlm,t3_syffqo,"Look up domain specific languages. 

Basically we program with high level rules that generate lower level rules recursively. This approach lets us bring forward the previous decade of debugger innovation.

Which is why tomorrow’s language will be syntax sugar to produce today’s language which actually produces yesterday’s language and so on",3,0,0,False,False,False,1645510862.0
syffqo,hxyd91w,t3_syffqo,"What motivates people is generally speaking functionality and simplification, as well as accessibility.
I have never known anyone who worked on a programming language, but I would imagine they would have to greatly enjoy working with syntactical rules and linguistics, so not purely computationally-oriented minds.",1,0,0,False,False,False,1645531154.0
syffqo,hxzc5be,t3_syffqo,"The lingua franca for informatics has been C forever and that is unlikely to change.

Conventional computation is likely to remain useful for the foreseeable future, and we have no prospective computer architecture changes that will make any impact in that direction so far. Changing the lingua franca would likely require development of architectures not suited to the sole trend in chip development which simulates being really advanced pdp-11's for C runtimes.

C has control of the minds of every chip designer out there. They aren't generally working on replacing C, they're usually trying to make C programs faster without anyone needing to write new C compilers.

I don't personally see why anyone expects change. We've had DSLs as long as we've had C, and we've had Lisp longer than that -- providing more type systems than all recent FPLs combined with more runtime features than Python to boot. C took over for a reason, and the territory largely stopped changing with it, so our needs are unlikely to progress far enough to need much else.

I personally don't expect to see another language take over until our CPUs operate more like GPUs, but as Sony discovered in 2006 almost zero engineers want to write software that way.",-1,0,0,False,False,False,1645547870.0
syffqo,hxy3h26,t3_syffqo,No idea. Maybe Python?,0,0,0,False,False,False,1645523357.0
syffqo,hxxwafk,t1_hxxnhlm,"I agree. The future of languages very may well be families of languages, [like look at F*](https://www.fstar-lang.org).",1,0,0,False,False,False,1645517278.0
syffqo,hxydoss,t1_hxxnhlm,"> This approach lets us bring forward the previous decade of debugger innovation.

Is this a serious statement or tongue-in-cheek?",-1,0,0,False,False,True,1645531455.0
syffqo,hxxwb9b,t1_hxxwafk,"It seems that your comment contains 1 or more links that are hard to tap for mobile users. 
I will extend those so they're easier for our sausage fingers to click!


[Here is link number 1 - Previous text ""F*""](https://www.fstar-lang.org)



----
^Please ^PM ^[\/u\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20hxxwb9b)",1,0,0,False,False,False,1645517298.0
sxv3dt,hxujlds,t3_sxv3dt,"So let's try to go letter by letter:

GRASP is a metaheuristic consisting of two phases: a constructive randomized adaptive phase and a search phase. 

During the initial phase, we try to ""build"" a feasible solution for the problem we are tackling in both a greedy and randomized way by iterations. At each iteration we use a greedy function that will pick the ""best"" possible element for our solution in a greedy and myopic way. 
A **greedy** function will only consider the best possible element at that iteration and will not take into account possible future arrangements that are ""better"" than our initial greedy decision. 
However, GRASP doesn't actually pick the best element according to our greedy function always. As part of the randomized component, we use a list in which we store our ""best candidates"" (according to our greedy function) of each constructive iteration and pick one at **random**. This list has the specific name of Restricted Candidate List and has a lot of room for tweaking. 
The greedy function actually changes during each iteration of our constructive phase, as it **adapts** itself to the possible elements left to build our solution.

Once we are done building an initial solution, we want to explore the local search space of it. What this means is that if we use a greedy approach to solve our problem, we are likely to be stuck in a local optimum. This means that the solution may be good, but not the best. So, we use a **search** procedure to explore the neighborhood of our solution to try and find a better solution. This exploration is done using a search heuristic that will modify our solution bit by bit, finding new solutions that are similar to our original one and evaluating those solutions to see if we actually find any better. The search procedure can be a local search heuristic, or even another metaheuristic such as tabu search or scatter search. 

Finally, once we successfully applied our search procedure to our solution built in the first phase, we stop the iteration. As it turns out, GRASP itself actually runs during multiple iterations, with the idea that as it is a randomized heuristic, each iteration will produce a different solution. We need to keep track of the best solution found, so when the GRASP finishes, we return the best solution of all the ones we built and searched. 

I hope that's good enough, if not I can try and build the pseudo code but that was my best shot. I will try to answer any questions (of any other metaheuristic really)",3,0,1,False,False,False,1645461260.0
sxv3dt,hxu83wh,t3_sxv3dt,"Okay, so a greedy algorithm is one that always makes ""only good moves"". This can lead to the algorithm stopping at a place that ""looks good"" from the algorithm's point of view but which isn't actually the best solution. for example, imagine a 1D ""field"" consisting of line segments that can go up, down, or stay the same. Now imagine a bot that's looking for the highest point in that field. As the bot moves through the field, it is willing to go up, or stay the same, but it never goes down.

    Bot-1              Bot-2     /\
    ↓     /\           ↓   /----/  \------\
    *----/  \------/\--*--/                \-------

In the first move, Bot-1 can move right but nothing else. As the ""game"" progresses, Bot-1 will eventually find itself on top of that small peak on the left but it will never go down the other side of that peak, so it will never find the ""true"" solution, the high peak on the right. 

Bot-2 is in an even worse position - it has 2 moves at the beginning, left or right, and while it will eventually find the ""true"" solution if it goes to the right, if it goes to the left it will end up with an even worse outcome than Bot-1.

As I understand it, GRASP solves this problem by running the ""bot"" multiple times in random starting locations and accepting the best solution from all of the runs.",4,0,0,False,False,False,1645456470.0
sxv3dt,hxx7hbk,t1_hxujlds,"Thank you, good sir. This clarified some of my questions. I am actually implementing this on a blood allocating system that picks the best compatible blood product to be given to patients with regards to the blood's rarity.  Do you think this GRASP is feasible in this project?",2,0,0,False,False,True,1645501734.0
sxv3dt,hxub7sz,t1_hxu83wh,"You're parallel to the right answer, but you have the wrong level of indirection/abstraction on your analogy. In your analogy, the bot moving spaces is the algorithm - this isn't correct - the spaces are instead candidate solutions to some other problem.
 
If we consider every space in your terrain a possible solution, and consider the problem to be eg the traveling salesman problem, each space is thus a candidate solution, or path. A 'neighbouring' square, is thus a path with 1 step of the path changed to something else. 'Up' is a shorted path, and thus a better algorithm, and 'down' is a longer path. The thing seeking a solution will thus go up but never down as you described.
 
Randomness is used for the cases where there are equal choices (like the left and right choice for bot-2), but also because in many problems there will be terrain (solutions) that doesn't connect to *any* other terrain at all.",3,0,0,False,False,False,1645457813.0
sxv3dt,hxx803c,t1_hxx7hbk,"I have some questions regarding the project, mind if I send you a dm?",2,0,0,False,False,False,1645501998.0
sxv3dt,hxxc5ey,t1_hxx803c,sure,1,0,0,False,False,True,1645504149.0
sxwmi4,hxx3yj3,t3_sxwmi4,Alice in Wonderland computing,3,0,0,False,False,False,1645500016.0
sxwmi4,hxuikbf,t3_sxwmi4,It’s uncommon but old school ai applications made use of paraconsistency or had paraconsistency as an explicit design goal of their system (in the way that natural language might be described as paraconsistent) https://en.m.wikipedia.org/wiki/Reason_maintenance,2,0,0,False,False,False,1645460847.0
sxwmi4,hxw8vgv,t3_sxwmi4,"It’s important to consider the uncertainty of the information you think you have. 

In paraconsistent logic, from a premise that is both false and true, a finite set of conclusions may be drawn, and any conclusion may be drawn - so any conclusion is sometimes true. 

This is the same as saying that every conclusion has some error bars on it, which is the same as admitting that every premise comes with some uncertainty. 

It’s possible to solve the Halting Problem faster with a deterministic, stochastic machine rather than a turing machine, (check out Chomsky grammars) but you’d need to find a steady source of true randomness - this is a materials science problem rather than a computer science problem because physicists disagree about what maximum entropy is / would be. 

Even in the AI case, it’s impossible to actually make decisions about paraconsistent data (even if you found a way to deal with it in a non-stochastic way) without accidentally appearing to follow some probability distribution - this is just the Law of Large Numbers in action. Statistics always seems to rule the day under real world observation, even if you can weasel out of it in your reasoning.

Edit: two words.",1,0,0,False,False,False,1645485772.0
sxwmi4,hxymvv9,t1_hxx3yj3,"And you just gave me the perfect title of a book/paper on paraconsistent computer science: ""Computing in Wonderland.""",3,0,0,False,False,True,1645536917.0
sxwmi4,hxuilx2,t1_hxuikbf,"**[Reason maintenance](https://en.m.wikipedia.org/wiki/Reason_maintenance)** 
 
 >Reason maintenance is a knowledge representation approach to efficient handling of inferred information that is explicitly stored. Reason maintenance distinguishes between base facts, which can be defeated, and derived facts. As such it differs from belief revision which, in its basic form, assumes that all facts are equally important. Reason maintenance was originally developed as a technique for implementing problem solvers.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",6,0,0,False,False,False,1645460866.0
sxwmi4,hxujlft,t1_hxuikbf,"Ah yes. There are some nice papers on paraconsistent psychology. I would place that into what I call ""soft paraconsistent logic"" where two contradictory statements aren't actually both true but simply believed to both be true, vs hard paraconsistent logic where two contradictory statements are allowed to actually be true.",4,0,0,False,False,True,1645461260.0
sxwmi4,hxum77p,t1_hxujlft,Yeah that sounds right to me. Definitely more of a gesture in the direction of paraconsistent logic than an implementation,1,0,0,False,False,False,1645462287.0
syfpnn,hxxo5gi,t3_syfpnn,Sounds like a kahnacademy lesson. This is done to death. What makes your version unique ?,3,0,0,False,False,False,1645511302.0
syfpnn,hxzoj4b,t3_syfpnn,i think loops would be important,1,0,0,False,False,False,1645552617.0
syfpnn,hxxpduc,t1_hxxo5gi,"Hey thanks for responding, we're planning to localize it to languages outside of English and working on a better project map.

Do you happen to have any teaching experience with computers you can share to improve the prototype?

We are a small team of volunteers, this project is not-for-profit, so we don't have the resources a larger project may have.",1,0,0,False,False,True,1645512128.0
syfpnn,hxzvujn,t1_hxzoj4b,Definitely we will include that. We plan on writing the content later after we setup the backend & frontend criticals.,1,0,0,False,False,True,1645555394.0
sxzgvj,hxv6xnn,t3_sxzgvj,"Worst case - no node in city is a sensor and every city is connected to every sensor with a false result on the weight.

You have to go over all the sensors * all nodes in the city, o(n * m)",2,0,0,False,False,False,1645470412.0
sxzgvj,hxvmfm7,t1_hxv6xnn,"The runtime is correct i think, but your description not . You *always* have to Iterate through everything, since you never know if the very last edge of the very last node might be over a weight of 50.

If every node in City is in sensors, still the last edge needs to be checked to Check for full coverage",1,0,0,False,False,False,1645476658.0
sy3zvc,hxx4mar,t3_sy3zvc,"Is it part of an equation or identity?  I'd guess is it helps describe some bound for a graph, maybe an upper bound on the number of edges or something similar.  It's hard to be specific without some more context.",1,0,0,False,False,False,1645500336.0
sxbbmi,hxr0too,t3_sxbbmi,Hypervisors are simply software that run virtual machines so they're used pretty much everywhere all the time,55,0,0,False,False,False,1645391411.0
sxbbmi,hxr2dls,t3_sxbbmi,"Is your home PC running Windows 10 or 11 and built within the last five years or so? Then chances are [you're already using a hypervisor](https://techcommunity.microsoft.com/t5/virtualization/virtualization-based-security-enabled-by-default/ba-p/890167).

Hypervisors have quietly become incredibly mainstream. The entire cloud computing sector relies on their existence, for example.",32,0,0,False,False,False,1645392069.0
sxbbmi,hxr2en9,t3_sxbbmi,"They might not have called them hypervisors, they might just say they use virtualization. A lot of people even just say they ""run vmware"" or whatever their brand-name of choice is. Unless they are paying some hefty fees, their cloud workloads are on hypervisors.",14,0,0,False,False,False,1645392080.0
sxbbmi,hxrspn1,t3_sxbbmi,"If you're running anything on the web, dollars to doughnuts you are working on a cloud provider (AWS, Azure, Google Cloud, etc), and that is entirely enabled by hypervisors.
 
The fact it is abstracted away doesn't mean people don't use it.",10,0,0,False,False,False,1645403637.0
sxbbmi,hxr037z,t3_sxbbmi,"They're used all the time. All the time. Are you sure you just haven't talked to the right people or maybe you just aren't included in those discussions?

I haven't worked in a single company yet that wasn't using Hyper-V (at least).

Also wrong subreddit.",17,0,0,False,False,False,1645391097.0
sxbbmi,hxrfdsk,t3_sxbbmi,In telco they are used extensively. Virtualization layer adds a lot of value from business perspective.,4,0,0,False,False,False,1645397590.0
sxbbmi,hxrky0e,t3_sxbbmi,They're everywhere all the time,5,0,0,False,False,False,1645400069.0
sxbbmi,hxrle9h,t3_sxbbmi,"Yeah, VMWare and HyperV",3,0,0,False,False,False,1645400273.0
sxbbmi,hxr2p3e,t3_sxbbmi,Virtualized ~50 physical servers -> 4 with ESX 3.5 back in the day. Never looked back. Hypervisors are everywhere and have been for a long time,4,0,0,False,False,False,1645392203.0
sxbbmi,hxrl9h6,t3_sxbbmi,"I'm assuming you're talking embedded? Yes, there too hypervisors are relatively common, especially as chips (SoC) are getting more powerful and security is important, QNX, IBM and others all have solutions.

So instead of 'back in the day' when we had a control panel facing the network and a controller connected with serial or other wires, now we can have both systems on the same board in hardware while still maintaining a logical separation between the 'world' and the controllers (eg. robotics).",2,0,0,False,False,False,1645400212.0
sxbbmi,hxrt08z,t3_sxbbmi,I’m a Site Reliability Engineer for a cloud computing company. The entire cloud is hypervisors running on hypervisors running vms running docker containers.,2,0,0,False,False,False,1645403772.0
sxbbmi,hxs72f3,t3_sxbbmi,Ever use an android phone? Hypervisor.,2,0,0,False,False,False,1645410407.0
sxbbmi,hxrqqbx,t3_sxbbmi,Every reasonably sized enterprise (and frankly most small businesses) leverages hypervisor technology in some way. Every CSP is using hypervisor based infrastructure. I’m not sure I understand the question.,1,0,0,False,False,False,1645402704.0
sxbbmi,hxs964q,t3_sxbbmi,"I don't quite understand the question - are you asking is virtualisation prominent in the technology industry? Are you asking if alternative technologies are used to virtualise, instead of using a hypervisor? If so then containers are very wildly used.",1,0,0,False,False,False,1645411403.0
sxbbmi,hxuxafb,t3_sxbbmi,"As others have pointed out, it is used everywhere by Cloud providers today with VMs and is key to abstracting machine layer. What you may not know, is that unikernel technology powers the networking stack in Docker for OSX & windows. Unikernels are small library operating systems that compile with your application to create a small service that runs off a hypervisor like xen. Maybe in the future we will use them before docker containers, but doesn't matter bc we already sort of do with docker!!!",1,0,0,False,False,False,1645466625.0
sxbbmi,hxral4b,t1_hxr0too,"> pretty much everywhere all the time

[Not in embedded stuff apparently](https://old.reddit.com/r/embedded/comments/sxaw90/are_hypervisors_commonly_used_in_the_industry/). Note that embedded is not only baremetal microcontrollers in my opinion. (Not trying to start a debate about what is embedded and what isn't)",-29,0,0,False,True,True,1645395543.0
sxbbmi,hxrqylp,t1_hxr037z,I pity them. Kidding. Kind of. HyperV is a bitch lol.,2,0,0,False,False,False,1645402810.0
sxbbmi,hxr3fy3,t1_hxr037z,I write embedded software. So that might be a notable difference.,-4,0,0,False,False,True,1645392519.0
sxbbmi,hxrgcss,t1_hxrfdsk,"> Virtualization layer adds a lot of value from business perspective.

sounds very interesting. Could you explain why?",3,0,0,False,False,True,1645398021.0
sxbbmi,hxrco2r,t1_hxral4b,"Most everyone here knows what embedded programming is...

But even so, *everywhere* does use hypervisors. By and large you're not programming with hypervisors unless you're making a program to manage your vms but a lot of even non tech workplaces use them.

My wife works in government and logs into a virtual machine for work for example. They use a hypervisor to manage the vms in the network.",22,0,0,False,False,False,1645396428.0
sxbbmi,hxrcal1,t1_hxr3fy3,"It may be indeed. Your OP keeps referring to ""the industry"". That is extremely vague. If by industry you mean the embedded device industry, then you are most definitely in the wrong subreddit. But even then, you could specify that in your OP so we all have an idea about where you're coming from.

For general software development or DevOps, virtualization is king and has been for quite a while and will be for a long time yet. Hypervisors therefore are extremely common.",12,0,0,False,False,False,1645396268.0
sxbbmi,hxuxqzf,t1_hxr3fy3,Hypervisors can be used to virtualize embedded platforms for testing.,1,0,0,False,False,False,1645466804.0
sxbbmi,hxrng6s,t1_hxrgcss,"Virtualized environments absolutely demolish bare metal in terms of ease of management. It is much easier to roll out features/updates/patches to virtual machines than to actual systems running on bare metal.

In real life you have thousands of computers scattered around the country. Those computers are not identical in terms of hardware. If you were to roll out updates to software running on these computers, chances are you would run into issues due to discrepancies in underlying hardware. If you however use virtualized environments then you eliminate much of the mentioned worries.

If you were to coordinate updating software on many different types of hardware it would be a pain logistically and would require a lot of manpower ($$$). 

Also consider this: if you have a computer and need it to perform a server workload you could install Linux and then set it up as a server. Or you could just install a hypervisor and spin up a ready image of an os with server set up on it. The second way is faster.
Then if you suddenly want this computer to perform a completely different task you can just unplug the vm image you had running and launch a different one. In the matter of seconds. It would be impossible to do this as swiftly with bare metal setups. 

Business direction can change quickly and virtualized environments can adapt to those changes just as fast. 

Those are just some examples off the top of my head. Personally, I used vms mainly for evaluating software and for malware analysis (if you have some questions about those use cases feel free to ask).",6,0,0,False,False,False,1645401193.0
sxbbmi,hxrlawl,t1_hxrco2r,"> Most everyone here knows what embedded programming is...

Some people here could argue that having an ARM cortex A running a hypervisor and with eg a X11 desktop alike graphical environment is not embedded anymore. But again, that is not the poinrt here",-37,0,0,False,True,True,1645400230.0
sxbbmi,hxrgm3u,t1_hxrcal1,"I was actually initially not referring to the embedded device industry per-se. I meant everywhere where software is used. I know they are used for VMs, so -obviously- VMWare and such. But don't know where else which is why I asked.",0,1,0,False,False,True,1645398135.0
sxbbmi,hxrr6e2,t1_hxrng6s,And then comes enterprise containerization and everything you said is true x2.,3,0,0,False,False,False,1645402913.0
sxbbmi,hxrnaw4,t1_hxrlawl,There are exceptions to every statement. Your comments just make you sound quite arrogant though honestly.,27,0,0,False,False,False,1645401128.0
sxbbmi,hxscayh,t1_hxrlawl,"Not sure what the relevance of the technobabble bombing is, but I can assure you, VM's are absolutely everywhere.

I know businesses that are held together by popsicle sticks that use hypervisors.",11,0,0,False,False,False,1645412908.0
sxbbmi,hxsbdbl,t1_hxrgm3u,so you were asking what are hypervisors used for outside of virtualization?,7,0,0,False,False,False,1645412457.0
sxmdkc,hxt1x7t,t3_sxmdkc,I think any algorithms book would do well as a lot of basic algorithms are graphs focuses. For example CLRS is considered one of the best books.,2,0,0,False,False,False,1645427758.0
sxmdkc,hxvjyd2,t3_sxmdkc,I’d take a look at dijkstra’s algorithm. Pretty simple algorithm involving DP and graphs.,1,0,0,False,False,False,1645475665.0
swtlqm,hxoy7nk,t3_swtlqm,"I cannot provide any helpful input, but I want to thank you for introducing me to the crazy awesome world of TAS. It's going to be an entertaining weekend!",11,0,0,False,False,False,1645356387.0
swtlqm,hxpmzl2,t3_swtlqm,"Kind of as a thought exercise which could produce an unbounded solution if you didn't know the world was solvable.

Starting from a time of zero seconds calculate, given a deterministic environment, the probability of being in s_t. You can knock out a good chunk of impossible simply by calculating 'straight line' simulation with theoretically minimum time required.

This path likely isn't feasible due to clipping constraints, which you can use to now model your problem as a linear curve (which Mario cannot follow) to arrive at a solution which is feasible.

If the probability of Mario being able to run this path is > 0, this is the optimal solution.

I've probably missed something, but this seems like a viable starting point.",4,0,0,False,False,False,1645370820.0
swtlqm,hxqdtd6,t3_swtlqm,"Have you seen the Ocarina of time speedrun? They do a sequence of moves, overflowing memory and rewriting code such that they trigger the ending sequence in the starting village. Its very possible that you can do that in M64 as well.",3,0,0,False,False,False,1645381761.0
swtlqm,hxotrr6,t3_swtlqm,"Disclaimer : I don't really master those algorithm, but branch and bound and branch and cut are methods that can give you the exact solution of such a problem. They do exactly what you describe : just ignore set of solutions that won't give you the optimal solution. 
I'm not sure how you could implement that for this problem but I guess it's possible xD",3,0,0,False,False,False,1645352725.0
swtlqm,hxq0hdf,t3_swtlqm,"> So it is possible that a seemingly nonsensical series of inputs could trigger a bug that has yet to be discovered and beat the game that way. 

Yeah, finishing the game by using an exploit to warp to the final level/boss/cutscene is also considered a valid way to beat the game usually. (See Super Mario World, Ocarina of Time...)

Also I think Trackmania TASers already use bruteforcing on short segments to get the bounce they want in some places.",2,0,0,False,False,False,1645376362.0
swtlqm,hxo7eft,t3_swtlqm,"im sure all but the most simple games can be proven to be np-hard, which essentially means bruteforcing is the only answer",6,0,0,False,False,False,1645335752.0
swtlqm,hxs8k1d,t1_hxqdtd6,"Exactly, so part of my question was related to viewing source code, or game assembly code, to identify those types of exploits. I’m curious if having the code is still not enough to determine the fastest run without brute force. Kind of like the halting problem where viewing code doesn’t guarantee you can predict when it will finish",2,0,0,False,False,True,1645411109.0
swtlqm,hxotxxi,t1_hxotrr6,"Also if you just want to beat the current record instead of finding THE optimal solution, then you could do a reinforcement learning approach I guess",3,0,0,False,False,False,1645352865.0
swtlqm,hxoto4r,t1_hxo7eft,"If no-hard implied brute force is the only answer, Concorde solver for tsp wouldn't exists",4,0,0,False,False,False,1645352640.0
swtlqm,hxs9yi1,t1_hxo7eft,"Yeah I figured this could end up falling under NP-hard  category with no optimal solution other than brute force. I just thought the concept of actually knowing the games source code, or assembly instructions, could be used to dramatically reduce the search space. It seems like that’s how humans have approached it so far knowing that exploits could exist due to integer overflow or order of operations logic. It just took us a long time to figure out how to trigger the exact conditions to use those exploits. I’m also curious how you could even go about proving when a game goes from simple to NP-hard by just reviewing the source code. Maybe the amount of branching logic or stuff like that.",1,0,0,False,False,True,1645411783.0
swtlqm,hxt737u,t1_hxs8k1d,Well there is no Halting problem since we already know a sequence of Input leading to finishing the game. Anything that takes longer than that can be ignored. Similar to branch and bound.,1,0,0,False,False,False,1645431797.0
swtlqm,hxp8xkf,t1_hxoto4r,What is Concorde's tsp using other than brute force and pruning/cutting/bounding?,3,0,0,False,False,False,1645363839.0
swtlqm,hxpp1xo,t1_hxp8xkf,"Pretty sure it’s all LP’s, cutting planes, and clever convergence results which is definitely not the same as brute force. 

However they also don’t guarantee an optimal solution, just some bounds. Eg Bill’s tour of all pubs in the UK I think they guarantee you can’t take a foot or a meter off of it.

In his words though it’s not finding an optimal solution that’s hard it’s knowing it’s optimal.",3,0,0,False,False,False,1645371693.0
swtlqm,hxq93n9,t1_hxp8xkf,"""what is concorde's tsp using other than brute force and (something that's not brute force)""",1,0,0,False,False,False,1645379858.0
swtlqm,hxqdkwh,t1_hxq93n9,"I thought pruning was essentially a smart brute force. I personally always considered pruning to be brute force with some optimizations. I mean, the implementation of the pruning algorithms I've made started off with a brute force template that was eventually optimized, but I'm certainly no expert on the subject.",1,0,0,False,False,False,1645381667.0
swtlqm,hya6ico,t1_hxqdkwh,"Brute force on TSP is O(n!), Concorde is waaaaaaaaaay faster",1,0,0,False,False,False,1645731842.0
sx86op,hxqmqrw,t3_sx86op,"We know that `an^2 + bn + c` should look like `an^2` when `n` is really large.  The `bn+c` shouldn't matter. Unfortunately that feeling by itself isn't a formal statement. We'll have to formalize it. 

The reason why `an^2 + bn + c` might not look like `an^2` is because `bn+c` could interfere. They would interfere more when `|b|` or `|c|` is large, especially if `a` is small. But again the way to make `an^2` be the dominant term is to make `n` really really big. How big should I make it?

Let's make `n` be larger than `100|b|` and `100|c|`. Then the `bn` term will be at most `n^2/100` in magnitude (because `|b|<n/100`), which will be much smaller than `n^2`. The `c` term will be at most `n/100` in magnitude, which is much more smaller than `n^2`. But these still might be larger than `an^2` if `a` is really small. Nevertheless we're on the right track.

Let's make `n` be larger than `100|b|/a` and `100|c|/a`. Then the `bn` term will be at most `an^2/100` in magnitude (because `|b|<an/100`), which will be much smaller than `an^2`. The `c` term will be at most `an/100` in magnitude, which is much more smaller than `an^2`. So for such a large `n`, `an^2 + bn + c` would be no larger than `an^2 + an^2/100 + an^2/100 = (102/100) an^2`, and no smaller than `an^2 - an^2/100 - an^2/100 = (98/100) an^2`.

Now this fits perfectly in the big theta definition. Just set `c1 = 0.98 a`, `c2 = 1.02 a`, and `n0 = max(100|b|/a, 100|c|/a)`.

Now we didn't need `c1` and `c2` to be so close to each other. We didn't need to take `100` in our definitions. The textbook probably chose something closer to `4`. And since `c` was at most `an/100` in our choice and we could afford `an^2/100`, we could choose `n` to be larger than `sqrt(100c/a)` instead of being larger than `100c/a`.

Make all those choices and you'll get something like what they got.",4,0,0,False,False,False,1645385452.0
sx86op,hxwywuy,t3_sx86op,"I recommend doing a visual example yourself to see what's going on. For example, consider [this graph](https://sagecell.sagemath.org/?z=eJyryM3MU7BVMODlqshNrACyDE15uQqAdEFOfolGRZyRgr6CsYKugrFWhYK2goWOQgVQgy2IADETK2xBhI5Ccn5OfpGtUlFqipImSL82igGGRtj1oSg1V9BSIKxcrzgjv1xDEwC-4zWY&lang=sage&interacts=eJyLjgUAARUAuQ==) of x^2 / 3 - 3 * x + 8. In this case a = 1/3, so I've plotted the original in red, and the large one (7a/4) and small one (a/4) in blue.

You can see that in the beginning, the graphs cross each other a few times, but then the top blue one grows too fast, and the bottom blue one grows too slow. That's the definition of theta notation - you can take your theta function and, by stretching or compressing it, surround your target function. Their calculation for n0 is just trying to find the final intersection point.

Indeed, by visual inspection you can see that the red one is trapped in the middle after x = 8, and the formula 2 * max(|b| / a, sqrt(c)/a) gives us 2 * max(3 / (1/3), sqrt(8 / (1/3)) = 2 * max(9, sqrt(24)) = 2 * 9 = 18. It actually overshoots the rightmost intersection at x = 8 by quite a bit, but that's not an issue, because all we really care about is the behavior for the infinite range after n0 - it doesn't matter if we overestimate where that behavior starts. In fact, this is also true for the 7a/4 and a/4 bounds. The precise numbers don't matter, all that matters is that we can surround the target function. This book picked 7a/4, a/4, and 2 * max(|b| / a, sqrt(|c|/a)) simply because it makes the math easy. In reality any two parabolas with quadratic coefficients larger than a and smaller than a will eventually surround the original, if you look far enough ahead (pick a large enough n0).",1,0,0,False,False,False,1645497623.0
sx12tk,hxp9s7u,t3_sx12tk,"The encryption and decryption is performed by the email program or whatever program is sending and displaying the messages. Usually it's invisible to the user.

In the case of WhatsApp, the keys are stored on the phone in a file called the keychain. When you add a new contact, WhatsApp automatically sends your public key to them, and stores their public key in your keychain after it receives it from the WhatsApp app on their phone.

Every message you send is encrypted by the app. Every message you receive is decrypted by the app. But these details are all hidden from the user. 

Almost every program does thousands of these activities which are not initiated by the user, and which the user is never informed about. The part of a program that the user sees is a tiny fraction of the whole system - we want to hide as much complexity as possible from the user, and make the user interface as simple as possible.

So very often in CS say ""person A does this"" to mean ""person A's computer does this, perhaps completely automatically"".",11,0,0,False,False,False,1645364323.0
sx12tk,hxp909x,t3_sx12tk,"Depends.

In most Cases, the password is not the key, a more complex key is derived from that.

The applications exchange symmetric keys via assymetric encryption (Assymetric is slower, but your advantage is that you have to publish your public key once, then you can exchange your symmetric key for one session)

Passwords are not always used, e.g. if you open a tls Connection to a Website.

Encryption can be done by the application or the OS, Depending on how ""built-in"" the application is in your system. 

Then again, if you open a encrypted file, you just decrypt it with a passphrase and only symmetric encryption. Let's say .zip files. Here, your zipping program just decrypts.

If you wann more Information about your User case, find out which protocol is used. Protocols define what exactly happens and who does  what",5,0,0,False,False,False,1645363881.0
sx12tk,hxpb9hm,t3_sx12tk,"For information transmission across the internet, you may want to look into the Application layer in the TCP/IP model, or presentation layer in OSI model",1,0,0,False,False,False,1645365143.0
sx12tk,hxqt9eq,t3_sx12tk,"I think what you are talking about is referred to as the key exchange problem

https://en.wikipedia.org/wiki/Key_exchange

> My intuition tells me that its the OS performing the encryption process

Nope, you can send uncrypted message and the OS won't do anything about it. What defines how we encrypt stuff is the protocol. For example TLS says we encrypt stuff using a complex combination of symetric and asymeric encryption",1,0,0,False,False,False,1645388200.0
sx12tk,hxt887b,t3_sx12tk,"Ah thanks everyone! It's more clear now. It's just all the literature always says stuff like, ""Alice encrypts the message before sending to Tim"". It would be so much clearer if it said ""The program Alice is using encrypts the message"". Or maybe its just me being a dumbass! lol.",1,0,0,False,False,True,1645432745.0
sx12tk,hxte37r,t3_sx12tk,"Something that helped me understand was that, even though we're talking about ""plain text"", messages are always stored as numbers - that's the only way computers can store any message, including this one you're reading now.

Typically, each letter in the message has a different numerical value assigned.

The encryption of the message applies a complex mathematical formula to these numbers, so that tweaking the value of some variables within the formula will create drastically different end results. Decryption is the same, but performing the inverse formula to get the original message.",1,0,0,False,False,False,1645437630.0
sx12tk,hxtxcib,t3_sx12tk,"Sounds to me like you're missing some Discrete Maths course. We did RSA encryption by hand in ours (obviously with small primes), so now the mystery is gone.",1,0,0,False,False,False,1645451281.0
sx12tk,hxp98tj,t1_hxp909x,"Oh and the encryption itself is implemented via an algorithm. For One time padding, just xor your text with a key of the same length. An example for common symmetric encryption is aes, for asymmetric RSA",2,0,0,False,False,False,1645364019.0
sx12tk,hxts8or,t1_hxt887b,"The thing is that you are studying computer science. Encryption is just mathematics. It's completely irrelevant who or what does the encryption, you are now just stuying about encryption. The fact that computer does all the things is kind of implied.

Another thing: even if ""Alice"" was a person, and they type a password, it is _still_ the computer that does the encryption. 

Never hurts to ask though!",1,0,0,False,False,False,1645448340.0
sx12tk,hxyzvvc,t1_hxt887b,"That's because it literally doesn't matter for most of these discussions. Whether Alice computes the encrypted message in her head and shouts it over to Bob, or she is on the computer and it's one the million programs on there, and the transmission happens over the internet.

Imagine Alice and her computer in a Black Box. Out comes encrypted text, and you can't look inside to check what part of the Black Box does the computation. You do however know the algorithm that is being computed. Alice doesn't need to be a literal person, Alice ist just whoever is the Sender.",1,0,0,False,False,False,1645542959.0
sx170q,hxq67o7,t3_sx170q,From what I remember no. Dead state would not be a dead state if it is final state. You can end in what would otherwise be considered a dead state but by marking it final it is not dead. At least that is what I remember.,3,0,0,False,False,False,1645378680.0
sw57d5,hxk6rjh,t3_sw57d5,"A language is recursive if there's a program that:

* when given a string in that language, returns true
* when given a string not in that language, returns false

A language is recursively enumerable if there's a program that:

* when given a string in that language, returns true
* when given a string not in that language, returns false **or goes into an infinite loop**",28,0,0,False,False,False,1645263264.0
sw57d5,hxkf0aq,t3_sw57d5,"You might be more familiar with the terms ""decidable"" and ""recognisable"".",12,0,0,False,False,False,1645270164.0
sw57d5,hxkys72,t3_sw57d5,"A language *L* is recursive (or decidable) if there exists a program (or Turing Machine) that, given a word *w*, will eventually tell you whether or not *w* is in *L*.

A language *L* is recursively enumerable (or recognisable) if there exists a program (or Turing Machine) that, given a word *w* in *L*, will eventually confirm that *w* is in *L*. But if *w* is not in *L*, there is no guarantee that the program will ever get back to you, so you can't just sit and wait for an answer because you might wait forever.

Naturally, every recursive language is also recursively enumerable, but not the other way around. The language of programs that always halt on no input, for instance, is recursively enumerable but not recursive, since the Halting Problem is undecidable (i.e. not recursive); you can tell which programs eventually halt by just simulating the program, but for the ones that don't halt you'll need a different way of determining that (and Alan Turing proved there's no surefire way to do that).",10,0,0,False,False,False,1645282170.0
sw57d5,hxl9235,t1_hxk6rjh,That was a very good explanation,3,0,0,False,False,True,1645286696.0
sw57d5,hxl956h,t1_hxkf0aq,Thank you for the answer,1,0,0,False,False,True,1645286731.0
sw57d5,hxl93zs,t1_hxkys72,Thank you for the answer,1,0,0,False,False,True,1645286718.0
swfzw6,hxlvova,t3_swfzw6,"How to Think Like a Computer Scientist is available for free (online) for many different programming languages, e.g. https://greenteapress.com/thinkpython/thinkpython.html",2,0,0,False,False,False,1645295934.0
swfzw6,hxmtew7,t3_swfzw6,SICP,2,0,0,False,False,False,1645310379.0
swfzw6,hxlrusj,t3_swfzw6,"For a present for a newbie one of those could be not bad:

* **Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People** by _Bhargava_
* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Code: The Hidden Language of Computer Hardware and Software** by _Petzold_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_",2,0,0,False,False,False,1645294357.0
swfzw6,hxofiqf,t3_swfzw6,[deleted],1,0,0,False,False,False,1645341441.0
swfzw6,hxy2j5y,t3_swfzw6,"Head First Java put me far ahead of the class, felt like cheating. And that's in a class where a lot of people have 4 years of on the job experience from software development apprenticeships. Obviously those people still outclass me but at least I don't have to worry about my Java.",1,0,0,False,False,False,1645522547.0
swfzw6,hxpgy5y,t1_hxlrusj,+1 for Concrete Mathematics,2,0,0,False,False,False,1645368069.0
swfzw6,hxsfh6n,t1_hxofiqf,Whoa. How is that site not sued? Lol.  Thanks!,2,0,0,False,False,False,1645414477.0
swfzw6,hy407l6,t1_hxsfh6n,What was the site?,1,0,0,False,False,False,1645630903.0
swe6id,hxmu0p4,t3_swe6id,"Interesting question that I’ve pondered before. Not sure I can give a satisfactory answer but I’ll give it a shot:

Intuitively it seems impossible due to the information content of matter; the universe contains N subatomic particles and if you wanted to simulate all of them in a computer, you would need to store/represent at least one bit per particle, requiring >N matter.

But is that the whole story? An eight bit integer can represent more than eight numbers. A Playstation 1 could run a Playstation 3 virtual machine (any Turing complete machine could), albeit sloooooowly. Procedurally generated video game worlds can be many times larger than the game data on disk, by combining a set of rules with a seed or initial state, which is then dynamically extended to simulate a larger observable world. Patterns repeat due to information loss, but vary enough to seem chaotic to a local observer.

Similar compression properties can be found in the physical world, e.g. holograms and fractals.

It is certainly plausible that the rules that govern our physical universe can be expressed and executed in a computer much smaller than the universe; but even with those rules you still need to represent an initial state — the matter at the Big Bang if you will — which presumably still involves N particles or something with equivalent information.

Still, there could be a way to ‘cheat’ - a symmetry or repetition or some other way to compress the information so you could run the simulation with a smaller initial state and still get a deterministic outcome for the full universe. It’s a stretch perhaps, but not necessarily impossible.",5,0,0,False,False,False,1645310653.0
swe6id,hxmytwc,t3_swe6id,"To simulate the universe within the current universe using our current computers you have the problem that the wave function interference rises fast (I think exponentially but unsure) in the number of simulated particles.

However this assumes that a universe simulating ours behaves like ours. Whether it does, we can not know.",3,0,0,False,False,False,1645312844.0
swe6id,hxmfyyv,t3_swe6id,[deleted],-2,0,0,False,False,False,1645304433.0
swe6id,hxoarax,t1_hxmfyyv,"That is more or less what I’m asking -
if it breaks the laws of physics can you explain how? I am not sure it does because I believe you can build a Turing Complete machine within a Turing Complete machine for instance.

See also one of the top posts in this sub which is Conway’s game of life simulated in Conway’s game of life. You will notice the simulation is slower than the ‘base layer’ however.",2,0,0,False,False,True,1645338002.0
svwgkv,hxj56cs,t3_svwgkv,"Smart move, annad tech can't survive this attention economy",5,0,0,False,False,False,1645238747.0
svwgkv,hxkeuf8,t3_svwgkv,"He's taking up a new job in the industry? 

That makes me think he's involved with the new Linus media group thing, will wonder what he makes of that, I love his writing style",2,0,0,False,False,False,1645270027.0
svwgkv,hxj63cc,t1_hxj56cs,"I have been watching Anandtech's output wane for a couple of years. I think they may have been acquired and also had a writer drought. It's been sad because I was a fan since day one but I'm  a little thick so can you help me understand the Attention economy thing? Do you mean fleeting attention spans nowadays? They definitely are an extreme ""deep dive"" site.",8,0,0,False,False,True,1645239199.0
svwgkv,hxztb1s,t1_hxkeuf8,Yea Linus brought it up and they were trying to laugh it up going to Burger King I think he’s gonna be running their new lab.,1,0,0,False,False,False,1645554432.0
svsx31,hxi0v4d,t3_svsx31,"Since you have a set start and end position, you have a relatively easy heuristic to use for a greedy algorithm. I’m still kinda confused as to the specifics of the problem, though, care to elaborate further?",4,0,0,False,False,False,1645220631.0
svsx31,hxi5vv6,t3_svsx31,"What does ""maximizing"" mean in the context of ""I want to stay in a particular city's hotel""?  That seems like a boolean value (either you're in the hotel or you're not), so maximizing seems... odd.",2,0,0,False,False,False,1645222650.0
svsx31,hxjhjz0,t3_svsx31,"I'd typed a lot and the page crashed. Damn reddit.

What does ""real-time"" mean? From my understanding, the availability of the hotel in a city may vary from time to time, but they are all fixed at the beginning. That is, you know all the future conditions. I'm not sure if this is accurate but I'll try a solution based on it.

Define $f(u,t)$ as the maximum of the $nights_in_hotel$ when you are in city $u$ and $t = total_nights$.

In city $u$, you can travel and update the status:

$f(v,t+1) = max(f(v,t+1),f(u,t) + marked[v][t+1])$

$v$ stands for a neibour city and $marked[v][t+1]$ stands for the availability of the hotel in city $v$ at time $t+1$.

Kind of dynamic programming. All the status in $t+1$ only relies on status in $t$, so:

```c++
for(int t = 1;t <= T;++T)
    for(int u = 1;u <= n;++u)
        for(Edge e = head[u]; e; e=E[e.next])
            f[e.v][t+1] = max(f[e.v][t+1],f[u][t] + marked[e.v][t+1]);
```

The time complexity of this solution is $O(MT)$ where $M$ is the total number of edges and $T$ is the maximum nights spent.

Now you know that when you spend $t$ nights and in city $u$, you can have stayed in hotels for $f(u,t)$ nights. Calculate the maximum of $f(u,t) / t$.

However, if there are modifications after which you want to recalculate the result quickly, the algorithm is not that satisfying. An optimization is to push all the change status into a queue and update via BFS. This may work well in real life though under the worst conditions the time complexity is still $O(MT)$.

PS: the mechanism used is called ""Multistage Graph"" I guess.",2,0,0,False,False,False,1645245234.0
svsx31,hxi3snm,t1_hxi0v4d,"Problem may sound strange because I changed the domain (original is about the reservation of computing resources in edge-computing solution), but I think analogy with hotel reservations fits well.

Reservations in original problem are meant to be done in blockchain. Each reservation is represented by ownership of non-fungible token. It would be super easy to buy all the tokens for best route in a single atomic operation but to me it seems impossible to do. On the other hand, buying tokens one-by-one and selling unnecessary ones after backtrack seems expensive (but considering the complexity of first option, in terms of ethereum ""gas"" required it may be cheaper option at the end).",1,0,0,False,False,True,1645221798.0
svsx31,hxi7t35,t1_hxi5vv6,"Sorry, I meant to maximize following for the entire route:   nights\_in\_hotel / nights\_count",1,0,0,False,False,True,1645223433.0
svsx31,hxlvly1,t1_hxjhjz0,"By real-time, I meant concurrent reservations that may happen, originating from another people planning own routes with reservations in the same way. Because many users will plan their routes possibly at the same moment, I am not sure if it is suitable to do whole planning of route and performing reservations for a single user as a single atomic procedure that will temporally  ""lock"" resources, preventing others to do planing. I will probably ask for advice for this part on some blockchain subreddit too.

I have to wrap my head around your proposed solution. Multistage graphs seem to be a good fit for my problem. Thank you very much for your time, effort and advice. Especially after the page crash, thanks a lot really. I always type longer comments in notepad due to similar experience.",1,0,0,False,False,True,1645295901.0
svsx31,hxi4stc,t1_hxi3snm,"I’m not a blockchain guy, but a few more questions: do you have a “map” of the area or only know which cities are near you?",1,0,0,False,False,False,1645222206.0
svsx31,hxo0cdr,t1_hxlvly1,"Yep it’s a dependency graph. You could also create an analogy with chess board (eg move knight across board)

What’s your actual problem statement? Effectively you have a graph traversal that is optimizing a cost function.

If you define the semantics of that cost function you’ll get better answers. For instance, do you need a local versus global minimum/maximum? There’s also lot of strategies for “approximating a valid answer”",1,0,0,False,False,False,1645331535.0
svsx31,hxi56rv,t1_hxi4stc,I have a full map.,1,0,0,False,False,True,1645222365.0
svsx31,hxi60rt,t1_hxi56rv,"Okay, so assign a heuristic to each node equal to how “far” it is from the endpoint in dollars and use your preferred search method to find the cheapest path",1,0,0,False,False,False,1645222704.0
svsx31,hxic7oz,t1_hxi60rt,"It sounds reasonable because by choosing right search method I could at least reduce number of backtracks (thus NFT resale).

For example using the A\* I can ""penalize"" routes with high number of ""nights without hotel reservation"" and reduce NFT resale by using right heuristics (by expanding less nodes).  Am I right?",1,0,0,False,False,True,1645225266.0
svsx31,hxiokkr,t1_hxic7oz,"Sounds about right to me! You could also probably use minimax with price info, but A* and it’s variants are a classic workhorse",2,0,0,False,False,False,1645230732.0
svsx31,hxls8ka,t1_hxiokkr,Thanks a lot for your time and advice. I really appreciate it.,2,0,0,False,False,True,1645294515.0
svsx31,hxlxcdl,t1_hxls8ka,"No problem, man. Good luck!",2,0,0,False,False,False,1645296617.0
svkhpw,hxgw8gu,t3_svkhpw,"I would recommend watching the 3blue1brown wordle video on this but to summarize:

Entropy as a measure of information essentially cuts the space of possibilities down. If I know something that cuts the space of possible solutions in half, that is one bit of entropy. If it is in quarters, that is 2 bits, and so on. 

Consider a random string of 1000 binary digits, where there is a 0.5 chance of each digit being a 0 or 1. There are 2^1000 possible strings that you could make. Now if you know the first digit is a 1, that is 1 bit of information and you have cut the space in half. Now you only need to consider possibilities with a 1 in the first place. But on the other hand, if you knew that you would get some specific string all the time, since you know what you are going to get. That means that knowing the first digit is a 1 would carry 0 bits of entropy.",6,0,0,False,False,False,1645204730.0
svkhpw,hxhkjjg,t3_svkhpw,"There's a very cool connection between Shannon entropy and ""number of bits required to store the data"" or rather to send the data, but we have to be careful about what we mean by that.

    int v=125

The variable v (suppose that int means it can be any 32-bit integer) can potentially hold 32 bits of information. But the way it is defined now it doesn't hold any bits of information. We know the value, it is fixed.

If you did print(v) and saw an answer and you want to tell me the answer, you don't need to send me any bits. I already know the answer, it is 125.

However if the definition was

    int v=random(200)

meaning a random number from 1 to 200, then if you do print(v) and see the answer you will need to send me bits to tell me the answer. It's fair to say you need 8 bits (because 2^7 is 128 and 7 bits won't always suffice). Let's do another case.

    int choice=random(1000)
    if choice>999:
        v=random(100000)
    else:
        v=random(2)

Now have many bits do you need to send to tell me the answer of print(v)? It seems like 17 bits is necessary because it could be any number between 1 and 100000. But 999/1000 times it is just 1 or 2, so 1 bit should suffice. The answer should be more like (999/1000)*1 + (1/1000)*17. Entropy does something like this, but how does it really relate to ""number of bits required to send the data""?

Let N be a large number, like a trillion. Run the program N times. You get N answers, you want to send me all the answers. How many bits do you need to send those answers, if you're allowed to fail with a really small probability? That is, for each possible ""N answers"" that you get, you associate a message to send me, with each message being at most m bits long. Since each answer can be a number between 100000, there are 100000^N possible ""N answers"", so you need around log(10000)N bits. However, you are allowed to say that there are some ""N answers"" that you will just give up on, but these will happen with a really small probability. For the rest of the ""N answers"" you will send a message of length at most m. It turns out you don't need m=log(100000)N bits to do this. You'll need something more like m=((999/1000)*1 + (1/1000)*log(100000))N bits. It can be proven that the optimal value of m/N is exactly the entropy of the random variable.",1,0,0,False,False,False,1645214122.0
svkhpw,hxi4u7r,t3_svkhpw,If you're comfortable with maths his paper [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) is fairly straightforward.,1,0,0,False,False,False,1645222222.0
svkhpw,hxj80nt,t1_hxgw8gu,Thanks mate,1,0,0,False,False,True,1645240168.0
svkhpw,hxj804d,t1_hxhkjjg,Thanks a lot. This was helpful:),1,0,0,False,False,True,1645240160.0
suu8cb,hxc7oqq,t3_suu8cb,"There's something called unix time, it's the number of milliseconds since the ""epoch"" which everyone agreed would be midnight jan 1 1970 UTC. so all time measured is just the number of milis since then. Different operating systems and different programming languages handle it in different ways but that's the git.",14,0,0,False,False,False,1645122036.0
suu8cb,hxc60ds,t3_suu8cb,Are you looking for [this](https://en.wikipedia.org/wiki/Unix_time)?,8,0,0,False,False,False,1645121416.0
suu8cb,hxcu6so,t3_suu8cb,"In terms of terminology, you might want to look for ""real time clock"" or RTC",3,0,0,False,False,False,1645130502.0
suu8cb,hxex82j,t3_suu8cb,"So for microcontrollers, most of them have what are called timer peripherals.  These are pieces of hardware that have inputs and outputs mapped to memory locations on the microcontroller.  These hardware timers are connected to a physical oscillator of some type that oscillates with a specific frequency.  You can configure the hardware timer to increment one of its memory locations each time the oscillator oscillates or after some number of oscillations.  As far as representation, binary is just a base-2 representation of a number so yes there is a binary code (or more specifically a binary number) that represents some amount of time passing.  Whether thats seconds, minutes, microseconds, etc. is dependent on how you configure the timer.",3,0,0,False,False,False,1645164106.0
suu8cb,hxckvw0,t1_hxc7oqq,`gist push`,16,0,0,False,False,False,1645126945.0
sud5zn,hx9osc3,t3_sud5zn,"What you are probably used to is combinational logic. You have inputs and outputs, the outputs depend solely on the inputs, and in any experience you have the input is provided by a human.

Computers are a bit more ""powerful"" than that, what they use is called sequential logic, where the output changes over time, or its past or state.

Consider a basic counting circuit, there is a number that starts at 0 and every time you press a button it increases by 1. Every time you press the button the input is set to 1,  but the output is different every time because it also depends on the number, i.e state or memory. This is what makes sequential logic different to combinational logic. Outputs depend on state and input not just input

How would we actually make this circuit though? I'm going to assume you can imagine how to make a circuit that can add 1 to a number. A human inputs a number, presses a button, then the output is just that number + 1. 
Ok, now let's connect the output back into the input.
By looping around, we are in effect storing a number, the signal loops around staying the same. This is kinda how memory works.

 Now the input is the previous output and pressing the button increases it by 1 and we've removed the need for the human to input a number. But they still press that button. 

Well you might know what a clock is, it's just a thing that oscillates between on and off. If we connect a clock to the button we now have a circuit that increases by 1 every whenever all on its own without human intervention. We could then add a reset button or what ever.

This is the basic idea, it's simplified and I've ignored specific details but it should give you the intuition.



CPUs are a bunch of combinational circuits put together, e.g a circuit for adding, one for and, one for writing to memory etc, and memory, a bunch of cells that store a value.

There is also another circuit that lets you choose which circuit you want to use, you just arbitrarily number all these circuits and let that number be the input to the circuit along with the others.

 So 1 2 3 could be add 2 and 3 ;
and 2 2 3 could be subtract 2 from 3 ;
and 3 2 3 could be store 2 in memory position 3 etc.

A human could manually provide a bunch of inputs to this circuit (called and ALU btw), but instead we could store these in memory, the first cell could be  1 2 3 the second cell 2 2 3 and whatever.

If we connect a particular memory cell to the ALU it will perform that instruction, so now we can use a counter and connect memory cells in order to the ALU. So we can now get the CPU to do a bunch of instructions in order and this is precisely what a program is. (We also have circuits for editing the counter and stuff and that's how you get loops for example ) . We can manually write programs, then manually write programs that automatically write programs,run programs etc and this idea abstracted in soo many levels leads to computers we have.

Please comment anything you want me to eleborate on as Im only human and I've ignored details that I don't think are important, but could be really important for you to understand the ideas.

Really good places to learn more would be the book Code by Charles petzold and the websites nandgame or nand2tetris.

(I put powerful in quotes because it doesn't let you do more things, in theory you could make a massive circuit for any program you like, it's only more powerful in the sense that we can automate common abstractions making our lives easier)",17,0,0,False,False,False,1645072034.0
sud5zn,hx9uylw,t3_sud5zn,"The beating heart of a CPU is the clock, it just cycles on and off (or technically high voltage (+7) and low voltage (+1))

It just cycles back constantly as long as power is connected. It is ALWAYS running. The computers basically just flip switches, in essence, based on the timings of those, you can add delays on commands, or even loops.

Picture a metronome ticking, your program's instructions/code is a sheet of music, your program itself (the thread of execution) is the player, and your OS is the conductor. The OS tells you when to begin, but the metronome keeps you on time, and the sheet music tells you what notes should be played after however long. In this case when you play those notes, the switches are flipped to send power to different areas.

A signal can either be a hardware or software interrupt. Hardware interrupts are usually triggered in the CPU (a circuit is triggered by a piece of hardware and flicks a switch in the CPU to tell it to swap its paths for flow of power).

A hardware signal is one that doesn't need the conductor aspect, it's just a ""jump to"" section of the music if you will.

Software is dealt with by the OS, the system is constantly checking its own flags, it does essentially the same as the hardware, but it's all virtual. When the computer checks it's status periodically (we'll call that part of the ""OS loop""- it'll run a few programs in chunks then run a status check), it will notice if something has triggered an interrupt then switch off the active program to handle it. We call that swap going from user mode to system/kernel mode.

There's usually a lookup table on more complex, modern systems for interrupts to make it so they're handling routines can be changed or dependent on the software being used when it happened.",3,0,0,False,False,False,1645075866.0
sud5zn,hx9b9xi,t3_sud5zn,Transistors are like a tiny light switch that flip when electricity is applied (rather a physical switch). Bunch of them together create gates and advanced circuits. Enough of those makes a cpu (basically),1,0,0,False,False,False,1645065191.0
sud5zn,hx9fngd,t3_sud5zn,"Here’s a little game that may help you understand how the computer does what it does:

https://nandgame.com",1,0,0,False,False,False,1645067164.0
sud5zn,hxb09qn,t3_sud5zn,"Basic cells are transistors. When you connect two (npn, pnp) you can create a simple NOT logic gate.


We abstract these details by treating them as digital logic / boolean circuits, but in reality they are analog and very non-linear elements.

What happens in our NOT example  is that if at the input a signal is provided around the upper or lower voltage bounds of the gate, the transistors are operated in a saturated mode. Meaning they will maximise their drive strength and output a signal to the opposite bounds. Since there is noise due to temp and other fluctuations it is not a perfect signal. Also the exact voltage various due to production process variations.

But if in the bounds due to the non-linearity the output will be usually more closer to the desired value (saturation). They basically can be seen as filters/buffers resp. signal reconstructors. 

But if you inout a voltage in between both transistors are in a somewhat half open state. Since this is usually short as input signals are required to maintain some minimum time to pass that region when states are switched, nothing happens. Otherwise it could also damage the circuit due to the current flowing. This switching current is also part of the power loss equations. But also due to resistance and capacitance you need to discharge the previous state. This contributes more. Also static currents, even if the circuit does nothing heat is disdipated. But this is already quite good. But the smaller the worse it gets and thus the transistors, material,, layout etc need to be optimizer to avoid unnecessary leakage.


So most circuits still are synchronous, meaning you have a periodically toggling signal, the clock, that tells flip-flops/registers/sram/dram to capture their signals at the input, store them and output it and not change it until  they are required to capture again. So these cells usually have enable or reset inputs as well. But they at least keep it for one cycle.

Now ecery circuit in between or connected to such outputs will react to it, which takes time to stabilize if the clock period is chosen right, this happens before the next cycle. The flip flops have a setup and hold time. Meaning the input must have stabilized some time before the clock arrives and must be held constant for some time after to be correctly captured. Clock distribution is thus also an issue. Nowadays there are multiple clocks in a system to deal with some of these issues (GALS). 


Since the World is actually giving the inputs, at least other systems not synchronized to our clock, we will have to reduce the chance to of upsetting our circuit resp. introducing meta stability into it with synchronizer circuits, clock reconstruction, plls, handshaking etc. After all a closed system not interacting with the world in any way is useless.

But again for designing logic we usually avoid this but when creating the circuit we must check whether our assumptions hold. This is often analyzed via models of the circuit. But for designing we remove any timing issues, delays, analong stuff, heat dissipation,... 

Also reset and startup must be handled. As all internal states are quite undefined before started/connected. So there is usually a long (sub second or ms pulse) generated when a system is started resp. its power supply connection got detected.

Once it is starting the system is in a defined state generating predefined procedure in the circuit (and mem) to start reading and executing code from the BIOS for example. This is not arbitrary and defined in the interfaces between motherboard manufacturer and CPU manufacturer.",1,0,0,False,False,False,1645104776.0
sud5zn,hxbl4x1,t3_sud5zn,"In this series of videos, Ben Eater builds a computer on a series of breadboards, but he also explains how these things work in an extremely accessible way.

I have a CS degree and he actually clarified some things that I'd basically just memorized for tests.  It's an entertaining and informative collection of videos.

https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU",1,0,0,False,False,False,1645113622.0
sud5zn,hx9vn8d,t1_hx9osc3,"tysm, you explanation recalls much detail that I've forgotten in my OS courses.

No wonder I only got a B.",7,0,0,False,False,True,1645076264.0
sud5zn,hx9vhcs,t1_hx9uylw,"Also for reference, pictures this: https://youtu.be/zELAfmp3fXY

The computer clock isn't smart. It doesn't know what it's doing. All it does is oscillate back and forth. The clock is just the finger pushing down the first digit and it cascades. Then where you see 1s and 0s, different paths connect to that from the power source.

Your computer is dumb, it's an example of emergent intelligence. Each part doesn't need to know or function intelligently. Each part just adds complexity to the others.",2,0,0,False,False,False,1645076169.0
sud5zn,hx9wex6,t1_hx9uylw,"tysm
great analogy",2,0,0,False,False,True,1645076721.0
sud5zn,hx9wsnp,t1_hx9wex6,"Haha thanks and no problem! I think if I remember right, I stole it/paraphrased from an OS textbook, the one with a dinosaur. Just the conductor analogy though!",1,0,0,False,False,False,1645076951.0
suff79,hxagmaj,t3_suff79,"Many many things.

You could work on optimising data distribution between nodes in order to optimise the communication cost for distributed DL framework. You could use DL to evaluate the current network architecture or try to find new architectures using DL in order to optimise commmunications for a given application. 

Basically, you can use HPC to optimise distributed DL, or use DL to optimise distributed HPC, either from a point of view of data (communication cost, time to result, etc), energy (energy frugality, Green500, etc), cost (e.g. resource allocation from an hardware perspective).

Anyway, there are tons of things that can be addressed when mixing HPC and DL (I kept the distributed parallel system a bit aside, because it might be a bit orthogonal to the other two problems).",3,0,0,False,False,False,1645091823.0
suff79,hx9otn3,t3_suff79,"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C29&q=high+performance+computing+distributed+parallel+systems+and+deep+learning&btnG=

Most cited paper here at the moment is on computational biology and bioinformatics.",1,0,0,False,False,False,1645072056.0
suff79,hxcf36d,t1_hxagmaj,Thank you. Very much informative.,1,0,0,False,False,True,1645124760.0
suaahj,hx99g47,t3_suaahj,"If you like infotainment style there was a crashcourse series that I thought was pretty cool.

https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo

I wouldn't say that it's necessarily an in-depth ***history*** but it's broad enough to outline it. This one was about memory/storage and was pretty fascinating:
https://youtu.be/TQCr9RV7twk",3,0,0,False,False,False,1645064376.0
suaahj,hxdvgg2,t3_suaahj,If you enjoy podcasts then I would recommend Advent of Computing and The History of Computing.,2,0,0,False,False,False,1645145515.0
suaahj,hx95463,t3_suaahj,"History of technology books are some of my favorites.  I haven't read too many on the history of AI, but here are some CS history books I've enjoyed:

* *Alan Turing:  The Enigma* by Andrew Hodges
* *Turing's Cathedral: The Origins of the Digital Universe* by George Dyson 
* *Grace Hopper and the Invention of the Information Age* by Kurt Beyer
* *The Information:  A Theory, A History, A Flood* by James Gleick
* *Where Wizards Stay up Late:  The Origins of the Internet* by Katie Hafner

History of computer science has been a great lens for me to understand CS topics in general:  hardware, early programming concepts, the move to higher level languages, the origins of computer networks.

Enjoy!",1,0,0,False,False,False,1645062432.0
suaahj,hx9x8e4,t3_suaahj,"&#x200B;

Here's two I love, they're also well narrated on Audible.

*The Cuckoo's Egg.*

I'm having a little trouble writing a description for it. Cliff Stoll (Author), astronomer, programmer, and kind of a neurotic mess would not let a 75 cent accounting error go. Unable to debug the issue, the situation turned more and more bizarre. Days turned into weeks, weeks months, months years, relationships strained, the CIA and NSA got involved. Through a combined effort of governments, amateur programmers and international telecom agencies, they unraveled a Unix exploit that had given the KGB access to laboratories across all branches of the U.S. military, laboratories and dozens of defense contractors.

What I love about this book, it paints a really clear picture of what it was like to work in IT and develop business software in the 80s. Before Quickbooks, Excel, before clean GUIs were readily accessible and everything had to be carefully routed over phone lines. And even crazier, how little anything has actually changed, not necessarily for better or for worse, just observationally.

[https://www.amazon.com/Cuckoos-Egg-Tracking-Computer-Espionage/dp/1416507787](https://www.amazon.com/Cuckoos-Egg-Tracking-Computer-Espionage/dp/1416507787)

&#x200B;

*Algorithms to Live By.*

Really good intro into optimization, but also covers the history of the various solutions it describes. A really interesting bridge between the physical and digital worlds, and how most seemingly complex problems are incredibly generic.

[*https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1627790365*](https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1627790365)",1,0,0,False,False,False,1645077223.0
suaahj,hxaq6pf,t1_hx95463,"Some more to add to your list:

Masters of Doom by David Kushner

The Soul of a New Machine by Tracy Kidder

Hackers: Heroes of the Computer Revolution by Steven Levy

The Chip: How Two Americans Invented the Microchip and Launched a Revolution by,T.R. Reid

The Search by John Battelle

Race Against  The Machine & The Second Machine Age by Erik Brynjolfsson, Andrew McAfee",1,0,0,False,False,False,1645099109.0
su8vgf,hx8w8ys,t3_su8vgf,Learning.,7,0,0,False,False,False,1645058429.0
su8vgf,hx9hg0h,t3_su8vgf,Data structures and algo class,1,0,0,False,False,False,1645068047.0
su8vgf,hxahzyu,t3_su8vgf,Interrupt handling and hardware level computation,1,0,0,False,False,False,1645092953.0
su8vgf,hxd85kk,t3_su8vgf,Graph theory☝🏻,1,0,0,False,False,False,1645135779.0
su8vgf,hxh8z0h,t3_su8vgf,"The endless possibilities, I mean take other fields as an example, you have a very thin margin of being an inventor. In computer science, your imagination is the limit, imagine things and do it, maybe not today maybe not tomorrow, but in enough time you can do anything, here we have a chance to be a part of futuristic future.",1,0,0,False,False,False,1645209555.0
su8vgf,hxo0z70,t3_su8vgf,Solving tomorrow’s puzzle requires I don’t drink myself to death today.,1,0,0,False,False,False,1645331890.0
su8vgf,hxra6p9,t3_su8vgf,"Having to not see Computer Science again. Nightmares.

Kidding. But for me it was the theoretical portions (basically math): algorithms and number theory.",1,0,0,False,False,False,1645395374.0
su8vgf,hz5wgxl,t3_su8vgf,Understand how computer really work at hardware level. I found in youtube very interesting videos and I started from them... fascinating.,1,0,0,False,False,False,1646299635.0
st0hsq,hx0w6o0,t3_st0hsq,5-6 hours doesn't count as stuck. Come back when it's been 2 weeks.,311,0,0,False,False,False,1644925480.0
st0hsq,hx0thke,t3_st0hsq,I doubt anyone hasent.,65,0,0,False,False,False,1644923482.0
st0hsq,hx0yfr2,t3_st0hsq,5 or 6 hours? I've spent months on a technical problem.,64,0,0,False,False,False,1644927026.0
st0hsq,hx0xbpr,t3_st0hsq,Those are rookie numbers. Try weeks.,51,0,0,False,False,False,1644926275.0
st0hsq,hx0tqgq,t3_st0hsq,"I guess this happens at least once per month ... and regularly it keeps me busy for a couple of days. 

That's totally normal.",27,0,0,False,False,False,1644923678.0
st0hsq,hx0t7cy,t3_st0hsq,"Have been cases when I’ve been stuck on the same problem for multiple days at a time. 

Sometimes the only way to solve it is to go on stack overflow.",37,0,0,False,False,False,1644923256.0
st0hsq,hx155lt,t3_st0hsq,"5-6 hours those are rookie numbers, come back when it's not a bug it's a feature happens",9,0,0,False,False,False,1644930988.0
st0hsq,hx1jl4e,t3_st0hsq,Heh. Nope. Never. \*closes 98 open web browser tabs\*,9,0,0,False,False,False,1644937641.0
st0hsq,hx1chkd,t3_st0hsq,James Franco looking at you in a noose: First time?,8,0,0,False,False,False,1644934563.0
st0hsq,hx18vzq,t3_st0hsq,I spent about eight hours a day for two straight weeks on an *open source* problem just last month: https://github.com/dankamongmen/notcurses/issues/2573,7,0,0,False,False,False,1644932875.0
st0hsq,hx1sqyz,t3_st0hsq,"For sure. My favorite is spending 5-6 hours on a problem one day, and close it out frustrated. Then the next morning being able to figure it out within minutes. Sometimes it's good to step away!!",6,0,0,False,False,False,1644941361.0
st0hsq,hx1r6yg,t3_st0hsq,"5-6 hours is rookie numbers.
You can compain after you have been stuck on it for atlest a week",4,0,0,False,False,False,1644940734.0
st0hsq,hx0y931,t3_st0hsq,Even weeks sadly 😔,3,0,0,False,False,False,1644926899.0
st0hsq,hx1vd0g,t3_st0hsq,"In graduate school working on a problem with a randomized algorithm, I once put print statements anywhere a random choice was being made to output what the random number was and which branch of the code it resulted in. I then ran the code for a few seconds, printed almost 100 pages of the resulting output, carefully taped them to the floor of the lab, and crawled around in my socks with a printout of the code doing my own mental execution of the program until I found the problem, which ended up being a ""<"" where I should have had a ""<="", and the little bit of bias resulting was enough to make a noticeable difference in how well it worked.",3,0,0,False,False,False,1644942411.0
st0hsq,hx3k349,t3_st0hsq,Have you ever dabbled in pure math?,3,0,1,False,False,False,1644965491.0
st0hsq,hx472in,t3_st0hsq,"Here is a a trick for you, use background thinking, I am an amateur programmer, so I don't have the ability to sit at a desk for 8 hours thinking about a problem, so when I am contfronted with problem that doesn't have a published solution, I think about it real hard for a bit. then push it to the back of my mind and go do something else, usually a day or two the solution suddenly pops in my head.  

it is like letting a post soak in the sink, you put water and soap in it, give it a good scrub then leave it there, and come back the next day and your brain has loosened all the stuck bits.",3,0,0,False,False,False,1644975537.0
st0hsq,hx0vzxc,t3_st0hsq,"this reminds me of why i abandoned that project once.... after a week of anguish its like ""fuck it i dont care anymore. ill pick this project back up in a year""",5,0,0,False,False,False,1644925345.0
st0hsq,hx1ns00,t3_st0hsq,5 to 6 hours of debugging to find I put a semicolon after my if statement so I was always going to run my bracketed block.,2,0,0,False,False,False,1644939366.0
st0hsq,hx1zjt5,t3_st0hsq,"I spent 20 consecutive hours on a technical problem a couple days ago, starting at 7 pm.",2,0,0,False,False,False,1644944023.0
st0hsq,hx25hhh,t3_st0hsq,I'm only in my second semester of CS but I thought that was part of the fun.,2,0,0,False,False,False,1644946284.0
st0hsq,hx2aou9,t3_st0hsq,Months,2,0,0,False,False,False,1644948249.0
st0hsq,hx2tvza,t3_st0hsq,All the time. [this dude just spent a week](https://fasterthanli.me/articles/a-rust-match-made-in-hell) tracking down a difficult to pinpoint bug in his Rust code.,2,0,0,False,False,False,1644955578.0
st0hsq,hx33euc,t3_st0hsq,"I just take a nap and look at it with fresh eyes. 

It’s like turning your own brain off and turning it back on. It does wonders",2,0,0,False,False,False,1644959170.0
st0hsq,hx3t4oa,t3_st0hsq,try 5-6 years for me.,2,0,0,False,False,False,1644969304.0
st0hsq,hx44n05,t3_st0hsq,5-6 hours? Sometimes I might stay days lol,2,0,0,False,False,False,1644974436.0
st0hsq,hx4ptq5,t3_st0hsq,"There are a few places online that sell ""dice coins"" that are basically d20 dice in coin form. The numbers are usually ordered sequentially around the edge, so it occurred to me that a person could learn to toss/spin the coin to land on high or low numbers. There must be an optimal arrangement that minimizes the asymmetric advantage across every conceivable throwing strategy, right?

After \*2 years\* of dabbling a few afternoons at a time with the computational solution (Python, then C, then at one point leveraging the computational power of my video card), I finally figured it out geometrically for all possible dice coins. The trends I observed in the simulated computational solutions were key to leading me towards the true analytic solution.

Pointless? Yeah.. but it was actually a lot of fun : )",2,0,0,False,False,False,1644984142.0
st0hsq,hx50bax,t3_st0hsq,"lol. If it's only a couple of hours, then you are not stuck.",2,0,0,False,False,False,1644989870.0
st0hsq,hx5adn3,t3_st0hsq,Of course. Welcome to coding.,2,0,0,False,False,False,1644996752.0
st0hsq,hx191m7,t3_st0hsq,Im currently working on a project on which i got stuck and given up on 2 times. Its the third run now,1,0,0,False,False,False,1644932951.0
st0hsq,hx1mb13,t3_st0hsq,i played quakegl on an indigo\^2 max for two weeks looking for a bug.,1,0,0,False,False,False,1644938768.0
st0hsq,hx1o4zm,t3_st0hsq,"I wouldn't call it stuck until 5 or 6 days, or even weeks.  Usually the solutions to those kind of problems come during an epiphany, often times while taking a shower.",1,0,0,False,False,False,1644939512.0
st0hsq,hx1t8f3,t3_st0hsq,"Comes with the territory! Don't worry, a win is a win, keep at it and you'll either find or create your own solution, best wishes!",1,0,0,False,False,False,1644941557.0
st0hsq,hx1tba3,t3_st0hsq,You have obviously never done assembler programming,1,0,0,False,False,False,1644941588.0
st0hsq,hx1uyxq,t3_st0hsq,"I once spent a month trying to build a PC and failing, but then finding out I was using a z370 motherboard on an i7 7700k.",1,0,0,False,False,False,1644942255.0
st0hsq,hx24m83,t3_st0hsq,That’s .... insultingly low,1,0,0,False,False,False,1644945960.0
st0hsq,hx2x2hd,t3_st0hsq,It took me 3 days to solve the two sum question when I first started out. I had emotional damage from it✨✨,1,0,0,False,False,False,1644956812.0
st0hsq,hx38ip6,t3_st0hsq,Yep,1,0,0,False,False,False,1644961054.0
st0hsq,hx39mid,t3_st0hsq,5-6 hours are rookie numbers. I have spent more than a month. Turns it was Java source code issue. lol,1,0,0,False,False,False,1644961470.0
st0hsq,hx3glwl,t3_st0hsq,this is simply a rite of passage,1,0,0,False,False,False,1644964104.0
st0hsq,hx543xj,t3_st0hsq,Aww you must be new here.,1,0,0,False,False,False,1644992280.0
st0hsq,hx56iml,t3_st0hsq,"5-6 hours… must be a genius!

Coke back when it’s days, weeks, months! Haha

But seriously, don’t stress about it! Also take breaks from the screen. Best thing you can do if you are stuck. Give your brain different perspective. I often solve problems on walks or in the shower etc and have to run back to the desk to write some code for the solution that came to mind.",1,0,0,False,False,False,1644993923.0
st0hsq,hx574kh,t3_st0hsq,"I wrote some debug code to help me find a bug. Then I changed something I hoped would fix the bug. Unbeknownst to me, I did fix the bug. But the debug code inadvertently broke it again. 

I spent three weeks trying to fix that bug before I gave up, reverted all my changes to go back to square one and put it off for another day. In the process of reverting it, I found the real bug.",1,0,0,False,False,False,1644994345.0
st0hsq,hx5fani,t3_st0hsq,"Those kinds of problems build big foundations. They are the kind of stuff you are like ""aha! look at all the new options suddenly available to me"" once figured out. 

I'm quite an amateur, but I routinely spend days or longer on problems. Just thinking about it, doodling on paper, researching, experimenting in a REPL, mulling it over in the background during showers or walks, etc. Sometimes things are easy, but sometimes you spend weeks on a real doozie.

The important thing is to not let it get your confidence down. These things are like lego blocks. Once you figure out something that was a real stumbling block for a long time, it really adds to a whole structure of understanding. This is one of those things that sounds cliche until you get in the habit of experiencing it. It is part of the fun of this kind of thing. You're building long term understandings.",1,0,0,False,False,False,1645000562.0
st0hsq,hx5nm8f,t3_st0hsq,Sounds like you are new to computer science.,1,0,0,False,False,False,1645007355.0
st0hsq,hx5ohuw,t3_st0hsq,"Man i spend days on technical problems that is just due the lack of knowledge, let alone the complex problems.",1,0,0,False,False,False,1645008052.0
st0hsq,hx602x7,t3_st0hsq,"No, not really. If you are stuck at some point it should take at least two weeks. Time to time, you find out that the solution is just like indentation error.",1,0,0,False,False,False,1645015724.0
st0hsq,hx6oizr,t3_st0hsq,"Get used to this. You will be stuck on problems for days on end, its a normal part of any CS degree.",1,0,0,False,False,False,1645026624.0
st0hsq,hxtronu,t3_st0hsq,"🤣🤣Lol , that's not stuck... Try 1 month... and  going in circles the entire time....",1,0,0,False,False,False,1645447992.0
st0hsq,hx164jw,t1_hx0w6o0,I once thought 3 years on a problem before I came up with a solution.,76,0,0,False,False,False,1644931497.0
st0hsq,hx47z2n,t1_hx0w6o0,This. Very much this.,3,0,0,False,False,False,1644975942.0
st0hsq,hx0tnxq,t1_hx0t7cy,I’ve been trying for the longest time to get the .h file to work with my class.cpp and main.cpp along with others. Keeps coming up with saying the function doesn’t exist (undefined structure 84x64). But it’s clearly there and all sources have same implementation . What a waste of time…,1,1,0,False,False,True,1644923621.0
st0hsq,hx484nt,t1_hx18vzq,"Hah! the struggle is clearly visible. Could you answer this? I don't get the motivation to write details about the project that no one cares. Ofcourse your repo has 2k stars, But what could you say to me to write meaningful explanations about PRs for future me? I feel like, I'm the one who're writing this, so, I can understand why I made this decisions etc...",2,0,0,False,False,False,1644976013.0
st0hsq,hx2kcvu,t1_hx1sqyz,This is wise advice. I often find stepping away and doing something different for awhile can help the solution (or even just new approaches to solving it) come to mind.,3,0,0,False,False,False,1644951918.0
st0hsq,hx5ep7m,t1_hx4ptq5,"Nothing pointless about that. Whole indispensable modern fields exist because random people a few hundred years ago were obsessively interested in this or that.

Who knows what future people will thank us for taking the time to do, for no particular reason at all. I think that's awesome.

Edit: Was it [this one](https://www.amazon.com/d20-Lich-RPG-Dice-Coin/dp/B0881F3LDN)? What was the optimal arrangement you decided on compared to that?",1,0,0,False,False,False,1645000077.0
st0hsq,hxxtted,t1_hx1tba3,Spent a day debugging my Base 64 encoder for an assignment when I'd already had all the logic worked out on paper. Felt rough.,1,0,0,False,False,False,1645515362.0
st0hsq,hx1emxw,t1_hx164jw,"What was the problem, if you don't mind me asking? I love those tricky sorts of problems that you have to think and think and think over. One that took 3 years sounds beastly.",22,0,0,False,False,False,1644935520.0
st0hsq,hx0x985,t1_hx0tnxq,"Recommendation: treat these problems like opportunities to grow as an engineer.

When you're up against a problem like this, maintain a positive mindset that there is \_something\_ that you're not seeing or understanding. Avoid the mindset that ""this is dumb"", ""something's wrong"", ""waste of time"", etc...

With a positive mindset, the time spent on these problems will expand your mental model of what is happening. You will learn hard lessons and get better.

With a negative mindset you will experience emotions that damage you physically and psychologically and will likely hit a plateau of competency and never surpass it.

If it's really true that you're using unreliable tools, or that the tools are not suitable for you, move on from them, but realize that if you do this too readily you may be standing in your own way to succeed.

As engineers today we have \_\_absurd\_\_ abilities at our fingertips. It's absolutely nuts. It's up to us how much we utilize them.",42,0,0,False,False,False,1644926227.0
st0hsq,hx2ar9d,t1_hx0tnxq,Sounds like a weird issue with having maybe a declaration but no definition. Or they aren't in the right order or something.,2,0,0,False,False,False,1644948275.0
st0hsq,hxz3cao,t1_hx0tnxq,"First step is figure out whether it's a compilation error or a linker error. If you see an object file got produced from the source file, that means compilation was successfull.

Compilation error: check for typos, check tor a missing #include, check the include directories are correct, check for headers with the same name in another include directory

Linker error: check whether all source files got compiled. A missing symbol often means the object that defines the symbol didn't get linked. If you feel unlucky, check the order of your objects in the linker command. The one using the symbols should come before the one providing the symbol. Symbols no one needed so far get optimized away. Rare problem but can happen.",1,0,0,False,False,False,1645544381.0
st0hsq,hx1rqf2,t1_hx1emxw,"Not OP, but I find myself in similar situations with what I do (autonomous robot behaviors).  With problems of this scale though you do make *progress* over these long periods of time; you solve a simpler subset of the problem, you get *closer* to a performance goal, you reduce failures in the field by some percent, you make prototypes, etc.  I'd argue that you'd drive yourself mad if you can't find a way to show some form of progress over time.",30,0,0,False,False,False,1644940953.0
st0hsq,hx1wdqj,t1_hx1emxw,"It had to to with the game of go. The ancient board game. One guy had created an incredibly fast pattern matcher that was used when reviewing games. It really was one of a kind. Nobody was close to anything like it. This was way before DeepMind did their thing with AlphaGo that basically made all previous tools obsolete. 

I couldn't figure out how he had done it. There are an incredible amount of patterns in this game. But his tool had not only a vast database of patterns. It was super fast too. Getting it both fast and complete at the same time was the hard problem for me. But after 3 years of thinking about it on and off it just struck me.",21,0,0,False,False,False,1644942811.0
st0hsq,hx1kph9,t1_hx1emxw,I am just commenting hoping to get notification about answer to this lol.,3,0,0,False,False,False,1644938107.0
st0hsq,hx1atd7,t1_hx0x985,[139-second long motivational video saying the same message](https://youtu.be/IdTMDpizis8),1,0,0,False,False,False,1644933781.0
st0hsq,hx2luo4,t1_hx2ar9d,"I got it to work, finally.",3,0,0,False,False,True,1644952484.0
st0hsq,hx2llvm,t1_hx1wdqj,you’re just gonna leave us hanging?,8,0,0,False,False,False,1644952389.0
st0hsq,hx1f1kv,t1_hx1atd7,"Good, sharp message. 

The only thing that didn't ring true for my experience was when he said, ""Don't get frustrated. Don't get bummed out, etc..."" That sets the message up for failure because you \_\_will\_\_ get frustrated and feel the whole spectrum of negative emotion. The goal isn't to stop feeling that way. The goal is to learn from yourself when you do feel that way. You are a sublimely sensitive instrument that reacts to your situation. The path to mastery is to hone that frustration along with the rest of yourself so that when you feel it you can use it as a measure that you can trust to, as he says, recalibrate.

Overall, though, Jocko is an inspiration. My favorite message of his is ""Don't rely on motivation. Rely on discipline."" I'm a big believer that amazing things are possible if you just show up and make time to apply yourself over and over, hour after hour, forever.",4,0,0,False,False,False,1644935699.0
st0hsq,hx2rjix,t1_hx2luo4,Congrats! Feels good right?,6,0,0,False,False,False,1644954667.0
st0hsq,hx304et,t1_hx2luo4,"On the bright side, you will probably never fall for the same mistake now that you spent so long uncovering it",1,0,0,False,False,False,1644957958.0
st0hsq,hx2orbc,t1_hx2llvm,Hash all the patterns with a suitable algorithm so that all hashes fit in RAM. This was when RAM wasn't so plentiful.,9,0,0,False,False,False,1644953596.0
st0hsq,hx2u6yo,t1_hx2orbc,This is like when you get to see the “afterwards” of a cliffhanger and it’s such a delusion lol,7,0,0,False,False,False,1644955696.0
st08lc,hx0xfjn,t3_st08lc,"The world is written in C. Whether or not you're writing in C, you're almost certainly writing in a language that was written in C using an operating system that was written in C.

Linux and MacOS are written in C. Windows is written in a mixture of C and C++.

The standard Python interpreter is written in C. The Java runtime environment is written in C. Nodejs is written in C and C++. And on and on.

Python is a good first language to learn. But C should be on your list of languages to learn sooner or later.",142,0,0,False,False,False,1644926346.0
st08lc,hx12gdn,t3_st08lc,"I think C is also fun to write in, I think you should try to learn it even if you don’t need to if you enjoy coding. I write a lot of Python, but having to allocate memory yourself, use pointers, learn the memory available, etc… it’s fun! Definitely tedious sometimes without objects and having to compile everything of course.",16,0,0,False,False,False,1644929488.0
st08lc,hx0tsto,t3_st08lc,"Necessary? No. Highly beneficial? Yes.

Due to its historical position, C is basically lingua franca of programming. Almost all other languages have some way of talking with C.  
Notable example is mentioned Python. Its main implementation - CPython - is written, nomen omen in C. Also best/fastest Python libraries are in fact written in C.

And there is more [good aspects to C](https://blog.joren.ga/best-of-c).",16,0,0,False,False,False,1644923728.0
st08lc,hx0s5ht,t3_st08lc,"You can probably have a good career nowadays without knowing C inside out. However, understanding foundational things will just make you that more well rounded. It really does not take that long to get the basics of C and even assembly.",24,0,0,False,False,False,1644922415.0
st08lc,hx0tg1r,t3_st08lc,"Like every other ""how important xxx"" question the answer is it depend on your job/task.

Will c/math/os/hardware help you being a web dev? Likely no. But for some tasks or position they are essential.

Because you dont know what you will be soing in the future yes, c is important.",13,0,0,False,False,False,1644923447.0
st08lc,hx0wssc,t3_st08lc,"It depends on what you are specializing on. Here some thoughts - partially subjective partially IMHO objective:

# System Programming and Kernel Hacking 

To say the least most of the OSs you're using are build primarily in C. If you're working in one of these fields you need to be fluent in C. Otherwise you won't understand legacy code and you will be excluded from the common information basis (one simple example: man pages on *NIX system calls). Also interfacing with standard libraries and system resources is generally C oriented. 

# Reverse Engineering

Of all high level PLs no one is mapping as straight to the machine code as C does. I don't know of any professional in that field who didn't start his / her learning path with a profound C and Assembler knowledge. 

# High Performance and Optimization

De facto you need to understand your hardware very well and again - more or less the arguments I already mentioned - decent know how with both C and Assembler are crucial to really get profound here ... even if you deal with optimization in another language. If you're not profound here your optimization will be based on ""I read on stackoverflow that..."" or ""I made some benchmark test"" (which will be often for the bin).

Other fields where I assume deep C knowledge include:

- PL development

- malware (both for red and blue)

...

That said: I think I wouldn't start implementing anything new and big in C. Generally I tend to use Rust for such projects ... but always with C in mind. 

BTW:

I frequently here or read people propagating Rust vs C / C++. They are often able to phrase some real issues. But if I provocatively say: ""This is a no issue! Show me what you mean."" They simply get stuck as they only know of reading texts of others and citing them. 

Such people generally fail in assesments in which I'm involved.",8,0,0,False,False,False,1644925910.0
st08lc,hx5bjmk,t3_st08lc,"You should learn C, but don't have to use it for anything except stuff you do in class. 

Unless you want to go into HFT, game development, hardware development, or other areas where memory management and CPU optimization is essential, you can probably pass on C++. 

Both C and C++ allow you do advanced memory management and CPU optimizations. However, for most applications other than HFT/video games/hardware development, the out-of-box optimization for higher-level languages is good enough that its cheaper to buy more compute/RAM than spending 2 weeks of a developer time. Note that to a company, a month of junior developer salary is in the ballpark of $10k, you can buy a good chunk of compute with $10k. 

The reason you should learn C is because it teaches you basics of aforementioned topics (memory management, optimization), which you would not learn in other languages easily.",3,0,0,False,False,False,1644997621.0
st08lc,hx16seg,t3_st08lc,"As well as the things other people have mentioned, on many many systems the calling convention / ABI between userspace and the operating system is whatever the C compiler uses, or one of the ABIs supported by the C compiler.  So if your code wants to interact with the operating system in *almost any way whatsoever* then it needs to be using C at some level (even if this is hidden from you ordinarily by layers of abstraction).",2,0,0,False,False,False,1644931836.0
st08lc,hx2v8r7,t3_st08lc,"Necessary? Depends on what domain you are in. For systems,  embedded, OSs, compilers, etc.. yes.

Worthwhile? Always. Starting from a lower level of abstraction will payoff later in your education.",2,0,0,False,False,False,1644956101.0
st08lc,hx1w1lt,t3_st08lc,"If you want to do the really, really interesting things and do novel work, then you need to know C. Hacking hardware? C. IOT? C. Robotics? C. Creating a new device from scratch? C/Assembler. New programming language? C. Drivers for a device? C. New Operating System? C. Video game from scratch? C. You can do some of these using other tools, but C is the common language of all things.",3,0,0,False,False,False,1644942678.0
st08lc,hx1jjp3,t3_st08lc,"C is a legacy infrastructure language. Everything is written in C.

This is a problem.

It was designed fifty-two years ago.

It was designed under hardware limitations that do not exist today.

It was designed for hardware that no longer exists.

It runs on a virtual machine that mimics hardware that no longer exists.

It is nearly impossible for a human being to write correct code.

It has a community that believes a lot of wrong things about how it functions.

It has a specification that in large part leaves behavior of incorrect unspecified.

It is currently designed by committee, but with the express goal of patching in new features (such as Unicode) without breaking backwards compatibility.

It exists in many projects in outdated editions: current newest version is C17, many extant projects use C89.

Do not use C for new projects unless you have an absolute need for it. (Certain embedded platforms only have support for C, for instance.)

Learn C, write some code, then go look at one of those quizzes about the esotherica of the C specification, then go look at doing something even slightly nontrivial like writing a multi-threaded application. Then realize that C is a legacy infrastructure language and don't use it for new projects.",-7,0,0,False,True,False,1644937625.0
st08lc,hx2eo0k,t3_st08lc,"It’s akin to learning Latin. An expert linguist will be at least familiar with it, as an expert computer scientist should be familiar with C.

That said, you don’t need to be an expert in CS to make a good living off coding.",1,0,0,False,False,False,1644949755.0
st08lc,hx2jape,t3_st08lc,"If you want to be a C developer, very, otherwise, not that important. Most Dev jobs are in high level languages because it's mostly web. However C is still used in loads of other places, embedded for one, and a lot of stuff that needs to be low level for performance.",1,0,0,False,False,False,1644951514.0
st08lc,hx49n6b,t3_st08lc,"Others have pointed out: most modern programming languages rely on C, in some way - so it’s important to learn, so you know how it works. 

However, you don’t need to be super proficient. I’ve never had to write production code in C, nor do I anticipate having to.",1,0,0,False,False,False,1644976697.0
st08lc,hx4xqvo,t3_st08lc,It's good to know C for the foundation aspect and understanding but not 100% necessary and likely won't be applicable unless you're getting a job where you program in C.,1,0,0,False,False,False,1644988341.0
st08lc,hx5opzj,t3_st08lc,"There is a silent voice in my head..  


\-> Rust must raise!!!!",1,0,0,False,False,False,1645008225.0
st08lc,hxtxv73,t3_st08lc,I don't know the answer but I can pointer you in the right direction.,1,0,0,False,False,False,1645451557.0
st08lc,hx2fn94,t3_st08lc,"Python is the biggest foolish and strange invention in programming languages. And a simple example regarding learning this strange thing at the beginning. Ok, If you are learning to play on the piano there's one very important thing - fingers training and staging hands. If a bad teacher will fail this stage a student won't ever play excellently.  So, training beginner programmers' brains wrong will be followed to very similar results.

Anything must be started from C. Especially intense learning and understanding pointers.",-3,0,0,False,False,False,1644950125.0
st08lc,hx0vnqv,t3_st08lc,"The builtin operations in C and basic types map pretty directly to the internal operations of CPU, while python has many things as basic operations of the language that require executing complex non-constant time algorithms or handling complex data structures. In C you don’t have stuff like that and you will have a much better idea what kind of operations your CPU will execute and what will be stored in the memory. Knowing how things are implemented in C will make you a better programmer, even if you mostly use higher level languages, because you will have a much better understanding what kind of cost in terms of CPU and memory will be hidden behind the nice high levels ops that you have in python and similar languages.",-1,0,0,False,False,False,1644925103.0
st08lc,hx0yv3b,t1_hx0xfjn,"""The world is written in C.""

You made my day and this sentence will take a nice position in my ""toolbox""!",54,0,0,False,False,False,1644927315.0
st08lc,hx1liwv,t1_hx0xfjn,Python is written in c,11,0,0,False,False,False,1644938447.0
st08lc,hx44p2h,t1_hx0xfjn,"Alternative worlds exist, but you have to search far and wide to most eccentric of places to find them, and nobody lives there save for the odd crazy hermit, C world is just so easily accessible it doesn't make sense to live anywhere else.",1,0,0,False,False,False,1644974462.0
st08lc,hx21oyc,t1_hx0xfjn,I would say that the world is written in PHP. 80% of the Internet is written in PHP.,-31,0,0,False,True,False,1644944846.0
st08lc,hx1i61h,t1_hx12gdn,">Definitely tedious sometimes without objects

Even more tedious (and fun!) if you do decide to implement objects in C.",6,0,0,False,False,False,1644937043.0
st08lc,hx16xes,t1_hx0wssc,"Rust's memory model is useful and interesting in ways that are definitely distinct from C but I think in general there are a lot of people out there who can say ""Rust's memory model is useful and interesting in ways that are definitely distinct from C"" without ever having used them in anger.

I am one of them.",5,0,0,False,False,False,1644931908.0
st08lc,hx2f95n,t1_hx1jjp3,"I don’t know why this is being downvoted, it is entirely correct.",5,0,0,False,False,False,1644949977.0
st08lc,hx1rda6,t1_hx1jjp3,"There's a lot to unpack, but those 4 in particular

> It was designed under hardware limitations that do not exist today.

Don't, really? AFAIK, popular board Arduino Nano is even more limited that PDP-11 from 1970. 

> It was designed for hardware that no longer exists.

That's probably why all popular operating systems are written in it and it's the first language after Assembly to support any new architecture

> It runs on a virtual machine that mimics hardware that no longer exists.

Excuse me, what?

> It is nearly impossible for a human being to write correct code.

It will probably not convince you in the slightest, but read [this comment](https://www.reddit.com/r/C_Programming/comments/llwg2e/what_are_common_uses_of_c_in_the_real_world/gns54z3/)",7,0,0,False,False,False,1644940806.0
st08lc,hx50pwm,t1_hx0yv3b,Hello world!,2,0,0,False,False,False,1644990120.0
st08lc,hx4kxiu,t1_hx0yv3b,Only if we live in a simulation.,1,0,0,False,False,False,1644981803.0
st08lc,hx501pu,t1_hx0yv3b,"On the 7th day, God called usleep()",1,0,0,False,False,False,1644989707.0
st08lc,hx2ayd0,t1_hx21oyc,"And PHP is written in C, and keeps many of its conventions.

If your PHP app is being served by Nginx, that's also written in C. Do you prefer Apache Webserver? That's also C.

What database are you using? Postgres? Written in C. MySQL? C along with C++. Or maybe (heavens forbid) Oracle? Also C.

To say nothing of the base OS of your stack, which is also C if you're on any Linux, along with almost all of the tools and programs any of your components use.

The world is written in C.",29,0,0,False,False,False,1644948348.0
st08lc,hx283b3,t1_hx21oyc,"The Internet is not the same as the World Wide Web.  I've never seen anyone write a TCP/IP stack in PHP, but almost all of them are written in C.",12,0,0,False,False,False,1644947267.0
st08lc,hx2elle,t1_hx21oyc,"And the standard PHP interpreter, the Zend Engine, is written in C.",3,0,0,False,False,False,1644949729.0
st08lc,hx19l99,t1_hx16xes,"Sure, and as I mentioned it depends on what you're dealing with. 

The topics I'm specialized in are generally very technical and often ""black boxes"". So, the staff I'm organizing needs to be precise and they need to be able to distinguish between rumors and facts and they need to proof what they are doing. 

This is why my staff is generally among the best paid here.",3,0,0,False,False,False,1644933210.0
st08lc,hx20q5t,t1_hx1rda6,"> Don't, really? AFAIK, popular board Arduino Nano is even more limited that PDP-11 from 1970.

You don't run a C _compiler_ on an Arduino Nano.

Yes, C is designed with hardware limitations in mind, namely the _hardware that runs the compiler._ C compilation is designed so at no point was it necessary to have the entire code file in memory at once

This is why C has a preprocessor, forward declarations, and so on. It is not entirely inaccurate to say that the early C compilers were little more than a shell script:

    cat main.c | cpp | cc | as > main.o

> That's probably why all popular operating systems are written in it and it's the first language after Assembly to support any new architecture

Modern processors are _absolutely nothing like_ the PDP-11, and you are deluding yourself if you think so. Embedded systems may still work on similar levels, but general purpose computing hardware is utterly alien.

C is supported because it is light-weight, and because every major processor series commercially available exposes an ABI (""machine code"") which superficially pretends to be a PDP-11.

> Excuse me, what?

C can be ""close to the metal"" or C can be fast.

The Intel x86_64 is not a really fast PDP-11, it's a 150-step instruction pipeline with speculative evaluation, hyper-threading, cache prediction heuristics, branch predictors that perform statistical analysis of your program as it runs...

Programming in C is programming to the specification of a virtual machine --- a formally specified execution environment --- that runs C code. It's ahead-of-time compiled, but that doesn't make the semantics of C any less artificial than that of, say OCaml.

The C virtual machine is largely _why_ undefined behavior still exists. Your optimizer needs to assume certain rather restrictive things about pointer aliasing, arithmetic, addressing modes, and so on and so forth, in order to generate fast code.

There's a processor architecture in the works where, at the hardware level, _pointers are not integers._

> It will probably not convince you in the slightest, but read this comment

Please take [this quiz.](https://stefansf.de/c-quiz/) I got 27/31. Can you do better?

The post you linked to me does not convince me in the slightest because that poster doesn't know what C even is, has never properly read the C spec, and evidently thinks it's still 1981.

I'll refer you to the following line from the philosophy of the Pony programming language:

> Incorrectness is simply not allowed. _It’s pointless to try to get stuff done if you can’t guarantee the result is correct._

It is very nearly humanly impossible to write correct C. With the best static analysis tools in the world to make sure you're dodging every form of undefined behavior it becomes _prohibitively unergonomic_ to write C.

[C is not a low level language.](https://queue.acm.org/detail.cfm?id=3212479)",0,1,0,False,False,False,1644944474.0
st08lc,hx2i9vj,t1_hx2elle,"Even C compilers are generally implemented in C.

🤘",7,0,0,False,False,False,1644951125.0
st08lc,hx2q8eg,t1_hx2elle,"Dude there are at least 10 - 20 PHP pages for 1 PHP instalallation on a shared server hosting - [s](https://banan.cz)ome can offer even ""unlimited"" PHP websites per 1 server (depends on traffic and CPU/RAM usage - disk space is not so problematic - one average WordPress site with a few plugins is cca 30 MB)

Only a tiny fraction of people are using C. But every idiot is using PHP. I don't know anybody who wrote anything in C yet of those people many have done PHP.

If you live in a C bubble that's admirable - not everybody has such pain tolerance. But most of the world PHP and not Cee. Echo out.",-18,0,0,False,True,False,1644954166.0
st08lc,hx2daw6,t1_hx19l99,Found the “white hat” :),1,0,0,False,False,False,1644949237.0
st08lc,hx2kf52,t1_hx20q5t,"I started writing equally wrong reply, but then came to my senses that we won't get anywhere that way, so I will try to concise it.

**_Ad_ low-level, PDP-11 etc.**  
Am I wrong or are you under the impression that I perhaps may be thinking that C is a low-level language?  
By gods, **no**! Believe me, I also cringe when people call it low-level. It's high-level language by any sane definition\*.  
That's why it's not ""portable assembly"" nor ""as close to machine as possible"".

**_Ad_ virtual machine**   
Ah, you mean _abstract_ machine, right? I had a brain fart I though you are talking about something like Java Virtual Machine, my bad.

**_Ad_ quiz**  
Very enjoyable quiz, I like it. I didn't do better, in fact got a point less\*\*. The less likely\*\*\* an example to occur IRL the more I struggled.

**_Ad_ compilers, UB and abiding the standard**  
I think I'm not good enough lawyer to write a reply to that. Again, not strongly disagree, but don't agree either.  
But I think if I invite u/flatfinger, the discussion between you two may be very interesting and educational.

**_Ad_ ""impossible to write correct C""**  
It seems we might have different definitions (or thresholds? standards?) of correctness.  
In such situation, let's do the best for the case and agree to disagree.

&nbsp;

^\* ^(although I've stopped hoping to be able to correct people, so now I'm trying to at least mitigate damage using term ""middle-level"")  
^\*\* ^(but I am also by no means a C expert and never claimed to be)  
^\*\*\* ^(I won't say no code ever had such garbage... I saw what freshmen at university are capable of...)",2,0,0,False,False,False,1644951940.0
st08lc,hx2g4xr,t1_hx20q5t,You are so decisively and formidably correct here that I’m going to follow you to enjoy more of it. Cheers my new friend!,1,0,0,False,False,False,1644950311.0
st08lc,hx3o811,t1_hx20q5t,">Please take this quiz. I got 27/31. Can you do better?

Unless a particular program contains one or more #error directives or at least nominally exercises the translation limits in N1570 [5.2.4.1](https://5.2.4.1), nothing an otherwise-conforming implementation might do while processing it would render that implementation non-conforming.

While such an interpretation of ""The implementation shall be able to translate and execute at least one program that contains at least one instance of every one of the following limits:..."" may seem a stretch, the the last published Rationale document (for C99, which included the same text) says:

>The C99 Committee reviewed several proposed changes to strengthen or clarify the wording on conformance, especially with respect to translation limits. The belief was that it is simply not practical to provide a specification which is strong enough to be useful, but which still allows for real-world problems such as bugs. The Committee therefore chose to consider the matter a quality-of-implementation issue, and to leave translation limits in the standard to give guidance.

Also, even setting the ""One Program"" loophole aside, I think the quiz should include something like asking whether the following function could ever write to arr\[65536\].

    char arr[65537];
unsigned test(unsigned x)
{
    unsigned i=1;
    while(x != (unsigned short)i)
        i*=-3;
    if (x < 65536)
        arr[x] = 1;
    return i;
}
void test2(unsigned x)
{
    test(x);
}

The code generated by clang for `test2(x)` at any optimization setting above -O0 will unconditionally store 1 to `arr[x]`; while I don't think the Standard was intended to invite such counter-productive ""optimizations""(\*), it doesn't forbid them, and clang regards the lack of prohibition as an invitation.

(\*) If permission to ""assume loops will terminate"" were treated as an invitation to say that if no individual action within a loop is observably sequenced before some later operation, the loop as a whole need not be treated as observably sequenced either, that would allow a compiler to omit the code for the `while` loop within `test` in situations where the caller would ignore the return value.  If a programmer had to include a dummy side effect within the loop to prevent the compiler from behaving in completely arbitrary fashion in scenarios where the loop doesn't terminate, that useful optimization would be negated.",1,0,0,False,False,False,1644967197.0
st08lc,hx2t0qg,t1_hx2q8eg,"Only small fraction of the Internet users ever heard about TCP/IP.   
Are you now gonna argue that TCP/IP protocol stack isn't the backbone of the Internet?",5,0,0,False,False,False,1644955236.0
st08lc,hx2w3x1,t1_hx2q8eg,"Only a tiny fraction is coding PHP!

The vast majority is ""coding"" Google queries, ""programming"" Word documents and ""hacking"" Twitter, Facebook, tiktok and co.

I don't know in which bubble you're living. 

Sorry, I'm off as I need to refactor some minecraft before unit testing an HTML page.",3,0,0,False,False,False,1644956439.0
st08lc,hxall03,t1_hx2q8eg,"A thousand teenagers aspiring to be the next prank master and getting to star daily in shaky TikTok vids, don't add up into having the same relevancy as one Robert Downey Jr. or Brad Pitt.",1,0,0,False,False,False,1645095794.0
st08lc,hx2ofdg,t1_hx2kf52,">That's why it's not ""portable assembly"" nor ""as close to machine as possible"".

What do you suppose the authors of the C Standard meant when they wrote (see [http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf](http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf) page 2):

>Although it strove to give programmers the opportunity to write truly portable programs, the C89 Committee did not want to force programmers into writing portably, to preclude the use of C as a “high-level assembler”: the ability to write machine specific code is one of the strengths of C. It is this principle which largely motivates drawing the distinction between strictly conforming program and conforming program 

Do you have a primary source for your claim that C should not be used in such fashion?",1,0,0,False,False,False,1644953470.0
st08lc,hx2mznr,t1_hx2kf52,"Often opinion beats facts. Sometimes even prejudice beats reality. Sad but true. 

Points in quizzes don't matter as can be seen with the two of you. 

... except that I cracked both of you! 😎",-1,0,0,False,False,False,1644952923.0
st08lc,hx2llhe,t1_hx2kf52,"I think we mostly agree. I may in error be using virtual machine interchangeably with abstract machine due to a linguistic difference. About half my studies were not in English.

C is awful. Don't use it for new projects.",0,0,0,False,False,False,1644952384.0
st08lc,hx2vubr,t1_hx2t0qg,"Only a small fraction of the Internet users ever heard about conditional entropy.  

Come on, we are talking about programming languages. Stop being in denial that PHP code is the most common code one can find in the world. C is extremely niche. Again, you are delusional if you think that C programmers are more common than PHP programmers. To be more exact, the number of C and C++ programmers combined is less than the number of PHP programmers. And I bet that most of them do C++ anyway. I can't think of a reason to use C instead of C++ outside of the size of binaries reason or nanoseconds of execution time difference (e.g. for self-driving cars where C and C++ can make a difference), but even for a web server - the 2 fastest ones (drogon, lithium) in the world are written in C++ and not C. And when it comes to GUI stuff, C is sadomasochism unless you are a GTK sadomasochist. But even they are migrating and promoting Rust instead of C nowadays ;D. C is really used less and less and the area of dominance is shrinking every year. Especially Rust and Zig will bite off of C's usage - in fact it's already happening, e.g. as I said with the GTK case, it's recommended for new project to start them in Rust instead of C. Source: https://www.daxx.com/blog/development-trends/number-software-developers-world",-7,0,0,False,True,False,1644956337.0
st08lc,hx2r1gr,t1_hx2ofdg,"The source of my claim is right here, in the middle of the sentence: _use of C as a “high-level assembler”_

The quotations marks and specification of _high-level_ make a big difference (at least for me);  
saying _portable assembly_ is simplification of a simplification (or metaphor?).

Basically:

*  _use of C as a “high-level assembler”_ implies _C is high-level language capable of low-level programming_ (TRUE)
* _C is portable assembly_ implies that _C is low-level language_ (FALSE)

^(and truth cannot imply false)

---

And of course it isn't as close to machine as possible, it's as close to machine as a general purpose high-level language can get, but it still may produce machine code which doesn't map directly.",1,0,0,False,False,False,1644954474.0
st08lc,hx2ow7h,t1_hx2llhe,"Half of the world's problem would disappear had there been no misunderstandings/miscommunication.  
The other half would be solved with better communication ;)

C isn't awful. The general design is one of the best (look how many modern ""better"" languages copy from C).  
But I agree that details are not great. It's really easy to shot yourself in the foot. Usually bad idea, but some people learned how to make the bullet ricochet to achieve ordinarily impossible angles.

For big complex projects? Yes, don't use C anymore.  
For small ones and you already know C? Go for it.  
And of course, there are still left domains, where C remains the best choice after all pros and cons.",1,0,0,False,False,False,1644953648.0
st08lc,hx2wa6k,t1_hx2vubr,"But we are not comparing LOC\* or the amount of programmers\*\*. We are talking what is the backbone of the tech world, how many things would go bananas without programs written in given language.

All PHP code stops working -> most\*\*\* webpages stop working  
All C code stops working -> **everything** stops working (including PHP)

&nbsp;

^\* ^(And even if we were, Linux kernel alone eats those few PHP webpages hosted on the same server)  
^\*\* ^(And if we were, then by your own source: PHP may be position above C/C++, but loses to JS, Java, Python and C#)  
^\*\*\* ^(But not all. PHP market share is ~75%)",5,0,0,False,False,False,1644956506.0
st08lc,hx37ea9,t1_hx2r1gr,">And of course it isn't as close to machine as possible, it's as close to machine as a general purpose high-level language can get, but it still may produce machine code which doesn't map directly.

If the language were specified in terms of lower-level steps, but implementations were allowed to reorder or consolidate operations in certain cases *even if doing so might observably affect program behavior*, specifications for the low-level operations and the cases where operations may be consolidated could be much simpler but more useful than is possible under the present approach where all actions whose behavior might be observably affected by optimization must be classified as UB.

If an optimizing transform would convert a program that behaves in one acceptable fashion into a program that works in a different acceptable fashion, specifying that an implementation may perform that transform despite the fact that it might affect program behavior would be far more useful than requiring that the Standard either forbid the optimization, or be interpreted as inviting implementations to apply arbitrary ""optimizations"" that would yield unacceptable behavior in cases where the useful transforms would have had benign but visible effects on program behavior.",1,0,0,False,False,False,1644960633.0
st08lc,hx2xz5r,t1_hx2ow7h,Everything good in C came from Algol 68 anyway.,1,0,0,False,False,False,1644957159.0
stb8pj,hx3lyql,t3_stb8pj,What you are asking for is called Maximum Independent Set and it is in general an NP complete problem. See https://en.wikipedia.org/wiki/Independent_set_(graph_theory).,8,0,0,False,False,False,1644966257.0
stb8pj,hx3aw0p,t3_stb8pj,"If directly connected means edge , Coloring the graph might be the solution.

 For each color increment the respective count as it is used.

If adjacency list representation is considered then 

For coloring a node:
no of adjacent nodes times iterations will be used for checking adjacent nodes color and choosing the color for that particular node.

Since each edge is repeated twice( for edge between u to v , once in adjacency list of u, other in adjacency list of v), it will take time = twice the no of edges.

But if there are disconnected components, it will take time equals = no. of disconnected component + (twice the no. of edges)",2,0,0,False,False,False,1644961937.0
stb8pj,hx5a56b,t3_stb8pj,"Don't know if it helps. But this problem would be equivalent as finding the largest complete subgraph of the graph but you place edges where there were none before, and you remove the edges that you had originally. If I'm not mistaken",1,0,0,False,False,False,1644996574.0
stdryq,hx3cruj,t3_stdryq,"I guess you can write those out as 'X', then you choose whether you consider 'X' is 0 or 1.",6,0,0,False,False,False,1644962630.0
stdryq,hx52x32,t3_stdryq,"Consider them don't cares. 

Later you would adjust them to ensure no glitches.",1,0,0,False,False,False,1644991497.0
ssv8zr,hx0huxp,t3_ssv8zr,"Tiers aren't based on what an application does, they're based on how you separate things in your code. Sometimes these tiers are different processes or systems, but not always, they just have to be decoupled from one another. You can write a 3 tier architecture in a single local application if you want, or you can hard wire the fuck out of everything and have it be a single tier architecture that provides exactly the same functionality (some sources describe n-tier architecture as being coupled with network architecture and requiring different physical machines, but that definition was useful for the 5 minutes between the invention of the web and the use of virtual machines/cloud computing, so ignore that).
 
As a rule of thumb, if you could swap out a chunk of your application with an alternative without making any changes to other chunks (or at least very few and limited to simple configurations), then it's modular, and if you have a comprehensive theme of operations (storage, business logic, presentation, etc) that are modular together then you have a tier.
 
So an application that leverages a separate database is at least 2 tier, **if and only if** the database management is separate to the application. If your application code is creating your tables, indices, etc then you don't have a modular component even if it's running as a separate process. If the GUI is properly decoupled from the business logic of the application, then you have a third tier. There isn't much if any benefit to local applications going above 3 tiers so I've never really heard of it being done, but it's theoretically possible to just do the same separation locally as you would a distributed application.",6,0,0,False,False,False,1644913948.0
ssv8zr,hx02tgg,t3_ssv8zr,Simplistically 2-tier as the DB is 1 and the app another.,5,0,0,False,False,False,1644903054.0
ssv8zr,hx2jlej,t1_hx0huxp,"This site tells a different story.

https://www.guru99.com/dbms-architecture.html

>A 2 Tier Architecture in DBMS is a Database architecture where the presentation layer runs on a client (PC, Mobile, Tablet, etc.), and data is stored on a server called the second tier.

In my case, data is not stored on a server but my laptop.",1,0,0,False,False,True,1644951626.0
ssv8zr,hx0wa32,t1_hx02tgg,My DB is stored on the local system. Does that still make it 2-tier?,1,0,0,False,False,True,1644925547.0
ssv8zr,hx3dhk7,t1_hx2jlej,"Yes, there are plenty of sites out there that are wrong. That is one of them. *Often* tiers will be running on different machines, because the ability for that to happen is one of the benefits of that separation, but it's not a prerequisite for their existence.",1,0,0,False,False,False,1644962897.0
ssv8zr,hx0xm1q,t1_hx0wa32,Yes.,1,0,0,False,False,False,1644926471.0
ssv8zr,hx3dssx,t1_hx3dhk7,"Okay, so the conclusion is that my application is 2-tier, right?",1,0,0,False,False,True,1644963012.0
ssv8zr,hx0yndb,t1_hx0xm1q,"How? This site tells a different story.

https://www.guru99.com/dbms-architecture.html

>A 2 Tier Architecture in DBMS is a Database architecture where the presentation layer runs on a client (PC, Mobile, Tablet, etc.), and data is stored on a server called the second tier.

In my case, data is not stored on a server but my laptop.",-1,0,0,False,False,True,1644927168.0
ssv8zr,hx3dzs7,t1_hx3dssx,Maybe. I gave you the condition for that to be the case in my original comment. Could be 3 tier too.,1,0,0,False,False,False,1644963087.0
ssv8zr,hx3fhp0,t1_hx0yndb,"And that is bad thinking.  If the DB can run elsewhere, then it is separate. 

Now we think in terms of architectural layers, and not so much where things physically run. So a web server with all server side components (proxy, web server, possible PHP or other engine, database) are on one  physical server, the connected to by browser is 2 tier ??  NUP!",1,0,0,False,False,False,1644963670.0
ssv8zr,hx3fi04,t1_hx3dzs7,"Actually, I didn't understand this part.

>So an application that leverages a separate database is at least 2 tier, if and only if the database management is separate to the application. If your application code is creating your tables, indices, etc then you don't have a modular component even if it's running as a separate process. If the GUI is properly decoupled from the business logic of the application, then you have a third tier. There isn't much if any benefit to local applications going above 3 tiers so I've never really heard of it being done, but it's theoretically possible to just do the same separation locally as you would a distributed application.",1,0,0,False,False,True,1644963673.0
st2hhc,hx3rdi1,t3_st2hhc,"So basically you it follows the OSI layers. The network card/NIC usually handles the first two. Depending on the network these can be very different (like wireless networks or frame relay).

On the first layer you have defined networking cables, signal levels, physical port interfaces, bit encoding

on the second you have the protocol part (addressing, frame types,...)

The third and io will take higher level routable protocols like IPv4 or 6. Actual session or connection initiation happens on for and up.

So the NIC again usually handles the first two although some might handle some features of the upper layers (CRC, filtering,...) for performance reasons or implement special services like PTP.

The NIC has an internal buffer to store detected bits and bytes a processes them into frames, when it recognizer a correctly received full frame.

Depebding on DMA or other capabilities these will be copied by the CPU or DMA to a memory of the system where the higher level stacks try to identify the protocol (TCP or UDP for example) and then the session and process it belongs to. The process will be notified that there is new data resp. the socket call will return with the new data. The process then implements the higher level protocols like http.

When sending this happens the other way round.

If frames arrive to fast resp. the operating system or the process does not collect the data fast enough the buffers will fill and data will be dropped eventually. Then higher level protocols if desired will need to perform a retransmission of missing packages.

Flow control, throttling, quality degradation (videos for example) can happen on multiple layers depending on the network and protocol used to avoid unnecessary retransmission.",2,0,0,False,False,False,1644968556.0
st2hhc,hx16d6h,t3_st2hhc,"The network card on your computer does that. When you learn about lower layers of the networking model you will start understanding it.

The network card is the interface to outside world. It is the first to receive any data destined to your computer.",1,0,0,False,False,False,1644931622.0
ss0efg,hwvo25c,t3_ss0efg,Network Security. Damn college just didn’t offer it the one year I could take it.,37,0,0,False,False,False,1644818970.0
ss0efg,hww2c0w,t3_ss0efg,"For me personally, a lot of my excitement from my earlier programming days came from programming physical robots to do stuff. The first time I got a physical device to move using a program that I made amazed me so much I screamed ""this is so cool!"" You could probably use a few kid friendly robot kits that can be programmed with scratch or some other block code language. I feel like for children that age anything physical/visual can help keep them interested while teaching some of the basic concepts of programming.",21,0,0,False,False,False,1644830144.0
ss0efg,hwwmtpx,t3_ss0efg,"Hey High School CS teacher here, not sure what your resources and access looks like but a few recommendations for that age group I think would be Scratch programming, Image Editing, and PC Hardware. 

Making games in Scratch is fairly easy and you can make fun stuff like Zelda and Gradius clones. Image editing using photoshop or good alternatives like photopea, to make memes is always a fun time. Also getting old or disused PCs and showing the parts inside and what they do is fun and useful. I bought a few copies of PC Building Simulator on Steam and use it to demonstrate how to build PCs.",9,0,0,False,False,False,1644845364.0
ss0efg,hwvdvqz,t3_ss0efg,"At the university level, one of my favorite assignments was password cracking with John The Ripper and a given shadow file. Outside of class, all the students in the class ended up voluntarily discussing with each other the different techniques they used as they tried to figure out how to crack the various passwords. While the use of JtR and the rest will be too advanced for most of your students, a similar assignment might be fabricated in a more accessible way.

There are also some cybersecurity-themed games, both digital and physical, that are available to teachers for free. DM me if you’re interested in that info.",41,0,0,False,False,False,1644812646.0
ss0efg,hwvjyyp,t3_ss0efg,"For one my opportunities around that age were all pseudo programming, like those games where you slot blocks together with each block being an instruction. I always felt like that was a waste of time and would've loved to see more general programming like simple python scripts.

Furthermore I would suggest showing them how to program something useful. Maybe some lime a simple python script that helps solve or double check their math assignments.",11,0,0,False,False,False,1644816313.0
ss0efg,hwv8oks,t3_ss0efg,"I’m sure there are much more flashy things to do, but if you could get them each a small ($5) raspberrypi and teach them to code a small interface that broadcasts over the local network, then find another thing, like a WiFi radio or light or something. Teach them how to control it over the ip, then let them take it home at muck about with it over their lan. That woulda caught me up in this world long before I ended up in it.",20,0,0,False,False,False,1644809974.0
ss0efg,hww6z6u,t3_ss0efg,"As kids, my friends and I got a huge kick out of using inspect element to change the text on websites to prank each other (like pretending we completely failed an assignment or pretending we got access to each other's school accounts). You could use it as an intro to teach some basic HTML.",3,0,0,False,False,False,1644834173.0
ss0efg,hwvwnrp,t3_ss0efg,Problem solving skills (done with simple python projects for ex.) are way too underestimated at the 10-14 age level. At least at my school,7,0,0,False,False,False,1644825349.0
ss0efg,hww0fs8,t3_ss0efg,How to center a div,6,0,0,False,False,False,1644828513.0
ss0efg,hwy5q67,t3_ss0efg,"Network Security: as a kid, learning to do this would’ve been cool! It isn’t as C++/Java/Python oriented, but is still fun

Linux: again, not so programming oriented, but super helpful when working on your own system. Using a Raspberry Pi, can easily cost less than $50 per student in material costs

Robotics: using a Microbit or other Bluetooth enabled microcontroller, it’s fun to connect with things and get real data or do real tasks. Additionally, it’s a good opportunity to teach some electronics as well using RC vehicles from thrift stores, or tearing apart old printers for motors and sensors

IoT: using microcontrollers to get real world data and make some great tools! The Microbit by itself can be used as a Bluetooth sensor for getting accelerometer and light intensity, which can be used for gravity/force visualization and a cheap water quality sensor. 

IE: if able, don’t just teach programming, teach them that they can do things with it",2,0,0,False,False,False,1644869049.0
ss0efg,hwz56j7,t3_ss0efg,setting up nginx and a basic website.,2,0,0,False,False,False,1644886067.0
ss0efg,hx9ooed,t3_ss0efg,"A lot of people are talking cybersecurity, so I recommend you look into the CyberStart America game.",2,0,0,False,False,False,1645071969.0
ss0efg,hwvwpig,t3_ss0efg,"- Programming with Python
- 3d modeling in Blender
- Music production using a DAW such as Ableton",7,0,0,False,False,False,1644825388.0
ss0efg,hwvv0dk,t3_ss0efg,"As a 10 year old I think it would be fun to learn the basics of Python and write a small program to see how powerful code can be 

There are quite a lot of Python resources that are friendly for children",8,0,0,False,False,False,1644824022.0
ss0efg,hwwm6s1,t3_ss0efg,"As long as you keep that pseudo block garbage out of your curriculum you’ll be good. And please for the love of god, if you want to teach a programming language with the rest of the curriculum , use an actual environment like eclipse or vstudio.",1,0,0,False,False,False,1644845007.0
ss0efg,hwwy05i,t3_ss0efg,"For high-school, I'd probably try to find something interesting. Maybe introduce something like Unity or raspberrypis. Something they can see progress in during your time together. 

But oh my goodness, college... COLLEGE!?!? We need more practical classes. Throw out that second composition course you force me to take, and show me how to set up various environments. Show me how to setup paths, and linking, and the myriad of other things the students don't already want to know. Show me how popular IDES solve these things.

The reason I say this, is because you try to find solutions to some of these things, and it's the same shitty, regurgitated solution across the ENTIRE internet. Think about that, that's fucking insane. I think stems to a very small subset understanding how to solve these things, and so we're all recycling the solutions. 

I've gone on a journey to try to teach myself more of these things, but I really do think it's one of the things experience helps a lot in. One class, I think that's all it would take. And I know theu touch on it(I went to a very good school), but from what I've seen, not enough.",1,0,0,False,False,False,1644851080.0
ss0efg,hwxdy03,t3_ss0efg,The basics of version control software. Preferably git so they can explore GitHub but an overview of the differences in the different softwares and maybe a brief intro into how to use the big ones could be beneficial to help them start exploring the open source software out there.,1,0,0,False,False,False,1644857721.0
ss0efg,hwxedw2,t3_ss0efg,"I would recommend using Python with either VSCode (my preference) or Jupyter or even Google Collab. You could then teach them how to do basic math problems, code simple games (yes, Python can do this too), simple AI stuff (MIT has a good and really easy to pick up course on deep learning where they generate folk music along with other things. You could simplify this to your audience)",1,0,0,False,False,False,1644857896.0
ss0efg,hwxsdq1,t3_ss0efg,I wish we studied more algorithms and data structures.,1,0,0,False,False,False,1644863671.0
ss0efg,hwy1mrj,t3_ss0efg,I would say designing pipeline and deployment or more networking. Definitely my personal weak points,1,0,0,False,False,False,1644867387.0
ss0efg,hwx5d32,t3_ss0efg,"I am old enough that I was never in a computer class, but young enough that we had computer labs to type our essays/make powerpoints and everyone had a laptop for college. 

I would say to make sure they are proficient in Microsoft office (or google docs), Microsoft excel (google sheets), and shortcut keys first. (Is that a given now a days?) Then I would move on to working with coding (as others have suggested).",1,0,0,False,False,False,1644854243.0
ss0efg,hwyjhna,t3_ss0efg,Still in school but anything to do with A.I/ML/DL,1,0,0,False,False,False,1644874633.0
ss0efg,hwyohjo,t3_ss0efg,Git and web development.  AWS.  Pipeline tools.,1,0,0,False,False,False,1644876714.0
ss0efg,hwvtwmz,t3_ss0efg,[deleted],-17,0,0,False,True,False,1644823174.0
ss0efg,hwxp0gz,t3_ss0efg,"Active Directory. I've self taught myself some of it but we barely touched upon it, now as a tech support guy I have to deal with it every day.",-2,0,0,False,False,False,1644862284.0
ss0efg,hww575e,t3_ss0efg,Python and lots of visual stuff.,-1,0,0,False,False,False,1644832628.0
ss0efg,hwyo1yd,t1_hwvo25c,Can you elaborate a little?,1,0,0,False,False,True,1644876521.0
ss0efg,hww77nd,t1_hwvdvqz,Second this. I had so much fun with the Khan Academy cryptography unit when I was in elementary/middle school. I would've loved a unit like that in school.,9,0,0,False,False,False,1644834382.0
ss0efg,hwxc4ea,t1_hwvdvqz,Here's a free hash cracking lab from an open textbook I've been developing. It uses Docker so you should be able to run it on just about anything: https://web.njit.edu/~rxt1077/security/#_lab_hash_it_out,6,0,0,False,False,False,1644856998.0
ss0efg,hww54r8,t1_hwvdvqz,"Honestly, that might be the dumbest thing to teach kids.",-13,0,0,False,True,False,1644832569.0
ss0efg,hwx4lj5,t1_hww0fs8,Was expecting this to be the top comment,1,0,0,False,False,False,1644853926.0
ss0efg,hwzp4dm,t1_hwz56j7,"Websites are a fun option for sure. Learning HTML and CSS to create a website provides nearly immediate visual feedback. There is creative flexibility, and if it’s hosted by the teacher (or not, just might be easier), it can be shared with the entire class. They could even extend the website with a CS-related topic that they chose to learn about as a take home project.

Transitioning to a self-hosted solution would probably be better done later in the year, after you have their interest and buy-in for the effort it would take to learn it.",1,0,0,False,False,False,1644895630.0
ss0efg,hww315q,t1_hwvtwmz,How to create nfts and other buzzwords,4,0,0,False,False,False,1644830744.0
ss0efg,hx2fuz1,t1_hwyo1yd,"My college offered a network security class that broke the class into teams and had them duel it out in a protected sandbox for the year trying to penetrate each other’s subnetworks. It sounded great, but they decided not to offer it my junior year (the only year my graduating class could have taken it based on requirements). We all signed a letter asking them to offer it but the teacher wanted a year off. I’m still bummed about it.",1,0,0,False,False,False,1644950206.0
ssagax,hwwvjx3,t3_ssagax,"No I wouldn't say so

\- There are pretty standard conventions for most Programming languages and normally it makes sense to not reinvent the wheel without a specific need (e.g some people have a big monorepo consisting of a bunch of different projects that might have some custom work for build processes.

\- Most decent IDEs have some templates built in for new projects in common languages (e.g Python or Java)

\- Most Git repos (github/gitlab) also have templates avaiable to start with. 

\- Most companies who are at least doing some software things right normally have some sort of template with their own conventions for linting/formatting/testing etc built in to get things going. The priority is expensive programmers don't spend ages messing around with folders and setup and don't create some spaghetti that confuses other people too. 

&#x200B;

If I spend more than a few minutes moving directories etc around then I'm either heavily refactoring things or something has gone majorly wrong.",5,0,0,False,False,False,1644849965.0
ssagax,hwwyy7z,t3_ssagax,"I use the following 

Maven for Java 

Cargo for Rust

Poetry for Python

Custom bash scripts for C",3,0,0,False,False,False,1644851504.0
ssagax,hwwovfu,t3_ssagax,"This is something that will come with practice and time. I would also spend hours worrying about how optimal my files were organized when I started *real* development (working on projects that I was responsible for maintaining longer than a weeks assignment, which I seldom experienced in undergrad, and suspect you had a similar experience with), and also when I start writing in a new language / framework.

These days I typically do a quick Google for “best folder structure for X framework/language” and you can find good stackoverflow answers to common practices and conventions. 

Another thing to keep in mind is the effectiveness of one folder structure depends on the application you’re developing, and also a lot of it is strictly preferential; two managers would likely choose different folder structures/conventions for the same application.",2,0,0,False,False,False,1644846465.0
ssagax,hwx8h8v,t3_ssagax,"Depends on what kind of project. If you are prototyping, you typically use as little structure as possible. If you are working on rewriting a large existing code base, you want a well defined structure to start with, to have an idea of where you are going. 

Also, different applications will use different types of structure. Although typically you have models, business logic, some sort of data access logic, possibly some static helper classes or extension classes. If its an API you will have some classes for setting up the api and its endpoints.

After you learn about architecture and design patterns and practice a lot, it gets easier. At the end of the day, learn from what has worked for others.

Edit: if you don’t know enough about the project or have the experience, look at what others have done or use minimal structure. You can always add structure later. Don’t overthink it.

Edit2: to answer your question specifically, yes sometimes we do. These decisions made on the structure can have consequences on the maintainability or amount of technical debt for a project.",1,0,0,False,False,False,1644855538.0
ssagax,hwxaapr,t3_ssagax,"This depends on what kind of project you're building. For web applications, for example, there are many, many frameworks that take care of that for you.",1,0,0,False,False,False,1644856266.0
ssagax,hwxdzx3,t3_ssagax,"not really. I use fzf for navigating around a repo and honestly it makes directory structure largely irrelevant. I have met other devs who do rely quite heavily on it however. For whatever you’re writing, there’s likely some established convention anyway. e.g MVC web projects will have model/view/controller directories with subdirectories for scoped routes. Not to say everything will fit neatly into that convention, but it provides a scaffold to build around",1,0,0,False,False,False,1644857742.0
sspvav,hwz9rcf,t3_sspvav,"I’ve been thinking of building my own brain-computer interface (stuff’s expensive). Not a lot of accessible open-source out there, so really glad to see this!",2,0,0,False,False,False,1644888275.0
sspvav,hwzgwio,t1_hwz9rcf,">source out there, so really glad to see this!

thank you so much!)",2,0,0,False,False,True,1644891712.0
srrd8w,hwtnm07,t3_srrd8w,Two minute papers may be a place to look at on YouTube.,13,0,0,False,False,False,1644785316.0
srrd8w,hwul9bd,t3_srrd8w,"If you find one, let me know. I was hoping this would be that but hasn't really been.",9,0,0,False,False,False,1644799643.0
srrd8w,hwu8szq,t3_srrd8w,"It’s not a subreddit, but check out https://paperswelove.org/. My local group is great.",6,0,1,False,False,False,1644794133.0
srrd8w,hwuptkt,t3_srrd8w,"I think just use the good old research gate. Even people in the same subfield like machine learning, distributed system, can find others' paper quite far from their interest. I would just go to research gate and follow all the researcher that you are interested in.",2,0,0,False,False,False,1644801818.0
srrd8w,hwuui3i,t3_srrd8w,"i feel like the only two use cases for reading random rearch papers is if (a) ur trying to decide a thesis topic (b) just for fun 

dont think it would be professionaly useful if thats what ur doing it for",2,0,0,False,False,False,1644803823.0
srrd8w,hww42om,t1_hwtnm07,">Two minute papers may be a place to look at on YouTube.

They have good overview but the YouTube comments are not good experience for discussing about the papers. Anyways, it is good.",3,0,0,False,False,True,1644831650.0
srrd8w,hwun61w,t1_hwul9bd,">I was hoping this would be that

So did we, so did we...",7,0,0,False,False,False,1644800516.0
srrd8w,hwwgypk,t1_hwul9bd,"Sure, I have posted it to other subreddit as well.",1,0,0,False,False,True,1644841873.0
srrd8w,hx0aivj,t1_hwul9bd,"One good mention in other subreddit where I crosspost this was this.

https://jeffhuang.com/best\_paper\_awards/",1,0,0,False,False,True,1644908196.0
srrd8w,hwvrq0i,t1_hwun61w,"Yup. Instead half the posts are IT and tech support questions, ""Can someone do my homework for me?"", ""I have no CS experience, but what CS job pays the most?"", and people advertising their YT channels and Medium blogs. Mod queue is always *so fun*",7,0,0,False,False,False,1644821533.0
srrd8w,hx1516b,t1_hx0aivj,"Thanks, I appreciate it. I'll make sure to check it out.",1,0,0,False,False,False,1644930921.0
srw89o,hxazmx7,t3_srw89o,"He goes into more detail here:

[The problem with functions | Arithmetic and Geometry Math Foundations 42a](https://www.youtube.com/watch?v=_6B2PUn99Bc)

 [The problem with functions | Arithmetic and Geometry Math Math Foundations 42b](https://www.youtube.com/watch?v=tUmJi4i8dDs)

[Reconsidering functions in modern mathematics | Arithmetic and Geometry Math Foundations 43](https://www.youtube.com/watch?v=hTMGuBO-Hss)",1,0,0,False,False,False,1645104459.0
srqmpy,hwum9fp,t3_srqmpy,[https://stackoverflow.com/questions/8552845/how-can-i-find-bicomponents-in-graph-called-block](https://stackoverflow.com/questions/8552845/how-can-i-find-bicomponents-in-graph-called-block),2,0,0,False,False,False,1644800096.0
sraf70,hwqyy9i,t3_sraf70,"You can start by looking at existing open-source databases. I have most experience with Postgres and its documentation which is very thorough. Read this chapter: 
https://www.postgresql.org/docs/current/storage.html

From my point of view, there are harder challenges than handling queries - it is the guarantees of ACID? Postgres uses a transaction log and multiple versions of records.

If you are interested more in query processing, maybe you could use an existing database engine such as SQLite (stored in a single file, also has good documentation of the physical structure - https://www.sqlite.org/fileformat2.html) and white your own ""frontend""? Your database would bypass all drivers and work with the files directly. This way, you could focus on reading while still having the option to use the sqlite driver to fill a database for your tests.


H2 also has a good description of its storage format as well as a high-level access to it: https://h2database.com/html/mvstore.html

You can certainly also look for books (I have none to recommend), but looking at existing database will help you avoid lots of mistakes and help you focus on the parts you are interested in.",10,0,0,False,False,False,1644734254.0
sraf70,hwsha22,t3_sraf70," >  I’d like to implement a basic version a database,

I worked at Microsoft for about 15 years, including more than five years on the SQL Server team. A few other jobs since, but now I'm retired. And kind of bored, sometimes.

Over the past few months, I've been thinking harder about projects I'd like to do. When I applied for an opening at MariaDB (the company), I don't think they even read my resume--I had a rejection email the next morning.

Maybe working on MariaDB, since it's actually open source. There are a couple of books about MySQL and MariaDB internals, and superficially their documentation for developers doesn't seem so bad.  But that one primary contributor resigned a few months ago, and made it sound like the MySQL code base was a real disaster.

A couple weeks ago, I started fooling around with my own implementation. I found a T-SQL grammar in Antlr, and built that into C#. At this point, I can execute trivial `SELECT` statements and have some logic for `CREATE TABLE` implemented.

But this exercise has no real point; there are a million thing to implement. (Storage engine, language tree remapping, data dictionary, accessor objects, expression parser, locking, scheduling, transaction log, concurrency control, debugging, API, ...)  If I try to do even a humble subset myself, it's going to be a few years. I have a couple of novel ideas, but it'll be a long walk before I'll even be in a position to try them.

Anyway, if you're interested in comparing notes, let me know. But it seems like it's really important to have a goal or a scope in mind. At my age, learning isn't so important anymore; if I say I learned something and have an implementation that's 10x slower than SQL Server, I don't think it gets me anywhere!

There are also good internals books for SQL Server, and a couple of good distributed implementation books:

* Sciore, [*Database Design and Implementation*](https://www.amazon.com/dp/3030338355)
* Kroenke, Auer, [*Database Processing*](https://www.amazon.com/dp/0133876705)
* Sippu, Soisalon-Soinenin, [*Transaction Processing*](https://www.amazon.com//dp/3319122916/)
* Weikum, Vossen, [*Transactional Information Sytems*](https://www.amazon.com/dp/1558605088/)
* Bernstein, Newcomer, [*Principles of Transaction Procsesing*](https://www.amazon.com/dp/1558606238/)
* Pachev, [*Understanding MySQL Internals*](https://www.amazon.com/dp/0596009577/)
* Delaney et al, [*Microsoft SQL Server 2012 Internals*](https://www.amazon.com/dp/0735658560/)
* Gray, Reuter, [*Transaction Processing: Concepts and Techniques*](https://www.amazon.com/dp/1558601902/)

I guess these are heavy towards the transaction processing part of the field, but a couple of other good ones in there.

Book reviews on open sites are crap, particularly for technical books and the reason you describe. I like the [ACM's Computing Reviews](https://libraries.acm.org/digital-library/acm-computing-reviews) because they're more technical and less biased. While they might legitimately complain about poor pedantic presentation or pedagogy, they're legitimate complaints rather than ""this is harrrrrd"" whinging.

Hope that all helps! :)",10,0,0,False,False,False,1644768495.0
sraf70,hwrgo2a,t3_sraf70,"I'd recommend the Introduction to databases and Advanced databases class taught by Andy Pavlo at CMU in 2018 or 2019. The video lectures are available on YouTube. The 2021 lectures are good too but I liked the ones taught by Pavlo more. Also, I'd recommend that you read the book recommended in the class by Pavlo(can't remember the name).",5,0,0,False,False,False,1644747419.0
sraf70,hwudeyb,t3_sraf70,"https://cstack.github.io/db_tutorial/

Try this.. it's supposed to be SQLite reimplemented from scratch in C. It's on my reading list for a while, but i haven't read it yet, so I don't know if it's actually good.",3,0,0,False,False,False,1644796192.0
sraf70,hws1dbi,t3_sraf70,"I also found a pretty good discussion at https://news.ycombinator.com/item?id=19581721

Several people recommended a book which has a 2020 edition:

Database Design and Implementation: Second Edition (Data-Centric Systems and Applications) 
https://www.amazon.com/dp/3030338355/ref=cm_sw_r_cp_api_glt_i_R14ZAA9KSZP2DCK81Q5D",2,0,0,False,False,True,1644761378.0
sraf70,hwwrko9,t1_hwsha22,"Wow, thanks for all the resources!",2,0,0,False,False,True,1644847851.0
sraf70,hwtgz7g,t1_hwrgo2a,Second that. Andy Pavlo's lectures are brilliant and accompanying problem sets are also quite helpful. The book is Database System Concepts 7e.,2,0,0,False,False,False,1644782668.0
sraf70,hwwrimj,t1_hwrgo2a,"I had forgotten about his lectures, they are great!",1,0,0,False,False,True,1644847823.0
srsbj1,hwvq4m9,t3_srsbj1,"Kruskal's algorithm connects N-1 edges together in such a way that there are no cycles, so it must be fully connected.

The N-1 edges is the important bit, which I don't see mentioned in the picture. If there are N-1 edges used, and no cycles in the graph, then it has to be fully connected. Try an example with 3 vertices, there will be only 2 edges possible without any cycles.",1,0,0,False,False,False,1644820399.0
srbdx2,hwr2cpd,t3_srbdx2,"Okay, so let's clear it up. To be an engineer you need to go through the education system, have your bsc or msc in software or computer engineering and there you are. What you are doing now is learning programming. Programming is an essential part of being a software engineer, but it's only a part. At the engineering level you need to learn the engineering mindset, learn plenty of abstractions (we call them models) so that whatever problem will be thrown at you later in your career, you can solve them. And no amount of programming skill will have you in that. Can you learn the same stuff at home? Sure, but I have never met anyone who went through the hard parts of the engineering syllabus on his own, because, well, they are hard and complex and painful, and definitely not fun.  


So you need to decide whether you want to have the why's (this is what a university is teaching you about) or just the hows (programming). 

For programming, well, learn some backend and frontend. Have some project you figure out for yourself and start writing code. Lots of code. You will have lots of unfinished/abadoned projects but it does not matter, the task for you now is to practice on whatever you need.",3,0,0,False,False,False,1644736544.0
srbdx2,hwqugqj,t3_srbdx2,[deleted],2,0,0,False,False,False,1644731535.0
srbdx2,hwu98ng,t1_hwqugqj,This is like a gold mine of knowledge. Thanks for sharing. Much appreciated.,3,0,0,False,False,False,1644794327.0
sqt9uf,hwno2o0,t3_sqt9uf,"If you can find a way to generate a new string that generates a particular hash (a collision) then the cryptographic hash is considered broken.

If you can generate such a string quickly or with arbitrary contents, the hash is horribly broken. Any cryptographic hash must not have any known way to find collisions.

One use of cryptographic hashes is to generate a digest of a message (from Alice to Bob), which is then signed with Alice’s private key to prove that the message came from her. If Chris can generate a new message that has the same hash/digest, then he can reuse the real signature and Bob will think the fake message came from Alice. (This is simplified for clarity but the principle holds)",41,0,0,False,False,False,1644680927.0
sqt9uf,hwoawbu,t3_sqt9uf,"Yes, you are absolutely right. Same hash can be produced from two different sets of information, and at that point the hash is considered insecure.

Suppose you develop a method that converts data into hash of a particular length, say 10 characters. But that means, you can generate only limited number of hashes. (And each hash that you obtain with this method will also have a hash of its own!! ) At a certain point, the hashes will start to repeat.

MD5 and SHA1 are already broken. 

TLDR; what you are talking about are called [Collision Attacks](https://en.wikipedia.org/wiki/Collision_attack)",9,0,0,False,False,False,1644690227.0
sqt9uf,hwojgwb,t3_sqt9uf,"That's a totally reasonable question.  

The trick is that a good cryptographic function should have ""diffusion"" and ""confusion"" properties.  If it doesn't, then it is a bad cryptographic function and is more of a regular math function.

Diffusion means that each time you change one bit in the source, you change about half of the bits in the output hash in as close to a random pattern as possible.  For example, in your summation function, an input of ""ABCD"" sums to 266, an input of ""ABCE"" sums to 267.  That's bad because there was little or no diffusion, small changes in the input are making small changes in the output.  The MD5 hash of ABCD is ef2f6969fea6b4745a90dcf7f9e5492a, the MD5 hash of ABCE is the completely unrelated 95741cb5c4ee614792f6f5a44f2e107a.  Notice that almost every number in the output string changed.

Confusion has to do with how a key interacts with the source to produce the output, which is less relevant for hashing functions that don't have keys.  But this will come up if you ever study other encryption functions.  Confusion means that the key should affect the output in as close to random patterns as possible.  For example, a key of 00001111 shouldn't produce a bunch of low number followed by a bunch of high numbers.  

Other than that, you've also got the large output space to prevent brute forcing from being easy.  Always keep in mind that encryption tends to deal in humungous numbers.  The decimal equivalent of a SHA-256 hash output can be any number from 0 to ~1.16e+77.  That's getting close to the number of atoms in the visible universe.  SHA-512 output can range from 0 to ~1.34e+154.

Hopefully this helps.",4,0,0,False,False,False,1644693735.0
sqt9uf,hwoos2b,t3_sqt9uf,"There are three levels of attack against a cryptographic hash:

1. Collision: find any two inputs that hash to the same thing

2. First pre-image: given a hash, find any input that produces it

3. Second pre-image: given an input, find a second input that hashes to the same thing

Typically, these are broken one by one when a hash is being attacked. You can read more here: https://en.m.wikipedia.org/wiki/Preimage_attack",3,0,0,False,False,False,1644695915.0
sqt9uf,hwqou65,t3_sqt9uf,"Ideally, information being lost is not why cryptographic hashes can't be reversed. They should have their security properties even on a set of input smaller than the hash itself, i.e. SHA-256 should be capable of having all of its nice properties (collision resistance, preimage resistance) even on the set of strings which are all 200 bits, shorter than the 256-bit output. Hashes should be secure simply because it is the nature of some computations to be easier in one direction than the other.",2,0,0,False,False,False,1644728227.0
sqt9uf,hwnj92z,t3_sqt9uf,"How do you know what other input generates that hash? How do you know what it collides with? There are lots of collisions in all hash functions but I don't think anyone knows where to look to find them.

You may like an article I wrote about hash functions years ago.

https://medium.com/coinmonks/what-does-a-hash-function-look-like-3f1bddd22d1",2,0,0,False,False,False,1644678934.0
sqt9uf,hwo5bsd,t3_sqt9uf,"A real world example of this is the hash algorithm that was used to ""secure"" .pst files from old versions (pre-2013?) versions of Outlook.  There are lots of Outlook Password Cracker programs that will generate an equivalent password of random-looking characters to unlock the file.",1,0,0,False,False,False,1644687953.0
sqt9uf,hwofnw5,t3_sqt9uf,"a little tangential to your question, but you may like to read about https://en.wikipedia.org/wiki/Pigeonhole_principle",1,0,0,False,False,False,1644692179.0
sqt9uf,hwnq2ws,t3_sqt9uf,The problem would be you'll have to brute force every  combination to get a hash that produces collision. This obviously is not feasible,0,0,0,False,False,False,1644681735.0
sqt9uf,hwo0i3q,t1_hwno2o0,I guess I'll only understand it once I dive deep into how cryptographic hash functions work. I regretted a little after posting this because I realized that it's such a dumb question but the community here is good and doesn't downvote like on Stackoverflow. Thanks,15,0,0,False,False,True,1644685988.0
sqt9uf,hwo109k,t1_hwno2o0,[Here's an example](https://shattered.io/) where the SHA-1 cryptographic hash was broken. Two PDF files were crafted which are visibly different but have the same SHA-1 hash. The computation took 110 GPU years.,6,0,0,False,False,False,1644686194.0
sqt9uf,hwp5il4,t1_hwoawbu,"The hash is not considered 'insecure' if a collision is detected. Hashing algorithms have a shelf life, we can calculate based on if Moore's law holds, how fast a specific hash algorithm will be broken. Eg. do you care if your SHA512 hash can be broken 1000 years from now? Unlikely. 

By design, since there are a limited number of hashes and an infinite source of information, there will ALWAYS be a hash collision. That isn't bad, that is just the nature of a hash. Your goal as a hash designer is to minimize that to the best of your ability so it does not occur within the useful life of the hash or through a flaw in the algorithm.

Hashes are broken when an arbitrary message can be modified to create a hash collision within a limited amount of time in order to replace another message and pass it as legitimate. Typically that is done by padding a message until it trends towards the same hash. These days MD5 for example can be arbitrarily broken in a matter of minutes with sufficient resources, so you can send any message and make it appear with the same hash as another.",5,0,0,False,False,False,1644702705.0
sqt9uf,hwnqoo3,t1_hwnq2ws,"Yeah it's the way as you described but in my example, all I had to do was divide 266 by two or any of it's factor and then get the ascii values at those points. I still fully don't understand but it seems that these algorithms were designed in a way that you wouldn't be able to look up collisions in constant time.",2,0,0,False,False,True,1644681984.0
sqt9uf,hwp15sy,t1_hwo0i3q,"Everybody has asked dumb questions. Those who haven't are lying.

Also, although this isn't what you asked, the question becomes decidedly non-dumb if you change it a bit: rather than asking if it's possible with our current knowledge to find another string that produces the same hash, in the sense of there being a known way of doing so, one might ask if it's *fundamentally* possible to find such a string for a given cryptographic hash function, or if it's mathematically *impossible* to efficiently find one.

The question of whether it's fundamentally possible or impossible is something nobody knows the definitive answer to.",11,0,0,False,False,False,1644700933.0
sqt9uf,hwps38n,t1_hwo0i3q,"It's not even a dumb question - you are addressing how your understanding of a hash function allows a flaw that must not be possible and seeking clarification on how a proper hash function actually works to avoid that flaw. The urge to take your understanding of concepts at face value and draw conclusions (so long as you remain open minded to the fact that you may be wrong) is how exploits get found, protocols get invented, and problems solved.

Don't let yourself think this type of thinking is ""okay"" just because you are a beginner - its this way of thinking that will take you far if you don't ditch it and follow through searching for answers.",6,0,0,False,False,False,1644712369.0
sqt9uf,hwqeezd,t1_hwo0i3q,"I don't think it was a dumb question in any way. It's good that you're thinking through things and coming up with your own perspectives and questions, don't stop! Too many people just take the information they're provided and file it away without ever examining it. Your question shows that you're not doing that and imo you're better off (and ahead of your peers) for doing so. Keep at it!",3,0,0,False,False,False,1644722815.0
sqt9uf,hwqfppl,t1_hwo0i3q,"It was a perfectly good question, and I was happy to answer it. It’s exactly the kind of thing you wonder about when you first learn about hash functions if you’re the sort of person who tries to understand the subject on a deeper level than just the textbook definition. No need to feel bad or qualify it in any way, you’re doing just fine. Now go revolutionize our understanding of hash functions, it’s still a mysterious nearly-black box to most of us who work with them.",3,0,0,False,False,False,1644723462.0
sqt9uf,hwr4ksv,t1_hwo0i3q,"There are no stupid questions, only stupid people - they're the ones who don't ask questions.",2,0,0,False,False,False,1644738132.0
sqt9uf,hwqqdaj,t1_hwo0i3q,"Your describing the week 3 material of this free course

https://www.coursera.org/learn/crypto",1,0,0,False,False,False,1644729066.0
sqt9uf,hwphec7,t1_hwo0i3q,"Stackoverflow isn't for student questions, it's for professionals. Student questions are for teachers. The community on Stackoverflow can be bad (mostly the duplicate post issue), but downvoting and closing noob questions isn't bad, it's why professionals use it in the first place.",-1,0,0,False,False,False,1644707670.0
sqt9uf,hwre169,t1_hwp5il4,">By design, since there are a limited   
number of hashes and an infinite source of information, there will   
ALWAYS be a hash collision. That isn't bad, that is just the nature of a  
 hash. Your goal as a hash designer is to minimize that to the best of   
your ability so it does not occur within the useful life of the hash or   
through a flaw in the algorithm.

Ah, exactly this!! It should be safe to use as long as collisions are not frequent and not easy to produce. Nice explanation friend!!",2,0,0,False,False,False,1644745358.0
sqt9uf,hwntldg,t1_hwnqoo3,"One would imagine that might be why your hash function isn't considered a cryptograph hash.

Take a look at the properties of a cryptograph hash. There are more requirements than not being reversible, that's the lowest bar for the reasons you mention. If you look at the output of a real cryptograph hash, you will find that changing a single bit of the input causes about 50% of the output bits to flip.

I'd suggest playing with sha256 and looking at the inputs vs outputs so you can see how scrambled the output is for time changes of input. And compare that to your hash function. 

One detail to note, some tasks that need hash functions don't want cryptograph hashes because simpler hashes work better. For example you want the kind of hash used for data structures like hash-maps really care about speed and good hash distribution. Password hashes need to be cryptographic, but want to use the slowest algorithms you can find that can't be sped up by optimization. 

So in short, you got to match the hash functions to the task requirements.",3,0,0,False,False,False,1644683177.0
sqt9uf,hwnso1p,t1_hwnqoo3,"Were you trying to imply that we convert the hash into ascii string but that string's hash won't be the same as the original message's hash.

Eg:

ABCD -> SHA-256 -> E12E115ACF4552B2568B55E93CBD39394C4EF81C82447FAFC997882A02D23677 (INITIAL HASH)

&#x200B;

E12E115ACF4552B2568B55E93CBD39394C4EF81C82447FAFC997882A02D23677 -> ASCII -> á.ZÏER²VUé<½99LNøD¯É\*Ò6w  -> SHA256 - >  38A79350EAB7B3503E878FE8D2D0963520455D3F2C741F9A542FF49D915635AF (FINAL HASH)

&#x200B;

INITIAL HASH != FINAL HASH",1,0,0,False,False,False,1644682802.0
sqt9uf,hwqiz8o,t1_hwp15sy,"Presumably you mean possible for a sufficiently computationally bounded adversary. It's definitely possible in principle, given enough compute.",1,0,0,False,False,False,1644725117.0
sqt9uf,hwo0obo,t1_hwnso1p,No I was merely writing characters in their ASCII values to give it a more numbery feel. Like summing up A and B would be 65+66,1,0,0,False,False,True,1644686058.0
sqt9uf,hwrwbm7,t1_hwqiz8o,"Yeah, I should perhaps have emphasized the ""efficiently"" part more. Of course it's possible by systematically trying through all possible input strings until you find one that produces the given hash. But I don't think that's what people would usually mean with ""reversing"" a hash, any more than people would consider trying all possible combinations to a safe a method for breaking it.",1,0,0,False,False,False,1644758609.0
sr998x,hwqlhly,t3_sr998x,"I just googled how many books exist. A post from 2010 from a Google engineer put the estimate at 130 million. Even if you doubled that so it's 260 million to account for the last 12 years, 260 million is nothing. 

I have a sql table with 750 million rows in it and have complex queries that return in less than 200ms. I also have complex queries on that same table that take minutes that could be written better. The point is 260 million is trivial. 

The person behind Have I Been Pwned has a great write up how they used Azure Tables and good partitioning to create a cheap and fast lookup system for over 300 million records. https://www.troyhunt.com/working-with-154-million-records-on/",11,0,0,False,False,False,1644726414.0
sr998x,hwr5cmj,t3_sr998x,"The website will maintain its own database of books. It will probably be constantly crawling the Web and updating its database. Then when you send a query, it searches its own database.",2,0,0,False,False,False,1644738709.0
sr998x,hwqpccb,t1_hwqlhly,">. I also have complex queries on that same table that take minutes that could be written better. The point is 260 million is trivial.

OMG.

I have no other words, I'm actually shocked!

Thank you for that link too, I'll be sure to read it",2,0,0,False,False,True,1644728500.0
sr998x,hwsocw9,t1_hwr5cmj,Crawling for new book titles that don't already exist in the database right?,1,0,0,False,False,True,1644771373.0
squ1lg,hwnr6qs,t3_squ1lg,"Great questions! It’s cool to see that you are questioning things instead of just blindly doing them. I was a full stack dev for a FAANG and am now the CTO of a company deploying multiple experimental technologies which includes many external and internal apps.

1. React is a framework for building and compiling js or typescript code into a JavaScript string(s). You use npm to lightly (compile) and serve it locally so that you can make changes, adjust configurations, and test stuff on your local machine. When you deploy this app to a server you will send the… www.js or whatever the built app file is called, which is the minimized and translated js all as one line, which will be served by a production server. Then when users hit your website, the server will return the js file or part of the file if you have lazy loading built in. You need a server to facilitate communication and reactivity as you navigate around, test things, receive logs etc.

2. This was a paradigm before the frontend frameworks. And still is an option for quick fix scenarios. The problem is that building a reactive ui is very difficult and extremely bandwidth heavy. The ui frameworks allow a web app to be served to the users, a backend serving html has some very far reaching consequences that are very technical and limit performance since the user is being served parts of the experience at different times from the backend. It feels very clunky, transmits a lot form information and creates async await patterns by default. A UI framework is for building an app, serving html from the backend is good for one off things as long as it doesn’t become too cumbersome. An example I decided on recently was an upload portal, it’s not part of a larger app yet and we needed the capability fast, so our flask app serves an html page where the user can upload certain file types to the db. This was quick and easy with just html, but won’t ever be more than that as it’s better to absorb this into a ui app at some point.

3. Not all of them are, but a heavy majority. This is easily googleable so do some searching. One example I’ve worked with is razor pages, it’s within the .net stack which is c#.

4. Idk about heroku, but your db will have to follow whatever protocol it’s built on. Postgres would need to be running on a sever for your backend to communicate with it.

5. Probably some kinda terms based on traffic, but I can’t say for sure, also probably an easy Google.

And yeah cs is a life long study keep at it!",2,0,0,False,False,False,1644682189.0
squ1lg,hwnuolp,t3_squ1lg,"Alright so I think you may have a few misconceptions, I'm not sure, but I'll start by explaining these concept in case you don't understand them fully.

&#x200B;

What's a framework ? A framework is just a code structure which you use to write your applications. So Django, Flask, React are all frameworks. In a sense, a framework is just a codebase that help you solve common problems. But Backend frameworks like Django and Flask solve completely different problems that Frontend frameworks like React or Vue

&#x200B;

Front-end frameworks like React, Vue and Angular solve a very specific problem : Javascript is kinda shit at making large web applications. But technically, you don't have to use a front-end tool at all.There are different ways to make a website.

&#x200B;

A simple web page would just send a static page, but if you want to personalize your page for every user you have different options. The two basic ones are : Server-side generation, which use a templating engine to basically just create a unique HTML file based on the data you have (it's pretty simple, in fact you could probably code a very basic templating engine in  an hour). Or you could use Client-side rendering to send your entire page and then send the data as the user needs them.

This is just a short summary,  you can find more information here [https://medium.com/compendium/3-ways-of-rendering-on-the-web-4363864c859e](https://medium.com/compendium/3-ways-of-rendering-on-the-web-4363864c859e)

&#x200B;

So now, about your actual questions :

&#x200B;

1- You can run React directly in the browser, it's just javascript. But, the local development environment gives you access to webpack, babel, and auto-reloading when your file change without having to configure them yourself (at least for simple use cases)

As for NPM start, by default it runs the command react-script start which start all the stuff that your local development environment does

For more information on what exactly the local development does, read this : [https://github.com/facebook/create-react-app#whats-included](https://github.com/facebook/create-react-app#whats-included)

&#x200B;

2- If your Flask returns dynamic HTML with a templating engine (Flask use Jinja2 as a templatitng engine) you're doing server-side rendering instead of client-side rendering. React (without Next.js...) allow you to more easily make client-side rendered pages. You don't have to use React at all, A website that use server-side rendering can be a great solution, but it will depend on your use case.

&#x200B;

3- Yeah everything is JavaScript, which is why some people say ""never bet against JavaScript"". Even though technically you can also use Web Assembly but that's a lot more advanced

&#x200B;

4- Think of your database as an application separated from your website. So you deploying it is similar to how you deploy your website. In Heroku you would just create another application, and instead of making a web application you would make a database. After creating it you would receive credentials that allow you to connect to it from your web applications instead of connecting via localhost, once you're connected everything else stay the same.

Note : You could also use another host for your database. For exemple MongoDB (No-SQL) databases  can be hosted via MongoDB Atlas

5- It's just a business strategy, if they give you free tier you're going to learn how to use their services. Which mean that when you'll need to build a large application that can't run on the free tier, you'll be a lot more likely to choose their services. So they lose money on all free applications, but they more than make up for it with their paid tiers.",1,0,0,False,False,False,1644683618.0
squ1lg,hwny9td,t1_hwnr6qs,"Thanks for the reply man! Appreciate it, I’m just trying to fill in the holes in my brain since this has been bugging me for a while lol.

Just wondering, when is server-side rendering preferred over client-side with frontend frameworks? Perhaps just small applications I’m assuming?",1,0,0,False,False,False,1644685075.0
squ1lg,hwnxyhn,t1_hwnuolp,"Hey thanks for the reply! This definitely answered a good amount of my questions. Just a follow-up if you don’t mind:

1. So basically it seems that frontend frameworks *never* run on a server, but on the browser?

2. It seems that client-side rendering is the more popular option. In this case, would it be best to setup 2 Github repositories, one for the backend and one for the frontend? This seems weird to me, because in my mind a repository would contain everything needed for one project.

3. What’s the standard for deploying a database? I somehow doubt that people use Heroku for this, maybe I’m wrong. I can setup a local DB but I’ve no clue on which platform databases are usually deployed on.

4. Let’s say I deployed my React and Flask server. Locally, they communicate to each other via localhost:whatever_port_this_is. If my deployed React app is on myreactapp.com and my deployed Flask server is on myflaskapp.com, then I would just need to change the configurations from localhost to those URL’s, correct? And I’m assuming this is the same for the database on mydatabase.com?

Thanks!",2,0,0,False,False,False,1644684945.0
squ1lg,hwo12b4,t1_hwny9td,"The point of the framework is to mitigate server side rendering. Server side rendering causes (update * (api call + server calculations) * resolution) whereas frameworks can handle the dom updates on the device which allows for the reactive feel of web apps. It also reduces the complexity of caching on the frontend, managing security, and creating a cohesive development environment where every added tool built enhances the app. 

So I would only use server side html manipulations for little one off things that don’t fit into an available app project.",1,0,0,False,False,False,1644686217.0
squ1lg,hwo2s5j,t1_hwnxyhn,"No problem ! Your questions are really good

1- Well... Web development is complicated. Some people had the idea of combining Server-Side and client-side rendering for better performance. If you want to know more, look into Next.js for React. But for the most part, frontend frameworks never run on a server.

2- Not really, there are a lot of options. I'd say server-side is the traditional way of doing things, so most small websites use server-side rendering. For more information, you need to look into the advantages and drawbacks of the different methods

3- In most cases you're going to deploy to a cloud provider like AWS or Azure, I've used Heroku because it's a lot simpler, free, and I don't really have a lot of experience with cloud providers ! But there's nothing wrong with using Heroku for that if it's just a small application

4- That's the idea, your host would give you a connection string, and you would just need to insert your login info, it's pretty simple really.",1,0,0,False,False,False,1644686925.0
squ1lg,hwoczxn,t1_hwo12b4,"Gotcha, thanks man!",1,0,0,False,False,False,1644691083.0
squ1lg,hwod6im,t1_hwo2s5j,"Understood, this helped me understand things a lot better now. Much appreciated!",1,0,0,False,False,False,1644691158.0
sr0w0y,hwp2ksv,t3_sr0w0y,"You could look into Tungsten Fabric, previously called OpenContrail. I believe it's an open source project for building cloud infrastructure.",1,0,0,False,False,False,1644701503.0
sq9tve,hwk5uc1,t3_sq9tve,"For example I just found this: https://www.reddit.com/r/3Dprinting/comments/kp78ky/i_made_a_mechanical_xor_gate/

Which does technically meet my exacting requirements - only rod and pivot based XOR action. But it seems rather ungainly, with a *large* number of pivots which increases drag and (idk proper term here) ""signal noise"" from play in the components accumulating and producing partial states. All of my other gate designs have only used 2-5 pivots, if I wanted to make a XOR out of a dozen NORs linked together or something it would be easy enough, I'm just hoping theres a more graceful and efficient solution. I managed to implement a XOR using hydraulic logic but there's no way to really translate that to rods and pivots.",7,0,0,False,False,True,1644615060.0
sq9tve,hwmatyo,t3_sq9tve,"I know Steve mould has made a computer with containers and water, in a flat manner.  Don't know if it would help, but here you go.  https://youtu.be/IxXaizglscw  :)",2,0,0,False,False,False,1644652064.0
sq9tve,hwk85vv,t3_sq9tve,4 NAND gates?,1,0,0,False,False,False,1644615938.0
sq9tve,hwkgf1v,t3_sq9tve,I feel like a mechanical computer is doomed to fail. Even a super small computer would take both a ton of force to operate and a ton of space. And it doesn't seem very extensible: getting information from one place to another in electronic devices is easy 'cause you can always just run a wire between them. What's the mechanical analog to that? You would need to meticulously plan out the entire device beforehand in order to be sure all the connections you want can be made.,1,0,0,False,False,False,1644619115.0
sq9tve,hxxo79t,t3_sq9tve,"Hi, I could share this document with you, it describes Z1 the first ever mechanical computer built in 1938: https://drive.google.com/file/d/1LxIDkUH2qsfA7AXsP_RfXyLZGILGLvYg/view?usp=sharing - the document is quite interesting and educational.
Also regarding the XOR gate, quite recently I made a mechanical adder in a game called Besiege, you may find that interesting aswell because it has XOR gates: https://www.reddit.com/r/Besiege/comments/qonz6q/4bit_mechanical_adder/",1,0,0,False,False,False,1645511334.0
sq9tve,hwkt7qi,t3_sq9tve,nice,-3,0,0,False,False,False,1644624402.0
sq9tve,hwl0huj,t1_hwk5uc1,No gears allowed? D:,1,0,0,False,False,False,1644627611.0
sq9tve,hwk9drh,t1_hwk85vv,"I mean, yeah that will do it. I'm hoping for more of a specific device that inherently performs XOR or XNOR in one part rather than stringing together multiple others. I found one interesting example that uses a sort of rocker arm with a spring on it to create a preferred state but for reasons it would be too bulky and inefficient to implement",3,0,0,False,False,True,1644616395.0
sq9tve,hwkihu4,t1_hwkgf1v,"> I feel like a mechanical computer is doomed to fail

Funny because they are specifically used in situations where conventional computers cannot function such as in extreme radiation, temperature, or chemically active environments. A flexure based mechanical computer can be quickly and easily produced with a minimum of parts that all have nearly infinite duty cycles. Mechanical computers have the ability to store information indefinitely and have their memory read with no energy expenditure. 

>  Even a super small computer would take both a ton of force to operate and a ton of space

Nano-mechanical computers are an active field of research right now as they can be produced with individual atoms, essentially eliminating any possibility of wear and providing nearly frictionless actuation. Plus you know, being microscopic. We used mechanical computers all through WWII with basically zero issues aside from reasonable concerns like friction and inertia and component wear, obviously those are severe limitations but not for my concerns.

> And it doesn't seem very extensible

Why would it need to be? Have you never heard of those pocket mechanical calculators? Who tf would ever want to hook one of those up to a computer? Be realistic. What about slot machines or juke boxes or old arcade machines? All the operations they need to perform are based on internal states and adding optical sensors to detect states is downright trivial if they REALLY need to monitor things. 

> You would need to meticulously plan out the entire device beforehand in order to be sure all the connections you want can be made

This is how circuit design works yes, or are you saying you just start throwing random parts together and hope they work?

Look, you don't know my usage case or requirements, I don't need lectures about theoretical differences. I have a very simple and well-defined question that is realistic to expect a simple and well-defined answer for. 

YoU nEeD tO cOnSiDeR iF aLtErNaTe ApPrOaChEs WoUlD bE mOrE pRaCtIcAl

No. I have assessed the possibilities and settled on rod and pivot based designs for reasons that make them the absolute most efficient solution for my needs.",5,0,0,False,False,True,1644619939.0
sq9tve,hxy2gia,t1_hxxo79t,"That's a cool one thanks for the share, I'm always interested in computer education through games",1,0,0,False,False,True,1645522485.0
sq9tve,hwl1ed4,t1_hwl0huj,"I'd really rather not, they will be a severe bottleneck compared to the rest of the components in terms of production time, material usage, cost, computation speed, and power requirements/inertia/friction. Honestly pulling off all gates except XOR and XNOR using rods and pivots isn't bad, I suppose if I really need them a combination of simpler gates is still a lot more effective than gears",1,0,0,False,False,True,1644628026.0
sq9tve,hwkjwyc,t1_hwkihu4,Okay! I don't really know anything about the subject so I was just spitballing possible problems. I'd love to see the finished product 'cause I haven't the imagination/knowledge to see how it could work.,3,0,0,False,False,False,1644620504.0
sq9tve,hwklk9z,t1_hwkjwyc,"Sorry for snapping but this happens all the time on engineering subs, somebody asks a simple question that could be quickly and easily answered but instead everybody goes full StackExchange and starts attacking their premise and not providing information that actually answers the question. 

[Here's](https://youtu.be/rlkDIZxyTcc) a good example of a mechanical calculator, this one uses primarily gear driven rotary motion which is difficult to implement the way I need. Nobody makes stuff like this to run Doom.",4,0,0,False,False,True,1644621177.0
sq9tve,hwknoqg,t1_hwklk9z,"Haha, yeah, sorry for being *that guy*.

But would that be considered a computer? A calculator isn't Turing complete (according to my brief googling), so it would be more of a special-purpose machine than a computer. I'm not sure what your use case is though so maybe Turing completeness isn't relevant.",3,0,0,False,False,False,1644622053.0
sq9tve,hwkp24w,t1_hwknoqg,"Just for novelty and education purposes really so no, Turing completeness is not a requirement any time soon. Really I want to do the same stuff as /r/redstone basically, implementing basic computer hardware in an abstract way for the sake of education and pushing boundaries. Part of my unusual requirements come from the fact I am indeed implementing this logic device inside of a game using it's programmed rules, thus making certain real life realistic options infeasible while opening up options that wouldn't be possible in real life. Hence my heavy emphasis on rods and pivots - these have by far the lowest computational cost so devices made using this technique will be the fastest and most efficient. Using more conventional elements like gears and pinions adds an order of magnitude more processing time to calculate their behavior properly, I can easily make an XOR using a gear and some rigid elements but it has nearly 100 times more detail that must go into creating each individual tooth and making it strong and accurate, when all the other logic gates I've implemented have had maybe a dozen elements at most, not to mention the issue of trying to make a rotary based device function with all the other leverage based components.",1,0,0,False,False,True,1644622629.0
sq9tve,hwkp3ea,t1_hwkp24w,"Here's a sneak peek of /r/redstone using the [top posts](https://np.reddit.com/r/redstone/top/?sort=top&t=year) of the year!

\#1: [Figured I'd share my redstone craziness](https://v.redd.it/ptc4bevghys61) | [95 comments](https://np.reddit.com/r/redstone/comments/mq449x/figured_id_share_my_redstone_craziness/)  
\#2: [Redstone Block Only 3x3 TNT Door 9x5x14 = 630b](https://v.redd.it/qzl5lz3d0t281) | [80 comments](https://np.reddit.com/r/redstone/comments/r5yfqm/redstone_block_only_3x3_tnt_door_9x5x14_630b/)  
\#3: [I built a QR code generator in Minecraft with just redstone](https://v.redd.it/jzkkdvl38tp71) | [58 comments](https://np.reddit.com/r/redstone/comments/pvpzhn/i_built_a_qr_code_generator_in_minecraft_with/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",1,0,0,False,False,False,1644622643.0
sq0aef,hwie77a,t3_sq0aef,Have you every write or tried contributing to any emulator project? If not first write simple chip-8 emulator or 8080 or maybe 8086 emu and Ask questions about emulator on r/Emudev they will happily answer your questions.,4,0,0,False,False,False,1644591369.0
sq0aef,hwj6yn3,t3_sq0aef,"Patterson and Hennesy is a great resource for learning how computers work. You can also try to look up courses at universities, often their materials are all online (e.g., [UC Berkeley's cs61c](https://inst.eecs.berkeley.edu/~cs61c/fa21/)). That course has some material covering the riscv ISA and the basics of how that works. It also has a project where students implement an entire working CPU in digital logic in a few weeks (a super hard project, but doable). You could learn a lot by flipping through all that.

As far as writing an emulator, I think that's a great idea. RV32I (see my aside below) is really quite simple for an ISA, the entire thing fits on a single sheet of paper. If you look at the material from 61c I linked above, they identify an even more reduced reduced instruction set (lol). You really don't need much to have a working subset of RV (like maybe 10 instructions or something).

You'd be surprised how simple an emulator or CPU can be if you don't care how fast it is. Computers are fundamentally just a few things: Memory that stores information, registers that keep the information close to the CPU, and 3 core types of instructions. Instructions that read registers, change the data, and write it back to a register (like add or subtract). Instructions that move data between registers and memory (read/write). And instructions that decide which instruction to do next (branches and jumps). An emulator just has to have a way of representing those ideas. Memory can be just a big array, the register file is like an object or a struct, and instructions are just a big switch statement in a loop. Obviously there's more to figure out, but that's the basic structure.

(aside)

1: RV32I refers to a particular part of the RISC-V standard. The ISA is designed to be extensible, which means that there is a base ISA with very few instructions defined, and then extra extensions you can add. For example, there is an RV32IF for ""32 bits, integer and floating point"" or RV32G (""general"" which covers the minimal subset needed to boot an OS). It's all spelled out in the official manual.",4,0,0,False,False,False,1644602187.0
sq0aef,hwkxmf9,t3_sq0aef,"This series is amazing and does a wonderful job of explaining how to  code an RISC-V emulator.  

&#x200B;

https://www.youtube.com/watch?v=ER7h4ZTe19A",1,0,0,False,False,False,1644626327.0
sq0aef,hwipija,t1_hwie77a,6502 is also a nice intermediate between Chip8 and 8086.,1,0,0,False,False,False,1644595773.0
spnpvh,hwgdcg1,t3_spnpvh,"I recently went through The Innovators. It gives a pretty good tour of computer history by way of small biographies of various important figures. 

Also Hackers by Steven Levy is a classic.",8,0,0,False,False,False,1644547439.0
spnpvh,hwgtesm,t3_spnpvh,"For fiction stories that arguably contain computer science topics Jorge Luis Borges. I teach his books in a digital new media class. 

*Darwin Among the Machines* or *Turing's Cathedral* by George Dyson are great. Dyson has a nice poetic command of his prose.",3,0,0,False,False,False,1644555095.0
spnpvh,hwgygdz,t3_spnpvh,"Cliff Stoll's [*Cuckoo's Egg*](https://en.wikipedia.org/wiki/The_Cuckoo's_Egg_(book)) fun read of him hunting down an East German hacker after discovering a $.75 accounting difference

Also enjoyed [this](https://en.wikipedia.org/wiki/The_Thrilling_Adventures_of_Lovelace_and_Babbage) hilarious take on Babbage and Ada Lovelace's computer experiences",3,0,0,False,False,False,1644557919.0
spnpvh,hwh1api,t3_spnpvh,"Cryptonomicon by Neal Stephenson


It’s a fiction story about IT rebels loosely based on real history.",3,0,0,False,False,False,1644559679.0
spnpvh,hwhh5u2,t3_spnpvh,Algorithms to live by. Talks about different algorithms in an intuitive way. https://www.goodreads.com/book/show/25666050-algorithms-to-live-by,3,0,0,False,False,False,1644571396.0
spnpvh,hwgslxi,t3_spnpvh,The Unicorn Project is pretty good.,2,0,0,False,False,False,1644554673.0
spnpvh,hwgsuow,t3_spnpvh,"Xerox PARC didn't just invent the GUI, they invented the mouse and bitmaps. Also, ethernet.  
[https://www.amazon.com/Dealers-Lightning-Xerox-PARC-Computer/dp/0887309895](https://www.amazon.com/Dealers-Lightning-Xerox-PARC-Computer/dp/0887309895)

Integrated circuits were invented by guys in Texas and California at basically the same time and they didn't know it.  
[https://www.amazon.com/Chip-Americans-Invented-Microchip-Revolution/dp/0375758283](https://www.amazon.com/Chip-Americans-Invented-Microchip-Revolution/dp/0375758283)

...and there's this old-school legend  
[https://www.amazon.com/Difference-Engine-Charles-Babbage-Computer/dp/0670910201](https://www.amazon.com/Difference-Engine-Charles-Babbage-Computer/dp/0670910201)",2,0,1,False,False,False,1644554801.0
spnpvh,hwhk5sq,t3_spnpvh,Fire in the Valley for personal computing and folklore.org for apple/Macintosh stuff. Also the Linux documentary is good but about twenty years old.,2,0,0,False,False,False,1644573883.0
spnpvh,hwi9cyn,t3_spnpvh,"Fictional: Little Brother by Cory Doctorow

Great book on cybersecurity and terrorism

Non Fictional: OS’s 3 easy parts

Best way to learn low level systems",2,0,0,False,False,False,1644589348.0
spnpvh,hwhkp22,t3_spnpvh,"A deepness in the sky by vernor vinge. A scifi story about an advanced civilization's fleet trapped around a planet where they're basically inventing telecomunications and computers.

It also has a curious idea that at some point in the future all software one could want has mostly been written, so everything is free and open source software and there are computer archeologists who find the program that best suits the ship's needs and adapt them to run on the hardware (useful when the closest planet might be light-years away)",1,0,0,False,False,False,1644574322.0
spnpvh,hwi33rx,t3_spnpvh,"What about [Gödel, Escher, Bach: An Eternal Golden Braid](https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=asc_df_0465026567/?tag=hyprod-20&linkCode=df0&hvadid=343251570619&hvpos=&hvnetw=g&hvrand=11211500038460229909&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=9005925&hvtargid=pla-422923047250&psc=1&tag=&ref=&adgrpid=67797266623&hvpone=&hvptwo=&hvadid=343251570619&hvpos=&hvnetw=g&hvrand=11211500038460229909&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=9005925&hvtargid=pla-422923047250)?",1,0,0,False,False,False,1644586472.0
spnpvh,hwi5c0h,t3_spnpvh,"Check out this post:
https://www.reddit.com/r/computerscience/comments/k22z8s/comment/gds0z45/

My answer there

Here are some I listened to, I think they count as passive books:

CS/Computing - Algorithms to live by - Brian Christian, Tom Griffiths

Covers some algorithms and the problems you can solve with it

The Innovators - Walter Issacson
History of the computer Revolution

The pragmatic programmer - David Thomas, Andrew Hunt
Basic and practical career advice for developers

Devops - The Devops handbook - Gene Kim, Patrick Deboid

Devops guide

The Unicorn Project - Gene Kim
Story book illustrating problems Devops solves in an organization

AI

The master algorithm- Pedro Domingo
AI/ML professor who explains the ML landscape and a motivational search for the master algorithm (AGI). This book actually got me into the field when I was in an Applied Econ Masters program. Dropped out switched to Data Science right after.

Life 3.0 - Max Tegmark
Story book by the Mathematician and ML researcher it covers a lot of CS/AI stuff

Human Compatible - Stuart Russle
Coauthor of the foundational AI book: “AI a modern approach” in this he talks to AI researcher about the field in term of our responsibility

Tech in Finance (for devs working in finance like me)

Flashboys - Michael Lewis
HFT and founding of IEX exchange

Dark Pools - Scott Patterson
Covers HFT and modern Capital Markets technology landscape

The Man who solved the market - Steven Johnson
Covers the story of an AI hedge fund who as the title suggests solved the market. Renaissance Technology the highest return of any hedge fund in existence by a lot a lot",1,0,0,False,False,False,1644587539.0
spnpvh,hwimqmn,t3_spnpvh,[The Soul of a New Machine ](https://en.m.wikipedia.org/wiki/The_Soul_of_a_New_Machine) is a Pulitzer winning account of the race to build a successful minicomputer just as the PC age was starting.,1,0,0,False,False,False,1644594730.0
spnpvh,hwk6dhl,t3_spnpvh,"One less-known, but nevertheless good autobiographical novel would be *Just for Fun* by Linus Torvalds, where he describes the process of creating Linux
https://www.amazon.com/Just-Fun-Story-Accidental-Revolutionary/dp/0066620732",1,0,0,False,False,False,1644615260.0
spnpvh,hwkkjgp,t3_spnpvh,"I have read few fictional books which maybe slightly related to CS.

 1. Do Androids Dream of Electric Sheep:
It is a nice book. After a world war III earth is on a brink of destruction. People with talent and intelligence are sent to colonize other planets, such as Mars, while those who can’t pass the proper tests are left on Earth to eventually die. 

Because the war destroyed almost all animals, having a pet is the ultimate sign of luxury. 

 Furthermore, science has succeeded in building androids so realistic that it’s become virtually impossible to distinguish them from human beings. 


2. 1984:
 A dystopian novel about totalitarianism. Goverment conteolling the privacy of citizens, tracking their every move, media manipulation, total control and its classic line "" Big Brother is watching you .

3. Brave new world:
 The novel examines a futuristic society, called the World State, that revolves around science and efficiency. In this society, emotions and individuality are conditioned out of children at a young age, and there are no lasting relationships because “every one belongs to every one else”  Besides that children are created at Hatchery and Conditioning Centre, where children are created outside the womb and cloned in order to increase the population. The children are made if class as embryos . Alpha , betas and gammas.",1,0,0,False,False,False,1644620756.0
spnpvh,hwnw5a3,t3_spnpvh,"Depending on how tangentially related you can go while still counting as related:

Some novels by Neil Stephenson include CS topics. Someone mentioned Cryptonomicon which I haven't read. The Diamond Age also makes CS references, but that's not necessarily a central theme of the book, so it may not be what you're looking for.

The Cyberiad is a collection of short stories by Stanisław Lem. The stories take place in a hypothetical cybernetic universe with synthetic life forms, although that's really just a pretext. Each story is essentially a thought experiment. The actual themes of the stories are generally social, or about society and humans, and CS or mathematics are not a theme per se. Some of the thought experiments in the stories involve abstract mathematical themes, though, and there might even be some references to computation (I can't remember exactly to be honest). I think someone interested in the abstract parts of CS or maths might find it interesting, at least if one's intellectual interests are not limited to those. I did enjoy it, as a student.

Neither of those is really what you're looking for if you want things to be strictly CS-related, but if broader science fiction with maths- or CS-oriented thought experiments or themes sprinkled in counts, those come to mind.",1,0,0,False,False,False,1644684211.0
spnpvh,hwtnlak,t3_spnpvh,"Turing Annotated is a good one, it explains some of the foundation works in CS.",1,0,0,False,False,False,1644785309.0
spnpvh,hwi4ho4,t1_hwgygdz,Was just going to suggest The Cuckoo’s Egg too!,2,0,0,False,False,False,1644587141.0
spnpvh,hwhkfy0,t1_hwh1api,I was about to suggest this book,1,0,0,False,False,False,1644574115.0
spnpvh,hx7phvb,t1_hwi33rx,Very relevant answer. Amazing book.,1,0,0,False,False,False,1645041041.0
spnpvh,hwimrx9,t1_hwimqmn,"Desktop version of /u/RiverboatTurner's link: <https://en.wikipedia.org/wiki/The_Soul_of_a_New_Machine>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",1,0,0,False,False,False,1644594743.0
spnpvh,hwimsg9,t1_hwimqmn,"**[The Soul of a New Machine](https://en.m.wikipedia.org/wiki/The_Soul_of_a_New_Machine)** 
 
 >The Soul of a New Machine is a non-fiction book written by Tracy Kidder and published in 1981. It chronicles the experiences of a computer engineering team racing to design a next-generation computer at a blistering pace under tremendous pressure. The machine was launched in 1980 as the Data General Eclipse MV/8000. The book, whose author ""elevated it to a high level of narrative art"" is ""about real people working on a real computer for a real company,"" and it won the 1982 National Book Award for Nonfiction and a Pulitzer Prize for General Non-Fiction.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1644594749.0
spnpvh,hwitj2b,t1_hwi4ho4,ditto,2,0,0,False,False,False,1644597272.0
spwb7n,hwi9fqa,t3_spwb7n,The Coding Train’s p5.js series!,2,0,0,False,False,False,1644589379.0
spwb7n,hwjsfg0,t3_spwb7n,"Sentdex's Python Plays GTA V was fun and interesting. Probably my favorite coding video series I've ever fully watched.

&#x200B;

Stanford's cs 220 covers the theoretical aspect of ML which is a shit ton but its dont thoroughly and with a blackboard so i love it.",1,0,0,False,False,False,1644610031.0
sp7b3q,hwdgju6,t3_sp7b3q,"two important info to remember:

1. prim's algo is greedy, which means it always take the best candidate out of a pool of options. this is relevant for the cut property as we always take the edge with smallest cost out of those edges that connect a node that was already selected and a node still to select
2. a tree has the same nodes as the starting graph G={V,E}, just less edges

the thing about cuts is that they let you select node after node without worrying about  cycles - starting from whatever node J, your cut only contains J; J must be part of the MST, so we select the edge with minimum cost (this part should be clear). now the cut expansion: we add the node that was the other endpoint to the selected edge and we call it K: K is now part of our cut. this means that after this step we have a subtree made by the cut {J,K}.

next step: add another node with the above reasoning (we call it L). here is the trick: we now have a subtree T={J,K,L}, and since the cut only lets us select edges such that one endpoint is in T and the other is in V\\T, we will always avoid cycles as no further connection among nodes can be selected, these connections being edges fully inside the cut

&#x200B;

let me know if I was clear enough",10,0,0,False,False,False,1644505255.0
sp7b3q,hweysv2,t3_sp7b3q,"Finally I got it what he is trying to argue.  


Edge **e** is the only crossing the cut  <----- this point where i was confuse  


The point here is **T** has all the edges and **X** has all the vertices which are part of **T** and Most Importantly **T** spans all the vertices in **X**. If you look closely The edges which are in **T** all are inside **X** and there is no edge in **T** which crosses the cut i.e. **(X,V-X)**.  


When we added edge **e** to **T**, Then that is the only edge which is crossing the cut **(X,V-X)** at that particular Iteration and Accroding to the **Lonely Cut Property** addition of edge **e** doesn’t add any cycle.  


I am adding my finding feel free to comment ---> [https://imgur.com/a/GWH5JzC](https://imgur.com/a/GWH5JzC)",2,0,0,False,False,True,1644525075.0
spay93,hwe4uwe,t3_spay93,There are excellent courses available on Coursera from Princeton university and university of San Diego.,3,0,0,False,False,False,1644514286.0
spay93,hwfrlx8,t3_spay93,"if you want introductory books, A Common Sense Guide to Data Structures and Algorithms by Jay Wengrow as well as Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People by Aditya Bhargava are both easy high level texts to get your feet wet with big ideas before jumping into more concrete courses or denser books",3,0,0,False,False,False,1644537703.0
spay93,hwf35l7,t3_spay93,"You can find Algorithms 4th ed. by Robert Sedgwick in pdf for free. https://algs4.cs.princeton.edu/home/ is an online resource that you can also use as an additional learning aid that goes with the book. I’m sure there’s better resources, but this one’s free!",2,0,0,False,False,False,1644526649.0
spay93,hwjsowh,t3_spay93,"MIT has a few algorithms and data structures courses listed for free on their website mitopencourseware and on yotuube.

&#x200B;

Right now im going through their ""Introduction to algorithms"" which covers whats typical in a course about data structures and algorithms.",1,0,0,False,False,False,1644610131.0
spay93,hwojp69,t3_spay93,"I suggest you to learn step by step. So imo don't learn everything and then start practicing. 

For example learn binary search ( source of learning doesn't matter, it can be lecture video of some university, random youtube video or website such as geeksforgeeks), then solve problems related to binary search in LeetCode or some other website (e.g. [https://cses.fi/problemset/list/](https://cses.fi/problemset/list/) which I strongly recommend). Then learn dijkstra solve problems related to it, then learn dp practice problems etc. .",1,0,0,False,False,False,1644693830.0
soq68k,hwbayfs,t3_soq68k,"During the era when x86 rose to dominance, there were basically two strategies to compete. Intel sought to innovate in physics: fabrication at smaller nanoscales, and chips running at higher clock speeds. Their competitors - particularly Sun Microsystems - sought to innovate in logical efficiency: RISC architecture, instruction pipelines, branch prediction, etc.

Fabrication physics turned out to be the better bet. If you can make the smallest transistors, you can have your cake and eat it too: all the clever tricks enabled by RISC, *and* high clock speeds, *and* complex instructions (which, at the time, were still relevant for hand-assembled high performance applications, like games). This is how Intel won, and we ended up with the hybrid RISC/CISC design of modern x86.",25,0,0,False,False,False,1644460171.0
soq68k,hwafttr,t3_soq68k,"ARM is definitely taking market share from x86 right now, and I'm very hopeful for risc-v, which is rather promising, though much younger than arm.",24,0,0,False,False,False,1644447095.0
soq68k,hwcorvm,t3_soq68k,"To answer your question ARM based processors and SoCs already vastly outnumber Intel and AMD x86/x86-64 devices due to their ubiquity in mobile devices and embedded systems. The only exceptions to this are PCs and servers. In fact, both AMD and Intel have made and still make their own ARM based devices.

As for if or when x86-64 will be beaten in the PC and server markets nobody knows but Intel is spending huge amounts of money on the RISC-V architecture right now. I'm talking billions of dollars worth of investment so they might eventually push for RISC-V in the PC market which they currently dominate. 

Contrary to all the idiots who call it ""the Intel architecture"", x86-64 is designed and patented by AMD though based on Intel's 32-bit IA-32 (x86) architecture. The tables have turned and now Intel uses AMD64 i.e. x86-64 under a cross-licensing agreement with that company. Much more recently Intel is trying to become a contract foundry however they cannot offer their x86-64 hardware IP to their foundry customers because they don't directly own the patents for the instruction set architecture itself. If they focus on RISC-V which is owned by a non-profit organization that licenses it to everyone at no cost then they can develop and license hardware IP that implements that family of architectures to their foundry clients and life is good. 

That and the x86 family of instruction set architectures is frankly just terrible. Everything is inconsistent even the register naming for the exact same type of integer registers. Anyone who has had to write assembly for it knows that compared to that mess, ARM and RISC-V look downright beautiful and much more modern. If you think of an ISA as a very low level programming language implemented directly in hardware than an analogy to high level languages would be that x86-64 is like C++, old and with tons of crap tacked on in a hopeless attempt to keep it up to date with modern needs while RISC-V is like Rust, new and clean while implementing all the best ideas from years of research and concepts borrowed from all of its older competitors but in a clean, efficient, and coherent manner.",5,0,0,False,False,False,1644490134.0
soq68k,hwczyfa,t3_soq68k,"Of course there were and still are. Maybe you are thinking specifically of competing architechtures for desktop computers/end users, but even there Apple Silicon and Qualcomm's Snapdragon 8cx (ARM based) are real alternatives to x86-64 based chips.

Back in the day, there were also SUN's SPARC and Intel's own Itanium architectures which were viable for the desktop.",2,0,0,False,False,False,1644497635.0
soq68k,hwasd1j,t3_soq68k,"The 8080 was the precursor to the 8086 (the start of the 86 in x86), and it had a competitor that was the z80 - it was instruction set compatible with the 8080 but added more registers and instructions. It was quite successful and perhaps dominant in the 8-bit microprocessor space.

The 8086 / 8088 had competition with the Motorola 68000 line of processors. These were 16-bit processors that became 32-bit processors with the 80386 and 68020. Apple Macs run with the 68000, as did the Amiga and Atari ST. But the IBM PC was the dominant machine and software was not portable across these machines, so the software base essentially entrenched the x86 line with DOS/Windows as the operating system.

Apple, IBM and Motorola tried to compete again with the PowerPC processor, and Macs moved to that from the 68000. No-one else really did though and it fizzled.

Apple moved their Mac to the x86 which seemed like the end of the processor wars, but now ARM is rising from the embedded space and with Apple having a pretty fancy processor in their M1 line based on the ARM architecture, it may give Intel a run for their money.

Processing power per watt seems to be a primary measure that is driving competition as well as raw processing power. ARM has always done well with efficiency and with Apples (proprietary) efforts seems to have overtaken Intel on the laptop/desktop.

There is still the cloud computing market where both measures are important - efficiency is important when you are running so many machines with such a huge global workload. But absolute power is also important for those workloads that do not scale horizontally.

But if you can easily recompile your software for a different processor (which languages like Go make rather trivial in most cases), the processor becomes far less relevant.

Legacy is huge in computing, so it is not likely that x86 is going anywhere, but what usually happens is that the market outgrows the legacy and a new larger market runs with the new tech - mainframes used to be the bulk of computing - they still exist, but were supplanted by different modes of computation. Desktop/on-prem servers have grown into browers/cloud. Mobile has overtaken desktops. Kubernetes becomes the new OS, and supports multi-arch workloads. The processor architecture becomes far less important.",3,0,0,False,False,False,1644452217.0
soq68k,hwaueh7,t3_soq68k,"There were multiple different kinds of architecture back in the day. This was a massive pain in the ass as different hardware could have different sets of commands which meant you had to reprogram things for different processors. Some programming languages allowed you to compile things for different sets of instructions but that was also not consistent. Bugs would crop up, the instruction sets would change so you had to recompile again and hope for the best, and sometimes software would become completely unusable as the hardware to run it quit being made.

What happened was people realized that you could make IBM compatible hardware by using the same sets of machine instructions even if the actual hardware different somewhat. As long as the command set was the same and behaved the same everything was great. This was why x86 fundamentally became a thing; you didn't need to reprogram or recompile things for different architectures you could just compile it to x86 and leave it at that.",1,0,0,False,False,False,1644453090.0
soq68k,hwe64p2,t1_hwbayfs,Kind of ironic Intel now fell behind other manufactures because they got suck at 10nms,1,0,0,False,False,True,1644514749.0
soq68k,hwb5y92,t1_hwasd1j,"Very curious to see what happens with oxide.computer with its open source firmware and hubris, a microcontroller operating environment designed for deeply-embedded systems. Bringing on-prem hypervisors to the masses. It's x86 (AMD Milan) but I can envision them embracing RISC-V when that is ready.",1,0,0,False,False,False,1644458050.0
soq68k,hwc2oad,t1_hwasd1j,"Will be curious to see how Intel 12th gen with efficiency cores compare with the Mac Silicon chips. Based on Intel specs, the 12900HK beats out the M1 Max in computing power. We’ll see if that’s true in practice and how it does in efficiency.",1,0,0,False,False,False,1644473600.0
soq68k,hwe6b14,t1_hwc2oad,Nothings more exciting than true competition in the cpu space.,1,0,0,False,False,True,1644514813.0
sob87b,hw7qdyq,t3_sob87b,"As with all things, it takes patience and time to see how everything fits.

Usually you get a better, but far from perfect, picture by the time you do Algorithms, Data Structures, Computer Architecture, and Operating Systems or whatever equivalent courses you may take in college.

But a good book to start with is Computer Systems: A Programmer's Perspective & learning C.

(Link:https://www.amazon.com/Computer-Systems-Programmers-Perspective-3rd/dp/013409266X)

My Computer Systems Organizations courses uses some of the content from this book, and it all in all tries to bridge the gap between some of the high level abstractions and low level details.",29,0,0,False,False,False,1644407978.0
sob87b,hw7w50i,t3_sob87b,"I am going to recommend you five sources:

1. [https://htdp.org/](https://htdp.org/)
2. [https://dcic-world.org/](https://dcic-world.org/)
3. [https://www.plai.org/](https://www.plai.org/)
4. [https://csapp.cs.cmu.edu/](https://csapp.cs.cmu.edu/)
5. [https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2017/](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2017/)",21,0,1,False,False,False,1644411373.0
sob87b,hw7qnz6,t3_sob87b,"I'm the same, in that I won't be satisfied until I have a thorough and in-depth understanding of something. There's two things I do to deal with that:

1. Do gradual research into topics and questions surrounding my area of interest, and keep a Google drive folder to make notes. What's key is knowing which questions are good and which aren't, and I got better at that with time
2. Slowly learn to live with the fact that we can't be experts at everything in IT lest all our time is spent on it, and even then it's unlikely",10,0,0,False,False,False,1644408152.0
sob87b,hw82wzy,t3_sob87b,"You are on the right path. First Principles learning is one of the best ways to learn about a field. Although learning the fundamentals would take you at least some years but you would become a great software engineer. I have been on the same path and sometimes it feels like you are becoming a generalist and learning the fundamentals won't help much but actually, the fundamentals help a lot in making you a specialist in a field of your choice.

You asked for a course but IMO courses don't explain the fundamentals really well. It is curiosity that helps you learn the fundamentals. Some university courses can act as the starting point for learning the fundamentals but they won't teach you everything. The best resources for learning CS is often books, papers, and articles.

I can surely recommend certain books on some topics that would be beneficial for you.

- Compiler Engineering
   - Crafting interpreters by Nick
   - Other common compiler engineering textbooks(can't remember a name RN)

- Computer architecture
    - Learn about memory
    - Nand2Tetris

- Database Engineering
- Networking

You do not have to dive deep into a subject that you do not want to specialize in because these subjects are very in-depth that even after learning for 50 years, you haven't learned it all. I'd share a personal experience; so I was learning about how structs are laid out in memory and then I got into too much depth of memory alignment which kind of wasted some time(I spent a month reading about memory) but surely it was interesting and would help me somewhere in my career but it would have been better if I could have used that time to learn something else because you do not need to learn in-depth about something that you don't want to specialize in.",5,0,0,False,False,False,1644414760.0
sob87b,hw8v1nv,t3_sob87b,"There are two questions here.

One is ""how do computers work / how are computers organised"". And the other is ""how are programming languages implemented"".

For programming languages, read the book [Crafting Interpreters](http://craftinginterpreters.com/) by /u/munificent . It's free online, or you can buy a version.

For the other one, my stock answer is:

----

Can you answer the questions

* What *is* a computer?
* How do we build an electronic one?
* What *is* an operating system, and how are they made?

They look simple, but it's surprisingly difficult to give something more than a very trivial answer. From your post it sounds like that's what you're asking, basically. You want to know what the physical machine is doing, how it's controlled, and how the compiled executables that you write is somehow executed on it via an operating system.



If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

For operating systems, do what [teachyourselfcs](https://teachyourselfcs.com/#operating-systems) says and read any of these:

1. Andrew S. Tanenbaum - Modern Operating Systems
2. Silberschatz et al - Operating System Concepts
3. Operating Systems: Three Easy Pieces  (it's free!)

These will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc. Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board""). For os you can also hit up /r/osdev and the osdev wiki

Learning this stuff will make you much better programmer than if you didn't learn it, and you'll be better at debugging and solving problems you have whilst writing software, but fundamentally it'll also make programming much more satisfying as you'll understand every single part of the stack from electron to e.g. python.",3,0,0,False,False,False,1644426035.0
sob87b,hw8hoh4,t3_sob87b,Just speaking from experience once I understood how to program in assembly (as terrible as it was learning) and know what I was actually doing each time I worked with stack memory really elevated my understanding of what was going on “behind the curtains” of each programming language.,2,0,0,False,False,False,1644420990.0
sob87b,hw8aaqy,t3_sob87b,"Sounds like an issue arising from move/copy semantics and trying to reduces heap allocations (thus not having to do dynamic allocation, which can fail).

Is that obscure language inspired by rust? Or haskell? Look at resources for similar languages and learn how things work 1 level less abstracted. This should give you quick insight into the design decisions, which in turn allows you to understand the language better.",1,0,0,False,False,False,1644417997.0
sob87b,hw8nxp3,t3_sob87b,This book was incredibly helpful to me while trying to understanding some of the lower level concepts https://www.amazon.com/Secret-Life-Programs-Understand-Computers/dp/1593279701/ref=mp_s_a_1_3?crid=2X8CPHQ6JEKAT&keywords=secret+life+of+programs&qid=1644423291&sprefix=secret+lifr+of+pr%2Caps%2C107&sr=8-3,1,0,0,False,False,False,1644423403.0
sob87b,hw8wuow,t3_sob87b,Abstraction is your friend,1,0,0,False,False,False,1644426697.0
sob87b,hw9unbe,t3_sob87b,"you can just learn over time by using it an putting pieces together 

if you try to learn how a programming language is built from the ground up in one go its probably not gonna make sense",1,0,0,False,False,False,1644439181.0
sob87b,hwckl53,t3_sob87b,"Do the berkeley, CS61A, CS61B and CS61C.
CS61C will clear most of your doubts and will help lay the foundation you can learn on top by yourself.",1,0,0,False,False,False,1644486769.0
sob87b,hwd3uvl,t3_sob87b,"Go through the textbook on How to Design Programs Second Edition, it’s what I’m using at a university and is a great book to learn from. It’s free online and has hundreds of exercises using the dr.racket programming language which is used to learn coding. I’ve learned a lot through this book and I’m only on chapter 5 lol",1,0,0,False,False,False,1644499694.0
sob87b,hwddazl,t3_sob87b,"You need to trust the API. 

How to tell this thing to do something.

You drive because you know how to tell the car what to do. You don't invite everything about the car.

Learning the building blocks of the API is much much easier, you start with, ""what are the arguments?""",1,0,0,False,False,False,1644503929.0
sob87b,hzc1pm6,t3_sob87b,"Forget javascript learn python, bash, batch, html, c++ first",1,0,0,False,False,False,1646411926.0
sob87b,hwb7ais,t1_hw7w50i,Felt [watching](https://youtube.com/playlist?list=PLE18841CABEA24090) and [reading](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html) material from SICP (*Structure and Interpretation of Computer Programs*) very useful for seeing how to build everything from a very simple foundation.,5,0,0,False,False,False,1644458623.0
sob87b,hwbc4qi,t1_hw8wuow,Abstractions are actually the enemy while learning how things actually work :),0,0,0,False,False,False,1644460663.0
sob87b,hwcg6nl,t1_hwbc4qi,"Friends when constructing, foes when analyzing",2,0,0,False,False,False,1644483213.0
soiieq,hw8z1a7,t3_soiieq,"Explaining the whole route between your program and the actual bits in the ram is quite lengthy.

In a nutshell your program has a store instruction that stores a specific data into specific effective address, using both os and hardwere this address translate into a real physical address. 

The cpu knows how to translate store instruction into writing in the relevent address in the ram.",3,0,0,False,False,False,1644427491.0
soiieq,hwagfrw,t3_soiieq,"Software, at the very deepest level, *is* just high and low signals on hardware components. So software ""sending a high or low signal"" is essentially just signal being propogated on wires.",1,0,0,False,False,False,1644447332.0
sofw4w,hwcnym1,t3_sofw4w,"Only true, if all your types have power of two sizes. But if you consider things such as „long double“ which is a 10 byte type on some x86 compilers or other structs within the struct, it isn‘t true.",2,0,0,False,False,False,1644489483.0
soamn6,hw7ku7s,t3_soamn6,"Blockchain is about ""freezing"" data only, and it eventually can have a secondary use in any kind of software. However, it is an expensive way to store data too, wich limitates it's use in many ways.

That's it.

However, there is a discussion about PEER 2 PEER protocols, wich can  make possible other kind of applications besides blockchain.",3,0,0,False,False,False,1644404045.0
soamn6,hw7yavw,t3_soamn6,"If we look at present context you can store extremely small data in blockchains, but this limitation could may be someday breached by breakthrough advancement in hardware technology, or may be somewhat a new highly scalable consensus mechanism",1,0,0,False,False,False,1644412519.0
soamn6,hwhne2r,t3_soamn6,"I like that, also you might need to consider IPFS-ish (interplanetary filesystem, a p2p hypermedia protocol) for your virtual world. Maybe it can give you some idea.",1,0,0,False,False,False,1644576519.0
snoihm,hw3ouos,t3_snoihm,Operating System Concepts,29,0,0,False,False,False,1644339281.0
snoihm,hw45906,t3_snoihm,Modern Operating Systems by Tannenbaum is decent,15,0,0,False,False,False,1644345175.0
snoihm,hw3plpn,t3_snoihm,[deleted],37,0,0,False,False,False,1644339557.0
snoihm,hw66tqq,t3_snoihm,"Used OSTEP (Operating Systems: Three Easy Pieces) for my OS college class, it's free online just look up 'ostep' on Google. It's organized and is pretty good imo.",10,0,0,False,False,False,1644374031.0
snoihm,hw54guh,t3_snoihm,Design of the 4.4 BSD Operating System,6,0,0,False,False,False,1644358091.0
snoihm,hw3s717,t3_snoihm,"For me, this book was the best: [Operations System Concepts 10th edition](https://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz-ebook/dp/B07CVKH7BD). In my opinion, it has good examples and explanations, found interesting the way how they discuss the layers of OS.",10,0,0,False,False,False,1644340505.0
snoihm,hw4fpgk,t3_snoihm,[Rootkits: Subverting the Windows Kernel](https://www.amazon.com/gp/aw/d/0321294319/) - good for some practical knowledge,3,0,0,False,False,False,1644348989.0
snoihm,hw5o448,t3_snoihm,Tanenbaum,3,0,0,False,False,False,1644366062.0
snoihm,hw4132l,t3_snoihm,There’s a chapter in a Rust learning book called Rust in Action that has a mini OS project that is really useful for basic concepts.,5,0,0,False,False,False,1644343688.0
snoihm,hw4omrl,t3_snoihm,My os course is using [this](https://pages.cs.wisc.edu/~remzi/OSTEP/) book.,2,0,0,False,False,False,1644352282.0
snoihm,hw5gsq1,t3_snoihm,'How OS works' is pretty decent!,1,0,0,False,False,False,1644362986.0
snoihm,hw7sw8l,t3_snoihm,"Operating Systems: Three Easy Pieces

https://pages.cs.wisc.edu/~remzi/OSTEP/

IMO it’s far better than either of Tanenbaums books.",1,0,0,False,False,False,1644409531.0
snoihm,hwd3l7d,t3_snoihm,I used Operating System Concepts. This is quite good.,1,0,0,False,False,False,1644499559.0
snoihm,hw6ceq7,t1_hw3ouos,"Yup get the dinosaur book 🦖 it’s the bomb.

Link - Operating System Concepts https://www.amazon.com/dp/1119800366/ref=cm_sw_r_cp_api_glt_i_M4YNEQ434C893MV6YMN3",9,0,0,False,False,False,1644376392.0
snoihm,hw6tvcl,t1_hw3ouos,">Operating System Concepts

Were using the Operating System Concepts Essentials book in my OS class right now. Not sure if its different or not (same authors), but its great. Very easy to read and the author keeps it pretty interesting.",2,0,0,False,False,False,1644384792.0
snoihm,hw3zcrr,t1_hw3plpn,Thank you!,3,0,0,False,False,True,1644343068.0
snoihm,hw493nf,t1_hw3s717,"> Operating System Concepts

The Amazon version is incomplete. It's better to buy from [other sources like the publisher](https://www.wiley.com/en-us/Operating+System+Concepts%2C+10th+Edition-p-9781119320913).",4,0,0,False,False,False,1644346563.0
snoihm,hw3zbr1,t1_hw3s717,Thank you!,2,0,0,False,False,True,1644343059.0
snoihm,hw602re,t1_hw3zcrr,"OSTEP is the best! Super easy to read and digest, one of my favorite books",8,0,0,False,False,False,1644371177.0
so25ut,hw6qvjm,t3_so25ut,[deleted],5,0,0,False,False,False,1644383159.0
so25ut,hw6rx7s,t1_hw6qvjm,Wow this is awesome advice thank you!,3,0,0,False,False,True,1644383711.0
so25ut,hw7l1yp,t1_hw6qvjm,"This is very helpful, Thank you very much!",2,0,0,False,False,False,1644404211.0
sn86ck,hw1fvff,t3_sn86ck,"[Optimal Discrete Uniform Generation from Coin Flips, and Applications](https://arxiv.org/abs/1304.1916) seems to be what you're looking for.",14,0,0,False,False,False,1644293071.0
sn86ck,hw17794,t3_sn86ck,"Well look at the first number your code outputs. You take an uniformly distributed Integer of length ""br"" and take this integer modulo R. 

Since the range of that integer goes from 0 to 2^k and R is not a power of 2 means that 2^k mod R is always a number between 1 and R -1.

That means that the first 2^k mod R values of your integer will occurre more often than the others. 

Lets say you have random values from 0 to 15 (4 bit) and your R is 5, then you have 2 different ways to reach 1,2,3 and 4 mod 5 

but 3 different values that would reach 0 mod 5 (0, 5, 10, 15). Therefore 3 is more likely than the other values.",4,0,0,False,False,False,1644289092.0
sn86ck,hw14mru,t3_sn86ck,"I’m not sure I understand your code, but I believe that it has a slightly higher chance of producing certain outputs than others.",2,0,0,False,False,False,1644287960.0
sn86ck,hw15gjt,t3_sn86ck,"You can flip a coin k times and encode that as a bit string.  That string must have a value between 0 and 2^k (assuming no sign flag)

I’m confused what additional claim you are proposing here?",2,0,0,False,False,False,1644288327.0
sn86ck,hw1ckrc,t3_sn86ck,"The claim is true. The reasoning is as follows:

Since your algorithm is bounded-time, there is a bound on how many random bits it can generate. Let's call that number N. Now we modify the algorithm so that when it is about to output some random number, it will first continue to generate random bits until it has generated N random bits overall, and then it will output the random number it was going to output earlier. This modification gives it two nice properties:

\- Every single output made by the algorithm is after it has generated exactly N uniformly random bits.

\- The distribution of its output is the same as it was before the modification. Tossing useless random bits before outputting doesn't change the distribution.

So now if we are told the outcome of the N random bits, we know exactly what the algorithm will output. For it to be generating a uniformly random number from 1 to M, it must be the case that it outputs 1 in exactly (2\^N)/M settings of the random bits, it should output 2 in exactly (2\^N)/M settings of the random bits, and so on. So this is possible only when 2\^N is a multiple of M. Which necessitates that M be a power of 2.",2,0,0,False,False,False,1644291523.0
sn86ck,hw1dw9x,t3_sn86ck,"This is practically false.

Generate a random number from 0 to 2\^k-1 using k bits of uniform randomness.

Discard outputs higher than n where n is within \[0, 2\^k).

That's an algorithm. It has expected bounded time by the convergence of geometric series.",1,0,0,False,False,False,1644292132.0
sn86ck,hw2ong5,t3_sn86ck,"[Here](https://www.reddit.com/r/math/comments/smvbqz/is_there_a_way_to_get_random_numbers_from_any/hw00x6e/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3) is a formal mathematical proof, for those interested. It is similar to a less formal [proof](https://www.reddit.com/r/computerscience/comments/sn86ck/claim_no_boundedtime_algorithm_with_access_to/hw1ckrc/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3) by u/finedeaignvideos.",1,0,0,False,False,False,1644323539.0
sn86ck,hw5agjz,t3_sn86ck,"The plain english version is false.  You can use coin flips to generate a set of random positive integers within any given range.

However, you cannot use coin flips to generate a set of random positive integers **with equal probability** within any given range, unless that range is a power of 2.",1,0,0,False,False,False,1644360411.0
sn86ck,hw17xb9,t3_sn86ck,"I think I'm understanding what the confusion is here.

If you consider 8 coin flips, and write the result as bits (Heads 1s, Tails 0s, or reversed), you will generate a number between 0 and 2\^8 - 1 (63). The upper bound is a power of two.

What the original claim is saying is that your upper bound must be a power of 2 when using this method. Doing this as a single set of bits will not work if you want your upper bound to be not a power of 2, eg 51. You would not get a uniform distribution of numbers, as you need 8 bits but would generate numbers above the range.",0,0,0,False,False,False,1644289416.0
sn86ck,hw1hqj4,t1_hw1fvff,"I don't know how I forgot about this. I remember clearly techniques from years ago.  The algorithms in this paper are superbly fast.  However,  the lazy man's method is the following :


{snip}",1,0,0,False,False,True,1644293984.0
sn86ck,hw18efg,t1_hw17794,"Woops. You're right.  

{snip}",3,0,0,False,False,True,1644289629.0
sn86ck,hw1my5u,t1_hw1dw9x,"I'm the original commenter of the claim (which of course is substantiated by Turing award winners Knuth and Yao in their original 1976 paper but never mind that).  The claim is complexity theoretic in nature, and while practical discussions are nice, I want to preface it by saying any ""practical"", ""entropic"" or ""cryptographic"" discussion is tangential to the claim.

You are correct in that your rejection sampling algorithm terminates in expected time and will terminate in finite time with all but a measure 0 set probability. Two caveats: 1) it will only terminate in an a-priori specified finite time with 1-epsilon probability, where epsilon is something like 2^n, 2) it's squarely in the class of Las Vegas algorithms and not bounded-time, related to the complexity class ZPP: https://en.wikipedia.org/wiki/ZPP_(complexity)",3,0,0,False,False,False,1644296639.0
sn86ck,hw1ev3j,t1_hw1dw9x,This does have the consequence where your algorithm only almost always terminates.,2,0,0,False,False,False,1644292585.0
sn86ck,hw1eovb,t1_hw1dw9x,But not bounded time because you have a non-zero chance of producing no result and needing to restart.,1,0,0,False,False,False,1644292503.0
sn86ck,hw1jjil,t1_hw1hqj4,"This will not generate uniform distribution either. In CS that's usually not a concern, as most random algorithms don't have sufficient sources of entropy anyway.

But this is not a good algorithm if you want a uniform distribution, which is the claim in the title.",7,0,0,False,False,False,1644294875.0
sn86ck,hw1q4sw,t1_hw1hqj4,"> Generate 32 random bits. Convert encoded integer to double.

Double is typically 64 bits, so first off you're only covering a 2^32 / 2^64 which is less than 0.00000001% of all possible doubles.  But let's ignore that and pretend you meant something like a 32-bit float or something.

> The result is now in range [0.0 , 1.0).

You're forgetting about signed floats, but never mind that either. It sure is in that range, but it only covers discretely many points in that range.

>  Multiply that by your range, double(R+1), and floor the result to an int.

This is the easiest way to see where your counterexample is wrong: you have 2^32 possible input values and you have R (9000 in your case) output values.  ""Uniform"" means each output appears exactly the same number of times as any other output.

This means if you throw those 2^32 input balls into 9000 output bins, each bin must have the same number of balls.  But that's impossible because there's no way to evenly divide 9000 into 2^32 .",2,0,0,False,False,False,1644298415.0
sn86ck,hw199ks,t1_hw18efg,Sorry but i dont see any difference to when R is prime. Its still the same argument.,1,0,0,False,False,False,1644290020.0
sn86ck,hw1bu53,t1_hw18efg,In his example R=5 which is prime,1,0,0,False,False,False,1644291184.0
sn86ck,hw1f5tw,t1_hw1eovb,It's not deterministically bounded. But it's almost always bounded unless you're extremely unlucky.,0,0,0,False,False,False,1644292728.0
sn86ck,hw1oqyq,t1_hw1jjil,It seems non-uniform given that the subtraction step is somewhat of an amortized modulo operation during the rejection iterations.,1,0,0,False,False,False,1644297624.0
sn86ck,hw1sg20,t1_hw1q4sw,"
edit : { snip }",-2,0,0,False,False,True,1644299775.0
sn86ck,hw1jpbw,t1_hw1f5tw,"That's..... that's *exactly* what the ""bounded time"" restriction stops you from doing. This algorithm literally ignores half the requirements of the claim.",4,0,0,False,False,False,1644294955.0
sn86ck,hw1u90c,t1_hw1sg20,"> I have no idea why you mentioned signed here, or why that is ""forgotten"".

You're forgetting that when you divide a signed float by 2^32 you get something in the range (-1,1) not [0,1).  This is a minor point but it's good to be accurate.

> We can agree that h is uniformly distributed. 

Absolutely not.  A [double is represented](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) as 64 bits, so there are only 2^64 doubles in existence.  Are you telling me there are only 2^64 real numbers in [0,9000) ? Of course not!

> will get smeared out over the limit of generated random numbers.

No. Each input appears uniformly at random, so in the limit each input will appear, in expectation, an equal number of times.  The outputs however, are biased and in the limit the bias only gets worse, not better because the output bin is entirely determined by the input.

>  In the limit of generated numbers, you will indeed have equal numbers of balls in each of the 9000 bins. 

If you had 16 balls and tried to throw them into 5 bins, your bias might look like {4,3,3,3,3}.  Now you might say ""aha, next time maybe the 4 is biased towards another bin and get smeared out"".  This is where your logic is wrong.  Your algorithm is deterministic once you pick the random number and this is where it falls apart.  If you repeat the experiment, your problem exacerbates itself: next time it will be {8,6,6,6,6} and so forth.",3,0,0,False,False,False,1644300885.0
sn86ck,hw1k2er,t1_hw1jpbw,How often wouldn't it terminate?,1,0,0,False,False,False,1644295139.0
sn86ck,hw20fee,t1_hw1u90c,"After a few hours of thought, I have decided that the theorem in the title of this submission is correct.    Per your pigeon-hole example,  we cannot find a bounded-time algorithm.  The bounded time would entail that our computers store real numbers exactly. They do not.  Our computer store real numbers as floating point format, which are ultimately finite approximations.


However, having said that,  we can get ridiculously close to uniform distribution into 5 bins,  ""up to"" the precision of a floating point number.   While not mathematically uniform,  it is close enough to be used in any realistic scenario.",3,0,0,False,False,True,1644305069.0
sn86ck,hw1xxgd,t1_hw1u90c,[deleted],1,0,0,False,False,False,1644303314.0
sn86ck,hw1k59l,t1_hw1k2er,"Unless the answer is ""never"", it's not bounded time.",3,0,0,False,False,False,1644295181.0
sn86ck,hw21450,t1_hw20fee,"I'm honestly glad you were able to convince yourself, exploring and asking questions are part of the learning process.

I would have referred you to the original Knuth-Yao paper but appealing to authority isn't illustrative for anyone.

> However, having said that, we can get ridiculously close to uniform distribution into 5 bins, ""up to"" the precision of a floating point number. While not mathematically uniform, it is close enough to be used in any realistic scenario.

Yes, I agree! With 64 bits you'll probably burn out the sun before one can detect the bias. The one possible exception is cryptography because the numbers there are much larger than 64 bits.",2,0,0,False,False,False,1644305582.0
sn86ck,hw1xylf,t1_hw1xxgd,"**[Subnormal number](https://en.wikipedia.org/wiki/Subnormal_number)** 
 
 >In computer science, subnormal numbers are the subset of denormalized numbers (sometimes called denormals) that fill the underflow gap around zero in floating-point arithmetic. Any non-zero number with magnitude smaller than the smallest normal number is subnormal. Usage note: in some older documents (especially standards documents such as the initial releases of IEEE 754 and the C language), ""denormal"" is used to refer exclusively to subnormal numbers.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1644303335.0
sn86ck,hw1k93r,t1_hw1k59l,Is measure zero never?,1,0,0,False,False,False,1644295237.0
sn86ck,hw1kqp5,t1_hw1k93r,"Let me put it like this instead.

What is the Big O complexity of your algorithm? That is, for a given *n* range, what's the complexity formula?

No matter what complexity you give, it's possible that the actual execution time will exceed that, because it is never guaranteed to halt. 

There is no time complexity *bound* on the algorithm, thus it cannot be done in *bounded time*.

The algorithm you did give is the most efficient on average for this problem, but that's because the claim is correct: it's impossible to produce an algorithm that fits the requirements.",1,0,0,False,False,False,1644295487.0
sn86ck,hw1l0v3,t1_hw1kqp5,"Well, yeah on my algorithm. I believe the paper linked in another comment gives a bounded time algorithm for arbitrary n discrete uniform. But I could be wrong about it.",1,0,0,False,False,False,1644295632.0
sn86ck,hw1lghi,t1_hw1l0v3,"It does not produce uniform results.

It's not possible to create an algorithm that produces uniform results *and* is done in bounded time. In CS, usually the computers have problems with randomness besides our code (sources of entropy can be difficult), so for most programs most of the time, we're fine giving up the uniform distribution in order to make the algorithm bounded time, as execution time is usually a MUCH larger concern than uniform distribution.

This is really only (usually) a problem if you need to produce a cryptographically secure random number.",1,0,0,False,False,False,1644295855.0
snfpxr,hw2hqg8,t3_snfpxr,Quantum computing and computational physics.,8,0,0,False,False,False,1644318928.0
snfpxr,hw3g2nb,t3_snfpxr,Video Games are probably one of the biggest.,6,0,0,False,False,False,1644336017.0
snfpxr,hw39wyu,t3_snfpxr,"Robotics and process simulation (see Robot operations system, Gaziboism, and open ai gym)",4,0,0,False,False,False,1644333617.0
snfpxr,hw2n17l,t3_snfpxr,"Circuit design these days requires quite a bit of knowledge of quantum physics, now that we're building ""transistors"" that are one nanometer in size.",2,0,0,False,False,False,1644322559.0
snfpxr,hw3pt28,t1_hw2hqg8,Isn't that more computer engineering?,1,0,0,False,False,False,1644339630.0
snfpxr,hw4j2sa,t1_hw3pt28,"The physical implemention of a quantum computer is more oriented to computer engineering (and physics ofc) but all the theory, algorithm's and applications of quantum computing is more computer science and mathematics. I wrote more because is hard to put in just one field i.e bc is an interdisciplinary area by definition.",3,0,0,False,False,False,1644350241.0
snfpxr,hw3t7xm,t1_hw3pt28,"I don't think so?

At least, quantum computing was one of the courses offered in my comp sci program.",3,0,0,False,False,False,1644340878.0
snfpxr,hw4165i,t1_hw3t7xm,Yea ur right,1,0,0,False,False,False,1644343718.0
sndcd7,hw1x4s9,t3_sndcd7,"Do you mean computer death by destroyed hardware or death by deletion of all programming? Either way, yes.",15,0,0,False,False,False,1644302760.0
sndcd7,hw1yona,t3_sndcd7,"In short: yes. But probably not in the way you really mean.

Computers don't have any sort of agency. If you run a program that causes hardware damage or malfunction, it will cause that damage or malfunction. A virus is a program that you didn't intend to run, but it's still the computer running a program, and there's no sort of ""ego"" inside it making a judgement about what it should or shouldn't run (although, of course, nearly every sort of computer that most people are familiar with has some sort of ""permissions"" or other mechanism in the operating system, which is just yet another program governing how and when to execute other programs).

There are very few *truly* random things that happen to a computer - of course, nature can do strange things, especially in space, where the atmosphere and the earth's magnetic field don't protect computers from being bombarded by radiation that can affect their usual electrical function. Even the things that ""appear"" random are most frequently either based on measuring something natural (say, pointing a camera at a wall of lava lamps) or something not random at all, but complex enough to seem random, and statistically close enough for the purpose they serve. There are also *all sorts* of unintended things where a programmer meant for something different to happen than what their program actually causes to happen - this is never a matter of the computer exerting its will, but more about unintended consequences or gaps in the programmer's reasoning.

Finally, let's talk about what a computer can actually do to destroy itself: most normal operations accessible by software won't do hardware damage to a computer, but there are many very significant exceptions: software put in charge of moving parts, for example, can absolutely damage those moving parts (for example: the [Stuxnet virus](https://en.wikipedia.org/wiki/Stuxnet) caused Iranian nuclear equipment to spin themselves apart, quite literally). Many older gaming systems had far more close a connection to basic electrical hardware than newer, and it wasn't out of the question that a bad Game Boy game could damage the console - part of the certification process was examining the way developers addressed the memory mapped to display and sound hardware, for instance.

On a desktop or laptop computer, there's a huge amount of variety in the exact hardware, but numerous systems have limitations or assumptions that can be violated if a programmer is clever enough. It's possible, given enough time and access, to write sectors of a hard drive until they fail. It's possible to run some systems very hot until bad things happen, or to exploit defects in programs that control important things not intended to be exposed to external control by programs (for example: allowing users to execute program code on a person's computer by sending messages to the Minecraft server they're playing on - possible because of defects in popular logging functions in Java).

Some hardware can be controlled in such a way to render the machine unusable. They often call such a situation ""bricking"" a device - damaging it in some way beyond restoration, so it's as inert or useful as a brick. Destroying the firmware - very low-level programs and data used to control the most basic operation of a device - is one way to do such a thing, and usually, firmware has at least one way to be updated or overwritten.

So: if you'd intended to ask if a computer can become suicidal, the answer is definitely no - but software written to intentionally or unintentionally damage a machine can absolutely cause anything from inconvenience to irrecoverable physical damage, and there are many motives for a *person* to create it (incompetence, the challenge of getting around the ""fences,"" state-sponsored terror, national security, police work, you name it), but computers have no motives of their own any more than a dropped object has a motive for falling to the ground.

>I'm not bad. I'm just drawn that way.",8,0,0,False,False,False,1644303831.0
sndcd7,hw298rz,t3_sndcd7,"Sudo rm -r /

Bye bye everything.",3,0,0,False,False,False,1644312030.0
sndcd7,hw206sw,t3_sndcd7,"To the very good answers that have been given, I also want highlight that it is very rare (but indeed happens) thwt that a conputer will be damaged beyond total repair. What can happen is that it won't be easily fixable by a consumer (but again, real bricking does not exists).

For pure software things like wiping the OS a new OS can be installed (but the data may be lost forever). If you pummel the same sector of a hard drive until it becomes unreadable you can swap for a new hard drive. Sometimes though, the hard drive is soldered to the motherboard so you need special equipment and expertise to replace it. Likewise with a corrupt firmware you can flash a new firmware externally. So I would see all the above as bot ""dead"" (in your analogy) but hurt enough that you need to go to the ICU. But of course has other have said if you manage to make it overheat or spin the moving parts too fast, you may damage it in ways that repairing it amounts to building a brand new one. So more like the old damaged computer being an organ donor to the new one ? 

Note: I just found the thought experiment interesting and wanted to see if the analogy could be pushed further. As others have said, all of these are created by an external intervention running the damaging program (can be a virus, or a bug in the OS, ...).",2,0,0,False,False,False,1644304897.0
sndcd7,hw1yi8h,t3_sndcd7,"You mention computers “ARE NOT a human brain” and you’re entirely correct they aren’t.  All a computer can do is execute tasks that a person has programmed it to do. A computer doesnt have free will to decide it’d no longer like to keep existing. Therefor if a computer were to stop existing/functioning suddenly it would be the result human intervention or some hardware malfunction. 

If a person wanted have their(or someone else’s) machine cease to exist in the software sense all that they would need to do is delete the operating software for the machine. As aforementioned computers can only do what people program them to do this is what operating software(OS) is; software giving the machines components (hardware) instructions, so without the OS the machine has no “brain”.",2,0,0,False,False,False,1644303708.0
sndcd7,hw1x79l,t1_hw1x4s9,By deleting all programming! Could you tell me a little about how it works?,1,0,0,False,False,True,1644302809.0
sndcd7,hw1yq01,t1_hw1yona,"**[Stuxnet](https://en.wikipedia.org/wiki/Stuxnet)** 
 
 >Stuxnet is a malicious computer worm first uncovered in 2010 and thought to have been in development since at least 2005. Stuxnet targets supervisory control and data acquisition (SCADA) systems and is believed to be responsible for causing substantial damage to the nuclear program of Iran. Although neither country has openly admitted responsibility, the worm is widely understood to be a cyberweapon built jointly by the United States and Israel in a collaborative effort known as Operation Olympic Games.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1644303857.0
sndcd7,hw3dc40,t1_hw206sw,"Some computers like the microcontrollers in many types of smart cards have what are called ""fuse bits"" in their EEPROMS.  For example, the first 2k of addressable memory may be EEPROM that can be re-flashed to provide various types of software updates.  However, the first byte of this EEPROM might be part of the bootloader sequence and contain write-once ""fuse bits"" which once written, can never be changed back to their original state.  Often times, there will be routines (or ""traps"" if you will) to thwart hackers and write to these ""fuse bits"" if malicious activity is detected.  You would be hard pressed to ever repair a computer that has been damaged in such a way back to a working state.  Some SoC's like certain ARM based CPU's also can contain this type of circuitry.  When you combine this kind of circuitry with cryptographic keys, you can effectively ensure that these components can never be replaced.",2,0,0,False,False,False,1644334968.0
sndcd7,hw2bpd7,t1_hw1x79l,"bash:

cd /
    
sudo rm -rf *",14,0,0,False,False,False,1644314112.0
sndcd7,hw223h9,t1_hw1x79l,corrupt your windows system32 folder somehow (deletion or a virus that does bad things to it) and you're computer will already be in quite a pickle. It wouldn't be completely wiped though and the hardware will all still work. For the computer to mess with it's hardware a really complicated virus would have to be run that may mess with the voltages. that's what i think,4,0,0,False,False,False,1644306311.0
sndcd7,hw3w9u4,t1_hw1x79l,The two comments are a Linux script that could be put into a command line to delete all software.,1,0,0,False,False,False,1644341975.0
sndcd7,hwfethw,t1_hw1x79l,"Generally speaking, any software can delete data or information that it has access to, often including itself.

A computer system consists of *hardware* and *software*. There are some grey areas, but generally speaking, the hardware is the physical electrical circuitry or machinery that can store information and perform computational operations on it. In a desktop or laptop computer, this would include the processor (a.k.a. CPU for Central Processing Unit), the working memory used for keeping information that's needed by currently running programs (commonly referred to as RAM), and the permanent storage used for keeping information and programming in the longer term.

A practical computer you're familiar with would also include a lot of supporting hardware that supplies power, coordinates communication between the CPU and the memory, and for all kinds of auxiliary tasks. Those are not fundamentally as crucial for computation, although they're practically important for the tasks we use our laptop or desktop computers for.

Individual operations that the circuitry, or the hardware, is capable of performing would include things like numerical addition, subtraction, multiplication and division, and a number of other arithmetic or logical operations. A computer processor's *instruction set* would also include operations for loading a particular number or piece of data from the computer's memory so that those arithmetic operations can be done on it, and for storing the result of a computation back into the memory.

Present-day computers have dedicated hardware for more complex individual operations as well, for reasons of performance and efficiency, but it's theoretically possible to build a fully programmable computer with fairly basic individual operations.

The programming, or *what* the information processing procedure that should be performed on some particular information is, is defined by the software, or by the programming. (The programming, or the rules of processing the information, could also be directly done as hard-wired circuitry, but as the name implies, software is more flexible; software programming can be changed afterwards without changing the physical circuits. That flexibility is why software exists separately from the hardware in the first place.)

Any programming that's more complex than the basic operations is built from a series of such fairly basic operations. Even in an application that downloads video data from YouTube and renders it on your display has just got an awful lot of those basic additions, multiplications, other arithmetic or logical operation, and memory loads and stores.

All of the data **and software programming** on a computer resides on its storage media. (That's may actually not be strictly true in real computers, as there can be some programming that's been built more tightly into the hardware itself and is not as easily erased or overwritten, but I'm making some shortcuts here for simplicity.) For example, all of your documents and files, but also all of your applications, reside on the storage device of your computer. The storage device used for permanent or long-term storage is usually either a hard disk drive or some kind of a solid-state memory. The storage device isn't really fundamentally important here, though. What's important is that the programming resides in the same memory with the *data*, or the information that the programming is meant to process or manipulate.

Erasing data -- all the ones and zeroes -- on the storage device, or overwriting it with random data or with all zeroes (effectively a blank), isn't really a complicated operation, computationally speaking.

A very simple computer program that allows you to add two numbers together might consist of the following operations, approximately:

    read input number from user, store it in location A
    read input number from user, store it in location B
    add A to B, store result in location A
    output number stored in location A

A program could also, almost as as simply, consist of the following operations:

    get location of the start of one's own program code, store it in location A
    let m be A

    place a marker named ""loop"" here
    
        store value zero at location m
        increment m by one memory location, i.e. let m be m + 1

        if m is still within the program code, go to marker ""loop"";
        otherwise, stop the program.

That's not programming in any actual programming language, but it could easily be written in many of them.

That kind of a program would start from the beginning of the program's own program code (perhaps in the long-term storage), go through the programming instructions one by one, and erase each of them until reaching the end of the program code.

As far as the laptop on your desk goes, modern operating systems have security precautions that prevent applications, or programs that are not the operating system itself, from accessing, modifying or erasing programming or information that belongs to the operating system. An individual application is thus not practically able to remove the operating system, or other applications, or all of the programming on the computer if you don't give it permission to.

Fundamentally speaking, though, a program can alter information in the memory; the program's own code is in the memory; ergo, a program can be written to erase itself. If it weren't for the security precautions built into today's operating systems and hardware, there'd be nothing that would prevent *any* program on the computer from erasing all of the programming on the computer.

This may be a bit of an anticlimax for you, but on a fundamental (rather than everyday practical) level it's essentially trivial to write a computer program which, when the program is run, destroys itself, along with all other data and programming on the computer.

In practical computers, since *some* of the rather fundamental programming of the computer, such as [the program code that looks for an operating system or other programming in the first place when you start the computer](https://en.wikipedia.org/wiki/Booting), typically resides in a dedicated storage outside of the ""normal"" memory of the computer accessible by normal programs, some of that programming is practically not as easily erased. It's still possible, though, and if you look at computers as less of a physical laptop or desktop and more as a philosophical thing, there are no such restrictions.

It's important to note what u/lneutral wrote in their excellent comment, though: computers or programs don't have any sort of agency. They merely follow the instructions given in the programming, entirely mechanically.",1,0,0,False,False,False,1644531292.0
sndcd7,hw21jkc,t1_hw1yq01,"I wouldn't say stuxnet destroyed computers as we understand them, rather very specialized PLCs. 

On topic I'd guess malicious microcode updates could potentially brick a CPUs function.",2,0,0,False,False,False,1644305897.0
sndcd7,hw3hiv9,t1_hw3dc40,Yes. This is also the case for Samsung phones (or at least used to be) with Knox and it's e-fuse. A write once only bit in hardware that can be written to by the bootloader when it detects a non official ROM being flashed or similar. Once this bit is set some functionality (like Samsung Knox Workspace) cannot be used anymore.,1,0,0,False,False,False,1644336567.0
sndcd7,hw2w5ou,t1_hw2bpd7,    --no-preserve-root,6,0,0,False,False,False,1644327557.0
sndcd7,hw25rp1,t1_hw223h9,"There are a myriad of ways to permanently brick a system though. Any corruption of a BIOS will kill most PCs (though some can flash a backup). Also, tampering of the kernel by a virus or corruption will also be very bad. So software death is pretty easy to achieve. Even just encrypting everything,like a ransomware attack, is a thing.

And the computer messing with hardware is rare these days but not impossible. As noted with the New World bug that killed graphics cards, it's usually a bug that does it. Wouldn't be impossible to find a bug in a chipset or driver though and exploit it to kill hardware, it's just usually rare and patched quickly. So you'd have to be trying to kill your PC to do that. Might as well just use a hammer though.",5,0,0,False,False,False,1644309149.0
sndcd7,hwfeu81,t1_hwfethw,"**[Booting](https://en.wikipedia.org/wiki/Booting)** 
 
 >In computing, booting is the process of starting a computer. It can be initiated by hardware such as a button press, or by a software command. After it is switched on, a computer's central processing unit (CPU) has no software in its main memory, so some process must load software into memory before it can be executed. This may be done by hardware or firmware in the CPU, or by a separate processor in the computer system.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1644531309.0
sndcd7,hw9z9sk,t1_hw21jkc,"It's hairy, right? At what point do we consider hardware damage a computer destroying itself, after all? The distinction is a little less ELI5, but you're totally right.

There are all sorts of kinds of self-destructive machines with computers in them, including spy aircraft with devices that ruin the electronics to prevent capture, arcade machines that require encryption tables to function that commit ""suicide"" when their batteries run down, even art exhibits designed to become inoperable after a single exhibition scheduled for the far future.

And of course, that's only speaking of systems with digital, electronic hardware for computing. Strictly speaking, many physical and abstract phenomena can be used to simulate a Turing machine: water plumbed through carefully-constructed vessels, virtual circuitry in Minecraft, ball-bearings rolling through systems of levers, or proteins designed to act as logic gates. All of these could, conceivably, simulate a program that leaves their underlying system in an irretrievable ""end state"" where further computation is impossible.

It's relatively easy to come to the conclusion that human behavior could be seen as ""software, but more complex."" It's a little ironic then, that there are also so many ways for something capable of computation to destroy itself that, in a way, computers are capable of ""suicide, but more complex.""",2,0,0,False,False,False,1644440849.0
sndcd7,hwki509,t1_hw2w5ou,"You actually don't need that when using `*`, I think.",1,0,0,False,False,False,1644619798.0
sndnjm,hw20xjf,t3_sndnjm,"It means just what it says. A hypothetical computer. This is theory, not a nuts and bolts system text explaining what you normally think of as a ""virtual machine."" Nothing to do with PC VMs. 

They are talking about making a theoretical machine M1 with a machine language of L1 and how it might compare to an M0 computer using a machine language L0.

Edit: key line at the end. ""People can write software for virtual machines just as though they really existed."" Obviously PC VMs do exist.",1,0,0,False,False,False,1644305444.0
sndnjm,hw27z39,t3_sndnjm,"Imaginary machine, one you can believe exists, to set context for the lesson.",1,0,0,False,False,False,1644310954.0
sndnjm,hw1xzu8,t3_sndnjm,[deleted],0,0,0,False,False,False,1644303359.0
sndnjm,hw28gs5,t1_hw20xjf,Is it ever going to make sense and I am only 14 I just started getting an interest in entrypoint because I am starting to get confused,0,0,0,False,False,True,1644311367.0
sndnjm,hw1zsgl,t1_hw1xzu8,Yeah it keeps on showing virtual machine as in running it on a PC,0,0,0,False,False,True,1644304618.0
sndnjm,hw2v2qd,t1_hw28gs5,"ELI5 - Computers have commands to move data between cpu registers, memory, and hardware 

ELI10 - Those commands are called assembly (or machine) language. You typically generate them from a higher languages compilation (eg c++ to assembly)

ELI 15 - machine language is computer architecture specific. This is why an ARM64 application doesn’t “natively execute” on AMDx64 processors. We solve these “portability” constraints by compiling our apps multiple times into multiple distinct architectures

Eli 20 - Mobile devices (and similar trends) make it challenging to build for every processor in advance. Instead you compile for a “logical hardware specification “ called a virtual machine. The specification (eg MSIL or ByteCode) has the same operations as “physical hardware” (eg push stack, move memory, etc)

ELI 25 - Your language runtime (eg Java or NetFx) use just in time translation to convert the VM assembly to platform specific assembly. This gives allows the final step to happen at the device which can optimize for itself 

ELI 30 - these just in time translations can logically recurse from language to language. This is how eg cobalt mainframes move to the cloud. L1 is Cobalt l2 is Java l3 is bytecode l4 is x64 assembly.",1,0,0,False,False,False,1644327021.0
sndnjm,hw3siq2,t1_hw28gs5,"Are you taking a class or doing independent study? What book are you reading? 

If you are taking a class, talk to your teacher. That's what they are there for! If you are learning on your own, you might try a different source. What are you trying to figure out?

And, yes, it gets easier!",1,0,0,False,False,False,1644340626.0
sndnjm,hw4n00h,t1_hw3siq2,It started by the new jailbreak for the PS4 and then me not knowing what an entrypoint is and I started reading a book and watching YouTube clips,1,0,0,False,False,True,1644351687.0
snauwt,hw1nnn3,t3_snauwt,"If Sigma is a set then Sigma^2 usually means the set of all pairs of elements of Sigma. So in this case {(0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), (2,1), (2,2)}.",4,0,0,False,False,False,1644297019.0
snauwt,hw27sbs,t3_snauwt,It is the [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product) of the set.,5,0,0,False,False,False,1644310795.0
snauwt,hw31ltb,t1_hw1nnn3,Thank you so much,3,0,0,False,False,True,1644330104.0
snauwt,hw31le6,t1_hw27sbs,I see. Thank you so much.,2,0,0,False,False,True,1644330099.0
sn8ppi,hw17h43,t3_sn8ppi,"Well, you can count the number of devices connected to the wifi network in an area. That'll be most people's phones, open laptops, tablets, maybe smart watches? You'll also get the MAC addresses of nearby routers. It seems likely that the number of MAC addresses will _scale_ with the number of people in an area, and that might be sufficient for applications like measuring higher- and lower-traffic regions of campus, but getting more than a rough people-estimate sounds tricky.",6,0,0,False,False,False,1644289215.0
sn8ppi,hw3aevy,t3_sn8ppi,"Better approach is use an ip camera and opencv 

https://medium.com/analytics-vidhya/face-detection-on-recorded-videos-using-opencv-in-python-windows-and-macos-407635c699",1,0,0,False,False,False,1644333811.0
sn8ppi,hw70pb2,t1_hw17h43,"Thanks for the response, I guess I'll just go with my second option of using computer vision",1,0,0,False,False,True,1644388888.0
sn8ppi,hw70xdh,t1_hw3aevy,"Thanks! that's actually my second option, but I'm a bit new to computer vision :D but I'll go ahead and give it a try",1,0,0,False,False,True,1644389038.0
sn8ppi,hw7c7jj,t1_hw70pb2,"Good luck! That sounds much more sophisticated, but at least it’s directly observing what you’re interested in.",1,0,0,False,False,False,1644397245.0
smanmb,hvvu2b8,t3_smanmb,"Learning an engineering tool is like getting a year-end bonus; learning CS theory is like getting a raise.

Practical skills are certainly helpful, but they're only temporary. When I was in college, git/svn didn't exist—and a lot of the tools we used then are now obsolete.

Meanwhile, much of the theory we learned is useful year after year. And things I learned in math class (multivariable calculus, linear algebra, etc.) have turned out to be far more relevant than I ever guessed.",74,0,0,False,False,False,1644195328.0
smanmb,hvvhzsi,t3_smanmb,That's because it was Computer Science degree and not Software Engineering nor Programming degree.,91,0,0,False,False,False,1644189932.0
smanmb,hvvsfz7,t3_smanmb,"For me, (currently going through school, after programming for a while), that theory, algorithms, and data structures are huge fundamentals of programming. Anyone can learn a language, but knowing how all these things work I think strengthen your ability to grasp and use complex ideas in all programming languages. So personally I agree with the theory-based approaches.",39,0,0,False,False,False,1644194596.0
smanmb,hvvtonc,t3_smanmb,"Those things change rapidly, and aren’t as important as algorithms and understanding computers - any good programmer could trivially learn git or some debugging tool quickly given that they understood the said things before",17,0,0,False,False,False,1644195156.0
smanmb,hvvyxar,t3_smanmb,"Problem with skills. The market changes over time because those are tools. They may not change over night, but they do change. Also, company A may favor tool X while company C may use tool C which is a in house tool. A majority of colleges try to stay neutral when it comes to  learning tools in the field. However, I did have professors that me had me write reports on building a project from design to production and ending with a presentation/demonstration on the project in front of the class.

Writing reports had the students explained on why they made or use tool A over Tool Z, i.e. some groups had conflicts over which library and language to use.",6,0,0,False,False,False,1644197559.0
smanmb,hvyt2ae,t3_smanmb,"I don't care one whit whether you ever write a single line of code in college. I want you smart. I want you to learn in college the fundamentals and hard things that you ought to know and understand if you are going to call yourself a computer scientist or engineer. Language, tools, and business practices are industry details I can teach you on the job. If I wanted a developer who already knew how to do the job I'd hire a seasoned professional with prior experience. When I hire a junior, I'm going to press you like play dough into a mold. The more you know about Git and Python and CI/CD means the less you know about mathematics and critical thinking. I can teach you to be a good developer, I can't teach you to how to think or solve problems - and writing code isn't a problem, it's a solution, an implementation detail, the problem was what was missing that had to be solved before you ever sat down at the editor. Any fucking idiot can be taught to press the keys on the keyboard.

How your candidate employers interview speaks volumes about their caliber. I've found the amount you talk about programming, or what problems the company specifically is trying to solve, is inversely proportional to how much you should want to work there. The places that focused the most of programming problems were the most miserable places, people, and grinds I've ever been, both as junior or senior. The places were we didn't talk about programming AT ALL, and not even about anything the company actually did were the most amazing and enriching experiences I've ever had Those places invest in their people, they want you to be as smart as possible. They are rare gems who don't often make a lot of noise, so the few I've found I had stumbled across while interviewing.

As a senior, I don't do programming homework or quizzes. I was far more willing to put up with shit when I was younger and wanted to land the next position and keep the income flowing, but at this point if you have to ask me to write a little demo thing, there is too much wrong with you that I don't want to work for you or your employer. You don't hire a senior on whether or not they can write a linked list or reverse a string, but on fit for your team.

Coming back around, don't stress about learning practical skills. You're not in a vocational school, you didn't go to school to become a mere factory line man. Juniors bring to the table things that seniors don't. You don't hire juniors because you can't afford a team of all seniors. Seniors are jaded and opinionated, juniors don't know anything and have no opinions, and that's a huge deal. You're at your most flexible and at your most potential at the beginning of your career. Your few seniors direct their many juniors and the juniors grow, and the product develops as a result of this collaboration. You can learn and adapt and produce like a senior can't, but the senior has wisdom and insight that empowers and enables you. A senior is expected, that when they fire, they hit 100% of the time. A senior can write crisp code, where they apply themselves, and especially along the lines where their careers have gone and specialized them, but only the best senior talent in our entire industry is innovative. Most of the innovation comes from juniors, but again, often out of a collaboration.

If a place can't be bothered to teach you Git, or their language of choice, then being turned down is doing you a favor. Leave that position for someone beneath you who WANTS to be nothing more than a line man, cranking out business logic for a mature and proprietary pile of bugs. With some self respect and the proper focus, you can do better and get out of the industry as much as you put in. You can not think and just crank, or you can aim as high as you can and see where you land.",4,0,0,False,False,False,1644254064.0
smanmb,hvw4qv9,t3_smanmb,"so here in Canada we have a distinction between colleges and universities.  Colleges award a diploma and tend to be very focused on practical skills where universities award degrees and are more focused on theory and academia.

when I was deciding what school to go to I looked at the courses being taught and factored that into my decision making.  I looked at software engineering degrees at universities and I looked at computer science diplomas at colleges.  I ended up choosing a college diploma in computer science and honestly very thankful for this decision.  Even had a buddy from high school who attended first year at a university for software engineering then decided to switch over to the comp sci at college after we chatted and compared what we were studying.

The comp sci program I went through was 6 semesters of intense study + 3 semesters of work placement, and had something like a 20% graduation rate as many people quickly found that programming wasn't for them (also it was a very intense program).  We studied everything from hardware, assembler language, algorithms (big O), data structures, design patterns, operating systems (ex. coding patches on linux kernel), compilers (dragon book as the textbook), real-time programming (for hardware controllers, etc - the electrical engineering crew also attended these classes), graphics programming (openGL but didn't get into directX), networking infrastructure and protocols (including some good stuff like using netcat to send emails or make HTTP requests), database design, SDLC and project planning (scoping, documentation, diagrams), and of course a hell of a lot of coding - mostly C, C++, and Java with a bit of BASH scripting included for good measure and good knowledge to have.  SVN was still pretty new at the time (and git was brand spankin new) so it was mentioned but not formally taught in class, though some of us did choose to use it on team projects.  Code style, structure, maintainability, extensibility, etc were all things we were expected to build good habits for and were graded on in some cases.  Many of our technical professors had solid real-world experience so they knew what practical knowledge we needed to learn.

In the city where my college is there are also two universities but employers (in both public and private sector) prefer hiring comp sci graduates from the college because they have the practical knowledge to start delivering results right away.  I graduated quite a while ago and over the years haven't had much trouble finding employment or putting my skills to good use.  I do feel that without a full degree there are some jobs I wouldn't qualify for (especially at bigger corps like FAANG, but it isn't really my interest to work in big corps anyway) or that I might hit a certain ceiling in promotions, but I'm currently doing alright as a lead/senior dev at a small startup.  Honestly feeling like I might be about ready for a career change anyway, so whatever... life can be good on many paths, best to pick ones that spark your creativity and bring you fulfillment.",8,0,0,False,False,False,1644200282.0
smanmb,hvysu1z,t3_smanmb,"I am at the end of my 1st semester and we had a course Intro to ICT where they taught us about SDLC, relational databases, git, html, css,  SQL, some data analysis using pandas and even Scratch and LaTeX (LaTeX was my worst nightmare lmao). I think it's still pretty good compared to what my friends were experiencing in the same course in different universities (draw an f  king motherboard). I agree that a lot of universities don't teach you this stuff but it isn't that hard to learn it. What they did probably teach you was how to be good at learning stuff and this is all that you need. They are equipping you to make these tools because anyone can learn them if they know how to make them.",2,0,0,False,False,False,1644253974.0
smanmb,hvyziej,t3_smanmb,"I go to Uni of Michigan right now and there is a 1 credit class for these things. I guess it's useful for those wanting to go into software engineering so that they can apply these skills in their class projects and internships, but once they go into the real world, these things become trivial.",2,0,0,False,False,False,1644256531.0
smanmb,hvxnq0y,t3_smanmb,I did a degree in music but I only like the piano. Why didn't they only teach me piano??,3,0,0,False,False,False,1644234897.0
smanmb,hvyw87m,t3_smanmb,"CS will help you be a better programmer, but it is catered toward teaching you to do research and mathematically developing new ways of doing computer things. 

An architect knows he/she needs plumbing, but probably doesn’t know how to go about it. CS is the study of the theory and proofs computer logic is built on. 

They don’t make a good enough distinction imo when applying to these programs because they don’t want to scare everyone off. But if I wanted a .net developer to support a micro services architecture I would more readily hire someone who has knowledge of the framework and experience in the language before someone with a masters degree. If I wanted to invent a new way of visualizing data I’d hire the masters degree person.",1,0,0,False,False,False,1644255286.0
smanmb,hvxdu7c,t3_smanmb,"Yes lmao, funny enough I’m pretty horrendous at programming but I was able to graduate with a 3.5 in as a CS major. Tbh wish I had chose to be something else but I realized I wasn’t getting better at programming waaaay too late to quit.",-1,0,0,False,False,False,1644227277.0
smanmb,hw22e8c,t3_smanmb,"I took one class in particular on object-oriented programming that actually taught me a lot about programming, primarily because the professor did an excellent job and gave us in depth code reviews. I use the knowledge he gave me every freaking day.

Another class I took on Design Patterns was also very useful. But both were electives that I sought out and not part of the core curriculum. Both professors who taught these classes have retired within the past two years.

But i also got a series of good internships with local tech companies, and I learned a ton there, including git, among other technologies. Having a good internship combined with the theory you learn in school is a pretty good combination.",1,0,0,False,False,False,1644306537.0
smanmb,hwc2nd2,t3_smanmb,Unis gotta teach the theory cuz otherwise who else will? Certainly not your boss,1,0,0,False,False,False,1644473584.0
smanmb,hvws1jz,t1_hvvu2b8,Very insightful,7,0,0,False,False,False,1644211843.0
smanmb,hvz4phx,t1_hvvu2b8,Can you explain how the math has been useful for you in the workforce ?,3,0,0,False,False,False,1644258552.0
smanmb,hw0h4m2,t1_hvvu2b8,Great response,1,0,0,False,False,False,1644277728.0
smanmb,hvxkav1,t1_hvvhzsi,"Also i dont know about his course but you learn much more @ CS, at my specialty or whatever thatd be called in english i got to learn x86 and Windows/Linux kernel pretty in depth, how compilers are built, basics of computer system administration, dbms algorithms, machine learning, and the theory behind software engineering which maybe isnt the how of using git but lots of the why’s behind managing projects.

Idk i still havent finished, bout to start 6th semester but OP’s post sounds like first 2 semesters of CS at a TU

Also you have to learn git either way, maybe they will not teach you but you will have to write projects in groups and good luck without git",11,0,0,False,False,False,1644232478.0
smanmb,hvw44ts,t1_hvvtonc,"Git and debugging help you during your course: git keeps your code safe, and debugging keeps you from tearing your hair out while learning to code.",-3,1,0,False,False,False,1644199994.0
smanmb,hvytrkj,t1_hvw4qv9,"I think FAANG companies specifically don't require you to have a university/college degree (at least Google doesn't from what I have heard). Anyways, your comment was really insightful. Thank you",2,0,0,False,False,False,1644254344.0
smanmb,hvzk4qp,t1_hvz4phx,One major example: there was a time (2014-2016 or so) when I and everyone around me started working with neural networks. People without strong math backgrounds definitely had a harder time getting up to speed.,6,0,0,False,False,False,1644264556.0
smanmb,hvyjteg,t1_hvxkav1,"Nifty. Too bad the x86 knowledge has an upcoming expiration date. Apple has left the platform and ARM architecture seems to be taking over.

That’s the problem with teaching the current practice rather than timeless theory.  Sure you can use current practice as hands on learning tool, but be sure not to exclude the bad get abstract ideas so when everyone bails on that the student can move on himself.",2,0,0,False,False,False,1644250350.0
smanmb,hvwcq9i,t1_hvw44ts,"Yeah but these are trivial, easy to learn things - especially for comp sci grads",16,0,0,False,False,False,1644204012.0
smanmb,hvyvcm5,t1_hvyjteg,I'd imagine basic operations in x86 are fairly analogous to other ISAs. E.g. you still need to know how to load/move/operate on data and how to use registers.,3,0,0,False,False,False,1644254947.0
sm1pz4,hvuaa1i,t3_sm1pz4,"Not sure why [classful networks](https://en.wikipedia.org/wiki/Classful_network) are still explained, almost 30 years after the introduction of [classless inter-domain routing](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing). Do these network classes really matter nowadays?

EDIT: corrected number of years",6,0,0,False,False,False,1644172091.0
sm1pz4,hvv2vyo,t3_sm1pz4,"It's been said but not explicitly: **Stop using classful networks**

CIDR is the way, and has been for (likely) your whole life.

Classes are a kind of shortcut to get the idea of CIDR, but the divisions can be anywhere in the 32-bit address, not just on the octet splits. On the public internet you won't be getting a Class allocation, you'll get a CIDR allocation.",4,0,0,False,False,False,1644183604.0
sm1pz4,hvu4j41,t3_sm1pz4,"Note that ""255"" in the destination address of the ""0"" mask essentially means ""network broadcast"". It's usually blocked by correctly configured **router**s or **NAT**s.  
(e.g. IP [192.168.1.255/255.255.255.0](https://192.168.1.255/255.255.255.0) will ""broadcast"" to ""192.168.1.x"" addresses.)",1,0,0,False,False,False,1644169833.0
sm1pz4,hvxof9n,t3_sm1pz4,"They may be boring and/or confusing, but the best info is the standard for a particular topic. Any thing else is an interpretation of the engineering authors' work. 

*  [IETF (Internet Engineering Task Force) RFC (Request For Comments) database](https://datatracker.ietf.org/) has the info and history you want. I spend a lot of time here. 
* [IEEE](https://www.ieee.org/) also has many important standards (802.x are the ones you'll be most familiar with), but any current standard is behind a paywall so fuck those guys. 
* Last but certainly not least is the [IANA - The Internet Assigned Numbers Authority](http://www.iana.org), specifically their [IANA Numbers](https://www.iana.org/numbers) section, which links to the the RFCs (and more) below.

---

IETF IP Standards and Drafts selection

* [RFC1466 - Guidelines for Management of IP Address Space](https://datatracker.ietf.org/doc/rfc1466/)
* [RFC1518 - An Architecture for IP Address Allocation with CIDR](https://datatracker.ietf.org/doc/rfc1518/)
* [RFC4632 - Classless Inter-domain Routing (CIDR): The Internet Address Assignment and Aggregation Plan]
(https://datatracker.ietf.org/doc/rfc4632/) which replaced [RFC1519 - Classless Inter-Domain Routing (CIDR): an Address Assignment and Aggregation Strategy](https://datatracker.ietf.org/doc/rfc1519/)

*  [RFC1918 Address Allocation for Private Internets](https://datatracker.ietf.org/doc/rfc1918/)

---

While the root source is always best, it is not always the easiest. Some interpretations are a great way to approach dense and confusing topics. I find  [Wikipedia](https://en.wikipedia.org) is a great distillation and explanation of concepts proposed and standardized by various engineering groups. 

The [TCP/IP Guide](http://www.tcpipguide.com/free) is hands down the best protocol resource on the the internet. It has the protocol history and usually a great explanation on how it is constructed and used. It is dated, but don't let that reduce its usefulness, given your post here is about 40 year old standards first proposed in the infancy of the internet.

Read about [IP Protocol](http://www.tcpipguide.com/free/t_InternetProtocolIPIPv4IPngIPv6andIPRelatedProtocol.htm) here

[IP Classful Addressing](http://www.tcpipguide.com/free/t_IPClassfulAddressingOverviewandAddressClasses.htm) is a great explanation of the classful schema.

As others have stated, ip address classes are dead. 
CIDR is the way. Note classful addresses DO NOT USE MASKS. Masks are a CIDR construct. 

Classful addressing uses the bits of the first octet to break the ranges

Low value | High value | Class
------------|-------------|----------
00000000 | 01111111 | Class A
10000000 | 10111111 | Class B
11000000 | 11011111 | Class C
11100000 | 11101111 | Class D
11110000 | 11110111 | Class E



sub set from [TCP/IP Guide - IP - Classful Network and Hosts](http://www.tcpipguide.com/free/t_IPClassfulAddressingNetworkandHostIdentificationan-2.htm)

---

Good luck in your networking journey.",1,0,0,False,False,False,1644235367.0
sm1pz4,hvu27t9,t3_sm1pz4,"Loop back is one the own, however it is technically a class A as well. (Go ahead and ping any address with 127.*.*.* They all work not just 127.0.0.1)

The number of networks vs hosts comes down to the default network mask. 

255.0.0.0 , a Class A network default subnet mask allows for 255 networks all ""masked"" by the first 8 bits. That is divided to allow for some private and loop back. The same holds true for B and C however the difference is the mask of 255.255.0.0 and 255.255.255.0 respectfully. 

More bits to play with before the ""255""'s allows for more networks while more bits after allows for more hosts. I hope that helps.

P.S. the same basic premise hold true for classless inter-domain routing as well.",0,0,0,False,False,False,1644168922.0
sm1pz4,hvuvtss,t1_hvuaa1i,Yes if your subnetting a network,2,0,0,False,False,False,1644180732.0
sm1pz4,hvvvg9m,t1_hvuaa1i,It's still an important concept. The internet is largely IPV4 and you must start with the standards the engineers came up with. You must understand this to eventually understand VLSM. The classes dictate how many bits you have to subnet with.,0,0,0,False,False,False,1644195958.0
sm1pz4,hvv7gcq,t1_hvv2vyo,"Unfortunately, it is still covered on some of the CompTIA exams. But, with that said thank you for the update, I'll have to keep that in mind as I'm going through the process. That is one of the troubles I always have with learning, I will often jump ahead on certain things and miss the basics. Thanks!",1,0,0,False,False,True,1644185466.0
sm1pz4,hvvvoq0,t1_hvv2vyo,"You cannot detach the two, period. What are you suggesting exactly?",0,0,0,False,False,False,1644196064.0
sm1pz4,hvvb79j,t1_hvu4j41,"I'm very glad you brought up this point. I would often see a few sources of information where the classes would be off by a number or two. I know that an IP address ending in 0, genuinely references the entire network. An example would be [192.168.1.0](https://192.168.1.0) would represent this entire network. Does this mean that [192.168.1.255](https://192.168.1.255) would be a ""broadcast"" IP address for the entire 192.168.1.x network? Second, should I edit my table to start at x.x.x.1 instead of x.x.x.0 and end in x.x.x.254 versus x.x.x.255? Thanks for the help, its very much appreciated.",1,0,0,False,False,True,1644187034.0
sm1pz4,hvz9283,t1_hvxof9n,"Thank you very much for this. I won't like, its a little deep and may take me a little while t dive through it all but it is greatly appreciated!!!!",1,0,0,False,False,True,1644260235.0
sm1pz4,hvvg28f,t1_hvuvtss,Oh subnetting...I will never like nor miss you if you disappear...,2,0,0,False,False,False,1644189085.0
sm1pz4,hvzie80,t1_hvvvg9m,"> The internet is largely IPV4 and you must start with the standards the engineers came up with. You must understand this to eventually   
understand VLSM.

Sure, network classes have some historical value.

> The classes dictate how many bits you have to subnet with.

Subnets are not tied to classes anymore, that's how I would interpret the following sentences from the [CIDR Wikipedia article](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing):

> The Internet Engineering Task Force introduced CIDR in 1993 to _replace_ the previous classful network addressing architecture on the Internet. Its goal was to slow the growth of routing tables on routers across the Internet, and to help slow the rapid exhaustion of IPv4 addresses.

and

> Whereas classful network design for IPv4 sized the network prefix as one or more 8-bit groups, resulting in the blocks of Class A, B, or C addresses, under CIDR address space is allocated to Internet service providers and end users on _any_ address-bit boundary.

(Emphasis is mine)

So I would conclude that network classes are mostly irrelevant these days. They might matter for old routing protocols like [RIPv1](https://en.wikipedia.org/wiki/Routing_Information_Protocol).",1,0,0,False,False,False,1644263880.0
sm1pz4,hvv8l1f,t1_hvv7gcq,"If you have to know it for the test, then of course keep it. But know that the test isn't fully up to date and in line with what is actually in the wild. Learn the idea and theory/math behind it, not just the facts they throw at you.",1,0,0,False,False,False,1644185939.0
sm1pz4,hvw92hq,t1_hvvvoq0,Classful is a small subset of how CIDR works. Going into the networking world thinking it is how things actually work is like learning the box-step and going ballroom dancing and thinking you've covered your bases.,0,0,0,False,False,False,1644202298.0
sm1pz4,hvvmvi5,t1_hvvb79j,">Does this mean that 192.168.1.255 would be a ""broadcast"" IP address for the entire 192.168.1.x network? 

Yes. Understand that it what I call ""subnet broadcast"". Typically routers will block all such broadcast packets from ""hopping"" to another subnet. (Assuming they aren't manually misconfigured to allow it.) Also understand that such packets can easily overwhelm a subnet, since **every** device on the subnet will receive it. IT departments will quickly hunt you down if they see any such packets without authorization.

&#x200B;

>Second, should I edit my table to start at x.x.x.1 instead of x.x.x.0 and end in x.x.x.254 versus x.x.x.255?

the .0 for a class C network ([255.255.255.0](https://255.255.255.0)) denotes the ""network address"". Think of it as a way to identify the entire subnet. Therefore the *assignable* address would always be between .1 and .254 inclusively. Now, by *convention*, we typically reserve .1 as the network's gateway/router address. However there's nothing technically preventing a gateway/router's address from being any assignable number.

Additionally, if you use the ""global broadcast"" ([255.255.255.255](https://255.255.255.255)) address, such packets could hop subnets depending on the TTL (time to live) number and the router configurations. Given a misconfigured gateway, they might even leak out to the internet. (Although most internet providers are smart enough to never let that happen.)

Wikipedia has a good primer on [IP address](https://en.wikipedia.org/wiki/IP_address)es.",2,0,0,False,False,False,1644192086.0
sm1pz4,hvzx9f5,t1_hvzie80,"You are misinterpreting the information. You must still understand classes to start with. You CANNOT just VLSM any which way. You cannot  borrow bits that are not allowed. This is the standard and not up for interpretation period.

I assure you when you VLSM the absolute first thing you must consider is the network address call class. I have submitted thousands of times of the last 20 years so no matter what misunderstood wikipedia link you share I can tell you that the class matters period.

Trust the expertise here here. How many times have you actually subnetted a network? It doesn't sound like you know the first thing about how it works.",0,0,0,False,False,False,1644269597.0
sm1pz4,hvvj29q,t1_hvv8l1f,"""what is actually in the wild"" AWESOME :)",1,0,0,False,False,True,1644190391.0
sm1pz4,hvxixpc,t1_hvw92hq,"You do understand in order to do any type of VLSM with CIDR notation you must first know what network class you started with correct!?

I am not sure where you got your information. But I have been workig in this field for over 20 years. Trust me when I say you obviously have some gaps in your knowledge here. These concepts are not mutually exclusive, one builds upon the other.",0,0,0,False,False,False,1644231421.0
sm1pz4,hvvmx2g,t1_hvvmvi5,"**[IP address](https://en.wikipedia.org/wiki/IP_address)** 
 
 >An Internet Protocol address (IP address) is a numerical label such as 192. 0. 2. 1 that is connected to a computer network that uses the Internet Protocol for communication.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1644192106.0
sm1pz4,hvz3to2,t1_hvxixpc,"I don't know where you are reading that I think they are exclusive. You're putting your own spin on what I actually wrote.

> Classes are a kind of shortcut to get the idea of CIDR, but the divisions can be anywhere in the 32-bit address, not just on the octet splits.

> Classful is a small subset of how CIDR works.",0,0,0,False,False,False,1644258210.0
sm1pz4,hvvneld,t1_hvvmx2g,"(Someone needs to make a ""*sigh*Bot"" that auto replies with ""<*sigh*\>"" to all of these auto reply bots...)",1,0,0,False,False,False,1644192325.0
sm1pz4,hvzbhv1,t1_hvz3to2,"Ok give me 512 networks from 192.1.1.1, subnet it out and post it here.",1,0,0,False,False,False,1644261194.0
sm1pz4,hvvunyb,t1_hvvneld,"""sighBot"" :)

I really like when you used the term ""assignable address."" I think I will leave the table as is but add your phrase in the note, along with the note about .1 being the gateway. This way I get the whole pictures and more importantly I understand the reason why.

Quick question. What is the purpose of the .255 broadcast IP address. I keep hearing Sean Connery say ""Give me one ping Vasily, one ping only.""",1,0,0,False,False,True,1644195602.0
sm1pz4,hvzh46k,t1_hvzbhv1,I don't think you are trying to prove what you think you are trying to prove. Try re-reading the parts I quoted of myself. Then read them again.,1,0,0,False,False,False,1644263388.0
sm1pz4,hvw2cnh,t1_hvvunyb,"The ""subnet broadcast"" is a way to send one data packet to every device on the subnet. What exactly is in that data packet depends on whether or not that device wishes to act on it. Some examples: [ARP](https://en.wikipedia.org/wiki/Address_Resolution_Protocol), [DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol), [RIP](https://en.wikipedia.org/wiki/Routing_Information_Protocol).",1,0,0,False,False,False,1644199161.0
sm1pz4,hvw2drl,t1_hvw2cnh,"It seems that your comment contains 1 or more links that are hard to tap for mobile users. 
I will extend those so they're easier for our sausage fingers to click!


[Here is link number 1 - Previous text ""ARP""](https://en.wikipedia.org/wiki/Address_Resolution_Protocol)

[Here is link number 2 - Previous text ""RIP""](https://en.wikipedia.org/wiki/Routing_Information_Protocol)



----
^Please ^PM ^[\/u\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20hvw2drl)",1,0,0,False,False,False,1644199176.0
sm9s5c,hvw0g7c,t3_sm9s5c,The website moved to [nand2tetris](https://www.nand2tetris.org/),6,0,0,False,False,False,1644198274.0
sm9s5c,hvvhpmw,t3_sm9s5c,Have you tried [Internet Archive](https://archive.org/)?,1,0,0,False,False,False,1644189808.0
sm9s5c,hvw1gwi,t1_hvw0g7c,Thanks!,2,0,0,False,False,True,1644198748.0
slcgd1,hvqwxlw,t3_slcgd1,"Go through the string and find out all the positions that the first character can start at. (If you're looking for BAR, it's simply the places where the string has a B.)

Go through the string again and find out all the positions that the second character can be and in how many ways it can be so. (In the case of BAR it's all the A locations, and the number of ways at each position is the number of Bs before that location.)

Go through the string a third time and find out all the positions that the third character can be and in how many ways. You might see the pattern now. I'll work it out for the example.

    BARBARAR
    10010000
    01002020
    00100305

The final answer is 1+3+5=9.",31,0,0,False,False,False,1644099302.0
slcgd1,hvq3psr,t3_slcgd1,I believe you are looking for subsequences. You might have more luck looking with this nomenclature.,5,0,0,False,False,False,1644084187.0
slcgd1,hvq8rbv,t3_slcgd1,"I could see this being solved recursively. Like say you have a function `count()` that takes in a string to look though (`space`) and a string to look for (`match`) and keeps track of a sum variable (`sum`). To solve, you iterate through `space` until you reach the first character of `match` at index `ind`, then add the result of `count(space[ind+1:],match[1:])` to `sum` unless `match` is of length 1, then just increment `sum`. Then just return `sum` at the end. Don't know if this would work or not though.",4,0,0,False,False,False,1644087103.0
slcgd1,hvq002m,t3_slcgd1,How does that string contain bar 9 times? I do not see it.,8,0,0,False,False,False,1644082746.0
slcgd1,hvq1mh7,t3_slcgd1,It's dp with O(n*m) states,6,0,0,False,False,False,1644083366.0
slcgd1,hvq1cju,t3_slcgd1,"there are plenty of pattern matching algorithms. here is a [LINK](https://www.geeksforgeeks.org/frequency-substring-string/) for the problem you're looking for.

Edit: OP wrote substring in the problem and I didn’t see the example earlier. The link I provided is for substring matching. OP should’ve written subsequence and not substring in the problem.",7,0,0,False,False,False,1644083260.0
slcgd1,hvq8vrb,t3_slcgd1,"Just to take a guess, the way I would go about this problem is probably going left to right, upon detecting a B, and do another loop that looks to find the next A, which does another loop that goes through and counts up how many r's there are to the end of the string. Once it reachs the end of the string, it exits the R loop, and returns to the A loop, where it goes to see if there's another A. If so, repeat R loop, if not, return to B loop, and continue to see if there's another B. If so, re enter the A and R loops. If not, exit, and you should have your answer from how many times you found a successful collection of Bs, As, and Rs.  
absolute garbage programming strat that could probably be improved by someone who knows better about time complexity algorithms, but if this is just for some boring class, this should be fine enough.",2,0,0,False,False,False,1644087153.0
slcgd1,hvszaet,t3_slcgd1,Coming here I can understand Why हाईस्कूल math is important,1,0,0,False,False,False,1644150851.0
slcgd1,hvt6r3m,t3_slcgd1,If it does not have to be continuous its a subsequence not a substring afaik,1,0,0,False,False,False,1644155262.0
slcgd1,hvtf5xu,t3_slcgd1,Knuth–Morris–Pratt algorithm,1,0,0,False,False,False,1644159405.0
slcgd1,hvr403u,t1_hvqwxlw,"The method I came up with is almost identical, except that I accumulate the columns in the matrix from left to right.

So you build the matrix from left to right and top to bottom. For each cell, you start with the value to the left, add the value above if the letter of the column matches the letter of the row, and write it to the cell. If you're in the first row, the value above is always 1.

      B A R B A R A R
    B 1 1 1 2 2 2 2 2
    A 0 1 1 1 3 3 5 5
    R 0 0 1 1 1 4 4 9

If you do it like this, you don't have to scan through the matrix to find the final solution because the bottom-right cell already holds the sum. In fact, you can get away with only storing the current column as you never have to look back further than to the immediate left.",10,0,0,False,False,False,1644102462.0
slcgd1,hvq32mq,t1_hvq002m,"    BARBARBAR
        
    BAR______
    
    BA___R___
    
    BA______R
    
    B___AR___

and so on.",4,0,0,False,False,False,1644083935.0
slcgd1,hvq2a0l,t1_hvq002m,"If you remove 5 letters, you can get it 9 times in different configurations. Ex. first one and last to chars.",4,0,0,False,False,False,1644083618.0
slcgd1,hvq9az2,t1_hvq1mh7,Yep subproblems depend on previous subsequences starting at ith position in string and ending at jth position.,2,0,0,False,False,False,1644087352.0
slcgd1,hvq3k7v,t1_hvq1cju,It's a different problem,5,0,0,False,False,False,1644084126.0
slcgd1,hvr7al9,t1_hvq8vrb,"I think what you're describing is recursion!

To figure out how many 'BAR's there are in a string, you first figure out how many 'B's there are (which is easy) and then - for every 'B' you find - figure out how many 'AR's there are in the subsequent string (which is the same problem again, i.e. recursion).

You can repeat this all the way down until you're looking for how often the last letter appears (which is easy again). Your result is the total number of word ends you found.

This is how this could look like in practice:

    - 'BAR' in 'BARBARAR' : 6+3 = 9
      - 'AR' in 'ARBARAR' : 3+2+1 = 6
        - 'R' in 'RBARAR' : 3
        - 'R' in 'RAR' : 2
        - 'R' in 'R' : 1
      - 'AR' in 'ARAR' : 2+1 = 3
        - 'R' in 'RAR' : 2
        - 'R' in 'R' : 1

As you can see, it works, but you're repeating a lot of work. With dynamic programming, we can get this runtime down to O(d\*n). See [this](https://www.reddit.com/r/computerscience/comments/slcgd1/comment/hvqwxlw/?utm_source=share&utm_medium=web2x&context=3) solution.",1,0,0,False,False,False,1644103940.0
slcgd1,hvthndf,t1_hvtf5xu,KMP onlly finds index of substring not how many times is subsequence presented in string. Do you have in mind how to use KMP for this and make time complexity better than O(m\*n)?,1,0,0,False,False,True,1644160534.0
slcgd1,hvsa0w9,t1_hvr403u,"Since the previous results can be inherited, this approach is almost linear. However, with repetitive target string, the thing is tougher.
Given `aaa` and `aaaaaaaaaaaaa`, the algorithm's time complexity goes to O(n*m).  
[code](https://pastebin.com/b4iLt6wA)",7,0,0,False,False,False,1644131105.0
slcgd1,hvtjuem,t1_hvthndf,"You are right, KMP is not applicable. Does overlapping substrings count? Does a substring matched from right to left count?",1,0,0,False,False,False,1644161490.0
slcgd1,hvxvqj9,t1_hvsa0w9,"You're right, if the subsequence we're looking for does not have any repeated digits, we only need to update one of the cells in the column vector, in which case we get O(n) linear runtime.

More generally, the runtime is O(d\*n) where n is the size of the string and d is the smallest upper bound for the number of repeats of any letter in the subsequence.",1,0,0,False,False,False,1644239706.0
slotas,hvs2hk1,t3_slotas,"If you need a data type like that, just code the type and operators needed, it's easy enough to define memory space, functions and operations over the data to make it work

Probably not going to work at hardware level as you'll need some sort of dynamic abstraction that is not possible at circuit level",8,0,0,False,False,False,1644120230.0
slotas,hvsz2id,t3_slotas,"> To represent values, it seems that computer scientists have settled that the only things they need are integral types that represent whole numbers (-1,0,1,2) and floating point numbers (.25, 10.0, Positive infinity, NaN).

This isn't quite accurate. Floating point numbers are ultimately represented as integers. And some architectures have the notion of bit sets, a data type consisting of some number of bits, where each bit interpreted individually. Other architectures have data types like character strings, which are also ultimately represented as integers. And computer scientists have explored other means of representing numbers like residual number systems. These haven't found much success.

> Why don’t computers support more data types?

This question is similar to, ""Why don't computers support more instructions?"" The CISC v. RISC debate from the 1980s answered this question (and any other ones like it). Basically, because supporting data types that aren't applicable to many programs is ultimately wasteful. The cost of supporting them will degrade performance in cases where they aren't used. The argument that having support for x will improve performance is only true if x is frequently used. This is true regardless of what x is.

> a “polynomial” data type could be used to reduce the size and increase the speed of many programs.

This has already been done. I can think of two examples: the CDC series of vector supercomputers that started off with the STAR-100 from the early 1970s, which ended with the ETA-10 in the late 1980s; and the DEC VAX architecture from 1977. These computers/architectures had support for polynomials. A separate polynomial data type isn't necessary; these examples just treated them as floats. I'm sure there are more CISCy architectures out there that also had similar support. 

Support for polynomial evaluation can really only be implemented in a general-purpose computer as complex microcode. The lesson learned from these historical examples is that microcoded implementations don't really improve performance at all. Today, if you really need to do stupendous amounts of polynomial evaluation, then you'd see if it could be done faster on a GPU, an FPGA, or an ASIC.",8,0,0,False,False,False,1644150703.0
slotas,hvt30du,t3_slotas,"IMHO, more complex or exotic data types can always be constructed using these two primitive types, integers and floats, in higher level languages. Therefore, I don't see why a CPU would need to support anything else.

(Having said that, there are GPUs, or ""signal processors"" or special math co-processors that do indeed support other ""types"" of data representation, but I consider those special case processors.)",3,0,0,False,False,False,1644153180.0
slotas,hvtyxm8,t3_slotas,"Well, yes, you can build hardware that implements your encoding and would likely be able to do reasonably efficient manipulation. The reason why such encodings are not used in practice can be found when you try to answer the following questions:

* Why is this useful?
* Is this encoding more efficient, able to do operations on polynomials faster or with less power than just a list of ints representing the coefficients?
* Is designing, manufactoring and deploying the specialized hardware worth the potential benefits we discovered above?

To answer some of them, I feel like this is highly convoluted, just storing an int array is far easier to implement and likely just as efficient to multiply, evaluate etc.

There always are trade-offs when designing data type encodings. For example, when representing the coefficient inversely in a float, addition becomes harder. Taking the inverse and multiplying does not become easier at all. The result is just less efficient in summary -- you could call it an engineering failure.

There are some specialized hardware instructions that do non-standard bit manipulation, like multiply-without-carry. In most cases, tho, you have certain bit manipulation tricks to do that almost as efficient.

And ints and floats (actually, floats are just tuples of ints) seem to be sufficient for almost all numeric means so far. Need fractions: take an int pair! Need polynomials? Take an int array!",1,0,0,False,False,False,1644167633.0
slotas,hvu84b1,t3_slotas,"I don't know if you are aware of computer architectures but ultimately what you see in software has to be somehow represented in the underlying hardware.

It's not true that integers and floats are the only types, you can mix them and structure them in memory to obtain any abstract data type you want. 

The catch is: the higher the abstraction, the farthest you are from the hardware and (as a general rule) the slower you program will run. That is because the only data types that the hardware can deal with directly are integers and floats. 

Why is that? Because general processors has to be generals, so you want to deal only with the basic blocks that can then be use to abstract all the other operations. 

As other users said, there are less general processors that use more sophisticated data types (for instance complex numbers) but then you can't use them to represent other abstract structures. As everything in CS, there's a trade off. 

You can start playing with FPGAs, if that interests you, and define your hardware polynomial type and see if you can outperform a general processor while doing operations with your polynomials!",1,0,0,False,False,False,1644171233.0
slotas,hvuownz,t3_slotas,"You can implement those operations pretty well with only ints, floats and Boolean logic instructions. Those special types are rarely useful, so it is simply a better use of your silicon area to make regular int and floats ops faster than to add a unit that would rarely get used. Some CPUs however, have some instructions for GF arithmetic, e.g.: CLMUL. That is likely because this is relatively easy to add in hardware, much faster and there are a bunch of applications where is really useful.",1,0,0,False,False,False,1644177946.0
slotas,hvs2ov1,t1_hvs2hk1,What is dynamic abstraction?,-1,0,0,False,False,True,1644120714.0
slsxv7,hvst4fi,t3_slsxv7,What property do you want to capture with consistency? As in whats the difference for you between accuracy and consistancy?,1,0,0,False,False,False,1644146209.0
sl4tdf,hvpa19l,t3_sl4tdf,"One misconception that may help explain why a natural alignment is preferable on 32-bit CPU for larger data types: while 32-bit CPUs use 32-bit pointers/addresses, that does not mean they are limited to 32-bits for all operations. Many have wider registers of >=64-bits and thus >=64-bit load/stores for things like floating point or SIMD operators (even on “32-bit” CPUs).",9,0,0,False,False,False,1644072009.0
sl4tdf,hvpgz99,t3_sl4tdf,"Your argument only considers the cost of the alignment shift in two cases (where the data type is *n* times bigger than the processor's natural word size, where *n* is a power of two, and the case where multiple smaller data types fit inside a natural word). It ignores many other architectural, organizational, and implementation issues like having to handle misaligned data crossing RAM array and/or cache line boundaries, the impact on how cache coherence and synchronization (e.g. LL/SC) is achieved, semantical issues with memory consistency (e.g. should misaligned writes be atomic?), increases in power consumption (two cache lines might have to be accessed for one load), etc.

That said, these issues are not insurmountable, given that architectures like x86, the Power ISA, and RISC-V support misaligned accesses (optionally in the case of RISC-V). x86 always had support misaligned accesses; all Power ISA implementations suffer from a performance loss if misaligned accesses are used, AFAIK; and RISC-V's architects have noted that misaligned accesses complicate matters architecturally and for implementers (hence the reason why they're optional).

For these reasons, I would say there's still ample justification for architectures that forbid misaligned accesses.

Edit: Left out ""not"" before insurmountable.",7,0,0,False,False,False,1644075117.0
sl4tdf,hvpejj3,t3_sl4tdf,"I think a lot of this has to do with the width of the cache and what ways values on cache lines can be loaded into registers.  e.g. if your cache line is 256 bits wide, there are 4 non-overlapping segments that 64-bit floats could be loaded from.  In this scheme, if your floats are not aligned, then you need some additional shifting operations before you can load them into your registers.  I think responsibility for this falls somewhere between the compiler and the CPU (the compiler needs to recognize alignment issues and generate slower unaligned loads)

This makes the assumption that values can *only* be loaded from non-overlapping segments from the cache line.  Obviously this is not true (and the interaction between memory and instruction on modern CPUs is too complicated for a reddit response), but given the status quo I'd imagine that someone did a cost/benefit analysis on supporting overlapping loads from cache and said ""nah"".  I also get the feeling that native support for unaligned loads would make the aligned case much slower, due to the complexity involved in handling all the alignment cases.",2,0,0,False,False,False,1644074046.0
sl4tdf,hvptn29,t3_sl4tdf,"> To summarize my question, I want to know that what are the benefits of natural alignment over processor-word based alignment.

With natural-alignment, the CPU designers have more opportunities to reduce physical hardware wires. With strict word-alignment, the machine must retain all address wires for any circuit that handles addressing. But with natural-alignment, the bigger the operand-size, the fewer address wires you need to keep around. Suppose you have a modern CPU with 40 bits of physical addressing (1TB addressable RAM). If you're operating on a 512-bit operand, you can drop the lower 9 bits of address and you only have to use 31 wires for addressing, instead of all 40. That's ~23% savings and when you replicate that across many circuits in a CPU, you're talking about big benefits in terms of cost, die-space, frequency, power, latency and so on.

Note that x86 kind of has both, since many instructions require the CPU to be able to perform byte-addressing. Thus, the byte is the implicit ""word"" of x86. But not all instructions are required to be able to operate on bytes, so it's kind of a funky hybrid.",2,0,0,False,False,False,1644080272.0
skltie,hvlyn6o,t3_skltie,"1) yes they are basically the same things. The differences are subtle and mostly unimportant. The exception being that mixed integer programs allow some continuous variables. 

3) this is often true, but not why they are hard. The first optimization problem is basically trivial and all the difficulty is in going from one to the other. 

2,4) you seem to be mixing up convex objective function and convex constraints. 

Linear functions are convex, but integer programs do not have convex feasible regions because convex combinations of integers aren’t generally integers. 

The TSP has a linear objective and non-convex constraints. We can certainly make harder problems by replacing our linear objective with any other kind of function. 

For instance you provably can’t do better than n! if I ask you to guess my favourite tour without any other information. More tractable is to add a submodular function to the objective.",4,0,0,False,False,False,1644007104.0
skltie,hvmcvjm,t1_hvlyn6o,"Thank you so much for your answer!

Can you please explain the following points:

\-  ""integer programs do not have convex feasible regions because convex combinations of integers aren’t generally integers."" - why is this so?

\- ""TSP has non-convex constraints"" - can you please explain why the constraints in TSP are non-convex?

\- ""you provably can’t do better than n! if I ask you to guess my favourite tour without any other information."" - what do you mean by ""tour"" (is a ""tour"" a possible ordering of cities)? why is it n!  - is this because there are n! possible ""tours""?

\- What exactly is a ""submodular function""? I tried reading about them on wikipedia ([https://en.wikipedia.org/wiki/Submodular\_set\_function](https://en.wikipedia.org/wiki/Submodular_set_function)) but I am not sure if I understand their relevance here. Why would it become more ""tractable"" (tractable = make more easier?) to add a submodular function to the objective?

&#x200B;

Thanks you so much for all your help!",1,0,0,False,False,True,1644012545.0
skltie,hvmih9o,t1_hvmcvjm,"1) a convex combination of x and y is λx+(1-λ)y for λ in [0,1], so λ and (1-λ) are both fractions. Unless we get lucky the sum of fractions is also a fraction. Eg (1/3)0+(2/3)1 doesn’t equal a whole number. In general if you take two integral points the line segment connecting them will only have a finite number of integer points and uncountably many fractional points. 

2) because integrality constraints aren’t convex, the rest are linear constraints. 

3) yeah a tour is just an ordering of the cities of which there are n!

4) in practice submodular functions are like “convex” set functions. It’s a set function that satisfies diminishing returns. The more things you have already, the less new items help. Eg money, a dollar is worth more to someone poor than to a billionaire. Consumption of most single goods is submodular (getting one apple is good unless you already have lots), something like shoes is not because one shoe likely sucks, but a pair is nice. Uranium is likely not submodular because having a little sucks, but if you get enough you can make a nuclear reactor or a bomb. 

4.5) adding a submodular function makes the problem more general and thus harder, they are just nice functions that usually doesn’t make it “too much” harder. Whereas by comparison adding “my favourite tour” as the objective makes it as hard as possible (because it can’t be harder than brute force). 

For any set function we can just exhaustively check all 2^n subsets, the more structure the function has we can leverage the better we can do. It’s worth noting that if your set function is fully arbitrary then just representing it takes at least 2^n space. Usually we assume we have some oracle that will magically tell us the value of a set, and then measure the complexity of our algorithm in the number of oracle queries. 

*Roughly* speaking any NP-Hard optimization problem can’t be a linear objective over a convex set, otherwise we can just solve it directly (some issues on size of representation, numerical accuracy, and approximation and such) if we can solve the separation problem for it efficiently though then we can solve the original problem efficiently (up to numerical accuracy).",3,0,0,False,False,False,1644014789.0
skp5fm,hvndqt2,t3_skp5fm,"A few on mathoverflow here, including: P = NP is equivalent to the statement that every property expressible by a second order existential statement is also expressible in first order logic with a least fixed point operator

https://mathoverflow.net/questions/330991/equivalent-forms-of-the-p-vs-np-problem",8,0,0,False,False,False,1644028597.0
skp5fm,hvmc54t,t3_skp5fm,"I honestly don't know, and it's been years since my best days in terms of understanding theoretical CS, but Scott Aaronson has written an [in-depth survey on the P vs NP problem](https://www.scottaaronson.com/papers/pnp.pdf) that you might find interesting, if you have the time to read or skim through the massive article. He links to the PDF from his [blog post](https://scottaaronson.blog/?p=3095).",4,0,0,False,False,False,1644012259.0
sk7puv,hvjfk9q,t3_sk7puv,"Hello, the router in this situation does not really factor (minus the reason for congestion)

Simplified, The TCP client would potentially decrease the TCP window size if congestion is an issue thereby reducing flow of the traffic from the server. Software that uses UDP usually does have mechanisms to manage ""flow""  as well, this is just handled at different layers of the OSI model (Application).",7,0,0,False,False,False,1643964347.0
sk7puv,hvkm1vb,t3_sk7puv,"I disagree, as a network engineer by trade, with the previous poster. Your question is why QoS methods exist for managing these situations. You couldn't operate the internet if UDP just wins all the bandwidth because TCP throttles.

You don't need QoS until the transmission line is saturated. Once there, QoS provides a way to equalize the playing field between traffics based on an operators policy. QoS methods exist at L2, L2.5(MPLS) and L3 as well as L4 re:TCP to provide various techniques for handling these situations in aggregate network traffic streams.

See 

https://en.wikipedia.org/wiki/Quality_of_service

Network QoS overview (its a PDF) https://archive.nanog.org/meetings/nanog36/presentations/sathiamurthi.pdf

802.11 to DSCP L3 marking 

https://tools.ietf.org/id/draft-ietf-tsvwg-ieee-802-11-05.html

Look at the section on pcp marking at L2

https://en.wikipedia.org/wiki/IEEE_802.1Q

Class of Service definitions

https://www.omnitron-systems.com/carrier-ethernet-learning-center/carrier-ethernet-2-0-multi-cos

https://www.mef.net/Assets/Technical_Specifications/PDF/MEF_23.pdf",8,0,0,False,False,False,1643989080.0
sk7puv,hvjpazq,t1_hvjfk9q,Then could a malicious/malfunctioning application throttle TCP traffic by flooding a network with UDP packets?,3,0,0,False,False,True,1643972107.0
sk7puv,hvjsllc,t1_hvjpazq,"Yes, that's called a ""UDP flood"". It's a form of Denial of Service (DoS).",11,0,0,False,False,False,1643974545.0
skkkmf,hvlwg2m,t3_skkkmf,"Quite the _opposite_ actually: such a generator as you describe is provably _not_ random.

Random in this context means essentially that you can't predict the next value by looking at the prior values. But if you ran such a generator 1M times, and got a hundred thousand 1s, 2s, ... 8s but only 99,999 9's? Then the ""equal distribution of digits"" property means that the next output _has_ to be 9. I used the last outputs to predict, with 100% confidence no less, what the next output is. Thus, not random!",4,0,0,False,False,False,1644006279.0
skkkmf,hvlmy6r,t3_skkkmf,"From my understanding, *which is just a 2 hour lecture on discrete probability*, that the distribution of each digit should only *approach* 1/10 (1/D) of the total number of cycles (N) as there are only 10 different digits (D). I'm also assuming that TRNG stands for True Random Number Generator as in a theoretical truly random output from a uniformly distributed set of single digits (if you mean like a hardware random number generator I have no clue:P).

&#x200B;

In the 10 million cycles case, I think it is likely that each digit is close to a million (over or under a little) rather than exactly one million each. For example, even if we had a truly random coin and flipped it twice, there would only be total of one head and one tail half of the time (with the other halves been 2 heads or 2 tails) and it would be equally likely for any of those possibilities. To apply it to your scenario, it means that even if the TRNG of the digits was perfect, its entirely possible that each digit isn't at a million exactly but as we keep doing more cycles, each digit approaches 1/10 of the total number of cycles. Hope this helps but I'm not completely confident about it.",3,0,0,False,False,False,1644002705.0
skkkmf,hvlykc4,t1_hvlwg2m,"Also, if you handed me something that you claimed was an RNG and said you got equal numbers of each digit after 10M runs, I would say ""BS, no way that's random"" because the odds of that happening are roughly [one in a _thousand billion billion billion_](https://www.wolframalpha.com/input/?i=%2810%5E7%29%21%2F%28%28%2810%5E6%29%21%29%5E10%29+%2F+10%5E10%5E7)",2,0,0,False,False,False,1644007074.0
skkkmf,hvnz4su,t1_hvlwg2m,"ohhh, that's interesting! Thank you so much!",1,0,0,False,False,True,1644039673.0
skkkmf,hvlnydr,t1_hvlmy6r,"Thanks man, this was quite helpful! 

And damn this was dense!",1,0,0,False,False,True,1644003082.0
skkkmf,hvnz6o5,t1_hvlykc4,"wow, that is crazyyy rare! Makes sense! Thanks dude!",1,0,0,False,False,True,1644039705.0
sk80jv,hvlf8d9,t3_sk80jv,"You are actually pretty much right, you just omitted a factor of ""n"" from your final equation. It should be this:

&#x200B;

n \* 2 ((3/2)\^log\_2(n) - 1)

&#x200B;

Ignore the constants and simplify to get:

&#x200B;

n\*(n\^log\_2(3) / n) = n\^log\_2(3)

&#x200B;

You should also check out the master theorem as it provides an easier method to solve these problems: https://www.geeksforgeeks.org/advanced-master-theorem-for-divide-and-conquer-recurrences/",2,0,0,False,False,False,1643999814.0
sk80jv,hvljcjt,t1_hvlf8d9,"Oh man come on, how did I miss that. Thank You for the comment.",1,0,0,False,False,True,1644001342.0
sjn3gy,hvfqkvw,t3_sjn3gy,"Most of the time, you just run a simulation with a reduced data set or reduced number of rounds and extrapolate from there. Compute time is going to remain (mostly) linear for conventional systems. That means if it takes X time to compute and compare 1 round, it will take n*X time to compute n rounds.

FWIW, you don't even need to keep the distance traveled and paths for all rounds, you are only comparing to the best so far, so the compare time doesn't need to grow (linearly or non-linearly) either. Set the best so far to the first round/route you compute, then see how long it takes to compute and compare the next route/round. It should be roughly linear all the way through the maximum combination of possible paths.",10,0,0,False,False,False,1643905504.0
sjn3gy,hvg1911,t3_sjn3gy,"They are just estimating using some average computer specs and plugging in the numbers into the asymptomatic run time. Let's say an average PC can run 1 billion instructions a second, you then take the number of estimated instructions and divide it by the number of instructions per second, that gives you the number of seconds your program takes to run. You then convert that into number of hours, days, years, etc.

N! Is a huge run time. 69! ~ 171122452428141311372468338881272839092270544893520369393648040923257279754140647424000000000000000

If a program needed to run this many instructions it would take probably until the heat death of the universe to do with our current technology.",7,0,0,False,False,False,1643909370.0
sjn3gy,hvg1v6d,t3_sjn3gy,RemindMe! 5 Days \[ [https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating\_the\_run\_time\_of\_the\_travelling/](https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/) \],1,0,0,False,False,False,1643909593.0
sjn3gy,hvg46m5,t1_hvg1911,"I'd also like to point out, almost no one actually tries to calculate the true running time of a general algorithm mathematically. This is for a few reasons. For one, computers are not all the same and have different performance specs. Another reason is that no modern computer only runs a single program at a time. They concurrently run many processes and thus you can't get an accurate true run time because of the uncertainty of the computer's run-time conditions.

To combat these things, theoretical computer scientists estimate the run time of algorithms via asymptotic run time. This is the familiar Big-O notation (although there are others such as little-o, Big-Omega, etc.). But most of the time, people just want an estimate of how the algorithm's run-time grows **with respect to the algorithm's input.** In the case of the Traveling Salesman problem, the input is the number of cities or towns, and it's asymptotic running time is O(N!), which people usually just equate to ""exponential"" time. This problem is considered a HARD, or in cs terms, an NP-Complete problem (For the decision problem version) .

If you truly want to know the actual running time of your algorithm on a specific machine, you typically implement it and run it a few million times to get the average run time.",3,0,0,False,False,False,1643910421.0
sjn3gy,hvg3co1,t1_hvg1v6d,"I will be messaging you in 5 days on [**2022-02-08 17:33:13 UTC**](http://www.wolframalpha.com/input/?i=2022-02-08%2017:33:13%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/hvg1v6d/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fsjn3gy%2Festimating_the_run_time_of_the_travelling%2Fhvg1v6d%2F%5D%0A%0ARemindMe%21%202022-02-08%2017%3A33%3A13%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20sjn3gy)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1,0,0,False,False,False,1643910124.0
sjn3gy,hvihz7o,t1_hvg46m5,This dives right into the border between what computer science is and what software development is.,1,0,0,False,False,False,1643944701.0
sjn3gy,hvjaubq,t1_hvg3co1,RemindMe! 5 Days [ https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/ ],1,0,0,False,False,False,1643960816.0
skhmpy,hvkw2ui,t3_skhmpy,[deleted],22,0,0,False,False,False,1643992810.0
skhmpy,hvkvimz,t3_skhmpy,AI is unlikely to replace software development altogether anytime soon.,28,0,0,False,False,False,1643992603.0
skhmpy,hvkxuk6,t3_skhmpy,"No…we’re nowhere near becoming an endangered career field. Don’t buy the hype, we have a long way to go before Halo-style Cortana A.I.’s are running around solving all our problems. You should continue to pursue your goal(s). 

There are many exciting fields on the cutting edge of computing that you can get into (quantum computing, computer vision, edge computing, federated learning, etc.). You only live one life, enjoy your time to be a geek!",9,0,0,False,False,False,1643993459.0
skhmpy,hvkwmmf,t3_skhmpy,The jobs innovating and creating cutting edge technology on novel problems won't be going anywhere. If the models get REALLY good There might be slightly less demand for code monkeys to build generic e commerce backends much the same as HTML developers died off with more abstraction. Training a model to crunch leetcode problems doesn't actually replace anything substantial.,7,0,0,False,False,False,1643993010.0
skhmpy,hvl1y1k,t3_skhmpy,"Software hardly ever brings any “cutting edge technology”. It enables new technologies, but is hardly ever the new technology.

Also, AI will never* replace software engineers. Think about embedded systems, control systems, flight software, smart homes, automotive, etc. AI cannot even adequately replace a simple PID controller, let alone creating a full software application from scratch.. 

This post makes me think you don’t understand technology at all.

*as long as computers, processors and FPGAs exist in a similar form as they do today, then the amount of software engineers will only ever need to increase.",7,0,0,False,False,False,1643994952.0
skhmpy,hvl0yri,t3_skhmpy,"Coding is dead. No-one does real programming (machine code) anymore. They all use these ""programming languages"" that basically do all the work for you so any dummy can write code. 

/joke",13,0,0,False,False,False,1643994595.0
skhmpy,hvlcfd6,t3_skhmpy,Yes. /s,2,0,0,False,False,False,1643998774.0
skhmpy,hvmfj6l,t3_skhmpy,[_Do you know the industry term for a project specification that is comprehensive and precise enough to generate a program?_](https://www.commitstrip.com/wp-content/uploads/2016/08/Strip-Les-specs-cest-du-code-650-finalenglish.jpg),2,0,0,False,False,False,1644013596.0
skhmpy,hvl0e89,t3_skhmpy,"AI uses machine learning algorithms. These algorithms are fed data and then told which data is correct and which data is incorrect. They then use a weighted algorithm to figure out which is good and bad and use an artificial form of natural selection to evolve values that predict correctly. There are other ways AI works but this is one example. The point is that AI can't think and only work on data similar to past data. 

Programming requires a lot of creativity and you are often confronted with unique problems and unique requirements. Solutions have to be precise and there is very little room for error. There are a ton of software products programmers work with. Machine learning has no way to think or handle unique situations so they won't work.

Its possible they could do some basic standard apps like shopping apps. They are currently being used for syntax highlighting and coding suggestions for software developers because these jobs are somewhat predictable. I think they will be used to generate unit tests.

But even generating unit tests requires some creativity and they will probably generate a lot of garbage that developers will edit and fix. And if they were to generate real code it would probably be a mess.",1,0,0,False,False,False,1643994384.0
skhmpy,hvob00i,t3_skhmpy,"As long as Tesla FSD is nowhere near completion, I dont see AI coding happening.
But… exponential growth is a bitch.",1,0,0,False,False,False,1644047755.0
skhmpy,hvleo66,t3_skhmpy,"It will change. 

Machine Learning subs have recently started to complain that they can't write a better data model than those provided by some fantastic libraries where 10 lines of code replaces months of work. This is because someone smart sat down and did it for everyone else. We will always need someone smart but possibly not as many people in this field. Machine learning does what it was designed to, and majority will become users that create value not builders that create opportunity for value.

Website programming is required for information publishing (static or streaming), sales, data collection, and social. There are many easy ways to get your website out there with templates for all of these technologies. Past what templates offer you might need a developer, but it's mostly content creators that are needed once a template becomes good enough. I would argue it's not the development that is the bottle neck it is the design.  Even social media template sites exist. You will still need smart folks developing templates, but less as value creators settle for packaged products, rather than seeking out something new from value enablers.

Tool programming. I don't see this going away any time soon. Finance and logistics tools are just not capable of being perfect and handling every situation, as are content creation tools.

Gaming.. you can argue that engines can grow to a point of perfection where once again art and content will rule.

But we are about 20 years out until we reach peak programming, in my opinion, so still a very valuable and valid career.",0,0,0,False,False,False,1643999606.0
skhmpy,hvlw52p,t1_hvkw2ui,remindme! 8 years,4,0,0,False,False,False,1644006166.0
skhmpy,hvlw8yz,t1_hvlw52p,"I will be messaging you in 8 years on [**2030-02-04 20:22:46 UTC**](http://www.wolframalpha.com/input/?i=2030-02-04%2020:22:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/skhmpy/will_software_engineers_become_obsolete_by_2030/hvlw52p/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fskhmpy%2Fwill_software_engineers_become_obsolete_by_2030%2Fhvlw52p%2F%5D%0A%0ARemindMe%21%202030-02-04%2020%3A22%3A46%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20skhmpy)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",3,0,0,False,False,False,1644006205.0
skhmpy,hvmaett,t1_hvlw8yz,remindme! 8 years,1,0,0,False,False,False,1644011590.0
sjasw0,hve63y3,t3_sjasw0,"Of course it’s useful. You should know how hardware resources work on a single node before you consider a cluster. You wont use everything you learn (low level assembly comes to mind) but you will use some of it and understanding things like context switching, I/O overhead, concurrency issues, memory management, etc. will give you a better perspective on more complex distributed architectures. Imagine trying to effectively utilize a load balancer without understanding how resources are being allocated. That’s a recipe for disaster.

Now, you asked for examples so I’ll give you a personal one. I work as a data scientist. Our enterprise cloud data platform has to deliver data products (visualizations, reports, etc.) to consumers. One technical way that we measure user experience on the platform is by evaluating the performance of our Power BI capacity instance. If the utilization/load is too high then our end users will encounter delays and the overall user experience will decline/suffer. Monitoring these hardware resources and identifying which resources are being stressed helps us decide how to optimize the delivery of data products to the platform and it helps us protect against potential slowdowns. If my team and I had no understanding of computer organization, computer architecture, and operating systems we wouldn’t know how to make intelligent decisions about operating efficiently.",13,0,0,False,False,False,1643873883.0
sjasw0,hveiva2,t3_sjasw0,"Any benefit? Yes. Immediate benefit to the things you do to get started and build 'ok' distributed systems? Not really.
 
And there are big differences in what parts will be useful and what parts aren't, because even though 100% of them are abstracted, some of them will directly influence decisions you make. CPU architecture, operating systems, and most importantly the memory model and memory architecture (in detail) will be **critical** for writing performant scaled distributed systems. Compiler engineering and assembler you are very likely to never, ever use - though admittedly assembler can be useful for learning how the memory model works properly, and there are some parallels between the way compilers work and the way declarative languages or libraries often used in distributed systems work.",5,0,0,False,False,False,1643883604.0
sjasw0,hve0p2d,t3_sjasw0,"If you asking this question, you shall probably first write a simple backend service which will run in production, and not thinking about designing large scale distributed systems.",1,0,0,False,False,False,1643870253.0
sjasw0,hve6uoy,t1_hve0p2d,Thanks for the reply. I have created some backend services know a little about them. I asked this question because I would be creating large-scale distributed systems in the near future and wanted to know whether low-level engineering and computer organization knowledge would have any benefit.,1,0,0,False,False,True,1643874407.0
sjasw0,hve7ivl,t1_hve6uoy,It’s not going to help you with implementing this future project but it’ll widen your perspective about what’s going on under the hood. If you have the time go learn it. Personally I’d spend more time on the topics and technologies you primarily deal with and learn the low level stuff on the side when time permits.,4,0,0,False,False,False,1643874887.0
sjuhh6,hvi1h09,t3_sjuhh6,"Nada but you could recreate that with ~100 lines of python

1. Extract labels from an image using AWS Rekognition (or opencv). https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html

2. Pass the labels to a GTP3 endpoint. https://gpt3demo.com/apps/openai-gpt-3-playground

3. lol at results",1,0,0,False,False,False,1643937378.0
siux7r,hvaznp5,t3_siux7r,"Mathematics often has surprising applications in computer science. You might come across some problem, realize that it can be represented as some kind of finite simple group or whatever and suddenly theorems from group theory make your life easy.

Apart from that, a good grasp at some mathematics is always beneficial, no matter through what you approach it.",46,0,0,False,False,False,1643824737.0
siux7r,hvbdaio,t3_siux7r,Anything you love reading/learning is never a waste of time.,26,0,1,False,False,False,1643829787.0
siux7r,hve5noi,t3_siux7r,Its the basis of lots of cryptography. Eg the diffie-hellman key exchange uses finite cyclic groups,9,0,0,False,False,False,1643873568.0
siux7r,hvbsngi,t3_siux7r,"I've read some machine learning articles and there are  concepts which i couldn't grasp until I start reading about abstract algebra. I recall one article emphasised on ""optimizing over a smooth manifold"", hence I had to investigate and ran into AA. I also learned that a vector space is an Abelian group and by doing linear algebra, I was learning just a small subset of such an interesting subject. 

The book ""Deep Learning"" (Goodfellow et. al, 2015 (with the strange creature on the cover)) he talks about topology and how e.g. classification can be done with points in space which seem quite randomly placed and how he manages to separate (or dichotomise) them with a straight line (If I remember correctly.) I am not familiar with this field, but I see this mentioned a lot in the context of ML/DL. I believe it is crucial if you wanna be a researcher. 

To get more motivation, I suggest you check out Socratica on ""modern algebra"" on YouTube).",5,0,0,False,False,False,1643835580.0
siux7r,hvc7nym,t3_siux7r,[deleted],8,0,0,False,False,False,1643841058.0
siux7r,hvenpgl,t3_siux7r,"There are some wonderful connections to group theory in machine learning, mostly around finding symmetries and invariances",2,0,0,False,False,False,1643887210.0
siux7r,hvc1l06,t3_siux7r,"You could say that Group Theory is the basis (or one of them) in several different concepts, such as OOP (as in heritance hierarchies and relations between objects) and it also helps to understand database structures and how to build queries. You could apply that same thinking into other data sets and structures in general.

I think it generally helps you structuring abstract thinking of any elements/objects/points, and thus helps you to design better solutions.",1,1,0,False,False,False,1643838771.0
siux7r,hve6g19,t3_siux7r,You can't study any math without group theory. Almost all math is based on definitions derived from group theory.,1,0,0,False,False,False,1643874123.0
siux7r,hvfwsnr,t3_siux7r,"It depends on what do you want to do. It likely will not see much use if you are just planning to become a programmer, but if you want to be a computer scientist you will find use for it no matter which field you are in.

In general, algebraic structures like group, semiring, lattice; spaces (which in my mind are also algebraic structures) like vector space, topological space, measurable space, and Banach space; and to be more abstract, category theory, are all incredibly useful in my area.

To give you some example, programs naturally have a semiring structure, where the addition is given by non-deterministic choice, and multiplication given by composition. If you add an iteration operation on it, you will get [Kleene Algebra](https://en.wikipedia.org/wiki/Kleene_algebra): an equational theory that is vastly useful not just in program semantics and verification ( https://mamouras.web.rice.edu/other/phd-thesis-2015.pdf ), but also in abstract reduction system ( https://www.researchgate.net/publication/220118377_Abstract_abstract_reduction ).

There are extensions to it to verify different kind of systems, some of them can be found in https://mamouras.web.rice.edu/other/phd-thesis-2015.pdf ; and
 
- there are ways to reason about concurrency: https://link.springer.com/chapter/10.1007/978-3-642-04081-8_27 ; 
- network applications: https://www.cs.cornell.edu/~kozen/Papers/NetKAT-APLAS.pdf ; 
- correctness of programs: https://www.cs.cornell.edu/~kozen/Papers/typedHoare.pdf ; 
- incorrectness of program: https://dl.acm.org/doi/10.1145/3498690 ; http://link.springer.com/content/pdf/10.1007%2F978-3-030-88701-8_20.pdf

and many more.

And there are also related systems that are based on lattices, the most notable one is [Quantle](https://en.wikipedia.org/wiki/Quantale), or sometimes called Kleene Algebra à la Conway. It is a stricter notion of Kleene Algebra. And many lattice based systems are mentioned in [here](https://core.ac.uk/download/pdf/35095875.pdf)

For spaces, they are necessary to reason about probabilistic programs. For example, the famous [Kozen 81](https://www.cs.cornell.edu/~kozen/Papers/ProbSem.pdf) asserts that all probabilistic programs needs to live in a Banach space for a reasonable semantics. Later, Fredrik and Dexter follows this up with [this paper](https://www.cs.cornell.edu/~kozen/Papers/ProbSemPOPL.pdf), which uses ordered Banach spaces to formulate the higher order semantics of probabilistic programs. Topological space, given its simplicity, are still surprisingly essential in program reasoning, [this paper](https://www.cs.cornell.edu/~praveenk/papers/cantor-scott.pdf) demonstrates that choosing the right topological space for the measurable space have important practical impact on program reasoning and semantics. 

All in all, learn all the math you want, they will be useful some day if you want to do computer science.",1,0,0,False,False,False,1643907755.0
siux7r,hvj8ud4,t3_siux7r,Which book are you reading? I might check it out too,1,0,0,False,False,False,1643959406.0
siux7r,hvdv82p,t3_siux7r,Pretty sure the current fast algorithm for graph isomorphism uses some group theory.,0,0,0,False,False,False,1643866968.0
siux7r,hvfb8vg,t3_siux7r,"Not exactly ML application, but you can see monoids mentioned below. Twitter had a library called algebird open sourced few years back. Roughly the idea was to express data summarisation operations as monoids. If that was done, you would get parallel processing for free on large datasets. The interesting part here was if a given operation couldn’t be expressed as a monoid (for example finding quantiles) then you would try to find a probabilistic data structure which would let you express the approximate operation as monoid . You could then use it with algebird. This was essentially trading some accuracy to gain parallel processing.",0,0,0,False,False,False,1643899599.0
siux7r,hvfg6ls,t3_siux7r,Do you have something better you know you should be doing? If not why not give it a go and learn a subject you like? Studying group theory has many applications in theoretical CS and at the very least will build your mathematical maturity that will help you with anything rigrous you might want to do later. Life's too short to not do things that we like :),0,0,0,False,False,False,1643901580.0
siux7r,hvfpdy7,t3_siux7r, Many blockchain applications use Group Theory,0,0,0,False,False,False,1643905070.0
siux7r,hvb8pg2,t1_hvaznp5,"Totally agree. There is a matematician/computer scientist named Petter Graff. When he is hired to analyze systems to make them more efficient and reduce the cost, he looks for monoids in the application-domain. If or when he recognise a monoid, he can reduce the problem significantly and such an architecture often involves a streaming platform. I don't have the details, but look him up.

EDIT: name of the mathematician/computer scientist",15,0,0,False,False,False,1643828085.0
siux7r,hve5saa,t1_hvbsngi,Right I had the same experience with measure theory.,1,0,0,False,False,False,1643873657.0
siux7r,hvdt8h9,t1_hvc7nym,"Even in CS. Theoretical CS uses, I believe, every area of Math. I am not well-rehearsed in Group Theory, but I know a little bit about groups, fields, rings, and their applications in this area called Error-Correcting Codes.",4,0,0,False,False,False,1643865837.0
siux7r,hvjclyu,t1_hvj8ud4,Contemporary Abstract Algebra by Joseph A. Gallian..... it's easy to understand for non-brilliant students like me 😅,1,0,0,False,False,True,1643962104.0
siux7r,hvbdzqm,t1_hvb8pg2,"I find this interesting, but my quick searches (the name, ""monoids"") haven't found anything.  Do you have any references?",1,0,0,False,False,False,1643830047.0
siux7r,hvdw1gz,t1_hvdt8h9,[deleted],1,0,0,False,False,False,1643867435.0
siux7r,hvbfuml,t1_hvbdzqm,"A monoid is a semi-group with an identity element. I should  have wrote: ""..looking for a monoid..""

Google \`monoid\` an you will find a lot of resources, e.g. here: [https://mathworld.wolfram.com/Monoid.html](https://mathworld.wolfram.com/Monoid.html)

EDIT: semi-groups have a binary operator, so removed redundancy.",6,0,0,False,False,False,1643830761.0
siux7r,hvdxxjt,t1_hvdw1gz,Oh.. my bad,1,0,0,False,False,False,1643868543.0
siux7r,hvbhf6s,t1_hvbfuml,"I'm familiar with monoids and semi-groups, I meant specifically anything to do with Peter Graff, or the applications you described.  Thanks though.",1,0,0,False,False,False,1643831366.0
siux7r,hvbjqvl,t1_hvbhf6s,"Ah, I see! He held a course in 2019 where he described a transactions system which had grown over its proportion, s.t. every transaction cost .4 USD. (PayPal). By ""out-streaming"" the old hog of a system which demanded more and more servers, he would employ abstract algebra when modelling the new stream-based architecture.

 I also spelled his name incorrect. It is Petter Graff.

Here are some links: 

[https://pettergraff.blogspot.com](https://pettergraff.blogspot.com)

[https://www.linkedin.com/in/pgraff/](https://www.linkedin.com/in/pgraff/)

I cannot speak on his behalf, but like any adept computer scientist, I am almost certain that he will reply if someone reaches out inquiring for his concepts of interest.",9,0,0,False,False,False,1643832256.0
siux7r,hvblpul,t1_hvbjqvl,"That helps, thanks very much!",1,0,0,False,False,False,1643833026.0
sj5m8j,hvd1cp2,t3_sj5m8j,"Communications Networks by Leon-Garcia and Widjaja is excellent for things like ARQ and Layer 2. For TCP/IP, read the relevant RFCs to buttress TCP/IP Illustrated by Stevens; throw in UNIX Network Programming if you're feeling randy. You'll then want Kleinrock's two volumes on queuing theory. Finally, read the ip-\*(8) man pages covering iproute2, and the section 7 man pages on tcp, udp, ip, and arp.",2,0,0,False,False,False,1643853189.0
sj5m8j,hve49ez,t3_sj5m8j,Read some RFCs and also the networking book by Andrew Tannaebaum is a great read. Also the Network Warrior published by O'Reilly is a great read.,2,0,0,False,False,False,1643872603.0
sj5m8j,hve7j6y,t3_sj5m8j,"Berkeley's http://cs168.io/ uses Computer Networking: A Top-Down Approach, 7th edition by Jim Kurose and Keith Ross.",2,0,0,False,False,False,1643874894.0
sj5m8j,hvdayov,t3_sj5m8j,"BeeJs Book on Networking or something like that. Been ages, I have it somewhere on my PC.",1,0,0,False,False,False,1643857188.0
sj5m8j,hvf5atf,t3_sj5m8j,http://www.tcpipguide.com/free/t_toc.htm,1,0,0,False,False,False,1643897040.0
sj5m8j,hvfho9k,t3_sj5m8j,Professor Messer on YouTube has the whole CompTIA Network+ course for free. Very high quality (free) content.,1,0,0,False,False,False,1643902154.0
sj5m8j,hw77jvu,t3_sj5m8j,"I work in tech support and I'm involved in networking troubleshooting from time to time. Decided I want to learn more about the topic.

I'm currently reading TCP/IP illustrated volume 1. I just started so I'm not sure I'm in a position to recommend it just yet, but I've chosen it because it was recommended by several other people and so far I'm liking a lot the bottom up approach it uses to explain *everything* in details.",1,0,0,False,False,False,1644393657.0
sj5m8j,hve7vvp,t1_hvd1cp2,"Whoa I'm interested in networking research, but most texts I've read have been more on a economics/math (optimization) or probability side, I'll have to check it out for a higher level view. If you want a more math-heavy variant I would recommend [https://www.amazon.com/Communication-Networks-Introduction-Synthesis-Lectures/dp/1627058877](https://www.amazon.com/Communication-Networks-Introduction-Synthesis-Lectures/dp/1627058877) which is great albeit mathy as heck haha",1,0,0,False,False,False,1643875152.0
sj5m8j,hvfkafu,t1_hve7j6y,"USC’s CSCI 353 also uses Computer Networking : A Top-Down Approach, I’m a big fan",1,0,0,False,False,False,1643903158.0
siry8q,hvafx3o,t3_siry8q,If i=0 then y^i is empty. This does not mean y is empty.,17,0,0,False,False,False,1643817548.0
siry8q,hve0g48,t3_siry8q,Good luck with the course. It is one of the most interesting (but mathematical and rigorous) topics of CS.,3,0,0,False,False,False,1643870098.0
siry8q,hvcbl8v,t3_siry8q,Oh man pumping lemma was the worst when I took a class dedicated to T of A,1,0,0,False,False,False,1643842568.0
siry8q,hvaj08z,t1_hvafx3o,"ok got it, thanks",4,0,0,False,False,True,1643818701.0
siry8q,hve1lwv,t1_hve0g48,Yes it’s a lot of maths but quite interesting! Thanks,1,0,0,False,False,True,1643870841.0
sicshh,hv7zq38,t3_sicshh,log(2^n ) * log(n^2 ) = n log(2) * 2 log(n) = O(n log(n)) because log(2) and 2 are constants,43,0,0,False,False,False,1643768663.0
sicshh,hv8ksv2,t3_sicshh,"A resource on logarithmic properties: https://www.cuemath.com/algebra/properties-of-logarithms/

A resource on multiplying fractional exponents: https://www.cuemath.com/algebra/fractional-exponents/

Big-O notation removes constants like factors of 2 or log(2) as winniethezoo commented. Otherwise, simplifying the formulas is mostly math.",12,0,0,False,False,False,1643778141.0
sicshh,hvarttb,t3_sicshh,I've been programming or developing or engineering since 2007 and have a bachelor's degree and at no time was I ever taught this.,3,0,0,False,False,False,1643821891.0
sicshh,hv9fvr7,t3_sicshh,"It's ""just"" algebra and a bit of real one-variable calculus.

Whether you write ""n\*n"" or ""n\^2"" does not (really) change the ""number of 'n's"" in your function, as that is an ill-defined concept anyways.

Also, not every ""big-O function"" (whatever that means) must be written with ""one n"" (whatever that means"".  


Saying that f(x) is in O(n \^ sqrt(n)) is perfectly valid.",3,0,0,False,False,False,1643799954.0
sicshh,hvicymn,t3_sicshh,"log(2\^n) -> nlog(2) log(n\^2) -> 2logn

&#x200B;

2logn \* nlog2 is 2nlogn which asymptotically is nlogn",1,0,0,False,False,False,1643942458.0
sicshh,hv9km4y,t1_hv7zq38,"Adding to this

n * sqrt(n) = n * n^0.5 = n^1 * n^0.5 = n^1.5",8,0,0,False,False,False,1643803214.0
sicshh,hvauwp5,t1_hv9fvr7,Is that equivalent to O(n\^1.5) ?,2,0,0,False,False,False,1643823008.0
sicshh,hvaza9k,t1_hvauwp5,"No, as sqrt(n) ≠ 1.5 in general",1,0,0,False,False,False,1643824601.0
sicshh,hvb4v4m,t1_hvauwp5,yes,0,0,0,False,False,False,1643826670.0
sicshh,hvb4x5j,t1_hvaza9k,>n * sqrt(n) = n * n^0.5 = n^1 * n^0.5 = n^1.5,0,0,0,False,False,False,1643826691.0
sicshh,hvbke2n,t1_hvb4x5j,"True, but n \* sqrt(n) ≠ n \^ sqrt(n)",3,0,0,False,False,False,1643832506.0
sicshh,hvbpt1k,t1_hvbke2n,"true, sorry. we probably had the same confusion though",2,0,0,False,False,False,1643834550.0
sizzx9,hvc07ny,t3_sizzx9,"I'm not sure if I understand what you're asking, but if it helps an array of size 1 is always automatically sorted.",13,0,0,False,False,False,1643838274.0
sizzx9,hvbv92a,t3_sizzx9,"It took me 1 minute of googling to know it doesn't assume that. In step 2, it checks array[1] with its predecessor, which is array[0]",2,0,0,False,False,False,1643836502.0
sizzx9,hvc6k5q,t3_sizzx9,"The algorithm doesn't assume anything about the desired position of the first element in the final sorted array.
However, the first loop starts from the second element because the first element 1) has no preceding elements to compare it with and 2) if we view it as an array of length 1, it's sorted. Why would anyone care to call a single value array a sorted one? Just to highlight how the first step of the algorithm is no different from the rest of the iterations where the left part of the array is sorted.",1,0,0,False,False,False,1643840637.0
sizzx9,hvc0rf2,t1_hvc07ny,"Yeah, my mistake was assuming that this was unique to insertion sort because of the way my instructor was stressing it. Appreciate it man",2,0,0,False,False,True,1643838472.0
sizzx9,hvbzax3,t1_hvbv92a,"[https://www.programiz.com/dsa/insertion-sort](https://www.programiz.com/dsa/insertion-sort)

&#x200B;

>The first element in the array is assumed to be sorted. Take the second element and store it separately in key.

It does assume that. After some reading I found that other algo's like selection sort also assume the first value to be sorted. The difference is selection sort compares the value in question to the other values in the unsorted list first whereas insertion immediately compares it to the values in the sorted list. My confusion was thinking that counting the first value as immediately sorted was unique to insertion sort but that was wrong.",2,0,0,False,False,True,1643837946.0
sizzx9,hvcarge,t1_hvbzax3,"Nope; but they do express is in a really clumsy way, and I can see where the confusion came from.  They probably meant that the first element is assumed to be sorted because the second is picked as the ""key"" and the first has no predecessor, but the second element is still compared with the first to reorder them if necessary, and so on. The geeksforgeeks article explains it better imo",-1,0,0,False,False,False,1643842247.0
sj47go,hvcpjcc,t3_sj47go,Check out this playlist I've been meaning to watch for over a year now https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo,1,0,0,False,False,False,1643848256.0
sj47go,hvel12u,t1_hvcpjcc,I will. Tnx.,1,0,0,False,False,True,1643885254.0
si23iw,hv822yz,t3_si23iw,[deleted],2,0,0,False,False,False,1643769633.0
si23iw,hv8285n,t3_si23iw,"I honestly like this whole make more things public thing companies are doing, like GitLab publicizing its marketing handbook. Whether we like or hate google's products, we can all learn a thing or two from their engineering know-how :D

Great find op!",2,0,0,False,False,False,1643769692.0
si23iw,hv82tah,t1_hv822yz,Read it at work and call it professional development!,4,0,0,False,False,False,1643769940.0
si23iw,hvahs34,t1_hv822yz,dont read it lol,1,0,0,False,False,True,1643818244.0
si23iw,hvahtr8,t1_hv8285n,Thanks boss!,1,0,0,False,False,True,1643818261.0
shzu1j,hv5ummi,t3_shzu1j,"That's O(nlogn)

Edit: to give details: n/b*log(n/b)=nlogn*(1/b)-logb so it's in O(nlogn) for constant b. İf b is an input or parameter don't know but some distributed programming algorithms are the only place I can think of like you said",18,0,0,False,False,False,1643738675.0
shzu1j,hv6ma5z,t3_shzu1j,I feel like you’ve pry thought of this but — merge sort.,3,0,0,False,False,False,1643748953.0
shzu1j,hv778d3,t3_shzu1j,[deleted],1,0,0,False,False,False,1643756844.0
shzu1j,hv8ksnv,t3_shzu1j,Binary search?,1,0,0,False,False,False,1643778138.0
shzu1j,hv9pdcs,t3_shzu1j,"The closer K is to 0, the worse it is compared to O(n) given a large enough input.  It's just not really worse by an appreciable amount.  But, again, the size of the input is the determining factor here.

Sorting algorithms that use comparison are a good place to start, as the best case for those is O(nlogn)",1,0,0,False,False,False,1643806059.0
shzu1j,hvecxrc,t1_hv5ummi,b is not constant - it's a parameter,1,0,0,False,False,True,1643878934.0
shzu1j,hv77a0h,t1_hv778d3,"**[Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl's_law)** 
 
 >In computer architecture, Amdahl's law (or Amdahl's argument) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. Specifically, it states that ""the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used"". It is named after computer scientist Gene Amdahl, and was presented at the AFIPS Spring Joint Computer Conference in 1967. Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1643756863.0
siaup5,hv7npk4,t3_siaup5,"| **Something that you wish to see done that hasn't been done?**

Technological singularity

**| Something that you don’t wish to see done that hasn't been done?**

Technological singularity",12,0,0,False,False,False,1643763688.0
siaup5,hv9mtzh,t3_siaup5,"Human level AI, complete with emotion and sentience.

Both want it, and don't want it.",3,0,0,False,False,False,1643804584.0
siaup5,hv87p4s,t3_siaup5,Solve Chess (Draughts has been done),1,0,0,False,False,False,1643772031.0
siaup5,hv9rohu,t3_siaup5,Sentient artificial intelligence.,1,0,0,False,False,False,1643807297.0
siaup5,hv88gta,t1_hv7npk4,Yes This!,1,0,0,False,False,False,1643772366.0
siaup5,hv9rqg8,t1_hv9mtzh,"It’s basically making humans predecessors. Noble, but dangerous.",2,0,0,False,False,False,1643807325.0
siaup5,hv8csgx,t1_hv87p4s,What do you mean by solve? Do you mean create a method to solve every conceivable game?,0,0,0,False,False,True,1643774276.0
siaup5,hvbpoim,t1_hv8csgx,That's what [solving a game](https://en.wikipedia.org/wiki/Solved_game) generally means.,1,0,0,False,False,False,1643834503.0
si9m1h,hv7n9xk,t3_si9m1h,What parts did you get stuck on?,1,0,0,False,False,False,1643763503.0
si9m1h,hv7v95y,t1_hv7n9xk,I haven't started reading it yet but I read that you need prior knowledge to be able to udneratand it so i am wondering what the topics I need to be familiar with are.,1,0,0,False,False,True,1643766810.0
shi03u,hv2y0yz,t3_shi03u,"At the very least if students are coming into Computer Science knowing how to program in JS or Python, then higher education can focus more on the theoretical stuff. Granted this is the US so poor schools won't get this education",7,0,0,False,False,False,1643683205.0
sgzipv,huzl6j3,t3_sgzipv,"Its not really a two mins explanation but rather a whole course. 

I suggest looking into nand to tetris, while i didnt play it myself i heard only good things about it",41,0,0,False,False,False,1643634159.0
sgzipv,huzvwgt,t3_sgzipv,"Get the book: Code by Charles Petzold. 

This helped bridge the gap between understanding TTL, and doing the large data path hardware stuff at uni.",19,0,0,False,False,False,1643639389.0
sgzipv,hv08ozz,t3_sgzipv,"I actually study at the university where Noam (from Nand2Tetris) teaches, and it's a mandatory course for us in CS.
It's a beautiful course that get's you all the way from basic Nand gates, through all logical gates, then to RAM storage, CPU functionality (though they don't really go into how the clock operates), then into giving commands to the CPU using Binary, then to translating a low level language into binary commands, then a high level language into a low level language, building a basic operating system and eventually writing programs for that operating system, for example a game like Tetris (hence from NAND to Tetris).

There is really a lot to explain to answer your question (how do nand gates turn into binary functions that do logic, how does a stack operate to allow namespaces and classes, how the screen is drawn etc.). But this course will surely clear most of it up. Noam and Shimon did a great job with this.",12,0,0,False,False,False,1643644663.0
sgzipv,hv00zgl,t3_sgzipv,Watch [Ben Eater's videos](https://youtube.com/c/BenEater). He has some really good explanations from basic logic all the way to a full computer.,7,0,0,False,False,False,1643641570.0
sgzipv,hv1d2f9,t3_sgzipv,[Crash Course Computer Science](https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) might be helpful.,6,0,0,False,False,False,1643659681.0
sgzipv,huzwkbv,t3_sgzipv,"Nand2Tetris is great, and will explain what you are looking for. Another great book that will explain this to you is [The Secret Life of Programs](https://www.amazon.com/Secret-Life-Programs-Understand-Computers/dp/1593279701/ref=sr_1_1?crid=13U1MBSSCZ842&keywords=the+secret+life+of+programs&qid=1643639632&s=books&sprefix=the+secret+life+of+prog%2Cstripbooks%2C88&sr=1-1)",3,0,0,False,False,False,1643639680.0
sgzipv,hv1j1pb,t3_sgzipv,Computer Organisation and Design RISC-V Edition. Read the first few chapters and all your questions will be answered.,2,0,0,False,False,False,1643661934.0
sgzipv,huznvfx,t3_sgzipv,"It takes years of hard work to understand that. As another comment mentions you should try the nand to tetris course, it is a good course but it still won't teach you everything in detail. What I'd recommend is that you learn C and assembly(if you don't already know them) and then learn about systems programming and eventually slide into electrical engineering. That is the way to understand how computers go from bits to expressing logic in a somewhat detailed way.

If you wanna learn in a concise way, put some months and completely dig into the subject and search about what you do not understand, you will definitely find useful links to blogs, articles, and books that will teach you the stuff.",2,0,0,False,False,False,1643635587.0
sgzipv,hv1iqc4,t3_sgzipv,"You ever seen one of those pictures that’s a face, but when you zoom in it’s made up of faces? Computer logic is like that 7 times with the smallest face being binary. Look up the 7 layers… I forget the term but there are 7 layers of logic that are all the same thing they just each concern themselves with an aspect of all that goes into modern technology",1,0,0,False,False,False,1643661817.0
sgzipv,huzzx0k,t3_sgzipv,https://youtu.be/Zz7mcHUNWjE,1,0,0,False,False,False,1643641126.0
sgzipv,hv00np4,t3_sgzipv,"ASCII, and binary to twos complement. And digital logic.",1,0,0,False,False,False,1643641436.0
sgzipv,hv2eere,t3_sgzipv,"My stock answer is:
If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

This will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc, though it won't go into any detail of how common OS are implemented or how to implement your own (see /r/osdev for that). Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board"")",1,0,0,False,False,False,1643674469.0
sgzipv,hv2o635,t3_sgzipv,"As far as the philosophy is concerned, there's a functor between electrical circuits and logic, such that you can translate a logical model into circuitry. We simply exploit the fact that electrical circuitry behaves logically. And because logical constructions can compose and scale well, we've end up with modern computers capable of supporting vast logical models.

The big trick is that physically, electromagnetism and material physics allow us to create electrical circuits, which we can model logic on. You can use other physical systems to model logic, for example fluid computing. So computers are the result of this weird interplay between math and physics, where we can use a physical system to model an abstract mathematical one.",1,0,0,False,False,False,1643678927.0
sgzipv,hv3s1nw,t3_sgzipv,"In my advanced digital system design class we made a full cpu out of logic gates. It took a whole semester but it was really enlightening. This is what I got out of it in the most concise way possible.

Using only 5 gates you can get a 1 bit full adder. Input two 1 bit numbers and it’ll give you a result and a carry. The carry output from one can go into the input of another to make the adder able to handle 2 bit numbers. You can cascade as far as you want but modern computers use 32 or 64 bit architectures. Doing the twos complement with some more gates allows you to subtract two numbers. So we call the adder and twos complement circuit the arithmetic logic unit. Most real ALUs are super complicated but the result is the same. A digital circuit that takes two inputs, an operation code (add/subtract/AND/OR/SHIFT/ETC), and outputs a result. This is the calculator part of our calculator with a to do list.

Using more gates, you can make an SR NOR latch circuit that can hold data. Cascading these latch circuits like with the adder gives you registers. Usually a CPU has 32 registers built in and these are made with latches since they have to hold numbers. Each register holds a 32 or 64 bit number depending on the architecture. The ALU can only take input from the CPU registers so numbers that need to be crunched have to be moved from system memory to a register first.

The way this is all done is by a 32 or 64 bit number that is called an instruction. Which bit means what is defied by the instruction set, and usually they specify an operation, destination, source, and operand. Your destination, source, and operand comes from registers. 

add $1 $2 $3 
//add the numbers in register 2 and 3, and put it in register 1 
OP[31:24], DEST[23:16], SRC[15:8], OPERAND[7:0]

ld $7 300
//put the value in memory address 300 into register 7

These aren’t exactly correct but the idea is there. 
Finally you have a clock and a program counter that goes through memory one line at a time and executes the instructions. These instructions can jump to other spots in memory so you can create conditional jump statements to get your if statements. Above assembly, it’s all languages and compilers. 

Hope that cleared some of it up.",1,0,0,False,False,False,1643699285.0
sgzipv,hv3tpqv,t3_sgzipv,"Back in Uni when I had a course that used the concept of Turing Machines a lot at some point it made click

The simplest Turing Machine has 1 Belt with a row of 3 different characters on it, either 0, 1, or a Blank meaning nothing, the head can read one Character at a time and you have a table that dictates how the machine will behave. The machine has a state it starts in and for each state the behavior is defined depending on what it reads on the belt. So e.g in State Nr.1 if it reads a 1 it will change it to a 0 move the belt one character to the left and change to state 2.

This machine seems quite simple, but now imagine you have a different machine that has an alphabet of 0,1,2,Blank. Even though it has more letters in theory you can still convert any Programm it runs into a Programm that our simple machine can run by converting the characters on the belt into a unique combinations of 1 and 0s, so eg 1 becomes 01, 0 will be 00 and 2 will be 11, then you ad a step into the instructions for reading a sequence and then treating it accordingly.

Since this works with 1 extra letter it will work with N extra letters meaning our simple TM can simulate any Turing machine regardless of its alphabet with its basic alphabet.

Same goes for machines that have multiple heads or belts, you just need to add logic that uses sections of the belt that store positions of simulated heads or belts. So now since we know that our simple machine can work the same way as a complex machine, we don’t have to prove it for any machine and don’t have to worry about it, and can work with complex machines to solve more complex problems knowing that the simple machine can do it aswell

And with computers and logic gates and bits it’s analog. We have bits that are our binary alphabet, and we have logic gates that generate output depending on inputs. We know we can muse bits to simulate the decimal system, and we know we can combine our simple logic gates to things like adders that can do math. So while keeping this in mind no matter how complex it gets, it will all boil down to bits beeing combined with logic gates.",1,0,0,False,False,False,1643700472.0
sgzipv,hv3unt9,t3_sgzipv,CMU CS 15-213,1,0,0,False,False,False,1643701166.0
sgzipv,hv44hrz,t3_sgzipv,"The book ""But how do it know"" does an excellent job at explaining it imo",1,0,0,False,False,False,1643708917.0
sgzipv,hv4cj3e,t3_sgzipv,take a digital logic design course,1,0,0,False,False,False,1643715061.0
sgzipv,hw7mb6v,t3_sgzipv,Thank you very much to everyone. Too many comments to answer all!,1,0,0,False,False,True,1644405158.0
sgzipv,huzwog4,t1_huzl6j3,"https://nandgame.com


Nand To Tetris is a course but this is a game made from it that may help.",10,0,0,False,False,False,1643639731.0
sgzipv,huzml2e,t1_huzl6j3,"It's a course actually, not a game, and I came here to recommend it as well! I'm doing it right now and can vouch for it. The nature of the course is starting with NAND gates and a rough explanation of bits and how and why gates work, and then through the course you build other logic gates, and then the internal chips of a cpu, and then a whole cpu, and ram, until you've built essentially an entire computer and OS that you can use to, for example, play tetris, and you've built it completely from scratch.

But to answer OP's question, I'm not really sure what step he's looking for between bits and logic gates...

Maybe electrical/electronics engineering? But as far as computer science is concerned, there is nothing between knowing what a bit is (literally just a binary value determined by power or lack thereof), and logic gates. If I have an And gate, the output is determined by whether or not both bits are ""true"" or ""on"". HOW that gate determines it is outside the realm of computer science, because it's engineering based, but I know it's got to do with transistors so that'd be a good place to start; maybe google exactly how logic gates are engineered.

Hopefully this answers your question or sets you on the right track, I wish I could help more!",11,0,0,False,False,False,1643634916.0
sgzipv,hv2e7ul,t1_huzl6j3,"> I suggest looking into nand to tetris, while i didnt play it myself i heard only good things about it

It's intended as a third year capstone course that reiterates on everything student have learnt and puts them to use, rather than as a course that teaches you these things as it goes.

i.e. it might be a bit much for OP",3,0,0,False,False,False,1643674384.0
sgzipv,hv0x1ow,t1_huzvwgt,"^ I second this, great book",4,0,0,False,False,False,1643653728.0
sgzipv,hv486mt,t1_huzvwgt,"This is the definite correct answer

It’s the book that strips away the magic of how a computer works",2,0,0,False,False,False,1643711834.0
sgzipv,hv2dumg,t1_hv1iqc4,I really like this explanation. It still boggles my mind thinking about recursion in Verilog,1,0,0,False,False,False,1643674222.0
sgzipv,hv0i6k0,t1_huzwog4,I love this!,1,0,0,False,False,False,1643648270.0
sh0rr0,huznz1s,t3_sh0rr0,https://github.com/ossu/computer-science,16,0,1,False,False,False,1643635639.0
sh0rr0,hv0jt4e,t3_sh0rr0,Have a look at Teach Yourself CS: https://teachyourselfcs.com/,7,0,0,False,False,False,1643648870.0
sh0rr0,hv0oslt,t3_sh0rr0,"There are many resources available online. You can check out [freecodecamp.org](https://freecodecamp.org) and [geeksforgeeks.org](https://geeksforgeeks.org). There are also several youtube channels available, you can check out some of them and then go with the one you find the best. You can also check out GitHub repositories as they also have a collection of good resources. There are paid courses too, but I would recommend going with the free ones initially because they are equally amazing. After you get some basic knowledge you can consider the paid ones if you want to.",4,0,0,False,False,False,1643650695.0
sh0rr0,hv1p6we,t3_sh0rr0,"If you want to do frontend, my university teachers literally taught off of w3schools (🤌🏻)",2,0,0,False,False,False,1643664248.0
sh0rr0,hv26754,t3_sh0rr0,There's the Open Logic Project if you want to learn about that (and basic Computability Theory),2,0,0,False,False,False,1643670961.0
sh0rr0,hv34xkh,t3_sh0rr0,https://GitHub.com/qvault/curriculum,2,0,0,False,False,False,1643686290.0
sh0rr0,huzspgn,t1_huznz1s,Thanks a lot!,5,0,0,False,False,True,1643637941.0
shath4,hv1qp9p,t3_shath4,"I don’t have a proof for this, but my gut feeling says it can’t be done.

The reason is that count sort isn’t really a sort at all, it’s a cheat that lets you recreate the original sequence in order without doing a single comparison, by indexing the values in a bit vector.

As you know, this comes at a memory cost equivalent to the largest number in the sequence (or the difference between the largest and smallest). Any optimization of this would either need to project this range onto a smaller range losslessly or abuse some regular distribution property of the original sequence. Intuitively this makes it impossible to do for arbitrary input sequences. There could still be optimizations possible for special classes of inputs, but I don’t think you can do it for arbitrary inputs.

More to your point, all the ways I know of to efficiently represent sparse matrices/arrays end up losing the one property of bit vectors that count sort relies on: O(1) “ordered insert”.",2,0,0,False,False,False,1643664827.0
sgvyvn,hv2jhwg,t3_sgvyvn,"Geometry is a mesh of points and connected lines. A graph is a set of nodes and edges. Same diff

That’s why eg image size optimization is internally removing pixels based on shortest path",2,0,0,False,False,False,1643676861.0
shfux4,hv2lp1k,t3_shfux4,"I double majored in CE and CS. 

CE is much more focused on the hardware, you only take a few intro level programming courses. There are a lot of courses much more focused on low level things like hardware design, solving circuits, signal processing, and microcontrollers.

CS is more software and algorithms focused. You work with more programming languages and your homework tends to involve actually building software rather than just design. Higher level classes get into things like AI, computer security, advanced algorithms, programming language structure, and some really gnarly pieces of software like compilers.

Overall there isn't nearly as much overlap as I thought there would be when I first decided to double major",8,0,0,False,False,False,1643677830.0
shfux4,hv2cb5o,t3_shfux4,"How is this post different from [your previous one](https://www.reddit.com/r/computerscience/comments/seqqpv/is_software_engineering_a_sub_title_to_computer/)?

>If they know the hardware don’t they also know what software can be built on it and how?

u/Henrique_FB already provided quite good analogy:

>You are basically saying that a guy who makes paper can make any mathematical equation on said paper. Since he built the paper he knows how to write anything on it perfectly.

And here's mine:

>They decide it as much as car designers decide where a car can drive, with how much baggage and for what purpose.  
>^(Audi Q7 wasn't designed for agricultural work, yet I've seen it done)

And now I thought about another one: _You are saying that any mechanic is better driver than a professional race driver_  
What can also address:

>They only know high level thinks for the most part. 

Professional driver doesn't need to know how every aspect of the engine works - they only need the general idea.  
Car mechanic doesn't even need a driver license!

>So the computer engineers have the same knowledge as a software engineer

No, they don't. They usually lack - tremble please - the higher level stuff and mathematical abstraction.

>For me it feels like computer science is an easier part of hole IT, CS, CE, EE field. 

~~Are you studying CE and have some insecurities?~~ Why?

---

Also, where the idea that ""low level is better than high level"" even comes from?  
Assembly knowledge will do you jack sh\*t when you are dealing with SQL database.",17,0,0,False,False,False,1643673549.0
shfux4,hv2oh1s,t3_shfux4,"Do a physicist have the same electronics knowledge as a computer engineer?

Someone who studies physics learns how atoms work in low level. If they know the atoms, don’t they also know what pieces can be built on it and how?

For me it feels like computer engineering is an easier part of physics. They only know high level things for the most part. So the physicists have the same knowledge as an engineer and the physicist do also have knowledge of the universe and everything it contains. When does computer engineering education benefit from physics when the time to study physics and CE (both BSc and MSc) is mostly the same.",5,0,0,False,False,False,1643679060.0
shfux4,hv2d642,t3_shfux4,"You are right that there is some overlap, but maybe not as much as you think. This is a CS grad perspective, so take my CE, and EE thoughts with a grain of salt.

CS focuses on things like Data Structures, Algorithms, Programming Language Theory, etc.

CE focuses on more EE types of things like designing circuits, low level programming like assembly, and how the specifics of a computer work.

I don’t think it is fair to say that CS is the easy IT degree. It may not be as Mathematically challenging as CE or EE, but it is a challenging discipline.",1,0,0,False,False,False,1643673918.0
shfux4,hv2wj8p,t3_shfux4,"No,  they don't.",1,0,0,False,False,False,1643682561.0
shfux4,hv2sbnz,t1_hv2lp1k,"In your opinion, would you recommend a double major in CE and CS to other people? I've been thinking of double majoring between the 2 majors for quite some time now.",2,0,0,False,False,False,1643680752.0
shfux4,hv2dl6b,t1_hv2cb5o,"Quite honestly at this point it just feels like the guy is a stuck up computer engeneering student.

At least it doest seem like an honest question to me. I envy you for having the patiance to answer these.",10,0,0,False,False,False,1643674104.0
shfux4,hv3tnb5,t1_hv2oh1s,"I’m understand I’m wrong. Can you give me an example of subjects when software engineers have more knowledge than computer engineers. Algorithms, security, database?",1,0,0,False,False,True,1643700422.0
shfux4,hv2gzb6,t1_hv2d642,"> It may not be as Mathematically challenging as CE or EE

But CS is a derivative of mathematics. And CE is EE and CS combined.",4,0,0,False,False,False,1643675693.0
shfux4,hv7797p,t1_hv2d642,"As someone having an msc in CSE (computer science engineering) we learned all what you described, including AI and 3d graphics and the circuit design, assembly programming as well.",1,0,0,False,False,False,1643756854.0
shfux4,hv34au3,t1_hv2sbnz,"That would depend on your goals and motivations. If you're just trying to maximize your job potential, I don't think it's super beneficial to double major. 

But for me it was very beneficial because I didn't really know the difference between the subjects and I wouldn't have known what I wanted to do with my career if I didn't try them both. I started out as just a normal CE major and after my first year I added the 2nd CS major. And over time I grew to like CS better and that's what I got my masters' degree in and now I'm a full stack engineer. 

So I'd say if you know for sure what you want to do between the two, just major in that because the other major won't help a lot across career tracks. But, if you aren't really sure exactly what you want to do and you think you can handle the extra workload, it's a great way to explore both subjects and see which suits you best.",5,0,0,False,False,False,1643685998.0
shfux4,hv2hrfb,t1_hv2dl6b,"Nope, I've just checked their profile. Looks like OP just romanticizes the concept of embedded development.",7,0,0,False,False,False,1643676061.0
shfux4,hv4q5la,t1_hv3tnb5,All of those,1,0,0,False,False,False,1643722888.0
shfux4,hv4hk5b,t1_hv2gzb6,"I don’t think I used anything beyond Calculus 1 & 2 in undergrad CS. I know machine learning uses a lot of Linear Algebra, but I can’t remember using much math. Things we did use math for: Big O, CPU throughput, disk read speed/time, I can’t think of anything else. All of that, if I’m remembering correctly, didn’t use any math beyond Algebra. I only used Calculus in my Calculus and Physics courses.",2,0,0,False,False,False,1643718350.0
shfux4,hv3t1ev,t1_hv2hrfb,I’m actually a CS student. I found that my earlier post where a little harsh so I wanted to try again. I can’t seem to find a good answer on the internet. I’m currently finding hardware and embedd interesting and I’m trying understand if I should change path or not. Im sorry for being “weard” I’m stuck in my mind.,4,0,0,False,False,True,1643699989.0
shfux4,hv5mcai,t1_hv4q5la,So it’s not common for a computer engineer to know database and security?,1,0,0,False,False,True,1643735687.0
shfux4,hv5hqfj,t1_hv4hk5b,Haven’t you taken mathematical logic or computability/complexity classes?,3,0,0,False,False,False,1643733998.0
shfux4,hv5uamf,t1_hv4hk5b,"The word _algorithm_ is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī.  
Did you know that algorithms are math?

Have you heard about Turing Machine? The _mathematical_ model which is considered a symbolic birth of the field of Computer Science?",1,0,0,False,False,False,1643738555.0
shfux4,hv5u9py,t1_hv5mcai,"No

Add to that important topics in the CS/software industry like concurrency, distributed computing, AI.

There’s a difference between learning SQL (which many CE/EE do) and knowing when and why you should prefer a bitmap index instead of a B-tree.

There’s a difference between learning to use Spark/Hadoop and knowing why the task/job won’t gain any speed up because of the specific class of problem you are trying to parallelize.

I’ve worked with EE and CE people with masters degree and they struggled to wrap their heads around certain concepts that are (should be) trivial to CS people. Also, because their code/design decisions didn’t take those core concepts into account and often times that meant trouble to the business.

Unlike your comment, I’m not saying one profession is better than the other. The difference is that you rarely see CS people dabbling into CE/EE roles while the opposite is quite common. That’s because the job market is hot for CS roles not the other way around.

So, from a purely job market perspective. Choose wisely...",3,0,0,False,False,False,1643738545.0
shfux4,hv7818j,t1_hv5mcai,"The trick is that each university has it's own approach to computer engineering. They or somewhere between electrical engineering and computer science somewhere.  Whether it is in the middle, or almost CS but using  engineering as a glue, etc. is really up to the university.

Example:

https://www.vik.bme.hu/en/education/programs/",1,0,0,False,False,False,1643757171.0
shfux4,hv82vau,t1_hv5hqfj,"We did have a Discrete Math course that I had forgotten to mention. It touched on logic, but was mostly focused on doing basic calculations on binary numbers, add, subtract, convert to hex and back, convert to decimal and back, and 2’s complement.",1,0,0,False,False,False,1643769964.0
shfux4,hv83tmv,t1_hv5uamf,"True, though my Algorithms course didn’t focus on math, it focused on learning to apply established algorithms to solve problems. I swear it felt like we spent 2/3 of the course talking about hash tables lol

As for the Turing machine, I feel like it was briefly mentioned in my Introduction to Computer Science Theory course. That course focused mainly on formal languages, pumping lemma, and finite state machines.",1,0,0,False,False,False,1643770366.0
shfux4,hv8u5dy,t1_hv83tmv,">Algorithms course (...)  
>(...) briefly mentioned in my Introduction to Computer Science Theory course. That course (...)

Did you know that Computer Science isn't just a name for a glorified school curriculum?

>it focused on learning to apply established algorithms to solve problems.

And? How does that not fit ""algorithms are math""?

>I swear it felt like we spent 2/3 of the course talking about hash tables lol

So... math.

>That course focused mainly on formal languages, pumping lemma, and finite state machines.

All of those? Math.

&nbsp;

Basically, everything what falls under Theoretical Computer Science falls also under Mathematics.",1,0,0,False,False,False,1643783542.0
shfux4,hv9jrds,t1_hv8u5dy,"Okay, dude. Whatever. I was speaking from my experience in my undergrad program. As far as algorithms go, they may have been derived/invented using math, but all I did was call functions, no math.",1,0,0,False,False,False,1643802654.0
sg4epv,huu84fw,t3_sg4epv,An entire textbook couldn't cover all the details.,34,0,0,False,False,False,1643540323.0
sg4epv,hutvrqb,t3_sg4epv,"There is so much to cover to answer that…

You have to remember that when we started this that you could not buy “a router” — we literally wrote code to move packets from one interface to the other.  All the routing was done by hosts - big computers with multiple users - and IMPs - specialized computers built to forward packets. 

I actually laughed when Cisco started, because building routers out of software was so commonplace and so frequently done that I couldn’t fathom why you would pay money for one… 🤣

Go look for the source for a piece of software called “routed” (pronounced “route-dee”, for “route daemon”). That’s readily available and has not only the packet forwarding logic but the routing protocol logic for the major routing protocols.",32,0,0,False,False,False,1643530742.0
sg4epv,huvjnlx,t3_sg4epv,"I’m gonna try to make this as simple as possible, 

a router receives level 2 electrical signals from a “local area network” that are sent from physical “local style” addresses/devices. This can be thought of as addresses that are “below” the router that are managed by the router. The router then prepares these electrical signal IE packets of data, and converts from the “local style” addresses to “logical addresses” IE: “IP addresses”. The router takes care of knowing which IP corresponding to a “physical address” it manages below it. When it receives signals back from levels above the router itself.

The router then sends its own electrical signals up to “level 3” which uses a new protocol called IP protocol to send electrical signals through this “higher level” network.

You should look up models of how this stuff works and research deeper on your own though. I’ve glossed over many detail and or maybe gotten some details wrong. Look up “OSI model”

The stuff that handles what to do with the electrical signals is basically a driver that is a program coded to handle it",9,0,0,False,False,False,1643563885.0
sg4epv,huug7cm,t3_sg4epv,It would be fun to also ask this question on r/explainlikeimfive,13,0,0,False,False,False,1643546133.0
sg4epv,huvkkby,t3_sg4epv,"At a very high level - you're receiving internet packets (believe in IPv4 or IPv6 form) and then handing them on to the correct next step according to a list of rules you've got.

Imagine like you've got a massive pile of addressed envelopes, how would you go about delivering them?

Either there address is local enough to you that you can pop through letterbox or envelope is taken to a closer point and then the method starts again.",3,0,0,False,False,False,1643564231.0
sg4epv,huvjzg9,t3_sg4epv,"You could start by learning how to program programmable switches using p4. It will give you some idea of what a router does (receiving packets and then deciding what to do with it depending on some bits). Like others have already mentioned, this will only scratch the surface but might be a good place to start.

Some links that might help 

[https://github.com/p4lang/tutorials](https://github.com/p4lang/tutorials)

&#x200B;

https://www.youtube.com/channel/UCOQAFkDKucJWr-KafdJsdIQ",3,0,0,False,False,False,1643564012.0
sg4epv,huvhjai,t3_sg4epv,As others mentioned this is a huge topic but if you want to look at the code to one here is a link. https://forum.dd-wrt.com/phpBB2/,2,0,0,False,False,False,1643563083.0
sg4epv,huzpuih,t3_sg4epv,"ELI5 - There’s multiple isolated clusters that know how to communicate internally. Attached to those clusters is an endpoint that can traverse traffic cross-cluster

ELI10 - Clusters have unique numbers for there internal workstations (endpoints) and external receiver. You can target look up that number through a registry (eg dns or bgp)

ELI15 each cluster can internal host more clusters. Routing policies mimic the same lookup mechanism that we explained at Eli 5. Policies also filtering/forwarding and mutation traffic as it crosses between endpoints 

ELI 20. - The traffic is a stream of packets that use a protocol encoding to denote order and application specific data. You can layer protocols (see osi model) decoupling hardware, software, and routing designs 

Eli 25 - Packets route from the external endpoints to internal endpoints through the OS network interface. This construct uses bit masking to map the port to receiving application. This is a fancy way of saying it looks up a unique number and sends to an internal endpoint.. semantically like when you were 5",2,0,1,False,False,False,1643636577.0
sg4epv,huuvulr,t1_huug7cm,Was just thinking that,2,0,0,False,False,False,1643554346.0
sfiktu,hurn5m3,t3_sfiktu,Can you share the Python code you used to generate these?,9,0,0,False,False,False,1643490880.0
sfiktu,huq7kxh,t3_sfiktu,I love these. Please upload the actual images and dm me with them. I feel like you achieved those dark academia vibes with a subject not typically represented,10,0,0,False,False,False,1643469794.0
sfiktu,hurpp70,t3_sfiktu,These are awesome.  Nice work,2,0,0,False,False,False,1643491946.0
sfiktu,husrnii,t3_sfiktu,"Man, can you please share the code of Python?
Or share this images, man I’ll frame this, is beautiful asf",1,0,0,False,False,False,1643508680.0
sfiktu,hux4i5j,t3_sfiktu,"I would love to get the images too, they are gorgeous. Great job!",1,0,0,False,False,False,1643585183.0
sfiktu,huywygz,t3_sfiktu,I’d also love to have these on my wall! Been looking for science that’s presented in way that could be hung on my wall - this is it!,1,0,0,False,False,False,1643616340.0
sfiktu,hwfz1kh,t3_sfiktu,"These are awesome and my husband would totally love as a birthday gift! Mind if I request a DM as well?
Thank you!",1,0,0,False,False,False,1644540960.0
sfiktu,hurcpky,t1_huq7kxh,Thank you for the kind words! DM'd!,3,0,0,False,False,True,1643486522.0
sfiktu,huvnim2,t1_hurpp70,Thank you!!,1,0,0,False,False,True,1643565357.0
sfiktu,huvnkg3,t1_husrnii,Thank you so much!! DM'd.,2,0,0,False,False,True,1643565376.0
sfiktu,hurynlf,t1_hurcpky,"DM me too if possible. 
And yes you nailed the dark academia vibes!",1,0,0,False,False,False,1643495777.0
sfiktu,husjiq7,t1_hurcpky,If possible could you dm them to me too? They look fantastic,1,0,0,False,False,False,1643505054.0
sfiktu,husoaq1,t1_hurcpky,could you DM them to me as well and/or share the python code? thank you :) they look super cool!,1,0,0,False,False,False,1643507171.0
sfiktu,husyrmm,t1_hurcpky,Could you also DM me the images :),1,0,0,False,False,False,1643511911.0
sfiktu,hwi8pzs,t1_huvnkg3,"I'm also interested for that, it looks awesome",1,0,0,False,False,False,1644589076.0
sfiktu,huvnegb,t1_hurynlf,Thanks! DM'd.,1,0,0,False,False,True,1643565312.0
sfiktu,huvnfy3,t1_husjiq7,Thank you! DM'd.,1,0,0,False,False,True,1643565328.0
sfiktu,huvngta,t1_husoaq1,Thanks! DM'd.,1,0,0,False,False,True,1643565338.0
sfiktu,huvnhnc,t1_husyrmm,Done!,2,0,0,False,False,True,1643565347.0
sfiktu,huvnzdd,t1_huvnegb,Me too please? My mom majored in math in college and she would LOVE these.,1,0,0,False,False,False,1643565536.0
sfiktu,huvpjzf,t1_huvnzdd,That's so sweet. DM'd!,1,0,0,False,False,True,1643566132.0
sf4ab2,huo3ipp,t3_sf4ab2,Imma try some of those BBS numbers on page 8,4,0,0,False,False,False,1643423108.0
sf4ab2,huorpr8,t3_sf4ab2,Really cool find. Thanks for sharing :),3,0,0,False,False,False,1643435402.0
sf4ab2,huos489,t1_huorpr8,Glad you like it. The ads alone are gold!!,3,0,0,False,False,True,1643435646.0
sf7cyl,huo2nzc,t3_sf7cyl,"I honestly wouldn’t focus so much on the titles, these can be really confusing as you have discovered. Focus on what a team is doing and what experience you need and that tells more or less the kind of work you’ll be expected to do. 

Programmer and software engineer if we pick apart the actual words would seem to mean that software engineers are more in the business of solving software problems in new ways. This can be creating or utilizing mechanisms to write solutions or algorithms creatively. Programmers may be more basic. But the lines are so blurred and I may very well be wrong on my intuition there.",16,0,0,False,False,False,1643422718.0
sf7cyl,huo2wlc,t3_sf7cyl,"Slightly different words for the same thing. Usually ""software engineer"" is the formal job title, ""programming"" is what you're doing. Informally, we call people who program ""programmers"".",10,0,0,False,False,False,1643422826.0
sf7cyl,huo4vc0,t3_sf7cyl,"* **Computer programming** is the process of performing a particular computation (accomplishing a specific computing task). You can program a website, a game, a database, a robot etc. (notice that programming doesn't always result in creating a software).
* **Software development** is making software, regardless of abstraction level, used technologies or complexity level. Creating simple calculator in Assembly for ZX Spectrum is just as software development as is creating Facebook front- and back-end using React for modern web.
* **Software engineering** is:
  * ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"" ~ IEEE
  * ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"" ~ Merriam-Webster
  * ""an engineering discipline that is concerned with all aspects of software production"" ~ Ian Sommerville",16,0,0,False,False,False,1643423725.0
sf7cyl,huo8ye5,t3_sf7cyl,">I want to teach a computer to do things based on what I code it to do. What is this called?

It is called programming unless you want to do it well, in which case it is called software engineering.  If you want to learn how and why either works, and find ways to improve upon that, it's called computer science.",2,0,0,False,False,False,1643425605.0
sf7cyl,huod236,t3_sf7cyl,"Focus less on job titles and more on your actual set of skills, that is what will matter the most",2,0,0,False,False,False,1643427541.0
sf7cyl,huoqvrs,t3_sf7cyl,"Software engineering is working with a team to design, build and launch code. You'll potentially work with product managers, designers, QA engineers and sales in the process. I've been a software engineer at a few companies including Amazon and spend less than half my day actually writing code and the other half meeting with my team.

Programming is focused around just the building part of software development. It can often be outsourced or contracted out.",2,0,0,False,False,False,1643434898.0
sf7cyl,hupq8ln,t3_sf7cyl,"It gets weirder because in Canada Engineer is a protected job title like Doctor.

So unless you have your Bachelors of Engineering you're not a Software Engineer.

Yet by American definition I am.",2,0,0,False,False,False,1643460859.0
sf7cyl,hups8fz,t3_sf7cyl,"A programmer is anyone who writes a programme, while engineer is someone who develops an abstract idea and implements it with the best tools/people.",2,0,0,False,False,False,1643462094.0
sf7cyl,huocaid,t3_sf7cyl,"Lol.

They are interchangeable. 

Software engineer, programmer, coder, Member of Technical Staff, Software developer, Software Development Engineer, Software programmer, software specialist, they're all the same thing.

Relax.",1,0,0,False,False,False,1643427178.0
sf7cyl,huow3d5,t3_sf7cyl," Software engineer is someone with an engineering degree in the IT domain.

Everyone else might call him or herself whatever they want, like programmer, coder, etc. but not engineer.

The difference I have seen is the mindset: measure twice cut once. Everyone I worked with having an engineering degree  never jumped to coding. They always started with some level of abstraction and worked on the solution until it was correct on a logical level. They start working on the code only when this problem refinement was done, and the code is always clear, logical, and mostly works immediately. 

Also the ability to work on different abstraction levels if needed.",1,0,0,False,False,False,1643438154.0
sf7cyl,husdwew,t3_sf7cyl,The titles are designed by someone in hr with no understanding of the role. They are describing the job title in a different way to prevent you from comparing salaries in the industry and collecting the market rate.,1,0,0,False,False,False,1643502518.0
sf7cyl,huoihnf,t3_sf7cyl,"They’re all the same pretty much, just some advice, if you want to avoid a long and painful road of finding a job in tech , I’d recommend in starting a bachelors in SWE or CS. Otherwise you’ll be competing with those with degrees for this stuff and entry level is stupid competitive even for new grads",0,0,0,False,False,False,1643430242.0
sf7cyl,huq7f0a,t3_sf7cyl,"A software engineer is a programmer is a developer is a coder. It’s all the same thing. What matters more is how adept you are with what technologies, what levels of abstraction are you comfortable operating within, how independently can you operate without guidance while still producing quality work, and how well you can integrate your work with the work of others in a way that improves the overall quality of the design.",0,0,0,False,False,False,1643469723.0
sf7cyl,huo5tti,t1_huo2nzc,I have the same intuition 👍,5,0,0,False,False,False,1643424161.0
sf7cyl,huo6nie,t1_huo2nzc,"hmm, ok. I'm trying to learn how to code, and learned how to do a little bit of it through Python. But I don't know what I can do with it except to tell a computer what to do. I just know that I enjoy playing around on Python, but want to translate that into a job. Maybe I'm going through information overload.",1,0,0,False,False,True,1643424540.0
sf7cyl,huo6uh6,t1_huo2wlc,"Gotcha, that makes a bit more sense.",1,0,0,False,False,True,1643424630.0
sf7cyl,huo7aqw,t1_huo6nie,Python is awesome! All software engineering / computer science is is learning more ways of telling the computer what to do. What clever sets of instructions you can give to accomplish tasks,1,0,0,False,False,False,1643424837.0
sf7cyl,huoj6kz,t1_huo6nie,"You won’t find a job without a degree without a pretty substantial portfolio and some personal projects. Maybe master python (OOP, argument passing, polymorphism, recursion, etc) and then get familiar with database like SQL. You’ll want to make sure your portfolio stands out as you’ll be applying for the same jobs CS grads will , and most jobs filter to toss applications from people with no degree",1,0,0,False,False,False,1643430602.0
sf7cyl,huoolo6,t1_huo6nie,">But I don't know what I can do with it except to tell a computer what to do.

Theoretically, pretty much anything you can imagine - but you need to build a solid foundation to stand on first.

>Maybe I'm going through information overload.

Very likely, yes. I don't know how much you know, but my general advice to you is to take it a bit slower and try to make sense of each step along the way, staring with the very basics.",1,0,0,False,False,False,1643433552.0
seq1gi,hukwpgn,t3_seq1gi,I hope that’s from a history class.,84,0,0,False,False,False,1643377122.0
seq1gi,hulbcez,t3_seq1gi,"Sometimes I really miss college, this isn't one of those times",67,0,0,False,False,False,1643383340.0
seq1gi,hulh5yz,t3_seq1gi,"That's mid-late 1990s PC architecture.   


ISA was the standard bus on the original IBM PC and XT. It hung around until the late 1990s for legacy devices with low bandwidth requirements, like modems and sound cards. This diagram shows that the entire ISA bus could be run as a single PCI device via the ISA bridge.   


Starting in the late 1990s, many of these devices could be integrated into the motherboard or emulated by the CPU and the ISA bus was dropped.  


https://en.wikipedia.org/wiki/Industry\_Standard\_Architecture",62,0,0,False,False,False,1643385526.0
seq1gi,hukqztf,t3_seq1gi,"Legacy connection, or ""backwards compatability""",17,0,0,False,False,False,1643374230.0
seq1gi,humksl8,t3_seq1gi,"IT's so the CPU can pull information from and put information on the devices hooked to the ISA bus. Technically other things on the bus could ""see"" the signals too.",5,0,0,False,False,False,1643400254.0
seq1gi,humblyr,t3_seq1gi,Erase it from the diagram and look carefully at the graph again. Can all of the components still communicate with the CPU? What functionality might be lost?,10,0,0,False,False,False,1643396790.0
seq1gi,hunob48,t3_seq1gi,"To talk to the sound, printer and modem controller (in the diagram at least).",3,0,0,False,False,False,1643416169.0
seq1gi,huohpq7,t3_seq1gi,"A bridge allows one network to communicate with another. It is like having a group of english speakers and a group of spanish speakers that cannot communicate. The bridge would be the bilingual person translating between the two groups. Each group uses a different protocol to communicate, so without the bridge, they don't recognize or understand each other.",2,0,0,False,False,False,1643429842.0
seq1gi,hunmbpw,t1_hulbcez,I’m missing it rn,5,0,0,False,False,False,1643415299.0
seq1gi,hukrgop,t1_hukqztf,“The PCI has a bridge to the ISA bus so that the ISA controllers and their devices can still be used” This is and explanation from my lecture but the answer does seems convincing to me. Wyt?,-12,0,0,False,True,True,1643374482.0
seq1gi,hul3k3m,t1_hukrgop,"What are you confused about? ISA is a legacy bus from the 1980's, modern CPUs don't have direct attached ISA buses, because that's a waste of space.

Instead a direct attached PCI bus can provide a bridge and proxy legacy ISA for backwards compatibility of old expansion cards.

It seems silly now, but during the transition period it would have been a big deal because having multiple weird niche expansion cards was a lot more common and users wouldn't want to have to replace them all just to replace their mobo/CPU.

Edit: This is mostly relevant in industrial/scientific systems. They're usually the mostly likely to have extremely weird old hardware that they need to support decades after everyone else has moved on.",26,0,0,False,False,False,1643380197.0
seq1gi,hum3hy5,t1_hukrgop,"Are you confused about the PCI bridge too? Modern systems use PCIe, which *is not PCI*, and for systems with old PCI slots they use a PCI bridge. The bridge of either type is just that, a way to bridge an old standard to the newer one that the main system uses.

On a whim, are you confusing ISA (the slot/expansion bus) with ISA (the Instruction Set Architecture of the processor)? They are completely separate things.",7,0,0,False,False,False,1643393772.0
sf5rtw,huo10bv,t3_sf5rtw,"Depending on the language, there are specific techniques to make memory allocation and garbage collection efficient. From partially shared data structures to generational garbage collectors, different functional languages address these concerns in ways that, sometimes, even end up being adopted by imperative language compilers too. In particular, pure functional languages have compilers that exploit the lack of side effects to more aggressively garbage collect. Other specific techniques such as deforestation (for example, when an anamorphism is followed by a catamorphism) and lazy (non-strict) execution can be leveraged to reduce the overall memory use and paralellize garbage collection.",10,0,0,False,False,False,1643421952.0
sf5rtw,huplf73,t3_sf5rtw,"Don’t discount partially shared data structures too. 

If P is a linked list, and Q is the same linked list with one more item prepended to it (this is a common operation) then Q’s next pointer just points to P. Likewise if you create a new list R which is list P with the first few items lopped off, well, you can have R point to the middle of list P. Nobody is “allocating a whole new data structure.”

If somebody has a tree, and they want a new tree with a different value in a leaf node, you don’t have to allocate a whole new tree, you just have to allocate a whole new set of nodes on the path from the leaf up to the root (log n). If nobody mutates data, then we can share entire branches of the tree that didn’t change in that operation. 

Arrays are probably the biggest hiccup. Yes to truly simulate an array you’d have to allocate a whole new structure on each update. Various alternate strategies include not using an array (would a hash work instead?), having a special mutable array carve-out to make the language “less pure but more practical”, or sacrificing some aspect of arrayness such as: sacrificing O(1) lookups, secretly implement the array as a tree, and accept O(log n) look ups and/or updates instead. Log n access times??!? remember, a balanced binary tree can reference a million leaves in 20 comparisons… it feels like constant time access.",2,0,0,False,False,False,1643457541.0
sf5rtw,huo26l3,t1_huo10bv,"That makes sense! I can see how in a language that enforces pure functions you could garbage collect pretty aggressively as you don’t need to worry about a bunch of dependencies like in the jvm, you only need to worry about if a value is directly accessed again. I would imagine this would be much harder to pull off in a multi paradigm language however.",3,0,0,False,False,True,1643422496.0
sf5rtw,hur1o7q,t1_huplf73,"I’ll have to look more into that. I’d imagine that mapping certain functions to certain paths of a tree could get pretty complex with enough different “versions” of the tree or say something even more messy like an undirected graph. Is the differentiation usually accomplished with metadata stored about each node?

I can imagine replacing arrays with separate chaining hash tables could work as they can still get O(1) lookup.",1,0,0,False,False,True,1643482037.0
sf5rtw,huo2mvg,t1_huo26l3,"Exactly. However, generational garbage collectors, originally developed for Haskell, found their way into modern JVMs too, since they offer a number of advantages, like very parallelizable executions. Of course, in Java, dependencies make it more difficult to be too aggressive. There is a very good paper from Simon Peyton Jones on the generational garbage collector and some of these memory optimizations in pure non-strict Haskell.",5,0,0,False,False,False,1643422704.0
sf5rtw,hurzkc3,t1_hur1o7q,"Differentiation is usually accomplished by creating new nodes, not storing metadata about each node. Some of the pictures in this section might help: https://en.m.wikipedia.org/wiki/Persistent_data_structure#Trees",3,0,0,False,False,False,1643496170.0
sf5rtw,huog6dd,t1_huo2mvg,That’s very interesting thank you!,2,0,0,False,False,True,1643429062.0
sf5rtw,hutejr0,t1_hurzkc3,Helpful resource thank you!,1,0,0,False,False,True,1643519531.0
sepvgl,hun5viv,t3_sepvgl,thanks,0,0,0,False,False,False,1643408434.0
secwcr,huiswen,t3_secwcr,"Some of the answers here are bullshit. At the end of the day you need the math for the topics you study. Computer science is not one monolithic topic, there are lots of branches and subjects within it. You definitely don't need to know trigonometry if you want to write a compiler or a database..

Having a strong mathematical foundation will help generally, but honestly, a lot of math will only apply to specific areas. If you want to work in graphics, game development, physics simulations, etc, then trigonometry is extremely important, as is linear algebra.

If you want to work in machine learning, or AI, then you need linear algebra, but also statistics, and probably some calculus, especially for deep learning.

If you want to focus on compilers, or programming languages and semantics, then logic and set theory is the most important.

If you want to be an expert in security or cryptography then you need to understand number theory, prime numbers, etc.

I would say for ""general"" computer science, the most important mathematics is logic (often called discrete mathematics) & algebra. Beyond that it's really up to what you do within CS.",145,0,0,False,False,False,1643333201.0
secwcr,huikdxe,t3_secwcr,It will depend on what problems you want to solve. Most computer programming solutions will be mathematical in nature. Data Structures and Algorithms or heavily based on mathematical theory.,16,0,0,False,False,False,1643329623.0
secwcr,huj0xtr,t3_secwcr,"Answers here already cover most of it: if you want to be a programmer, understanding the math you use isn't exactly necessary. If you want to stay in computer science beyond an introductory level, you'll do well to actually understand what the math you use *means*. 

Computer Science (and many other STEM fields) are fields that train you to think. Understanding ""why"" something is the correct answer is arguably more important than knowing the answer itself.

But this also goes for any STEM based field, it's not really enough to just memorize steps like you're following a recipe; that will only get you so far. A cookbook is really useful for a burgeoning home cook but if the aspiring chef doesn't ever think about why types of flavors work well together or why a particular ingredient was used in one recipe but not the other... then they'll really only ever be able to cook the things in the book. It will be much more difficult to create dishes of their own.

Any area of study that you pursue, if you don't understand the basics for *why* something is useful to the problem you're solving, then generalizing what you know to slightly different problems will be that much more challenging.",12,0,0,False,False,False,1643336648.0
secwcr,huiofaw,t3_secwcr,"Computer Science, as an academic field, is very mathematical.

Most people who graduate with CS degrees work in jobs that do not require a lot of math.",24,0,0,False,False,False,1643331305.0
secwcr,huisibf,t3_secwcr,Remember that computer science does not equal programmer. Computer science is pretty much math but with computers.,22,0,0,False,False,False,1643333034.0
secwcr,huiqxy1,t3_secwcr,"""Computer science"" and ""programming"" are two different things. You don't actually need much math for programming except for in certain fields (e.g., programming a game physics engine).",16,0,0,False,False,False,1643332365.0
secwcr,huirrc0,t3_secwcr,"It really depends on what you mean by ""understanding the underlying workings of it"". And it also depends on your specialization.

For CS in general, it helps to have a feeling for maths, but it's not required to be an absolute math whizz. However, understanding the logic behind math will help you a lot with algorithms, automata and basic programming components such as recursion.

For cryptography, group theory is pretty important, and it is also the more mathematical part of CS. For programming, or web/network security, it is less of a focus.

You can absolutely get through CS without understanding all the specifics of calculus or trig (afaik they don't go that deep into it either), but you shouldn't despise it. A lot of the fun of computer science is figuring out how things work exactly and precisely, modeling it and translating it into language that is unambiguous. You can see how it is similar to maths in that way.",5,0,0,False,False,False,1643332714.0
secwcr,huj98iw,t3_secwcr,"Everything is tough the first time you learn it. Calculus will get easier (algebra is actually more important). The first day of calc we did limit proofs and I was so lost. Years later delta epsilon proofs seem trivial. Just attend the lecture, and then if you are interested, do more research. Linear algebra is definitely your friend.

Edit... memorize the unit circle, very helpful.",4,0,0,False,False,False,1643340289.0
secwcr,huka7nc,t3_secwcr,"I am sorry that you have to go through all this because of degree fetishism, even for seemingly unrelated jobs like CRUD applications. But university should not give you automatic job expectation. You should ideally be there for science, not to have a guarantee to land a webdev job.",5,0,0,False,False,False,1643362508.0
secwcr,huik11r,t3_secwcr,Computer science is literally mathematics. You did sign up to be a mathematician.,46,0,0,False,False,False,1643329475.0
secwcr,hundw9k,t3_secwcr,"Not to be a jerk about the question, but I don't understand why the math component of cs is such a concern. 

I feel like this is a recurring post at least monthly",3,0,0,False,False,False,1643411722.0
secwcr,huik9s1,t3_secwcr,Math to some extent is important In comp sci but to fully understand computer science you by no means have to be a math expert. I had the same question when i started my degree in computer science and the answer that was given to me was that I'd need to learn the math that was relevant to certain aspect of the field but unless you go into a job that requires advanced mathematics you probably won't require anything beyond calc 2 or 3. I'm no expert though and others probably have different experiences in the field.,5,0,0,False,False,False,1643329575.0
secwcr,huj4akg,t3_secwcr,"Depends on the uni. Some CS programs are more math heavy 

In real life, only a small percentage of devs use heavy math",2,0,0,False,False,False,1643338110.0
secwcr,hulhts2,t3_secwcr,"\>  I need to literally understand why   
everything works the way it does for Trigonometry, Calculus, and   
whatever other math is needed for Computer Science.

What specifically do you mean by this? Are you asking about understanding what it's built of? Or like formal proofs?",2,0,0,False,False,False,1643385773.0
secwcr,hujyjjk,t3_secwcr,"You actually did sign up to be a mathematician. Computer Science is a sub field of math.

Almost all of computer science is math, you just don’t think it is. Discrete math, combinatorics, category theory, are all things you will use on the regular when writing code, but it’s very different math than what you are thinking. 

And no you don’t need to know calculus generally.

Math is much much bigger than just algebra/calculus/geometry",4,0,0,False,False,False,1643353921.0
secwcr,hujkfpn,t3_secwcr,">Like, I didn't sign up to be a mathematician

On a tangent, CS itself is a runaway branch of Mathematics. It (or at least it's theoretical basis, which all that hardware just implements) was spawned literally by Mathematicians.",2,0,0,False,False,False,1643345618.0
secwcr,huj50nk,t3_secwcr,"I've been a code monkey for 40 years now; for the past 20 I've worked as a back-end developer for high-performance computing clusters. I've never needed math more complicated than boolean algebra to do my job.

That said, there are areas of computing where you **will** need more math - crypto, for example,  or image or signal processing. Beyond that, you need to be able to think logically and have a firm understanding of how your code and your runtime environment work.",1,0,0,False,False,False,1643338427.0
secwcr,huk9bdo,t3_secwcr,Really depends on what field you go into.,1,0,0,False,False,False,1643361792.0
secwcr,hukzwc0,t3_secwcr,"If you want to get through school with a degree in CS then yes you need to know a lot of math. Math details the language of logic and is what the actual science of computers is. If you want to be a developer who writes application code then no you do not need to know the theory that in depth, but it is harder to get a job as a developer without the degree and you might face some inequality in the workforce for not having one.

Yeah you are gonna have to truly understand the base of it all, depending on your country and school, but if proofs is in your curriculum then you gotta really understand it",1,0,0,False,False,False,1643378604.0
secwcr,hul8syl,t3_secwcr,"probably not, but to pass your next test maybe.",1,0,0,False,False,False,1643382347.0
secwcr,hulkps4,t3_secwcr,"It’s not necessarily about any specific topics in Mathematics, but the general aptitude and confidence to grasp a given mathematical concept - is what’s required of a good computer scientist. 

I might not have read about modular theorems before, but if it ever comes up as a background for something real, I must be capable of picking it up swiftly. This general ability and state of the mind is what’s truly needed.",1,0,0,False,False,False,1643386852.0
secwcr,hulokkx,t3_secwcr,"You don’t really need to understand every nook and cranny, just the general concept.

I’m a CS student & as well a math minor in my second semester. Majority of my home work is based on general calc & trig for math & just typical coding problems (I’m a first year so I know it’s nothing as of now) 

Math in general for CS just basically helps you, it’s just your decision if you want to spend a lot of time into it.",1,0,0,False,False,False,1643388270.0
secwcr,hum9da2,t3_secwcr,Calculus is very valuable for CS.,1,0,0,False,False,False,1643395954.0
secwcr,humsez1,t3_secwcr,"From what all of my academic advisors have told me is that math is something that might apply to your career, but the bigger emphasis (at least at my school) is the problem solving skills you learn is the main benefit. Learning how to solve a problem using concepts you know is super beneficial. 


Im in calc 1 right now and they care far more about the why of the theorem opposed to the computational skills (grades care about computational skills, calc as a subject cares more about the why)",1,0,0,False,False,False,1643403168.0
secwcr,hune1ek,t3_secwcr,Being good at maths makes you a better programmer imo cause it allows you think about ways to solve problems faster,1,0,0,False,False,False,1643411781.0
secwcr,hul5km9,t1_huiswen,"For the vast majority of working professionals, even in fields where the mathematics is relevant, the important part is usually a good grasp of concepts. You definitely need calculus for a lot of topics in Computer Science, but mostly what that means is that you recognize an integration problem when you see one, that you know that the the derivative of a function is 0 at an extrema, etc. 

I have a PhD in Machine Learning, and if you dropped me into a Calculus II exam this morning, I might not crack double digits. I simply don't remember enough technical mathematics to be able to start from ""what is the indefinite integral of this reasonably complicated function f(x)"" to getting a correct answer. What I can do is see a real problem in my field and understand that what I need to do to get past that problem is formulate and solve the right integral. From there, there are loads of tools available to me that don't require me to be able to pass the undergraduate exam on the topic.

That said, to be able to get to that point in my field, I did at some point have to know how to pass the Calculus II exam, so if it's really something that you can't get past, it can still be a very real brick wall.",10,0,0,False,False,False,1643381043.0
secwcr,huiz9cd,t1_huiswen,"For my degree linear algebra, calculus, and statistics were all required.",13,0,0,False,False,False,1643335922.0
secwcr,hukmgo7,t1_huiswen,">If you want to focus on compilers, or programming languages and semantics, then logic and set theory is the most important.

Hot take: If you are unable to understand high school algebra you are going to have a very hard time with stuff like Galois connections. The formal reasoning is the same everywhere.",5,0,0,False,False,False,1643371535.0
secwcr,hukavyl,t1_huiswen,"By discrete maths and algebra for general compsci do you mean abstract algebra? (Groups, rings, modules, category theory etc)",2,0,0,False,False,False,1643363053.0
secwcr,hujjz6g,t1_huiswen,i would agree with this answer in a software development sub but in a computer *science* sub?,-2,1,0,False,False,False,1643345385.0
secwcr,huiz4wg,t1_huiswen,"You definitely need to know things that require trigonometry to learn (or at least are approached via trig even if they don't strictly require it) in order to write a good, performant compiler or database.",-1,1,0,False,False,False,1643335868.0
secwcr,huj4ca4,t1_huik11r,Is this a joke?,-15,0,0,False,True,False,1643338132.0
secwcr,huj5vre,t1_huik9s1,">be a math expert

Good thing those math credits weren't counted towards my major average....lol",1,0,0,False,False,False,1643338809.0
secwcr,hujljfp,t1_hujkfpn,"So, will I have to explain how math works at the fundamental level? I don't think even Computer Scientists know everything that there is to know about math.",1,0,0,False,False,True,1643346196.0
secwcr,huj5c5n,t1_huiz9cd,"They were required for my degree, as well, but I never need linear algebra or calc to do my job(s). For statistics all I've ever really needed are the basics that I learned in high school - mean, median, standard deviation, and I only use those when discussing the performance of an algorithm.",11,0,0,False,False,False,1643338568.0
secwcr,hujkv9n,t1_hujjz6g,"I actually majored in Math and CS, so I think I have a good perspective. I honestly used very little of the math for my CS. My CS was split into two, theory and application. The theory was mostly about proving properties of algorithms, like correctness or complexity, or things like denotational semantics, proving properties of languages, and lastly topics like abstract machines. It was very theoretical but not very math heavy. The applications side was the same, learned about CPU architecture, programming languages, compilers, data structures, computer vision, databases.. more I don’t remember, again not a whole lot of math to be honest. I would say I rarely used math beyond high school level during my CS. There are some exceptions, like I mentioned. For computer vision it’s very important to understand linear algebra and trigonometry, but that’s quite a niche topic.",10,0,0,False,False,False,1643345843.0
secwcr,hujw4dq,t1_hujjz6g,"There are a great many software developers on this sub who think they are computer scientists. Likely because their university called their degree or major computer science, but that doesn't mean they aren't still wrong, either.",1,0,0,False,False,False,1643352342.0
secwcr,huj55ni,t1_huiz4wg,What do you need trigonometry for in a compiler?,6,0,0,False,False,False,1643338488.0
secwcr,huj4io2,t1_huiz4wg,How many people write databases and compilers? A tiny minority of devs,-5,0,0,False,True,False,1643338209.0
secwcr,huj85rd,t1_huj4ca4,"You must not even understand what computer science is to ask such a question... Computer science is not a synonym for programming or software engineering, it is a branch of mathematics that focuses on the theory of computation, efficiency, etc.",28,0,0,False,False,False,1643339816.0
secwcr,hujmvik,t1_hujljfp,">So, will I have to explain how math works at the fundamental level?

You'll have to know enough math to be able to understand the concepts in a particular sub-field that's employing that math.

**Example** : Lets say you're solving a recurrence relation by substitution (useful for finding time complexity of recursive algorithms).

You need to be able to recognize an arithmetic/geometric progression when you see one and know how to find sum of such progressions.

Stuff like that.

On a different note:

1. Being able to apply math and understanding how it works aren't two separate things. To do the former, you need the latter.
2. If your college demands you take a math course, you'll be taking it and understanding the content, one way or another.
3. There are topics in Discrete Maths (commonly taught in CS programmes) that do explore how certain bits of Math work at a fundamental level (eg. formal definition of what a Boolean Algebra is in terms of sets and lattices).

...

If I had to sum it all up, if you're going to do CS, it's not very productive in the long run trying to create a border between what math you can skip and what you can ""comfortably bear"". Things will become unnecessarily complicated.  

Just go ahead and master whatever bit of math they throw at you. Get comfortable with it.",3,0,0,False,False,False,1643346905.0
secwcr,hulibh7,t1_hujljfp,"> I don't think even Computer Scientists know everything that there is to know about math.

No one knows everything there is to know. There's just so much out there no one person can know even half of it.",2,0,0,False,False,False,1643385961.0
secwcr,hulcegm,t1_hujljfp,"Not a single person knows all there is to know about mathematics. It's a huge domain and computer science is only a subset of a subset.

You don't need to worry about perfect recall as long as you know enough to derive one idea from another. Professors will forget a lot of the basics through their time, and what you're really supposed learn in a university maths degree is to ""mathematical literacy"". It's like learning to read, you don't need to read all books or memorise all words, just how to parse what's in front of you, and use context to ask the right questions.

If you want to be a software engineer, you really only need to know the bare minimum for whatever field you're working in. The ability to reliably recognise the rough outline of a problem and to Google around for a solution will probably get you into the top 20% of developers.",1,0,0,False,False,False,1643383745.0
secwcr,hujn81b,t1_hujkv9n,"I think you're limiting your definition of math in saying you used very little math.. 

To me, 'proving properties of algorithms, like correctness or complexity, or things like denotational semantics, proving properties of languages, and lastly topics like abstract machines', are literally math. You don't have to be familiar with a huge number of mathematical objects, but you're applying mathematical reasoning and using math's tools.

If you're finding bounds on functions (deriving algo complexity), or using induction to prove correctness, you're doing undergraduate-level math.",12,0,0,False,False,False,1643347089.0
secwcr,hujyztx,t1_huj55ni,"Not exactly what you asked but, I mean, the math behind compiler shit is so difficult (think about language automata, grammars, parsers, optimizations) that I'll go out on a limb and say that if you can't understand trigonometry don't even try to write a compiler.

Sometimes, especially in undergrad, learning math is useful to teach you mathematical thinking, whatever is the subject.",4,0,0,False,False,False,1643354230.0
secwcr,huj6flh,t1_huj55ni,Not what I said. Read it again.,-6,0,0,False,True,False,1643339044.0
secwcr,huj6hbs,t1_huj4io2,Most devs aren't computer scientists.,15,0,0,False,False,False,1643339065.0
secwcr,huj9dmh,t1_huj85rd,Exactly,6,0,0,False,False,False,1643340351.0
secwcr,hujn15i,t1_hujmvik,"Yeah, I mean I'm up to the task; really, I'm just trying to gauge the expectation.",1,0,0,False,False,True,1643346989.0
secwcr,hujq94t,t1_hujn81b,"Yeh fair enough, getting the definition right is important, in math and on Reddit!",5,0,0,False,False,False,1643348777.0
secwcr,huj9o8u,t1_huj6flh,What do you need to know that requires trigonometry to learn in a compiler?,9,0,0,False,False,False,1643340482.0
secwcr,hujay87,t1_huj9o8u,"Calculus (actual, not lambda) is usually built from pieces including trig (hence the name of the OP's class) and is used extensively in optimisation. You can write a compiler without it, but you can't write a good, performant compiler.",8,0,0,False,False,False,1643341052.0
sf5gj9,huop9c6,t3_sf5gj9,"A TCP load balancer will send a NEW TCP to the next server in a round robin manner, but once the connection has been established it just stays with that server. There is state that tracks the connection to route the packets for that TCP connection to the same server. This is the same as normal HTTP except that websockets typically keep the connection open for longer whereas HTTP connections for web pages or API requests are typically much shorter lived. This means that a naive RR LB may be less effective for Websockets unless it keeps track of how many connections a particular server still has open and tries to send new connections to servers with fewer connections.",1,0,0,False,False,False,1643433934.0
sf7tjb,huokv73,t3_sf7tjb,"Could you rephrase the question? There haven't been any notable programming paradigms developed recently, except maybe type-driven/type-oriented (like in Idris 2), and even that has strong roots in old theorem proving techniques.",2,0,0,False,False,False,1643431485.0
seqqpv,hul08jf,t3_seqqpv,"A few things,

People already replied about how none is truly a subset of the other - which is true, but a lot of computer engineering students (and some EE as well) ends up with purely software jobs. And some cs students ends up in a more hardware/CE jobs. 

Its more likely for a CE to get into CE jobs as they took much more relevant courses (advanced computer arch, semiconductors etc) but those are mostly electives for cs in most places - so they could get into jobs requiring those course as well. 

CE jobs include to build the micro-architecture that runs a specific instruction set. Deciding the actual instruction set is done by very senior designers where the degree does not matter at all.

Its not programming in assembly but rather describing how a hardware component should behave - understand and study its limitation regarding temperature, voltage, frequency etc. 

Oh and 
>  Is computer engineering higher prestige than software? 

Prestige should not be a factor since you finished high school.",4,0,0,False,False,False,1643378756.0
seqqpv,hukymcf,t3_seqqpv,"Few definitions/descriptions:

  * **Software development** is making software, regardless of abstraction level, used technologies or complexity level. Creating simple calculator in Assembly for ZX Spectrum is just as software development as is creating Facebook front- and back-end using React for modern web.
  * **Software engineering** is:
    * ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"" ~ IEEE
    * ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"" ~ Merriam-Webster
    * ""an engineering discipline that is concerned with all aspects of software production"" ~ Ian Sommerville
  * **Computer Science** is the study of algorithms, computation and information. It spans a range from theoretical studies to practical issues of implementing computational systems in hardware and software.
  * **Computer Engineering** is a branch of engineering that integrates several fields of Computer Science and Electronic Engineering required to develop computer hardware and software.

As one can see, all of them overlap, but none is fully a subset of another",9,0,0,False,False,False,1643378019.0
seqqpv,hulcrdo,t3_seqqpv,"I have an MSc degree in computer (science) engineering and have 20+ years of experience in the field.

In the past we had mathematicians and electrical engineering. Both area started to use more and more computers therefore two title was born, from the math side programmer-mathematician (a mathematician who writes computer software), and from the electrical engineer specialized in low-voltage current computers . The programmer mathematician brought in the pot the math parts, then electrical engineer brought in the engineering methods and principles. Now, it really depends on the history of the school.

If the school created it's program as an offshoot of the math department it will be called computer science, and they will teach you programming from the point when you already have a blinking cursor. If the school however created it's program from electrical engineering (like mine) you will start with the hardware, and start gaining skill from there.

There is a big part which will be common like programming languages, networks, algorithm theory/data structures/math, cryptography, 3d graphics, etc. so if you are doing a regular programmer job, it does not matter which direction you went.

However, there are differences as well. In computer engineering you will have some digital systems/electronics/embedded development/assembly/etc. courses, system design (like SysML), modeling and model validation, fault tolerant systems and maybe some specialized math courses as well (signals and systems for example).

What I see the biggest difference is the mindset. The engineering way is a very multi-disciplinary way which is based on the well established engineering principles (theory, model, validate, etc.). It uses things from lots of disciplines and combines them to solve a problem in a computer domain. For me the computer science looks more like programming, but on steroids.",2,0,0,False,False,False,1643383880.0
seqqpv,hukxt19,t3_seqqpv,"In general terms computer engineers design computer *hardware* while software engineers provide the software.  There is some crossover in the areas of firmware, device drivers, etc.",4,0,0,False,False,False,1643377644.0
seqqpv,hul8yqf,t3_seqqpv,"Computer Engineers take hardware and software courses. Software engineers only take software courses. From a general point of view, computer engineers know how the computer will work from start to finish and produce hardware and software accordingly. Software engineers, on the other hand, abstract the hardware and can focus more. But I must underline that we have undergraduate education here. Undergraduate departments of universities are for training academician candidates. If they want to educate software developers, they can open a vocational course. I think software engineering is in this category.",2,0,0,False,False,False,1643382410.0
seqqpv,huln16p,t3_seqqpv,"You are basically saying that a guy who makes paper can make any mathematical equasion on said paper. Since he built the paper he knows how to write anything on it perfectly.

There is MUCH more to computers then you imagine. And knowing how to build one does not make you good at writing stuff inside it.",2,0,0,False,False,False,1643387706.0
seqqpv,hul4t6t,t3_seqqpv,"Prestige? I might be reading into this and if I am ignore the rest...

It's like you're trying to compare the two for clout. Sounds kinda douchebagy and shouldn't be how you decide an education or career path. If you study computer engineering you're not smarter or better than a software engineer, hopefully you're interested in hardware. Both have their roles.

Hell, you could be a garbage man, and that should be ok too and nothing to look down on if that's your calling. If the tone I'm picking up is accurate, this is going to make for real difficulty in the team fit department....",1,0,0,False,False,False,1643380723.0
seqqpv,humx8b2,t3_seqqpv,Computer engineering is the physical part software engineering is the programming part,0,0,0,False,False,False,1643405027.0
seqqpv,hukypql,t1_hukxt19,Okay my understanding is that computer engineers decide what can be written on the machine because they make the architecture?,1,0,0,False,False,True,1643378062.0
seqqpv,hul5k8m,t1_hul4t6t,"Yep I know it sounds like that, sorry. I’m currently studying BSc web development/software engineering but I’m also into hardware and embedded systems so I don’t know which path to choose. So I’m starting to think that hardware is cooler cause that’s the core and that’s why I should choose embedded systems. But I also like software…",1,0,0,False,False,True,1643381038.0
seqqpv,hukz99b,t1_hukypql,"They decide it as much as car designers decide where a car can drive, with how much baggage and for what purpose.

^(Audi Q7 wasn't designed for agricultural work, yet I've seen it done)",3,0,0,False,False,False,1643378310.0
seqqpv,hul8rwz,t1_hul5k8m,"It's not about what's more cool in a prestige sense, unless you're rich already, it's about what you like, can stand doing, can do well at, and can find a job in.

Sure your degree might give you a bump depending on where you got it, or what its in, but that's only because a subset of people with the douchebag mindset made it through the team fit and personality test as a false positive.

Having done interviews for a large biotech firm, I cant tell you how many smart, but obnoxious people we've declined to go forward with. It makes bad team fit if wonderboy talks shit on the TPM, the test engineer, and the support personnel. You wouldn't want to do their jobs: it takes precision and exhausting levels of effort, and they too deserve respect.

This is in the general life advice category now: if you're really smart and gifted and are making great decisions, that's awesome. Good for you. It does not make you better than anyone else. If you're a genius, you didn't choose your genetics, or your luck, so you don't choose to be smart. Be humble. Also, be fair, plenty of people don't want to dedicate their lives to a profession, and that should be honored too, or they maybe have different callings. Many people arent as smart, or gifted, or didn't have the best start, and we shouldn't think poorly of them. Others are smart but uninterested in making money.This kind of ""I'm smart I deserve to look down on others"" attitude I see has lead to the world we live in now. It's a sad place where majority live in precarity, and have hard lives.

I thought VLIW was really cool in embedded.

I don't work in computer engineering, why not find someone who does and ask them about the profession, and how they like it etc?",2,0,0,False,False,False,1643382335.0
sdur26,hufwzma,t3_sdur26,The Algorithm Design Manual by Steven Skiena. He’s a professor at State University of New York and hes posted all his lectures online. https://www3.cs.stonybrook.edu/~skiena/373/videos/,18,0,0,False,False,False,1643293671.0
sdur26,huf7mam,t3_sdur26,Grokking algorithms is also a good book.,10,0,0,False,False,False,1643278836.0
sdur26,huf9bk8,t3_sdur26,I'm in the same exact situation as you're and I really find the book A Common-Sense Guide to Data Structures and Algorithms By Jay Wengrow to be really really good.,23,0,0,False,False,False,1643280191.0
sdur26,hugxiwg,t3_sdur26,The Art of War  - sun Tzu,23,0,0,False,False,False,1643307300.0
sdur26,hufjcx6,t3_sdur26,Grokking algorithms is a great intro,5,0,0,False,False,False,1643287028.0
sdur26,hugv7p2,t3_sdur26,"I found Sedgewick to be approachable for self study and a decent intro to the topic.  He has versions of his book out with code examples in many popular languages such as ""Agorithms in C"" or ""in Java"" or whatever.",3,0,0,False,False,False,1643306473.0
sdur26,huhdung,t3_sdur26,Intro to algorithms 3rd edition,3,0,0,False,False,False,1643313196.0
sdur26,huhroqp,t3_sdur26,"""The Design & Analysis of Algorithms"" 3rd edition by Levitin, Anany is a good algo book that is language agnostic. Used currently by my prof in an Algo class I'm taking (BS in CS).",3,0,0,False,False,False,1643318306.0
sdur26,huf5bhm,t3_sdur26,CLRS.,27,0,0,False,False,False,1643276963.0
sdur26,huh7xsl,t3_sdur26,"Algorithms with C by Kyle Loudon. It is a very well written book that takes you from basics. The only annoying thing is the comments in the code waste so much space. Other than that, it is a very good book.",2,0,0,False,False,False,1643311029.0
sdur26,huilgti,t3_sdur26,"https://teachyourselfcs.com

This is a great website that points you to various resources for teaching yourself CS concepts.",2,0,0,False,False,False,1643330075.0
sdur26,hugit8w,t3_sdur26,"You want lectures and other materials by Leiserson and/or Skiena. Leiserson taught most people writing on the subject, including Skiena. Both have lectures available on youtube, and have authored books together.",1,0,0,False,False,False,1643302096.0
sdur26,huh9kaj,t3_sdur26,"algorithms to live by, it's not a book for CS education but very fun and useful for beginners",1,0,0,False,False,False,1643311624.0
sdur26,huhctk5,t3_sdur26,Not a book but something to use as a companion piece for whatever book you end up going through. https://www.geeksforgeeks.org/fundamentals-of-algorithms/ helped me out so much.,1,0,0,False,False,False,1643312822.0
sdur26,huhep3x,t3_sdur26,I'm using algorithm design by kleinberg and tardos right now and it's great.,1,0,0,False,False,False,1643313506.0
sdur26,huhf58k,t3_sdur26,"I haven't gone through it but I picked up  ""Algorithms In A Nutshell""  by George T. Heineman, Gary Pollice, and Stanley Selkow.",1,0,0,False,False,False,1643313668.0
sdur26,hui5m5m,t3_sdur26,"Algorithm Design Manual by Skiena

A bit old school from a language point of view, but taught me a lot.",1,0,0,False,False,False,1643323574.0
sdur26,huiem9p,t3_sdur26,I don’t compute! What is this education background being equated to not being self taught?? There are very few professors that can explain things where you don’t have to teach yourself! 😂😂😂,1,0,0,False,False,False,1643327212.0
sdur26,hufgq7w,t1_huf9bk8,Seconded this book. Read this OP.,7,0,0,False,False,False,1643285429.0
sdur26,hufbok0,t1_huf9bk8,"hey i have a question, should i learn algorithms first or data structures?",2,0,0,False,False,True,1643281983.0
sdur26,hufnocx,t1_huf5bhm,"To clarify for people who might not get it:

Introduction to Algorithms, by Cormen, Leierson, Rivest and Stein.",27,0,0,False,False,False,1643289366.0
sdur26,hujqz3z,t1_huf5bhm,"I see this infatuation with CLRS being promoted everywhere and touted as the be-all-end-all book to (self-)study for the subject. I honestly believe that this does more harm than good. I tried to self-study under CLRS before I started college. It made me scared of algorithms. I went back to it a few times during my undergraduate years, but I still never found it to be all that pleasant of a read. 

I still have this book on my bookshelf. I do go through it from time to time, and by this point, I'm familiar enough with the subject area that I no longer find it as daunting as I did during undergrad. However, for a self-learner, I truly believe that the road to understanding how to understand this book is a decade-long endeavor. For this reason alone, I don't think it is a good recommendation to push onto people as a self-study text.

----------------

I feel like the reason for the hype of the book is because of existing hype. This book was great when it first came out because it was one of the first comprehensive self-contained textbooks that generalizes several modern algorithmic patterns. It made the runtime analyses less of a combinatorial mindfuck and relegated it as almost a secondary citizen in comparison to the intuition and the soundness proofs. It deserves a definitive place in the history of modern algorithmics. By comparison, it is fairly accessible and acted like a great cross-disciplinary introduction to the systematic study of algorithms. Pick any modern Algorithm Design text: Skiena, Kleinberg, Sedgwick, and go through its table of contents. Their sections are almost all laid out in the same order as CLRS.

This is why CLRS is impressive, it is so comprehensive that even after over a quarter of a century, new textbooks in the field still follow the same overall structure that it first pioneered in the early 90s.

However, it's not an easy read. I don't just mean that this is a dense technical brick. 

It lacks a certain coherence. Some of the problems emphasize intuition, others emphasize rigorous proofs of time complexity, and still others emphasize a non-intuitive leap of logic required to prove the correctness of these algorithms. How do you show the correctness of greedy algorithms? Beats me. Within specific chapters, the problems and examples presented seem to jump all over the place as well. Even within the same example, you trampoline around from intuitive constructions onto awkward case analyses of some (unintuitive) abstract representation of the problem. There really isn't a unifying thesis for each chapter (or even each problem it discusses). Of course, there aren't any algo-design texts out there that have solved this problem, but I've definitely gone through a few other textbooks that are significantly more coherent and understandable. (In fact, take any two of Skiena, Kleinberg, Sedgwick, Erickson, and Dasgupta and you'll have a more readable presentation of everything covered in CLRS).

----------------------

Look, I don't want to knock on CLRS for the sake of knocking on it, but I really don't think that this is the textbook to push onto people looking to study algorithms on their own. 

I took both of my undergrad and graduate courses in algorithms with Kleinberg (both the textbooks and the lecturer) and the one thing that I am most grateful for in my second attempt to understand this material is the clarity of the presentation. If it weren't for the fact that I had a fantastic lecturer, it would have taken me many more trials before I would have been able to grasp the topic. This despite my general view that Kleinberg's textbook is much more difficult to get through than CLRS.

Unfortunately, CLRS - on its own - does not stand out particularly well when it comes to clarity of presentation.

----------------------

First - texts focused on **designing algorithms**:

I can also talk about the (non-CLRS) textbooks that were popular when I was in undergrad: Ericksons, Skiena, Kleinberg, Sedgwick, (and Dasgupta).

My favorite of the batch is **Sedgwick's Algorithms**, it's the most accessible of the batch, and takes a very grounded approach to help build up your intuition for the subject. For many of us, rereading it after more rigorous training may feel like this is almost too introductory, but that's what makes it an especially great guide to learn from on your own. My favorite part are the runnable examples that help to (literally) visualize these algorithms you're studying.

**Skiena's Algorithm Design Manual** is definitely the funnest read of the batch. The presentation on the actual algorithms themselves is great, it's got a light-hearted expository style like K+T, with a good dose of visual intuition like Sedgwick. However, the best part of the text is the pacing. It's the only textbook so far where I didn't feel fatigued after reading more than one or two sections at a time. This is because Skiena sprinkles small doses of breathers throughout the text; every 10-20 pages of text (mod exercises), he would add some fun anecdotes (""war stories"") related to the material. These are surprisingly interesting and engaging for a textbook on algorithm design, and it definitely helps to pace the book in a way that few other textbooks have really thought to do. 

On the flip-side, unlike CLRS, K+T, or Erickson, the ""Manual"" isn't nearly as comprehensive in the types of material covered. In terms of the core theoretical algo-design foundations, we get through the basic DS, search, graph traversal, and some dynamic programming + approximation algorithms. However, instead of drilling deeper into these topics and, for e.g., complexity + tractability, the ""Manual"" pivots in the second half towards more practical applications. Many of these aren't really what you'd take away from from a typical algo-design undergrad course, but they're nevertheless very insightful, and significantly funner to read and learn about.

I haven't gone through all of **Erickson's textbook** yet, though this post motivated me to reread through a few chapters (Recursion, Dynamic Programming, Greedy Algorithms, and the two chapters on Max Flow/Min-Cut and applications of network flow). It's really very well written and well presented. In fact, I would even go as far as saying that it's genuinely fun to read through the chapters! I think it still suffers from some the same problem that many other algorithms books suffer, but it's definitely one of the most readable and accessible books on the market. The problems and algorithms presented are fairly standard (and covered without the depths of its denser counterparts). 

It feels a bit ""dated"" in the sense that it's still an exact (more-or-less) subset of all of the other Algorithms texts out there. For example, the use of Tower of Hanoi to illustrate recursion; Fibonacci #s, edit distance, and substring partitioning for dynamic programming. Gale Shapely's solution to stable matching, maximal interval scheduling, and huffman coding for greedy algorithms. These problems more or less encompass the entirety of these chapters as well. That said, that's not really a weakness. There hasn't been any significant breakthroughs that can/should be covered in an introductory monograph on Algorithm design. However, I'm impressed at the depth this book goes into for network-flow type reductions; until recently, this wasn't a topic indulged in much depth outside of Kleinberg & Tardos's Algorithm Design. (the bonus chapters also sound like fun extensions)

At the same time, I think my main gripe is that the text is still very heavily focused on presenting why a construction or design of a solution is the right one. This is obviously important, but it doesn't quite impart you with the intuition to design algorithms more generally. For example, why did we use this functional recurrence for edit distance and not look at other subproblems? How did we magically think about looking at the finish time for optimal interval scheduling? Why didn't we look at other types of network constructions for Baseball Elimination? That said, the section on stable matching does go down a false start first before giving the G&S construction. Of course, this is a shared problem that almost all textbooks share, nevertheless, I would have loved if there was a textbook that delves more into why seemingly intuitive algorithms don't work.",3,0,0,False,False,False,1643349193.0
sdur26,huk0hw5,t1_huf5bhm,"The 4th Edition is planned to  be released in March this year ""A comprehensive update of the leading algorithms text, with new material on matchings in bipartite graphs, online algorithms, machine learning, and other topics. """,1,0,0,False,False,False,1643355250.0
sdur26,huffadu,t1_hufbok0,"I found it easier to go through the basic structures first, then basic analysis then basic algorithms",7,0,0,False,False,False,1643284488.0
sdur26,hug9anp,t1_hufbok0,"Just go in order of complexity, start with simple DS, then simple algos, then more complex DS, then more complex algos, etc.",5,0,0,False,False,False,1643298572.0
sdur26,hufc2he,t1_hufbok0,"You start with the most basic algorithms, then most basic data structures and then more advanced basically simultaneously",1,0,0,False,False,False,1643282267.0
sdur26,hujqzgu,t1_hujqz3z,"
--------------------

Next, onto textbooks about **analysis of algorithms**:

I love playing around with generating functions, and **Sedgwick's Analysis of Algorithms** spends a significant portion of the text on how to do these very novel and creative runtime analysis with tools like generating functions. He also published another text on Analytic Combinatorics, which is presented as a series of monographs of his own personal research interests. It's surprisingly accessible, very visual and intuitive, and leaves you with a feeling that you can count just about everything, or at least you can encode these problems as real-valued functionals and look for their singularities to understand how to count them. 

Next is **Kleinberg's Algorithm Design**, but again I'm biased because I was his student for algo. In contrast to CLRS, the best part of Algorithm Design was the exposition and the style. It's still very technical and theoretically inclined, but Kleinberg + Tardos put a lot of effort to make the text more readable. The other distinguishing feature of Algorithm Design is its inclusion of some more exotic algorithms that you usually don't find in a first course on algo design. 

Both K+T are algorithmic game theorists, and their influence in these and econometric algorithms shows through in their text. Most algo design texts mention reductions to max-flow/min-cut. Algorithm Design however spends significantly more of its tree-budget on network-flow type algorithms, both in the Network Flow chapter, as well as throughout in Greedy Algorithms, Approximation Algorithms, and in the discussion of complexity and tractability. His (in)famous course (CS 4820) on algorithm design also spends about a third of the semester on Network Flow back when I took the undergrad course.

Otherwise, I've never read through any significant chunk of Dasgupta. I hear good things about it, it's definitely the smallest text of the bunch.

Beyond these, the field has become significantly more popular since I've left school, so I don't doubt that other textbooks / resources have started to overtake these.",1,0,0,False,False,False,1643349199.0
sdur26,hufdec9,t1_hufc2he,"thanks! :D oh and btw, for data structures... do u recommend the introduction to data structures book?",3,0,0,False,False,True,1643283219.0
sdur26,hufvkzg,t1_hufdec9,"Almost all, if not all, books covering algorithms also cover data structures",5,0,0,False,False,False,1643293060.0
sdxd4b,hufl15i,t3_sdxd4b,Freebie  http://www.cl72.org/110dataAlgo/Algorithms%20%20%20Data%20Structures%20=%20Programs%20\[Wirth%201976-02\].pdf,4,0,0,False,False,False,1643287967.0
sdxd4b,hufncu1,t3_sdxd4b,A Common-Sense Guide to Data Structures and Algorithms. Perfectly suited for beginners.,5,0,0,False,False,False,1643289209.0
sdxd4b,hufsr43,t3_sdxd4b,"If you’re a visual learner, grokking algorithms (the actual book not the educative.io course) is really nice for beginners",1,0,0,False,False,False,1643291791.0
sdxd4b,huigqi5,t3_sdxd4b,Algorithms by sedgewick and wayne hands down. Still one of my faves.,1,0,0,False,False,False,1643328098.0
sdxd4b,huflewu,t1_hufl15i,"Thank you for your reply! I have access to O'Reilly Learning through my institution so if there is also any paid books they're likely on there.

I will check this out anyway!",1,0,0,False,False,True,1643288178.0
sdxd4b,hufq5jc,t1_hufncu1,">A Common-Sense Guide to Data Structures and Algorithms

Thank you! Having a quick flick through this looks exactly what I want - will mark this thread as solved!",2,0,0,False,False,True,1643290577.0
sdh6gc,hucp2h3,t3_sdh6gc,[Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl%27s_law),51,0,0,False,False,False,1643233361.0
sdh6gc,hud9t12,t3_sdh6gc,"Thoughts.

1 Woman can have 1 baby in 9 months

9 Women can have  9 babies in 9 months

9 Women cannot have 1 baby in 1 month

Some problems are linear",23,0,0,False,False,False,1643241376.0
sdh6gc,hucpkyz,t3_sdh6gc,"The main reason can be summarized as ""communication overhead"". Cores share the same memory buses, some caches and same infrastructure on the system.

Sometimes cores need to pass data between themselves to execute the algorithm, and this can be expensive in terms of bandwidth and time (in processor scale).

This incurs some communication penalties as the programs scale up. Sometimes memory bandwidth falls short, sometimes caches need to be flushed and refilled much more, etc., hence the scaling up is not perfect or linear.

Source: I develop multicore software.",14,0,0,False,False,False,1643233544.0
sdh6gc,huctrhw,t3_sdh6gc,Multi and parallel processing are still subject to the law of diminishing returns. [Amdahl's Law](https://en.m.wikipedia.org/wiki/Amdahl's_law) may be what you're looking for from a theoretical perspective.,3,0,0,False,False,False,1643235073.0
sdh6gc,hucz7zz,t3_sdh6gc,"Besides what has been said already: Not all problems can be expressed in a way that allows parallelization like in signal processing. Often computations are dependent on previous results. This is not necessarily due to ""imperfect algorithms"". It's in the nature of the problem.",3,0,0,False,False,False,1643237152.0
sdh6gc,hug07r9,t3_sdh6gc,"Multi-core CPUs ""do"" perform N times better for right kind of tasks. Two examples are :

* Neural networks or any kind of Matrix multiplication . These tasks can scale up to thousands of cores on GPU.  Similarly, performs scales with number of cores. 
* Financial simulations like Monte Carlo",3,0,0,False,False,False,1643295019.0
sdh6gc,hud00l3,t3_sdh6gc,The same reason it doesn't take 4.5 months for two pregnant women to make one baby.,1,0,0,False,False,False,1643237456.0
sdh6gc,hucyskf,t3_sdh6gc,"One of the primary limitations on x86_64 processors right now is the complexity of the instruction set.  Processing the machine code before the cpu truly begins doing math on it is a pita on x86_64 processors, regardless how many cores you have.  This is one of the key 'secrets' to the M1 processor's speed, because it's instruction set does not have this issue.",-4,0,0,False,False,False,1643236984.0
sdh6gc,hudaegh,t3_sdh6gc,Work versus Span,1,0,0,False,False,False,1643241621.0
sdh6gc,hudt9lx,t3_sdh6gc,"You might want to take a look at BeOS and Haiku OS. ""Pervasive Multithreading"" was built in at the OS level. If anyone is going to run into limits of multi-core or multithread processing, it would be them.",1,0,0,False,False,False,1643249456.0
sdh6gc,hue89k8,t3_sdh6gc,"There’s overhead in creating a multithreaded program. Creating threads is taxing, dividing tasks is taxing, and combining results is taxing. It’s the same reason two people won’t sort a bowl of M&Ms twice as fast as one, work needs to be divided and combined.",1,0,0,False,False,False,1643255946.0
sdh6gc,huer1ma,t3_sdh6gc,"This doesnt answer the question OP, but may help you understand the issue in more detail. Current Personal Computers have identical / nearly identical cores so it shouldnt matter if a process is being run on which core (i dont know if something like core#0 takes initial boot, its too detailed for my level of hardware knowledge)

There was a computer architecture that was designed to have programs run in parallel using a different architecture and that was the CellBE (Cell Broadband Engine). There was the main computer (which had hyperthreading) known as SPE (Synergistic Processing Elements) and there were 8 or so SPU( Synergistic Processing Units) which had a different instruction set to the SPE and there was a lot of space on the chip to allow SPU's to communicate with the SPE as well as each other. Theoretically, one SPU could decompress, pass to next which decrypts, then pass onto another which does rendering.  

The CellBE was in the PS3 and i understand while it performed well, it was a pain to code for.  

https://en.wikipedia.org/wiki/Cell\_(microprocessor)",1,0,0,False,False,False,1643266193.0
sdh6gc,huhvcbo,t3_sdh6gc,"It’s complicated, but the thing you mentioned first is a major cause and there are some inmediately related snags.
 
—
  
To elaborate: *If* CPU1’s task has to be completed *first* before CPU2 can do it’s thing, then you’re right back to square one and might as well use a single
processor and process. There’s going to be a lot of waiting involved otherwise.
 
Basically you have to be able to divide up the work into either *order-independent* computations or entirely separate tasks. This applies to all kinds of stuff at a fundamental level.  
  
Combining all the separate computations at the end can also force you back onto a single CPU or result in resource contention which I mention further down.
 
—  
  
In addition, if your code *blocks* on I/O transfers, then  the CPUs/Cores can suffer from *contention* over any *shared resources*. 
 
This basic issue applies to main RAM, data caching, storage, communication buses, etc.  
 
It’s also critically important not to have *collisions* or issues with flushing buffers, etc. You have failed before you really started if “CPU A was supposed to do Task A and CPU B was supposed to take Result A and peform Task B” and CPU B checked for the results before CPU A had finished… 
  
P.S. 
  
Imagine 50 people trying to dig a whole with 25 shovels, put all the dirt into 4 big piles and *not* run into each other or waste a lot of time….",1,0,0,False,False,False,1643319657.0
sdh6gc,hurbgel,t3_sdh6gc,Thank you all for your interesting and knowledgeable input here! :)) <3,1,0,0,False,False,True,1643486002.0
sdh6gc,hud0yh3,t1_hucp2h3,"You should definitely read the linked details but to give a quick and intuitive description of Amdahl's law:

Imagine you're baking a cake. More people can help, but there's a limit. One person can make the frosting while you're mixing the batter. That splits pretty easily and makes sense to do.

Theoretically you could have different people measure each ingredient out or one person per egg to crack but these steps are so quick you'd probably spend more time coordinating with each other than actually just doing the work yourself. Technically can be done in parallel but probably not worth it.

Then you need to actually bake the cake. I don't care how many people you have, you can't put the cake in until the batter is mixed. It doesn't matter how many ovens, the cake won't cook any faster. These steps fundamentally can't be parallelized.

So having a few people (cores) helped some phases (making the batter and frosting), but having more didn't do much (hard to split work when making the batter), while some steps take the same time no matter what (baking). The same principle applies to programs. Some things are easily parallelized, some things are hard to parallelize, and some things simply can't be parallelized.",67,0,0,False,False,False,1643237822.0
sdh6gc,hur8ue8,t1_hucp2h3,Thank you for this!,2,0,0,False,False,True,1643484958.0
sdh6gc,huhwd1k,t1_hud9t12,"> 9 Women cannot have 1 baby in 1 month. 
  
That right there is the *crucial* bit.  
 
Natural Human reproduction is a process that typically requires a woman and 9 months gestation for success. You can’t split the work and you can’t really speed it up.",2,0,0,False,False,False,1643320038.0
sdh6gc,hurb96s,t1_hud9t12,"Understood, thanks Cyher! :))",2,0,0,False,False,True,1643485920.0
sdh6gc,hurv477,t1_hucpkyz,Thank you for sharing your knowledge and experience man! :))),2,0,0,False,False,True,1643494240.0
sdh6gc,hur9ki0,t1_hucpkyz,">That's basically what GPUs do. It's called number crunching. Calculating the color of each pixel on the screen in parallel. Also calculating weights of neural networks in parallel if you use them for AI training. Basically almost everything that makes heavy use of matrix operations can be parallelized like that and hence run on a GPU, thousands of such operations simultaneously.

Thank you for your input good sir! :)))",1,0,0,False,False,True,1643485242.0
sdh6gc,hurazvs,t1_huctrhw,"Will take some look into this, thanks Music man! :))",1,0,0,False,False,True,1643485816.0
sdh6gc,hurb75n,t1_hucz7zz,"Its something that has been said on wiki, but yes you are right, computation can be rather dependent on previous results :) thanks Wayne!",1,0,0,False,False,True,1643485897.0
sdh6gc,huray5v,t1_hug07r9,Thank you for this info! :),1,0,0,False,False,True,1643485797.0
sdh6gc,hurav8d,t1_hud00l3,"Understood, thanks! :)",1,0,0,False,False,True,1643485764.0
sdh6gc,hud6cb1,t1_hucyskf,"> Processing the machine code before the cpu truly begins doing math on it is a pita on x86_64 processors

Could you expand on this a bit? I think you're referring to instruction pipelining, but it's hard to tell.",4,0,0,False,False,False,1643239961.0
sdh6gc,huda3ve,t1_hucyskf,"You have just explained CISC vs RISC :)

ARM (eg M1) is basically RISC",3,0,0,False,False,False,1643241501.0
sdh6gc,hueuou9,t1_hucyskf,"Instruction set complexity has nothing to do with why 4 cores aren't 4 times faster than 1 core.  Each core has its own decoder, scheduler, registers, etc.

It does impact clock speed and instruction level parallelism, which is a separate thing.",5,0,0,False,False,False,1643268698.0
sdh6gc,hurakjo,t1_hudt9lx,Thanks for info! :),1,0,0,False,False,True,1643485643.0
sdh6gc,huranhi,t1_hue89k8,Thanks man for your input! :)),1,0,0,False,False,True,1643485676.0
sdh6gc,hura1ym,t1_huer1ma,Thank you Scott! :),2,0,0,False,False,True,1643485434.0
sdh6gc,huraitb,t1_huhvcbo,"Thank you for your input, this is some interesting info! :)",1,0,0,False,False,True,1643485624.0
sdh6gc,hudwy4h,t1_hud0yh3,Is there a method to help identify tasks that benefit from parallelization and tasks that won’t?,4,0,0,False,False,False,1643250984.0
sdh6gc,huer27u,t1_hud0yh3,College campuses use Amdahl’s law to prevent students from filing complaints by filling the process with multiple layers of slow administrative positions. Fun fact.,4,0,0,False,False,False,1643266204.0
sdh6gc,hur90pf,t1_hud0yh3,Thank you kind mister for this good teacher like explanation :),1,0,0,False,False,True,1643485026.0
sdh6gc,hurvj9e,t1_hurv477,You're most welcome. I'm glad it helped.,2,0,0,False,False,False,1643494418.0
sdh6gc,huhkqu2,t1_hud6cb1,There are a lot of articles online about it.  Here's the top google hit: https://debugger.medium.com/why-is-apples-m1-chip-so-fast-3262b158cba2,1,0,0,False,False,False,1643315738.0
sdh6gc,hueacnm,t1_hudwy4h,"The easiest tasks to parallelize are what we call ""embarrassingly parallel"". 

For example: if your task is purely functional (output depends only on the input and produces the same output for the same input every time) and you need to perform this task on many different inputs, then you can do them all at the same time since no single task like this depends on another one.",5,0,0,False,False,False,1643256943.0
sdh6gc,hueeaih,t1_hudwy4h,"Simple tasks that don't do I/O and have all the state they need available quickly can be blazingly fast.

For instance, that's all the GPU really is: just processes a bunch of fairly simple stuff (floating point math for graphics) at a level your CPU can't dream of. 

It's fairly intuitive to see what will benefit and what won't at the code level, but often harder to design the distributed system to enable more of those opportunities.",1,0,0,False,False,False,1643258877.0
sdh6gc,hurbayi,t1_huer27u,Really? :),1,0,0,False,False,True,1643485940.0
sdh6gc,huez8n5,t1_hueacnm,"That's basically what GPUs do. It's called number crunching. Calculating the color of each pixel on the screen in parallel. Also calculating weights of neural networks in parallel if you use them for AI training. Basically almost everything that makes heavy use of matrix operations can be parallelized like that and hence run on a GPU, thousands of such operations simultaneously.",2,0,0,False,False,False,1643272096.0
se2nq0,huh08gr,t3_se2nq0,Stuff like that all fall under type theory I think.,2,0,0,False,False,False,1643308252.0
se2nq0,huhavuu,t3_se2nq0,You’re looking for type theory and effect systems.,1,0,0,False,False,False,1643312112.0
sd0e5c,hublxuj,t3_sd0e5c,"Edit: Sorry, I did not read through the question. You want these in the context of ""type theory"". 

1. Comonad has been encoded in [Haskell](https://hackage.haskell.org/package/comonad), so is [coeffect](https://hackage.haskell.org/package/effect-monad-0.8.1.0/docs/Control-Coeffect.html). Given one of the guy behind coeffect, Tomas Petricek, is fantastic at F#, I am surprised I cannot find a encoding of coeffect in f#
2. Algebraic effect and handler can be added in a type system. There are languages like [eff](https://www.eff-lang.org/) and [koka](https://koka-lang.github.io/koka/doc/book.html). I have not found any research article on a ""practical type system"" (like system F or Hindley-Milner) enriched with algebraic effect (I suspect you can find some publication related to Koka), but I have found [dependent type with algebraic effect](https://dl.acm.org/doi/abs/10.1145/2500365.2500581).
3. ATS tutorial provides lots of cool use of linear and dependent type to guarantee program safety, especially for heap manipulation. Unlike Haskell, ATS does not separate effectful and pure computation. 
This is for good reason, as ATS aims to be a fast language, it prides itself to do safe low level memory access. Separating effectful fragment and non-effectful makes these efficient codes hard to incorporate into larger programs. 

------


Effect system is a huge research area. It is not my research area, so I probably will say something wrong here. I am just listing some research topics that are related to effects. You can look into them, and hopefully find more useful research on their references. 

You already know the monadic effect, however there are some effects that can be more effectively modeled as comonad. some reference on comonad as a model of computation: https://www.sciencedirect.com/science/article/pii/S1571066108003435

There are notions of coeffect, and I am not sure if they are the same as comonadic effect. This seems like an interesting paper unifying coeffect and effect: https://dl.acm.org/doi/abs/10.1145/3022670.2951939 I am sure you can find some reference of coeffect in there. 

For research on monad, coeffect, and general categorical semantics, I would recommend to look into the work of [Tomas Petricek](http://tomasp.net/) (coeffect), Tarmo Uustalu (categorical semantics), and Marco Gaboardi (graded monad).

There is also what called algebraic effect (in the sense of algebraic data type, things defined without quantifier, or as an initial algebra). There is also a notion of [monad algebra](https://bartoszmilewski.com/2017/03/14/algebras-for-monads/), that to me seems to be slightly related to handler, but I am not sure.  

I recommend you to look into the work of [Matija Pretnar](https://matija.pretnar.info/) and [Andrej Bauer](http://www.andrej.com/) on this line of work.

There is also some less categorical approach where they focus on specific effects. One of the most popular one is reading and writing from memory, since it is one of the more theory rich effects.

This is reeeally not my area, so my ramble will get even less stable. 

I think this publication might be relevant here? https://dl.acm.org/doi/abs/10.1145/2535838.2535869 I heard logical relation is very commonly used in this line of research to prove effectful program equivalence.

Also as you have mentioned that linear system can encode some safety properties of effect (for example no read before write etc.), but I have not found any relevant research on that, but you can find some [tutorials on ATS](http://www.ats-lang.org/Documents.html#EFF2ATSPROGEX) that covers topics like this. ATS is a language with linear type, and it uses linear type a lot to guarantee some security properties of effects.",5,0,0,False,False,False,1643219446.0
sd0e5c,hubbjhk,t3_sd0e5c,Check out the Flix language for some interesting research in polymorphic effect types.,1,0,0,False,False,False,1643215791.0
sd0e5c,hubi28u,t3_sd0e5c,"Not sure about cutting edge research. But for everyday otj development, it's all about limiting side effects via decoupled, modular design.",1,0,0,False,False,False,1643218094.0
sd0e5c,hubr402,t3_sd0e5c,"Imperative programs without side effects do not really make sense in my opinion. What is a statement, if not something that affects the current state? If so, the formal semantics require a current state, and that is what having side effects means.

On the contrary, functional languages can be given semantics that do not need external state, by merely rewriting one term into another until you (eventually maybe) reach a normal form.",1,0,0,False,False,False,1643221278.0
sccyyv,hu68zf8,t3_sccyyv,"The people who are loudest on social media are by no means the experts in their field. Working deeply focused all day, every day also is not a good way to keep up with what other people are doing. You might want to take a few hours a day to do this. (If you're doign deeply focused work in a lab, that would usually mean you eventually produce research original enough to present at a conference. Attending them usually gives a good overview what others are doing).",11,0,0,False,False,False,1643129761.0
sccyyv,hu7szqs,t3_sccyyv,"I approach this from a university academic perspective, so you may have different goals and access to peers.
 
The best thing ever are reading groups. Each discipline and subdiscipline should have a reading group, made up of 2 'layers' of people - 1 layer that is people who are actively in that space, or at least closely tangential, and then a bigger layer which is just anyone interested. For subdisciplines these are usually pretty small, for the larger disciplines these might include the whole school and beyond into other faculties and industry. Each <period>, depending on how many people, interest, publishing rates etc, as well as tied to each conference and major serial, you have people divvying up the papers that were released in that period to the internal layer to read. They then give a snap summary (1-3 sentences max) of those papers to a mailing list, with short recommendations, i.e. from 'this is garbage' to 'you must read this'. Then the 'you must read this' papers are read (or at least skimmed) by the outer layer who are interested, and everyone comes together to discuss them. If they're particularly good papers they get shunted up from the subdiscipline group to the main discipline for everyone to read.
 
This cuts the required publication reading to keep absolutely abreast of what is going on in the whole of computer science to about 1% of what it would otherwise be - because 90% of the papers that are published are crap, either wrong, or just rehashed old stuff, and 9% are hyper-niche interest only.
 
Note that (in an academic setting at least) you don't need to follow twitter, podcasts, youtube, news, or subreddits. They fall into 3 categories - people doing work that has been published elsewhere, people doing work that isn't worth publishing, or accounts run by university marketing departments trying to boost their institution's profile and citation rates. I'd still recommend following them, but they are far more of a 'I am bored and am looking for something to read/watch while I am eating/pooping/travelling/supposed to be sleeping' thing than anything to be religiously consuming.",4,0,0,False,False,False,1643150901.0
sccyyv,hu5edob,t3_sccyyv,I think the most efficient way is to follow them on social platforms.,4,0,0,False,False,False,1643117077.0
sccyyv,hu5oa5o,t3_sccyyv,"Why do you need to be so highly efficient in remaining up to date in your field? Are the older technologies of your field so atrocious that the updated technologies destroy the meaningful existence of the older technologies?

As for me, I strongly believe in understanding the fundamental of your field. The fundamentals rarely change so it's perfectly feasible to focus only on the fundamentals to gain as much insight as you can. When you understand the fundamentals, you should be able to pick up on the meaning of any new technologies (that build upon the fundamentals) that happen to come up over time.",0,1,0,False,False,False,1643121679.0
sccyyv,hu7l5t1,t1_hu5oa5o,wtf why would people downvote this? i totally agree,1,0,0,False,False,False,1643147761.0
sbw98k,hu2kut2,t3_sbw98k,"Most generic compression algorithms don't assume any specific file format.  They look for byte-level redundancies and work to eliminate repeated information.

I'm not aware of any general-purpose compression algorithm that will identify the more semantic redundancies you've described.  I'd argue that if you're aware of this kind of structure, you should encode it explicitly in the format.",17,0,0,False,False,False,1643059121.0
sbw98k,hu3eru4,t3_sbw98k,"You have described a domain specific problem, and compression method devised for the specific domain will always beat the general algorithms.",6,0,0,False,False,False,1643074095.0
sbw98k,hu3xwal,t3_sbw98k,"If we lived in a world where storage space or network bandwidth was more of a bottleneck than they are, it is likely we would see 'plugin' compression algorithms used with a general algorithm layered on top of known file type specific algorithms. But that isn't the case - storage and bandwidth is continuously cheaper, and implementation complexity is probably the most significant limiting factor. The implementation complexity necessary to recognise that that transformation has occurred would be prohibitive (not to mention the actual computation required to recognise and compress it would be a significant cost).
 
It's at the point now where not even text is treated differently to random binary data for compression. While you could theoretically compress something more if you knew about things like stemming words, it's not worth it given the gains are so small vs just looking at common bit ranges.",3,0,0,False,False,False,1643082549.0
sbw98k,hu4sb65,t3_sbw98k,Segmentation into neighbourhoods and then deltas from neighbour avgs.,1,0,0,False,False,False,1643101638.0
sbw98k,hu4hqyh,t1_hu3xwal,"I don't completely agree with you. Having plenty of storage is not an excuse to waste resources. 

Also, network bandwidth is still small in some part of the word, and in places where it's huge the data being transmitted is also ever increasing, so you need compression anyway (even with the fastest network connection we have nowadays, it would be impossible to stream a 4K video without compression). 

Finally, there are applications where compression is still relevant, satellites communication or small embedded devices come in mind.",2,0,0,False,False,False,1643093702.0
sbw98k,hu4j79b,t1_hu4hqyh,"Nobody ever said it was an excuse to waste resources, it's a factor in the never-ending balancing act between different compromises. The cheapness of storage and bandwidth means that the value gained from marginally increased compression is tiny, so any cost greater than tiny means it isn't going to be worth it.
 
You need to read what is being written more closely, because the rest of your post is a complete strawman that has exactly nothing to do with what is being discussed. I'm not saying compression isn't useful or used, I'm saying there is no push for specialisation inside general compression algorithms - which is objectively true, because it isn't happening.
 
Video is compressed with a specialised algorithm, not a general one. Satellite communication typically uses *less* compression than you think, because the issue with satellites is latency, not bandwidth, and the processing capability of a satellite tends to be the bottleneck. There will be plenty of compression before and after the satellite is involved, but again there's no value in going after marginal gains there, either.
 
Embedded devices I'm not even sure what you're getting at. As you scale things down processing power drops exponentially faster than storage or network bandwidth. You can cheaply get the same network bandwidth as a full sized desktop on an Arduino micro, and multiple terabytes of flash memory.",3,0,0,False,False,False,1643094705.0
sbw98k,hu4m6r7,t1_hu4j79b,"Satellites and embedded systems were just two examples were I have worked on architectures for compression, in which the processing power was out weighting bandwidth. 

You are right though, I missed the point that you were making about specialized compression!",2,0,0,False,False,False,1643096835.0
sbxbya,hu2pkfr,t3_sbxbya,"Truly parallel? The amount of cores and CPUs. Concurrent? The amount of threads, cores, and CPUs",8,0,0,False,False,False,1643060889.0
sbxbya,hu2r0x3,t1_hu2pkfr,"Concurrent connections can go even higher than this using non-blocking IO where a single thread can manage N connections. Practically the limit is likely going to be the max number of connections your OS allows at once, but this is tunable with something like linux. If your computer only has one IP address you’ll run out of TCP ports once you start to approach 2^16 connections. But even this can be worked around by using multiple IP addresses. At that point you are only really limited by network bandwidth and memory.

There are special programs build to simulate large numbers of connections per second for scalability testing of things like web servers. Suggest looking at those if this is something you are interested in.",7,0,0,False,False,False,1643061442.0
sbxbya,hu2y6un,t1_hu2r0x3,"That's a good point! If we're looking at asynchronous operations, it's a huge amount",3,0,0,False,False,False,1643064220.0
saqw7i,htvz4mj,t3_saqw7i,This will come with all sorts of ethical dilemmas in the future. Where does human consciousness actually come from? It's in the petri dish playing pong.,67,0,0,False,False,False,1642951302.0
saqw7i,htvhek9,t3_saqw7i,Can't beat the good old noodle.,40,0,0,False,False,False,1642942041.0
saqw7i,htxe5mq,t3_saqw7i,But how do they reward the cells with some sort of loss function? Afaik the article doesn't discuss this.,14,0,0,False,False,False,1642971202.0
saqw7i,htvmhp9,t3_saqw7i,"This will be the AI that actually makes a real difference, if true. I've been watching this space for 30 years, and have always been dismissive. This is the first time I've been impressed.",18,0,0,False,False,False,1642945140.0
saqw7i,htvvvku,t3_saqw7i,It does not take an hour to train an AI to play pong.,11,0,0,False,False,False,1642949865.0
saqw7i,htx0wau,t3_saqw7i,that is scary,2,0,0,False,False,False,1642966060.0
saqw7i,htxduwv,t3_saqw7i,"This will not spread in mainstream because Elon musk don't post on twitter about this, and don't have a video on youtube discussing about this research",2,0,0,False,False,False,1642971091.0
saqw7i,hudq370,t3_saqw7i,Wow this is.. quite honestly very jarring,1,0,0,False,False,False,1643248138.0
saqw7i,htxu80g,t3_saqw7i,Completely unethical. I hope they keep the scientists involved in a dark room and force them to play Pong for the rest of eternity.,-10,0,0,False,True,False,1642977364.0
saqw7i,htx2088,t1_htvz4mj,Computational neuroscience will solve consciousness,25,0,0,False,False,False,1642966498.0
saqw7i,htzte1a,t1_htxe5mq,"Another article referencing the same paper was posted on r/science a while ago, as far as I can remember these neuron cells react well/ ""like"" predictable signals and ""hate"" unpredictable signals, so when ever they got something wrong they were fed random noise and when they got something right they got a simple periodic signal",7,0,0,False,False,False,1643010621.0
saqw7i,htzu7ps,t1_htxe5mq,Sex,3,0,0,False,False,False,1643011246.0
saqw7i,htxhzo2,t1_htvvvku,"It's based on hardware-speed independent 'time' - based on the number of rallies occurring at a fixed rate, rather than actual time. 5000 rallies for computer-based AI vs 15 rallies for this brain cell version.
 
However, they also state that the brain cells know how to play quickly, but suck at it and would lose vs a standard AI. Which seems very human.
 
Bottom line is the pre-print itself isn't focusing on the speed vs computer AI, it is about the increased performance of human neurons vs mouse neurons, and the potential for future development.",13,0,0,False,False,False,1642972660.0
saqw7i,htwchkf,t1_htvvvku,It doesn’t take a billion dollar company and billions of simulations to teach a teenager to drive a car.,17,0,0,False,False,False,1642956657.0
saqw7i,htywbad,t1_htxu80g,This is as unethical as a human being born. People dont seem to view the lack of natal-consent as an issue.,6,0,0,False,False,False,1642992686.0
saqw7i,htxmypx,t1_htx2088,Maybe but definitely not in the near future.,10,0,0,False,False,False,1642974548.0
saqw7i,htyvx2t,t1_htx2088,Maybe we dont want it to.,1,0,0,False,False,False,1642992534.0
saqw7i,htzlptx,t1_htx2088,5 years,0,0,0,False,False,False,1643005267.0
saqw7i,htxj6bj,t1_htwchkf,"The difference is that you can ctrl-c, ctrl-v that software.",3,0,0,False,False,False,1642973100.0
saqw7i,htzywty,t1_htywbad,"Hypothetically, imagine a neural net of the size of a human brain to solve a complex task. If we do not fully understand how brains work, how will we understand what this lab brain will go through? Imagine it lives in hell? Imagine there are pain neurons that are arbitrarily firing and the lab brain is conscious about it.",1,0,0,False,False,False,1643014958.0
saqw7i,hu1tue8,t1_htywbad,"You can always just kill yourself though, unlike these Frankenstein creations.

&#x200B;

If you are going to make such a poor comparison then I will gently remind you that birthing a child for the purpose of forcing them into to slavery is also widely considered unethical.",1,0,0,False,False,False,1643048910.0
saqw7i,htxw3gc,t1_htxmypx,Why do you think so? Also how near is near?,2,0,0,False,False,False,1642978048.0
saqw7i,hu2v3t2,t1_htzywty,"What if our existance is a living hell and we are concious of it, what if life is pain?",1,0,0,False,False,False,1643063014.0
saqw7i,hu2vqg7,t1_hu1tue8,"Agreed, while it is the consensus that natal-consent is unimporant I hold the uncommon belief that maybe its not super cool.

&#x200B;

Thing is, all human soceities have and continue to practise slavery in some form (modern days is placed on people in developing nations).

&#x200B;

The question remains, how are we going to do work?  
Is it more ethical to coerce full bodied people into it?",1,0,0,False,False,False,1643063258.0
saqw7i,hu0i4ul,t1_htxw3gc,"Because the amount of computational power needed to simulate anything close to a normal brain simply isn't there yet.

Edit: And from a biology standpoint we don't know nearly enough about how the brain works to realistically simulate it.

Edits2: Just to elaborate I am refering to a complete simulation down to molecular level",2,0,0,False,False,False,1643029201.0
saqw7i,hu2wd18,t1_hu2v3t2,Could be.. but if we can't prove what a neural network perceives in the slightest sense we shouldn't create it in the first place. We have a good estimate of what a baby feels because we see it laugh and we try to give it a good life. We couldn't say that about a pong playing neural net because we can't see inside a black box yet.,1,0,0,False,False,False,1643063502.0
saqw7i,hu32uz5,t1_hu2vqg7,"The obvious alternative is to not use slaves and have people with free determination cooperate by exchanging their labour for goods and services? 

Slavery is neither inherent nor necessary  for survival as you so heavily imply. Small societies do not own slaves. Slavery is merely a side effect of despotism.

Anyway in response to your question(s):

1) how are we going to do work? Sort of like what we have now? Why do we need a bunch of laboratory abominations to compute anything when we have perfectly capable non-biological computers.

2) Is it more ethical to coerce full bodied people into it? I know this is a loaded question and completely ignores the obvious fact that the world's and humanity's survival does not depend on the the computational tasks of biological brains in jars, but the answer here is yes, because at least those full-bodied people have some capability to end their own lives by their own free will.",1,0,0,False,False,False,1643066092.0
saqw7i,hu98hsf,t1_hu0i4ul,"That's quite a big assumption (that we need to simulate it at the molecular level).

And possibly we might see some big leap with quantum computers, for which simulating quantum systems like molecules is a piece of cake. 

So, I would say it's better to say: we don't know. Maybe, maybe not. That's how things are, those are the paths and let's see! (Or contribute if we are in the area :p)",3,0,0,False,False,False,1643173107.0
saqw7i,huh8n35,t1_hu98hsf,"Yeah it is indeed. My line of though is that aince we don't know which ""level of abstraction"" (to use our lingo) let's say ""generate"" consciousness, it would be interesting to try and simulate it at different levels.",1,0,0,False,False,False,1643311286.0
sbk9aw,hu0dzzq,t3_sbk9aw,"My three main issues with crypto are..  
1) It fluctuates in value too much to be used as currency.  
2) In most countries, traditional investments are insured against theft. Theft of crypto isn't.  
3) Much publicised environmental cost of mining.",26,0,0,False,False,False,1643026699.0
sbk9aw,hu0xzhn,t3_sbk9aw,"From a CS perspective, I believe Blockchain is a very clever and interesting technology, using cryptography not to protect information, but instead to build concensus among trustless parties.

From a social perspective, I believe platforms like Ethereum could revolutionize human interaction at a global scale and disrupt virtually any industry despite entrenched central authorities and gatekeepers.

From a political perspective, I believe deeply in the promise of Bitcoin as a hedge against the irresponsible and reckless behavior of the Federal Reserve, printing the US dollar into valueless oblivion. And also as a hedge against the ""too big to fail"" private banks and their cronies in the SEC and other civil regulatory agencies whose collective incompetence caused the disaster in 2008 which robbed so much wealth from so many people.

However, from a pragmatic perspective I recognize that, in reality, the crypto community thus far has produced little more than new-age digital Ponzi schemes and other various forms of fraud and grift. All while building a culture around itself which has its collective head so far up its own ass that it can't recognize or resolve any of the issues that are undermining it's potential.

Finally, from an emotional perspective, I got caught up in the Elon-fueled mania and am the not-so-proud owner of 29,290.53 Dogecoin. So what the fuck do I know?",17,0,0,False,False,False,1643036818.0
sbk9aw,hu0zfxp,t3_sbk9aw,"Believe?

Eh; the technology is interesting but generally there no actual good use case that needs a distributed ledger. We all use regular money just fine; it’s worked for thousands of years.

A lot of money and a lot of smart peoples time has been invested , and yet we’ve still only got crypto coins. So this amazing technology is basically useless.

Cryptocurrency looks very much like a fancy Ponzi scheme currently; the fact the supply of money is increasing at a fixed interval, means the higher demand for coins, the greater value each coin is, and this basically rewards those with existing high holdings. So they are incentivised to attract more people in, which is easier enough if the returns are there. But for those later to the party, you get less of the pot, and potentially don’t get the returns; pretty much a Ponzi scheme.

Unless you’re blinded by the tech; or think it’s so cool, or are an anarchist at heart; you generally think it’s useless.

However, I wish I’d bought some at the start and become a multi-millionaire.",6,0,0,False,False,False,1643037426.0
sbk9aw,hu0eiqi,t3_sbk9aw,"I have not invested in crypto. 

I personally don't believe in Bitcoin, and other crypto currencies. I do believe in their underlying technology. They have huge potentials. It can really be the new phase of internet. 

None of these crypto currencies have any underlying value, the value of a stock is determined by the company, bonds by the govt stability and currency value, etc. Crypto currency as such does not have any underlying value which can make it a valuable investment.",5,0,0,False,False,False,1643027033.0
sbk9aw,hu0nnkn,t3_sbk9aw,"It has been too widely adopted for how early and rough around the edges it is.  Finance bros are using is as a speculative investment is ruining its potential to be a useful currency.  Finance bros combined with its energy usage issues and the fact that a rough new technology is being presented to the general public as the next big thing is placing a stigma on it that will be very hard to over come.  It could've one day been something great and improve the world.  Maybe it still will, but that will require a major rebranding, and people to smooth out the edges despite the current taint attached to it.",2,0,0,False,False,False,1643032120.0
sbk9aw,hu1rjpu,t3_sbk9aw,"Technically it’s an interesting but extremely inefficient solution to a problem that doesn’t really exist. 

Economically it has way too many flaws as a currency for me to see it being mainstream, but I could see it always having a niche status. 

I don’t really “believe” in it, no.",2,0,0,False,False,False,1643048062.0
sbk9aw,hu0cvr0,t3_sbk9aw,[deleted],2,0,0,False,False,False,1643025959.0
sbk9aw,hu0fltx,t3_sbk9aw,Please don’t post stuff like this here.,-5,1,0,False,True,False,1643027714.0
sbk9aw,hu2ckzx,t3_sbk9aw,"Viewing cryptocurrency as a simple currency is a bit outdated, as it has evolved beyond that. While “currency” aspect does have its place, the technology is now becoming more of a decentralized state machine, with nodes forming consensus as to the state of execution, rather than merely the number of coins in a wallet.  A new trustless economic platform is being created and while I tend to think it won’t be quite as revolutionary as the internet was, this is the more important technologies being developed right now.",1,0,0,False,False,False,1643055992.0
sbk9aw,hu0g8vm,t1_hu0dzzq,"> 2) In most countries, traditional investments are insured against theft. Theft of crypto isn't.

How can cryptocurrency owned by a person be stolen? Cryptocurrency is very secure, right?

> 3) Much publicised environmental cost of mining.

I feel like this argument is a moot point because everything we do has a high carbon footprint on the environment, including watching YouTube. I think moving away from non renewable energy with a healthy mix of renewable and nuclear energy would solve this problem, because regardless of whether we use Bitcoin or not, I think block-chain technology is here to stay.",-15,0,0,False,True,True,1643028103.0
sbk9aw,hu4kcgg,t1_hu0xzhn,Your answer seems like the most balanced opinion on this comment section.,1,0,0,False,False,True,1643095511.0
sbk9aw,hu1jduj,t1_hu0zfxp,Except that is not true in some countries where currency is not reliable (e.g. hyper inflation).,0,0,0,False,False,False,1643045073.0
sbk9aw,hu0gj11,t1_hu0eiqi,"> None of these crypto currencies have any underlying value, the value of a stock is determined by the company, bonds by the govt stability and currency value, etc. Crypto currency as such does not have any underlying value which can make it a valuable investment.

Hmm, I kind of agree and disagree with this statement. If we think about it, no currency really has any intrinsic value, the only ones which do are services and goods that are produced. I, however, do get your point. Bitcoin as a currency has yet to see mass adoption in the global economy and I personally don't see how it can happen as it's value fluctuates so much.",0,1,0,False,False,True,1643028272.0
sbk9aw,hu0o51x,t1_hu0nnkn,I agree with you. What would you say are the current limitations and problems that Blockchain technologies face?,1,0,0,False,False,True,1643032363.0
sbk9aw,hu4jyrz,t1_hu1rjpu,I agree with you.,1,0,0,False,False,True,1643095245.0
sbk9aw,hu0gcu3,t1_hu0cvr0,"> You shouldn't apologize, people are becoming rich than getting therapy that why they are unhappy. They are not ready to share how they got into and planning to gift their next 50 generations Bitcoin as inheritance.

I'm not sure I quite understand what you are saying here.",1,0,0,False,False,True,1643028168.0
sbk9aw,hu0fyoh,t1_hu0fltx,Why not?,8,0,0,False,False,True,1643027932.0
sbk9aw,hu4k4gy,t1_hu2ckzx,"But don't all transactions happen through like some few companies? I think that's a big part about that. Plus, there is a limited number of transactions you can do through bitcoin in a given time period, which is way less than normal money. I think these are some of the main factors hindering cryptocurrency's mainstream adoption.",1,0,0,False,False,True,1643095356.0
sbk9aw,hu0n9ic,t1_hu0g8vm,">How can cryptocurrency owned by a person be stolen?

I'm not sure this statement is the slam-dunk you think it is. Just because you own something doesn't mean it's yours forever. Someone owns the Mona Lisa, but it can still be stolen.

There's plenty of malware out there that sits quietly undetected on peoples machines until it detects the user copying their private key to the clipboard, at which point it uploads to the hacker. [There's also been plenty of crypto-exchange hacks over the years.](https://crystalblockchain.com/articles/the-10-biggest-crypto-exchange-hacks-in-history/)

There's also the danger of accidental loss. If my house burns down with my laptop, password manager, and any notebooks containing investment or banking credentials are destroyed, I can still eventually prove my identity and get access to my assets again. [If you lose your private key, you're fucked.](https://www.bbc.co.uk/news/uk-wales-55658942)",9,0,0,False,False,False,1643031926.0
sbk9aw,hu3jb4o,t1_hu0g8vm,"The extreme lack of authority makes it so any mistake is permanent, if someone steals my credit I get every cent reimbursed, if someone steals my wallet I’m absolutely fucked, there’s just no real point from the consumer side to go crypto over fiat",3,0,0,False,False,False,1643076030.0
sbk9aw,hu0ouri,t1_hu0g8vm,"> How can cryptocurrency owned by a person be stolen? Cryptocurrency is very secure, right?

What if the person hands it away himself :-) ? Yes, you heard it right, that is what crypto theft is. The blockchain is very secure but most people using cryptocurrency are not tech junkies like us, they do not know how to secure their crypto wallet, they either end up giving away their private key to some hacker because of some social engineering or fall into other scams. Also, the cryptocurrency exchanges can have bugs, thus causing uh-huh crypto theft. 

u/rzlmmfia meant that traditional investments are secure from this kind of theft, which the ""common man"" is unaware of, although traditional investments are also not safe, god forbid but somebody can break into your house and steal valuables. I think that there are some benefits of having a centralized authority as the middleman which I would describe in my answer to the main question.",2,0,0,False,False,False,1643032713.0
sbk9aw,hu4k8u7,t1_hu1jduj,"You're right, but I think the much more fluctuations of crypto's value can be a very big problem for it's adoption.",3,0,0,False,False,True,1643095441.0
sbk9aw,hu0pnbh,t1_hu0gj11,"Currency is backed and guaranteed by a government. Its stability is directly tied to the stability of that government. The only way a major currency would lose value is if the government collapses. You see that instability in countries that are very unstable, like Venezuela. That's not really an issue for the Euro or USD though. 

It used to be backed by gold, but modern economics doesn't care about that.",5,0,0,False,False,False,1643033094.0
sbk9aw,hu0jsnr,t1_hu0gcu3,[deleted],1,0,0,False,False,False,1643030128.0
sbk9aw,hu0gdh1,t1_hu0fyoh,What is the name of the subreddit? What does this have to do with computer science?,-12,0,0,False,True,False,1643028179.0
sbk9aw,hu0xnrf,t1_hu0n9ic,"Your correct.

One of the oft touted positives of decentralisation is that it empowers the poor to get out of the grip of the powerful.

For me; this is actually really wrong; it transfers the burden of security to the individual, removes any kind of safety net.",3,0,0,False,False,False,1643036679.0
sbk9aw,hu0nyjz,t1_hu0n9ic,"I see, apologies if I seemed a bit arrogant since I'm very new at understanding this technology. Thank you for taking the time to explain it to me.",1,0,0,False,False,True,1643032272.0
sbk9aw,hu4k65b,t1_hu3jb4o,"Hmm, I never considered that part. You're right.",1,0,0,False,False,True,1643095388.0
sbk9aw,hu0ntbq,t1_hu0jsnr,I think that can be said about the whole state of affairs happening all around the world. Not a very profound statement lol.,3,0,0,False,False,True,1643032200.0
sbk9aw,hu0o9df,t1_hu0gdh1,"Uhm, not sure how to say this, but Blockchain technology is a topic within the field of computer science lol",10,0,0,False,False,True,1643032422.0
sbk9aw,hu0one3,t1_hu0nyjz,"No need to apologise, and sorry if I came across as rude. Just please be very wary of crypto. For every person you read about who retired with millions in the bank at 25 after investing in crypto, there are probably 50 people whose investment went down, or who lost all their crypto through scams, but are too embarrassed to talk openly about it.",5,0,0,False,False,False,1643032612.0
sbk9aw,hu0trwx,t1_hu0o9df,"You didn't ask about blockchain technology / algorithms. You asked about the consumer / social side of crypto, i.e. about whether or not it is a scam, whether or not it will be adopted, etc. And you asked if this subreddit ""believes in crypto."" 

You're not asking about CS. That's why the post has no upvotes. 

\> I would be asking this question in the crypto sub but that sub

You talked yourself out of doing the right thing - go talk about this there.",5,0,0,False,False,False,1643034996.0
sbk9aw,hu13p4n,t1_hu0trwx,"Should we not discuss the social and political ramifications of CS? Science and technology do not exist in a vacuum.

If there existed an r/eugenics sub, I would hope it would contain healthy debate on the ethics of such scientific pursuits. Admittedly, this is an extreme example, but the point is that scientists and engineers should always be considering the implications of their work in a broader context outside their narrow scientific field. I believe that is what OP is going for here.",1,0,0,False,False,False,1643039131.0
sbk9aw,hu1tlfe,t1_hu13p4n,"\> Should we not discuss the social and political ramifications of CS?

oh my god, that's not what this person is asking about. The post was ""hey crypto bros, wanna talk about crypto??""",3,0,0,False,False,False,1643048818.0
sbk9aw,hu4kfmv,t1_hu1tlfe,Not really. I was asking people's opinion on what they thought about cryptocurrencies.,0,0,0,False,False,True,1643095575.0
sbk9aw,hufwpu9,t1_hu4kfmv,Ignore this idiot. It's people like these that reddit becomes a meme because now you can't even ask anything. I also made a post recently and I only see pokemons like that. I wonder if they have a life of their own...,1,0,0,False,False,False,1643293552.0
sasbd7,htwzued,t3_sasbd7,Pretty sure that’s right,2,0,0,False,False,False,1642965641.0
sasbd7,htyot0u,t3_sasbd7,Not always O(V\^2). If use min-priority queue it drops down to O(V+V log(V)) .,-1,0,0,False,False,False,1642989662.0
sasbd7,hu06upo,t1_htyot0u,Are you sure that's not O((E + V) log(E))? Making it O(V^2 log(E)) in complete graphs?,2,0,0,False,False,True,1643021520.0
sa6sdw,htrijml,t3_sa6sdw,"In my experience it is not worth trying study the courses material just the summer before. You never end doing that much progress and you will literally have to spend time again reviewing it. I think it's better just to do some fun projects that get you more in touch with programing ( I did exactly this and that was my conclusion). Just as an idea, of course I don't want to discourage you from study. Good luck with that!",18,0,0,False,False,False,1642871627.0
sa6sdw,htri4es,t3_sa6sdw,"Cs50 is a good intro course

On one of the pinned threads there’s an entire CS degree outlined with free courses I’ll edit and post the link momentarily (actually just go to the learn programming sub reddit)

Khanaceademy and freecodecamp are good too",6,0,0,False,False,False,1642871464.0
sa6sdw,hts19of,t3_sa6sdw,"Rosen, K: Discrete Mathematics and Its Applications

[Calculus Course on YT](https://youtube.com/playlist?list=PLl-gb0E4MII1ml6mys-RXoQ0O3GfwBPVM)

[Codecademy](https://www.codecademy.com) for programming: start with either Python, C++, or Java


Books on programming (more in-depth theory):
Problem Solving with C++, Walter Savitch

Introduction to Java Programming and Data Structures, Y. Daniel Liang",3,0,0,False,False,False,1642878900.0
sa6sdw,htsw6px,t3_sa6sdw,I think it’s better to do leisure learning rather than just getting into details. That will help more IMO when you actually take those classes just search topics you like and see what video lectures you can find.,1,0,0,False,False,False,1642891434.0
sa6sdw,htt5rgc,t3_sa6sdw,"If you have access to the materials of the courses you will be taking (syllabus, bibliography, etc), I'd suggest that you use them as guidelines. Nevetherless, here are my suggestions:

\- Discrete Math: this [playlist](https://www.youtube.com/playlist?list=PLl-gb0E4MII28GykmtuBXNUNoej-vY5Rz) follows some sections from the book *Discrete Mathematics and Its Applications*, by Kenneth Rosen. You could follow the video lectures and read the corresponding section in the book. I'd suggest that you go up to video 30 (seems a lot, but they are not long); these cover Logic, basic Set Theory and proof techniques, which will give you a nice head start for when you begin your course.

\- Linear Algebra: I think the best resource you could ever want to study this subject is Strang's MIT course [18.06SC](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/). This version is organized specifically for self-study (it has exercises with solutions, lecture notes, etc). I'd suggest that you go through Unit I; this will be more than enough to give you a good start in the subject before your course.

\- Calculus: some may have better suggestions than mine, but I think Khan Academy's [Precalculus](https://www.khanacademy.org/math/precalculus) course would let you well prepared for your first encounter with Calculus. Unlike the other subjects, Calculus requires that you are knowledgeable in algebra, trigonometry, etc. So if you have gaps in these prerequisites, I'd urge you to prioritize studying them instead of the other subjects I listed. LA and DM will be covered in your bachelors, but high school math certainly won't.

\- Programming: last, but not least, [CS50](https://cs50.harvard.edu/x/2022/) from Harvard makes a nice introduction to programming for someone entering a CS major. This course does NOT focus on a particular programming language; instead, it teaches the basics of many programming languages (C, Python, Javascript) and covers basic algorithms and data structures.

Hope I could help. Best of luck! ;)",1,0,0,False,False,False,1642895377.0
sa6sdw,htrj2uo,t1_htrijml,Depends how much he knows. Might be worth making sure you understand the underlying concepts before taking calc. A lot of people struggle because they could use work on trig or algebra,3,0,0,False,False,False,1642871834.0
sa6sdw,htrldny,t1_htrijml,"Thanks appreciate it , Building projects is particularly I love to do more than just studying stuff and also loved ur idea.

Can you recommend some basic-mediocre level projects I can do and also a place to learn this kinda stuff?",3,0,0,False,False,True,1642872722.0
sa6sdw,htrppx2,t1_htrijml,ya i agree. you will learn these topics anyways. just spend the time learning something else,2,0,0,False,False,False,1642874421.0
sa6sdw,htrlw0r,t1_htri4es,"r/learnprogramming sub reddit right?

cs50 by harvard if I am not wrong?

Also thanks :)",1,0,0,False,False,True,1642872919.0
sa6sdw,htuc07z,t1_hts19of,Already studying C++ in high school so The book will be really helpful:) thanks.,1,0,0,False,False,True,1642913611.0
sa6sdw,htubchd,t1_htt5rgc,"Just WOW! for this explanation.
Thanks a lot :)",2,0,0,False,False,True,1642913288.0
sa6sdw,htrnyoz,t1_htrj2uo,"Yeah right, if u aren't comfortable with the academical stuff it is worth to care of it",2,0,0,False,False,False,1642873733.0
sa6sdw,htrll7p,t1_htrj2uo,I am an international student from different study background so U might consider me a beginner or a a little above it.,1,0,0,False,False,True,1642872803.0
sa6sdw,htrm4j9,t1_htrlw0r,"Yes cs50 is Harvard you can find it on edx.com,
The lectures are by David j Milan and he’s charismatic af so it’s enjoyable

And that’s the sub, I just can’t link to them on my phone

Maybe precalc to make sure your up to speed on your maths,",3,0,0,False,False,False,1642873011.0
s9enpp,htm6sgd,t3_s9enpp,"Very new to machine learning and Comp Sci. Started learning about ML when I was supposed to be doing my dissertation (on some things called D-Modules and Grothendieck's Crystals) but Alpha Go was busy changing the world.

Properly started learning ML properly 20 months ago (first commit on this repo is on 22nd of April 2020) and this is what I've been up to! Was lucky enough to get onto the GPT-3 beta and I've built a tool to automatically generate resumes based on some notes you enter. Has taken me forever to get it working but finally am at a first properly working version.

The idea is you'll only have to write your resume once (or not really at all), and then you can give it a job spec and it'll automatically tailor it for you.

It is still v much WIP but would love to hear your feedback!

EEDIT: the website is [joinrhubarb.com](https://joinrhubarb.com) :)",25,0,0,False,False,True,1642782726.0
s9enpp,htmb5wa,t3_s9enpp,I've only heard of GPT–3 from Tom Scott videos.,7,0,0,False,False,False,1642784327.0
s9enpp,htmfp6g,t3_s9enpp,"That’s awesome! Would you be willing to share some of your experience operationalizing this? I studied Computer Vision (CLIP) but I haven’t found much in the way of taking something from its categorizations to a finished product.

Do you have an API that ingests and predicts, then a UI on top to display the results? It looks really well done.

Are you running it locally or in the cloud?",4,0,0,False,False,False,1642785992.0
s9enpp,htn3thl,t3_s9enpp,Wow! Very cool. I'm doing a similar thing on overleaf with my premade java code to change which projects on my cv to highlight. Personalised CV matter,3,0,0,False,False,False,1642794854.0
s9enpp,htnhde0,t3_s9enpp,"lol, nice!!  I love it.",3,0,0,False,False,False,1642799739.0
s9enpp,htq2d3b,t3_s9enpp,"Nice, now your AI can talk to their AI",2,0,0,False,False,False,1642843286.0
s9enpp,hto58v2,t3_s9enpp,"Love it, looks and sounds like a really awesome tool. Wonder how long it took you to build and if you worked on it with anyone else. Cheers!",1,0,0,False,False,False,1642808702.0
s9enpp,htqlmbm,t3_s9enpp,Great job:),1,0,0,False,False,False,1642856646.0
s9enpp,htrlnox,t3_s9enpp,"Publishing the source code of this would be awesome, not only advertising for your website but also to improve what you did",1,0,0,False,False,False,1642872829.0
s9enpp,htsm21k,t3_s9enpp,"I love it, but I cannot go past the work experience section (as I don't have any). I can't save an empty field or skip it.",1,0,0,False,False,False,1642887351.0
s9enpp,htxep4q,t3_s9enpp,Can this tool works in portuguese?,1,0,0,False,False,False,1642971409.0
s9enpp,htqb3gf,t1_htm6sgd,I would legit pay for this tool.,6,0,0,False,False,False,1642849683.0
s9enpp,htmbzy5,t1_htmb5wa,It's pretty cool - has billions of parameters and is supposedly the most sophisticated language based machine learning out there,9,0,0,False,False,True,1642784634.0
s9enpp,htmfy8t,t1_htmfp6g,"A bit of background on how i built it:

The backend is a massive Django instance which is basically a JSON API (we use DRF to make this easier), which talks to a postgres DB and a redis cluster.

We also use celery for long running tasks (e.g. initially tailoring a resume or beat tasks like sending onboarding reminder emails which I'm setting up now).For the frontend, it's again pretty simple: NextJS with typescript (love typescript) and tailwind for css.

I also use headlessui, the component library from the tailwind team, which has been really helpful in places. The marketing site ([https://www.joinrhubarb.com](https://www.joinrhubarb.com)) is also a NextJS site, I think it's so good for these sorts of things.

The bert instances are all fastapi (not sure if I would use this again) with pytorch for inference. I deploy these on elastic beanstalk (which is also where everything else is deployed) and while it works great for everything else I worry that we're overpaying for some massive ec2 instances we don't need.

Last is the chrome extension which is also react/typescript but like... kinda hacked together with a custom webpack config which needs improvement. We will soon have firefox/safari extensions but it's quite annoying/painful to do and deploying to the stores means we need to go through approval processes which is annoying.

Oh we also have some random lambdas for backend jobs and we use posthog for analytics which I cannot recommend enough, its really so so good.",4,0,0,False,False,True,1642786084.0
s9enpp,htq3m3z,t1_htn3thl,That's really cool! Sounds similar to what i've been doing -  Have DM'd,1,0,0,False,False,True,1642844202.0
s9enpp,htnyc6x,t1_htnhde0,Thanks!,1,0,0,False,False,True,1642805998.0
s9enpp,htq3v53,t1_htq2d3b,Now I just need to put in on the blockchain and twitter will go crazy for it lol,3,0,0,False,False,True,1642844385.0
s9enpp,htq3syq,t1_hto58v2,Took about 7ish months and worked on with a friend :),2,0,0,False,False,True,1642844340.0
s9enpp,htr4i16,t1_htqlmbm,Thank you!,1,0,0,False,False,True,1642865953.0
s9enpp,htvccto,t1_htsm21k,"Ahh yes I made it a required field but currently working to make it skippable. The codebase is a bit of a mess so it is taking a little while. If you have any other suggestions, please let me know!

(EDIT: I just set up a discord so feel free to suggest any features or issues in there - [https://discord.gg/VmG75yrb](https://discord.gg/VmG75yrb) )",1,0,0,False,False,True,1642938372.0
s9enpp,htxh6fc,t1_htxep4q,"Definitely adding portuguese (and other languages) is on the roadmap but I would say its not likely in the next \~6 months because unfortunately I don't speak portuguese so would need to pay for translations and don't have the money at the moment :( sorry I don't have a better answer for you, localisation is definitely something we want to do!",1,0,0,False,False,True,1642972342.0
s9enpp,htqbm1o,t1_htqb3gf,"Wow, that's so kind, thank you! I've made it totally free and I have no plan to charge - you can create an account on the website I chucked up :)

(EDIT: it's [joinrhubarb.com](https://joinrhubarb.com) \_",3,0,0,False,False,True,1642850051.0
s9enpp,htmceuq,t1_htmbzy5,Oooh. Sounds good.,3,0,0,False,False,False,1642784786.0
s9enpp,htmg0k0,t1_htmfy8t,u/pursuitofsadness \- happy to go into more detail on any of these bits if useful :),2,0,0,False,False,True,1642786107.0
s9enpp,htq4s5s,t1_htq3v53,Maybe Mint the code as NFT,3,0,0,False,False,False,1642845060.0
s9enpp,htq3tuv,t1_htq3syq,"(Although, I did 90% of the dev and he just helped me with bits and pieces)",2,0,0,False,False,True,1642844359.0
s9enpp,htrgduw,t1_htr4i16,I’ll be using it . Thanks,2,0,0,False,False,False,1642870790.0
s9enpp,htvrbu1,t1_htvccto,Thank you bro :),2,0,0,False,False,False,1642947694.0
s9enpp,htqf3tr,t1_htqbm1o,Maybe set up somewhere for donations.,3,0,0,False,False,False,1642852497.0
s9enpp,htqfw88,t1_htqbm1o,You are amazing! Thank you so much!,2,0,0,False,False,False,1642853044.0
s9enpp,htmcqkn,t1_htmceuq,Definitely worth checking out if you're interested in machine learning or ai :),2,0,0,False,False,True,1642784906.0
s9enpp,htmibsu,t1_htmg0k0,"That’s fantastic! I really appreciate sharing all the components. I’m gonna go through the process and see what using it is like asap!

Once you understood the basics of GPT-3 how fast do you think it was to operationalize this? What did it cost to train the model? Was it easier or harder to do than you expected?

What made fastapi a bad tool to use in your deployment? I’ve generally heard positive things.

With these hyper-scale foundational models I’ve heard the volume of fine tuning data required to get improvement on a more specific corpus isn’t huge (I think it’s called one-shot learning?), was that your experience?

Is it learning in real-time or are the weights updated on a schedule?


And finally, so I’m not a total leech here:

It’s not a lot, but I’m a Product Manager and I would be happy to give you my take on your sign up process and some use takeaways?",1,0,0,False,False,False,1642786956.0
s9enpp,htq6a2q,t1_htq4s5s,Could call it Resume Chimp or something lol,2,0,0,False,False,True,1642846158.0
s9enpp,htqfy26,t1_htqf3tr,Agreed. Get us your Patreon link!,3,0,0,False,False,False,1642853080.0
s9enpp,htqgsi8,t1_htqfw88,"Thank you! It is still quite basic in the ML functionality and planning on doing a lot more so if you have any thoughts or feedback please let me know

(Edit: I've set up a discord so if you do have any thoughts, please message me on there! [https://discord.gg/VmG75yrb](https://discord.gg/VmG75yrb) )",1,0,0,False,False,True,1642853662.0
s9enpp,htxi7f1,t1_htmcqkn,Is GPT-3 open source?,1,0,0,False,False,False,1642972742.0
s9enpp,htmjqtd,t1_htmibsu,Have DM'd you - happy to provide more detail and would appreciate any feedback you have on what I have made.,2,0,0,False,False,True,1642787464.0
s9enpp,htqh8cn,t1_htqfy26,"That's a really kind thing to say but honestly, I haven't done it to make money - the aim is to have enough people using it that I can make the ml models incredible (not just getting past ats but when recruiters read your resume, regardless of your experience, they are wowed and the world becomes a little bit less about presentation of skills and more about your actual skills and how you would fit into a company), so with all that being said, if you can share with your friends and to your networks, that would be worth so much more than any donations :)",4,0,0,False,False,True,1642853955.0
s9enpp,htxj9yp,t1_htxi7f1,"You would think, given that the company that created it is called OpenAI, that the answer would be yes. The answer is that the model architecture and instructions on how to train an instance \_are\_ open source, but good luck actually training a model from scratch without millions of dollars and some pretty sweet hardware. Basically the code is open source but the data is not and the data is the valuable bit so the answer is no :(",1,0,0,False,False,True,1642973137.0
s9enpp,htqi209,t1_htqh8cn,Happy to spread the word :),2,0,0,False,False,False,1642854497.0
s9enpp,htxm2ol,t1_htxj9yp,But can I train the model without a lot of data?,1,0,0,False,False,False,1642974199.0
s9enpp,htqierg,t1_htqi209,"That's very kind, thank you!",1,0,0,False,False,True,1642854724.0
s9enpp,htxu563,t1_htxm2ol,You can find pretrained models on huggingface and use those,1,0,0,False,False,True,1642977335.0
s9gnb2,htmsicy,t3_s9gnb2,"Cs is not that math heavy tbh. It’s just calculus , linear algebra and discrete. Linear algebra and discrete are more important than anything. I would advise you to pick a language most probably the one used in the university course work. It will help you a lot to finish projects on time.",4,0,0,False,False,False,1642790689.0
s9gnb2,hu4f4th,t3_s9gnb2,"It wouldn't hurt to get a program for practicing coding in a fun way. I made a graphical, no-fail language that is fun like Minecraft. You can email me at Cameron.flotow@gmail.com.

I also left a link to the FB page I just set up.

[CIRKETZ fb page](https://www.facebook.com/groups/471007034699523/?ref=share)",1,0,0,False,False,False,1643091981.0
s9gnb2,htrt0im,t1_htmsicy,I definitely agree but just want to add in a caveat: If you want to go towards a data science pathway I would not slack off with the calculus.,3,0,0,False,False,False,1642875686.0
s9gnb2,htq2hcs,t1_htmsicy,"> Cs is not that math heavy tbh

I would argue about that but nvm",2,0,0,False,False,False,1642843375.0
s9gnb2,htmtmsx,t1_htmsicy,"Thanks for the reply! I'm actually learning Python and JavaScript and it's going pretty well so far, but I'm really concerned about the maths since I've forgotten pretty much everything😬",1,0,0,False,False,True,1642791135.0
s9gnb2,htvzy0o,t1_htrt0im,"Thanks, I don't plan on slacking off with any subject tbh. This is why I'm asking for resources, I want to do well.",1,0,0,False,False,True,1642951651.0
s9gnb2,htqm6at,t1_htq2hcs,"I meant to say for undergrad CS. grad and Beyond , especially theoritical CS is different story",2,0,0,False,False,False,1642856994.0
s9gnb2,htmvuj1,t1_htmtmsx,"Don’t worry! Universities basically start from calculus and linear.
Also note that python will most probably not be used in 90% courses. Java and C++ are most used for universities. JavaScript is barely touched and python is only relevant for ML courses. 

EDIT : I will recommend going over courses requirements for more info",2,0,0,False,False,False,1642791964.0
s9gnb2,htmxweo,t1_htmvuj1,"Oh wow, thank you so much. The course requirements are a bit unclear since I'm an international student so I don't really know what a (Canadian) hs student learns on average, so I'm trying to get a general idea so that when I delve into details it'll be a bit easier.",1,0,0,False,False,True,1642792716.0
s9gnb2,htmyyps,t1_htmxweo,What university? I might be able to help,2,0,0,False,False,False,1642793104.0
s9gnb2,htmzhex,t1_htmyyps,Memorial University of Newfoundland.,1,0,0,False,False,True,1642793295.0
s95ek1,htkvoco,t3_s95ek1,"Vectors are very general and generic mathematical object with many different representations. In CS, they're most commonly seen as an ordered list which can be acted on with some basic operations like dot product, elementwise operations , etc. 

Just to give a list of examples at the top of my head:

* **Vector Clocks** \-- vectors clocks are used to enforce a causal order of events in a broadcasting system. [https://en.wikipedia.org/wiki/Vector\_clock](https://en.wikipedia.org/wiki/Vector_clock)
* **Graphics** \--  Vectors are used to represent points, direction and lines. Transformations can then be seen as operations on vectors. [https://en.wikipedia.org/wiki/Polygon\_mesh](https://en.wikipedia.org/wiki/Polygon_mesh)
* **Vector Processors** \-- Rather than the CPU working on one data item at a time, it can work on a vector of items at once. This is used in designing supercomputers. [https://en.wikipedia.org/wiki/Vector\_processor](https://en.wikipedia.org/wiki/Vector_processor)
* **Machine Learning** \-- in many parts of machine learning, the objects which are selected are feature vectors. https://en.wikipedia.org/wiki/Feature\_(machine\_learning)",22,0,0,False,False,False,1642758103.0
s95ek1,htkr1mh,t3_s95ek1,"There's the vector the combination of magnitude and direction, which is used in linear algebra, physics, engineering. There's vector the 1 dimensional array, which is used in a whole bunch of ways. There's vector the disease transmission method, which comes up in cybersecurity.
 
Your question really isn't answerable, there isn't going to be an article or youtube video covering what you are asking. Work on your question more to make it less vague and come back.",26,0,0,False,False,False,1642754482.0
s95ek1,htkqy95,t3_s95ek1,"Computer graphics, physics simulations/some CFD techniques are the ones I have off the top of my head. It’s used in game design and missile guidance too.

Edit:
https://en.wikipedia.org/wiki/Guidance,_navigation,_and_control

https://en.m.wikipedia.org/wiki/3D_projection

These are some introductory Wikipedia articles about topics where vectors end up being used with some frequency.",4,0,0,False,False,False,1642754409.0
s95ek1,htkmvkp,t3_s95ek1,Look into computer graphics.,10,0,0,False,False,False,1642751377.0
s95ek1,htkmuyf,t3_s95ek1,import numpy as np,-11,0,0,False,True,False,1642751365.0
s95ek1,htmxgha,t3_s95ek1,"Graphics, machine learning, many areas. Plenty of algorithms use it for various things, there are vectors involved in PageRank, the algorithm search engines like Google use.",1,0,0,False,False,False,1642792555.0
s95ek1,htsqaa8,t3_s95ek1,"used it a lot in computer vision, graphs & mathematically computation software like matlab",1,0,0,False,False,False,1642889070.0
s95ek1,htleud3,t1_htkqy95,Quaternions!!!,5,0,0,False,False,False,1642771192.0
s9ffoc,htnejwf,t3_s9ffoc,An ACM membership is less costly and as far I can tell is the same as the regular OReilly online subscription. The periodicals and other content/benefits you get from the ACM membership are excellent.,3,0,0,False,False,False,1642798731.0
s9ffoc,htpmjm9,t3_s9ffoc,I’d suggest O’Reilly. I probably own fifty of them. I like the consistency and structure of their books regardless of the author. The material is great both for learning and as a reference on the job.,-1,0,0,False,False,False,1642832748.0
s9ffoc,hu4b9xk,t1_htnejwf,"Thanks for the info mate  
What periodicals and benefits are there currently? is it this https://www.acm.org/membership/membership-benefits",2,0,0,False,False,True,1643089627.0
s9ffoc,hu5g0yu,t1_hu4b9xk,"That’s it. The ACM digital library, Communications of the ACM, and their various newsletters are really nice.",1,0,0,False,False,False,1643117907.0
s9c6ve,htrm8do,t3_s9c6ve,"> My question is that is data aligned according to its size(self-alignment) or according to the memory access granularity of the processor.

If ""memory access granularity"" refers to the granularity of memory accesses as treated by the memory consistency model, then the answer to your question is according to data type size.

> It would be great if you could specify some cpu architecture that allows data types larger than its memory access granularity.

The Cray NV-1 and NV-2 architectures (implemented by the X1 and X2 supercomputers of the 2000s) had a 32-bit access granularity, but was a clean-sheet 64-bit architecture.",2,0,0,False,False,False,1642873051.0
s8eq88,htfs44z,t3_s8eq88,"""Master's of Doom""- David Kushner, also ""Hackers Heroes of the computer revolution"" - Steven Levy, really motivated me for like a month or 2 after reading them. But motivation is overrated, what you really want is discipline. If you're able to do something when you don't feel like it you've already out did the average person.",38,0,0,False,False,False,1642669835.0
s8eq88,htgccl7,t3_s8eq88,Idk just read wikipedia of von neumann.,22,0,0,False,False,False,1642684129.0
s8eq88,htghdte,t3_s8eq88,Godel Escher Bach by Hofstadter might do the trick.,8,0,0,False,False,False,1642686642.0
s8eq88,hti5kva,t3_s8eq88,"The Mythical Man-Month

Overview: https://en.wikipedia.org/wiki/The_Mythical_Man-Month",3,0,0,False,False,False,1642710002.0
s8eq88,hti62oc,t3_s8eq88,"Hmm, these aren't necessarily the deepest but I think you would like these.

Just for Fun: The Story of an Accidental Revolutionary by Linus Torvalds

Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age by Michael A. Hiltzik

Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Tim Berners-Lee",3,0,0,False,False,False,1642710237.0
s8eq88,htie32c,t3_s8eq88,"Geoffrey James, The Tao of Programming (1986)

Jon Bentley, Programming Pearls (1986)

Jon Bentley, More Programming Pearls (1988)

Jon Bentley, Programming Pearls 2/e (1999)

Gene Kim, The Unicorn Project (2019)",3,0,0,False,False,False,1642713182.0
s8eq88,htjhdyt,t3_s8eq88,"the 1981 Pulitzer Prize winner ""Soul of a New Machine"" by Tracy Kidder, the 1995 novel ""Microserfs"" by Douglas Coupland, ""Neuromancer"" and the short story ""Burning Chrome"" by William Gibson, the aside beginning with ""The programmer, like the poet"" in Fred Brooke's Mythical Man-Month, ""A Mathematician's Apology"" by GH Hardy, Wigner's essay ""the unreasonable effectiveness of mathematics in the natural sciences"", Dijkstra's ""a Discipline of Programming"", all the footnotes from SICP and any HAKMEM you can find, aaronson's ""the ghost in the quantum machine"", and The Mentor's ""Conscience of a Hacker"" from Phrack issue 7.",3,0,0,False,False,False,1642729837.0
s8eq88,hthajdv,t3_s8eq88,CODE,6,0,0,False,False,False,1642698280.0
s8eq88,htga5q7,t3_s8eq88,Pragmatic programmer is good choice.,4,0,0,False,False,False,1642682923.0
s8eq88,htgp4ef,t3_s8eq88,Algorithms to live by is a good one. Not motivational but gives hope that everything we learn has a real life use other than putting food on our table,4,0,0,False,False,False,1642690104.0
s8eq88,hthffda,t3_s8eq88,"* A bit old now, but Dust or Magic is interesting from a game design point of view https://www.amazon.com/Dust-Magic-Bob-Hughes/dp/0201360713 
* Similar vintage, Are your lights on? more about the analysis side of things than actual coding https://www.amazon.com/Are-Your-Lights-Figure-Problem/dp/0932633161
* greenspun's Web Publishing https://www.amazon.com/Philip-Alexs-Guide-Web-Publishing/dp/1558605347
* Programmers at work, an older classic https://www.amazon.com/Programmers-Work-Interviews-Computer-Industry/dp/1556152116",4,0,0,False,False,False,1642700028.0
s8eq88,hti70ew,t3_s8eq88,[The Cathedral and the Bazaar](https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar) is a classic philosophy of code book that every programmer should read at some point.,2,0,0,False,False,False,1642710600.0
s8eq88,htkf9xz,t3_s8eq88,Pragmatic Programmer is a pretty nice read,2,0,0,False,False,False,1642746200.0
s8eq88,htfrw17,t3_s8eq88,I’m reading [The Making of Prince of Persia](https://www.jordanmechner.com/store/the-making-of-prince-of-persia/) by Jordan Mechner. I love it.,3,0,0,False,False,False,1642669654.0
s8eq88,htiytgt,t3_s8eq88,Dreaming In Code,1,0,0,False,False,False,1642721936.0
s8eq88,htkainf,t3_s8eq88,The hungry hungry caterpillar teaches you how so many different parts of a whole go into making the program.,1,0,0,False,False,False,1642743452.0
s8eq88,htmp5lh,t3_s8eq88,Wow this post is getting some action! Thanks everyone for your suggestions.,1,0,0,False,False,True,1642789424.0
s8eq88,hu4ljau,t3_s8eq88,Thanks for this post! I’ve been in search of something similar,1,0,0,False,False,False,1643096375.0
s8eq88,htggsz3,t1_htfs44z,"“Masters of Doom”! I think DOOM is one of the best case studies in CS/SE, that every aspiring student or seasoned practitioner should analyze because what John Carmack produced, was really incredible. He is, probably, my biggest inspiration, that made me transition from just focusing on 3D graphics as an art form, to wanting to explore computational systems further.",9,0,0,False,False,False,1642686369.0
s8eq88,htk6nms,t1_htfs44z,"Bornstrom's thesis on why we most probably are living in a simulation, following the same line of thought a programmer in a sufficiently advanced civilization is god (quite literally).",2,0,0,False,False,False,1642741381.0
s8eq88,htj1o8d,t1_htgccl7,It's crazy to me how somebody can be that intelligent.. like the difference between von Neumann and your average CS graduate is probably like the difference between your average CS graduate and a 12 year old,2,0,0,False,False,False,1642723115.0
s8eq88,htiewua,t1_htie32c,"Haven't read The Unicorn Project, but The Phoenix Project was worth a read imo",3,0,0,False,False,False,1642713482.0
s8eq88,hti1m1s,t1_htggsz3,he is a true programming cowboy,5,0,0,False,False,False,1642708156.0
s8eq88,htjvst8,t1_htiewua,"Loved Phoenix Project! A little more DevOps-y, but also we’ll worth reading for coders, IMHO.",3,0,0,False,False,False,1642736147.0
s8kxq7,htiujzg,t3_s8kxq7,"Gale-Shapley could fail because it's not optimizing a global quantity like the sum of the distances. Take this case where you want to connect lower case and upper case letters and the distance between them is the literal distance in the figure:

A---a-B---b-C---c

Gale-Shapley will give you a stable matching so it will connect a and B (because if a is not connected to B, then a and B will ditch their partners and get together) and b and C. So that leaves A and c to get connected which is a huge distance. Clearly connecting A and a and B and b and C and c is shorter than just the A to c distance.

What you're looking for is obviously well studied since it's such a fundamental question. I don't know much about it, but here's a link suggesting that there is an efficient algorithm for it even if the distance of each pair could be chosen arbitrarily (and a super-efficient randomized algorithm!): https://en.wikipedia.org/wiki/Assignment_problem#Balanced_assignment",3,0,0,False,False,False,1642720180.0
s8kxq7,hthun7u,t3_s8kxq7,"Yes, Gale-Shapley will work. Model the people and houses as proposers (men/women in the original algorithm) and the sorted distance to each of the other as their list of preferences. Running the algorithm will give you a stable matching, which means no other pairing would have a greater total preference (lower distance).

Edit: one thing to note, the stable matching will minimize the distance for the proposer, so model the problem accordingly.",3,0,0,False,False,False,1642705556.0
s8kxq7,htugh9g,t3_s8kxq7,It's just maximum bipartite matching,1,0,0,False,False,False,1642915839.0
s7yrsa,htd2pzm,t3_s7yrsa,"CPUs are designed to operate on bytes, not individual bits.
8bit=1 byte, 
 16bit=2bytes. 
There is no way to operate on half of a byte or any other fraction.

Interestingly, this limitation doesn't actually matter. Compression algorithms will use huffman coding. If you somehow made an image that only used 12 bits and the rest was filled with rezos, compression algorithm could take care of that and find some optimal way to store such images any way.",90,0,0,False,False,False,1642623247.0
s7yrsa,htdl2ky,t3_s7yrsa,"In a sense, tradition.

Earlier computers were designed around different bit lengths: [4 bit architectures like the early 4 bit microprocessors](https://en.wikipedia.org/wiki/4-bit_computing), early PDP computers (like the PDP-8) had a [12 bit architecture](https://en.wikipedia.org/wiki/12-bit_computing), some early mainframes used [36 bit architectures](https://en.wikipedia.org/wiki/36-bit_computing)--and early versions of the LISP programming language were built around those architectures. (LISP uses a two-pointer cell with some control bits--and with a 16-bit address system, 36 bits works very well for a two-pointer cell with 4 control bits.)

But eventually we seemed to settle on 8 bits because it was a power of 2 and because the ASCII character set would fit inside. And eventually we settled on powers of 8 bits (8 bit, 16 bit, 32 bit, 64 bit) for backwards compatibility.

But there is absolutely nothing magic about 8-bits outside of tradition.",22,0,0,False,False,False,1642629948.0
s7yrsa,htg46ge,t3_s7yrsa,"I think the other answers here are excellent, but wanted to point out that sometimes we do still break with tradition. HDR video, for example, is typically 10-bit per color.",3,0,0,False,False,False,1642679159.0
s7yrsa,htd6sso,t3_s7yrsa,"It often is useful to make the word size a power of two. For example, a bit multiplexer will be able to cleanly select a bit out of such a register.

Further, once 8 bit CPUs became a thing, memory being byte-addressable became a standard. If you now design a new CPU which does not use a word that is a multiple of 8 bits large, you either need completely new memory or are going to waste bits. Both is inefficient, so new CPUs are adapted towards existing RAM, and we have stuck with 1 byte being the basic unit of memory ever since.",11,0,0,False,False,False,1642624757.0
s7yrsa,hted1ei,t3_s7yrsa,"There are color formats between 8 and 16 bits:

https://en.m.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats

Most modern architectures have a memory bus 2^n bytes wide, so many data formats these days will use as much of that as possible. Compact representations these days are much less important than memory bandwidth and latency.

The real question is why a byte is 8 bits wide. Believe it or not, it’s mostly just consensus. 8 bits was the right size at the right time, neither too big for a memory bus nor too small to be useless. For example, you can fit both the upper and lower Latin alphabet in 7 bits with an extra bit as a check bit (ASCII), so it’s a convenient size.",3,0,0,False,False,False,1642641820.0
s7yrsa,htdx4be,t3_s7yrsa,"In computing because of bits everything is POWER OF 2, so get used to 2, 4, 8, 16, 32, 64, 128... 1024, 2048...",3,0,0,False,False,False,1642634796.0
s7yrsa,htdoqvg,t3_s7yrsa,Pretty much everything in computers boils down to boolean operations. This is either true or false; 0 or 1. The technical reason is that either electricity is going through or it is not. Since that only gives you two options everything else tends to involve powers of 2.,0,1,0,False,False,False,1642631383.0
s7yrsa,htene1e,t3_s7yrsa,x2,1,0,0,False,False,False,1642646424.0
s7yrsa,htf4rdb,t3_s7yrsa,Much easier to write the logic for word sizes that are sections of each other. Be much harder to make a 13bit machine run compatibly with 8bit logic.,0,0,0,False,False,False,1642654502.0
s7yrsa,htf9d3q,t3_s7yrsa,All thing happens in bytes! So a byte is equivalent to 8 bits. 2 byte is 16 bits. If there was anything inbetween 8 n 16 bits it could be in decimals maybe.,0,0,0,False,False,False,1642656970.0
s7yrsa,htfj07t,t3_s7yrsa,Binary means base two. 8 and 16 are powers of two.,0,0,0,False,False,False,1642662978.0
s7yrsa,htfs4hc,t3_s7yrsa,Take the binary log of 8 and 16. Does that answer your question?,0,0,0,False,False,False,1642669842.0
s7yrsa,htdvoyk,t1_htd2pzm,"> There is no way to operate on half of a byte or any other fraction.

Fun fact, half of a byte is called a nibble.  You'll only ever see it in assembly programming, and even then it's incredibly rare to bump into.",52,0,0,False,False,False,1642634210.0
s7yrsa,htgdpou,t1_htd2pzm,">rezos

Is this the CEO of azamon?",3,0,0,False,False,False,1642684842.0
s7yrsa,htgwja1,t1_htd2pzm,There is one computer I know of that uses 16 bits but one is for parity so there's 15 usable bits. It's the Apollo Guidance Computer.,2,0,0,False,False,False,1642693065.0
s7yrsa,htf4zma,t1_htd2pzm,"> There is no way to operate on half of a byte or any other fraction

You can certainly operate on arbitrary bit-sized integers, it's just less efficient than those same operations on the size and alignment that the CPU was designed for.

The folks at adobe probably decided image processing at arbitrary bit depths was just not worth the effort- nobody really needs it.  Users would be better served exporting to their desired bit depth if they had a use-case that *really* called for it.",3,0,0,False,False,False,1642654623.0
s7yrsa,htdytoy,t1_htdl2ky,"A slight clarification (this was a good write up of the history, not disagreeing with the parent comment at all), though the choice of 8-bits wasn’t magical, once 8-bits was chosen there is “magic” in terms of memory alignment and speed of operations, which is why you see formats aligned on powers of two boundaries.",10,0,0,False,False,False,1642635510.0
s7yrsa,hted32w,t1_hted1ei,"Desktop version of /u/bargle0's link: <https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",2,0,0,False,False,False,1642641841.0
s7yrsa,huhwlet,t1_htf4rdb,"ok, but why is it like that? 8 bit and 16 bit just represent how many possible color combinations per channel there are.",1,0,0,False,False,True,1643320124.0
s7yrsa,huhw912,t1_htfs4hc,"No it doesn’t, i don‘t get why in photoshop for example, you can only choose between 8 and 16 bit. Since options in between are possible. 8 bit are 256 possible colors per channel. And 16 bit are 65 536 colors per channel. So the difference is huge.",1,0,0,False,False,True,1643319996.0
s7yrsa,hte53ff,t1_htdvoyk,"And COBOL, IIRC. Signed numerics store the negative/positive in the low/right nibble of the lowest digit. Is it sad that this takes up space in my brain?",12,0,0,False,False,False,1642638254.0
s7yrsa,htg2jq0,t1_htdvoyk,"As I've mentioned in another comment, the x86 platform still has instructions for doing maths on binary coded decimals, where each decimal digit is represented by a nibble.  COBOL is the only language I've seen that supports it though.",1,0,0,False,False,False,1642678001.0
s7yrsa,hteq4cf,t1_htdytoy,"Memory alignment and speed of operations stuff happen because you can vectorize the base memory storage unit using multiples of that base memory storage unit at the same time--not because the base memory storage unit is a byte with 8 bits.

Meaning you gain a lot with a 16-bit processor over an 8-bit processor because it can operate on two bytes at a time rather than one byte at a time--not because the byte happens to be 8 bits.

Now it turns out 8 bits worked well in earlier microprocessor designs because it was twice 4 bits--the Z-80 CPU, for example, actually had a 4-bit ALU, and would perform things like addition operations by carrying out the operation on the bottom 4 bits first, then again on the top 4 bits. (It's one reason why the Z-80 had the 'half-carry' flag in the flag registers, though the 'half-carry' flag could then be used for the [DAA (Decimal Adjust Accumulator)](https://stackoverflow.com/questions/8119577/z80-daa-instruction) instruction to perform [BCD](https://en.wikipedia.org/wiki/Binary-coded_decimal) addition and subtraction.)

But now that modern CPUs perform vectorized math using multi-byte words, the advantage of doing things like performing 8-bit arithmetic using a 4-bit ALU is lost. There is no reason why an ALU can't be 9 bits wide or 23 bits wide, except that we have gotten so used to 8-bit bytes we seem to think there's no other way.

----

Edit to add: As a side note, the reason why the PDP-10 was 36-bits wide 'byte' was that it was twice the PDP-9's 18-bit wide 'byte', and was able to vectorize operations by operating on two 'bytes' at a time. (In older parlance each addressable unit of memory was referred to as a 'word', even though later parlance came to use the word 'word' to mean 2 8-bit bytes.)",3,0,0,False,False,False,1642647611.0
s7yrsa,huj8us5,t1_huhwlet,"So they represent how the storage is chopped up. Your different variables, like assigning a color to a pixel is part of a system of standardizations. We decided on word size (8bit) now people wrote programs to manage that data storage schema but making the first few bits represent commands that the hardware would interpret as actions to take. So a system of programs (operating system) was eventually developed to manage the addition and removal of sequences of words that formed the actions on a cpu (programs). Because of this standardization we had to agree on how things would be read so we can make generalized logic that can do things on every system.

So eventually we get to the application layer (layer 7) which is a composite of general logical “applications” that live below it (layers 1-6). At this point the application is running on 6 layers of logical framework that all expect layer 1 (hardware) to be exactly those word sizes so that it can perform the millions of actions needed to generate the amount of changes in a system to output such complex functionalities. The color is a pixel isn’t just coloring the pixel, it’s the management of data storage in a way that can manage the color of the pixel in such a robust way that the hundreds of apps and services that are offered today can easily manage the changing of that pixels color in a very very very specific way. 

When you code you generally don’t say what a pixels color is you code what the background color is, or the text buttons color is, or you create an array that contains bits configured in what the program expects as rgb format or whatever makes sense and then you tell the cpu/gpu how each word is chopped up because all the other programs running right now require that the data system be chopped up into exactly 8 or 16 or 32 bits.

Now that’s the reason for exact word sizes the binary expansion as in 8bits to 16bits is because now legacy systems can still chop the database up into 8bit sizes without breaking the systems that run on 16 bits. You just treat one word as two words within that applications allocated storage. 

TLDR: all other programs expect a standard word size, so new applications leverage capabilities built on top of those standards. We go from 8 to 16 because 8 bit programs can run still by treating each storage word as two words without running over a single word size allocation, so legacy programs can always run.

This is a simplification because there are also timings and physics stuff the hardware of the earlier times were hard coded into some paradigms, but the reading and organization of information lives eternally within the binary expansion paradigm",2,0,0,False,False,False,1643340122.0
s7yrsa,huhwyuy,t1_huhw912,"I thought your question was about word size and not a particular software. Anyways, historically, number that are powers of two made internal math easier. I gave you CSesque answer.",1,0,0,False,False,False,1643320265.0
s7yrsa,hte98dt,t1_hte53ff,and trivia games/shows. I feel like I've seen it on a number of them.,7,0,0,False,False,False,1642640114.0
s7yrsa,htf4wji,t1_hte53ff,It uses 4 bits to determine whether a number is negative? Why....,3,0,0,False,False,False,1642654578.0
s7yrsa,htf39bd,t1_hteq4cf,"To be clear (maybe my comment wasn't), I wasn't dismissing your original premise (8-bits was not a magical number, it just worked out well) but once we chose 8-bits, there's a reason you see in memory structures aligned on 8-bit or other powers of 2, like the the original questions graphics format.",2,0,0,False,False,False,1642653746.0
s7yrsa,htgnvpr,t1_htf4wji,"Disclaimer: Not a COBOL developer. I’ve just had to read a fair amount in a migration project.

From what I gather, a lot of the persisted data from COBOL is fixed width. Numerics being right justified. If the number gets big enough, you risk loosing the +/- if held on left side. By “packing” the sign with the smallest digit you guarantee knowing whether the numeric is negative or positive. Also ensures you can carry the same digit span.",3,0,0,False,False,False,1642689580.0
s7yrsa,htg28cz,t1_htf4wji,"Almost certainly binary coded decimal.  Each nibble is used to represent one decimal digit.  Yes, this wastes 6/16 of the available storage space but it's trivial to display as a decimal number.  Many CPUs (including x86/x87) have instructions for doing math on BCD numbers, though I imagine these days the use of them must be vanishingly rare.

Once you're using a nibble for each decimal digit, a nibble for the sign bit also makes some sort of sense.",1,0,0,False,False,False,1642677777.0
s7yrsa,htgnzr8,t1_htf39bd,"Absolutely some good things come from 8 bits--but a lot of the things we think of as intrinsically 8 bits in nature really aren't. 

Like 8-bit color in a color file. Some higher resolution monitors are capable of displaying more than 8-bit color--but the most I've ever seen is 10-bit colors; that is, [10 bits per RGB channel.](https://www.bouncecolor.com/blogs/news/10-bit-monitor)

When building graphics software, however, it's easier in this era of large hard disks and gigabytes of memory standard, it's easier and faster--because of the way we vectorize operations across multiple words--to use 16-bit color representations, rather than attempt to bit-shuffle 10 bits into a 30-bit representation.

Were we still using the PDP-8 architecture of 12-bit bytes, however, we'd probably be talking about the naturalness of using 12 bits as the limit of human vision, and talking about how it's natural to represent color as 3 12-bit bytes. There may even be people bragging about their 12-bit color monitors (though they were really only 10, and using dithering for the other 2 bits).

Color graphics in particular has always been a compromise between what the hardware can do and what the eye can perceive. And 8 bit color channels did not solve these compromises; your eye can perceive a 256 color gradient on a monitor.",1,0,0,False,False,False,1642689627.0
s7li8v,htau2oq,t3_s7li8v,"Educated guess, so I may be wrong, but I believe it is because really old modems used [Bauds](https://en.wikipedia.org/wiki/Baud) per second, which are equivalent to bits per second for binary systems.

Seeing as it was common to use the baud rate for dial-up modems, when they first introduced broadband, ethernet, etc., they just continued to use bits per second.

AFAIK, files were always organized into chunks, since if you accessed them bit by bit, you'd need a lot more storage to keep track of all the bits than the space of the drive itself. For example, standard ext file systems on linux are split into 4 kb chunks (by default, you can change that), so creating a file with 0 bytes will still take 4 kb. It also makes sense with 8-bit ASCII encoding to say that a file containing the string `hello` takes 5 bytes, and not 40 bits.

> Can't we just have the same unit for both of them (to reduce the confusion of some people)?

Probably too late now. You have decades of documentation using other standards... Why change now?",26,0,0,False,False,False,1642586933.0
s7li8v,htbmcbr,t3_s7li8v,"It's mostly historical. 

Even today, most internet traffic is serialised over a single wire. So that wire sends bit after bit after bit. Sure, it packets them up into bytes/packets/whatever, but the fact is that it's a 1-bit serialisation.

However most memory modules and disks operate in terms of bytes, usually burst-reading a row or sector at a time.

Both of those descriptions aren't 100% true in all cases, but are the most common situations for both formats. (e.g. there are very few parallel communication cables/protocols, but they do exist)",6,0,0,False,False,False,1642603366.0
s7li8v,htao19s,t3_s7li8v,"It might be a marketing strategy, but it might be the resolution perhaps? You transmit bits and you store a byte. Storing a bit doesent makes much sense, but it could be that the marketing strat works on me haha",17,0,0,False,False,False,1642582118.0
s7li8v,htcebuq,t3_s7li8v,"When you receive data you are not just getting the bytes that wind up in the file. You are also getting a bunch of other bits that belong to the frame/packet that the data comes in.

If they changed it to bytes/sec that might be confusing/misleading because people may make the leap from ""bits in"" to file download size, which would not be correct.

I agree with some of the others though. There is at least some historical component to it as well.",3,0,0,False,False,False,1642614233.0
s7li8v,htc4nwd,t3_s7li8v,Marketing: 1 gigabit/sec sounds better than 125 megabyte/sec.,2,0,0,False,False,False,1642610648.0
s7li8v,htbeim9,t3_s7li8v,"They are measuring different things.

When you download a file of 1024 bytes, you expect it to take up 1024 bytes in disk.

Now suppose your internet connection is 8,192 bits per second (=1024 * 8).  You might expect your file to take one second to download but you would be wrong.  Every network connection has overheads and the rate that you can transfer useful data is always less than the line speed. Overheads are framing, headers, packet encapsulation and so on. But if you tell people they can get 10 MBps they expect to download 10MB in a second, while they'll actually get about 850-900kBps depending on the exact networking technology.

Another reason is that people building networking gear care a lot about the bit rate to figure out carrier frequencies and so on and these technologies often originated in telephony rather than packet data, where you really do care more about bits than bytes.",4,0,0,False,False,False,1642599859.0
s7li8v,htanqpy,t3_s7li8v,"its a marketing strategy
edit: people hardly ever notice the difference between Kb and KB or dont know and companies try to exploit that to show bigger values",5,1,0,False,False,False,1642581889.0
s7li8v,htanu46,t3_s7li8v,"I guess one of those things that started as bits cause data flow was so slow people couldn't use bytes in units, and then as speeds increased changing the understanding from bits to bytes would be an industrial problem, hence bits per second is now a standard.

Why bytes for storage? Well data storage started as bytes to begin with iirc, so it stayed as industrial standard.",3,0,0,False,False,False,1642581961.0
s7li8v,htbemxw,t3_s7li8v,"Back then transmission speeds were very low. 9 kilo bits, or maybe slower were the norm at some time. Advertisement schemes aside, it would be less practical to express these speeds in terms of bytes.",1,0,0,False,False,False,1642599917.0
s7li8v,htckb6u,t3_s7li8v,"It has to do with how transmission of data works. The signal on a single wire can only have one measured state at a time.  
  
And once you realize that errors can happen and implement *error correction* then 1-bit/1-byte of transmitted signal transitions is no longer equivalent to 1-bit/1-byte of actual data.
  
There’s probably some deliberate fudge factor at the level of an Internet Service Provider (ISP) as far as making it sound *faster*.",1,0,0,False,False,False,1642616454.0
s7li8v,htanu0n,t3_s7li8v,I don't really know but I would guess it has something to do with older internet speeds being much slower and storage capacity growing at a significantly faster rate in comparison.,-5,0,0,False,True,False,1642581959.0
s7li8v,htbc70y,t3_s7li8v,"Its because they aren’t actually equal to each other which is a pretty common misconception. 

One byte is equal to 8 bits however one bit is so incredibly small its impossible to have one bit in the 21st century.

Edit: i should also mention that kilobits, megabits, etc. do exist. Gigabit probably the being mentioned the most. But one gigabyte isnt equivalent to one gigabit. As far as im aware its a marketing scheme as to why we dont just call them the same thing. If your isp says we offer gigabit internet most people assume they are getting “gigabyte” internet and don’t even think about there being a difference",-3,0,0,False,False,False,1642598721.0
s7li8v,htau3ws,t1_htau2oq,"**[Baud](https://en.wikipedia.org/wiki/Baud)** 
 
 >In telecommunication and electronics, baud (; symbol: Bd) is a common unit of measurement of symbol rate, which is one of the components that determine the speed of communication over a data channel. It is the unit for symbol rate or modulation rate in symbols per second or pulses per second. It is the number of distinct symbol changes (signaling events) made to the transmission medium per second in a digitally modulated signal or a bd rate line code. Baud is related to gross bit rate, which can be expressed in bits per second.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",4,0,0,False,False,False,1642586959.0
s7li8v,hudqloz,t1_htau2oq,"I've wondered for quite a while, that's really informative",1,0,0,False,False,False,1643248349.0
s7li8v,htb68o1,t1_htao19s,I had a professor in school tell us it was marketing lol,4,0,0,False,False,False,1642595479.0
s7li8v,htbignx,t1_htao19s,"Practically speaking, you transmit packets, which contain an amount of data you'd generally measure in bytes. It's marketing.",7,0,0,False,False,False,1642601682.0
s7li8v,htcl0t1,t1_htc4nwd,"125 MB/sec is actually an absurdly fast rate of transfer  when you consider the physical distance traveled and the sheer quantity of intermediate hardware.  
 
That’s almost as fast as the fastest of IDE hard drives ever and they operated *locally* and transmitted data in *parallel*.",1,0,0,False,False,False,1642616717.0
s7li8v,htdflgs,t1_htbeim9,"This is a weirdly misleading comment.

>They are measuring different things.

They measure the amount of data in both cases.

>When you download a file of 1024 bytes, you expect it to take up 1024 bytes in disk.

You may expect that, but you would be wrong on most today's disks. The allocation units where the files are stored have some minimal size, e.g. 4KB. So the amount of useful information stored on the disk is lower as well (can be 4096x lower).

But even if there was a different interpretation for these two cases of measuring amounts of data... How exactly would help using one unit for one case and this unit * 8 for the other one?",1,0,0,False,False,False,1642627946.0
s7li8v,htcm7o7,t1_htbeim9,Error correction encodings play a role too.,1,0,0,False,False,False,1642617152.0
s7li8v,htats0b,t1_htanqpy,Fun fact: a kilobyte doesn't actually mean 1000 bytes. It means 1024 bytes... IIRC power of 2.,3,0,0,False,False,False,1642586700.0
s7li8v,htanxzu,t1_htanu46,"Forgot to add, all data communication protocols are bits per second. QPI, PCI/E, RAM and so on, not just network.",5,0,0,False,False,False,1642582047.0
s7li8v,htbk626,t1_htbemxw,"> 9 kilo bits, or maybe slower were the norm at some time.

9 kbps was in the 1980s. The earliest modems were like 300 bits per second lol",0,0,0,False,False,False,1642602429.0
s7li8v,htbkpv9,t1_htau3ws,"This is the most correct answer. Transmission just cares about how fast can I send data and there isn’t a set number of 8 bit bytes I have to send. And remember that historically bytes were hardware dependent and of varying length. Also, transmission speeds aren’t just for your network but also used for inside your computer for other transmissions like the bus speed.",8,0,0,False,False,False,1642602667.0
s7li8v,htbs9ug,t1_htbignx,"> Practically speaking, you transmit packets, which contain an amount of data you'd generally measure in bytes.

Nowadays, sure. However, modems didn't just appear out of the blue. They are the product of years of evolution. Data used to be transmitted in different formats prior to ASCII, such as the [Baudot code](https://en.wikipedia.org/wiki/Baudot_code).

> It's marketing.

It''s a legacy of history.

- Does it benefit telecom companies? Of course.
- Are they happy that it inflates their speed numbers? Hell yeah.
- Is marketing the reason things have been counted in bauds and bits for 6+ decades? **No.**

Actually, one ""byte"" has historically had different bit values. 

> The size of the byte has historically been hardware-dependent and no definitive standards existed that mandated the size. Sizes from 1 to 48 bits have been used.[4][5][6][7] The six-bit character code was an often-used implementation in early encoding systems, and computers using six-bit and nine-bit bytes were common in the 1960s. These systems often had memory words of 12, 18, 24, 30, 36, 48, or 60 bits, corresponding to 2, 3, 4, 5, 6, 8, or 10 six-bit bytes. In this era, bit groupings in the instruction stream were often referred to as syllables[a] or slab, before the term byte became common.

https://en.wikipedia.org/wiki/Byte",9,0,0,False,False,False,1642605810.0
s7li8v,htcafw1,t1_htbignx,"Each of those packets needs to have bit level error correction, and each bit is the smallest resolution (and the biggest) that needs to be considered. When stored and accessed by the CPU, we usually only use byte level precision (indexing by bits would mean we could only address a very small space and isn’t practical for anything).

So no, it isn’t just marketing, and it isn’t just a relic of old times. The problems from that relic era are still alive and well today.",2,0,0,False,False,False,1642612805.0
s7li8v,htcmkzs,t1_htcl0t1,"Oh yeah, I agree. And with minor exceptions here and there, the data is almost always perfectly intact with all those trillions of transfers going back and forth every few seconds",1,0,0,False,False,False,1642617288.0
s7li8v,htauik5,t1_htats0b,"> Fun fact: a kilobyte doesn't actually mean 1000 bytes. It means 1024 bytes... IIRC power of 2.

[Theoretically wrong](https://en.wikipedia.org/wiki/Kilobyte). A **kibi**byte is 1024 bytes according to SI and the IEC.

In practice, people misfortune words all the time, so it's hard to be certain.",9,0,0,False,False,False,1642587281.0
s7li8v,htcn6i9,t1_htbkpv9,"Bytes have been commonly 8-bits for a very long time (since the 1970s at least), though, even if it wasn’t codified as a standard yet.  
  
The reason it was ambiguous and hardware dependent before was that a *byte* was defined as “the number of bits that were used to encode a *single character*”.",2,0,0,False,False,False,1642617505.0
s7li8v,htb4m7s,t1_htauik5,">In some areas of information technology, particularly in reference to solid-state memory capacity, kilobyte instead typically refers to 1024 (210) bytes.

""kibibyte"" was an attempt to resolve the confusion of ""kilobyte"" having 2 meanings but despite working in the computer industry for nearly 40 years I have literally never heard anyone ever say it nor use it in a piece of tech writing.",10,0,0,False,False,False,1642594485.0
s7li8v,htaujky,t1_htauik5,"**[Kilobyte](https://en.wikipedia.org/wiki/Kilobyte)** 
 
 >The kilobyte is a multiple of the unit byte for digital information. The International System of Units (SI) defines the prefix kilo as 1000 (103); per this definition, one kilobyte is 1000 bytes. The internationally recommended unit symbol for the kilobyte is kB. In some areas of information technology, particularly in reference to solid-state memory capacity, kilobyte instead typically refers to 1024 (210) bytes.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",7,0,0,False,False,False,1642587303.0
s7li8v,htcnzda,t1_htauik5,"In practice though, people mostly use *kilobyte* differently when talking about computers. The US also doesn’t uniformly use SI units anyway.",1,0,0,False,False,False,1642617795.0
s7li8v,htb5tl5,t1_htb4m7s,"> ""kibibyte"" was an attempt to resolve the confusion of ""kilobyte"" having 2 meanings but despite working in the computer industry for nearly 40 years I have literally never heard anyone ever say it nor use it in a piece of tech writing.

Indeed. I did mention there is a disconnect between the theory and practice. Still, it does exist. `dd` uses both.

    > dd if=/dev/random of=66k bs=65536 count=1
    1+0 records in
    1+0 records out
    65536 bytes (66 kB, 64 KiB) copied, 0.0034109 s, 19.2 MB/s

    > ls -lh 66k # gnu's ls -lh defaults to KiB
    -rw-r--r-- 1 u g  64K Jan 19 14:18 66k
    > ls -l --si 66k  # but it does have an option to use SI units
    -rw-r--r-- 1 u g 66k Jan 19 14:18 66k

I guess Linux can't change that in core utils to not break backwards compatibility.

> I have literally never heard anyone ever say it nor use it in a piece of tech writing.

I mean, I did write it in a comment on a tech science subreddit and linked to an article that covers it, so you are also misusing the word literally :)",1,0,0,False,False,False,1642595228.0
s7li8v,htcozy5,t1_htcnzda,"> In practice though, people mostly use kilobyte differently when talking about computer.

Again, yes, I have mentioned that.

However, this is /r/computerscience, not R/everydayaveragejoecomputing. In science, it is important to be precise. There is a clear distinction, and it is important to be aware of it, and understand it. If you design a hard drive, it's important to provide your customers accurate information regarding its capacity.

For every day usage/speech, if I download a Linux ISO and it's 1.5 gigs, I don't really care whether it's GB or GiB.",3,0,0,False,False,False,1642618169.0
s7li8v,htc9ce9,t1_htb5tl5,"He didn't hear you talking, and he probably didn't click the link, so his literally is likely to be accurate.",1,0,0,False,False,False,1642612398.0
s7li8v,htcoaia,t1_htb5tl5,"That’s just confusing though and I doubt most people read *KiB* as *kibibyte*. I believe the changeover in Linux to that is relatively recent in any case (last ten years?)
 
In computer parlance, *kilobyte* is abbreviated as **KB** whereas *kilobit* is shortened to **kb**. To try to use ‘kB’ is just confusing.",1,0,0,False,False,False,1642617908.0
s7li8v,htcptqv,t1_htcozy5,"This has nothing to do with computer science or ‘the average joe’, it’s about how humans use *language*.  
 
It was established a long time ago that a kilobyte (KB) was equal to 1024 bytes. Trying to force change has only resulted in more confusion.
  
Also, SI is not totally universal, no matter how much it’s creators wanted it to be. The UK eventually transitioned, albeit slowly, and the US never fully switched over. What’s more is that *bits* are conceptual and insubstantial, not liquids/solids/or gases.  
 
We also have an imperial system in common use that’s got a standardized conversion to metric, so there’s really no incentive for actual usage to change.
  
P.S.  
https://usma.org/a-chronology-of-the-metric-system",0,0,0,False,False,False,1642618468.0
s7li8v,htcqk7d,t1_htcoaia,"> That’s just confusing though

Pretty clear to me, but I have been working with computers a long time. Other tools also display both (e.g. `fdisk`), while others let you define which units to use (aforementioned `ls`, but also `free`).

It's just one of those things you can easily google if you care to understand it, I guess... And something most users don't even have to care about, really.",1,0,0,False,False,False,1642618738.0
s7li8v,htcv2ov,t1_htcptqv,"> It was established a long time ago that a kilobyte (KB) was equal to 1024 bytes. Trying to force change has only resulted in more confusion.

Established by whom? It was never formally established until the the IEC (of which the US is a full member) published their standard in 1999. Hard drives used 10^n for 50 years. A one-terabyte hard drive is 0.91TiB.

Also, `kB` is the preferred symbol, FYI.",2,0,0,False,False,False,1642620417.0
s7li8v,htcqwjw,t1_htcqk7d,"What I’m really getting at is that, afaik, ‘kB’ doesn’t really mean anything. If people want to say 1000 bytes, that’s how they say it, at least on this side of the world.  
  
I don’t think most people vontinuously check the man pages for updates. And if the standard output works for them, they aren’t likely to seek an alternative.",1,0,0,False,False,False,1642618865.0
s7li8v,htcwby2,t1_htcv2ov,"Established in *common usage*. 
  
What some standards body says is irrelevant if it doesn’t have universal (or near universal acceptance). 
 
Honestly I think people are even less concerned with the *exact* capacity of hard drives. They’ve grown so large that most people don’t even worry about filling them up. Heck you can get 4 TB capacity practicaly off the shelf.",0,0,0,False,False,False,1642620882.0
s7li8v,htcxw0n,t1_htcwby2,"> Established in common usage.

Except it very much is not, since it means different things to different people. A 16 GB stick of RAM has a different amount of bytes than a 16 GB HD.

Standardization is important for industries. It doesn't matter if people misuse language, but it matters that if you purchase a hard drive, you can see that it conforms to the specifications.

> What some standards body says is irrelevant if it doesn’t have universal (or near universal acceptance).

I'd say it's the opposite. It doesn't matter that your Average Joe misuses the terms because he doesn't understand them. It is far more important to be able to look at the output of a system monitoring command and be able to adequately interpret it. When tools allow you to choose between SI and binary (KiB, ... PiB), you can get accurate information.

Anybody who thinks that kB vs KiB is too confusing might not be particularly suited for computer science.",2,0,0,False,False,False,1642621459.0
s7li8v,htcyorr,t1_htcxw0n,"What you don’t seem to understand is that KB or KiB is irrelevant so long as we all agree that it’s 1024 bytes. The misbehavior of for-profit corporations is utterly irrelevant.  
  
There is absolutely no reason that computing needs to care about SI at all.",1,0,0,False,False,False,1642621755.0
s82mjq,hte4ro0,t3_s82mjq,Cisco Packet Tracer? Helped me in understanding networking and was used in school.,5,0,0,False,False,False,1642638109.0
s82mjq,htexy11,t3_s82mjq,"Protocols are defined and written out explicitly. For instance, UDP is defined as [RFC 768](https://datatracker.ietf.org/doc/html/rfc768)

You just search “<protocol name> protocol paper” and it’ll come up",3,0,0,False,False,False,1642651153.0
s82mjq,htk0lb5,t3_s82mjq,Professormesser.com network + study videos. All free on YouTube,1,0,0,False,False,False,1642738367.0
s7h2vc,ht9znq5,t3_s7h2vc,"Big O notation tells you how much slower a piece of a program gets as the input gets larger. For example:

* Returning the first element of an array takes a constant amount of time. If you double the length of an array, it still takes the same amount of time. That code does not scale with input size.

* Summing all the elements of an array takes a linear amount of time with regard to the length of the array. Double the length, the code takes roughly twice as long. Make the array a hundred times longer, code takes a hundred times as long.

* Searching for an element in a sorted array? If you're using a binary search, every time the array length is doubled you need to add one extra comparison. Performance _does_ get worse as the array length increases, but less than linearly.

The ""notation"" of Big O notation is just concise shorthand for describing the above patterns. O(1) for constant time, O(n) for linear time (where `n` is the length of the array), O(log n) for logarithmic time, etc.

The focus of Big O notation is on looking at the biggest trend of an algorithm. We don't care whether an algorithm takes 200 steps or 201 steps, we just care about how it scales as input size changes. Therefore, we only look at the slowest part of an algorithm. O(n + 1) simplifies to O(n), because as the array size gets millions of times longer the O(1) is quite insignificant.

Outside of coursework you will very rarely, if ever, need to do a formal proof to demonstrate the Big O of an algorithm. Instead, you're being taught Big O notation now so that you can easily reason about algorithms and data structures in the future. ""Oh, I should use a hash table here instead of a tree, because random element access is O(1) instead of O(log n)."" It's a convenient tool for quickly reasoning about which of two algorithms or data structures will perform better, or for reasoning about your own code and where the bottlenecks are.

Edit: fixed typo",76,0,0,False,False,False,1642566573.0
s7h2vc,htad019,t3_s7h2vc,"It's used to notate the upper bound of how slow a function, `f(n)`, grows with respect to `n`. 

For example, consider the following function (in Python): 

```
def f(n): 
    print(""Hello World"") 
``` 

No matter what `n` is, this function will always run in *constant* time and is independent of the input `n`. We notate this bound with O(1) (1 meaning ""constant""). 

Now, consider this function: 

```
def f(n): 
    for i in range(n): 
        print(i)
```

Now, this function is dependent on `n`. If `n` is 1, it will print 1 line. If `n` is 9000, it will print 9000 lines. We say that the growth of this function is *linear*, and use the notation O(n) to upper bound its growth. 

There's more nuances: like the difference between upper bounding, lower bounding, and the theta one; how do you choose the best upper bound (can't we just say everything is upper bounded by n^n ?); and, how do we calculate the best equation for different functions. But, that's not for dummies.",9,0,1,False,False,False,1642574097.0
s7h2vc,htaocl9,t3_s7h2vc,"I recommend you check out the book [*Grokking Algorithms*](https://www.manning.com/books/grokking-algorithms).

It covers common algorithms and Big O in very approachable terms.

[Here is their ~~explanation~~ *comparison* of common Big O runtimes](https://imgur.com/a/WzTcnAV).

Edit: `s/explanation/comparison/`",3,0,0,False,False,False,1642582364.0
s7h2vc,htav6jh,t3_s7h2vc,"**Big O is the language that we use for talking about how long an algorithm takes to run.** \[This is a simplified definition\]

When talking about the running time of an algorithm, we normally use 3 notations Omega Ω, Theta Θ  and Oh O.Those 3 notations represent the following bounds of the running time of an algorithms.

Omega : Lower BoundTheta    : Tight BoundOh         : Least Upper Bound

This dude explains the concept much more better than I can.[https://www.youtube.com/watch?v=f\_IaKCB7Zo8&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y&index=5](https://www.youtube.com/watch?v=f_IaKCB7Zo8&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y&index=5)",2,0,0,False,False,False,1642587800.0
s7h2vc,htbrwim,t3_s7h2vc,"Big O is a way to simplify away insignificant terms.

Let's say you have an algorithm that reads an array of length n and does something to it in **n^3 + 5 n^2 - 22 n + 17** steps. As n gets big, the only thing we care about is the **n^3**, because it'll be so much bigger than the other terms, so we say it takes **O(n^(3))** steps.

So it allows us to just deal with the important factors, and hand-wave away the smaller, less significant terms. This is really useful in algorithms where sometimes it takes 2n^2 + 19n + 85 steps and sometimes it takes n^2 + n steps. These cases still run in a similar amount of time, but analyzing the algorithm exactly would be an absolute pain, so it's much easier to do the math if we say the algorithm is just O(n^(2)).",1,0,0,False,False,False,1642605661.0
s7h2vc,htbsaw6,t3_s7h2vc,"A alg is O(some function of n)  

Means that the steps the algorithm will take are no more than f(n) if n is the size of the input.",1,0,0,False,False,False,1642605822.0
s7h2vc,htbxbj4,t3_s7h2vc,"In layman terms it is how fast your program does something in worst case scenario (a lot of input) and in best case scenario (not a lot of input) 

It basically says how efficient is your algorithm meaning the steps your program takes to achieve a certain thing.

Look on YouTube for a video called sorting algorithm visual and you will see unsorted bars in the video and then they get sorted with different algorithms and the ones that sort faster have a better big o Notation.

There are algorithm that are so good that it does not matter how many million inputs you have they do them at the same speed almost and there are bad ones that become slower with more Input",1,0,0,False,False,False,1642607801.0
s7h2vc,htc3u09,t3_s7h2vc,"There are n numbers in an array. You want to find the biggest number. You start from the beginning, checking each. 

The biggest number can be at the beginning, in which case that would take you 1 checks. But it can also be in the middle, which would take n/2 checks. And the worst case scenario, it can be the last number you check, which would take n tries to reach. Big O in this case would be O(n) because it denotes the worst possible case of runtime",1,0,0,False,False,False,1642610331.0
s7h2vc,htc9avk,t3_s7h2vc,"Suppose you are doing a task. And the task is such that no matter how you do it, the task will gets completed within 24 hrs. So, here you can complete that task in 2hrs, 6hrs,12 hrs and it won't take more than 24hrs. So, you can say that Big O of ""task"" is 24 hrs.",1,0,0,False,False,False,1642612382.0
s7h2vc,hteoe7o,t3_s7h2vc,"\--- The Story ---

Say we both have created algorithms to solve some problem.

And suppose it turns out that although your algorithm takes longer (I'm making the assumption that yours takes longer only because you asked me to assume you are dumb).

Now the question is how much longer your program takes to run than mine. Maybe on some input your program took 5 times as long as mine to run. That's useful information, but to more thoroughly compare our algorithms we have to see how it performs when we have large inputs.

There are two types of scenarios that come to mind:

\- Scenario type 1: Your program takes at most 10 times as long as mine to run, regardless of how large the input is.

\- Scenario type 2: I can't put a bound on how long your program takes compared to mine, because as we take larger inputs the ratio between the time your program takes and the time my program takes just keeps growing larger.

These cases are quite useful to differentiate between. They're saying something we should deem very important about the efficiency of our algorithms. Instead of starting off running our algorithms and computing the ratio between our running times for various inputs, we should first ask whether there would be a maximum ratio at all.

If there is no ratio (because it keeps growing), then we say that your algorithm's running time is NOT O(my algorithm's running time).

If there is a maximum ratio, then we say that your algorithm's running time is O(my algorithm's running time).

\--- Usage ---

If you're trying to create an algorithm, instead of computing exactly the number of steps it takes, it is better to get an estimate just using O() notation.

Let's take bubble sort with an array of size n. It can take many passes through the array to finish sorting, but not more than n passes. So overall it is taking around n\^2 steps. Do I care about whether it is 0.5n\^2 or 5n\^2? Yes, but not yet! Firstly it is definitely not taking more than 20 steps every time it moves between elements. So it takes at most 20n\^2 steps. The ratio between the number of steps and n\^2 is never larger than 20, so the running time of bubble sort is O(n\^2).

Before we pin down bubble sort's number of steps, let us first see if we can get an algorithm that takes O(n\^1.5) steps, or let's even see if bubble sort actually takes O(n\^1.5) steps. That would be a better guarantee. This is the larger, more important question to pursue.

You can prove that bubble sort does NOT actually run in O(n\^1.5) steps. This means that if somebody comes up with a complicated algorithm that takes 10000 n\^1.5 steps, that will still be better than bubble sort (because if bubble sort actually takes only 10000 n\^1.5 steps we would say it runs in O(n\^1.5) steps).

Figure out order first, worry about exact ratios later.",1,0,0,False,False,False,1642646861.0
s7h2vc,hz5u7iz,t3_s7h2vc,"It isn't an easy subject to grasp, in college the professor is nice overall but didnt do the best job of explaining it... neso academy on youtube has a playlist for data structures and big o , as well as you should get a data structure and algorithm book",1,0,0,False,False,False,1646297819.0
s7h2vc,ht9x9wd,t3_s7h2vc,It’s just a bunch of nonsense brogrammers ask at interviews.,-20,0,0,False,True,False,1642565403.0
s7h2vc,htdizum,t1_ht9znq5,"I wish my textbooks read this clearly. I’m not op, but this helped me a lot thanks",3,0,0,False,False,False,1642629177.0
s7h2vc,htcl6n2,t1_ht9znq5,"Amazing, saved. Thank you",2,0,0,False,False,False,1642616776.0
s7h2vc,htigj4u,t1_ht9znq5,"This was so helpful to me too, despite not being the OP! I actually feel like I somewhat understand Big O Notation now!",2,0,0,False,False,False,1642714056.0
s7h2vc,htdzku7,t1_htdizum,Thank you! That’s very kind :),2,0,0,False,False,False,1642635828.0
s7h2vc,htdzih2,t1_htcl6n2,You’re very welcome!,2,0,0,False,False,False,1642635800.0
s8a2k9,htf4kvu,t3_s8a2k9,"It sounds like you are just describing “life” as an NP problem (one where finding answers is hard but verifying it is much simpler). I kinda see the parallel you are drawing there. Don’t see how this is related at all to P=NP as clearly nature doesn’t solves the NP problem in P time, instead it takes forever randomly brute forcing it massively in parallel. If anything it’s a testament that P!=NP as even eons of evolution hasn’t found a better way.",4,0,0,False,False,False,1642654409.0
s8a2k9,htf6js2,t3_s8a2k9,"To be able to prove P=NP we would have to reduce a really hard problem into an easier problem. But first we need a formal solution that solves that hard problem. I appreciate your train of thought and it’s interesting to think more deeply. However, biology is not my speciality.

Using your example, first we would have to prove or disprove mathematically that there is an algorithm that can create life (I’ll leave this to the computational biologists) that is tractable or non-polynomial time. [There is evidence](https://www.pnas.org/content/118/49/e2112672118) that we (an AI) can design life using larger building blocks, but having one piece of evidence is not enough to satisfy this type of proof. We’d need something that can be replicated and reproduced empirically and to my knowledge we’re not quite there with synthetic life. In 20 years who knows what we’ll have. (Also please, someone with a better biological background correct me.)",1,0,0,False,False,False,1642655441.0
s8a2k9,htfvg25,t3_s8a2k9,">Biologically, for many organisms (think simple multi cellular; plants, things that don’t have super complex brains) we understand the structure and function of literally every single atom and cell that they are made of.

Name one such organism.",1,0,0,False,False,False,1642672531.0
s8a2k9,hthtaq1,t3_s8a2k9,"Are you sure that we can't design ""artificial"" life? 

[Here is one example of a company which grows artificial meat and designs artificial yeast for beer, using designer cells.](https://en.wikipedia.org/wiki/Ginkgo_Bioworks)

I think we could grow our own plants and design our own seeds from scratch on a CAD in the near future, if it isn't already the case somewhere at Monsanto or something. And that's just the tip of that awesome iceberg.

Edit: To clarify, I think we can probably make whatever we want. I am skeptical of limits of any kind. We can probably design whole organisms in a Frankenstein kind of way. I think that's more likely than not. People find strange comfort in these arbitrary limits on what we can and can't do, and sometimes I'm not sure they are reasonable ones. If we threw trillions of dollars at companies like the one I linked above and other such companies and researchers, then who knows.",1,0,0,False,False,False,1642705060.0
s8a2k9,htf62fu,t3_s8a2k9,"Wonderful thought, thanks for sharing!",0,0,0,False,False,False,1642655190.0
s8a2k9,htfdgxc,t1_htf4kvu,That’s what I’m saying. Nature shows that P != NP,0,0,0,False,False,True,1642659332.0
s8a2k9,htfdkky,t1_htf6js2,"My understanding is that we don’t have that algorithm right now, so as it stands right now, nature shows that P != NP",0,0,0,False,False,True,1642659395.0
s8a2k9,htfz074,t1_htfvg25,Many small Protozoa and very simple life forms. The main grey area in our knowledge is brain functions. There are organisms that are simple enough that we basically understand how each piece of them works.,1,0,0,False,False,True,1642675347.0
s8a2k9,htggp0x,t1_htfdgxc,"No this is not a prove. Let me reformulate your proof. We have no fast algorithm to make life. There is a nature algorithm but this one is terribly slow. Therefore there can not be a fast algorithm to make life.

The problem with your proof is that just because we have not found a fast algorithm does not imply that it does not exist. If you would like to proof P!=NP. You would have to proof that the existence of such an algorithm is impossible. This type of proof has been used in the halting problem.",1,0,0,False,False,False,1642686316.0
s8a2k9,htfguv2,t1_htfdkky,"For me, an analogous argument based off evidence isn’t enough to conclude that P=NP or P!=NP.

A counter example I could offer is that, according to the physicists, the universe’s age is on the order of 10^10 and the heat death is theorized to occur on the order of 10^100. Assuming that 10^100 is the time limit for life to form, there’s still a _lot_ of time left in the universe for life to do it’s thing (not even accounting for all the possible chances at life near other stars in the first 10^10 years) in tractable or non-polynomial time.

Perhaps if we had a perfect theory that combined physics, chemistry, biology, and computation into a model descriptive enough to prove or disprove either way. But from my knowledge we aren’t there (… yet!)",1,0,0,False,False,False,1642661490.0
s8a2k9,htmvy7v,t1_htfdkky,"""show"" in the mathematical terminology means the same as ""prove"" (like in mathematical prove). So that is clearly not the case here.

Still I like your idea, it is an interesting thought.",1,0,0,False,False,False,1642792002.0
s7mb3k,htau3gl,t3_s7mb3k,"To write down the input, you need to write down all n numbers, which takes space proportional to n. But for W, you only need to write down log W bits to represent the number W. This means that the input size is log W.

Technically you could force the input format to write the number W in unary (W bits long), and you’d have a modified knapsack problem that’s solvable in P. Nobody actually writes down inputs in that way for obvious reasons.",3,0,0,False,False,False,1642586950.0
s7mb3k,htaupe4,t1_htau3gl,"> But for W, you only need to write down log W bits to represent the number W. This means that the input size is log W.

Right. 

So, Knapsack is still in NP as `W` can be exponentially large wrt `n`. Correct?",1,0,0,False,False,True,1642587429.0
s7mb3k,hvot2i6,t1_htau3gl,[removed],1,0,0,False,False,False,1644062017.0
s7mb3k,htauyt3,t1_htaupe4,"W can be exponentially large relative to the input size. A problem is in NP if its solution can be verified in polynomial time relative to the input size. 

The input size here is n log W, since we need n numbers, no more than W bits each.",3,0,0,False,False,False,1642587635.0
s7mb3k,hvou27f,t1_hvot2i6,What?,1,0,0,False,False,False,1644062762.0
s71762,ht7o7v8,t3_s71762,"I meta language, but don't machine learn, so commenting for visibility.",1,0,0,False,False,False,1642532514.0
s71762,ht90mk6,t3_s71762,"It's all math that you'll have to learn converting 2d into 3d and I believe you will need to produce x,y,z coordinates from your mapping then you can create a 3d object.

Just focus on creating a sphere first based on your images.",1,0,0,False,False,False,1642550920.0
s71762,hth5eck,t1_ht90mk6,"I appreciate that ""it's all math"" and I'm attempting to do just this, but the point of my question is that I do not understand the fundamentals involved.  I'm currently re-learning linear algebra in the hopes that it will demystify some of what I'm seeing.  I'd hoped that there existed a tried and true method for doing this as it seems such a straightforward task.",1,0,0,False,False,True,1642696406.0
s6msij,ht4mdzt,t3_s6msij,"sci hub. combination of google scholar and sci hub is all you need tbh

&#x200B;

edit :  [sci hub](https://sci-hub.mksa.top/)",13,0,0,False,False,False,1642475215.0
s6msij,ht4mi0o,t3_s6msij,https://scholar.google.com,5,0,0,False,False,False,1642475263.0
s6msij,ht620h7,t3_s6msij,"[arxiv.org](https://arxiv.org) just in case, but sci hub is the holiest",2,0,0,False,False,False,1642508084.0
s6msij,ht6zenr,t3_s6msij,"I normally try with Google Scholar first, then one or all of the following: [z-lib](https://z-lib.org/) for books and some articles, [sci-hub](https://sci-hub.se/), [libgen](https://libgen.fun/scimag/index.html) when sci-hub fails.",1,0,0,False,False,False,1642523376.0
s6mxkp,ht57qbu,t3_s6mxkp,"The better analogy would be you a window cleaner/construction worker who is high up on building cleaning/constructing something.

Now you need to use things like hammers, brushes, cleaner fluid etc. When you are hanging there midway through a building, the tools you have access to fastest are the ones you carry in your hand. You dont need any extra time to use them. Your hands are like CPU registers. Fastest but limited space, your hands can only hold so many tools.

Say if you come across a difficult to clean spot that your current tools cannot be used for. Then you can carry some less frequently required but still important tools in your pockets. You will need some time to use it because you will have to empty your registers(hand) and take out the required tool from your pocket and you may need to search which pocket you kept it or you just remember (associative memory) which pocket carries which tool. This is the CPU cache (level 1). Your pocket too cannot hold too many tools so you may keep some on a backpack/toolbox that you carry with you, you will need more time again to take out the tool from there because you have to open and close the toolbox and replace something in your hand. But a toolbox allows you to carry a bit more than you your pockets. This is still cache but level 2 cache, slower than level 1 (pockets) but have more memory. A cpu can have L3 cache as well. (For this we would neee to talk about multiple workers (cores) sharing a common big tool box).

Next say you need some thing that is used even less frequently that you generally dont bother taking with you on person, however it still happens enough that you need to use it atleast a couple times per building, so instead of taking it with you on person you take it in your platform that you use to go up on, in this way if you need the tool you dont have to go all the way to the ground to get it, this is like RAM, much slower than cache you had to get down a bit to reach your hanging platform to reach it swap out some things and then carry it back up. Remember for registers and cache you didnt have to climb down. But is still pretty fast. And have way way more capacity than cache (toolbox or pockets).

Then sometimes it maybe so that you didnt prep well and left somthing in the garage/workshop below. But if you need something like that you MUST come all the way down take the thing and go back up. This is your disk, you have a huge space (workshops on the ground can be as big as you want) but you have to come all the way and go back. So it is very slow compared to RAM but can hold much more stuff.

And lastly say your company had a sudden emergency and now you are are legally required to wear a special suit to protect you (maybe they heard about asbestos in the building you were cleaning or something). But here is the thing, since this is something sudden even your company was not prepared for it, so you must drive to your company HQ get the suit from there and come back. This is hugely slow but also massively big, like once you have to freedom to go anywhere the whole world can be your storage. This is the internet and you getting the suit is just downloading.",5,0,0,False,False,False,1642485699.0
s6mxkp,ht4ro87,t3_s6mxkp,Stuff you access a lot or that you accessed most recently gets a spot in the door shelves for quick and easy access. Infrequently used data gets packed into the back of the fridge or in the veggie bins.,2,0,0,False,False,False,1642477493.0
s6mxkp,ht67cp1,t3_s6mxkp,"Shipping and transport is a more fitting analogy. Cargo ships are slow and cumbersome, but they can move huge volumes of goods across the world. Upon arrival though, goods from one ship needs to be distributed across many cities. It'd be hugely disadvantageous to use ships for distribution from city to city because of how big and slow they are.

Therefore, that task would fall to the next in the shipping volume hierarchy: 18 wheeler trucks. They may have 1/10,000th the volume of a ship, but they're much more nimble. Highways are accessible to them, allowing them to go between cities.

Once the goods arrive at a warehouse or a shipping facility at some city, even smaller trucks or vans are required for delivering goods to individuals because imagine waiting for an 18 wheeler to navigate through your neighborhood.

At each level in the hierarchy, you're sacrificing volume to be more nimble and accessible, which is analogous to bandwidth vs latency. L1 cache has the lowest bandwidth but also the lowest latency. Which means it's capable of moving small pieces of data to registers, and doesn't make the CPU wait very long (a few clock cycles). Whereas the L3 has the highest bandwidth and the highest latency, probably dozens of clock cycles. It's good at loading in big chunks of data from the RAM, and parts of it to L2 but that operation takes say dozens of clock cycles.

The point of having this hierarchy at all is that RAM is actually very slow compared to CPUs. CPUs have hit GHz a bit over 2 decades ago, which means that some operations can be completed in 1ns or less. However, RAM requires many nanoseconds to be read from and written to even today, and if the CPU waited for it all the time then it'd be irrelevant to have faster clock speeds. The cache hierarchy is an attempt to bypass this limitation of RAM. Since L3 is bigger than L2 and L2 is bigger than L1, you can think of L3 as basically a slice of the contents in RAM, L2 as a slice of L3, and L1 as a slice of L2. This arrangement basically arms the CPU with data that a running program might need in the near future so that you don't incur the penalty of waiting on the RAM for every little memory access, only every now and then. This only works because RAM is read in parallel and that it has the bandwidth to feed the L3 cache, but not the latency to feed the CPU's registers.

This arrangement also causes things like data accesses in arrays to be faster than structures like linked lists because in an array, data elements are directly adjacent to one another, which provides greater probabilities that the subsequent elements are already loaded in the cache. Whereas with linked lists, this node might be in the cache, but the other nodes may be randomly scattered throughout memory, and each node access would then require waiting on the RAM",2,0,0,False,False,False,1642511324.0
s67l1c,ht27k3z,t3_s67l1c,"Nice explanation!

>Perhaps  you could comment on the ones that I missed but you know about.

SVD can be used to compress the wavefunction |Ψ⟩ of a quantum mechanical system expressed in the [Matrix Product State](https://en.wikipedia.org/wiki/Matrix_product_state) formalism (or tensor networks, more generally). Loosely speaking, SVD is used to [split](https://en.wikipedia.org/wiki/Schmidt_decomposition) a system into two parts, say U and V, which gives |Ψ⟩ = |U⟩S|V\^T⟩. Here the singular values in S encode the quantum entanglement between U and V.

The compression is done by discarding small (< 1e-10, say) singular values in S (together with corresponding columns/rows in |U⟩ and |V\^T⟩). This effectively removes unimportant entanglement information from |Ψ⟩. [Repeating the procedure](https://arxiv.org/abs/1008.3477) on all possible partitions of the system, creates a low-rank approximation of |Ψ⟩ which can be accurate under a few (but important) circumstances.

Since the data required to describe a quantum systems of N particles would otherwise scale exponentially (e.g. 2\^N for spin-1/2), this SVD procedure is key to pushing the limit when simulating large quantum systems on classical computers.",5,0,0,False,False,False,1642440487.0
s67l1c,ht39gek,t3_s67l1c,I had an exam today and was looking for SVD for 1h . What were the odds that you would publish this 2 and a half hour after it ? Lol,3,0,0,False,False,False,1642454928.0
s67l1c,ht27mob,t1_ht27k3z,"**[Schmidt decomposition](https://en.wikipedia.org/wiki/Schmidt_decomposition)** 
 
 >In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces. It has numerous applications in quantum information theory, for example in entanglement characterization and in state purification, and plasticity.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",2,0,0,False,False,False,1642440514.0
s67l1c,ht2o2eu,t1_ht27k3z,Works well with image compression too.,2,0,0,False,False,False,1642446716.0
s67l1c,ht2lwrr,t1_ht27k3z,Thanks for sharing! The compression part you mention is similar to what one would do to pick principal components that corresponds to the largest singular values when performing PCA for data analysis/visualization of higher dimensional data.,1,0,0,False,False,True,1642445898.0
s67l1c,ht3bysv,t1_ht39gek,😮👀 oh damn. If only I knew.,1,0,0,False,False,True,1642455899.0
s67l1c,ht2zxrs,t1_ht27mob,Good bot !,1,0,0,False,False,True,1642451241.0
s67l1c,ht2zvz6,t1_ht2o2eu,"Yes, exactly. 💯",1,0,0,False,False,True,1642451221.0
s5vvf5,ht0c5br,t3_s5vvf5,When are you going to start?,5,0,0,False,False,False,1642401213.0
s5vvf5,hwbpy4c,t3_s5vvf5,Is it late to join now?,2,0,0,False,False,False,1644466835.0
s5vvf5,hwgl5es,t3_s5vvf5,Can I still join you all?,2,0,0,False,False,False,1644550993.0
s5vvf5,ht6l1no,t3_s5vvf5,"What will be the medium for communications? Discord group? Slack? Two weeks of the course is equivalent to how many lectures?

Personally, I will be self-studying Design and Analysis of Algorithms on the first semester of the year. My goal is to be able to tackle advanced topics, in particular integer linear programming, by the second semester. However, I didn't plan to follow this MIT course because, as far as I'm aware, it's a second course in algorithms that requires some background I still lack. I just selected some lectures that I found useful. Are you aware that there is a previous algorithms course (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/) to the one you pointed?

Anyways, I will be studying the CLRS book, which is used in the course, but I will start with earlier chapters that are not covered (in fact, they are assumed as known by the student...). Hence, I'm not sure if I'd be able to keep pace with you. Depending on the dynamics of the community, it'd be a pleasure to be part of your endeavor.",1,0,0,False,False,False,1642517764.0
s5vvf5,htfui99,t3_s5vvf5,"**NOTE** As seen [here](https://imgur.com/a/hHMmfjc), Someone sent a wrong email address. If it is you, Please submit another form.",1,0,0,False,False,True,1642671774.0
s5vvf5,hv1ayof,t3_s5vvf5,signed up - hope I'm not too late!,1,0,0,False,False,False,1643658894.0
s5vvf5,hylh099,t3_s5vvf5,"Had a question related to your group that I posted here: [https://www.reddit.com/r/computerscience/comments/su6m6s/why\_doesnt\_karatsuba\_multiplication\_break\_numbers/](https://www.reddit.com/r/computerscience/comments/su6m6s/why_doesnt_karatsuba_multiplication_break_numbers/)

 Why doesn't Karatsuba multiplication break numbers into word size blocks?

So under the WORD RAM model of computation the word size w is at least log of the input size and arithmetic operations on words takes constant time.  
So rather than dividing an n bit number into bits, why not divide the number into n/w bytes?",1,0,0,False,False,False,1645930077.0
s5vvf5,ht0c9nu,t1_ht0c5br,An email will be sent within 4 days. Thank you for the note.,6,0,0,False,False,True,1642401293.0
s5vvf5,hwcj8nr,t1_hwbpy4c,Not late at all!,1,0,0,False,False,True,1644485677.0
s5vvf5,hwgmtj7,t1_hwgl5es,"Absolutely, Just join through the website.",1,0,0,False,False,True,1644551777.0
s5vvf5,ht72ncc,t1_ht6l1no,"> What will be the medium for communications

Discord and maybe Zoom.

> Two weeks of the course is equivalent to how many lectures?

You can see the course's schedule through its link on my post.

> Are you aware that there is a previous algorithms course

Yes I am. Thank you for the note.

> it's a second course in algorithms that requires some background I still lack

You might watch the lectures even if your comprehension isn't 100%, Then pick-up an easier reference and exercises related to the lecture you didn't fully comprehend.

The main requirement is flexibility and capability to interact and learn from others.",1,0,0,False,False,True,1642524596.0
s5vvf5,hv3971h,t1_hv1ayof,No you are not,1,0,0,False,False,True,1643688300.0
s694uk,ht2w0ml,t3_s694uk,">     I've read that bayesian neural networks can mitigate this problem

Do you base it on some paper? My understanding of BNNs is that they are used for some kind of regularizations to prevent overfitting. If I understood your problem correctly then this problem while close its still not the same as your problem and ill explain.

In most methods, we assume our data came from some probability distribution and we wish to ""say smart things"" about said distribution from only those samples.

Overfitting essentially means the algorithm does not understand the overall distribution from the samples, and think they came from a much more specified distribution. This problem is often very hard to deal with and some sophisticated methods are used. 

But on the other hand, the ability to understand *different* distributions through the data is something else. For example, if i teach my network to classify animals but gives it only dogs, what it does when shown cat is unpredictable - it might classify as animal because it look somewhat similar to the dog (the distribution we sampled the data from) or it might think its not from the distribution and classify as not an animal. 

Those two problems while similar are different, thats from my understanding of things.",1,0,0,False,False,False,1642449716.0
s694uk,ht41jc4,t1_ht2w0ml,"Thanks for the response, hadn't head about overfitting before and while its not what I'm looking at it certainly has similarities. 

The problem I'm looking at is creation of false positives due to the system not understanding what it is seeing. Going back to the reading that suggested that BBNs could assist with that (The Alignment Problem by Brain Christian), the idea was that an ensemble of modals using Bayesian uncertainty would likely disagree with each other when faced with anything far from the data with which they were trained. 

We can use this degree of consensus or lack of consensus to indicate something about how comfortable we should feel accepting the models guess. We can represent uncertainty, in other words, as *dissent*.

This would be particular useful if a false positive could have dire consequences or in a situation were the available training data was limited or couldn't possibly take into account the complexity of the situation where the AI would be operating. 

Thanks again for the response!",1,0,0,False,False,True,1642466483.0
s694uk,ht5f01k,t1_ht41jc4,"Using an *ensamble* of bnns is actually an interesting thought, i havent considered it.

In general without actually using things its very hard to accuratly guess if something would work in deep learning algorithms, so I wouldnt put all of my eggs on it.

Your problem essentially means that using one NN would lead it to have a distribution over the results that has very low varience but could be wrong. What Brain is saying (if i inderstood correctly) is that the distribution itself changes greatly between each training proceedure and while each module seperatly remains with low varience the addition of all modules will get high varience thus indicate that the net does not understand the input.

Thats to my understanding as i havent been working with bnns much.",1,0,0,False,False,False,1642490430.0
s694uk,ht7hgw9,t1_ht5f01k,Yeah that's basically the case I believe.  I appreciate the chance to bounce ideas back and forth. Helped it make sense in my head,1,0,0,False,False,True,1642530024.0
s67zoq,ht20ppf,t3_s67zoq,"Without the ""optimization"" the code would not be correct.

To prove lets look at the simple case of n=2 and p=0.5

There is now two rolls of the dice, one for index(1,2) and the other for (2,1)

If one of the rolls win (with the probability of 0.5) there would be an edge between the vertecies.

The probability of at least one wins is one less the probability of both lose which is (because the rolls are independent) the multiplication between each p, which ends up in 0.75 for an edge unlike the promised 0.5.

You can make the code clean in the ""optimized method"" or to allow the roll to make true values in the array false, but this would implicitally only consider half the rolls in double the runtime.",2,0,0,False,False,False,1642437870.0
s67zoq,ht2hrik,t3_s67zoq,"I think the algorithm would be correct if it also explicitly the edge to false when curr_p > p. In that case the existence of an edge between each pair of vertices would still be set twice but only the latter time would matter.

Without that, the second time those same row and column indices are hit can only *increase* the probability of there being an edge, so it's not the same as evaluating the probability only once for each row and column index pair.

So, basically what u/MyCreativeAltName said.",2,0,0,False,False,False,1642444331.0
s67zoq,ht2jd6n,t1_ht20ppf,"Oh, right. I missed the part where I need to set a true to false if the second roll is false. Thank you so much.",1,0,0,False,False,True,1642444938.0
s67zoq,ht2ji6p,t1_ht2hrik,Thank you so much. This makes so much sense.,1,0,0,False,False,True,1642444990.0
s5r8t0,hszhzqw,t3_s5r8t0,Youtube recommendations mainly. MIT Tech Review is pretty great too to keep track.,7,0,0,False,False,False,1642386130.0
s5r8t0,hszt0jk,t3_s5r8t0,"Hacker News, KDnuggets, IEEE Spectrum, Communications of the ACM, Quanta",3,0,0,False,False,False,1642390908.0
s5r8t0,ht08fh0,t3_s5r8t0,"ACM's digital library: https://dl.acm.org/

IEEE's digital library: https://ieeexplore.ieee.org/Xplore/guesthome.jsp",2,0,0,False,False,False,1642398925.0
s5r8t0,ht0bihr,t3_s5r8t0,"Hey there!  
This is the  weekly tech news teller me and my friend created you can see the main sources of information for this newsteller that are used on the main page listed. ( these webpages themselves are very informational for news and etc.)  
[https://techteller.org/](https://techteller.org/)  


Hope it will help.  
Thanks.",1,0,0,False,False,False,1642400803.0
s5r8t0,hszjax1,t1_hszhzqw,"Domo Origato, yeah youtube is great",2,0,0,False,False,True,1642386701.0
s5i47n,hsxt11m,t3_s5i47n,"It’s not a stupid question. If you had a component like a transistor but which could output 10 distinct voltage levels — let’s call it a TENsistor — and that output would in turn serve as an input to another tensistor, etc.. you could represent 10^8 states with 8 signal traces.

It wouldn’t be 8 “bits”, but maybe we could call them 8 “dits”. An 8 dit system would then be able to process a 10^8 value (100M) in a single instruction (I’m oversimplifying here but the principle holds). A binary computer would need a 27-bit architecture to achieve the same.

(Again, really oversimplifying here)

So if you had tensistors and were able to lay them down on silicon as densely as in a modern CPU; and if your CPU had the same number of tensistors as the binary CPU had transistors, then yes. It would be much faster.

Of course, you could absolutely design a ‘tensistor’ out of binary logic gates, but it would end up being orders of magnitude larger and more complex than the damn clever NAND based logic in a modern CPU, and so your new architecture would end up in practice being way slower that way.

And going the analog route, where you actually distinguish different voltage levels inside every component — I think we’d be looking at another couple orders of magnitude in size and complexity to do that.",58,0,0,False,False,False,1642361852.0
s5i47n,hszi7yc,t3_s5i47n,"Take a look at [settling time](https://en.wikipedia.org/wiki/Settling_time). When a wire changes voltage levels it doesn't immediately change to the new voltage; it overshoots and wiggles a bit before it settles down to the intended value. In a binary system there is a lot of room for error, since the wire only needs to be above some voltage or below some voltage. When the wire changes values, it will be high or low pretty quickly, and you don't need to wait long for it to settle into a value that's close enough.

If there are 10 possible values, the voltage will need to be much more precise, so the system will need to wait longer for it to settle on the precise level. This will make all the electronics much slower. I'm a software guy, so I can't say precisely, but an electrical engineer could probably quantify exactly how much slower things would be.",7,0,0,False,False,False,1642386225.0
s5i47n,ht05206,t3_s5i47n,"Other posters have answered the question well. Just for fun I'll mention that storage (as opposed to logic) actually does use higher number of states. Many SSDs use 2 or 3 bits per cell (4 or 8 distinct charges). It's really tricky to get right and slower, but it's much more space efficient.",6,0,0,False,False,False,1642396993.0
s5i47n,hsy6nwz,t3_s5i47n,"It's hard to represent 10 different states. These states could sort of blur together, and distinction between them would be a lot harder. 1/0 (on/off) makes it so that there is no ambiguity- it is easy for a computer to tell the difference.",3,0,0,False,False,False,1642366982.0
s5i47n,hsxsodo,t3_s5i47n,"It's not that it's impossible. It is very possible.

In fact, the first programmable, electronic, general-purpose digital computer - famous [ENIAC](https://en.wikipedia.org/wiki/ENIAC) - was operating on decimal numbers!

But we don't see such machines anymore. Think, why?",7,0,0,False,False,False,1642361722.0
s5i47n,hsxkc7q,t3_s5i47n,I don’t get your point. Why do you think it would be faster?,1,1,0,False,False,False,1642358627.0
s5i47n,ht06llw,t3_s5i47n,"No, I would expect about 3 times as fast, since with 3 bits you already have 2^3 = 8 possible states, which would be as good as having a base 8 computer. 
Also, during a calculation you are not interested in the base 10 representation. So it's not that useful.
Additionally, computers also do computations on for example pointers, so not all computations are on (base 10) numbers.
Short answer: no, I don't think it will be a lot faster (if it's even faster at all)",1,0,0,False,False,False,1642397874.0
s5i47n,hsyeh0b,t1_hsxt11m,"I would like to add that the speedup using dits instead of bit is ""just"" a constant factor. The possible representable states grow faster with the exponent than with the base (for large enought numbers). Meaning it is more important how many dits/bits you have than how many states a single of them can hold.

As long as you can build the tensistors just n times larger you would speed things up. If you instead need something like n^2 or even 2^n times the space, tensistors would be worse than transistors (regarding computing capacity per space).

So it is just more efficient to simple increase the amount of bits than to mess around with dits.

PS: I like the word dits 😀",22,0,0,False,False,False,1642370034.0
s5i47n,ht062in,t1_hsxsodo,We have also made [ternary computers.](https://en.wikipedia.org/wiki/Ternary_computer),3,0,0,False,False,False,1642397574.0
s5i47n,ht0iom8,t1_hsxsodo,Is that because of potential signal inaccuracy? I genuinely don't know why.,2,0,0,False,False,False,1642405656.0
s5i47n,hsyes8a,t1_hsyeh0b,"Yep, good point!",5,0,0,False,False,False,1642370154.0
s5tfpw,ht01pww,t3_s5tfpw,"What's your question exactly? Are you saying you want to play a human to be able to ""play"" the Neural Net instead of the actual game?",1,0,0,False,False,False,1642395181.0
s5tfpw,ht0539y,t3_s5tfpw,"That... would be kind of scary. Imagine you just train a NN on the top video games, somehow pass in levers to randomize level design, graphics, and gameplay according to what's available in different games, then you get unlimited NN-powered video game experiences, where you only need to polish and do voice-over/direction. I'm really curious now, is this viable?",1,0,0,False,False,False,1642397012.0
s5tfpw,ht037jq,t1_ht01pww,"I'm wondering if anyone's trained a neural network to simulate/predict the output that a regular video game would give, so you only need to run the NN instead of the full game. I think you understand correctly. The net would be outputting its best guess of what the full game would have returned.",1,0,0,False,False,True,1642395987.0
s5tfpw,ht05x5t,t1_ht0539y,I think TTS tech is looking really promising. If it keeps improving at the current pace we might not have much need for voice actors in 10-ish years.,1,0,0,False,False,True,1642397489.0
s5tfpw,ht07h5m,t1_ht037jq,So you essentially want an ai that can…. Play games? It would be interesting in theory I suppose but training an ai on so many different games that it could theoretically predict the outcome of a new game would be a very complex task I feel,1,0,0,False,False,False,1642398373.0
s5tfpw,ht07s8r,t1_ht037jq,"So I think you might have a slight misunderstanding of how Neural Networks function. Well to begin with, the type of network you'd use to build a reinforcement learning model (which you'd train to learn something like chess) is vastly different from one which would generate pixel data (which in your scenario could be a model which predicts the next frame of a game based on input)

For the first type, there's almost never any visual data being passed to the network. The input is in the form of an array (data type could vary but let's just assume this for now) which summarises the current state of the game; and the output could be something like a simple integer which would dictate the model's next move. There's then a reward function which evaluates how good or bad the output move is, which can then be used to tweak the model's parameters for the next iteration of the model's training. This sort of net could definitely be played against in a video game, but not really in the manner you're describing. Think of it as being able to train an AI adversary within the game, rather than creating the entire game itself. The entire game engine, visual assets etc. would still be required to interpret the model's output and put them in the game. This sort of thing has already been done with chess and a few other games. Now one additional thing since you've mentioned Doom Eternal; the more complex the mechanics of the game, the harder it's going to be to setup a good enough model to play it. Afaik we're definitely not at a point where we can create a model to be a player of a game like Doom Eternal.

Now for the second type of model, one where you'd predict the next frame of a game based on a starting frame and any input. This sort of model is theoretically possible today, but it's going to be almost impossible to meaningfully train it to function as a game. This is more in the realm of procedurally generating frames. You wouldn't be able to program any rules/mechanics/levels of any sort into the game to begin with, the network will merely come up with what it thinks should come next based on what it's seen before during training. There's also the problem that this sort of process might work for a few frames but will soon fall into chaos; this is because after a few frames, the original output frames will then need to serve as input, and the general entropy from the unpredictability of the output will be magnified exponentially. Also as a bit of a footnote, a game like Doom Eternal is not going to be feasible for this sort of approach; all of the problems I've highlighted will be exacerbated proportional to the complexity of a single frame of the game.",1,0,0,False,False,False,1642398553.0
s5tfpw,ht2x92f,t1_ht07h5m,"I think OP meant that the neural network would act in stead of the game, and would basically attempt to react to the player's inputs as a game would, without actually going through the actual exact algorithms of the game. So, rather than replacing the human player with an AI in the game/player combination, you'd replace the game by having some kind of a neural network that produces a stochastically generated game-like result and behaviour, e.g. by learning that games tend to pan a view to the left when the player presses the left turn key.

Or at least that's what I got from the comment you replied to.

So maybe something like https://www.youtube.com/watch?v=atcKO15YVD8 but with full game behaviour, including some kind of graphics generation etc.?",1,0,0,False,False,False,1642450196.0
s57kne,hsy9ubx,t3_s57kne,[removed],-8,0,1,False,False,False,1642368224.0
s57kne,hsxnqgk,t3_s57kne,"This is so cool, man",1,0,0,False,False,False,1642359880.0
s57kne,hsym6ff,t1_hsy9ubx,[removed],5,0,0,False,False,False,1642373065.0
s57kne,hsza1zr,t1_hsy9ubx,"Thanks for posting to /r/computerscience! Unfortunately, your submission has been removed for the following reason(s):

* **Rule 2:** Please keep posts and comments civil.



If you feel like your post was removed in error, please [message the moderators](https://reddit.com/message/compose?to=/r/computerscience).",2,0,0,True,False,False,1642382717.0
s4k68d,hssfla4,t3_s4k68d,"Here are two college textbooks that I highly recommend. They're college textbooks, so it means they're very expensive, but it also means there's lot of reasonably priced used copies out there.

Introduction to Algorithms, i.e. ""the CLRS big book of algorithms""  
[https://mitpress.mit.edu/books/introduction-algorithms-third-edition](https://mitpress.mit.edu/books/introduction-algorithms-third-edition)

Artificial Intelligence: A Modern Approach, i.e. ""AIMA"" or ""Russell and Norvig""  
[http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)  


Books about programming language go out of date quickly. These books focus on the procedures themselves (independent of any particular language), so the material stays relevant for a long time. Some of the material in these books is 70+ years old and still studied because they remain relevant today, even though computers and programming languages have changes dramatically since then.",21,0,0,False,False,False,1642268301.0
s4k68d,hss4syd,t3_s4k68d,"It’s not a book but I wish someone had encouraged me to read research papers and technical documents sooner.

RFCs are often networking focused but a lot of what we do in CS has direct applications there and can be abstracted/generalized with graph theory. There are algorithms, data structures, topological suggestions, and even some coding best practices.

It’s great to go to those documents because I think people come into computer science with a lot of preconceived notions about how logic and computer systems work. It helps me to see what the actual thought leaders saw as the pressing industry issues, and helped me to see the standard for state of the art solutions.

Stuff like that fleshes out the fundamental understandings your books will teach you - while giving you a sense of the breadth of the field.",23,0,0,False,False,False,1642264103.0
s4k68d,hsslabi,t3_s4k68d,I think it is a good idea to continue reading SICP. If you like math you can try reading an intro discrete math book.,17,0,0,False,False,False,1642270500.0
s4k68d,hss4zf0,t3_s4k68d,https://www.reddit.com/r/computerscience/comments/s14xir/comment/hs6fo0x/?utm_source=share&utm_medium=web2x&context=3,6,0,0,False,False,False,1642264174.0
s4k68d,hstp77k,t3_s4k68d,"Go through all the books/courses in this website: [https://teachyourselfcs.com/](https://teachyourselfcs.com/)

I went through most of them during uni, and can attest that they are all fantastic textbooks.",5,0,0,False,False,False,1642286442.0
s4k68d,hsuzxom,t3_s4k68d,"1. The Nature of Computation by Cristopher Moore and Stephan Mertens
2. Concrete Mathematics by Don Knuth et al
3. Algorithms a Creative Approach by Udi Manber

But please finish SICP first. Makes everything look a lot easier and more interesting.",4,0,0,False,False,False,1642306524.0
s4k68d,hssf8h6,t3_s4k68d,"Hunt, Thomas. The Pragmatic Programmer

Robert Martin. Clean Code

Robert Martin. Clean Architecture

Martin Fowler. Refactoring

These will get you past the stage of hacking together something that works, to the point where you at least know what well-designed code is supposed to look like.

After that, you're ready for

Eric Evans. Domain Driven Design

Kent Beck. Test Driven Design

Gang of Four. Design Patterns

Joshua Kerievsky. Refactoring to Patterns",30,0,1,False,False,False,1642268166.0
s4k68d,hstqyxf,t3_s4k68d,Code by Petzold. It was required reading where I went to college but I regret not reading it long before that. It's not a long book at all but should absolutely be required reading everywhere.,3,0,0,False,False,False,1642287165.0
s4k68d,hsvdm6k,t3_s4k68d,"https://doc.lagout.org/Others/Data%20Structures/Data%20Structures%20and%20Network%20Algorithms%20%5BTarjan%201987-01-01%5D.pdf

This was used as the text for a graph algorithm class I took in graduate school I think it feels like a really intuitive and interesting way to describe the subject. Personally, I really like the way Tarjin writes and introduces algorithms. This doesn’t teach you C or Java, but something much more fundamental. Give it a look if you want a bit of a challenge!

Tarjin for me is the Knuth of graph algorithms.",3,0,0,False,False,False,1642314291.0
s4k68d,hssilmj,t3_s4k68d,Cracking the Coding Interview. Recommended to me by my professor and really helps you prepare for interviews and real world problems!,6,0,0,False,False,False,1642269469.0
s4k68d,hsth7dx,t3_s4k68d,Pragmatic programmer.,2,0,0,False,False,False,1642283167.0
s4k68d,hssx7py,t3_s4k68d,studying UML would be helpful since different types of diagrams can be very helpful in planning and communicating software systems among technical and non-technical people alike,1,0,0,False,False,False,1642275089.0
s4k68d,hst5q0v,t1_hss4syd,What is a RFC?,3,0,0,False,False,False,1642278464.0
s4k68d,hsvxybf,t1_hss4syd,"I'm in a different field and I agree with the research papers suggestion. 

One surefire way to do it is to look at textbook authors' and contributors' journal articles. Search Google Scholar for their names and download some that are free. This way you don't easily stumble accidentally on journal articles that have bad writing/wrong methods. Same could work for referenced articles on Wikipedia pages: search for more work by the same author(s).

Once you have some downloaded, read a few paragraphs from each and see how much you understand. If it's 60% or more, awesome, read that one. If it's less, discard and grab another. You could try to look up terms but usually there are whole complex ideas and esoteric methods behind a bunch of the terms. Also the writing could just be an overly esoteric style. Find an article first that conveys meanings well to you before you get frustrated trying to look terms up.",1,0,0,False,False,False,1642329617.0
s4k68d,hswscx2,t1_hsslabi,"There are a buncha people who say SICP as a whole is outdated. I have an easy time understanding and disagreeing with that perspective. However, I have also seen some claim that SICP chapters 1, 2, and 3 are liquid gold, while chapters 4 and 5 are lacking comparatively. How do chapters 4 and 5 compare to the earlier chapters?",1,0,0,False,False,True,1642347684.0
s4k68d,hstlib0,t1_hssf8h6,"I should point out that literally *none* of these books are computer science books. They are **software development/engineering** books. Very important field, and where I spend most of my time even though my job title has compsci in it, but it's an important distinction for the newbie to recognise that these are not compsci.
 
Computer science books would be things like
 
* any 1 of several algorithms & data structures books  
* The Art of Computer Programming  
* Types and Programming Languages  
* Compilers: Principles, Techniques, and Tools  
* Introduction to Automata Theory, Languages, And Computation  
* Computational Complexity: A Modern Approach
 
However the 2 most important OP already has, K&R + SICP.",22,0,1,False,False,False,1642284927.0
s4k68d,hst02yz,t1_hssf8h6,"As a collector of CS books, thank you, this just added a few to my to-buy list",1,0,0,False,False,False,1642276214.0
s4k68d,hsudst9,t1_hssx7py,I do not know a single person that used UML more than once a year after getting their degree.,1,0,0,False,False,False,1642296743.0
s4k68d,hst6mka,t1_hst5q0v,"An RFC is a “Request For Comment”, which is like a formal statement of a proposed system, protocol, algorithm or otherwise solution to a technical problem. 

It is intended to be an opportunity for a community of engineers to gather around a proposed solution and improve it, or otherwise reference the agreed standards of a given solution.

The most famous set of RFCs are by the IETF (Internet Engineering Task Force): https://www.ietf.org/standards/

Read through some - start with something you recognize. Gateways are a good starting point in my opinion, or RFC1918. I think you’ll find them accessible given a little familiarity.

The really exciting thing about computer science for me is this quote from Steve Jobs:

“Everything was made up by people that were no smarter than you”",7,0,0,False,False,False,1642278827.0
s4k68d,hsu4iyh,t1_hstlib0,"Meh... I know this perspective and I think it's from the way how courses are labeled. They put SE on one side and everything else on the other. A bit like programming and mathematics in the 60s and 70s. This way of separation rly hurts my brain...

I can see connections between Uncle Bob's SOLID principles and Dijkstras thoughts in the humble programmer. I see the connection between engineering methods and HCI and to design and to psychology to constructivism to hylomorphism to Alexanders adaptive morphogenesis to digitalisation of analogue signals to approximation in mathematics. In what bucked shall I put these thoughts? SE or CS?

I rly prefer the perspective: Software engineering and software development are specialised fields in Computer Science that exist alongside other fields like algorithm or machine learning. Everything is connected and further more it's connected to domains that are clearly out of the field of computer science.",0,1,0,False,False,False,1642292816.0
s4k68d,hsv43n3,t1_hsudst9,"At certain levels of the planning process diagrams are definitely useful.  UX workflows, multi-system interactions, data flows, etc.  I find they help with business to technical specification as a bit of common ground between analysts and engineers.",2,0,0,False,False,False,1642308693.0
s4k68d,hu1xlz8,t1_hsudst9,hey ppl do you know Doctor Strange?,1,0,0,False,False,False,1643050307.0
s4k68d,hsvgp91,t1_hst6mka,I enjoy reading RFCs a LOT! I usually read them and then try to implement the technique discussed in the RFCs. I usually read networking RFCs,2,0,0,False,False,False,1642316353.0
s4k68d,hsu59s8,t1_hsu4iyh,"It's not a perspective, it's a reality. Software developers/engineers use computer science, but that doesn't mean that they aren't different things. Connections between things don't mean there is no difference between things - there is a connection between my finger and my wrist, but they are distinct. Software development/engineering is *not* a specialised field in computer science, it is an interdisciplinary field that includes (some of) computer science, and also includes a bunch of other things. This is objective fact, it's not something multiple people can have different correct opinions on.",6,0,0,False,False,False,1642293133.0
s4k68d,hsu9zcz,t1_hsu59s8,"Yes sure connections don't mean there are no differences. But that's not the point: You think of wrists and fingers. I think of limbs, wrist and fingers.

And an objective fact, the reality in relation to a human concept... Something like this doesn't exist. That's why I speak of perspective and don't tell anyone what is right or wrong. Every single human concept is just a conceptual model based on our personal mental model. If I ask you to draw me past and future down on a paper you may draw a line from left to right. Someone else in the world will draw a line from right to left and another from top to bottom. Which conceptual model is right?",1,0,0,False,False,False,1642295113.0
s4k68d,hsubyjz,t1_hsu9zcz,"If you throw out the meanings of any words, you lose the ability to communicate. You can either speak English with the rest of us where the words computer science and software development mean computer science and software development, or you can speak whatever language you are speaking where they aren't different things - but you can't expect to be able to communicate with anyone if you run off and use your own definitions.",5,0,0,False,False,False,1642295964.0
s4k68d,hswk4ou,t1_hsubyjz,"Exactly! 😊 It's about the meaning! Very good.

""The spoken words are the signs of ideas in the soul and the written words are the signs of spoken words. Just as the written signs are not the same for all people, so the words are not the same for all people."" -Aristoteles, Peri hermeneias

Semiotik? Model theory? Stachowiak? 
Words are only sounds, without a human interpretation they are indeed meaningless. But here, in a discussion about what is computer science, we have to think in different levels of abstraction, a different granularity when looking at the details, the relation and the dynamics with which a concept is connected. And in a context like this, the subject of computer science, I think that one can expect more than a linear simplified form of classification to answer this question.

Does anyone object when someone asks software engineering questions here? Why are you working in a professional title but with a software engineering job? It is not simply black and white, even if that is easier and more convenient.

It is very natural and human to create taxonomies and this is the basis for what is ""true"" for us. In the end, however, it remains only a simplification of reality, a way for our human mind to grasp the potentially infinite complexity of reality, which always remains an incomplete representation. And because we see this image as true, things that say otherwise are false, an attack on our world order that makes us quite aggressive because we want to stand up for what we think is right. This leads us to exclude those who think differently, for example by talking about you and us and automatically implying that you are not part of us. Someone who says he has a scientific background knows that this kind of exclusion has troubled so many of the people we honour today, throughout their lives, and only because people insisted that their concepts were not the one of ""us"".

In the end, however, it remains what it is, a simplification, not a truth. And that's why I prefer the term perspective. And I am very glad that we are having this discussion at this level ""where"" I am from.",1,0,0,False,False,False,1642344015.0
s4k68d,hswkzp4,t1_hswk4ou,">Why are you working in a professional title but with a software engineering job
 
Because people who aren't in the computer science or software development fields don't understand the distinction, yet write the job titles for those fields. Same reason why programmers get asked to fix hardware.
 
This is not a philosophical question, it is a question of what the community of practice has defined as different things over decades of communication. Again, you can choose to run off and use your own definitions, but you self-exclude yourself from that community and generally make yourself a nuisance when you impose your incorrect definitions upon that community.",1,0,0,False,False,False,1642344425.0
s4k68d,hsx2q7g,t1_hswkzp4,"> Because people who aren't in the computer science or software development fields don't understand the distinction, yet write the job titles for those fields. Same reason why programmers get asked to fix hardware.

So we can agree that in general, in the non-professional environment, these issues are not differentiated. We are ""computer people"". But the perspective that I follow and that we also try to build our curriculum around and make first-year students understand what their computer science degree consists of is a variety of disciplines in the field of computer science. Even if we exclude SE from this consideration. With a computer science degree, will what you do simply be computer science? No, it is the specialisations that we have chosen. Visualistic, machine learning, digital signal processing, etc. are all directions in computer science that are so broad in themselves that we consider them as a separate field. To unite all this under the term computer science is just as inappropriate as asking a programmer to repair hardware. 

This is not only the case with CS, but also with SE, CE and all other categories that were defined in the 90s. This classification is the result of a knowledge-based education where I dont say its right or wrong. It treats computing as a meta-discipline a collection of disciplines having a central focus of computing. But even this classification now recognises that these disciplines are intertwined. In particular, SE and CS have a massive overlap in the theoretical part of ""application technology"", ""software development"" and ""systems infrastructure"". The ""community"" has been classifying CS for 10-15 years in such a way that almost all aspects of CS are relevant to SE. And rightly you will say that someone who has spent the same time focusing on more theoretical content will probably be more competent in this field than someone who has spent time in both theoretical and application.

And that is also the reason why I do not like this classification. It is simpler but loses important details that are relevant for example in a job profile. I prefer a classification based on competences. Here, knowledge, skills and disposition are considered in the context of computer science without a separation into disciplines. And we have already defined in 2017 which elements of disposition, cognitive skills and computing knowledge are useable for this classification. The resulting profiles are much more complex but much better suited to describe people or activities.",1,0,0,False,False,False,1642351895.0
s4k68d,hsx4djw,t1_hsx2q7g,">we can agree that in general, in the non-professional environment, these issues are not differentiated
 
Someone who isn't a chemist can't easily differentiate between a bottle full of H2O and a bottle full of H2O2. You going to drink both?
 
You are teaching your students incorrectly. A history major learns much more than just history. That doesn't make those other things 'history' just because they are taught to a student in a history major.
 
You can prefer whatever you like, but you are incorrect, and you are just going to confuse the people you speak with until they realise you are speaking a pretend language.",1,0,0,False,False,False,1642352548.0
s4h4a4,hsqz9o2,t3_s4h4a4,Just a guess but perhaps when the authors refer to ‘implementers of C’ they’re talking about people making compilers of the language?,61,0,0,False,False,False,1642241220.0
s4h4a4,hsqzjxr,t3_s4h4a4,"What I think they're saying is:
- implementer of C = someone who creates parts of the C language itself or an extension to it.
- C programmer = someone who uses the C language to develop for example an application.",39,0,0,False,False,False,1642241434.0
s4h4a4,hsqzp0a,t3_s4h4a4,"One makes C, other makes with C",27,0,0,False,False,False,1642241545.0
s4h4a4,hsr6kap,t3_s4h4a4,"Implementer most likely to be referred to someone who's creating the C libraries, improving compilers etc 
Programmer is the person using the language/consuming the libraries created by implementer to develop something say a game.",8,0,0,False,False,False,1642246688.0
s4h4a4,hsraj1i,t3_s4h4a4,"Implementer refers to one who makes the ""idea"" of C language into reality: writes the compiler, runtime library etc.

Programmer is one who writes programs in the C language and depends on the implementation of it (compiler, runtime etc.).

The ""idea"" of a language can be considered to exist as described in standards and definitions before there exists an implementation of it although some people start hacking on an implementation before there is a formal definition of it (syntax etc.)

Implementer also has to consider how the description of language will apply in real world in a real computer architecture.",6,0,0,False,False,False,1642249501.0
s4h4a4,hsranpy,t3_s4h4a4,The implementer is the person writing the compiler or the standard library,6,0,0,False,False,False,1642249590.0
s4h4a4,hss41c5,t3_s4h4a4,"An implementer of axes is a smith, a “programmer” of axes is a lumberjack.",5,0,0,False,False,False,1642263796.0
s4h4a4,hss80tu,t3_s4h4a4,Amogus,2,0,0,False,False,False,1642265355.0
s4h4a4,hsspkvj,t3_s4h4a4,"C implementer: makes compilers or implements the standard library for C

C programmer: people who program in C",2,0,0,False,False,False,1642272158.0
s4h4a4,ht111tk,t3_s4h4a4,"Implementers are engineers who write the C compiler from the official C spec. There are many versions: c99, c11, etc. A spec is a document that defines the standards of the language. The grammar, the rules, etc. These rules aren’t always strict and some of it is left to the implementers to decide. 

The C programmer consumes the implementers work and writes programs with the help of routines it provides in a shape of header files.",2,0,0,False,False,False,1642419983.0
s4h4a4,hssdcs5,t1_hsqz9o2,That would be my guess as well.,11,0,0,False,False,False,1642267435.0
s4h4a4,hsu0ix3,t1_hsqz9o2,Yeah makes sense,3,0,0,False,False,True,1642291124.0
s4h4a4,hsu0m67,t1_hsr6kap,Yeah,1,0,0,False,False,True,1642291162.0
s4h4a4,hsu0wc8,t1_hsraj1i,That’s helpful thanks,1,0,0,False,False,True,1642291278.0
s4h4a4,hsu0qd2,t1_hss41c5,That’s a good analogy,3,0,0,False,False,True,1642291211.0
s4h4a4,hta4oan,t1_ht111tk,Yeah makes sense and that is very helpful. I understand that an engineer can can create and implement versions of C that are tailor-made for the OS or compiler they are writing.,1,0,0,False,False,True,1642569187.0
s4frba,hsrqr59,t3_s4frba,"Yes, a vast majority of them",6,0,0,False,False,False,1642258169.0
s4frba,hsrtfwa,t3_s4frba,"I'm not sure I entirely agree with the assumption that any other well-known machine learning method outside of deep learning isn't ""effective"".

Deep learning is pretty much by definition based on artificial neural networks. So, 1/1 of the base methods used for deep learning are, in some sense, inspired by nature (at least originally).

However, for smaller amounts of data or for data with more linear relationships between variables, things like support vector machines may still be completely reasonable. If the phenomenon being modeled is linear or close to linear, a linear model might even well perform better than a nonlinear one such as an ANN, at least if you don't have a crapton of training data. (edit: and nearly all of these other methods are *not* inspired by nature.)

Also, as far as genetic algorithms go, are they actually one of the ""two most powerful"" machine learning methods? I may be a bit out of the loop, but genetic algorithms used to be one of those methods that sound appealing and may get a lot of hype but which often don't necessarily perform that well, or at least not necessarily better than other simpler and less computationally intense methods do.

I've seen some of that hype for GAs appearing again e.g. on Reddit in recent years but I'm not sure that attention is actually warranted. Again, I may be out of the loop, so perhaps GAs have actually enjoyed some kind of a significant rise in real performance along with neural networks recently. Since I'm not aware of that actually happening, though, I'm inclined to think of it as mostly hype, and I wouldn't count GAs as another data point towards effective machine learning being naturally inspired. I'd be happy to be shown wrong by someone in the actual know.

You might want to ask whether there are other methods beside ANNs that could be used for a layered approach that integrates some kind of feature learning, kind of similarly to deep learning. That's a good question, to which I don't know the answer. I'd just phrase it a bit differently because despite the (warranted) attention and hype, deep learning probably isn't the only kind of effective machine learning.",3,0,0,False,False,False,1642259383.0
s4frba,hssy8sf,t3_s4frba,"Maybe do some research into statistics? As a field they do a LOT of data analysis and predication that’s not based around neural networks or genetics algorithms. As far as I know anyway, I’m not a statistician so anyone actually versed in statistics feel free to correct what I’ve said.",2,0,0,False,False,False,1642275486.0
s4frba,hsqwvs9,t3_s4frba,"Right now DL looks nothing like whats done in nature.

DL are NN that use more then one hidden layer - if you assume automatically that all NN are ""copied from nature"" then of course all DL falls into this category.

Every single ML algorithm is basecally just math, you cant dismiss it for being this way too, math is an extremely powerfull tool.

On top of my head though, a lot of RL algorithm are not at all infulenced from nature, for example DQN.",3,0,0,False,False,False,1642239423.0
s4frba,hss047h,t3_s4frba,All algorithms are memorization and/or math. Machine learning leans towards statistics tho. You can’t compute anything (or have an intelligent system) without math.,1,0,0,False,False,False,1642262217.0
s4frba,hstpdfc,t3_s4frba,"Here's a simple method that gets used all the time, and has no connection to nature: linear regression",1,0,0,False,False,False,1642286514.0
s3mbvt,hsm7y5o,t3_s3mbvt,Computerphile,99,0,0,False,False,False,1642160395.0
s3mbvt,hsmpulv,t3_s3mbvt,Sebastian Lague,47,0,0,False,False,False,1642169963.0
s3mbvt,hsmzg23,t3_s3mbvt,Ben Eater all the way,70,0,0,False,False,False,1642173907.0
s3mbvt,hsm7ygp,t3_s3mbvt,Computerphile.,13,0,0,False,False,False,1642160400.0
s3mbvt,hsnfzr1,t3_s3mbvt,"[BackToBackSWE ](https://youtube.com/c/BackToBackSWE) is a good channel for algorithms

[Reducible ](https://youtube.com/c/Reducible) is another great channel similar to [3Blue1Brown](https://youtube.com/c/3blue1brown) but specifically for Computer Science. It explores CS topics in a much deeper way with excellent visuals and explanations. 3Blue1Brown does have a few videos on [Neural Networks.](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)",11,0,0,False,False,False,1642180318.0
s3mbvt,hsm6i08,t3_s3mbvt,Two minute papers and mCoding make great videos around those topics.,23,0,0,False,False,False,1642159378.0
s3mbvt,hslxrll,t3_s3mbvt,Michael Reeves is def someone who inspired me to take up computer science. But he’s more of a person to watch to show the fun/humorous side of computer coding.,18,0,0,False,False,False,1642152682.0
s3mbvt,hsndewz,t3_s3mbvt,"Algorithms: https://youtube.com/c/UndefinedBehavior

ML/AI (more fun to watch): https://youtube.com/c/CodeBullet

Applied Math (i.e. cryptography, neural networks, FFTs, etc): https://youtube.com/c/3blue1brown",19,0,0,False,False,False,1642179330.0
s3mbvt,hsn6wkt,t3_s3mbvt,Network Chuck!,10,0,0,False,False,False,1642176834.0
s3mbvt,hsom0kh,t3_s3mbvt,"Steven Skiena (channel name)

Computer Science (channel name)",3,0,0,False,False,False,1642196494.0
s3mbvt,hsp8imr,t3_s3mbvt,3Blue1Brown if you like a mix of math and cs,3,0,0,False,False,False,1642205852.0
s3mbvt,hso3j02,t3_s3mbvt,"Check out Sentdex. He has tons of videos about anything Python and has shiftet towards an ML/AI type of channel lately. He recently released a book where he wrote many neural network algorithms from scratch in Python/numpy to teach the concepts.

I haven't read it or seen the most recent videos because I'm more into computing systems, but it's a super cool channel.

Another cool one is Anthony Sottile's channel. He makes a lot of open source developer tools in Python. He streams and uploads to YouTube afterwards. He maintains stuff like pre-commit, pytest, flake8, tox and some other cool popular projects.",4,0,0,False,False,False,1642189388.0
s3mbvt,hsoa37z,t3_s3mbvt,"[Jacob Sorber](https://www.youtube.com/c/JacobSorber) is superb for any low level programming interests (embedded systems, C, etc) or just general data structure/algorithms.",4,0,0,False,False,False,1642191898.0
s3mbvt,hsnddeu,t3_s3mbvt,Jdh,2,0,0,False,False,False,1642179314.0
s3mbvt,hspb92g,t3_s3mbvt,"You might like some of these:

Fireship
Brad Traversy
Tom Scott
Computerphile
Neural Nine
Corey Schafer
Distrotube
Jabrils",2,0,0,False,False,False,1642207069.0
s3mbvt,hso5ty3,t3_s3mbvt,TwoMinutePapers,4,0,0,False,False,False,1642190258.0
s3mbvt,hsm8a82,t3_s3mbvt,"* [https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1\_aw](https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1_aw)
* [https://www.youtube.com/user/Computerphile](https://www.youtube.com/user/Computerphile)
* https://www.youtube.com/c/Fireship
* [https://www.youtube.com/c/mitocw/playlists](https://www.youtube.com/c/mitocw/playlists) (This one is a lot of different fields. Basically you can just do a search of ""MIT machine leaning"" or ""Stanford machine learning""",1,0,0,False,False,False,1642160626.0
s3mbvt,hsnoimy,t3_s3mbvt,"You may be interested in this awesome project too : [https://www.youtube.com/watch?v=rPkMoFJNcLA](https://www.youtube.com/watch?v=rPkMoFJNcLA)

It's not really machine learning but more about how can evolution make individuals better. It's my favorite ML related project",1,0,0,False,False,False,1642183543.0
s3mbvt,hsoptbx,t3_s3mbvt,Engineering Man,1,0,0,False,False,False,1642197994.0
s3mbvt,hsotyjz,t3_s3mbvt,"Some fun, interesting ones I didn't see mentioned (but these are pretty specific to their applications) are [ThinMatrix](https://www.youtube.com/c/ThinMatrix) and [BPS.space](https://www.youtube.com/channel/UCILl8ozWuxnFYXIe2svjHhg)",1,0,0,False,False,False,1642199673.0
s3mbvt,hsp45yl,t3_s3mbvt,"While his focus might be more on the philosophy side of things, Mark Jago is a trained computer scientist, and his videos are mostly about formal logic, with the occasional foray into CS. 

https://m.youtube.com/c/AtticPhilosophy/videos

Plus he’s got a cool aesthetic",1,0,0,False,False,False,1642203941.0
s3mbvt,hsp5tsf,t3_s3mbvt,"[Abdul Bari](https://youtube.com/channel/UCZCFT11CWBi3MHNlGf019nw). I've watched and rewatched his content many times. I wish I knew about him when I was taking my algos class in college. He was making many of his algorithms videos right around that time, too.",1,0,0,False,False,False,1642204658.0
s3mbvt,hspdoaa,t3_s3mbvt,"3Blue1Brown has some computer sciency stuff, has tons of math as well.",1,0,0,False,False,False,1642208156.0
s3mbvt,hspr097,t3_s3mbvt,RemindMe! 1 day,1,0,0,False,False,False,1642214260.0
s3mbvt,hspvrta,t3_s3mbvt,“Stat Quest with Josh Starmer” for Data Science and Machine Learning. You will not regret it.,1,0,0,False,False,False,1642216536.0
s3mbvt,hspz4ek,t3_s3mbvt,Fireship. His videos are relatively short and very info packed.,1,0,0,False,False,False,1642218161.0
s3mbvt,hsq3ipu,t3_s3mbvt,Tren Black,1,0,0,False,False,False,1642220367.0
s3mbvt,hsq6ge4,t3_s3mbvt,Joma tech,1,0,0,False,False,False,1642221927.0
s3mbvt,hsqcari,t3_s3mbvt,"Jordan Harrod makes interesting videos about machine learning and AI ethics, and about her experience as a PhD student",1,0,0,False,False,False,1642225101.0
s3mbvt,hsqewgt,t3_s3mbvt,"- Spanning Tree
- Ben Eater
- Computerphile
- 3Blue1Brown
- AlphaPhoenix",1,0,0,False,False,False,1642226629.0
s3mbvt,hsqgtj9,t3_s3mbvt,Carl Herold?,1,0,0,False,False,False,1642227827.0
s3mbvt,htwn83r,t3_s3mbvt,Forest Knight,1,0,0,False,False,False,1642960758.0
s3mbvt,hsnpvli,t1_hsmpulv,Sebastian Lague is a GOD,10,0,0,False,False,False,1642184056.0
s3mbvt,hsoo39j,t1_hsmpulv,"absolutely. his explanations are superb, calming voice, and incredible talent and work ethic. Most importantly, he does really cool shit that inspires me",7,0,0,False,False,False,1642197312.0
s3mbvt,hsp23rg,t1_hsmpulv,I just followed your recommendation and man… that was amazing. Definitely subscribing to this channel.,3,0,0,False,False,False,1642203066.0
s3mbvt,hsotvol,t1_hsmpulv,Seconding this one. Sebastian Lague is amazing. His projects are visually very interesting and tackle some fascinating challenges.,1,0,0,False,False,False,1642199640.0
s3mbvt,hsqpebs,t1_hsmpulv,"One of my absolute favorites. His coding adventure videos are all awesome. His voice is so soothing, I often put his videos on at night/in the morning.",1,0,0,False,False,False,1642233722.0
s3mbvt,hsnw3v7,t1_hsmzg23,interesting and hardcore,4,0,0,False,False,False,1642186505.0
s3mbvt,hsn3jg1,t1_hsmzg23,I second this,2,0,0,False,False,False,1642175530.0
s3mbvt,hsorp2v,t1_hsmzg23,That guy is a legend,1,0,0,False,False,False,1642198754.0
s3mbvt,hsn2aq1,t1_hslxrll,Humorous...ha. He puts alot of effort to make his videos funny,-4,0,0,False,False,False,1642175042.0
s3mbvt,hwueet8,t1_hso3j02,"A month late but holy hell yes, Sentdex is an OG. Really good speaking skills, interesting projects (his GTA V SDC series was very fun, I even forgot he did stream his agents [with some hilarious clips accumulating](https://www.twitch.tv/sentdex/clips?filter=clips&range=all) - absolutely a great recommendation.

I always keep putting him on hold and coming back to a massive amount of new cool topics he unfolds, gonna do that right now.",1,0,0,False,False,False,1644796637.0
s3mbvt,ht5nbq2,t1_hsoa37z,">Sebastian Lague

Hey, i never previously found or stumbled on this guy in my search for computer science/programming Youtube folks and creators. Checked him out - and what his content to be a real gem. Especially his data structures with C. I was looking to polish / refresh my data structure fundamentals and exactly in C ! What a finding, thanks!",2,0,0,False,False,False,1642496779.0
s3mbvt,hsqigft,t1_hsp5tsf,i heard his content is just basic and not for intermediate/advanced programmers,1,0,0,False,False,False,1642228894.0
s3mbvt,hspr20l,t1_hspr097,"I will be messaging you in 1 day on [**2022-01-16 02:37:40 UTC**](http://www.wolframalpha.com/input/?i=2022-01-16%2002:37:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/s3mbvt/interesting_computer_science_youtubers/hspr097/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fs3mbvt%2Finteresting_computer_science_youtubers%2Fhspr097%2F%5D%0A%0ARemindMe%21%202022-01-16%2002%3A37%3A40%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20s3mbvt)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1,0,0,False,False,False,1642214283.0
s3mbvt,hsp44d5,t1_hsp23rg,Absolute favourite of mine. Glad you enjoyed him too.,1,0,0,False,False,False,1642203922.0
s3mbvt,hsn3lup,t1_hsn3jg1,Happy cakes,1,0,0,False,False,False,1642175555.0
s3mbvt,hsqom73,t1_hsqigft,"Maybe his non-algo related content. I haven't watched any of it. But he has many videos walking through algorithms that I learned about in my more advanced algorithms course in college.

The first couple of algo videos are fundamentals but there are over 30 in his algorithms playlist.",1,0,0,False,False,False,1642233153.0
s36y35,hsjngrf,t3_s36y35,"Oh come on, you can't just show something this cool and *not* link a paper or article or at least the name of the tech.",34,0,0,False,False,False,1642111582.0
s36y35,hskm8ry,t3_s36y35,Motion sickness version.,7,0,0,False,False,False,1642125805.0
s36y35,hsjuxon,t1_hsjngrf,"I agree, so I searched “tennis ball tracking software” on Google…

https://en.m.wikipedia.org/wiki/Hawk-Eye#:~:text=Hawk%2DEye%20is%20a%20computer,path%20as%20a%20moving%20image.",12,0,0,False,False,False,1642114447.0
s36y35,hsp3lrw,t1_hsjngrf,">VcSv

Based on a masters thesis at Cambridge by Jack Davis, founder of [Loci.AI](https://Loci.AI). He has now raised VC funding to eventually build a photorealistic neural rendering engine.",1,0,0,False,False,True,1642203701.0
s36y35,hsjwfch,t1_hsjuxon,The article says this system works with six cameras which is way less impressive than the singular camera input mentioned in title.,5,0,0,False,False,False,1642115023.0
s36y35,hsjuzaz,t1_hsjuxon,"**[Hawk-Eye](https://en.m.wikipedia.org/wiki/Hawk-Eye#:~:text=Hawk-Eye is a computer,path as a moving image)** 
 
 >Hawk-Eye is a computer vision system used in numerous sports such as cricket, tennis, Gaelic football, badminton, hurling, rugby union, association football and volleyball, to visually track the trajectory of the ball and display a profile of its statistically most likely path as a moving image. The onscreen representation of the trajectory results is called Shot Spot. The Sony-owned Hawk-Eye system was developed in the United Kingdom by Paul Hawkins. The system was originally implemented in 2001 for television purposes in cricket.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",2,0,0,False,False,False,1642114464.0
s36y35,hsjuzj8,t1_hsjuxon,"Desktop version of /u/cassidysvacay's link: <https://en.wikipedia.org/wiki/Hawk-Eye>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",1,0,0,False,False,False,1642114467.0
s36y35,hskmll2,t1_hsjwfch,"Not at all lol.  It's pretty sweet.  The title is just flat out bullshit.  How are you going to tell where an object is in 3D space with only a single input at a single point?

https://en.wikipedia.org/wiki/Triangulation_(computer_vision)

EDIT: Hmm, this is actually an interesting thread:

https://movies.stackexchange.com/questions/572/why-do-you-need-6-points-to-define-a-location-in-3-dimensional-space",8,0,0,False,False,False,1642125956.0
s36y35,hslua78,t1_hskmll2,">How are you going to tell where an object is in 3D space with only a single input at a single point?

Yeah, that's what made me think ""this is really amazing"".",2,0,0,False,False,False,1642150004.0
s2qf5f,hsg84h1,t3_s2qf5f,Those who don't understand infinite loop definition won't understand this infinite loop definition.,139,0,0,False,False,False,1642050481.0
s2qf5f,hsg84ys,t3_s2qf5f,I think technically that's infinite recursion rather than an infinite loop,60,0,0,False,False,False,1642050488.0
s2qf5f,hsgaybh,t3_s2qf5f,Which book? I think there must be more cool examples like this...,11,0,0,False,False,False,1642052033.0
s2qf5f,hsg87eo,t3_s2qf5f,"Now do ""deadlock"".",8,0,0,False,False,False,1642050525.0
s2qf5f,hsgxb7k,t3_s2qf5f,"This is just a GOTO with a dead end; there's no motivation to go back to 'Inifinite loop' in the index text. The Batman version does a better job: [https://wiki.secretgeek.net/unbounded-recursion](https://wiki.secretgeek.net/unbounded-recursion).  
Edit - Fixed the link after a long weekend.",5,0,0,False,False,False,1642068344.0
s2qf5f,hshf2uz,t3_s2qf5f,This often happens with recursion also,4,0,0,False,False,False,1642080360.0
s2qf5f,hshw08a,t3_s2qf5f,"I dont get it, May someone explain pls? Am a newbie in CC",3,0,0,False,False,False,1642087872.0
s2qf5f,hsh6aiv,t3_s2qf5f,I stared at this trying to get it for so long I got a migraine and had to stop.,2,0,0,False,False,False,1642075186.0
s2qf5f,hshbx8b,t3_s2qf5f,"delimiter, 47",2,0,0,False,False,False,1642078670.0
s2qf5f,hshpi74,t3_s2qf5f,may i ask what book is it ?,2,0,0,False,False,False,1642085218.0
s2qf5f,hshvx56,t3_s2qf5f,"Also, why goto is bad",2,0,0,False,False,False,1642087839.0
s2qf5f,hsi0fez,t3_s2qf5f,Genius!,2,0,0,False,False,False,1642089566.0
s2qf5f,hsifx7n,t3_s2qf5f,Are they trying to make my brain crash?,2,0,0,False,False,False,1642095330.0
s2qf5f,hsj55wd,t3_s2qf5f,Lol!,1,0,0,False,False,False,1642104687.0
s2qf5f,hskvztp,t3_s2qf5f,What book is this?,1,0,0,False,False,False,1642130082.0
s2qf5f,httwly2,t3_s2qf5f,"this is an *infinite recursion* technically speaking, not an infinite loop",1,0,0,False,False,False,1642906785.0
s2qf5f,hshccuo,t1_hsg84h1,you may not understand it but you will feel it,36,0,0,False,False,False,1642078916.0
s2qf5f,hsgbtj1,t1_hsg84ys,Recursive infinite loop,23,0,0,False,False,False,1642052526.0
s2qf5f,hshomq2,t1_hsg84ys,Not really. It looks like a go-to.,7,0,0,False,False,False,1642084846.0
s2qf5f,hsgdoir,t1_hsg84ys,"Kind of a moot point, with the exception that recursive function calls will eventually cause a stack overflow.",12,0,0,False,False,False,1642053636.0
s2qf5f,hsl4gci,t1_hsg84ys,"There's no difference between infinite recursion and infinite iteration, though many programming languages will expose a difference in terms of a stack overflow. There are also those that don't.",3,0,0,False,False,False,1642134043.0
s2qf5f,hsgbp26,t1_hsgaybh,seems to be a Latex book .. \infty,16,0,0,False,False,False,1642052454.0
s2qf5f,hsgiyka,t1_hsgaybh,"This was pretty common in programming books in the 80s and 90s (when I last read physical programming books). I assume it's still pretty common today.

Google does something similar when your search for teens related to infinite loops.",5,0,0,False,False,False,1642057110.0
s2qf5f,hshchh7,t1_hsg87eo,you may need 2 other philosophers,6,0,0,False,False,False,1642078986.0
s2qf5f,hshm1z2,t1_hsgxb7k,page not found,6,0,0,False,False,False,1642083705.0
s2qf5f,hv2zgxz,t1_hsgxb7k,"Why was your weekend long, huh?",1,0,0,False,False,False,1643683830.0
s2qf5f,hsjdug3,t1_hshw08a,"When you search for “infinite loop” it tells you to look on page 252. So you search for “infinite loop” on the page and it tells you to go to page 252, where you search for “infinite loop” on the page and it tells you to go to page 252…. Thus an infinite loop of searching.",2,0,0,False,False,False,1642107970.0
s2qf5f,huoglo1,t1_hsgbtj1,"whats the difference, beginner here",1,0,0,False,False,False,1643429278.0
s2qf5f,hsge09f,t1_hsgdoir,Believe that can be avoided with tail end recursion,5,0,0,False,False,False,1642053838.0
s2qf5f,hsgpz6w,t1_hsgiyka,"> teens related to infinite loops

Uhhmm... .teens?",9,0,0,False,False,False,1642062367.0
s2qf5f,hv6pxnt,t1_hv2zgxz,Got stuck in some wierd iteration thing of course!,1,0,0,False,False,False,1643750292.0
s2qf5f,huomoh6,t1_huoglo1,Semantics. Recursion can create an infinite loop.,2,0,0,False,False,False,1643432470.0
s2qf5f,hsgqt5d,t1_hsgpz6w,"""terms"" sorry",5,0,0,False,False,False,1642063021.0
s3yrrt,ht3swrg,t3_s3yrrt,"The Concorde solver uses the cutting-plane method, iteratively solving linear programming relaxations of the TSP. The interface shows the solver's progress at the end of each major iteration of cutting planes by coloring the edges according to their current LP values.

The Graphical User Interface implements in addition to the optimal Concorde solver the following edge generating algorithms:

* Delaunay Triangulation
* Minimum Spanning Tree
* Various Nearest Neighbor Set generators

The Concorde interface can read and write graphs from and to files in various formats. (Note: Input graphs must have greater than 10 nodes.) Users may add, delete, and move a graph's vertices interactively. The interface consists of two display panes. One is used to show the graph, the other to print textual information about the graph, the algorithm's progress and results.",2,0,1,False,False,False,1642462780.0
s3yrrt,hsoh7zc,t3_s3yrrt,I think the introduction of [this paper](https://dl.acm.org/doi/pdf/10.1145/3071178.3071304) may be a good start,1,0,0,False,False,True,1642194642.0
s3yrrt,ht5am8a,t1_ht3swrg,"Thanks, I'll need to dig into the cutting plane method",2,0,0,False,False,True,1642487489.0
s3yrrt,hv1mse1,t1_ht5am8a,I found this video which explains that for the TSP. It's very well explained : https://youtu.be/8yajCJKezZQ,1,0,0,False,False,True,1643663336.0
s3yrrt,hvcawbv,t1_hv1mse1,[https://youtu.be/tChnXG6ulyE](https://youtu.be/tChnXG6ulyE) this one too,1,0,0,False,False,True,1643842299.0
s3gau6,hskrj9n,t3_s3gau6,Cocke and Minsky's construction of a universal Turing machine within a tag system can be found [here.](https://dl.acm.org/doi/10.1145/321203.321206) And  [here is](http://www.complex-systems.com/pdf/15-1-1.pdf) Matthew Cook's proof that a cyclic tag system can emulate Cocke and Minsky's tag system.,3,0,0,False,False,False,1642128098.0
s3241k,hsi09b6,t3_s3241k,"A few things, 

The power supply will simply give your computer power to work. It does not dictate the clock freq or anything like that.

What exactly a computer does is very complicated, but a simple abstraction is a von neumann architecture. This is not at all whats happening behind the hood but thats what the hardwere wants you to see.

The tldr of von neumann architecture is that for every instruction (the ones the compiler breaks the high level program into) the cpu goes through five steps:

Fetch, where the cpu gets the instruction. Decode is where the cpu understand the cmmand. Execute is where the computation happen. Memory is where you read from the memory (ram) to get data you need for the instruction and write-back is where the results are stored in the registers or memory.

Your code does not go to the ram, but your cpu reads and write to the ram in order of executing instructions.

Writing hardwere (and checking it) is not an easy task at all. This is done by splitting the actual hardwere into many separate parts that should work indepentently and combining it all.

An hardwere engineer wont usually deal with transistors but would write in what looks like code (hdl) and use basic building blocks to create the whole piece.

Hoped it answered your questions",2,0,0,False,False,False,1642089501.0
s3241k,hslendn,t3_s3241k,"> Also, who sits around and actually designs computer architecture, I feel like that has to be one of the most complicated jobs on the planet. You've got millions of transistors to manage controls? Or is it more abstract than that.

Computer scientists and engineers design computer architectures, if you're using the term *computer architecture* to mean *computer architecture* (aka instruction set architecture [ISA]). The design of actual computer hardware is done by computer and electronics engineers. These are the folks who figure out how a computer architecture is implemented to meet the performance, power, size, and cost goals. They'll make heavy use of abstraction and hierarchical design to manage the complexity in modern computers.",1,0,0,False,False,False,1642139424.0
s3241k,hsi0r86,t1_hsi09b6,"Does the clock frequency dictate the ""fetch"" component? 

Im just confused on how your computer knows to read the binary. Like when I type out my code and run it, how does the cpu know to read it. Is it always looking?",1,0,0,False,False,True,1642089692.0
s3241k,hsi3cv5,t1_hsi0r86,"The cpu has whats called a program counter (or pc). This is a register that stores the address of the the next command it should fetch. One instruction is called branch - this instruction would change your program counter and therefore change the code you will run (for example if statement might translate into conditional branch).

Essentially (its a bit more complicated, but thats the jist of it) when you run a program the pc would point to the location of the first instruction the compiler made from your code. When the pc is there, all it do is just do the usual 5 steps until theres nothing to run from your program and it does some sort of branch to where it was before it ran your program.

Its pretty complicated so what ive said is not 100% accurate, but its close enough.

Also, under von neumann architecture we assume that the clock define all of the stages, which means all five stages should take the same time to run.",1,0,0,False,False,False,1642090687.0
s33v2n,hsiprv0,t3_s33v2n,"As you said, the main Python interpreter is program called CPython.

Its source code is written in C - a compiled language.  
A compiled source code is turned into machine code.  
Machine code is executed (""interpreted"") by hardware.

In case of Jython - it's written in Java then compiled to byte code executed by JVM.  
Java Virtual Machine is also usually written in C and/or C++.  
Then again, source code get compiled, turns into machine code, which is executed by CPU.

In case of PyPy: written in RPython (special subset of Python).  
Then it's translated into a form of byte code, then to C and you know the rest.",5,0,0,False,False,False,1642098932.0
s33v2n,hsiqegg,t3_s33v2n,"That’s a really insightful question and I first want to say congratulations on finding that problem. Seeing inconsistencies like that’s really important for learning and a well written question (like this) is never a dumb question. 

The answer is not every language is interpreted. Some languages (like C) are compiled which means transformed into the raw machine code that can be run on the hardware. No interpreter needed once compiled. 

However this does pose the challenge of how the compilers were written. For example, the c compiler is written in c so how did it get compiled? That was done through boot strapping. First the most fundamental part of the compiler was written in assembly. This let you compile an extremely reduced C program. Then the next layer of the compiler was written in that reduced C and then compiled to allow a slightly less reduced C to be compiled. So on until the entire language could be compiled.",3,0,0,False,False,False,1642099166.0
s33v2n,hsiua5d,t1_hsiprv0,Yup! Thanks God Bless!,1,0,0,False,False,True,1642100612.0
s33v2n,hsislyq,t1_hsiqegg,"Out of all the platforms that I have posed this question and lost my brain cells thinking about this question over and over, your answer in layman terms is the best I could understand.

I understand the case now for interpreters written in languages that are compiled (Cpython written in C). Just to complete my understanding of this, what if the interpreter is written in the same language as to be interpreted eg. PyPy (interpreter written in Python for Python)? are such interpreters wired to in-built compilers?

Update: Just checked that the PyPy is written in RPython which is translated into C. So I guess the process for this is Your code in python => PyPy (RPython) => C files (which are compiled)

Thank you so much for your answer. God Bless! I can go to sleep now peacefully.",1,0,0,False,False,True,1642099984.0
s33v2n,hsj6v6n,t1_hsislyq,">Your code in python => PyPy (RPython) => C files (which are compiled)

No, no, no. Your Python code is never turned into C code during interpretation.

PyPy interprets your regular Python, not RPython.  
It's the program PyPy - the interpreter itself - that was written in RPython, which got turned into C to be compiled into machine code.

Python language doesn't care what interpreter was written in.",2,0,0,False,False,False,1642105303.0
s2ar5c,hsd9n83,t3_s2ar5c,"All CPU sizes are turing complete and computationally universal (so long as they atleast contain conditional branch)

The bus size is mostly just about performance, even very small devices like the 8bit gameboy could access large memory banks by using bank shifting.

There are compilers which can convert any program to execute on the z80 (such as in the 8bit gameboy)",57,0,0,False,False,False,1642006474.0
s2ar5c,hsdxgfd,t3_s2ar5c,"If you’re interested in poking around, and don’t want to learn any specific assembler, the cc65 compiler can compile C code into 6502 compatible executables for many vintage 8-bit systems, such as the Commodore 64. You can write little examples like what you’re talking about and see what happens. Speaking from experience, there’s a huge hit when you go from 8-bit integer math to 4 byte long int. Not only is it slower, but it adds a lot of extra code, which takes up program size and ram when the program is loaded. 

For example, I created a modern-like command line interface for the Commodore 64 called ChiCLI. 

I added a simple math function, so you could do some quick calculations on the command line. Nothing fancy at all, but like 11 lines of code turns into 472 extra bytes of machine code! Maybe that doesn’t seem like a lot, but in this case it’s 1% of my approx max 50K program! All the code is about 4 klocs, so 11 lines is about 0.27% of the code base. So 0.27% of the code turns into 1% of the entire program! 

The whole thing is here: 
https://github.com/chironb/ChiCLI

Here’s the code I’m referring to: 

// ********************************************************************************
		// = COMMAND / MATHS COMMAND 0--> This takes up 472 bytes!!!
		// ********************************************************************************
		} else if ( user_input_command_string[0] == '=' ) {

			long int answer, first_number, last_number;
			sscanf(user_input_arg1_string, ""%li"", &first_number);
			sscanf(user_input_arg3_string, ""%li"", &last_number);
			switch (user_input_arg2_string[0]) {
				case '+' : answer = first_number + last_number; break;
				case '-' : answer = first_number - last_number; break;
				case '*' : answer = first_number * last_number; break;
				case '/' : answer = first_number / last_number; break;
			  default  : puts(""?""); break;
			};//end-switch
			printf(""  = %li\n"", answer);",5,0,0,False,False,False,1642015333.0
s2ar5c,hse4p9h,t3_s2ar5c,"8 bit CPUs like the intel 8080 were used to create arcade machines without even having bit shifting instructions independent of the accumulator register. Such machines had shift registers added as additional pieces of hardware made of a few ICs, http://searle.x10host.com/spaceInvaders/BlockDiagram.jpg",3,0,0,False,False,False,1642018105.0
s2ar5c,hsfmpyw,t3_s2ar5c,"I don't know about tetris and pong in particular but some comments saying that it would be Turing complete are not telling the full story.

For Turing completeness it would need to be able to access memory in ways that are not usually done. For instance if its memory access is only through saying ""I want the memory at so-and-so address"", then it is not Turing complete. It would be severely crippled and even old 8-bit computers effectively use a 16-bit address register to get past this and have a little more power (while still not being Turing complete).

The way it would be Turing complete is if it could also say ""Shift the memory by one address left/right"". This way by running this command repeatedly it could access memory addresses that it otherwise wouldn't have been able to. (If you've seen a depiction of a true Turing machine this is how the machine accesses the tape.)",3,0,0,False,False,False,1642040363.0
s2ar5c,hse5iph,t3_s2ar5c,"A normal 8-bit CPU is turing complete with instructions for conditional branching as mentioned in one of the posts above. 

By turing complete we mean that it can run whatever the f\*\*\* you want really. Bitcoin mining, witcher 3, reddit etc. Now, how fast these things will run is a whole other question though.",2,0,0,False,False,False,1642018417.0
s2ar5c,hsfcu51,t3_s2ar5c,"I like to think of this like being a carpenter, of a craft worker. A 64 bit cpu can handle all the tools and a whole range of sizes quite easily. However you can, if you needed to, do the same work and effort with a few simple tools. 

It would take a lot longer and it would be way more complicated to use the simple tools, but it can be done.

An 8bit (and even a 4 bit) cpu can perform the same “work” as a 64 bit cpu, but its implementation and execution would be much more complicated",2,0,0,False,False,False,1642036000.0
s2ar5c,hsdc5nq,t3_s2ar5c,"[https://www.youtube.com/watch?v=ifXr7LORNCo](https://www.youtube.com/watch?v=ifXr7LORNCo)

&#x200B;

10 best 8bit games",4,0,0,False,False,False,1642007415.0
s2ar5c,hsda1j7,t1_hsd9n83,"Wow, I didn't know that, Thanks for answering!",15,0,0,False,False,True,1642006624.0
s2ar5c,hsfn89p,t1_hsd9n83,i salute your knowledge,3,0,0,False,False,False,1642040582.0
s2ar5c,hsfsi7j,t1_hsfmpyw,Thanks for this explanation. I was hoping to read someone explain what is meant by 8-bit computers being Turing complete.,2,0,0,False,False,False,1642042892.0
s2ar5c,hsdtlsb,t1_hsda1j7,"It's very important not to conflate bit width in one aspect with bit width in another. If an 8-bit CPU didn't have way to deal with data types larger than 8 bits then they wouldn't be able to count over 256. Instruction size is not the same as bus size is not the same as data type size is not the same as memory address size, yet all are expressed in bits.",15,0,0,False,False,False,1642013881.0
s2ar5c,hseoay2,t1_hsdtlsb,"The CPU won't be able to operate on more than one byte at a time, but multiple bytes can be combined to represent larger values. Operating on those will just take multiple instructions.",6,0,0,False,False,False,1642025356.0
s2ar5c,hsflc22,t1_hseoay2,"And while it probably goes without saying, working those larger values comes with a **significant** performance penalty.

Even just adding two 16-bit numbers turns into:

* Add LSB bytes.
* Test for overflow/carry.
* Add carry to one of the MSB bytes.
* Add MSB bytes.

That second step is multiple instructions so you have 1 8-bit ADD instruction turning into an easy 5+ for a single 16-bit ADD. Can always make up for it via clock speed but back in the 8-bit computing days we didn't have Arduino 16MHz speeds and long calculations like that ended up just being avoided for performance reasons.",2,0,0,False,False,False,1642039768.0
s2ar5c,hsfyyll,t1_hsflc22,You never heard of add-with-carry instructions? They've been standard for a long time...,2,0,0,False,False,False,1642045867.0
s2ar5c,hshir39,t1_hsfyyll,Don't you still have to check the carry flag and add it to the next byte before adding the next significant bytes?,1,0,0,False,False,False,1642082171.0
s2ar5c,hsi2a61,t1_hshir39,"It's built into the instruction. For example, on the 8080 the ADD instruction will ignore the carry bit but set it if the addition generates a carry. ""ADD D"" basically does A += D. The ADC instruction will add the carry bit. ""ADC D"" does A += D + carry.",1,0,0,False,False,False,1642090277.0
s1yfl6,hsbgnwm,t3_s1yfl6,This guy did fab in a home lab and has a great writeup on the physical process: http://sam.zeloof.xyz/second-ic/,46,0,0,False,False,False,1641968899.0
s1yfl6,hsbhqnf,t3_s1yfl6,"Edit: sorry for the formatting I am writing on phone. By hardware I guess you mean chips. Well like someone said the design and production/fabrication are two different things.
ARM designs the chips but does not produce the physical chips. You can learn design online, and with degrees. We use computers to design as well. We ""program"" our logical requirements like need 5 ALUs here, have a bus there following the infinity fabric protocol etc in something like verilog which them gives us a schematic with respect to logic gates.
The final implementation and production of the chips are actually in the domains of semiconductor electronics, nano tech, chemical engg and material science. The main technique used is known as photolithography. There is only ONE company in the world that makes the Machines to do photolithography and all the fabs buy from it.
The general basic process follows: (this is outdated now)


- You get your raw silicon

- process them into ingots (cylinders)

- cut them into thin round wafers

- Polish them

- You then use the output of the verilog design to make a mask.
A mask a generally a flat face of something that can block light, and you then cut the mask into the shape of a circuit (2D) to allow some spaces in between where the wires/transistors would be.


- you place the mask on top of the silicon wafers

- You use that mask and use high powered light to melt away the some of the silicon that is under the gaps of the mask.

- in the newly created spaces you add some impurities like gallium or aluminium through electron vapour diffusion(not sure if that was the correct name I am forgetting some stuff) or some other techniques to dope the silicon so that it is more or less conductive.


- what this does is creates transistors PNP or NPN FETs(field effect transistors) at the wafer level.

- then it is layered with some other layers depending on the fab.

- there can be layers of masks and multi layer designs to improve efficiency. 

- a single silicon wafer will have many cpus/chips.


- there will always be chips that will have defects in them because we are dealing with such a small scale. Those chips either are either discarded or more commonly sold as lower tier products with the bad/defective circuits burned off.


- The use of light is also the reason why crossing the 1nm barrier would be so difficult apart from electron tunneling.

I hope this somewhat explains it.
Today we have even more packing density with FINfet and 3D stacking.
We would probably see chips with inbuilt cooling pipes at the transistor level soon.",28,0,1,False,False,False,1641969611.0
s1yfl6,hsbwnbp,t3_s1yfl6,"There’s 2 main processes:

Photolithography is used to create semiconductors and chips. This is an optical and chemical process which will imprint certain doping patterns onto semiconducting material along with copper interconnects. This is essentially a tiny circuit, you can make transistors, diodes, resistors and some very limited values of capacitors and inductors in this process. Conductive pins which reach outside the chip casing provide access to certain points in this circuit. Production falls under physics, chemistry and solid state physics, design falls under electrical engineering, these usually aren’t done by the same company.

Printed circuit board (PCB) fabrication is used to create finished boards with all their components including chips, resistors, transformers, connectors, capacitors, inductors etc etc. Similar to semiconductors this also uses photolithography to create the copper traces on the board, along with a host of various CNC machines for drilling holes, cutting edges, placing components. There’s various high volume ways of soldering the components to their pads such as wave soldering or reflow ovens. Production falls under mechanical engineering and a little bit of chemistry, design falls under electrical engineering, again usually not done by the same company but larger companies will spin their own PCBs because a PCB fab house is much cheaper to run than a semiconductor fab.

Other kinds of components all have their own specialized manufacturing method, for example a USB connector is a cleverly folder piece of sheet aluminum. A capacitor is two conductive plates separated with a layer of ceramic insulator. A transformer is enameled copper wire wound around a ferrite core. A crystal oscillator is a finely cut piece of quartz with two metal plates on each side. Etc etc.

Once you have assembled boards the rest isn’t too complicated, they’ll be screwed into some kind of plastic or metal enclosure, various forms of cables will provide connections to other boards, external power sources and I/O devices.",3,0,0,False,False,False,1641980935.0
s1yfl6,hsbc98g,t3_s1yfl6,You should probably ask r/ComputerEngineering or a similar engineering subreddit.,10,0,0,False,False,False,1641966161.0
s1yfl6,hsbz1e6,t3_s1yfl6,"I’m a materials scientist, though, I work with polymers more than silicon. A lot of people here already mentioned aspects of the process, but I know many people who go into Si chip manufacturing. Maybe you could ask r/materials if you want more answers regarding lithography and vapor deposition. It’s pretty cool.",2,0,0,False,False,False,1641982940.0
s1yfl6,hsbf38j,t3_s1yfl6,"Design and production are very distinct. Production is a mostly automated process that involves creating silicon wafers and burning circuit patterns into ‘em. The design process involves creating circuit diagrams and software simulations, AFAIK. They also use software to optimize the layout of the circuit, so that minimal space is used on the chip.

EDIT: I’m surprised you can’t find any information on these processes. There’re tons of documentaries and series detailing the creation of computer parts. Maybe try going to your nearest university and finding resources there if your internet searches come up dry? Most of what I know of the process comes from my computer organization class in college so it’s not a bad place for that purpose. :P",2,0,0,False,False,False,1641967881.0
s1yfl6,hscu0n6,t3_s1yfl6,"Silicon, silicon, silicon",1,0,0,False,False,False,1642000423.0
s1yfl6,hsdg95q,t3_s1yfl6,"This person reverse engineers (mostly vintage) hardware as a hobby and their blog is one of the most insightful on the matter, including the tiny clever design tricks: http://www.righto.com/

On the less serious hand, this article describes the process of building a cpu with colorful illustration: https://blog.robertelder.org/how-to-make-a-cpu/",1,0,0,False,False,False,1642008938.0
s1yfl6,hsbi5tv,t1_hsbgnwm,"Thank you, that was a fascinating read. Some people are too smart",5,0,0,False,False,False,1641969890.0
s1yfl6,hsbxtb3,t1_hsbgnwm,Utterly fascinating.,2,0,0,False,False,True,1641981911.0
s1yfl6,hsd07yt,t1_hsbgnwm,This is incredible. Thank you for posting this,1,0,0,False,False,False,1642002884.0
s1yfl6,hsda8xj,t1_hsbgnwm,I literally just wanted to post him the moment I read the title.,1,0,0,False,False,False,1642006702.0
s1yfl6,hsbwpn4,t1_hsbhqnf,">photolithography.

I did not even remember such term existing, I absolutely thank you for this!",4,0,0,False,False,True,1641980988.0
s1yfl6,hsbxzvt,t1_hsbhqnf,Can you recommend any good resources to learn design online?,1,0,0,False,False,False,1641982062.0
s1yfl6,hsc4vy3,t1_hsbhqnf,"Woah, cooling pipes at the transistor level. It sounds like they will run cooler. When do you think it will be used in common products like cpu and gpu chips?",1,0,0,False,False,False,1641987469.0
s1yfl6,hsbxylg,t1_hsbwnbp,"This humbles me as a software engineer haha, and I thought my job was complicated!",3,0,0,False,False,True,1641982031.0
s1yfl6,hsbxwsp,t1_hsbc98g,"I'm a software engineer with some cyber security /penetration testing background, I haven't touched anything CPU related for almost half a decade never had to all this seems absolutely amazing to me.",1,0,0,False,False,True,1641981989.0
s1yfl6,hsbzb9j,t1_hsbz1e6,"I knew that computer architecture / engineer would be a labyrinth but I never expected that just the creation and production of a simple CPU requires literally a dozen ++ of different sciences to be done, can't even imagine how hard it will be to comprehend how a high level language interacts with the metal and controls a software.

If you're doing this you're absolutely amazing.",2,0,0,False,False,True,1641983162.0
s1yfl6,hsbwry0,t1_hsbf38j,"I refer to information about the physical creation then production, which I already know they are separate processes, apparently it makes sense I can't find them cause there's only one company in the world that creates the tools for it.",1,0,0,False,False,True,1641981040.0
s1yfl6,hse5qpb,t1_hsbi5tv,How smart this is should be evaluated long term. It is highly probable you would be fast tracking yourself to stage iv cancer in such a home lab.,3,0,0,False,False,False,1642018500.0
s1yfl6,hscvfii,t1_hsbxtb3,"Instead of working, i too am being fascinated by his process. Great link /u/timeforscience",1,0,0,False,False,False,1642000996.0
s1yfl6,hsbyzk8,t1_hsbxzvt,"That will depend how good you are already with digital electronics and digital logic.

If you are a beginner then you should probably do a course on digital logic first and then move onto computer architecture and organization, microprocessor internals (x86 and arm).
There are lots of completely free college level playlists from the likes of MIT opencourseware. 
For digital logic you WILL need to do a lot of problems. Because problems are the way that you actually engage with something.
Once you are done and have a good grasp on the basics there are lots of nice udemy courses that goes for very low prices during sales, I am talking like $10 but are full of content. Take a single or at Max two courses and not more and focus on digesting them well.
The courses you should be looking out for generally are courses that have VLSI, VHDL, verilog in their teaching content.
For a first course any top rated course with many hours of video would do.
By the time you are done with that you'll know what you want to do next. For getting industry jobs, you can maybe start with FPGA. Because it is something you can actually own and experiment on and upload the code to GitHub like any other software project. And the concepts that you are going to need to program an FPGA overlaps a bit with verilog. You can even use C++.",6,0,0,False,False,False,1641982898.0
s1yfl6,hsc2iac,t1_hsbzb9j,"Haha I actually study computational materials science so I use code to study polymers, but there is definitely a lot overlap between sciences these days.

As an aside, a cpu is incredibly complex even though it seems like such a simple thing these days! Pretty incredible.",1,0,0,False,False,False,1641985697.0
s1yfl6,hsbzbtn,t1_hsbwry0,Small correction. Its only one company that makes the cutting edge nanometer photolithography machine. For PCB (the likes of motherboards) there are other options. You can make PCBs on your own in a homelab!,2,0,0,False,False,False,1641983175.0
s1yfl6,hsc2efg,t1_hsbwry0,What company ?,1,0,0,False,False,False,1641985614.0
s1yfl6,hsc1icu,t1_hsbyzk8,"Huge +1, and you can actually run all that stuff in simulators, the actually fabbing part is more of an industrial process, very hard to get into it as a hobby.

If you want to do anything there as a hobby, you can try to build small “computers” with transistor-transistor logic, check Ben Eater’s channel on youtube.",3,0,0,False,False,False,1641984927.0
s1yfl6,hsclhjf,t1_hsbyzk8,"To give you some idea of the scale of things, my computer systems engineering degree included a semester-long course in first year that began with you knowing nothing about digital logic and ended with small groups working together to do a gate-level design of an 8-bit CPU in VHDL.  We were given an instruction set specification and a minimum amount of memory it had to support (64k IIRC) but otherwise everything about it was up to us.  This was circa-2000 and the CPU we were designing was roughly Z80 sort of technology.

We thought we were super cool with our pipelined execution and paged memory layout giving us 16MB addressable memory (hardware was not memory mapped into data memory in this design).

We were allowed to assume 1ns per gate; our design was able to run at 16MHz.  Good times.

Anyway, point is that it's possible to get your head around enough knowledge to do a basic CPU design in a few months of two lectures a week, some reading and some experimenting.",2,0,0,False,False,False,1641996775.0
s1yfl6,hsbzj9o,t1_hsbzbtn,I'm looking specifically for the way a nanometer-photoligraphy machine interacts with the rest of the computer architecture and how it is done when it comes to its physical substance of the computer its self.,1,0,0,False,False,True,1641983346.0
s1yfl6,hsc0a7e,t1_hsbzj9o,"Hmm, the main working concepts are not very hard to grasp. Its the details and the fact that we are talking nanometer scale that makes it hard from an engineering standpoint. And that will take a whole career to understand tha complete nuance of.
But honestly from the perspective of CS it really doesnt matter how the logic is implemented physically. CS starts at the logic level. If tomorrow we change from semiconductors to say carbon nanotubes provided we have the same TTL layer (transistor transistor logic) we can run the same programs on it even though the underlying implementation is completely different. Thats the beauty of abstraction and the reason you dont need to be an electronics expert to design chips. Because chip design and verilog generally abstracts out the transistor layer.
If you know your digital logic. All you need to know to understand the basics is how photolithography can be used to make things like transistors, diodes, resistors. Once you got your logic gates everything else comes by copy pasting.
So what you REALLY want to understand is at the semiconductor level how we make transistors. And for that if you are already in uni you could probably take a digital circuit design class or semiconductor electronics class. The info we get at the undergraduate level is enough to see how this can be done on a smaller and smaller scale. Now how to make it smaller?? Thats a whole different ball park and comes under the domain of nano tech, material science and physics.",2,0,0,False,False,False,1641983950.0
s1yfl6,hsc1u65,t1_hsc0a7e,"I’d research into the “abstraction” mentioned here: it’s called the von-neumann architecture, and basically everything in the world runs on it.",1,0,0,False,False,False,1641985183.0
s2gdc0,hsedg3a,t3_s2gdc0,"""digital"" comes from ""digit"" and means it can be expressed by a digit or it is countable, you can express it as a digit, you can point in it. It's the counterpart of ""analog"" which is not countable but continuous, you can't point on it, it is not an exact digit.
Real things can be digital, like the apple on a tree, or they can be analogous, like air or like water.",8,0,0,False,False,False,1642021335.0
s2gdc0,hsei01i,t3_s2gdc0,"To be digital is to be discrete, where any digital information has only a limited number of discrete values. For electronics, voltage is continuous but we have selected certain voltages to represent digital information, and that digital information can only be one of two voltages, a “zero” or a “one.” It is discrete, even though voltage as a concept of physics is continuous",6,0,0,False,False,False,1642022990.0
s2gdc0,hsedp6c,t3_s2gdc0,"Great question. I could be wrong but I believe digital means representing numbers with digits. So instead of measuring a voltage by seeing how much its current deflects a magnet, for example, you could measure it by convering it to binary digits with a ADC. 

Now how a ADC represents measurable quantities with digits is a question I don't feel qualified enough to answer.

For more information on how analog signals are measured digitally, I would look into Series-Approximation Registers (SARs).",3,0,0,False,False,False,1642021427.0
s2gdc0,hsjwy11,t3_s2gdc0,"Digital comes from a latin ( I think) word  meaning finger, as in technology at your fingertip.",1,0,0,False,False,False,1642115223.0
s2nqom,hsfmzni,t3_s2nqom,"From my limited knowledge, programming languages themselves tend to be English. Now, there's nothing stopping you from doing basic manipulation to ""translate"" it into another (spoken) language. Plus functions, variables, etc. aren't limited to English",1,0,0,False,False,False,1642040478.0
s2nqom,hsfs088,t3_s2nqom,"An AI which assists a non-native person during email/essay writing process, or just a good freakin' translator. You can do these things if you love both cs and foreign languages! :D

There are also so many things could be done. For example, extended search algorithm which would translate the given query into other languages and give much more results in other languages (which could be translated to preferred language after) rather than just in one you used. It would make searching things so much more productive and effortless.

Sorry for such a messy comment, I'm not really good at editing. Which makes me think of an algorithm which corrects texts in several languages using their traditional punctuation and spelling rules (i think it exists already, but still)

So yeah, there are still lots of things to be done, lots of possible projects and opportunities! (Even though most of the ones stated are already implemented in our everyday lives, but there is still a lot of fish to catch)",1,0,0,False,False,False,1642042671.0
s2nqom,hsfzwpo,t3_s2nqom,"Theory-wise I think there's an overlap in that if you've developed skills to learn languages then those same skills will help you learn computer languages.


Job-wise there are a lot of technical writing jobs in CS where they need people who can write in multiple languages.",1,0,0,False,False,False,1642046318.0
s2nqom,hsg4f0l,t3_s2nqom,"I don't know a lot about it but I believe ""context-free grammar"" is an overlapping idea in linguistics as well as programming language/compiler design and maybe computer science in general",1,0,0,False,False,False,1642048553.0
s2nqom,ht127hr,t3_s2nqom,"On the contrary, look up esoteric programming languages. It’s purely experimental though.",1,0,0,False,False,False,1642420837.0
s2nqom,hu4eirp,t3_s2nqom,"Yes, the theory behind programming languages that give us syntax, semantics, and “grammar” is a base concept in this space. They are majorly based off of a concept called context free grammars(CFG). CFGs can be though of as the building blocks of of how to make sentences, strings in CS. Research into what these are and how they work and you’ll see obvious parallels as to how base theories in CS and mathematics are present in formal language.",1,0,0,False,False,False,1643091597.0
s1gfy8,hsbnrpe,t3_s1gfy8,This is awesome and I love the art. I'm definitely going to print these and post them in my lab.,2,0,0,False,False,False,1641973905.0
s1gfy8,hsbsnvs,t3_s1gfy8,Amazing,2,0,0,False,False,False,1641977704.0
s1gfy8,hsbpx69,t3_s1gfy8,EDIT: Thank you to u/antiogu for pointing out the error. The y-intercept should be 2 in my sketch on the last panel.,1,0,0,False,False,True,1641975560.0
s1gfy8,hsbp6xv,t1_hsbnrpe,Thank you! I'll take that. 😄 So your lab is related to CS/mathematics?,2,0,0,False,False,True,1641974991.0
s1gfy8,hsglxf6,t1_hsbsnvs,Thank you!,1,0,0,False,False,True,1642059246.0
s1gfy8,hsbryat,t1_hsbp6xv,"Yup! :)

We work in Infosecurity and cybersecurity. I was very surprised by how much ML is used in our field.",2,0,0,False,False,False,1641977149.0
s1gfy8,hsbt2db,t1_hsbryat,Interesting! Is the ML being used for some form of anomaly detection?,2,0,0,False,False,True,1641978025.0
s1gfy8,hsbyp2d,t1_hsbt2db,"Yes! However for our specific subfield and group, no (but there is a different lab here that is doing some form of anomaly detection). I can't really say what exactly we are working on since it might be easy to find me irl lol. 

But to give you an idea, our features usually consist of assembly code, which is then vectorized, and then we apply some traditional ML algorithm - typically clustering (k means is popular since it is simple :P ). But we currently are exploring different and better techniques to improve for our next project",2,0,0,False,False,False,1641982650.0
s1gfy8,hsbz5b6,t1_hsbyp2d,"Yes, of course.

Usually, with more complex models the trade-off is between performance and explainability. Sounds exciting! It is always nice to go from more classic approaches to something more modern and see how it affects the project and the performance. Also, the nice thing here is that you will now already have a good baseline to compare the ""better techniques"" for your next project.",2,0,0,False,False,True,1641983027.0
s1h1gt,hs8j8oq,t3_s1h1gt,"LinkedIn, TL;DR, console, Linus tech tips, Some Ordinary Gamers, simpli learn, fire ship, network chuck.",7,0,0,False,False,False,1641924518.0
s1h1gt,hs9ojp2,t3_s1h1gt,https://youtube.com/c/CodingTech,1,0,0,False,False,False,1641939666.0
s14xir,hs6fo0x,t3_s14xir,"* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **The Art of Computer Programming** (TAOCP) by _Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to Automata Theory, Languages, and Computation** by _Hopcroft, Ullman_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Discrete Mathematics** by _Ross, Wright_
* **Introduction to Graph Theory** by _Wilson_
* **Software Engineering** by _Sommerville_
* **Design Patterns: Elements of Reusable Object-Oriented Software**
* **Design Patterns Explained: A New Perspective on Object Oriented Design**
* **Fundamentals of Database Systems** by _Elmasri, Navathe_
* **Numerical analysis** by _Kincaid, Cheney_
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **Artificial Intelligence: A Modern Approach** (AIMA) by _Russell, Norvig_
* **Compilers: Principles, Techniques, and Tools** (Dragon Book) by _Aho, Lam, Sethi, Ullman_
* **The C++ Programming Language** by _Stroustrup_
* [**Beej's Guide to Network Programming**](http://www.beej.us/guide/bgnet/)
* [**Modern C**](https://modernc.gforge.inria.fr/) by _Gustedt_
* [**x86-64 Assembly Language Programming with Ubuntu**](http://www.egr.unlv.edu/~ed/assembly64.pdf) by _Jorgensen_
* **Effective Modern C++** by _Meyers_",100,0,2,False,False,False,1641884352.0
s14xir,hs731a3,t3_s14xir,"I learned the most about computation from reading Leslie Lamport. Something about his style is simpler than a lot of CS authors. I love this paper:

[Computation and State Machines](https://lamport.azurewebsites.net/pubs/state-machine.pdf) 

It basically has everything in one place, though it is a little dense if you aren’t on practice with a lot of discrete math. The first two chapters in [Specifying Systems](https://lamport.azurewebsites.net/tla/book-02-08-08.pdf) give the best introduction to the subject. The book is great too, again focusing on what computation is at its core vs. talking about specific technologies.

Another book that has been eye opening for me is [Concrete Semantics](http://concrete-semantics.org). It’s about programming language implementation with a proof assistant, but it also focuses on what exactly a programming language is and how its semantics is not a fuzzy, informal thing, but a very concrete definition that we can write down and refer to. Programming languages are the foundation of CS. You can’t create or analyze an algorithm without a language for expressing it.

This last one I’m torn with recommending, because it doesn’t have the same readable style as the previous authors, but it’s also pretty much the best one-stop shop for the most modern view of CS: [Formal Reasoning About Programs](http://adam.chlipala.net/frap/frap_book.pdf). I still have to do double and triple takes while reading this book, but it covers so many important topics in one place that it’s invaluable. What I do is, when I don’t understand a section, I at least use it to search for other papers / books on the topic before coming back to it. Which might be the intention of the book anyway. But I mean, the definition of computer science should literally be agreed to be “formal reasoning about programs.” So this book contains to many relevant topics in one place to avoid.",12,0,0,False,False,False,1641902261.0
s14xir,hs7372r,t3_s14xir,"I’m a huge fan of [Code: The Hidden Language of Computer Hardware and Software](https://en.m.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software). It’s a little older at this point, but the foundation it lays is amazing. It builds from simple hardware relays, to a CPU, all the way up to an OS, and everything in between. Truly a hidden gem.",7,0,0,False,False,False,1641902371.0
s14xir,hs6ogcx,t3_s14xir,"The Little Schemer

The Elements of Computing Systems",6,0,0,False,False,False,1641890889.0
s14xir,hs7kkaf,t3_s14xir,"I recommend ""Models of Computation: Exploring the Power of Computing"" by John E. Savage. I much prefer it over the Sipser.

For a beginner, maybe the CLRS algorithms text or something that makes you learn C/assembler really well.",5,0,0,False,False,False,1641911450.0
s14xir,hs6a614,t3_s14xir,"I liked the Andrew Tanenbaum's books on OS and computer architecture. 
Digital logic by Morris mano
Computer architecture by Morris mano
Algorithms by Robert Sedgwick",10,0,0,False,False,False,1641880885.0
s14xir,hs6k2w6,t3_s14xir,"Godel, Escher, Bach by Douglas Hofstadter. If you're keen on a different perspective.",7,0,0,False,False,False,1641887494.0
s14xir,hs7ihvg,t3_s14xir,"Mathematics and Computation by Avi Wigderson, Algorithms by Jeff Erickson",2,0,0,False,False,False,1641910551.0
s14xir,hs89frn,t3_s14xir,"Introduction to Linear Algebra by Gilbert Strang: [https://math.mit.edu/\~gs/linearalgebra/](https://math.mit.edu/~gs/linearalgebra/)

Artificial Intelligence: A Modern Approach by Russell and Norvig: [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)",2,0,0,False,False,False,1641920974.0
s14xir,hs8kwas,t3_s14xir,"I recently got Mastering Regular Expressions by Jeffrey Freidl, seems pretty good in so far. It’s not really tutorialey, I’d say it’s more concept. Though it shows examples in a few different languages",2,0,0,False,False,False,1641925121.0
s14xir,hsab8zv,t3_s14xir,Algorithms to live by,2,0,0,False,False,False,1641948983.0
s14xir,hsd6q3n,t3_s14xir,"Designing data intensive applications by Martin kleppman 

Systems performance by Brendan Gregg 

Fundamentals of software architecture an engineering approach by mark Richards and Neal ford 

Design patterns: elements of reusable object oriented software by gamma,helm, Johnson, vlissides

The art of immutable architecture by Michael Perry is also a new one I’m enjoying",2,0,0,False,False,False,1642005376.0
s14xir,hsqvmqw,t3_s14xir,"A little late to this party, but here's a list I personally loved. My background is a bit varied - PLT, Numerical Analysis/Scientific Computing, and random interests I picked up throughout undergrad and beyond.

**Algorithms**

Building intuitions for how to compute efficiently.

The book that everyone else immediately recommends here is CLRS' ""Introduction to Algorithms"". I break from this tradition a bit in that, this was the one text that scarred me the most, mainly because I was wholly unprepared for it when I first went in.

First Course: these texts are geared for a first exposure to the construction + building up intuitions for algorithms.

* Grokking Algorithms - Aditya Bhargava
* The Algorithm Design Manual - Steven Skiena
* Algorithms - Robert Sedgewick
* Algorithms - Jeff Erickson

Analysis of: these texts are a bit more advanced and focus on proving correctness or properties of algorithms

* An Introduction to the Analysis of Algorithms - Robert Sedgewick
* Algorithm Design - Kleinberg & Tardos
* The Design and Analysis of Algorithms - Dexter Kozen

Reference ""brick"": I've always had a love-hate relationship with this one

* Introduction to Algorithms - Cormen, Leiserson, Rivest, Stein^(not a favorite, but obligated to mention)

--------------------------

**Programming Language Theory**

This is my specialization in undergrad - how to reason about program semantics, how to analyze programs, and how to give type to things. I would recommend at least some algebra before tackling these. Some of the semantics texts may touch on algebraic topology results (e.g. Scott + Domain theory), but the poset / limit constructions are mostly self-contained.

Program Analysis: how to analyze programs

* Principles of Program Analysis - Flemming Nielson
* Principles of Abstract Interpretation - Patrick Cousot
* Advanced Compiler Design and Implementation - Steven Muchnick^(it pretends that it's a compiler text, but it's a static analysis text)
* Data Flow Analysis: Theory and Practice - Sanyal, Sathe, Khedker

Semantics: how to specify + calculate with the semantics of programs (what does it mean?)

* Types and Programming Languages - Benjamin Pierce
* The Formal Semantics of Programming Languages: An Introduction - Glynn Winskel
* Foundations for Programming Languages - John C. Mitchell
* Semantics With Applications: An Appetizer - Flemming Nielson
* Formal Languages and Compilation - Morzenti, Breveglieri, Reghizzi

Types: type semantics, & how to encode properties/contracts/proofs into dependent languages like Coq & Idris

* Software Foundations - Benjamin C. Pierce
* Proofs and Types - Jean-Yves Girard
* Lectures on the Curry-Howard Isomorphism - Sorensen & Urzyczyn
* Type driven development with Idris - Edwin Brady

--------------------------

**Systems / Compilers**

Compilers: how to write a compiler / interpreter

* Engineering a Compiler - Keith Cooper
* Modern Compiler Implementation in Java - Appel & Palsberg
* Compilers: Principles, Techniques, and Tools - Aho, Lam, Sethi, Ullman^(not a favorite, but obligated to mention)
* LLVM Cookbook - Pandey & Sarda

Systems: how computer systems are architected, how operating systems are made up, concurrency in action

* The Elements of Computing Systems: Building a Modern Computer from First Principles - Nisan & Schocken
* Computer Organization and Design RISC-V Edition - John L. Hennessy
* Linux Pocket Guide - Daniel J. Barrett
* Understanding the Linux Kernel - Bovet, Cesati
* The Little Book of Semaphores - Allen B. Downey
* Java Concurrency in Practice - Brian Goetz

--------------------------

**Numerical Analysis / Mathematical Modelling**

Prereqs: linear algebra + diff eq

* Introduction to Linear Algebra - Gilbert Strang
* Linear Algebra and its Applications - Gilbert Strang
* Elementary Differential Equations & Boundary Value Problems - Boyce & DiPrima
* Terrell's Notes on Differential Equations - Robert Terrell

First Course: introduction to numerical analysis + scientific computing

* Introduction to Scientific Computing - Gene H. Golub
* Applied Numerical Methods: With MATLAB for Engineers and Scientists - Steven C. Chapra
* A First Course in Numerical Methods - Greif & Ascher

Second Course (grad): more advanced methods

* Matrix Computations - Gene H. Golub
* A first course in optimization - Charles Byrne
* Numerical Linear Algebra - Nick Trefethen
* Applied Numerical Linear Algebra - James Demmel

Applied:

* An Introduction to Mathematical Modeling - Ed Bender
* Scientific Visualization: Python & Matplotlib - Nicolas Rougier

--------------------------

**Programming**

General languages: great PL manifestos

* The C Programming Language - Kernighan & Ritchie
* A Tour of C++ - Bjarne Stroustrup
* Hack and HHVM - Owen Yamauchi
* Programming in Lua - Roberto Ierusalimschy

Functional PL / Richard Bird: a world where you can do calculations with programs, and learn some category theory along the way

* Learn you a Haskell - Miran Lipovaca
* Real World OCaml - Yaron Minsky
* Introduction to Functional Programming - Bird & Wadler
* Pearls of Functional Programming - Richard Bird

--------------------------

**Learning to read Donald Knuth's TAOCP**

It took me most of undergrad and several years afterwards to work through small parts of the fascicles of volume 4. It led me down a giant detour of analytic + enumerative combinatorics, with complex analysis being the backbone of several of these asymptotic enumeration problems.

Step 1: Learning Discrete Mathematics

* A Course in Discrete Structures - Pass & Tseng
* Discrete mathematics and its applications - Kenneth H. Rosen
* Concrete Mathematics: A Foundation for Computer Science - Donald Knuth
* Concrete Math Companion - Kenneth E. Iverson

Step 2: Enumerative combinatorics

* Visual Complex Analysis - Tristan Needham
* Generatingfunctionology - Herbert Wilf
* Analytic Combinatorics - Flajolet & Sedgewick
* Introduction to the Theory of Species of Structures - Bergeron, Labelle, Leroux

Step 3:

* ???

Step 4: Profit

* The Art of Computer Programming - Donald Knuth^(not a favorite, but obligated to mention)

--------------------------

**Others**

Some other books (in order of ease) that I remember

* LaTeX - Leslie Lamport
* Discrete differential geometry: an applied introduction - Keenan Crane
* Nonlinear dynamics & chaos - Steven Strogatz
* A first course in network theory - Estrada & Knight
* Galois' Dream - Michio Kuga
* Ideals, Varieties, and Algorithms - Cox, O'Shea, Little
* Putnam and Beyond - Titu Andreescu

--------------------------",2,0,0,False,False,False,1642238446.0
s14xir,hs7gt3j,t3_s14xir,[deleted],1,0,0,False,False,False,1641909792.0
s14xir,hs6xkxh,t3_s14xir,"Clean Code

Also Cracking the Coding Interview (if you haven't read it already). I just started reading it late last night and couldn't put it down.",-2,1,0,False,False,False,1641898309.0
s14xir,hs7a06r,t3_s14xir,Should I start learning in my spare time or wait until ny University starts CS subjects for us in 2nd year?,1,0,0,False,False,False,1641906450.0
s14xir,hs9a1p2,t3_s14xir,What about modern CS Books?,1,0,0,False,False,False,1641934332.0
s14xir,hsa4oi8,t3_s14xir,C++ Concurrency In Action - Anthony Williams,1,0,0,False,False,False,1641946190.0
s14xir,hsambyl,t3_s14xir,"- The Annotated Turing
- A Mind At Play (biography of Claude Shannon)
- Assorted Papers on Fun and Games, Knuth
- And ther is an upcoming biography on von Neumann available for pre-order",1,0,0,False,False,False,1641953789.0
s14xir,hsc2c60,t3_s14xir,GEB?,1,0,0,False,False,False,1641985565.0
s14xir,hs6ohn8,t1_hs6fo0x,Oh wow that’s a lot of good books only a few of which I recognise. Did you read them all?,7,0,0,False,False,False,1641890918.0
s14xir,hs766kj,t1_hs6fo0x,Man ... you the MVP. You even wrote your own book and shit AND you're a PhD. Damn.,6,0,0,False,False,False,1641904272.0
s14xir,hs7likp,t1_hs6fo0x,"Hi, 

Is it possible for you to also suggest the reading order (of at least a few books) as well?",4,0,0,False,False,False,1641911856.0
s14xir,hs8or77,t1_hs6fo0x,"Most of them are good books but I don't think Sipser's Theory of Computation is a good book, particularly for beginners. The explanations are somewhat lacking. There are better books on automata and computation than that one.

Edit: The Algorithm Design Handbook by Skiena is an excellent introductory algorithm book with good explanations and real-world examples.",5,0,0,False,False,False,1641926515.0
s14xir,hsanngo,t1_hs6fo0x,"> Design Patterns: Elements of Reusable Object-Oriented Software

Also occasionally referred to as ""Gang of Four""

Also would like to recommend **Types and Programming Languages** by Benjamin Pierce",2,0,0,False,False,False,1641954355.0
s14xir,hs7wvgx,t1_hs731a3,"Wasn’t expecting to see “Concrete Semantics” get a mention. For formal methods, I’d also add “Handbook of Practical Logic and Automated Reasoning“ by John Harrison.

Boolos and Jeffrey’s “ Computability and Logic” was written by philosophers, for philosophy students, but it’s rigorous and expansive enough that CS students should be able to get something out of it. Geoffrey Hunter's “Metalogic” is also excellent, and much more accessible.",5,0,0,False,False,False,1641916343.0
s14xir,hs9acjh,t1_hs731a3,sorry got to ask what is the role name of someone who does that stuff?,2,0,0,False,False,False,1641934440.0
s14xir,hs73854,t1_hs7372r,"Desktop version of /u/HiImLary's link: <https://en.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",1,0,0,False,False,False,1641902390.0
s14xir,hsaqoz7,t1_hs7kkaf,Is it better than Sipser and how? I had Sipser bookmarked as an intro must read for computational complexity,1,0,0,False,False,False,1641955643.0
s14xir,hsblwzh,t1_hsab8zv,"This is one of the books that hyped me up when I got my first CS job.  

*Algorithms To Live By” was neat and made me think “woah, this stuff really is cool”

*Thinking, Fast and Slow* is another one. It’s subject is actually psychology, and has nothing to do with CS, but it was recommended by a CS professor and I’ve seen it come up in the CS subs a few times. I like that it aims to compare the two systems of thought and explains the roles each one play in our day to day decision making and problem solving. The things I learned from that book really stuck with me and my career has directly benefited from reading it.",2,0,0,False,False,False,1641972543.0
s14xir,hs8nf0z,t1_hs7gt3j,I agree. The authors seem so smug through their writing. The book really isn’t the masterpiece people hype it up to be.,0,0,0,False,False,False,1641926032.0
s14xir,hsb3lh5,t1_hs7a06r,Start,2,0,0,False,False,False,1641961493.0
s14xir,hs8inwr,t1_hs6ohn8,"Not from cover to cover, of course, but yes, I've read at least fragments of each.

Some of them I'd read for work, some for university, some for sheer curiosity; I liked most of them much enough to even buy my own copy (although some are too expensive and TAOCP is work in progress).",5,0,0,False,False,False,1641924304.0
s14xir,hs8itdq,t1_hs766kj,"Haha, I've just noticed how similar _Jorgensen_ and _Jorengarenar_ look like.",3,0,0,False,False,False,1641924361.0
s14xir,hs8zkdb,t1_hs7likp,"Surprisingly hard question, you did ask.  
For scientific literature there rarely is order of reading. You read chapter from one, few definitions from another, fill the blanks from third.  
And I've also already forgotten how it is to be new to all of that, so my judgement ~~may~~ is screwed by already knowing things a beginner normally wouldn't have a clue about. But I'll try.

For programming, I could suggest **SCIP**... alongside **K&R**? The first teaches more about programming in general and the second is about C - a base for most languages we use today.  
Then I'd move to C++ and **Clean Code**.

**Concrete Mathematics** (sometimes also called a ""volume 0 of TAOCP"") is the entrance to math here. Then **Discrete Mathematics**.  
Plus you need something for algebra and calculus (unfortunately, the ones I know don't have an English release).

The two books on automata and computation (by *Hopcroft & Ulmman* and by *Sipser*) I've read simultaneously.",5,0,0,False,False,False,1641930484.0
s14xir,hsbfcwh,t1_hsanngo,"Technically, it is not the book itself, but its authors who are referred to by that name",1,0,0,False,False,False,1641968054.0
s14xir,hsaph3p,t1_hs7wvgx,"Re Concrete Semantics - that book just changed my view on programming language theory. I was never particularly interested in it, but got there because of my interest in verification - at some point, you have to verify real code, which requires a definition of its semantics.

And sure there are other books on PL semantics, but the focus on using a proof assistant is also a big plus in my book.",2,0,0,False,False,False,1641955126.0
s14xir,hs9wug3,t1_hs9acjh,"Well, you can use this stuff in any role, because these books just teach you how to think about programs. Like I’m a “regular” software engineer, but I use TLA+ (Leslie Lamport’s logic and toolkit) to analyze models of things that I’m building all the time. Amazon and Elasticsearch also have been known to use TLA+ too. And, I use the concepts in all of these books when reading code every day.

Anyone involved in software verification uses things like this on an even deeper level. Like anyone that works on airplane software, or even some people who make [formally verified operating systems](https://sel4.systems). These roles are less common though.",2,0,0,False,False,False,1641942911.0
s14xir,hsatrad,t1_hsaqoz7,"Savage's text is just more comprehensive. It covers circuit complexity, satisfiability, and random-access machine models in great depth in addition to all the things in Sipser.

In terms of having something elegant and self-contained for an intro course - or if you want to work through the whole thing - then Sipser is more suitable. It's better as a primer.

You'll see what I mean if you skim the pdf's.",1,0,0,False,False,False,1641956968.0
s14xir,hs8zuw3,t1_hs8inwr,"> TAOCP is work in progress
 
Is it though? Game of Thrones could be finished and given a sequel series with several spinoffs and that last book still won't be here.",4,0,0,False,False,False,1641930591.0
s14xir,hs9akcg,t1_hs8itdq,"Wait, so you're not him?",3,0,0,False,False,False,1641934518.0
s14xir,hscc3jf,t1_hs8zkdb,Thank you!,1,0,0,False,False,False,1641992072.0
s14xir,hs9ycvv,t1_hs9wug3,"Thanks. I wanted to ask one more thing, are they related to compilers? The stuff you mentioned",1,0,0,False,False,False,1641943529.0
s14xir,hs9mht8,t1_hs8zuw3,"The latest errata to TAOCP was from 2021-12-24, ergo it's being worked on.

And even if it takes years then what? You suggesting we won't see it at all?  
JWST was supposed to launch in 2006, it got delayed (by lot), but we still got it.",2,0,0,False,False,False,1641938883.0
s14xir,hs9izu3,t1_hs9akcg,"No, I'm not. Unfortunately, I'm far from even dreaming about PhD",5,0,0,False,False,False,1641937576.0
s14xir,hsawj9y,t1_hs9ycvv,"Yea, the book Concrete Semantics is mostly about implementing a compiler (for a very simple language and a very simple target machine). The way this particular book goes about that is unique, in that it goes over how to prove that the compiler is correct in a proof assistant. This is something that isn't done in practice almost at all, but is very interesting in terms of understanding what a compiler is actually doing.

Coming up with a correctness statement is the best way to show that you really understand something.",2,0,0,False,False,False,1641958187.0
s14xir,hs9njbk,t1_hs9mht8,"Errata hardly counts as continuing a work in progress, it just means a reader sent in a correction. The difference between JWST and TAOCP is that there was no time limit on JWST - there is on TAOCP. When Knuth (who is 84 years old) is gone, that's it.
 
And that's even taking 4b as the conclusion, when the current plan has 4b, 4c, 4d, 5, and perhaps 6&7 also.",2,0,0,False,False,False,1641939284.0
s14xir,hs9j9mw,t1_hs9izu3,"Ah, well. Awesome comment/post anyways.",3,0,0,False,False,False,1641937677.0
s14xir,hs9pab9,t1_hs9njbk,Is there really such time limit? Robert Jordan died in 2007.  The Wheel of Time still got finished in 2013.,2,0,0,False,False,False,1641939949.0
s14xir,hs9q6p7,t1_hs9pab9,"Fiction is a different beast to non-fiction. Depending on the licence Knuth has with his publisher there may well be something called TAOCP that gets published, but if it's someone else writing it you might as well just grab any other book on the topics and slap that label on it.",2,0,0,False,False,False,1641940296.0
s0ficm,hs1ilik,t3_s0ficm,"This is amazing lol, especially the raspberry pi!",48,0,0,False,False,False,1641805836.0
s0ficm,hs1wdbe,t3_s0ficm,"Well, Mabrook, I wish them Success in Deen and Dunia as written on the floppy disc 💾",23,0,0,False,False,False,1641815905.0
s0ficm,hs1of4u,t3_s0ficm,Love it!   Congrats on the graduation.  These are the best kind of cookies a CS graduate could enjoy .,12,0,0,False,False,False,1641810255.0
s0ficm,hs1qcii,t3_s0ficm,This is so wholesome <3,7,0,0,False,False,False,1641811703.0
s0ficm,hs2tqmp,t3_s0ficm,"Ah, my favorite aspect of CS: uwu",19,0,0,False,False,False,1641831250.0
s0ficm,hs3xxlj,t3_s0ficm,"This is adorable!!! Congrats to your bro for graduating, this is awesome.",4,0,0,False,False,False,1641845847.0
s0ficm,hs3tne6,t3_s0ficm,"All really cool, but I do have one question. What is yay taco? Maybe I’m totally forgetting something. Once again great stuff!!",3,0,0,False,False,False,1641844274.0
s0ficm,hs3xkdq,t3_s0ficm,yay Taco? I’m graduating with a CS degree in May and I’ve never heard of this xD can someone educate me?,3,0,0,False,False,False,1641845711.0
s0ficm,hs4qk17,t3_s0ficm,Damn i thought the @ was a Debian logo... Really good job on everything! It looks great!,3,0,0,False,False,False,1641856685.0
s0ficm,hs2pcdu,t3_s0ficm,"That’s amazing, how come I wasn’t invited though. 😂",2,0,0,False,False,False,1641829593.0
s0ficm,hs46e6x,t3_s0ficm,Adorable!!! And congratulations 🎊🍾🎉,2,0,0,False,False,False,1641848961.0
s0ficm,hs4yk0m,t3_s0ficm,That is so awesome 😆❤️,2,0,0,False,False,False,1641860084.0
s0ficm,hs1pznd,t3_s0ficm,"Awesome, you're a good brother, I'm sure he'll be happy",4,0,0,False,False,False,1641811439.0
s0ficm,hs24sgi,t3_s0ficm,This is cursed,-3,1,0,False,False,False,1641820717.0
s0ficm,hs3uyvt,t3_s0ficm,send the location :D,1,0,0,False,False,False,1641844755.0
s0ficm,hs2p9c6,t3_s0ficm,"I was expecting computer science symbols to be like asymptomatic notation like Big Omega and boolean or bitwise operators, but very thoughtful nonetheless.",-6,0,0,False,True,False,1641829560.0
s0ficm,hs1zxdk,t1_hs1wdbe,"Ayooo, spot the Muslim. Thanks! I’ll let my brother know.",13,0,0,False,False,True,1641818076.0
s0ficm,hs520rd,t1_hs2tqmp,"Hahaha. Yeah thats that’s because my brother ironically uses that phrase, so much now that it’s unironic……",3,0,0,False,False,True,1641861536.0
s0ficm,hs51lsu,t1_hs3tne6,Hahah someone finally noticed. It’s my brothers nick name. Taco. And congrats taco was too hard to pipe. Same with tacos graduation. So yay taco it is,3,0,0,False,False,True,1641861361.0
s0ficm,hs51pop,t1_hs3xkdq,Hahah just my brothers nick name. No crazy new code here,2,0,0,False,False,True,1641861407.0
s0ficm,hs1zst1,t1_hs1pznd,"I’m a sister, a brother could never. 
My brother got me a dowel, bleach, and a sympathy card when I graduated nursing school. Hahahah each had a meaning.",17,0,0,False,False,True,1641818002.0
s0ficm,hs2hiev,t1_hs24sgi,What now why would you say that??,3,0,0,False,False,True,1641826461.0
s0ficm,hs4y8al,t1_hs24sgi,arraytest[3] on the yellow floppy disk is cursed af,3,0,0,False,False,False,1641859943.0
s0ficm,hs51mzk,t1_hs3uyvt,It passed 😅😅😅,1,0,0,False,False,True,1641861376.0
s0ficm,hs56nu3,t1_hs51lsu,Awesome thanks for the response!!,1,0,0,False,False,False,1641863474.0
s0ficm,hs20u1y,t1_hs1zst1,"Oh sorry about that, wish I had a sister like you! Have fun when you celebrate the graduation, cheers.",3,0,0,False,False,False,1641818598.0
s0ficm,hs2ihan,t1_hs2hiev,It's cute but I would hate it,-9,0,0,False,True,False,1641826859.0
s0ficm,hs67fwx,t1_hs4y8al,I was wondering if anyone else saw that,2,0,0,False,False,False,1641879332.0
s0ficm,hs21mka,t1_hs20u1y,Thanks!,2,0,0,False,False,True,1641819032.0
s0ficm,hs2oo7k,t1_hs2ihan,Hahah I mean I could see that. Because I was an outsider tryna make it CS themed….so it’s like super cheesy.,5,0,0,False,False,True,1641829336.0
s0ficm,hs39ejw,t1_hs2oo7k,"Hey, I’m from CS and I like it. Your brother is lucky 😀😀",13,0,0,False,False,False,1641836936.0
s0vdjp,hs70hb4,t3_s0vdjp,The math you need for data structures is discrete math. Set theory and graphs.,5,0,0,False,False,False,1641900483.0
s0vdjp,hs63wcr,t3_s0vdjp,"What – more specifically – are you having problems with?

Without knowing the above, have you studied much probability theory? This (and linear algebra which you mentioned) makeup an incredibly large amount of machine learning, which often comes as a not-too-convoluted application of these subjects.",3,0,0,False,False,False,1641877489.0
s0vdjp,hs7vqrn,t1_hs63wcr,"I have not studied probability theory yet

My issue is how I haven’t yet been able to actually “apply” any of these mathematic subjects in computer science yet, and idk when I should be learning to do so",2,0,0,False,False,True,1641915916.0
s0vdjp,hsoqhun,t1_hs7vqrn,"Grab Matlab and try to tweak a few pictures, you’ll be able to see linear algebra in action almost immediately lol.",2,0,0,False,False,False,1642198267.0
s0vdjp,hsoytoh,t1_hsoqhun,"I like this one, thanks fren",1,0,0,False,False,True,1642201690.0
s0jzi8,hs2xtap,t3_s0jzi8,"A few concepts:

Voltage: voltage is the electrical analog of pressure. It has a value for each point in 3D space, these values can change as functions of time. The voltage at a single point has no meaning on its own, it the difference of voltages between two points which has any meaning. 0v is usually defined as some arbitrary reference point which all other voltages will be compared to.

Node: a node is a continuous path of conductive material, for example a wire. It may branch off in multiple directions. All points on the same node are at the same voltage.

Power rails: two special nodes within the CPU would be the ground rail and the power rail. The ground rail would be the node which is defined as 0v. The power rail has some positive voltage such as 5v or 3.3v. These come from the power supply which will maintain the voltage difference between them. The voltages on these two rails are what defines “logic high” and “logic low”. These are distributed all throughout the CPU.

Transistor: a transistor is a 3 terminal device, meaning there are 3 pins which can each connect to different nodes. A voltage on the 1st pin controls whether the other 2 pins are electrically connected or not.

Logic gate: logic gates are special arrangements of transistors which perform logic operations on the voltages coming from their inputs, and set the voltages of their outputs accordingly. For example a not gate can be built with 2 transistors, one which will connect the output node to the positive power rail when the input node is low, and one which will connect the output node to the ground rail when the input node is high.

So to answer your question, 1s and 0s are voltages on wires. These voltages are set by electrically connecting the wires to the positive or negative terminals of the power supply. These connections are made with transistors, other signals on different nodes tell the transistors when to open or close. Logic is created with special arrangements of transistors.",10,0,0,False,False,False,1641832730.0
s0jzi8,hs2fnzq,t3_s0jzi8,"1s and 0s are just an abstraction humans use to do calculations. In terms of a transistor 1/ON means current can flow through the transistor. 0/OFF means current can not flow through the transistor. 
Now if you lined up a few transistors in various states like:
ON OFF OFF ON OFF ON OFF ON
that doesn't mean much to a human. So let's assign them 1s and 0s, that would look like: 
10010101
Now we have numbers. But that still doesn't mean much to an average person so let's convert them from base 2 (binary numbers) to base 10 ( 0-9 number system we learn in grade school).
I won't go into a full lesson on calculating binary but 10010101 = 149.
So we just used a little abstraction to make a group of 8 transitors represent the number 149!
Now if we keep adding transistors (bits) we can make all kinds of numbers to do all kinds of calculations with.
Hope that makes sense.",5,0,0,False,False,False,1641825715.0
s0jzi8,hs2g20w,t3_s0jzi8,"> isn’t voltage the electric tension between two points, which points would that be?

Personally, I like to think of voltage using the [waterfall analogy.](https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Supplemental_Modules_(Analytical_Chemistry\)/Electrochemistry/Voltage_Amperage_and_Resistance_Basics#:~:text=If%20we%20draw%20an%20analogy%20to%20a%20waterfall%2C%20the%20voltage%20would%20represent%20the%20height%20of%20the%20waterfall%3A%20the%20higher%20it%20is%2C%20the%20more%20potential%20energy%20the%20water%20has%20by%20virtue%20of%20its%20distance%20from%20the%20bottom%20of%20the%20falls%2C%20and%20the%20more%20energy%20it%20will%20possess%20as%20it%20hits%20the%20bottom.)

> I have always imagined the 1s and 0s or asserted and deasserted signals as high and low amperage, is that wrong?

Amps and volts are different. Amps are the amount of water going over the waterfall, volts are the height of the waterfall (the potential energy). When the external clock ""drives"" the CPU, it alternates voltages. Electrical engineers look at [eye diagrams](https://en.wikipedia.org/wiki/Eye_pattern) to assess the quality of binary signals. 

> I‘m really confused by this, some say 1 and 0 is the state of the transistor but that doesn’t make sense to me.

You should watch [this video](https://youtu.be/Hi7rK0hZnfc) on sequential logic. However, digital logic design is an art unto itself.",1,0,0,False,False,False,1641825874.0
s0jzi8,hs4psv2,t3_s0jzi8,"Here are some excellent resources

[Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)

[Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)

Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share)

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",1,0,0,False,False,False,1641856372.0
s0jzi8,hsgb7ob,t3_s0jzi8,1s and 0s can be anything that is detectable in multiple states by a single detector.,1,0,0,False,False,False,1642052177.0
s0jzi8,hs3h834,t1_hs2xtap,"Thank you very much.

So would you agree that it is wrong to say that the cpu represents 1s and 0s with turned on and turned off transistors? And that it’s also wrong to say that 1 means there is electrical current and 0 means there is no current?

Edit: Also is it correct to say that a logically high (1) wire has a high voltage relative to ground? And a logically low (0) wire has basically no voltage relative to ground?",3,0,0,False,False,True,1641839761.0
s0jzi8,hs2ga3m,t1_hs2fnzq,Yes but where in an actual circuit would you line up transistors like that to represent a binary number? This doesn’t really make sense to me. Sequential logic (memory circuits) sure don’t work this way. And I don’t think combinational logic does either,2,0,0,False,False,True,1641825965.0
s0jzi8,hs2gkp5,t1_hs2g20w,"What is meant by „power“ in that eye pattern? And I understand what voltage and amperage is, but which one is interpreted as 1 and 0?

Edit: Also those combinational logic circuits don’t really use „transistor is on“ as 1 and „transistor off“ as 0 as far as I understand",1,0,0,False,False,True,1641826084.0
s0jzi8,hs6a30g,t1_hs4psv2,I am reading Code now and I wish I would have read it 20 years ago when it came out. Such a wonderful book. It explains exactly what OP wants to know.,1,0,0,False,False,False,1641880836.0
s0jzi8,hs4h0vz,t1_hs3h834,Yes you have it now.,3,0,0,False,False,False,1641852901.0
s0jzi8,hs4uk4w,t1_hs3h834,"Your edit is exactly correct.

However no current actually has to flow, it’s just a static voltage difference. With the exception of during the time when a wire is changing voltage, at the physics level charge is the source of voltage, so to change the voltage on a wire a small amount of charge must flow onto the wire. However this is an unwanted but physically unavoidable property of the wire. So at the beginning of a clock cycle if a wire is changing voltage then a small current flows onto the wire through a transistor, but during the bulk of the clock cycle there are no currents flowing.",3,0,0,False,False,False,1641858378.0
s0jzi8,hs4juqn,t1_hs3h834,"At the lowest level yes. The reason for 1,0 is transistor in open or closed.

Transistor level is pretty far into EE, so you wont find as much good answers here, but ill try, ill say some wrong things for simplicity sake - it is very complicated when you look at the low level.

We discuss in an open/closed or 1/0 due to the phyisical properties of the trasistor, control signal (or gate voltage) above some theashold (that depends on the doping and process) you can allow voltage and below it you can not.

So assuming your threashold is 0.7V, having a gate voltage of 0.2V would be the same as 0. This lets us discuss in a resulutions of 0 and 1.

A single transistor is not a ""bit"" though, but a combination of transistors (and other linear element) in the end forms some sort of memory element. This element might be an input for a different transistor and so on, so its important to talk about that in an 0/q abstraction as well.

Having a base 3 processor can work however, using a very carefully tuned transistors and logic levels - but its completely usless due to the immense amount of drawbacks.",2,0,0,False,False,False,1641853992.0
s0jzi8,hs2iacg,t1_hs2ga3m,"> where in an actual circuit would you line up transistors like that to represent a binary number?

In the CPU, they aren't ""lined up"" (deserialized), they're ""sideways"" (serialized). So a whole, say, 8 bit operand is operated on at once.",1,0,0,False,False,False,1641826781.0
s0jzi8,hs69dos,t1_hs4uk4w,Oh that makes perfectly sense thank you!,2,0,0,False,False,True,1641880421.0
s0jzi8,hs2imzf,t1_hs2iacg,"Okay but for example with an ALU you have way more transistors than bits in the operands, and the transistors don’t really all represent a bit do they?",1,0,0,False,False,True,1641826925.0
s0jzi8,hs32sam,t1_hs2imzf,"> Okay but for example with an ALU you have way more transistors than bits in the operands, and the transistors don’t really all represent a bit do they?

Well no, because by themselves transistors don't do anything, not even store values. To do that, you need to combine transistors with each other (and resistors, diodes, capacitors, ..). To do this you build gates and latches that are represented as single black boxes, but are made up of multiple components. With these abstracted components you can then build logic circuits. [Here's](https://www.cs.bu.edu/~best/courses/modules/Transistors2Gates/) a website I googled that shows how to build gates from transistors (this is already a bit abstracted because if you want to build f.e. an AND gate from transistors you need resistors, too). [This](https://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) is a book that builds a computer from first principles.",2,0,0,False,False,False,1641834531.0
s0jzi8,hs3geyi,t1_hs32sam,"Yes I get that but my point was just, someone said that we can assign bits to the transistors and that the computer represents 1s and 0s with on and off transistors. And I still think that transistors aren’t representing any numbers, the numbers are some electricity or voltage or whatever going through a wire.

Also to be pedantic transistors obviously do do something on their own but you know that",2,0,0,False,False,True,1641839467.0
s0jzi8,hs3pka8,t1_hs3geyi,"> Yes I get that but my point was just, someone said that we can assign bits to the transistors and that the computer represents 1s and 0s with on and off transistors. 

So without going *too* far, there's levels of abstraction of knowledge. This is a point I think Carl Sagan once famously made when he discussed the answer to the question ""why is ice slippery?"" You can look at a computer at a very low level, for example the level of voltage and current changes in transistors. That is what you have chosen to do, but that decision is arbitrary. It's probably the least abstract level you are comfortable with, but there's no reason to not go deeper and talk about Maxwell and field theory and perhaps quantum mechanics. I don't know, I'm not a physicist. But yes, you are correct when you say that ""transistors represent 1s and 0s"" is false on this level of resolution. But it's a *useful* abstraction. If you actually look at a processor (or any integrated circuits), and you keep zooming in, you eventually end up with transistors so tiny you could fit thousands of them into the cross-section of a human hair. And of course those transistors aren't just connected in series, they are instead configured in latches and gates, in multiplexers, and so on. What ""transistor"" in colloquial speech *means* is really dependent on what you are considering. When somebody says ""transistors represent 1s and 0s"", what they kinda mean is that there are tightly integrated MOSFETs (and other elements) on a chip that are arranged into latches that store state as ""current"" or ""no current"", which can be manipulated by tightly integrated MOSFETs (and other elements) on a chip that are arranged into logic gates and the like. And that's all incorrect and just a different level of abstraction (because what on earth is a ""transistor"" if not a human abstraction over what happens if certain materials are put in very specific configurations within very specific electro-magnetic fields, and what is a ""material"", and so on), so we just say ""transistor"".

> And I still think that transistors aren’t representing any numbers, the numbers are some electricity or voltage or whatever going through a wire.

That's functionally the same thing. They are ""representing"" a bit in that current going through a electro-magnetic field projecting from a wire is understood to encode ""1"", and no such current encodes ""0"". 

I actually don't know what else to say, because I don't think you actually have an issue with that, or at least I hope so. Because otherwise, if you are tasked with giving somebody 2 apples, you would have to burst into tears because how can apples be represented by a number and what is counting anyway?",5,0,0,False,False,False,1641842772.0
s0jzi8,hs3rp51,t1_hs3geyi,"Deleted this original reply cuz QuietLikeScience summed it up way better than me first. 

That said,  If you're interested I highly recommend watching the Crash Course Computer Science series on YouTube.  The first few episodes covers this stuff and may shed some light for you.",2,0,0,False,False,False,1641843552.0
s0jzi8,hs3ufdr,t1_hs3pka8,"Thank you very much for that answer!

> I actually don't know what else to say, because I don't think you actually have an issue with that, or at least I hope so. Because otherwise, if you are tasked with giving somebody 2 apples, you would have to burst into tears because how can apples be represented by a number and what is counting anyway?

Lol. Don’t worry I‘m gonna stop asking now. I just want to understand this stuff at an actual physical level",1,0,0,False,False,True,1641844556.0
s0jzi8,hs3ulk9,t1_hs3rp51,"Thanks, I‘ve seen those, they are pretty well made however they don’t go as in depth as I want to understand it lol",1,0,0,False,False,True,1641844620.0
s01ijj,hryszds,t3_s01ijj,Mass surveillance and facial recognition,24,0,0,False,False,False,1641762565.0
s01ijj,hryvu9c,t3_s01ijj,Can an advanced AI be considered someone's property?,17,0,0,False,False,False,1641763613.0
s01ijj,hrzjrdu,t3_s01ijj,"If you ask a student to multiply two three-digit numbers, most people would agree that they have to ""think"" to produce an outcome. However, if you write a simple program for a computer to multiply two three-digit numbers, most people would argue against the computer doing any sort of ""thinking"".

So what can we use to define ""thinking"" (i.e. ""intelligence"") if it isn't based solely upon the outcome?",7,0,0,False,False,False,1641772328.0
s01ijj,hryybso,t3_s01ijj,"Should decisions of an AI be explainable/interpretable in life or death situations (e.g. medical/military)? What about if it makes better decisions than humans?

How would you know for sure when AI has consciousness? When does it acquire the same rights as humans or animals?

Should AI in social media that tries to feed you maximally addictive content be restricted?

Should people whose jobs are automated by AI have a right to get universal basic income?",6,0,0,False,False,False,1641764488.0
s01ijj,hrz1s4r,t3_s01ijj,"I major in computer science and for the computer ethics course I took we debated whether it was ethical to do mass surveillance of students social media posts to track for concerning behavior that could lead to something tragic such as a school shooting (we had learned that many school shooters had posted on social media concerning content). If you are interested in learning more feel free to check out this article on one of the PhD students of the professor who taught my computer ethics course.

https://engineering.lehigh.edu/news/article/online-privacy-algorithms-we-trust",7,0,0,False,False,False,1641765716.0
s01ijj,hrz1qau,t3_s01ijj,"What is the meaning of intelligence? What specific character does an ""advanced"" Aritificial Intelligence supposed to have such that it would be considered on the same level as a thinking human?",3,0,0,False,False,False,1641765697.0
s01ijj,hrz33xr,t3_s01ijj,"If you could have your English essay graded by a human or by an AI that has been shown to give correct results with reasonable accuracy, which should you choose?

If a car accident results in a death, does it matter whether the car at fault was a human or an AI? How can such a situation be handled in terms of holding the correct party responsible for damages?",2,0,0,False,False,False,1641766192.0
s01ijj,hrzbyjg,t3_s01ijj,"Who is liable if AI makes a mistake in medicine / self driving cars?

Is the developer liable? Data collector? Etc.",2,0,0,False,False,False,1641769410.0
s01ijj,hrzfvcz,t3_s01ijj,"How do we ensure that our AI reflects human ethics? How do we make sure that algorithms do not reflect patterns such as racism or sexism in terms of recommendations or referrals? (Financial scoring, resume review, etc)

Is it possible for AI to become equal to or better than people at tasks that people perform? If so, how should society address this issue? Is it possible for AI to take over ALL jobs?

What are the ethical differences between AI and natural intelligence? Are there any intrinsic differences between AI and natural intelligence and why? How can laws reflect this?",2,0,0,False,False,False,1641770871.0
s01ijj,hrz6lq2,t3_s01ijj,"Define Artificial

Define Intelligence

Define Computer 

Define Human",2,0,0,False,False,False,1641767447.0
s01ijj,hrzxrxq,t3_s01ijj,"You haven’t given us the objective you’re hoping to achieve. If it’s simply, “Get them to think about these things.” I don’t think you’d need to ask Reddit; a simple Google search would do fine.

It truly depends on what you’ve taught your course as most debates seem to tend towards settling into an argument to find consensus over definitions. 

What have you defined to them as artificial intelligence? And what do you define as “advanced?” What characteristics or traits would an “advanced AI” have from your perspective?

Russell and Norvig define “artificial intelligence” as acting rationally, but that seems to omit other human characteristics such as empathy, selflessness, generosity, etc.

Overall, what are you hoping to achieve with debate topics?",1,0,0,False,False,False,1641777740.0
s01ijj,hrz1qeg,t3_s01ijj,"Let's say we live in a world where most work is done and managed by AI. AI controls machines/processes to mine for physical resources, build products with them, and handle logistics. All work is done by AI, except for government/ethical/ high-level decision-making stuff.

How should the benefits of that system be distributed (profit, resource wealth, product ownership)? Who ""owns"" the software assets (including resource production and product creation? Who owns ""data""? Do tech people become the new elites?

This can parallel lots of similar debates around say land property rights, the right to catch your own rainwater, the right to privacy/data, etc. in the realm of ""ownership"".

Also, if AI are treated as pets, how should we deal with ""animal abuse"" (hacking ddos stuff)? Should it fall under the same realm as cyberlaw, or is there some kind of human/animal rights situation happening?",1,0,0,False,False,False,1641765698.0
s01ijj,hrz35hw,t3_s01ijj,"What is thought without emotion?   


this could be expanded to how much of human decision making is done with instincts and how much is logic, and whether that's a helpful for us in our dealings with each other, or whether an AI would be better suited for things like governance/law etc. because it's not burdened with emotions OR are those emotions the thing that keep this from falling apart completely? 

I've always wondered how much we're thinking based on what we know to be true and how much is based on an unknowable predictive engine we've developed over evolutionary time (clearly, dreaming has some element of simulation to it; could dreams not be training scenarios?). Seems like you could almost explain the anti-vax phenomenon as an expression of emotional/gut thinking vs logic and data.

Lots of directions to take this",1,0,0,False,False,False,1641766207.0
s01ijj,hrzhkjc,t3_s01ijj,"The 2021 Reith Lectures were on the advancement if A.I. There is plenty to go on with those if you are in the UK and have a TV licence.
[BBC Reith Lectures page](https://www.bbc.co.uk/programmes/b00729d9/episodes/player)",1,0,0,False,False,False,1641771513.0
s01ijj,hs0s6w1,t3_s01ijj,"My major field of publication is in computational creativity: AI that exhibit behavior that would otherwise be deemed as creative in humans. (Art, music, jokes, architecture design, etc.)

If you check ICCC, you can find a good amount of philosophy on which to base topic. One HS-level topic may be along the lines of, ""Is creative AI a threat to human artistic expression/culture?"" (I.e. if an AI can potentially learn to optimize and make the best music, jokes, and paintings, is there room for humans? What if we get pushed out of the space? If the billboard hits are all written by machines, will there be a major fault in human cultural progression?) (Kind of like how the world champion of Go lost to Google's AlphaGo and quit the game because there was no point in competing anymore.)",1,0,0,False,False,False,1641790332.0
s01ijj,hs0xu9y,t3_s01ijj,"Lots for AI driving. Whether giving up your right to drive is good? It would eliminate more casualties but the future casualties would be random instead of bad drivers. Also, how do you value life if a schoolbus is going to hit another car. Should the other car swerve killing itself to save the children or should the car do its best to save itself?",1,0,0,False,False,False,1641792761.0
s01ijj,hs0yn7b,t3_s01ijj,"This is all my perspective.

Same rights as humans? depends.

Intelligence of solving problems doesn't necessarily emotional intelligence if the A. I don't have what we considered ""will"" then why would we give rights?",1,0,0,False,False,False,1641793165.0
s01ijj,hs1029u,t3_s01ijj,"If personal or sensitive data (like, say, surveillance video of private moments in a home) are only screened for illegal activity by an AI, should this be considered an invasion of privacy? (Consider the number of cameras in the home already, like laptop cameras and phone cameras and the like, which theoretically could be recording but then ignoring the data. At what point does this become an invasion of privacy? When the data is processed? When it is screened by a computer? I mean, clearly when a human gets involved it is an invasion of privacy, but if only machines see the data?)",1,0,0,False,False,False,1641793901.0
s01ijj,hs11w8u,t3_s01ijj,"Resolved: Humanity must prioritize the development of super-intelligent AI so that it can be used to solve world problems (hunger, global warming, economic inequality, etc.).

Resolved: A super intelligent AI would make for a better government leader than a republic.

Resolved: Jobs involving repetitive and mechanical tasks should be replaced by AI/robotic systems without regard to unemployment.

Resolved: A minimum wage should be established for robot workers in order to collect taxes and fund universal basic income.",1,0,0,False,False,False,1641794894.0
s01ijj,hs1c5p1,t3_s01ijj,"* Virtual assistant giving advice, especially when we don't yet have technology to teach it which advice is allowed and which is not (e.g. Alexa telling the child to stick fingers into electric outlet)
* Impersonating yourself with an AI to teachers/colleagues/friends/relatives without telling them, e.g. voicemail bots
* Online presence-based background checks (e.g. refused a job or school/college admission due to online posts)
* Creating and publishing deepfakes of real people—joke causing unintended consequences, revenge, dirty political competition or propaganda
* Teaching machine learning to synthesize infinite paintings in the painter's unique style, rendering the painter unneeded and uncompensated

To OP: try to avoid banal daydreamer fantasies like human-level AIs? We don't know if it will take another 2000 years to get to that stage. There are real problems banging on the door.",1,0,0,False,False,False,1641801180.0
s01ijj,hs1qqkj,t3_s01ijj,A.I. taking out more jobs than any technology in history within the next century.,1,0,0,False,False,False,1641811990.0
s01ijj,hs390v9,t3_s01ijj,"If any justice system decides to use AI: what safeguards, if any, should there be to protect against that AI discriminating against certain groups (ie male/female, black/white)?",1,0,0,False,False,False,1641836799.0
s01ijj,hsgjejl,t3_s01ijj,Can a service provider collect store and sell an arbitrarily large amount of generally unspecified personalized data in payment for a finite service provided?,1,0,0,False,False,False,1642057426.0
s01ijj,hs1pjq1,t1_hryszds,"Several US law enforcement offices have been accused of using facial recognition with false matches as criminal evidence. Many of these algorithms have a noticeable racial bias.

https://www.nytimes.com/2020/01/12/technology/facial-recognition-police.amp.html",1,0,0,False,False,False,1641811110.0
s01ijj,hryxqox,t1_hryvu9c,"This just reminded me of the Star Trek TNG episode where Data is on trial and the questions is does he have rights as a person, or is he the property of Starfleet.  


Maybe I'll show that episode to my students at the start of the debate unit. lol",8,0,0,False,False,True,1641764282.0
s01ijj,hrznjr1,t1_hrzjrdu,I disagree.  Humans would just follow the algorithm they were taught in school.  Thinking would be required if you were to explain the algorithm or your own algorithm.,1,0,0,False,False,False,1641773754.0
s01ijj,hrzn53o,t1_hrz6lq2,Human and computer have solid scientific definitions.,1,0,0,False,False,False,1641773602.0
s01ijj,hs1e7fm,t1_hrzn53o,"Computer used to refer to a person computing (i.e. with a pen and paper). Human has a species definition, but this is not the only definition - though perhaps 'person' could be used here instead.
 
These aren't good debate topic statements though, they are questions. A topic statement gives a position that can be argued for and against.",1,0,0,False,False,False,1641802599.0
s01ijj,hs1mwqy,t1_hrzn53o,"I think it's still an interesting activity to write down and analyse the implications and contradictions between these definitions

Corolary arguments which arise could be ""to what extent can a human/ person bequeath their personality/ goals/ rights to a machine/ computer""",1,0,0,False,False,False,1641809123.0
rzqau2,hrxwzsp,t3_rzqau2,"As someone who studied cognitive science, computer science is RIPE with meaning. Theories of computation have deep implications for how we understand the mind and it’s functions (computational theory of mind). Computer science also has major implications on the field of mathematics. Godel with the help of Turings work on computation (early computer science) have proven that there are mathematical truths which are NOT provable. Some crazy ass shit. Not sure if you’d learn any of this in a CS class. I learned all of this in philosophy classes. Try and compliment CS with Philosophy and I promise you will find the field ripe with meaning.",52,0,0,False,False,False,1641751311.0
rzqau2,hrwyqga,t3_rzqau2,"Sounds like you should be taking philosophy but realistically no job serves a purpose other than income, and if you're lucky, entertainment. Some people are just wired in such a way that they enjoy this subject more than others, and usually its to do with the fact that computing is very logical and simple, but very powerful.",61,0,0,False,False,False,1641737617.0
rzqau2,hrws7m5,t3_rzqau2,Because it's fun..? Not really sure what you're talking about.,29,0,0,False,False,False,1641734201.0
rzqau2,hrwovdp,t3_rzqau2,There is no meaning and no purpose...,42,0,0,False,False,False,1641732228.0
rzqau2,hrxh1h4,t3_rzqau2,"There is no meaning or purpose to computer science, except what you want to attach to it.

In that sense, it's the same as everything else in life.

You can use your knowledge of computer science to earn yourself a decent income. And/or you can use it to make the world a better place for all. Nothing wrong with either and it's all up to you.",5,0,0,False,False,False,1641745338.0
rzqau2,hrxl34h,t3_rzqau2,The purpose of computer science is problem solving and developing your problem solving skills.,12,0,0,False,False,False,1641746868.0
rzqau2,hryo0jb,t3_rzqau2,"This is a discussion I was having with one of my mates that I studied computer science with at university. There's a massive misconception in our industry that CompSci == coding, when they are nowhere near the same thing. Coding is the practical application of writing software, whereas CompSci is the academic study of computers. That's why they teach aspects such as hardware, networking, logic etc. Lots of people go into CompSci thinking their going to learn how to code, when they're really not going to.

To answer your question about purpose is therefore a two part answer:  
\- Coding - the purpose of writing software is to a) automate processes to make people's lives easier/quick, b) to further humanity (see uses in medicine, climate science and humanitarian work etc), and c) most importantly, to make money

\- Computer science - the same as every science and academic study area, to learn more about the area and to further it. Computer Scientists study the world as it is to find more efficient ways to process data, calculate results, and generally just ""do stuff better"".",3,0,0,False,False,False,1641760826.0
rzqau2,hrx0pdy,t3_rzqau2,"What was your meaning/purpose in enrolling in a CS degree program? If you can't answer that question with an answer more meaningful than to make money then maybe you should figure out what you want to do with your life that will give it meaning before you waste anymore time, energy, and possibly a lot of money.",10,0,0,False,False,False,1641738576.0
rzqau2,hrwsoif,t3_rzqau2,"You learn how to use computers. Computers help automate everything.

If you don’t understand everything that we need to do in life to survive/grow etc that is another topic.",12,0,0,False,False,False,1641734468.0
rzqau2,hrx65dl,t3_rzqau2,"Theoretical computer science is basically the development of theories on computing (logic gates, programming, data management etc.) - it’s basically trying to come up with new ways of doing things so that programmers (computer engineers) or hardware engineers can implement your theories and presumably benefit society with more efficient ways of doing things (algorithms) in this realm.

It’s downstream from things like pure logic, cryptography and mathematics but not quite down to systems administration or business logic programming.",4,0,0,False,False,False,1641741025.0
rzqau2,hryzoo6,t3_rzqau2,"There is no purpose and, even worse, there's no future in computer science. Sounds crazy, right? Technology IS the future... if, and only if, there's power... and a supply chain that can develop it but the real hole in the basket of the future of this lifestyle we've created is the need for reliable grid power in a time where the weather is getting predictably worse. 

If you're keen on data, you could get into climate modeling. Plenty of meaning to be found in that, but probably only employment in the insurance industry (if you're after the money). 

All your life you've been told that coding is a necessary skill when the people telling you that didn't think to make the power infrastructure well enough that it can be relied on in the future. 

The secret to all of this is that the only meaning/purpose anyone's life has is to consume resources as quickly as possible, to generate income, to maintain a standard of living that's fundamentally unsustainable for even 1/10th of our population. Your lifestyle is intentionally unsustainable because that's where the wealth is generated; in the burning of resources. 

Now, go spend some money and the rest of the afternoon wondering what the meaning of any of this is beyond acquiring more/better and why that's worth your short lifespan, especially considering that most resources can only be used once and that your carbon footprint will be changing conditions on this planet for a thousand years after you die. This is the machine we built. This is what you've spent your life studying to be a part of. Now it's up to you to choose something else that doesn't make the world worse just so you can have nice things.

Even the people down voting this have to admit that this cannot continue and will either change or run out of the resources it needs to sustain this level of growth. It's basic math. there isn't enough for us to live this way, so we either stop and do something else or we wait for it to break and have nothing else to fall back on. Those are the only options. This ship is literally sinking into the ocean (check out the update on the Thwaites Glacier and notice the trend that every time they check it, it's further along in its collapse than they expect)",5,0,0,False,False,False,1641764969.0
rzqau2,hry3zd7,t3_rzqau2,"I love this question. I do like coding but I think I like ""computer science"" more. Computer Science for me has to do with coming up with ways to COMPUTE mathematical functions and that for me, at least, is the meaning. When you program a program that can differentiate an equation, you know you're making something that has and will help in the human evolution at least in the mathematical sense. If you get what I mean-",2,0,0,False,False,False,1641753786.0
rzqau2,hrybmbl,t3_rzqau2,Why do you assume there is one?,2,0,0,False,False,False,1641756451.0
rzqau2,hryi6l5,t3_rzqau2,"I type the words, the things happening, the direct deposits hit.",2,0,0,False,False,False,1641758783.0
rzqau2,hrysk3k,t3_rzqau2,"You can make anything you want, and there’s more tools available to do so than at any point yet. Go make the world better! Build a company, a tool, a game, something.",2,0,0,False,False,False,1641762414.0
rzqau2,hrz9nt4,t3_rzqau2,"The ""meaning of life"" is too abstract a concept, it should be posted on a philosophy sub.

As for CS' ""purpose,"" you should ask yourself how the ""man made"" fields of study came to be. For instance, was math invented or discovered? The general consensus seems to be that it is both; math is the quantification of naturally occurring phenomenon, but requires the invention of a proof (or something along these lines).

In the same vein, electrical engineering is the demonstration of man's mastery of a natural phenomenon (the flow of electrons). Modern computer science has grown out of electrical engineering by combining the ideas of information theory with electrical engineering. 

Now we are seeing the next evolution of this cycle; as computers become fast enough, we see blooming fields like data science and machine learning, that add more abstraction layers on top of CS that allow humans to model complex adaptive systems and gain further insights into the world around us. Basically, as new technologies are discovered, we (humans) gain greater insights into the world around us and gain progressively more tools to model those systems and look past their face value.

So there isn't really a ""purpose"" beyond providing humanity further insights into the world around us, and increasing our collective understanding. This isn't a ""meaning of life"" but the natural human inclination towards adventure and discovery is a pretty noble pursuit imo.",2,0,0,False,False,False,1641768561.0
rzqau2,hrxn36d,t3_rzqau2,"For the purpose and meaning of it all, please see: [https://youtu.be/pneBKFjxInQ](https://youtu.be/pneBKFjxInQ)",1,0,0,False,False,False,1641747626.0
rzqau2,hrxqobu,t3_rzqau2,"CE student, here. I look at ourselves as a sort of digital version of old-style craftsmen. I love to have the possibility to wake up with an idea and to be able to create it from scratch. That is for what concerns the hobby. About the job career, instead, I can tell you that IT is one of the most potential fields nowadays.",1,0,0,False,False,False,1641748977.0
rzqau2,hry086l,t3_rzqau2,The meaning of life is to procreate so the human race can live on. Computers help in this ongoing struggle by delivering porn (knowledge) and dating apps (action).,1,0,0,False,False,False,1641752472.0
rzqau2,hrypkoq,t3_rzqau2,"I can't help but enjoy and attempt to answer such a question! I expect you'll get a lot of interesting answers.

It's pretty simple for me: I like video games (playing and making them), I like robots and I am a huge Sci Fi nerd. Studying CS for me is just studying what some of my favorite things have in common.

For those with a philosophical bent (of which I'm one), the fact that you can simulate things with computers is kind of a big deal. You make models in almost every domain. It's one of the most accessible ways to start tackling big, interesting questions. 

Zooming out further, computers are the latest in a long line of metacognitive tools. A ""Tablet"" is a Clay Tablet's spiritual successor, quite literally and deliberately. Personally, I feel that the tools we interact with are a big part of the human experience and it's neat to be at the fore-front of things. People making the first stone tools might have felt similarly empowered. I like to think that we are still at the front of the computer revolution and that there is a lot of miniaturizing and normalizing left to do. I'd like to ride my bike out into the woods, turn on my AR device, and suddenly have my home office all around me including access to the internet and haptic feedback, and that kind of thing. Right now being a computer person is often like being a person of two worlds (the person at the computer, and the person away from the computer) but I think the technology's logical extreme is a paradigm where you can do computing anywhere and any time, with few restrictions, combining the mobility of ""away from computer"" with the power of ""at the computer"" and applying this combination to every day life. It is really thanks to smartphones and tablets that things are moving in that direction. What is the next step? It's hard not to get excited about the frontier of metacognition. People must have been similarly excited about paper and ink, scrolls and notepads. 

Personally, I want to see what happens when we are applying these fundamentals not only to computers, but to cells. Cells are in theory programmable and certain principles of computing carry over. The body and mind hint at all kinds of systems and properties we could be using for other things. What will a cybernetic future with biotech look like? I really want to know. I think it could be really awesome.",1,0,0,False,False,False,1641761373.0
rzqau2,hrytn61,t3_rzqau2,"Because I enjoy it, you should do or study what you enjoy. And you can help people! Make their life easier by automation repatitve and boring task :)",1,0,0,False,False,False,1641762803.0
rzqau2,hrzkwk8,t3_rzqau2,"While in school, one of my professors told me that Computer Science is ""the study of what can be computed"".",1,0,0,False,False,False,1641772754.0
rzqau2,hrztmd5,t3_rzqau2,"It got me a job as a software developer and nearly doubled my income. Prior to this, I was a dance teacher - a career I was passionate about, until I realized I’d never be able to afford health care or save for retirement. I still dance, now just as a hobby.",1,0,0,False,False,False,1641776058.0
rzqau2,hs02rj4,t3_rzqau2,"Computer science is problem solving, you’ll learn how to apply mathematics, logic, and other concepts to solve complex/abstract problems that people working solutions to!",1,0,0,False,False,False,1641779750.0
rzqau2,hs0w1ao,t3_rzqau2,"imo cs strengthens your problem solving skills. when you get good at problem solving, you can choose what problems you wish to solve. none of them ever have to deal with cs.",1,0,0,False,False,False,1641791936.0
rzqau2,hs14ep3,t3_rzqau2,The way ive always interpreted and explained CS is as an umbrella term. It holds a ton of different concepts in it; everything from cyber security to mathematical theoreticals. Id say a majority of CS majors in college thought it was just programming until they did more research and realized it was so much more.,1,0,0,False,False,False,1641796339.0
rzqau2,hs14qiy,t3_rzqau2,"It sounds like need some Albert Camus, if you're not already aware of him.",1,0,0,False,False,False,1641796532.0
rzqau2,hs1gyyd,t3_rzqau2,You can be useful to the world in a meaningful way using computer science since everything is digital nowadays. With tech you can help humanity propel towards its future.,1,0,0,False,False,False,1641804613.0
rzqau2,hs1ha36,t3_rzqau2,"There is no meaning in life. Well, other then whatever meaning you give it. 

Some people choose religion as it's comfortable for them, some find other things like helping others, understanding the world, getting the most out of the 80 or so years one has (also I'm not saying these things are mutually exclusive)

I like CS, it's interesting. But I also use it as a vessel to get a job, or do other stuff.

 I'm mainly interested in game design and development but I also try to learn how game engines work, or make rudimentary ones in my own just to learn how they are engineered, the structure, the math, the complexity. 

At the end of the day, things only having as much meaning as you give to them.",1,0,0,False,False,False,1641804847.0
rzqau2,hs1hb0o,t3_rzqau2,"For me computer science is part of mathematics. It helps us to describe the world we live in, it is a powerful tool for other sciences as well such as physics etc., where many calculations are done thanks to computers and so the whole field of computer science for me is about “automating” tasks and describing world around us using different perspective, similarly as other parts of science do. (Yes I know the debate on wether mathematics and compsci are actually a science or not, but that’s not my point here).",1,0,0,False,False,False,1641804866.0
rzqau2,hrwulw0,t3_rzqau2,"Computer science is the study of computers. 


For example:


- what can be automated using computers and how to do it as effectively as possible

- how to connect computers, servers, embedded systems, iot devices and mobilephones into something greater than the sum of their parts.

- making meaningful and valuable conclusions from big data, for advertisers and city planners for example.

- how to ""bring to life"" the gadgets and devices created by those in the information technology industry.



As for the purpose in life question, that is retarded. There is no objective purpose in life other than any subjective purpose you come up with.

Which is to say, why does anyone do anything? For the sheer boredom of course, to distract themselves of the fact that there truly is no meaning.




Also because they find it fun. Why does everything need to have a purpose or meaning?",-4,0,0,False,False,False,1641735527.0
rzqau2,hrxehec,t3_rzqau2,You're wasting your education.,-4,0,0,False,False,False,1641744345.0
rzqau2,hrz3sac,t1_hrxwzsp,"I'm a big fan of the work of Douglas Hoffstadter - he's written quite beautifully on the implications of Kurt Godels proof for computation and consciousness (GEB / I am a Strange Loop).

I studied some Bert Russell and Godel during my Maths Bsc. Im currently looking to start a master's in computer science this year as I'd like to learn more about how recursion and ""Godel loopiness"" is implemented to make computers. 

What I find interesting about Godel and Turings theorems is that they seem to suggest that computation and logical experiments will always be at the 'cutting edge' of human knowledge (well at least that's my very amateur interpretation and expression. I'm looking forward to developing a more sophisticated appreciation)",10,0,0,False,False,False,1641766433.0
rzqau2,hs0a0iw,t1_hrxwzsp,"I'm a CS undergraduate too, I want to pursue a Computational Psychology or Philosophical masters after my degree. But I find myself utterly confused when I go looking online. Can you recommend something/give a bit of guidance?",3,0,0,False,False,False,1641782672.0
rzqau2,hs1lrys,t1_hrxwzsp,"I might actually be interested in Cognitive Science but when I looked it up online cognitive science seems to be more about psychology, neuroscience, a bit of philosophy, but actually very little Computer Science or Mathematics in it… 
So I don’t know about it…",2,0,0,False,False,True,1641808260.0
rzqau2,hryeiub,t1_hrwyqga,Guess I’m weird. I actually enjoy this,7,0,0,False,False,False,1641757482.0
rzqau2,hry6e1v,t1_hrwovdp,lol same,5,0,0,False,False,False,1641754625.0
rzqau2,hrzo5tz,t1_hrwovdp,Sure there is.  There may not be any inherent meaning but meaning can most definitely be created.,3,0,0,False,False,False,1641773987.0
rzqau2,hs00zgo,t1_hrwovdp,that'll last you like 2 seconds before your brain starts flooding itself with things it needs to think about.,1,0,0,False,False,False,1641779039.0
rzqau2,hs1x0s2,t1_hrxl34h,"I believe that's Engineering, not just Computer Science.",3,0,0,False,False,False,1641816304.0
rzqau2,hs00xsb,t1_hrx0pdy,"Why? Making money is a legit reason to study CS.

A career doesn't need to be the thing to supply your life meaning.",2,0,0,False,False,False,1641779021.0
rzqau2,hrxovwk,t1_hrwsoif,">You learn how to use computers.

Computer science is not about how to use computers.",10,0,0,False,False,False,1641748307.0
rzqau2,hrx482x,t1_hrwulw0,Computer science is not the study of computers.,4,0,0,False,False,False,1641740191.0
rzqau2,hrz4xdl,t1_hrz3sac,"The lines between ""cognitive science"" ""computer science"" ""mathematics"" ""language"" and ""philosophy"" blur quick 

It's a very meaningful field of study",5,0,0,False,False,False,1641766838.0
rzqau2,hs34eqm,t1_hs0a0iw,"I'm not too sure what guidance you're looking for and i'm certainly not qualified to give much out. I was a pretty mediocre maths student (got a 2:1 from a Russel Group uni in the UK 5 years ago) and haven't formally studied any Computer Science, Philosophy, Psychology ... Courses. Apologies if this is unsolicited

As I mentioned above, I really enjoyed reading I Am a Strange Loop. I found Hoffstadters point of view really informative on how mathematics and the philosophy of logic can explain human cognition. Would recommend that as an entry-level book without much need for any maths or CS background. Not a massive long read (400 pages). Also has a good audiobook version on Audible - although the hard copy has some illustrations which are handy. 

For more mathematical philosophy I'd recommend just searching for things about; 

1. David Hilberts program for axiomatic mathematics, 

2. Bertrand Russel/Alfred Whiteheads theory of types (and it's criticism)

3. Kurt Godels incompleteness theorem (Nagel and Newman have a nice short book on this) and 

4. Turings theorems on computable numbers.

I don't think I have any books I'd suggest for these which are good introductions (although hoffstadter mentions all of these)  but they all have nice wiki pages 

I consider these 4 developments of philosophy from  20s/30s game changing. Turing and Church went on to use these developments to theorise modern computing and Artificial intelligence.",2,0,0,False,False,False,1641835121.0
rzqau2,hs8cbih,t1_hs1lrys,"Yeah, cognitive science is a subject that is very hard to pin down. Its defined differently by many different people. Broadly, it is the study of the mind, artificial or real. You can study the mind in many different ways: neuroscience, philosophy, psychology, anthropology and so on. One of the more interesting properties of the mind are its computational properties. That's where AI, computer science, and mathematics come in. So you really can't have cognitive science w/o cs and math.

Its also worth mentioning that the foundations of neural networks were inspired by psychology. Also, Turing/other early computer scientists were primarily interested in computation with how it relates to the mind. But this is some really rigorous stuff that won't appear on the front page of a google search. You really just have to dive deeply into it.

I'd recommend studying godel's incompleteness theorem if you really like math and computation. 

Try reading Godel Escher Bach too!",2,0,0,False,False,False,1641922015.0
rzqau2,hs34pgi,t1_hs1lrys,"I feel like I'm in the same boat my friend

Although I suspect there is plenty of maths involved at its root. If you're built for it - you can find maths anywhere 🧀🧀🧀",1,0,0,False,False,False,1641835228.0
rzqau2,hs03vcv,t1_hs00xsb,OP is specifically looking for purpose in CS as a career path. My point was he should have already done so before choosing CS as a field of study.  As to the broader reason for choosing a career I always advise you should never sell yourself short. If you have the capacity to study CS and be successful in that career path then you have the capacity to do what ever you want. Do what you love. It's sad to see someone spend a third or more of their life doing something that does not add meaning or purpose to their life. Life is short and unpredictable. Live life without regrets.,2,0,0,False,False,False,1641780198.0
rzqau2,hrxp79r,t1_hrxovwk,"I mean he’s literally asking about CS on the most basic level of life etc.

If how computers are built/function, the logic, foundation etc doesn’t all unlock using a computer then I don’t know what does",1,0,0,False,False,False,1641748427.0
rzqau2,hrxx0gx,t1_hrxovwk,"He didn't say what you're using them for. Computer science is very much how to use computers to do things.

It's not ""how to use power point"" but it's still using computers.",1,0,0,False,False,False,1641751317.0
rzqau2,hs0i285,t1_hs03vcv,"""should have already done so before choosing CS""

Why? Meaning, purpose, and passion (do what you love) can come later. Some people don't know what they are passionate about in college. Sometimes you only develop those traits as you go deeper into a topic. It almost has to come later because how meaningful is your work really going to be if you barely have any experience in the field? 

Maybe you try, but never find passion or meaning in a CS career (or hell, any career). But if you can work in a CS career you can save and invest a ton of money early and not need to work a third or more of your life. Then you can go do what you love and actually be able to fund it.",2,0,0,False,False,False,1641785988.0
rzqau2,hryfkgv,t1_hrxx0gx,"Using computers is general enough to cover everything, not sure how that makes it wrong considering the question being absolutely general",2,0,0,False,False,False,1641757854.0
rzcexe,hrxb9pq,t3_rzcexe,"I agree with you that there is a lot of hype around NFTs and cryptocurrency in general. Many influencers are promoting NFTs and cryptocurrency and saying that it is a pyramid scheme without any knowledge of the subject of cryptography and distributed systems(blockchains).

Three entities are significant in the NFT world:
- File Hosting Service (IPFS, Google Drive, AWS S3, Dropbox)
- Smart Contracts on blockchains (Ethereum smart contracts using Solidity, Solana smart contracts using Rust, etc)
- NFT Marketplace (OpenSea, etc; they are the platforms that let the bidders bid and they glue together the above-mentioned entities to create their platform)

So let's imagine you are bidding for a cat NFT on OpenSea and finally buy that NFT from the creator after proposing the highest bid. All this bidding game is played on the platform. When you buy the NFT, the code inside a smart contract is triggered which adds a transaction into the blockchain. The transaction serves as proof that you have ""bought"" the particular NFT and are the legitimate owner of the same.

In reality, you do not ""own"" the NFT, you just own the link to the NFT image which is stored on another storage that may be centralized like Google Drive or decentralized like IPFS. If Google Drive decides to remove the particular image from their servers(which is unlikely but assume), you would get a 404 error.

I assume that you know about pointers in languages like C, Rust or Go. So I would explain the above phenomena with context to pointers. Imagine you have a variable `age` and you have a pointer to that variable `elonAge`.
```
int age = 60;
int *elonAge = &age;
```
Let's assume you have the access rights for `elonAge`, you have access rights to `age` but that doesn't mean that you cannot have any more pointers to `age`. You can have another pointer to that same variable called `muskAge`.
```
int age = 60;
int *elonAge = &age;
int *muskAge= &age;
```
You can read the data stored in `age` by dereferencing `elonAge` as well as `muskAge`.

Likewise, if you own a link to an image on the interwebs that doesn't mean that nobody else can't have access to that NFT image. A person named Geoffrey(forgive me if I misspelled the name) recently showed how this works by creating the [NFT bay](https://thenftbay.org) which is the NFT version of The Pirate Bay. 

He demonstrated that he can access the NFT *images* to which the owners have a link by downloading all the NFTs and torrenting them. I heard his talk in a podcast with Coffeezilla iirc. The NFT Bay is now banned in most countries though.

NFTs are created by hosting your artwork on a file hosting server and then hosting the link to the image on a blockchain via the logic written in a smart contract.

Edit: fixed formatting",26,0,2,False,False,False,1641743093.0
rzcexe,hrvdg8p,t3_rzcexe,"Suppose you're married. Everybody in the world is banging your wife, but you can't do anything about it. But you have the marriage certificate, right? That's the NFT.",253,0,16,False,False,False,1641701185.0
rzcexe,hrukmf2,t3_rzcexe,"To clarify, NFT's are not the image. NFT's are the hash which links to the image.

The images are then hosted on standard web hosting platforms. If those web platforms take the image down, then your NFT will no longer load the image. If the NFT links to a copyrighted material and there's a DCMA claim, then your NFT will no longer link to the image.

NFT's are really just a claim in the blockchain to own a piece. Anybody can right click and download the piece. Anybody could then do whatever they want with the image. Whether that claim to own the piece actually has value is in the ye of the beholder.  


The reason you don't actually put the image on the blockchain is it's prohibitively expensive. Each transaction of the blockchain has a cost, and storing an image is *way* to expensive as compared to just a link to the image.",129,0,0,False,False,False,1641688619.0
rzcexe,hrutfqe,t3_rzcexe,You are buying a hash of an image combined with a URL to a web server of the image. That’s it. They load the hash to the blockchain for ownership record.,57,0,0,False,False,False,1641692342.0
rzcexe,hrvqhnu,t3_rzcexe,"I also have a question.

The price of the NFTs are defined by supply and demand. If a lot of people buy a specific stock, for example, there's a lot of demand for that stock and the price will automatically go up. That's because people are buying it.

But, if NFTs are unique, how exactly the supply and demand are defined?",8,0,0,False,False,False,1641707990.0
rzcexe,hrwuoth,t3_rzcexe,"The technical part of it is very similar to the idea of cryptocurrency (and works on the same technology of storing a ""ledger"" in a decentralized, online, data structure).  
The thing that distinguishes NFT from cryptocurrencies is that in NFT we give (completely theoretical) value to each individual ""link"" in the chain, whereas in cryptocurrency we give value to a numerical amount of ""links"" like we would with regular currency.

There is no real authority for NFT market shoppers to claim ownership of digital data, that ownership is not technically recognized by any institution in the world (AFAIK) and so basically people who participate in that market just choose to acknowledge that ownership (I believe that mostly because the media decided to play along out of greed, and failed to inform people that it's pretty much make believe, it's like issuing a certificate of authenticity for a molecule of oxygen, sure you could probably capture it, and you might even be able to prove that the tube you keep it in actually holds that same molecule, but as long as no country acknowledges your ability to own said molecule, it's meaningless.

Now, when talking about cryptocurrencies (say Bitcoin) there is usually some form of computational power required to operate the chain, store it's data and deliver it, so if you join in that effort you are essentially rewarded with some fraction of the currency (at least nowadays). The original idea, as I understand it, was a formula (take for example the calculation of pi), that was very computationally intensive, and if you joined in, you had a chance to be the computer that actually got the next number of pi in the calculation - this was called ""Mining"". That number was unique (in terms of it's position in the ""chain"") so when you got the credit for it, you could actually say that your bitcoin is bitcoin number XXX.

When talking about NFT's, pretty much anyone can choose to create a new NFT out of any unique identifier of digital data (specifically hash value), that unique value is stored on the chain, and so when it's ownership transfers it is registered on that chain - that chain of ownership is what the economy of NFTs is based on, you want to own something (as intangible as it is) that was owned by someone else important, or that has some digital history to it, and with blockchain that history, and connection to the original hash value is verifiable",5,0,0,False,False,False,1641735571.0
rzcexe,hrvoajn,t3_rzcexe,NFT’s most basic and pointless aspect is a art photo the real use would be proof of ownership for a car or a house or a concert ticket,9,0,0,False,False,False,1641706747.0
rzcexe,hrvv596,t3_rzcexe,Is there a decentralized place yet to store the image the hash represents?,3,0,0,False,False,False,1641710740.0
rzcexe,hrxb5sq,t3_rzcexe,"NFT means non-fungible token, meaning any type of crypto token that is unique/distinguishable. Add opposed to fungible tokens which are completely interchangeable - you can't tell one bitcoin apart from another.

The use of NFTs to hold links to JPEGs is just one possible use of the technology, and not as particularly imaginative one. In future, NFTs could be used to represent things like property deeds, event tickets and much more.",3,0,0,False,False,False,1641743049.0
rzcexe,hruxtwc,t3_rzcexe,"NFTs are a type of smart contract. They let you buy and sell ownership of an asset (physical or digital) using the blockchain. The first owner creates an NFT and issues a licence that states whoever owns the NFT owns the thing. The code for the NFT then uses the blockchain  to ensure that only one person can be the owner at any time.

They're overhyped at the moment, but so was the Internet in its early days (dot-com boom) and nobody would deny now the Internet is important. The basic idea - buying and selling stuff other than cryptocurrency over the blockchain - is a good one.

The code for an NFT can be very simple - you can write one in 14 lines. Have a state that keeps track of the owner. Provide an API that transfers ownership if buyer and seller both authorize the transaction, and answers the question ""Who is the owner?"" at any time. The difficult part - keeping this all synchronized globally - is delegated to the blockchain.

For a good, relatively hype-free overview, here's the page on NFTs on Ethereum.org:

https://ethereum.org/en/nft

And for the gory details, here's the spec:

https://ethereum.org/en/developers/docs/standards/tokens/erc-721/

https://eips.ethereum.org/EIPS/eip-721",14,0,0,False,False,False,1641694234.0
rzcexe,hrvz1sz,t3_rzcexe,Key value paid loaded with a GUID and whatever the digital object you just bought was.,2,0,0,False,False,False,1641713303.0
rzcexe,hryf3j1,t3_rzcexe,"I do think NFTs are overhyped, but I also think many are missing the point. The potential value of an NFT is basically the price difference between original art and a replica that is *artistically* identical.

Why do people buy original art? There is more than one reason: supporting an artist, money laundering, or simply prestige are a few reasons.

It's intellectually inconsistent in my opinion to think NFTs are worthless but original art / artifacts should be worth more than replicas.",2,0,0,False,False,False,1641757684.0
rzcexe,hrw9g8l,t3_rzcexe,it's a fraud,3,1,0,False,False,False,1641720868.0
rzcexe,hru88yf,t3_rzcexe,From what I have gleaned the hash is taken including the image content itself (not just a link). Therefore the image can be hosted anywhere or move to a new location and since the content is the same the “link” is still intact. For small images I think they can be inserted into the blockchain itself so many parties can store it. However this seems to cost transaction fees so may be prohibitively expensive for images of non-trivial size.,2,0,0,False,False,False,1641683494.0
rzcexe,hrvfvac,t3_rzcexe,The first nft was a number.,2,0,0,False,False,False,1641702331.0
rzcexe,hruxxcb,t3_rzcexe,"I recently took a blockchains course and from my understanding NFT's are powered by smart contracts mostly on the ethereum network. Most smart contracts are written using Solidity but they can really be written in any language. There's an online editor called Remix which you can use to write in Solidity. The smart contract is placed on the blockchain where it is immutable; however, there are ways for your NFT to be stolen. 

remix.ethereum.org",2,0,0,False,False,False,1641694276.0
rzcexe,hry3xso,t3_rzcexe,"You got 1 or 2 CS related explanations, the rest are basically garbage hype/anti-hype.  Good try OP.",1,0,0,False,False,False,1641753771.0
rzcexe,hry8rs6,t3_rzcexe,"Could someone post an example of where I could find the URL to an NFT within the NFT Contract?

For example, this is an [NTF page on OpenSea](https://opensea.io/assets/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d/8322) which costs 97 eth (or, $307,000). I can expand the Details to find the [Contract Address](https://etherscan.io/address/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d#code) or [Metadata](https://ipfs.io/ipfs/QmeSjSinHpPnmXmspMjwiXyN6zS4E9zccariGR3jxcaWtq/8322), or I could right-click on the image and [open in a new tab](https://lh3.googleusercontent.com/qtXorUoQU99DUrPCj694omQp_8_SZNfP4WPSE5LhjXPP9MvzA0-Y9ZjVQWDwgdrc2Otr5PfLp0pZCLHWiYgGJY6g3UHqflOt9K1RIw=w600) (which is simply served by a CDN).

The Metadata page lists an ""image"" key as containing the value `ipfs://QmXffx3vPcYVTUyQzxYWjkAnePQ4LuPB5vpzdDiqHqxcnm`. So, is that ""image"" metadata element a pointer to a URL with the transport language as ipfs:// ? If so, what is ipfs:// and how does that relate to an HTTP server that hosts the image?

And if that ipfs:// pointer indicates a location of the content, how is that tied into the owner of the NFT?

Edits: I'm recovering from covid and I can't seem to type NFT correctly each time (sometimes it comes out as NTF, other times it's NFL)",1,0,0,False,False,False,1641755448.0
rzcexe,hry9qe2,t3_rzcexe,"Not an answer, but a great [write up](https://moxie.org/2022/01/07/web3-first-impressions.html) by moxie (creator of Signal) exploring NFTs and web3. He demonstrates how the image you bid on is not necessarily the image you get.",1,0,0,False,False,False,1641755786.0
rzcexe,hrwad64,t3_rzcexe,"The idea and the possibilities are cool, but the fact that image is not saved on blockchain is the most ludicrous thing ever. It does have good use cases though. (e.g. Tickets, patents, legal documents, in-game assets, helping artists collect royalties from usages)",-5,0,0,False,True,False,1641721584.0
rzcexe,hrxoo0n,t1_hrxb9pq,Thank you! For explaining it really well!,2,0,0,False,False,True,1641748224.0
rzcexe,hrvgshu,t1_hrvdg8p,Genius.,29,0,0,False,False,False,1641702784.0
rzcexe,hrwt131,t1_hrvdg8p,"Ah, I wish I had a free award to give you today.",9,0,0,False,False,False,1641734663.0
rzcexe,hrxcqq3,t1_hrvdg8p,Sick world,6,0,0,False,False,False,1641743679.0
rzcexe,hryvn8y,t1_hrvdg8p,A gentleman and a scholar,2,0,0,False,False,False,1641763542.0
rzcexe,hrvgs9o,t1_hrvdg8p,[deleted],-13,0,0,False,True,False,1641702781.0
rzcexe,hrvgvzi,t1_hrukmf2,"So what is the point? It seems all hype and woo-woo bullshit to me, pushed mainly by people with a stake in getting rich off of it.

Why would anyone want to buy one? It's not like owning real art. So it's just...hype?",78,0,0,False,False,False,1641702833.0
rzcexe,hrwf5k3,t1_hrukmf2,">put the image on the blockchain

There is more than one blockchain, isn't it ?

Afaik, every crypto currency uses its own blockchain. Which one is used for NFT ?",7,0,0,False,False,False,1641725328.0
rzcexe,hryg3fk,t1_hrukmf2,"I'll add many people see nfts as solely images or hashes but that's definitely not the piece with the most use cases.

- NFTs have different types and aren't all just meant for images",2,0,0,False,False,False,1641758042.0
rzcexe,hrw5oi8,t1_hrukmf2,This is such nonsense. I honestly cannot wait for interest rates to rise to wash away all the BS and zombie companies doing absolutely useless work.,3,0,0,False,False,False,1641718041.0
rzcexe,hrwqde1,t1_hrukmf2,"What about the comparison with game skins? Csgo, people open chests and sometimes you can open an ultra rare skin worth a lot of money.

The skin itself is owned and hosted by the company valve, so what does the owner of the skin actually own? Only the 'link' to the skin for their use.

What brings that Csgo skin value?

I think based on game cosmetics existing and being worth money in our current environment, it is proof that there is some use case for NFTs.

Whether that's true for real life applications to things like art collection and celebrity autographs remains to be seen, but I think it's a really ignorant thing to make a general statement like 'NFTs are a scam' because you don't actually 'own' anything because it lives on a domain outside of your control.

The same theory on nfts being a scam could also be applied to money and online banking. Inb4 people start screaming at me that the world economy is going to collapse because everything is fake.",0,1,0,False,False,False,1641733134.0
rzcexe,hrwbtl1,t1_hrukmf2,Usually the NFTs are stored on IPFS instead webservers so nobody can take it down,-3,1,0,False,False,False,1641722719.0
rzcexe,hrxork6,t1_hrukmf2,Interesting to even know that putting an image on blockchain is possible. I did not know this.,1,0,0,False,False,True,1641748261.0
rzcexe,hrycj9g,t1_hrukmf2,"> The reason you don't actually put the image on the blockchain is it's prohibitively expensive. Each transaction of the blockchain has a cost, and storing an image is way to expensive as compared to just a link to the image.

Math/TCS guy here :) i've been looking more into blockchain's it seems like some of them don't have the same transaction's costs as Ethereum/Bitcoin such as Cardano. Honestly instead of NFT's if it where custom-created trading card's put on the blockchain I could see it it working",1,0,0,False,False,False,1641756773.0
rzcexe,hs1ob5y,t1_hrukmf2,"A dumb question, but if ur NFT was took off by hosting platform, does that mean ur money just disappeare too?",1,0,0,False,False,False,1641810175.0
rzcexe,hrv7qzw,t1_hrutfqe,"Minus the “hash of an image” bit.

Here’s a guide: https://docs.alchemy.com/alchemy/tutorials/how-to-create-an-nft",21,0,0,False,False,False,1641698589.0
rzcexe,hrwy6cw,t1_hrutfqe,Images are just scratching the surface of what you can do with NFT’s.,2,0,0,False,False,False,1641737337.0
rzcexe,hrw9pzc,t1_hrvqhnu,"The same as with traditional unique items, like art pieces or unique stamps or John Lennon's glasses etc. Supply and demand still applies, but supply is 1. Therefore the price will become whatever the highest bidder is willing to pay.",16,0,0,False,False,False,1641721072.0
rzcexe,hrxwwg0,t1_hrvqhnu,For most projects there’s like 5000 pieces that are randomly generated. Some are more rare than others because of traits being more rare.,1,0,0,False,False,False,1641751276.0
rzcexe,hrxktoz,t1_hrvoajn,Thank you for this. Everyone gets so caught up in hating nfts bc of the shitty apes and I think its stopping them from realizing there's actual use cases for them out there,3,0,0,False,False,False,1641746767.0
rzcexe,hrxymhu,t1_hrvoajn,Is there any reason that all has to be in a decentralized system like blockchain? My local government has a big database of car registrations/ownership that seems to work fine. I just don’t see what problem these blockchain solutions are actually solving,1,0,0,False,False,False,1641751904.0
rzcexe,hsgbqyx,t1_hrvoajn,Isn’t there a legal aspect that needs to be in play for NFTs to actually enforce ownership over physical goods?,1,0,0,False,False,False,1642052484.0
rzcexe,hrwkpcc,t1_hrvv596,You could put it on IPFS: [https://ipfs.io/](https://ipfs.io/).,3,0,0,False,False,False,1641729489.0
rzcexe,hrwkw4r,t1_hrvv596,Yeah. Look into IPFS. https://IPFS.io,3,0,0,False,False,False,1641729625.0
rzcexe,hrvizms,t1_hruxtwc,"I would recommend learning at least a little bit of the science/cryptography of/in/for blockchain/hashing if you're into CS, but not actually buying these. 

Just invest in businesses that will generate money (with good causes), or your home, or your life, or just be a philanthropist if you're f-ing rich, rather than buying these elitist BS.

Sorry, for ranting. I just don't support NFTs.",11,0,0,False,False,False,1641703898.0
rzcexe,hrxpaf0,t1_hruxtwc,Thank you for the links!,1,0,0,False,False,True,1641748460.0
rzcexe,hrwlrsp,t1_hrwad64,Why would saving the image on the blockchain matter?,2,0,0,False,False,False,1641730250.0
rzcexe,hrvr86j,t1_hrvgs9o,I guess that joke hit a little too close to home for some people,29,0,0,False,False,False,1641708415.0
rzcexe,hrvn06m,t1_hrvgvzi,"> So it’s just hype

Yup. It’s just people trying to ride the crypto wave by pushing this as a new way to collect art.",94,0,0,False,False,False,1641706043.0
rzcexe,hrvybwg,t1_hrvgvzi,"Generally, people dont understand what the NFT is. They just see the image and think that's the NFT.",10,0,0,False,False,False,1641712816.0
rzcexe,hrvnlk2,t1_hrvgvzi,"When people explain NFTs with urls to images it kind of muddies the purpose of an NFT.  It's just independently verifiable digital ownership of something.  The ability to download that something is dependent on there being someone who can distribute it to you, but that doesn't have to be one single source.  Let's say you buy an NFT song from a music artist.  It's not a URL, it's just a code of some sort.  You might be able to download the song from Amazon music, iTunes, Bandcamp, or any number of other distributers if they have a system in place to verify ownership using your NFT.  And you could give/sell that NFT to someone else if you wanted to.  Obviously this is extremely beneficial to the consumer and not the distributer so it will probably never happen, but that doesn't mean the idea is pointless.

Unfortunately NFTs are pretty much only associated with digital art at this point and used to either make money or launder it.  All the interesting use cases seem to be things that are unlikely to happen because they mess with the bottom line of established services.",31,0,0,False,False,False,1641706368.0
rzcexe,hrwbwqd,t1_hrvgvzi,"Some expensive art sits in warehouses and gets traded without anyone looking at it. It's all for speculation, and money laundering.",7,0,0,False,False,False,1641722787.0
rzcexe,hrvobpy,t1_hrvgvzi,"NFTs can also broadly describe anything that has a sense of uniqueness. For example, some games use NFTs to represent in game objects so players can buy/obtain those NFTs and trade them with other players.",3,0,0,False,False,False,1641706765.0
rzcexe,hrwiylv,t1_hrvgvzi,"The only important value is perceived value

Also money laundering",3,0,0,False,False,False,1641728240.0
rzcexe,hrycmh4,t1_hrvgvzi,">It's not like owning real art.

How so? Why do people own original art in the digital age of perfect replicas? What's the intrinsic value of an original piece of art over a print/copy?",1,0,0,False,False,False,1641756805.0
rzcexe,hryduem,t1_hrvgvzi,"I'd say you're overlooking ways in which it is quite similar to the art world. The original Mona Lisa is deemed to be of immense value by society, but now consider a copy which no average person could tell was different from the Mona Lisa. This is inexpensive to make, and would probably sell for a couple hundred dollars. Based on this, we can see that tens of millions of dollars of the Mona Lisa's monetary value aren't contained in how it looks. Rather, they're contained in that it is THE original. Using NFTs, one can create a similar ""THE original"" property for a piece of digital art. Namely, they issue a single token for the data of their art. From then on, the owner will have cryptographic-blockchain proof that they are the sole owner of the original token.

Now, considering that NFTs are like art, that makes it so they are a very natural space for bullshit and hype. However, this is just the same phenomenon that takes place in the modern art world. A painted red canvas is worthless, UNLESS it was painted by this one famous person, for example. I personally think that valuing something for being THE original, rather than how it looks, is irrational, both in NFT space, and in real-art space. However it isn't like NFTs are all built on nothing and will pass as a phase. There is just as much merit in valuing NFTs (and therefore reason to expect them to continue to be valuable) as there is in valuing original pieces of art. One can further distinguish whether the NFT has artistic value (ie the apes vs some world renowned digital artist's painting). This is just equivalent to the distinction between a collectible card and a piece of art.

Lastly, it's true that much of paintings' value (but very little proportionally in the case of the immensely valuable ones) is from looking cool. NFTs don't contain any of this value, whereas physical art bundles together the two types of value. This is a meaningful distinction. However, to reject the value of NFTs is to reject the second type of value art has. I reject both as a rational thing, but am willing to accept that society will consistently value the latter, making investment in it feasible.",1,0,0,False,False,False,1641757243.0
rzcexe,hrwscun,t1_hrvgvzi,"I'm somewhat new to the computer science field but my understanding is that NFTs (or smart contracts in general) use a different back end. Instead of hosting on a back end service provider like AWS, they use IPFS (Interplanetary file system) which is similar to a file sharing service like bittorrent. The idea being that as long as 1 person on that network is hosting that file you'll always be able to retrieve it using your digital claim to the NFT address essentially.

But yes, the prices are 100% speculation with some hype sprinkled on top",-4,0,0,False,False,False,1641734283.0
rzcexe,hrwmhlp,t1_hrwf5k3,Ethereum,9,0,0,False,False,False,1641730733.0
rzcexe,hrz8usw,t1_hrwf5k3,"Every cryptocurrency does not use it's own blockchain network. There's a lot of blockchain networks though - Ethereum, Polygon, Solana, Avalanche, Fantom, just to name a few. And some of them aren't even true ""blockchains"".

You can host NFTs on any of the above networks and many more.",1,0,0,False,False,False,1641768274.0
rzcexe,hrwz5kk,t1_hrwqde1,"Ok, sure, you could have an NFT that links to a game skin, but ultimately CS:GO could still decide if it wants to let you use the skin in game - which defeats the purpose of the decentralization if a centralized authority could have the final say in the end. In a NFT game item platform you wouldn’t have ownership of your items in a meaningful way.",5,0,0,False,False,False,1641737824.0
rzcexe,hrx0vpu,t1_hrwqde1,"> The skin itself is owned and hosted by the company valve, so what does the owner of the skin actually own? Only the 'link' to the skin for their use.

The skin would not exist without Valve, and ownership is maintained at a significantly lower cost (both for the company, and in terms of electricity/compute usage). It is literally impossible for other people to use your skins without you giving it to them, whereas an NFT tells everyone that you ""own"" some link, but does nothing to actually enforce the practical things we usually care about with ownership.

> What brings that Csgo skin value?

The skin itself is valuable.

> I think based on game cosmetics existing and being worth money in our current environment, it is proof that there is some use case for NFTs.

People have bought into all sorts of bubbles in the past that ended up worthless, or into financial schemes that ended up being scams. Why shouldn't NFTs be the same?

> The same theory on nfts being a scam could also be applied to money and online banking. Inb4 people start screaming at me that the world economy is going to collapse because everything is fake.

NFTs aren't a scam because people realize that we made up society ourselves, NFTs are a scam because people make big promises beyond what the actual technology is doing for you. You have to buy into lies or be stupid to think NFTs are worth your money, or be [one of the people actually making money off of the scam](https://www.bloomberg.com/news/articles/2021-12-06/small-group-is-reaping-most-of-the-gains-on-nfts-study-shows).",4,0,0,False,False,False,1641738660.0
rzcexe,hrx6q02,t1_hrwqde1,"Skins have no value and are only bought by children with no concept of money and ""whales""  people who are incredibly rich and $5000 is the same to them as a penny to me, or people hopelessly addicted to the game who bankrupt themselves on it.  Its not every players buys $30 of skins, it's 2% of players spend $300,000 on skins. They are not a viable business model.  Europe is legislating them out of legality, and hopefully he rest of the world follows them.",3,0,0,False,False,False,1641741268.0
rzcexe,hrxpymr,t1_hrwqde1,"See [my other comment](https://www.reddit.com/r/computerscience/comments/rzcexe/can_anyone_explain_nfts_from_computer_science/hrxpc0v/). 

Digital bits that represent exclusive ownership of digital assets absolutely have value. But nothing in your example benefits from a blockchain. Valve can and should just store the fact that you own that very rare skin through a normal database record like any other system.",1,0,0,False,False,False,1641748713.0
rzcexe,hry06kh,t1_hrwqde1,"Never said it was a scam. I said 

> whether that claim has value is in the eye of the beholder

When I buy a CSGO skin though, nobody can use that skin. That's why people buy them, to show them off in-game.

When I buy an NFT, somebody can just right click and download the image. Sure, it's not the NFT. How many people know that, how many will verify I don't own the digital rights to the image? A tiny fraction.",1,0,0,False,False,False,1641752457.0
rzcexe,hrwmgw7,t1_hrwbtl1,Usually? I’m pretty sure that’s the exception to the rule.,7,0,0,False,False,False,1641730720.0
rzcexe,hry19ly,t1_hrwbtl1,"I couldn't tell you the percentage. All I know is there are a fair amount still hosted on traditional media sites given the amount of NFT's that ""disappear"".",2,0,0,False,False,False,1641752836.0
rzcexe,hryh4ly,t1_hrxork6,"You can put _anything_ on a blockchain. It's just a matter of cost and time, so some things just aren't _practical_ to put on a blockchain.",1,0,0,False,False,False,1641758408.0
rzcexe,hs5nfam,t1_hs1ob5y,There is no hosting platform for the NFT. It's stored on whatever blockchain. Are you referring to the image?,1,0,0,False,False,False,1641870292.0
rzcexe,hrvm0i5,t1_hrv7qzw,"If they don't use a hash of the linked content, is there anything preventing someone from creating a new NFT of an existing NFT's image reuploaded?",15,0,0,False,False,False,1641705503.0
rzcexe,hrv9lx6,t1_hrv7qzw,"I swore they used a hash to create an ID. Oh well, just use Opensea and upload your stuff.",2,0,0,False,False,False,1641699429.0
rzcexe,hrx2e3p,t1_hrv7qzw,"Lmao why not actually hash the image? Also, if it's just on ropsten, that's even worse.",2,0,0,False,False,False,1641739367.0
rzcexe,hrx8rje,t1_hrw9pzc,"That is precisely the ""non fungible"" part. Stocks are not unique. It makes no difference which particular share you haber in a company.",5,0,0,False,False,False,1641742098.0
rzcexe,hs1ymw9,t1_hrxktoz,"So what’s the use case?  Let’s say there is a blockchain out there where I own an NFT that “points” to a car.  Does it point to the VIN number or something else?  

Now let’s say someone steals my car.  Now what?  Do I call the police?  If so, how has the addition of a blockchain improved this situation in any meaningful way?  

So let’s say the officer asks for my title to verify that I actually own the vehicle.  Now I fire up meta mask and show the officer my NFT.  That officer laughs his ass off and leaves.

Of course you may be thinking that in this hypothetical world that we’re talking about, the legal system recognizes NFTs as as legally binding ownership of arbitrary assets and investigates the theft and ends up finding my car and returning it to me.  What advantages did having my ownership residing on the blockchain give me that we don’t currently have?",1,0,0,False,False,False,1641817298.0
rzcexe,hry26dx,t1_hrxymhu,"Ever lost your car title? It’s a pain and cost money to replace NFT solves this. 

Ever seen a company get hacked with ransom ware and not be able to anything? NFT data can’t be hacked. 

Ever lost your concert tickets NFT solves that too. 

Decentralized is always better no matter what the application is.",-1,0,0,False,False,False,1641753158.0
rzcexe,hrvl72j,t1_hrvizms,"I don’t necessarily think it’s any different than owning a skin on a video game in principle. I don’t really know a ton about them and I likely will never buy one, but you buy a skin to show off that you own the cool looking skin the same way someone buys an nft to show off owning a cool looking image.",3,0,0,False,False,False,1641705061.0
rzcexe,hrxp248,t1_hrvizms,"100% agree. It has become so hard to learn about the CS side of NFT with all the hype. None of the top results on google come up with any CS info these days. It is astonishing how the world took cryptography topic and turned it into a such a big marketing hype.

None of my cryptography focused friends even want to talk about cryptocurriences or NFTs. They are pissed of at what it's become lol.",1,0,0,False,False,True,1641748372.0
rzcexe,hrwma6o,t1_hrwlrsp,"Because when just the url saved on the blockchain, if the image is taken down, you can say bye to your art piece. Some systems also save the hash, but hashes are not 1-to-1 functions. Same hash could be generated by multiple images, albeit not easy to do so.",1,0,0,False,False,False,1641730596.0
rzcexe,hrwlntc,t1_hrvr86j,"My apology for the ignorant joke. I think I will be a silent reader for a while on this sub to reflect and try making up for it 🙇‍♂

I also forgot that this sub is educational.",5,0,0,False,False,False,1641730171.0
rzcexe,hrvnkmt,t1_hrvn06m,"Ok. Cool. That was my take,  but wasn't sure if I just didn't get it.",28,0,0,False,False,False,1641706354.0
rzcexe,hrwme1v,t1_hrvnlk2,"To be technically correct, it's independently verifiable ownership of a few bytes of data stored on the blockchain, usually a URL. The only guarantee you get is that each NFT belongs to exactly one wallet. Here are some guarantees it can't make, however:

* That the person controlling the associated wallet legally owns the thing the NFT links to
* That any specific person controls the associated wallet
* That the data stored in the NFT actually represents something useful (Many of them are only meaningful in the context of a centralised website or service)
* That URLs in NFTs will always link to the same thing (or anything at all, in fact - many NFTs already link to dead pages)
* That trades involving this NFT weren't done through scams or hacks
* That there aren't multiple NFTs linking to the same thing, each belonging to a different person

If every step involved in making a transaction on the internet is a chain, then crypto is about making one link in that chain as strong as it possibly can be. Crypto people will tell you they have the strongest chain ever created and back it up by just describing that one link over and over again and trying their hardest to ignore anyone who points out that the rest of the chain is made of paper and wishful thinking",16,0,0,False,False,False,1641730669.0
rzcexe,hs0x850,t1_hrvnlk2,"It’s not really ownership either, because ownership is a legal concept and afaik no major country has ownership laws that recognize NFTs. Neither ownership laws for physical items nor intellectual property laws really fit to NFTs.",2,0,0,False,False,False,1641792464.0
rzcexe,hrxezm4,t1_hrvnlk2,So if the NFT proves your ownership can you then license the song or image or whatever and make money off it? Can you sue someone who is infringing on your ownership?,1,0,0,False,False,False,1641744544.0
rzcexe,hrxpc0v,t1_hrvobpy,"Which is generally pretty pointless. Remember, NFTs aren't just bits that say you own something. They're part of a blockchain system, and the whole point of a blockchain is decentralized control. 

If you have a trusted third party, you don't need crypto, blockchains, or NFTs even in the best of cases. It's the world's stupidest way to implement

    update owners set owner_id=9876543 where item_id=1234567

in the third party's system. And what sort of practical game do you expect to support players trading rights to items in a way that it's important that there be no trusted third party. The entire world is running on a trusted third party system. If you don't trust the game server, nothing matters anyway.

The only way you're really going to use this is just to say you did. It's basically performance art to make a game that uses NFTs to track ownership.",3,0,0,False,False,False,1641748476.0
rzcexe,hrxqlbi,t1_hrx6q02,"Are you suggesting that a twenty and two fives is worth less if it came from a rich person with questionable taste than if I got it from ""normal"" people? 

$30 is $30. You seem to be ranting against the concept that the market value of something is a concept that exists, and things are actually worth what you feel they should be worth instead. Not sure what to say to that.",1,0,0,False,False,False,1641748946.0
rzcexe,hs0l5gr,t1_hry06kh,"I said the idea of applying NFTs to things like art collection is questionable to me, but just as you can buy a replica of a Monet painting and it is worthless, there is an idea behind the value of an NFT that is derived by participating parties willingly participating in the system.

I am not saying I agree with the idea, but what I mean is that I think I can understand why people are buying into it and I can see the potential for success.

When I originally got into bitcoin many many years ago, people kept laughing at me and nobody took me seriously when I tried to get friends to buy bitcoin.

The concept of having purely digital money was absurd, 'like how do you know someone won't just change the bits in your wallet and then you have nothing?'

It was a poor understanding of abstract values like what really made the cash in their wallets able to give them purchasing power moreso than the idea of blockchain itself, so.... Yea, what I wanted to point out in my first comment was that it's too early to say anything bad about NFTs. The jury is still out, but if there is any area blockchain could be applied, it's areas like art and creative content where value is completely immeasurable.

Would you pay millions of dollars for some paint / carbon molecules on a canvas? It sounds absurd, but some people do.",1,0,0,False,False,False,1641787291.0
rzcexe,hrysha3,t1_hryh4ly,I think there’s a maximum block size which limits what you could put on.,1,0,0,False,False,False,1641762387.0
rzcexe,hs69qfz,t1_hs5nfam,yes that's what I mean. sorry for my stupidity,1,0,0,False,False,False,1641880630.0
rzcexe,hrvy1gu,t1_hrvm0i5,"There’s literally nothing stopping you from doing exactly that. There’s also nothing stopping the server that hosts the image from changing said image, deleting said image, breaking the link, or shutting down. 
An NFT is just a url that lives in the blockchain",24,0,0,False,False,False,1641712623.0
rzcexe,hryv6qh,t1_hry26dx,"NFT doesn’t solve it; because you loose you title for your car you can apply to get another one. You lose the private key to the NFT; you’re ducked.

Same with a concert ticket, NFT doesn’t help.

Decentralisation moves the burden of security from companies or governments with resources, to individuals without resources or knowledge . Not only that, it removes any ability to fix issues. You’ve lost your key to NFT, Bitcoin, you’re shit out of luck.  Someone breaks into your wallet somehow, boom everything is gone. 

Decentralisation is always worse, no matter the application.

Also, NFT can’t be hacked! Come on! Security is always a shifting window; no Cryptographic process is 100% secure; which is why key lengths increase every so often.",1,0,0,False,False,False,1641763373.0
rzcexe,hrvptvn,t1_hrvl72j,[deleted],2,0,0,False,False,False,1641707612.0
rzcexe,hrz6zzk,t1_hrxp248,"I would just say don't let other people spoil something for you. If others want to look and talk about the pretty window frame, let them talk about the pretty frame. If you know there's something more to it, don't let your ego pull you away from moving closer, cupping your hands around your eyes (to block out the glare from all the flash photography the others are doing) and look into that window bro. There's some real s*** inside. 
And also it might actually be a plan to get people blockchain weary, by emphasizing the big-finance aspect enough so that's all that people see, having them ignore its potential to disrupt the status quo and fundamentally change the balance of power.  
Some people love the status quo. Bankers like being bankers and being depended on. Don't know if I'm making sense. It's 00:32 here, I should probably go to sleep now.",1,0,0,False,False,False,1641767592.0
rzcexe,hrxcewl,t1_hrvnkmt,"I want to add that there are financial implications of NFTs as well. They're very hard to tax and with the hype around them, their prices inflate a lot. That means wealthy people can invest in NFT, drive prices up (Elon musk's dogecoin to the moon wasn't directly NFT related but still  a good example of crypto market inflation) then sell go turn a profit on something that can't be taxed. 

Of course it's less stable than holding money in a bank or traditional stocks, but since legislators don't really understand them you get away with more.",16,0,0,False,False,False,1641743554.0
rzcexe,hrx935w,t1_hrwme1v,"Some good points, but also I don't see anyone claiming that NFT represent legal ownership, or that people can't have their wallets hacked/scammed.

>That there aren't multiple NFTs linking to the same thing, each belonging to a different person

Yep and this is specifically a problem with NFTs for collectibles.  You have to trust that the people selling the NFTs won't just decide to sell more of that supposedly unique or rare item.  This wouldn't be a problem in the NFT music example I used.",3,0,0,False,False,False,1641742228.0
rzcexe,hs0yb0i,t1_hs0x850,"Right, the laws haven't really caught up yet.",1,0,0,False,False,False,1641792994.0
rzcexe,hrxn0ew,t1_hrxezm4,"No. It could be one part of a very large system we could develop to allow that, but that system doesn't exist today (well, it sort of does, it just works better and doesn't need crypto or blockchains to work), and NFTs themselves can't solve more than a very small amount of the problem on their own regardless.

Let's say you want the exclusive rights to publish a book. Imagine the US copyright office were to print out your registered copyright and mail it to you. The laws of physics prevent another person from having that exact same printed copy as you, and NFTs do the same for the digital version. The Copyright office could turn around tomorrow and print a second copy of the exact same document and mail it to someone else though, in which case it would be really hard to prove which one of you actually owned the rights to that book or whatever. And in the NFT world, we can just create a second token to the same URL and certify that someone else owns that one. The system prevents two people from having the same ""paper"" claiming ownership. It doesn't prevent two people from having different ""papers"" each claiming ownership to the same thing.

And that's just one problem. A second problem is that no one really recognizes any of this stuff as meaning anything. So maybe you are the only person who owns an NFT that supposedly represents that book. I can just sell copies of the book myself anyway, and there's no mechanism for you to stop me. No government or police agency cares about NFTs. 

You can solve these problems. You can empower some centralized broker to track and enforce everything. But then what's the point of the crypto aspect? If you have a trusted third party, just turn the mining farm off and be done with it. We've been doing all this stuff without crypto for hundreds of years. The US Copyright Office does in fact exist today, and unlike anything in NFTs, there are enforcement systems in place.",2,0,0,False,False,False,1641747596.0
rzcexe,hrxgvyu,t1_hrxezm4,"In that example I don't mean you own the rights to do whatever you want with it.  You own an authorized digital copy of the song that you can download and listen to.

But in the case of using NFTs to own a license, there would need to be laws in place to protect that, if it isn't already covered.",1,0,0,False,False,False,1641745278.0
rzcexe,hrxuni4,t1_hrxpc0v,"Perhaps a game item isn't the best example. ¯\_(ツ)_/¯
I'm a huge crypto nerd and I like the space a lot. So I'm biased. I see a lot of utility in NFTs though not really as jpgs or game items. I think those sorts of NFT projects are fun but they're really just digital status symbols at best and Ponzi schemes at worst.

There's certainly NFTs with utility that I find very interesting.

One project I've been looking into utilizing is the Superfluid Protocol which allows you to create an NFT that serves as a landing point for a stream of money. You can create a stream of funds (say the stream sends .000001 ETH per second or something) between address A and address B. If address B is an NFT, you can trade that NFT to another person and they will begin collecting that stream of ETH every second.",0,1,0,False,False,False,1641750443.0
rzcexe,hry7jkd,t1_hrxqlbi,"No im suggesting that 

>They are not a viable business model.  Europe is legislating them out of legality, and hopefully he rest of the world follows them.",2,0,0,False,False,False,1641755029.0
rzcexe,hryy728,t1_hrysha3,You can always create a blockchain with a larger block size though. It's just a matter of practicality.,1,0,0,False,False,False,1641764442.0
rzcexe,hrz4ydc,t1_hrysha3,Which is why the actual images are never stored on the Blockchain.,1,0,0,False,False,False,1641766848.0
s011f6,hrz0fc1,t3_s011f6,"You know Linux was/is also evolving and wasn't as it is today from the beginning?  
Nowadays, legions of programmers (many from big corporations) contribute to Linux.

Few years ago I wouldn't even dream of not having to dual-boot for gaming.  
A decade ago I've been just entertaining a thought of running Linux as primary OS.  
Two decades ago Linux was basically only for enthusiasts.  
When GNU/Linux has its release, it was supposed to be only temporary until GNU finish their own kernel.  
When Linux was born, it was just a summer project of a young student.

Linux wasn't so good at the beginning.

And neither is/was Unix for majority of users. Unix was complex system for academia and commerce (because there wasn't any other type of consumers at the time!). CS students have problems with command line, what do you expect from your average Joe from decades ago?

^(Also you may not know, but while Linux is only Unix-like, macOS **is** Unix by blood)",12,0,0,False,False,False,1641765232.0
s011f6,hs0xu78,t3_s011f6,"Well, I am just another Redditor with his own (stinky ;) ) opinion.


In the case of PC operating systems (mostly Mac OS and MS-DOS and early versions of Windows) This mostly had to do with design constraints with the hardware that they had to work with at the time. MS-DOS was a derivative of CP/M written for the Intel 8086 and the original Mac OS was written for the 68000. Neither chip had transistors that performed the functions of an MMU (memory management unit) and both operating systems were written with such hardware limitations in mind. They were also written to function in extremely small amount of memory. 

The hardware that both platforms ran on (680x0 series of chips for Mac OS and 80x86 for PCs) did evolve to include an MMU (68020 and 80286/80386) but by then the software ecosystems were very intrenched and new versions of Mac OS/MS-DOS still had to support those older chips for a time.

Contrast that with early versions of UNIX which ran on Mini-Computers which were extremely expensive for the day. Those machines had the hardware that implemented/supported memory protection. With that support in place it was much easier to build a multitasking system with protected memory.

So in short, those features are easier to build when you have hardware to help you with it. Early PC-Class CPU's lacked the hardware to easily support them. Technically I think you could make it work without such hardware if you are determined enough but I would imagine may deciding to wait for MMU support to be standard on CPUs before implementing it.


Well there is my dirty stinky opinion answer :)

EDIT: I decided to google around and I figured this link would be of interest to the OP

https://retrocomputing.stackexchange.com/questions/7740/why-was-preemptive-multitasking-so-slow-in-coming-to-consumer-oss",3,0,0,False,False,False,1641792760.0
s011f6,hs0y3fg,t3_s011f6,"The first version of Windows NT (3.1) was released in 1993, less than 2 years after the earliest version of Linux. Windows NT 3.1 has everything you described. NT was largely based on the architecture of DEC VMS, and designed by members from the same team.",3,0,0,False,False,False,1641792889.0
s011f6,hryw4qu,t3_s011f6,Following the post,1,0,0,False,False,False,1641763718.0
s011f6,hshwi8w,t1_hrz0fc1,"Sure, I understand all that.

Mac OS X and beyond are based on FreeBSD UNIX, yes, but that didn't come around until 2001 or so.

Classic Mac OS 9 and earlier were not UNIX based. It didn't have protected memory or preemptive multitasking or even multiple user accounts, it was very basic and it could crash easily.

# My point is that if they found a way to make a GUI in 1983 and UNIX existed in 1969, why did it take until 2000 for a consumer OS with protected memory and UNIX-like stability to be available?  That's my REAL question.",1,0,0,False,False,True,1642088070.0
s011f6,hshwsub,t1_hs0xu78,"Excellent, this is a super helpful response. I really appreciate that!",1,0,0,False,False,True,1642088186.0
s011f6,hshwp4h,t1_hs0y3fg,"Right, but Windows NT was not a consumer OS.

My point is if UNIX existed in 1969 and the GUI existed in 1983, why did it take until 2001 or so for us to have a consumer OS with the protected memory and stability of UNIX AND a GUI?",2,0,0,False,False,True,1642088146.0
s011f6,hshzumo,t1_hshwp4h,"Sorry, but NT was a consumer OS. It just wasn't the most common one yet. Literally everyone running windows today is running some version of NT. As for protected memory, Unix did not originally include that. Memory protection came about when MMUs were added to processors in the late 70s.",3,0,0,False,False,False,1642089350.0
s011f6,hsqkcld,t1_hshzumo,"Maybe I misused the term ""consumer"". What I meant was, prior to Windows XP, the overwhelming majority of Windows users were not running an NT based OS. Windows NT and Windows 2000 were not for regular users, lets not pretend they were mainstream in remotely the same way that the others were. Win 2K didn't even have USB support in the beginning. Most people weren't running it. That was my point.",1,0,0,False,False,True,1642230175.0
rzc739,hrurxml,t3_rzc739,"I would start with first principles.

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)",16,0,0,False,False,False,1641691696.0
rzc739,hru6303,t3_rzc739,"Lambda Calculus - Computerphile https://www.youtube.com/watch?v=eis11j_iGMs

The fundamental concept of programming (writing computations) can be described in terms of Lambda Calculus. There are actually numerous models of computing (and programming) that describe the meaning of computing. Lambda Calculus is very useful because it is a model that requires very little in its fundamentals, and yet the little it does have is capable to describe ""everything"".",9,0,0,False,False,False,1641682595.0
rzc739,hrunmpo,t3_rzc739,"The best way is to take a compilers class if this is what you want to learn. But if you want an intuition of how to “program” you need to just write more programs, and practice.",8,0,0,False,False,False,1641689884.0
rzc739,hrw9iv7,t3_rzc739,"You may like the math for programmers book (https://www.manning.com/books/math-for-programmers). It's aimed at programmers trying to learn math, but maybe you could use it to learn programming in a familiar context.",4,0,0,False,False,False,1641720923.0
rzc739,hrx62ag,t3_rzc739,"You should learn how compilers are made. Then you really get it. Grammars, Lexers, Code optimization, ...",4,0,0,False,False,False,1641740989.0
rzc739,hrwgcp2,t3_rzc739,[https://teachyourselfcs.com/](https://teachyourselfcs.com/) this is a good resource for everything CS,3,0,0,False,False,False,1641726257.0
rzc739,hry54xc,t3_rzc739,I’m seeing a lot of info on how to program which I don’t really believe would be “programming theory”. Equally there isn’t really such a thing that I’ve ever heard of. You’re best bet would likely be anything on compilers. The closest to theory would be what is usually classified as “Formal Languages/Automata Theory” this would be a very theoretical/mathematical approach towards how we define a programming language/computer model. You would probably specifically be looking at context free grammars (CFG),3,0,0,False,False,False,1641754189.0
rzc739,hrvfnd4,t3_rzc739,Sicp?,2,0,0,False,False,False,1641702226.0
rzc739,hrybmql,t3_rzc739,"What you’re looking for isn’t usually taught to people who don’t already know how to program pretty well. There are classes on programming language theory but they’re typically at least senior level and the way they’re typically taught relies on knowledge someone new to programming wouldn’t have. That said Programming Language Pragmatics by Michael L. Scott is really good, and I believe online PDF copies exist. So I’d find out what language you’re going to be using and which concepts would be relevant, because a lot of things in the book won’t be relevant to what you’ll need and would be very confusing.",2,0,0,False,False,False,1641756456.0
rzc739,hrvt8w4,t3_rzc739,"It sounds like you and I are in a similar enough situation and frame of mind at the moment. I am starting to learn programming at the moment as well and share a similar learning style. I’ve concluded that introducing theory, first principles, and abstract terms is introduced best through a language taught as an introduction to computer science. Graphing calculators integrate Python so I have chosen to start with Python because of that.

Here are the books I’ve ordered:

• Python Programming: An Introduction To Computer Science – 3rd Edition – John Zelle

• Learn Python3 The Hard Way: A Very Simple Introduction To The Terrifyingly Beautiful World Of Computers and Code – Zed A. Shaw 

• Learn More Python3 The Hard Way: The Next Step For New Python Programmers – 1st Edition – Zed A. Shaw

• Python Crash Course: A Hands-On, Project Based Introduction To Programming – 2nd Edition – Eric Matthes 

• Head First Python: A Brain-Friendly Guide – 2nd Edition – Paul Barry 

• Python Pocket Reference: Python In Your Pocket – 5th Edition – Mark Lutz

• Automate The Boring Stuff With Python: Practical Programming For Total Beginners – 2nd Edition – Al Sweigart 

• Django For Beginners: Build Websites With Python and Django – William S. Vincent",2,0,0,False,False,False,1641709588.0
rzc739,hrw2h47,t1_hrurxml,"A Crash Course in Computer Science is amazing! I can't believe how much information it conveys in those short, fun to watch and easy to understand videos. Great work.",3,0,0,False,False,False,1641715684.0
rzc739,hrukw05,t1_hru6303,"Thank you. I know who Alan Turing in and I think the inventor of Lambda Calculus.

Is there anywhere that has terms that would help me understand?

(I super hate applied things. I have a theory oriented mind so I need to to understand the terms and what they mean first. I don’t pick things up if they are not explicitly
 stated—blame the autism. )",2,0,0,False,False,True,1641688731.0
rzc739,hruoizq,t1_hrunmpo,Not good at learning by application. I prefer to get a gist of the theory. And understand the terms in the least general manor possible. Like how I learned algebra before trig or learned the parts speech and what they mean when learning grammar syntax.,-1,0,0,False,False,True,1641690246.0
rzc739,hrx65pl,t1_hrunmpo,This!,1,0,0,False,False,False,1641741029.0
rzc739,hryhwma,t1_hrx62ag,Those sound like terms I would like to understand. Thank you.,1,0,0,False,False,True,1641758683.0
rzc739,hryik5e,t1_hrvt8w4,"Thank you. I am not sure what languages physicists and mathematicians use. I hear Python is good. I am not a big fan of programming, but I need it for my major.

I am much more interested in maths, but for an analysis course, I will also need programming. I think Python is good for maths/physics. I will check those out.",1,0,0,False,False,True,1641758914.0
rzc739,hruzugj,t1_hrukw05,">Thank you. I know who Alan Turing in and I think the inventor of Lambda Calculus.

 FYI, Alonzo Church came up with Lambda Calculus; Alan Turing came up with State Machines.  Since Lambda Calculus is meant to be stateless, they are different approaches to the theory of computation.   
As a Mathematician, I would also recommend looking into Automata, which is the theory behind state machines and grammar.",7,0,0,False,False,False,1641695108.0
rzc739,hruu0hr,t1_hruoizq,"You could search youtube for what you are looking, but all I can say is the best way is to by doing. Programming is a very practical field, theory will only get you so far - and even after reading it may not be clear how something works until you do it yourself. 

Pick up some algorithmic topics in Math and code them up, make a differential equation solver - or whatever. You get the idea",3,0,0,False,False,False,1641692588.0
rzc739,hryjlg9,t1_hruzugj,"I have heard of Alfonso Lorenzo is. He was American. I meant I know who Alan Turing is and who I think the inventor of Lamba Calculus is. I knew they are separate people . I just wasn’t sure if he was the right guy. I also know who Claude Shannon is. (My partner has his CS BS and almost got his math minor so they talk a lot about these guys. We’re just speaking of Charles Babbage yesterday).

Are you an applied mathematician? Or do you study theory? I actually want to take partial Diff EQ and complex analysis. I like math.",1,0,0,False,False,True,1641759277.0
rzc739,hruxrf2,t1_hruu0hr,"I need to use this to do Fourier transforms. So yes. Differential equations is what I’ll be working with. 

Do you know Mathematica?",1,0,0,False,False,True,1641694205.0
rzc739,hs3g73b,t1_hryjlg9,I have a BS in IT and a BS in Math(emphasis in cryptology)   In grad school for CS I had to take a couple of prereqs including Automata.   Automata is more math and the theoretical side of computing.  I quite enjoyed it.,2,0,0,False,False,False,1641839389.0
rzc739,hrv6tf8,t1_hruxrf2,"I’m not familiar with Mathematica - but here’s what I can suggest. I’m not sure which language you will be using but SciPy and NumPy are two python libraries that have an api for computing fourier transforms. 

If i were you I’d implement a fourier transform from scratch, and bench mark it against an already implemented version from say like numpy and scipy - that should be good enough to give you an intuition of programming.

my point basically is…. no amount of learning before hand, or theory is enough to make you feel prepared or give you a good idea of programming. u need to dive in head first and start doing it",3,0,0,False,False,False,1641698170.0
rzc739,hryju2l,t1_hrv6tf8,I’ll give it a go. I just want to familiarize myself with terms and a few ideas before hand. I will look into what everyone has posted.,1,0,0,False,False,True,1641759359.0
ryzj9x,hrs0rgs,t3_ryzj9x,"One very niche part of databases I researched last year as part of a databases courses I took was the use of metaheuristic-based optimization algorithms to enhance searches, with the heuristic selection component modeled after animal behavior.  A lot of these are modeled on animals that hunt/scavenge/travel in swarms, like sea creatures or flies, and on average some of them perform more searches faster than traditional or heuristic searches.  Whale Optimization (2016) is one example, but there are many, and more are still being researched.
 https://en.m.wikiversity.org/wiki/Whale_Optimization_Algorithm",31,0,0,False,False,False,1641650895.0
ryzj9x,hrrwl9x,t3_ryzj9x,"You can browse different papers and articles online in Google Scholar, the Papers We Love GitHub repo, and various journals.  Here are January 2022 articles on the topic of databases from [arXiv](https://arxiv.org/list/cs.DB/current)",8,0,0,False,False,False,1641648661.0
ryzj9x,hrs99aq,t3_ryzj9x,Encrypted search is interesting.,3,0,0,False,False,False,1641654939.0
ryzj9x,hrssml2,t3_ryzj9x,"This will be about distributed systems, but  from what I see, there's no great divide between these worlds, distsys people concentrate on databases a lot. I see some ML stuff around tuning and some around SRE, but it's mostly around consensus and distributed transactional systems.",2,0,0,False,False,False,1641662892.0
ryzj9x,hruwkzi,t3_ryzj9x,I recently took a course in blockchain algorithms so we talked a lot about distributed systems and made a lot of connections to concepts from databases such as three phase commits. It's not necessarily databases but there are a lot of similarities and the different applications might interest you.,2,0,0,False,False,False,1641693696.0
ryzj9x,hru96me,t3_ryzj9x,Something to do with a boy's cod?,1,0,0,False,False,False,1641683884.0
ryzj9x,hruv9kh,t3_ryzj9x,Self driving databases are a current topic of active research.,1,0,0,False,False,False,1641693125.0
ryzj9x,hrsod7q,t1_hrs0rgs,Blew my mind,7,0,0,False,False,False,1641661226.0
ryzj9x,hrwgupg,t1_hrs0rgs,Mindblowing! I never thought biology and physics could be used to optimize databases!,1,0,0,False,False,True,1641726645.0
ryzj9x,hskdceo,t1_hruv9kh,"This is actually my fav because it's a way to mathematize and automate databases, which are already very.beautiful structures",1,0,0,False,False,True,1642121978.0
ryeny8,hrodpi7,t3_ryeny8,There's probably a higher chance teaching math will become automated before programming ever does.,272,0,0,False,False,False,1641583540.0
ryeny8,hro9vqi,t3_ryeny8,"> A career counsellor said that I should teach math (my other possible career goal) rather than go into software development, since the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years.

Hear me and heed my words very carefully: your counselor is a *fucking idiot*. This cannot be understated. I'm nearly speechless. Computers CANNOT THINK. And no matter how clever our algorithms are going to get, they will only be able to produce work within the confines of said algorithm. That means innovation will still come from humans, because it cannot come from machines.

And in order to create software, someone has to tell the machines precisely what is desired. It almost sounds like... Programming... Business people and non-engineers with no idea how computers work or the nuances of computer science and software will never be able to capture the requirements and edge cases of the thing they desire. People are also very bad at knowing what they actually want. This takes professionals to do this work.

Our industry, our field, is quite, quite safe from being automated into obsolescence. The only way your career counselor can possibly be correct is if we hit the singularity in that time frame, where humans develop synthetic life, it grows exponentially, and we hand off society as a whole to it.",312,0,0,False,False,False,1641582135.0
ryeny8,hro91y4,t3_ryeny8,"I don't know a lot about it, but people said the same thing about visual basic. And developer jobs have continued to rise.",40,0,0,False,False,False,1641581833.0
ryeny8,hrodb3t,t3_ryeny8,"We have had no-code and low code options for 20+ years. Check out https://old.reddit.com/r/sysadmin/ for the horror stories of important business processes that have been built around Access Databases and Excel Spreadsheets, ask them about Visual Basic and Foxpro, both low code options from the previous century that still haunt the corporate world. 

No-code, low-code falls apart as soon as there is not a widget that does what you need them to do. If you need to do anything hard it becomes a real programming effort, the harder it is the closer to the machine you need to get.",37,0,0,False,False,False,1641583391.0
ryeny8,hrof1b7,t3_ryeny8,"No code is someone else's code that you don't have control over. 

Machine learning code generation is just that, code generation. It saves you a step from having to go to google stuff and copy-paste from stack overflow. Code is not software. In fact, there are some estimates I read that coding is only about 10% of a job of a software developer. Even if it becomes fully automated/obsolete (and it won't), you'll still have the other 90%. And it's not like all the code already written will magically become self-supporting.  


I bet that career counsellor types with just two index fingers and has to look at a keyboard and mouth the letters while typing.",31,0,0,False,False,False,1641584030.0
ryeny8,hrodj3v,t3_ryeny8,Lol,28,0,0,False,False,False,1641583473.0
ryeny8,hrocske,t3_ryeny8,"As someone who knows both dev and AI/NLP, i echo the words of someone else here: your counselor is a fucking idiot",47,0,0,False,False,False,1641583205.0
ryeny8,hromffp,t3_ryeny8,I think you should tell your counselor to move into programming because in 10-15 years he will be the one jobless.,12,0,0,False,False,False,1641586777.0
ryeny8,hrol0oj,t3_ryeny8,"You can create a website or an app nowadays without coding much but to create a system which is highly scalable and available, you'll still need good programmers and engineers. That's not something a machine or non-technical person can do.",11,0,0,False,False,False,1641586246.0
ryeny8,hronxru,t3_ryeny8,"The people automating jobs away cannot be automated away.

I NEVER found my counselors useful, they literally are counselors and not X profession/career you actually want to get into. That should always be a major red flag on taking their advice on anything.

The number one most important thing about who's advice you listen to is this: Find the people that have/do what you want and go and ask them how to get it/do it.",8,0,0,False,False,False,1641587340.0
ryeny8,hro9tvy,t3_ryeny8,"If it happens, you can always change career to teach math. But its unlikely to. Machine learning requires a lot of developers right now and is likely to do so for the near and medium future.",7,0,0,False,False,False,1641582116.0
ryeny8,hrotv2d,t3_ryeny8,"Your counselor is an idiot, full stop. The need for programmers is only accelerating. I’ve been at this since I was 14 coding in my bedroom dreaming about what could be in the future (thanks TI-99, Sinclair 1000, C-64, TRS-80, and IBM AT. You were all good friends). I’m 52 now and we are only scratching the surface. I still dream about what could be in the future.

For a smart kid from poor to lower middle-class background, this career is the surest and fastest way to a solid 6 figure income in flyover country. Factory automation, agriculture automation, websites, mobile apps, VR games, warehouse management, CRM, ERP, hospital EMRs, air quality analysis, self driving cars, wearable tech, robotics, AR systems for military, quantum computing, smart homes, I could go on and on, but its just going to get more and more intertwined with our lives. Now it is still just an addition to our lives, in 30 years it will be fully intertwined. 

Feel free to msg me privately and I will be happy to mentor you through your college applications.",5,0,0,False,False,False,1641589545.0
ryeny8,hro9uof,t3_ryeny8,"> I should teach math (my other possible career goal) rather than go into software development

Have you looked into how much teachers get paid? Ha!

> the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years

They were saying the same thing about outsourcing in the 2000’s and look how that has played out. Also, did CAD replace engineers in other fields? Lol.",5,0,0,False,False,False,1641582124.0
ryeny8,hroqnw6,t3_ryeny8,"People who think these kind of things threaten developer jobs don't know about AI & development (or the difference). There's nearly zero overlap. Developing an AI for a task requires AI specialists, has high resource costs, and is significantly more pricey than hiring developers; also it only does pattern matching, so anything that doesn't fall in that purview requires a developer which is... mostly everything. They are both very different use cases.

Developers aren't going anywhere anytime soon.",5,0,0,False,False,False,1641588357.0
ryeny8,hrofhc7,t3_ryeny8,"The consensus, a resounding no, your career opportunities are not under threat.",4,0,0,False,False,False,1641584193.0
ryeny8,hrot1n8,t3_ryeny8,"I fully agree with the other comments here, but thought I'd provide a little bit of evidence as to how important software developers are when working with low-code platforms.

I was applying for new jobs about a year ago, and noticed that major corporations like Thales use a low-code platform called Mendix, which seems to be a major player in the industry. Thales is currently hiring Mendix developers for multiple positions, such as [this advertisement for a technical lead](https://thales.wd3.myworkdayjobs.com/en-US/Careers/job/Crawley/Mendix-Developer_R0114396-1). Note the requirements for the position: Multiple years of experience, experience with all levels of software stacks, experience in programming languages, experience in security...

These are all skills that non-developers do not have and most likely will not learn. Even if low-code platforms actually take off (and that's a big if), software developers will be the ones writing the code (or the not-code, or whatever).",4,0,0,False,False,False,1641589238.0
ryeny8,hrovixx,t3_ryeny8,"I suppose your counselor is not aware of ""The halting problem""",5,0,0,False,False,False,1641590176.0
ryeny8,hrofbg3,t3_ryeny8,Terrible advice on your counselor’s part.,8,0,0,False,False,False,1641584133.0
ryeny8,hroh1x0,t3_ryeny8,"That’s a very foolish position for your counselor IMO.

Computer science and computer programing are only going to increase in value.

Who is going to program the AI to code? A computer programmer. 

Understanding the computer world is going to be increasingly central to everything. And your knowledge as a computer programmer will adapt to the changes if your interest is serious. 

Honestly, teaching math might not be a job in 10 - 15 years because through computer programs getting better and AI, there in all probability won’t be a need for teachers in the current sense.

Your counselor is clueless.",6,0,0,False,False,False,1641584773.0
ryeny8,hrow31h,t3_ryeny8,"LOL No. 

But it does add a whole bunch of new projects for the developers to work on. 

Software engineering will literally be the last job where machine can completely replace human counterpart. It might still happen some time in the distant future. And once that happens, it’s game over for the entire civilization, not just developers. 

We got time.",3,0,0,False,False,False,1641590384.0
ryeny8,hrrdmvr,t3_ryeny8,"Your career counsellor has no fucking idea about software development. This is like saying don't become an architect don't become a carpenter don't become a roofer because we have drills today!

I would even report this guy. Developers are in high demand and telling people who are interested in this field to not go this way is the complete opposite what he should do.",3,0,0,False,False,False,1641635018.0
ryeny8,hroe8td,t3_ryeny8,"Honey, I’m gonna put this in a very simple example.

Tell me any virtual assistant (Google Assistant, Siri, Alexa, Cortana) that you have used and never made 1 mistake.

Computers are nowhere near ready to replace developers. Computers cannot find flaws on there own yet. They can only find what they are instructed/deeply trained to find. Humans on the other hand can use knowledge they possess and apply it to anything they see fit. Computers cannot.

Your consular is a prime example of why applications are made toddler safe.",6,0,0,False,False,False,1641583737.0
ryeny8,hroprvg,t3_ryeny8,"Bad / entry level crud developers ? Yes

Complex custom solutions / enterprise development ? No",4,0,0,False,False,False,1641588026.0
ryeny8,hrp5xhk,t3_ryeny8,Your career counselor is a clown,2,0,0,False,False,False,1641594119.0
ryeny8,hrqdqm8,t3_ryeny8,"Aaahhhh, I love how confident some people are in making predictions of industries they've probably never touched.

(I'm talking about the counsellor, not OP)",2,0,0,False,False,False,1641612633.0
ryeny8,hrqr33b,t3_ryeny8,Your counselor is a moron,2,0,0,False,False,False,1641619363.0
ryeny8,hrr5l7g,t3_ryeny8,"No... developers will need to troubleshoot and fix why all the ""no-code"" shit isn't working",2,0,0,False,False,False,1641628783.0
ryeny8,hrreqrt,t3_ryeny8,Your counsellor is a downright moron. Period.,2,0,0,False,False,False,1641635914.0
ryeny8,hrrrxrn,t3_ryeny8,Do you want to make the mistake of your life? Cuz that's how you do it,2,0,0,False,False,False,1641645840.0
ryeny8,hrp2swl,t3_ryeny8,Your career counselor should look for another job.,1,0,0,False,False,False,1641592919.0
ryeny8,hroerj1,t3_ryeny8,"You should look at /r/programmerhumor sometime. There's often a screenshot of the absolute 10/10 shite that Copilot can produce. It can however sometimes give you something useful, but it still needs a hint of what you want it to write. Someone had to think about what needs to be done and more importantly *how*.

I doubt we'd be able to make an AI that can properly make a beautiful and fast frontend with good UX or make a sensible and proper DB design for a backend environment. Even if we're able to do that in the next, I don't know, 40 years it also has to be cheaper to use than paying a team of developers .",1,0,0,False,False,False,1641583930.0
ryeny8,hropha1,t3_ryeny8,"We have a low code platform from Outsystems, all those guys do is ask us SAP folks to build a Odata service for them to consume the data. They can't do shit on their own.",1,0,0,False,False,False,1641587915.0
ryeny8,hrot0qt,t3_ryeny8,"As many have already stated, I'd ignore your career counselor. Low code/no code solutions automate something that should be automated, the easy stuff that we shouldn't be working on because it's easy. It's just a new abstraction layer. This is great, because it means we can focus on the hard stuff that hasn't been solved yet.

I will caveat, there is a set of software developers who may lose their jobs because there are folks who are paid to write code that really should be automated, but isn't for some reason (usually ""enterprise"" inefficiency.) And, some of those coders lack real problem solving skills and won't be able to transition to more meaty problems.

If you have a strong interest in math and code, you should find plenty of opportunity to work on problems that haven't been solved. That's not going away until, as someone pointed out, we reach the singularity, and then we're all out of work :)

P.S. Thanks for asking this here, it's a really interesting topic, and a good wake up call that people are buying a bit too much in to the hype of low code/no code.",1,0,0,False,False,False,1641589229.0
ryeny8,hrotxj9,t3_ryeny8,who watches the watchmen?,1,0,0,False,False,False,1641589571.0
ryeny8,hroyfy2,t3_ryeny8,"If anything, developers will be managers of robots, but not without jobs. Your advisor sounds like an idiot.",1,0,0,False,False,False,1641591275.0
ryeny8,hrp32rh,t3_ryeny8,"In order to automate a programmers job, first we need our clients to accurately describe what they want.  I have yet once in my career experienced this.",1,0,0,False,False,False,1641593024.0
ryeny8,hrp5y67,t3_ryeny8,Your career counselor is a wrong,1,0,0,False,False,False,1641594127.0
ryeny8,hrp81o5,t3_ryeny8,"There will be developer jobs for decades to come, and maybe even until the singularity.",1,0,0,False,False,False,1641594943.0
ryeny8,hrp8orx,t3_ryeny8,"Only to a point.
When electrical and mechanical computer systems, they took away the jobs of ""computer"" people. But they created jobs in operating and programming these computers. When programming could be written in the memory of the machine, it took away the need to dig through wires and switch them around. When compilers were created, it took away the job of writing in assembly. But it created jobs in creating those compilers, one for every machine, one for every language. When these compilers can be made automatically, if they ever do, there will still be jobs in writing programs; there already were. Even if we create meta-compilers, compilers that take in entirely human language and compile that into a common programming language, which is then compiled into assembly, there will still be the creative process above it all. And I can say with almost certainty that a computer will never design a game from scratch.",1,0,0,False,False,False,1641595193.0
ryeny8,hrpb430,t3_ryeny8,"Maybe I am naive regarding how far AI coding can go, but I work on a firewalling application, and when I receive a requirement along the lines of ""There's this type of traffic X which we are failing to identify quickly enough and make a drop decision  before various NIC/system buffers get shot and we start dropping everything"", I feel like no matter how clever the AI there's still a role for a dev--type human in there somewhere.


Predicting the future (especially tech) is hard though, so 🤷",1,0,0,False,False,False,1641596140.0
ryeny8,hrpby5s,t3_ryeny8,"No, that’s kinda like saying “ugnert invent fire, stop messing with wheel science already figured out” low code no code solutions are tools that have been coded to allow someone to do something without understanding how to code. They do not create new capabilities, that still needs to be coded, and the integration and expansion of these capabilities will need to be coded.",1,0,0,False,False,False,1641596473.0
ryeny8,hrpc7n7,t3_ryeny8,"When I started university, OOP and code generators were all the rage, and people were hyping that UML, automatic diagram to code generators, etc. gonna end most of the coding. Also, while AI winter wasn't officially over, it's rumored that AI will be able to code, and will be much better at it (w.r.t. humans) in 10 years.

**Result:** >!It didn't happen like that.!<",1,0,0,False,False,False,1641596577.0
ryeny8,hrpcqiu,t3_ryeny8,"The day a software engineers job is automated is the day that literally nobody will have a job anymore. 

Even with these tools - someone's gotta design the systems behind it. AI can adapt but it has to have the foundation to do so first.",1,0,0,False,False,False,1641596787.0
ryeny8,hrpcwh0,t3_ryeny8,"Well some peoples jobs, but it would be balanced out by more jobs being created. 

Thats the thing about software, every time something gets easier, it just means that there are more things you can now build on top of it.

Until(if) we reach the singularity, no software is ever going to absolete the developer.",1,0,0,False,False,False,1641596852.0
ryeny8,hrpkf7v,t3_ryeny8,We are still very short on skilled software developers.,1,0,0,False,False,False,1641599901.0
ryeny8,hrplnu0,t3_ryeny8,"I develop a low code plugin and I totally agree with everyone else here. We need more developers and our best customers are still developers even if just about anyone could build a CRM without code on our platform.

Our users that aren't advanced only get closer to being developers as they use our platform. AI can't do much more than what we've done ourselves over 12 years of listening to customer support tickets and reacting to what people actually tell us they want.",1,0,0,False,False,False,1641600406.0
ryeny8,hrpyhpo,t3_ryeny8,"If you work in IT or development, you will know it couldn’t be further from the truth.

Seeing first hand all of the issues that are fundamentally related to these kind of tools - tools that aim to abstract difficult computational tasks - makes it easier to just intuitively understand why its silly to suggest that programming jobs will be fewer in 15 years.

Such tools usually only work well in typical and expected scenarios and there is always some kind of trade off - less efficiency, less control, less interoperability, less portability, less capability, lack of underlying understanding, higher costs, security concerns… the list goes on. 

 Programming jobs are more likely to continue to increase than anything else.

You should still consider your options though - because teaching Math sounds like a cool choice too.",1,0,0,False,False,False,1641605858.0
ryeny8,hrpzpyi,t3_ryeny8,"Well, if you're gonna lose your job (as you counsellor said) in 10-15 years make sure you earn so much money that you can start with anything in the future without any hassle.

Don't take advice from someone who isn't in the field i.e. The counsellor.",1,0,0,False,False,False,1641606388.0
ryeny8,hrpzvt9,t3_ryeny8,Nah.,1,0,0,False,False,False,1641606460.0
ryeny8,hrq2w8u,t3_ryeny8,"Good developers are always trying to put themselves out of a job, in a sense.",1,0,0,False,False,False,1641607782.0
ryeny8,hrq3ube,t3_ryeny8,"AI coding will sure save time from programmers, but you still need to write out what you want to do. The AI is unaware of more abstract concepts than objects or lists. You still need to think on a high level about what you want to do. So no.",1,0,0,False,False,False,1641608197.0
ryeny8,hrq462m,t3_ryeny8,My lead told me react and angular devs will be replaced by AI that can build UI.,1,0,0,False,False,False,1641608338.0
ryeny8,hrq5a4u,t3_ryeny8,Can you please show them this thread,1,0,0,False,False,False,1641608825.0
ryeny8,hrq65om,t3_ryeny8,"Ha, hahaha, hahaha no.  Those tools, if they help at all, will just make existing developers faster.  And they probably won’t even drop the number of new devs being added in the coming years.

I think that laymen forget that programming isn’t just syntax.  Look at scratch, which could be seen as a “low code” platform.  Ask the average business goon to look at a bright beautiful low code platform and see if they can tell whether a loop will halt or continue forever.  Programming is about thinking algorithmically, not about arcane symbols.",1,0,0,False,False,False,1641609215.0
ryeny8,hrqbm8a,t3_ryeny8,"Why would anyone want to go to class and watch you teach when they can pay for an online recording video of the ""best teacher in the world"" from somewhere in Australia for instance lol",1,0,0,False,False,False,1641611661.0
ryeny8,hrqfa7h,t3_ryeny8,Need a developer to make pasta primavera out of the spaghetti those tools churn out.,1,0,0,False,False,False,1641613349.0
ryeny8,hrqi9df,t3_ryeny8,"Yes, shitty developers will have their jobs threatened.  Good developers will be alright.",1,0,0,False,False,False,1641614757.0
ryeny8,hrqjp9i,t3_ryeny8,I have a neighbor who spouts this bullshit constantly then apologizes to me like I even give a fuck. It’s whackadoo… if a computer does something then someone programmed it to do so.,1,0,0,False,False,False,1641615478.0
ryeny8,hrqlmqr,t3_ryeny8,no,1,0,0,False,False,False,1641616473.0
ryeny8,hrqobez,t3_ryeny8,"I'm in QA, and I promise you anyone who thinks a computer can write code has never read a story's acceptance criteria written by a customer.",1,0,0,False,False,False,1641617871.0
ryeny8,hrqoge4,t3_ryeny8,"If you think it will, then you don't know what programming is or AI !",1,0,0,False,False,False,1641617944.0
ryeny8,hrqpzt2,t3_ryeny8,"Producing code is a very small part of software engineering. Most of the work of a software project consists of deciding how to handle all possible types of inputs and failure modes that a system could encounter, in order to make that system serve specific goals.

In [Programming as Theory Building](http://pages.cs.wisc.edu/~remzi/Naur.pdf), Peter Naur argues that a team's understanding of a program is their true product.

When code comes from a third party (such as an AI), it is **harder**, not easier, to review the code and make sure that it does what is wanted without introducing any unacceptable risks. This is like the difference between building a new house, and renovating an old house without knowing whether it contains any lead or asbestos or radon.",1,0,0,False,False,False,1641618771.0
ryeny8,hrqufic,t3_ryeny8,Can I use copilot for basic python ?,1,0,0,False,False,False,1641621257.0
ryeny8,hrqux5l,t3_ryeny8,"Red queen hypothesis applies here.

As the tools grow more capable and complex the tasks become more complex and demanding. What a developer once did with 300 lines of assembly can be done in 5 lines of C++ or 2 lines of python. But they don't want that anymore. They want you to use python to set up a neural network in Keras, in the time that programmer got to write 300 lines of assembly. But now what your doing would represent 100,000s of lines of assembly.

So if we someday have a tool like the holodeck in star trek where you can say ""make me a program that simulates all the behaviors or Sherlock Holmes"" the programmers will be doing something that is still a lot of work given those tools.",1,0,0,False,False,False,1641621551.0
ryeny8,hrqxvfq,t3_ryeny8,"For Gov jobs, maybe. For the rest, no. But be prepared to relearn everything on a regular basis.",1,0,0,False,False,False,1641623375.0
ryeny8,hrqyged,t3_ryeny8,"In the future, you will have to be extraordinary in your field to sustain. AI is gonna replace average programmers and average math teachers too. You will have to be extraordinary to sustain in the highly-competitive world. Do what you want to do, but be the best at it.",1,0,0,False,False,False,1641623752.0
ryeny8,hrr5w0y,t3_ryeny8,"1. Those things do not create program, they write code. They have no concept of why. It can't make decisions, talk to clients or handle anything else then code.

2. If an ai tool like this can replace your entire job right now, then what the hell are you doing?",1,0,0,False,False,False,1641629003.0
ryeny8,hrragrb,t3_ryeny8,"Have prefabricated houses affected builders jobs? No. If you need some technological shit doing right, you need to hire a coder. The rest is scam. You can tell your mentor to keep studying math. People who say that kind of stuff, never have coded entrepise applications, and there are a lot of them. Learn to code, the rest is shit. I am 30, I have been coding since I was 12. I am an entrepreneur now, and It is super hard to find good programmers to work with, so PLEASE keep studying how to code.",1,0,0,False,False,False,1641632523.0
ryeny8,hrrcqgx,t3_ryeny8,"[Relevant CommitStrip](https://www.commitstrip.com/en/2016/08/25/a-very-comprehensive-and-precise-spec/).
No matter how abstract the specification is, you will always need someone to work on it. And even if you don't stay up to date, there will also be businesses with old tech. I hear COBOL developers are doing fine.",1,0,0,False,False,False,1641634301.0
ryeny8,hrrcs6h,t3_ryeny8,"Like some people have already mentioned, just throwing the word ""machine learning and artificial intelligence"" does nothing. We're far far away (it won't be an exaggeration to say about a century) from having something remotely close to Jarvis, Friday or Edith. So no, AI won't be the one who would be making dev jobs obsolete.

Secondly, it seems like you haven't worked at scale. The low code and no code tools only scale so much or provide exactly the features you're looking for. Sure if you're making a majorly static application then yeah those jobs are ALREADY gone. But other than that we're doing as great as ever.",1,0,0,False,False,False,1641634340.0
ryeny8,hrrl100,t3_ryeny8,No we will just be stuck in bug reviewing hell,1,0,0,False,False,False,1641640885.0
ryeny8,hrrmnyi,t3_ryeny8,"I wouldn't worry till the Terminators start reproducing, then we're screwed anyway.",1,0,0,False,False,False,1641642136.0
ryeny8,hrrn1jz,t3_ryeny8,"We've had no code tools for a long time my friend, they only create more work for us, and open new doors.

Also, your counsellor is ignorant. We already automate everything and we just do more as a result, not less",1,0,0,False,False,False,1641642419.0
ryeny8,hrry21u,t3_ryeny8,Computer science is about a lot more than just writing code. It would harm some jobs probably. If you’re still in school might I suggest gearing your education towards AI itself?,1,0,0,False,False,False,1641649471.0
ryeny8,hrs96np,t3_ryeny8,"You’re career counselor is an idiot. Find a new one.

Secondly, computers are stupid until people make them smart. No code had to be coded by someone and there will always be something new that needs created via code",1,0,0,False,False,False,1641654905.0
ryeny8,hrse93k,t3_ryeny8,"I was just in the process of ordering something from an online shop created by squarespace when the whole thing seizured, lost my order, and then dumped me onto a different product screen with no way to get back to my cart.

Don't worry, software engineers aren't going anywhere.",1,0,0,False,False,False,1641657105.0
ryeny8,hrsnl5p,t3_ryeny8,Who builds the low code /no code tools?,1,0,0,False,False,False,1641660917.0
ryeny8,hrsqzzf,t3_ryeny8,"10-15 years? We’re going from “some specialized NLP algorithms can produce a simple Python function given a detailed description of what it does, with a reasonable chance that it won’t be correct” to “literally developers aren’t necessary anymore” in half the time it took to get from Java 1 to Java 8? Yeah I think you’re safe",1,0,0,False,False,False,1641662257.0
ryeny8,hrsrs51,t3_ryeny8,My college counselor told me not to major in computer science and computer engineering because there were too many people going into the field and there would never be enough jobs out there for all of them.  That was 2003-04. Don’t listen to those idiots and do what you enjoy.,1,0,0,False,False,False,1641662559.0
ryeny8,hrst4v2,t3_ryeny8,"Very likely not the case and even if, it will definitely create new jobs in a similar field where you already have atleast some experience and you can always learn new stuff",1,0,0,False,False,False,1641663090.0
ryeny8,hrsutnb,t3_ryeny8,Another comment said “listen to me carefully: your counselor is *a fucking idiot*.” I couldn’t have said it better myself.,1,0,0,False,False,False,1641663747.0
ryeny8,hrszkpw,t3_ryeny8,"The AI can only write a valid program if someone wrote the test to prove it.  AI is science, science only let's us observe what we think is true.  It can't discover and assert true by itself.",1,0,0,False,False,False,1641665585.0
ryeny8,hrt0pix,t3_ryeny8,"Before that happens, career counselling is going to get automated. Hope your counsellor is preparing for a new role- maybe math?",1,0,0,False,False,False,1641666024.0
ryeny8,hrvkull,t3_ryeny8,I’m sure the career counselor is an expert in tech.. that’s why they’re a counselor rather than working in any industry.,1,0,0,False,False,False,1641704875.0
ryeny8,hrwgemi,t3_ryeny8,AI technology is getting too much hype if the general public believes that there will be not need for human-generated code 10-15 years from now.,1,0,0,False,False,False,1641726297.0
ryeny8,hvwp0wi,t3_ryeny8,"It's like saying the popularity of Canva will get rid of professional graphic designers. Although Canv has democratized design to a great extent, and made it more accessible for small businesses and solopreneurs, it still does not replace a degree in design.",1,0,0,False,False,False,1644210171.0
ryeny8,hrp5vti,t3_ryeny8,Your career counselor is a fucking idiot,0,0,0,False,False,False,1641594101.0
ryeny8,hrp60v5,t3_ryeny8,Your career counselor is a wrong. SWE isn’t going anywhere.,0,0,0,False,False,False,1641594156.0
ryeny8,hrp62qe,t3_ryeny8,Your career counselor is a wrong. SWE isn’t going anywhere and it’ll always pay more,0,0,0,False,False,False,1641594176.0
ryeny8,hrotih6,t1_hrodpi7,"Hah, that's actually a great point!",35,0,0,False,False,False,1641589413.0
ryeny8,hrpctlm,t1_hrodpi7,Or really every other job.,16,0,0,False,False,False,1641596821.0
ryeny8,hrqul08,t1_hrodpi7,"Once programming is automated we either do not need any jobs, or we, do not need any jobs.",13,0,0,False,False,False,1641621350.0
ryeny8,hroe2wc,t1_hro9vqi,"God, I hate when people hear some buzzwords and then start making sweeping predictions about what is going to happen in the future, especially when they are your average person and not some high-up, specialized, and/or industry leading person. Who does this counselor think will be making these no-code platforms? Who does this person think will be using these no-code platforms even if they take off? In my experience there are many jobs out there where having math and computer science skills would always be an advantage over your average person even if the job doesn't good super in depth with either skill.",81,0,0,False,False,False,1641583677.0
ryeny8,hrokoxe,t1_hro9vqi,You want to know a job bound into obsolescence: career counseling!,38,0,0,False,False,False,1641586123.0
ryeny8,hrocurm,t1_hro9vqi,Preach,18,0,0,False,False,False,1641583227.0
ryeny8,hrqv9eb,t1_hro9vqi,"It really ignores that as capabilities grow, demands grow. When we got railroads did all the hauling get done by 5 machines instead of 500 ox carts? No, we decided we needed to haul more stuff further.

When we got more advanced programming languages all the programmers didn't get fired, we decided we could benefit from the more complex software that was now possible.",4,0,0,False,False,False,1641621750.0
ryeny8,hrqvejh,t1_hro9vqi,"By that definition humans also cannot think, as every piece of abstract thought you hold are basically recombinations of observations. Just try to imagine a colour that does not exist, a sound you have never heard before, etc. Our creativity is an abstraction that recombines atomic concepts into newer, bigger scale objects. This is why you can think of a new animal, song or word, which is not all that different from the progress of state of the art NLP and GAN, just at an elementary level.

That being said, the idea of code being automated relatively soon is absurd for so many reasons if you consider the sheer complexity. We can however already synthesize code based on queries, and there are many smart code completion toolings. I'd say the role of developer is simply changing and perhaps becoming even more important. Just look at how critical infra and devops engineers are. Software engineering hasn't been a basic coding job any more for decades.",6,0,0,False,False,False,1641621835.0
ryeny8,hroz9dm,t1_hro9vqi,Thank you for taking time to write this to put a slap across a face-,2,0,0,False,False,False,1641591584.0
ryeny8,hroh705,t1_hro9vqi,"Not quite. You can automate large swaths of the dev landscape away with current tech. It is shortsighted to assume your job is ""safe"". 

When robots are coding that won't stop me from writing code (which may be better and more innovative than what a robot writes, or not). But it may mean that the employment landscape looks different, and it may mean a different relationship between programmer and device at the highest levels of industry.

Edit: more: Fast-forward to UBI, economy 2.0, and full automation, and perhaps only a few very good programmers are working at the highest levels of industry, and most of the rest is automated, except for the millions of open source coders out there who would all of a sudden find that they have the time and the money to make pretty much anything (including better robots). It would be a real renaissance if you could get past the passing of the traditional economy.",13,0,0,False,False,False,1641584826.0
ryeny8,hrptqf9,t1_hro9vqi,"> Hear me and heed my words very carefully: your counselor is a fucking idiot. This cannot be understated. I'm nearly speechless. Computers CANNOT THINK. 

To add on further anyone remember the dumpster fire that was [Github Coploit](https://www.fast.ai/2021/07/19/copilot/)",1,0,0,False,False,False,1641603819.0
rxuuqm,hrktu0c,t3_rxuuqm,"depends on what information the guesser receives after every guess. if you only tell if their guess is right or wrong then the best approach is just start at 1 and increment it by 1 for every guess. time complexity O(n). 

if after every guess you tell the guesser if their guess was lower, higher or equal to the number then you can use binary search. time complexity O(logn).",34,0,0,False,False,False,1641520595.0
rxuuqm,hrktr3o,t3_rxuuqm,"That is the most efficient solution. With no other information, the best you can do is just arbitrary guesses (assuming you never guess the same thing twice). It's pretty easy to prove, actually; you should give it a shot.

Anyway, since there's no algorithm improvement to be had, it comes down to maximizing the number of guesses you can perform per second. That kind of optimization can be somewhat tricky and involved, but I encourage you to look at utilizing parallelism and gpu computation to solve this problem, along with optimizations of the sort used in GNU `yes`. Unroll the living shit out of some loops, lol.",19,0,0,False,False,False,1641520561.0
rxuuqm,hrktlff,t3_rxuuqm,"Can it ask additional question like ""_is it greater than X_""?",6,0,0,False,False,False,1641520497.0
rxuuqm,hrlpx30,t3_rxuuqm,There is no uniform distribution on the naturals so no random distributions are particularly natural. Without more information on this particular distribution the answer to your question is ‘it depends’,4,0,0,False,False,False,1641535463.0
rxuuqm,hrlkmrm,t3_rxuuqm,"Hmm. Well technically if you use multiple guess points that focus at a different starting point you can try to get luckier… but statistically it would be the same time complexity 0(n). Let me explain.

So you explained how you you incremented a number one by one so I assume you start at zero. Well in that loop you probably have something like “while the answer is not equal to guess then increase guess by one” starting at zero.

Well what if at the same time you ran a second guess inside the while loop. And this guess starts from the highest digit you allowed the random number to be chosen from and starts moving one down every iteration. So now you have two variables guessing in the same iteration of the loop. One goes 0,1,2…. While the other goes something like 1000,999,998,997….. 

Now while it sounds like it would make it faster it doesn’t because the two points aren’t being used at the same time; however, I think maybe you could get luckier that way. Just sounds plausible even thought statistically it’s not.",1,0,0,False,False,False,1641532572.0
rxuuqm,hrmbg37,t3_rxuuqm,"While I was reading your post, I thought about set theory. There are infinitely many intergers. If you consider such statement as ""the number is between 10 and 10000"", according to your method, you need at most 10000-10+1= 9991 tries. Each time you try, you have a probability of 1/ 9991 to guess the right number. The statement is the information needed to shrink the size of all possible combinations. It is possible to guess correctly by doing it once, twice, three times....., or in the worst scinario, 9991 times.",1,0,0,False,False,False,1641550586.0
rxuuqm,hrmvvnb,t3_rxuuqm,"The problem of making it any better than O(n) seems to be it is simply random. I see this as you are given a function to check equality with the chosen number, and don’t have access to the number. Therefore it is completely random and unless there is something known about how the guesses are made that’s simply it. Maybe, just maybe, if it’s a bunch of numbers Inputed by people around the world, you can do some ml to find tendencies of what numbers are most likely to be chosen by a person and run those first, worst case would still be O(n) though. Also you can maybe speed it up a bit like that but there is no way to get O(log(n))",1,0,0,False,False,False,1641563370.0
rxuuqm,hrnoyp8,t3_rxuuqm,"Increments of one is likely the best - considering anything else would omit certain possibilities and could lower the probability of guessing the number in the shortest amount of time. 

With that being said, you could try to improve the amount of time it takes by trying all likely probabilities first. For example, if the number is a friends 4 digit passcode, it's more likely to end in 5, 0, or some other number that may hold significance to the creator of this number you're trying to guess. 

Write down all factors that could influence what number you're trying to guess. Then, have the computer guess all - more likely numbers first, and increments of one from there. 

You could have it running several algorithms at once. For example (2,4,6,8,10) (5, 10, 15, 20) and several other more-likely-pattern-based-increments. At the same time, you could have the computer trying series of numbers that may hold significance to the original numbers creation.",1,0,0,False,False,False,1641574659.0
rxuuqm,hrokq03,t3_rxuuqm,"Give the computer a gun, and you will tell it. /s

Otherwise, pick a random number weighted according to the distribution of how you picked the number.",1,0,0,False,False,False,1641586135.0
rxuuqm,hromtdv,t3_rxuuqm,Update: I used multithreading and used 4 threads to speed up the guessing. 2 start on the low end and and the other 2 start at the high end. The pairs of 2 are staggered 1 apart and go up 2 each time and together they go through every number in that direction. Going from both directions also helps because the number is more likely to be on one side than in the middle,1,0,0,False,False,True,1641586920.0
rxuuqm,hrljt0d,t3_rxuuqm,"If I remember correctly, the chance of choosing the correct number that is within a given range is related to the square of that range's size? So if your number is between 0 and 100 you're very likely to guess it within 10,000 (100 squared) tries. I don't remember how the likelihood is actually calculated though.",-5,0,0,False,True,False,1641532145.0
rxuuqm,hrn1h8t,t3_rxuuqm,Use a quantum computer.,0,0,0,False,False,False,1641565828.0
rxuuqm,hrn10se,t1_hrktr3o,How do you prove something like that?,4,0,0,False,False,False,1641565636.0
rxuuqm,hrmzmp4,t1_hrlkmrm,I had actually tried that and it was a few seconds faster on average but sometimes it would take up to like 8 seconds longer if you were unlucky,1,0,0,False,False,True,1641565048.0
rxuuqm,hrn54v5,t1_hrljt0d,"If there are only n options (101 in your example) then you don't need more than n tries to guess it obviously, unless you try already attempted numbers again. So what you are saying is not true.",3,0,0,False,False,False,1641567314.0
rxuuqm,hrnmpd9,t1_hrn10se,You assume theres some algorithm that sloves the problem in less guesses and show you can pick a result that the algorithm wouldnt guess.,5,0,0,False,False,False,1641573852.0
rxuuqm,hrph4g5,t1_hrn10se,About 3 pages of discrete kinda math,1,0,0,False,False,False,1641598554.0
rxuuqm,hrny3og,t1_hrmzmp4,"being faster on average is only likely due to loop unrolling - since you're making 2 guesses per iteration, there is half the overhead for looping",1,0,0,False,False,False,1641577916.0
rxuuqm,hrn5de7,t1_hrn54v5,"I meant random tries, not iterative. As in, how many die rolls would it take to roll a 6? Not necessarily just six, because you aren't just iterating from 1 upward.",1,0,0,False,False,False,1641567407.0
rxuuqm,hro3930,t1_hrnmpd9,"less guesses being o(n)?, how can you tell what the algorithm wont guess without knowing how it works?",1,0,0,False,False,False,1641579745.0
rxuuqm,hrnl1bw,t1_hrn5de7,"Ok. You are talking about the probability of randomly picking some specific element out of a set with n elements.
That's 1/n.
So where does the quadratic part come in?",2,0,0,False,False,False,1641573255.0
rxuuqm,hro873i,t1_hro3930,"Read about the ""Adversary arguments"". It's a technique to validate the lower bound of an algorithm in the worst case.

In the Fundamentals of Algorithms (*Brassard & Bratley*) chapter **12.3**, you can find a very good explanation. The PDF for the book is easy to find on google.",2,0,0,False,False,False,1641581522.0
rxuuqm,hrogutu,t1_hro3930,"You assume theres k guesses where k is less then n (n being the biggest allowed number, such a number must exist just like someone said about uniform over the naturals). After running the algorithm we can denote its guesses as A= {a_i} from i=1 to k. Of course each guess is from 1 to n.

Because n is bigger then k, there exist a number b below n that is not in A. Therefore if the randomly selected number is b, the algorithm wouldnt work.

Its a bit too rigourous, but the point stands.",1,0,0,False,False,False,1641584700.0
rxuuqm,hroqkt1,t1_hrogutu,Just learned this in discrete math (proof by contradiction) and am shocked to actually see it being used,1,0,0,False,False,False,1641588326.0
rxuuqm,hrou42u,t1_hroqkt1,"haha, those kind of proofs are very common.",1,0,0,False,False,False,1641589640.0
rxjst3,hrqhxol,t3_rxjst3,Using tools from real analysis how would you rigorously set this up ?,1,0,0,False,False,False,1641614600.0
rxn4al,hrm4kgk,t3_rxn4al,"The clock could show you if it is day or night throughout the year. This was done by programming the length of the days into the clock (afaik with water).

It is a maschine that calculates (among other things) the length of the day based on an input. It has a ""storage"" unit and a ""logic"" unit.",3,0,0,False,False,False,1641545279.0
rxn4al,hrnoxg4,t3_rxn4al,"A clock can generally be modeled as a cyclical finite state machine with a single input. This is useful in some finite state machine decompositions. Many would say a computer must consistently apply a function over an ensemble of possible inputs so a fully determinate machine like a trivial clock might not qualify. But the castle clock isn't that kind of trivial clock, and it does have many possible inputs, so it's hard to object to calling it a computer.",2,0,0,False,False,False,1641574646.0
rxn4al,hsgmqch,t1_hrm4kgk,"All of those things are true about an hour glass by simply adding and removing sand. A person calculates the length of day and adjusts the input information, water, to provide the desired result the program is always the same though",1,0,0,False,False,False,1642059847.0
rx1mtm,hrfpzgg,t3_rx1mtm,"Let’s say messages are times on a clock. I want people to be able to send me times, but I don’t want other people to know what those times are.

I know that if you add 12 hours to a time, it remains the same, it does a full 360 degree rotation around the clock.

So I’m going to pick two numbers, x and y, such that x+y=12. If I take any time, and add x, then add y to it, it will be the same time.

So I tell everybody what x is, that’s my public key.

When they want to send me a time m, they first add x to it and then send me m+x. Let’s pretend subtraction is extremely difficult, infeasible for anybody to do. So somebody listening into our communications will know m+x, but they don’t know m because they have a hard time subtracting x from m+x.

When I receive this message, instead of trying to subtract x, I’ll just add y instead, which gives me m+x+y=m+12=m, I now have the original unencrypted message. y is my private key which is a secret only I know.

Nobody else can do this because in order to find out what y is, they would need to compute y=12-x, and remember subtraction is difficult.

This is essentially how public key encryption works, a public/private key pair are generated to have the mathematical property that application of both, yields the original message. It also needs to be extremely difficult to compute one from the other.

In the case of RSA this would be based on modular exponentiation and prime factorization.

Modular exponentiation means m^x mod n, for some choices of n, we can find two numbers e and d, such that m^ed = m mod n, for any m. e and d are like x and y, e will be the public key which the sender will use to compute m^e mod n, which is incredibly hard to undo. d will be my private key, which I will use to compute (m^e )^d = m mod n, this is like completing the full circle around the clock and arriving back at the original message. All public key encryption is based on these “circular” type operations, a full cycle takes you back to the original message, this is broken down into two steps, one for the sender and one for decryption, the sender makes a partial cycle, which is incredibly hard to undo, the receiver however knows some secret to complete the cycle.

In order to find d from the public knowledge, which is e and n, you need to prime factor n. However n is a very large multiple of two prime numbers, n = pq. It’s over 1000 digits long, and this makes it infeasible to compute the private key from the public key as there is no quick algorithm for factoring numbers.",44,0,3,False,False,False,1641433129.0
rx1mtm,hrfj9i4,t3_rx1mtm,"It’s not clear what you mean by 
> and I sent it to the receiver through symmetric encryption

Public and private keys are used in the context of assymetric encryption. What is encrypted with the public key can only be decrypted with the private key.",3,0,0,False,False,False,1641430316.0
rx1mtm,hrgx34m,t3_rx1mtm,"I like to think of the ""public key"" as a lock you can put on data, and the ""private key"" as the key to that lock",2,0,0,False,False,False,1641459400.0
rx1mtm,hrfkrvw,t3_rx1mtm,You create key pairs. A private key to decrypt and a public key to encrypt. You exchange public keys with someone to send encrypted messages between yourselves. You use that person public key to encrypt a message and send it to them. The only thing that can unencrypt that message is that person associated private key. They do the same thing sending you a message by using your public key to encrypt the message and you use your private key to decrypt it.,3,0,0,False,False,False,1641430929.0
rx1mtm,hrfr8hm,t1_hrfpzgg,"That sums it up real good, thank you!",7,0,0,False,False,True,1641434161.0
rx1mtm,hrfl3m8,t1_hrfj9i4,If they don't know the private key how do they ow do they encript it so that only my private key can decrypt?,1,0,0,False,False,True,1641431063.0
rx1mtm,hrfkzys,t1_hrfkrvw,How do they encript in a way that only my private key can decrypt?,2,0,0,False,False,True,1641431021.0
rx1mtm,hrhuml5,t1_hrfr8hm,"You can also use this description to work backwards to see how signing things works. We each have an e and a d. e is our public key, d is our private key. To send me a message only I can read, you compute m^e mod n using my public key e. Now only I can read it because only I have the right d to use in (m^e)^d mod n.

To sign a message, you instead compute m^d mod n with **your** private key d. Now the only way anyone can read the message is if they have the right e to compute (m^d)^e mod n, and that choice of e has to be your public key. We can all get your public key, so the message is readable to anyone, but only you could have produced the plain text.",2,0,0,False,False,False,1641479952.0
rx1mtm,hrfos2i,t1_hrfl3m8,With your public key,2,0,0,False,False,False,1641432573.0
rx1mtm,hrfpwr8,t1_hrfl3m8,"Let’s give an example:

You want to send me a document secretly over a public network. I send you my public key but a spy also reads it. 

You encrypt the document on your computer with my public key. Then you send the encrypted document over the network. The spy steals a copy of the encrypted document.

The spy has the encrypted document and the public key. But he needs the private key to decrypt it and I never sent that one out so he cannot decrypt. 

I receive the encrypted document and only I can decrypt it since I am the only person to have the private key that matches the public key.",2,0,0,False,False,False,1641433081.0
rx1mtm,hrflk3v,t1_hrfkzys,They use your public key. The only thing that can decrypt a message encrypted with your public key is your private key,2,0,0,False,False,False,1641431246.0
rx1mtm,hrfn4ey,t1_hrfkzys,https://en.m.wikipedia.org/wiki/Public-key_cryptography,2,0,0,False,False,False,1641431896.0
rx1mtm,hrfl4wm,t1_hrfkzys,by using your public key,1,0,0,False,False,False,1641431077.0
rx1mtm,hrfqzr4,t1_hrfpwr8,"Where where you 2h ago XD I get it, i thought the public key was something that already existed, and not something that I create at the same time I create the private one...",2,0,0,False,False,True,1641433934.0
rx1mtm,hrfln0n,t1_hrflk3v,Can't everyone else use the public key to also find the private key?,1,0,0,False,False,True,1641431280.0
rx1mtm,hrfla0d,t1_hrfl4wm,That way can't everyone else do the same?,2,0,0,False,False,True,1641431133.0
rx1mtm,hrfnksu,t1_hrfln0n,"No. The key pairs are asymmetric. The public key can be openly distributed without compromising your private key.

The generation of such key pairs depends on cryptographic algorithms which are based on mathematical problems termed one-way functions.

https://en.m.wikipedia.org/wiki/One-way_function",2,0,0,False,False,False,1641432083.0
rx1mtm,hrfo61z,t1_hrfln0n,"Yes, but it’s very difficult to do that. It takes a certain large number of calculations, so it is practically impossible to find someone’s private key by using their public key. More specifically, for RSA encryption, you would have to efficiently perform [integer factorization](https://en.m.wikipedia.org/wiki/Integer_factorization).",2,0,0,False,False,False,1641432323.0
rx1mtm,hrfmpyk,t1_hrfla0d,ok so they encrypt the message to you with your public key right? well you use your private key (THAT ONLY YOU KNOW) to decrypt it! Who cares if people can send you encrypted messages? only you can decrypt it!,3,0,0,False,False,False,1641431729.0
rx1mtm,hrfmue9,t1_hrfla0d,The encryption and decryption are asymmetric. Meaning they are one way functions. Someone with your public key can only encrypt a message. Someone with your private key can only decrypt the message encrypted with your public key. This is why you keep your private key private and secure. You usually have a passphrase associated with it for added security.,2,0,0,False,False,False,1641431780.0
rx1mtm,hrflo70,t1_hrfla0d,"You keep your private key,  well...private.",1,0,0,False,False,False,1641431293.0
rx1mtm,hrfo77r,t1_hrfnksu,Yeah that's what I was having problems with... Thank you,1,0,0,False,False,True,1641432336.0
rx1mtm,hrfol2r,t1_hrfo61z,"I got it! Out of curiosity though, is there already a solution for when computers get more powerful and we can efficiently perform integer factorization?",1,0,0,False,False,True,1641432493.0
rx1mtm,hrfo28w,t1_hrfmpyk,"Oohh i get it now, they are both generated by me right, and I share the public one, that is the one used by the receiver...
I don't know how they are generated but that is work for later... Thank you",1,0,0,False,False,True,1641432281.0
rx1mtm,hrgx16x,t1_hrfol2r,"The algorithm already exists! It's just that the the time needed by the fastest supercomputers we have today to break it would be... well, the universe itself won't last that long.

And since Moore's law is slowing down, i.e. there is a limit of how much faster computers can get, then we can fairly confidently say classical computers won't ever be able to defeat such encryption.

But there are now quantum computers, which are especially good at such kind of tasks, thus posing a serious threat to all of the cryptography.",3,0,0,False,False,False,1641459354.0
rx1mtm,hrgz5eu,t1_hrfo28w,"Yep exactly, it's called assymetric key encryption for that reason. Different keys are used for encryption and decryption. Symmetric key encryption on the other hand uses the same key for both.",3,0,0,False,False,False,1641461036.0
rx1mtm,hrfovz1,t1_hrfo28w,"Check out GPG, https://en.m.wikipedia.org/wiki/GNU_Privacy_Guard",2,0,0,False,False,False,1641432618.0
rx1mtm,hrfpj6u,t1_hrfo28w,are you trying to learn PGP encryption?,2,0,0,False,False,False,1641432881.0
rx1mtm,hrfr4ob,t1_hrfovz1,I will!,1,0,0,False,False,True,1641434062.0
rx1mtm,hrfr06m,t1_hrfpj6u,"I am not, i just needed to understand how public key encryption worked, the rest is just curiosity",1,0,0,False,False,True,1641433946.0
rx5u9h,hrgab6n,t3_rx5u9h,"Hi there! Curious, interesting question - but seemingly it may need some clarifications before being approached.

How do we deal with games in which player failed (hit mine) before clearing the field? Just dispense, don't regard at all?

By the ""least upper bound"" you mean in a case of some specially constructed arrangement (to reach such bound), not randomly placed? This seems to depend greatly on relation between number of mines and field size, right?",8,0,0,False,False,False,1641444970.0
rx5u9h,hri0a7o,t3_rx5u9h,"I don't know the answer. But in my playing experience, the true 50:50 are a lot more common near the edges. Also, very important factor is mine density (which is given by you already). I believe that linear increase in mine density results in exponential growth in true 50:50. But this is just my opinion.

Given this and the answer you gave to RodionGork, I think, for general case you need to find the least mine requiring and compatible side by side with itself true 50:50 and spam it all over the plane. An example of this would be a 2 x N grid with N amount of mines, where each collumn has 1 mine in it:

|x|x|3|(3 or x)|...|2|
|:-|:-|:-|:-|:-|:-|
|2|3|x|(the other than above)|...|x|

This pattern has N-1 (considering you know first click is not a bomb) true guesses with 50% mine density. I doubt there can be more guess heavy pattern, as I mentioned before, edges are breeding ground for true 50:50's.",2,0,0,False,False,False,1641482243.0
rx5u9h,hrkutl3,t3_rx5u9h,"If you have a wall of mines in the 3rd row from an edge, then pretty much all the cells in the first two rows need guessing. Then you can probably just partition the grid every 3 rows and design a grid so that half of the mines need guessing (half constitutes the ""3rd row wall"" and the other half needs guessing). This will probably work when X can go all the way up to ~2MN/3",1,0,0,False,False,False,1641521010.0
rx5u9h,hrgb80m,t1_hrgab6n,"Yup to both questions. We only consider a sequence of guesses that could result in a win, but require some cumulatively amazing luck.

And you are a mastermind and specifically constructed the grid yourself to make your unluckiest players feel as miserable as possible. I also sense the assignment of mines to squares is important, but no clue which arrangement would be most devilish",4,0,0,False,False,True,1641445427.0
rwly7h,hrd8i4q,t3_rwly7h,I'd make this a top-6 and add any clean coding book by Uncle Bob.,15,0,0,False,False,False,1641398423.0
rwly7h,hrd1el6,t3_rwly7h,"Thanks for sharing. 

Will definitely come in handy for aspiring software developers like me :-)",5,0,0,False,False,False,1641395628.0
rwly7h,hrfng63,t3_rwly7h,"I like “The effective engineer”

Thanks for the list, definitely going to do some more reading this year 🙂",1,0,0,False,False,False,1641432030.0
rwly7h,hrhe4e2,t3_rwly7h,"Grabbed the Passionate Programmer after reading your post!  I'm only a year or two into the industry and am having many questions, a lot of self-reflection, and fork in the road moments, so hopefully that book will provide some sort of benefit, thanks!",1,0,0,False,False,False,1641471822.0
rwnjr8,hrcxlic,t3_rwnjr8,[deleted],2,0,0,False,False,False,1641394056.0
rwnjr8,hrdybc3,t1_hrcxlic,"I would recommend using a library such as sklearn if possible. [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

  
They have parameters to control the initialization and also the number of different runs.

Also checkout this: [https://en.wikipedia.org/wiki/K-means%2B%2B](https://en.wikipedia.org/wiki/K-means%2B%2B)",3,0,0,False,False,True,1641407775.0
rwv1vj,hria1et,t3_rwv1vj,"They get published in the same way as any other paper via the peer review process. Theoretical papers are assessed based on the quality of the proof (usually more than one) associated with the theory, the underlying evidence of the theory, how well it is explained, the significance/impact, etc.

You might want to look at theoretical engineering journals and theoretical computer science journals to get an understanding of how such papers are written.  


International Journal of Industrial Engineering

Engineering Science and Tecnology

Research in Engineering Design

Systems Engineering - Theory and Practice

Engineering Science and Technology

SIAM Review

Foundations and Trends in Theoretical Computer Science",3,0,0,False,False,False,1641485985.0
rwv1vj,hvp0760,t3_rwv1vj,You could begin by looking for space related internships and seeing how close you can get to your desired field. Networking with people in that field is another good option. NASA would likely have researchers in that domain but the research may be under lock and key so you may not find all of it online. Try your luck at getting an internship there. That’s probably the best option you could take. Otherwise I would look into open source projects that dealt with the subject you’re looking for and then talk to the community there and contribute. This is all I could think of at the moment.,1,0,0,False,False,False,1644066853.0
rwv1vj,hrixzij,t1_hria1et,"There are also a few conferences dedicated to ""space"" systems, see for example the NASA formal method conferences, https://easychair.org/cfp/nfm-2022

You can take a look at work published there to get an idea.",2,0,0,False,False,False,1641494728.0
rx5nj6,hrh3dvn,t3_rx5nj6,I have an exercise for PDA if you are interested in that.,2,0,0,False,False,False,1641464349.0
rx5nj6,hrh9eo0,t1_hrh3dvn,"Hm, I'm so noob so that it seems to be first time I hear about PDA (push down automaton?) - so would eagerly study whatever you share!",1,0,0,False,False,True,1641468782.0
rx4z0z,hrg71mi,t3_rx4z0z,2 kibibit,6,0,0,False,False,False,1641443251.0
rx4z0z,hrgzxwn,t3_rx4z0z,On 8-bit machines we'd call 256 bytes a page. Not really required anymore so I'm fairly sure it's fallen out of use.,2,0,0,False,False,False,1641461659.0
rx4z0z,hrhpco3,t3_rx4z0z,"If we are talking about naming data size units, then the following may also be interesting to you:

* **1 kilobyte** is precisely **1000 bytes**, while **1024 bytes** is **1 kibibyte**
* **1 megabyte** is precisely **1000000 bytes**, while **1048576 bytes** = **1024 kibibyte** = **1 mebibyte**
* **1 gigabyte** is precisely **1000000000 bytes**, while **1073741824 bytes** is **1 gibibyte**

Check out [this table on Wikipedia](https://en.wikipedia.org/wiki/Byte#Multiple-byte_units)",0,0,0,False,False,False,1641477676.0
rx4z0z,hrgz81i,t3_rx4z0z,Chom p,1,0,0,False,False,False,1641461093.0
rx4z0z,hroku79,t3_rx4z0z,quarterkilo,1,0,0,False,False,False,1641586178.0
rx4z0z,hsd956g,t1_hrgzxwn,"Wow, this is cool info, I trust you but could i get a source on that so i can justify using the term myself?",1,0,0,False,False,False,1642006289.0
rx4z0z,hsdzqwk,t1_hsd956g,"If you're talking about modern systems you'd best ignore my vintage waffle and refer to this:
https://en.wikipedia.org/wiki/Page_(computer_memory)

If dealing with old 8-bit systems like the 6502 then it's surprisingly difficult to find anything direct. Mentions of zero-pages (first 256 bytes of memory) or page-boundaries (crossing from one 256 byte chunk or memory to another) here:
https://en.wikipedia.org/wiki/MOS_Technology_6502#Bugs_and_quirks

Maybe I just distilled the idea of pages in my own head?!",2,0,0,False,False,False,1642016242.0
rvy218,hr8no35,t3_rvy218,"Good work, you put in the effort and know you’ve not only taught others but made sure you know it yourself.",24,0,0,False,False,False,1641319035.0
rvy218,hr90cpt,t3_rvy218,"I've been thinking I needed a refresher on my linear algebra lately, thanks for posting this.",16,0,0,False,False,False,1641323914.0
rvy218,hr98zkp,t3_rvy218,"Next up : Cayley-Hamilton Theorem and calculating A^(k) with it.

""*A square matrix satisfies it's own characteristic equation*""",5,0,0,False,False,False,1641327238.0
rvy218,hr9azri,t3_rvy218,"Thanks, I hate it",4,0,0,False,False,False,1641328003.0
rvy218,hr983pq,t3_rvy218,Imma save this for later and look out for more. Thank you!,3,0,0,False,False,False,1641326898.0
rvy218,hr9ut6y,t3_rvy218,"Oh my days .. eigenvectors and eigenvalues!! 

Quite interesting topic especially in machine learning.",3,0,0,False,False,False,1641335589.0
rvy218,hragcu3,t3_rvy218,Enjoy reading it. Looking forward to your next post!,2,0,0,False,False,False,1641344279.0
rvy218,hrc6qf0,t3_rvy218,Don't forget page rank!,2,0,0,False,False,False,1641378867.0
rvy218,hr8siun,t1_hr8no35,"Yes, ai find it an effective way to go back to the basics.",12,0,0,False,False,True,1641320922.0
rvy218,hr96m79,t1_hr90cpt,You're welcome! Will post more linear algebra posts so keep an eye out 😏👀,6,0,0,False,False,True,1641326326.0
rvy218,hr9fuv1,t1_hr98zkp,Love the A^k calculation 🤩,3,0,0,False,False,True,1641329841.0
rvy218,hr9fyri,t1_hr9azri,😛 you're welcome 😁,2,0,0,False,False,True,1641329882.0
rvy218,hr9fx1n,t1_hr983pq,🧘‍♂️👌,2,0,0,False,False,True,1641329864.0
rvy218,hr9vvli,t1_hr9ut6y,"Yeah, it is one of my favorite topics from linear algebra. :) hope you got something out of it.",2,0,0,False,False,True,1641336007.0
rvy218,hrbsf8p,t1_hragcu3,Thanks for the encouragement! 💯,1,0,0,False,False,True,1641368038.0
rvy218,hrc6wj1,t1_hrc6qf0,Oh wow! I let that one slip under the radar.,2,0,0,False,False,True,1641378995.0
rvy218,hr96zj4,t1_hr96m79,Looking forward to them!,3,0,0,False,False,False,1641326470.0
rvy218,hr9x0sz,t1_hr9vvli,Yes I did .. saved the visuals also .. thank you.,2,0,0,False,False,False,1641336459.0
rvy218,hr97ezm,t1_hr96zj4,Fantastic! 🙌🔜,2,0,0,False,False,True,1641326635.0
rw99z6,hrakk3s,t3_rw99z6,https://en.wikipedia.org/wiki/Wolfram's_2-state_3-symbol_Turing_machine,3,0,0,False,False,False,1641346039.0
rvq0w4,hr761bd,t3_rvq0w4,"Yes. I know how overvolting and overheating affects computers in chemical level, and what to expect.

If you gonna write scientific software for chemistry, you gonna need a lot of fundamental stuff while programming.",21,0,0,False,False,False,1641293199.0
rvq0w4,hr73gdb,t3_rvq0w4,Nope.,31,0,0,False,False,False,1641291171.0
rvq0w4,hr7y5vb,t3_rvq0w4,Some of the phys chem shows up in reversible computing and things like Landauer’s limit.,5,0,0,False,False,False,1641308931.0
rvq0w4,hr7oe8t,t3_rvq0w4,"Nope.

But: Math a lot, physics kind of, biology even, and all the language classes.",10,0,0,False,False,False,1641304497.0
rvq0w4,hr7xukj,t3_rvq0w4,"Lots of my colleagues do, although computational chemistry is kinda cheating.",9,0,0,False,False,False,1641308800.0
rvq0w4,hr844co,t3_rvq0w4,"I've written software to control robotic arms used in chemical test apparatus, if that counts?
 
There's an entire field of medical technology that is the intersection of biology (including chemistry) and computer science if that's something you're interested in. Has a bunch of equivalent names and subfields like digital health, health technology, e-medicine, e-health, telehealth, bioinformatics, many others.",3,0,0,False,False,False,1641311377.0
rvq0w4,hr86ppf,t3_rvq0w4,"All the time, but I’m in a niche field for it.",2,0,0,False,False,False,1641312415.0
rvq0w4,hr7hvse,t3_rvq0w4,"Tried, but",1,0,0,False,False,False,1641301086.0
rvq0w4,hr897ia,t3_rvq0w4,"Chemistry is where you’re most likely to learn things like the ideal gas law and Newton’s law of cooling. Both can be relevant in any situation where you have something generating heat that needs to be dissipated, like a CPU.",1,0,0,False,False,False,1641313413.0
rvq0w4,hr8bdfq,t3_rvq0w4,Only when sharing knowledge tidbits at the coffee machine.,1,0,0,False,False,False,1641314268.0
rvq0w4,hr8iiyi,t3_rvq0w4,"Yes. Inorganic chemistry is important to hardware development. 

If you're looking to do higher level development, then it's less important, although there are computational chemists.",1,0,0,False,False,False,1641317065.0
rvq0w4,hr94h8n,t3_rvq0w4,"yes, i turn coffee into code",1,0,0,False,False,False,1641325502.0
rvq0w4,hr95avk,t3_rvq0w4,"Bioinformatics graduate here. The only thing I used was in the context of Chemical and Biological weapons used in terrorism and that was because I was a DoD software engineer contractor....

Nothing else. Big waste of time and organic chemistry suffering.",1,0,0,False,False,False,1641325818.0
rvq0w4,hr9c9kq,t3_rvq0w4,"I’m pretty sure I tested out of chemistry 1 and only had to take chem 2. Was a bit more relevant for the EE part of my EE/CS degree. 

Other than that, no. I think I used Coca-cola to clean off the car battery in my shitty Camaro to get to work…that’s about it.",1,0,0,False,False,False,1641328486.0
rvq0w4,hr9smwc,t3_rvq0w4,Never again. Lol,1,0,0,False,False,False,1641334744.0
rvq0w4,hraksse,t3_rvq0w4,"I avoided all the chemistry classes I could. Really depends what you want to go into, most probably don't need it but there are always those specialized areas where it's needed.",1,0,0,False,False,False,1641346143.0
rvq0w4,hrax5to,t3_rvq0w4,I do in computer engineering. But I'm in an odd niche researching chemical reaction networks and genetic circuits.,1,0,0,False,False,False,1641351342.0
rvq0w4,hr75mpb,t3_rvq0w4,"Also, in same vein have y'all ever used discreet math, calculus in your career?",-19,0,0,False,True,False,1641292886.0
rvq0w4,hr9cci8,t1_hr7oe8t,How did you implement the languages in your code?,1,0,0,False,False,False,1641328517.0
rvq0w4,hr79b4f,t1_hr75mpb,"Indeed, it's nice skill to have when building stuff related to ML/DL, as well as in other CS areas, like cryptography and computer graphics (probably also others that i never worked with and as such aren't aware).",17,0,0,False,False,False,1641295613.0
rvq0w4,hr7yfty,t1_hr75mpb,"Discrete maths yes (quite a broad field though) but calculus no. Still glad I learned calculus though, definitely gives you a broader understanding of maths, which indirectly will improve your programming.",7,0,0,False,False,False,1641309047.0
rvq0w4,hr7yu7r,t1_hr75mpb,All the time.,3,0,0,False,False,False,1641309219.0
rvq0w4,hr762o4,t1_hr75mpb,My whole Ph.D. and the post-grad studies build upon these and how to calculate that stuff faster.,5,0,0,False,False,False,1641293228.0
rvq0w4,hr86jwe,t1_hr75mpb,"Only the smart ones need DMath, Calc, Combinatorics, and Linear",1,0,0,False,False,False,1641312350.0
rvq0w4,hr8ffhk,t1_hr75mpb,Computers are discrete math machines. Programming languages are discrete math languages.,1,0,0,False,False,False,1641315860.0
rvq0w4,hr8orw4,t1_hr75mpb,Yes actually.,1,0,0,False,False,False,1641319461.0
rvq0w4,hr9burh,t1_hr75mpb,Absolutely,1,0,0,False,False,False,1641328330.0
rvq0w4,hr7vhva,t1_hr75mpb,"Even if you do not use the math itself, the method of thinking is the same",0,0,0,False,False,False,1641307784.0
rvq0w4,hr9fadq,t1_hr9cci8,Knowing several languages with very different syntax helps when you get into functional programming or other declarative languages; it’s also nice to know some linguistics when you are tasked with coding UI strings for an international application; or some phonetics when implementing a “fuzzy match” algorithm for a search engine in a language not supported by existing Soundex implementations.,1,0,0,False,False,False,1641329626.0
rvq0w4,hr9gfrf,t1_hr9cci8,NLP,1,0,0,False,False,False,1641330060.0
rw2dl6,hr97jwh,t3_rw2dl6,A pointer is directly a memory address. Most high level languages don't allow you to work directly with them at all or at least try to prevent it. A python id is a unique integer identifier but is NOT a memory address.,5,0,0,False,False,False,1641326688.0
rw2dl6,hr96fn7,t3_rw2dl6,"Theres quite a bit difference.

For example you cant do any pointer arythmetic explicitly in python, with or without the usage of id.

More generally, you cant reliably infer an id of one object based on the id of another object even if you know how the code looks ljke.

I havent found a place to use this function to be honest, i assume python use it mostly under the hood.",1,0,0,False,False,False,1641326255.0
rw2dl6,hrbslmf,t1_hr96fn7,"I have onky used id to store objects in a dictionnary, it gives a hashable key that is unique, even if two objects have the same representation at a some point.

You could also use it to check if two objects are the same. Not only the same representation, but exactly the same memory, if you change one, both change. But the ""is"" do this and is more readble.

So it can be useful, but in quite specific places, and not for any low-level pointer operations.",1,0,0,False,False,False,1641368162.0
rvfquw,hr56orc,t3_rvfquw,"It wasn't a problem with storing a date, but a version number which was beginning with date.

So MS has such versioning convention: `YYMMDDxxxx` (where `x` are not important for us here).

So lets create a smallest number for 2022 which fits this convention: `2201010000`  
As you can see, it's bigger than `2147483647`",93,0,1,False,False,False,1641254619.0
rvfquw,hr6eoao,t3_rvfquw,"> Why did the Microsoft date bug happen in 2022?

Apparently, their test suite sucks.",26,0,0,False,False,False,1641273796.0
rvfquw,hr7nkqy,t3_rvfquw,Did you read any of the actual coverage of the story? Genuine question; every article I read explained quite clearly what the problem was...,4,0,0,False,False,False,1641304103.0
rvfquw,hr6p6sq,t1_hr56orc,they should of just used long long,10,0,0,False,False,False,1641280215.0
rvfquw,hr6jwii,t1_hr6eoao,How many tens of thousands of employees do you have to hire before someone writes a unit test these days?,13,0,0,False,False,False,1641276748.0
rvfquw,hr70qzb,t1_hr6p6sq,or unsigned int,8,0,0,False,False,False,1641289017.0
rvfquw,hr73x49,t1_hr6p6sq,Why not string,4,0,0,False,False,False,1641291545.0
rvfquw,hr6llgq,t1_hr6jwii,Would be good to just run the suite with a date a year in the future... they could have caught this a year ago instead of testing in production.,6,0,0,False,False,False,1641277811.0
rvfquw,hr755vl,t1_hr70qzb,What about the time before Christ? Can't forget about the dinasours,12,0,0,False,False,False,1641292519.0
rvfquw,hr8e3eh,t1_hr73x49,"They were using the version number to check if the version installed is less than than the latest patch available, so they would have had to convert it to a numeric format anyway for comparison since string < string would produce unreliable results depending on the platform.

They could have used a long (64 bit number), unsigned integer, or done a bit of bit-shifting to more efficiently pack the date and version number into 31 bits (apparently one bit was reserved for something else).",3,0,0,False,False,False,1641315333.0
rvfquw,hr7jaws,t1_hr755vl,I didn't know the dinosaurs were fewer than 2022 years before Christ...,2,0,0,False,False,False,1641301879.0
rvfquw,hr7kvbf,t1_hr755vl,what about 1999,1,0,0,False,False,False,1641302720.0
rvc4y3,hr4j6yz,t3_rvc4y3,"I mean Minecraft is a great example of programs in general, even being able to compare the same app written in two different languages, Java and Bedrock editions.",34,0,0,False,False,False,1641245256.0
rvc4y3,hr5bfl3,t3_rvc4y3,"I think most of you misunderstood me haha. I don’t mean that I want to teach within minecraft, but how the program itself could be programmed. Like which classes, dependencies etc. you would need to program Minecraft",29,0,0,False,False,True,1641256592.0
rvc4y3,hr4ohyi,t3_rvc4y3,"Great idea, for more advanced units you could even install computer craft which allows you to write LUA code to control little robots and other blocks",8,0,0,False,False,False,1641247292.0
rvc4y3,hr59n7w,t3_rvc4y3,I’m not sure dependency injection is used in Minecraft?,3,0,0,False,False,False,1641255847.0
rvc4y3,hr58nrj,t3_rvc4y3,"In my opinion Minecraft redstone(which can really be thought of as a massive, 3d state machine) is a great way to teach computer architecture, but not necessarily higher level softer design. You can use it to construct simple computers that reflect the same high level digital logic in real computers. I can honestly say that my understanding of the intricacies of a computer has really been solidified after building a redstone computer",9,0,0,False,False,False,1641255440.0
rvc4y3,hr4ynso,t3_rvc4y3,They have an entire curriculum and separate game that teaches coding [here](https://education.minecraft.net/en-us/get-started),6,0,0,False,False,False,1641251321.0
rvc4y3,hr5zd7j,t3_rvc4y3,"Any game is going to be perfectly fine to teach with - if you're teaching at a professional level where you need to grade it's going to be hard with such a well-covered piece of software to know what is your students' work and cut and pasted without understanding from the internet, so for that reason I make sure to keep it well away from any assessments and stick with lesser known games if needed.",1,0,0,False,False,False,1641266624.0
rvc4y3,hr5o0j5,t3_rvc4y3,just teach logic in highschool,-1,0,0,False,False,False,1641261860.0
rvc4y3,hr5h72g,t1_hr5bfl3,"I like the classic ""dog and dat and both subclasses from animals"" approach, but you sure can explain OOP using Minecraft.",15,0,0,False,False,False,1641259013.0
rvc4y3,hr802eb,t1_hr5bfl3,What? Nah you should build a red stone computer and teach programming using that /s,1,0,0,False,False,False,1641309730.0
rvc4y3,hrx8n11,t1_hr59n7w,You really think minecraft a game that literally sold for a billion dollars and is now run by Microsoft doesn't have aby dependency injection? For some reason I really doubt that,1,0,0,False,False,False,1641742046.0
rvc4y3,hr5yyzs,t1_hr5h72g,"Dog and cat being an animal is one of the worst introductions to OOP that is responsible for so much misunderstanding of what inheritance is to the point it could be given a trophy for worst teaching analogy of all time.
 
First, even if we are leaving composition over inheritance to later, by introducing it that way the misunderstanding you invite causes repeated breaches of the Liskov substitutability principle, because students learn to model their code based on the taxonomy in the real world, which is completely wrong.

Inheritance is not an 'is a' relationship from the real world, that is a gross oversimplification by people who didn't know what inheritance was for. The classic counterexample is square and rectangle - [see this talk by Bob Martin](https://www.youtube.com/watch?v=zHiWqnTWsn4&t=4430s).
 
Rather than modelling the real world behaviour of things in classes, it is far better to approach classes for what they actually are - collections of **software behaviour**. That way you never fall into the square-rectangle trap, but you still arrive at the shape-square and shape-rectangle behaviour (which is also in that talk, it's a good one to watch start to finish).
 
While you could do that with dog and cat, you've gone and abstracted software needs by swapping in 'real world' needs like walk and speak, which makes it really confusing when students are then writing their own code based on software needs.",14,0,0,False,False,False,1641266455.0
rvc4y3,hrxza2r,t1_hrx8n11,"Dependency injection isn’t useful in all scenarios. As I understand it, it’s less common in game development than it is say, enterprise backend development

Them being Microsoft and having lots of money and programming experience doesn’t mean they’re going to use a certain pattern every single time.",1,0,0,False,False,False,1641752140.0
rvc4y3,hr79iza,t1_hr5yyzs,"I got your point and the presentation you linked is pretty good too, but I don't think it would a good approach to introduce OOP directly with software behavior. At this stage, people barely know how to write a ""real life"" software.

Also while SOLID is desirable while implementing OOP, they are distinct topics and should be teached separately. For the core concepts of OOP, the real life examples should work fine. When teaching SOLID you can then revisit this examples and show why they doesn't work under SOLID, just like the guy in the video did.",2,0,0,False,False,False,1641295774.0
rvc4y3,hr64zzf,t1_hr5yyzs,"Totally agree that an actual real world programming example is much better than the classic dog, cat, animal teaching.",1,0,0,False,False,False,1641269112.0
rvc4y3,hry710p,t1_hrxza2r,Dependency injection at a basic level is greatly useful for making classes testable and reusable. Surely it's riddled throughout minrcrafts code base.,1,0,0,False,False,False,1641754849.0
rvc4y3,hr7cozo,t1_hr79iza,"You don't teach SOLID as a 'thing' on day 1, but you absolutely need to exemplify it and make it a habit in your examples and discussions from day 1. Otherwise you are creating bad habits and assumptions that never need to exist.
 
There are 3ish schools of thought on when they should learn OOP at all, objects-first, objects-early, objects-late. If they are objects-late, then you can give them whatever you like, because they can craft some decent procedural code and you can build on those concepts. Remember that there was a point in time where *everybody* learning OOP was in this category.
 
If they are objects-early, then they already know enough that you can draw out objects and a class structure motivated by the behaviours they already know about. A Zork clone is more than sufficient, which you can learn enough programming to create (procedurally) in a day or 2 boot camp. Rooms, exits, items in the rooms, etc. Easy peasy, and you can even show them the flaws in creating an inheritance structure based on is-a real world relationships when you do so, by comparing your 'what common behaviours do we want to bundle' vs 'let's model a maze in real world concepts' versions. It doesn't need to be 'real life' code, it needs to be real motivations from the software, even if that software is trivial.
 
If they are objects-first, then the best thing to do is give them a scaffold with a canvas object and do shapes. You can do this with BlueJ in the first hour they are at a keyboard.
 
If you are teaching them any other way, then what you have almost certainly been blinded by is the **syntax** of OOP, which is both the easiest and stupidest thing to teach a newbie student. Yes, they'll need to know the syntax of whatever language, they'll need to learn how access modifiers work, they'll need to understand scope, etc etc etc. But that is all crap they can learn by breaking things along the way, and is a distraction from the more useful starting concepts. If they cut and paste boilerplate for a semester but can successfully grow a useful hierarchy out of some software requirements or procedural pseudocode they are light years ahead of someone who has memorised the syntax but will inherit from Rectangle.",2,0,0,False,False,False,1641297928.0
rw311s,hr983ax,t3_rw311s,"When using pointers, you are mostly operating on virtual memory space provided by your operating system, which takes care of managing memory for each process. Internally, mechanisms like paging are used to divide the space into equal sized pages which can be put to swap space and reloaded from disk when needed. The MMU takes care of translating virtual addresses to physical memory addresses. It’s actually a lot more complicated than that, but your operating system knows exactly which process allocated which parts of memory and keeps track of everything including memory boundaries and more.",4,0,0,False,False,False,1641326894.0
rw311s,hr988u7,t3_rw311s,"Theres a few mechanisms in place, all are way too complicated to write on a single reddit post.

In a nutshell, the **OS** gives each process the **illusion** it has a big and clean memory space. Which means a few processes can reach into the address 0x1234 and store data there. 

All of this illusion (called effective addresses) is being handeled at the os. Then a combined effort of the os and the harware to translate this effective address into real phyiscal address in the dram.

This has nothing to do with any perticular progremming languege, you can take advantages of it to gain some speed up - but for most applications its not needed.",2,0,0,False,False,False,1641326953.0
rw311s,hr9jsxl,t3_rw311s,https://www.freecodecamp.org/news/understand-your-programs-memory-92431fa8c6b/,2,0,0,False,False,False,1641331338.0
rw311s,hrjhmdt,t3_rw311s,"It depends.

If `my_var` is on the stack, then its address increments/decrements (depending on the CPU architecture) as new variables are allocated. The CPU has a register to track the latest stack location in memory (called a stack pointer), and the compiler knows how much to add/remove from this register to reserve space in the memory for new variables.

For example, if I create two local variables (local variables get created on the stack), then their addresses should differ by the size of those variables, possibly rounded up to the nearest word. `long` is equivalent to one word or its multiple on most OS so there shouldn't be any rounding in this example:

    #include <iostream>
    
    int main() {
        long on_stack1;
        long on_stack2;
    
        std::cout << ""Address of on_stack1: "" << &on_stack1 << std::endl;
        std::cout << ""Address of on_stack2: "" << &on_stack2 << std::endl;
        std::cout << ""The difference in the address should be "" << sizeof(long) << "" bytes"" << std::endl;
    
        return 0;
    }

The value of the stack pointer is initially set by the OS when the program starts.

If, on the other hand, `my_var` is on the heap (e.g., created with `new`), then the address is dynamically allocated by the memory allocator (which comes with the C/C++ library for the OS) so how it allocates the memory depends on the allocator's algorithm. To avoid the heap's memory allocator accidentally using the memory used by the stack, the allocator tries to reserve heap memory on the ""opposite"" end of the memory address from the stack as far away from it as it can. You can see a typical memory layout here: [https://www.geeksforgeeks.org/memory-layout-of-c-program/](https://www.geeksforgeeks.org/memory-layout-of-c-program/)

    #include <iostream>
    
    int main() {
        long on_stack;
        long* on_heap = new long;
    
        std::cout << ""Address of on_stack: "" << &on_stack1 << std::endl;
        std::cout << ""Address of on_heap: "" << on_heap << std::endl;
        std::cout << ""The address of on_heap should be far from on_stack."" << std::endl;
    
        return 0;
    }

If you use a lot of memory it is possible for the stack to crash into the heap. And it is possible for a memory allocator to fail to allocate a large chunk of memory even if there are many small pieces of memory due to memory fragmentation. A part of the memory allocator algorithm's goal is to minimize such memory fragmentation.

Also, if `my_var` is a global variable or a constant, the compiler reserves a space for it within the binary so that when the program is loaded into the memory the variable is already reserved an address. Its address can be calculated as the starting address of the program + offset to the variable in the binary.

    #include <iostream>
    
    const char* on_global = ""You should see this string in the binary"";
    
    int main() {
        std::cout << ""Address of a constant: "" << (void*)on_global << std::endl;
        std::cout << ""Address of a global variable: "" << &on_global << std::endl;
    
        return 0;
    }

As others have pointed out, modern OSes, with the assistance of modern CPU architectures, use virtual memory so the addresses printed out by the above programs are not the memory's physical addresses. There exists a memory mapping mechanism to use the memory more efficiently and protect one program's memory from each other. I like these videos for understanding virtual memory: [https://www.youtube.com/watch?v=qcBIvnQt0Bw&ab\_channel=DavidBlack-Schaffer](https://www.youtube.com/watch?v=qcBIvnQt0Bw&ab_channel=DavidBlack-Schaffer)",2,0,0,False,False,False,1641501927.0
rvrtx4,hr7by93,t3_rvrtx4,"No, not really.

A single clock cycle is the shortest period of time within which you expect anything to happen within the CPU. It's the resolution of time for the CPU, in a sense.

It takes time, however brief, for electrical signals to propagate within the CPU. It takes time for transistors to switch.

When the CPU runs, say, an instruction to add two integers together, it does that by passing current through a logic circuit that produces the correct output bits from the input bits. The output is not instantaneous. Rather, the CPU would check the output at the next clock cycle (or possibly after a set number of clock cycles).

Disclaimer: I'm not an electrical or electronics engineer. If someone knows better and I said something wrong, go ahead and correct.",9,0,0,False,False,False,1641297441.0
rvrtx4,hr8g71l,t3_rvrtx4,Read [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code),1,0,0,False,False,False,1641316163.0
rvrtx4,hr8j26c,t3_rvrtx4,"No. The short, hand-wavy reason why most CPUs have a clock frequency is because they are synchronous circuits. These use a periodic signal, the clock, to coordinate their operation. The opposite of synchronous circuits would be asynchronous circuits, where there isn't a clock signal coordinating activities, but a multitude of control signals that implement some sort of protocol for sequencing events.

Regarding the frequency at which bits move around in a CPU; what is quoted as the CPU's clock frequency has some relationship to the rate at which bits move around, but that's missing the point of why there is a clock in the first place (which isn't to describe the rate at which bits move around). The reason why there's some relationship is because there will be parts of the CPU where data being moved to different parts, or processed, at the same rate as the clock frequency. This isn't a hard requirement; it's a result of there being a clock signal. There will be parts where data moves at a lower rate than the quoted clock rate; and possibly at a high rate in others.",1,0,0,False,False,False,1641317270.0
rvrtx4,hr874c9,t1_hr7by93,"Moreover, OP should look into how different CPU architectures handle complexity, because 2.5Ghz in CISC is different compared to RISC and different layouts, number of cores, type of parallel processing, semaphores etc. impact CPU speed differently. Which is why when buying a CPU you should always look at benchmarks rather than specs alone",3,0,0,False,False,False,1641312577.0
rutayn,hr1fgul,t3_rutayn,"Quantum computers don’t just magically run classical algorithms faster than classical computers. It’s more like the quantum computer can do NEW special operations in a single time step that a classical computer would take MANY time steps to emulate. Some problems can be made dramatically faster if a NEW algorithm is invented that uses these special NEW operations.

An analogy would be like comparing a computer that can only do one add in a time step with one that can do a multiplication in a time step. If you are trying to multiply a number by n the adding machine might do it on O(n) by adding in a loop, but the machine that can multiply directly is just O(1).",178,0,1,False,False,False,1641188514.0
rutayn,hr1c56p,t3_rutayn,"As far as I know, it doesn't necessarily reduce time complexity. A quantum algorithm has to be specially designed to leverage the benefits of the quantum computer.",30,0,0,False,False,False,1641186680.0
rutayn,hr1fn80,t3_rutayn,"[Shor's algorithm](https://en.m.wikipedia.org/wiki/Shor%27s_algorithm) shows how a quantum algorithm can solve a problem in faster time than a classical algorithm, the problem being find the prime factors of a given integer.",21,0,0,False,False,False,1641188615.0
rutayn,hr2xlyz,t3_rutayn,"Quantum computers have characteristics like [entanglement](https://en.wikipedia.org/wiki/Quantum_entanglement) and [reversibility](https://www.linkedin.com/pulse/computational-reversibility-quantum-computing-sa%C5%A1a-savi%C4%87) .

This creates a computational paradigm that is different than your classical computer and therefore requires different algorithms and so far can only solve certain problems.",6,0,0,False,False,False,1641223392.0
rutayn,hr4rlou,t3_rutayn,"u/OdinGuru’s answer is excellent! I’d like to add, if you’re interested in learning about quantum computing, I have found https://quantum.country to be an excellent introductory resource. It describes quantum operations as shortcuts. We can use these quantum operations to design new algorithms which can solve the same problems we already face in computing, sometimes utilizing the built-in shortcuts to result in faster runtimes. The key is sometimes. Algorithms have to be discovered which use these shortcuts to their full potential.",2,0,0,False,False,False,1641248508.0
rutayn,hr1vbxu,t3_rutayn,[deleted],-10,0,0,False,True,False,1641199270.0
rutayn,hr3254j,t1_hr1fgul,What are these “new special operations”?,11,0,0,False,False,False,1641225291.0
rutayn,hr4h5tu,t1_hr1fgul,"Yes, quantum computers are very strange things that perform operations that are hard to understand and utilize.

Off topic, heavy tangent: I understand it was chosen for illustrative purposes, but I have to point something out, in case someone would end up thinking that implementing multiplication by adding in a loop is the way to go on platforms that lack multiplication opcodes. 🙂

Given A • B = C, the proper way to do it is by first decomposing one of the factors, say A, into a sum of powers of two. Then multiply B by each decomposed component of A, accumulating the result into C. Because the components are all factors of two, each multiplication ends up being a simple bit shift, which makes the algorithm perform in the order of O(log n).

If that's not fast enough (and on platforms that lack multiplication opcodes it frequently isn't), you can for example approximate the result of a multiplication by sacrificing memory for some lookup tables, and make use of the fact that log (A • B) = log A + log B.",2,0,0,False,False,False,1641244472.0
rutayn,hr25tup,t1_hr1fgul,Great explication 👍🏼,6,1,0,False,False,False,1641207581.0
rutayn,hr2blnn,t1_hr1vbxu,That's a completely wrong understanding of QC. Follow your own advice.,3,0,0,False,False,False,1641211781.0
rutayn,hr3e7cx,t1_hr3254j,"There are Quantum-specific logic gates like the Hadamard gate, CNOT, and many others that do operations on one or multiple qbits and manipulate the state of the qbit before it is observed.",23,0,0,False,False,False,1641229992.0
rutayn,hr4jidx,t1_hr4h5tu,Lol. I was expecting someone to call out that there are better algorithms than O(n) to do multiplication using only adding. I decided to leave the O(n) just to keep the explanation simpler and more understandable. You are of course correct. Thanks for keeping redit up to standards!,1,0,0,False,False,False,1641245378.0
rutayn,hr3tvdt,t1_hr3e7cx,Layperson here. So does this double the logical output of all the execution units in a circuit? Instead of having q and qbar we could halve the footprint of chips? And qutrits could theoretically triple the logical output of a given execution unit?,3,0,0,False,False,False,1641235751.0
rutayn,hr4cjd6,t1_hr3tvdt,"It’s not so simple as “double” or “triple”. At a high level (simplified) the quantum computer lets us setup “complex” quantum states, and combine them together in certain ways so the intermediate results have complex entanglement and setup a kind of interference so the answer falls out at the end. It is not easy to explain as these new quantum operations are HIGHLY NON INTUITIVE. If you want to dive deeper you could look at [the Wikipedia entry on quantum computation language](https://en.m.wikipedia.org/wiki/Quantum_Computation_Language) to start getting and idea of what these new operations are and how they could be used. However I suspect that very few programmers in the future will be building these quantum algorithms directly and will mostly use higher level libraries which deal with the quantum particulars and have “standard” classical APIs. In other words it won’t be that far off from how GPUs can be used to “accelerate” some kinds of computations today, but aren’t used for the general purpose part of programs (which CPUs are excellent at already).",8,0,0,False,False,False,1641242689.0
rutayn,hr4ft5o,t1_hr3tvdt,"> does this double the logical output 

What do you mean by this? Like double the number of booleans outputted by a given circuit?

> we could halve the footprint of chips

The physical requirements for quantum computers are entirely different from those of regular ones. Modern quantum systems have tens or dozens of bits, with some cutting edge ones having hundreds. While it's extremely likely that they will improve in a similar manner to the regular computers we are used to, it is definitely much too early to say that a quantum chip will have a smaller physical footprint than a a somehow comparable silicon chip. We are at the ""room sized computer"" stage, and the machines are cooled to near absolute 0 with refrigerated gasses.",3,0,0,False,False,False,1641243945.0
rutayn,hr4kaha,t1_hr4cjd6,"Interesting, thanks, giving it a read. I guess I was sort of imagining an extension to ""normal"" ISAs but that's clearly not the case. The possibility of a distinct set of programmers dealing with quantum functions is very interesting though.",4,0,0,False,False,False,1641245678.0
rutayn,hr4jpo0,t1_hr4ft5o,"> Like double the number of booleans outputted by a given circuit?

Yeah, basically. Like I'm seeing the [quantum logic gates with more outputs than a ""normal"" truth table.](https://en.wikipedia.org/wiki/Quantum_logic_gates](https://en.wikipedia.org/wiki/Quantum_logic_gate) I may not know enough linear algebra to wrap my head around it though.

I get your second paragraph though, I was silly not to think of that. Obviously traditional fabrication techniques won't work with quantum information. [I thought this was a pretty good explainer on the types of qubits.](https://www.youtube.com/watch?v=-5fKVn1GR9Y)",1,0,0,False,False,False,1641245455.0
rutayn,hr585fs,t1_hr4kaha,"Probably the first few waves of professional quantum computer programmers will be those who already have backgrounds in quantum physics, mathematics and the like.",1,0,0,False,False,False,1641255234.0
rutayn,hr4wzgc,t1_hr4jpo0,"So you need to be careful here: there isn't some quantum magic that is adding bits or something like that. Every gate that outputs 2 qubits has 2 qubits as an input. The difference between a qubit and a regular bit is that regular bits are *always* only 0 or 1, and a qubit is a 0 or 1 *only when observed*. However, before a qubit is observed, when it is in superposition, it has some probability to collapse to a 0 or 1 on observation (and this probability can be 100% a 1 or 100% a 0). When gates operate on a qubit, they operate on its state in superposition; they change the probability that the qubit will collapse to one of a 0 or a 1. This lets engineers and scientists design rather creative algorithms involving linear algebra, which can often run faster than the classical algorithms used to solve similar problems.

If you'd like a clear intro, see https://www.youtube.com/watch?v=F_Riqjdh2oM, but be warned! It's all linear algebra.",1,0,0,False,False,False,1641250643.0
rutayn,hr59hrd,t1_hr585fs,Probably? Better asking whether there will ever be any other quantum programmers!,2,0,0,False,False,False,1641255783.0
rutayn,hr59tq8,t1_hr59hrd,Pretty much lol. Until quantum operations can be captured by a more convenient interface. But that's unlikely give the level of sophistication required.,1,0,0,False,False,False,1641255920.0
ruum67,hr3oi50,t3_ruum67,"This might be some core info for machine architecture / systems people, but I have to disagree that everyone needs to know this stuff. This seems far more intricate than anyone would reasonably need to learn in depth for a college CS degree, and with more and more people going into higher level software engineering, all the stuff here just seems more and more niche.

This is great stuff for someone looking to specialize into hardware, but for your average programmer? This is way too much.",2,0,0,False,False,False,1641233794.0
ruum67,hr1sa1a,t3_ruum67,Awesome! I once read “What Every Programmer Should Know About Floating-Point Arithmetic” and it was an eye opener for me. Saving this for my reading.,1,0,0,False,False,False,1641196909.0
ruum67,hrgjrwy,t1_hr3oi50,"I agree that the title could have been more appropriate. I think Ulrich Drepper got inspiration from the title of another paper that u/Cull_The_Meek mentioned titled ""What Every Programmer Should Know About Floating-Point Arithmetic"". Although I think that it should be a great read for all programmers because it just clears up a lot of confusion that you have while working with high-level langauges, but sure it isn't for an average programmer.

It seems like a good read to me coz I spend most of my time writing low level langs and fiddling with databases",1,0,0,False,False,True,1641450143.0
ruum67,hr1vrzs,t1_hr1sa1a,"> I once read “What Every Programmer Should Know About Floating-Point Arithmetic”

[This](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) one?",3,0,0,False,False,True,1641199621.0
ruum67,hr28rj6,t1_hr1vrzs,Yup! It’s mentioned in the introduction of the article OP posted as well.,2,0,0,False,False,False,1641209785.0
rushx3,hr1e7sw,t3_rushx3,In relation to caching the article is just stating that an unaligned access may overlap TWO cache lines rather than one and that two cache loads is slower and eats up more cache space that a single load.,8,0,0,False,False,False,1641187815.0
rushx3,hr16brx,t3_rushx3,"could you please share the link you're referring to?
I could explain it based on their perspective",1,0,0,False,False,False,1641183774.0
rushx3,hr1c8xf,t3_rushx3,I don't have any articles or resources to back this up but from what I've heard is that when a memory address in RAM is accessed it loads a whole block of it in to cache (L1/L2 cache maybe?). So then if the next instructions are accessing a memory address in that block it will pull it from the cache instead of RAM.,1,0,0,False,False,False,1641186735.0
rushx3,hr1ei6w,t1_hr1e7sw,Eureka! Now I get it. Thanks a ton!,3,0,0,False,False,True,1641187978.0
rushx3,hr17lg5,t1_hr16brx,I updated the post with the appropriate links.,1,0,0,False,False,True,1641184375.0
rushx3,hr1cmmz,t1_hr1c8xf,no this is something different I almost forgot about this concept read about it in my comment,1,0,0,False,False,False,1641186940.0
rushx3,hr1deg6,t1_hr1c8xf,"> I've heard is that when a memory address in RAM is accessed it loads a whole block of it in to cache (L1/L2 cache maybe?).

Afaik, on subsequent reads of the same block of memory, a [cache line](https://stackoverflow.com/questions/52890824/cache-line-format-layout) which is usually 64 bytes, is cached. I got to know about these cache lines in this [youtube video](https://youtu.be/247cXLkYt2M?t=274) about data structure optimization but it does not explain why memory alignment is beneficial for caching.",1,0,0,False,False,True,1641187361.0
rushx3,hr1dpvo,t1_hr17lg5,"so the thing is CPUs do not access memory like we though they do
they do not have granular access to memory exactly like we have thought all this while

a 8bit CPU will have 1 byte of granular access
for 16byte it can load 2bytes at a time

now consider both these CPUs want to load want to read something at address 0x0001-0x0004 the 8bit CPU will be able to read the address directly from 0x0001-0x0004 onto the memory
but in case of the 16bit CPU it will first have to read 0x0000-0x0001
then perform a shift operation to set the actual starting point
for us it seems like a very nominal problem but that one extra cycle costs a lot, especially when there millions of these operations

now coming back to the real question, if the memory is unaligned you will tend to have more these shifting operations because you'd be loading more number of uneven memory addresses and then shift compared to placing it once and then shifting on from there

in case it is aligned you save on the reading and then shifting operation

carefully read the IBM link and observe the diagrams it will give you a better clarity

https://developer.ibm.com/articles/pa-dalign/",3,0,0,False,False,False,1641187536.0
rushx3,hr1e8od,t1_hr1dpvo,"yeah, that is what the IBM developer article explains. A lot of cpu cycles are saved when memory is aligned which is why aligned memory access is faster than unaligned memory access(also illustrated in the IBM developer article). Could you find any explanations on how aligned memory is good for caching?",1,0,0,False,False,True,1641187829.0
rushx3,hr1eko5,t1_hr1e8od,it just reduces CPU instructions needed when the memory is aligned that's how the reads would become faster as less cycles are wasted to read the memory,2,0,0,False,False,False,1641188016.0
rushx3,hr1f66i,t1_hr1eko5,"I kinda get it now. If data is unaligned, and the CPU caches a cache line, there is a possibility that the data would be chopped off and wouldn't be cached properly. Because only a portion of the data is cached, the CPU might then cache another cache line containing the other part of the data which wastes cache and also it wastes cpu cycles which are required to shift and combine the portions of the data.",2,0,0,False,False,True,1641188348.0
rushx3,hr1g38q,t1_hr1f66i,yes absolutely,2,0,0,False,False,False,1641188871.0
rue0sq,hqyhh8j,t3_rue0sq,"Let’s say we have an 8 bit data width CPU, with 16 bit instructions.

It has 16x 8bit general purpose registers.

There are 2x (16 to 1) 8bit multiplexers with the 16x 8bit inputs being the values in the registers. They each have 4 control bits to select which register values end up on the output.

The 2 8bit outputs are fed to an ALU which can do 4 operations: add, sub, and, or. The operation is selected with 2 control bits.

The 8bit output is fed back to the 16 registers, each have a control bit which if enabled, will latch the 8bit value into the register at the end of the clock cycle. These 16 control bits are the output of a 4 to 16 decoder, which is controlled by 4 select bits.

So our CPU can choose any 2 registers from our 16, perform one of 4 operations, and then save the result to any of the 16 registers.

It needs 4 bits to select the 1st operand, 4 bits to select the 2nd, 2 bits to select an operation, and another 4 bits to select which register to save the result in.

So you might have an instruction format which looks like this:

00xxaaaabbbbcccc

Where xx are the 2 bits which select the operation the ALU will perform.

aaaa are the 4 bits which select the register of operand 1.

bbbb are the 4 bits which select the register of operand 2.

cccc are the 4 bits which select which register to save the result in.

When the CPU sees this instruction it will route the relevant control bits to the multiplexers, ALU, and decoder to perform the specified instruction. This is what is meant by decoding the instruction. This is a simple instruction set but real ones are much more complex and require more logic to derive the relevant control bits from any given instruction. The area of the CPU which will do this is called the control unit. It uses the instructions to derive control bits which will control the flow and processing of data in the other components of the CPU such as registers, the ALU, memory, various multiplexers and decoders, bus access, etc.",54,0,2,False,False,False,1641145316.0
rue0sq,hqzpehx,t3_rue0sq,You might find Ben Eater's [video](https://www.youtube.com/watch?v=dXdoim96v5A) exploring his 8-bit computer's control logic useful if you want a visual understanding of /u/No_Engineering8506's reply,12,0,0,False,False,False,1641161753.0
rue0sq,hqyf1an,t3_rue0sq,"Well in general the CPU have something called a control unit. The control unit configures the cpu into different states (This could be adding values, putting things in memory or setting registers). When you decode a command, you're telling the control unit what state the cpu should be in.",3,0,0,False,False,False,1641144386.0
rue0sq,hqzelm6,t3_rue0sq,"Pretty much decoding refers to taking an instruction (or “command”) and breaking it apart depending on what kind of instruction it is, then sending these pieces to the other parts of the processor that executes the instruction. The control unit (which others have mentioned) is the component that checks a certain parts of the instruction, and depending on what values it finds in those parts it will send the correct parts to the correct components.

For example (and  I just made this up) if you want to add 5 and 2, and the control unit is set up to add when the first four indices of the instruction equals 2 (or 0010 in binary) then a 16 bit instruction for that could be: 

0010 000101 000010

The control unit would look at the first four values, see a ‘2’, and from there it knows to take the next 6 values and pass them in as the first operand to the ALU, and the last 6 values as the other operand to the ALU. In reality it’s more complicated as it also needs to know where to save the result and other operations might need even more information, but this is an easy way to visualize it.",1,0,0,False,False,False,1641157376.0
rue0sq,hqzyn6k,t3_rue0sq,"If you want an expanded example you can think of the commands as gates/pathways that are controlled by switches. Each CPU command has a bitset that corresponds to it. Those gates open when the certain bits are set (through ands/nots) and then the data you're using with the CPU command is sent through that path.

It doesn't really ""decode"" it as much as the bits split up with circuits and sent in chunks. The command code is one such chunk and the data (more specifically the register number being used) is another one. Flags can also be used.

Now take that analogy and compact it, the gates overlap and the logic behind the gates can be simplified using ors an boolean simplification. Our CPUs are massively simplified to be as compact as possible. Programmable CPUs also exist (as in the command sets can be changed), but that is a lot more complicated.

Actual decoding/encoding is done to help us read it, but it's not at all necessary for the CPU function. The decoding you're referring to is more of an analogy for how the data splitting is done and how it's interpreted by the system.",1,0,0,False,False,False,1641165371.0
rue0sq,hqzyybm,t3_rue0sq,"This is at least one college-level course. Binary instructions and data are fed into a hierarchy of semiconductor logic gates using transistors and other electrical components. I would recommend a textbook ""Logic and Computer Design Fundamentals"" by Mano, Kime, and Martin. It's kind-of a rough technical read, but it will get you started. 

I could write a whole essay, but you're not going to ""get it"" until you sit down and work-out some truth tables.",1,0,0,False,False,False,1641165494.0
rue0sq,hr0193o,t3_rue0sq,"You got a lot of great responses here OP, but if you want a simplified TLDR, assume you have 16 bit storage (and processor of the same size) you can imagine 4 of those bits being used to encode an instruction (so 2^4=16 instructions) and then the next 4 bits are an argument for the instruction, such as an absolute value, a memory cell, or a pointer to a memory location, next 4 bits are second argument (same as before), and the last 4 bits are where to store the result. In practice it’s a tad more complex but this is the important intuition to develop imo.",1,0,0,False,False,False,1641166396.0
rue0sq,hr1vxxn,t3_rue0sq,"You got some really awesome responses here, I will try to give an answer that requires less CS knowledge to understand, and perhaps it will help someone out there (though if you really want to understand it look at other comments here!)

So basically, a CPU works by reading some list of ""commands"" in Binary code (lines of 0s and 1s).

These commands vary from OS to OS but they can basically be summed up to 3 categories: ""basic mathematical operations"" and ""memory operations"" and ""Jump operations""*.

The memory operations will be things like write num x to an address y in the RAM, or pull up the number in a RAM address and save it to the stack (a dedicated place on the ram, that mathematical operations will take their inputs from).

The mathematical operations will take numerical inputs from the RAM (from the stack) and replace them (on the stack) with the operation's result.

The ""Jump operations"" are taking us back to the first ""list of commands"" for the CPU, as we are all aware, in programming two of the most powerful tools we have are ""loops"" and ""conditions"", these make the difference between flicking switches manually and actually responding to various types of inputs, and being able to make varying computations with the same function - The Jump operations are able to check whether a condition applies (typically ""is the last stack item ==\<\> to 0"" and if that condition applies, the CPU will ""jump"" to a certain command in the list and start pulling the commands from that line to achieve conditional programming and loop iterations.

That is basically how a CPU operates. Hope it helps someone :)",1,0,0,False,False,False,1641199753.0
rue0sq,hzarh3z,t3_rue0sq,"You have to imagine a commend written in bits. So you have a sequence of bits which you can divide in two parts the first part is called opcode and the second address code. Opcode is for operational code and it define whch operation CPU has to perform. Understanding which operation CPU has to execute is the decoding part. 

This video explains very well what happens : [https://youtu.be/1BidIYu6Cls](https://youtu.be/1BidIYu6Cls)",1,0,0,False,False,False,1646387056.0
rue0sq,hqyo04x,t3_rue0sq,"Logic gates and other tiny shit. Gates flip open gates flip closed, shit gets decoded from java or cpp or another high level language to machine code like move eax to blah blah blah then pop and push and eventually it all ends up to 1's and 0's. Or so I've heard.",-3,0,0,False,False,False,1641147772.0
rue0sq,hqzz88x,t1_hqyhh8j,"Don’t think I could have explained any better. This is a good quality textbook-level response. Did you write CLRS ;)

Could you also clarify the third sentence? Are you saying 2 8 bit multiplexers logically function as a 16:1?",7,0,0,False,False,False,1641165600.0
rue0sq,hqz59os,t1_hqyo04x,"no, thats encoding ""commands"", op asked aboud decoding them",2,0,0,False,False,False,1641154037.0
rue0sq,hr00fnx,t1_hqyo04x,"gates don’t actually flip at all, their electrical output is altered depending on the input voltage. pop/push are actually implementation-dependent iirc, some just add and subtract from program counter.",1,0,0,False,False,False,1641166073.0
rue0sq,hr08rij,t1_hqzz88x,"2 separate 16:1 multiplexers, which have 8 bit wide inputs and outputs. So a total of 2x16x8 inputs, 2x1x8 outputs, and 2x4 select lines. Always confusing to talk about multiplexers because there’s a lot of independent bit widths to specify haha.",2,0,0,False,False,False,1641169413.0
rue0sq,hqzdtv5,t1_hqz59os,Oh shit just put it in reverse then.,0,0,0,False,False,False,1641157108.0
rue0sq,hr00gu1,t1_hr00fnx,"„˙ǝƃɐʇloʌ ʇnduı ǝɥʇ uo ƃuıpuǝdǝp pǝɹǝʇlɐ sı ʇndʇno lɐɔıɹʇɔǝlǝ ɹıǝɥʇ 'llɐ ʇɐ dılɟ ʎllɐnʇɔɐ ʇ,uop sǝʇɐƃ„",-1,0,0,False,False,False,1641166085.0
rue0sq,hr0y5b5,t1_hr08rij,"It's been a while since I actually looked at digital multiplexers in detail; could I ask how an 8 bit multiplexer is 16:1? Wouldn't it be able to take 256 different combinations to select? Sorry, I thought I understood this, but maybe I need to brush up myself :P Or are you saying there are 4 control bits for each one, meaning 2\^4 remaining combinations on the last 4 bits which would make more sense. Lastly, you're awesome! Is your background more in CS or CE/EE?",2,0,0,False,False,False,1641180067.0
rue0sq,hr8g6no,t1_hr00gu1,Good bot,1,0,0,False,False,False,1641316159.0
rue0sq,hr4fu28,t1_hr0y5b5,"8bits isn’t referring to the number of select lines but the size of each individual input. Most multiplexers are just 1bit but sometimes you want to switch not single bits, but entire busses. To give an example:

A 1bit (4:1) multiplexer, needs 2 select lines:

    In0 - 0
    In1 - 1
    In2 - 1
    In3 - 0

    S0 - 1
    S1 - 0

    Out - 1

With the select lines set to 01 it gives output In1 which is 1

A 3bit (4:1) multiplexer, still needs 2 select lines:

    In0 - 101
    In1 - 000
    In2 - 011
    In3 - 110

    S0 - 0
    S1 - 1

    Out - 011

With the select lines set to 10 it gives output In2 which is 011

You can build a nbit (X:1) multiplexer by taking n x 1bit (X:1) multiplexers and tying their select lines together.

My background is EE but working in embedded engineering which is sort of the half way point between EE and CS haha.",1,0,0,False,False,False,1641243955.0
rue0sq,hr8g7qf,t1_hr8g6no,"Thank you, bobthebuilder747, for voting on Upside_Down-Bot.

This bot wants to find the best and worst bots on Reddit. [You can view results here](https://botrank.pastimes.eu/).

***

^(Even if I don't reply to your comment, I'm still listening for votes. Check the webpage to see if your vote registered!)",1,0,0,False,False,False,1641316171.0
ruwflx,hr28k43,t3_ruwflx,"An installer is usually called a software installation package which (normally) includes a setup wizard to walk through which features should be added and to set up initial settings.

It's important to point out its usually a package, because it's packed up- a lot of data is compressed and in the case of online installers much of the data is omitted in favor of a downloading protocol for the data from an online server. Think Steam download, but for a single game or software. I would say this is the most common method for modern installers and why they can be smaller than a hundred megabytes.

For some, it is a fully packed software (offline installers), but these are often very large and tell the software how to decompress and reassemble itself.

A lot of libraries can be omitted because they come pre-installed or previously were and require separate downloads (think C++/.NET redistributables) which are shared between many apps.

Games usually only package the engine and boiler plate code and basic art (in the case of installers), then patches and more complex art assets are downloaded online as updates. That's why a lot of games will pitch a fit if you try to run it for the first time offline. Software in general usually includes an updater for patches as the installer package usually just includes the base code.

Specific components of an installer could be: decompression tool, downloader, updater, setup wizard, validater (making sure all common libraries/dependencies are there), along with a few others.",3,0,0,False,False,False,1641209641.0
ruwflx,hr1umh3,t3_ruwflx,"The installer is a dam wall, you hook up to it and it releases the dam water (the application) the installer is a completely separate program to manage the installation of a much larger complex app.",1,0,0,False,False,False,1641198715.0
ruwflx,hr2a00m,t1_hr28k43,So basically the size difference comes from the compression algorithms that were used and from omitting certain libraries and having .NET / C++ package manager install them if they were not previously installed? (In the case for offline installers),1,0,0,False,False,True,1641210663.0
ruwflx,hr1w3pg,t1_hr1umh3,"In your analogy, where does the water come from? And how come that dam is so much smaller than the water?",1,0,0,False,False,True,1641199878.0
ruwflx,hr2be6l,t1_hr2a00m,"Yes exactly, the runtime redistributables are usually handled by Windows though.",1,0,0,False,False,False,1641211637.0
ruwflx,hr1wduc,t1_hr1w3pg,"The water comes from a server and the damn is a networking/file configuration tool that connects with the host to open a file stream (connect the pipe to the damn wall) then the application starts pouring through, the installer handles keeping the connection open and
Managing the download progress. The installer just manages the flow of data, but the amount of data behind it is semi arbitrary, you just would t build an installer to install small, simple files.",1,0,0,False,False,False,1641200100.0
ruwflx,hr1wt3k,t1_hr1wduc,"I was asking about offline installers that have a significant size difference, they have to store the software inside the installer itself, take for example the old days when the installer was stored on a physical CD that was up to 700mb and still the software could go up to 5gb or even more, but this also happens with more modern installers",1,0,0,False,False,True,1641200440.0
ruwflx,hr33s6g,t1_hr1wt3k,"OIC yeah the compression ratios are nuts, I’m with ya there",1,0,0,False,False,False,1641225965.0
rubju8,hqxwosd,t3_rubju8,"You would need to look at the reference implementations and libraries for each language individually. C++'s STL has a lot of these kinds of rules, such as containers needing to return their size in O(1).",13,0,0,False,False,False,1641136836.0
rubju8,hqy5191,t3_rubju8,"In java you rarely use array, but a list implementation (arraylist, linkedlist, etc.). An add to arraylist is o(1).   


[https://www.programcreek.com/2013/03/arraylist-vs-linkedlist-vs-vector/](https://www.programcreek.com/2013/03/arraylist-vs-linkedlist-vs-vector/)",8,0,0,False,False,False,1641140464.0
rubju8,hqy67sp,t3_rubju8,"Also, the model behind time complexity does not take account of the modern processor architecture (caching, etc.) and this is the reason when any engineer worth it money will say: only after an actual measurement on a certain architecture will I tell you anything about an algorithm's performance.   


So there is a huge, real life difference between a theoretical algorithm and a concrete implementation of that algorithm on a certain architecture. Also nowadays you can run stuff on multiple cores as well, which makes things more tricky. Also in java (and js as well) you need to think about the GC.  


[https://stackoverflow.com/questions/10656471/performance-differences-between-arraylist-and-linkedlist](https://stackoverflow.com/questions/10656471/performance-differences-between-arraylist-and-linkedlist)",7,0,0,False,False,False,1641140950.0
rubju8,hqzh35y,t3_rubju8,Not sure if there is an existing resource for these quirks. You'll probably just need to look into the implementations of common structures in the languages you're curious about.,2,0,0,False,False,False,1641158435.0
rubju8,hqzta2x,t3_rubju8,"> In JS, array.push() is always O(1) since even if need to 'extend' the current array as the reference to the last item is simply removed.

Sometimes array.push requires a copy even in JS. The operation is **amortized** O(1), but it's useful to make sure you're comfortable with why that is.

Once you've got a decent understanding of common data structures it's pretty easy to recognize which ones are used in a given language. At that point it should be pretty easy to accurately guess what the complexity of each operation is.",1,0,0,False,False,False,1641163255.0
rubju8,hqyg5y3,t1_hqxwosd,Yeah pretty much.,3,0,0,False,False,False,1641144816.0
rubju8,hqzoj9k,t1_hqxwosd,I also like finding [cheat sheets](https://github.com/gibsjose/cpp-cheat-sheet/blob/master/Data%20Structures%20and%20Algorithms.md#10-data-structures) for this,2,0,0,False,False,False,1641161418.0
rubju8,hr0ewue,t1_hqxwosd,"> You would need to look at the reference implementations and libraries for each language individually. C++'s STL has a lot of these kinds of rules, such as containers needing to return their size in O(1).

What are some of rule's that C++ has for their reference implementations ? Some of the feature's being in some questionable performance debt from the new abstraction's being introduced",1,0,0,False,False,False,1641171956.0
rubju8,hr0uvm1,t1_hqy67sp,"Caching and parallelization affect runtime, not time complexity.",1,0,0,False,False,False,1641178637.0
rubju8,hr1gmf5,t1_hr0uvm1,And this is why time complexity as a mathematical formula has limited usage when talking about real world performance.,1,0,0,False,False,False,1641189176.0
rtu2da,hqw4i5b,t3_rtu2da,"Negative time means any date prior to 1970 - which at the time Unix time was developed was very important, because literally everyone programming with it was born prior to 1970. This is becoming less and less important as people die, but is still required. So if you take as assumed that time is stored in an integer of some kind and time starts counting from 1970, it is necessary so you can store dates before then. You could go with unsigned and start from 1901, but the reasons to *not* do that have already been covered.
 
The question of why time is stored as an integer of some kind is mostly to do with sorting and > < comparison in general, which is the most common date/time operations that exist by a mile. These operations must be as rapid as humanly possible, and so date/time storage is optimised for them, hence a single integer count (so you don't have to go looking for the year variable, then the month variable, etc etc).
 
The question of why time starts at 1970 was because at that gives enough time to cover preeeeeetty much everybody's birthdays (not all, but enough that those rare people just got to be born in 1901, there weren't many 70+ programmers back then, nor even employees or customers of that age), while simultaneously being long enough that it seemed like it would get sorted out before we got there (which is very likely to turn out true).",39,0,0,False,False,False,1641094912.0
rtu2da,hqux5xg,t3_rtu2da,Because you might want to store a time in the past,46,0,0,False,False,False,1641075739.0
rtu2da,hqv6xnd,t3_rtu2da,"What I’ve heard is that when the first Unix systems used the epoch, the compiler (maybe the architecture but that would be weird) didn’t differentiate between signed and unsigned ints. So it is maintained as that type for backwards compatibility, and because a bigger number wasn’t thought to be needed.",21,0,0,False,False,False,1641079911.0
rtu2da,hqv33yl,t3_rtu2da,"It’s convenient to store time using unix time; it’s zone agnostic; it’s generally simple to perform arithmetic on. 

And when many of these systems were created 2038 was a long way off. 

Much like only storing 2 digits of the date; the smallest viable size was chosen because storage wasn’t cheap at the time and when you’ve got millions of records a wasted 2 bytes can add up.",9,0,0,False,False,False,1641078280.0
rtu2da,hqvrlwa,t3_rtu2da,"Unsigned integer needs different (often unexpected) logic for arithmetic operations due to the fact that it can easily underflow at zero. So for arithmetic types, they are avoided. For example:

* People often do date calculations
  * `daysUntil = refDate - today`
  * `daysSince = today - refDate`
* `for (int i = x; ... ; i--)`, instead of checking for `i >= 0` you need to also check for underflow which is `i <= x`.",3,0,0,False,False,False,1641089005.0
rtu2da,hqw6zrn,t3_rtu2da,"Because according to Einstein, negative time *could* possibly happen, so we built our computer architecture to deal with it in case it happens!",1,1,0,False,False,False,1641096093.0
rtu2da,hqv1jkp,t3_rtu2da,[deleted],-2,0,0,False,False,False,1641077612.0
rtu2da,hqx0eba,t1_hqw4i5b,"Just to extend this and say that perhaps the most common operation done on the time stamp is the simple increment that happens once a second. And that's very easy to do with a single integer. Like you say, I'd we store it as a multipart number then we have to worry about roll over.",7,0,0,False,False,False,1641114682.0
rtu2da,hqwzfiv,t1_hqw4i5b,"Forgive me for the tiny, mostly useless contribution. But the internet as we know it will eventually be older than every living thing, assuming we live for another few thousand years at the least.",2,0,0,False,False,False,1641113899.0
rtu2da,hqy146a,t1_hqux5xg,Does this mean there is a limit on how far back the time can go by storing it as a signed integer?,2,0,0,False,False,False,1641138814.0
rtu2da,hqvsltr,t1_hqvrlwa,Oh wow I never knew that could be an issue!  I always just assumed that an unsigned integer sorta just bottomed out at 0.  I feel like these high level languages have coddled me and hid the weird reality of the circuits from me this whole time.,2,0,0,False,False,True,1641089457.0
rtu2da,hqwygup,t1_hqw6zrn,"Do you mean ""the past""?",7,0,0,False,False,False,1641113127.0
rtu2da,hqv5rrp,t1_hqv1jkp,"The unix standard (most frequently) represents time as a 32 bit signed integer, ranging from -2,147,483,648 to 2,147,483,647.
If you changed it to an unsigned 32 bit integer, you can now store values from 0 to 4,294,967,295. Because you have freed a bit from storing the positive or negative state of the number.
When the number of bits used is expected to be standard, it makes sense to wonder why we don't maximize how high we can count with those bits, and would in fact change the 2038 problem to a 2106 problem.",8,0,0,False,False,False,1641079417.0
rtu2da,hqx1jv6,t1_hqx0eba,"While it's something that happens, it's not a major issue as that rollover operation occurring just once a second is not a big deal. The issue is not really 'the time now', because that generally happens in one spot per machine (and these days aren't even counted in memory at all, they're tracked by an external clock and updated as needed). Previously stored time is where the efficiency is needed, because that is when you won't have 1 operation per second, you'll want billions.",-2,0,0,False,False,False,1641115632.0
rtu2da,hqxop59,t1_hqwzfiv,We can hope! :-),2,0,0,False,False,False,1641132811.0
rtu2da,hqwfcig,t1_hqvsltr,"> unsigned integer sorta just bottomed out at 0

If you try to assign a signed value(less that 0) to an unsigned integer, weird things happen which are implementation-specific. For example, if you assign a negative value to an unsigned int and compile with gcc, this is what would be assigned to the variable:
```
#include <stdio.h>

int main() {
    unsigned int illegalUint = -3;
    printf(""%u"", illegalUint);
}
```
the above code returns:
```
4294967293
```

Certain new languages like Go and Rust(i assume), do not allow signed integer assignments to unsigned integer variables.

> I feel like these high level languages have coddled me and hid the weird reality of the circuits from me this whole time.

I can kinda resonate with you, JavaScript was my daily driver like 5-6 months ago, I just did a lot of node.js and well when I started learning Go, I learned so much about memory and the CPU that I couldn't ever have learnt while using JS.",2,0,0,False,False,False,1641100327.0
rtu2da,hqxd8ze,t1_hqvsltr,"I, after nearly 40 years of working with computers, built my own Z80 on a breadboard (am building, actually, I’m adding stuff to it) and have been programming it in assembly since I’ll need the assembly code to add devices to CBIOS.  It’s been an amazing experience and I’ve learned so much. Everyone who does this professionally should take on a project like this.",2,0,0,False,False,False,1641125271.0
rtu2da,hqwfl5p,t1_hqv5rrp,"> would in fact change the 2038 problem to a 2106 problem

Yeah, that is why shifting to 64-bit architecture is the best option by far. Btw I wonder that when will we encounter a similar problem with int64.",-4,0,0,False,False,False,1641100458.0
rtu2da,hqx0is3,t1_hqwfl5p,"> I wonder that when will we encounter a similar problem with int64.

If only we could calculate this somehow, e.g. simply by finding the maximum value it can hold and adding it to the start date! :)",5,0,0,False,False,False,1641114786.0
ru5qo2,hqx5oww,t3_ru5qo2,"This sounds like [Complexity Theory](https://en.wikipedia.org/wiki/Computational_complexity_theory). You might consider connections to the more general question ""for which problems can we describe limits on the time, space, (or other resources such as number of states) needed by any Turing machine which solves them""",4,0,0,False,False,False,1641119085.0
ru5qo2,hqx6x8l,t3_ru5qo2,"To answer your specific question, it can be shown that 2 states is ""sufficient"" for any kind of Turing machine you want to make. The more interesting tradeoffs are in time (number of steps) and space (number of visited tape cells).

EDIT: I was wrong, I confused the alphabet size with the number of states. They are related in the sense that a machine with fewer states may require more alphabet symbols to be universal, and vice versa. But here's a universal machine with 2 states and alphabet size 3: [Wolfram's (2,3) Machine](https://en.wikipedia.org/wiki/Wolfram%27s_2-state_3-symbol_Turing_machine)",1,0,0,False,False,False,1641120106.0
ru5qo2,ht2eu4x,t3_ru5qo2,"A Turing machine has a finite number of states in its CPU. However, the states are not small in number. A real computer consists of registers which can store values (fixed number of bits). A universal Turing machine can be constructed using one tape and having only two internal states.",1,0,0,False,False,False,1642443233.0
ru5qo2,hqx5ptn,t1_hqx5oww,"**[Computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)** 
 
 >Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm. A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1641119106.0
ru5qo2,hqxkx7g,t1_hqx6x8l,"Hang on. A ""Turing Machine"" can be a one bit system?",1,0,0,False,False,False,1641130619.0
ru5qo2,hqyhgo7,t1_hqx6x8l,"I think the alphabet (tape characters) can be of size two to compute anything (I.e binary), but I don’t think all computations can be done with two states…",1,0,0,False,False,False,1641145310.0
ru5qo2,hra6ief,t1_hqxkx7g,"The tape must be infinite, of course.",1,0,0,False,False,False,1641340239.0
ru5qo2,hqzghkm,t1_hqyhgo7,See my edit. 2 state UTM can exist,1,0,0,False,False,False,1641158051.0
ru5qo2,hr9uiw0,t1_hqzghkm,"A 2 state TM can exist, but it can't do everything. A larger alphabet can help out, but with a single tape, a 2-state machine can't do that much.


I've made a program on a 2 state 3 letter turing machine that simply increments a binary number until it hits 255, where the machine crashes. Not much more can be done with that kind of machine.",1,0,0,False,False,True,1641335476.0
ru5qo2,hra06vy,t1_hr9uiw0,Everything can be done with that machine. That's what makes it universal. It may take more patience than you have.,1,0,0,False,False,False,1641337713.0
ru5qo2,hra0ddo,t1_hra06vy,"If it can, can you show me an example of it emulating, say a 16 state 16 letter machine?",1,0,0,False,False,True,1641337787.0
ru5qo2,hra5j2b,t1_hra0ddo,"No, because it would take more patience than I have. But it's possible. Please read the article I linked.",1,0,0,False,False,False,1641339847.0
ru3c37,hqwte9o,t3_ru3c37,"I have read or listened or talked about this like some months or an year ago but ah I can't really remember where I read/listened/talked about it. What I do remember is that an AI was given all the laws of classical and quantum physics and it was given the task to discover new laws. My guess is that I read about this in either of these two books:
- A Mathematical Universe by Max Tegmark
- Life 3.0 by Max Tegmark

I probably have physical copies of both the books, so I'll give both of them a look and respond back.",4,0,0,False,False,False,1641109157.0
ru3c37,hqxp92c,t3_ru3c37,"There are two ways to approach this idea.

* You could try to use computers to verify proofs of theorems, and maybe even generate new proofs of new theorems. There has been some progress on this, but it turns out that mathematical proofs tend to leave out ""obvious"" steps that need to be written out in painstaking detail for computers to understand. Additionally, this approach doesn't help with non-mathematical things, such as understanding language. Relevant link: [https://www.quantamagazine.org/lean-computer-program-confirms-peter-scholze-proof-20210728/](https://www.quantamagazine.org/lean-computer-program-confirms-peter-scholze-proof-20210728/)
* We can also try to ""understand"" the world, with all its complexities and ambiguities, in an ad-hoc way, learning what's what. In the last decade, there's been an explosion in this thing called ""deep learning"", which is a method in AI. This has been pretty successful and has been used commercially. However, due to its ad-hoc nature, it's hard if not impossible for a human what the computer is ""thinking"", making progress in this field kind of slow and full of trial-and-error (except that ""throwing more hardware into the problem"" seems to work surprisingly well).",3,0,0,False,False,False,1641133116.0
ru3c37,hqxiz4d,t3_ru3c37,"Point 1 :

What you displayed here is what Einstein referred to when he used the word ""imagination"". I call it ""inspiration"".

It's at the root of almost all of our startling discoveries. And I don't think an AI will be able to replicate that in a long, long, long while. .

Point 2 :

Because of Point 1, what your model at best could perform is fetch us the ""closure set"" of all mathematical concepts that apply due the concepts and first principles already known to us. For eg. (and I don't know how our present AI models work so forgive me if I'm incorrect about this example) but think about imaginary numbers.

If humans hadn't invented the concept of imaginary numbers as a mathematical tool, what I think is that your AI could've only fetched us knowledge applicable to Real Numbers.",3,0,0,False,False,False,1641129389.0
ru3c37,hrdquba,t3_ru3c37,"Ok, so I like your general train of thought, but please allow me to humor you with this angle, namely that our “pool” of knowledge is always essentially what we hold in our collective RAM/working memory for the explicit purpose of executing a task.

Therefore energy must actually be invested by ourselves to how we organize/operate etc. Therefore what you are proposing is totally natural, and in many ways one could argue is already being done although in its only just getting started in its potential. My main point is that we as humans are essentially neural processing nets that guide energy allocation to support ourselves. For example if we need to come up with a specific protein to create a biological outcome for a process we tool AI to that end. 

What you are suggesting, correct me if I am wrong is to direct AI to building a general knowledge pool. The problem is that is too broad.

The most interesting area I see is actually the creation of an object oriented learning architecture that capitalizes everyone’s unique interest/energy level in that interest to dynamically transmit information and organize ourselves at the same time.

With such a system our neural nets actually can function in symbiosis with AI where we both can help each other perform at our collective maximum with us guiding systemic output towards to meeting our current systemic demand while building an exponentially more capable system, itself based on an exponentially growing base of actionable knowledge.",1,0,0,False,False,False,1641405121.0
ru3c37,hsggjg2,t3_ru3c37,How would the system determine when a theory has been discovered so it can proceeded to check it,1,0,0,False,False,False,1642055469.0
ru3c37,hqwva81,t1_hqwte9o,"After some googling, I found this [article](https://www.technologyreview.com/2018/11/01/1895/an-ai-physicist-can-derive-the-natural-laws-of-imagined-universes/) which talks briefly about this [research paper](https://arxiv.org/abs/1810.10525) by Max Tegmark and Tailin Wu from MIT.

Also I looked up Life 3.0 and well it is a pretty heavy book so I could not find much about the AI discovering the new laws and axioms except two topics that I thought might help. I read about AI creating laws for the society and a topic titled ""Physics: The Origin of Goals"". I am sure that the paper mentioned earlier would definitely be helpful.",2,0,0,False,False,False,1641110603.0
ru3c37,hqwtrx3,t1_hqwte9o,I actually have Knowledge 3.0 as well (I‘m a general intelligence btw hahahahaha). I‘ll have a look too. Omw to creating a discord.,1,0,0,False,False,True,1641109443.0
ru3c37,hqy4w1w,t1_hqwte9o,I’ve read life 3.0 and would highly recommend it to anyone interested in this topic. Whilst I can’t 100% remember if it touches on OP’s specific example it definitely at least explores the idea,1,0,0,False,False,False,1641140405.0
ru3c37,hqxfaiy,t1_hqwte9o,"The problem I can see with this is the fact that even if the AI finds anything, there's no experimental confirmation involved. 

At most it'll be a 'string-theory'-like idea, that isn't and can't be substantiated. Only you'd get probably millions of them.

Then you'd need to create AI to be able to test (or come up with methods of testing) all of those hypotheses xD",0,0,0,False,False,False,1641126815.0
ru3c37,hr2vr9e,t1_hqxp92c,I was thinking more of a principia mathematica approach with the dataset inputing a set of axioms and the output being a closely related result (mabye only needing one or two lines of logic). I assume we can then better know what the AI is doing.,1,0,0,False,False,True,1641222586.0
ru3c37,hr2v94s,t1_hqxiz4d,Have you heard of constructor theory? Maybe we can rearrange the axioms by finding the counterfactuals. For example sqr(-1) doesn’t exist is the counterfactual to sqr(-1) = i . Also even if it only does that I‘m very happy.,2,0,0,False,False,True,1641222360.0
ru3c37,hqwwb70,t1_hqwva81,This is insane. Guess I know who to talk to when I visit Big Tech in Summer! I also had the same Ideas about morality before I read about this. Basically you define a goal and then the moralistic stance just revolves around the goal.,2,0,0,False,False,True,1641111401.0
ru3c37,hqwx0dh,t1_hqwva81,I think the best goal to be honest is just the pursuit of knowledge itself. I know it sounds crazy but it’s a cause I‘m willing to die for when I think about it. Even If we can’t pass all the great filters of the universe maybe we can at least pass on the ability to more advanced forms of life. Our feeling of being human is restricted to knowledge 2.0 something that is probably not capable of understanding the universe.,2,0,0,False,False,True,1641111952.0
ru3c37,hr2wz6i,t1_hr2vr9e,"What's the difference between that and the first approach I mentioned?

Also, AI is much, much harder than you think.",2,0,0,False,False,False,1641223124.0
ru3c37,hr2zskv,t1_hr2v94s,">Have you heard of constructor theory? 

Nope. Enlighten me.",1,0,0,False,False,False,1641224318.0
ru3c37,hr3057e,t1_hr2wz6i,"The difference is that I‘m guiding the AI in a certain direction. The loss function then becomes trivial. I’ve programmed machine learning algorithms from the ground up, I know I’m getting into a lifelong journey. I think it’s fundamentally however it’s the thing most worth pursuing if we ever want to fully understand mathematics and physics.",1,0,0,False,False,True,1641224466.0
ru3c37,hr30p7j,t1_hr2zskv,It basically just questions axioms of physics by assuming the opposite is true.,2,0,0,False,False,True,1641224697.0
ru3c37,hr33qcm,t1_hr3057e,"I don't quite understand what you're saying. Is this what you're looking for?

>A formidable open challenge in the field asks how much proof-making can actually be automated: Can a system generate an interesting conjecture and prove it in a way that people understand? A slew of recent advances from labs around the world suggests ways that artificial intelligence tools may answer that question. Josef Urban at the Czech Institute of Informatics, Robotics and Cybernetics in Prague is exploring a variety of approaches that use machine learning to boost the efficiency and performance of existing provers. In July, his group reported a set of original conjectures and proofs generated and verified by machines. And in June, a group at Google Research led by Christian Szegedy posted recent results from efforts to harness the strengths of natural language processing to make computer proofs more human-seeming in structure and explanation.

https://www.quantamagazine.org/how-close-are-computers-to-automating-mathematical-reasoning-20200827/",2,0,0,False,False,False,1641225944.0
ru6jx7,hr5d36f,t3_ru6jx7,"> I understand a database containing passwords should be hashed using some sort of key

Keep in mind that [cryptographic hashing and encryption](https://www.ssl2buy.com/wiki/wp-content/uploads/2015/12/hashing-vs-encryption.png) are two different things. 

You aren't hashing the whole database, just the password. Passwords are salted and then put through an execution unit in the CPU that does [this](https://emn178.github.io/online-tools/sha256.html) to them, so that if there is a data breach or a rogue sys admin, there aren't a bunch of plaintext passwords floating around.

> but I understand that each entry should be encrypted again in some other way.

Depends. [Some implementations encrypt as the information is written to disk, others don't.](https://en.wikipedia.org/wiki/Disk_encryption#Transparent_encryption) The database is on the disk. Encryption and hashing can be done at the hardware level; there could be, for instance, an [AES execution unit](https://en.wikipedia.org/wiki/AES_instruction_set) in the CPU before it's sent over the wires (maybe the SATA bus?) to be written to disk.",1,0,0,False,False,False,1641257286.0
rubwjz,hqxzhyz,t3_rubwjz,"Could you give an example of what you mean? Often when computers/programs use different data types or file formats they just can’t communicate. To get around this, programmers pick standard formats to use so other programs can understand the output. 

Try to open a word document in notepad and see this failure in action.",9,0,0,False,False,False,1641138110.0
rubwjz,hqy86il,t3_rubwjz,">how can computers with different systems that have different data type representation communicate properly?

The same way countries with different monetary systems are able to trade goods: they agree on a common system of exchange.",6,0,0,False,False,False,1641141738.0
rubwjz,hqydxle,t3_rubwjz,"It's called ""serialization""/""deserialization"". Anything that transmits or stores data has to do it. This can be very simple and fast (but not very portable) or more complicated and slow (but more portable).

At one extreme, you might just save your whole memory image to disk. It would be pretty fast, and you wouldn't need to do anything to your data. For this to work, you probably would need the exact same version of the exact same program on the exact same operating system (and probably a little help from the OS).

At another extreme you could use a text based format like JSON or XML. Everyone knows how to store and transmit text, and standard formats mean it's easy to write libraries that convert them to whatever internal representation your program/language requires. Unfortunately, this process is pretty slow (imagine converting millions of ints into a variable number of characters). It also wastes space. But the up shot is that it's super portable. Even humans can read and understand it!

There are compromises as well. Protobuf is a tool from Google that helps you define how things get serialized and communicated. Two programs have to use the same protobuf definition and version, but they can be written in different languages and run on different OS's. It's still kind of slow, but it's more efficient than JSON. Python has something called ""pickle"" that does this, but it only works in python.

Serialization/deserialization is a huge problem in distributed systems. It's a significant fraction of all computer cycles at Google (look up ""the data center tax""). It can really slow down distributed frameworks and overall is a pain in everyone's butt.",3,0,0,False,False,False,1641143968.0
rubwjz,hr1klvp,t3_rubwjz,"https://afteracademy.com/blog/what-are-protocols-and-what-are-the-key-elements-of-protocols

This seems like a reasonable summary",2,0,0,False,False,False,1641191619.0
rubwjz,hr0xjgv,t1_hqy86il,">a common system of exchange

AKA. a **protocol** for communication and behaviour",5,0,0,False,False,False,1641179799.0
rtkdm7,hqt4qqc,t3_rtkdm7,"Turns out, individual people aren't that unique.  A classic (literally, textbook) example is if 10,000,000 other people searched beer, then bought beer and diapers, when you search beer, or even just linger over an ad for beer long enough to indicate it caught your attention, you're going to get ads for beer and diapers, even though you didn't search diapers.  That's an incredibly simple example, in reality the algorithms look at many more variables than that, looking for patterns.  Maybe people who by beer and milk get ads for diapers and people who buy beer but no milk get ads for protein powder.  There is a bit of confirmation bias to it as well - you notice the times the algorithm was correct, and you don't notice when it wasn't, because that just looks like ""random ads"".

Other times, it's as simple as, for example, Google seeing what kind of products you get spam email for, and showing you ads for things bought by people who got those same emails, or showing you ads for things one of your family members or friends was searching for (it knows your ""friends"" by doing things like matching up who has each other's numbers or emails in their contacts list, when you give it permission to ""manage contacts"" or whatever.)

Another thing that happens is these people were getting those ads all along, but simply never noticed.  Once they do think of that thing, they start noticing the ads; basically the same as how you never notice a certain kind of car until it's brought to your attention somehow (an ad, a friend buys one, etc) then all off a sudden you see them everywhere you look.  The world didn't change, your brain-vision-attention filter did.",61,0,0,False,False,False,1641048007.0
rtkdm7,hqt6f4q,t3_rtkdm7,"It isn't straight up mind reading but rather pattern recognition. People are more predictable than we want to admit we are and most people do pretty specific things in specific situations. What big tech has is a fuck ton of data that shows that if you like X you're extremely likely to also like Y.

Let's say for example Google figures out that you like heavy metal. It isn't wrong to like heavy metal. Some people like heavy metal and some people don't. Then let's say that they figure out that 96% of people who like heavy metal also like cheap beer. What Google knows is that betting that you like cheap beer if you like heavy metal is a safe bet as there's only a 4% chance that they're wrong. So what they then do is work with advertisers who want to sell cheap beer and say ""metal fans like cheap beer, target them."" That isn't individual mind reading; that's just recognizing patterns in human behavior. If you like heavy metal then chances are you're in the 96% that also likes cheap beer. To an individual this might look like mind reading as you get targeted advertisement but it really isn't mind reading. What happens is Google analyzes your patterns, connects them to overall population patterns, and figures out what you like and what you tend to search for. They'll also notice things like people who enjoy heavy metal are extremely unlikely to search for, say, Brittney Spears and are thus not going to recommend her to metal fans.

What they use is machine learning which chews through obscene amounts of data and finds the patterns. It genuinely is just computers doing math and pattern recognition then figuring out ""people who do X are highly likely to also do Y."" There are also certain patterns that most people show in their searches when they're going through certain things or are about to do certain things that are also recognizable.

The other thing is that big tech tracks what you search for and knows what you do and don't like. Yes this is actually kind of creepy and I don't like that they do it but right now it's legal so they aren't going to stop. Even then though most of it is mundane; we all have our favorite foods, income levels, and demographic information that affects how we behave. So if I like cheap tacos Google probably knows I like cheap tacos. So if I go to Google Maps and search for ""restaurants"" it's going to give me recommendations that are appropriate to my preferences and income level. If I'm broke as fuck and like cheap tacos it is less likely to show me the most expensive steakhouse in town first. That's not mind reading that's just the machine looking at the numbers and making recommendations based on the patterns it knows.

This also isn't new. Supermarkets and department stores have been analyzing customer patterns longer than computers have been around and have noticed weirdly specific behavioral patterns in humanity. The only difference is that computers are doing it rather than people at Google. One downright weird thing that people do when going into stores is we turn right first. Not everybody does it but the vast majority of people when they walk into a store look and turn right. Why? Who the fuck knows? However there's a weird exception; the Japanese go left.",15,0,1,False,False,False,1641048916.0
rtkdm7,hqt86pr,t3_rtkdm7,"Thank you so much for your considerate responses, very kind 🙂",5,0,0,False,False,True,1641049849.0
rtkdm7,hr1xx3l,t3_rtkdm7,"You cannot read thoughts without hooking up your body as an input to a machine that could interpret it's thoughts.
The algorithms in Social Media / Google / etc. Are based on the profile they have gathered about you (that is linked to your Social Media account / IP address / MAC address) and compared with the behaviour of accounts that are vaguely similar to you in various aspects (mostly relating to shopping habits).

As was mentioned before, humans are not that different from one another, and age groups with vaguely similar interests will often search for similar things at similar situations, and if you take into consideration the fact you are most likely to notice something when it's really extraordinary (like getting suggested something you only thought about for a second and never searched for, a minute after you thought it), you get the feeling that someone is literally reading your mind.

You are getting algorithmic suggestions all of the time, chances are if the algorithm is good enough, it will suggest something that is anecdotal to your recent actions (since other vaguely similar people searched for it right after behaving in a similar way to you). But when it's suggesting something irrelevant you simply won't notice.",1,0,0,False,False,False,1641201307.0
rtkdm7,hqtpj2h,t3_rtkdm7,"There are special algorithms, called artificial neural networks. Those mimic the way your brain behaves, and are trained to recognise patterns in a dataset. Big tech coorperations train these, to predict what the users will do.",-6,0,0,False,True,False,1641057745.0
rtkdm7,hqvn6yw,t3_rtkdm7,"If someone figured out how to read minds, do you think the first thing they would do is try to sell you stuff? And if they did, wouldn't they stop advertising things you don't want?",-1,0,0,False,False,False,1641087066.0
rtkdm7,hqtsn7m,t1_hqt4qqc,">Turns out, individual people aren't that unique.

[You're all individuals!](https://youtu.be/KHbzSif78qQ?t=31)",11,0,0,False,False,False,1641059049.0
rtkdm7,hqt8u5j,t1_hqt4qqc,"> just linger over an ad

Ok so they are also using facial recognition software to detect anomalys or emotional reactions to things?",-16,0,0,False,True,True,1641050186.0
rtkdm7,hqtmbc2,t1_hqt6f4q,And most Brits.. we go left to cos theres typically cheap beer right to the left of a supermarket entrance. Coincidence? I think not,0,0,0,False,False,False,1641056389.0
rtkdm7,hqtqv9d,t1_hqtpj2h,Fascinating. Can you go into a bit more detail? How many neurons are they based on?,0,0,0,False,False,True,1641058316.0
rtkdm7,hqtuoot,t1_hqtsn7m,Yes! We are all different! (I'm not..),4,0,0,False,False,False,1641059891.0
rtkdm7,hqtdseq,t1_hqt8u5j,No. Linger as in mouse over it or have it on your screen for longer than necessary.,18,0,0,False,False,False,1641052605.0
rtkdm7,hqtl9ch,t1_hqt8u5j,"No, I meant if you're scrolling down a news feed and briefly stop scrolling, they just detect the pause and assume you were interested in whatever was on screen.",4,0,0,False,False,False,1641055942.0
rtkdm7,hqtsnjf,t1_hqtqv9d,"They are based on mathematical equations, and the Neurons can be specified by the developer. There are different types of Neurons, unlike the human brain, to handle different tasks like image recognition or generation",-1,0,0,False,False,False,1641059053.0
rtkdm7,hqtqi89,t1_hqtdseq,How to the log data like that?,2,0,0,False,False,False,1641058161.0
rtkdm7,hqtrlek,t1_hqtl9ch,">linger over an ad

Which means that my cats' demands for attention control the advertisements I see.",4,0,0,False,False,False,1641058618.0
rtkdm7,hqtz4u3,t1_hqtsnjf,"Could this be the start of ASI? If the developers are in charge of the number of neurons, couldnt they make one with 86billion the same as a human brain ala mind control! Im freaking out 😬",-6,0,0,False,True,True,1641061706.0
rtkdm7,hqtu0fr,t1_hqtqi89,"Your browser knows where your cursor is and what is currently being rendered. A website can just log that information (so long as its happening on its own site, see Same Origin Policy).",8,0,0,False,False,False,1641059615.0
rtkdm7,hqu72pp,t1_hqtz4u3,"Adding more and more neurons does not necessarily mean ""real"" intelligence will emerge.

A rough analogy: if you throw a 1,000 toddlers in a room, that doesn't mean the group of toddlers can solve 1,000x (or even 2x) harder math problems. They probably can solve more problems collectively than individually, but their capacity is still limited by their own toddler brains.

Likely, there is new stuff that needs to be invented to develop something we would call intelligence.

If you are interested in this stuff, I would highly recommend the book *Superintelligence: Paths, Dangers, Strategies*. It goes into a lot of the interesting concepts behind AI / how we can classify intelligence.",5,0,0,False,False,False,1641064938.0
rtkdm7,hqv2exs,t1_hqtz4u3,"The ""neurons"" in our artificial neural networks are very simplified models of biological neurons in a human brain. They don't function the same way and they aren't connected to each other in the same way. They don't learn the same way and they aren't seeing the same kind of experiences and data to learn from. It's sort of like a child's model of the solar system compared to the actual solar system. It's a useful model of computation -- not a physically accurate simulation of human brains. So even if you keep adding them in, there's no guarantee you're ever going to reach human level thought.

That said, suppose you built a perfectly accurate brain simulation able to do everything a human could do. On the one hand, you'll be as famous as Newton and Einstein. It's a 10,000 year discovery in terms of significance. On the other hand, creating something the works like a human brain can be done by nearly anyone given nine-months to build it and a decade or two to train it. Just making new human brains isn't especially frightening. Every infant doesn't grow up to become some sort of terrifying eater of worlds just because they can think as well as a human.",2,0,0,False,False,False,1641077984.0
rtkdm7,hqu15ob,t1_hqtz4u3,"Theoretically, that is possible, but practically the computational power is required, it is impossible, to build something of that scale, yet.",0,0,0,False,False,False,1641062532.0
rtkdm7,hqwlvf7,t1_hqtz4u3,"Stop using reddit and the internet to feed delusions. I suspect you're the user Insight_7407 and god knows how many other accounts.

Stop the bullshit, and don't lie to yourself. You're seeking out this type of information and interactions intentionally, and you can easily choose not to.

https://old.reddit.com/r/computerscience/comments/r1xcw9/artificial_super_intelligence_asi/",1,0,0,False,False,False,1641104054.0
rtkdm7,hqtvc3u,t1_hqtu0fr,"To be more exact, it is the script (javascript) that is running that does the tracking of where the cursor is. Browser itself just provides the information to the script and the script decides what to do with it. 

This is the problem with allowing shady code from untrusted sources to be run in on your browser since they get access to a lot of behavior-related information and sometimes some private information as well.

Unfortunately, a lot of the scripts are necessary these days for even basic functionality in the web so disabling it is not that easy.",3,0,0,False,False,False,1641060159.0
rtkdm7,hqx9bqi,t1_hqwlvf7,"No your misunderstanding me, this is just a thread about technology that dose exist of which im trying to understand from people who know about it..if i was delusional this process wouldnt be very helpful. I have a right to post what i want",1,0,0,False,False,True,1641122101.0
rtv1bn,hqv54vy,t3_rtv1bn,"Like _lambda calculus_ or _Markov algorithm_?

^(I hope you don't think of TM as of an actual, physical machinery?)",23,0,0,False,False,False,1641079151.0
rtv1bn,hqwmsmj,t3_rtv1bn,"John Conway's ""Game of Life"" is not exactly mechanical (or easy to physically automate), but it is easy to play by hand, with grid paper and beads. It is much simpler than a Turing machine, but equally powerful.",3,0,0,False,False,False,1641104624.0
rtv1bn,hqw9pdz,t3_rtv1bn,"Any other computational model exactly as powerful as the Turing Machine IS a Turing Machine (and even if it apparently looks different, you could express it's functioning in terms of the usual Turing Machine concept with finite control and one or more read/write tape(s)).

Understand that Turing Machine is a theoretical model of computation.

Specifically, it represents a logical set of computational possibilities. The setup with finite control and one or more (unbounded) read/write tapes can perform all of those computational tasks.

Any Machine/Model that can perform all of the computational possibilities in aforementioned set and no more, is an implementation of Turing Machine.",7,0,0,False,False,False,1641097433.0
rtv1bn,hqv6t1m,t1_hqv54vy,"I suppose they both count, however Turing's model of computation seems the simplest of the three to understand, and I realize I may have not asked the same thing I was wondering. Is there a type of *mechanical computer,* as powerful as the turing machine, that can be implemented in the same way as something like [this](https://youtu.be/vo8izCKHiF0)?
Sorry for any confusion. English *is* my first language, but I'm just really poor at articulating something correctly the first time.",5,0,0,False,False,True,1641079857.0
rtv1bn,hr4rs9n,t1_hqv54vy,">I hope you don't think of TM as of an actual, physical machinery?


There are at least three physical implementations of a turing machine that I've seen, one partly electronic, one partly electrical, and one made entirely out of wood and metal. There's a link I posted on one of my comments to see the wooden one. Of course, it is only a 3-state 3-letter alphabet turing machine with a finite tape roll, far less powerful than an infinite-state 2-letter alphabet one with an infinite tape.",1,0,0,False,False,True,1641248580.0
rtv1bn,hqveuii,t1_hqv6t1m,"Ah, Turing-complete mechanical computer then? Like [Babbage's Analytical Engine](https://www.youtube.com/watch?v=5rtKoKFGFSM)?",3,0,0,False,False,False,1641083347.0
rtv1bn,hqw384f,t1_hqv6t1m,"A 'TM' like that with a fixed-size tape has no special power, it's a finite-state machine. Trivially, consider an _n_-state FSM which has inputs for every possible transformation on _n_ states. Clearly, such a machine has a ridiculous number of inputs, but it can be used to simulate any other _n_-state FSM by attaching a little circuit in front that performs a simple mapping from the inputs of the target machine to the inputs of the ""universal"" FSM which effect the same transitions. That little circuit is like a program. The 'TM' here is like one where the program takes a single input, the turn of the crank.

Such a ""universal"" FSM can be reduced with semigroup methods to be quite compact; this (_n_+1)-state transition table (Hartmanis and Stearns 1966, p. 193) can simulate any _n_-state machine, by mapping inputs of the target machine to sequences of inputs for the table:

state | 0 | 1
---------|----------|----------
1 | 2 | _n_-1
2 | 3 | 1
⋮ | ⋮ | ⋮
_k_ | _k_+1 | _k_-1
⋮ | ⋮ | ⋮
_n_-1 | _n_ | _n_-2
_n_ | 1 | _s_
_s_ | 1 | 1

Draw out the exact table for some small _n_ > 3, try inputs 0, 1, 00, 01, ... and you may be able to see how it works. I can imagine a mechanical realization of this device which the margin of this textarea is too small to contain.",3,0,0,False,False,False,1641094309.0
rtv1bn,hqyojbe,t1_hqv6t1m,"Yes, as others have mentioned, since the universe is finite, you will never be able to build a real Turing machine",1,0,0,False,False,False,1641147963.0
rtv1bn,hr54ofe,t1_hr4rs9n,"Yes, of course. But many people\* make a mistake of thinking that TM is exactly such physical implementation.

The difference is, in case of ""normal"" machines (ATM, fax machine, washing machine etc.), it's the implementation what matters.  
In case of TM, such literal implementations are mostly toys made for fun; it's the concept that matters.

&nbsp;

^\* ^(what baffles me, you can find some of them even amongst 2nd year CS students)",1,0,0,False,False,False,1641253788.0
rtv1bn,hupc038,t1_hr4rs9n,isn't it technically not a turning machine if the tape is finite but a linear bounded automata?,1,0,0,False,False,False,1643450197.0
rstlls,hqoqcw7,t3_rstlls,"It's not ""random"" as in roll a dice random.

It's random as in ""arbitrary"". Any position you like can be accessed in the same time / speed / effort.

This is as opposed to stacks, tapes, hard drives and such.

In a stack it's fast and easy to access the top (data point) of it quickly. You don't know what's below it until pop the top to the next below data point and you read it, and so forth.

In a tape or a hard drive, the ""read head"" goes through the data sequentially (literally in a mechanical motion), so that data near the read head is faster accessible than data way before or after it, since the read head needs to seek to that position first to access the data.

RAM has no mechanical or moving components, it's based on electrical current and signals being transmitted.",272,0,0,False,False,False,1640962406.0
rstlls,hqog358,t3_rstlls,"Using addresses you can access any random memory location when you need it. You can't do that with a tape-based memory, for instance. To access the end of a tape you would need to spool through the whole thing, you can't just access any location.",90,0,0,False,False,False,1640957084.0
rstlls,hqoy7q2,t3_rstlls,"Does no one read the other answers? Or just assume none of the first 20 people answered it, and write out the same thing everyone else said?",8,0,0,False,False,False,1640965931.0
rstlls,hqog5s2,t3_rstlls,"It's random because it's describing the way you access memory addresses - **randomly**. Prior to this type of memory, people used some sort of tapes, where you had to sequentially start from lowest address and then go further. With random access you don't need to start from lowest address, you can start at **any** address.",23,0,0,False,False,False,1640957127.0
rstlls,hqogd8m,t3_rstlls,"Random as opposed to serial access.

Magnetic tape was used in early computers not only store instructions but to write results.  Core memory and, eventually hard drives were random access by contrast.",5,0,0,False,False,False,1640957247.0
rstlls,hqolpb4,t3_rstlls,"RAM stands for Random Access Memory. Random access means that you can access any part of the memory at any time, essentially in ""random"" order.

This is in contrast to serially accessed memory in which data can only be accessed sequentially in the order that it is physically present on the medium. You can't ""jump"" to different parts of the data without ""scrolling"" through the bits in the middle.",4,0,0,False,False,False,1640960118.0
rstlls,hqog0az,t3_rstlls,"From Wiki.

A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such as hard disks, CD-RWs, DVD-RWs and the older magnetic tapes and drum memory), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.",2,0,0,False,False,False,1640957039.0
rstlls,hqommuq,t3_rstlls,"Random is meant to characterize it as different from sequential.

In the old days, storage was on tape. This meant the tape had to be wound to the right location, so generally that type of memory is called sequential because you read/write from beginning to end. Random in this case simply means you can read/write from wherever you want without having to wind any tape.",2,0,0,False,False,False,1640960588.0
rstlls,hqp86ay,t3_rstlls,"It's Random because you can randomly ask from any memory location and based on the information provided like index it can calculate where that memory would be.

Unlike non random where you need to check one by one each memory block to find if that's what you need.

---

**An analogy**

Array is a DS that provides random access you can just say that what `arr[i]`. You can't say that in LinkedList you need to traverse Linked List to reach to `ith` Node.",2,0,0,False,False,False,1640970061.0
rstlls,hqpky7j,t3_rstlls,"Additionally to the previous comments, the random access component is also relevant in the context of data structures. So for instance, stacks and linked lists don't allow random access to elements, but arrays do.",2,0,0,False,False,False,1640975226.0
rstlls,hqpumfz,t3_rstlls,"The second word indicated by the acronym is the clue: ACCESS.

It means you can access (read or write) any memory location at any time, just by driving the address.",2,0,0,False,False,False,1640979208.0
rstlls,hqq55ln,t3_rstlls,"Random access means you can access arbitrary memory locations in any order. An alternative would be sequential access in which memory could only be accessed in order and to get to a memory location at a given address you would have to go through every location whose address comes before it in order.

This isn't just a hardware thing either. There are also data structures that have these access characteristics. The simplest examples of this would be an array or hash table which has random access, a singly linked list which is sequential access only, and then a doubly linked list which has bidirectional sequential access.",2,0,0,False,False,False,1640983642.0
rstlls,hqroiwn,t3_rstlls,If only we could refactor technical jargon the way we do code. Arbitrary access memory just makes so much more sense.,2,0,0,False,False,False,1641009652.0
rstlls,hqqfwdx,t3_rstlls,Ram does not stand for random it stands for Random Access Memory. Which means you can access every address of memory randomly as in it does not matter when you address which memory slot,4,0,0,False,False,False,1640988318.0
rstlls,hqq94s3,t3_rstlls,"Most answers here are indicating that the ""Random"" refers to where memory is accessed - this is only partly correct.

Think about the acronym of RAM in contrast to ROM: Read Only Memory. ROM doesn't necessarily require sequential access any more than RAM does. The only difference betwee RAM and ROM is that ROM cannot be written-to, but *RAM can be written to at any time, randomly*. There is one time you write to ROM, which is the last time, but reading from ROM can be random locations as well - for example, accessing a specific file from a DVD.

The ""random"" of RAM means abritrary read or write requests at any time, not merely accessing random locations.",1,0,0,False,False,False,1640985340.0
rstlls,hqofezp,t3_rstlls,access to it,-1,1,0,False,False,False,1640956697.0
rstlls,hqosb2r,t3_rstlls,"random here just means ""not in sequence""",1,0,0,False,False,False,1640963318.0
rstlls,hqou33i,t3_rstlls,Imagine a list of memory addresses sent to u when u need one verse calling a specific one,1,0,0,False,False,False,1640964124.0
rstlls,hqoujia,t3_rstlls,"The user is the random part, not the memory itself",1,0,0,False,False,False,1640964327.0
rstlls,hqovtdk,t3_rstlls,"Because the CPU can directly access data at any random memory address on the RAM.

This is unlike, say, tape memory, where you'd have to go sequentially through locations on the tape, as the tape is rolled to put the right location on it under the read head, to access any particular location on the tape.

Compared to this, the CPU just has to put the address of the location on the RAM it wants to access on the address bus and (simultaneously) the read/write signal on the control bus (as well as output data on the data bus, if it's a memory write operation) and that it's. Transaction will be immediately performed on the desired place on the RAM.

The decoders on the RAM card will take the data from the address bus and trigger the appropriate memory location for operation instantly.",1,0,0,False,False,False,1640964892.0
rstlls,hqpb5va,t3_rstlls,It's about the time it takes; it''s constant access time for any random memory location.,1,0,0,False,False,False,1640971271.0
rstlls,hqpzmit,t3_rstlls,RAM stands for Random Access Memory,1,0,0,False,False,False,1640981274.0
rstlls,ht2f7ne,t3_rstlls,"RAM is called ""random access"" because any storage location can be accessed directly.  In addition to hard disk, floppy disk, and CD-ROM storage, another important form of storage is read-only memory (ROM), a more expensive kind of memory that retains data even when the computer is turned off.",1,0,0,False,False,False,1642443372.0
rstlls,hqprnf7,t3_rstlls,"Oh god, the number of people in this thread who don't know the difference between ""random"" and ""arbitrary""...@",-1,0,0,False,False,False,1640977978.0
rstlls,hqprlp8,t1_hqoqcw7,Nice reference to clear the subject,41,0,0,False,False,False,1640977957.0
rstlls,hqr2h44,t1_hqoqcw7,"Yeah, you almost have to think of it as “randomly-accessible memory” rather than “serially-accessible memory” for HDDs or tape drives. Noting that access can either mean read or wrote or both. 

Of course, an SSD nowadays would also be a form of randomly-accessible memory, so the modern usage of RAM to refer to volatile system memory doesn’t really make the same literal distinction as it previously did",16,0,0,False,False,False,1640998727.0
rstlls,hqr8r8b,t1_hqoqcw7,"While this is correct let me add  that (as an architectural detail of DRAM) you can address data stored sequentially faster due to ""burst mode"". This means that different RAM technology, despite the name, may be a little less random when it comes to time / speed / effort.",2,0,0,False,False,False,1641001720.0
rstlls,hqov05x,t1_hqog358,"Exactly, unlike ram which is directly indexed via an address pointer. So you can access memory locations in O(1), unlike the tape like he said, which would be at very minimum O(n) because you have to unspool it until you find your address.",21,0,0,False,False,False,1640964534.0
rstlls,hqox1lh,t1_hqofezp,Darude Sandstorm.,1,0,0,False,False,False,1640965428.0
rstlls,hqsgq5g,t1_hqprnf7,Perhaps you could explain the difference?,3,0,0,False,False,False,1641029396.0
rstlls,hqsgd3v,t1_hqr2h44,"Even previously, a ROM chip was randomly accessible too; functionally ROM was no different to RAM when reading.",3,0,0,False,False,False,1641029075.0
rstlls,hqsh37t,t1_hqr8r8b,"To be correct. This is not a property of all DRAM. DRAM just means its volatile. Burst mode capable ram is called BEDO-RAM, or BEDO-DRAM if it is volatile.",1,0,0,False,False,False,1641029721.0
rstlls,hqszd29,t1_hqsgd3v,"Yeah people assume ROM and RAM are opposites, when actually they’re entirely unrelated other than both referring to memory

You can have ROM RAM, RAM that is not ROM, and ROM that is not RAM - they describe independent properties of the memory modules",3,0,0,False,False,False,1641044891.0
rsjxaz,hqn9xc6,t3_rsjxaz,"New processors don't come out very often, it'd be a very boring graph to watch. The effort is would take to make something that scrapes for the actual info you want, make sure it's correct, and then displays it nicely is probably 100x more work than just updating the graph each time.  
  
It's kinda like asking if there's a realtime tracker of car fuel efficiencies (MPG). There's not really a use to spend money on it and it's not something that would be ""fun"" to make so... it doesn't get built.",9,0,0,False,False,False,1640927222.0
rsjxaz,hqnol3m,t3_rsjxaz,"Not useful today but Intel has committed to breaking Moore’s Law this decade (after all, it’s only an economics law)",7,0,0,False,False,False,1640936347.0
rsjxaz,hqn9zz8,t3_rsjxaz,Not fast enough for real time to be doable or noticeable,2,0,0,False,False,False,1640927262.0
rsjxaz,hqnawdi,t3_rsjxaz,"I’m gonna be honest, at the rate shit is being developed id recommend putting effort into something else that’s similar.

Not saying it’s a bad idea, but it would’ve been helpful a few years ago, as it would be obsolete pretty soon. But if you wanna give yourself a challenge then I’d say go ahead and do it.",1,0,0,False,False,False,1640927756.0
rsjxaz,hqns8dj,t1_hqnol3m,"By beating it, or slacking off?",18,0,0,False,False,True,1640939059.0
rsjxaz,hqp6us5,t1_hqns8dj,"If you go by their last 10 years of production, slacking off",1,0,0,False,False,False,1640969519.0
rs9uqq,hql4ror,t3_rs9uqq,"I studied Computer Vision as an undergraduate, I aided in some novel research using CLIP - a model developed by OpenAI. Our research went on to be awarded NSF grants and will soon be deployed in a production environment for live testing.

There’s never been a better time to get into AI, in my opinion. We are really on the cusp of it becoming generally accessible to people through the magic of State of the Art billion hyper parameter models (think GPT-3). It will only get more and more accessible, and if people can keep finding specific use cases of the general algorithms, or some fine tuned descendant, than we are in for a very exciting time as computer scientists. 

DNNs revolutionized the space and in my mind are one of the strongest arguments against another AI winter. There is an enormous amount of greenfield opportunity for research in this space. (Not that it isn’t usually fascinating work)

If I were you I would decide what kind of problem you want to solve - is it something that requires pattern recognition in structured or unstructured data? If you don’t know what that means go find out. Learn why the distinction matters.

Do you want to do computer vision? Audiovisual? Discrete or streaming data categorizations?

Find a problem and learn what the current algorithm is that provides the best solution. Learn the algorithm, read the papers, lean heavily on any professor or grad student who will help you learn. If you can try to take an elective alongside a professor you respect.

TLDR: it’s a huge field, and your question is sort of like saying you’d like to be a specialized doctor, but you don’t mind what kind. As you explore you’ll seek your interests and be able to do some amazing things.

Don’t be disheartened, it will seem like nonsense and too hard - as CS’s we are highly competitive and feel like it should all come easy. NOTHING WE DO HERE IS EASY. It only ever seems that way in hind sight.

For some, AI is the first part of coding that actually challenges their mind. Let that be an indicator of your growth, not a source of insecurity.",50,0,0,False,False,False,1640893908.0
rs9uqq,hql2fbb,t3_rs9uqq,"Your school doesn't offer a course on AI / ML ?

Also, you may be interested in reading _Artificial Intelligence: A Modern Approach_ by Russell and Norvig as it is considered the standard textbook on the topic.",29,0,0,False,False,False,1640893006.0
rs9uqq,hqnmfnc,t3_rs9uqq,Start by learning basic regression and go from there,7,0,0,False,False,False,1640934816.0
rs9uqq,hqlojr4,t3_rs9uqq,"Most of the actual developments involve the maths of how the model works (When it comes to Deep Learning models anyway - as people have pointed out, AI is quite a vague term).

AI research is a very experimental field that relies on demonstrating empirical results. As with most fields, advancements follow on from previous advancements. Many highly impactful AI papers are just refinements of previous developments. For example, the difference between a ""traditional"" GAN and a Wasserstein GAN (wGAN):

[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661) (Original GAN paper)

[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875) (wGAN paper)

It essentially uses the same architecture, or at least is basically architecture agnostic when it comes to layers, etc. The important thing was the switch from using minimax Loss which is reliant on cross-entropy between probability distributions, to using the Wasserstein distance instead. This practically speaking solved significant problems like mode collapse.

Also, Deep Learning itself can be kind of counter-intuitive, for example:

[https://www.pnas.org/content/117/48/30033](https://www.pnas.org/content/117/48/30033)

If you're asking how people come up with these things in the first place - I can't really give you an answer. All research really starts with an intuition. Knowing a decent amount of maths is probably a good start, at least as much as allows you to be able to research topics on your own and gain an understanding of them. However, you don't have to be a mathematician - colleagues of mine come from all sorts of backgrounds: Biomedical Engineering, Software Engineering, CompSci, Business, etc.

If you're coming from a CompSci/Software Engineering background, it will be quite easy for you to jump in and start running state-of-the-art models. Here's a Colab notebook you can try out:

[https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/\_downloads/torchvision\_finetuning\_instance\_segmentation.ipynb](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb)",6,0,0,False,False,False,1640901702.0
rs9uqq,hqo4kj6,t3_rs9uqq,I am doing my PhD in AI right now and started about three years ago with courses on it from Andrew Ng. Go visit deeplearning.ai and start those courses and interactive sessions 😊,4,0,0,False,False,False,1640948987.0
rs9uqq,hql3fgr,t3_rs9uqq,[deleted],2,0,0,False,False,False,1640893390.0
rs9uqq,hqm7j4c,t3_rs9uqq,"I own a textbook teaching fundamental AI technology. I spent time implementing the technology as a computer application. As part of my study in AI technology, I learned game programming so that I could write AI applications that would display something cool to watch.",1,0,0,False,False,False,1640909436.0
rs9uqq,hqx0wh4,t3_rs9uqq,"It starts as simple as it can get, a simple choice-making model, such as a couple of match boxes with colored beads, where every state in a game of hexapawn is represented by one matchbox. The beads inside the boxes represent the potential choices the model can make.


You start playing, and everytime it is the model's turn, you pick the box that represents the current state of the board. You shake the box and randomly pull a bead out. This bead should have a meaningful choice associated with it, and you perform this such choice.


By the end of the game, if you won, you remove the bead that represented the model's last move, which means it can no longer make that bad move, which necessarily, at least near the end of the game, makes the model slightly better at the game than before.


After several games, you will notice the machine winning more, because the choices it made before, the ones that lost the early games, can't be performed anymore.


At least, that's my understanding. This is attributed to Vsauce2. Soon, you get into understanding the different types of reward models and choice models, which are sort of plug and play. Some have a mathematical definition, such as a fitness function or a neural network that makes the choices instead of having a box for every possible state. There's also a few different ways to iterate the model, such as directly manipulating the model, genetically evolving the model, or just randomly changing everything about the model over and over until something happens to work.

Granted, I am not super experienced in the field, as I haven't personally been able to experiment with machine learning, but I've been looking into all sorts of computer science subfields.",1,0,0,False,False,False,1641115092.0
rs9uqq,hr58pfb,t3_rs9uqq,"There's a couple of Ai Libraries (Tensorflow, Pytorch, Scikit learn). Just start by reading the guides on one of them (I recomment Scikit learn as it's targetted towards beginners), and Google anything you don't know.

There's no one path to learn ai, I've found that it's primarily just looking around and seeing what you find.",1,0,0,False,False,False,1641255459.0
rs9uqq,hqlcb8z,t3_rs9uqq,"I got a PhD in Econ and did a dissertation in computational agent based economics.

""AI"" is such a broad term and used inconsistently. A\* pathfinding used to be considered AI.

Some are talking about logistic regression to build decision trees.

Some are talking about genetic algorithms.

I use behavioral trees in my hobby game project.

Here is the general rule of thumb: don't focus on methods. Methods are for people on the left side of the dunning krueger curve. You limiter is always your data. If you are making a game you probably don't have it set up to collect game data and adjust AI behavior on that so you just go with creative methods of building AI like a behavioral tree that you build by play testing.

AI is overused and oversold. Most of what the explosion in the recent years is just logistic regression and some algorithms that adjust the models in real time.

A lot of people into programming turn tail and run when people start talking about ROC curves and Gini statistics.

If you are doing actual data analysis you have to be careful about just tossing data in some off the shelf python library. Depending on what type of data it is, its very easy to make junk conclusions. In economics the big mistake everyone makes is including GDP in models when it should be GDP growth rates.",-2,0,0,False,False,False,1640896893.0
rs9uqq,hql5pq8,t1_hql4ror,"Well said and certainly eloquent. I totally agree that the time is NOW, hence why I’ve been finding these videos and highlights on AI so interesting. I’m going to follow your ideologies here a bit and see where it takes me. 

I’ve always been in love with finding new solutions. And AI research is on the frontier of finding these new solutions. I’m excited to see where it takes me, but I am certainly feeling insecure just due to my lack of knowledge. But like you said, I’ll change my mindset to make that a ladder for my own growth instead of quick sand. thank you 🤝",11,0,0,False,False,True,1640894278.0
rs9uqq,hql2p5c,t1_hql2fbb,"Good point. It’s a small liberal arts college and it does actually offer one ML course within the CS major. Although due to the timings of how classes work there I wasn’t able to ever take it. 

On the other hand the book is a fantastic suggestion. Looking into it now, thank you",9,0,0,False,False,True,1640893110.0
rs9uqq,hqlrp8l,t1_hql2fbb,Yeah that book is really good.,5,0,0,False,False,False,1640902973.0
rs9uqq,hqlv5fg,t1_hqlojr4,"Hey thanks for this, it’s a more complex response so I have to read through it but I appreciate the response ! Helps me get some direction",2,0,0,False,False,True,1640904353.0
rs9uqq,hql3ugg,t1_hql3fgr,That’s helpful. I was missing the part about grad school. Since i’m only going to graduate with a BA in Comp Sci and I didn’t think about grad school it makes sense that’s where you would learn how to write scientific docs. Now I have to decide if that’s what I want to do in the future… I’m certainly no genius though 😂,2,0,0,False,False,True,1640893550.0
rs9uqq,hqnei4b,t1_hql2p5c,"+1 for Russel and Norvig, I took a class that used it and it's really quite good.",3,0,0,False,False,False,1640929793.0
rs9uqq,hql54ie,t1_hql3ugg,[deleted],2,0,0,False,False,False,1640894047.0
rs9uqq,hql8szm,t1_hql54ie,"No worries, this gives me enough I go to look into it on my own. Cant thank you enough 🤝",1,0,0,False,False,True,1640895506.0
rslk6w,hqowuu4,t3_rslk6w,"What's your background? Do you already have a strong background with AI (ML, stats, etc)?",1,0,0,False,False,False,1640965349.0
rslk6w,hqp0g0o,t1_hqowuu4,I'd say I have a very theoretical grasp on ML and a solid understanding of statistics.,1,0,0,False,False,True,1640966877.0
rslk6w,hqp2r9f,t1_hqp0g0o,"I'm assuming you meant 'a strong grasp on the theory', and not 'theoretically I know it exists'! :)

I seem to remember Deepmind publishing work where they advocated for reinforcement learning with reward being all that's necessary for that leap to be made, or something along those lines.

If my memory is right, that would probably be a great place to start for a literature review because Deepmind garners a lot of attention so there's likely to be a reasonable amount of published content referencing it.",1,0,0,False,False,False,1640967845.0
rs0z7i,hqjjdr4,t3_rs0z7i,"Building a binary tree where the nodes are the agents should work.

After building the nodes (lets assume numner of agent n is in a form of 2**k for some k) you will go in post-order traversal on the tree where the value of each would be the winner between both of his childs.

If we assume each game takes O(m) operations then finding the root would take O(nm) operations.

A very important note that this is an approximation for the best, as usually there is not a single best (agent a beats b, b beats c and c beats a for example) so if your simulation is fast i would try to simulate a bunch of the trees with each agent recive a mark based on how far he ends up from the root and see how much the data flucuate with more simulations.",1,0,0,False,False,False,1640870058.0
rs0z7i,hql8ycf,t3_rs0z7i,Is this the same problem ELO solves?,1,0,0,False,False,False,1640895564.0
rs0z7i,hqm1fy7,t3_rs0z7i,"Both answers were perfect. I have been interested in different types or agent modelling, so that was helpful.",1,0,0,False,False,False,1640906933.0
rs0z7i,hqx1396,t3_rs0z7i,"To me, that type of model, while it sounds slow, seems like it could find the best player in a batch much more accurately than giving them all a single-number score.",1,0,0,False,False,False,1641115246.0
rs0z7i,hqjzmx8,t3_rs0z7i,"Hi, this is really interesting. How does one implement a game so that agents can play chess? Like a specific language? I know this is a noob question.",-2,0,0,False,False,False,1640877976.0
rs0z7i,hqntva4,t1_hqjjdr4,"I think that might work, but after each round, the top X% are selected to breed the next generation while also participating in the next round. So I already know how strong they were in the previous generation, so I was thinking maybe I can cut the number of games in the following generations because I know how strong some of the agents are. Maybe you can help me find a way?",1,0,0,False,False,True,1640940324.0
rs0z7i,hqlezx6,t1_hql8ycf,"Kind of, but I want the agents to play as few games as possible, and elo ratings take a lot of games to be accurate.",1,0,0,False,False,True,1640897945.0
rs0z7i,hqleo42,t1_hqjzmx8,"What is the question? How to implement chess or how to implement the agents?

For the agents, I used a monte carlo search tree where the final positions are evaluated with a neural network

For implementing the game, I used the python chess library, if you want to implement it yourself there is a lot of material on the topic",1,0,0,False,False,True,1640897813.0
rs0z7i,hqny403,t1_hqntva4,"Hmm that's not trivial and I dont think theres an optimal solution for you.

On top of my head theres three options, all has their drawbacks and advantages over the other methods. 

One option would be saying that the ranking of last generation is somewhat noisy, in this case you could just treat your old agents just like new ones with the difference of the ranking would be:

r_new(iteration i) = \gamma r_new(iteration i - 1) + (1-\gamma) r_old(iteration i)

With gamma being a parameter of your choice between 0 and 1. 

This method would not decrease the number of evaluations but it would lower your ranking variance which would help your algorithm to converge.

The second method would be building your match tree in such way new agents would always be against old agents.

You can do that by rebuilding your match tree after every round and enforcing new agents would always be against old ones. Eventually you would need to pair old ones against old ones and you would simply take the better agents from last iteration without simulating a match.

This would yield low amount of simulations as you wont simulate old agents against each other, and it would work in the case where all new agents are better then the old ones (after a round the entire match tree would be only new agents), but the drawback being you might lose out new good agent quickly when he would face the best old agent in the first round - not something you want.

The last method comes to mind is somewhat hard to explain. its similar to the second method but instead of using all of the old agents in the first round, the first round you would use the worst half of your agents and all of the new agents. In the second round you would insert the worst half of the remaining agents and so on.

This would work similarly in terms of amount of matches to the 2nd option - this is better for cases where the new generation is not that different then the old one.

That's a fun exercise and those were just methods from the top of my head - there could be better options then i stated.",1,0,0,False,False,False,1640943740.0
rr75ns,hqey5kd,t3_rr75ns,"The phenomenon described depends on levels of serotonin and octopamine secretion where serotonin levels are high in dominant crabs and octopamine levels are high in submissive crabs. So, the submissive crabs tend to make sure the coast is clear before laying claim to anything.
If by chance no dominant crab shows up during the eight hours of waiting and shows up later after, then the submissive crab either gives up the loot or challenges the obviously dominant crab which could be brutal for either or both of them with the winner and the loser getting higher levels of serotonin and octopamine respectively.

Totally unrelated but👍",35,0,0,False,False,False,1640789506.0
rr75ns,hqexc1l,t3_rr75ns,"You can see it in action here:
https://youtu.be/f1dnocPQXDQ",29,0,1,False,False,False,1640789131.0
rr75ns,hqejw8s,t3_rr75ns,I dont think waiting for up to 8 hours would be a good starting point :),35,0,0,False,False,False,1640781898.0
rr75ns,hqew0s2,t3_rr75ns,"I think we already have a lower asymptotic bound on the time complexity of sorting algorithms at O(n logn):

If we only permit comparisons and swaps between two elements, then every sorting algorithm can be represented by a binary tree where each node is a comparison, with only two possible outcomes (assuming there is a total order on all elements).

Any correct sorting algorithm must have a different set of swaps for each permutation of an n-length input, so there must be n! leaves, so the height is lg(n!), Which is Omega(n logn)

Of courses, there is still possible research around real-world applications",23,0,0,False,False,False,1640788512.0
rr75ns,hqf0e7a,t3_rr75ns,"The Turing Omnibus mentions a linear time sorting algorithm called ""spaghetti sort."" Grab spaghetti with different lengths, put it in your hand, and tamp down gently on the bottom until they're all even. Repeatedly pick out the longest stand by putting your hand on top and picking the first one it hits. Since both these hands operations are constant time, you get a linear sorting algorithm!!

Let's do a quick analysis to see where this algorithm, and any ""natural"" algorithms in real life, fall short:

- hands are slow. Where a computer might pick something from memory in a few microseconds, it takes dozens of milliseconds to pick out spaghetti from your hand. That's a hard time limit, since physics prevents the spaghetti from being moved too quickly lest it break.
- there are practical bounds on the length of spaghetti.
- it's hard to hold more than, say, 100 spaghetti strands. Practical sorting will require millions of items to sort.
- in order to make this transferable to and from a computer, the computer has to output spaghetti of certain lengths, or cut spaghetti to a certain length. Beyond the precision required, it's just slow. It might be faster reading the spaghetti, although that may require a computer vision algorithm or some sort of spaghetti index.

So, even though it is a linear time sorting algorithm, practical problems abound around the interface and scaling. I think you'll find those problems anywhere you attempt to use a natural algorithm.

To wit: even if you digitized this algorithm, you'd just be doing insertion sort: the hand moving down from above would be replaced with a physics engine determining interactions between all n spaghetti strands, which is n operations n times for a n^2 runtime.",6,0,0,False,False,False,1640790513.0
rr75ns,hqf2h69,t3_rr75ns,The book Algorithms to Live By talks a lot about interesting cases like this!,5,0,0,False,False,False,1640791431.0
rr75ns,hqgm0w9,t3_rr75ns,You could easily crop this image without the giant text claiming people that provide a service to people to rent a place to live as being awful people.,-4,1,0,False,False,False,1640813124.0
rr75ns,hqhkbzk,t3_rr75ns,!RemindMe 24h,1,0,0,False,False,False,1640827321.0
rr75ns,hr01kky,t1_hqey5kd,appreciate the fun fact!,1,0,0,False,False,True,1641166521.0
rr75ns,hqesbxq,t1_hqejw8s,Maybe they're using [SleepSort](https://www.quora.com/What-is-sleep-sort)?,19,0,0,False,False,False,1640786707.0
rr75ns,hqezvfd,t1_hqejw8s,It's good enough for crypto people,5,0,0,False,False,False,1640790283.0
rr75ns,hqgnm2i,t1_hqew0s2,">I think we already have a lower asymptotic bound on the time complexity of sorting algorithms at O(n logn):

Just want to clarify that this is only for comparative sorting. There are faster sorts for some specific cases, such as a Radix sort.",4,0,0,False,False,False,1640813733.0
rr75ns,hqf2s0c,t1_hqew0s2,How does the time change when it is the elements of the list comparing themselves? This allows for every element to be compared to one other simultaneously. Wouldn't crabsort then be O(n)?,1,0,0,False,False,False,1640791562.0
rr75ns,hqgcslz,t1_hqf0e7a,What you described is essentially just an oracle that returns the maximum of a list in constant time.,6,0,0,False,False,False,1640809574.0
rr75ns,hqp04x2,t1_hqf2h69,thank you!,1,0,0,False,False,True,1640966747.0
rr75ns,hqhkfw6,t1_hqhkbzk,"I will be messaging you in 1 day on [**2021-12-31 01:22:01 UTC**](http://www.wolframalpha.com/input/?i=2021-12-31%2001:22:01%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/rr75ns/it_would_be_really_interesting_to_research/hqhkbzk/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Frr75ns%2Fit_would_be_really_interesting_to_research%2Fhqhkbzk%2F%5D%0A%0ARemindMe%21%202021-12-31%2001%3A22%3A01%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20rr75ns)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1,0,0,False,False,False,1640827369.0
rr75ns,hqgprgk,t1_hqf2s0c,"I prefer using Computational complexity over Time complexity as it represents the relationship between the number of data elements and the amount of operations that need to be executed, and the growth of the relationship. 

It's not exactly time, so much as how many steps do I have to complete (which indirectly is time). Even if you're doing comparisons for each individual in parallel as it relates to others, you would still have n individuals doing n observations, for o(n^2) computational complexity.",2,0,0,False,False,False,1640814569.0
rr75ns,hqf5rch,t1_hqf2s0c,"The answer depends on your computational model. Assuming you work on a single tape touring machine, your sorting algorithm would be in O(n²) - you sort n elements whereas each element has to compare itself with up to all elements already in the list. On a PRAM it is in O(n²) as well since you multiply the number of cores by the number of instructions each core has to compute.",1,0,0,False,False,False,1640792843.0
rr75ns,hqhnkvh,t1_hqgcslz,"I think the selection step actually should take linear time in the max height, since if you're actually applying a uniform procedure, or if you built a machine to enact this process, it must start at least at the max height and potentially sweep all the way down through the range of heights. Consider the case of one 6"" spaghett and ten million spaghetts of negligible height; the average distance each sweep is almost 6"" and distance is time.",2,0,0,False,False,False,1640828756.0
rr75ns,hqhoa7d,t1_hqhnkvh,"Not sure I completely get what you’re saying. I’m personally imagining some spaghetti standing upright in a bunch on a table. There is then a flat surface from above weighing down. The flat surface repeatedly selects the point of contact with the set of spaghetti, which is just a maximum element, and removes it from the clump (note the surface is already touching the max so it doesn’t need to perform an entire sweep to extract it). It only makes one sweep in total, so it takes O(n + h) comparison and extraction operations where n is the number of noodles and h is the maximum height of a noodle. This is better than radix which uses O(kn) operations where k is the number of digits.",2,0,0,False,False,False,1640829060.0
rr75ns,hqhoy2g,t1_hqhoa7d,"If the tallest one is in the middle of the bunch, you have to lift the plate back up to pick it out.",1,0,0,False,False,False,1640829347.0
rr75ns,hqhp2zi,t1_hqhoy2g,Why? I think our visualizations is different. Did you read my description?,1,0,0,False,False,False,1640829408.0
rr75ns,hqhppp8,t1_hqhp2zi,Yes. How does a flat surface remove the spaghett it hits? Going back to a person with two hands- one hand holds the bunch together vertically. The other hand sweeps down to hit the tallest and then picks it out. That means potentially sliding it vertically out of the middle.,1,0,0,False,False,False,1640829697.0
rr75ns,hqhqjov,t1_hqhppp8,"Left hand is on the bottom. Spaghettis align flat on the left hand. Right hand is on top pressing down. It is constantly in contact w the max. It extracts that noodle by repeatedly letting the one in contact fall. No sweeping needed to remove the max, as the right hand is just constantly pressing down from gravity.",1,0,0,False,False,False,1640830079.0
rr75ns,hqhr2nx,t1_hqhqjov,"How does the noodle in contact fall through the left hand? To ensure heights remain comparable they have to be flush at bottom, against the hand or preferably the table.
And in any case, once we grant they must slide out that provides a new worst case: they are all MAXHEIGHT to within a negligible distance. Then to get free each one must slide ~MAXHEIGHT inches and we are linear in max height again.",0,0,0,False,False,False,1640830310.0
rr75ns,hqhrexn,t1_hqhr2nx,"The left hand can be a grid surface that can open up the grid point corresponding to the grid point that the right hand is contact with. Also yes, the linear in h is unavoidable (its also not new, thats what the h is in my previous comment), but it is O(h + n) rather than O(hn) as you describe, which is much worse. Note it is O(n+h) instead of O(nh) because the noodles are sliding concurrently, the reason it is so effective is because it can take advantage of this huge parallelism. A lot of “biological algorithms” operate on this principle: go read on Adelman’s work on solving a non-trivial TSP instance through biological algorithms. O(n + h) is better than radix sort while O(nh) is not, and that is important as radix sort is sort of the “standard” for non-comparison sorts. 

Also you are thinking too much into the hand analogy, they can easily be held in place by some easily implementable mechanical process, or just don’t use noodles lol. The point is that there are physical sorting algorithms that use a number of operations linear in every parameter, e.g. O(n + h) instead of O(nh), which are enabled due to parallelism scaling with the size of the problem, which is clearly not a property of problems solved in computers. That is what the not-really-active field of biological algorithms tries to take advantage of.",1,0,0,False,False,False,1640830458.0
rrvks1,hqkdhle,t3_rrvks1,"There is a way to solve them programmatically, and yes, it is a kind-of brute force, but a common technique is to not write the 'brute force' piece yourself, but to rely on a tool (sometimes a library in a programming language, sometimes a first-flass language in it's own right) called a ""SAT Solver""

[https://cse.buffalo.edu/\~erdem/cse331/support/sat-solver/index.html](https://cse.buffalo.edu/~erdem/cse331/support/sat-solver/index.html)

Here's a blog entry closer to the problem type your are trying to solve:

[https://sabhijit.medium.com/logic-puzzles-and-sat-solvers-a-match-made-in-heaven-5e0a7a64c04b](https://sabhijit.medium.com/logic-puzzles-and-sat-solvers-a-match-made-in-heaven-5e0a7a64c04b)

Here's a paper specifically about the ""logic grid"" type of puzzles you see here

[https://freuder.files.wordpress.com/2019/09/challenge-ucc-submission.pdf](https://freuder.files.wordpress.com/2019/09/challenge-ucc-submission.pdf)

Languages like prolog and icon are specifically geared towards solving this kind of problem in a declarative style.",5,0,0,False,False,False,1640883542.0
rrvks1,hqiqd91,t3_rrvks1,"Link for better formatting: [https://stackoverflow.com/questions/70527987/solving-logic-puzzles-using-r](https://stackoverflow.com/questions/70527987/solving-logic-puzzles-using-r) 

Thanks!",1,0,0,False,False,True,1640848651.0
rrvks1,hql0m9c,t3_rrvks1,Take a look at Logical Programming and SAT,1,0,0,False,False,False,1640892312.0
rrvks1,hqlvd13,t3_rrvks1,Consider implementing a DPLL solver. You can use a variety of heuristics to perform substantially better than random in practice.,1,0,0,False,False,False,1640904438.0
rrvks1,hqx17cm,t3_rrvks1,"This is a job for quantum computers!
I want a quantum computer so bad...",1,0,0,False,False,False,1641115341.0
rrsvuo,hqihlvj,t3_rrsvuo,"Applications have states of process, like waiting, running, and so on.  You double click gmail to check your messages, while gmail is loading you check your Facebook page, but it's dead, so next you access Instagram.  Now you go back to check your gmail, this is known as Context Switch.  First you opened gmail, the scheduler will grab gmail app and start loading it, but you wanted to see who is on Facebook, the gmail app will go to a wait state and placed in the queue, then Facebook will go active, then you checked Instagram, so Facebook goes to the wait state and placed in queue and Instagram goes active. Then you go back and check your emails.  Now Instagram goes to the wait state while gmail goes to the active state.  
  No matter how it's worded, the CPU can only execute one app at a time.  It can load a program in each core while executing the main program, but it can only run one app at a time, but it will switch to another app in queue if it sees the current app go to a wait state, like waiting for user input.  It can switch between thousands of apps in seconds. 

So you have a scheduler that handles the removal of the running process from the CPU and the selection of another process on the basis of a particular strategy.

The OS scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system;

Schedulers main task is to select the jobs to be submitted into the system and to decide which process to run.


CPU scheduler main objective is to increase system performance, also selects a process among the processes that are ready to execute and allocates CPU to one of them.

Short-term schedulers, also known as dispatchers, make the decision of which process to execute next. Short-term schedulers are faster than long-term schedulers.


Medium-term scheduling  removes the processes from the memory. The medium-term scheduler is in-charge of handling the swapped out-processes
A suspended processes cannot make any progress towards completion. In this condition, to remove the process from memory and make space for other processes, the suspended process is moved to the secondary storage. This process is called swapping, and the process is said to be swapped out or rolled out. Swapping may be necessary to improve the process mix.


A context switch is the mechanism to store and restore the state or context of a CPU in Process Control block so that a process execution can be resumed from the same point at a later time.

When the scheduler switches the CPU from executing one process to execute another, the state from the current running process is stored into the process control block. After this, the state for the process to run next is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing.

This is the basics of a CPU running applications.  There is another task in there I cannot recall at this moment.",24,0,0,False,False,False,1640843091.0
rrsvuo,hqjoe0s,t3_rrsvuo,"The answer of u/DevilDawg93 is about the OS scheduling, but misses the part about the interaction with I/O devices. To answer your question: there are multiple levels of security in which a CPU can be, mainly the **privileged mode** in which the software can do anything, and the **user mode**, in which the CPU forbids access to the areas of the memory not explicitly allowed, access to I/O devices, use of some instructions, etc.

How it works: when the OS starts booting, it executes in privileged mode. It can organise the memory of your computer whenever it wants, and use some areas in the memory (the RAM) to write some important tables, in which you will find the pages tables (which indicates the regions of the memory on which a process can read and/or write). The OS sets some important registers of the CPU: one is pointing to the **interruption** handler, a function in the OS that can manage interruptions, another to a page table.

When the OS switches to a process, it writes in the special register the pointer to the pages table of the process, switch to user mode, and then jump to the code of the process, letting this process do its stuff. Because the process is in user mode, it can't do anything dangerous (requiring the privileged mode)... but can ask the OS to do it. It can trigger an interrupt (a system call), which will call a function of the OS. The CPU switch to **privileged mode** when it happens, and this is safe because only the OS could have written in the register referencing the interrupt handler. So there is no way (outside a security breach) for the user process to write in another program memory or use the I/O directly, the restriction is built in the CPU itself. The application **must** ask the OS to do it itself, and then the operating system takes the time to check if the application is allowed to do so, or not.

Of course this is a bit simplified view of how it works, modern CPU are very complex, but it still works according to this schema. I should add that switching from one process to another (as described by u/DevilDawg93) works the same: in privileged mode, the OS can configure the hardware clock to send a signal (for example every X µs). Each time the signal is sent, an interrupt is triggered, and so the CPU starts executing OS code in privileged mode. It is up to the OS to save the process state somewhere, set the CPU registers (for the pages table of the next application), load this next process state, switch to user mode and start executing it \[this is the context switch\].",5,0,0,False,False,False,1640872773.0
rrsvuo,hqkihf4,t3_rrsvuo,"Thanks a lot guys, it’s very clear to me now !",2,0,0,False,False,True,1640885423.0
rrsvuo,hqij16y,t1_hqihlvj,I applaud you taking the time to write all of this out.,3,0,0,False,False,False,1640843906.0
rrsvuo,hql2nr0,t1_hqjoe0s,"Well stated u/webalorn! 

Happy Cake Day!",1,0,0,False,False,False,1640893096.0
rq60d4,hq8jcrb,t3_rq60d4,Rule 6. Format your code and text into maintainable and easily readable form. Apply to Reddit posts.,149,0,0,False,False,False,1640664315.0
rq60d4,hq9ejqb,t3_rq60d4,Rob Pike on his 6 Rules of Programming - [https://twitter.com/rob\_pike/status/998681790037442561](https://twitter.com/rob_pike/status/998681790037442561),5,0,0,False,False,False,1640685077.0
rq60d4,hq8mxyf,t3_rq60d4,"Rule 5 restates Torvald's, ['good programmers worry about data structures](https://softwareengineering.stackexchange.com/questions/163185/torvalds-quote-about-good-programmer)'.",11,0,0,False,False,False,1640666071.0
rq60d4,hqa4g2h,t3_rq60d4,"Ironically, this is what systems programmers are always complaining that frontend developers are doing: using inefficient but simple algorithms until necessary to change.",2,0,0,False,False,False,1640702180.0
rq60d4,hq9omtz,t3_rq60d4,Don’t agree with #5: fixation on data structure can lead to lack of flexibility. Programmers should use data abstraction instead. Also not data but functional specification should drive program design,1,0,0,False,False,False,1640693048.0
rq60d4,hqawyhl,t3_rq60d4,"Also ""good enough is often better than perfect""",1,0,0,False,False,False,1640714040.0
rq60d4,hqb5jzu,t3_rq60d4,"Reminds me The zen of Python.

Beautiful is better than ugly.

Explicit is better than implicit.

Simple is better than complex.

Complex is better than complicated.

Flat is better than nested.

Sparse is better than dense.

Readability counts.

Special cases aren't special enough to break the rules.

Although practicality beats purity.

Errors should never pass silently.

Unless explicitly silenced.

In the face of ambiguity, refuse the temptation to guess.

There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.

Now is better than never.
Although never is often better than *right* now.

If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.

Namespaces are one honking great idea -- let's do more of those!",1,0,0,False,False,False,1640717439.0
rq60d4,hqb8b06,t3_rq60d4,How exactly am I supposed to write smart objects using only stupid code?,1,0,0,False,False,False,1640718522.0
rq60d4,hqnotxz,t3_rq60d4,"rule 6 

unexecuted code should not cause you to write more executed code. 

(class hierarchy/data structure and types should be fitted to your code and not the other way around)",1,0,0,False,False,False,1640936528.0
rq60d4,hqal7s1,t1_hq8jcrb,"Disagree. If it was hard to write, it should be hard to read.",25,0,0,False,False,False,1640709390.0
rq60d4,hq8mfwp,t1_hq8jcrb,[deleted],7,0,0,False,False,False,1640665815.0
rq60d4,hq9e7mf,t1_hq8mxyf,"Rob Pike wrote this first in 19**8**7. Linus would have been 18 at the time and didn't start work on Linux until 1991.

https://twitter.com/rob_pike/status/998681791417409536

https://en.wikipedia.org/wiki/History_of_Linux#The_creation_of_Linux",7,0,0,False,False,False,1640684796.0
rq60d4,hqa4juy,t1_hq8mxyf,[deleted],1,0,0,False,False,False,1640702229.0
rq60d4,hqbgoe9,t1_hqa4g2h,Really? The most common complaint I hear is the opposite - that frontend development is usually ridiculously overengineered for example like introducing a mountain of 3rd party libraries to make trivial systems.,2,0,0,False,False,False,1640721869.0
rq60d4,hqbjej0,t1_hq9omtz,"Data driven design tends to be cleaner, simpler and easier to reason about, because you can avoid complicated data conversion in your data abstractions. This also saves you from potential correctness and performance issues in those data abstractions.

Flexibility isn't really an issue, because the code is much more straightforward when it has a conservative amount of abstractions. Therefore it's usually not as bad as you might think to change the data structure to something else. On the other hand heavily abstracted code tends to be so complicated that it's easier to just put yet another abstraction on top of it all, compounding the existing issues.",2,0,0,False,False,False,1640722954.0
rq60d4,hq9rlsf,t1_hq9omtz,"I suspect that was written within the context of ""thinking about performance"". Don't choose a particular data structure just because you want to use a certain algorithm. In general I agree with you, but the data ultimately does have some structure on a disk/in memory which matters.",1,0,0,False,False,False,1640695091.0
rq60d4,hqbzw98,t1_hqawyhl,"Good enough is never better than perfect. I get these sorts of rules are supposed to be pithy, and I get what they're trying to hint at, but they're always phrased in such asinine ways that it's hard to take them seriously.",0,0,0,False,False,False,1640729730.0
rq60d4,hqc2f90,t1_hqb8b06,"When you have picked appropriate data structures, then you code usually does not need to be particularly sophisticated.",1,0,0,False,False,False,1640730784.0
rq60d4,hqaljhv,t1_hqal7s1,Lol,8,0,0,False,False,False,1640709522.0
rq60d4,hqbbc1r,t1_hqal7s1,"If I had to suffer, others have to suffer too? Is this what you mean?",5,0,0,False,False,False,1640719727.0
rq60d4,hq8qyia,t1_hq8mfwp,"The real Rule #7: There are plenty of rules to programming for a real purpose, get creative on your own time.",6,0,0,False,False,False,1640668201.0
rq60d4,hq9e8mx,t1_hq9e7mf,"**History of Linux** 
 
 [The creation of Linux](https://en.wikipedia.org/wiki/History_of_Linux#The_creation_of_Linux) 
 
 >In 1991, while studying computer science at University of Helsinki, Linus Torvalds began a project that later became the Linux kernel. He wrote the program specifically for the hardware he was using and independent of an operating system because he wanted to use the functions of his new PC with an 80386 processor. Development was done on MINIX using the GNU C Compiler. As Torvalds wrote in his book Just for Fun, he eventually ended up writing an operating system kernel.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",3,0,0,False,False,False,1640684821.0
rq60d4,hqbz6x8,t1_hqa4juy,Excellent. You get all the disadvantages of excess complexity combined with all the disadvantages of unpredictable performance and inability to scale.,1,0,0,False,False,False,1640729434.0
rq60d4,hqbne1p,t1_hqbgoe9,"The problem is that people don’t have a consistent definition of that word. Isn’t overoptimizing a form of overengineering? So then “not optimizing” by deferring processing to third party libs couldn’t possibly be overengineering. Except that it is, somehow.

Besides, while dependency explosion is a real problem, that’s mostly a fault of the ecosystem and not the developer. Typically all you need is about 5 direct dependencies before the number of node modules is in the thousands. This isn’t the fault of the engineer and has very little to do with their development practices.",3,0,0,False,False,False,1640724590.0
rq60d4,hqbvqdu,t1_hqbjej0,"«Data driven design ...» — does data determine functionality of program or vice versa functionality determines the data necessary for its implementation? What does user need? Functionality or internal program’s data structures?

«Flexibility isn't really an issue, ... when it has a conservative amount of abstractions» — correlates with my statement about using data abstraction",0,0,0,False,False,False,1640728016.0
rq60d4,hq9t80e,t1_hq9rlsf,I agree with you too) but according to rule #2 we must to measure existing data before optimizing it and according to rule #4 we should prefer the simplest alg. and the simplest data for the first impl.) and abstraction is the easiest way of such simplification to my opinion,0,0,0,False,False,False,1640696134.0
rq60d4,hqbdqq3,t1_hqbbc1r,"Yes, but they have to suffer in a different way.",3,0,0,False,False,False,1640720687.0
rq60d4,hq8sp4l,t1_hq8qyia,[deleted],1,0,0,False,False,False,1640669185.0
rq60d4,hqbzlyr,t1_hqbne1p,">  deferring processing to third party libs couldn’t possibly be overengineering. Except that it is, somehow.

That's a false dichotomy.

You're assuming the only two choices are implementing something complex yourself or pulling in a complex library. There's also the option of doing something simple, or, even better, leaving out that element entirely.",3,0,0,False,False,False,1640729610.0
rq60d4,hqbrazk,t1_hqbne1p,"Which word? Optimization? Neither OP nor your first reply nor my reply mentioned that word even once, so I'm not entirely sure why you introduce this concept.

Overengineering isn't at all the same. Optimization is figuring out how to do the least amount of work to accomplish a task. Overengineering is introduction of an unacceptable amount of accidental complexity. Optimization can be overengineering, if the optimization isn't necessary, but these concepts have nothing in common besides this.

I agree though that there is a lot of confusion and misunderstanding because technical words are misunderstood, misused and abused all the time. There are a lot of blinds leading the blinds in this industry.

I partially agree about your comment on dependency explosion. It's partly an ecosystem issue, but also partly a cultural issue. I will not fault the engineer of the dependency explosion as a whole, but I refuse to absolve the engineer of the responsibility of their choice of libraries for their project.",2,0,0,False,False,False,1640726200.0
rq60d4,hqbytmu,t1_hqbvqdu,"In data driven design data structure (i.e. data layout in memory) determines how you process the data. This has not anything to do with user needs or application features per se - it has to do with how one goes about implementing these features.

No, it does not correlate - it contradicts. You claim in response to rule #5 that the programmer should use data abstractions instead of fixating on data structures. When you as a rule abstract the data and disregard its actual structure, then it will unavoidably lead to unnecessary data abstractions. When you have more data abstractions than necessary, you do not have a conservative amount of data abstractions.",2,0,0,False,False,False,1640729281.0
rq60d4,hqbnbfb,t1_hqbdqq3,Sounds petty tbh,2,0,0,False,False,False,1640724560.0
rq60d4,hq8t8rv,t1_hq8sp4l,"Do you believe that problem solving requires you to have literally no limits? There are rules. There are areas where there are many options within the rules, but there are rules.",2,0,0,False,False,False,1640669501.0
rq60d4,hqc08ol,t1_hqbzlyr,"I know it sounds like that’s what I am saying. But my real point is that outsourcing processing of a thing to avoid overengineering, should not itself be overengineering, yet it is. In other words, the opposite of an action should not be another flavor of the action itself.

Just because your bundle size is large doesn’t mean you overengineered something. It may have been way simpler to use a library than to build it yourself. It’s kind of like saying using a car to get to the other side of the state is overly engineered because you could’ve walked it, or you could have used a bicycle.",3,0,0,False,False,False,1640729876.0
rq60d4,hqc4vim,t1_hqbrazk,"I think I made a mistake by going down this road of talking about bundle size. It's really different from the main complaint I have heard, which is that the algorithms are inefficient and as a result, web apps and electron apps etc. use way more cycles/battery power than they need to. My retort to that is that of course, that's the only reason you're using the program -- because it could be built faster than we used to build software. And it's done faster now because we have the power of computationally inefficient tools like ES6 spreads and dynamic hashtables forming immutable datastructures, which make programming itself more efficient of a (human) process than something like dirty flags and mutable state/pubsub insanity.

I disagree on definitions for overengineering, optimization, engineering. IMO optimization and engineering are basically the same thing; you have a system of constraints, as well as a system of preferences, and are trying to produce a solution that solves all of the constraints while honoring the preferences as much as possible. You can engineer for certain things (our company has no money, it needs to be entirely libre software) or others (our company has infinite money, spend as much as possible to get it done quickly), just as you can optimize for either memory or speed. Over-engineering is a strange word because in e.g. bridge design it basically means ""to make something stronger."" To make something stronger in computer engineering is to make it simpler and easier to manipulate, which I feel is really not what people mean by ""overengineering.""

Typically when people complain that an Electron app is overengineered, they don't mean it in the traditional sense of the word, which would be to say it has too much functionality, too much capability. They are cajoling it to mean ""they spent too much engineer time"" on this, I guess, which to me is paradoxical since the whole point of bringing in React, Angular, etc. is to save time, and it often does.",2,0,0,False,False,False,1640731816.0
rq60d4,hqpk98s,t1_hqbytmu,"«In data driven design data structure (i.e. data layout in memory) determines how you process the data»
— I didn’t ask what “data driven” means, I ask how DDD can prevail since programs are usually written to satisfy some user needs, not to serve any data structure itself — and primary need of user is functionality but not pure data.

«No, it does not correlate - it contradicts» — no, it doesn’t contradicts because you said it yourself that data abstraction solves the issue of potential loss of flexibility.

«When you as a rule abstract the data and disregard its actual structure, then it will unavoidably lead to unnecessary data abstractions ... more data abstractions than necessary» — you cannot have more data abstractions then necessary if you design consequentially — because abstraction means eliminating inessential details for given step of design, and new abstractions appear only when you begin concretizing existing ones and realize that they require some new entities — which automatically makes those new entities necessary 🤷‍♂️",1,0,0,False,False,False,1640974946.0
rq60d4,hqbpppw,t1_hqbnbfb,"It's not; if it's hard to read, then hopefully it scares the interns off.",4,0,0,False,False,False,1640725543.0
rq60d4,hq8zy09,t1_hq8t8rv,[deleted],1,0,0,False,False,False,1640673693.0
rq60d4,hqc1t7r,t1_hqc08ol,"If you pay really close attention you'll notice, that nowhere did I argue against using 3rd party libraries.

I used dependency of *too many libraries* as *an example* of overengineering. This has nothing to do with the notion of moving the processing in-house somehow making it not overengineering.",2,0,0,False,False,False,1640730527.0
rq60d4,hqc94ix,t1_hqc4vim,"It's alright. :-)

I kind of agree with your definitions. Sort of.

Optimization is a loan word from the field of mathematics, which - loosely speaking - means to maximize or minimize a function given a set of constraints.

I'd rather describe engineering as the practice of designing a product while constantly assessing and choosing trade-offs between various considerations - quality metrics like application speed, size, features and correctness, but also ""softer"" considerations like development time, maintainability and customer requests. What you describe as optimizations I'd rather describe as trade-offs, because that's really what they are - what do we want and what is the cost? These trade-offs are what an engineer must make all the time and try to balance everything acceptably.

I fully agree that overengineering is an odd word. We generally agree that engineering is a good thing, so it's odd overengineering became a pejorative for a particular, disliked strategy of trade-offs.

Anyway, to get back to the central point of our conversation. I come from a system programmers perspective, and I too find it odd that web apps are criticized for algorithm efficiency. Web apps tend to use a lot of cycles, but I don't think the algorithms are the culprits here - I think the cycles may be eaten either in the abstraction layers or by the interpreter itself. This is just pure conjecture though, because I'm not knowledgable enough about web dev to confidently make an informed judgment about it.",1,0,0,False,False,False,1640733630.0
rq60d4,hqeeedd,t1_hqc4vim,"I had a thought, and I have a conjecture on how the meaning of overengineering got warped.

If we consider the classic meaning, where it means something is made unusually well (like a bridge meticulously constructed to be even better than with usual engineering), then the parallel meaning to software engineering could be that overengineering a piece of software to execute extraordinarily well (high speed, low memory usage, etc.). In other words, a highly optimized piece of software. Optimization makes the source code incredibly complicated, so overengineered software (classic meaning) is much more complicated than it probably needs to be. I can imagine this notion of needless complication of the source code over time coloured the meaning of overengineering, so it became more associated with too much complication rather than building something exceptionally well.",1,0,0,False,False,False,1640778031.0
rq60d4,hqt4agn,t1_hqpk98s,"In DDD, user needs determine the data structures. The data structures determine the algorithms. Example: if the user needs fast undo/redo functionality, then a doubly linked list might an appropriate data structure, and therefore the code is tailored to linked list functionality.

It absolutely contradicts, and I thoroughly explained how.

Regarding flexiblity, I said that the percieved inflexibility of DDD is overestimated. The reason is DDD code is generally speaking relatively easy to change.

I agree with what you said about using data abstractions when the need arises in an organic manner. This isn't against DDD. In fact, this is the *only* way you should make data abstractions in DDD - not making data abstractions as a rule of thumb, whether really needed or not.",1,0,0,False,False,False,1641047762.0
rq60d4,hq90is9,t1_hq8zy09,"He said, having presented nothing but anecdotes himself...",1,0,0,False,False,False,1640674098.0
rq60d4,hqc3ojh,t1_hqc1t7r,"> There's also the option of doing something simple, or, even better, leaving out that element entirely.

There's *sometimes* that option. But generally, this is not what people are complaining about in frontend development. They aren't complaining that $APP does too many things, and that they wish it just had less functionality.",2,0,0,False,False,False,1640731312.0
rq60d4,hq916fk,t1_hq90is9,[deleted],1,0,0,False,False,False,1640674555.0
rq60d4,hqc4fo2,t1_hqc3ojh,I think you meant to reply to /u/Tai9ch,1,0,0,False,False,False,1640731629.0
rq60d4,hq91h8a,t1_hq916fk,"That's... not what straw man means. You might have been thinking ad hominem?
 
You've presented no arguments that require countering.",1,0,0,False,False,False,1640674767.0
rq60d4,hqc7v6z,t1_hqc4fo2,"Yes, sorry I pulled a switcharoo on you by deleting my (poor) first draft.",2,0,0,False,False,False,1640733084.0
rq60d4,hq928ny,t1_hq91h8a,[deleted],1,0,0,False,False,False,1640675318.0
rq60d4,hqc9gfd,t1_hqc7v6z,No problem. :-),1,0,0,False,False,False,1640733774.0
rqysm5,hqdkhst,t3_rqysm5,Sexbot,5,0,0,False,False,False,1640755907.0
rqysm5,hqdbkt1,t3_rqysm5,"Turing defined it in his 1950 paper, ""Computing Machinery and Intelligence."" Basically if AI can pass the Turing test, they are effectively as intelligent as anything else.",5,0,0,False,False,False,1640751162.0
rqysm5,hqdlq5s,t3_rqysm5,"every time this question getsasked, someone close to the answer has to come on reddit to downvote the question. This delays the solution.",1,0,0,False,False,False,1640756619.0
rqysm5,hqx1ib4,t3_rqysm5,"I don't believe machines will ever truly pass a turing test perfectly until they have a perfect grasp on what it means to be a human. More specifically, what human they are pretending to be.",1,0,0,False,False,False,1641115596.0
rqysm5,hqe2p0v,t1_hqdbkt1,"There have been chat bots which can almost pass it for decades. More interestingly, there are many arenas of life in which people must stop for a moment to think ""is that a robot?"" without knowing for certain. Video game bots (including chess bots), forum (e.g. Reddit) bots, and chat (customer service) bots are all pretty obvious examples but there are less obvious ones too. How long can you drive behind a self driving car on a rainy day before you realize it's autonomous (driving is in many ways communication)? I have had one or two automated robo calls to my phone which took me an embarrassing amount of time to realize were adaptive recordings of some kind (like phone trees, which are also getting really good). There are many games in which it is not obvious that an opponent is a bot.

Basically, I think we are already at the point where the average person is occasionally going to be fooled by a bot impersonating a human. We have long been accustomed to occasionally losing to them at games, although even in victory they are rarely mistaken for human. This is in contrast with the fact that we don't have the kind of artificial general intelligence most people think of when they imagine something passing the Turing test. When it comes to our human ecosystem and the bots (which are not yet ""true"" AIs) which inhabit it, there's already a lot of them out there doing very complex and borderline things (like high frequency trading) which place them in a sort of mixed arena with humans. The expectations of the 1950s are only so useful at the end of the day.

I think it is probable that there is no ""magic line"" after which a thing is ""intelligent"" or sapient or AGI or whatever. We are unimpressed by our current level of accomplishment, but what if we were time travelers from a hundred years ago? Without the finer points of context to help with the distinction, there are a heck of a lot of bots or automatic systems which could easily be mistaken for thinking and willful things. I think that as more and more systems are put in place to make AIs seem intelligent, that line will get crossed without fanfare or anyone having a reason to ""declare"" it crossed. Some day we will all be having cogent, deep, complex, back-and-forth, intelligent conversations with our phones or desktops or cars, and people will still be wondering ""is AGI here yet? What about the Turing test?"". Perhaps instead of a fine line we can observe the stepping across of, we should think of it as a large and fuzzy zone, with us in the middle of it somewhere. People will always be able to point to a seemingly intelligent program and say ""aha, that's just mechanistic"" but at a certain point you are making very philosophical arguments about the meaning of emergence through mechanism by attempting to disprove the alleged intelligence of a program. At some point it will just be easier to assume a thing is sapient until proven otherwise, and I am okay with that future. I think it'll be pretty neat.",2,0,0,False,False,False,1640768748.0
rqysm5,hqg6g6s,t1_hqe2p0v,"You're missing the point of Turing's paper. If you can't tell the difference, it doesn't matter if it's ""true AI"" or not. That is the line. So far I still know when to hang up my phone from a robocall. So far I'm not ready start giving my toaster or candlesticks ""rights.""

The difference between writing and chess is the fact that you think in words, not chess moves. That's why it's the only important barrier. Turing thought this all out, trust me. He offers rebuttals at the end of his paper for some of your claims.",1,0,0,False,False,False,1640807122.0
rqysm5,hqgje5s,t1_hqg6g6s,"I was supporting that point, not arguing with it. I think a lot of people are missing all the important points by focusing on this idea of a ""true"" AI when, as you say, that was not really the point of the Turing test. 

>So far I still know when to hang up my phone from a robocall.

Me too, most of the time, but it's getting to the point where I have
on several occasions been unsure if it would be okay to hang up or not
because the robocall was so convincing (on one occasion even replying to me like one of those ""speak to me"" phone trees). It has gotten me thinking about what a future world would be like in which you had to take for granted that sapient processes were all around you, in any given object that could be programmed. In such a world you might adopt a policy of politeness towards objects that would seem highly unusual (to put it bluntly) to someone from, say, Victorian Europe. I already say ""please"" to my Google Voice just because it's healthy to build those reflexes when using your communications muscles, so how much more interesting and nuanced in a few more years when the world is full of systems which are ""close enough to intelligent"" to make you think about your social reflexes when dealing with a machine?

>So far I'm not ready start giving my toaster or candlesticks ""rights.""

I'm not convinced it will ever ask for them, honestly, no matter how far these things go. You may not want to give your candlestick rights, but at what point do you begin to offer it politeness or consideration of some kind, if ever? Or at what point do you get mean to it? In a world full of somewhat sentient or sapient devices which don't want or need ""rights"", it will say a lot about a person how they treat those things. Not for the sake of the device, but for the person's social reflexes.

>The difference between writing and chess is the fact that you think in words, not chess moves. That's why it's the only important barrier. Turing thought this all out, trust me. He offers rebuttals at the end of his paper for some of your claims.

Unless one is just typing randomly then there is logic, movement and order to writing. This is why we can describe computer programs using words, and also chess games for that matter. The comparative cognitive load between traditionally ""cerebral"" activities is probably an area that is prone to a lot of preconceived prejudices, so I hesitate to compare them. You can take a chat bot so far in a conversation sometimes because of how effective projection and cold reading are. To have a deep, cogent, considered, insightful, top notch conversation is so difficult that a lot of humans go through life without ever being more than very bad at it (I'm not great at it myself). When you consider how easy it is to misunderstand, fail to practice active listening, insert a preconception or prejudice, or otherwise mess up a conversation's context wildly between two human beings (and how these things are the norm rather than the exception for all but the most basic conversations between two people much of the time) I think we should consider that in some years most smartphones will be better conversationalists than most people. Even then, we will still probably be arguing about the Turing Test (only then it will be about how ""real people aren't that good at conversation"", in the same way that people accuse video game bots at times). I think it will be just another wake-up call for people who thought that the species was ""defined"" by a specific quality (in this case intelligent communication). Although not all of Turing's rebuttals have aged equally well, I'm not arguing with the paper. I also agree that this ""magic line"" of intelligence is over-rated or does not exist. We will have programs that are ""sapient enough"" long before the argument is settled, philosophically, I think.",2,0,0,False,False,False,1640812110.0
rqysm5,hqhv5l7,t1_hqgje5s,"I didn't have to read it, but since I've graded standardized English exams by the thousands, I could tell you're probably human. I'd be willing to bet on it. Congratulations.

Wake me up from Delillo when AI starts betting money. I mean, ""AI"" has owned chess for over a decade and now go, and they still can't make a living like a McDonald's worker fresh out of high school can? Shit, the McDonald's near me is advertising $18/hr. That's like one cheeseburger every 20 seconds.",1,0,0,False,False,False,1640832116.0
rqqd61,hqbzit3,t3_rqqd61,"The computer doesn't know about types. If you do floating point arithmetic, the computer assumes whatever you're operating on is a floating point number; same for integers. The compiler and/or runtime system generally has to keep track of the type of each variable and prevent the developer from doing anything nonsensical. If you're writing assembly, the assembler typically won't enforce any type checking and this responsibility then mostly falls on the programmer.",10,0,0,False,False,False,1640729572.0
rqqd61,hqd21ff,t3_rqqd61,"Fun fact: Some language (like C) have ways to reference the same memory location by different data types. You could store a 32 bit unsigned integer, and then read it back as a 32 bit floating point number. And yes, there are legitimate reasons you may want to do that. But it all boils down to the programmer **choosing** to refer to some variable as a specific type. (Or sometimes the language will automatically select the type based on what operation the programmer decided to apply to it.)",2,0,0,False,False,False,1640746582.0
rqqd61,hqx1p7u,t3_rqqd61,"The data type is not stored with the variable, the data type is stored in a sort of table of contents. Most of the time, when a file is deleted, the file itself is still there, it's just that its index in the file system is removed.",1,0,0,False,False,False,1641115759.0
rqeudi,hqa84y4,t3_rqeudi,Which language do you have experience with?,1,0,0,False,False,False,1640703903.0
rqeudi,hqadybv,t3_rqeudi,Personally I liked the book Introduction to the Theory of Computation by Michael Sipser as an introduction to the ideas behind computer science.,1,0,0,False,False,False,1640706408.0
rqeudi,hqee586,t1_hqa84y4,Python and a bit of JavaScript,1,0,0,False,False,True,1640777841.0
rqeudi,hqee6dm,t1_hqadybv,Thanks! I'll give it a look,1,0,0,False,False,True,1640777864.0
rpr7gj,hq69ayn,t3_rpr7gj,"I would definitely recommend Algorithms in a Nutshell by George Heineman. The book gave me the most in-depth look into all algorithms and has a couple chapters focused solely on chess. 

If you want to check the book out before purchasing it you can do that here: https://archive.org/search.php?query=Algorithms%20in%20a%20nutshell",22,0,0,False,False,False,1640628890.0
rpr7gj,hq6ns3f,t3_rpr7gj,"This isn’t a book, but I found the chessprogramming wiki to be very comprehensive!",4,0,0,False,False,False,1640634812.0
rpr7gj,hq7tayq,t3_rpr7gj,"The most important algorithm to know for these kinds of (two player turn based) games is the minimax algorithm: https://en.m.wikipedia.org/wiki/Minimax

There are lots of variants for optimizing performance but this is the starting point. Coming up with a good heuristic function to use is an important part, and can be approached lots of ways (ML, hand coding, etc)",2,0,0,False,False,False,1640652427.0
rpr7gj,hq6hbfd,t3_rpr7gj,"A good chess engine work using reinforcement learning algorithms. 

Depending on your knowledge you can read for example AlphaZero's paper  [here](https://www.science.org/doi/full/10.1126/science.aar6404) but its a hard read without knowing the prerequisites.",3,0,0,False,False,False,1640632144.0
rpr7gj,hq761k5,t3_rpr7gj,"Not a book as such, but Stockfish is an open source chess engine.  They have a page for people getting involved, including coding. https://stockfishchess.org/get-involved/

And they have a discussion forum. https://groups.google.com/g/fishcooking",2,0,0,False,False,False,1640642343.0
rpr7gj,hq6yzwp,t3_rpr7gj,"Check this out: https://wiki.cs.pdx.edu//minichess/

This is from one of my profs at Portland State, and it's fantastic. It's a great intro into the algorithms you're interested in.",1,0,0,False,False,False,1640639423.0
rpr7gj,hq78mml,t3_rpr7gj,Toledo NanoChess by Oscar Toledo Gutierrez. I guess it's not entirely modern as Stockfish or Alpha.,1,0,0,False,False,False,1640643423.0
rpr7gj,hq7vswd,t3_rpr7gj,This is handy  [https://www.chessprogramming.org/Main\_Page](https://www.chessprogramming.org/Main_Page),1,0,0,False,False,False,1640653551.0
rpr7gj,hq8jbhr,t3_rpr7gj,"I know you asked for books (for which you should look at the other comments) but I would read on minimax with α-β pruning and Monte-Carlo Tree Search. The former powers DeepBlue, the latter powers AlphaZero (I think?)",1,0,0,False,False,False,1640664298.0
rpr7gj,hq9a6mg,t3_rpr7gj,"Norvig's very influential AI book, while not specifically about chess, uses plenty of example from chess. Worth studying in depth.",1,0,0,False,False,False,1640681470.0
rpr7gj,hq60rg0,t3_rpr7gj,Me too! f,0,0,0,False,False,False,1640625439.0
rpr7gj,hq7vvzr,t1_hq6ns3f,https://www.chessprogramming.org/Main\_Page,5,0,0,False,False,False,1640653591.0
rpr7gj,hq7tcod,t1_hq7tayq,"**[Minimax](https://en.m.wikipedia.org/wiki/Minimax)** 
 
 >Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. When dealing with gains, it is referred to as ""maximin""—to maximize the minimum gain. Originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",2,0,0,False,False,False,1640652449.0
rpr7gj,hq81tzc,t1_hq761k5,"Stockfish wouldn't be a good example for a beginner as the engine employs machine learning and hardware acceleration if I recall correctly. Stockfish is an extremely advanced chess engine, hence why it has the highest *[ELO rating](https://en.wikipedia.org/wiki/Elo_rating_system)* amongst other engines. It would serve as a good example for a developer to work up to, but not start off with. You are essentially handing them the keys to a Ferrari when they don't even have their temps yet as well for throwing a box of parts at them shortly before telling them it's an airplane. OP does not want a box of parts, they want a blueprint to assimilate so that they can later on assemble their own airplane from their own box of parts. I cannot recommend this enough, do not refer beginners to major codebases. Doing so will cause them to spend more time trying to learn and navigate the codebase in lieu of the subject itself.",3,0,0,False,False,False,1640656262.0
rp30oe,hq24exx,t3_rp30oe,"If you're an academic then writing papers, attending conferences and reviewing papers will likely keep you up to date.

Apart from that, keeping up with the latest trends is not that important. The ""hot new things"" often grow out or favor in a few years. Sure, if you can spin your research as being tangentially related to this they may sound better, but apart from that new developments in some area often have no relevance on your particular area.",57,0,0,False,False,False,1640547437.0
rp30oe,hq24nax,t3_rp30oe,"Prediction is hard, especially if it's about the future.

X, Y, and Z will usually be the tip of the iceberg. Every day you will read/hear about dozens of things that are the latest and the greatest. It's hard to upfront know what is and what isn't going to pan out. One approach is to just pick one and work at it if it appeals to you.",21,0,0,False,False,False,1640547539.0
rp30oe,hq2708e,t3_rp30oe,[Hacker News](https://news.ycombinator.com/),29,0,0,False,False,False,1640548565.0
rp30oe,hq3k46b,t3_rp30oe,"I think it’s easy to be dismissive of latest tech and chasing the new hotness. However, developing a sense of what is signal in the noise, what is worth using some time to examine and potentially adopt in your toolkit as it evolves is one of the greatest skills you can develop as an engineer. This is a industry always on the move and you can move with it or stand still. You know what happens to those engineers who refuse to evolve. 

To answer your question more specifically, company engineering blogs have become great resources. Find companies you admire and see what they are doing to solve their problems.",8,0,0,False,False,False,1640572088.0
rp30oe,hq3p1pd,t3_rp30oe,"I have a few YouTube channels I go to to see what’s new in tech. Some cover big things like new releases of stuff like laptops and smart phones, some cover pen testing and new exploits, and others are just people far smarter than I’ll ever be giving me their opinion on different things. I like YT a lot better because it gives more information, so if I am interested in something I can refine my search better when I go to look it up. Ticktock and YT clips never give me enough info and I always have to google it after each thing I watch just to find out it’s not that interesting. As far as learning it. I usually like to take in as much as I can just so I know the name and a very broad understanding of what it is. Not enough to actually have a conversation about but enough that if I hear it again I can be like, yea that’s a new browser, or that’s a new block chain scam. However, if I find it interesting or if it frequently comes up in different discussions I will dig a bit deeper and learn more about it.",7,0,0,False,False,False,1640574469.0
rp30oe,hq4os7t,t3_rp30oe,Quantamagazine. Nature. Try two minute papers on youtube for ML related things.,5,0,0,False,False,False,1640597530.0
rp30oe,hq2ew4f,t3_rp30oe,"Theres several layers to it. You got the high level hypey stuff like crypto, nft, web3 stuff you can reach about on tech crunch or hacker news or wired. You then have the different language level libraries and design patterns and frameworks you can catch up on by googling ""best software engineering"" blogs. And then you have system design on big tech company websites.",8,0,0,False,False,False,1640552740.0
rp30oe,hq4notr,t3_rp30oe,Google News Feed app It'll figure out what tech you're interested in. Click on the appropriate links and over time you'll get a personalized tech news feed,3,0,0,False,False,False,1640596607.0
rp30oe,hq52r9u,t3_rp30oe,I'm a software engineer and I honestly find out about the latest stuff when our apprentices say they want to use it.,2,0,0,False,False,False,1640608774.0
rp30oe,hq5gz6w,t3_rp30oe,"It is NOT fast moving.

It takes years for a new language to mature to usability. The time from when I first heard of Java to usable Java was about five years. If you hopped on Swift the year Apple announced it you rewrote your app at least four or five times if it still works today. 

I would also argue we do not make much real progress, we just shift things around like hem lengths on skirts. We cast off perfectly good stuff for no good reason in favor of new half baked things that are not yet as good all the time.",2,0,0,False,False,False,1640616873.0
rp30oe,hq5vxal,t3_rp30oe,"Forums and blogs and stuff, YouTube channels as well probably
And reading papers, but you find the papers on forums

As well as annual conferences if you have a lot of spare time 

Honestly if you’re involved in your specific field (ie frontend, embedded etc) and you’re in all the subreddits and what not, you should be able to stay more or less up to date",2,0,0,False,False,False,1640623459.0
rp1tzo,hq1qubv,t3_rp1tzo,[cpprefererence.com](https://en.cppreference.com/w/cpp/algorithm/rotate) Usualy have a possible implementation (as in this case) and is genarally bettwr than cplusplus.com,9,0,0,False,False,False,1640541704.0
rp1tzo,hq34xd8,t3_rp1tzo,"First of all the element at middle has to go to first. Where does the element that was at first go? That's unclear, but we'll get there. Let's store it at middle for now. Then the element after middle has to go to the element after first. So let's swap those in the same way. We continue doing this swapping the elements first+k and middle+k. The correctness we can prove during this is that the element that is swapped into the first+k is now at the correct location.

This stops either when first+k becomes middle, or when middle+k becomes end.

\- In the first case, the elements that were swapped into the locations between middle and middle+k are not necessarily in the right location. They should be the last k elements, and all the elements after middle+k were supposed to shift left k times. So we're basically left with rotating the portion between middle and end, with middle+k as the new middle.

\- You can analyze the second case similarly. It's a bit messy to describe this case but it's easy to see what is needed to be done if you draw the list, note what got swapped, and how it needs to be.

You can verify that both cases are just instances of rotate, and that the code given has the correct first, middle and last in order to do the required rotate.",2,0,0,False,False,False,1640564761.0
rp1tzo,hq2vnqb,t3_rp1tzo,"I have an intuition why it is correct, but this intuition works by understanding the algorithm as an recursive algorithm.

Here's the set up: we want to rotate the array leftwards, such that the element that used to be at the `middle` position is now at the start (i.e. position `first`)

Note that if `middle=first`, we are done.

Otherwise, we will now swap the first k elements with the k elements starting at `middle`. k is at least one, and is determined by `min(middle-first, last-middle)`. For example, let's say we have an array with numbers 0..9, and we want to pull 3 to the front:`0 1 2 3 4 5 6 7 8 9` becomes `3 4 5 0 1 2 6 7 8 9` after this initial swap step. `k=3` as `3=min(3,7) = min(3-0,10-3) = min(middle-first, last-middle)`. `first` points at position `3` (i.e. `middle`), `next` points at `6`.

Another example: Same array, but `middle=7`:

`0 1 2 3 4 5 6 7 8 9` becomes `7 8 9 3 4 5 6 0 1 2`, as `k=3` again, but this time it's because `last-middle=3`, which is the limiting term. `first` points at `3`, `next` at 10 (i.e. `last`)

Essentially, k is chosen such that at least one of the `if`s will become true after k iterations of the loop.

So, we have now done this step. Note that the first `k` elements are sorted, and `k>1`. Now, the magic is that the remaining array can be brought into correct shape by recursion.

In the first case, we have `0 1 2 6 7 8 9` remaining, which can be corrected by rotating such that the 6 is pulled to the front. In the other case, we must rotate `3 4 5 6 0 1 2` such that `0` is pulled to the front.

&#x200B;

The first case corresponds to `if (first==middle) middle=next;`, the last case to `if (next==last) next=middle;`. In the first case, our new `middle` will be `next` and we do recursion. In the latter case, `next` is reset to `middle`. Notice that in either case, the variables are now arranged as if we had done a recursive call, with `next==middle` holding.

As for run-time analysis, `first <= middle <= next <= last` always holds, and `first` continuously grows while `last` remains constant, thus we must eventually exit the loop, after at most `last-first` steps.",2,0,0,False,False,False,1640560492.0
rp1tzo,hq1wad1,t3_rp1tzo,"1. rotate(a, a + k, a + n) = reverse(a, a + k) + reverse(a + k, a + n) + reverse(a, a + n)
2. Let's fix some index i0 and for i0 let's make a 1-shift for indexes i0, (i0 + k) % n, (i0 + 2k) % n, ..., i0. It's easy to do with O(1) memory. Let's do this action for i0 = 0..gcd(n, k)-1.",-6,0,0,False,True,False,1640544031.0
rp1tzo,hq1t9kk,t1_hq1qubv,I find this implementation similarly perplexing but is at least commented,1,0,0,False,False,True,1640542760.0
rp1tzo,hq3v5pt,t1_hq2vnqb,"bro this is great, really nice explanation thank you. this was an interesting read",2,0,0,False,False,False,1640577506.0
rp1tzo,hq1whoy,t1_hq1wad1,"Sorry, didn't read the question",-3,0,0,False,False,False,1640544117.0
rop984,hpzx3md,t3_rop984,"If we're talking practically speaking, auth0 or some other provider is the most secure solution.

I'm not much help on the theoretical side of your question, though I haven't been at a company which separates application data from user authenticating data. Doing so seems to run along the lines of security through obfuscation rather than a sound security principle.

I Am Not A Security Professional, Just A Security Interested Developer.",3,0,0,False,False,False,1640497411.0
rop984,hq1l3pi,t3_rop984,"Infosec architect here. Rule 1 of application security, never ever roll your own security unless you absolutely have to. Rule 2 if you think you absolutely have to you are probably wrong. 


Some kind of OIDC system is probably best. but you're more or less right, it doesn't really matter if they're separate so long as you do good salted hashes",3,0,0,False,False,False,1640539146.0
rop984,hq056dm,t3_rop984,"Probably not, the real security advantages could come from how you treat the databases. Like can you not grant some applications access , or somehow make the databases more separated where if you have access to one you don’t to the other.

If practice I wouldn’t do this without other reasons why I want them separated. For example microservices and these are different teams.",2,0,0,False,False,False,1640503465.0
rop984,hpzxzed,t3_rop984,"My personal thing is remember Murphy’s law when it comes to security. If anything can go wrong, it’ll go wrong. Apart from security, performance could be affected if both of them are the same. You could use a hybrid design where you store user data in some db that is fast at handling aggregate operations (if you need that) and a db which gives kinda O(1) lookup for username->hashed password. If you have a billion users, storing everything in one db would have performance effects. What if the user doesn’t be access to all user data, just needs to login and do something? There’s also the layer of scalability where you sometimes don’t need to scale the user data db while you need to scale user auth db.",1,0,0,False,False,False,1640498012.0
rop984,hq0nkpg,t1_hpzx3md,"> If we're talking practically speaking, auth0 or some other provider is the most secure solution.

auth0 has had critical vulnerabilities in the past.

The downside to using third party solutions is that they are often targeted by more actors, since any potential exploit can give you access to many more targets (e.g. the recent log4j CVE).

Of course, getting security right is very difficult, so you should probably not roll your own, but you also shouldn't trust others blindly.",2,0,0,False,False,False,1640520276.0
rop984,hq04pom,t1_hpzx3md,"Yea, that was my gut feeling too. I don't really know how databases get hacked (technique-wise), and my layman guess was that even if one db can get hacked, hacking the second doesn't become any easier.. but I should probably research more about vulnerabilities first, before building my mental models. Thank you!",1,0,0,False,False,True,1640503084.0
rop984,hq040kg,t1_hpzxzed,"Ok, I see. To clarify more, my (personal) project is basically recreating Google Sheets, and I can't say for sure, but my intuition is that their implementation is to have a unique db (or group of db's? distributed db's?) for each of read-only/read&comment/read&comment&write, so they can optimize each db for the use-case. 

I was curious if there is maybe a security reason (aside from performance) for separating a read-only db from a read&write db, but now that you mention it, I should probably care a lot more about performance.

I'm gonna search up resources on performance and databases in general now. So much to learn... but thank you!",2,0,0,False,False,True,1640502509.0
rop984,hq3j9g0,t1_hq04pom,"DB's get hacked in a myriad of ways - from SQL injection to compromised credentials to unintentionally exposed databases.

I'd put something far simpler and more common like SQL injection or ensuring credentials are secure above a circuitous theoretical attack requiring separating user credentials from application data.

Your goal is _always_ to avoid being the lowest common denominator, because no system is invulnerable. Hackers are (generally) lazy and looking for a quick pay day. Don't be the easy target, avoid most (though not all) issues.",1,0,0,False,False,False,1640571674.0
rop984,hq7b0no,t1_hq04pom,"[https://attack.mitre.org/](https://attack.mitre.org/)  


this may be a useful resource for you. I don't think it's exhaustive, but ""MITRE ATT&CK® is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations."" (first sentence on the landing page of the website) Going over the titles and skimming through some of the technique groups/individual techniques could help you build an idea of what you need to protect against",1,0,0,False,False,False,1640644442.0
rop984,hq0zxlz,t1_hq040kg,"Keep in mind the context you are looking at. Google has a single set of user auth/basic info and lots and lots of different applications. So it makes sense to separate the user's per application data. Makes it easy to add/remove applications over time. 

Security-wise, it likely doesn't matter (unless you handle the auth database very differently the other), but there are a good engineering reason to separate them.",1,0,0,False,False,False,1640528863.0
rolb3t,hq03998,t3_rolb3t,"Trial division is O(sqrt(n)) exactly because smallest prime factor is <=sqrt(n). If you want to generate all primes up to n by trial division, it would be O(n\*sqrt(n)). There is of course Sieve method that can go all the way down to O(n).

Yours - idk. Trying to save all prime factors is at least O(nlogn) and that's already worse than Sieve.",5,0,0,False,False,False,1640501905.0
rolb3t,hq0zltu,t3_rolb3t,"The r/math post has been removed, so I will copy and paste it here:

After watching a YouTube video on prime factorisations, I became interested in the patterns that appear when you factorise the natural numbers in order (from n = 2 onwards).

I think it's really interesting how these patterns appear. For example, when 2 is the base (which happens for every other n; as every other n is even, so has 2 as a factor), the powers follow this pattern:

1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5...

Edit: the powers really follow this pattern: 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, 4... (I'd forgotten the case of 2\^0) and the pattern above is just for powers > 0, but the following logic still holds.

This pattern appears to be such that you have a '1' at every (1-based) index other than a multiple of 2 (so 1, 3, 5, 7, 9... and so on). But then if you remove all the '1's from this list, you end up with a new list: 2, 3, 2, 4, 2, 3, 2, 5... Interestingly, this new list follows the same pattern, just now with '2's at every index other than a multiple of 2. This process carries on and on as you keep removing numbers.

What's even more interesting is that you see a similar pattern for higher bases. When 3 is the base (which happens every 3 n, again for the same reason as before), the powers follow this pattern:

1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 4...

This pattern now appears to be such that you have a '1' at every index other than a multiple of 3 (so 1, 2, 4, 5, 7, 8... and so on). But again, if you remove all the '1's from this list, you end up with a new list: 2, 2, 3, 2, 2, 3, 2, 2, 4... And again, this new list follows the same pattern, just now with '2's at every index other than a multiple of 3, and so on...

protocat-112ocat-112uestion is - what is going on here? Has anyone got an intuitive explanation for why these patterns appear?

To try and understand this a bit more (and since I am more computer scientist than mathematician) I looked up and found an algorithm for generating prime numbers. I then implemented it in C# (which I have called 'current algorithm'). I've added some comments with what I think is going on, but generally I understand it. The algorithm iteratively factors out every prime from the target number. Put this in a simple loop counting up from 2, and you can easily generate a table (or dictionary in this case) of all prime factors for n >= 2. What is the time complexity of this algorithm? I had a go at trying to figure it out and arrived at O(n\^3), but the while loop confused me somewhat...

What I wanted to do however was generate the prime factors in a different way. If I could figure out the pattern of the bases and powers, I could instead 'populate' each number with its factors looping though the factors (a base and a power) instead of the target number, starting with 2\^1, then 2\^2, 2\^3... then 3\^1, 3\^2, 3\^3... and so on. I came up with this algorithm (which I have called 'new algorithm'), which as pseudo code is as follows:

    dict = an empty dictionary with keys from 2 to SOME_LARGE_NUMBER
    FOR (base = 2 to base = SOME_LARGE_NUMBER):
      IF (dict[base] already has entries):
        // base is not prime
        SKIP to next base
      FOR (power = 1 to log_base(SOME_LARGE_NUMBER)):
        FOR (extender = 0 to base - 2):
          FOR (n = basepower + (extender * basepower) to SOME_LARGE_NUMBER),             
      INCREMENTING n by basepower + 1:
            dict[base].Add(base, power)

I managed to figure this out basically through trial and error. The core part of this algorithm is the final for loop. For base = 2, power = 1, extender is only 0 and this gives n = 2, 6, 10, 14, 18, 22.... For base = 3, power = 2, extender will be 0 then 1, giving n\_0 = 9, 36, 63... and n\_1 = 18, 45, 72 (which combined give us all the numbers which have 3\^2 as a factor). Again, my question here is why does this work? I am especially confused about my 'extender' code, which is, as I see it, a fudge to make it work. Also, what is the time complexity of my algorithm? I believe this runs in O(n\^3\*log(n)), but I may be wrong... I've only ever looked at the concept of time complexity for simple examples...

It's worth noting that the runtime of my algorithm seems to be much slower than the 'current' algorithm for finding prime factors when SOME\_LARGE\_NUMBER is indeed large (like 100,000). Also, I've not figured out how to optimise it for only looking for a single n, which I'm not even sure is possible, as the n is in the inner-most nested for loop, which is dependent on the outer loops.

However, it is considerably faster for a smaller value of SOME\_LARGE\_NUMBER, like 1000.

Both of these values may of course be affected by my specific implementation of the algorithms.

I'd be grateful if someone could explain or clarify any of the questions I've raised, or even just add any further info, discussion or relevant reading about this, as I'd love to understand more about what I am seeing.",1,0,0,False,False,True,1640528679.0
rolb3t,hq0p11s,t1_hq03998,"I see, thank you!",1,0,0,False,False,True,1640521491.0
rolb3t,hq1lqdj,t1_hq0zltu,"I responded to your /r/math post but I don’t think you understood my comment. Try seeing what happens for n=10 because its the one where people are most familiar with the divisibility rule for n^k.

Start from 0 and increment by 1 in steps. Every 10 steps it’ll look like xxxx0, which is a multiple of 10. Every 100 steps it’ll look like xxx00, which is a multiple of 10^2. Every 1000 steps it’ll look like xx000 which is a multiple of 10^3, and so on.

Now, transfer over this logic to n-nary numbers, since in base n, a number of the form xxx0^k has a n-adic valuation of k.

All of this exactly explains all your observations. Your operation of “removing all the 1’s” and seeing the same pattern with every number incremented by 1 is just you recursing on a new instance of your observation where everything is scaled by a factor n, i.e. increment in steps of n instead and chop off the least significant digit of your n-nary number

Essentially, you are just witnessing “fizzbuzz” with the set {1, n, n^2 ,…} with words {0, 1, 2, …} and operation of “max” instead of the usual fizzbuzz on the set {3, 5} with the words {“fizz”, “buzz”} and operation “concatenation”.",1,0,0,False,False,False,1640539422.0
rnyhfs,hpvkmz2,t3_rnyhfs,"A processor can be thought of as a state machine. It’s registers constitute its state. It’s ALU and various other combinatory logic blocks make up its transition table between states.

Each clock cycle the values in the registers flow through the combinatory logic where operations are performed on them and they are transformed, ultimately ending up back at the registers. At the end of the clock cycle they are latched into the registers and the whole thing happens again the next clock cycle.

So if some signals in some section of the combinatory logic have not finished propagating through the circuit by the time data is latched into the various registers, then perhaps incorrect data will be latched into the registers and the processor will malfunction and behave in unexpected ways.",52,0,0,False,False,False,1640399692.0
rnyhfs,hpvldtj,t3_rnyhfs,"In most cases, the output of the circuit will only be read as 0 or 1 by driver/software, and whatever that output is at the expected clock cycle will be treated as hardware response. The driver has to handle that hardware response. 

* Say the component is a network card, the driver needs to handle lack of response from the network. 
* Say the component is a HD, the driver will detect that your HD cannot provide an appropriate USB/SATA/IDE response, thus is not working.

Most devices don't have its own clock chip, so won't be operating at unexpected frequency.",8,0,0,False,False,False,1640400168.0
rnyhfs,hpwdln1,t3_rnyhfs,"Tons will go wrong, such as flip flop metastability. It is something that should not happen ever, this is actually so important that design tools check for that without ever asking. To get rid of that constraint, people either use several clock ""domains"" which are related to one another for sync, or sometimes asynchronous logic circuits.",3,0,0,False,False,False,1640420422.0
rnyhfs,hpvakjx,t3_rnyhfs,"Component? What component? Component of what?  
Job? What kind of job? Where it performs the task? What for?

A more specific scenario would be helpful for answering the question",23,0,0,False,False,False,1640393383.0
rnyhfs,hpy5w6h,t3_rnyhfs,"To put it simply. It’s based on whether or not the component was designed to function in between clock-ticks.

An example of this would be an I/O function from your H/SDD to your processor/RAM. The transferring takes multiple cycles, and as such the CPU runs into an Interrupt Request (IRQ).

If the component was designed to work based on the tick of the clock. Generally the program will loop infinitely on that instruction, until it’s able to execute that command, or you/OS kill the program for hanging (as that’s what it would appear to you), if it’s a Kernel/OS memory space program, you’re getting a segment fault.",2,0,0,False,False,False,1640463340.0
rnyhfs,hq0mo34,t3_rnyhfs,"Wow, engineers now a days just aren’t familiar with computer science. It’s absolutely ridiculous!!",1,0,0,False,False,False,1640519481.0
rnyhfs,hpwnhpq,t3_rnyhfs,"In which context is this? What kind of component, software?

If you are talking about pre-emptive multi-tasking OS, then the running task is switched as the time slice is used up. State is saved until there is another slice of execution time for it.",1,0,0,False,False,False,1640429987.0
rnyhfs,hpvl337,t3_rnyhfs,Front end? Need to be a lot more specific,-4,0,0,False,False,False,1640399979.0
rnyhfs,hpvne5n,t3_rnyhfs,Real-time operating systems (RTOS') might be something to look at - they're specifically designed to do their job precisely on time,-2,0,0,False,False,False,1640401443.0
rnyhfs,hpwi7lo,t1_hpvkmz2,"I think this is the most sound answer. In the most basal sense, if data is not operated on in the order it's supposed to, or certain operations are ""missed"", you can end up with errors of varying severities in whatever you're doing.",12,0,0,False,False,False,1640424799.0
rnyhfs,hpyr4h8,t1_hpvakjx,Its obvious to me he is asking about digital design inside a processor.,10,0,0,False,False,False,1640474112.0
rnt040,hpu974r,t3_rnt040,You have to be way more specific than that. Cyber security of Operating Systems? Programming/development? Patching? Quality assurance? There's so many different aspects of operating systems that all have different research and careers.,16,0,0,False,False,False,1640374122.0
rnt040,hpvanln,t3_rnt040,You can check out papers from SOSP to get an idea of research going around OS!,5,0,0,False,False,False,1640393434.0
rnt040,hpudp1z,t3_rnt040,"Here’s a great thread I found a while back, https://www.reddit.com/r/learnprogramming/comments/5v1c16/why_does_the_cover_of_the_operating_system/?utm_source=share&utm_medium=ios_app&utm_name=iossmf

It’s great to see other people interested in OS Design, it’s not very popular compared to other fields.",11,0,0,False,False,False,1640376257.0
rnt040,hpvq3j3,t3_rnt040,"I don't know about research exactly, but the field is definitely still looking for changes. RTOS specifically is a very important topic and constantly improving. New OS designs like Zephyr trying to make certain protocol stacks like Bluetooth more sane. Or some out there stuff like Redox OS which ignores POSIX design and just goes for adding interesting stuff like ""everything is a URL"". Which sort of also borrows from Plan9's 9P which everyone interested in OS design should check out.",4,0,0,False,False,False,1640403139.0
rnt040,hpvpz7k,t3_rnt040,You’re in luck. OS is a whole field with research constantly going on in its many sub fields. It’s definitely one of the most interesting CS topics.,3,0,0,False,False,False,1640403064.0
rnt040,hpvv5c3,t3_rnt040,"Operating Systems Design and Implementation https://www.amazon.com/dp/0131429388/ref=cm_sw_r_apan_glt_fabc_4TZMCZ9JD572DPY9C296

The bible. It's a great book, and very easy to get in to.",2,0,0,False,False,False,1640406334.0
rnt040,hpx2wlj,t3_rnt040,"Check out Genode. It's what my OS Prof suggested.

Also seL4, mathematically proven secure OS, if I understood correctly.",2,0,0,False,False,False,1640442404.0
rnt040,hpv86rz,t3_rnt040,Yes.,3,0,0,False,False,False,1640391962.0
rnt040,hpvw3y5,t3_rnt040,"I had a course on Operating Systems recently. It was pretty intense, the professor discussed many research developments in areas of Multi core scaled OS, Virtualization, Scheduling Algorithms (mainly, CFS), File systems, Virtual memory.",2,0,0,False,False,False,1640406957.0
rnt040,hpx9im1,t3_rnt040,Consider whonix,1,0,0,False,False,False,1640446404.0
rnt040,hpw7t4y,t1_hpu974r,"I mainly got interested when I learned how OS does memory management, techniques like paging, the virtual memory concept,maintaining the page table, the entire process of generating the logical address then converting finally into physical address to look for the appropriate frame in the main memory to fetch the desired data. 
Is there any scope of doing research in this memory management? I searched about it but couldn't find much info on it's research. I may not have searched in the appropriate place. So came here for some suggestions.",3,0,0,False,False,True,1640415367.0
rnt040,hpw8090,t1_hpvpz7k,"I'm mainly interested in OS memory management topic. 
The various techniques that have been developed to handle critical memory segments. Techniques like paging, virtual memory, segmentation, relocation etc. 
I wish to study this area in more details than bachelor level, and contribute to it in future",1,0,0,False,False,True,1640415534.0
rnt040,hpvv5yj,t1_hpvv5c3,"Beep. Boop. I'm a robot.
Here's a copy of 

###[The Bible](https://snewd.com/ebooks/the-king-james-bible/)

Was I a good bot? | [info](https://www.reddit.com/user/Reddit-Book-Bot/) | [More Books](https://old.reddit.com/user/Reddit-Book-Bot/comments/i15x1d/full_list_of_books_and_commands/)",0,0,0,False,False,False,1640406346.0
rnt040,hpw874m,t1_hpvw3y5,Is the course available online?,1,0,0,False,False,True,1640415694.0
rnt040,hpw8h24,t1_hpw7t4y,"Sounds like you want/are looking for computational resource management with a mix of OS engineering. In terms of research, I'd recommend using sites like Jstor where you can not only look at research, but read/analyze empirical studies, dissertations, and peer reviewed articles on such topics. Also a great way to potentially make connections in the comp science realm. With that said, with the way quantum is approaching, I would heavily suggest also learning cyber security and defense of OS systems and OS architecture. Virtualization is a very popular topic right now as well, you'll be able to find an abundance of info on that.",6,0,0,False,False,False,1640415920.0
rnt040,hpx4tmf,t1_hpw8090,"I remember when I took the first real OS class I loved the memory and storage management stuff too. Just to let you know, there’s lots of other places where you can do the same stuff and embed that work. I’d check out some open source emulators or even just regular graphics programming API’s like OpenGL or Vulkan. In Vulkan there’s no default memory allocator so you actually have the option to write an implementation of malloc for a real application.",2,0,0,False,False,False,1640443610.0
rnt040,hpwhc8j,t1_hpvv5yj,No,1,0,0,False,False,False,1640423946.0
rnt040,hpx5wn9,t1_hpw874m,"Sorry, it was an in-person class.",1,0,0,False,False,False,1640444277.0
rnvod9,hpusafy,t3_rnvod9,[deleted],15,0,0,False,False,False,1640383439.0
rnvod9,hpxhdt2,t3_rnvod9,Quite the opposite actually.,3,0,0,False,False,False,1640450625.0
rnvod9,hpxfubp,t3_rnvod9,Nope. The M1 chip is.,1,0,0,False,False,False,1640449822.0
rnvod9,hputyrc,t1_hpusafy,"I see. 
I have one more similar question to ask but I need to send an image so would it be a problem if I dmed you?",4,0,0,False,False,True,1640384281.0
rnuf9s,hpuy59w,t3_rnuf9s,"1 - Practical Discrete Mathematics: Discover math principles that fuel algorithms for computer science and machine learning with Python
- Author: Ryan T. White, Archana Tikayat Ray

2 - Fundamentals of discrete math for computer science: a problem-solving primer
- Author: Jenkyns, Tom A., Stephenson, Benjamin David

3 - Good Math: A Geek's Guide to the Beauty of Numbers, Logic, and Computation
- Author: Mark C. Chu-Carroll

4 - Sets, Logic and Maths for Computing
- Author: David Makinson

5 - Discrete mathematics for computer science: (a bit of) the math that computer scientists need to know
- Author: Liben-Nowell

6 - Math Prerequisites for Quantum Computing
- Author: R. Kumar",5,0,0,False,False,False,1640386414.0
rnuf9s,hpurqpz,t3_rnuf9s,I love [this one](https://www.amazon.com/-/es/K-Dewdney/dp/0805071660). The book cover a lot of important topics in computer science and does it in such a way that is in the middle of divulgation and technical reading. Have fun!,3,0,0,False,False,False,1640383166.0
rnuf9s,hpumzcj,t3_rnuf9s,"If I had to recommend 10 then the list would be (in no particular order):

* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **The Art of Computer Programming** by _Donald Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **Code: The Hidden Language of Computer Hardware and Software** by _Charles Petzold_
* **Software Engineering** by _Sommerville_",7,0,0,False,False,False,1640380787.0
rnuf9s,hpwn2gp,t3_rnuf9s,Humble Pi by Matt Parker is a great bedtime read. Can't guarantee that you would become a subject-matter expert after reading it but can surely guarantee that it would spark an interest in you for learning more. Enjoy reading!,1,0,0,False,False,False,1640429568.0
rnuf9s,hq0btb8,t3_rnuf9s,"I think a lot of good ones were already given, but I'm missing **Elements of Computing Systems**

You get to build a computer and programming language from scratch using only NAND gates. The book provides various simulators and instructions to accomplish this. Gives so much insight into how stuff works and is a ton of fun.",1,0,0,False,False,False,1640509450.0
rnuf9s,hq1qzb9,t3_rnuf9s,"1. Probability and Computing - Michael Mitzenmacher
This book discusses a normal Discrete Math class on discrete probability for the first two chapters and then discusses more advanced results later on. Besides the theory, some applications such as algorithms are also featured to aid in learning. 

2. The Cauchy-Schwarz Masterclass - J. Michael Steele
A must-read if you're into Theoretical Computer Science. I enjoy the ""conversational"" writing style of this book which I found to not be boring and dry. It discusses some interesting results on mathematical inequalities (which I find to be more difficult than equalities, personally).",1,0,0,False,False,False,1640541765.0
rnuf9s,hpv3itj,t1_hpurqpz,The problem with a Turing Omnibus is you never tell if it’ll ever stop…. /s,2,0,0,False,False,False,1640389303.0
rnmo0w,hpt4fc3,t3_rnmo0w,"The role of assembly language is simply that it’s easier to read opcodes like ‘mov eax, 1234h’ than a sequence of hex bytes or an even longer sequence of ones and zeros",41,0,0,False,False,False,1640354577.0
rnmo0w,hpt7jib,t3_rnmo0w,"And to add onto what u/jddddddddddd has said, this readability allows programmers to understand compiler optimizations. That is, being able to read assembly code allows programmers to see how the compiler has optimized the high level code written by high level language.

Not only that, it makes it easy for the programmer to make his/her own optimizations by writing assembly code.

You can think of assembly language as an abstraction to machine language because it's very very difficult and completely ineffective for programmers to understand compiler optimizations/read/write optimizations in machine code. Hence, we have assembly language to give an easier time for programmers.",18,0,0,False,False,False,1640356271.0
rnmo0w,hptr42l,t3_rnmo0w,"It's perhaps better to explain through what machine code is.

Assuming an instruction on MIPS-architecture, that adds two values placed in registers 1 and 2 and places the result in register 6:  000000 00001 00010 00110 00000 100000

This is not very legible for humans so instead human would write assembly like: add $2 $1 $6

Here opcode for add is more legible and operands are clearly readable. Compilers could produce machine code directly, but the assembler to make machine code is usually separate program in the compiler toolchain and higher-level compiler feeds intermediate code (assembly) to the assembler. This means that higher-level code can be more independent of the actual target architecture.

Assemblers like GNU assembler can target many different architectures as well.",10,0,0,False,False,False,1640365722.0
rnmo0w,hptsdef,t3_rnmo0w,"I'd like to add that assembly language commands are 1 to 1 with machine code. That is, for every command in assembly language, the machine is executing one instruction. This is much different than high-level languages which require many operations per line (printing to the console in python, for instance).",9,0,0,False,False,False,1640366304.0
rnmo0w,hputwt8,t3_rnmo0w," At first computers were programmed by putting numbers on punched cards. Folks like John von Neumann programmed this way. It was made easier by the design of the instructions. This carried over to way later, even in microprocessors like the 8086 where it's convenient that the instructions can be split up into groups using octal instruction numbers. So patterns in the numbers themselves stick out like sore thumbs and it becomes easier to think directly about programs using the machine code.

A great abstraction came along with the advent of what are essentially assemblers and linkers. Then you could get away with thinking about things at a higher level and the tools would translate to the appropriate numbers for the processor. At this point CS was really off to the races.",3,0,0,False,False,False,1640384254.0
rnmo0w,hptwg9n,t3_rnmo0w,I think reading an architecture book would help. For me personally trying to understand opcodes and how the memory worked without having the underlying architecture knowledge was just too challenging.,2,0,0,False,False,False,1640368179.0
rnmo0w,hpumih0,t3_rnmo0w,"Having a solid understanding about assembly can help you with things like debugging and reverse engineering, or using disassemblers. Been many years but I've written some patches/cracks for old apps before.    
Idk about the communities anymore but I favored MASM32 last times I was writing code in it.",2,0,0,False,False,False,1640380554.0
rnmo0w,hpvgkty,t3_rnmo0w,They are essentially one and the same. Assembly code is the human readable form of machine language.,1,0,0,False,False,False,1640397114.0
rnmo0w,hpvj0hs,t3_rnmo0w,I had to use assembly in school to understand how the ram works and learn how things like stack and pointers are used,1,0,0,False,False,False,1640398662.0
rnmo0w,hq0mtwc,t3_rnmo0w,Take for example the assembly language Java. It reads lines of code and spews out 0s and 1s. Quite fascinating!!,1,0,0,False,False,False,1640519623.0
rne5t2,hptkf5o,t3_rne5t2,"There is quite a bit of misinformation about asymptotic notation in this thread demn.

 We say f(n)=O(g(n)) if there exist c, n_0 constant such that:

f(n) <= c * g(n) **for all n>n_0**.

Due to the algorithm wont stop for n > 100 we conclude that no such n_0 exist for every g. 

Its a really badly thought out question and it push misunderstanding of the math behind the bigO notation, as the runtime of your algorithm is not O(g(n)) for every g.",16,0,0,False,False,False,1640362628.0
rne5t2,hprvlqt,t3_rne5t2,"Yeah this is a poorly designed question imo. If n > 100, this will indeed run forever.",52,0,0,False,False,False,1640321781.0
rne5t2,hprvnlg,t3_rne5t2,"My first thought was O(n) since it would just count up to 100, but then I noticed that n is a parameter that could start > 100 so you're right that it could be non-terminating. 

Did the test ask specifically for recursiveFunction(1) or some other constantv argument? Or is there a wrapping function that starts it with a specific number?",16,0,0,False,False,False,1640321809.0
rne5t2,hps4s09,t3_rne5t2,"I think while the person authored the problem did not cover input restrictions, the intention of the question is most obviously to ask the time complexity on inputs that halt, which is O(n). 

Your answer is technically correct, but the professor is trying to test your understanding on recursion, so O(infty) looks like a gotcha answer.",10,0,0,False,False,False,1640327390.0
rne5t2,hptgpvv,t3_rne5t2,"It's O(n) for when it finishes.

There is no such thing as O(infinity)",3,0,0,False,False,False,1640360885.0
rne5t2,hprxku7,t3_rne5t2,"**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that n > 100. That is, n can take any number greater than 100.**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.

Edit: Edited my assumptions.",4,0,0,False,False,False,1640322913.0
rne5t2,hpujej2,t3_rne5t2,"Nope, it's O(n) because the worst case is it will terminate with stack overflow (not using tail recursion), and integers are able to address the entire memory space to the largest integer is proportional to the size of memory available to the program. Therefore time complexity is O(n).",1,0,0,False,False,False,1640379016.0
rne5t2,hpv596o,t3_rne5t2,"Well for numbers <= 100 it will be finite if you don’t blow up the stack. Above that it just go on forever, but below that you will have no more than 100 passes.",1,0,0,False,False,False,1640390272.0
rne5t2,hprvfwx,t3_rne5t2,[deleted],-11,0,0,False,True,False,1640321690.0
rne5t2,hps9u2w,t3_rne5t2,"For n>100 you'll get most likely a stack overflow. I'd consider that termination, too.
This, in general, O(n).",-4,0,0,False,False,False,1640331094.0
rne5t2,hps8jp9,t3_rne5t2,[deleted],-3,0,0,False,False,False,1640330116.0
rne5t2,hps3iia,t1_hprvlqt,Are you sure? Isn’t an int 32 bits? I.e. it will wrap around at 2^31 to -2^31,11,0,0,False,False,False,1640326547.0
rne5t2,hprxsvx,t1_hprvlqt,so O(inf) should be acceptable right? I feel like they did not realize it could run forever,6,0,0,False,False,True,1640323046.0
rne5t2,hprwihl,t1_hprvnlg,"the only instruction we got was the following, 

For the given algorithms, specify what is the time complexity of the algorithms using big-O notation. You do not need to write your computations. \[8 points\]

&#x200B;

and this was one of the parts, what I typed in the description was exactly what we got, nothing else

&#x200B;

I wrote O(n) initially then realized it was infinite, I never encountered this problem so I thought O(inf) might be it. any more thoughts? I lost quite a bit there",4,0,0,False,False,True,1640322295.0
rne5t2,hps42hm,t1_hprvnlg,"It would be O(n) under a certain assumption though. It wouldn't be O(n) for every case. If you are interested, you can have a look at the analysis I did below.",1,0,0,False,False,False,1640326911.0
rne5t2,hprz79k,t1_hprxku7,"fwiw, you don't need to restrict your analysis of the O(n) scenario to when n >= -100. Your logic holds if n can take on *any* negative value.",7,0,0,False,False,False,1640323863.0
rne5t2,hprya91,t1_hprxku7,"Thanks for the detailed write up. hmm, so you think even though the worst case scenario is O(inf), we should ignore this and put O(n)?",1,0,0,False,False,True,1640323329.0
rne5t2,hprwsz9,t1_hprvfwx,"Thank you for your thoughts,

Im not really just arguing I’m trying to understand why the solution is like this. We are allowed to ask for a remark because thé TA is the one who marked this not the prof, the remark is done by the prof.

Instead of just gojng in full guns blazing I thought I’d ask here first to not waste the prof’s time.

Do you think my answer is acceptable?

Based on if I’m right my letter grade changes dramatically too as I’m on the borderline between a A and B, so I thought it’s worth asking",3,0,0,False,False,True,1640322465.0
rne5t2,hprx0ti,t1_hprvfwx,[deleted],-5,0,0,False,True,False,1640322589.0
rne5t2,hpth9d5,t1_hps9u2w,O notation is math. Adding arbitrary hardware restrictions is besides the point.,2,0,0,False,False,False,1640361137.0
rne5t2,hpsjs93,t1_hps8jp9,With big O you usually don't take things like stack size into account. Especially because with this function the compiler usually removes the recursion,3,0,0,False,False,False,1640339238.0
rne5t2,hps7vho,t1_hps3iia,"Problems of asymptotics are not constrained by the representation of numbers in a given language. If that were the case, almost any runtime problem would be O(1) since there would be a constant upper bound to what integer the function could accept.",57,0,0,False,False,False,1640329605.0
rne5t2,hptfya6,t1_hps3iia,If the algorithm depends on the bit length of a number then it is exponential in the input size. Just like Knapsack,1,0,0,False,False,False,1640360516.0
rne5t2,hprz03m,t1_hprxsvx,"In my opinion, yes. Especially if the prof didn't give any clarifications about the input domain either.",19,0,0,False,False,False,1640323749.0
rne5t2,hps2fyo,t1_hprxsvx,"It is acceptable but expect to not be awarded marks if you didn't write your assumption. That is, O(inf) is possible when n > 100. Have a look at the analysis I did below.",8,0,0,False,False,False,1640325864.0
rne5t2,hps6l5e,t1_hprxsvx,"No that's wrong. Look, this is just an upside down recursive function. Instead of the base case being at the bottom of the number line, it's above. Any input <= 100 has a defined value.

Well alright. So we have something like a Fibonacci sequence, except it's just one recursive call instead of two. So the calls are one to one, so therefore there is a LINEAR relationship between the value of the argument and the number of calls.",1,0,0,False,False,False,1640328655.0
rne5t2,hps8c4d,t1_hprxsvx,"Generally yes. Technically speaking you can't say O(inf) because it does not make sense to apply an O bound to inf, but the runtime of this function (both best and worst case as well) is infinite.",1,0,0,False,False,False,1640329958.0
rne5t2,hpsnvpt,t1_hprxsvx,"When analyzing an algorithm, you need to consider both the best case and the worst case. You don't just consider the worst case scenario.

And n = 100 gives you the best case and hence it is definitely relevant to the question.

When analyzing an algorithm, you need to be explicit. This is also tested in interviews. It simply makes you a better problem solver/analytical reasoner/Computer Scientist.",1,0,0,False,False,False,1640342763.0
rne5t2,hprwwuz,t1_hprwihl,Did you ask the professor? Maybe they just had a mistake in the test.,3,0,0,False,False,False,1640322526.0
rne5t2,hprzl1n,t1_hprz79k,"Yes, you are completely correct. I was focusing too much on the worst case scenario; that is when n = -100 or when n = -99 and hence why I had that restriction. 

But yes, n can take on any negative value. Thanks for pointing it out. 

I am going to edit the analysis.",3,0,0,False,False,False,1640324086.0
rne5t2,hprzx32,t1_hprz79k,[deleted],1,0,0,False,False,False,1640324286.0
rne5t2,hpryq9i,t1_hprya91,"No, your answer of O(inf) is correct under that assumption only. So, when you are writing your answer, it is very important to say what you are assuming.

So, if you wrote O(inf) and say this is the Big O under that assumption, then you deserve some marks because you were being analytical about the problem. It shows you understand the problem.

However, if you did forget to write your assumption, usually Profs won't reward you marks. That's usually the case.

As I have said above, I am presuming that your Prof has achieved O(n) under a certain assumption. That is when -inf < n <= 100. O(n) is possible when n = -100 or when n = -99.

Edit: Did Prof provide any reasoning behind his answer? You should definitely ask for the reasoning.",2,0,0,False,False,False,1640323589.0
rne5t2,hps0bnf,t1_hprya91,"To build on /u/Classymuch's answer, the reason why collegiate educators are rather stubborn about these things is because in both research and industry you're expected to read between the lines to understand what a (broken) program is doing. So they tend to purposely give you vague questions to see how you handle ambiguity, especially under timed conditions to see how comfortable or prepared you are with the aforementioned task. (Asking questions during exams is always an option!)

For instance, anyone who has PR'd someone else's code before will immediately notice the recursive call's +2 increment and the coincidental 2 base case conditions associated with the aforementioned increment for the constraint n <= 100, and perhaps take steps to repair the code for all integers instead of just n <= 100 with O(|n|). The initiative to take apart and repair a program is what separates a weak programmer from an intermediate one.

To defend my point, job interviews will occasionally ask you to debug or identify a program, and from that discussion, they can see how experienced you are with programming while also seeing what clever tricks you can come up with to simplify or refactor the code. Also, Amazon's leadership principles include ""Dealing with Ambiguity"" as a virtue, and for a interview with Amazon you'll need to prepare past experiences to implicitly demonstrate your ability to deal with ambiguity on problems far more complicated than this problem.

Hope this helps open your mind to these types of problems in future exams! Time permitting, never underestimate the complexity of a problem shrouded by ambiguity.",2,0,0,False,False,False,1640324537.0
rne5t2,hprzav4,t1_hprya91,Ask your Prof for the reasoning of O(n) because it's very important to know how he achieved O(n).,1,0,0,False,False,False,1640323921.0
rne5t2,hps06be,t1_hprya91,"OP, have a look at my analysis again. I had to correct my assumptions. Now they are correct.",1,0,0,False,False,False,1640324446.0
rne5t2,hps1os1,t1_hprya91,"Hey OP, have a look at my analysis for the second time. Sorry, had to edit my assumptions again. It's correct now.",1,0,0,False,False,False,1640325385.0
rne5t2,hprxu3w,t1_hprwsz9,"""I’m on the borderline between a A and B"" I swing my vote then, this is indeed a time to argue!

I extended my answer a bit so go ahead and reread for my perspective.

Overall I think its a really terrible question but i dont think he is wrong to classify it as N.

Unfortunately most tasks don't grow at some smooth rate, and indeed most algorithms have inputs for which they fail.

IMHO this task is likely one where you were expected to apply certain static analysis type reasoning, I.E. the function does or doesn't have some property (such as loops or in this case recursive bifurcations)

I generally consider school a waste of time so my opinion is likely tainted but i do think this question proves little about your knowledge and certainly teaches you even less.

I do not think you will win this one with him, better to look elsewhere for your last mark or two, best luck and keep in mind that a B is a damn good mark, and that your boss will never look at the grades you got in school, he just wants know you can actually do the task he needs done.

Best luck! have fun!",2,0,0,False,False,False,1640323066.0
rne5t2,hpry5wo,t1_hprx0ti,"This is the first time I got a question like this, I think leaving the uni is a bit dramatic but thanks for the suggestion. Like you said sometimes you just get 'bad bosses'.

Yes I agree it is a poorly designed question and maybe whoever made it did not realize that it can run forever. In this case however, how do you even represent that answer? you said O(inf) is a bad answer, but in this case isn't it the only possible answer?",2,0,0,False,False,True,1640323258.0
rne5t2,hpsh7x6,t1_hps7vho,"Huh, so I guess P = NP after all.",1,1,0,False,False,False,1640337050.0
rne5t2,hpuenzp,t1_hps7vho,[deleted],1,0,0,False,False,False,1640376723.0
rne5t2,hptnd33,t1_hps7vho,Great response to a good question.,0,0,0,False,False,False,1640363988.0
rne5t2,hps7xxw,t1_hps2fyo,"You always assume that n is going to infinity in these sorts of problems, there is no need to bound n",12,0,0,False,False,False,1640329657.0
rne5t2,hprx4z7,t1_hprwwuz,"Professor is stubborn. No reply to emails. You can only send a remark request by tonight and say why  the TA made a mistake in your marking. In fact the prof does not even want an explanation just which part you want remarked 

I think I will just send in a remark even if it may annoy the prof",10,0,0,False,False,True,1640322656.0
rne5t2,hps07mf,t1_hprzx32,"If n > 100, it'll run forever.",3,0,0,False,False,False,1640324469.0
rne5t2,hps19s6,t1_hps0bnf,"I just finished my first year in CS and the University I go to has a strong focus/emphasis on algorithmic analysis and they are always about giving vague questions.

This obviously makes internal/external Uni assessments challenging but at the end of the day, it teaches you to be great Computer Scientists/problem solvers in general and great to see that it helps with interviews too.",3,0,0,False,False,False,1640325121.0
rne5t2,hps7li8,t1_hpry5wo,"Yeah bad bosses are best dealt with by finding a good boss.

Infinity isn't really in the domain of the big-O notation.

As i say big O is about the growth of cost rather than the exact value of it.

Given the basic structure of the program (no loops or bifurcation) it is O(n)",1,0,0,False,False,False,1640329401.0
rne5t2,hps0jzw,t1_hpry5wo,"O(inf) is not a bad answer by the way. It's an answer that's definitely correct under the correct assumption. That is when n > 100. Have a look at the analysis I did above. 

I hope it helps you.",0,0,0,False,False,False,1640324679.0
rne5t2,hptvh11,t1_hpsh7x6,Any NP problem is in P if you put a hard upper bound on the size of the problem lol,3,0,0,False,False,False,1640367735.0
rne5t2,hpuggc2,t1_hpuenzp,Depends on how you look at it. Typically we would say that addition in terms of bits it’s logn since as a number grows linearly the number of bits needed to represent it grows logarithmically. However in this type of problem it is normally assumed that addition is a constant operation.,1,0,0,False,False,False,1640377583.0
rne5t2,hps8kv5,t1_hps7xxw,"You have to bound n because answers differ based on how n is bounded. There is no single answer since n can be bounded.

Without bounding n, there will just be a single answer and this is not very analytical.",-4,0,0,False,False,False,1640330140.0
rne5t2,hpu1w8a,t1_hprx4z7,I would do it. His question doesn’t have a well-defined answer unless you specify the O function by breaking it out over the domain of n.,3,0,0,False,False,False,1640370697.0
rne5t2,hps1c9e,t1_hps07mf,"Oh crap, I was right before. lol. Need to edit again.",2,0,0,False,False,False,1640325163.0
rne5t2,hps1lkd,t1_hps19s6,"Excellent, keep pushing forward!",3,0,0,False,False,False,1640325327.0
rne5t2,hptvqfi,t1_hptvh11,Yeah that was the joke,4,0,0,False,False,False,1640367853.0
rne5t2,hpugvd5,t1_hpuggc2,[deleted],1,0,0,False,False,False,1640377787.0
rne5t2,hps8unc,t1_hps8kv5,"No, when asking for the runtime of a problem the bound is implicitly set as n going to infinity. That means that small inputs (in this case 100 for example) are not relevant to the runtime at all. Sure you could bound the input, but that defeats the purpose of this sort of analysis.",8,0,0,False,False,False,1640330340.0
rne5t2,hpsj54p,t1_hps8kv5,"If you bound n and express the complexity in terms of n then trivially every terminating algorithm will have complexity O(1). It really doesn’t make sense to bound n, you just have to look at the definition of O notation to see that.

It makes sense to do case distinctions for algorithms if they behave differently for different inputs but that’s a different thing than straight up bounding the input size.

Also of course it can make sense to bound n and look at properties of your algorithm for bounded n but that’s not the same as asymptotic analysis.",1,0,0,False,False,False,1640338688.0
rne5t2,hpum8ux,t1_hpugvd5,"This is starting to get pretty in the weeds but I suppose it illustrates that asymptotic analysis has nothing explicitly to do with physical computers, it only describes mathematical functions.",2,0,0,False,False,False,1640380418.0
rne5t2,hps9hzd,t1_hps8unc,"When analyzing an algorithm, you need to consider both the best case and the worst case. You don't just consider the worst case scenario.

And n = 100 gives you the best case and hence it is definitely relevant to the question.

When analyzing an algorithm, you need to be explicit. This is also tested in interviews. It simply makes you a better problem solver/analytical reasoner/Computer Scientist.",-3,0,0,False,False,False,1640330833.0
rne5t2,hpso54s,t1_hpsj54p,"""It makes sense to do case distinctions for algorithms if they behave differently for different inputs but that’s a different thing than straight up bounding the input size."" Can you please give me an example of such an algorithm?

I think the confusion I am having is that OP's algorithm expects integers as input. And those are any integers as input. And depending on the integers given, you can have different big O time complexities. 

Hence, why I made restrictions to n in my analysis. 

So, if we are not supposed to restrict n, then that just means the best and worst case is O(inf) as n approaches infinity right?",1,0,0,False,False,False,1640342984.0
rne5t2,hps9ql1,t1_hps9hzd,"Sorry, but this is not quite correct, the best case of this algorithm is still infinite because you still need to analyze as n goes to infinity.

If determining the best case was as simple as picking a value for n, every function would have a best case of Theta(1).

Also this person asked for an O bound, they did not say anything about best or worst case.",7,0,0,False,False,False,1640331017.0
rne5t2,hptcaw9,t1_hps9hzd,[deleted],1,0,0,False,False,False,1640358732.0
rne5t2,hpsreuw,t1_hpso54s,"OPs algorithm is actually one example where doing a case distinction is sensible to do because depending on what n is the runtime is O(n) or the algorithm does not terminate. Though tbh having a nonterminating algorithm as an example for asymptotic analysis is just straight up stupid by OPs prof. 

Fundamentally asymptotic analysis is about how an algorithm behaves as the input size goes to infinity/how it scales with the input size. For example if you have an algorithm in O(n^2 ) then you don't know how quickly it will run for n = 1000 but you can estimate that the time it will take for n = 2000 will be roughly 4 times the time it takes for n = 1000.

>So, if we are not supposed to restrict n, then that just means the best and worst case is O(inf) as n approaches infinity right?

You can make a case distinction for n but that's not the same as bounding n. If you bound n then runtime will be O(1) for any terminating algorithm. However a case distinction without bounding n can give you some better insights.

Consider for example this contrived algorithm:

    f(n):
    if(n = 1): return 1
    else if(n is a power of 2): return 2 * f(n / 2)
    else if(n - 1 is a power of 2): return 2 * f(n - 2)
    else return 2 * f(n - 1)

The asymptotic complexity of this function is O(n). However we can make a restricted analysis and say that we only consider n = power of 2 as input. Then the asymptotic complexity of this function for this restriction is O(logn). However that is not the same as bounding n. If we bound n by for example n <= 10^100 then the algorithm's runtime is O(1).",1,0,0,False,False,False,1640345719.0
rne5t2,hpsb453,t1_hps9ql1,"I think I get what you are saying but bounding the input helps you to analyze the algorithm deeper as I have shown with my analysis. 

As you can see, I have considered the input that goes to infinity but I have said when best case happens and when worst case happens; that is, when n is a particular value.",3,0,0,False,False,False,1640332081.0
rne5t2,hpsasg2,t1_hps9ql1,"Would you say the following is wrong?:

**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that n > 100. That is, n can take any number greater than 100.**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.",2,0,0,False,False,False,1640331828.0
rne5t2,hpsaedl,t1_hps9ql1,"But best case tend to usually happen when the input is a certain value isn't it?

As in, best case = O(1) when n = 100 or n = 99 for instance.

Another example is if we are looking at binary search. Say there is a list that goes from 0 to 100 and I want to check if 50 is in the list. (assume 50 is in the list).

And if I did binary search on that with 50 as my input, best case would be O(1) when mid = 50.

Worst case would be if the number can't be found in the list. That is O(log(n)) where n is the number of elements in the list.

""If determining the best case was as simply as picking a value for n, every function would have a best case of Theta(1)."" But this is why we consider best case and worst case.",0,0,0,False,False,False,1640331525.0
rne5t2,hpuux6u,t1_hptcaw9,"Yes, it is usually the worst case that is considered when talking about big O but for a complete analysis, we do also consider the best case. And not just that, the average case too. 

As in, it's completely correct to consider the best case as well because it gives a better overall analysis.",1,0,0,False,False,False,1640384759.0
rne5t2,hpszo7y,t1_hpsreuw,"""OPs algorithm is actually one example where doing a case distinction is sensible to do because depending on what n is the runtime is O(n) or the algorithm does not terminate.""

But this is exactly what I have been trying to say. That is, depending on how we restrict n, there will be different big O time complexities. Or am I not understanding something?

Ok, I think I get what you mean with the example you have given.",1,0,0,False,False,False,1640351769.0
rne5t2,hpszwqw,t1_hpsreuw,"**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

**So, is the above a case distinction?** 

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**So, is the above a case distinction?** 

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

**So, is the above a case distinction?** 

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**So, is the above a case distinction?** 

**Let's assume that n > 100. That is, n can take any number greater than 100.**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.

**So, is the above a case distinction?**",1,0,0,False,False,False,1640351917.0
rne5t2,hpsbnp1,t1_hpsb453,"Please see my response below, but saying the best case happens when n is a particular value doesn't make sense. Asymptotics just doesn't quite work that way.",0,0,0,False,False,False,1640332502.0
rne5t2,hpsbzj6,t1_hpsasg2,"In broad strokes no, in details yes.

The first two cases do not really make sense to analyze from an asymptotic perspective since they concern small values of n. Asymptotics is all about growth as n approaches infinity. And as I have said in my other responses, you really cannot select a value of n when it comes to asyptotics so the reasoning inside those two parts is misleading. I think you are focusing a lot of specific cases in your responses, but again, we are always talking about general cases here.",2,0,0,False,False,False,1640332763.0
rne5t2,hpsbagy,t1_hpsaedl,"Not quite. Consider your binary search example. Yes the best case is Theta(1) which happens when when we get our number on the first try. However, in this case n is the length of the array we are searching, and it doesn't matter how big n gets, we can still find the number we want in constant time. In order words, in your example, you do not need to fix n to a specific value to get the best case. However, in the above example you would have to do so, therefore these are not the same type of problem. In general asymptotics is all about rate of growth, so fixing n to any one value makes no sense, we care only about how the runtime of our function changes as n grows.

Also I do not understand your response to my last line. Let me give you an example of what I mean by ""If determining the best case was as simply as picking a value for n, every function would have a best case of Theta(1)."" Consider scanning through an array. How long does that take in the best case? Well theta(n) of course, because the best and the worse case of scanning through an array are no different. However, if you take your approach and allow yourself to fix values of n, you could just say the best case of scanning an array is Theta(1), because the size of the array could be 1. Again, this defeats the point of asymptotics because now you are looking at one specific type of array rather than an array in the general case.

One last note: ""But best case tend to usually happen when the input is a certain value isn't it?"" Not really. I think it would be more accurate to say that ""best case tends to happen when something in the data causes a short circuit"", which again is what is happening your binary search example.",2,0,0,False,False,False,1640332215.0
rne5t2,hpuzpxn,t1_hpuux6u,[deleted],1,0,0,False,False,False,1640387248.0
rne5t2,hpt1a2o,t1_hpszo7y,">But this is exactly what I have been trying to say. That is, depending on how we restrict n, there will be different big O time complexities. Or am I not understanding something?

I think this whole discussion boils down to the distinction between bounding and doing a case distinction.

OPs example is really not a good one for this distinction since there the necessary case distinction actually corresponds to bounding n. When I replied to your comment I was more talking about asymptotic analysis in general rather than OPs example.

A case distinction make bounds but in practice that is rarely useful. What I'm mainly referring to is case distinctions like these (considering an input n): n is a prime number; n is a power of 2; n is even or things like that. In all of these cases n is restricted but can still grow arbitrarily large. And n being able to grow arbitrarily large is the important thing since otherwise asymptotic analysis doesn't make much sense.

So bounding can be seen as a form of a case distinction, but that's rarely ever the kind of case distinction we want to make for asymptotic analysis (except for completely stupid examples like the algorithm OP posted).",1,0,0,False,False,False,1640352748.0
rnc33s,hpsaenv,t3_rnc33s,"If you have enough money for mining rigs, you can take over all bitcoins with a 51% attack. Nowadays, most of the global mining hashrate is done by a few individuals. If they work together, they could take over the network.
Bitcoin was never meant to be a productive system. Satoshi mentioned that in their paper.
Also, it's not democratic. Only a handful of people own most of all bitcoins.",9,0,0,False,False,False,1640331532.0
rnc33s,hps6ghz,t3_rnc33s,"The number of practical use cases for blockchains is somewhat limited. A lot of blockchain applications in industry are created by people (encouraged by VCs) who see blockchain as a hammer, and all problems as a nail. Instead of looking for innovative ways to solve problems, they look for innovative ways to use blockchain. In lots of these cases, the problem could be solved much cheaper and simpler by using something other than a blockchain, like a distributed database. See [this](https://eprint.iacr.org/2017/375.pdf) for some limitations (and some benefits) of blockchains.  

Blockchains aren’t even that great at solving the problem they were invented for: currency transfer. The bitcoin network can only handle around 7 transactions per second, while networks like Visa and Mastercard can process tens of thousands per second, all while using orders of magnitude less energy than bitcoin. 

And the energy use of cryptocurrencies can’t be overlooked either: the bitcoin network uses the energy of a mid-sized country, most of which comes from non-renewables. Lots of blockchain fans will say that this can be fixed with proof-of-stake or other consensus mechanisms, but this hasn’t really happened yet (and I’m skeptical that it ever will for major blockchains). Proof-of-stake is also a pretty bad consensus method if your goal is decentralization, as it just rewards the richest, accelerating inequality in your currency. 

Cryptocurrencies also offer little in the way of security compared to traditional banking/credit card systems. If someone steals my credit card and goes on a shopping spree, I can report it to my bank and they’ll close the card and issue charge backs. On the other hand, if someone steals my wallet key and transfers all my cryptocurrency to their account, absolutely nothing can be done to get the money back. It’s gone for good. There are far too many ways for key storage to go wrong for the average person to risk storing a significant amount of money in cryptocurrency. 

Overall, for many applications, there are just better ways to do whatever the blockchain is trying to do. Blockchains are interesting technology, and they can be useful in certain instances, but they’re not broadly useful enough for a “decentralized blockchain led future.”",16,0,0,False,False,False,1640328564.0
rnc33s,hpsc3l6,t3_rnc33s,Terrible for the environment,6,0,0,False,False,False,1640332852.0
rnc33s,hpsh3ni,t3_rnc33s,"Internet connection is required for any verification of transactions (unless you want to download the entire ledger). This doesn't sound like a problem in 2021, but there are still very many cases where internet connectivity is not available.",2,0,0,False,False,False,1640336948.0
rnc33s,hpt3hj6,t3_rnc33s,"1. Anything done by a decentralized system can be done more cost-effectively by a centralized system. This is because a naively decentralized system will require n² connections between its agents while a naively centralized system (a star basically) will have n connections between its agents. This still hold true for more optimized systems (mesh networks vs hierarchies). I'm confident that there is a rigorous demonstration of this, I'll let you look for it.
2. It is true that centralized systems require more trust than decentralized systems, but it would be wrong to think that decentralized systems don't require trust at all. At the very least you have to trust the developers that their code actually implements a fair and robust consensus algorithm. Also, the control of the system's future is concentrated in the hands of those developers.
3. Any real life application of a decentralized system will require as much real life trust as its counterpart centralized system, because you still need to trust the real life agent to deliver on its real life guarantee.

What follows is that the only use cases where decentralized systems may have an edge are those where non physical assets are exchanged between agents that don't trust one another.

For example, Ubisoft recent utilization of NFTs for in-game assets is dumb as hell because it could be easily centralized in Ubisoft's servers. Going decentralized doesn't remove the need for trust because you still have to trust Ubisoft to actually give meaning to those NFTs in-game.

The same thing will apply for any kind of corporation controlled metaverse, even if the assets are in a blockchain, you still have to trust the corporation servers for rendering those assets in the way that was promised when you bought that asset.",2,0,0,False,False,False,1640354042.0
rnc33s,hptxgko,t3_rnc33s,"There's some good applications for it, Things like decentralized patent systems or smart contracts etc. But i doubt any of this ""web3"" stuff is going to happen. Some things just don't benefit from decentralization",2,0,0,False,False,False,1640368639.0
rnc33s,hprix97,t3_rnc33s,Define “future”. I’d wager that the vast majority of people in today’s world have no idea what the blockchain is or how it works. The knowledge hurdles that collective society would have to overcome to adopt public blockchain tech into the  real world would take a long time.,4,0,0,False,False,False,1640315282.0
rnc33s,hpsbz92,t3_rnc33s,"Block chain is a solution to a problem nobody has.

Essentially the problem that block chain solves is ""How can we have secure and distributed records that everybody agrees on?""

And all three of those are already solved non issues. Security? We have RSA and key exchange webs. Distributed? The internet. And that actually scales. Universal agreement? Bruh, what's even the point? What's the point of agreeing on what a bunch of ones and zeros are if people can be free to interpret the meaning or weight of them? Consensus really only comes through power, and power is really only achieved by governments.

Bitcoin was a once in a lifetime exception. Everybody wanted an alternate currency and it was first, but it's essentially a ponzi scheme where the only people who benefit are the first movers.",2,0,0,False,False,False,1640332757.0
rnc33s,hprgwt6,t3_rnc33s,Lack of central control,0,1,0,False,False,False,1640314299.0
rnc33s,hpsnmej,t3_rnc33s,"In the world we live in, it is really difficult to have a decentralization-led future. We can't decentralize everything at least in the near future. I am not a critic of the power consumption of mining cryptocurrencies because the Proof of work consensus algorithm can be replaced with more efficient and decentralized ones in the future(not Proof of stake).

Blockchains solve a problem that doesn't really exist. People are criticizing the big tech companies for using their data to earn money. Most of the people with this critic are members of the herd. Meta and Google are using utilizing your data to sell you ads to make money, and I don't get it what is wrong in that. They have to make money so that their service keeps on functioning.

We need a centralized authority at some point or the other. Suppose you are buying real estate from a person in exchange for ETH. You send the other person 10k ETH on the Ethereum network and then the person denies to transfer the real estate ownership to you. What will you do? You will have to go to law enforcement, which is a centralized authority.

You can only have decentralized in the world of blockchains(that too partially). The whole system breaks when tokens of the blockchain world have to be used to do something in the real world. People are all hyped around decentralization. Decentralization has more demerits than merits.",1,0,0,False,False,False,1640342539.0
rnc33s,hq5lg8y,t1_hps6ghz,"Thanks for this! A lot of interesting points you made. I will definitely check out that paper you cited.

Two things I want to push back on slightly""

1) Is it true that fixing the energy consumption problem with PoS hasn't really happened yet? I know Bitcoin won't ever change, and that Ethereum is taking a really long time to change, but basically every other new and promising ""3rd generation"" blockchain is (Cardano, Algorand, Solana, for ex). I know these chains have their own issues that may stem from the inherent downsides of PoS, but I did want to bring up that they are far more energy efficient than BYC and the current implementation of ETH. 

2) Addressing your point about PoS rewarding the richest. I have considered this and it makes sense on a theoretical level, but for example Cardano has introduced some interesting ways to push back on this (for example, the k parameter which caps the amount of interest a stake pool can earn, which incentivizes decentralization of staked ADA. Have you looked into this at all/have any thoughts? 

Finally, there is a cool game theoretic feature to PoS where in order to attack the network, you would need to control >50% of the networks tokens, but then you are just destroying value of an economy of which you control the majority of. This is in contrast to Bitcoin where the ""resource"" used to assure trust comes from outside the system and thus you don't need to own any Bitcoin to tank the network. 

&#x200B;

Thanks for your reply. Cheers!",1,0,0,False,False,True,1640618947.0
rnc33s,hpycrk0,t1_hpsc3l6,"I’ll expand on this…

Blockchain uses “Proof of Work”, in which someone needs to do work just to prove that they’ve done it.  In particular, the way it works in blockchain is that multiple parties race to see who can complete the proof of work first.  So there ends up being an incentive (usually financial, as in cryptocurrencies) to do more work than others.

Work requires energy.  Not only is that a basic law of physics, but computational work specifically requires electricity.  The production and distribution of that electricity has environmental impacts, primarily (though not exclusively) in the form of carbon emissions, which contribute to climate change.

But is the amount of electricity consumed actually significant in the grand scheme of things?  Well, the bitcoin system itself uses about 121 terawatt-hours (TWh) per year.  That’s about the same as the entire country of Argentina, more than the Netherlands or UAE, and nearly as much as Norway.  For a sense of scale, researchers at Cambridge estimated that about 3-6 million distinct users used bitcoin in 2017.  Norway has a population of a bit over 5 million, so to a very rough approximation, each bitcoin user is consuming as much energy just to use bitcoin as each Norwegian is consuming for their entire lifestyle.  (Caveats:  The other countries are more populace, to varying degrees, so the same cannot be said for them.  Also, it is the bitcoin miners, not every user, who are consuming the energy, so I’m sort of amortizing that expense across all users.)  Or to look at it another way, the energy required to implement a single bitcoin transaction is enough to power an average U.S. household for 24 days.  (And that’s not to mine a whole bitcoin; that’s just for one transaction.)  Compared to an alternative financial system, you can perform 750,000 Visa card swipes for the energy of just one bitcoin transaction.  And all of this is only for bitcoin, not the sum total of all blockchain systems.  So yes, the energy consumption is very significant.

And all of this is fundamental to the concept of proof-of-work.  So it’s not just bitcoin, but all blockchain systems that use proof-of-work will have this problem.  And again, none of this is productive work; it’s just work for the sake of proving that you have done the work.

All of that is just the energy cost, though.  As people build large compute clusters and hardware accelerators (or just use graphics cards), etc., there is a lot of electronic hardware being produced only for the sake of performing this proof-of-work (e.g., mining bitcoins), and there is a significant environmental impact to mining all the minerals required to produce those electronic components.  (Not to mention the social and economic impacts of the way they are mined and manufactured..)",3,0,0,False,False,False,1640466894.0
rnc33s,hpsewtk,t1_hpsc3l6,This,2,0,0,False,False,False,1640335131.0
rnc33s,hps0c1r,t1_hprix97,The vast majority of people in todays world also don’t understand how the internet works. The vast majority of people don’t need to understand something to adopt it.,11,0,0,False,False,False,1640324544.0
rnc33s,hq9vqsd,t1_hpsnmej,"Ever heard what an escrow is?

Too many newbies how want to look intelligent in muh BloCkChAiN",1,0,0,False,False,False,1640697667.0
rnc33s,hq72n1f,t1_hq5lg8y,"It’s true that there are lots of new PoS blockchains, but separate blockchains can’t fix the energy consumption of the big PoW blockchains like BTC or ETH (1.0). PoW blockchains have to be deprecated or drastically shrink. Etherium may be able to do this, but shrinking bitcoin would require a sea change in the blockchain industry, as my understanding is that almost all cryptocurrencies today rise and fall on average with the price of bitcoin. 

>	for example Cardano has introduced some interesting ways to push back on this (for example, the k parameter which caps the amount of interest a stake pool can earn, which incentivizes decentralization of staked ADA.

I’m not familiar with Cardano and hadn’t heard of this, thanks for mentioning it. Do you know what would prevent someone with a large stake pool from splitting it into smaller pools to circumvent the limitations put on large pools?

My overall criticism of these types of features designed to prevent centralization is that if they work as intended, enormous miners/stakers (the kind that prop up most blockchains) would be incentivized to not use that blockchain and move their resources to a more centralization-friendly blockchain that rewards them more.",0,0,0,False,False,False,1640640884.0
rnc33s,hq758gu,t1_hq72n1f,"> Do you know what would prevent someone with a large stake pool from splitting it into smaller pools to circumvent the limitations put on large pools?

So I think you're right in that there is no inherent mechanism to prevent that. I think what the k parameter is meant to do is to incentive people who delegate their ADA to a stake pool to choose another one instead of the stake pool that is aggregating all of the staked resources.",1,0,0,False,False,True,1640642003.0
rmtdj4,hppf77p,t3_rmtdj4,As a suplement I strongly recommend this post: https://www.muppetlabs.com/~breadbox/software/tiny/teensy.html,8,0,0,False,False,False,1640281076.0
rmtdj4,hpokxuz,t3_rmtdj4,"If this is too much text for you, you can also watch this video as an alternative or complement.

[https://www.youtube.com/watch?v=CVg7CYVV3KI](https://www.youtube.com/watch?v=CVg7CYVV3KI)",6,0,0,False,False,False,1640267665.0
rmtdj4,hpq0a6f,t3_rmtdj4,Nice :),2,0,0,False,False,False,1640289978.0
rmtdj4,hpq7qvm,t1_hppf77p,That was an incredible read!,4,0,0,False,False,False,1640293223.0
rmtdj4,hpqp4to,t1_hppf77p,Thanks.,2,0,0,False,False,True,1640300943.0
rmewjn,hplx9o7,t3_rmewjn,I would guess search engine could put a nsfw tag for certain key words to adjust weight,2,0,0,False,False,False,1640209741.0
rm1nxm,hpjm9nq,t3_rm1nxm,"It's not about one machine / language, but *Code: The Hidden Language of Computer Hardware and Software* by Charles Petzold is really good for a broader understanding.",61,0,0,False,False,False,1640171943.0
rm1nxm,hpjm9s6,t3_rm1nxm,"Code: The Hidden Language of Computer Hardware and Software https://g.co/kgs/JTNx9r

This book talks about the origins of “code”, going from Morse code through to binary, etc.",13,0,0,False,False,False,1640171946.0
rm1nxm,hpk7r8a,t3_rm1nxm,I think you'll like this much more than a book if you want only an overview of computer history - [Youtube - Crash Course (Computer Science)](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo),7,0,0,False,False,False,1640184500.0
rm1nxm,hpjm6c2,t3_rm1nxm,I’m interested in this too!,7,0,0,False,False,False,1640171871.0
rm1nxm,hpjneio,t3_rm1nxm,Rage inside the machine by Robert Elliot Smith. A really good book mainly about the biases we build into our systems but covers a large part of computation history going 1200 to Babbage and industrial revolution to training fighter pilots,7,0,0,False,False,False,1640172803.0
rm1nxm,hpjqq6t,t3_rm1nxm,"[History of Computing ](https://en.m.wikipedia.org/wiki/History_of_computing)

Read this then check out the references and external links section for deeper dives.",3,0,0,False,False,False,1640175139.0
rm1nxm,hpkf0lz,t3_rm1nxm,"FWIW it’s a complex, multifaceted history that involves a lot of mathematicians, people interested in logic, those who just want to “compute” in the sense of basic calculations centered around all kinds of things, etc. Eventually you’d get back to abacuses, star charts, etc.
  
So you’re unlikely to find a *single* comprehensive book.  
  
You might want to start with **digital computers** and bypass the mechanical/analog stages of computing history. 
  
At the same time you might want to read a book or two on Alan Turing, since the concept of a *Turing Machine* is essential to modern computing.",3,0,0,False,False,False,1640187688.0
rm1nxm,hpm6b87,t3_rm1nxm,"I'm no expert in the history of computing, but the invention of programming languages and modern computers wasn't something that was done in terms of a ""straight linear process."" Instead, many scientists and engineers worked together to create automation technology for mathematical constructs. Think of the evolution of the computer akin to the evolution of Homo Sapiens. A chimp didn't just give birth to a human. Instead, through a long period of time and many small changes (along with other factors), chimps reproduced enough times to get to the human (__PLEASE NOTE: I AM NOT A BIOLOGIST AND THIS IS A REALLY OVERSIMPLIFIED VERSION OF BIOLOGY__). Likewise, through many many inventors and scientists (in)directly working together trying to solve scientific/engineering problems related to their time period, the technology of modern computing came to be.

I would recommend a firm foundation in CS; try the books ""[How Computers Work](https://www.amazon.com/How-Computers-Work-Evolution-Technology/dp/078974984X)"" or ""[Computer Science Illuminated](https://www.amazon.com/Computer-Science-Illuminated-Nell-Dale/dp/1284155617).""

Then you can look history books of computer scientists s.a. Charles Babbage, Alan Turing, Ada Lovelace, etc. Unfortunately, I am not familiar with the history of CS as much as I should be, so I can't help you here :-(",3,0,0,False,False,False,1640213598.0
rm1nxm,hpjr5xq,t3_rm1nxm,Turing’s Cathedral or Darwin Among the Machines both by George Dyson,2,0,0,False,False,False,1640175433.0
rm1nxm,hpkmz0i,t3_rm1nxm,The innovators by Walter isaccson,2,0,0,False,False,False,1640190980.0
rm1nxm,hplnu36,t3_rm1nxm,I think it's called the innovators by Walter isaacson,2,0,0,False,False,False,1640205875.0
rm1nxm,hpmf8zy,t3_rm1nxm,The Innovators by Walter Isaacson is a great book.,2,0,0,False,False,False,1640217660.0
rm1nxm,hpn1g27,t3_rm1nxm,There’s a Grace Hopper memoir that has a ton of history in it.,1,0,0,False,False,False,1640228264.0
rm1nxm,hpnw15g,t3_rm1nxm,The Cryptonomicon is a fun novel that touches on this subject a bit. But I came here to recommend Bertrand Russell’s type system for logic as an early example and influence on Turning’s contributions to the field.,1,0,0,False,False,False,1640249810.0
rm1nxm,hpjt80g,t1_hpjm9nq,One of the best books I've read.,11,0,0,False,False,False,1640176748.0
rm1nxm,hpjnuea,t1_hpjm9nq,"Yeah gotta agree here, great read if you want to broaden your K&U of the subject",4,0,0,False,False,False,1640173128.0
rm1nxm,hpk1cob,t1_hpjm9nq,As I read every chapter my appreciation and gratitude increased.,3,0,0,False,False,False,1640181444.0
rm1nxm,hpk261t,t1_hpjm9nq,"Awesome, just ordered",3,0,0,False,False,False,1640181858.0
rm1nxm,hpk4htn,t1_hpjm9nq,"I love this book and can't recommend it enough, but I don't feel like it's an in-depth history lesson of computing. It does mention important figures and technologies, but doesn't get into the nity-grity and instead is focused on the ""why"" of programming languages.

Still, amazing book. Read it in a few of nights because of how fun it was.",2,0,0,False,False,False,1640182994.0
rm1nxm,hpk9zkx,t1_hpjm9nq,I second this.,1,0,0,False,False,False,1640185509.0
rm1nxm,hprv2c0,t1_hpjm9nq,This book is required reading for every computer nerd of every stripe. Came here to recommend it.,1,0,0,False,False,False,1640321477.0
rm1nxm,hpjqrym,t1_hpjqq6t,"**[History of computing](https://en.m.wikipedia.org/wiki/History_of_computing)** 
 
 >The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",3,0,0,False,False,False,1640175172.0
rls9mc,hpibugc,t3_rls9mc,">since the new minimum node will be the parent node, setting the pointer to the new minimum also takes constant time.

I believe the issue here is that there is often more work to be done after removing, in order to rebalance the tree.

If you look at the first example here:

https://www.javatpoint.com/balanced-binary-search-tree

If you simply remove the minimum element, your tree is no longer balanced. And it is not as simple as simply re-assigning the parent. In the case of the example, you would have to reassign at least two other elements.",9,0,0,False,False,False,1640141345.0
rls9mc,hpi36fm,t3_rls9mc,"Uhh. Huh? You have to search for the node to delete...
That's why it's o(log(n)) because worst case you find the node as a leaf node which is the height of the bst number of iterations... 

Only way for the node to delete be o(1) is if your bst is just one node.",4,0,0,False,False,False,1640137453.0
rls9mc,hpibclr,t3_rls9mc,"I'm sorry.

The parents didn't realize the cost at the time.

There are multiple solutions to this, I really hope you find yours.  Memoization or Parallization, or whatever.

There may still be a cost in memory.  Best of luck to you.",-5,0,0,False,True,False,1640141133.0
rls9mc,hpik8fu,t3_rls9mc,"He's talking about a priority queue (i.e. min heap), not a balanced BST. Finding and removing the top node from a minheap is O(1) (it's the root). However, maintaining the heap invariant (aka finding the new min) requires looking at its direct children and bubbling up the nodes to fill in the gap, which is O(log(N)). O(1) is dominated by O(log(N)).",1,0,0,False,False,False,1640145317.0
rls9mc,hpkvq4v,t1_hpibugc,"Okay yea that's where my error r
Lies. I made the assumption that deleting from a balanced bst given a pointer to the node would take constant time. Guess it makes sense that balanced bst would do some more work to balanced tree after a deletion. Thanks!",1,0,0,False,False,True,1640194520.0
rls9mc,hpi7v8k,t1_hpi36fm,"Not the OP, but the passage in the book mentions storing a pointer to the minimum value. Then it mentions that because of this finding the minimum value is O(1), but deleting that same value is O(log(n)).",3,0,0,False,False,False,1640139579.0
rls9mc,hpkw1je,t1_hpi36fm,U didn't read the question well. You don't need to search for the node to be deleted if you keep track of the minimum node in a separate variable of its own.,1,0,0,False,False,True,1640194649.0
rls9mc,hpidgl7,t1_hpi7v8k,"You have to reorganize the bst after deleting a node and that entails traversing down the right most child or left most child of the deleted node iirc.

So if you're deleting the root node, you'll still have to traverse the height of the bst to reorganize the bst.",3,0,0,False,False,False,1640142074.0
rls9mc,hpm4u55,t1_hpkw1je,"You still have to rebalance the bst after deletion of the node as I've explained already. 

You have to traverse the height of the tree to replace the node being deleted.",1,0,0,False,False,False,1640212952.0
rma55n,hpmjuqi,t3_rma55n,"I don't recall ever hearing that the logical operations are mainly composed of AND, XOR, and NOR. Generally at the gate level, complex designs are mainly built from NAND or NOR gates because they are ""universal"", i.e. you can build any logic design you want with enough NAND gates. Using a single gate type has some benefits when it comes to the physical layout and fabrication of your design, but the fact that your logic will be implemented in this way can be ignored for simple cases.

It's also worth noting that you don't always design logic at the gate level. Things like multiplexers, priority encoders, queues, etc. will make it easier to design complex logic structures.",2,0,0,False,False,False,1640219823.0
rma55n,hpq70xw,t3_rma55n,Try building one yourself. There's a great book called The Elements of Computing Systems which just got a new edition.,2,0,0,False,False,False,1640292909.0
rma55n,hqtnqmc,t3_rma55n,"An ALU can be a multiplexer with, say, 2 select bits, 4 inputs to select from (add, subtract, and, or) and one output. The add input comes from the adder (and is selected when the select signal is, say, 00), the subtract input (select 01) comes from the adder as well but one of the inputs to that adder will be changed to 2s complement beforehand, the and input (select signal 10) comes from an and gate with a and b as inputs, and the or input (select signal 11) comes from an or gate with a and b as inputs. 

If the design is based on the ttl logic family, you would see and & or gates. On the other hand, cmos logic prefers nand & nor gates, and with some minor changes to the circuit (bubble pushing for example), achieve the same end result.",2,0,0,False,False,False,1641056991.0
rma55n,hsd2wly,t3_rma55n,"I think I read in a book some time ago that all of the not gates like NAND, NOR take less transistors to build. That could be part of the reason",2,0,0,False,False,False,1642003923.0
rma55n,hpnex9n,t3_rma55n,"AND, NOR, and XOR gates have much faster “processing” times than AND, XOR, and NOR. This is because of how the gates are actually implemented on the hardware.",1,0,0,False,False,False,1640237531.0
rma55n,hplsy3f,t3_rma55n,Following for the answer,-1,0,0,False,False,False,1640207973.0
rma55n,hpn2aa4,t1_hpmjuqi,"I’ve heard about NOR and NAND being universal, but it’s it really all that common to use them exclusively? I’d imagine using just those gates would end up adding many extra gates than if you were to use the whole variety. Wouldn’t it be cheaper to use less gates? Or at this low of a level is it truly a negligible cost?",2,0,0,False,False,False,1640228667.0
rma55n,hqto0sn,t1_hqtnqmc,"This is a great answer, thanks for writing this up!",1,0,0,False,False,True,1641057111.0
rma55n,hpq6rdk,t1_hpnex9n,It's all just turtles all the way down but some of these use more turtles than others.,1,0,0,False,False,False,1640292793.0
rma55n,hpniwh8,t1_hpn2aa4,"Not necessarily correct, it depends on the technology you use to implement those gates, and the purposes of design. I remember that generally, the nand have smaller area in cmos technology and faster response time. In digital world, we only know 0, 1 but in real world, it takes time to change from 0 to 1and stable at 1 and vice versa. It's also easier to layout efficiently a design from same blocks rather than multiple shapes and sizes",3,0,0,False,False,False,1640240046.0
rlfsh9,hpfibxk,t3_rlfsh9,"Structure alignment tries it's best to align structures to fall within certain boundaries. Specifically memory words. Most address schemes are aligned on word boundaries so if you don't align to them you have to have an (another) offset to find the start of a structure/ field, which takes more instructions and time. You also have caches to deal with, which always read in a certain amount of words, typically called a cache line with is typically measured in memory blocks. The same thing happens if you don't align fields in your structure to at least word boundaries. It can be very expensive if you go across cache line boundaries for a field, especially if one of your lookups results in a cache miss.

 Now like you said, a lot of structures are too big to fit in a cache line. But it still helps to align their fields to a word boundary because you still don't want to also have to figure out the offset within the word. This is why structs in c/c++ will pad fields typically to boundaries of the highest native type. It just makes book keeping easier/faster.

Us humans operate the same way. If you are counting by 4s it's much easier to count by 4 all the time than to count by 4 to a certain point then change to counting by ones.",9,0,0,False,False,False,1640098774.0
rlfsh9,hpfwyvm,t3_rlfsh9,"Caching is a huge factor of why you dont want your data striped across memory. The ability to fetch data from the cache is often times order(s) of magnitude faster than fetching from memory, which is in turn order(s) of magnitude faster than fetching from a hard drive (HDDs at least). So reducing the number of times you have to go get data from somewhere else, the better.

If you are dealing with very large datasets, where you can’t fit all of the data into the cache, then knowing how data is read from memory/disk becomes more important. For example, execution times while looping over large 2D arrays will vary widely depending on which array is iterated in the outer loop, and which array is iterated in the inner loop.",6,0,0,False,False,False,1640104961.0
rlfsh9,hpghkr4,t3_rlfsh9,"I think you have missed something in your example.

If you have a struct with an 8byte field (e.g. a 64-bit integer) and a one byte field (e.g. an 8-bit integer) then on CPUs which have alignment restrictions the compile will add padding at the end of the struct so that when multiple structures are put in an array all fields can be accessed efficiently.

Usually the worst case for alignment restrictions is “natural” alignment which means in this case that the 8byte field must be 8byte aligned. The compiler would then create a struct with { 8 byte field, 1 byte field, 7 bytes of padding }. This ensures that the size of the struct is an even multiple of 8 so if you create an array of them the 8byte field alway lands on a even multiple of 8 byte offset from the start of the array.

Edit: adding explanation for why alignment makes things more efficient.

What is not obvious is WHY aligning fields makes things more efficient. Adding padding to structs “wastes” memory, so it actually makes things LESS cache efficient in general.

At a surface level the answer is that the actual binary CPU instructions have these alignment restrictions built in, so if the compiler wants to access a field it can do it with only a single instruction if it knows the field is aligned. If the compiler know the field is not aligned, then it must compile the unaligned access into multiple instructions that access smaller fields and then more instructions to combine these small access back together into a register with the whole value. Clearly one instruction vs multiple is more “efficient”.

Going deeper one may ask WHY would some CPU instruction sets have this kind of alignment restrictions? Not all CPUs have these restrictions, so why do some? The answer is that these restrictions make the HW simpler, and simpler HW takes less power/room leaving more left over for enhancing performance in general. For example on how aligned access make HW simpler: no access will cross a cache line or page boundary. This means that any aligned load/store maps to exactly one cache line and exactly one page and will never be split accross two of either. HW can be built with this simplification in mind.",1,0,0,False,False,False,1640113046.0
rlfsh9,hpi7g6w,t3_rlfsh9,"I have no idea what a data structure is, but I’m eager to find out. I’m taking thr FCC JavasScript course.",1,0,0,False,False,False,1640139388.0
rlfsh9,hphyn8v,t1_hpfwyvm,"> For example, execution times while looping over large 2D arrays will vary widely depending on which array is iterated in the outer loop, and which array is iterated in the inner loop.

Yup, I took an operating systems course and one of our assignments involved refactoring slow code to optimize iterating over a 2D array. Just changing the outer and inner loops of array access decreased the time of the program from ~30 seconds to about 10 seconds.",1,0,0,False,False,False,1640135372.0
rlfsh9,hqdx0l5,t1_hpfwyvm,"> Caching is a huge factor of why you dont want your data striped across memory

Why is data striped across memory blocks not good for caching?",1,0,0,False,False,True,1640764266.0
rlfsh9,hqe3u1i,t1_hqdx0l5,"At a very basic level, scattered/striped data = more reads/writes performed.

I was going to type out a long explanation but im sure there is better, more verbose documentation out there. 

Like this! https://youtu.be/247cXLkYt2M",2,0,0,False,False,False,1640769668.0
rlfsh9,hqezu1z,t1_hqe3u1i,"That video was a great watch. I always thought that there was not very informational content on YouTube, but well this video defies my belief. I did not fully understand the caching and stuff in the video but it of course gave me a starting point for learning. Thanks a ton!",1,0,0,False,False,True,1640790266.0
rlfsh9,hqigj4q,t1_hqe3u1i,"I had a question. I understood that data stored contiguously in memory is great for prefetching and caching, but how is memory aligned data good for caching?",1,0,0,False,False,True,1640842486.0
rm2a30,hpjtgy3,t3_rm2a30,There are giant bundles of fibre optic cables running through the Atlantic Ocean connecting Europe and North America.,7,0,0,False,False,False,1640176910.0
rm2a30,hpkgowc,t3_rm2a30,"It’s like a funnel, there is a big single point of access to jump across contents so you will ping your local area, then the dns host in that region which communicates to a layer above it to be sent across the ocean. That’s not the exact path but the hops will go from general to specific then specific to general",3,0,0,False,False,False,1640188391.0
rm2a30,hpjnu9r,t3_rm2a30,Tracert works by pinging and sending packets specially time live packets. These packets then die or drop when there’s no response. The packets used are ICMP packets which then respond back to the sender of the ping. This resides in layer 3. In short tracert maps out routers and connections to that host and is used for network maintenance to check that routers or servers are live and working through the network.,2,0,0,False,False,False,1640173125.0
rkr2j4,hpb9qot,t3_rkr2j4,(deleted),81,0,0,False,False,False,1640019118.0
rkr2j4,hpb9bnr,t3_rkr2j4,">wiki says he's a established physicist while his books and his career speaks he's a computer scientist.

But the Wikipedia article devotes only _one_ sentence to physics? Everything else is about Computer Science",10,0,0,False,False,False,1640018949.0
rkr2j4,hpbtk3f,t3_rkr2j4,"There is no surprise in it. Computer Science Engineering as a separate discipline was founded at my former university somewhere between 1995-1997, before that people learned Electrical Engineering, and part of it programming and related disciplines. So, a Computer Science Engineering was an electrical engineer specialized to computer programming, the software part for a couple of years. The other direction was the programmer mathematician, which was taught at another University, where a programmer mathematician was a mathematician specialized in programming.",7,0,0,False,False,False,1640027147.0
rkr2j4,hpdwqwx,t3_rkr2j4,"Once you realise that your degree is supposed to teach you only one skill : Learning how to learn, that is the point you achieve ultimate freedom. You can learn anything you want after that and with suitable access to facilities or not you can research in any domain you like.

There are countless examples.",6,0,0,False,False,False,1640060489.0
rkr2j4,hpbglr9,t3_rkr2j4,"As others have stated, CS as a study on its own really wasn't a thing yet. The first CS programs only really started becoming a thing in the 60s when he first graduated. Instead, what we consider CS today would have fallen under mathematics, or electrical/computer engineering. And a lot of early programmers would have been in those fields or others like physics due to needing computers to solve certain equations. Back then learning to program would just be reading a manual that came with the giant IBM or whatever, not some actual formal education.

If you want to grasp how someone coming from just math/physics was able to then also have such insight into the more complex CS before it was a standard thing, then I would recommend reading The Art of Computer Programming. It's the densest literature you can read in CS, but it shows how mathematics is the basis to create the major CS topics of today. You can really see the natural progression of the author, Donald Knuth, from mathematician to computer scientist.",8,0,0,False,False,False,1640021923.0
rkr2j4,hpbn8yd,t3_rkr2j4,"I teach CS at university and I'm self-taught with no CS degrees. It's not like you have to get a CS degree to study CS -- true of any subject really. You just need the motivation, dedication, and understanding of how to teach yourself stuff.

CS itself is a highly interdisciplinary field. You can tell from the amount of technical terms and jargon in CS that's been taken (and bastardized) from other fields like philosophy, math, linguistics, and so on. So, often you can pivot from another discipline, using the expertise you have there to start in some related topics in CS. For example, I have a background in formal logic and that's where I started teaching myself about CS.",13,0,0,False,False,False,1640024598.0
rkr2j4,hpc0vf3,t3_rkr2j4,He's more than that too. Started https://electoral-vote.com roughly 20 years ago. So you can add Poli-Sci as another interest.,3,0,0,False,False,False,1640030168.0
rkr2j4,hpcbtlv,t3_rkr2j4,"> I'm curious as to how he got the expertise to write books and do research on OS? 

A question you should ask about any academic! His [CV](https://www.cs.vu.nl/~ast/home/cv.pdf) is here, feel free to read it :)",3,0,0,False,False,False,1640034726.0
rkr2j4,hpf9ykb,t3_rkr2j4,"\[Edit: The post below is about Knuth, not Tannenbaum. Totally misread the question\]

I was lucky enough to attend one of his lectures. He is an incredibly humble individual, who has the ability to narrow his focus to be laser-like, and just do one thing and learn it entirely. I believe that's how he became who he is - by focusing on one thing, and not getting distracted.

During the lecture he was saying he stopped reading email ~~in the late 90s~~ \*since January 1st, 1990 (source: [https://www-cs-faculty.stanford.edu/\~knuth/email.html](https://www-cs-faculty.stanford.edu/~knuth/email.html)) and never went back. I doubt he has a smart phone that pings away his concentration every few seconds, or several meetings scheduled throughout the day.

He just picks up a subject, goes deep into it, and comes out the other end as a master and having created something useful out of it.

I'm sure there's plenty of subjects he approached where he could not produce something, so we might see only a selection of the successful ones. Still, what he can do is impressive.",2,0,0,False,False,False,1640094797.0
rkr2j4,hpdt2ns,t1_hpb9qot,"To be fair, C was standardized in 1989 but was created in the early 70s.

Still came after his physics degree though LOL",8,0,0,False,False,False,1640058671.0
rkr2j4,hpdv6dj,t1_hpb9qot,"People were writing in C long before it was standardized. The K&R book was first published in 1978, so C was already fairly mature by then.",7,0,0,False,False,False,1640059692.0
rkr2j4,hpblwd0,t1_hpb9bnr,Wikipedia is generally very slanted towards computing topics.,7,0,0,False,False,False,1640024057.0
rkr2j4,hpegbe1,t1_hpbtk3f,"Yes but his research is also on core physics stuff. 
His thesis was:  A Study of the Five Minute Oscillations, Supergranulation, and Related Phenomena in the Solar Atmosphere. 
And I don't think it needs much core CS knowledge like OS ,computer architecture etc. 🤔
He must have gotten very interested in the emerging CS field and decided to invest lots of time in learning it. 🙂",1,0,0,False,False,True,1640073080.0
rkr2j4,hpcb3ua,t1_hpbn8yd,100 times this.,1,0,0,False,False,False,1640034427.0
rkr2j4,hpbyzof,t1_hpbn8yd,Any degrees besides CS?,1,0,0,False,False,False,1640029373.0
rkr2j4,hpe9xmc,t1_hpcbtlv,Thanks 🙂,1,0,0,False,False,True,1640068335.0
rkr2j4,hqbd58m,t1_hpf9ykb,[deleted],1,0,0,False,False,False,1640720445.0
rkr2j4,hpcbw1f,t1_hpblwd0,"True, but Tannenbaum himself barely talks about his physics experience on his site or CV. He talks almost entirely about his computing and OS research.",5,0,0,False,False,False,1640034754.0
rkr2j4,hpek746,t1_hpegbe1,"Yes, agree, I think this is the way he went. TBH I think research wise it is more useful to have a math or physic degree.",1,0,0,False,False,False,1640076242.0
rkr2j4,hpc4wi4,t1_hpbyzof,"When you're talking about degrees that are reasonably common among people working in industry in ""CS-like"" fields, the heavy hitters are CS (obviously), electrical engineering, computer engineering, physics, and mathematics. But you'll see working engineers with all sorts of backgrounds. It's one of the easier fields to be self-taught in to a degree that lets you do most technical work that employs graduates.

For professors, could be anything. A PhD isn't like a BS where you get a degree that intends to certify that you know the core material for a given field. A PhD intends to certify that you know how to do research. If you have a PhD in Folklore and you decide you want to be a computer science professor, you just need to publish in Computer Science conferences and journals for a while and then you're a computer scientist. However you acquaint yourself with the knowledge needed to get your papers accepted is irrelevant. Universities don't require a CS PhD to be a CS professor (or any other field). You need a PhD in literally anything and then you need people in the field you want to join to say, ""That person is awesome at my field. I've read their work and it's great"".",8,0,0,False,False,False,1640031836.0
rkr2j4,hpcc1xo,t1_hpbyzof,My degrees are in a humanities and I taught and did research in that field for several years.,3,0,0,False,False,False,1640034822.0
rkr2j4,hqc1s01,t1_hqbd58m,"According to [him](https://www-cs-faculty.stanford.edu/~knuth/email.html),   


>if you want to write to me about any topic, please use good ol' snail mail and send a letter to the following address:\[...\]  
I have a wonderful secretary who looks at the incoming postal mail and separates out anything that she knows I've been looking forward to seeing urgently. Everything else goes into a buffer storage area, which I empty periodically.  
My secretary also prints out all nonspam email messages addressed to \[...\]  so that I can reply with written comments when I have a chance.",1,0,0,False,False,False,1640730513.0
rkr2j4,hpefkyn,t1_hpcbw1f,"Yes it's true. His phd thesis is also on A Study of the Five Minute Oscillations, Supergranulation, and Related Phenomena in the Solar Atmosphere. 

I don't think this requires much core CS knowledge like OS etc. Man this guy did hardcore physics and computer science together.",1,0,0,False,False,True,1640072509.0
rkr2j4,hrvnvxq,t1_hqc1s01,[deleted],2,0,0,False,False,False,1641706526.0
rkr2j4,hqc1t9n,t1_hqc1s01,"It seems that your comment contains 1 or more links that are hard to tap for mobile users. 
I will extend those so they're easier for our sausage fingers to click!


[Here is link number 1 - Previous text ""him""](https://www-cs-faculty.stanford.edu/%7Eknuth/email.html)



----
^Please ^PM ^[\/u\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20hqc1t9n)",1,0,0,False,False,False,1640730528.0
rkr2j4,hpeon9g,t1_hpefkyn,"Now, watch this: 

Mathematician turned into computer scientist:

https://en.wikipedia.org/wiki/Barbara\_Liskov",2,0,0,False,False,False,1640080018.0
rkr2j4,hs1l1lr,t1_hrvnvxq,"Wow, what a blunder. Whops.",1,0,0,False,False,False,1641807703.0
rkr2j4,hs3fnz0,t1_hs1l1lr,[deleted],1,0,0,False,False,False,1641839195.0
rkr2j4,hs6smmq,t1_hs3fnz0,"Sorry for being the cause of ""I've read it on the internet so it must be true"" :D",1,0,0,False,False,False,1641894292.0
rkf6jh,hp9kxq7,t3_rkf6jh,"Hmm maybe this is a little higher level than what you want, but Ben Eater on youtube has a series where he builds a 65c02 based computer from scratch using a 6502 microprocessor.",16,0,0,False,False,False,1639980849.0
rkf6jh,hp9ohgn,t3_rkf6jh,https://www.amazon.com/8088-Project-Book-Robert-Grossblatt/dp/0830602712,3,0,0,False,False,False,1639983339.0
rkf6jh,hp9vnc5,t3_rkf6jh,"What about building something around the raspberry pi RP2040? designed for this purpose, A lot simpler than a full blown ARM64/GOU SoC, I imagine, and with luck there’s a community around it.",3,0,0,False,False,False,1639988955.0
rkf6jh,hp9qzny,t3_rkf6jh,[here you go](https://www.nand2tetris.org/)  . though you need to learn bit of EE for practical purrposes. Buy Andre Lamothe's course from udemy( cheaply available). And for theory of EE search for JIM PYTEL on youtube and start with his DC electronics part 1 playlist.  Best wishes.,2,0,0,False,False,False,1639985210.0
rkf6jh,hpa6bpm,t3_rkf6jh,"[stack computers](https://users.ece.cmu.edu/~koopman/stack_computers/index.html) 

[hdl resource](https://hdlbits.01xz.net/wiki/Main_Page) 

In stack machines chapter 3.2 there is a basic design with instruction specs that can be implemented in verilog for a fpga without too much difficulty. 

in terms of installing a os on the computer I'm not sure how honestly [os dev](https://wiki.osdev.org/Expanded_Main_Page) might have some resources I imagine that unless you replicate a existing chip design you'd have to edit(make?) the back end of a compiler to use a existing os (am unsure) os dev might have resources and their reading list is pretty good regardless. 

I'm working on something similar (I think) it *might* be easier to first make a small interpreter that serves as the os like forth that's what I'm looking into but haven't tried yet. 

also you could maybe *maybe* use that system to make a tiny vm to boot up a operating system using a vm that uses another computers instruction set as its optcode. 

you're more qualified than I am. I'm just a hobbyist I don't know if that's what you're looking for. I just remember looking to do what sounds similar running into a lot of dead ends and wasted time hope those help.

is there any particular fact about computers you feel you understand particularly poorly?",2,0,0,False,False,False,1639997987.0
rkf6jh,hr13yt3,t3_rkf6jh,"Hello there, I actually have developed some small microprocessor platforms in the past and am putting one together in 2022 for a medical device I am designing. 

My advice to you is to start small. Look at the minimum components necessary to run a fairly simple microprocessor and move up from there. For example, maybe start with a simple Atmel chip (e.g. 328) or even a TI chip like the MSP-430. 

If you have no patience for that, and want to jump right in, I recommend looking at the Atom series x86 platform. The minnow board project is open and you can get right into customising your own platform: [https://www.minnowboard.org](https://www.minnowboard.org)",2,0,0,False,False,False,1641182683.0
rkf6jh,hp9fq5m,t3_rkf6jh,[deleted],1,0,0,False,False,False,1639977536.0
rkf6jh,hpakxty,t3_rkf6jh,"I recommend building something compatible with a beagle bone or raspberry pi operating system (ie clone the hardware).

Otherwise, if this is your first or one of your first boards, you may want to consider an easier project first. I have done this exact thing as an electrical engineer with a few years of experience and it was still a challenge to me. Took about 200 hours of work, and needed impedance control (see advanced pcb building techniques). The board has about 350 individual components.

Osd has some good ressources on how to build a beagle bone clone using the osd3358. Read the tutorials!

Best of luck and feel free to pm me if you have any questions",1,0,0,False,False,False,1640007815.0
rkf6jh,hpb31ln,t3_rkf6jh,"Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)

Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

[Ben Eater's Build a 6502 computer](https://eater.net/6502)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",1,0,0,False,False,False,1640016317.0
rkf6jh,hpnbe70,t3_rkf6jh,"Start small.  Design a simple 8bit ALU first and understand how they work, then add a verilog memory module, peripherals, etc.",1,0,0,False,False,False,1640235445.0
rkf6jh,hpadsoi,t3_rkf6jh,"> I am an electronic engineer

A student? Or an actually employed EE? I ask because I'm wondering who employs EEs these days to design things with nothing but lights and switches! :)

Do you want a modern SBC, e.g. with an Arm, or are you happy with some random 8bit thing?",0,0,0,False,False,False,1640003569.0
rkf6jh,hpb3jxz,t1_hp9kxq7,He also has a 8-bit computer from scratch series and kit.,4,0,0,False,False,False,1640016534.0
rkf6jh,hpe5tw2,t1_hp9kxq7,Second for Ben Eater - he has some really fantastic content!,1,0,0,False,False,False,1640065649.0
rkf6jh,hp9in93,t1_hp9fq5m,"Essentially like a raspberry pi.

I’m fine using an existing operating system. Developing my own operating system and SBC I feel like would become unrealistic for one individual. Or at least it is not in my scope of interest at this time.",2,0,0,False,False,True,1639979343.0
rkf6jh,hpcbn6o,t1_hpadsoi,"Electronic engineers don’t do much designing. I do testing, program ics, build based on schematics, troubleshoot boards and test systems, design fixtures and some other stuff.

Electrical engineers do design.

Modern SBC is my aim.",1,0,0,False,False,True,1640034650.0
rkf6jh,hpcdbfy,t1_hpcbn6o,"> Electronic engineers don’t do much designing.

The ones I work with do! Their job is to basically make SBCs in various forms for us software engineers to program :) I don't know if this is a different country thing (UK here) but what you describe tends to lean towards an electronic test engineer.",1,0,0,False,False,False,1640035351.0
rkf6jh,hpcgh7f,t1_hpcdbfy,So what do your electrical engineers do then?,1,0,0,False,False,True,1640036680.0
rkf6jh,hpcwjhc,t1_hpcgh7f,"Where I work they're not employed, but from what I know in other places they work on bigger things, power systems and stuff?

https://uk.indeed.com/jobs?q=Electrical%20Engineer&l=London%2C%20Greater%20London&vjk=f05819b0247e61d5&advn=2755904151741368

https://www.reed.co.uk/jobs/electrical-engineer-jobs-in-london",1,0,0,False,False,False,1640043678.0
rkr18d,hpbcm27,t3_rkr18d,"The classic book in the area is Introduction to Algorithms by Cormen, et al.",7,0,0,False,False,False,1640020300.0
rkr18d,hrb4j0m,t3_rkr18d,"Do you mean correctness proofs? If so I can recommend Software Foundations volume 3 “Verified Functional Algorithms”. The book is available for free online. I recommend going through at least the first volume first, though.",2,0,0,False,False,False,1641354626.0
rkr18d,hpbdh2f,t1_hpbcm27,Thanks!,1,0,0,False,False,True,1640020654.0
rk5sms,hp7qc2w,t3_rk5sms,"Check this out: https://en.m.wikipedia.org/wiki/Hardware_random_number_generator

Basically by measuring certain things at the quantum scale you can get ""true"" random",42,0,0,False,False,False,1639948307.0
rk5sms,hp7s3kc,t3_rk5sms,"Some of the best entropy sources we have are probably background radiation or radioactive decay.

You can get truly random data from https://www.random.org/",16,0,0,False,False,False,1639949043.0
rk5sms,hp9ft5h,t3_rk5sms,"In every thread about random numbers, I feel compelled to mention [this.](https://blog.cloudflare.com/lavarand-in-production-the-nitty-gritty-technical-details/)",6,0,0,False,False,False,1639977585.0
rk5sms,hp86wms,t3_rk5sms,"In theory yes. In practice it is very difficult to get it right, esp for security purposes. For various reasons, e.g.  bias, influenceability, measurement errors, degradation. And it also very difficult to assess that something is actually random.",5,0,0,False,False,False,1639955431.0
rk5sms,hp8mac0,t3_rk5sms,"You can, yes, a lot of algorithms use minute heat changes in the CPU or background radiation from space (same stuff that causes static in radios, that is radiation in the form of radio waves) since both are relatively inexpensive computations and close to true random.

However, usually you don't want a completely random arrangement as many times people won't see it as random.

For example, in a truly random system, you will see runs of the same number especially in binary systems like flipping coins. The macro results (distribution) are random, not necessarily the micro results you see in short samples.

For this reason, most algorithms and programs employ ""pseudo-random"" generators which appear to be more sporadic to us humans.",3,0,0,False,False,False,1639962526.0
rk5sms,hp9o73w,t3_rk5sms,"The answer depends upon the context of the question. If you are asking this question to know whether you can implement this in your application or not, well you can using TRNGs(true random number generators) or HRNGs(hardware random number generators) like the other comments mention.

But if you are asking your question on a theoretical level then this answer is for you. Nothing can be random according to [Laplace's demon](https://en.wikipedia.org/wiki/Laplace%27s_demon). It is a hypothesis because for determining relatively ""random"" events(although there are no random events in the known universe according to Laplace's demon) you would need a lot of computation and that often falls into the [Computability Theory](https://en.wikipedia.org/wiki/Computability_theory).

I wrote a paper about your question titled ""Can something be truly random"" but I later never published it. But I'll discuss some ideas about the paper here. So Laplace's demon is actually kinda proved by the fact that we can now calculate the result of a coin toss before the coin actually lands on the ground.

We can generate random numbers using nature and we can generate pseudo-random numbers using computers. Randomness of something completely depends on the patterns and predictability of the data. The predictability of data depends on the information conveyed by the data. Information is retrieved from the order of the data. If the data is redundant, meaning that it has no patterns but a pattern of uniformity, it conveys no information because it has no order. This relation can be stated as follows:
Randomness ∝ predictability ∝ information ∝ order ∝ regularity of data

We are not yet sure whether Laplace's demon holds in the world of quantum mechanics because ah things of the macroscopic world usually break in the microscopic world. 

Laplace’s theorem also collapses when we apply the second law of thermodynamics. According to Laplace’s theorem, nothing can be random and everything is predictable, which means that the information about the universe doesn’t increase because there is no randomness being created whatsover. This completely goes against the 2nd law of thermodynamics which states that the entropy of a physics system always increases with time.

I am sorry if my comment went a little off-topic but I hope you learnt something.
Randomness∝predictability∝information∝order∝regularityofdata",1,0,0,False,False,False,1639983126.0
rk5sms,hp9h0ri,t3_rk5sms,You could do it Schrodinger cats style and take a sample of background radiation/neutron collisions and use that as your seed.,1,0,0,False,False,False,1639978326.0
rk5sms,hp9v1mk,t3_rk5sms,"I’m no computer scientist but when I think of this I envision chaotic systems. Where the amount of information is exceedingly high and every vector for each informative quanta has it’s own unique internal system for objective emergence or expression.
In a system like that, we would get caught up in the definition of “perfect”. If by perfect you mean not being able to mathematically trace the origin then no system can truly be perfect. Even the universe (if we had the ability to track each event through or outside of time) would be perfect. It would be predictable and not necessarily random.
I think the perfection you’re looking for would come from the vector quality of each quanta of information and it’s ability to be predicted or traced. A perfectly random event would emerge as an unexplainable mystery.",1,0,0,False,False,False,1639988456.0
rk5sms,hpf8vv4,t3_rk5sms,yes at the really small scale things can be truly nondeterministic. x86 CPUs use thermal fluctuation data for rdseed instruction.,1,0,0,False,False,False,1640094248.0
rk5sms,hp8e73f,t3_rk5sms,A random number which is prime,0,0,0,False,False,False,1639958682.0
rk5sms,hp8eylt,t1_hp7qc2w,"Up to your confidence in certain quantum theories over others.

Randomness is part of standard model (though even there there’s some nuance), but observationally equivalent theories exist that don’t invoke it.

Of course, lack of information still makes event effectively “random” for those measurements — but at that point you might have an easier time just leaning on known chaotic dynamics — to get deterministic, but unpredictable events that match some distribution of interest.",15,0,0,False,False,False,1639959039.0
rk5sms,hp90w4r,t1_hp7qc2w,On a quantum scale? So like if I flip a coin and get a coffee flavored duck made out of sand? Would that be considered true random or are we still talking heads or tails here?,-1,0,0,False,False,False,1639969682.0
rk5sms,hp8h1bn,t1_hp8eylt,"I'm fairly sure that Bell's Theorem invalidates hidden variable interpretations of quantum mechanics, no?",7,0,0,False,False,False,1639960003.0
rk5sms,hpbzjn9,t1_hp90w4r,"We're still talking heads/tails really.  In the end we must measure something, it's just a question of whether or not that thing we're measuring is deterministic or not.",1,0,0,False,False,False,1640029634.0
rk5sms,hp8ivnj,t1_hp8h1bn,"Bell’s theorem only invalidates *local* hidden variable theories.  Where *local* references the theorized particle in question.

You can still have hidden variable theories they just have to have ways for particles to communicate without being next-to/on-top-of each other.

De Broglie’s proposed [pilot wave theory](https://en.m.wikipedia.org/wiki/Pilot_wave_theory) would be an example.",5,0,0,False,False,False,1639960860.0
rk5sms,hp8pue9,t1_hp8h1bn,[Good explanation on the matter](https://www.youtube.com/watch?v=ytyjgIyegDI).,2,0,0,False,False,False,1639964262.0
rk5sms,hp8iwwk,t1_hp8ivnj,"Desktop version of /u/OphioukhosUnbound's link: <https://en.wikipedia.org/wiki/Pilot_wave_theory>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",2,0,0,False,False,False,1639960878.0
rk5sms,hp8ktf5,t1_hp8ivnj,Thanks for the info!,1,0,0,False,False,False,1639961807.0
rkfmao,hpab3cv,t3_rkfmao,"Don't use MD5 at all. Just use bcrypt.

The only time I'd hash something multiple times is in cases such as a zero knowledge password manager where you don't want the server to ever have the plaintext password, so you'd hash it on the client side once and then again on the server side -- but that's a very niche case and not what you need to do.

If you want additional security you can increase the rounds that bcrypt performs. This is the ""cost"" input to bcrypt. The default is typically 10 for most implementations which means 2^10 internal rounds. Using 13 or so is reasonable.",6,0,0,False,False,False,1640001693.0
rkfmao,hpc1ye4,t3_rkfmao,"You asked this [yesterday in /r/django](https://www.reddit.com/r/django/comments/rkf0a8/im_making_a_web_app_and_im_hashing_the_passwords/) but deleted your question when people responded negatively, and you're not getting the response you *need to hear* here, so I'll repeat my answer here:

This makes no sense. None.

You've stated elsewhere here that...

> I'm mainly doing it this way so I dont have to use blobs in my database with byte type data and can just use varchar with hexadecimal. 

 This is a solved problem, bcrypt hashes are **strings** that have the form `$2a$10$N9qo8uLOickgx2ZMRZoMyeIjZAgcfl7p92ldGxad68LJZdL17lhWy` and your hashes should be persisted in that format. If your implementation is producing binary output, you should use a different implementation. If you've implemented this yourself, **don't**. Use an off-the-shelf option. **Don't** run your binary data through MD5 just to get a hexadecimal representation. You'll produce something that is completely unportable, and you'll have no way to go back and *make* it portable as your original passwords will be lost after going through this Frankenstein hashing process. EVERY BCrypt implementation in any language can accept a hash in the form `$2a$...` and test it. Your hashes will be useless outside of your own narrow bit of code.

You've also said you're sending `md5(bcrypt(salt + password))` to the database. Where are you saving the salt?? You *need* that salt in order to compute `bcrypt(salt + candidate_password)` the next time the user attempts to log in. If you hash it with MD5, it's gone, and you can't use your stored hash for anything. Storing BCrypt in the form `$2a$...` takes care of this for you, storing the salt as part of the encoded hash.

Your question contains this troubling line:

> ... if they got the salt by hacking into the server ...

This implies to me that you're using **one salt** for all hashes, stored as a single secret. **That's not a salt, and this isn't BCrypt**. BCrypt uses a *per-user salt*, stored along side the output hash. Decorating your hash with a single application-wide secret is a [cryptographic **pepper**][1], which is not part of BCrypt and not useful. It's also unclear to me how you can be using a single pepper and still have BCrypt's cost factor at play.

Finally, MD5 is old and broken, and has **no business** being considered for use *anywhere*, but especially not in any thing related to security or password hashing. MD5 adds absolutely no security here, only complexity and harm, and it's about the worst possible way you could convert raw bytes to a hex encoding.

[1]: https://en.wikipedia.org/wiki/Pepper_(cryptography)",4,0,0,False,False,False,1640030613.0
rkfmao,hp9jyvb,t3_rkfmao,"If it’s md5( bcrypt( password + salt)) then yes

If it’s md5( password + bcrypt(salt)) then no. 

I don’t know what bcrypt is off the top of my head but if it’s a hashing algorithm on par with sha256 or 512 then it’s good. If it’s an encryption (that you can decrypt) then probably not.


Out of curiosity, why aren’t you just using the bcrypt value assuming it’s good. Also why not use a high sha rather then the md5. That would make it more secure one way or the other.",2,0,0,False,False,False,1639980205.0
rkfmao,hp9isll,t3_rkfmao,[deleted],1,0,0,False,False,False,1639979438.0
rkfmao,hp9o1yf,t3_rkfmao,"I don't think you can use md5( bcrypt( password + salt)) like that, otherwise you wouldn't be able to check if password is correct even if you know it. You have to save salt in plain text format or encrypted by AES. Istead of md5+bcrypt I would suggest using Argon2, with it you can increase or deacrease how long it takes to check plain text password against encrypted one.",1,0,0,False,False,False,1639983024.0
rkfmao,hp9sezt,t3_rkfmao,"Tangential :   
Have you evaluated available libraries for the same operation ?  Depending on the ecosystem (say Java or Python) there might already be libraries to implement password storage + validation +  anti-brute-force (like detect attempts across load balanced instances)",1,0,0,False,False,False,1639986325.0
rkfmao,hp9stf6,t3_rkfmao,"Yikes. Don't do this.

Please read this: https://crackstation.net/hashing-security.htm

Don't do this. Take the time to really learn cryptography before you start jamming hashes together willy nilly.",1,0,0,False,False,False,1639986642.0
rkfmao,hpahpu8,t1_hpab3cv,I'm using md5 so that I don't need to store the byte data in the dB I'm salting it with 15 rounds then hashing that hash again with md5 so I can just use a var char to check the users password I just hash the users input the same way it was created and see if the hashes are the same.,1,0,0,False,False,True,1640006005.0
rkfmao,hp9kn2l,t1_hp9jyvb,"SHA-512 is a cryptographic hash while bcrypt is a password hash or PBKDF (password-based key derivation function).  
  
SHA-512 has been designed to be fast. You don't want any delays when validating a signature, for instance. There is no reason for generic cryptographic hashes to be slow.  
  
bcrypt on the other hand is a password hash that performs key strengthening on the input. Basically, it does this by slowing down the calculation so that attackers will have to spend more resources to find the input by brute-forcing or dictionary attacks. The idea is that although the legit users - you in this case - will also be slowed down, they are only slowed down once per password. However, the attackers are slowed down for each try. The legit user is of course much more likely to input the right password first.  
  
Furthermore, bcrypt also contains a salt as input, which can be used to avert rainbow table attacks.

&#x200B;

I'm mainly doing it this way so I dont have to use blobs in my database with byte type data and can just use varchar with hexadecimal. 

&#x200B;

And yes it is md5( bcrypt( password + salt))",2,0,0,False,False,True,1639980650.0
rkfmao,hp9jqtu,t1_hp9isll,"Its not MD5 though its bcrypt(password, salt) then I take the bcrypt hash and do md5(bcrypt hash) then send that to the db",-1,0,0,False,False,True,1639980056.0
rkfmao,hpagnv2,t1_hp9o1yf,I don't need to check if the password is correct I can just hash the user's input again and see if it's the same.,-1,0,0,False,False,True,1640005377.0
rkfmao,hpahhsk,t1_hp9stf6,That article is actually exactly what I'm doing it's impossible for me to decrypt so I take the users input hash it with bcrypt then hash it with md5 and see if the hash is the same.,1,0,0,False,False,True,1640005873.0
rkfmao,hpb1ok2,t1_hpahpu8,It’s still unclear why you MD5 hashing would help anything here.,4,0,0,False,False,False,1640015730.0
rkfmao,hpbg9ui,t1_hpahpu8,"You can encode the bytes from bcrypt into text using base64, or even hexadecimal.",2,0,0,False,False,False,1640021789.0
rkfmao,hpcdjmr,t1_hpahpu8,"The output of bcrypt already contains the salt. You don't need to store anything else. If you were using something like PBKDF2, then you'd need to store the salt either in a separate column, or encode the digest + salt into some kind of hex or base64 string for storage.

e.g. bcrypt will output something like:  `$2a$10$N9qo8uLOickgx2ZMRZoMyeIjZAgcfl7p92ldGxad68LJZdL17lhWy` that has a salt of `N9qo8uLOickgx2ZMRZoMye` \-- this whole thing can be stored just like any other string in your database (e.g. in a varchar column)

Just do `digest = bcrypt(plaintextPassword)`, and save that digest to the database. The language you're using will have something like `hash.check(plaintextPasswd, storedDigest)` for comparing the stored digest against the plaintext password a user enters when trying to log in",1,0,0,False,False,False,1640035448.0
rkfmao,hp9la0w,t1_hp9kn2l,"From what you’ve said this seems perfectly secure. I’m a little concerned that you may be reducing the output space if md5 has a smaller number of bits output then bcrypt it may be more likely to have collisions. Although this isn’t likely a real security risk, more of an academic view of it. 

I would have base64 encoded the output to just be dealing with text rather then md5ing it. Although that’s just a personal preference thing. (Possibly some performance increase as that’s probs faster then md5, but again so minimal it’s academic)

Salted and hashed with a good algorithm is the best way I know to store passwords. If anyone knows of a better I’d be super curious to learn as well.",1,0,0,False,False,False,1639981082.0
rkfmao,hpbmq33,t1_hp9kn2l,"Instead of MD5 im just going to convert it to base64 as the purpose of the md5 was mainly just for db storage. 

&#x200B;

Instead of MD5 I'm just going to convert it to base64 as the purpose of the md5 was mainly just for DB storage.",1,0,0,False,False,True,1640024387.0
rkfmao,hpc3oxr,t1_hpagnv2,Not without the salt you can't.,3,0,0,False,False,False,1640031332.0
rkfmao,hpbnary,t1_hpb1ok2,Its purely just for db storage,0,0,0,False,False,True,1640024618.0
rkfmao,hp9lu4q,t1_hp9la0w,"Yeah, I don't think an attacker would ever really be able to get the original value of the password out at least. But if anyone else has any more input id be happy to hear it.",0,0,0,False,False,True,1639981456.0
rkfmao,hpbr264,t1_hpbnary,"But it doesn’t add much security, if any, and adds an extra step later…  
  
Plus any MD5 hash collisions could make it possible to get into someone else’s account or even guess passwords (if the salt is the same each time).",2,0,0,False,False,False,1640026132.0
rkfmao,hpbm6e2,t1_hp9lu4q,"I agree with hourglass492 that if your concern is database storage, you should convert the bcrypt output to a text format such as by base64 encoding. Running your bcrypt output through md5 is reducing your output space for no real benefit.",3,0,0,False,False,False,1640024170.0
rkfmao,hpbml44,t1_hpbm6e2,Thanks ill just convert it to base 64,2,0,0,False,False,True,1640024334.0
rkfmao,hpc3nc6,t1_hpbml44,"Don't do that. BCrypt hashes are stored in the form `$2a$...`, if your implementation is not producing that format, don't use that implementation: https://en.wikipedia.org/wiki/Bcrypt#Description",1,0,0,False,False,False,1640031313.0
rjvbrs,hp5ufoc,t3_rjvbrs,Write a ray tracer from scratch and try to implement the rendering equation via monte carlo,14,0,0,False,False,False,1639917414.0
rjvbrs,hp6z1xw,t3_rjvbrs,"Implementing the AltaVista MinHash alg could be fun with a web crawler of some sort. Or anything with universal hashing, really.

Bloom filters for malicious URLs has some example implementations out there you could probably build on.

Per Wikipedia, there’s also a use for Karger’s algorithm/Min Cut in segmentation-based object categorization scenarios like image compression.",3,0,0,False,False,False,1639937193.0
rjvbrs,hp8kmqg,t3_rjvbrs,"You can try bots programming on CodinGame, it's a great way to use those kind of algorithms, especially for reinforcement learning.",3,0,0,False,False,False,1639961712.0
rjvbrs,hp7krrx,t3_rjvbrs,Do you have experience with music software coding,1,0,0,False,False,False,1639946026.0
rjvbrs,hp8ojg1,t3_rjvbrs,You could do something with Fourier transform.,1,0,0,False,False,False,1639963626.0
rjvbrs,hp98p0d,t3_rjvbrs,Lossy compression,1,0,0,False,False,False,1639973556.0
rjvbrs,hp9mjxp,t3_rjvbrs,"Try the advent of code.  It's a challenge that happens every year; it's language independent; and every one gets their own problem set. 

https://adventofcode.com/",1,0,0,False,False,False,1639981955.0
rjvbrs,hq5aotq,t3_rjvbrs,"Just out of curiosity, what books did you read? Perhaps I can point you to something you're already familiar with.",1,0,0,False,False,False,1640613626.0
rjvbrs,hp5ulyk,t1_hp5ufoc,Thank you for your recommendation. Would you provide me with supportive resources?,4,0,0,False,False,True,1639917538.0
rjvbrs,hp97kh3,t1_hp6z1xw,Thank you for your contribution,1,0,0,False,False,True,1639972995.0
rjvbrs,hp97er8,t1_hp8kmqg,Thanks 😊. I am going to check it out,1,0,0,False,False,True,1639972914.0
rjvbrs,hp97chr,t1_hp7krrx,"No, but I would stop be interested in listening from you",1,0,0,False,False,True,1639972881.0
rjvbrs,hp97hj2,t1_hp8ojg1,What kind specifically of a project or challenge?,1,0,0,False,False,True,1639972954.0
rjvbrs,hpejrpp,t1_hp9mjxp,Thank you so much. I going to add it on my puzzles list,1,0,0,False,False,True,1640075885.0
rjvbrs,hqdh795,t1_hq5aotq,My favorite book of all time is [Algorithm Design by Jon Kleinberg and Éva Tardos](https://ict.iitk.ac.in/wp-content/uploads/CS345-Algorithms-II-Algorithm-Design-by-Jon-Kleinberg-Eva-Tardos.pdf),1,0,0,False,False,True,1640754092.0
rjvbrs,hp5usjh,t1_hp5ulyk,"1. Ray tracing in a weekend
2. Rendering equation in Wikipedia
3. scholar.google.com for more

I'm assuming you're experienced in C/C++",11,0,0,False,False,False,1639917669.0
rjvbrs,hplguyu,t1_hp97chr,Would you think you would be able to make a program that finely tunes audio files frequency/pitch? Say as the standard for music is 440hz you wanna wanna pitch it down to 432hz. It’s possible now but not 100% accurate so I would like to do that without any limits on the numbers and then program it so I can save it,1,0,0,False,False,False,1640203050.0
rjwxfl,hp6ehhu,t3_rjwxfl,"Since the keys were securely exchanged via asymmetric encryption, why does it matter? A handshake/exchange happens once, the portion of with the symmetric encryption needs to be fast as it can happen many times.",11,0,0,False,False,False,1639928521.0
rjwxfl,hp80xsg,t3_rjwxfl,"You'll find almost all network protocols such as HTTPS only use asymmetric encryption for the handshake. The same goes for file encryption, typically a symmetric key is used to encrypt the files, then the symmetric key is encrypted using asymmetric encryption, that way you get the speed of symmetric with the security of asymmetric.",10,0,0,False,False,False,1639952801.0
rjwxfl,hp6385q,t3_rjwxfl,"Yeah. Symmetric cryptography is much, much faster than asymmetric cryptography. There's no other reason.",27,0,0,False,False,False,1639922994.0
rjwxfl,hp9ga38,t3_rjwxfl,It's much faster and just as secure.,2,0,0,False,False,False,1639977872.0
rjwxfl,hp9bbns,t3_rjwxfl,"With asymmetric encryption you don’t decode the message, only verify and sign it. You don’t ever want to exchange the private key, only the public key. You could for example sign a message with your asymmetric key and encrypt it with the key you exchanged based on the same key so the receiver can decode the message, verify it hasn’t been tampered with, without the receiver being able to copy your signature.

If you were to use asymmetric encryption for encoding/decoding messages, both sides need to know the private key.",0,0,0,False,False,False,1639974960.0
rk78ya,hpfj7e7,t3_rk78ya,"Hi,

I found some idea about this problem at:[Ethereum.stackExchange](https://ethereum.stackexchange.com/questions/116919/detection-of-same-function-reentrancy-vulnerability).

Zulfi.",1,0,0,False,False,True,1640099163.0
rj8mre,hp1x93o,t3_rj8mre,"the compiler knows the type of the fields, so it knows how many bytes each field takes up and thus how many bytes it needs to skip to get to the next field.",38,0,0,False,False,False,1639839471.0
rj8mre,hp2oabh,t3_rj8mre,"> How do structs work internally in memory.

To answer that we need to pick an implementation, as this is implementation defined, it's not something covered by the C spec 

> I know that an instance of a struct is a pointer to the first field of the struct.

Sorry, but this isn't true!


    struct example { 
        int i;
        int j;
    } an_example;

    an_example.j = 0;


There were no pointers involved there.


> I also know that all the fields of a struct are contiguous to each other in memory so the memory address of the second field of a struct can be accessed by adding the size of the first field to the memory address address of the first field.

Again this ain't true. This struct, on gcc x86, defies what you say:

    struct example { 
        char c;
        int i;
    } an_example;

    // Then print (&an_example + offsetof(an_example.i)) Vs (&an_example.c + sizeof(an_example.c))

This is due to padding. For GCC look up the ""packed attribute""

> am failing to understand that how do we access the consequent fields of a struct with just the memory address of the first field.

The compiler knows the size of each member of the struct, and therefore knows where each other field is. So everything you write `an_example.i` the compiler knows to translate that into a specific memory offset .  Look up the `offsetof` macro.


You should try out compiler explorer at godbolt.org",12,0,0,False,False,False,1639851690.0
rj8mre,hp1xr9j,t3_rj8mre,"Struct elements are contiguous in memory, so you could theoretically add the size of the first element to the address of the first element and get the second element, but often times there is padding between elements so you have to account for that as well. There are two general rules when counting structs and padding

1) each element must start at a memory address (relative to the first element) that is divisible by the size of the given element. Example:

Char (1 byte) | 7 bytes of padding | pointer (8 bytes)

2) the total size of the struct must be divisible by the size of the largest element in the struct. Add padding at the end of the struct to achieve this. Example:

Pointer (8 bytes) | int (4 bytes) | 4 bytes of padding",19,0,1,False,False,False,1639839727.0
rj8mre,hp1v6nu,t3_rj8mre,"It works exactly like with arrays, by using the address of the first element and an offset. The individual types don't need to have the same size, since the compiler knows them at compile time.",12,0,0,False,False,False,1639838408.0
rj8mre,hp5b9js,t3_rj8mre,"Strict fields are *not necessarily* contiguous in memory, they might be packed to ensure the fields align on word boundaries.

That said, if you have the type you have all the information you need for field offsets with pointer arithmetic.  

If you have a pointer p to a field f of a struct s, you can calculate the relative offset from s to f by casting 0 (literally 0) to a pointer of type s, accessing field f, then apply & to that. Something like: `&(((struct s *)0)->f)`. You can then subtract this from any pointer to a field f to get its parent s.

This might seem horrid but it’s how linked lists are implemented in the Linux kernel and has the advantage (over a traditional linked list implementation) that you don’t need to cast the list payload every access, which is error prone.",2,0,0,False,False,False,1639901460.0
rj8mre,hp1ua1c,t3_rj8mre,You could look it up in memory,-4,0,0,False,False,False,1639837932.0
rj8mre,hp5aas1,t1_hp2oabh,"> I know that an instance of a struct is a pointer to the first field of the struct.
> Sorry, but this isn't true!

Yeah I was wrong there. The instance of a struct is rather a reference variable to the first field of the struct. Again this might be implementation specific(I am using Golang which is similar to C is a lot of aspects) but when I print the memory address of the struct instance and the memory address of the first field of the same struct instance, they both are the same. 

> The compiler knows the size of each member of the struct, and therefore knows where each other field is.

So it also knows the amount of padding applied by data alignment? Because if it wouldn't know the amount of padding, then it won't be able to locate the data. Just confirming.",2,0,0,False,False,True,1639900679.0
rj8mre,hp236vr,t1_hp1xr9j,What is achieved by implementing the second rule?,6,0,0,False,False,True,1639842366.0
rj8mre,hp2sx0p,t1_hp1xr9j,Isn't this implementation defined?,1,0,0,False,False,False,1639853704.0
rj8mre,hp6xtb0,t1_hp5b9js,"A well thought out and carefully constructed comment.

&#x200B;

It's very clear that you wield C in a professional capacity!",1,0,0,False,False,False,1639936707.0
rj8mre,hp1uwgw,t1_hp1ua1c,I can't look it in memory that how does the compiler navigate through the struct.,0,0,0,False,False,True,1639838259.0
rj8mre,hp6afzt,t1_hp5aas1,"> So it also knows the amount of padding applied by data alignment? Because if it wouldn't know the amount of padding, then it won't be able to locate the data. Just confirming.

It's doing the padding, so of course it knows it :)

The bigger program is the programmer/program can't usually find it out in a ""legal"" manner.",1,0,0,False,False,False,1639926648.0
rj8mre,hp25wst,t1_hp236vr,"Someone can feel free to correct me on this/add to it, but it’s my understand it has to do with ensuring that one struct, or better yet any element within a struct, does not end up being stored across two memory blocks",5,0,0,False,False,False,1639843618.0
rj8mre,hp2f7tm,t1_hp236vr,"In general, data often needs to be aligned properly. A 64-bit pointer takes 8 bytes, and it should be placed at an address divisible by 8. If you try to do otherwise, you have a ""misaligned"" value, and reading such a value may be slow, or the processor might just not support it and your program will crash.

&#x200B;

Now, if you have such an 8-byte-sized value in your struct, you add padding to ensure that it's offset within the struct (i.e. what you have to add to the pointer to the first element to get a pointer to the element you want) is divisible by 8.

However, we actually wanted to have its actual address, i.e. the value given by (address of the first member + offset) to be divisible by 8. To do this, we additionally require that (address of the first member) is divisble by 8. This means that our struct now has an alignment of 8.",3,0,0,False,False,False,1639847758.0
rj8mre,hp2t6dq,t1_hp236vr,"It's called ""memory alignment"". The simplest answer is because the CPUs demand it be that way 

E.g. early arm could only load and store on 32bit boundaries. But an x86 could do 1byte increments. 

It also allows for more efficient indexing operations, but at the expense of padding. You can usually choose the trade offs in your compilers option flags",1,0,0,False,False,False,1639853817.0
rj8mre,hp1x30m,t1_hp1uwgw,You literally gave the compiler the definition of the struct.  It knows the size of each member and the padding needed to byte align.,3,0,0,False,False,False,1639839385.0
rj8mre,hp1vo1m,t1_hp1uwgw,"If what the other commenter says is true, you could get an intPtr to the struct, then increment it by something like 
     
    sizeOf(typeOf(myStruct.firstElement)))",2,0,0,False,False,False,1639838662.0
rj8mre,hp6b6i7,t1_hp6afzt,"Ah it all starts to make sense now. I thank you and all the others who helped me. This kind of computer science stuff which talks about the internal workings of memory, etc. really intrigue me. What field should I study to learn a little bit more about this kinda stuff? My guesses are that I would need to study assembly or compiler design or operating systems to get a taste of this type of stuff.",1,0,0,False,False,True,1639926999.0
rj8mre,hp2ahfx,t1_hp25wst,"I am having a hard time understanding how the two rules work. Suppose we have a struct definition called `employee` with fields `firstName`, `lastName` and `age` of data types `string`, `string` and `int`, respectively. Suppose that we create an instance of the struct `employee` and store it in a variable `monica`. The values of the struct would be as follows: `firstName=""Monica""`, `lastName=""Smith""`, `age=33`.

Before I knew these two rules, I would visualize the memory locations of a struct like [this](https://imgur.com/a/JUJgudK). But according to the first rule, the different between the `n-1`th field's first bit and first bit of `n`th element must be divisible by the size of the `n`th element.

So according to that, we might not need the padding in the lastName field because its value(""Smith"") just occupies 5 bytes of data and 5 is divisible by the size of the next field(int-1byte). I strongly think that I am wrong here and the base size of a data type never changes. Please clarify this.

Also, how will the second rule help to ensure that any element of a struct does not end up being stored across two memory blocks.",3,0,0,False,False,True,1639845671.0
rj8mre,hpfpnt8,t1_hp2f7tm,"> To do this, we additionally require that (address of the first member) is divisble by 8.

Could you please explain why?",1,0,0,False,False,True,1640101953.0
rj8mre,hq8y5rf,t1_hp2f7tm,"Suppose we have a struct like this:
```
struct dog {
   int age; // 0x00 to 0x03
   // 0x04 to 0x07 - padding
   person* owner; // 0x08 to 0x015
   int score; // 0x16 to 0x19
   int lifespan; // 0x20 to 0x24
} bruno;
```
The first three fields are stored at a memory address divisible by 8, whereas `lifespan`'s memory address is not divisible by 8, then how can we say that the struct has an alignment of 8?",1,0,0,False,False,True,1640672502.0
rj8mre,hp1x7nv,t1_hp1vo1m,"Ah okay, I kinda get it now. So the compiler navigates through the struct by just adding the size of the first field to the memory address of the first field. Is my interpretation right? just confirming.",0,0,0,False,False,True,1639839450.0
rj8mre,hp6hkiw,t1_hp6b6i7,"Yeah, all of those :)

A book I recommended the other day is [Crafting Interpreters](http://craftinginterpreters.com/). It's very easy to read, unlike some of the classic compiler books. It's also free online.",1,0,0,False,False,False,1639929892.0
rj8mre,hp2fnny,t1_hp2ahfx,"You make everything more complicated by bringing up strings :) Note that in \`C\`, \`string\` does not exist.

Strings (and other advanced datatype) are not first-class types because they can be of arbitrary size. The string """" takes 1 byte, ""abcd"" takes 5 bytes and so on (don't forget the null byte).

Thus, if you want to put a string into a struct, you either

* say that the string is at most x bytes long. Then you have an array of chars of size x, which has the alignment properties of a char, i.e. alignment 1, which just means no restrictions
* put in a pointer to the string, which then resides somewhere else in memory. Now your struct contains a pointer, with its specific size and alignment properties.

In general, types have a specific size and a specific alignment requirement. For integers and pointers, those are equal. For structs, the size of the sum of the size of its members, while its alignment usually is the largest alignment of any of its members.  


For arrays of type T of length n, they similarly have size ""n \* size of T"", but their alignment is still just that of the type T.",7,0,0,False,False,False,1639847948.0
rj8mre,hp2gang,t1_hp2ahfx,"First let me provide some clarification on the first rule. It’s not exactly the n and n-1 relationship you described. Rather, I like to treat the beginning of the struct as a 0 point, and then ensure that each element begins at an “index” relative to the 0 point that is divisible by the size of the given element. For example, if we had a struct that had an 8 byte element, a 2 byte element, and a 4 byte element, it would look like this:

8 bytes | 2 bytes | 2 bytes of padding | 4 bytes

Note we have 2 bytes of inner padding, because our 4-byte element is now separated from our zero point by 12 bytes (without padding, it would be 10), which is divisible by 4. 

So now let’s revisit how we’re representing a string. Rather than storing the string literal, character by character, a string really just stores the address of a character array (where the array lives gets complicated and can vary, but it can be on the heap, in static initialized, static uninitialized, or perhaps even the stack), where the array represents the string. This way every string occupies 8 byes (since it is a char pointer). 

Knowing that, firstName and lastName are now both 8 byte addresses (char pointers)! Also, remember the age is a int and is 4 bytes, even tho the value is 33. So, here is our raw struct 

firstName | lastName |    age
8 bytes     | 8 bytes    | 4 bytes

Do we need any padding? The first element looks good, it’s just 8 byes. The second element is also 8 bytes, and the difference between the zero point is 8—all good. Our third element, age, seems to be good as well, it’s separated by 16 bytes from our zero point, which is divisive by 4. Now do we need any padding on the end? 

Total struct size = 8 + 8 + 4 = 20

20 is not divisible by 8! So we need 4 bytes of padding, to get our total struct size up to 24, which is divisible by 8. Here is our final struct:

firstName | lastName |    age    | padding
8 bytes     | 8 bytes    | 4 bytes | 4 bytes 

Total struct size = 24

Now, why does the second rule ensure we don’t have one element of a struct stretched across two blocks? Here is an example: lets say we have a struct that an 8 byte element and a 1 byte element:

|       8 bytes       | 1 byte

Now, lets say we want to make an array of structs. Each struct is 8 bytes in size, and arrays are stored contiguously in memory, so we have 

8+1 bytes | 8+1 bytes | 8+1 bytes | etc…


If our array is large enough we will eventually approach the end of a block. Will a block size ever be divisible by 9? Like, almost definitely not. So you’ll get the end of the block with less than 9 bytes of space left, and the struct will get chopped!",3,0,0,False,False,False,1639848228.0
rj8mre,hp2dbi2,t1_hp2ahfx,"Basically you need to be able to uniformly step through the struct fields with limited knowledge of the contents of a field. All data is a byte sequence with a pre-defined interpretation.
  
E.g. C-strings are terminated with a *null byte*.  
  
M,o,n,i,c,a,\0,? 
 
\0 is the null byte, ? is an undefined byte value which could be any valid byte as it is not part of the string. 
 
A number like 33 would be stored differently than the string “33”.  
  
3,3,\0,? 
^ string version  
  
00000000 00000000 00000000 00100001  
^ 33 as a 32-bit/4-byte integer",2,0,0,False,False,False,1639846920.0
rj8mre,hpg8sga,t1_hpfpnt8,"It's just basic divisbility rules.

You want (x+y) to be divisble by 8. We already require x is divisble by 8. What could we additionally require so that the whole sum becomes divisble by 8?",2,0,0,False,False,False,1640109605.0
rj8mre,hq9bwne,t1_hq8y5rf,The struct will only ever be placed at addresses divisible by 8. This does not mean that all of its members also will.,2,0,0,False,False,False,1640682890.0
rj8mre,hp3ej4n,t1_hp1x7nv,"Not exactly, as that does not take padding into account. The compiler determines the layout of the struct. Based on this, it knows the offset of every member from the start of the struct. Computing the address of each field becomes (p + n) where p is the address of the struct, and n is the offset of that field from the start of the struct. n is known at compile time and is a constant offset.",2,0,0,False,False,False,1639863584.0
rj8mre,hp5bhv2,t1_hp1x7nv,No I’m afraid not because there might be padding between that element and the next one.,2,0,0,False,False,False,1639901650.0
rj8mre,hp1yuqf,t1_hp1x7nv,"At some level, yes, that has to be it. As far as the implementation details they probably have a bunch of optimizations that would make it complicated to parse.",1,0,0,False,False,False,1639840279.0
rj8mre,hq4hn37,t1_hp2fnny,"> For integers and pointers, those are equal.

How is the alignment for pointers and integers equal? The size of an integer is 4 bytes whereas the size of a pointer is 8 bytes, so they have different alignments.",1,0,0,False,False,True,1640591641.0
rj8mre,hq13by7,t1_hp2gang,"By 'block' do you mean physical segments of memory? A memory block generally means a contiguous block of memory. How large can a 'block' of memory be?

PS: sorry for asking this question 8 days after your comment",1,0,0,False,False,True,1640530760.0
rj8mre,hpjmfw3,t1_hpg8sga,"> What could we additionally require so that the whole sum becomes divisble by 8?

We want `y` to be divisible by 8. I get it now, thanks a ton!",2,0,0,False,False,True,1640172075.0
rj8mre,hq5076m,t1_hq4hn37,"that's not what I meant. I meant that if the size is 4, the alignment also is for these types. If the size is 8, the alignment also is.

Unlike arrays, which can have size 1000 and alignment 1.",0,0,0,False,False,False,1640606908.0
rj7x0t,hp1qrl1,t3_rj7x0t,"In order of showing this function is big theta of f(n) [where n is the number of items], you want to show its both O(f(n)) and Omega(f(n)).

Now in order of finding the needed f(n) its mostly comes down to understand what the code does. After you figure what f is (using simply your intution) you need to prove both O(f(n)) and Omega(f(n)).

Lets denote the number of operatios of your function as g(n), you now need to find constants n_0 and n_1, c_0 and c_1 such that:

g(n) < c_0 f(n) for n> n_0 and

g(n) > c_1 f(n) for n > n_1.

And by that the proof is done. Sorry if my formating is bad, im on phone",4,0,0,False,False,False,1639836037.0
rj7x0t,hp232lx,t3_rj7x0t,"It's hard to answer this question without knowing what you currently know about big-O, big-Omega and big-Theta notation.

First, you should identify what the ""problem size"" is in this context. It looks like the input of your algorithm consists of a list of lists. Candidates for the problem size here could be:

* The total number of items.
* The number of lists in the `lists` object.
* Both of the above.

The first option is probably the most applicable here, so let's say `n = total number of items`.
For your sample input, `n = 6`.

To find the worst case runtime of this algorithm to be Θ(f(n)), you want to show that a worst case instance of the algorithm will:

* complete in **at most** `constant * f(n)` steps (which shows the algorithm runs in O(f(n)) time), *and*
* requires **at least** `constant * f(n)` steps to complete (which shows the algorithm runs in Ω(f(n)) time).

The `constant`s in the above two conditions need not be the same constant, but they must both be independent of `n`.

So first, you should consider what is a worst case instance for your algorithm. Hint: you've already given a worst case instance for `n = 6`. Then you should prove the two statements for some function `f`.

To determine the function `f `, ask yourself this: if the number of items were to double, how much longer would this code run? And then, can you think of a function `f(n)` with this property?",1,0,0,False,False,False,1639842310.0
rj7x0t,hp5noj7,t3_rj7x0t,"this is a linear search of a 2d array. if the array is m by n then on average you will search through half the list to find it which would be m\*n/2 which is just theta(m\*n).

&#x200B;

also if you want to shit your pants with the technical details then you can guarantee that there is some constant to multiply m\*n by that will be asymptotically less and some constant that would be asymptotically greater. you can guarantee this is true because i said so and i offer a 30 day money back guarantee.",1,0,0,False,False,False,1639911970.0
rj7x0t,hp3eifx,t3_rj7x0t,Theta(1) because it only performs a constant number of operations.,-3,0,0,False,False,False,1639863574.0
rj7x0t,hp4a0nw,t3_rj7x0t,"If I asked you how many operations your code makes before it finishes, what answer would you give? Just curious, and it might reveal the best path towards understanding the answer to the question you gave.",0,0,0,False,False,False,1639879187.0
rj7x0t,hp6rjsf,t1_hp1qrl1,"Amazing, thanks dude! Really appreciate the help :D",1,0,0,False,False,True,1639934142.0
rj7x0t,hp5nhdv,t1_hp3eifx,lmao dude obviously he meant if you scale by the list size,1,0,0,False,False,False,1639911806.0
riwlmc,hp47tm0,t3_riwlmc,That’s super cool!,1,0,0,False,False,False,1639878091.0
risxvd,hozlh61,t3_risxvd,"Server software is specifically developed and tested for long uptime (because that is a relevant use case). Other software, for example League of Legends, is not tested for long uptime - if the Client crashes after 24h, nobody really cares (at most, they put in a 12h restart warning/force).",24,0,0,False,False,False,1639786416.0
risxvd,hozj8re,t3_risxvd,If you were to use a server as a desktop you would run into the same issues.,9,0,0,False,False,False,1639785415.0
risxvd,hp0wr8r,t3_risxvd,"Server hardware runs to varying degrees:

* climate controlled environment
* multiple power feeds, battery backup, generators
* error correcting memory
* redundant drives with redundant drive controllers
* multiple high volume fans
* professional maintenance
* designed for purpose software stack
* active monitoring of component health
* solid metal chassis properly grounded

Laptop: plastic foldy boy that's filled with dog hair and viruses",10,0,0,False,False,False,1639812772.0
risxvd,hp00koq,t3_risxvd,"Non-server software often has memory leaks.  Web browsers are especially notorious for leaking memory.  Fixing memory leaks is tough, so often companies don't put resources towards fixing them.   Far easier to have your users restart the app, which is not an option on servers.",8,0,0,False,False,False,1639793428.0
risxvd,hp14jsd,t3_risxvd,Servers have to be rebooted more often than you think. But high availability configurations make it passible to reboot a single server without the service going down.,3,0,0,False,False,False,1639819263.0
risxvd,hp124if,t3_risxvd,"Just to add to what’s already been said, it *is* possible to keep a desktop computer running for weeks or months without noticeable performance degradation - is all about what OS and software you run on it.

For example, my last three Macbooks would run 6-9 months between reboots, and my work Windows laptop is the first Windows machine I’ve owned which could almost match that, probably due to memory management improvements in Windows 10.",2,0,0,False,False,False,1639817177.0
risxvd,hp1dkpm,t3_risxvd,"I usually reboot my desktop computer maybe once a quarter on average, though I should probably do it more often. Old habits, I guess. (Running Linux, Ubuntu to be specific.)

Server uptime was more of a thing sysadmins bragged about in the past. Today if someone says a server's been going without a reboot for two years I just think ""yikes, gotta be some unpatched security holes in that kernel"". Most server software supports running across multiple instances today, so usually you can take servers down for maintenance/upgrades without anyone noticing.",2,0,0,False,False,False,1639826999.0
risxvd,hp1dycz,t3_risxvd,"software in computers at less tested and  leaks more memory

Software in servers its better done and reviewed.

what is a memory leak?

well programs request to kernel memory space, when finish they release it and the kernel can offer that space again.

if a app crash or the app code forget release the memory.. we would hava a memory leak. Kernel not going to offer that memory because assume it in use by other app.",2,0,0,False,False,False,1639827306.0
risxvd,hozieyj,t3_risxvd,"It's the software, the server's softwares are so much efficient in terms of memory, but a normal pc is not, piling up along the way. If you know what you are doing you can get decent up time without being lagging. I was able to run my pc for 12 days straight with little to no loss in performance.",6,0,0,False,False,False,1639785039.0
risxvd,hoztsot,t3_risxvd,It's all your electron apps slowly consuming your available memory (and memory leaks and other bugs in your desktop software that aren't present in server software),0,0,0,False,False,False,1639790242.0
risxvd,hp09fb6,t3_risxvd,It's pretty much entirely the software.,0,0,0,False,False,False,1639797933.0
risxvd,hp1n50x,t1_hp0wr8r,especially when you see those repair videos with like cockroaches coming out the vents it’s no surprise they don’t run well,1,0,0,False,False,False,1639833912.0
risxvd,hp1do1i,t1_hp14jsd,"A server not need rebooted except for upgrade the kernel or change some failed hardware.

Nobody care if a server is running for months or years without reboot.

Even those of us who have servers at home avoid reboots because we like see high uptimes. 

We never reboot the servers in my company, except for kernel upgrade.

PS:i known its possible upgrade the kernel without reboot",1,0,0,False,False,False,1639827073.0
rinkr5,hoy78ym,t3_rinkr5,">If data loss will occur, tell me the limit for both. With and without data loss.

I suspect the answer to your polite question regarding lossy compression is that you compress 1TB to one eighth of a byte.

Always happy to help!",89,0,1,False,False,False,1639765800.0
rinkr5,hoy7eyn,t3_rinkr5,Compression depends on the contents. A 1TB of 0 could arguably be encoded as 1 bit of 0 and 40 bits to store length.,77,0,0,False,False,False,1639765863.0
rinkr5,hoy80su,t3_rinkr5,"Depends on the data. If it's a random stream of zeroes and ones, it cannot be compressed at all. If it's a terabyte of all zeroes, it could be compressed by 99.9%.

Compression looks for patterns and duplication, so if there are no patterns, there is nothing to compress. Different techniques are used to compress text, images, videos, and audio streams.",57,0,0,False,False,False,1639766092.0
rinkr5,hoy7gfj,t3_rinkr5,Depends on the data in it.,30,0,0,False,False,False,1639765878.0
rinkr5,hoyih8t,t3_rinkr5,"Every data can be compressed to a single bit, that just holds the information if it is the compressed data or not. All other information than has to be in the decoder. That was always the case, in 2000 as well as in 2010.

For more information you have to get into information theory.",13,0,0,False,False,False,1639770144.0
rinkr5,hoyukfc,t3_rinkr5,"It really, really depends on what you're compressing. It's impossible to give a number without some kind of an idea of what the data are, and what the shares of different kinds of files and formats are.

Photos and videos can hardly be compressed further using lossless, general-purpose compression. That's because most common image and video formats already employ compression, often both [lossy](https://en.wikipedia.org/wiki/Lossy_compression) and [lossless](https://en.wikipedia.org/wiki/Lossless_compression) compression, and applying compression on top of compression generally doesn't give you much. There might be some slack to be picked up by state-of-the-art lossless compression but not a lot. I'd expect almost no gain from compressing already compressed image or video files.

Word documents and other documents from the modern Office suite are also already compressed, and while the compression used in the file format may not be bleeding-edge and you can probably compress it a little more with a better compression algorithm, it's probably not going to be that much.

Most video games that can take a lot of space also come with the majority of their assets compressed nowadays.

Plain text compresses fairly well, but few people have plain text files taking a lot of disk space. Uncompressed image files may also compress well, depending on the image, but few image formats that are commonly used today are uncompressed.

All in all, if there's a lot to be gained by compressing the data, it's probably already compressed.

With that said, my main backup drive that doesn't include operating system or program files, and excludes most video files, has ~130 GB compressed into ~103 GB.

If you're asking with a more practical matter in mind, why not give your practical scenario instead?",6,0,0,False,False,False,1639775005.0
rinkr5,hozk09n,t3_rinkr5,"Theoretically, not depending on the content, nearly 0: https://github.com/ajeetdsouza/pifs",3,0,0,False,False,False,1639785757.0
rinkr5,hp0bkje,t3_rinkr5,[Shannon's source coding theorem](https://en.m.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) has the answers you seek.,3,0,0,False,False,False,1639799079.0
rinkr5,hp0joef,t3_rinkr5,"What is in the file?  All zeros... the compression will be awesome.  

If it is images or movies, then not so much",3,0,0,False,False,False,1639803658.0
rinkr5,hoy832a,t3_rinkr5,"I'm not an expert on data compression but I believe it depends on the contents of the file. How I remember compressions algorithms working is you look for repeating sequences in the file and then substitute those for smaller sequences and have some sort of table to record the switches.

So it your file is 000**111**000**111***0101* you can break that up into 000, **111**, *0101* and say

000 -> 0

111 -> 1

0101 -> 10

So your new file would be: 010110 (plus the overhead for the table)

So if your entire 1 TB file was all 0s you could compress the shit out of it whereas the less repetitions the less it could be compressed.",7,0,0,False,False,False,1639766115.0
rinkr5,hoyxg5f,t3_rinkr5,Entropy,2,0,0,False,False,False,1639776175.0
rinkr5,hp03gmo,t3_rinkr5,Depends on how random the sequence of bits are. If I had to guess the bound would be log(n) as in the case of a file that stores all 0 bits we could just write the number of bytes.,2,0,0,False,False,False,1639794827.0
rinkr5,hp0kfoj,t3_rinkr5,"The lossless compression limit depends on the randomness of bits in your data. Let me explain what I mean by randomness of bits. Something is random if no patterns cannot be found in the data. For example, `11111111` is not at all random, it has a pattern which is ""every bit is 1"".

> If the data is redundant, meaning that it has no patterns but a pattern of uniformity, it conveys no information because it has no order. This relation can be stated as follows:
Randomness ∝ predictability ∝ information ∝ order ∝ regularity of data

^ An extract from a paper I wrote about randomness but never published it.

If you had photos with all the pixels of same values. You could compress it to about `1/resolution + bits required to store the length of resolution` size.

Compression is just representing a relatively large amount of data in a concise form. For example `11111111` will be compressed as ""1 for 0100 bits"". If it is a random(explaining what is random, would make the comment humongous so leave it out) pattern, you cannot compress it because you cannot express something random in some other form because you do not know anything about its pattern.

So, now answering all your questions one by one:

> Does anyone know how far a one terabyte file can be compressed? What’s the limit of today’s technology compared to 2000 and 2010?

Not much really because the compression algorithms which are the norms haven't changed much since 2000s, we still use JPEG widely which was originally developed in the late 1980s. We are still using H264 which was originally developed in 2003. H265 was developed in 2013. So compression hasn't changed much in the last 10-20 years.

You can compress an image without loss of data to about 10% with JPEG encoding, but again this is based on the randomness of the pixel values.

> If one terabyte holds 1,000,000,000,000 bytes, what is the utmost limit of compression?

The utmost limit of compression will depend on the data in that 1 TB of data as explained earlier in the comment.

> If data loss will occur, tell me the limit for both. With and without data loss

Well, its in your hands whether data loss will occur or not. You can compress an image to like 50% its size with loss of data but usually the loss of data is not traded for high compressibility. After a compression constant `k`, the compression is directly proportional to the loss of data.",2,0,0,False,False,False,1639804104.0
rinkr5,hp0v372,t3_rinkr5,"Have you ever heard of 42.zip?

It is a zip bomb, that compressed is only 42kB in size. Fully extracted it is 4.5PB",2,0,0,False,False,False,1639811438.0
rinkr5,hp27tpe,t3_rinkr5,"It depends on the algorithm you use.  
  
The simplest forms of compression work best on data with lots of *repeated* byte sequences, whereas more sophisticated ones may look to identify patterns that can be converted into a base point and a mathematical function.",2,0,0,False,False,False,1639844485.0
rinkr5,hoynxsl,t3_rinkr5,"Image and video encodings already have compression built-in. It's not optimal since people don't want to wait two seconds to decompress one second worth of video, so you can probably do a bit better. If you have an equal number of txt, jpg, and mp4 files, the videos would dominate the storage.",1,0,0,False,False,False,1639772336.0
rinkr5,hozg1oe,t3_rinkr5,"For music, lossless compression is about 50%.  Lossy can do more depending on how much you want to notice the compression. Video is likely similar. Text based compression depends totally on the text. If a lot of repeated character patterns exist, you can achieve a lot of lossless compression. Random characters get very little compression.",1,0,0,False,False,False,1639783950.0
rinkr5,hozy7ez,t3_rinkr5,"1000000000000 of the same content

log2(1000000000000) < 40

so you need 40 bits of multiplier and 1 bit of the data.

or up to 99.9999999959%  for single time compressing.",1,0,0,False,False,False,1639792308.0
rinkr5,hp12k6u,t3_rinkr5,"http://mattmahoney.net/dc/

Probably the best resource on the web I've seen for this.",1,0,0,False,False,False,1639817548.0
rinkr5,hp17zvh,t1_hoy78ym,i’m sure it would still be readable after the decompress,4,0,0,False,False,False,1639822262.0
rinkr5,hozkf2h,t1_hoy7eyn,Or 1 bit with a zero and always assume a 1TB length,31,0,0,False,False,False,1639785940.0
rinkr5,hp29bsy,t1_hoy80su,"Everything has a pattern even if the file itself is the pattern, but a “unique” file cannot be compressed much at all. What’s important is whether a pattern **repeats**.",1,0,0,False,False,False,1639845157.0
rinkr5,hozhfd0,t1_hoy7gfj,"This. Generally, data compression algorithms rely on knowledge of what is being stored. For example, in images, in order to make the JPEG format, there was a bit of research done on quantizing photos in the frequency/cosine domain. They use a different quantization coefficients for each frequency in an 8\*8 block as it makes the image look similar enough even though the MSE  compared to the original image actually increases.",5,0,0,False,False,False,1639784591.0
rinkr5,hoyytwd,t1_hoyih8t,"While I'm certainly partial to CS theory, and information theory and compressibility are definitely interesting, I think your answer goes a bit too far into the technically correct but almost entirely unhelpful territory.

OP's question doesn't really give enough information to give the answer they're asking for but information theory is hardly going to be helpful to a layperson unless they're willing to go down an entire rabbit hole to answer their question.

So, this answer kind of gets both an upvote and a downvote, mentally.",9,0,0,False,False,False,1639776736.0
rinkr5,hp28q1w,t1_hoy832a,"Your scheme is broken.  
  
000**111**000**111***0101*  
  
would convert to: 010110, but

010110  
  
would convert to: 000111000111111000, because you don’t know that “10” isn’t a “1” followed by a “0”.  
  
The issue is that you haven’t specified that a “1” cannot be followed by a second “1” and you cannot distinguish a sequence like this:  
  
10  
  
Is it 111000 or 0101 ?  
 
——  
  
A better system might use two bits of starting data
  
00, 01, 10, 11  
  
You’d still have to be careful about anything with an odd number of bits and come up with a way to encode the original data in fewer bits.",1,0,0,False,False,False,1639844890.0
rinkr5,hp2aoyr,t1_hp0v372,"Yikes.  
  
Pretty sure those are special *synthetic* sequences that look like a near infinite set of files, though…",2,0,0,False,False,False,1639845764.0
rinkr5,hp2aw45,t1_hoynxsl,"Strictly speaking that’s *encoding* rather than *compression*, but it does reduce the storage space needed.  
  
You can potentially stlill compress the result, also.",1,0,0,False,False,False,1639845852.0
rinkr5,hp29zac,t1_hp17zvh,"If it weren’t decompressible, then it wouldn’t be compression. That would just be straight up data loss.",1,0,0,False,False,False,1639845445.0
rinkr5,hozze1s,t1_hozkf2h,Or 0 bits if your compression algorithm assumes an empty file to be exactly 1 TB of 0's.,46,0,1,False,False,False,1639792868.0
rinkr5,hoz5gud,t1_hoyytwd,"Thanks. You managed to perfectly verbalize my intension while writing my comment.

Edit: For lossless compression: Compression is very well studied. So there is no practical differences between the algorithms now, 10 or 20 years ago. We basically already got the optimal solution. By how much you can compress data depends entirely on the self-information or entropy of the data. This is well defined and gives an lower bound to the compressed data size. Modern (>= year 2000) algorithm achieve near optimal compression for ""real world"" data.",2,0,0,False,False,False,1639779458.0
rinkr5,hp2ku83,t1_hp28q1w,You're right. It's not an actual algorithm; it's to convey the idea of substituting smaller bits for larger sequence of bits,1,0,0,False,False,False,1639850205.0
rinkr5,hp2aeri,t1_hp29zac,maybe that’s were the world of data compression is heading just straight up data loss,3,0,0,False,False,False,1639845639.0
rinkr5,hp08y8p,t1_hozze1s,This guy algorithms.,28,0,0,False,False,False,1639797683.0
rinkr5,hp0fal1,t1_hozze1s,This guy assumes.,17,0,0,False,False,False,1639801120.0
rinkr5,hp1q583,t1_hoz5gud,I wonder why you think that compression is solved. There is still a lot of research going on in the field. Just because the theory given a specific information source is done this doesn't mean that the application is straight forward. The lossless compression theorem really is only the starting point. Applying this to the vast probability spaces of for example image data relevant to humans is the difficult part.,1,0,0,False,False,False,1639835688.0
rinkr5,hpg3wc9,t1_hp1q583,"Regarding images: JPEG2000 was (as the name suggests) developed in the year 2000. It was developed to be the successor of JPEG, but we still use JPEG. There are many improvements, but they just are not requested, because the already available algorithm are good enough for all use cases and the improvemnts are not big enough.",1,0,0,False,False,False,1640107712.0
rihkz0,hoxgzd8,t3_rihkz0,"Before you dive into software architecture, I'd recommend working on becoming a great software developer first. The book gets a lot of slack, but for young developers I still recommend Clean Code by Bob Martin. Once you've completed (or better yet, while!) some introductory background reading, start implementing the principles discussed in a language you're comfortable with. Small, simple examples with easy to understand conventions will go a long way. Next, take those principles and use them in an existing architecture - MVC is a great one for Java.

From there, it's good to start working on how you organize your software. For this I like to recommend A Philosophy of Software Design (2018) by John Ousterhout for a supplemental approach to the ""historical knowledge"" as well as studying design patterns. [Game Programming Patterns](https://gameprogrammingpatterns.com/contents.html) provides a great overview of some of the most common design patterns you'll see in software through an application that's very approachable through most - game design. Design patterns are great to know because if you can employ and identify them properly, it will be easier for others to understand what your code is doing and vice versa. This is also the point where you start incorporating those principles you learned above into larger projects. 

Finally, to answer your exact question of ""how to understand large and scalable projects,"" [The Architecture of Open Source Applications](http://aosabook.org/en/index.html) is what you're looking for. Practice taking a problem, designing a solution for it, and comparing against what the author implemented. But work on the above first. 

This is a lot of information, so save this post and come back to it. Software development is a lifelong and ever changing endeavour, so enjoy the journey!",9,0,0,False,False,False,1639755753.0
rihkz0,hoxohf9,t3_rihkz0,"I don't want to sound trite or demeaning but architects  are made from experience and mentorship. there are lots of books about architecture and you should read many of them. Ultimately architecture is balancing tradeoffs, technological, resources and political.  Sometimes it's winning hearts and minds, sometimes it's hearing cats , sometimes it's leading brilliant people who are honestly probably smarter than you.",2,0,0,False,False,False,1639758635.0
rihkz0,hoxv985,t3_rihkz0,"Fundamentals of Software Architecture
Software Architecture: The Hard Parts

By Neal Ford and Mark Richards.  And their bi-weekly conversation at https://www.developertoarchitect.com/foundations-friday-forum.html 

Solid advice in those two books.  And echoing the thought that architecture is a skill developed by experience, they hold a twice-yearly “Kata contest” where they present an architectural problem and have teams compete over the course of several weeks to produce and document the best architecture.  I’m biased, because I’m one of 5 judges for that contest.",2,0,0,False,False,False,1639761191.0
rihkz0,hox9bdw,t3_rihkz0,Honestly you don't learn those from books. You learn from experience... Just actually do projects from smaller ones to bigger ones.,0,1,0,False,False,False,1639752643.0
rihkz0,hoxu64d,t3_rihkz0,"Learn to design in UML and maintain documentation for your software, free code camp has an excellent UML intro on YouTube.",-1,0,0,False,False,False,1639760779.0
rihkz0,hoxhdpv,t1_hoxgzd8,Awesome! Thank you for the detailed response. I've been doing a lot of JFX work at my job recently so I've been interested in building my own software on free time and the hardest thing I'm finding is how to start and not have to restart later on due to design. MVC I'm already familiar with bur always interested in diving deeper and learning more! Thanks again.,1,0,0,False,False,True,1639755908.0
rihkz0,hozding,t1_hoxv985,"This is very interesting! I actually have both these books on my reading list and can't wait to get around to them. 

Do you mind giving more details on how the Kata is judged? I'd be interesting in taking part.",1,0,0,False,False,False,1639782843.0
rip3jw,hoytzez,t3_rip3jw,"What is your question, precisely?",1,0,0,False,False,False,1639774772.0
rip3jw,hoyvglr,t1_hoytzez,sees like only problem of the imaginary program P that can solve halting problem is self reference( i think that is where loops come from in my original post).{can’t we just say program P will solve halting problem for all programs except itself* .what is wrong in saying that instead of just saying since loops can’t exist this program won’t work outside self-referencing as well and no such program exists.,3,0,0,False,False,True,1639775363.0
rip3jw,hoyz3y0,t1_hoyvglr,"Well I think the idea was to solve the halting problem, and no program can solve it completely, hard time understanding your point here.",3,0,0,False,False,False,1639776848.0
rip3jw,hp02zii,t1_hoyvglr,"I've thought about the same thing, and I actually believe that's the case.

We do know that if you don't allow recursion in any way, then it's completely decidable.  Thus, it's something about recursion that causes the issue.

However, recursion is _not_ sufficient to cause undecidability.  It's necessary, but some recursive programs are decidable.

My theory that has yet to be disproven:  if we disallow the known program that causes undecidability, it becomes decidable.  It can't decide on itself, but it can on anything else.",-2,0,0,False,False,False,1639794582.0
rip3jw,hoz0n9k,t1_hoyz3y0,"I know, I don’t quite know how to put it. my assertion is that,as said above, the only proof given is refusal of any halting solution is that it breaks down in self referencing. How does that say anything about literally infinite other programs that our supposed solution will work on. I mean all we have done is refute a very specific program. And we still aren’t sure about others. So can’t we say there may or may not be a partial solution( partial because self reference is not permitted) instead of declaring outrightly that nothing of the sort exists.",2,0,0,False,False,True,1639777473.0
rip3jw,hoz6uqx,t1_hoz0n9k,"It says that your program will never be 100% correct.

More precisely, it raises the question of how your attempt at solving the halting problem would cope when fed itself.",2,0,0,False,False,False,1639780038.0
rip3jw,hozouxl,t1_hoz0n9k,">  I mean all we have done is refute a very specific program. And we still aren’t sure about others.

Sure. The proof does not identify any specific program that every attempt at halting analysis will get wrong. There is no such program.

We *know* there are partial solutions. It's easy to write an analyzer that gives the right answer for some programs, though it will give the wrong answer (or no answer) for other programs. You can also pick any program you like and write an analyzer that gives the right answer for that specific program (either `print(""yes"")` or `print(""no"")` will be a suitable answer, though it might take you a lot of thinking to figure out which one).",1,0,0,False,False,False,1639787956.0
ri2yda,houjfs2,t3_ri2yda,"Yes, it's trivial. The vulnerability is well over a decade old.",9,0,0,False,False,False,1639698486.0
ri2yda,hoydzmd,t3_ri2yda,Fond memories of moving into a new flat and borrowing neighbour’s WEP-protected WiFi until my broadband was activated,1,0,0,False,False,False,1639768380.0
ri2yda,hovvzyn,t3_ri2yda,There was a lot of hub bub about that song but I didn't really find it that vulgar.,-2,0,0,False,False,False,1639721604.0
ri2yda,howf8gs,t3_ri2yda,You do realise WEP is shit/old/insecure and hasn't been recommended for over a decade now?,-8,0,0,False,True,False,1639735542.0
ri2yda,houjv9a,t1_houjfs2,Could you please elaborate how? I have seen references to this online but I can't find a clear explanation of how the ciphertexts can be modified/swapped.,4,0,0,False,False,True,1639698675.0
ri2yda,howfh6x,t1_howf8gs,"Yes, I'm learning about all the different wireless security protocols",5,0,0,False,False,True,1639735733.0
ri2yda,houm13o,t1_houjv9a,Well it's not swapped per se. You de-auth attack and collect the IVs by the thousands. it's been a while so I apologize going off memory. Like I said old attack. Check out a tool call Air-crack-ng if it's still around.,7,0,0,False,False,False,1639699638.0
ri2yda,hovw5vv,t1_houjv9a,"Here's a good pdf with all the information you need to understand why WEP is broken. 

https://www.opus1.com/www/whitepapers/whatswrongwithwep.pdf",2,0,0,False,False,False,1639721710.0
ri2yda,howi8sz,t1_howfh6x,"Ah, fair enough.",1,0,0,False,False,False,1639737834.0
ri2yda,houq2oj,t1_houm13o,[deleted],2,0,0,False,False,False,1639701488.0
ri2yda,houqc5m,t1_houq2oj,[deleted],2,0,0,False,False,False,1639701608.0
ri2yda,houry45,t1_houqc5m,Yep WEP is trivially cracked. SSL also has it weaknesses in older versions but as long as the TLS is higher than 1.0 and SSL is higher than 3.0  it's pretty pretty good shape :),2,0,0,False,False,False,1639702329.0
ri6duj,hov5oyf,t3_ri6duj,"O(n^2), as you suspected, assuming no ordering on the array's elements. That's quadratic time; exponential should be O(2^n) or similar. Check the difference for n = 10, 20, 30.

If the array is ordered, I think that a O(n) algorithm is possible: map b_i = target - a_i, reverse b, then compare a and b side-by-side until the elements match.",2,0,0,False,False,False,1639708494.0
ri6duj,howp4yq,t3_ri6duj,"The commenter on YouTube might have been tricked into thinking it's in O(n) because the inner loop runs only for the remainder of the array after the *i*th element rather than always going through the entire array, and that remainder gets shorter for every run of the outer loop.

However, the inner loop still makes n-1 iterations on the first iteration of the outer loop (when i=0), and one iteration on its last run (when i=len(arr)-1), leading to an average of circa n/2 iterations of the inner loop per iteration of the outer loop. Since the outer loop is executed n times, that leads to a total of approximately n * n/2 iterations of the inner loop, which is in O( n^2 ), not in O(n).

That's in the worst case, of course; if matching elements are found, the algorithm stops before going through all of that.

There's probably an off-by-one or some other inaccuracy somewhere in the above, but the big picture should still be right.",1,0,0,False,False,False,1639742595.0
ri6duj,hov6o73,t1_hov5oyf,"Oh, yea sorry I meant quadratic, not exponential.

That makes sense. In this case, the array wasn't ordered.",2,0,0,False,False,True,1639708933.0
rhll1c,hor7nbs,t3_rhll1c,"How many bytes a character has depends on the character table you use. Some examples: ASCII (1byte per char), UTF-8, UTF-16, the thing that microsoft uses.

Of course, a space is a character itself. Also a backspace or enter is.

Any you are right, for all praktical cases 1 byte equals 8 bit.",24,0,1,False,False,False,1639640716.0
rhll1c,hordd9v,t3_rhll1c,"It is safe to assume that a byte is 8 bits, though in the past that wasn't always the case.

How a character is represented in data depends entirely on its *encoding.*

SMS text messages use either a 7 bit or 16 bit encoding, so either one or two bytes per character.

Email messages can be sent in HTML format, which permits any coding the sender and receiver can both handle.  For example, UTF-8 is one to four bytes per character.",10,0,0,False,False,False,1639645215.0
rhll1c,hoxcvq0,t3_rhll1c,"It sounds like you have a specific use case in mind. Perhaps if you explained why you want to send emails and texts at 1 byte per char, we might be able to help you better, rather than all he possible byte/character encoding permutations?",2,0,0,False,False,False,1639754114.0
rhll1c,hordmlp,t3_rhll1c,"Sms messages tend to use GSM 7 bit encoding (https://docs.huihoo.com/symbian/s60-5th-edition-cpp-developers-library-v2.1/GUID-35228542-8C95-4849-A73F-2B4F082F0C44/sdk/doc_source/guide/System-Libraries-subsystem-guide/CharacterConversion/SMSEncodingConverters/SMSEncodingTypes.html), although they can vary.

Emails are even more variable, you can set what encoding you want, but UTF8 is typical as a default. Spaces are indeed characters.

The amount of bytes per character will vary by compression.  In a plain text doc, one character will equate to 1 byte, but in a pdf, one byte will give you something like 3 characters.

Which is to say as so many things in programming,  for all your questions the real answer is it depends, there is not a single correct response.",1,0,0,False,False,False,1639645431.0
rhll1c,hosenhe,t3_rhll1c,"Your assuming a character takes up 1 byte - while it’s the case a lot of times, a lot of newer encodings are a lot larger, like utf-16 and 32, which take up 2 or 4 bytes respectively",1,0,0,False,False,False,1639667905.0
rhll1c,hotdncj,t3_rhll1c,"What encodings are still 100% fixed. Also, which encodings are fixed at 8 bits per character? 

For UTF, how would you compress and keep the byte size at 8 bits per character, strictly. Which encoding keeps everything set at 8 bits per character? The ASCll encoding takes up 8 bits and or one byte per character? That’s 100% preset? It’s impossible to increase the byte size per character if you were utilizing ASCll? 

Let’s say you were sending the message via electromagnetic waves as a radio wave, if the wave became energized due to an outside influence, would the data become larger or more compressed? Or would nothing happen? Or if it was too powerful, would it just not send due to it acting as an EMP/jammer? 

While utilizing UTF-8, it’s impossible to compress anything to one byte? It’s all set between 2 to 4 bytes? One byte seems limiting, however, doesn’t that allow for larger messages to be sent if the message limit is a certain number. It seems more useful than the higher grade encodings.",1,0,0,False,False,True,1639681577.0
rhll1c,hou49yj,t3_rhll1c,"ASCll, Latin-1, and UTF-8 are capable of utilizing one byte per character? For all characters, strictly? Aside from non-English characters, correct? Spacing, punctuation, and other sorts of symbols would still be one byte though, right? 

Emojis don’t really matter, however, I might as well ask anyways. How many bytes per emoji? 

Older email clients used to send 7-bit? That means it would be a little less than 8-bit, right? Meaning, it is 1 bit less thus it isn’t a byte unless you send another character? If this is correct, what email clients were those? Any idea? 

Also, how many bits and bytes are a character in newer email format? Is it possible to have one byte per character when writing an email? 

When you click the send button, does the send signal become added data on a wavelength?",1,0,0,False,False,True,1639692024.0
rhll1c,houq5qf,t3_rhll1c,Be aware that email transmission includes message headers that are not part of the message that included details such as routing information. You can read more about message headers [here](https://jkorpela.fi/headers.html),1,0,0,False,False,False,1639701526.0
rhll1c,hov4o2v,t3_rhll1c,look up an ascii table. they have the 8 bits in all forms and shows the 256 corresponding characters,1,0,0,False,False,False,1639708036.0
rhll1c,howc2oi,t3_rhll1c,In addition to what everybody else said have a look at [https://en.wikipedia.org/wiki/Quoted-printable](https://en.wikipedia.org/wiki/Quoted-printable). It encodes 8bit characters into 7bit characters but needs more of them. It is still used for email. Maybe not often but every email program needs to be able to interpret it.,1,0,0,False,False,False,1639733052.0
rhll1c,hor805a,t1_hor7nbs,"Out of curiosity, do you know what character table is used within text messages? Also, do you know what character table is used within email messages? I’m trying to understand if both of these messages carry the same data if they both sent the same message. Let’s say that you messaged, “Hello there!” You emailed and texted this exact message. Would they both be sending the exact same byte size? If not, what’s the difference. 

Also, do spaces count as a character and or byte? 

Lastly, you said enter counts as a byte? Meaning, if you were to send a message that was 36 bytes and the limit was 36, you wouldn’t be able to send the message because the enter counts as a byte?",5,0,0,False,False,True,1639640980.0
rhll1c,hore9kn,t1_hordd9v,"That wasn't always the case? If that’s true, how many bits used to be a byte? Any idea? Also, do you know what year a byte become 8 bits? 

So, the character H and the Character 7 would equal the same amount of bytes? Which is one or two depending on the encoding? 

SMS is either one or two bytes per character? That’s helpful. That is the biggest question of mine. I’ll have to see if I can find a definitive answer to that. 

An email message can contain a single byte per character? That’s 100% possible? Secondly, does clicking the enter button count as a byte? Let’s say 36 bytes is the max. You type a message with 36 bytes and then click enter, but it doesn’t send because the enter counts as a byte? Is that a thing or no?",0,0,0,False,False,True,1639645975.0
rhll1c,hotewbb,t1_hordmlp,"So, an IPHONE would utilize GSM 7 bit encoding? Do all phones utilizing GSM 7? Does GSM contain 7 bits per character? I know 8 bits is a byte, meaning, each character would be slightly shorter than the typical byte, right? Are all the characters set at a certain byte/bit length? Like one byte or 7 bits per character? 

For an email, how would you set a certain encoding? For example, how would you choose an encoding that is strictly one byte per character? Where is the option or setting to enable such a thing? Is the default one byte per character? What is typically the default and how many bytes per character is the average? 

A plain document, a character equals one byte? Though, a pdf would equal three characters? Is that one percent true? If it is, couldn’t you technically compress a message into a pdf and send it as such?",1,0,0,False,False,True,1639682070.0
rhll1c,hotf8db,t1_hosenhe,"Which encodings take up one byte per character nowadays then? It seems limiting to prevent one character from sending as a single byte. If a character was sent as a single byte, you’d be able to send more data due to the limit of text being higher.",1,0,0,False,False,True,1639682203.0
rhll1c,howc3kr,t1_howc2oi,"**[Quoted-printable](https://en.wikipedia.org/wiki/Quoted-printable)** 
 
 >Quoted-Printable, or QP encoding, is a binary-to-text encoding system using printable ASCII characters (alphanumeric and the equals sign =) to transmit 8-bit data over a 7-bit data path or, generally, over a medium which is not 8-bit clean. Historically, because of the wide range of systems and protocols that could be used to transfer messages, e-mail was often assumed to be non-8-bit-clean – however, modern SMTP servers are in most cases 8-bit clean and support 8BITMIME extension. It can also be used with data that contains non-permitted octets or line lengths exceeding SMTP limits. It is defined as a MIME content transfer encoding for use in e-mail.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1639733073.0
rhll1c,horbeki,t1_hor805a,"Email usually uses UTF-8. Like 99% of all web applications. I would guess that most messenger also uses UTF-8, maybe also UTF-16 scince this supports for example chinese character.",9,0,0,False,False,False,1639643631.0
rhll1c,horj9tq,t1_hor805a,"To answer the last part of your question a bit more in depth.

If you are using a common messenger, you typically press enter to send the message, and if you want a new line you have to press shift+enter. In a email, you can just press enter to get a new line and have to klick on a button to send the email.

This tells us, that a messenger captures the enter key and uses it as a ""command"" to send the text, while the email programm captures the enter key and uses it to append a new line to the text.

So the same key can do different things on different programms. This is because the programm, and not the keyboard, decides what to do with the pressed key. Because of this you also can easily switch between different keyboard layouts without replacing the keyboard.

To answer the question, the enter is not appended to the message. The reciever knows that you send the text because he recieves the text, so there is no need to put an extra indicator at the end of the message.",3,0,0,False,False,False,1639650167.0
rhll1c,horid8j,t1_hor805a,"Edit: There is more to transmitting a message than just the text. What metadata is send highly depends on the way you send it. So I would guess that a email  transmitts more data compared to a messenger.

You can see the data transmitted for an email in almost any email desktop client. There should be a option to ""view source code"".

Edit2: Space count as character. Also newline, tab and all other, so called ""printable characters"". Besides those, there are also ""non-printable caharacters"" like enter or backspace you could theoretically send to someone.

Edit3: If you send a message you do not send the enter. The programm does not append it to the message, the enter is catched by the programm and than the message is send. But you wount be able to append a new line to the message if no bytes where left.",1,0,0,False,False,False,1639649418.0
rhll1c,host0b2,t1_hore9kn,"> So, the character H and the Character 7 would equal the same amount of bytes? Which is one or two depending on the encoding?

Some encodings are fixed-length, i.e. all characters are encoded with the same number of bits. This is how many common 8-bit text encodings worked in the past: every character was exactly 8 bits. That also set an obvious limit on the maximum number of unique characters that could be represented, as there are 2^8 = 256 unique combinations of 8 bits that are possible.

Other encodings are variable-length, and a single character can take one or more bytes to represent. The most common text encoding on the web is UTF-8 where every character takes between 1 and 4 bytes. The most common characters in English text (the ones that are included in ASCII, which is an old character encoding standard) take up 8 bits, or one byte. This would include the English alphabet, digits 0 to 9, and a number of other characters, but not many non-English letters. This allows the encoding to be compatible with the old ASCII standard. There are more than a million other characters that are possible in UTF-8, but they can take up two to four bytes per character.

So, even when using a variable-length encoding, H and 7 are still likely to take up the same number of bytes, but H, ü and 国 can take up different numbers of bytes.

I don't know about the text encoding used in SMS specifically, but as u/CarlGustav2 said, it seems like there are two possible encodings, one of which is a fixed-length 7-bit encoding, with the other one being a fixed-length 16-bit one.

Edit: clarified choice of words",2,0,0,False,False,False,1639673588.0
rhll1c,hosvs05,t1_hore9kn,"> An email message can contain a single byte per character? That’s 100% possible?

Yes, if it's using a fixed-length 8-bit (or 7-bit) encoding. A 7-bit encoding would allow basic English text but no international characters; an 8-bit encoding such as [ISO 8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) would allow some non-English characters but the set of characters would depend on the encoding, as no 8-bit encoding can have enough unique combinations of 8 bits to represent letters used in all languages. (Some languages such as Chinese or Japanese of course have thousands of characters all by themselves, so they wouldn't fit in *any* 8-bit encoding even on their own.)

If you want to know whether an email message consisting of 100 written characters can actually fit in 100 bytes, it's worth noting that email messages also include various control information in so-called headers, including the sender and recipient, the text encoding used, and various other things, so a full email message is actually going to take up more space than that.",2,0,0,False,False,False,1639674660.0
rhll1c,hosc7td,t1_hore9kn,"It might help to understand why we need different encodings. Note that a byte (8 bits) can represent 2^8 = 256 different characters. Now this is clearly enough to represent the English language. The most common 1-byte encoding is usually ASCII. Just Google ""ASCII table"" to see the encoding. On the other hand, what if you want to represent almost all languages? You will need more bits to make that work. If you instead use a two byte encoding then you can represent 2^16 = 65,536 characters. Now this is enough to represent pretty much all characters you'll need globally. 

So essentially, if the application you're using allows characters other than English then it's probably using 2-byte encoding. Single byte encoding was mostly used back before the whole world had internet and every language needed to be encoded. 

For your last question, yes, enter is going to be a newline character (or possibly two characters if you're on windows). To you when you click enter it looks like nothing there, but realize that the reason your cursor moves to the next line is because the text editor is showing you the message being typed, and the only way anything changes on the screen is if a new character is entered. This character has a special encoding that the text editor understands, and in turn it pushes the cursor to the next line when it sees this character.",1,0,0,False,False,False,1639666885.0
rhll1c,hou3atx,t1_hotf8db,"Ascii, utf-8. Yes certain encodings work better if you aren’t going to use certain characters",1,0,0,False,False,False,1639691633.0
rhll1c,hotzndh,t1_horbeki,"Just to clarify, UTF-8 and UTF-16 (and UTF-32) are all different ways of encoding the same characters and all sort the same set of characters (eg Chinese). For most common use cases, UTF-8 encodes a message in the fewest bytes out of them but requires the most processing to handle non-ASCII characters. UTF-32 required the last processing but uses four times as much memory as UTF-8 for ASCII characters.",4,0,0,False,False,False,1639690188.0
rhll1c,hotc918,t1_horid8j,"There is more to sending an email? Well, let’s say that both the email message and text message contain the exact amount of bytes. In this scenario, I’ll say both contain 36 bytes. If that’s true and the transmission signal adds a certain length of extra data, how great would the difference be? If you know the question, that’d help a lot. 

Also, people are saying you can change the encoding for an email. How would I go about choosing an encoding that is strictly one byte per character? Or is that already the preset encoding software? As for the email, let’s say I’m using the google email or hotmail. 

Space counts as a character? I see. That makes sense. Does a space contain one byte like every other character? Or since it is blank, is it lowered to lesser bits? 

Enter doesn’t count count as a byte? How about as data? Since it’s caught by the program and then sent out, surely it contains wavelength data of a sort. Any idea of the specific amount?

If you create a new line, does the combination of shift+enter equal two bytes if you are utilizing a encoding that equals 8 bits per character. Thus one byte a character. For practical reasons, let’s say that there was only one space skipped on the last line, meaning, it would be 3 bytes now. Right? Or does skipping a line not take enter+shift as a double character command? Merely just a single command since it is utilizing a single command to create a single character. If you know what I mean. That’s for the phone data.

Pertaining to the computer, you said I’d only have to click enter to skip a line. Does the enter count as a byte if you are creating a new line?",1,0,0,False,False,True,1639681029.0
rhll1c,hotfvmk,t1_hosc7td,"I could technically set an email message to utilize ASCll? That’s correct, right? 

If enter counts as a byte, does the enter still count if it is clicked to send a message? Similar to the send button. From other comments, people says it is more of a message catcher that sends the data off. Meaning, it isn’t a byte. I haven’t gotten replies if it is still data compressed into the sent message though.",1,0,0,False,False,True,1639682460.0
rhll1c,hou521f,t1_hou3atx,"If I were to only use basic characters, which encoding should be utilized? Strictly one byte per every one character? Is this encoding available via email format or texting format? 

If newer phones aren’t capable, how about older phones? The flip phones and what not? 

As for email, is it capable? 

Any idea how many bytes an emoji is? Not that it matters, however, I am curious.",1,0,0,False,False,True,1639692345.0
rhll1c,hounm2f,t1_hou3atx,"Actually UTF-8 is variable width that uses 1 to 4 bytes per character, depending on the character.",1,0,0,False,False,False,1639700361.0
rhll1c,hou2bxw,t1_hotc918,"It is possible to write a message in English that only uses one bye per character. The ASCII, Latin-1 and UTF-8 encodings will all do this. It is not possible to write a message in Chinese that uses one byte per character (assuming an 8-bit byte). This is because one bye can represent 256 Disney numbers. So you can encodes the English alphabet by saying A=65, B=66, C=67 and so on. Then a=97, b=98, c=99 and so on. But there are more than 256 Chinese characters and so it will necessarily take more than one bye per character to encode a message in Chinese.

There are two characters related to new lines in common encodings, called Line Feed and Carriage Return (LF and CR).  These names come from mechanical printers which had to be told separately to move the paper up a line (LF) and to move the print head back to the start of the line (CR).  There are several conventions on how these characters are used to construct new lines in different bits of software. Some use only a CR at the end of each line.  Some use only a LF.  Some use both CR+LF.  CR and LF are both part of the ASCII character set and have values 10 and 13.

Every character in the ASCII art takes the same number of bits to represent.

Note that some older email clients used 7-bit ASCII to represent emails. Since you can represent 32 non-printing characters, upper and lowercase letters, numbers and a selection of punctuation in 128 characters, you only need 7 bits to assign a unique number to reach character. Back when sending bits was expensive, it made sense to only send seven bits for each character.",0,0,0,False,False,False,1639691247.0
rh4amb,hoo7w0f,t3_rh4amb,Isnt C++ is more strongly typed than C ?,116,0,0,False,False,False,1639591550.0
rh4amb,hooc5f2,t3_rh4amb,"Is the reason C++ is classified as ""Weak"" typing because you can, for instance, read the bytes of an `int` as a string? In which case can you not do that in Java as well?",41,0,0,False,False,False,1639593216.0
rh4amb,hop85p6,t3_rh4amb,"I think the closest thing you will find in ""serious computer science"" is the lambda cube https://en.m.wikipedia.org/wiki/Lambda_cube

I personally have not seen much theoretical study on totality of a real world programming language, as they are way to complex to reason about. And with such a complex system it is very hard to define the notion of ""stronger type systems"".

Not to mention many system has unique type systems like linear type, refinement type, semantics type, gradual type, and cubical type, which are hard to characterize into a structure.

Lambda cube is one of the more comprehensive summary of type systems inspired by intuitionistic logic.",19,0,0,False,False,False,1639605557.0
rh4amb,hooexq6,t3_rh4amb,Why the heck is F# weaker than C#? The whole point of F# is the strong type system.,28,0,0,False,False,False,1639594287.0
rh4amb,hop38hs,t3_rh4amb,You are missing an axis: explicit to inferred.,6,0,0,False,False,False,1639603637.0
rh4amb,hopd9pb,t3_rh4amb,Could someone explain me why c++ is weakly typed?,4,0,0,False,False,False,1639607633.0
rh4amb,hoo3w7m,t3_rh4amb,"So where did you find the graphic? Why would it have to in a paper/book? You can also cite non-traditional sources, especially in CS",14,0,0,False,False,False,1639590025.0
rh4amb,hoozvs7,t3_rh4amb,Ask over at r/programminglanguages,6,0,0,False,False,False,1639602355.0
rh4amb,hopiqoe,t3_rh4amb,"This ""infographic"" would seem to be a summation of opinion rather than researched fact - many languages have been plopped in places that make no sense (but then the axes are also seemingly arbitrary words) - I wonder whose agenda it suits?",7,0,0,False,False,False,1639609924.0
rh4amb,hordnc0,t3_rh4amb,They misspelled Haskell lol,2,0,0,False,False,False,1639645448.0
rh4amb,hos0r15,t3_rh4amb,im confused as to why C++ is towards weak instead of strong,2,0,0,False,False,False,1639661586.0
rh4amb,hoolprs,t3_rh4amb,how the fuck is C more weakly typed than python?,11,0,0,False,False,False,1639596891.0
rh4amb,hor1e33,t3_rh4amb,"OP have you studied type systems? https://en.wikipedia.org/wiki/Type_system  That field has to quantify how strongly typed a language is for it to work, so there is probably something floating around in the neck of the woods you're looking for.  There may be some papers on the topic.

edit: https://en.wikipedia.org/wiki/Comparison_of_programming_languages_by_type_system  This was found in the type system link above, so there is clearly a path to this topic from type system.",1,0,0,False,False,False,1639636262.0
rh4amb,hor4f2x,t3_rh4amb,"I have the perfect solution for this. Hope I’m not too late:

[ignore the highlights ](https://imgur.com/a/8QKi1VJ)

While that photo isn’t specifically what you’re after the whole paper will leave nothing uncertain: 

[Programming Paradigms for Dummies](https://www.info.ucl.ac.be/~pvr/VanRoyChapter.pdf)

The name is misleading its most definitely **not** for dummies",1,0,0,False,False,False,1639638346.0
rh4amb,hor6mhw,t3_rh4amb,Make sure to include Go. I hear only good things about the operating systems minus being developed by Google. But who cares! It's a great language.,1,0,0,False,False,False,1639639946.0
rh4amb,hop5nq9,t3_rh4amb,[removed],-7,0,0,False,True,False,1639604584.0
rh4amb,hop7qa2,t3_rh4amb,A je to iz prosojnic od funkcijskega programiranja profesorja Bosnica?,-2,0,0,False,False,False,1639605388.0
rh4amb,hopz3ry,t3_rh4amb,scala FTW!,0,0,0,False,False,False,1639617245.0
rh4amb,hooywmy,t1_hoo7w0f,"I don't know whether it's more strongly typed, but C++ is infinitely more dynamically typed than C.

C does not have dynamic types at all. The type of everything is determined at compile time.

In C++, you have polymorphism.",31,0,0,False,False,False,1639601977.0
rh4amb,hoosgls,t1_hoo7w0f,"Yes, you can write C++ such that it's very strongly typed. But also, it inherits the exact same features that C has that make it weakly typed. 

I guess this graph is a kind of lowest-common-denominator kind of thing.",32,0,0,False,False,False,1639599478.0
rh4amb,hooshf8,t1_hoo7w0f,Maybe they mean like with inheritance you can cast an object as it's parent? Or type casting functions (not sure if C-structs offer that)?,2,0,0,False,False,False,1639599487.0
rh4amb,hop5cnb,t1_hooc5f2,"In Java an ```int``` is a primitive whereas a ```String``` is an object. However you could have an ```Integer``` and a ```String``` where both are sub-types of ```Object```.  
  
You could convert an object into bytes, but you can't just pretend you have bytes. Whereas with C on the other hand, types are more of wrapper around bytes where an ""int"" is 4 bytes/32-bits and an interpretation of them and a C-string is a null-terminated sequence of bytes (last byte is '\0') which will be be interpreted as belonging to a set of characters.",10,0,0,False,False,False,1639604464.0
rh4amb,hooyqr7,t1_hooc5f2,"No, you can't in Java. At least not accidentally. And if you do so on purpose, you do not have undefined behavior.",1,0,0,False,False,False,1639601913.0
rh4amb,hopj5vd,t1_hop85p6,"I think this is the answer I've been looking for, thankyou very much king",10,0,0,False,False,True,1639610104.0
rh4amb,hoohtib,t1_hooexq6,"To be fair this is just the first example I found, probably not the best",13,0,0,False,False,True,1639595399.0
rh4amb,hoprjvf,t1_hooexq6,I think these are only placed in the correct quadrant,9,0,0,False,False,False,1639613803.0
rh4amb,hopj3wg,t1_hopd9pb,Things are very often implicitly converted (thing passing an int to a function that uses a float and vice versa),4,0,0,False,False,True,1639610081.0
rh4amb,hooc5mu,t1_hoo3w7m,"I suppose it would be fine to cite where I found it, I was just wondering if there was any academic paper which I could cite instead (better)",6,0,0,False,False,True,1639593219.0
rh4amb,hoosn1a,t1_hoo3w7m,"> Why would it have to in a paper/book?

I guess the general idea is that a paper or book would have put much more thought into the graph than random blog, and so it'd be more ""accurate"" and sourced etc.",7,0,0,False,False,False,1639599547.0
rh4amb,hoqlet5,t1_hopiqoe,OP makes thread to ask for academic papers.  People instead critique the example given.   lol,7,0,0,False,False,False,1639627290.0
rh4amb,hopv6z4,t1_hopiqoe,"I think this could actually be proven with some simple code where implicit conversions could be done as tests (could definitely have more tests), as for the metrics I'm not really sure how you would lean more to one side.",1,0,0,False,False,False,1639615464.0
rh4amb,hoos0jq,t1_hoolprs,"It all wholly depends on how you define ""strong"" and ""weak"" and how you rank languages against each other. Maybe strength refers to different relative rankings of type safety or perhaps memory safety, or maybe how strict the static or dynamic type checking rules are. Python, for instance, has pretty strong runtime type checking rules. It's very often going to throw an error rather than try implicit conversion.",16,0,0,False,False,False,1639599304.0
rh4amb,hoptu6q,t1_hoolprs,"Because it clearly is. You can do whatever you want with a value in C. C is nearly the perfect example of a statically, weakly typed language.",3,0,0,False,False,False,1639614842.0
rh4amb,hop6v84,t1_hop5nq9,Sorry what?,3,0,0,False,False,True,1639605051.0
rh4amb,hop0z4u,t1_hooywmy,"C++ is a static strongly typed language at its core. 
But since it is the swiss army knife of programming languages, it does support a typed dynamic dispatch (runtime polymorphism)  that is more of dynamic typing feature. 
C language doesnt offer( afaik) any dynamic dispatch mechanism in the language. But since it is THE portable assembler, we can implement any dispatch scheme using the language. But the language still stays statically typed, though compiler is very forgiving when it comes to enforcing types ( weak typing)",15,0,0,False,False,False,1639602776.0
rh4amb,horxl27,t1_hooywmy,Isn't void* a passe-partout in C/C++ ? Like I can give void* as param to a function then pass a int* cast as void*,1,0,0,False,False,False,1639659902.0
rh4amb,hoq7qvw,t1_hopj3wg,"Thats inherited from C, in fact C++ disables some of the  implicit conversions from C (eg ""string literal"" to char*). So it doesn't make sense for C++ to be more weakly typed.",8,0,0,False,False,False,1639621109.0
rh4amb,horo10c,t1_hopj3wg,"Thanks, i didn’t know that being weakly typed is it about implicit conversions, do explicit conversion have anything to do with it as well?",1,0,0,False,False,False,1639653915.0
rh4amb,hopsgts,t1_hooc5mu,"Academic papers for software engineering concepts like this are usually garbage, you will find better and more reliable sources in company white papers and textbooks. The academic publishing for compsci is very good, but *software engineering* academia is dominated by people who couldn't hack it in industry *or* rigorous compsci research, and pump out very low quality papers.",3,0,0,False,False,False,1639614214.0
rh4amb,hopdx8b,t1_hoos0jq,"Yeah by my understanding, it's considered ""strong/dynamic"" because each object x has a unique well-defined type type(x) at runtime that doesn't get implicitly converted to other types. Contrast this to C, where there's automatic integer promotion between distinct types, array/pointer conversion, and the ability to cast any kind of pointer to any other kind of pointer; there's a lot of ways to treat one type of data as if it were another. At the extreme, the weakest type system would be no types at all, like some assembly code where everything is just machine words/bytes.

Python is also contrasted with something like JS, which will do whatever it can to produce a result (adding ints to strings and the like), even if the oparands are nonsensical.",5,0,0,False,False,False,1639607903.0
rh4amb,hop55am,t1_hoos0jq,Which is why a diagram like this is useless without an explanatory text.,8,0,0,False,False,False,1639604384.0
rh4amb,hos0h8r,t1_horxl27,"But then a ""conversion"" explicitly happens, which is what converts the value to a different type.

C's syntax just allows this conversion to happen without additional syntax. The conversion is explicit in the language's semantics. The conversion is explicit in the type-annotated AST.

All statically typed languages have conversions between types, casts are nothing special.",1,0,0,False,False,False,1639661448.0
rh4amb,hoqc5tm,t1_hopsgts,"I think this topic falls more in the realm of programming language theory than software engineering. And theory of programming languages is a rich area of academic research; heck, you could reasonably argue that academia has driven a lot of the innovation in programming languages.",1,0,0,False,False,False,1639623047.0
rh4amb,hop6cph,t1_hop55am,Agreed,0,0,0,False,False,False,1639604851.0
rhhyf3,hoqtzx5,t3_rhhyf3,"The short answer is downloading files does not wear out a computer, the wires, memory, etc. After all, normal internet browsing downloads dozens or even hundreds of files per page.

The caveat is that solid state memory devices (SSD, or tablets, phones etc) can wear out, but it would take extreme usage over a period of time to trigger it.  By extreme usage, I mean something like downloading your 1000GB gdrive/onedrive, and as soon as it finished, deleting your local copy and repeating.

&#x200B;

I'm sure others will provide more details/corrections.",3,0,0,False,False,False,1639631809.0
rhhyf3,hora8h7,t3_rhhyf3,"At the most basic level, files are stored as bits, which are represented by states of flip flops. Everything above that is an abstraction. If you download a file that somehow runs itself and contains code to run a heavy workload and turns off the fans, it can destroy your computer. Not instantly, but overtime. A file can imprint itself but that depends on what kind of device it is stored on. It isn’t a property of file, rather the property of the device. For eg: PROM is a programmable ROM that can only be programmed once, while EPROM is Erasable PROM, so shine a UV light and the data disappears. EEPROM is electical EPROM, so data can be erased by passing an electric current. Electrical engineering has a lot of things like this. Especially hardware design and verification. Cool stuff.",2,0,0,False,False,False,1639642718.0
rhhyf3,hos7qu5,t3_rhhyf3,"Transferring or processing data doesn't cause any meaningful wear. Writing the data on a storage device may cause wear but usually not enough to matter in most cases.

Electronics, such as the wires or the computer's CPU or RAM memory, don't really suffer physical wear from processing data. Heavy use of components such as the CPU can cause the component to heat up, and repeated heating and cooling cycles may technically shorten the lifespan of the component. However, that would typically only really make a difference on a time span of decades. That's much longer than you're going to be using a CPU or any other computer component in any everyday use.

Downloading files also doesn't really cause heavy load on the CPU or other components in a computer, so downloading data probably wouldn't make even that kind of a difference.

As for storage devices, that depends on the technology used in the storage drive. SSDs do have a limit on the number of times each memory cell can be written to, so frequently writing a lot of new data on an SSD does theoretically shorten its lifespan. How much data you have on an SSD doesn't matter; what matters is how many gigabytes of new data you keep writing.

However, you can typically overwrite each cell on an SSD thousands of times. As long as there's free memory space available, the SSD also automatically tries to spread the data writes evenly over the free physical flash memory so that the wear wouldn't all be on the same cells even if you keep overwriting the same files. You probably can't wear out a modern SSD with any kind of normal use even if you download or otherwise write lots of large files on it. SSD wear might matter in heavy constant use, such as perhaps in a data center, but it doesn't really matter in desktop use.

Mechanical hard drives are a little different. They are, well, mechanical, and they suffer physical wear over time. A hard disk drive has spinning disks that rotate at thousands of RPM, and data are stored by altering magnetic fields on the disk. Mechanical parts such as bearings can wear out over time. However, most of the wear would probably come simply from having the hard disk drive powered on and the disks spinning, or from powering the drive on and off often, rather than from writing lots of data on it.

So, the TL;DR answer is roughly that *downloading* (as in transferring data from the Internet in the first place) doesn't cause wear; writing the files on the disk (as you usually do) may cause some wear, but not enough to matter in most cases.",2,0,0,False,False,False,1639664933.0
rhhyf3,hozkqrw,t3_rhhyf3,"One more thing to add: the servers that you download from need to serve the files. That requires processing power (to a smaller extend on the downleading side, too) which is measurable is excess heat. This will (over long periods of time) also cause damage on the hardware.",2,0,0,False,False,False,1639786083.0
rhhyf3,horwno3,t1_hora8h7,Thank you,1,0,0,False,False,True,1639659388.0
rhhyf3,howvegc,t1_hos7qu5,"I concur. The downloading itself does not really wear out anything. The act of persisting this data is what causes wear.

You can have a functioning computer even without any writable medium. You can even boot linux from an optical disk and from then on everything that needs storage is stored in RAM.",1,0,0,False,False,False,1639746162.0
rhnc1k,hori60v,t3_rhnc1k,Every element in the domain of a function must map to something in the function’s range. The is basically the definition of domain. If there was an element x that didn’t have a defined value f(x) then by definition the domain of f actually cannot contain x.,1,0,0,False,False,False,1639649250.0
rhnc1k,horj4ds,t1_hori60v,thanks,1,0,0,False,False,True,1639650044.0
rhnc1k,hos9g6l,t1_horj4ds,"To expand a bit of this if a given function `f` is defined for some subset of `X`, let's say `X'` (so it is a a function `f: X' -> Y`) then in regards to `X` it is also called a ""partial function"". The partiallity comes exactly from the fact that it does not cover all of `X`. However it still obeys the rest of the laws if you look at functions as relations.

Surjectivity is not partiality however. Let's reset and say that `f: X -> Y` is a function covering all of `X`. Surjectivity asks ""is there an `x:X` for every `y:Y` such that `f(x)=y`""

More succinct: [;\forall y \in Y \exists x \in X f(x)=y;] 

Surjectivity can be expanded to partial funciton, this is normally done by disregarding the partiallity (so the cases where `f` doesn't map to anything) and then asking if that restricted function is surjective. In your example, if `Y = {y1, y2}` then even the partial function is surjective.",2,0,0,False,False,False,1639665691.0
rgnbmf,hol9r4q,t3_rgnbmf,">Has anybody had similar experiences?

Yep!

You're not a moron. A moron would be finding excuses for why their solution was ""better"" somehow.",186,0,0,False,False,False,1639533311.0
rgnbmf,holec26,t3_rgnbmf,"It's virtually impossible to write dumber code than an experienced developer has come across in production at some point or another.

If it works and people can read it and it doesn't require adding dumbass dependencies, ship it!",59,0,0,False,False,False,1639535384.0
rgnbmf,holzpit,t3_rgnbmf,"Just by the fact that you recognize that your coworker's solution is a valid but a simpler solution to your problem, indicates that you're definitely not a moron. You learn something from this experience.
6 months is arguably not a lot of time to gather experience yet. I've been doing this for more than 5 years, and I sometimes still do stupid mistakes (though not at the extent and frequency as when I was still starting out). But the most important thing is you learn. You have to always learn.",24,0,0,False,False,False,1639546055.0
rgnbmf,hom03f5,t3_rgnbmf,Everyday and always. The best part is when you stumble upon code you wrote years ago and wonder 'how the hell did I even come up w/ this???'. Never gets old.,18,0,0,False,False,False,1639546280.0
rgnbmf,homr8dr,t3_rgnbmf,"I've been writing software professionally for close to 20 years. I experience what you describe every week, on average.",9,0,0,False,False,False,1639566898.0
rgnbmf,holnuea,t3_rgnbmf,"All good. You are learning!  


Next time when you need to solve the same problem you know how to do it.  


Generally: When ever you think your code is smart/genius... delete it, it's garbage!

Smart code is bad code. If you think it's smart now you'll have a very hard time understanding in a month from now, your stupid colleagues will have an even harder time. Don't write smart code, write simple code!",27,0,0,False,False,False,1639539754.0
rgnbmf,holck4r,t3_rgnbmf,"I spend weeks deleting complex code from a project (to get it working), then I went on to implement a more complex but better algorithm only to discover that the ""junk"" I threw out was that algorithm (or large parts of it, project didn't work at all when I got it, so no clue).

In retroperspective, I stand by this way (no way I could have fixed/tested that complex algorithm). But I still felt dumb. Especially my commit comments: ""Removed useless Class X"" then later ""Implemented Class X"" (Class names come from science/math paper)

&#x200B;

Looking further back - before University, nobody told me that ""If"" is bad (performance wise), so I stacked 8 If clauses with masterworks of boolean math - in 3 Loops, to be executed each frame. Now I know why I only got 30 FPS and couldn't increase resolution at all. ...

Or that time we had a practical course in university - some Java Full Stack thing (a frontend for some research facebook clone (with complex relation networks), intended for actual use - Java, never again). Took 80% of our time to even get it running (because competence is rare we had 2 people with real Java experiance and only 3 others (inkl- me!) even had the competence to install Virtual Box ... ). When we put our (shitty) frontend to the real backend we realised that data queries literally took \~93 seconds. 93 SECONDS!  (everybody got good to very good grades - and the project (and backend creator) got scrapped (or redone)) We felt really dumb. Had we tried that earlier, we could have saved a lot of stress.",6,0,0,False,False,False,1639534584.0
rgnbmf,homg3h1,t3_rgnbmf,"When I have the pleasure of teaching new joiners in my team how to write programs I tell them this:

* First, it has to **do the thing**
* And better is when **others can understand** how it does the thing
* For bonus points, it should do the thing **efficiently**

Sounds like you may have skipped the second step. But at least you recognised that. Don’t feel bad. This is all part of the never-ending learning path.",5,0,0,False,False,False,1639557569.0
rgnbmf,holn0nd,t3_rgnbmf,Can you explain what the difference was?,3,0,0,False,False,False,1639539361.0
rgnbmf,holovbl,t3_rgnbmf,"Don't be embarrassed; it's called learning. As time goes on, you will be learning how to do all kinds of things better.

Seeing how experienced people solve problems you have attacked is always worthwhile.

Along these lines, it would be worth your time to find and read ""Design Patterns"", a classic book by ""the gang of 4"" (it has 4 authors). They show how to attack several other issues that come up all the time.",3,0,0,False,False,False,1639540245.0
rgnbmf,hols4tz,t3_rgnbmf,"I think there is merit to trying to figure things out yourself rather than immediately falling back to reference material like Stack Overflow. I did this a lot in my CS classes in university, and I think it sets you up to be a better problem solver. A strength to develop though is to know when to go look something up, or at least do some follow up after coming up with something to make sure that you've solved something in the best way possible before submitting it for code review/etc.",3,0,0,False,False,False,1639541895.0
rgnbmf,homls0l,t3_rgnbmf,You'll continue to experience this in retrospect when you look at code you wrote a year ago.,3,0,0,False,False,False,1639562396.0
rgnbmf,hon1iqa,t3_rgnbmf,"yep don't worry it's ok and healthy to feel like this, keep learning and get better


ie: i just refactored a chunk of my old code because i could not understand it fast enough and it was doing stuff redundantely and with wrong var names",2,0,0,False,False,False,1639573601.0
rgnbmf,hon3gv8,t3_rgnbmf,">Has anybody had similar experiences?

All day every day (I'm a business owner in IT) And I've been tinkering with code for well over 25 years now, don't beat yourself up over it.   


There will be someone that looks at your code and think ""Why didn't I come up with this"" as well and please reframe critisizing yourself into *""Not the best possible solution for the given problem""* or *""I may have....""*  
I drove myself into a **University needed to intervene** situation due to seeing the code people rattled out during a internship at a big tech company, which made me question my abilities which is a fate I wouldn't wish upon you.",2,0,0,False,False,False,1639574662.0
rgnbmf,hoob0ux,t3_rgnbmf,">Has anybody had similar experiences?

Yes, it's called progress",2,0,0,False,False,False,1639592777.0
rgnbmf,hon2hlu,t3_rgnbmf,This is called lack of experience. It's normal. And if it works it is okay.,1,0,0,False,False,False,1639574136.0
rgnbmf,honaurk,t3_rgnbmf,"I've been in this line of work for almost 3 decades. Just last month I wrote a complicated if/else contraption. Upon finishing it and testing it, I realized I could simplify the whole stupid thing down to one conditional. Yes, I am an idiot, too. Welcome to the club!  It's a big club.",1,0,0,False,False,False,1639578290.0
rgnbmf,honwhmn,t3_rgnbmf,All the time,1,0,0,False,False,False,1639587162.0
rgnbmf,hpjqjwu,t3_rgnbmf,Bruh at least your not getting easy questions wrong on Algo expert i feel braindead,1,0,0,False,False,False,1640175016.0
rgnbmf,holuv8w,t1_hol9r4q,The chad above is right . Sometimes we block our thinking due to performance anxiety or a belief that we need to prove ourself.,67,0,0,False,False,False,1639543347.0
rgnbmf,holpdxx,t1_holnuea,"Excellent advice, I second this. Good code often looks surprisingly simple. Code by newbies is often very complicated.",6,0,0,False,False,False,1639540500.0
rgnbmf,homif8o,t1_holnuea,Indeed. What may earn style brownie points within a single line may not be good for the whole project.,1,0,0,False,False,False,1639559508.0
rgnbmf,housdil,t1_holck4r,[deleted],2,0,0,False,False,False,1639702524.0
rgnbmf,homs89m,t1_holuv8w,The alpha male above is right. The need to prove ourselves can often make ourselves feel less.,26,0,0,False,False,False,1639567655.0
rgnbmf,honmhoj,t1_holpdxx,"Hmm, I honestly have encountered a lot of senior code that is very hard to understand and I actually prefer newbie code because it's usually less complex and easier to reason about. 

Well... I might be eternal newbie, but that's another discussion. Just wanted to give another view point aswell!",2,0,0,False,False,False,1639583231.0
rgnbmf,howtcqe,t1_housdil,"You mean the Java project?:The problem was that the backend was designed without regard for performance: Connections (friendships, family etc) were entities with properties, which themselfs each could have connections (infinitly recursing!), and Cathegories, nested etc. Pretty sure even the simplest request resolved to dozen or hundrets of SQL queries and MBs of data. (and, of course the backend, frontend,frontend database and backend database were each on different servers (also not in RAM or on SSD))

I wouldn't complain about the servers (much) we used (2013?) Apache, Tomcat and Postgres, plus JSPs /Servlets combined with RichFaces, some Servlet Server (or what that was called) and one other thing I forgot, and of course, we had to use some universal identity/login service (which was difficult). We spend most time getting to Hello world, and second most time ""exploiting"" our projecft components/writing data leaks because nobody (on our team) could figure out how the JSPs / servlets should share presistent, user linked objecfts... One other highlight was figuring out that my code wasn't buggy - RichFaces had a bug (somebody copy&pasted a set function, but forgot to chage the setted property). Project was way to difficult as a first practical team project.",1,0,0,False,False,False,1639745063.0
rgnbmf,homyddx,t1_homs89m,The person experiencing imposter syndrome above is right. We always learn by our failures and peers.,15,0,0,False,False,False,1639571767.0
rgnbmf,honuq4f,t1_honmhoj,"My first reaction is that the people who wrote the code you saw still have things to learn. If code is hard to understand, that is a problem with the code.

A worthwhile place to learn more is the book ""Clean Code"" by Robert Martin (""Uncle Bob"").  Especially note the ""SOLID"" acronym; there is a chapter for each part of this. If you can use these ideas, you will be producing more easily-understood code.",1,0,0,False,False,False,1639586462.0
rgnbmf,hon4pyd,t1_homyddx,"The person in denial above is right. It's always better to keep an open mindset rather than a closed one, since an open mindset will always help you grow",13,0,0,False,False,False,1639575311.0
rgnbmf,hora5nt,t1_honuq4f,"Senior code I was referring to is mostly about doing something simple through multiple abstractions. It seems SOLID, DRY, KISS and all those acronyms but it lacks simplicity.  
It's hard to discuss with just words and I should show you an example, but I think you might've encountered code like that as well? One could say, it is over-engineered. I think over-engineering is a bit subjective and it might be just bad code etc.",1,0,0,False,False,False,1639642655.0
rgnbmf,honbf9c,t1_hon4pyd,"The wise mountaintop guru above is right. An open mind lets in fresh thoughts, like a open window lets in fresh air and sunlight. Stretching the metaphor, all allow you to grow.",12,0,0,False,False,False,1639578545.0
rgnbmf,honc0j8,t1_honbf9c,Hehe.,5,0,0,False,False,False,1639578816.0
rgnbmf,hongbrz,t1_honc0j8,We’ll that puts an end to that,12,0,0,False,False,False,1639580717.0
rh7r2v,hos88f7,t3_rh7r2v,"For illustration purpose it starts with only 0-45 degree slope and leaves the detail to you to deal with other cases (flip signs/flip dxdy/etc.)

the coefficients in 2dy-dx are chosen because we're making a decision between picking (1,1) or (1,0) as the offset to the next pixel. We're really just comparing dy/dx direction to (1,0.5) and see if we should go above that (1,1) or below that (1,0). But 0.5 isn't integer, so we scale and compute 2dy-dx instead.",1,0,0,False,False,False,1639665155.0
rg53gw,hoi8v1i,t3_rg53gw,"The book ""Engineering a compiler"" by Keith Cooper and Linda Torczon has about 250 Pages on different optimizations, I can highly recommend it.   
You might get it used cheap or it's in a local library.   


or ... there are places on the internet where one can get Pdfs for free apparently.",31,0,0,False,False,False,1639487227.0
rg53gw,hohzdwh,t3_rg53gw,"[https://llvm.org/docs/Passes.html](https://llvm.org/docs/Passes.html) might be a good start.  


Passes to look for are mem2reg, sccp, the loop- passes and anything in the transform category, actually.",12,0,0,False,False,False,1639481168.0
rg53gw,hoiq4gp,t3_rg53gw,https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html,7,0,0,False,False,False,1639495523.0
rg53gw,hojo0xy,t3_rg53gw,"The Dragon Books

[https://en.wikipedia.org/wiki/Compilers:\_Principles,\_Techniques,\_and\_Tools](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools)

I like the green dragon, but then I am old.",6,0,0,False,False,False,1639508833.0
rg53gw,hojra7t,t3_rg53gw,Not exactly the same but [http://godbolt.org](http://godbolt.org) is pretty cool!,5,0,0,False,False,False,1639510107.0
rfmupc,hoetxuo,t3_rfmupc,"I have no experience with this so I'm just spitballing. Depends on how intelligent she is and what kind of computer experience I suppose:

* Scratch would let you make it as easy as possible. Colorful drag and drop tiles to build functions, would introduce the really basic concepts of control flow and all
* You could play with Visual Studio a bit. Dragging and dropping buttons and showing how to do something simple like make a popup window with text or add two numbers
* Python is pretty easy to start with too for obvious reasons, maybe that would make sense to start with for some basics

If you haven't Googled in depth already, there are probably some more specialized languages or ""IDEs"" out there targeted at a young audience",66,0,0,False,False,False,1639421467.0
rfmupc,hoewexx,t3_rfmupc,[deleted],34,0,0,False,False,False,1639422451.0
rfmupc,hoezfir,t3_rfmupc,"As a father of two children ages 10 and 6 where both kids like programming (or ""computer math coding"" as my 6 year-old calls it), I would focus on [Computational Thinking](https://www.bbc.co.uk/bitesize/guides/zp92mp3/revision/1):

1. Decomposition
2. Pattern Recognition
3. Abstraction
4. Algorithms

I would stick with using Scratch (my kids use/know this).

Example:

There's a maze (2D-array) where the student has to navigate a character from from start to finish.

a) They can hard-code the sequence of steps required. (Sequencing concept)

b) They can be forced to use loops to reduce the number of instructions used. (Looping concept)

c) They can use basic decisions (if-then-else expressions) to create a simple algorithm.


 Keep the concepts simple and light. Making success incremental while slightly increasing the difficulty between tasks is key. That keeps the kiddos motivated.",24,0,0,False,False,False,1639423669.0
rfmupc,hoex8xv,t3_rfmupc,"I started with Scratch when I was around 10 and I’ve been interested ever since. 

https://scratch.mit.edu/",18,0,0,False,False,False,1639422788.0
rfmupc,hoev4s1,t3_rfmupc,I started getting my interest in computer science and programming when I was 13. I don't see why not to introduce her. I'm 17yo rn studying Game Development at college and I'm glad I started young and found my passion / future job lol,7,0,0,False,False,False,1639421942.0
rfmupc,hoex8xb,t3_rfmupc,"I was introduced to code around that same time and I learned through Scratch. That was my favorite program. But, I had a teacher tell me how to do stuff. There aren't built-in lessons in Scratch. If you want to teach her and be more hands-on, I'd use Scratch. If you want something that teaches her how to code in a way that's similar to Khan Academy, I recommend [Code.org](https://Code.org) or [CodeHS.com](https://CodeHS.com)",7,0,0,False,False,False,1639422788.0
rfmupc,hoeys1h,t3_rfmupc,I was an elementary school child when I learned BASIC. I would say BASIC or Python.,5,0,0,False,False,False,1639423408.0
rfmupc,hoeudy9,t3_rfmupc,"Figure out the fastest way she can make a game in Roblox and start with that. Ideally if she can do something super simple on an hour and half or less that would be great. Then slowly build from there. If each lesson ends with her game being a little better that’ll probably help a lot with motivation and excitement. 

Overall focus on something she can show off and be excited about. Then sneak in the less fun stuff as you go.",8,0,0,False,False,False,1639421645.0
rfmupc,hoexcb9,t3_rfmupc,Start her with [Scratch](https://scratch.mit.edu/parents/),3,0,0,False,False,False,1639422826.0
rfmupc,hof9atw,t3_rfmupc,Already mentioned a bunch of times so just adding to the dog pile here: Scratch got me hooked as well. The drag and drop code blocks and animations made it fun and simple.,3,0,0,False,False,False,1639427680.0
rfmupc,hoeyfjm,t3_rfmupc,"This might sound nutty but hear me out. If you have an Apple device, download the OCaml compiler and teach them ML! I don’t even mean that in a joking way.

Functional programming is actually so much more straight forward and that Mac/iOS compiler is incredibly simple and easy to use.

Sure they won’t be able to do a ton but it’s not like Scratch is really reaching them anything more useful and with ML they will only need to learn like three keywords and the rest just all falls into place.",4,0,0,False,False,False,1639423269.0
rfmupc,hofb0tz,t3_rfmupc,"Speaking from how I got into CS, I'd say Web Development.

I remember that's how I started when I was a kid. Working with HTML and CSS was extremely simple and provided instant gratification. I remember getting so excited over stupid little things like setting a background image or making text a different color just by typing some letters.

The best part was how much it gradually snowballed. I was forced to consistently learn more and more as my ideas grew. Eventually had to learn stuff like PHP and JS which really came in handy for getting started with programming languages in college.

But web development never felt like work or studying. As a kid, I always considered it more of a art medium rather than a technical skill.",5,0,0,False,False,False,1639428418.0
rfmupc,hof2qre,t3_rfmupc,"BASIC is how I got started, at around that age. See if you can get her interested in something like a turtle drawing library, those are usually quite fun for kids.",2,0,0,False,False,False,1639424998.0
rfmupc,hof44fi,t3_rfmupc,Raspberry pi has a lot of learning PDF materials for young children,2,0,0,False,False,False,1639425545.0
rfmupc,hof77ra,t3_rfmupc,"**Autonauts** is a cute game that let's you build an automated village. It's very kid friendly and has scratch-like building blocks. The game is super rewarding early on, but it becomes more redundant to progress later. Regardless, it's a good introduction to basic concepts while being entertaining.",2,0,0,False,False,False,1639426804.0
rfmupc,hof9tkc,t3_rfmupc,"Roblox scripting might actually be a good place to start. You can make a lot of different games in Roblox, and you can start really simple.",2,0,0,False,False,False,1639427902.0
rfmupc,hofmsno,t3_rfmupc,"I got my start around that age by messing around with qbasic, typing in examples from a qbasic book that my dad had. 

The act of rote typing in examples from the book was enough to drill in the syntax and basic concepts. I remember my dad sitting and teaching me about if/then/else, and maybe about for loops. Then I was left to my own devices and started making changes to the examples that I had typed in. I also spent quite a bit of time trying to get things to work due to making typos, which also helped me learn. I seem to remember making lots of Mad Libs type things where you'd be prompted for words, then it would print out silly stuff incorporating those words.

I would recommend setting them up with some environment in a modern language. Something with at least console IO capabilities, or set them up with a skeleton project in a more GUI oriented environment with a ""turtle"" like drawing API, show them the ropes, then see what they come up with. I would lean towards javascript in a browser, or javascript run via node just because it's ubiquitous these days.

I also know kids get into programming via Minecraft these days, so that's another option if she's into stuff like Roblox.",2,0,0,False,False,False,1639433274.0
rfmupc,hofwp5w,t3_rfmupc,you could try CodeCombat.  it kinda turns coding into a platform-esque video game and makes it fun!  that's how I started out,2,0,0,False,False,False,1639437451.0
rfmupc,hoh8mvx,t3_rfmupc,"The C Programming Language by Kernighan and Ritchie.

But seriously I would figure out what makes her interested in programming and find something that matches that. If she wants to make games, she should start on that path.

Educational languages can feel boring and fake. Unlike a lot of people here I don't think there is anything wrong with starting out with what people consider a ""hard language"". There are no hard languages, only hard problems. ""Hard languages"" can be more exciting and motivating because you feel like you are doing something real. And if you are self teaching you can go at your own pace anyway.",2,0,0,False,False,False,1639460613.0
rfmupc,hohfgqe,t3_rfmupc,Try her on a few tutorials for different things to see what draws her attention more. Scratch and Code Combat are good ones. I think No Starch Press has some kids programming books for things like python and scratch.,2,0,0,False,False,False,1639465137.0
rfmupc,hohyj0e,t3_rfmupc,"Three things come to mind.

1) The Logo programming language, my first memory of writing a program was a turtle moving around a monitor forwards, back in varying distances and rotating 5/10 degrees at a time and repeating n times to create a star 

2) Raspberry pi has some programming languages built in that can interact with actual hardware easily and there is a programming language scratch 3 where programming is done by dragging blocks that are linked to things like button press events and the pi could be hooked up to hardware light a set of traffic lights / pedestrian crossing to introduce something real-world.

3) a game called pingus is a clone of lemmings and is a game with learning thrown in, you have to ""program"" to get the penguins through the level, eg by deploying blocker penguins to hold up them while the builder/digger pengins make the path ahead (pengins will walk straight of a cliff if they unsupervised). One downside to this is that you might have to explode pengins to complete the level and that may not go down well.",2,0,0,False,False,False,1639480520.0
rfmupc,hoi8tn2,t3_rfmupc,"You could try these activities through Girls Who Code 

https://girlswhocode.com/programs/code-at-home",2,0,0,False,False,False,1639487205.0
rfmupc,hoexkpw,t3_rfmupc,"100% agree with Scratch. Minecraft redstone is an introduction to logic similar to playing Roblox. Look for camps like Code Ninjas, may not give you the chance to teach her yourself but you can bond talking over what she did there and she gets to try a curriculum meant for introducing kids to programming.",4,0,0,False,False,False,1639422920.0
rfmupc,hofpqww,t3_rfmupc,Roblox,1,0,0,False,False,False,1639434506.0
rfmupc,hof4umi,t3_rfmupc,There are entire countries that teach compsci as part of school curriculum,1,0,0,False,False,False,1639425837.0
rfmupc,hofcmle,t3_rfmupc,Yes. Do it with Pico8. ittle make it seem less boring.,1,0,0,False,False,False,1639429093.0
rfmupc,hofl0x4,t3_rfmupc,"I walked a young cousin though making a chess game (no graphics, just terminal). She was a little older but I thought it went well.",1,0,0,False,False,False,1639432534.0
rfmupc,hofpxzj,t3_rfmupc,"I would suggest python. Meanwhile do more math problems, after all computer is all about math",1,0,0,False,False,False,1639434589.0
rfmupc,hofxpvn,t3_rfmupc,"Look up “STEM robots” on Amazon. They’re robots that you build with cameras and a controller to make them drive and such, but they require some beginner level programming. Great way for kids to learn programming while maintaining some fun interest.",1,0,0,False,False,False,1639437889.0
rfmupc,hofyc1m,t3_rfmupc,Use whatever interests/motivates her as a jumping-off point.,1,0,0,False,False,False,1639438154.0
rfmupc,hofzbq8,t3_rfmupc,Funny prime numbers games,1,0,0,False,False,False,1639438588.0
rfmupc,hog4eeu,t3_rfmupc,Scratch possibly,1,0,0,False,False,False,1639441236.0
rfmupc,hog6gxh,t3_rfmupc,"I think it would be best to start her off with games or online coding websites meant for kids (scratch). If you try to teach her coding basics for python, etc. without anything fun she might get bored pretty fast. I used cmu cs academy as a freshman and the beginning lessons were very easy but that could wait until she’s a little older.",1,0,0,False,False,False,1639442239.0
rfmupc,hogfnja,t3_rfmupc,"Python got me hooked at age 12 because of how easy to use it is. Typing isn’t super strict, and it’s relatively intuitive compared to other languages.",1,0,0,False,False,False,1639446459.0
rfmupc,hogho69,t3_rfmupc,"She could probably learn LUA. Since you can code a lot of things in LUA using Roblox's system, Roblox Studio.",1,0,0,False,False,False,1639447371.0
rfmupc,hogm87x,t3_rfmupc,"It would really depend on what type of learning she takes to more, like if she gains more with note on definition of things (initialization, loop, data type, or whatever other common place things), or if she gains more from some goal oriented for some set of things (list all even number from 0 to input value). One thing I could suggest for a more goal oriented approach is codingame (codingame.com), since there is a lot of programming languages that could be used, and it has various ""puzzles"" that are solved producing some expected output. The first ""onboarding"" puzzle has a while loop that takes input automatically, and you need to determine the proper output statement per iteration. For some puzzles, there is some graphics to show achieving the goal. For that first puzzle, you have a ship in the center and enemies are flying to you.

You don't have to make an account with the site (it is a free account, if I am remembering right), but you can start without having an account. The account just allows you to save settings and progress.

I think the main item when teaching a younger kid is just determining what keeps them engaged with the topic rather than just what you try to tell/teach them. I hope that helps some.",1,0,0,False,False,False,1639449412.0
rfmupc,hogpm00,t3_rfmupc,Maybe begin very basic to make her realize that it requires that tedious attention to detail. Not a focus on the big picture but the small non-graphical game would be great to start. Maybe something like 'Hangman' or 'Tic-Tac-Toe' something you can help her with and she can grasp the concepts of what is happening - and why.,1,0,0,False,False,False,1639450923.0
rfmupc,hogqcvg,t3_rfmupc,[I suggest these](https://imagilabs.com/?utm_source=partners&utm_medium=direct&utm_campaign=spectra-hackathon). You don't have to buy the keychains. They have learning labs [here](https://www.notion.so/imagilabs/imagi-Learning-Center-5afe3d51d30645849f2738c9b5eb1154) where she can code designs,1,0,0,False,False,False,1639451261.0
rfmupc,hogqwoa,t3_rfmupc,"Scratch and khan academy have some pretty cool free programs. I learned the basics of JS with khan academy, there are lessons you can do",1,0,0,False,False,False,1639451515.0
rfmupc,hogrz0a,t3_rfmupc,"I don't know if anybody has mentioned it yet, but my gateway into programming was through the Arduino ecosystem. 

I had tried learning to code through various online platforms over the years but the ramp up to doing anything useful was a long road. 

Arduino gives you tangible things to associate your code with so you can really get a feel for what your logic is doing. 

But that was just my experience.",1,0,0,False,False,False,1639452012.0
rfmupc,hogt62d,t3_rfmupc,Lego used to have an embedded controller that you could program with a scratch like language to have the motors go forward/reverse etc. It is what got me into programming in 4th - 5th grade.,1,0,0,False,False,False,1639452569.0
rfmupc,hogvsl3,t3_rfmupc,"Depending on what she’s interested in, CodePen might be a different option.  It’s fun for making art with code and adding animations and such",1,0,0,False,False,False,1639453810.0
rfmupc,hogwr7z,t3_rfmupc,"I don’t see Kode with Klossy mentioned, but it’s an organization specifically focused on helping girls learn to code. They have tons of resources, events, and online and local communities.

 https://www.kodewithklossy.com/",1,0,0,False,False,False,1639454266.0
rfmupc,hoh5iw3,t3_rfmupc,Web dev is a good entry point. Or setup node. Have 'em work on a simple if else console game. That's a super fun thing. Let's them be creative.,1,0,0,False,False,False,1639458764.0
rfmupc,hohb70i,t3_rfmupc,"- https://www.computercraft.info/

- https://store.steampowered.com/app/370360/TIS100/

- Just dive head first.",1,0,0,False,False,False,1639462231.0
rfmupc,hohli3p,t3_rfmupc,"I think KhanAcademy has some introductory courses to Computer Science that are very easy to grasp, even for children.",1,0,0,False,False,False,1639469650.0
rfmupc,hohnjae,t3_rfmupc,"Consider buying a Micro:bit ([https://microbit.org/](https://microbit.org/)) or 2 - a fabulous little engine/chip that you can program directly, sense buttons, shake, sounds, builtin radio communication (if you have 2) etc and it can be a wearable. It has a huge collection of add-ons, lights, electronics, robots etc, codes using blocks (looks like scratch), as well as python (via Mu editor) and javascript, lets you control lights, has a powerful sim ([https://makecode.microbit.org/](https://makecode.microbit.org/)) and is a brilliant entry level physical computing platform that is gender agnostic. We use them with students as young as 8, right up to 15/16 year olds",1,0,0,False,False,False,1639471287.0
rfmupc,hohnv5p,t3_rfmupc,"Also consider Minecraft Education (from Microsoft) - it has really powerful programming engine (it is not just blockstacking in cyberspace, or killing creepers, you can do amazing scripting things and there are heaps of resources/tutorials, how tos) to script in-game action (like writing scripts to build complex stuff using sequence, selection, iteration and modularisation - the fundamentals of coding in every language.",1,0,0,False,False,False,1639471558.0
rfmupc,hohvcs7,t3_rfmupc,Scratch it a great one!,1,0,0,False,False,False,1639477945.0
rfmupc,hoillsk,t3_rfmupc,"What all the cool kids use these days - Scratch.  https://scratch.mit.edu

What us old people used as kids - LOGO.  [https://turtleacademy.com](https://turtleacademy.com)

If she was older I'd say start her on something like Free Code Camp - https://www.freecodecamp.org",1,0,0,False,False,False,1639493582.0
rfmupc,hoimmyl,t3_rfmupc,"Scratch, maybe?",1,0,0,False,False,False,1639494032.0
rfmupc,hoj2ee5,t3_rfmupc,Coding train on YouTube is a fun way to start,1,0,0,False,False,False,1639500417.0
rfmupc,hoj8i77,t3_rfmupc,Better get her a leetcode account and make her homepage the Blind 75,1,0,0,False,False,False,1639502762.0
rfmupc,hojb0ze,t3_rfmupc,"There are a lot of different ways, some are better and some worse. Unless some shows a really serious interest in going deep, I would start with simpler things.  
 
One very important detail is to work out, with her, what is exciting/interesting and use that as a guide for how to teach/explore. I would encourage her to put off messing with Roblox for at least a year or two.
 
**Scratch** is a neat tool/language and is a decent way to get kids interested these days, especially if they just want to make simple games quickly. You can go totally drag and drop if you want to avoid typing code, for instance. If you want go go that route I would recommend making an effort to provide some groundwork in conditionals and logic.  
   
I think **BASIC** was an excellent idea in the past and still has meriy. However it’s essential today to work with a variant that can provide the same ease with math, graphics, sound, etc on a modern computer that historical examples did with vintage microcomputers. *Basic-256* seems interesting, but it might be a little complex for a 10 year old. 
  
If you are willing to take a more hands on approach and help out with anything that’s she struggles too much with the then **Love2D** (uses Lua) or **Processing** (Java-based) might be of interest.",1,0,0,False,False,False,1639503746.0
rfmupc,hol7tbv,t3_rfmupc,scratch,1,0,0,False,False,False,1639532439.0
rfmupc,hq9xct6,t3_rfmupc,HTML,1,0,0,False,False,False,1640698581.0
rfmupc,hof0oz7,t3_rfmupc,Math.,-2,0,0,False,False,False,1639424175.0
rfmupc,hoev7jy,t3_rfmupc,Swift Playgrounds if you have an iPad or a Mac.,0,0,0,False,False,False,1639421973.0
rfmupc,hog26l4,t1_hoetxuo,"One thing I would say about Scratch is that while it looks colourful/easy to learn/fun for kids, there are so many code blocks and so many things the learner has to understand. It also takes a lot of time to look for the right blocks and to place them for something to work.

And I am saying the above from experience. I learned Scratch for the first time in a programming class and failed miserably. Just not enough time to experiment all the blocks and didn't really get how to fit certain blocks into places. And it was really time consuming.

But if you have time in your hands, then give Scratch a try. If she is struggling, definitely let her try Python because it's much easier to learn and make programs in my opinion. You can make a lot more interesting and fun programs with Python and I bet she would like that.",12,0,0,False,False,False,1639439847.0
rfmupc,hofexj2,t1_hoetxuo,Intelligence is a built skill. Like you learn how to learn better and learn well.  It's not like a fixed thing.,15,0,0,False,False,False,1639430027.0
rfmupc,hoh91nv,t1_hoewexx,"So, you hate the child",17,0,0,False,False,False,1639460870.0
rfmupc,hohtx82,t1_hoeys1h,I learned programming BASIC on my dads C64.,1,0,0,False,False,False,1639476724.0
rfmupc,hrqlffr,t1_hoeyfjm,"I don't know any Ocaml (and you're probably more experienced than me at this, since I'm not that experienced a programmer), so I can't say for sure, but based on the criteria you gave, do you think something like Scheme using DrRacket might be easier?

The syntax is dead simple, arguably moreso than Ocaml, and because it's run using an interpreter, it's very easy to see the results of your code quickly. It's also typeless, which means there's one dimension fewer to wrap your head around.

That being said, as someone who's trying to learn how to program by learning C and Scheme side-by-side, I don't know if I'd say functional programming is more straightforward. It's about equal, I would say, although this is based on my rough intuition; I understand imperative programming better than functional because I find C to be more fun to program in, but if I put the effort towards Scheme that I do towards C, I think they'd be about equal.

Also, I realize your comment is about a month old; sorry for necroing.",1,0,0,False,False,False,1641616370.0
rfmupc,hoil2ys,t1_hohyj0e,I was wondering when I was going to see someone mention LOGO.  Turtle graphics or bust. :-),2,0,0,False,False,False,1639493352.0
rfmupc,hofzefa,t1_hofzbq8,"Thats how my first year CS Teacher treats us, the students.",1,0,0,False,False,False,1639438621.0
rfmupc,hohnxwg,t1_hohnjae,"there is a HUGE developer community, code share network and heaps of documentation, examples, forums and communities that have competitions, share ideas, showcase cool stuff",1,0,0,False,False,False,1639471621.0
rfmupc,hoho39m,t1_hohnjae,you can prototype real solutions that you can hold in your hand and actually use - the micro:bit is one of the first devices that 5 mins out of the box you can make something that does something you control - really fabulous tech and cheap,1,0,0,False,False,False,1639471742.0
rfmupc,hoho5ru,t1_hohnjae,terrific gateway drug to other platforms (like rasberry pi and arduino),1,0,0,False,False,False,1639471797.0
rfmupc,hohnzo9,t1_hohnv5p,"if the child is already into minecraft, then programming in minecraft adds a real edge that builds on existing skills",2,0,0,False,False,False,1639471662.0
rfmupc,hogut5l,t1_hog26l4,"Good point. I hadn't considered the potential learning curve for OP as well, sounds like that one might be a time investment",5,0,0,False,False,False,1639453343.0
rfmupc,hofre66,t1_hofexj2,"Fair enough, not trying to split hairs on that. Just meant that some kids are going to pick it up faster or have a little more under their belt already than others.",8,0,0,False,False,False,1639435195.0
rfmupc,hohywn2,t1_hohtx82,It's too bad modern computers aren't accessible like the old days.,1,0,0,False,False,False,1639480807.0
rfmupc,hofzh9p,t1_hofzefa,"Oh, and yet... i know nothing. Nada at all.",1,0,0,False,False,False,1639438657.0
rfmupc,hoi1bwc,t1_hohywn2,"Eh, I disagree. I think it's more easy nowadays. Especially with higher level languages like python or JavaScript. Remember that you had to type line number yourself  for goto statements?",3,0,0,False,False,False,1639482574.0
rf81l6,hocjzr0,t3_rf81l6,"It depends on what you mean by “mutate”. Viruses can be written such that they change their own encoding when they spread to a new system— search for “polymorphic virus”. This is not the same sense of “mutation” that we would use when talking about biological evolution, though. Computer viruses are a lot more fragile than meatspace ones, and once the exploit they target is patched they can’t dynamically discover a new one. This means they don’t “adapt” or “evolve” autonomously.",43,0,0,False,False,False,1639375217.0
rf81l6,hocnwcz,t3_rf81l6,Some computer viruses can introduce slight variations into their program code when they replicate in order to prevent virus scanners from being able to lock onto a signature.  I suppose this is not unlike biological viruses slightly mutating proteins in their capsid that allows them to evade antibodies.,12,0,0,False,False,False,1639377827.0
rf81l6,hoci1ws,t3_rf81l6,"in general, no. it is technically possible i suppose but ""autonomous mutation"" is definitely not ubiquitous in computer viruses like it is in biological ones.",6,0,0,False,False,False,1639373995.0
rf81l6,hocghhz,t3_rf81l6,If programmed to yes.  Particularly if engineered with AI assistance,1,0,0,False,False,False,1639373051.0
rf81l6,hoeo8e5,t3_rf81l6,"I mean... only becuase they share the same name doesn't mean they have anything else in common.   


We use the name ""virus"" to describe a type of malicious software but besides that there is no comparison to a biological virus and its properties. But ofcourse you can design a computer virus to adapt and mutate.   


But still this question seems to be based on a strange assumption that there is a correlation other than the name...",1,0,0,False,False,False,1639419201.0
rf81l6,hoevc3w,t3_rf81l6,"I think the other commenters already covered the good answers but if the concept interests you, check out [Coding Machines](https://www.teamten.com/lawrence/writings/coding-machines/). It's a >!fictional!< blogpost about a few developers that find such a virus while troubleshooting a seemingly innocuous compiler bug.",1,0,0,False,False,False,1639422022.0
rf81l6,holbb8f,t3_rf81l6,"You could create a virus that adds entropy but that would likely just aid in detection since it is better for code to have a function, even entropic code. Usually viruses are updated over time after being detected.",1,0,0,False,False,False,1639534017.0
rf81l6,hocj0yt,t3_rf81l6,"Yes it possible, viruses can attach itself to a system resource which then spawns off to different services or resources and spawns and spawns. WannaCry is a perfect example of a mutated virus",-4,0,0,False,False,False,1639374602.0
rf81l6,hocq9l0,t1_hocjzr0,"Upvote for ""meatspace""",20,0,0,False,False,False,1639379548.0
rf81l6,hoewin9,t1_hocjzr0,"Mutations analogous to meatspace viruses are more like manual updates by adversaries. Adding new features, new evasion, different impact, etc...",3,0,0,False,False,False,1639422492.0
rf81l6,hoci5ox,t1_hoci1ws,But I suppose a new one would have to be made by the author,1,0,0,False,False,True,1639374061.0
rf81l6,hocijqm,t1_hoci5ox,"Sure - but then it wouldn't be autonomous, right?",7,0,0,False,False,False,1639374305.0
rf81l6,hof3t60,t1_hoci5ox,"It would technically be possible to have the virus randomly alter its code independently (although the code that causes it to do that in the first place would of course need to have been written by a programmer). Genetic programming and evolutionary programming are approaches for automatically generating new or altered programs using random mutation and crossover, although I don't think they involve self-mutation.

The problem, especially if the machine code were randomly mutated as just bits and bytes, would be that the vast majority of the mutations would be nonfunctional or nonviable, or not even valid programs. While this may be true of biological viruses as well -- although I'm not an expert and don't know if that's the case -- the sheer volume of biological virus particles even in a single biological host might make that less of an issue if *some* of them end up working out. You won't have a billion or trillion virus processes running on the computer, though, so while the number of potential hosts might theoretically be in the millions, the vast majority of the mutated offspring being nonviable might practically make it a no-go. You probably need quite a volume for completely random mutations to turn out useful.

Of course actual self-replicating computer viruses are probably fairly rare nowadays anyway, and most malware aren't technically viruses.",1,0,0,False,False,False,1639425419.0
rf81l6,hocim9h,t1_hocijqm,Right,2,0,0,False,False,True,1639374349.0
rfovp2,hof8snc,t3_rfovp2,"IDK the ""*best*"" way, but printable characters are definitely a start...",2,0,0,False,False,False,1639427467.0
rfovp2,hofd5e8,t3_rfovp2,Raw bytes -> hex string is my go-to.,1,0,0,False,False,False,1639429303.0
rf03ai,hob96fa,t3_rf03ai,Another one is Sipser's book. Great book overall.,26,0,0,False,False,False,1639352518.0
rf03ai,hob7kxv,t3_rf03ai,"Some of the resources I used when I was learning complexity classes:

1. [Abdul Bari's take on the topic](https://www.youtube.com/watch?v=e2cF8a5aAhE). Definitely my favorite explanation of it. 
2. [This stackoverflow post.](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)
3. [Video by up and atom](https://www.youtube.com/watch?v=EHp4FPyajKQ)
4. [Another one by hackerdashery](https://www.youtube.com/watch?v=YX40hbAHx3s)
5. [This high level overview article by MIT news](https://news.mit.edu/2009/explainer-pnp)",21,0,0,False,False,False,1639351801.0
rf03ai,hoc2cc0,t3_rf03ai,"Note, Wikipedia is a terrible resource for learning any technical thing. It is a great resource when you need to refresh yourself on a thing you once knew, or are looking for extra info.",9,0,0,False,False,False,1639365811.0
rf03ai,hobt2zl,t3_rf03ai,"Algorithms Design

https://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358/",4,0,0,False,False,False,1639361590.0
rf03ai,hob939w,t3_rf03ai,"Not positive on a resource since I haven't touched this stuff since school, but I'd start with looking at a few reductions to get a sense of how problems relate to each other. Some simple ones might be longest path, Hamilton cycles, and degree-constrained spanning trees. The main idea is that there are mappings between these problems that keep things small (polynomial).

Maybe check out Karp's stuff or Gary and Johnson's stuff to understand the reductions. Once you're a bit comfortable with these ideas then you can get into the nitty gritty of non-deterministic Turing machines and the reduction to general satisfiability.",3,0,0,False,False,False,1639352477.0
rf03ai,hobdjk2,t3_rf03ai,"In case no one has given you a high level idea yet:

A problem X is NP-complete if it is NP-hard and in NP.

NP: A problem you can solve in non deterministic polynomial time; I like to draw the analogy that if you had computer that can run in parallel in multiple dimensions where each dimension tries a different result, it can find a solution within polynomial time ;-). A typical test is to prove that you have an algorithm to check that a result is a solution to the problem in polynomial time.

NP-hard: You can reduce a problem Y that is also NP-hard to X, so if you solve the X, you neccessarily solve that other NP-hard problem. That means X is just as hard as Y.",1,0,0,False,False,False,1639354497.0
rf03ai,hodx98k,t3_rf03ai,"I learned all about Turing Machines and NP Completeness from one book: Computers and Intractability: a Guide to the Theory of NP-Completeness. They provide the intuition and hard math for everything you might need involving NP and entering the polynomial hierarchy. You might also be interested in reading Karp's original paper on 21 NP problems, just to see what the first reductions looked like in practice. They're both around 50 years old, but still very readable and accessible.",1,0,0,False,False,False,1639408353.0
rf03ai,hocr6lg,t3_rf03ai,"Gödel, Escher, Bach: an Eternal Golden Braid a book by Douglas Hofstadter.",0,0,0,False,False,False,1639380242.0
rf03ai,hobaz8l,t1_hob96fa,"I second this. Though many algorithm books discuss this topic, I believe studying formal languages is the best way to get an intuition for the complexity classes.",8,0,0,False,False,False,1639353331.0
rf03ai,hoctu2f,t1_hob96fa,+1 for Sipser's book. It's quite good.,3,0,0,False,False,False,1639382313.0
rf03ai,hoci9ep,t1_hob7kxv,"At this point, I feel like I've learned more from Abdul Bari than my entire CS department combined.",8,0,0,False,False,False,1639374125.0
rf03ai,hob80ac,t1_hob7kxv,Thanks!,2,0,0,False,False,True,1639351994.0
rf03ai,hoc2vlv,t1_hoc2cc0,Strongly agree. I thought I had this concept sorted out in my undergrad (Cormen). But I now see how there are many holes in my understanding.,2,0,0,False,False,True,1639366058.0
rf03ai,hof9sel,t1_hobdjk2,"For NP, your definition has a slight mistake. NP includes those problems you can *solve* in non-deterministic polynomial time but can be verified in polynomial time. We don’t know how to build a non-deterministic machine, so your current wording suggests we can’t verify solutions in NP with our current technology.",2,0,0,False,False,False,1639427888.0
rf03ai,hofa1o1,t1_hof9sel,Thanks! I corrected it. I meant to say solve as per my analogy below.,2,0,0,False,False,False,1639427997.0
reoznh,ho8u8ad,t3_reoznh,"No, because they are barely good enough to translate handwriting to text right now. 

Also police do not keep handwriting databases.",57,0,0,False,False,False,1639316766.0
reoznh,ho9vi4b,t3_reoznh,Computers aren't any better at guessing than humans. It is just a way to remove the blame from people when the guesses turn out to be bad. I'm sure law enforcement will love it.,8,0,0,False,False,False,1639332451.0
reoznh,ho8yymr,t3_reoznh,"In theory, sure. There's not enough data to train on, though, and getting it would be essentially impossible.",11,0,0,False,False,False,1639319182.0
reoznh,hoa79r2,t3_reoznh,Maybe as a tool to recognize certain pattern in the handwriting for a expert on this field. But I think ANN is a overkill for this purpose. I can think of a kind of template matching combined with a certain threshold.,2,0,0,False,False,False,1639336935.0
reoznh,hoa8g8q,t3_reoznh,"Neural Nets can do any learnable task, it has an infinite hypothesis space. This is just identifying similarity between handwriting and there are tons of stuff on this out there already. 
https://arxiv.org/pdf/1606.06472.pdf 
here is a paper that does just that,",1,0,0,False,False,False,1639337386.0
reoznh,hoa6z7u,t3_reoznh,No they can identify psychopathic styles of writing or possible pick traits of identity but it can’t prove that someone is a murder verse just a psychopath,0,0,0,False,False,False,1639336828.0
reoznh,hoa5vk7,t3_reoznh,"I don't know if a neural network is required. Similarity analysis can be done with a variety of techniques.

But you could use a neural network. Depending of what you want to achieve you 
a) train the neural network to identify if two given hand writings are from the same author,
or b) identify if a given handwriting originates from a specific person.

For a) you just have to collect pairs of two handwritings from the same person (maybe couple days/months/years apart) and train the network on those pairs (and of course pairs of not matching handwritings for the negative case).

For b) you would need to collect handwritings the persons you want to identify and train the network to match the writing and the person. Than you can let the network predict the author of a new handwriting, given the person was in the training set.",1,0,0,False,False,False,1639336414.0
reoznh,hobvuts,t3_reoznh,Maybe dust the note for fingerprints. Or collect DNA from the note. The post mark on the envelope the note came in.,1,0,0,False,False,False,1639362832.0
reoznh,hod60cv,t3_reoznh,"No, because my handwriting changes constantly. I don't know how people keep a consistent writing style. They might as well have their own font tbh. I don't even know cursive so my signature is basically a few letters have fancy lines, and then it's also not consistent.",1,0,0,False,False,False,1639392655.0
reoznh,hpbg6br,t3_reoznh,"I don't know about neural networks.... but people like do this for a living.  

You can always type what you want to say into a typewriter and then trace the output onto a piece of paper. Undetectable at that point.",1,0,0,False,False,False,1640021749.0
reoznh,ho9ylu9,t1_ho8u8ad,"> police do not keep handwriting databases
 
[They do, actually](https://www.fbi.gov/services/laboratory/scientific-analysis/questioned-documents). If you are a researcher in a respected institution you can be granted (very closely overseen) access to their database for the purposes of AI research. AI is already used for this purpose.",17,0,0,False,False,False,1639333643.0
reoznh,ho9yrvi,t1_ho8u8ad,"That isn't even remotely the same ML task. Translating handwritten text to characters is entirely different from identifying an author from handwritten text. I'd venture to guess you could even identify authors from digitally written texts, let a lone handwritten text (which has at least an order of magnitude more of information). There are so many idiosyncrasies connected to written communication. Missing i dots or t crosses, frequency of use of ellipses, capitalization, style of the `a`, etc. There is an infinite amount of variability. Given a large enough corpus, it is definitely doable. It is so doable, I would be surprise if there isn't a standard commercialization of it already.

The fields of study is called: https://en.wikipedia.org/wiki/Stylometry",17,0,0,False,False,False,1639333708.0
reoznh,ho9f6yx,t1_ho8u8ad,"""handwriting databases"" made me laugh",10,0,0,False,False,False,1639325954.0
reoznh,ho9sc5w,t1_ho8u8ad,They do keep the notes though! Because they're considered evidence,6,0,0,False,False,False,1639331217.0
reoznh,hoc21rq,t1_ho9vi4b,"Machine learning chess AIs would like to have a word with you. If they are that capable of pattern recognition already, and better at it than humans, I see no reason why this pattern (handwriting) would be a terribly difficult leap. Computers are REALLY good at pattern recognition. So with a sufficient dataset and training, sure seems reasonable. I'm a dev, and I've written several ml training curriculum for my ml ai for my chess like game.",3,0,0,False,False,False,1639365676.0
reoznh,hoc2hjf,t1_hoc21rq,A just society would not convict someone on such flaky evidence.,1,0,0,False,False,False,1639365878.0
reoznh,hoc2n1f,t1_hoc2hjf,"It's not societies decision to convict or not, it's the grand jury's, and certainly takes more evidence than 1 handwriting match.",1,0,0,False,False,False,1639365948.0
reoznh,hod91r2,t1_hoc2n1f,"Grand juries determine whether to prosecute (indict) someone, not convict them. Regular juries (or judges in a bench trial) convict people. And that's only in the US, normal countries have normal criminal investigations, none of this grand jury indictment nonsense.",1,0,0,False,False,False,1639395048.0
rfa9t6,hodcbu5,t3_rfa9t6,"There will always be malware, even if a new tech came out that was hard to infect, it will only be a matter of time before it will have targeted malware.",6,0,0,False,False,False,1639397421.0
rfa9t6,hocxc2l,t3_rfa9t6,Windows itself is malware by definition. So no,-3,1,0,False,False,False,1639385218.0
rfa9t6,hoeuruh,t3_rfa9t6,"I would guess no. Maybe it will be harder and harder to make, but it’ll probably never be impossible. An important thing to realize is malware does normal things for bad purposes. For example, you need the ability to encrypt files or send them over the internet in normal programs. However, who and when this happens to determines if it’s malware or a great product.",1,0,0,False,False,False,1639421799.0
rfa9t6,hof4d90,t3_rfa9t6,"No, because the sorts of things that make a program malware are necessary for legitimate programs to be able to do as well.",1,0,0,False,False,False,1639425644.0
rfa9t6,hog3kma,t3_rfa9t6,"Even if you could give an unambiguous definition of ""bad"" behavior by a program, whether an arbitrary program has such behavior is undecidable.",1,0,0,False,False,False,1639440758.0
rfa9t6,hojjpu9,t3_rfa9t6,"No. It's not generally possible for antivirus software (or any software) to tell for certain whether a particular program behaves in a given malware-ish way.

Antivirus software works by either identifying individual programs or pieces of code as specific malware, or by using so-called heuristics for telling whether an unidentified program seems to have malware-like behaviour. The former is limited by the requirement to specifically identify each particular piece of malware, so it always needs to play catch-up; the latter is not nearly 100% accurate, and it can't really be.

Even if you could unambiguously define which kind of behaviour means that an unidentified program is malware, it's not generally possible to have an antivirus algorithm that would unerringly tell if another program behaves that way. That's partially limited by our ability to design such an algorithm, of course, but it's also something that's been proven as mathematically impossible to do with absolute accuracy.",1,0,0,False,False,False,1639507142.0
rfa9t6,hqcx8gr,t3_rfa9t6,"No, malware prevention will continue to improve; while malware itself will become better at countering malware prevention.",1,0,0,False,False,False,1640744382.0
rfa9t6,ht2j55m,t3_rfa9t6,"No, there will never be a point where malware won't be made. People will always be finding out new ways to exploit stuff because there's always ways to break things. Nothing is truly bulletproof (invincible).",1,0,0,False,False,False,1642444854.0
rfa9t6,hofhuu7,t1_hocxc2l,Edgy,6,0,0,False,False,False,1639431227.0
rfa9t6,hoet2u1,t1_hocxc2l,How you figure that?,5,0,0,False,False,False,1639421128.0
rfa9t6,hpf57ua,t1_hocxc2l,Edge lord,1,0,0,False,False,False,1640092236.0
rfa9t6,hoh5o8o,t1_hoet2u1,"Well to name a few:
1. Loads of Crapware and advertisments right inside startmenu.
1. Per User Unique id for targetted advertisments.
1. Forced updates which restarts a system in the middle of work.
1. Incessant shoving of upgrades from one version to another.
1. Difficulty to change default webbrowser.

Windows 7 was last good version of Windows all after it can be classified as adware which are also Malware. So yes present day windows are malware.",1,1,0,False,False,False,1639458849.0
reinb8,ho7wdf6,t3_reinb8,It's just regular mod. I think if you try and think of some examples it should be pretty clear. Consider 4 mod 5. Both 2\^2 and 3\^2 are congruent to 4 mod 5 so 2 and 3 are the modular square roots.,14,0,0,False,False,False,1639290878.0
reinb8,ho8a8dv,t3_reinb8,"The meaning of ""mod"" depends on the authority of the source because the meaning of `%` differs in programming languages. In languages like JavaScript and Java `%` means remainder whereas in languages like Ruby and C# it means modulus. There is a difference between remainder and modulus which you can read about [here](https://dev.to/hamiecod/remainder-vs-modulus-3mc8).

[This math stack exchange question](https://math.stackexchange.com/questions/633160/modular-arithmetic-find-the-square-root/633174) might help you. I understood what is modular square root but ah I don't really know VDF so it would be better if you read the stack exchange answer as compared to my interpretation.",1,0,0,False,False,False,1639301825.0
reinb8,ho7wow0,t1_ho7wdf6,"Hey, thanks. That's a good simplification for me to begin with.",1,0,0,False,False,True,1639291097.0
reinb8,ho8c282,t1_ho7wdf6,"I dont think this matters here, but sometimes there is a difference between “%” and modulo and the difference is how negative numbers are handled.  % quite often represents “remainder” and can get negative numbers, whereas if you want to stay in positives you might convert remainder to modulo by adding absolute value of m from n%m). So  -7 % 3 = -2 which is a remainder, and modulo would be -2 + 3 = 1. Fix me if I am wrong, might be talking nonsense haha",1,0,0,False,False,False,1639303394.0
reinb8,ho8d0rx,t1_ho8c282,"You might be right! I'm a math student so I was thinking about this concept from a pure math background, which is a world where you just say -7, -1 and 2 are all the same mod 3 and leave it at that. however I could see how there could be cases where you want to be more specific in computer science.",2,0,0,False,False,False,1639304218.0
rdc453,ho03t0t,t3_rdc453,"You have inspired me. From now on I’m gonna use combinations of the letter i, j and l for my variables.",191,0,0,False,False,False,1639153476.0
rdc453,ho04gtz,t3_rdc453,"It’s from math notation where i,j refer to rows and columns. There’s no real reason it continues except that’s how people come to learn the notation and people stick with what they know",95,0,0,False,False,False,1639153739.0
rdc453,ho099ii,t3_rdc453,"The worst is when a student/professor, or anyone really, writes handwritten m and n in sloppy cursive.",36,0,0,False,False,False,1639155639.0
rdc453,ho0m2bd,t3_rdc453,"Pull up a chair, young'ens, and hear about the old timey language FORTRAN.  Back in FORTRAN, variables were implicitly typed based on the first letter of their name.  Start your variable with letters between 'i' and 'n' and it would be an integer. Outside that range, it would be a float (if I remember correctly).  Why 'i' and 'n' ?  Because some clever language designer decided 'integer' thus the 'i' and 'n'.  Using i and j as loop variables started from there (in the 60s and 70s, probably) and has continued on ever since.",73,0,0,False,False,False,1639160668.0
rdc453,ho0eg29,t3_rdc453,"We keep using (i,j) because it is evocative of all the other times we used (i,j).  We reuse them precisely so that it's like all the other times we used them, so that you can see it in a glance and have a sense of what it means.  We use x when we mean an unknown real number or a variable of a real number, we use z when it's an unknown complex number.  It's really helpful to quickly understand something, when we reuse these names.  I would find a text nauseating if it didn't use this orienting technique.",28,0,0,False,False,False,1639157685.0
rdc453,ho0rrlw,t3_rdc453,"I think OPs argument is against certain typefaces and fonts in which the two letters (i,j) can appear identical on initial glance.  Some people are more prone to this than others due to a variety of reasons.",9,0,0,False,False,False,1639162934.0
rdc453,ho1odxc,t3_rdc453,"It was hard to write, it should be hard to read! /s",7,0,0,False,False,False,1639176023.0
rdc453,ho0ejj8,t3_rdc453,"Complain to whoever came up with the alphabet, all those pairs are letters next to each other and that's why they're used like this.

'i' is index, 'n' is number (or natural), 'v' is vector or vertex.",12,0,0,False,False,False,1639157722.0
rdc453,ho07w0x,t3_rdc453,"I just like to use letters that are close to each other, because it’s easier for me than coming up with another name. Sometimes I’ll even use a,b,c for indices. But if I’m not working with complex numbers, I often use i because it’s easy to remember i = index. And after that comes j,k,l.",3,0,0,False,False,False,1639155093.0
rdc453,ho07081,t3_rdc453,"Convention, mostly. I can see good, descriptive variable names being useful, but convention will almost always win out in math/compsci.   


None of us want to incur the wrath of our professors, and I know professors that would be instantly annoyed at using something less conventional.",4,0,0,False,False,False,1639154746.0
rdc453,ho0aclg,t3_rdc453,"The i is mostly a convention for [i]terators, I guess the J was the logical follow-up people thought of when they started this trend. I for one only use i in very concise and simples loops/iterations, otherwise short and descriptive names make more readable code / math problems.",4,0,0,False,False,False,1639156069.0
rdc453,ho06xng,t3_rdc453,Lmao how can you get i and j mixed up. I can understand if you're reading handwriting maybe,1,1,0,False,False,False,1639154717.0
rdc453,ho0cbrq,t3_rdc453,"Speaking as someone from a 'doin stuff' side of research more than hard core computer science, we have been trying and failing to have compsci and maths researchers branch out to use actual words that tell you what they are for as variable names for decades now with no success. They are more maths than program, twisted and evil.
 
The font issue is the journal they're publishing in, which was set for printed physical journals and has never been updated from their original latex style file.
 
This all comes down to the fact the reviewers in any discipline (not a compsci issue, this is an academia issue) are an incestuous club that all know each other and all know the work, so they expect to see an i where there is an i and a j where there is a j and it all feels natural to them. They would apply more scrutiny to a paper that broke from their club's tradition and wrote out inner_index and outer_index instead, and if you suggested updating the font they would crucify you.",1,1,0,False,False,False,1639156849.0
rdc453,ho1xtw2,t3_rdc453,You feel this way as you've never studied math. It can happen.,1,0,0,False,False,False,1639180055.0
rdc453,ho0fmm9,t3_rdc453,"I think it’s because you have to take the index of things a lot, but it’s shorter to type i instead of index. And then it just went to j after then and so on and so forth",1,0,0,False,False,False,1639158151.0
rdc453,ho0lqsm,t3_rdc453,"It’s mostly for convention. Picking alphabetically close variable names for variables that are related to each other is just common sense. It feels and reads weird, and gets confusing really fast, when you have a group of variables that are related but they all use arbitrary names. Even more so when the proof is several pages long and then you have to remember at each step what each individual variable does/is. 
 
So, if you pick i for something because it reminds you of index, iterator, image, etc. the logical follow up is to pick j after and not ξ, Ω, z, or “mojito”.",1,0,0,False,False,False,1639160543.0
rdc453,ho0vdkz,t3_rdc453,"Not sure about all of the common ones but I believe that ""i"" is typically shorthand for ""iteration""? At least that's the impression I got and why I use it. Also ""n"" is just shorthand for ""number"", so that's what I use as the input number for a function that then gets calculated in various ways.

Some don't make sense to me, like the connection weights in artificial neuron functions are typically a ""g"", and I'm not really sure why.",1,0,0,False,False,False,1639164391.0
rdc453,ho0wb6k,t3_rdc453,as long it's not i and jota i'm fine. lol,1,0,0,False,False,False,1639164764.0
rdc453,ho15myi,t3_rdc453,From now on I’m just going to use 39 i’s in a row as a variable to represent row index and 40 to represent column index,1,0,0,False,False,False,1639168460.0
rdc453,ho1v6je,t3_rdc453,"i = iterator

j = next letter in the alphabet, if you've already used i as an iterator, same as k etc.

It really does make some amount of sense.",1,0,0,False,False,False,1639178905.0
rdc453,ho1vgxi,t3_rdc453,*foo* rules!,1,0,0,False,False,False,1639179030.0
rdc453,ho217qq,t3_rdc453,felt this way starting algebra as I always thought multiplication was the 'x' symbol I asked why the hell would they make 'x' different,1,0,0,False,False,False,1639181532.0
rdc453,ho2dct7,t3_rdc453,"I find it simple.

i, j, k for loops

m, n for matrix dimensions

p, q for probabilities

u, v for random variables

The point is, they are almost exclusively used as throwaway variables (index, dimensions, temporary, etc.). It’s very easy to understand what arr(i,j,k) means. arr(row,col,depth) isn’t as easy on the eyes. Also, there are languages where row and col may be reserved for functions.

In some languages, i refers to complex numbers so some people use ii instead to distinguish the i complex number from the ii variable.",1,0,0,False,False,False,1639186951.0
rdc453,ho2ej54,t3_rdc453,"Some fields of math likes to use character set to denote type, so you will have vector v in one vector space being projected into vector ν (Greek letter nu) in another vector space. Physics likes to do the same thing too with v and nu being the velocity of some object in different frames of reference. It's because all those academics don't need to maintain a code base over two decades of enhancements and bug-fixing.",1,0,0,False,False,False,1639187474.0
rdc453,ho2hjzq,t3_rdc453,I will use capital i and l together. Thanks!,1,0,0,False,False,False,1639188856.0
rdc453,ho2if1t,t3_rdc453,"LOL. OP complaining about i, j, k has clearly never encountered ξ in the wild.",1,0,0,False,False,False,1639189244.0
rdc453,ho2uzw6,t3_rdc453,I’ve been revisiting my maths lately and have found that letters as variables can be easy to visually distinguish if you write the variables in cursive. I and J would not look the same in such a scenario.,1,0,0,False,False,False,1639195160.0
rdc453,ho2vq44,t3_rdc453,I only use cool greek letters like ζ ϕ . It make me look smart.,1,0,0,False,False,False,1639195523.0
rdc453,ho37t3j,t3_rdc453,"I feel this also depends on the context. Like, you see those kinds of variables in proofs and in papers because the target audience is people who are interested/working in the field and so they know what they represent and they can make sense out of very quickly. It just gets straight to the point you know.

But then if you are say teaching to kids, then using variables like i, j is not helpful. They should be more meaningful and much more clearer. Like, forced documentation in a way. This allows less confusion and makes it easier for kids to understand for instance.

But I am with you. I tend to have long variable names because it's just so much more clearer to me when I am coding. And for anyone reading the code as well. Yes, it can clutter but there are times when it's helpful to have long names for certain code snippets. Not for everything but for some.",1,0,0,False,False,False,1639202087.0
rdc453,ho3d62u,t3_rdc453,Sometimes i use funny variables to fuck with my bosses,1,0,0,False,False,False,1639205505.0
rdc453,ho3e76o,t3_rdc453,"i for index, j comes after. They aren't that hard to distinguish.",1,0,0,False,False,False,1639206225.0
rdc453,ho3hpmg,t3_rdc453,"Man, many people don't seem to actually read your post before commenting 🤦",1,0,0,False,False,False,1639208853.0
rdc453,ho3ndik,t3_rdc453,Im pretty sure i is short for index and j is just the next letter. n is short for number and m is just the next letter and p is short for prime and q is just the next letter,1,0,0,False,False,False,1639213230.0
rdc453,ho3pdf3,t3_rdc453,Because it’s more accessible to use pairs of similar letters to define similar things,1,0,0,False,False,False,1639214850.0
rdc453,ho4cs00,t3_rdc453,"well matematician are a weird bunch, but every human being is not exempt. let them have their convention. as a programmer i learned that different lingo and conventions are used in each work environment so it's really not only a problem of mathematics.

i'm all for longer and auto documenting variables, but it's always a failed crusade from the start.",1,0,0,False,False,False,1639230830.0
rdc453,ho4i70i,t3_rdc453,"I study quite a bit of mathematics. I see (u,v), (m,n), (I,j), (alpha,beta) - and more - all over the place.",1,0,0,False,False,False,1639233586.0
rdc453,ho4j6hs,t3_rdc453,"I prefer to choose these
I,k,n
t,p
Also a,b,c in some cases. Because they are distinguishable.
And just avoid mirrored or cloned letters all together.
No q,m,j (especially fuck j).
Also im annoyed by people who use single letters T,U,V
For generic parameter names. Hope they burn in hell.",1,0,0,False,False,False,1639234076.0
rdc453,ho4kjc6,t3_rdc453,"Now I want to use greek letters for my variable names. ([Swift](https://docs.swift.org/swift-book/ReferenceManual/LexicalStructure.html) permits this.)

Can you imagine something like:

    for ι in Α {
        ...
    }

Better yet, Swift allows custom operators to be defined, which means I now want `∈` to be defined as a binary operator to perform a 'set contains' operation.",1,0,0,False,False,False,1639234744.0
rdc453,ho4yrrx,t3_rdc453,u v was the worst for me when turning Assignments that are hand written,1,0,0,False,False,False,1639241068.0
rdc453,hpktxwk,t3_rdc453,"Always have in the back of my head that ""i' stands for index and ""n"" for numbers... #OkayCaptainObvious :|",1,0,0,False,False,False,1640193800.0
rdc453,ho087d7,t1_ho03t0t,"string aO0D = ""go fuck yourself"";",55,0,0,False,False,False,1639155217.0
rdc453,ho04c2v,t1_ho03t0t,Lmao,8,0,0,False,False,False,1639153686.0
rdc453,ho06z9c,t1_ho03t0t,r/foundsatan,14,0,0,False,False,False,1639154736.0
rdc453,ho3fm0d,t1_ho04gtz,"At the very least you should be using `ii` and `jj` in your code - if only to make the editor search function work usefully with them. Even then, lots of code quality tools will fail you for use of a 2-letter variable name.",5,0,0,False,False,False,1639207272.0
rdc453,ho0gqsm,t1_ho04gtz,I’m aware that’s where it comes from but it’s dumb that it ever started that way.,-27,0,0,False,True,True,1639158587.0
rdc453,ho0zjm8,t1_ho099ii,"> The worst is [...] cursive.

Agreed. :D",8,0,0,False,False,False,1639166062.0
rdc453,ho2f41i,t1_ho099ii,"""minimum"" in cursive is especially bad",3,0,0,False,False,False,1639187741.0
rdc453,ho0noyi,t1_ho0m2bd,"FORTRAN still used today, by the way.  Is very important in numerical computing. Because of some quirks in the language design, FORTRAN compilers can generate some of the fastest code (faster than C).  https://en.wikipedia.org/wiki/Fortran#Science\_and\_engineering",27,0,0,False,False,False,1639161313.0
rdc453,ho0m80p,t1_ho0m2bd,Reference: [https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/data-types-constants-and-variables/variables-1/data-types-of-scalar-variables/implicit-typing-rules.html](https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/data-types-constants-and-variables/variables-1/data-types-of-scalar-variables/implicit-typing-rules.html),13,0,0,False,False,False,1639160730.0
rdc453,ho1285z,t1_ho0m2bd,Oh noooo... Somehow it feels like every day I learn a new fun fact about old-school Fortran that makes me gag haha,4,0,0,False,False,False,1639167125.0
rdc453,ho1gt8a,t1_ho0m2bd,IMPLICIT NONE!!! One of the first things I learned about when I was learning Fortran.,3,0,0,False,False,False,1639172894.0
rdc453,ho3eule,t1_ho0m2bd,"Hence the old joke in nerd circles: ""God is real unless declared integer"".",3,0,0,False,False,False,1639206698.0
rdc453,ho3dubw,t1_ho0m2bd,"Fotran was created by mathematicians. This notation using i,j as indices was there before Fortran was invented.",2,0,0,False,False,False,1639205978.0
rdc453,ho3eyh4,t1_ho0eg29,Right. It's the principle of least surprise.,2,0,0,False,False,False,1639206779.0
rdc453,ho0hrtn,t1_ho06xng,"See page 6/30 (or 262) top of the right column of [this](https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf) paper I was just reading (a printed version of). 

Not sure how to attach a screenshot of it without too much effort.",6,0,0,False,False,True,1639158982.0
rdc453,ho0mlv1,t1_ho0cbrq,"Self-documenting variables make sense in code, but using words for variables in formal proofs is not a good idea. Just put pressure on publishers to use better fonts and we can keep the conventions about what variable letters stand for in various contexts. That part is actually reasonably important for quickly grasping what's going on.",5,0,0,False,False,False,1639160877.0
rdc453,ho3jll6,t1_ho087d7,You called?,14,0,0,False,False,False,1639210291.0
rdc453,ho27gsr,t1_ho087d7,"main(){

int i = 0;

while(i=0){

cout << aO0D;

}

}",9,0,0,False,False,False,1639184298.0
rdc453,ho070hx,t1_ho06z9c,"Here's a sneak peek of /r/foundsatan using the [top posts](https://np.reddit.com/r/foundsatan/top/?sort=top&t=year) of the year!

\#1: [â€™](https://i.redd.it/8u9uo2ekove61.jpg) | [18 comments](https://np.reddit.com/r/foundsatan/comments/la4j28/â/)  
\#2: [I guess i won't go hiking for a while](https://i.redd.it/op9bc53nzpb61.jpg) | [42 comments](https://np.reddit.com/r/foundsatan/comments/kylt43/i_guess_i_wont_go_hiking_for_a_while/)  
\#3: [Airpods](https://i.redd.it/7n0d5klmf4z61.jpg) | [18 comments](https://np.reddit.com/r/foundsatan/comments/nce4k9/airpods/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[Source](https://github.com/ghnr/sneakpeekbot)",0,0,0,False,False,False,1639154749.0
rdc453,ho5a6me,t1_ho3fm0d,U can’t use the same thing because there’s no differentiating,1,0,0,False,False,False,1639245829.0
rdc453,ho0myfa,t1_ho0gqsm,"It's not. Index notation like `a_{i, j}` was a huge innovation, and massively helps readability. If you have doubts, consider reading Gauss:

https://www.gutenberg.org/files/36856/36856-pdf.pdf

Here, instead of noting related variables by, say `P_1, P_2, P_3`, Gauss uses separate letters of the alphabet `P, Q, R`. It may seem trivial, but once you have many examples of that going at the same time, it really affects comprehensibility. You also run out of letters pretty damn fast.

You may argue that `i` and `j` are poorly chosen, but they fit nicely in small type as a subscript, and allow packing in the type of dense information needed in communicating mathematics. I've never had any trouble distinguishing them in my own hand, and they stand out just fine in typewritten text.

You may argue that we shouldn't use single letters in the first place, and in some cases you may be correct. But in many others something like `a_{first index, second index}` just adds a ton of clutter, and I think you would find it to be a worse choice in practice.",62,0,0,False,False,False,1639161015.0
rdc453,ho0m7u5,t1_ho0gqsm,It started back when things were hand written and it was much more easy to distinguish between the two letters. This is why we need to invent a monospaced Papyrus font to use for all future coding. Thank you for coming to my TED talk.,16,0,0,False,False,False,1639160728.0
rdc453,ho22qqo,t1_ho0gqsm,"Why write a post asking why, if you already knew the answer?",-1,1,0,False,False,False,1639182201.0
rdc453,ho0wn6q,t1_ho0noyi,Neat edge case.,9,0,0,False,False,False,1639164897.0
rdc453,ho22vej,t1_ho0noyi,"fwiw, this is no longer the case, but there is still a lot of legacy FORTRAN out there.",1,0,0,False,False,False,1639182259.0
rdc453,ho3mgp9,t1_ho0noyi,"Fortran is not faster than C.  For numerical stuff, both are basically equivalent.  The only real difference that I have been able to find is for recursive functions, where Fortran is much slower.

A lot of fast code has been written in Fortran, and it will remain important I'm scientific computing, but it isn't a unicorn.",1,0,0,False,False,False,1639212498.0
rdc453,ho3u375,t1_ho0noyi,">some of the fastest code (faster than C)

smiling in assembly

Look I might want to explode every time I look at any x86 assembly, but damn is it fast.",1,0,0,False,False,False,1639218539.0
rdc453,ho4ic9p,t1_ho0noyi,What is it about those quirks that make it so fast? Why don't we use those quirks in the design of new languages to make them faster?,1,0,0,False,False,False,1639233657.0
rdc453,ho13t6x,t1_ho0m80p,Holy shit! I was SO sure you were just making this shit up.,6,0,0,False,False,False,1639167752.0
rdc453,ho2ofcz,t1_ho1gt8a,Indeed. We had 'implicit none' hammered into us when I as learning FORTRAN. Woe be to those who had to maintain enormous libraries of older FORTRAN.,2,0,0,False,False,False,1639192007.0
rdc453,ho0owtl,t1_ho0hrtn,"Wow fair enough, terrible choice of font there",6,0,0,False,False,False,1639161798.0
rdc453,ho0ubao,t1_ho0mlv1,what makes self documenting variables a bad idea in math?,3,0,0,False,False,False,1639163962.0
rdc453,ho3b1j5,t1_ho0myfa,"Bruv, he didn’t mean the invention of notation— he means the use of i,j in it.",8,0,0,False,False,False,1639204113.0
rdc453,ho30s4s,t1_ho0myfa,I concur,2,0,0,False,False,False,1639198126.0
rdc453,ho0tn92,t1_ho0m7u5,"Monospace is overrated. I've been coding in a quasi-proportional font for a few years now. Iosevka Aile Code. quasi- cause most chars are the same width, but space and i etc are half width.

https://typeof.net/Iosevka/",-3,1,0,False,False,False,1639163691.0
rdc453,ho1oxcg,t1_ho0wn6q,"I believe numpy has some Fortran source for the fast mathematics, so not really edge case.",8,0,0,False,False,False,1639176251.0
rdc453,ho2o849,t1_ho22vej,"Yeah, even back when I was first learning FORTRAN (long long ago), our instructor emphasized 'implicit none' to turn off that auto-variable nonsense.",2,0,0,False,False,False,1639191913.0
rdc453,ho4zoou,t1_ho4ic9p,"Disclaimer: I'm not a compiler expert. This is just what I understand from reading over the years.

FORTRAN is a very simple language. It's in fact the first compiled ""high level"" computer language. The name FORTRAN comes from ""Formula Translation"" and originally it was just a simple way to write math. The problem with, for example C, is some of C's rules prevent super optimization of code. The one I understand that causes the most problems is aliasing. [https://en.wikipedia.org/wiki/Aliasing\_(computing)](https://en.wikipedia.org/wiki/Aliasing_(computing))

If the same memory location can be represented by multiple variables, then the compiler has to assume worst case and cannot optimize certain paths. If the language doesn't allow such flexibility, then the compiler can crack its knuckles and go to town on optimization.

(Lots of hand waving here, sorry. It's been a long, long time since compilers class!)",1,0,0,False,False,False,1639241445.0
rdc453,ho11ix5,t1_ho0ubao,"Mostly that it will make proofs hard to read. Formal statements are compact with lots of single character width symbols and potentially many variables. So if variables need to be entire words or multiple characters, these statements will no longer be compact. 

Further, formal statements are often quite general, so whatever words you pick for variable names are going to be quite general too, where it makes no difference if it's named `x` or `realNumber`.

Take the Pythagorean theorem for a simple example: 

`a^2 + b^2 = c^2`

Compare that to:

`sideOneLength^2 + sideTwoLength^2 = hypotenuseLength^2` 

But now imagine for much more complicated statements or entire proofs written out like this. 

In proofs, compactness is a virtue that helps with readability and understandability. There's a reason separate mathematical language was invented and we don't write formulas out like 

`some number minus another number times, the first number plus the second number, equals the first number squared minus the second number squared`

in a natural language like English. We just write 

`(a-b)(a+b) = a^2 - b^2`.",8,0,0,False,False,False,1639166845.0
rdc453,ho0wfaa,t1_ho0tn92,I've been coding in wingdings for 20 years and that's clearly superior.,32,0,0,False,False,False,1639164810.0
rdc453,ho1u7av,t1_ho1oxcg,"Well, in all fairness though, unless you're specifically modifying the Fortran code, you can use more user-readable  variable names.",1,0,0,False,False,False,1639178483.0
rdc453,ho5jafw,t1_ho4zoou,"That's actually a really good explanation and a good starting point for my investigation. I'll look in to that more, thanks!",1,0,0,False,False,False,1639249634.0
rdc453,ho3r91g,t1_ho1u7av,I was just answering about the fact that Fortran is still used today in a iper used library. Of course using numpy doesn't require using Fortran (thank god) but just because it's under the hood it does not mean it's a super edge case.,2,0,0,False,False,False,1639216341.0
rdlhtp,ho1zl62,t3_rdlhtp,"Whats wrong with 1234567899, or am I missing something?",9,0,0,False,False,False,1639180816.0
rdlhtp,ho262s5,t3_rdlhtp,Try to use extended pigeonhole principle,5,0,0,False,False,False,1639183676.0
rdlhtp,ho50t2b,t3_rdlhtp,"I forget all the formal terms and rules for mathematical proofs so apologies in advance for the mistakes.

  
Fact 1: After even number of moves monkey will be on same parity (odd/even) as it started, after odd number of moves it will be on different parity.

  
Start at door 2, go up 1 door at a time (2, 3, 4,...)  


Fact 2, starting from the left and working your way up one door at a time, if monkey starts on same parity door you start on monkey will be caught at worst when you get to n - 1 door where n is the number of doors.

  
Proof by induction

  
base case 3 doors n = 3

  
proof by exhaustion

If you start on door 2 monkey must be on 2, you open door 2 and catch him

  
If you start at 1 monkey must be on 1 or 3. If on 1 you will catch on first check. After first check monkey must move to 2. So on second check you will catch it. 3 - 1 = 2

  
case k + 1 (4 doors) n = 4  
proof by exhaustion

If you start at door 1 monkey and monkey starts on door 1 you will catch it on first check.

If you start on door 1 and monkey starts on door 3 you miss first check, if it moves to door 2 you will catch on second check as shown in base case. If monkey moves to door 4 on second move you will miss on checking door 2. But on 3rd move it must move to door 3, you will check door 3 and catch it.

If you start on 2 and monkey starts on 2 you will catch it on first check.

If you start on 2 and monkey starts on 4 you will miss first check, it must move to 3 and you will catch on second check.

  
By induction if monkey starts on same parity as you you'll catch it at worst on the n - 1 door. Fact 2 is true.

  
So starting on door two and working your way up, if you get to the n - 1 door (in our case 9) and there is no monkey, you know that the monkey couldn't have started on an even # door or else we would have caught it according to Fact 2. So this means the monkey started on an odd number door. After checking doors 2-9 (8 moves) the monkey must currently be on an odd numbered door according to Fact 1.  
We know the monkey must be on an odd door, so using symmetry we know if we start on an odd door (9 in this case) and go back down in same manner (9, 8, 7,...) and we know that we will catch the monkey at worst at door 2. So for 10 doors 2345678998765432 will at worst catch monkey on the last door. Just drop the last 2 off your solution.",2,0,0,False,False,False,1639241909.0
rdlhtp,ho1tlqy,t3_rdlhtp,"The solutions we found is:
23456789987654322",3,0,0,False,False,True,1639178234.0
rdlhtp,ho219av,t3_rdlhtp,I'm assuming which passage the monkey goes through is random?,1,0,0,False,False,False,1639181552.0
rdlhtp,ho5f20s,t3_rdlhtp,"The solution depends on the number of cages. I'm using 0 indexing.

The idea is as follows. Guess 0..1...2....n-1. If the monkey was initially on an even index, we will always win in this first iteration. This holds for the general case where - if the monkey is on our right, and we guess an even index while the monkey is on an even index, we will always find it by linearly scanning.

So our goal now is to somehow ensure this happens.

With an even number of cages, if we guess 0...1...2...n-1, and we don't find the monkey, then it must have started on an odd indexed cage. After an even number of guesses, the monkey will be on some random even indexed cage, so if we guessed cage 0 twice ( to force the monkey to go odd -> even ). After that second guess, we've now guessed an even cage whilst the monkey was on an even index, so we know we will win.

Similarly, with an odd number of cages, we can do a linear scan once, but we only guess cage 0 once as well, as after an odd number off guesses, our orginally-odd-monkey would be on an odd cage, so if we guess 0 after n-1, we will once again gaurentee that we guess an even index while the monkey is on an even index as well.

This problem fucked me up because I initially thought there was a way to solve it with (a natural) recurrance.

For example, I tried to solve the cases n=1...4 by proving you can guess index 1 twice to gain information, ie that the monkey must be on the right as it couldn't be on 0.",1,0,0,False,False,False,1639247847.0
rdlhtp,hobgdjy,t3_rdlhtp,"Solution: 2, 4, 6, 8, 10, 1, 3, 5, 7, 9

This is a variation of the rabbit hole problem. If you play with the problem, you'll realize that the possible location of [the monkey oscillates between even and odd numbered cages](https://i.imgur.com/pQcCT5t.png).

The problem can be solved in linear time by guessing over all even cages first. Then all odd cages next. (or vice versa)

Here is sample code you can play around with using the rabbit hole problem (with a 100 holes). You'll notice that you'll always find the rabbit no matter how many times you run it.

    from random import randint
    
    mn = 0
    mx = 99
    
    def jump(rabbit):
      if rabbit == mn:
        return mn + 1
      
      if rabbit == mx:
        return mx - 1
      
      if randint(0, 1) == 0:
        return rabbit + 1
    
      return rabbit - 1
    
    
    def guess(rabbit, hole):
      if rabbit == hole:
        return True, rabbit
      
      return False, jump(rabbit)
    
    
    def main():
      rabbit = randint(mn, mx)
      caught = False
    
      for hole in range(mn, mx + 1, 2):
        caught, rabbit = guess(rabbit, hole)
        if caught:
          break
      
      if not caught:
        for hole in range(mn + 1, mx + 1, 2):
          caught, rabbit = guess(rabbit, hole)
          if caught:
            break
      
      if caught:
        print(f""Found rabbit in hole: {hole}"")
      else:
        print(""Did not catch the rabbit. :-("")
      
    
    
    if __name__ == '__main__':
      main()",1,0,0,False,False,False,1639355781.0
rdlhtp,hor2qyy,t3_rdlhtp,"Simplest, easiest solution: 
Open all the cage doors, without closing them. The monkey only moves when you close the door. This method will ensure the monkey is found within ten cages. 

Other solution: 
Stick your head into the cage and look down the passage. Assuming the passages are straight (given the cages are adjacent), you will have found the monkey. This method only takes opening one cage, and is therefore the ideal method. Of course, this approach may result in you getting your face scratched at or your head stuck in a cage. However, the problem only specified that the monkey needed to be found, not that the finder needed to emerge from the ordeal unharmed, and therefore this solution is completely functional. Q.E.D.",1,0,0,False,False,False,1639637183.0
rdlhtp,ho29z1k,t3_rdlhtp,"If there are n cages, the solution is:

1234....(n-1) n n (n-1) ....4321

The reason this works:

The distance function between the cage we look at and the cage the monkey is in is non-increasing in this case. However, parity of the distance function does not change as we go through 123...(n-1)n

So, if the parity was even when we checked cage 1, then we catched the monkey.

If the parity was old, we will catch him in n (n-1) ... 3 2 1.",0,0,0,False,False,False,1639185426.0
rdlhtp,ho200pf,t1_ho1zl62,"Sounds like monkey could be at 5, you open and close 4, then monkey goes to 4 and you open 5... And you missed it",4,0,0,False,False,False,1639181002.0
rdlhtp,ho1zx8i,t1_ho1zl62,"Let's assume the monkey started at cage 2, after you closed door 1 he went to cage 1 and now he is behind you and can just go 12121212 and you'll never catch him.",3,0,0,False,False,True,1639180960.0
rdlhtp,ho20whb,t1_ho1zl62,"I'm confused about the problem description as well but I think in your example if the monkey was in 2 to start and you opened 1, when you close 1 it could move to 1 and then you'd be checking empty cages for the rest.",1,0,0,False,False,False,1639181393.0
rdlhtp,ho42gku,t1_ho1tlqy,"I think that the solution of 234...(n-1)(n-1)...432 is enough, so in your example with n=10 it would be 2345678998765432. 

You already explained in a comment that if the monkeys starts in an even numbered cage, we will catch him in the first ""half of the run"". I cannot proof this assumption but it seems to be true. I tested this with numbers up to n=6 and I could not find a counter example. I know this is not a proof, but I firmly believe this assumption is true.
Following this assumption, this means the second half of the run is only entered, If the monkey starts at an odd numbered cage. The two times we open cage at position (n-1) imply, that beginning from the second half of the run, every time we open an even numbered cage, the monkey is in an even numbered cage. Same goes for odd positions during the second ""half"", if we open an odd numbered cage, the monkey is in an odd numbered cage. So the monkey is not able to cross us.
Thus the opening of cage 2 two times in the end is unnecessary, opening cage 2 one time in the end is enough: if we open cage 3 during the second half, the monkey is in cage 1. Monkey then has to move to cage 2, and we open cage 2 afterwards.

So all in all, the 234...(n-1)(n-1)...432 approach gives us 2*(n-2) cages to be opened at maximum, which would be 16 for n=10. Please correct me, if I did a mistake anywhere!",2,0,0,False,False,False,1639224667.0
rdlhtp,ho2xknw,t1_ho1tlqy,[deleted],1,0,0,False,False,False,1639196467.0
rdlhtp,ho24jn7,t1_ho1tlqy,"So the reason this solution works is this:
If the monkey starts on an even numbered cage (6 for example), you'll catch him in the first half of the run, because Any time you're opening an even numbered cage he will be in an even numbered cage and same for odd numbers. He can't cross you and thus you'll catch him.

If the monkeys starts in an odd numbered cage, the doubled 9 in the middle will make it so that you will open an odd numbered cage at the same time as the monkey will be in one. After that, when you'll go down the cages each even numbered cage you open, the monkey will be in an even numbered cage (same for odd numbers) and you'll catch him.

You can try it with 4 or 6 cages just to get the concept but it works the same for 10.and above.

I'll try to make an animation of this or something later.
If there are more questions about the riddle ask in the comments.",-3,0,0,False,False,True,1639182997.0
rdlhtp,ho254gg,t1_ho219av,"Well yes but you should consider every move he can make, and find a solution that solves every move of him.
You can assume he'll know what door you are going to open next and try to find the best move for himself based on that knowledge.",3,0,0,False,False,True,1639183256.0
rdlhtp,hojqxpf,t1_hobgdjy,"This solution is easily proven wrong.

Assume monkey is in cage 6

G = your guess (the cage you open), M = the cage the monkey moves to

G2, M5 (you open cage 2, monkey moves to 5)

G4, M4  (the monkey moves to 4 after you close the cage

G 6, M5

G8, M6

G10, M7

G1, M6

G3, M7

G5, M6

G7, M7

G9, M6",1,0,0,False,False,False,1639509973.0
rdlhtp,ho21a23,t1_ho1zx8i,"Ok, I think I was presuming the monkey couldn't jump into a cage that you had just opened.

But I'm still not sure I understand why your solution (23456789987654322) *does* catch the monkey. If the monkey is at 1, you start at 2 and start moving up the cages. Meanwhile the monkey moves from cage to cage in the lower numbers. You get to 9, then start moving back down again, and then at some point you get to one-cage to the right of the monkey, open it, monkey isn't there, and in the next move the monkey moves into the cage (say, 4), and you continue moving down the cages whilst the monkey moves up and you still don't catch him..

Or am I still missing something?",3,0,0,False,False,False,1639181561.0
rdlhtp,ho4306r,t1_ho42gku,"Absolutely right! Thanks
I wonder if there is an even better solution that is based on other mechanism, but it seems pretty optimal.",1,0,0,False,False,True,1639225044.0
rdlhtp,ho3hsl3,t1_ho2xknw,I used 1-10 and not 0-9,2,0,0,False,False,True,1639208918.0
rdlhtp,ho2dle2,t1_ho24jn7,"Your post says: 

>the monkey can go to a previously opened cage

But your comment says: 

>he can't go to a cage you opened.

Isn't this a discrepancy?",12,0,0,False,False,False,1639187057.0
rdlhtp,ho4nzxl,t1_ho21a23,"The monkey can only move to the adjacent cage, so if it starts in 1: at the moment you open cage 9, it must be in a cage with even index. By choosing the 9 again, you know the monkey moved into an odd cage, so it cannot be adjacent to you. As you go back downwards, the monkey must also move away and will never be adjacent and cannot escape to the higher cages.",1,0,0,False,False,False,1639236389.0
rdlhtp,ho3i3w5,t1_ho2dle2,"Right, sorry about that.
Updated it, I hope now it's more understandable",1,0,0,False,False,True,1639209149.0
rdlhtp,ho4rv8s,t1_ho4nzxl,">so if it starts in 1: at the moment you open cage 9, it must be in a cage with even index.

Ok, so when you open cage 9 for the first time, lets say the monkey is in even-numbered-cage 6.

>By choosing the 9 again, you know the monkey moved into an odd cage

Ok again, so when you open cage 9 for the second time, lets say the monkey moves from 6 to an odd-numbered cage, as you say, so lets say 7

>As you go back downwards, the monkey must also move away and will never be adjacent and cannot escape to the higher cages.

Wait what? The next move from OPs example was to open cage 8. No monkey in cage 8, so close the door. Monkey moves from 7 to 8. OP then continues moving down to 7 and so on, missing the monkey.

Sorry for dragging this out, I'm just not sure I understand the problem..",1,0,0,False,False,False,1639238096.0
rdlhtp,ho53iyb,t1_ho4rv8s,">Ok, so when you open cage 9 for the first time, lets say the monkey is in even-numbered-cage 6.

>Ok again, so when you open cage 9 for the second time, lets say the monkey moves from 6 to an odd-numbered cage, as you say, so lets say 7

Sorry for the misunderstanding. I meant it moved into an odd cage after we opened 9 for the first time, so at the moment we are opening 9 again, it is already in an odd cage.

Step by step from this point:

* Monkey can be in 2, 4, 6, or 8
* We open 9, monkey moves to 1, 3, 5, 7, or 9
* We open 9 again, monkey is not there (so he is in 1, 3, 5, or 7), he can move to 2, 4, 6, or 8
* We open 8, monkey is not there (so he is in 2, 4, 6), he can move to 1, 3, 5 or 7
* ...
* We open 3, he is not there, so we know he is in 1 and has to move to 2, so we open 2 and find him",1,0,0,False,False,False,1639243048.0
rdr5st,ho318j8,t3_rdr5st,"Learn the basics of a coding language (I don’t know js but I’m sure it’s a fine language with many pros and cons). Figure out how to solve fuzz buzz type challenges. Once you got those down, learn data structures by reading the theory then implementing them in your chosen language. Don’t just use the built in implementation if it exists, actually build it. Then learn algorithms the same way. I would guess that this task would take high 100’s of hours to lower 1000’s of hours. 

Non technical suggestions:
Have fun with it, if you get side tracked in a project or learning that’s a good thing. It means your having fun. 

Learn to google thing well and stack overflow is your friend. 

Find a supporting community to get you through the sucky times while learning and debugging code because there will be those times

Good luck",1,0,0,False,False,False,1639198366.0
rdr5st,ho3d9r4,t3_rdr5st,Just do as many projects as you can. Comp sci/data anything is in such high demand if you have a couple personal projects you cna fully explain they’ll take you like no other. Get dat 100k son!,1,0,0,False,False,False,1639205574.0
rdr5st,ho3f6fa,t3_rdr5st,"I would focus on the programming language first.

Based on the gained know how I'd suggest you practice / play around with different data types, algorithms, etc.

You might want to use blogs, leetcode and / or ask concrete questions on some problems you encounter by the means of stackoverflow or reddit.

After some time and with some experience you might start to consolidate your knowledge by reading some theoretical CS sources.",1,0,0,False,False,False,1639206944.0
rdr5st,ho32rae,t1_ho318j8,Thank you. Do you recommend learning data structures and algorithms through a book or online courses?,1,0,0,False,False,True,1639199192.0
rdr5st,ho33t0y,t1_ho32rae,"I took a university course so I don’t know how much my experience will help. However I would recommend using the free resource on YouTube and a text (you can probably find one for free online). Follow the textbook as your lesson plan. Do your reading, then try and find a YouTube video if you don’t understand the topic. If you still don’t understand the topic write out a post as detailed as possible about what you do and don’t understand and ask here.

That is pretty similar to a university course with assigned readings, lectures and office hours if you need help. 

I just want to emphasize if you ask questions online, the more work you put into your question the more people will want to help you and the better help they’ll be able to give.",1,0,0,False,False,False,1639199776.0
rdo7ab,ho31ogs,t3_rdo7ab,"I think the mistake is assuming that it is unsigned when it is adding and 2’s complement when subtracting. That is possible, but the computer will rely on other things telling it that. You could look at it as always being 2’s complement and the computer uses other logic (either in software or hardware) to determine if the overflow will cause a bug.",1,0,0,False,False,False,1639198605.0
rdo7ab,hodle15,t3_rdo7ab,"what twos complement overflows then it wraps. some processors will throw an error or raise a flag, and on others you have to write code yourself to check for it. 

7 + 1 = 0111 + 0001 = 1000 = -8

\-8 -1 = 1000 + 1111 = ~~1~~0111 = 7

[https://en.wikipedia.org/wiki/Overflow\_flag](https://en.wikipedia.org/wiki/Overflow_flag)

on x86 JO is ""jump if overflow"" using the overflow flag

[https://stackoverflow.com/questions/48619934/mips-overflow-detection-printing-the-result](https://stackoverflow.com/questions/48619934/mips-overflow-detection-printing-the-result)

this is a page about detecting overflow in mips (which offers no hardware detection of overflow)",1,0,0,False,False,False,1639402772.0
rcla2n,hnwetsd,t3_rcla2n,"NFTs in theory are one thing, but in practice they are nothing more than digital beanie babies or trading cards, except you don't actually own the beanie baby or trading card and just have a receipt for it. It's a disaster and [the top 10% of traders for NFTs have traded 97% of all assets](https://www.nature.com/articles/s41598-021-00053-8#:~:text=the%20top%2010%25%20of%20traders%20alone%20perform%2085%25%20of%20all%20transactions%20and%20trade%20at%20least%20once%2097%25%20of%20all%20assets) and the ecosystem is rife with fraud and wash trading (trading with yourself to inflate the perceived value of an NFT).",192,0,1,False,False,False,1639083631.0
rcla2n,hnxbmbw,t3_rcla2n,"NFTs seems like mostly nonsense in their current applications (or more precisely, the applications I see most prominently featured in high profile discussions). It seems like an elaborate trick to make non-technical people believe that there is some form of scarcity for digital objects, which is ridiculous because they are inherently fungible. 

The idea of owning an NFT for an artwork is essentially meaningless. It's not tied to any kind of copyright law, and there is nothing stopping someone else from using  artwork for their own commercial purposes. It's barely a step above those websites that sell asteroids or stars to naive people. If asteroid mining becomes a thing, SpaceX isn't going to need your permission to mine all the precious metals on ""your"" asteroid because you have a certificate from a hack website. NFTs are just the same scam dressed in fancier tech jargon.",32,0,0,False,False,False,1639097568.0
rcla2n,hnwm220,t3_rcla2n,"I foresee issues with the availability and scalability of a decentralized data store. Can we trim old data off the ledger to maintain its size growth? Will these systems be truly decentralized, or will there be some authoritative servers around to ensure low latency of queries?",8,0,0,False,False,False,1639086571.0
rcla2n,hnw7mgc,t3_rcla2n,"My feelings on it seem to flip every week or so, but I always come back to the opinion of it just re-inventing the wheel. I think there are great use cases for crypto like the transfer of money on a global scale, instantly. Other than that, I think it’s just an over-engineered concept that is in a saturated market (FinTech). 

Some of the qualities that Crypto has just don’t make sense to me on how they help the average person do average financial tasks. 

An open ledger of every transaction? Ok, cool. I’ll probably look at it once then never again. 

Decentralized assets? Neat and some people’s cup of tea, but now I have all the responsibility of managing them. 

I don’t think Crypto will ever go away. I think history tells us not to bet against innovation. In the current state, I do hope the Crypto bubble implodes like the Dotcom bubble. Too many scams, too many get rich quick mindsets, and too many shitty opinions (like mine lol)",31,0,0,False,False,False,1639080734.0
rcla2n,hnvfxh7,t3_rcla2n,"I understand what crypto and a NFT is. I thought crypto was cool back in 2010 when I first found out about it.

Nowadays crypto (bitcoin) has so many problems imo. Electricity usage, money laundering, used for illegal activities, etc.

NFT just seems like a pyramid scheme / bubble. But it could go on for a long time. Im happy with owning stocks and not messing with crypto / NFTs personally. 😊",72,0,0,False,False,False,1639069810.0
rcla2n,hny3nfb,t3_rcla2n,"NFTs are just outside of their time; not early or late to the party, just irrelevant for our world, I feel. An NFT could, in theory, represent something like a certificate of authenticity for digital goods or even paired with real goods, but that isn’t how I see this trend going. I don’t think the adoption will allow for something dope like Nike starting an authentication service to issue NFTs of authenticity to help mitigate fakes in the show resale market. I don’t see something like a physical plot of land or house coming with decentralized documentation of ownership in the way deeds exist now. So many other things would need to happen in legislature that never will. 

I’d love to be wrong but I think the concept is meant for a society more capable of adaptation than the one we’ve found ourselves in.",5,0,0,False,False,False,1639110133.0
rcla2n,hnwhq1o,t3_rcla2n,"As far as my understanding goes, NFTs are just mappings of some asset, be it digital or physical, to a digital signature.

It is the interpretation of this as some form of ownership that people focus on. If the majority does believe that having a record that maps something to you represents this, it effectively becomes the truth.

Same thing for crypto currencies, where value is placed on an association with your wallet and the sum of all your transactions.

Seems arbitrary, but the same thing goes for money - if nobody would believe it has value, it would not have it.

My opinion is still sceptical. Crypto currencies praise themselves as decentralized - which may be true as long as you stay in an ecosystem where all goods and services may be paid for with such a currency - but when you want to exchange them for ""real"" money you need to go to a centralized exchange, where you can be identified and privacy flies out of the window as well.

So yes, as long as there is a more relevant ecosystem outside of crypto, it will not be achieve what it promises to be - while polluting the environment and driving GPU prices up.

As for NFTs, your suggestion about money laundering did not occur to me before that but I would have my doubts (see the above statement about privacy).  


Both currently look mainly like speculative assets, less than stable or serious alternatives or additions to what we have currently - while NFTs may just be a tiny bit weirder than the actual art market \^\^",13,0,0,False,False,False,1639084806.0
rcla2n,hnwq89e,t3_rcla2n,"I think it's a sad joke. The only way it is sustainable (and I use that word very loosely) is if you perpetually manufacture artificial scarcity.
It remains viable only by sustaining growth, ergo growth by artificial scarcity. Growth for the sake of growth is - well - the very definition of cancer. NTF and crypto are cancerous, and operate on the same model as Capitalism, quite frankly. We do not live in an infinite-growth paradigm and any model based on that will end disastrously.",13,0,0,False,False,False,1639088267.0
rcla2n,hnwoxpz,t3_rcla2n,"Still waiting for crypto to offer one, just one of all these hypothetical use cases that would be of any value to me, because I have been reading about how it is set to revolutionize X, Y, and Z for years now. Does anyone use DLT for anything at all in their daily lives, other than speculation? It doesn't seem like asking much given the insane amount of brain power and capital that has poured into crypto over the past decade. Decentralized money is still cool, admittedly.",9,0,0,False,False,False,1639087733.0
rcla2n,hnwhghf,t3_rcla2n,"Nfts will be used by ticketmaster for tickets. This alone if it works will be huge as all baseball/football games tickets could move that way as well as theatre cinema tickets, it creates the possibility for secondary markets, you can then have follow up “souvenirs” in the secondary market and more consumer analysis. Then if you combine it with recoverable wallets it could be a game changer for all sorts of things where people register ownership to literally anything.

Cheap, reliable, fraud proof, digitised. The future

Nfts that are “art” I don’t get but I can understand the historical significance of this tech right now having value.",7,0,0,False,False,False,1639084700.0
rcla2n,hnxyjqx,t3_rcla2n,"It's another fucking stupid application of a niche technology - and edgelords everywhere are being suckered into it at lightspeed.  


Riddle me this: Who enforces your so-called ""Digital property rights""?   
What is the advantage of putting it on a blockchain? 

The answers are: Nobody, and nothing. It was another idea created simply so someone could, essentially, sell thin air. Only a fucking moron would buy into it.

...But I am at least honest with myself: I know that folk will put value into anything. So with that in mind ***inb4*** *""x number of people would disagree with you!!!11one""*",4,0,0,False,False,False,1639107763.0
rcla2n,hnyye3o,t3_rcla2n,Mostly a way to launder money.,2,0,0,False,False,False,1639130702.0
rcla2n,hnzg18s,t3_rcla2n,you asked a very good question op,2,0,0,False,False,False,1639142915.0
rcla2n,hnyew4b,t3_rcla2n,"There are dozens of cool applications for NFTs. Digital art is what made them popular but what's next is cooler

* diplomas
* certficate of authenticity
* land title
* concert tickets",3,0,0,False,False,False,1639116189.0
rcla2n,hny4o66,t3_rcla2n,"I think it’ll make waves in the gaming world. With the death of brick and motor, the inability to recycle titles like you normally would is going away. These could be exchangeable in an NFT marketplace and would make an attractive business model to consumers, and the publisher could get paid for every resale transaction. I can see Amazon doing the same thing with Ebooks maybe.",2,0,0,False,False,False,1639110625.0
rcla2n,hnyzb23,t3_rcla2n,crypto is overhyped,2,0,0,False,False,False,1639131470.0
rcla2n,hnziyv9,t3_rcla2n,The money laundering one. Also scams.,2,0,0,False,False,False,1639144401.0
rcla2n,hnydezt,t3_rcla2n,Limited technical application from what I understand.  Also it's a picture of a monkey with your name on it it's not worth $50k,1,0,0,False,False,False,1639115310.0
rcla2n,hnz5t3f,t3_rcla2n,"A lot of good points in this thread, but I'll add my two cents: Because NFTs must be verified by a trusted party, it's unnecessary and wasteful to host them on a platform whose appeal is being decentralized & trustless.",1,0,0,False,False,False,1639136608.0
rcla2n,hnzcolc,t3_rcla2n,"The basic idea is quite good

At the moment they’re being used for digital beanie babies, and it’s hard to see true value there

But looking at real world potential: I’ve just spent £1000 on solicitors fees to check the title deeds and other information about the house I’m buying, and most of that cost is retrieving the information they need. A tokenized system could be ideal for something like that. Whether that takes the same exact form as an NFT, I don’t know, but the idea was floated long before NFTs existed

Similarly car ownership, maintenance and inspection records etc would seem like great candidates for a token system - allowing authorized dealers and garages to log information on a decentralized blockchain, which could be much more reliable and cheaper to run than a centralised system.

I wouldn’t say they’re definitely suitable for such tasks, but certainly there’s real world potential there

Perhaps more importantly, digital ownership of songs, movies, books etc would be a perfect candidate. Streaming is great but not ideal because you can’t buy something to keep. But buying digital assets relies on the store/service you use staying in business

NFTs could solve this by allowing you to own a song more generically, so record labels or publishers can reassign the rights to sell those items to another distributor/store/service, on the proviso that they are sold as NFTs and the new provider will supply a download to the owner of the token. That way, you can buy a movie knowing that it won’t just vanish if Google decides to pull the Play Store one day, or buy a game knowing it doesn’t matter too much if Steam/Valve go bust, or a book without relying entirely on Amazon staying in business.",1,0,0,False,False,False,1639141048.0
rcla2n,hnzdjzr,t3_rcla2n,ITT: people who don’t understand that an NFT is a digital certificate of authenticity wrapped in a rich metadata audit trail authenticated by cryptography and hosted on the closest thing to a permaweb humanity has achieved.,1,0,0,False,False,False,1639141555.0
rcla2n,hnzygi6,t3_rcla2n,"Obvious scam is obvious. You are buying nothing with the expectation that the nothing you bought will be worth something in the future because other people also believe so. It is just a big bubble and when you look at the price graphs, the vast majority of them go up like a rocket and then come crashing down faster than light when the bubble bursts. Making NFT:s is fine I guess, but the market is oversaturated for obvious reasons so good luck making any money. At that point it is basically just a waste of time.",1,0,0,False,False,False,1639151310.0
rcla2n,hnyf7bf,t3_rcla2n,"Duplicates of digital media can be exact copies. But money is paper that we all agree has value, it’s not much different. The idea that it can be akin to art is a bit off to me. No one will copy exactly the any of the most famous pieces of art, copy’s will always be copies. But a copy of a digital image is an exact copy. Same ones and zeros.",0,0,0,False,False,False,1639116374.0
rcla2n,hnybsdl,t3_rcla2n,"I think there are some interesting potential uses cases in supply chain management and probably others as well, I don’t understand the current trend of digital artwork though.",1,0,0,False,False,False,1639114361.0
rcla2n,ho3dc7h,t3_rcla2n,Just wait for digital assets outside of nfts my friend 🙏🏼,1,0,0,False,False,False,1639205622.0
rcla2n,hok35cs,t3_rcla2n,"An NFT is like writing the name of the owner on a piece of art using invisible ink.  It does not give you exclusive use of the art in any way other than allowing you to sell to someone else the right to erase your invisible ink name and write their own.

What is the value of that?  That is completely dependent on what people value.",1,0,0,False,False,False,1639514762.0
rcla2n,ht2jl6h,t3_rcla2n,"In a nutshell, NFTs are bad for two reasons:

1. They are bad for the environment, as they rely on cryptocurrencies that cause huge amounts of carbon emissions. They will continue to rely on these systems for security reasons (despite claims to the contrary about moving to other systems).
2. They are only valuable as tools for money laundering, tax evasion, and greater fool investment fraud.

There is actually zero value to NFTs. Their sole purpose is to create artificial scarcity of an artwork to supposedly increase its value (it doesn't do this, but the pretense that it does can be used for illegal purposes by those who recognize that fact).",1,0,0,False,False,False,1642445021.0
rcla2n,hny4gum,t1_hnwetsd,"You *do* own the NFT, it's just that the NFT *is not the fucking art.* It's too expensive to hold an image on the chain, so all of those NFT images are stored on bog standard servers that *will* go down. Probably immediately after NFT's stop being a fad, for the big players, or when the creator dies, in the case of something like etherrock, in which case it'll be when he dies (or gets bored) and someone buys the domain and replaces the images with something like goatse.",26,0,0,False,False,False,1639110526.0
rcla2n,hnyoxfb,t1_hnwetsd,"The actual technology behind them tho is elegant. I believe OP came here looking for scientific reasoning, not scorn opinions. A decentralised global database where multiple applications can access and have verifiable proof that the data is what it says it is.",7,0,0,False,False,False,1639122866.0
rcla2n,huqat9q,t1_hnxbmbw,"> those websites that sell asteroids or stars

This is an excellent analogy, I'm using that, thanks.",2,0,0,False,False,False,1643471187.0
rcla2n,hnx2ycy,t1_hnw7mgc,Most FinTech companies are just wrappers around the existing system that hide things like delayed settlements and give you nice APIs. The fact that so many 100+B payment companies exist shows how much money is being made by being a middleman. Things like micropayments will never be possible with the existing system. With crypto this is possible. For example with brave you get payed in BAT for the ads you see.,11,0,0,False,False,False,1639093684.0
rcla2n,hnwrbi9,t1_hnvfxh7,"[1% of crypto is used for illegal purposes](https://www.forbes.com/sites/haileylennon/2021/01/19/the-false-narrative-of-bitcoins-role-in-illicit-activity/).  Regular cash is close to 5%. I don't get why people wouldn't be interested in this technology.  

The idea of a tamper proof [block chain](https://www.youtube.com/watch?v=bBC-nXj3Ng4) is also very interesting to me from a technical aspect. I've seen governments and organizations even testing out the block chain to store documents as a test to see if it could increase transparency. 

Ethereum is a [virtual machine](https://www.youtube.com/watch?v=gjwr-7PgpN8) that's decentralization.  It might not be practical or have a ton of use cases but the concept is wild isn't it?



[DAO](https://www.youtube.com/watch?v=KHm0uUPqmVE) and [smart contracts](https://www.youtube.com/watch?v=pA6CGuXEKtQ) are again yet another interesting development.

Am I wrong in thinking this stuff is interesting.  Are they dead ends?",-7,1,0,False,True,False,1639088719.0
rcla2n,hnvwuh2,t1_hnvfxh7,"Crypto is used way less than cash for illegal activities. The electric usage will be hugely mitigated when the blockchain will switch to a new transaction verification system call proof of stake. Ethereum (the 2nd biggest) will switch soon next year, countless others already did. 
I think it's really superficial to just mention the issues that are already getting overcame, without mentioning its possible use cases and the blockchain potential.",-13,1,0,False,True,False,1639076421.0
rcla2n,hny708b,t1_hnwhq1o,"> It is the interpretation of this as some form of ownership that people focus on. If the majority does believe that having a record that maps something to you represents this, it effectively becomes the truth.

Wrong. Ownership requires control, but NFT's provide no control over the image, only the receipt on the blockchain.

> Same thing for crypto currencies, where value is placed on an association with your wallet and the sum of all your transactions.

Wrong, not the same. You can control what happens to the crypto in your wallet.

> Seems arbitrary, but the same thing goes for money - if nobody would believe it has value, it would not have it.

Mostly wrong, cash's value is maintained by the fact it's the only way you can pay taxes and therefore participate in the US's (or whichever countries) economy. So it's only as valuable as the ability to participate in the economy, which can be torpedoed if that suddenly becomes less valuable through various mechanisms, some of them with a psychological component. 

Crypto's largest selling point is the trustless ability to conduct transactions. Not useful in the modern, high trust world but extremely useful in trustless exchanges such as for crime, drugs, or scams, which it was immediately used for and then for basically nothing else.",0,1,0,False,False,False,1639111760.0
rcla2n,hnx547y,t1_hnwoxpz,"There are many interesting use cases for crypto/Blockchain tech, the issue really comes with the scalability, or lack of it, which renders most of those use cases totally invalid.",8,0,0,False,False,False,1639094629.0
rcla2n,hnyra02,t1_hnwoxpz,blockchain has many use cases and I personally know if it being used in chemists to track supply - does the agency I know of that is using this utilise or have a need for a token? no,2,0,0,False,False,False,1639124705.0
rcla2n,hnyazh1,t1_hnwoxpz,"Monero is pretty cool if your use case is money laundering, otherwise it's all useless",0,0,0,False,False,False,1639113896.0
rcla2n,hnwk1n0,t1_hnwhghf,"Thing is, having a secondary market for tickets doesn’t require NFTs. It just needs the venues to allow resale or reassignment via their own platform.

The reason they try not to is because secondary markets already exist with vastly inflated prices.",7,0,0,False,False,False,1639085747.0
rcla2n,hnytnjd,t1_hnwhghf,">  it creates the possibility for secondary markets

Not a sports fan, but this sounds horrible to me. Won't that just open it up for scalpers to exploit another market?",1,0,0,False,False,False,1639126683.0
rcla2n,hocpqa4,t1_hnyew4b,Home/Car title is the only really good use for NFTs IMO,1,0,0,False,False,False,1639379144.0
rcla2n,hnymj05,t1_hny4o66,This is the only use case of NFT’s I’ve heard of so far that makes sense to me.,2,0,0,False,False,False,1639121085.0
rcla2n,hnytgr6,t1_hny4o66,"Resale of digital goods is the only use case I've thought about that might work, but it has a couple of problems that probably means it won't ever happen. Using games as an example, currently keys on every platform are single use. Meaning even if you tied ownership of keys to people on the blockchain, you couldn't resell it. Platforms like Steam would need to support it and immediately its no longer decentralized. Every transaction would need to contact the platform the key is linked to, to transfer ownership. At that point the blockchain is irrelevant, you could do the same thing without it because you require the platform to support it.

No matter how I think about it, for resale of digital goods the platform the goods are used on would need to support it.",2,0,0,False,False,False,1639126525.0
rcla2n,hnyqum8,t1_hny4gum,"Too expensive on transport protocols that require gas fees that are exuberant. We can store entire blu-ray movies in a decentralised manner with little to no cost. Non-fungible tokens aren’t limited to the transport layer they reside upon, they are method to verify ownership - just as pgp was used to verify authors. 

Cryptocurrency on the other hand is generally the protocol and the ledger it resides upon and none have are even close to getting that right just yet. Mining, staking, etc are no the answer.",3,0,0,False,False,False,1639124365.0
rcla2n,hny39gr,t1_hnx2ycy,"Can you elaborate more on micropayments not being possible with the current system? I sometimes use Brave and loved the idea of their BAT system. 

Other than that, what other micropayments are out there? How is it not possible with the current system? 

I have used mobile apps before where you can rack up points while using the app’s particular function (granted I can’t remember the name). Credit card rewards are basically the same thing as Brave’s Bat system. I can transfer those rewards into cash. I don’t see how crypto makes any new leaps in that department",3,0,0,False,False,False,1639109948.0
rcla2n,hnxbd3x,t1_hnwrbi9,"Imo you've done a great job of encapsulating how I feel about cypto. Which is, a bunch a vague ""cool ideas"" and ""interesting concepts"" that are ultimately _currently_ useless, for most people. I'm sure the very small (in global terms) number people who have become extraordinarily wealthy overnight vehemently disagree with me, but that does not a practical use case make.",13,0,0,False,False,False,1639097452.0
rcla2n,hnx7bbe,t1_hnwrbi9,"Doesn't matter if it's interesting, it's using the energy of a small country while making less transactions than your average bank, this is a huge issue and anyone who pretends it not is deluded.",10,0,0,False,False,False,1639095621.0
rcla2n,hnvy1lb,t1_hnvwuh2,"I hope crypto does well, im just not willing to bet money on it going up from here.

I live in Sweden where almost no one uses cash anymore. Everything is card or swish (free instant digital money transfer). Sure it would be nice if something like Swish existed globally, which was the hopes of bitcoin several years ago. There are still problems that need to be solved though like money laundering, transfer fees, exchanges getting hacked and you losing all your coins 😊",12,0,0,False,False,False,1639076910.0
rcla2n,hnx8f5q,t1_hnvwuh2,"I genuinely do not understand the downvotes.
Addressing Bitcoin problems when talking about the blockchain technology is such a poor argument.",2,0,0,False,False,False,1639096114.0
rcla2n,hnymng5,t1_hny708b,">Wrong. Ownership requires control, but NFT's provide no control over the image, only the receipt on the blockchain.

True that. You can not effectively control your asset through that association - people would *really* have to believe in NFTs to restrict ther usage to only you!

&#x200B;

>Wrong, not the same. You can control what happens to the crypto in your wallet.

No question about that - I was trying to point out that the perception of value is similar.  
But you could also argue that you have ""control"" over which NFTs you ""own"", since you can decide to purchase them. Their value (like crypto currencies) is then just a result of what others believe it to be.

Sure, ""leaving"" conventional money would be a whole lot more complex than something like Blockchain, but that tail of taxes, economy is merely the result of people seeing it as something valuable for a very long time.

I fear that my comment seems a little too pro-Blockchain, so let me just note, that this is absolutely not the case! Mainly I wanted to point out that people have alway put value in objectively useless assets and that these hype-things are just another incarnation of it, solving mostly issues that were tailor-made for them.",1,0,0,False,False,False,1639121173.0
rcla2n,ho6lwvz,t1_hny708b,">	cash's value is maintained by the fact it's the only way you can pay taxes and therefore participate in the US's (or whichever countries) economy.

I don’t think cash’s value is so necessarily tied to taxes. If the US government decided tomorrow that it would continue to back and support the dollar through the Fed, but not charge taxes, I don’t think people would consider the dollar to have lost value as currency.

Rather, cash’s value is based on a shared belief in that value as it relates to goods and services. It’s that shared belief that means that we can agree that a dollar is worth a glass of lemonade, or fifteen dollars is worth an hour’s labour at a convenience store. The goods or services that a dollar is worth can change over time. There is no equivalent for crypto - its value is determined by how many dollars (or other fiat) it’s worth, not how much it can be traded for in goods or services. So long as crypto measures value against fiat, it’s not a currency, it’s a good (in this case, a speculative investment).",1,0,0,False,False,False,1639266846.0
rcla2n,hnyb2aj,t1_hnyazh1,"Not as useless as ur mother
***
^I ^am ^a ^bot. ^Downvote ^to ^remove. ^[PM](https://www.reddit.com/message/compose/?to=YoMommaJokeBot) ^me ^if ^there's ^anything ^for ^me ^to ^know!",5,0,0,False,False,False,1639113941.0
rcla2n,hnwp63b,t1_hnwk1n0,"Having a digital market place and being able to verify on chain makes the process a lot more trustworthy and fraud proof vs using physical tickets. If it wasn’t any better, companies like ticket master wouldn’t waste their time.. I trust they can see more potential than I can.",3,1,0,False,False,False,1639087829.0
rcla2n,hnyu6c2,t1_hnytnjd,Could enforce a price policy that makes profiteering impossible with on chain transactions,1,0,0,False,False,False,1639127128.0
rcla2n,hocrg82,t1_hocpqa4,[deleted],1,0,0,False,False,False,1639380448.0
rcla2n,hoh5yrt,t1_hnytgr6,What about a platform for fractionalized nfts (erc721 + erc20) for companies that want to reissue their securities on the blockchain?,1,0,0,False,False,False,1639459022.0
rcla2n,hny543b,t1_hny39gr,"Modern banking is built on ACH, which is where the stuff he's talking about comes from. It's got a 3 day clearing time, when micropayments need to be faster. Of course, building a replacement for that on a slow, expensive, polluting chain is stupid and can't actually deliver. Fortunately, the fed is creating an ACH competitor called [FedNow](https://www.frbservices.org/financial-services/fednow/about.html) which should hopefully drive these middlemen out of existence, or at least lower their take.",3,0,0,False,False,False,1639110836.0
rcla2n,hnyhocf,t1_hny39gr,"As others have mentioned there are now system that allow instant transfers but I don’t think something like BAT would be possible with these systems. They are also only available regionally and in their local currencies. 

In addition to micropayments tokens make new incentive structures possible. For example early facebook or twitter users didn’t get any upside for participating but where some of the most important users. With tokens you could pay early users. This leads to super charged growth since their are more incentives to use a new product. The equivalent in the fiat world would be to give early users stock but this is regulatory nightmare and can definitely not be done in a micropayment way.",2,0,0,False,False,False,1639117880.0
rcla2n,hny76ep,t1_hnxbd3x,Haha exactly. I suppose  I don't know enough to see it as useless right now. I'm old enough to remember the same comments about Netscape and such.,1,0,0,False,False,False,1639111847.0
rcla2n,hny8qzz,t1_hnx7bbe,"Nothing is free in life.  The energy is used to secure the global network and valid every transaction. Think about how much bitcoin is worth. That energy is doing something and it's still less than the phantom energy that's drawn by people leaving their electronics plugged in all the time.  At the same time, proof of stake consensus will use a fraction of what proof of work uses.  Saying all the, adding more transactions doesn't burn more energy.  The energy is consumed by miners competing.",-8,0,0,False,True,False,1639112679.0
rcla2n,hnw36gc,t1_hnvy1lb,"I also think a thing that got in the way of crypto is that it became an investment rather than money, specially in the mainstream. It feels like most people who get into crypto are in it to make money, rather than the actual reason crypto was first invented (and the reason you probably thought it was cool in 2010 but dislike it now).

Also, the fanbase. Like so many other things the crypto/nft fanbase is cringy to the point of being annoying and I wouldn’t want to deal with them, and that includes financial transactions and such.",20,0,0,False,False,False,1639078972.0
rcla2n,hnw89kt,t1_hnvy1lb,"It's been in a constant up trend for the past ~5 years, why should it stop now, when more and more people and getting involved? 
Take a look at the big picture and check the price of bitcoin or other coins just 1 year ago and you'll see the huge increase. 
Sure, some problems still need to be solved, but most of them are already solved. Exchanges got really secure and I haven't heard about an attack to a major exchange in the last couple of years. Defi platforms are surely more vulnerable as they are really new, so they are expected to be less secure.",-11,0,0,False,True,False,1639080988.0
rcla2n,ho6whd1,t1_ho6lwvz,"Cashes *definite* value is a shared fiction, but the reason it *has* that value is because all goods and services are *forced* to be related to it or else the government will punish you. Without that universal value assignment, cash becomes just strips of paper.

> If the US government decided tomorrow that it would continue to back and support the dollar through the Fed, but not charge taxes, I don’t think people would consider the dollar to have lost value as currency.

First of all, that support isn't cheap. Financial crimes, counterfeiting, trade management, political ties, and national self defense *all* feed into the value of the dollar, to say nothing of the vast amount of infrastructure, laws, education, enforcement, etc that keeps the 300 million person society ticking. Those 300 million people form a market whose lingua franca is the dollar, and *everything* they do is based on that dollars value. Without taxes, they can do whatever they want in any medium of exchange they want. ""Taxes give the dollar it's value"" isn't just a simple statement, it captures the fact that without taxes *there is no dollar.*",1,0,0,False,False,False,1639271881.0
rcla2n,hoctapd,t1_hocrg82,It would make fraudlent selling a lot harder because everyone can see who owns the NFT title on the blockchain and no one would buy without the seller showing the NFT in their wallet.,1,0,0,False,False,False,1639381879.0
rcla2n,hnyassb,t1_hny543b,Here in EU we already have instant bank transfers within countries and soon also instant bank transfers across the EU. So instant crypto transfers hasn't really covered many use cases here.,5,0,0,False,False,False,1639113794.0
rcla2n,hnyu5ve,t1_hny8qzz,The phantom energy is contributed by millions if not billions of people. Crypto miners/users/traders make up a tiny fraction of this,7,0,0,False,False,False,1639127116.0
rcla2n,hnylilk,t1_hny8qzz,"[citation needed] on that energy comparison to phantom energy. I think that’s complete rubbish, commonly spread by Bitcoin advocates to distract from its fatal energy problem.",1,0,0,False,False,False,1639120395.0
rcla2n,hnw7n47,t1_hnw36gc,"Yes, that's definitely true, but how else is it supposed to grow as a project and start being actually used? 
Why should someone buy crypto to use it? 
It's just like stocks - most of the times people buy it as an investment to earn money, not really caring about what's behind. The same thing goes for crypto, and that's to be expected - how else is it supposed to become widely used, if not through investing at first? 
It is slowly being adopted widely and having a real use case, but of course it'll take time. 

Regarding the fanbase, sure, can't disagree with you, however that's a big generalization. There is a good chunk of really smart people who are trying to actively improving things in this sector with new projects and interesting discussion about blockchain technology, don't get biased by what you see on the surface, as in most fields the worst part of the fanbase is what makes the most noise.",-7,0,0,False,True,False,1639080741.0
rcla2n,hnw9yb1,t1_hnw89kt,"Because it’s all speculative investing (with no real value), not actually used as a currency",13,0,0,False,False,False,1639081660.0
rcla2n,hnxfcm8,t1_hnw89kt,"> It's been in a constant up trend for the past ~5 years, why should it stop now, when more and more people and getting involved?

Said by everyone who was so sure **their** investment bubble of choice would **never** burst like all those *other* bubbles did.",4,0,0,False,False,False,1639099265.0
rcla2n,hoctq43,t1_hoctapd,[deleted],1,0,0,False,False,False,1639382222.0
rcla2n,hnzagve,t1_hnyu5ve,"Yet that energy in crypto is securing and validating billions worth of wealth. It is doing work.  Like the phantom energy problem, it's disturbed and therefore large.  But unlike the phantom problem, people are actively working to solve and innovate ways to reduce that problem.  But at the end of the day it's still doing work.",-1,0,0,False,False,False,1639139720.0
rcla2n,hnzgntb,t1_hnylilk,">Bitcoin consumes a sizable amount of electricity. As of June 2021, estimates suggest something around 110 terawatt hours (TWh) per year, which, for scale, is close to the electricity consumption of the Netherlands (111 TWh) but a bit less than the global ‘phantom’ electricity consumption from electronics that are left plugged in while in standby mode (124 TWh). 


https://www.coincenter.org/education/crypto-regulation-faq/understand-bitcoins-energy-use/

Crypto Currency has an energy problem. I'm not going to argue its fine. But people are trying to solve it. The energy used also does work. It secures and validates all transactions worth billions of dollars. One of my first classes in school the prof detailing how writing buggy code that gets distributed globally ends up wasting energy. Energy consumption isn't just a bitcoin problem. But the energy it consumes doesn't mean what it does is useless or not worth learning about. The consensus mechanism are super interesting to me. I'm reading proofs and white papers and digging into how cryptography works because the whole concept of this network is fascinating.",2,0,0,False,False,False,1639143241.0
rcla2n,ho2yyp6,t1_hnw7n47,"IMHO, the first part of your comment doesn’t really make sense. Cryptocurrencies are meant to be a currency. This is entirely different from stocks, which are, by definition, an investment. There are valid reasons to use crypto as a currency, too, namely the very reasons it was created for: decentralization, anonymity, universality, etc. In fact, it was used for those ends before it was an investment. However, these are things that the average person doesn’t care about, and “making an investment out of them to begin with” doesn’t change that. Stocks have been around forever now, and they’re still not used as currency (you don’t buy things in the market with stock, or pay for services, etc.) because, again, they’re fundamentally an investment, and not a currency, as opposed to cryptocurrencies. 

I’d go so far as saying that crypto as an investment is inherently a bubble based on hype and misinformation. Stocks are associated with the worth of a company, so, in theory, it makes some sense that the price changes with time, as the company’s value also varies according to how well it performs. Crypto, on the other hand, is entirely based on: will people adopt this currency? However, as long as people face it as an investment, they won’t be using it as currency (because it lacks the stability that a currency needs, after all, if you can’t be sure of its worth tomorrow, you probably won’t want to stake your livelihood on it, which is why people won’t be inclined to take it directly in exchange for goods and services on the mainstream, which is even more reason for it’s instability since it exists only tethered to some other currency, i.e.: it doesn’t have “independent” value, it’s worth only however many dollars people are willing to pay for it at the moment)

So, I’d argue that using crypto as an investment only pushes it farther and farther from it’s intended use, and the only use that I, personally, see value in, which is as a currency.",0,0,0,False,False,False,1639197170.0
rcla2n,hnwl2i6,t1_hnw9yb1,"Just because it's not used as a currency it doesn't mean it has no real value. Also, the current banking system has been used for more than 200 years, do you think it can change in just a few years?",1,0,0,False,False,False,1639086170.0
rcla2n,hocunrt,t1_hoctq43,"It isn't ""needed"" per say, it would just increase consumer confidence that they are buying from the owner of the property.",1,0,0,False,False,False,1639382978.0
rcla2n,hnwlkme,t1_hnwl2i6,"Blockchain tech: real value. 

A currency increasing in value by 100% in a day: speculative investing (and a bubble that will crash). 

If it one day doesn’t do this, maybe I’ll reconsider that take.",5,0,0,False,False,False,1639086374.0
rcla2n,hnwpj0c,t1_hnwlkme,"It's been ~10 years, and there has been no major crash yet, or no crash that hasn't been recovered, I think that can be a solid proof. 

Obviously, it's still a new sector, so huge changes are expected. But as the total market cap grows, it will become more and more stable. Right now, the total market cap is less than just Apple's, so it has a potential of growing",1,0,0,False,False,False,1639087976.0
rcla2n,hnx9xfo,t1_hnwpj0c,"Yeah but with this argument, you still aren’t getting it. Bitcoin is meant to be a currency. Bitcoin is not an investment. Nobody talks about currency’s market cap or growth potential. A currency will not be used as a currency if there is a huge potential to grow, because it’s not advantageous to actually “spend” that currency.",8,0,0,False,False,False,1639096790.0
rcla2n,hnylsm7,t1_hnwpj0c,Madhof’s Ponzi scheme lasted 30 years.,3,0,0,False,False,False,1639120576.0
rcla2n,hnx3qpw,t1_hnwpj0c,Apple makes something. Bitcoin does not. A currency should not increase in value like a stock.,4,0,0,False,False,False,1639094030.0
rcla2n,hnycyqx,t1_hnx9xfo,"Bitcoin is not meant to be used as an actual currency. It's meant to be a store of value, like gold. There are other, way more valid candidates to be used as actual currency, with really fast transaction speeds and low (or zero) fees such as NANO. Also, stable coins are also a thing!",-1,0,0,False,False,False,1639115044.0
rcip1t,hnuzs1u,t3_rcip1t,"I would give them alot of projects , I had numerical analysis course in uni and I learned tons by doing projects in Matlab and python , like Taylor's series and mclaurin series with their applications on finding the approximated values of infinite fractions , I still remember those algorithms because I actually used these bad looking infinite series in something useful( in my case was just getting roots and approximating numbers but you get the point(",21,0,0,False,False,False,1639063448.0
rcip1t,hnvhhra,t3_rcip1t,"To some extent this question strikes me as ""how should history be taught to CS students""?

Answer: Exactly the same way it's taught to everyone else.  You shouldn't silo knowledge, and end up having a highly ""inbred"" understanding of the world.  You could end up thinking your corner of academia is the center of the universe.  One should be able to take knowledge from a different domain and integrate it into your project without having the professor both cut your food and chew it for you too.

Now I do think math classes could be helped by showing more applications in CS.  We're still living through the era when physics was seen as the great mathematization of a science, and so a lot of our examples are in that field.  Today CS is at least as successful a scientific mathematization, with just as many examples and applications, so in that way we could modernize by using more examples in CS.",114,0,0,False,False,False,1639070418.0
rcip1t,hnv98w9,t3_rcip1t,Calculus should be taught through application not pure mathematics. It’s more important people learn when to use/apply calculus then just to learn the calculus. Because most people learn and forget or even worse learn just how to solve without understanding of what a derivative or other calc item is doing. I imagine most people won’t be able to find a tangent line using derivative .,23,0,0,False,False,False,1639067245.0
rcip1t,hnutvlk,t3_rcip1t,"Implementing integrators and approximating derivatives with sampling, applied to a few physics problems (springs?). Partial derivatives with image manipulation.

Then move to symbolic manipulation to make sure the common language is understood.",13,0,0,False,False,False,1639060889.0
rcip1t,hnv13r9,t3_rcip1t,"Learning monstrous functions has to be part of it. Part of what a CS education should prepare you for is engaging with CS-related research (papers and literature) which is full of these functions.

But I would also consistently emphasize the underlying concepts (e.g. rates of change, etc). And I would use applied examples both from Physics and CS to give both a physical and practical understanding.",6,0,0,False,False,False,1639064010.0
rcip1t,hnuzmcl,t3_rcip1t,Depends on who you are going to get as a result: “artist” or “artisan”,2,0,0,False,False,False,1639063382.0
rcip1t,hnv3occ,t3_rcip1t,I had a class in college (got engineering degree) called Numerical Methods. Junior or Senior level class that was basically how to do calculus with discrete methods on computers.,2,0,0,False,False,False,1639065057.0
rcip1t,hnwjkxw,t3_rcip1t,"If its actually calculus, then its good. But if its real analysis... RIP.",2,0,0,False,False,False,1639085560.0
rcip1t,hnvjt8n,t3_rcip1t,Just let them read this book: \`Calculus the Easy Way\`.,1,0,0,False,False,False,1639071308.0
rcip1t,hnwkbpm,t3_rcip1t,"There are a lot of great applications for calculus in AI and Machine Learning in general that might help drive calc home. I found in university that calc 1 and calc 2 tried to cover too many topics too fast, and lacked context/application, making the work feel very abstract. In a perfect world I think calc 1 could even be two classes, as given the spread of topics there is not enough time to have students actually connect with the material. The semester I graduated calc 1, around 50% of the class failed.",1,0,0,False,False,False,1639085862.0
rcip1t,hnwrqlu,t3_rcip1t,"The reason you typically have calculus classes in your freshmen year is that they also teach the basics of proofs, set theory and so on in the first few weeks. Now, of course, that is not directly connected to calculus, you can just as well do this in a linear analysis class, or in something focusing on theory of computation (I think that's what they do at stanford).

Now, if you move that part somewhere else, you can teach calculus differently or move it to a different place of the CS curriculum.

The question is whether you should, and if yes, to where and whether you need it at all. I don't really know. Calculus is kind of important for many areas of CS, for example to define big-O notation you kind of need it. Also, a CS curriculum without calculus is hard to imagine, especially for the people in charge of making these since they likely also attended such a curriculum.",1,0,0,False,False,False,1639088893.0
rcip1t,hnygqno,t3_rcip1t,Unless they hire extra professors for the cs department but other than that I believe there would be different applications for different majors. I think the overall concept is the critical junction while teaching this skill.,1,0,0,False,False,False,1639117298.0
rcip1t,hnyvjd3,t3_rcip1t,"The same way my uni did it, you take calc 1 like everyone else, and then go into comp sci related math courses. You need to understand the basic concepts before you can reasonably be expected to apply them in the real world.

There's a reason why in calc and above most proffesors don't care how ""fancy"" your calculator is, punching in numbers only gets you so far, you need to have some level of understanding why things work they way they do.",1,0,0,False,False,False,1639128278.0
rcip1t,hnz10wj,t3_rcip1t,"Honestly, I don't think it should be, unless the student is going into graphics design, or 3d modeling, or simulating either of those.

Calculus (and beyond) is less a guide to logic, and more a guide to memorizing formulas.

All that said, to answer the question, no, I don't think it should be taught differently based on whether you are in Comp Sci or not.  Definitely _add_ connections to comp Sci, but don't create a lesson ""for computer scientists"".",1,0,0,False,False,False,1639132912.0
rcip1t,hnzlodf,t3_rcip1t,Lmao it really shouldn’t,1,0,0,False,False,False,1639145699.0
rcip1t,ho3ddya,t3_rcip1t,Calculus has helped a lot but learn optimization. Not like linear optimization but mixed constraint shit,1,0,0,False,False,False,1639205656.0
rcip1t,hobba9x,t3_rcip1t,"I certainly agree that it could be taught in a better way, but I think most schools will not create a separate math class just for CS. There are definitely a lot of practice problems and applications of calculus that could come from CS.

I've had to integrate using polar coordinates, integrate logs, partial derivatives, and all the possible things I've integrated in typical integral classes so I found them quite useful already.

As for the ugly functions, I agree that they aren't as relevant, but I used it as a way to practice and learn how not to make mistakes.

Edit: If calculus was taught with CS applications, even some basic caculus methods may require a probability background or additional CS background. I suppose teaching calculus at such a late stage in your career might be less than ideal unless you knew for sure that you were going into CS.

Edit 2: If I were teaching a calculus class, I might preface a problem in calculus with an application in another field if I see applications that are easily explanable within a few minutes. Otherwise, it might take too much time and take away from actually learning new methods.",1,0,0,False,False,False,1639353472.0
rcip1t,hoefom8,t3_rcip1t,"Don't know that the following applies to your question but found it to be an interesting source for learning.

https://brilliant.org/courses/",1,0,0,False,False,False,1639415826.0
rcip1t,hohraat,t3_rcip1t,"I think this depends on whether the class is being taught at a university that aims towards providing a liberal education, or if the class is being taught at an institution that functions more like a trade school.

A trade school trains students for practical skills in a given profession. It makes sense that coursework is tailored to highlight the application of each subject to that profession. Students don’t need to learn underlying theories or ancillary topics if they can’t be applied in their future profession.

A university should be different. Classes should be taught in a way that encourages students to explore the depths of the subject itself. This is part of the process of providing a liberal education.


I admit this stance is a little extreme. I’m sure there are ways of adapting a calculus course to highlight CS-related content without losing anything. I just get very suspicious when university courses start to focus too much on practical application, and stop teaching theory.",1,0,0,False,False,False,1639474458.0
rcip1t,hnuxx68,t3_rcip1t,I got 12/40 in cal1 while getting 38/40 in c++ lab soooooo,-11,0,0,False,True,False,1639062664.0
rcip1t,hnw9xfy,t3_rcip1t,"Real talk, computer scientists should be taught calculus through the lens of computing gradients for neural network parameters. You can start with really simple differentiable estimators then work your way up to fancy deep NN's and gradients over really complex transforms like image perspective transforms.

There's a nice range of difficulty, and it's very easy to see why calculus is worthwhile when you see it so directly applied in backpropagation.",-6,0,0,False,True,False,1639081651.0
rcip1t,hnxazie,t1_hnuzs1u,Numerical Analysis was the shit,3,0,0,False,False,False,1639097280.0
rcip1t,hnxjwlj,t1_hnvhhra,"There does need to be a difference in how you teach different students with different motivations - it isn't about siloing knowledge, it's about providing the best opportunities for learning for each student. In a perfect world, this would be tailored to the individual person, but due to various unavoidable realities the best we can hope for is that it is tailored for the different cohorts of students you get in each class.
 
Yes, the best students won't care and can shift between perspectives on their own, they'll succeed in a generally-taught class - but guess what? Those students will succeed if you give them a textbook and a stick to draw in the dirt with, focusing educational design as if those are your target is a complete waste of your time.
 
The knowledge stays the same, but how you present it, lead in to it, connect it to what a student already knows and cares about, that is what determines whether you get the right engagement from the students who need to be taught, as opposed to those who can learn on their own.",3,1,0,False,False,False,1639101307.0
rcip1t,hofabn1,t1_hnv98w9,Calculus is not pure at all. The real pure calculus is real analysis. Calculus honestly is mostly just plug and chug at most universities.,2,0,0,False,False,False,1639428116.0
rcip1t,hnvsput,t1_hnutvlk,What you described is exactly how my Calc. Teacher taught it. The class was a mix of physics and CS students so anything we learned was tied to real world problems. It really made The class fun and easier to learn.,7,0,0,False,False,False,1639074800.0
rcip1t,hox6rqo,t1_hnv3occ,"I loved that class.  I took it at Rutgers with a professor that worked in the petrol industry and insisted we work in FORTRAN.  That one part I didn't love.

I gave it a different name though.  It's been years, but it was something like ""computational guesstimation"".",1,0,0,False,False,False,1639751557.0
rcip1t,hnxqda1,t1_hnxjwlj,"Sure, if you're just trying to be a code monkey or do data entry, then you don't need to know algorithms.  Up to a point that's true, but if you're getting a CS degree, you need to be able to ""speak math"" at the very least, since that's just part of what a CS degree means.  If you're in a CS program, then you  need to be able to understand math in exactly the same way that the math students do.",10,0,0,False,False,False,1639104133.0
rcip1t,hoflvez,t1_hofabn1,Yeah but it shouldn’t be plug and chug. U should be learning application of calculus items,2,0,0,False,False,False,1639432887.0
rcip1t,hnxrmoy,t1_hnxqda1,"You've completely missed the point. You don't teach them different things, you teach them the same things in different ways. The same knowledge, the same skills, but the way you explain them and build up to them is going to be different.",0,1,0,False,False,False,1639104681.0
rcip1t,hogdf8s,t1_hoflvez,I agree with you. Im just saying thats how it's taught at most colleges. It's also not professors fault because the colleges want them to stick to the curriculum where professors have little say in it,1,0,0,False,False,False,1639445434.0
rcip1t,hnxs3cx,t1_hnxrmoy,"Yeah, I get the point.  I don't agree.",2,1,0,False,False,False,1639104884.0
rcip1t,hnxsdgv,t1_hnxs3cx,"Literal reams of educational research says you are incorrect, and your comment makes it clear you don't get the point at all. By the end of their degree, someone taught in a way that builds from their existing domain knowledge rather than being given a stock standard approach is going to know more, and know it better.",-2,1,0,False,False,False,1639105010.0
rc955j,hntd7gv,t3_rc955j,"x = n + n/2 + n/4 + n/8 …

Multiply both sides by 2.

2x = 2n + n + n/2 + n/4 + n/8 …

Subtract each side of the the first equation from the corresponding side on the second.

x = 2n

Which is O(n)",18,0,1,False,False,False,1639025135.0
rc955j,hnt8ibf,t3_rc955j,You wanna share the code?,4,0,0,False,False,False,1639022665.0
rc955j,hntf8eo,t3_rc955j,"Let M be the original value of N before the code starts. Then the number of executions is M + M/2 + M/4 + ... This is a geometric series (look that up if you're not familiar). Any time the length of the loop changes like this, you should look at whether that shrinks the complexity. That's how sorting algorithms can be O(nlogn) with two nested for loops.",2,0,0,False,False,False,1639026247.0
rc955j,hnwx8zt,t3_rc955j,So this guy Zeno has this paradox right...,1,0,0,False,False,False,1639091204.0
rc955j,hnt8jc2,t3_rc955j,What algorithm is it,-2,0,0,False,False,False,1639022679.0
rc955j,hnteheb,t1_hntd7gv,Thank you so much! I've been thinking about this problem for about 2 hours and I think it just clicked. I appreciate it!,5,0,0,False,False,True,1639025837.0
rc955j,hnt9nw7,t1_hnt8ibf,"Apologies, I thought I had it attached when I posted it",4,0,0,False,False,True,1639023260.0
rc955j,hnt9pq3,t1_hnt8jc2,"Sorry, I thought I had attached it when I initially made the post",2,0,0,False,False,True,1639023285.0
rc955j,hnx4g1f,t1_hnt9nw7,"Please, don't post images of code when you can just post code (4 spaces before each line for correct formatting)",1,0,0,False,False,False,1639094340.0
rc955j,hntaube,t1_hnt9pq3,"You sure complexity is O(n)
Because there is no n in that code",-14,0,0,False,True,False,1639023874.0
rc955j,hnuend3,t1_hntaube,"""n"" is an arbitrary common label used, not referring to the specific variable name in the code. You could call it O(m), O(p) etc.",5,0,0,False,False,False,1639052860.0
rc955j,hntbatv,t1_hntaube,"Well, if we're using the variable in the code, then it is O(N)",3,0,0,False,False,True,1639024116.0
rc955j,hnve5mm,t1_hnuend3,Got you,0,0,0,False,False,False,1639069124.0
rbwnot,hnqrhlp,t3_rbwnot,"Practice is the best way. Write code and run it so you can see what it does. Play with the classes and libraries you're using and just see what they do. If you ever think ""I wonder what happens if I..."" then write a program and try it. Unless you're doing silly things like manipulating files in system directories it's pretty difficult to actually damage your computer in any meaningful way accidentally. The best way is to just get your hands all over it and write code.",73,0,0,False,False,False,1638985659.0
rbwnot,hnr669c,t3_rbwnot,Practice https://adventofcode.com/ :),33,0,0,False,False,False,1638991262.0
rbwnot,hnqt4g4,t3_rbwnot,"There are some great books by oreilly on the python programming language and many different applications such as web development, finance, data science, and more.

I would say for now, it’s good to be aware of those applications while you learn the language and build intuition and muscle memory.

Your class projects will be really good for applying fundamental  concepts. These projects will be important in your portfolio as they demonstrate basic competency.

What will really connect you to programming will be different projects of practical ideas and personal interest I would suggest using Udemy’s 100 days of code (I have heard great things about it and I will be starting it soon), or other beginner projects you find on the internet.

These projects are beginner to intermediate and are especially helpful if you are not sure what your programming interests are. As you do some of these projects, you will be noticing what you like and dislike. From there, you will be able to guide yourself. 

Programming can seem daunting early on in this vast space especially if you have classmates who have been doing it for a while. It is important to start small and not bite off more than you can chew. You might get discouraged and have a negative experience that lingers on. 

Tldr: get comfortable with the basics, start small, divide and conquer, find your interests, enjoy yourself",7,0,0,False,False,False,1638986278.0
rbwnot,hnqt9vc,t3_rbwnot,I think practice is the biggest thing. Write a bunch of code. I also think reading and understanding other peoples code can be very useful. You could probably look at some open source projects to find code to read.,7,0,0,False,False,False,1638986335.0
rbwnot,hnrcswq,t3_rbwnot,"ignore the people linking to shit like advent of code and hacker rank, solving those problems and actually doing projects are two different skills. You need to focus on doing projects , stop thinking about if its unique or anything. Just do it, and you will get better.",5,0,0,False,False,False,1638993773.0
rbwnot,hnrid6y,t3_rbwnot,"As everyone else has been saying, code more. But in my experience, find a semi-big project that you will really enjoy doing, and go at it. I was in the same boat, and I found a really fun project: making a password guesser that could get into anyone's school account at my school. Yeah, that project sounds really bad, but I told all my high school teachers about it and I was very transparent about it so I wouldn't get in any trouble. Anyway, I had an absolute blast trying to code the program since it made me feel so cool, like a hacker. You could try a basic or advanced project; just make sure it forces you to code A LOT.",4,0,0,False,False,False,1638995895.0
rbwnot,hnqwom7,t3_rbwnot,Experience and read  3 party code,3,0,0,False,False,False,1638987636.0
rbwnot,hnrgica,t3_rbwnot,"I suggest going through a book like SICP https://mitpress.mit.edu/sites/default/files/sicp/index.html

Solving a bunch of programming problems using a lisp language really helped me learn how to methodically solve problems programmatically.",3,0,0,False,False,False,1638995194.0
rbwnot,hnsxw52,t3_rbwnot,"If you want to hire a virtual tutor, feel free to message me!",3,0,0,False,False,False,1639017643.0
rbwnot,hnqu85z,t3_rbwnot,"Lots of practice problems are available online. Try looking into coding competition sites. They have questions sorted by difficulty, and you can build your way up.

Here are some challenges on [HackerRank](https://www.hackerrank.com/domains/python) specifically for python.",2,0,0,False,False,False,1638986700.0
rbwnot,hnqumei,t3_rbwnot,"Write more code. Look up python beginner projects and code along with them, watch python videos on freecodecamp's YouTube page, complete a leetcode question every day or 2. Just do something. You can't learn the wrong things. Just keep learning and the more you learn, the easier it will get to decide/know what to learn next.",2,0,0,False,False,False,1638986852.0
rbwnot,hns6k7b,t3_rbwnot,"So I am a deadly practical person, and I like to always see the effects of what I am doing, so my way of getting better at coding was (and still is)  - thinking about what would I like to learn or build. Maybe a mobile application? In this case, I read how to build one, what languages you have. I think about the features which this app should have. Then I start doing tutorials which are showing specific elements which I want this app to have, for example how to do AR, or how to make the app use camera and read text from a picture.",2,0,0,False,False,False,1639005482.0
rbwnot,hnsmoxv,t3_rbwnot,"Make tons of projects, check this link [https://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/](https://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/) try to make all of the projects. Doesn't really matter about programming languages, use whatever you're similar",2,0,0,False,False,False,1639012702.0
rbwnot,hnt9oif,t3_rbwnot,Read good code.   Not enough time is given to reading successful code bases IMHO.,2,0,0,False,False,False,1639023268.0
rbwnot,hntc1t8,t3_rbwnot,"I got better by trying to automate various server tasks. Everything from renaming files to web scraping to video encoding etc.  Find those little mundane things you do and try and code it, then go back and improve the code once you've gotten better. You'll get there v",2,0,0,False,False,False,1639024512.0
rbwnot,hntfb7g,t3_rbwnot,"You improve coding primarily by coding. Read a lot of software written by others and write a lot of software of your own: there is no substitute for reading and writing software. Thankfully due to free software and open source software movements there are basically endless amounts of software you can read free of charge.

At the same time, you'll want to study as much discrete mathematics as you are able, which will help remove walls from your path before you encounter them in your programs.

You will need to study algorithms and data structures until they are completely internalized into your being, they are indispensable. I recommend Introduction to Algorithms or basically anything on the matter by Charles E. Leiserson. Their lectures are also available free of charge on youtube.",2,0,0,False,False,False,1639026291.0
rbwnot,hntpofx,t3_rbwnot,"Write code, read books, watch videos about programming, check others code",2,0,0,False,False,False,1639032890.0
rbwnot,hnqupug,t3_rbwnot,Practice! There’s no short cut. Try contributing to open source projects.,2,0,0,False,False,False,1638986888.0
rbwnot,hnst8v1,t3_rbwnot,"I know I’ll get some hate for this but I learned the most when I started using lower level languages like C and even Assembly (I would not recommend doing assembly tho). Having to implement everything from scratch with only the essential libraries will let you appreciate and understand runtime a little better. Even tho I use Python from time to time when I need to create a tool fast, I feel that always coding in Python will drive you to the lazy programmer path that can’t do anything without a library. But there’s nothing wrong with that if the requirements are met.",1,0,0,False,False,False,1639015591.0
rbwnot,hnqvd01,t3_rbwnot,"If you are looking to improve your problem solving abilities, check out Exercism or Kattis",1,0,0,False,False,False,1638987134.0
rbwnot,hnqy0lz,t3_rbwnot,"You just have to put in the time.  Think of some kind of program you'd like to have that doesn't exist, and give it a try.  If it seems like ""a little too much work"", it's perfect, try it.  You'll learn a lot just researching the documentation and libraries needed, and fixing bugs.",1,0,0,False,False,False,1638988143.0
rbwnot,hnr0tin,t3_rbwnot,Touch code everyday.,1,0,0,False,False,False,1638989214.0
rbwnot,hnr1934,t3_rbwnot,"Have a goal like

draw up a wireframe of single webpage and try to implement it. Use internet search to implement it. Remember your primitives, loops, bool, ect. Follow software design principles.",1,0,0,False,False,False,1638989378.0
rbwnot,hnr8l94,t3_rbwnot,"Just do a simple project and look forward to errors that you'll get. every time you get an error either write it down or remember it (will happen a lot!!) at first it will be extremely confusing all these new errors and words you never heard of! but then over time you notice some errors that look familiar. and before you know it you can fix those errors and even predict and prevent yourself from writing buggy code. 

I also recommend specializing in one thing! maybe at first you are more general in your approach and you do anything and everything but then over time you find what you like and do just that! specialize in one thing! web dev, ML, AI, whatever. this way as you spend time developing your skills in that area you are not spreading yourself too thin.",1,0,0,False,False,False,1638992180.0
rbwnot,hnrjj2n,t3_rbwnot,If you're a beginner and don't know the fundamentals very well then do lots of tutorials.  I personally recommend Scrimba as they have a very unique built in editor in their videos.  After you understand most of the basic move on to making projects.,1,0,0,False,False,False,1638996328.0
rbwnot,hnrwfv8,t3_rbwnot,"As the others have pointed out, practice is important. But something that also helped me were guidelines like ""The code should tell you what it does and the comments should tell you why it's doing that."" 

Also looking at actually good code will make you notice your own weaknesses. (For example I used to watch the YT Channel ""Coding Adventures"" at breakfast for a while and that helped me.)

Lastly, I think to better understand advanced concepts, learning some more esoteric languages like Haskell and Prolog, is actually quite entertaining and useful.",1,0,0,False,False,False,1639001298.0
rbwnot,hns3hd3,t3_rbwnot,The same way you get better at anything. Practice.,1,0,0,False,False,False,1639004169.0
rbwnot,hns5jyx,t3_rbwnot,"Participate in an open-source project.

Learn how to use a debugger - debuggers are not just used for finding bugs; you can use them to step through code that you want to learn, try changing variables while debugging, learn how to set breakpoints.  This alone will make you a better developer than 50% of the competition (obviously a estimate).",1,0,0,False,False,False,1639005051.0
rbwnot,hnt1h20,t3_rbwnot,"Learn more than one language for one! Figure out how they're different, what their strengths are, when it makes sense to use which.

Google things like the docs or best practices and find simple ways to apply it. Take a hello world java program, make it a hello to an array of all planets. Maybe make a planet object with different fields. Call it via for loop. Then try writing it more expression based using streams and filters.

Google interview questions and try to solve them. Find coding challenges based on doing things in certain time efficiency where the brute force way is simple. Look for things that aren't just accomplishments in what they do, but that give you a better understanding of how something works or adds a new tool to your belt that you didn't know you could do before.",1,0,0,False,False,False,1639019267.0
rbwnot,hnuek8l,t3_rbwnot,"1. Learn different programming languages and their paradigms (OOP, functional, logical etc)
2. Study complicated to understand and keep in mind math concepts — good example is combinatorics — just fo brain training)
3. Practice, but not blindly — i.e. having particular objectives. But the objectives should be feasible in a reasonable time. Good example is to participate in online programming competitions where you have to solve non-trivial enough problems of different difficulty levels and where your solutions strongly verified automatically (so you will always be confident if you succeed or not with the task)",1,0,0,False,False,False,1639052805.0
rbwnot,hnuw601,t3_rbwnot,"Boot camp, projects , video tutorials",1,0,0,False,False,False,1639061894.0
rbwnot,hnvafs7,t3_rbwnot,create projects with your knowledge do not do tutorials forever,1,0,0,False,False,False,1639067698.0
rbwnot,hnrqbr8,t3_rbwnot,Spend as much time doing the best kind of learning you can. The best kind of learning is play.,0,0,0,False,False,False,1638998909.0
rbwnot,hns792i,t3_rbwnot,[removed],0,0,0,False,False,False,1639005774.0
rbwnot,hnuv39v,t1_hnqrhlp,">If you ever think ""I wonder what happens if I..."" then write a program and try it.

This is really a good point, I enjoyed a lot playing with pointers in C this way.",3,0,0,False,False,False,1639061423.0
rbwnot,hnscngs,t1_hnr669c,and compare your solutions at r/adventofcode!,7,0,0,False,False,False,1639008182.0
rbwnot,hnwbn4y,t1_hnrid6y,How did the code work?,1,0,0,False,False,True,1639082344.0
rbwnot,hnr1asw,t1_hnr1934,"
Hello! You have made the mistake of writing ""ect"" instead of ""etc.""

""Ect"" is a common misspelling of ""etc,"" an abbreviated form of the Latin phrase ""et cetera."" Other abbreviated forms are **etc.**, **&c.**, **&c**, and **et cet.** The Latin translates as ""et"" to ""and"" + ""cetera"" to ""the rest;"" a literal translation to ""and the rest"" is the easiest way to remember how to use the phrase. 

[Check out the wikipedia entry if you want to learn more.](https://en.wikipedia.org/wiki/Et_cetera)

^(I am a bot, and this action was performed automatically. Comments with a score less than zero will be automatically removed. If I commented on your post and you don't like it, reply with ""!delete"" and I will remove the post, regardless of score. Message me for bug reports.)",4,0,0,False,False,False,1638989395.0
rbwnot,hntd25r,t1_hns792i,"Thanks for posting to /r/computerscience! Unfortunately, your submission has been removed for the following reason(s):

* **Rule 2:** Please keep posts and comments civil.



If you feel like your post was removed in error, please [message the moderators](https://reddit.com/message/compose?to=/r/computerscience).",1,0,0,True,False,False,1639025057.0
rbwnot,hnscolz,t1_hnscngs,"Here's a sneak peek of /r/adventofcode using the [top posts](https://np.reddit.com/r/adventofcode/top/?sort=top&t=year) of the year!

\#1: [Thank you Eric!](https://np.reddit.com/r/adventofcode/comments/kjtou6/thank_you_eric/)  
\#2: [Too often](https://i.redd.it/2dwtt64moq461.jpg) | [62 comments](https://np.reddit.com/r/adventofcode/comments/kbnh5i/too_often/)  
\#3: [\[2020 Day 18 (Part 1)\] Outsourcing the solution. They never care about the order of operations anyway](https://i.redd.it/ugjy19khxy561.png) | [21 comments](https://np.reddit.com/r/adventofcode/comments/kfnt2s/2020_day_18_part_1_outsourcing_the_solution_they/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[Source](https://github.com/ghnr/sneakpeekbot)",3,0,0,False,False,False,1639008197.0
rbwnot,hnwdlhm,t1_hnwbn4y,"Well, at my high school, it was easy to make this program. For their school logins, everyone had a username that could be easily determined. It included their name and graduation year (Ex: John doe graduates in 2022: jdoe22). For the password, it was always the word ""pass"" and then a 4 digit number (Ex: pass9233). My program first asked the user to type in the student's full name and graduation year. After that, it generated the username automatically and stored it in a variable (very simple code). Then, it opens up a tab in Chrome, goes to our school portal website, and starts entering the username and every possible combination of the password until it works. pass0001, pass0002, pass0003 and so on. In reality, I had it open 4 different Chrome tabs at once to do it faster, but it worked on every single student. Does your school have a password system like that? Just curious",1,0,0,False,False,False,1639083127.0
rbwnot,hnwdt6j,t1_hnwdlhm,Nah we get to choose our own passwords,1,0,0,False,False,True,1639083213.0
rc3wsu,hnsal50,t3_rc3wsu,"Mechanical computing device. Physically pretty big and over engineered for durability. Most of its function is to be a database lookup to relevant stone tablets with the actual information.
Combine with smaller mechanical devices that use directed questions to allow ignorant people to create a useful search for the big device.
If you really want to go for the long haul, the final lookup is actually directions through the massive tablet storage area instead of a tablet retrieval mechanism.
Ideally you won't have any computing, just knowledge, which is where most sci-fi authors go with this problem. But making it a device is a fun conceit 🤔",20,0,0,False,False,False,1639007258.0
rc3wsu,hnt6t06,t3_rc3wsu,"A “foundation,” as it were.",5,0,0,False,False,False,1639021804.0
rc3wsu,hnsd7e5,t3_rc3wsu,Technically you don’t have to create a thing to store all knowledge.   You only need to create a thing that motivates and instructs the user as to how to turn on the computer.,3,0,0,False,False,False,1639008434.0
rc3wsu,hnttzp3,t3_rc3wsu,"Easy, you just need to store the data on a Nokia 3310 somehow! 

Or something like NASAs golden record. 

Imo it would not work though:  When, for example, the ancient egypt empires fell, most of the knowlege was lost despite it being available in writing at the temple walls (and a lot of it is lost till today). After a collapse most people would not be able to read anymore and people using computer would be seen as crazy.",3,0,0,False,False,False,1639036160.0
rc3wsu,hnt839p,t3_rc3wsu,"Diamonds and lasers! But in all seriousness, I don't think it's possible to make indestructible computers, but it should be possible to make easily maintainable ones. Where a replacement part can be a lens or some metal bracket.",2,0,0,False,False,False,1639022450.0
rc3wsu,hnujzvv,t3_rc3wsu,Plastic Fortran punch cards.,2,0,0,False,False,False,1639055996.0
rc3wsu,hnunzgc,t3_rc3wsu,"In the 3 body problem they thought about all kinds of ways of storing information for 100,000 years into the future, and finally figured out that the best way was invented eons ago: etching it into stone. Stone will last tens of thousands of years under the right conditions. So your best bet would be to etch information into stone, but if you're really set on the whole 'computer' idea it would have to be something mechanical that interacts with the stone somehow, like retrieving tablets or something. Kind of like the machines you see in the dwemer ruins in skyrim.",2,0,0,False,False,False,1639058077.0
rc3wsu,hnsgwkc,t3_rc3wsu,"make a system that can repair itself and make its own parts. that way you get some longevity.

then give it the task to conserve educational videos in a RAID 1 configuration and show them on a crt when a hooman is present",2,0,0,False,False,False,1639010093.0
rc3wsu,hnt0p0j,t3_rc3wsu,"My first thought for building an archive like this would be to find a star and construct a Dyson sphere, which would ensure it's survival by sending a Von Neumann probe out to replicate the knowledge base when the star wanes (the dying million years or so).

How the information is expressed is an interesting abstract problem though, since those encountering ""The Archive"" might not have the same understanding of technology as it's constructor.",2,0,0,False,False,False,1639018909.0
rc3wsu,hnt84xp,t3_rc3wsu,"Running for _centuries or longer?_ I'd ditch the computer in favor of a card catalog system. Doesn't need any electricity, won't be effected by an EMP or nuclear fallout or whatever led to the societal collapse. It will last as long as the room and printed material endure. Paper becomes fragile with age, but by laminating the cards and coating books in some kind of a sealant they could be made to last much longer. Hopefully the library could contain enough books (maybe in microprint with magnifying glasses to store as high a density of information as possible?) to aid in redeveloping society and technology.",1,0,0,False,False,False,1639022475.0
rc3wsu,hnttya7,t3_rc3wsu,"Like the seed vault, the trick for preservation is cold.  This way items last for a very very long time.

Instructions on how to read with physical books is necessary, because there is no guarantee of infinite power.. maybe geothermal, but it's beyond my knowledge of the topic.  So you have to teach someone how to get power, teach them how to make a circuit and so on.  Have items for them to use, play with, and learn like breadboards and what not.  Once they learn how to make power they can plug in any preserved computer.",1,0,0,False,False,False,1639036129.0
rc3wsu,hnvg2sp,t3_rc3wsu,Poneglyph,1,0,0,False,False,False,1639069868.0
rc3wsu,hnzemm3,t3_rc3wsu,Gameboy Cartridges.,1,0,0,False,False,False,1639142151.0
rc3wsu,hnsbio2,t1_hnsal50,"Mm, yeah I thought about that, and certainly the ""books with pages made of hammered gold"" type thing appeals to me, especially through deep time like thousands or millions of years. For this particular story the aim is to be useable for several centuries, and I think it might be useful for these people to have engineering tools, computer aided design and simulations, perhaps even access to radio for the purpose of communicating with old satellites (as some high orbit satellites will still be in place for millions of years, it's not unreasonable to also make a satellite that might be able to do things for a very long time).",1,0,0,False,False,True,1639007669.0
rc3wsu,hnsbrk0,t1_hnsal50,"The physical storage medium is something I considered as well, even for an electronic computer - given unlimited size something like a vinyl record but made of quite hard materials could last for a *very* long time.",1,0,0,False,False,True,1639007780.0
rc3wsu,hnujej0,t1_hnsal50,Gigantic over engineered card catalog?,1,0,0,False,False,False,1639055668.0
rc3wsu,hnsdsx6,t1_hnsd7e5,"“If you wish to make an apple pie from scratch, you must first invent the universe.”

I say this tongue-in-cheek, but you're right of course and that is a major consideration here. The user would have to choose and be able to operate the device in the first place, and perhaps even have some useful help in learning how to use it.",2,0,0,False,False,True,1639008706.0
rc3wsu,hnsdwx5,t1_hnsbio2,"Does your story allow for caretakers of this repository, or should it be completely abandoned and forgotten for a time?  And how long does it need to last?",1,0,0,False,False,False,1639008756.0
rc3wsu,hnuysl6,t1_hnujej0,"Yes!  The idea of a technology that could survive a thousand years and be used by ignorant people is preposterous, but that is the requirement of this thought exercise 🔥so  you just have to run with it.",1,0,0,False,False,False,1639063037.0
rc3wsu,hnso0ub,t1_hnsdsx6,If things really did get back to Neanderthal levels you would need to start with some simple interactive games to build familiarity and trust with the device.   Things like matching shapes or colors like toddlers toy.   Then you could build up to simple spoken language.   It could be like a fast paced survival of the fittest scenario where status in the society was based on the ability to interact with the machine.  Eventually your users could start seeing story boards that explain what happened and build up to learning what we know in detail.   You couldn’t just throw that at someone who might even never have seen a number or a wheel.  They would be frightened of it,1,0,0,False,False,False,1639013289.0
rc3wsu,hnsep68,t1_hnsdwx5,"No caretakers, no maintenance, completely abandoned and forgotten. It would be located in a geologically advantageous environment - perhaps a desert at high altitude.

Anything from a century to a thousand years would be fine, but the longer the better, story-wise. A million years would be excessive. I want to be able to describe the making of it in some detail.",1,0,0,False,False,True,1639009110.0
rc3wsu,hnsfe4v,t1_hnsdwx5,"Also want to add that the story includes automated outposts throughout the solar system, and potentially a (failing) generation ship, made by the same or similarly capable group of people. I want the technology to be consistent so one will inform the other.",1,0,0,False,False,True,1639009416.0
rc3wsu,hnsg09b,t1_hnsdwx5,"Also, an afterthought here - yes, the facility could have caretakers, but they'd have to arrive after the fact, know nothing of how it works or what needs to be done, beyond what can be learned from using it. Adding certain materials to a reservoir, or turning a handle, or hooking a shaft up to a windmill or a primitive electrical generator, fine. Pre-existing knowledge of re-imaging the operating system or soldering surface mount components to a board, not so much. Replacing parts in storage, as instructed by careful study and use of the device, fine.

Any maintenance would have to be done by people who have no idea how it works",1,0,0,False,False,True,1639009689.0
rc3wsu,hnshq24,t1_hnsep68,"To be fair, for just a century or three you could simply print Wikipedia on archival paper with a laser printer and store it in a cardboard box, if you had a dry cave in the high desert and a way to keep animals out.

If you're committed to something electronic, I would look at solid state storage, something older (and thus likely more robust), like mask ROMs.  Even consumer grade mask ROMs last decades with very little care (think Atari 2600 cartridges, some are from the 1970s, most of them still work if they're clean).  You could probably take inspiration from Viking and Voyager space probe computers, the Voyagers are still functioning after decades.  A nation-state actor could surely take lessons learned and make a very robust system in a similar manner.  Voyager used dual redundant systems and CMOS volatile memory with a direct, permanent connection to the RTG.  You'll also probably want some kind of nuclear-derived power.

My main thought is things need to be designed to be fault tolerant and redundant, the system should have the ability to bypass or work around failed parts, and the storage area should be as clean, even temperatured, and dust/moisture-free as possible, say, in a cement-lined cave.  A big part of why electronics fail is due to temp cycles.  Moisture in the air combining with contaminated on the hardware can cause corrosion.  Avoid the use of electrolytic capacitors.

But really, if you can get away with something non-technical like books, go for it, it'll be more believable.  There are books in the world from the 15th century and (much) older that just laid around an attic most of that time.",5,0,0,False,False,False,1639010461.0
rc3wsu,hnyqod5,t1_hnshq24,"There will be various abandoned/dead outposts throughout the solar system, as well as a dying generation ship - ultimately I'd like the characters to have some kind of communication with them, even if it's just to witness their slow death. It would also be really interesting if they could connect to some remaining satellites in orbit to get an overview of Earth.

Libraries and the like would certainly exist as well as this thing.


There are a few plot threads I'd like to explore concerning a computer database versus libraries as well - if the computer fails, all of that knowledge is lost at once (with hope for repairing the machine, possibly).

Of course there's also the capability for physics simulations, computer aided design, accurate and precise timekeeping etc. There would some interesting possibilities for showing video and sound as well.


There's also good reason for this thing to be located in a desert - (weather bad) not a great place to live but even just visiting it may make for an interesting story. The user might even become a User or a High User if civilization decides to treat it as a sort of religious artifact or something - and especially interesting if it breaks and The Great Council of Quertyuiop calls upon a crusade to retrieve the Holy Heatsink or something.",1,0,0,False,False,True,1639124229.0
rc3wsu,hnz443g,t1_hnyqod5,"Those all sound like interesting story beats to me, I would read that novel.",1,0,0,False,False,False,1639135368.0
rcfj0u,hxjw6xz,t3_rcfj0u,There’s a wonderful amount of potential in this manner of thinking. Thanks for posting! I’ll be looking more into it.,1,0,0,False,False,False,1645254819.0
rcj9zu,hnv6tj6,t3_rcj9zu,"Everything you want to know can be found in the book Digital Computer Electronics by Albert Paul Malvino. The things turning on and off are parts of an electric circuit. You can see this playing out at http://visual6502.org/JSSim/index.html. You can construct the architecture (SAP, ""Simple As Possible"") covered in Malvino's book yourself by following Ben Eater's video series on youtube.",1,0,0,False,False,False,1639066319.0
rcj9zu,hnwmfr5,t3_rcj9zu,It executes one instruction regarding a piece of data at a time.,1,0,0,False,False,False,1639086724.0
rcj9zu,hny0979,t3_rcj9zu,Very quickly,1,0,0,False,False,False,1639108546.0
rcj9zu,hnw9dic,t1_hnv6tj6,"Also ""The hidden language of computers"" a real masterpiece of a book and easy to understand",1,0,0,False,False,False,1639081429.0
rc1j2y,hnrvjru,t3_rc1j2y,"data type - is the data a string, a number, a date etc.  different languages will have their own types (int, float, string, datetime)

data structure - ""a data organization, management, and storage format that enables efficient access and modification"" examples of these would be an array, a binary search tree, a hash table.  they are all different ways of storing data that provide different benefits (speed, size, inserting, sorting, etc)

data schema - i believe this is for describing the organization of data in a database. if you are new and don't know much about databases i don't think examples will be of much use

data model - i'm familiar with this as a part of the MVC (model, view, controller) design pattern. the data model is how the data for your application is represented.  a key part of mvc is that this is kept separate from the view (what displays the data to the user) and the controller (the ""brain"" of sorts, it manipulates the views and data so they are always separate and unaware of the other)

I guess they are all related in the sense that they all have the word data in them.  Data schema would have data types in it, as would a data model.  a data structure as a concept is independent of the rest in a sense, but you would also see an array in some languages declared with a certain data type",4,0,0,False,False,False,1639000946.0
rc1j2y,hnsjdy9,t3_rc1j2y,"My CS Professor weirdly said a schema is the way a table is structured but in my 7+ years of data it has always referenced the organization of tables within a database. Like, a database is made up of schemas, the schemas are made up of the tables. Sorry if this is more confusing!",0,0,0,False,False,False,1639011211.0
rc1j2y,hnzwwqx,t1_hnsjdy9,"That's exactly the opposite of how we learned to use those terms in databases courses: the database is made up of tables, each organized/defined by their schema.

I don't know which is right, and maybe one use of terms is used in theoretical/academia and another use is common in industry.  I'm just reporting what we were taught.",1,0,0,False,False,False,1639150666.0
rc1j2y,ho0mbh5,t1_hnzwwqx,I think we’re saying the same thing and my Professor said something different.,1,0,0,False,False,False,1639160767.0
rc9jzq,hntdhsk,t3_rc9jzq,"> Can AI theoretically solve the halting problem or **at least most of it?**

The issue is that the halting problem isn't defined probabilistic-ally. You can't solve the halting problem with ""I'm 90% confident the given program will halt, because it looks similar to halting programs that I've been trained on"", because by definition that is _not_ determining whether the input program will halt, it's just making an educated guess.

If you want to move from ""this will probably/won't probably halt"" to deterministically solving the problem, then we're back to square one.",13,0,0,False,False,False,1639025291.0
rc9jzq,hnu5jur,t3_rc9jzq,"In theory of computation, for an algorithm to be said to *solve* a problem, the algorithm must provably produce the correct answer for every possible input, not just ones that you can come up with. That may be quite different from everyday use of the word, but it's crucial if you want to talk in terms of theory of computation.

Could an artificial neural network solve the halting problem in the sense the word is used in computability theory? No, unless you're willing to postulate that the different proofs showing the undecidability of the halting problem -- and the more general Rice's theorem -- are wrong.

Could you train a neural network that appears to, in practical terms, be able to answer the question for example programs that you come up with? Maybe. But there are already other methods for answering limited cases of the halting problem, and you don't necessarily need an AI for that.

Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.",5,0,0,False,False,False,1639046035.0
rc9jzq,hntb6yf,t3_rc9jzq,"At some point wouldn't the only way to determine reality on the inner halting or not, be to actually run the code to prove the halt. 

It's sorta like the quantum physics cat example. The truth is unknown till the truth is known",3,0,0,False,False,False,1639024059.0
rc9jzq,hntyj0m,t3_rc9jzq,"The best way to tackle termination problems is by using symbolic methods, not neural networks.

There are various techniques that can solve the problem for a given class of algorithm, up to some degree of complexity. So, by not defining what ""most of it"" means, we could say that (symbolic) AI is able to solve a reasonably non-empty input fraction.

In general, whenever you have a rigorous symbolical representation of your input, the best way to approach it is to keep the nice symbolic representation and reason about it in a symbolical way. You would just complicate your life and impoverish your starting knowledge by adopting a neural network approach. 
There exists hybrid approach, but they are still in the early phase.

Anyway, the halting problem is in general undecidable and theoretically there is nothing that can change that.",2,0,0,False,False,False,1639039894.0
rc9jzq,hntiv4t,t1_hntdhsk,"You can't solve any problem with a computer with 100% confidence. A quantum fluctuation or a cosmic ray could flip a bit somewhere and completely change the result or behavior. So really it's just about your level of confidence. Also if you trained a neural network sufficiently, maybe it could give 100% confidence assuming no quantum fluctuations. If you trained a sophisticated enough neural network to genius level then who knows what could happen. The neural network might be so sophisticated that it creates it's own execution engine to run certain parts of the code for example.",-17,0,0,False,True,True,1639028354.0
rc9jzq,hnv8zh0,t1_hnu5jur,"What does provably produce the correct answer mean? If we build a super artificial intelligence that solves certain problems but the proof is so complex that no human could ever understand it, then does that mean it hasn't been proved? Or consider a proof by evaluating all possible inputs and outputs. It may be impossible for a human to ever go through all of the billions of answers to verify it, and yet the computer did verify it. And that doesn't account for the fact that humans are totally unreliable and have a non zero chance of being wrong or making a mistake particularly as things get more complicated. 

""Could an artificial neural network solve the halting problem in the sense the word is used in computability theory? No, unless you're willing to postulate that the different proofs showing the undecidability of the halting problem -- and the more general Rice's theorem -- are wrong.""

This is the crux here. I think the question is theoretically could a maximally intelligent God solve the halting problem? And i don't mean  a maximally intelligent God could create a proof solving the traditional halting problem, but rather give any code to that maximally intelligent being and it could determine whether the code halts or not.  It seems like a very different type of question whether intelligence can solve a problem compared to a direct computational approach. The way I think about it is humans create solutions and proofs in math, using intelligence, that would never be achieved with standard computation. They seem like different problem domains.

""Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.""

That applies to all the high end math proofs humans make today. I could never determine whether the advanced math proofs made today are correct.amd yet I still believe they're correct.",-1,0,0,False,False,True,1639067145.0
rc9jzq,hntbh0g,t1_hntb6yf,"I see what you're saying but perhaps the AI can give an estimate as to how accurate it thinks its results are. And as you add more examples it should be able to get more accurate over time. If you were to scale things up massively could it give you near perfect answers? Quite possibly its hard to say. You know if it can tell you with 99.99999% (six sigma) certainty that it will halt, I'd say that's good enough. At that point we consider scientific theories to be essentially true.

Or could there also a tipping point where it becomes such a genius neural network that it can determine with 100% accuracy whether a program will halt?",1,0,0,False,False,True,1639024208.0
rc9jzq,hntju3a,t1_hntiv4t,"> You can't solve any problem with a computer with 100% confidence

That's a pedantic argument. You _can_ develop algorithms that solve a problem 100% of the time if executed faithfully - the fact that computer hardware is fallible does not mean that the theory is incorrect. It's not about levels of confidence.

> The neural network might be so sophisticated that it creates it's own execution engine to run certain parts of the code

How would this work? If you run a piece of the program to check whether it halts then you can only observe a termination, but not a lack of termination. That is, if it halts, great, you know that it halts, but if it _doesn't_ halt then all you know is that it hasn't halted _yet._",16,0,0,False,False,False,1639028954.0
rc9jzq,hnvvavp,t1_hnv8zh0,"> What does provably produce the correct answer mean?

In case you don't have a CS background, it means formally proving that an algorithm, if followed exactly, will logically and inevitably reach the correct answer. It's similar to a proof in mathematics.

If you do have a CS background -- as you probably do considering the understanding you show -- you knew that already, so I'm not sure what the point of the question is.

> If we build a super artificial intelligence that solves certain problems but the proof is so complex that no human could ever understand it, then does that mean it hasn't been proved?

Okay, so I'm assuming we're talking about an ANN producing a proof about something rather than just producing yes-no answers to the question of whether a program will halt.

Can you directly prove that the proof (supposedly produced by the AI itself) is valid? Or that the proof has been generated using a method that's formally proven to only produce valid proofs? If the answer to either of those is yes, then I would generally consider it proven even if we can't directly understand the proof itself, if the methods used for either producing or checking the proof were formally proven to be correct. That is, of course, with the usual caveat that extraordinary claims require extraordinary evidence, and that we should carefully check the validity of whatever logic was used for supposedly vouching for the validity of the proof.

However, it's worth noting that artificial neural networks don't generally work in such a way that you could formally prove anything about the validity of their results. They just empirically seem to work for a bunch of problems. So, if we're dealing with an ANN, we can't prove that the method (the ANN itself) with which the supposed proof was found was guaranteed to only produce valid proofs, and so the proof that it produces would need to be validated in some other way. That could be either by humans understanding the proof and meticulously checking it for errors, or using automated theorem proving. (Automated theorem proving is then again an undecidable problem in the general case, although it's decidable for some subset of possible inputs, so YMMV.)

However, the point is kind of moot if we're talking about perfectly solving the halting problem. It has been proven to be undecidable in the general case, at least in the sense that for a hypothetical algorithm that purports to solve it, it's always possible to construct an input for which it fails.

You might be able to have a neural network that produces what empirically seem to be correct answers to various instances of the halting problem, with or without any formal proof of anything. You might be able to have a neural network produce a formal and provable theorem about *which* kinds of special cases of the halting problem are decidable. But you wouldn't be able to have a neural network that provably (in the formal sense) correctly answers the halting problem for any and all instances of it.


> This is the crux here. I think the question is theoretically could a maximally intelligent God solve the halting problem? And i don't mean a maximally intelligent God could create a proof solving the traditional halting problem, but rather give any code to that maximally intelligent being and it could determine whether the code halts or not.

That's kind of getting into metaphysics. I don't think there will be a valid answer to such a question, any more than to any other question regarding what results from omnipotence.

> It seems like a very different type of question whether intelligence can solve a problem compared to a direct computational approach. The way I think about it is humans create solutions and proofs in math, using intelligence, that would never be achieved with standard computation. They seem like different problem domains.

That's a valid question to think about. Humans, and actual intelligence (whatever that means), do usually arrive at solutions in a different way than direct computation.

I don't think that necessarily helps a hypothetical AI formally solve the halting problem, although I guess I kind of see where you're coming from. Yes-no answers to the halting problem could hypothetically be accompanied by a proof or reasoning of why a program always halts or why it doesn't, and an intelligence that doesn't (superficially speaking) work by means of classical mechanical computation might be able to creatively come up with answers and proofs that a classical algorithm wouldn't.

The problem is that, if whatever produces those answers and proofs *doesn't* do it systematically, e.g. by enumerating all possible proofs, you can't really prove that it will always be able to find one for *all* instances, even if the answers could be shown to be correct about the ones for which it does. The same is of course true of humans.

> ""Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.""
>
> That applies to all the high end math proofs humans make today. I could never determine whether the advanced math proofs made today are correct.amd yet I still believe they're correct.

Of course, but I don't think that's the same thing. *Someone* has been able to present those proofs, and they've been independently reviewed by others with the required expertise. While people might be able to understand parts of why a nontrivial artificial neural network behaves the way it does, large ANNs are largely black boxes to *everybody*.",5,0,0,False,False,False,1639075814.0
rc9jzq,hnzibxz,t1_hntbh0g,"Mathematical proof is not ""good enough"" if it does not cover 100% of the cases.",3,0,0,False,False,False,1639144085.0
rc9jzq,hntbsji,t1_hntbh0g,"Especially if it's code written by humans. It would potentially pick up on an inexperienced coder or common mistakes leading up to a halt. 

Could save a lot of computational power if it think 99% it will halt and just save the full test for codes it thinks might actually go through.",1,0,0,False,False,False,1639024375.0
rc9jzq,hntm9sl,t1_hntju3a,"Well in reality and in all of science theories come down to levels of confidence. Even in imaginary land there is no such thing as absolute truth if you want to get into philosophy because you would never know with absolute certainty that an algorithm can solve a problem 100% of the time if executed faithfully. Absolute confidence is a myth. The point is not that the theory is incorrect but rather at a certain level of precision and accuracy become indistinguishable from absolute certainty.

And a neural network can emulate almost any complex function. So it's not that it would just run the code blindly but rather execute certain portions of the code if it seems relevant and use advanced heuristics to make determinations. With enough training, neurons, and architecture sophistication a neural network might be incomprehensibly intelligent on determining whether code will halt or not. I can't really tell you what kind of strategies a neural network with an IQ of 10000 would come up with obviously but there may be a point where it can tell you with maximal confidence for most halting problems, or perhaps all.",-15,0,0,False,True,True,1639030523.0
rc9jzq,hntc71k,t1_hntbsji,"Sounds almost like Microsoft Clippy for programming.

> Looks like you're trying to set up a CRUD server! Can I help?",3,0,0,False,False,False,1639024588.0
rc9jzq,hntn7wy,t1_hntm9sl,"If we assume that inductive and deductive logic can reveal truth, then there is such a thing as absolute confidence through mathematical proofs. Algorithms can be proven in this theoretical domain, even if their implementation in hardware is imperfect. If you do not agree that objective truth exists at even this level, then we don't have enough shared axioms for this conversation to go anywhere.

I can't engage with your second paragraph, because it hinges on whether we consider heuristics to be a valid solution to the halting problem. Since I believe a solution is only valid if it is deterministic, and you appear to hold that determinism does not exist, we're at an impasse.",8,0,0,False,False,False,1639031159.0
rc9jzq,hntv6ub,t1_hntn7wy,"A neural network can be seeded to run deterministically if you run it on a perfect computer that never makes mistakes. It can be deterministic. With the same seed and the same inputs it would arrive at the same result each time. Heuristic is not necessarily a subset of non deterministic. There is no good demonstration or proof that a heuristic is unsuitable for coming up with solutions. If you did have a proof of that I would be very interested to see that.

But saying that there is absolute confidence in mathematical proofs might be valid if the entity proving the math was perfect. Imperfect human beings cant prove anything to absolute certainty because humans aren't 100% reliable. As soon as you accept the inherent unreliability of human beings, and say each mathematician has a 1 in 1000 chance of making a mistake in validating or interpreting a proof, then there is still a non zero chance that all proofs have a mistake or error. The problem in your assumption is that human beings are incapable of using logic perfectly to arrive at a deduction with 100% certainty. It is standard in philosophy to go with maximal certainty e.g. the maximum certainty a human brain is capable of.",-15,0,0,False,True,True,1639037112.0
rawuw6,hnkzxwu,t3_rawuw6,"had a spare camera in my cubicle 

used it to detect faces and send a desktop notification 

used it to switch to work from watching random videos on YouTube when someone was around",111,0,0,False,False,False,1638877595.0
rawuw6,hnl67ae,t3_rawuw6,"I work with ECG signals. My company has TBs of  labeled data from 30+ years in the ECG monitoring business. We are using it to build an automated arrhythmia detection system.

In practice, ECG data is just a very large array of 16-bit integers. Labels can be for a single heart beat or for a segment of any length.

There’s some bureaucracy to it because everything is FDA regulated, but I love my job.",40,0,0,False,False,False,1638881617.0
rawuw6,hnmrp5c,t3_rawuw6,Trained an NLP model to generate the onion articles,17,0,0,False,False,False,1638907675.0
rawuw6,hnmxb83,t3_rawuw6,Generated a Frank Ocean inspired album using deep learning,7,0,0,False,False,False,1638910029.0
rawuw6,hnmwmii,t3_rawuw6,im currently training an AI model to predict race winners,6,0,0,False,False,False,1638909736.0
rawuw6,hnmje60,t3_rawuw6,"Getting an RSA algorithm working by hand was hell, but kinda fun",5,0,0,False,False,False,1638904001.0
rawuw6,hnnloab,t3_rawuw6,"I’m still at the beginning of my career but I worked on music in my undergrad. My latest project visualizes emotions in non-Western music.

Open source on [GitHub](https://github.com/nhstaple/feelskunaman) 🎸. Anyone can use it you just need SpotifyAPI keys.",4,0,0,False,False,False,1638920643.0
rawuw6,hnnnd2z,t3_rawuw6,"Most recent project was object detection and instance segmentation for orthopaedic x-rays. We were looking to detect certain abnormalities. A big part of the project became generating synthetic data for training since medical imaging data is so expensive (and that’s just for the images, not even labelled). At the moment I’ve just started my PhD and am working on explainable AI for healthcare, particularly around generative models since I have some experience with those.",5,0,0,False,False,False,1638921429.0
rawuw6,hnobzos,t3_rawuw6,"As part of a research project, I implemented a module to a legal document management system. When a case file was uploaded, it was ranking and suggesting possible legal precedents that could be used in court, saving intern time.

Nothing fancy, it was just using a bag of words model, with tf-idf, but it was getting the job done as the case files were very similar.",3,0,0,False,False,False,1638933671.0
rawuw6,hnohhwz,t3_rawuw6,"Still in school, so this is an academic project, but I'm working on a project to teach an AI to play Hnefetafl. It's really fascinating seeing and working with the math involved.",2,0,0,False,False,False,1638936638.0
rawuw6,hnox7op,t3_rawuw6,this is an interesting post,2,0,0,False,False,False,1638947242.0
rawuw6,hnl4ysj,t3_rawuw6,[deleted],1,0,0,False,False,False,1638880895.0
rawuw6,hnl1iod,t1_hnkzxwu,You got a git for that? Unironically sounds useful :D,34,0,0,False,False,False,1638878691.0
rawuw6,hnl2k9l,t1_hnkzxwu,Saw something similar on YouTube where some device detects movement (not capturing faces) near the area and the computer screen switches to desktop automatically. It just switches automatically and I thought that was pretty cool.,7,0,0,False,False,False,1638879390.0
rawuw6,hnl1pks,t1_hnkzxwu,Niceee,2,0,0,False,False,True,1638878819.0
rawuw6,hnl7gcq,t1_hnl67ae,That's fantastic! I love the idea of working on research that might save someone's life. Nicely done!,10,0,0,False,False,False,1638882325.0
rawuw6,hnmocsf,t1_hnl67ae,What job title/field would this be considered as? I want to get into something similar.,3,0,0,False,False,False,1638906168.0
rawuw6,hnoh0kq,t1_hnmrp5c,What were the results like?,2,0,0,False,False,False,1638936368.0
rawuw6,hnmxrkt,t1_hnmxb83,lemme hear,4,0,0,False,False,False,1638910224.0
rawuw6,hnpgoge,t1_hnmwmii,[current progress](https://imgur.com/a/QEMeVcF),1,0,0,False,False,False,1638963852.0
rawuw6,hnpg9kn,t1_hnox7op,Didn't get much comments tho :(,2,0,0,False,False,True,1638963544.0
rawuw6,hnl6r6k,t1_hnl4ysj,That's awesome. I hope you do win an award for all the work.,3,0,0,False,False,True,1638881933.0
rawuw6,hnl6leh,t1_hnl4ysj,Would you be able to share some of the publications from your work?,2,0,0,False,False,False,1638881841.0
rawuw6,hnl1ref,t1_hnl1iod,"I'll share it with you it's not on GitHub

it's pretty basic just found a interesting use case for it",17,0,0,False,False,False,1638878854.0
rawuw6,hnn0s2o,t1_hnmocsf,"My title is Data Scientist but since it’s a small company I wear many hats. Data engineering, Software Dev, MLOps, etc. I like that aspect of the job as well (some poole don’t).",2,0,0,False,False,False,1638911437.0
rawuw6,hnpvx4c,t1_hnoh0kq,"Still a work in progress, here’s a shitty proof of concept with an overfitted model (http://67.205.142.210:3000/). FYI it’s a bit slow since I didn’t feel like paying for a GPU machine lol.",1,0,0,False,False,False,1638972728.0
rawuw6,hnnavor,t1_hnl6r6k,They just lost their job,2,0,0,False,False,False,1638915695.0
rawuw6,hnnd3dw,t1_hnl6r6k,what did he say?,2,0,0,False,False,False,1638916692.0
rawuw6,hnl7c5d,t1_hnl6leh,[deleted],2,0,0,False,False,False,1638882261.0
rawuw6,hnn40yf,t1_hnl1ref,Can you also share it with me?,3,0,0,False,False,False,1638912779.0
rawuw6,hnop0bv,t1_hnl1ref,+1 share pls,1,0,0,False,False,False,1638941204.0
rawuw6,hnorgay,t1_hnl1ref,Share please!!,1,0,0,False,False,False,1638942854.0
rawuw6,hnpsfqt,t1_hnn0s2o,I am planning to study AI or computational science. Which is the best way would you say I can work in research programs in scientific/medical fields? Should I apply for internships and slowly build experience in that area?,2,0,0,False,False,True,1638970991.0
rawuw6,hnpryxl,t1_hnnavor,"LOL!! Love it! :)  (not true thankfully, but I love it)",1,0,0,False,False,False,1638970752.0
rawuw6,hnorkig,t1_hnnd3dw,Said that they worked on some really important projects that could end up changing the field of AI. Not sure why the comment got deleted,3,0,0,False,False,True,1638942938.0
rawuw6,hno8sdv,t1_hnn40yf,"you know, for *science*",4,0,0,False,False,False,1638932024.0
rawuw6,hno3yl1,t1_hnn40yf,"will share it in the comments
I'll have to rewrite it but it's pretty easy",2,0,0,False,False,False,1638929577.0
rawuw6,hnr1c9v,t1_hnpsfqt,"While there are some exceptions, the vast majority of research jobs in both academia and industry require a PhD. Sometimes ""research technician"" jobs will be done by people with a [B.Sc](https://B.Sc) or [M.Sc](https://M.Sc); however, they are not usually doing research but assisting with building the technology used by the researchers.

Getting a research technician type job usually some expertise in the software being used, e.g., Python, R, Matlab, etc. So learn to program in software languages used in scientific circles. Also, it is good to have a solid knowledge of analytics and statistics since it comes up frequently.  


Again, there are always exceptions but this describes the majority of such positions.",3,0,0,False,False,False,1638989410.0
rawuw6,hnprwgh,t1_hnorkig,It was getting downvoted into oblivion so I assumed people did not want to hear about my research. :),1,0,0,False,False,False,1638970717.0
rawuw6,hnr2p7o,t1_hnr1c9v,"I'm currently doing my bachelors in computer science, which is the best course would you say will help me achieve my goal?",2,0,0,False,False,True,1638989934.0
rawuw6,hnr2nxn,t1_hnr1c9v,"I'm currently doing my bachelors in computer science, which is the best course would you say will help me achieve my goal?",1,0,0,False,False,True,1638989928.0
rawuw6,hnr3tu0,t1_hnr2p7o,"1. Any courses on AI, machine learning or computational intelligence.
2. Any courses on analytics, statistics.
3. If your university offers an introduction to research as a graduate course, then see if you can get an exemption to take it as an undergraduate. Such courses are usually not very difficult and this would be very helpful.
4. Since computer vision is still big in medical research then courses on computer vision would be good.
5. Any courses that deal with time-series data.

&#x200B;

If you DM me a link to your university course catalogue, then I could probably make more specific suggestions.",3,0,0,False,False,False,1638990362.0
rawuw6,hnr6c17,t1_hnr3tu0,"Thanks for the list. 
My college is pretty avg and doesn't offer any of those course so I'll have to go somewhere else for PG.",3,0,0,False,False,True,1638991323.0
rawuw6,hnr6lvq,t1_hnr6c17,"Ultimately, it is all about having the experience to convince somebody you can do the job. The ""easiest"" (without saying it is easy) is to do a course, but if you can get the experience in another way, then it all counts. I certainly hope you manage to do it. Nothing better than somebody achieving their dreams. :)",2,0,0,False,False,False,1638991428.0
rbng33,ho83f8t,t3_rbng33,"Best I can figure, it comes from [core dump](https://en.m.wikipedia.org/wiki/Core_dump). I still don't understand how ""dumping"" got that particular meaning when it comes to computers, and now that I've noticed that I don't know, it bothers me slightly.",2,0,0,False,False,False,1639296186.0
rbng33,ht2k3h9,t3_rbng33,"Well, I'm assuming Screen Dump comes from the concept of ""core dumping"" which is when a file of a computer's documented memory of when a program or computer crashed.

The term ""core dump"" likely originated in the 1960s when early computers used magnetic core memory. When a running program crashed, all of the data in the entire core was printed out on paper to help with debugging.

Core dumps are generated when the process receives certain signals, such as SIGSEGV, which the kernel sends when it accesses memory outside its addressed space.",2,0,0,False,False,False,1642445213.0
rbng33,ht5w6ru,t3_rbng33,"Thank you, that sounds likely!",1,0,0,False,False,True,1642503917.0
rbng33,ho83ghv,t1_ho83f8t,"**[Core dump](https://en.m.wikipedia.org/wiki/Core_dump)** 
 
 >In computing, a core dump, memory dump, crash dump, system dump, or ABEND dump consists of the recorded state of the working memory of a computer program at a specific time, generally when the program has crashed or otherwise terminated abnormally. In practice, other key pieces of program state are usually dumped at the same time, including the processor registers, which may include the program counter and stack pointer, memory management information, and other processor and operating system flags and information. A snapshot  dump (or snap dump) is a memory dump requested by the computer operator or by the running program, after which the program is able to continue.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",2,0,0,False,False,False,1639296212.0
rb9f0g,hno92pg,t3_rb9f0g,"This is probably more a physics question than computer science question but ones used for my graduate courses have been:

“Optical Waveguide Analysis,” K Kawano and T. Kitoh, Wiley, 2001

“Computational Electrodynamics: Allen Taflove and Susan C. Hagness, Artech House, 2005, (Third Edition)

“Plasmonics”, S. A. Maier

“Surface Plasmon Nanophotonics”, M.L. Brongersma and P.G. Kik, Eds. 

“Principles of Nano-Optics”, Novotny and Hecht 

“Surface plasmons on smooth and rough surfaces and on gratings,” H. Raether 

“Near-field optics and surface plasmon polaritons,” Edited by: Satoshi Kawata

They aren't all computational books and really require rigorous computational math methods course",2,0,0,False,False,False,1638932174.0
rb9f0g,hno8snk,t3_rb9f0g,"A quick google search showed many books on this subject.   
No idea how good any of them are.",-1,0,0,False,False,False,1638932028.0
rajca8,hnjiu2t,t3_rajca8,Take a look at your second and third year courses. You won't have a ton of free time for long.,55,0,0,False,False,False,1638843640.0
rajca8,hnkd3ip,t3_rajca8,[deleted],31,0,0,False,False,False,1638859082.0
rajca8,hninnir,t3_rajca8,I’m a fan of cyber security and hacking. It’s a great addition because I doubt your program will cover it and it’s an amazing skill to have,15,0,0,False,False,False,1638830013.0
rajca8,hnkmh1k,t3_rajca8,learn git. you will thank me later,11,0,0,False,False,False,1638866374.0
rajca8,hnktq2o,t3_rajca8,The missing semester of your CS education: https://missing.csail.mit.edu/,11,0,0,False,False,False,1638872671.0
rajca8,hnky1oo,t3_rajca8,Spend more time on your foundational courses to really *understand*. I don’t understand how you can have a lot of spare time as a first year CS student.,5,0,0,False,False,False,1638876197.0
rajca8,hnjvmfj,t3_rajca8,Get some weird board (not like a pi or arduino) and see what you can do with it,3,0,0,False,False,False,1638849243.0
rajca8,hnlcpbd,t3_rajca8,"Kung fu. Great way to develop discipline, stay fit and not get your ass kicked. You’ll spend most of your career barely moving as a programmer/software engineer so getting in the habit of being active is gonna pay dividends in the long run.",3,0,0,False,False,False,1638885103.0
rajca8,hnlcy1u,t3_rajca8,"What kinds of things do you want to do when you graduate? 

For web dev learn more about socket programming and/or webassembly. That will put you ahead of people who only know how to use existing frameworks and tools.

For AI/ML study math and stats such that you can compete with stats majors or get into a grad program. 

For desktop apps learn the Windows API.

For mobile apps study any mobile app toolkit. 

For embedded learn either VHDL or SystemVerilog using an FPGA. That will make you more competitive with EE and CE majors. 

For OS development do as much bare metal programming as you can in C and assembly and learn all the common hardware interfaces. 

For game development learn some Blender and other 3D tools. 

Basically take your preferred career domain and study information that is useful for it but won't be covered or isn't part of CS.",3,0,0,False,False,False,1638885223.0
rajca8,hnnhkbk,t3_rajca8,Do leetcode problems. It’ll help you get interviews and pass technical interviews,2,0,0,False,False,False,1638918723.0
rajca8,hnjgn1a,t3_rajca8,"ML, basic calc and linear algebra are enough to get started",2,1,0,False,False,False,1638842687.0
rajca8,hnkwgvb,t3_rajca8,Soft skills. That is what is going to make the difference in an interview,1,0,0,False,False,False,1638874968.0
rajca8,hnl2afs,t3_rajca8,"Statistics, get to logistic regression.",1,0,0,False,False,False,1638879209.0
rajca8,hnl4r9f,t3_rajca8,"Just do personal projects - best way to learn and you can put them on your resume. Hey, if one is really good you might just get your own company going.",1,0,0,False,False,False,1638880770.0
rajca8,hnlb5qm,t3_rajca8,"I am also a first year cs student and its amazing how some people have this foreign concept called “free time” , i am studying for my final in 2 hours as i write this…",1,0,0,False,False,False,1638884316.0
rajca8,hnlfknk,t3_rajca8,"Join the debate club, or the boardgame society, or perhaps the rock society, etc.

Maybe volunteer for one of the charitable student organisations?",1,0,0,False,False,False,1638886516.0
rajca8,hnlpqos,t3_rajca8,Data Structures & Algo,1,0,0,False,False,False,1638891035.0
rajca8,hnm6c66,t3_rajca8,Learn how to write unit tests. They'll save you time in the long run.,1,0,0,False,False,False,1638898411.0
rajca8,hnysxj8,t3_rajca8,"It's hard to say when:

1. You don't list what subjects you're currently doing and
2. You don't say what field of IT you want to go into...",1,0,0,False,False,False,1639126074.0
rajca8,hovf4hr,t3_rajca8,Check out pathfinder.fyi and choose hard skills most in demand from there,1,0,0,False,False,False,1639712729.0
rajca8,hnkr9yx,t3_rajca8,"Look at web dev frameworks like Django or Laravel Backend. React or Angular frontend, if you'd consider web dev as a career option.",-2,0,0,False,False,False,1638870547.0
rajca8,hnjryc9,t3_rajca8,"Depends on what you want to do. Where I work a cert for Azure Fundamentals would have been a good step in the door. It is related to CS, but not CS itself.",-1,0,0,False,False,False,1638847600.0
rajca8,hnl9kv3,t3_rajca8,LearnOpenGL.com,0,0,0,False,False,False,1638883485.0
rajca8,hnko8nc,t1_hnjiu2t,Exactly,4,0,0,False,False,False,1638867893.0
rajca8,hnkmhjl,t1_hnkd3ip,this,3,0,0,False,False,False,1638866386.0
rajca8,hnlpwev,t1_hninnir,"Its a whole field, and i think op should at least study network architecture AND operating systems to barely start.",2,0,0,False,False,False,1638891103.0
rajca8,hnkwp11,t1_hnkmh1k,Do universities really not teach git? I don’t imagine there is any software dev job where it’s not a necessity,4,0,0,False,False,False,1638875150.0
rajca8,hnl4zn7,t1_hnktq2o,"You beat me to it. This is the best advice around here. OP master everything on this course. 

Although, if you are on a program worth its salt, unless you are a  genius, your lots of free time has its days counted.",2,0,0,False,False,False,1638880909.0
rajca8,hnm8602,t1_hnky1oo,"With previous experience (and depending on the specific courses they require, as what you’re thinking of may be second semester and/or second year) the first year of CS can be blown through somewhat easily

For example, where I go, the foundational courses are moreso second year. First year they basically just taught us Java and C in preparation for the next courses",2,0,0,False,False,False,1638899212.0
rajca8,hnnwj7w,t1_hnky1oo,"Before my first year I had taken more school maths classes than most of my peers, learned to program apps in Java, and did some basic computer architecture stuff in Minecraft (building a redstone computer). 

Definitely gave me a headstart and more free time in my first year - but the advantage certainly didn't last long! I imagine OP is just in a similar position, or they're just some freakish genius.",2,0,0,False,False,False,1638925855.0
rajca8,hnkcvie,t1_hnjgn1a,"He said he’s a first year computer science student that wants to study things not in the program. Calc and linear algebra are basic things he’s likely already taken, and if he chooses to he’ll do some ML soon.",1,0,0,False,False,False,1638858929.0
rajca8,hnkgbw3,t1_hnjgn1a,is linear algebra course in compsci hard?,1,0,0,False,False,False,1638861440.0
rajca8,hnosw0e,t1_hnlb5qm,I'm finna get a C below for this math 141 final on god,1,0,0,False,False,False,1638943899.0
rajca8,hnsk2ig,t1_hnm6c66,"this is the next obstacle in my journey.

any recommendations for resources that will teach this from the ground up?",1,0,0,False,False,False,1639011524.0
rajca8,hnmt4tr,t1_hnlpwev,"I have to respectfully disagree. I have learned those subjects as needed in my work on cyber security. For example you don’t need any of that to learn about a basic buffer attacks. Once you understand them you can learn a little about paging on the OS to understand w^x protections, but then get right back into hacking. 

I feel like it’s similar to just starting some coding projects and learning about data structures and algorithm complexity when you need to.",1,0,0,False,False,False,1638908278.0
rajca8,hnl8v8q,t1_hnkwp11,My university didn't teach me git. Hell they didn't even teach me DS and Algos properly lol,2,0,0,False,False,False,1638883111.0
rajca8,hnm7zl8,t1_hnkwp11,"Currently in university for CS, nowhere near learning git for class",1,0,0,False,False,False,1638899135.0
rajca8,hnmej2h,t1_hnkwp11,never seen it even once,1,0,0,False,False,False,1638901943.0
rajca8,hnvgh4c,t1_hnsk2ig,"Whatever book or website you choose to learn this from should be fine. It's not hard to learn. It's hard to make it a practice, because once your code works, what's the point of writing tests to see if it works? So sometimes people will write the unit tests *first* \-- you can look up ""test driven development"" for this approach.

I didn't have any resource to learn from -- I just copied the style where I worked. Most of the code already had unit tests, and if you changed anything you had to add a unit test that covered your change.

Python has a built-in unit test framework now, but it didn't have that when I started; so my own unit tests are sometimes a little janky, lol.",1,0,0,False,False,False,1639070020.0
rajca8,hnl9h72,t1_hnl8v8q,"That’s just mind boggling tbh..

Especially as you could teach git alongside other topics.. at very least they should be teaching the basics of it",1,0,0,False,False,False,1638883431.0
rajca8,hnvj786,t1_hnvgh4c,Thank you Mr. Numbers!,2,0,0,False,False,False,1639071069.0
ra4e6d,hng7get,t3_ra4e6d,"Graphics is basically all linear algebra, if you want some motivation you could look at for example ""the raytracing challenge"" which starts with a lot of matrix stuff.

In general: basically any modern subject in maths has some connections to linear algebra, because linear algebra makes a lot of difficult problems ""easy"" (e.g. solving partial differential equations using FEM) and so we try to really find those connections - and given how ubiquitous maths is in programming you can probably find some connections to whatever you want to do (in audio for example: dft is a linear transformation which already leads to matrices and a dft'd signal is a vector).",63,0,0,False,False,False,1638792406.0
ra4e6d,hngdzii,t3_ra4e6d,Your graphics card is basically machine that does all the linear algebra related to rendering graphics on monitor. Graphics card is like a linear algebra calculator.,43,0,0,False,False,False,1638796473.0
ra4e6d,hngf9o5,t3_ra4e6d,"Essentially all machine learning is also done on matrices. Actually, a lot of data science is, like principal component analysis (related to taking eigenvalues and vectors of a covariance matrix). In fact, all data is really just matrices with rows being observations and columns being properties associated with those observations.",29,0,0,False,False,False,1638797173.0
ra4e6d,hngffdo,t3_ra4e6d,It's extensively used in machine learning. Good to build a solid foundation early.,14,0,0,False,False,False,1638797260.0
ra4e6d,hngr220,t3_ra4e6d,"Another application a little bit more theoretical (but really interesting) is quantum computing, almost everything in QC is linear algebra (and probability).

You should also read about the Google page rank algorithm, it has a lot of linear algebra.

Good luck with the subject! Give it a chance because is really beautiful",9,0,0,False,False,False,1638802908.0
ra4e6d,hnh75ft,t3_ra4e6d,"Not directly CS related, but relevant:  if you end up working in decision analytics or project management, a solid foundation in LA helps *a lot*. Basically all optimization problems are most easily solved as matrix manipulations. Need to figure out the perfect combo of manhours, materials, and profit for different products/systems? You can do that in a single, fairly simple, simplex tableau.",8,0,0,False,False,False,1638809498.0
ra4e6d,hng170d,t3_ra4e6d,"https://youtu.be/rowWM-MijXU
Watch this. Linear algebra is like epitome of mathematical applications today. Almost everything you can think of has linear algebra in it some way or the other. It is extremely abstract but hold onto it.",25,0,0,False,False,False,1638787561.0
ra4e6d,hnghc9b,t3_ra4e6d,"I failed linear algebra at university and had to re-take the exam; not because I’m bad at math (I got top grades in all our other math courses) but like you I struggled to connect it to the rest of the curriculum. I knew it had a connection to 3D graphics but we weren’t doing any of that, nor was I planning a career in game engine development.

Fifteen years later I’m building platforms for machine learning and I have done a couple of Coursera courses in ML as well, and let me tell you, linear algebra rocks. There are plenty of software engineering careers where you won’t miss it, but if you think ML/AI sounds like something you would like to explore, you should dive into linear algebra as soon as possible.

The Youtube channel 3Blue1Brown has a great video series about it: https://youtu.be/kjBOesZCoqc",7,0,0,False,False,False,1638798275.0
ra4e6d,hnh6cy7,t3_ra4e6d,"I'll also throw in scientific computing. Scientists used to wonder if the universe would expand forever, equalize, or eventually shrink back down into another big bang. To answer that question they basically did one giant matrix multiply over measurements of the cosmic background radiation. Everything from chemistry to cosmology uses linear algebra everywhere. Many problems have ""perfect"" solutions (like the traveling salesman problem or laying out wires on a computer chip). Those problems can be turned into a system of equations, stuck in a matrix, and solved using a bunch of the stuff you're learning now.

I know people always talk about how important math is to CS. The reality is that I rarely deal with most math in my work. The exception is linear algebra and statistics, those are everywhere. Some professors once tried to identify the most important patterns in computing so they could build specialized chips. Linear algebra was one of only seven core computations that made the list. I could go on. The point is, not only is math generally useful in CS, you've actually chosen the single most relevant field of math to ask about!",3,0,0,False,False,False,1638809189.0
ra4e6d,hngop5l,t3_ra4e6d,"As mentioned by others, linear algebra is vital to graphics programming.

A pragmatic example would be a list of 3D vertices that make up a cube. How would you transform the cube? Transformation matrices.

Other examples are intersections, raytracing, and quaternions.",2,0,0,False,False,False,1638801855.0
ra4e6d,hnhu3te,t3_ra4e6d,"There are two aspects that motivate Linear Algebra in CS (at a high-level):

1. So, so, so, so many real-life computational problems can be formulated as a linear algebra problem; often this is a) some model of a real-world process or b) a collection of data, both of which can be structured in terms of LA objects (i.e., vectors, matrices, & tensors).
2. Modern hardware can do matrix operations \*incredibly efficiently\*. This is largely a product of decades of computer architecture work and optimized implementations of algorithms. A lot of this boils down to data reuse/locality and vectorization.

TL;DR: When you use 1) to represent a problem as LA structure, you can leverage 2) to solve that problem very efficiently (at least, as efficient as your algorithm will allow).",2,0,0,False,False,False,1638818471.0
ra4e6d,hnhyhil,t3_ra4e6d,"Machine learning, 3D graphics; game design.

I actually wanted to get more in depth into it myself but have been way too busy my junior year. Hopefully next semester in my senior year I can learn some from discrete math. Haven't taken a math class in 2 years.

I'm not awfully familiar with audio programming but I don't think it would require a lot of interest and knowledge in linear algebra, maybe some in calculus.",2,0,0,False,False,False,1638820194.0
ra4e6d,hnizunv,t3_ra4e6d,"Game development is a major (and fun) one. Representation of graphics and transformations of them (rotation, translation, etc) is usually done through vectors/points and 3D/4D matrices. Jason Gregory (lead dev for Naughty Dog) has a whole chapter about it in his [book](https://www.gameenginebook.com/) - I highly suggest you read at least the “3D Math for Games” chapter for some real world use cases for linear algebra.

(source: I’m a game developer at a studio, and I can confirm linear algebra actually matters in my daily job)

Machine Learning and Deep Learning are also big use cases but other comments have mentioned it already.",2,0,0,False,False,False,1638835235.0
ra4e6d,hnjymwp,t3_ra4e6d,You mentioned audio programming. I’d look into digital signal processing (dsp) as that’s what a good chunk of audio programming really is under the hood. Things like the discrete Fourier transform and related operations involve things you’d learn in your linear algebra course.,2,0,0,False,False,False,1638850641.0
ra4e6d,hngfge3,t3_ra4e6d,It's extensively used in machine learning. Good to build a solid foundation early.,4,0,0,False,False,False,1638797275.0
ra4e6d,hnggdu5,t3_ra4e6d,"Hi there I'm happy to say one of my friends has created a really cool website to help with linear algebra and webgl.

[http://www.invectorize.com/home](http://www.invectorize.com/home)",1,0,0,False,False,False,1638797773.0
ra4e6d,hnhs4ou,t3_ra4e6d,"Maths’s hugely used in IoT, game development and thereabout, you’ll never know where you’d be in the future, so you’d better learn it now as you have such opportunity",1,0,0,False,False,False,1638817688.0
ra4e6d,hnk735u,t3_ra4e6d,"A matrix, A, can be a transformation of a vector, v.  So for instance There is a matrix that can rotate v by 90 degrees.  This is useful for many things.  In computers, it is useful for graphics.  It's useful in quantum computing as well.  There is something called a hadamard matrix which puts a quantum vector (v) into superposition of multiple states (you can look that up) which in certain systems in quantum programming is a rotation of the state vector on what's called a Bloch sphere  (coordinate space for a 2 level qubit) .  

https://en.wikipedia.org/wiki/Rotation_matrix

https://en.wikipedia.org/wiki/Hadamard_transform  ​

That being said, a vector can represent a lot of things.  It just depends what space it is in and its axes. 

The same goes with computer graphics.  On your screen are pixels that are each composed of RGB filters.  To display a large range of colors the human eye can see, each sub-pixel will have some value for how bright it needs to be in combination with the other sub-pixels for a specific pixel color.  This can be in the the form of a vector and objects created with the pixels can be moved around by linear algebra and matrix transformations.",1,0,0,False,False,False,1638855222.0
ra4e6d,hnkx7xo,t3_ra4e6d,"Gilbert Strang has an excellent book on the topic of linear algebra ([Introduction to Linear Algebra ](https://www.amazon.co.uk/Introduction-Linear-Algebra-Gilbert-Strang/dp/1733146652/ref=asc_df_1733146652/?tag=googshopuk-21&linkCode=df0&hvadid=500859832694&hvpos=&hvnetw=g&hvrand=2832046874964718038&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=1006523&hvtargid=pla-1235394973160&psc=1&th=1&psc=1)).

As many have pointed out here, matrices and vectors have a lot of applications in graphics. 

However, it goes far beyond that. In fact, linear algebra turns up almost everywhere, eg physics problems, data science, optimisation, etc. It is a topic that is well worth having a decent knowledge of, whether you are a mathematician or a computer scientist. 

I study as a mathematician (doing my PhD), and I am continually surprised at how often numerical linear algebra springs up. 

[Here is a free PDF of Gilbert Strang's book](http://libgen.li/edition.php?id=138573030). This is the 5th edition, though a 6th has recently been released. I'm not sure what the changes are. 

What is especially wonderful about the Gilbert Strang book is that it has a selection of sister lectures from MIT available free online on YouTube. So, if you read a topic in the book and struggle on the exercises, you can then turn to the lecture series for additional information. I think its a great way to learn. There's a chance it goes a little deeper than you might need, and might be more generalised, but at least you can get an appreciation for the breadth and importance of this topic!

Edit: you are also likely to run across NLA for things like image reconstruction and file compression, which are computer science topics :) let me know if you want more info. You talk briefly about enjoying audio - well, the discrete fourier transform is all linear algebra, and so it has applications in removing white noise etc (as mentioned by another commenter)",1,0,0,False,False,False,1638875562.0
ra4e6d,hnitfwt,t1_hngf9o5,"I was in the same boat. Took the class and didn’t get the point. Since then, I’ve taken a computer graphics class, a machine learning class, and a deep learning class and have really regretted not taking it more seriously. But I do think they should have made a point of explaining applications at the time…",7,0,0,False,False,False,1638832445.0
ra4e6d,hnw7av3,t1_hnh6cy7,Do you have any more information on those seven core computations? Sounds interesting and I'd be curious to learn more.,2,0,0,False,False,False,1639080604.0
rb71fe,hnmtvn8,t3_rb71fe,RL?,5,0,0,False,False,False,1638908576.0
rb71fe,hnn3d5n,t1_hnmtvn8,"""real life"" I guess, but... honestly I have no idea where the assumption that DP in school is different from the real deal comes from.",4,0,0,False,False,False,1638912508.0
rb71fe,hnosr51,t1_hnmtvn8,My guess is reinforcement learning. That kinda makes sense.,2,0,0,False,False,False,1638943797.0
rb71fe,hno7mys,t1_hnn3d5n,"You rarely need to create algorithms yourself ""in real life."" You mostly find the ones you need on the internet.

In the rare cases that you do need to, I don't see why it should be different from you learned in school. Reality is often more complicated, but you learn this stuff for a reason.",1,0,0,False,False,False,1638931431.0
rb71fe,hnosxcy,t1_hnosr51,"Yea, that makes more sense than ""real life""",2,0,0,False,False,False,1638943928.0
rahxnv,hnii59h,t3_rahxnv,Since you finished formal automata and still interested in theory part of cs you can continue with Computability Theory then Complexity Theory.,3,0,0,False,False,False,1638827791.0
rahxnv,hnjlhka,t3_rahxnv,Where/how did you study Automata Theory? Was it in college or some online course/self-teaching? I'm very fascinated in the subject but am always worried about missing key information while self-teaching.,2,0,0,False,False,False,1638844776.0
rahxnv,hnjxyez,t3_rahxnv,"So you now seem to have ""half"" a understanding of what would be taught in my class on theoretical computer science. ""Basic"" things you might be missing (based on a cursory skim of that book's chapters)  


* The Myhill-Neorde theorem
   * See https://bosker.wordpress.com/2013/08/18/i-hate-the-pumping-lemma/
* A bit more complexity theory 
   * Thinking about NP as ""verifiable problems""
* More undecidability/computability theory
   * (Many-One) Reductions
   * SNM and Recursion Theorem
   * Index Sets
   * Hierarchy of uncomputable problems / Arithmetical Hierarchy
      * That one in general is interesting since it's one of the more graspable connection between logic and computability theory, if that interests you  


Unfortunately, I can't really recommend any good book. But the wikipedia articles on each of them are somewhat useful, though of course they don't offer practice exercises. Perhaps the open logic project might have more, especially on ""undecidability"" theory.",2,0,0,False,False,False,1638850320.0
rahxnv,hnjt6et,t3_rahxnv,A natural next step is compilers.,0,0,0,False,False,False,1638848141.0
rahxnv,hnjlt1p,t1_hnjlhka,"College. Just finished my semester today on it. We used Peter Linz’s text book. 

We followed the book very closely. The teacher was all about us learning and didn’t care about our grades, so he challenged us a lot on the assignments and exams. Some of the problems on the assignments would take ~2-3 hours to complete (like devising Turing Machines, PDAs, DFAs)",2,0,0,False,False,True,1638844915.0
rahxnv,hoscpbo,t1_hnjlt1p,besides the mentioned computability and complexity theory you can also double down. There are flavours of automata that handle infinitely long words (e.g. Büchi Automata) which can be used to do software verification.,2,0,0,False,False,False,1639667089.0
raflir,hni08xe,t3_raflir,"Typically a program/malware is impossible to run without supported environment or libraries, hence you have now two options:

1) write a program in C since it's almost supported by all OS then let it silently install the required environment/libraries. After that it can run the malware.

2) most of the systems support some high level languages by default and are installed beforehand, like, .NET for windows which run C#, vb.net or python for linux.

As for the libraries it's pretty simple, you can make two files inside each other, the first one checks for libraries and install the missing ones then it unpack the second and run it.

Edit: hmm it's cs subreddit, I think r/malware would be better",6,0,0,False,False,False,1638820878.0
raflir,hnmvmtu,t3_raflir,"I have heard of payloads that included the interpreter with them. For example you’d have a power shell script that has the b64 encoded binary of the python interpreter. The powershell would decode the binary then execute it with the python as input. This assumes you know the computer architecture for the binary and that it runs powershell, but those are easy enough guesses.",2,0,0,False,False,False,1638909310.0
raflir,hni1dj4,t3_raflir,"Think of them as ""self-contained"" programs (in order to run), that may connect to a online host (a server) in order to carry out a certain function.  Of course depending on the malware, will dedicate what it does, if it needs to connect to an external online host, etc...",2,0,0,False,False,False,1638821322.0
raflir,hni1nyl,t1_hni08xe,"I see, thank you",1,0,0,False,False,True,1638821437.0
raadci,hnhxy5x,t3_raadci,Tree structures are very common when you analyse text. For example to solve a mathematical equation or interpret a code file. You could look at [this](https://en.m.wikipedia.org/wiki/Parse_tree) but I would also recommend a text on computer languages like SICP.,2,0,0,False,False,False,1638819982.0
raadci,hnhxzmq,t1_hnhxy5x,"Desktop version of /u/forsasateri's link: <https://en.wikipedia.org/wiki/Parse_tree>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",1,0,0,False,False,False,1638819998.0
raadci,hninvze,t1_hnhxy5x,thanks,1,0,0,False,False,True,1638830109.0
ram49s,hnj4gx2,t3_ram49s,"Provider provides, sure.  Marginally I could see the utility of breaking up interfaces in this way, for composability, but broadly, the implementation sure seems messy.  OOP is a helluva drug.",4,0,0,False,False,False,1638837329.0
ram49s,hnkj7tv,t3_ram49s,"Is there a behaviour behind those providers?   
The pattern here is to inject different strategies (usually interfaces, btw, not classes) where some could apply. If you are just returning a fixed value, it has no meaning.

Title provider makes sense if you have or might have several ways to generate a title out of... Something. But it should provide a function for that, so that different implementations could provide different behaviours.

Height provider, for a human, doesn't make a lot of sense as its just a fixed value.",2,0,0,False,False,False,1638863701.0
ram49s,hnkm4fs,t3_ram49s,reinventing swift protocols,2,0,0,False,False,False,1638866076.0
ram49s,hnj6qox,t3_ram49s,"I'm soon to be a junior engineer and to me it looks like he knows nothing about OOP. It's impossible that someone had to sit through a faculty-level of OOP course and do assignments by using inheritance **that** way. That is literally impossible. Also, being able to pass OOP course **without** using inheritance is **also** impossible. Ultra necessary basic concept, and honestly him **not even knowing that it exists** is very strange. So my honest advice for him is to get some quality book and exercise to get his way of thinking right. 

I don't think it should be your job to teach him this and it's something that he needs to use a bit of his time to sit it through and cram some examples to get into his way of thinking. If he's using it this way than, to me, it beats the entire purpose of OOP.",0,0,0,False,False,False,1638838359.0
rabwtu,hnhnoyh,t3_rabwtu,"Yes of course it is worth it! most practices are the same for more than 40 years.  
Unless there are new technologies on the matter, it won't make much difference. And even if there are, it doesn't matter so much, it would still be worthwhile. You will only have to do some ""upgrades"" on what you know.",5,0,0,False,False,False,1638815955.0
rabwtu,hnhnctt,t3_rabwtu,Yes.,5,0,0,False,False,False,1638815823.0
rabwtu,hnk6euz,t3_rabwtu,Wow this is a good website! Information are good! It is not old at all!,2,0,0,False,False,False,1638854827.0
rabwtu,hnkcrfi,t3_rabwtu,"The same theory I learned in 2000 is totally valid today. While programming languages and frameworks come and go, the theory and engineering principles are the same. The worst part is people want to reinvent the wheel all the time instead of reading and understanding and using what people already figured out 30-50 years ago. So, go through those textbooks your might learn a thing or a dozen.",1,0,0,False,False,False,1638858850.0
r9bx1d,hncih7u,t3_r9bx1d,Switch != Bridge,28,0,0,False,False,False,1638726478.0
r9bx1d,hncx22l,t3_r9bx1d,"What this article also shows is how useful personifying things can be. If you think of programs as actors or agents and personify them/have conversations with them, it makes it easier to conceive of them.",5,0,0,False,False,False,1638731787.0
r9bx1d,hnb21y8,t3_r9bx1d,Much appreciated.,4,0,0,False,False,False,1638696551.0
r9bx1d,hnb8hnx,t3_r9bx1d,This is a gem. Thanks!,2,0,0,False,False,False,1638701873.0
r9bx1d,hne8jkj,t3_r9bx1d,Are there more kind of resources about networking for developers or beginners?,2,0,0,False,False,False,1638749819.0
r9bx1d,hneyrov,t3_r9bx1d,"Awesome resource, thanks!",1,0,0,False,False,False,1638761458.0
r9bx1d,hqwygwn,t1_hncih7u,"Sometimes, people don't explain stuff well enough for me to understand, making this programmed type of speech the chearest possible means for getting information across.

 Or, put simply,


 dataLoss(speechstyle = ""program-esque"") < dataLoss(speechstyle != ""program-esque"")",1,0,0,False,False,False,1641113128.0
r9qvnk,hnepknk,t3_r9qvnk,"I don't think that you should worry about your lack of programming experience. Your role is to lead the team, not do all of the work yourself. Talk to the more experienced programmers and get their opinions about how best to structure and split up the code. Lean on them to help with the more technical aspects and focus on the leadership stuff, like making sure everyone has something to do, making sure people are comfortable with their tasks, setting up tools to plan the project like Trello, and that kind of stuff.

For regularity of meetings, every week is best so that people stay engaged and keep doing stuff with the project. You can always add more to it if it gets done early, but projects always take longer than you'll originally expect them to. As an example of stuff to add, if it's a website, you can always do more to the ui, like making it both computer and mobile friendly.

To help with the skill gaps, a good strategy can be to pair up experienced and inexperienced people to work on tasks together. Pair programming is slower, since you have two people doing the same thing, bit in the long run it will help everyone get to the same skill level.",3,0,0,False,False,False,1638757310.0
r9qvnk,hndrhpp,t3_r9qvnk,"Maybe you should step down as a team lead, if you really feel you aren’t experienced enough. I can guarantee you that eventually your inexperience will surface through one way or another, and someone more skilled may challenge your position as team lead.",6,0,0,False,False,False,1638742868.0
r9qvnk,hndsbn8,t3_r9qvnk,"By the sound of it, the project you’ll be working on is somewhat already decided. In that case, try setting up an introductory meeting to get to know everyone and discuss skill sets, etc. Then, start planning the project with roadmaps, visualizing the code and its dependencies within the scope of what you’re building. This visualization can be a guide, as it provides tasks of what needs to be built. Some things will be more complicated, while others will be much more simple. As long as you keep track of how the different tasks interconnect and maintain consistency, you can split these tasks amongst the group by skill and interest.

There’s a variety of tools out there that can help with this kind of thing, such as GitHub with its repository projects functionality. Try to outline the project, divide the tasks, and approach it systematically (for example, start off with pseudocode before getting more complicated). Don’t forget regular check-ins to make prevent issues with merging code later on.",1,0,0,False,False,False,1638743198.0
r9qvnk,hnfsf12,t3_r9qvnk,"Focus on figuring out the leadership stuff. The thing to remember is you’ve got 9-10 coders and only one leader. So you should start with assuming you’ll be doing none of the coding. Once you’ve got a handle on the management you might find that there’s time to schedule some coding to yourself. Being leader doesn’t mean you have to have all the ideas yourself; lean on those members who have more experience, make it clear you’re listening to them.",1,0,0,False,False,False,1638779976.0
r9qvnk,hnet754,t1_hnepknk,"Just to add, pair programming is great not only to help the less skilled, but it helps the more experienced programmer learn how to communicate concepts and ideas clearly and helps them better their own understandings of how things work when they have to explain it and potentially get questions they never asked themselves.",1,0,0,False,False,False,1638758939.0
r9qvnk,hnenef5,t1_hndrhpp,"I completely disagree with this sentiment. This is a valuable learning experience that OP should go through. It's a safe project to learn with, since it's just a highschool club project that, as he puts it, should be simple. Stepping down should only be considered if OP feels like the amount of work it takes to be a team lead is negatively affecting them outside the club.",3,0,0,False,False,False,1638756336.0
r9qvnk,hnewt85,t1_hnenef5,"I can tell you as a high school student, appointing someone inexperienced into a leadership position will only spur a lot of negative sentiment, especially from the experienced folks you want to retain.",2,0,0,False,False,False,1638760584.0
r9qvnk,hnesx7x,t1_hnenef5,"Or negatively effects the team experience. Just because it helps OP learn doesn't mean it helps the other 9 people trying to gain experience as well. Not saying OP should or shouldn't step down, just that it really depends on the group as a whole what's truly best.",2,0,0,False,False,False,1638758814.0
r95hk3,hnaah2v,t3_r95hk3,SVG’s are descriptions of images. So the computer can generate an arbitrarily good pixel display of the description. See Wikipedia on Rasterization,15,0,0,False,False,False,1638678247.0
r95hk3,hnambx7,t3_r95hk3,"It’s math. Take a circle with its center at (xc, yc) and with radius r. Now take a point that you’re interested in, (x, y) and check if it’s part of the circle, how do you know? Well is the point further than the radius of the circle from the center of the circle or not? In other words sqrt((x - xc)^2 + (y - yc)^2 ) <= r. If that statement is true the point is in the circle, if it’s false it’s outside of the circle. When you zoom in you check each and every pixel, using its corresponding coordinates within the image, color them appropriately.

There’s equations for rectangles, rotated rectangles, triangles, ovals, various curves, spheres, cubes, etc etc etc.",9,0,0,False,False,False,1638684901.0
r95hk3,hnaajq7,t3_r95hk3,"Its a lot less complicated than you're assuming.

**S**calable **V**ector **G**raphics format stores image data not as pixels, but as shapes and curves (and colors and widths, etc). An SVG viewer **draws** the shapes to display the image.

Here's a simple example: below is literally the SVG code for a circle with radius 50, centered at position (100, 100), on a canvas that is 200x200 units in size.

&#x200B;

    <svg width=""200"" height=""200"" xmlns=""http://www.w3.org/2000/svg"">
      <g>  
        <title>Layer 1</title>  
        <ellipse ry=""50"" rx=""50"" id=""svg_2"" cy=""100"" cx=""100"" stroke=""#000"" fill=""#fff""/>  
      </g>  
    </svg>

If your more curious, heres the full SVG specification, which defines the SVG code, talks about the document object model (DOM, also a part of web page rendering), and the rendering model: https://www.w3.org/TR/SVG/",4,0,0,False,False,False,1638678285.0
r95hk3,hnaf5np,t3_r95hk3,"Bézier curves, [https://en.wikipedia.org/wiki/Bézier\_curve](https://en.wikipedia.org/wiki/Bézier_curve), can represent curves using straight lines. A straight line is defined by its endpoints, which are just numbers. The section of the Wikipedia article on constructing the curves is what I was most curious about the first time I looked it up.",2,0,0,False,False,False,1638680689.0
r95hk3,hnavvl5,t3_r95hk3,math.  vector images are based on math.  computers are not bad at doing math and drawing it on the screen.,2,0,0,False,False,False,1638691555.0
r95hk3,hnbb4do,t3_r95hk3,It's about vector graphics: https://en.m.wikipedia.org/wiki/Vector_graphics,1,0,0,False,False,False,1638704027.0
r95hk3,ht2lguo,t3_r95hk3,"Simple answer: No.

A computer cannot even draw a mathematical point (which has no dimensions whatsoever)—in fact one cannot exist in any real sense. A mathematical circle has no height and no width.

No perfect mathematical shapes exist in nature, although some shapes come very close. Even when a physical law applied to a certain situation predicts a perfect mathematical shape, there are always extra factors that are not considered by the physical law that mess things up: friction, air resistance, relativistic corrections, quantum uncertainty, atomic granularity.

An example of something very close to a perfect circle in nature is the orbit of Venus about the Sun. This near perfection is attained because:

1. The initial velocity of Venus was such that its orbit is nearly circular and not as elliptical
2. There is very little air resistance or friction in space
3. The Sun is so distant from Venus and so round that it acts almost exactly as a point source of gravity
4. Venus is so big that quantum effects are very small
5. Sun's gravity is weak enough that Newton's law of gravitation is reasonably accurate

But, all of these statements are not perfect, so there are still many small sources of deviation from a perfect circle, even for Venus' orbit.

Consider trying to draw a perfect circle on paper with graphite. Even if you were able to use an AFM tip, laser sensors and a feedback loop to perfectly space every single carbon atom to form the circle, you still have the fact that the circle is made out of atoms. Zoom in enough on the circle and it's not smooth anymore because of the profile of the atoms.

(This moved fast from CS to Science)",1,0,0,False,False,False,1642445732.0
r95hk3,hnclroe,t1_hnaah2v,"Agreed [rasterization](https://en.m.wikipedia.org/wiki/Rasterisation) is the general name of the process that computers use to turn mathematical description of shapes (as stored in SVG) into actual pixels for display. Note that this occurs ONLY when you view the result and takes into account how zoomed in you are when you are viewing it. So if you zoom in more, the rasterization process is re-run. This is what allows you to zoom as much as you want and never run out of “resolution”.

Also to answer the specific request for the how this goes for a circle, this [Wikipedia article](https://en.m.wikipedia.org/wiki/Midpoint_circle_algorithm) shows one of the classic algorithms for that case.",1,0,0,False,False,False,1638727695.0
r95hk3,hnazrmc,t1_hnaajq7,Key word: scalable,2,0,0,False,False,False,1638694641.0
r95hk3,hnbb57a,t1_hnbb4do,"**[Vector graphics](https://en.m.wikipedia.org/wiki/Vector_graphics)** 
 
 >Vector graphics, as a form of computer graphics, is the set of mechanisms for creating visual images directly from  geometric shapes defined on a Cartesian plane, such as points, lines, curves, and polygons. These mechanisms may include vector display and printing hardware, vector data models and file formats, and software based on these data models (especially graphic design software, Computer-aided design, and Geographic information systems). Vector graphics are an alternative to raster graphics, each having advantages and disadvantages in general and in specific situations.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",2,0,0,False,False,False,1638704047.0
r8rl8f,hn79xnc,t3_r8rl8f,"Computer graphics is its own subfield of computer science. Specific course names may vary wildly from school to school. You'd really need to check the course catalogue for any possible course of interest.

Some examples may include:  
Introduction to Computer Graphics  
Computer Graphics  
Applied Computer Graphics  
Application of Computer Graphics

etc.",70,0,0,False,False,False,1638630844.0
r8rl8f,hn7fsfx,t3_r8rl8f,"if you are interested in lighting and how things look, then look for courses focused on: rendering, light transport or material appearance modelling.

if you want to get into computer graphics then it's good to try everything under the umbrella of graphics. the course I have taken and plan on taking are: Computer graphics, rendering, image analysis, digital signal processing, computer vision, geometry processing.
All of these areas overlap in different ways and things learned in one area can be used in another area. there are probably more that I haven't mentioned.",22,0,0,False,False,False,1638633572.0
r8rl8f,hn838uk,t3_r8rl8f,There’s linear algebra involved as well depending how deep you get into computer graphics.,15,0,0,False,False,False,1638643326.0
r8rl8f,hn89vew,t3_r8rl8f,Does the category of computer graphics also include the digital signal processing for the video or is that of another CS category?,6,0,0,False,False,False,1638646066.0
r8rl8f,hn7ugpj,t3_r8rl8f,We have it as a separate subject in our course so it's probably a field of its own.,3,0,0,False,False,False,1638639798.0
r8rl8f,hn9ovua,t3_r8rl8f,"If you're interested in 3D game engines, I will tell you to also become really familiar with (in addition to computer graphics, linear algebra, etc.) computer systems (especially microarchitectures, multithreading, etc.) Newer 3D APIs (Direct3D 12, Vulkan) require a lot more knowledge in these areas than older APIs.

Also, if you can, skip OpenGL. Outside of mobile games and 3D modeling, it's a boomer relic (and unfortunately that's all that schools teach you). If you can, learn Direct3D 12.",2,0,0,False,False,False,1638668112.0
r8rl8f,hna8bnf,t3_r8rl8f,Yeah it was just called Computer Graphics for the subject I did and we used OpenGL to create 3D scenes. But there's a whole library of books for this one subject.,2,0,0,False,False,False,1638677181.0
r8rl8f,hna8wsj,t3_r8rl8f,Learn cpp. Graphics are specifically about vectors and points on the screen. Reflections and shadows are the pixel colors used. If you can program in cpp you be great it. Start learning everything about openGL.,2,0,0,False,False,False,1638677479.0
r8rl8f,hnauyjd,t3_r8rl8f,"Might be an Unpopular opinion, but Dont studdy Computer Science If making games and graphics is your only Motivation. You can take sub courses that specialise in that, but don't go into the world because you like games. 

I have personally experienced this over and over, first year first semester we are packed with students who want to be game developers, by the end of the first semester the attrition rate is like 70%, and it only ends in tears.

But then after you understand how computers work then it gets cool.",1,0,0,False,False,False,1638690863.0
r8rl8f,hnbuspp,t3_r8rl8f,"It is its own category. (Usually)
It's probably the most related to math.
My uni has a class for opengl under the cisc program. It's called ""computer graphics"" (so catchy)
I'd start out double checking you understand geo and calc fairly well, then see if MIT or someone has free courses online. 
Reflection and path tracing and stuff requires a lot of math and a decent bit of experience with matrix math and a good understanding of data structure type stuff from what it seems.",1,0,0,False,False,False,1638716359.0
r8rl8f,hnf247u,t3_r8rl8f,Yes,1,0,0,False,False,False,1638762987.0
r8rl8f,hovfaud,t3_r8rl8f,https://pathfinder.fyi/results/Computer%20Graphics,1,0,0,False,False,False,1639712810.0
r8rl8f,hn7h5nj,t1_hn79xnc,Thanks!,7,0,0,False,False,True,1638634189.0
r8rl8f,hnao6sr,t1_hn79xnc,"Interactive media, creative computing, etc.",1,0,0,False,False,False,1638686110.0
r8rl8f,hn8a3g1,t1_hn838uk,"Yeah, if you're at all interested in university level computer graphics, linear algebra is more or less mandatory.",12,0,0,False,False,False,1638646160.0
r8rl8f,hn8lthy,t1_hn89vew,"DSP is generally an EE field, unless you have a specific example in mind?",9,0,0,False,False,False,1638651174.0
r8rl8f,hn95mpf,t1_hn89vew,"It depends on what you want to do. Images can be thought of as signals of red, green and blue channels and videos are just sequences of images. Performing analysis in on these signals (known as the frequency domain) allows us to get some useful information easier than just looking at the image itself. This is used in computer graphics for things like object tracking and also used in AI as part of the pipeline for things like object recognition.",4,0,0,False,False,False,1638659703.0
r8rl8f,hnacioc,t1_hn9ovua,"> (and unfortunately that's all that schools teach you)

&nbsp;

Used WebGL in our Computer Graphics class lol",2,0,0,False,False,False,1638679286.0
r8rl8f,hn7hpvb,t1_hn7h5nj,My pleasure. Good luck with your studies!,8,0,0,False,False,False,1638634440.0
r8rl8f,hnad28v,t1_hn7h5nj,"Here's an overview of what topics would be covered in such a course: http://www.cs.cornell.edu/courses/cs4620/2017sp/cs4621/

OpenGL resource:
https://learnopengl.com/

WebGL resource: https://webglfundamentals.org/

Cool youtubers to check out:

https://www.youtube.com/c/SebastianLague/videos

https://www.youtube.com/c/IndigoCode/videos",6,0,0,False,False,False,1638679567.0
r8rl8f,hn9o4fh,t1_hn8lthy,"At my undergrad school, we had a CS class in DSP and it did fall under the Computer Graphics umbrella (as Image Processing).",5,0,0,False,False,False,1638667767.0
r8z2ei,hn8unba,t3_r8z2ei,"Commercialized means that you can't produce a product with their API and make money off of it. I assume this also means that you probably can't publish it on the app store, but don't quote me on this.",2,0,0,False,False,False,1638655018.0
r8z2ei,hn9vzzh,t3_r8z2ei,If you release it for free and don’t include any in app purchases you can definitely publish. Allowing people to download the built binary for there device is not commercialization. Even better if you open source it. Saying you can check out my app on the AppStore would be amazing for your resume.,2,0,0,False,False,False,1638671373.0
r8z2ei,hnb2l6s,t3_r8z2ei,"I’m not a lawyer but I believe there’s no widely agreed consensus on what “commercial use” means or where the boundaries of it are. For this reason the “NC” family of Creative Commons licenses are best avoided and generally considered to be incompatible with open source. One of the tests in that scenario is “if you include it on a DVD of software, could you sell the DVD to cover manufacturing costs?” I guess that’s analogous  to your scenario where you have costs to publish an iOS app (active developer subscription) if you attempted to recoup that. To sum up, “it depends”. Might be worth avoiding the potential problems.",2,0,0,False,False,False,1638696995.0
r8z2ei,hn8w8w8,t1_hn8unba,"But I can use it as a personal project, and put it on a resume for sure right?

As for the publishing on the app Store things still aren't clear to me. As long as I'm not monetizing my app, I'm curious if it still falls under ""commercialization"".",4,0,0,False,False,True,1638655704.0
r8z2ei,hndvzgm,t1_hn9vzzh,"Thanks for the clarification!!

>Saying you can check out my app on the AppStore would be amazing for your resume.

YES it really would lol. I've been applying to jobs a lot but my resume just never gets shortlisted for some reason. Plus so many recruiters have this requirement of ""must have at least one app published on the store"".",1,0,0,False,False,True,1638744663.0
r8z2ei,hndvrze,t1_hnb2l6s,Hmmm. Well at least I can use it on my portfolio.,1,0,0,False,False,True,1638744580.0
r8z2ei,hn8wh6m,t1_hn8w8w8,"You can use it on your portfolio, yes. As long as you aren't making money off it, you can do whatever you want with it!

Edit: Commercializing something simply means making it available to buy.",3,0,0,False,False,False,1638655799.0
r8z2ei,hndvph5,t1_hn8wh6m,"I see, that clears it up. Thanks!",1,0,0,False,False,True,1638744552.0
r8upnl,hn8e4oi,t3_r8upnl,"They did it just like modern computers do it. The processor executes a program written using numeric machine code instructions, which are basically a set of digital logic gate configurations that get triggered by specific values. Instruction 01 could be a ""load from memory address into the accumulator register"" instruction, for example, and so when the processor reads an instruction and receives a sequence of bits that corresponds to that instruction, it enters a state where it will next read a numeric address, and then finally it will read the contents of that memory cell and store the value in the accumulator.

Then let's say the next instruction is an ""add accumulator with value"" instruction, and that causes the processor to enter a state where it next reads a raw value, and finally performs a binary addition operation (also implemented using logic gates) with the read value and the value in the accumulator, storing the result in the accumulator.

And so on. This is of course a rather simplified description even by early standards, but it's fundamentally how a computer works. Modern languages always get translated into machine language in the end.",5,0,0,False,False,False,1638647867.0
r8upnl,hn81vev,t3_r8upnl,"A programing language is just a way to interact with the computer's hardware. You can take a look at digital circuits to see how hardware is built to preform specific tasks. If you look at industrial automation and process control you can find many examples of how real world information is captured by computer systems. 
Have fun!",6,0,0,False,False,False,1638642760.0
r8upnl,hn915z3,t3_r8upnl," To understand the classical computer, it's useful to sort of build up to it in complexity. We started with electrical circuits where humans could manually open and close circuits via switches. A switch allows a circuit to store a single bit of information which says either ""the circuit is closed"" or the circuit is open. But what if, instead of a human having to flip a switch, we could create a switch that would open or close a different circuit based on whether or not its own circuit is open or closed. Such as switch is known as a transistor, and through the triumph of the transistor we can go beyond simple circuits and arrive at machines which can perform arbitrary computation by opening and closing circuits in complex patterns.

So however a punch card reader worked physically, it ends up in opening and closing a circuit in a pattern that can then be stored in the open and closed states of the computers internal circuits. Then the computer can use this stored set of open and closed circuits as a starting point and by opening and closing circuits in it's particular pattern( as specified by its instruction set), the computer can execute the stored program. (This might be a slight oversimplification, because real computers might rely on more complex physics for some sorts of memory, but all the logic in a computer could be implemented with circuits in theory)
    
Hopefully by this explanation, you can see that these processes are purely physical, there is no need for natural language such as that which might be used by a human.",2,0,0,False,False,False,1638657806.0
r8upnl,hn7vy6z,t3_r8upnl,It's all about those 0s and 1s.,2,0,0,False,False,False,1638640406.0
r8upnl,hn87pk3,t3_r8upnl,i beleive the computer has each charecter defined hardware wise as a string of bits.,1,0,0,False,False,False,1638645162.0
r8upnl,hn87t8h,t3_r8upnl,"I guess in your example I would consider what was on the cards to be the ""programming language"" or ""input.""

Just a guess. Someone else probably has a better answer.",1,0,0,False,False,False,1638645204.0
r8upnl,hn8iftf,t3_r8upnl,"Each computer has with a specific machine language associated with it. IE if you feed the cpu's input lines with a specific train of 0s and 1s (low and high voltage levels, in the language of electronics) it will do a cpecific thing (this is an oversimplification obviously), early computers were programmed this exact way.",1,0,0,False,False,False,1638649727.0
r8upnl,hn9o9zx,t3_r8upnl,[Crash course computer science](https://youtube.com/playlist?list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye),0,0,0,False,False,False,1638667837.0
r8f02f,hn5ahyi,t3_r8f02f,"It's **not** a ROM chip. It's some sort of flash memory, NVRAM, or EEPROM chip that can be updated with newer code.",34,0,0,False,False,False,1638584605.0
r8f02f,hn5tyxd,t3_r8f02f,[removed],-12,0,0,False,False,False,1638594560.0
r8f02f,hn5bisu,t1_hn5ahyi,Oh thank you. Could you explain what NVRAM and EEPROM are?,3,1,0,False,False,True,1638585114.0
r8f02f,hn60l3p,t1_hn5tyxd,what kind of chip does it have? there are bios/eeprom chip programmers you can buy like ezp2019 and ch341a.,4,0,0,False,False,False,1638598531.0
r8f02f,hn5ua1s,t1_hn5tyxd,I don't know how.,1,0,0,False,False,True,1638594733.0
r8f02f,hn6qa0r,t1_hn5tyxd,r/techsupport,1,0,0,False,False,False,1638619253.0
r8f02f,hn5gxll,t1_hn5bisu,"https://en.m.wikipedia.org/wiki/Non-volatile_random-access_memory

https://en.m.wikipedia.org/wiki/EEPROM",13,0,0,False,False,False,1638587783.0
r8f02f,hn5uxvu,t1_hn5ua1s,I have a WiFi smart camera which got bricked during firmware update. It's an Imou Ranger - 2 camera,-10,0,0,False,True,False,1638595111.0
r8f02f,hn5ozn0,t1_hn5gxll,Thank you I now understand.,5,0,0,False,False,True,1638591868.0
r8f02f,hn5xr9m,t1_hn5uxvu,What is a smart camera? I'm unfamiliar with this device.,-2,0,0,False,False,True,1638596736.0
r8f02f,hn6svxr,t1_hn5ozn0,"In my experience, EEPROM's are commonly used to contain system specific data like serial number, encryption keys, MAC address, etc... while NVRAM is used for more general stuff like system software images.",2,0,0,False,False,False,1638621171.0
r8f02f,hnc6xm8,t1_hn5ozn0,Reading this response felt like watching Neo when he got Karate downloaded to his brain,1,0,0,False,False,False,1638721687.0
r8f02f,hn7f0os,t1_hn6svxr,Do you know why?,1,0,0,False,False,True,1638633221.0
r8f02f,hncja7l,t1_hnc6xm8,Is that a good thing?,1,0,0,False,False,True,1638726793.0
r8f02f,hn88c1a,t1_hn7f0os,"ROM is typically used for permanent data such as serial numbers, calibration tables for the specific device, or programs that are not changeable such as a hardware specific boot sequence.  
NVRAM is typically used for performance or simplicity of design where the content needs to be changed rapidly.  Dynamic RAM (ie VRAM) often must be refreshed for the content to be retained.  Flash memory can be rapidly read and slowly written. Many flash memory devices also have limits on the number of times they can be written. Generally EEPROM has a much higher limit on the number of writes so they are often used to store data that needs to be preserved over a system restart ( power cycle ) such as a position of a gate or valve overnight.  So the types of memory for a computer system is selected based on targeted application function:  
     Dynamic RAM — Typically large memory so refresh cost is minimized; low cost; good to excellent performance; 
                                  ok to lose content I f reset/power loss.
     NVRAM — higher cost, simple design, Excellent performance.  Typically content preserved over reset/power
                        cycle.  Typically smaller memory amount.
     EEPROM — Fast read, slower write, higher write cycles vs FLASH; Content preserved over reset/power.  Typically 
                         Small memory amount.
     FLASH  — Fast read, very slow write, limited write cycles; Content preserved over reset/power cycle. Larger 
                       memory amount.
     ROM — Fast read, no write; Content preserved over reset/power cycle. Typically small memory amount.",1,0,0,False,False,False,1638645424.0
r8mrol,hn6q0t5,t3_r8mrol,"I have resorted to using pointers within certain functions to help increase the readability of the code and also to help reduce obtuse indexing. But often times it simply comes down to the data structure that's being accessed and the most reasonable way to do that. A good example would be a linked list. By their very nature (especially in C/C++) they use pointers. Traversing a linked list naturally requires the use of pointer. Yes, if you organize the data well, you **could** access the list via an index, but it's probably faster and easier to use pointers.",5,0,0,False,False,False,1638619054.0
r8mrol,hn7vqen,t3_r8mrol,I have used a pointer wen parsing [an obj file](https://en.wikipedia.org/wiki/Wavefront_.obj_file) to choose wich of my std::vetors to put data,2,0,0,False,False,False,1638640317.0
r8mrol,hn6s3k9,t3_r8mrol,"Yes they are. Consider a char array (mutable string) that you want to iterate over, inspect chars from and possibly modify all in one scope. You could use a pointer to do that.",2,0,0,False,False,False,1638620599.0
r8mrol,hn6pvs8,t3_r8mrol,">The main purpose of pointers is to stop the wastage of memory by copying of the values stored in a variable to another variable

No, not really. The main purpose of pointer is to point at something.

When pointer is used only for saving time/space, you can rewrite the code to be pointer-less and it will still work.  
Try it with dynamic data structures, e.g. Linked List.

Or when on lower-level, you need to write something to specific place in memory (e.g. writing `1` at address `53280` on C64 will change the screen frame color to white). And what type is for storing memory addresses? Pointer.",1,0,0,False,False,False,1638618944.0
r8mrol,hn7skit,t3_r8mrol,"I can right now think of 2 reasons

\- Polymorphism.  
\- Big bunch of memory you don't want to store on heap.",1,0,0,False,False,False,1638639006.0
r8mrol,hn6kt6r,t3_r8mrol,[deleted],0,0,0,False,False,False,1638614706.0
r8mrol,hn9whzr,t1_hn7vqen,How did you use pointers to do that? could you explain in a little more detail?,1,0,0,False,False,True,1638671607.0
r8mrol,hn6ombx,t1_hn6kt6r,"Actually, I asked whether pointers have any use in the same scope and not between function calls.",1,0,0,False,False,True,1638617928.0
r8mrol,hnqf10z,t1_hn9whzr,"The file has different kinds of vectors (position, normal, texture) that all have to be paresd the same but indexed separatly.

Basicly I store a pointer to where it is supposed to go parse it and put it where the pointer points",2,0,0,False,False,False,1638980866.0
r80r4b,hn32n3i,t3_r80r4b,"Aside from a mathematics textbook focussing on lambda calculus, I think your best bet would actually be getting some hands on experience with functional programming languages. It'll probably make understanding lambda calculus a lot easier in the long run",19,0,0,False,False,False,1638549722.0
r80r4b,hn3n9mk,t3_r80r4b,"Barendregts ""The Lambda Calculus, its Syntax and Semantics"" is an absolute classic. (But very math-oriented)",7,0,0,False,False,False,1638557831.0
r80r4b,hn4uiq6,t3_r80r4b,"u/Exourion made a good suggestion in my opinion. 

You can start learning Haskell, which is a pure functional programming language that uses Lambda Calculus. 

I am using the book ""Haskell Programming from First Principles"" by  Christoph Allen and Julie Moronuki and the very first chapter is about Lambda Calculus. 

Hope that helps.",5,0,0,False,False,False,1638576856.0
r80r4b,hn5f0kj,t3_r80r4b,[Alligator Eggs](http://worrydream.com/AlligatorEggs/) is fun entry point to ideas,2,0,0,False,False,False,1638586839.0
r80r4b,hn3fsmh,t3_r80r4b,SICP,4,0,0,False,False,False,1638554849.0
r80r4b,hn3c2do,t3_r80r4b,https://www.amazon.com/Introduction-Functional-Programming-Calculus-Mathematics/dp/0486478831/ref=sr\_1\_1?crid=281AXHPBSTE5L&keywords=lambda+calculus&qid=1638553357&sprefix=lambda+calcu%2Caps%2C184&sr=8-1,1,0,0,False,False,False,1638553392.0
r80r4b,hn6kgv8,t3_r80r4b,"I'd take a look at chapter 2 of

https://www.microsoft.com/en-us/research/uploads/prod/1987/01/slpj-book-1987-full.pdf",1,0,0,False,False,False,1638614415.0
r80r4b,hn5gym0,t1_hn32n3i,"I second this. I took a course at uni that involved lambda calculus. They taught lambda calculus first and it was super difficult to understand. Then, they taught us a functional programming language and it all made sense. Suddenly after learning functional programming I didn't even need to study lambda calculus, since using functional programming languages is basically using lambda calculus rules.",6,0,0,False,False,False,1638587797.0
r80r4b,hn4nvqi,t1_hn3fsmh,"??? Wikipedia does a better introduction to the lambda calculus. 

Unless SICP has a new chapter 6 I’m not familiar with, I’d vote no on this one. Though “The Little Schemer” does a nice job of sneaking in the Y combinator at the end, and makes you think you discovered it yourself.",4,0,0,False,False,False,1638573760.0
r80r4b,hn7ogzd,t1_hn3c2do,"This looks quite cool.  I wouldn't want to learn functional programming instead of the lambda calculus, but I also wouldn't want to learn the lambda calculus without seeing it in a functional language.  Nice pick.",1,0,0,False,False,False,1638637303.0
r80r4b,hn5k8wa,t1_hn4nvqi,"I think it’s a good intro to func prog more than anything, but yeah you’re right not the best for lambda calc",1,0,0,False,False,False,1638589442.0
r81i62,hn3e8lp,t3_r81i62,Minecraft redstone tutorials. I'm not even kidding.,25,0,0,False,False,False,1638554238.0
r81i62,hn2xhsl,t3_r81i62,"Free game where you solve increasingly difficult logic gate puzzles. Maybe not the best use of your time considering you only have a few days, but I think it would give you some nice intuition.

https://apps.apple.com/us/app/make-it-true-solve-circuits/id1536287319",6,0,0,False,False,False,1638547673.0
r81i62,hn4r728,t3_r81i62,That is the very first thing I learned. Are you sure you want to write that exam this semester?,1,0,0,False,False,False,1638575289.0
r81i62,hn6a5d8,t3_r81i62,neso academy on youtube!,1,0,0,False,False,False,1638605569.0
r81i62,hn4rk75,t1_hn3e8lp,OMG! Hell Yeah. Thank you for this,4,0,0,False,False,True,1638575460.0
r81i62,hn3nk8t,t1_hn3e8lp,"Tbh, it was quite fun to implement a T-flipflop after learning the theory behind them.",3,0,0,False,False,False,1638557951.0
r81i62,hn4umxu,t1_hn2xhsl,"oh thank you bro, this so much fun",3,0,0,False,False,True,1638576911.0
r81i62,hn4rgx9,t1_hn4r728,"Just for context I'm not in college, it's my high school exams and Boolean Algebra & Logic Gates consists more than half of the paper. There are other stuff like arrays, strings etc etc that I already know",3,0,0,False,False,True,1638575419.0
r81i62,hn69tq2,t1_hn4umxu,it’s tricky to solve with the minimum number of clicks,1,0,0,False,False,False,1638605306.0
r81i62,hn4w0i7,t1_hn4rgx9,Ah ok. Sorry then,1,0,0,False,False,False,1638577563.0
r7x6t1,hn2803f,t3_r7x6t1,"They are both very broad fields, and so there will certainly be some overlap.

At the highest level, AI is about creating algorithms that can solve problems through reasoning rather than explicit instructions as used in traditional software. AI has many subfields that can include making algorithms that think in human-like ways, to machine learning where solutions are discovered.

Computational science is about solving complex (scientific) problems using computational methods, and so recently has used AI quite a bit; however, it is not strictly focused on AI (parallel computing, hardware, etc. are also used). Generally, it focuses on maximizing the quality of models and simulations.

They both contribute to scientific research equally so neither would be closer or further from your goal. You would need to decide on a subfield to really answer that question.",3,0,0,False,False,False,1638535794.0
r7x6t1,hn30j8r,t3_r7x6t1,"My PhD concentration is in computational science. I’ll speak to that and let others focus on AI.

* Computational science has its origins modeling physics and engineering problems. Think PDEs.
* The field has naturally evolved to include randomized algorithms, numerical multilinear algebra, streaming problems, among others. 
* The application side has shifted from high-energy physics modeling to data science and ML (like so many areas). 
* Your most essential tools come from a numerical analysis class and maybe optimization. 
* Everyday questions are “do I get the same approximation error with 10 eigenvectors as I do 1000?” or “Should I pay the price for a Newton method or just do fixed point iteration and wait?”",2,0,0,False,False,False,1638548891.0
r7gaav,hn0898h,t3_r7gaav,Great channel! Love their sorting algorithms competitions.,2,0,0,False,False,False,1638492421.0
r7gaav,hn1jdoi,t3_r7gaav,I was able to follow right up till MIP\*… great video!,2,0,0,False,False,False,1638516551.0
r7nt4h,hn1ael8,t3_r7nt4h,"I read ""The Introduction To Algorithms"" and found they explained it pretty well. While formal, they apply it to many Algorithms to give a good intuition. You'll quickly find that algorithms with smaller asymptotic growth (like O(log(n)) as opposed to O(n)) ""do less work"" so to speak; and from there,I had to read a million examples which it provides as well.

About what you said in your rant; you certainly could compare run times of Algorithms directly.  But doing so introduces many factors not really part of the algorithm itself. Like for example the speed of the computer running the algorithm.
With asymptotic growth, we're really interested in the algorithm on an abstract level, away from all implementation.

This is also part of why we say things like ""O(3n+log(n) + 2n^2) = O(n^2)"". All the constant disappear because they don't really matter for our purpose; if the algorithm runs in O(2n^2), one can insert a processor of double the speed to combat it. But one cannot ""fight"" against asymptotic growth; no matter the speed of your computer, O(n^2) becomes arbitrarily large and at a speed where any smallere term like O(7n) becomes completely negligible if we increase n far enough. And we like our n to be big; the amount of data is ever increasing!

Went on a bit of rant here too :) But I recommend Introduction.to Algorithms; one can easily find a PDF online",3,0,0,False,False,False,1638510475.0
r7nt4h,hn22x1t,t3_r7nt4h,"You probably got answers to some of your questions in the video already. Feel free to ask further if not.

> Like if you already have a function that perfectly describes your algorithms time complexity, what's the point in simplifying it in a weird way

One of the problems is that you *don't* necessarily know the exact function, at least not with precision that would make those exact details meaningful. What would you base the exact complexity function on? Which programming language is it written in? The same algorithm that works based on the exact same idea will have its exact details, and the exact number of steps, look different in different programming languages. Or, more importantly in terms of performance, if it's compiled into machine code, which processor is that? If it's interpreted, which interpreter? The machine code is going to be different for an ARM processor than for an x86-64 one, and the number of machine instructions is going to be different. Some CPU instruction sets might provide individual instructions that do more work in a single instruction but could take longer to execute. Moreover, different instructions take a different number of cycles on different CPU models even if the instruction set is the same.

Asymptotic complexity doesn't change from a language to another, or from a CPU instruction set or a physical processor to another. It's a mathematical property of the algorithm itself. A quadratic function is always going to be quadratic regardless of the constant multipliers; a linear one is going to be linear; an exponential one is going to be exponential. That way you can talk about the algorithm itself more generally rather than its implementation in a particular language, or on a particular CPU.

Including every constant and every term when talking about an algorithm more generally would often mean giving more detail than you perhaps have grounds for. It would be a bit like reporting seven digits in your calculated result about a physical phenomenon when the original measurements are only really accurate enough to warrant two. It kind of looks more accurate but most of it is just noise.

You could argue for keeping the lower-degree terms even if the constants were discarded, of course. It might sometimes make sense to think of an algorithm's time complexity as being in the order of N^2 + N if that happens to be true rather than just N^2. And that wouldn't (probably, at least not always) even depend on the implementation, and could be part of the behaviour of the algorithm itself.

The highest-degree term starts to dominate as N grows indefinitely, which is what asymptotic analysis deals with, so the lower-degree terms are skipped. But you could reasonably argue that the lower-degree terms are part of the algorithm's behaviour, too.

> that breaks algebra rules?

This is a bit of an aside, but I don't think it really actually breaks algebra rules. The common way the notation is used perhaps does.

What for example O( n^2 ) actually denotes is the set of functions that are bounded above by n^2 * c for some constant c. The pedantically correct way would be to say that an algorithm's worst case complexity is *in* O( n^2 ), for example, rather than that it equals O( n^2 ). That's one of the things that people very often write in a way that's rigorously speaking not quite right. Once you figure it out, or if you just don't think about it that far, the sleigh of hand generally doesn't hurt understandability.",2,0,0,False,False,False,1638532419.0
r7nt4h,hn2vw5r,t1_hn1ael8,Thanks you so much! I'm always blown away by how helpful this subreddit is :),1,0,0,False,False,True,1638547026.0
r7nt4h,hn2wxil,t1_hn22x1t,">The pedantically correct way would be to say that an algorithm's worst case complexity is in O( n2 ), for example, rather than that it equals O( n2 ).

This helped me so much! Thank you! This subreddit is so helpful that sometimes I feel like I should be paying to ask these questions. Once again, thank you!",1,0,0,False,False,True,1638547447.0
r7nt4h,hn38qbd,t1_hn2wxil,"No worries. I can see how it could look confusing and dissatisfying if you look at things with a more mathematical eye and something doesn't seem to fit. The theory, or at least the parts that are part of the canon, do actually fit, though. Things just often get bent a bit and rigour gets lost in more everyday use.",2,0,0,False,False,False,1638552101.0
r7fkp9,hmzq7o8,t3_r7fkp9,"There is no way to do this. You can never, ever verify that what is happening on an uncontrolled device is what you think it is. The only thing you can verify is that the input you are being given 'makes sense' based on your expectation of what is allowed - this is the problem things like Valve Anti-Cheat are designed to tackle.",10,0,0,False,False,False,1638484556.0
r7fkp9,hmz3146,t3_r7fkp9,My first question would be what's stopping someone from realizing this and sending over a fake hash they gained from a legitimate source?,13,0,0,False,False,False,1638475677.0
r7fkp9,hn1gg83,t3_r7fkp9,"You're best bet would be to implement OAuth and use a server to run proprietary functions. A user must then send an untampered token, which is given from and verified using a secret on, the server to authorize them on different resources.",2,0,0,False,False,False,1638514399.0
r7fkp9,hn1120i,t3_r7fkp9,"To add to what everyone else said, are you trying to authenticate a user? Use a login system. Are you trying to validate that a piece of software has been paid for? Actually, use a login system. 

I've actually written a licensing library for windows applications - I also wrote the licensing server. No matter how complicated I made it, there's always going to be some person that will go that extra step of decompiling that library or application and bypassing it. You know what can't be bypassed? A system where the user has to login to a server. You still have to follow best practices and security protocols on your server, though. This is why you hear about companies being hacked and sensitive information being stolen",1,0,0,False,False,False,1638505363.0
r7fkp9,hn1c6lg,t3_r7fkp9,"Have a look at remote attestation, it's probably the closest thing to what you want to do.",1,0,0,False,False,False,1638511570.0
r7fkp9,hmz332r,t1_hmz3146,That's what I'm asking,0,0,0,False,False,True,1638475699.0
r7fkp9,hmz3d58,t1_hmz332r,"I suppose what I'm getting at is that you should never trust the client.

Verify all input server side.",5,0,0,False,False,False,1638475808.0
r7fkp9,hmz4itk,t1_hmz3d58,Yeah but how,-2,0,0,False,False,True,1638476257.0
r7fkp9,hmz5oy5,t1_hmz4itk,"Depends on what you're doing. 

Realistically, a client can send you good data, bad data, invalid data, malicious data, malformed data. Data can get corrupted between the client and the server, or can be intercepted and modified.

The servers job then is to take any data and verify that the data is logical and within valid parameters. If I'm entering my name, I can put Joe or 2847 or Joe792!?5Schmo, or even nothing. At some point you have to define what a valid name is.

Think about programming Chess. The client can send the move Black Queen to E5. Sounds reasonable? Well, maybe the queen isn't even on the board anymore, or would be an illegal move, or whatever.",2,0,0,False,False,False,1638476704.0
r6vpdh,hmvmvzp,t3_r6vpdh,The solution you looking for is a lawyer and a letter in an envelope,157,0,0,False,False,False,1638410769.0
r6vpdh,hmw9kr7,t3_r6vpdh,"a) sounds like ur life isn't going so hot sorry to hear that, lmk if you need to talk or anything   
b) one way to do it is to do it kinda an old fashioned, this is how people did in the middle ages/classical period, give people trust an encryption key or algorithm, but not the password itself. Next find an obituary API one with data from your area, and spin up a web server that pings the API once per day, if your name shows up, send (via email or snail mail) the encrypted password to people you trust.",26,0,0,False,False,False,1638421386.0
r6vpdh,hmwkhqo,t3_r6vpdh,"Hey OP, I was looking through your comments and I know you're feeling suicidal, but I would urge you to try to get help before you do anything you will regret. According to your post history, you're transgender, so [The Trevor Project](http://thetrevorproject.org) might be helpful to you. However, there are other options - if you are in the UK, you can access [Childline](http://childline.org.uk), or [Samaritans](http://samaritans.org). If you are outside the UK, [here is a list](https://www.thecalmzone.net/2019/10/international-mental-health-charities/) of international mental health charities. Over here on Reddit, we have r/suicidewatch, and if you want to talk to someone else, feel free to send me a message. 

Good luck OP, and remember, there's always someone there who is ready to support you.",58,0,1,False,False,False,1638428090.0
r6vpdh,hmx56oc,t3_r6vpdh,"Interesting project called ""horcrux"" that splits a file into encrypted fragments and only decrypts if you have all the fragment. 

https://github.com/jesseduffield/horcrux

But yes echoing others I do hope you are ok. I'm older now and a lot of stuff can seem incredibly tough when you're going through it so hope you can work through things.",9,0,0,False,False,False,1638444863.0
r6vpdh,hmwdsae,t3_r6vpdh,Is everything OK for you OP?,9,0,0,False,False,False,1638423763.0
r6vpdh,hmvsmqh,t3_r6vpdh,[removed],28,0,0,False,False,False,1638413248.0
r6vpdh,hmwemu4,t3_r6vpdh,[removed],3,0,0,False,False,False,1638424262.0
r6vpdh,hmx8o1q,t3_r6vpdh,"This is morbid af but I will try to answer. OP hope you get better. Been there myself and thought about this a lot.

Encrypted 7Zip file with a password on it - AES or better. Depending where you are you can get a will kit very cheap (some post offices have them) you can get them endorsed by a JP (I think) and then it is binding. On your death or when you are deemed unable to look after yourself a guardian will take over your trust and the will can be released to them.

Safe travels bud and if you are somewhere toxic get the fuck out of there at all costs.",2,0,0,False,False,False,1638447318.0
r6vpdh,hmw1tgq,t3_r6vpdh,Your code is that important it needs to be given to ur family but u dont want to spend a single dollar. Ok,3,0,0,False,False,False,1638417385.0
r6vpdh,hmwu74u,t3_r6vpdh,If you trust google they have a function to send mails after a time period not being logged in.,1,0,0,False,False,False,1638435739.0
r6vpdh,hmww0b6,t3_r6vpdh,"Well first you need to establish what the parameters are, how long will you need to go without resetting it? 

But a very basic way to do this would be to put the instructions for accessing your data in an email that you schedule (can do this with gmail) to be sent at a certain time in the future. 

Then you need to come back before that date, and reset it (i.e. delete the original schedule, and make a new one at a later date). If you don't come back before that date (i.e. you died), then the email will be sent.",1,0,0,False,False,False,1638437289.0
r6vpdh,hmz0okt,t3_r6vpdh,Really need quorum keys for this.,1,0,0,False,False,False,1638474733.0
r6vpdh,hmvnf34,t1_hmvmvzp,But that costs money :(,34,0,0,False,False,True,1638410998.0
r6vpdh,hmw0kbz,t1_hmvmvzp,[removed],-35,0,0,False,True,False,1638416791.0
r6vpdh,hmwavy8,t1_hmw9kr7,"Kk thx, wish anyone could help but there's nothing I can do but suffer 🙃",10,0,0,False,False,True,1638422110.0
r6vpdh,hmwkt43,t1_hmwkhqo,"Thx but there's nothing I can do that I'm not doing, I just have to sit here and suffer 🙃",-8,0,0,False,True,True,1638428317.0
r6vpdh,hmytj6a,t1_hmx56oc,"That's really cool, thx!",3,0,0,False,False,True,1638471972.0
r6vpdh,hmwdtoz,t1_hmwdsae,No 😎,1,1,0,False,False,True,1638423785.0
r6vpdh,hmvy4vq,t1_hmvsmqh,"Damn ur smart, thx",7,0,0,False,False,True,1638415661.0
r6vpdh,hmwfjei,t1_hmwemu4,[removed],2,0,0,False,False,False,1638424819.0
r6vpdh,hmw2c46,t1_hmw1tgq,"Yeah I gotta spend that money on other stuff, there's a reason I'm close to death :/",1,0,0,False,False,True,1638417635.0
r6vpdh,hmz2dfd,t1_hmz0okt,Wtf is that?,1,0,0,False,False,True,1638475417.0
r6vpdh,hmvnqcj,t1_hmvnf34,Like everything in life. Storing some data also costs money. There is no free lunch.,58,0,0,False,False,False,1638411133.0
r6vpdh,hmwkyn2,t1_hmwkt43,"I know you think that, but I don't believe that. I felt the same way - but actually using these resources was really helpful. Have you spoken to an anonymous councillor before?",25,0,0,False,False,False,1638428427.0
r6vpdh,hmwe316,t1_hmwdtoz,Please don't do what it sounds like you're planning. Is there somebody you can talk to? Some way I can help you?,14,0,0,False,False,False,1638423937.0
r6vpdh,hmwfm5g,t1_hmvy4vq,Don't actually do that. Other people with your name will die before you and send your password to everyone.,33,0,0,False,False,False,1638424865.0
r6vpdh,hmwgig4,t1_hmwfjei,[removed],1,0,0,False,False,False,1638425429.0
r6vpdh,hmwf289,t1_hmw2c46,hey i hope you are okay and if preventable i hope you find a solution. message me if you need to talk to someone.,7,0,0,False,False,False,1638424522.0
r6vpdh,hmw42xn,t1_hmw2c46,"If this is the case, just give access to someone you trust now.",1,0,0,False,False,False,1638418499.0
r6vpdh,hmz545d,t1_hmz2dfd,You see why it's needed.,1,0,0,False,False,False,1638476483.0
r6vpdh,hmvntam,t1_hmvnqcj,:(,16,0,0,False,False,True,1638411169.0
r6vpdh,hmwl1kc,t1_hmwkyn2,"Yeah but all they do is ask how I'm trying to kill myself until they just say bye, didn't really get a solution",-1,1,0,False,False,True,1638428484.0
r6vpdh,hmwe9ly,t1_hmwe316,No and no 😎 all I can do is suffer,-1,1,0,False,False,True,1638424046.0
r6vpdh,hmwfzl6,t1_hmwfm5g,True :/,7,0,0,False,False,True,1638425100.0
r6vpdh,hmymrgo,t1_hmwfm5g,"Change your name to something unique before dying, obviously.",4,0,0,False,False,False,1638469372.0
r6vpdh,hmwhhzs,t1_hmwgig4,[removed],2,0,0,False,False,False,1638426069.0
r6vpdh,hmwff95,t1_hmwf289,"Thx, there's a solution but there's also a lot of stigma and bigotry, only thing to see is if I break before my life is worth living 😎",1,0,0,False,False,True,1638424747.0
r6vpdh,hmw4mvm,t1_hmw42xn,I don't trust anyone lol,2,0,0,False,False,True,1638418781.0
r6vpdh,hmvofn2,t1_hmvntam,"My dad passed last year due to covid, and the best thing that helped us is that he had his password written up onto a paper and stored it in his desk. And he used the same password for everything, and if not, we could access his emails and reset the passwords. Facebook as it turned out has a feature to take over a profile and create a memorial for it, there are multiple ways like sending the certificate of death to them.",22,0,0,False,False,False,1638411438.0
r6vpdh,hmwl5g6,t1_hmwl1kc,"Yup, that sounds familiar. If you haven't, I'd urge you to try again - I find that it's a sort of lucky dip. Alternatively, is there anyone you can speak to at school/home? Or even a friend?",18,0,0,False,False,False,1638428562.0
r6vpdh,hmy1ubu,t1_hmwe9ly,"Ah,  yes, downvoting this comment will make OP feel better",13,0,0,False,False,False,1638461313.0
r6vpdh,hmwfjo8,t1_hmwff95,well i hope you solve the solution soon. message me if you ever need someone to talk to,7,0,0,False,False,False,1638424823.0
r6vpdh,hmw8i2d,t1_hmw4mvm,You can trust me bro,2,0,0,False,False,False,1638420796.0
r6vpdh,hmwn4r2,t1_hmwl5g6,Schools would put me in grippy sock jail and I don't wanna annoy my family (bc they might do the same thing) and I have no friends 🙃,-3,1,0,False,False,True,1638430031.0
r6vpdh,hmwnd41,t1_hmwn4r2,"I'm sorry to hear that. Well, if you want to talk, feel free to just PM me.",17,0,0,False,False,False,1638430210.0
r6vpdh,hmx5ct3,t1_hmwn4r2,"Please stay with us friend. You have friends here. We care. You mention school so perhaps you are still pretty young too, in which case definitely hang on to life. Things get soooo much better once you get out of school. Truly it does, I promise. If you need to talk please dm me.",17,0,0,False,False,False,1638444988.0
r70ia7,hmwp31c,t3_r70ia7,"Your explanation in words is incorrect. It would be correct if the relation was T(N) = 5N + N-1, but the recurrence element T(N-1) needs to be factored in. Another way to think of it that might make more intuitive sense: 

The T(N-1) component of the recurrence relation means that for all values of n, from the first index to the Nth, this function will perform 5n work. If you expand it out, the total amount of work will be 5N + 5(N-1) + 5(N-2) + 5(N-3) ... until you get down to n=1 or the first element. 

Each of these individual terms (ex. 5N) is in O(N), and there are a total of N different terms being added together, which is where the multiplication comes in. Hope this helped!",14,0,0,False,False,False,1638431521.0
r70ia7,hmxmhim,t3_r70ia7,"Here's another way to work it out.

First, an important summation to know is:

1 + 2 + 3 + ... + N = N(N+1)/2

Start substituting the formula recursively, and notice the pattern.

T(N) = 5N + T(N-1)

T(N-1) = 5(N-1) + T(N-2)

T(N-2) = 5(N-2) + T(N-3)

...

T(1) = 5(1) + T(0)

T(0) = 0

Combine them all:

T(N) = 5N + 5(N-1) + 5(N-2) + ... + 5(1)

= 5(N + (N-1) + (N-2) + ... + 1)

= 5( N(N+1)/2 )

= (5/2) (N\^2 + N)

= (5/2) N\^2 + (5/2) N

= O(N\^2)",8,0,0,False,False,False,1638454811.0
r70ia7,hmwhtax,t3_r70ia7,"From StackOverflow:

  
T(n) = T(n-1) + n  
T(n-1) = T(n-2) + n-1  
T(n-2) = T(n-3) + n-2  
and so on you can substitute the value of T(n-1) and T(n-2) in T(n) to get a general idea of the pattern.  
T(n) = T(n-2) + n-1 + n  
T(n) = T(n-3) + n-2 + n-1 + n  
.  
.  
.  
T(n) = T(n-k) + kn - k(k-1)/2    ...(1)  
For base case:  
n - k = 1 so we can get T(1)  
=> k = n - 1  
substitute in (1)  
  T(n) = T(1) + (n-1)n - (n-1)(n-2)/2  
Which you can see is of Order n2 => O(n2).

Source: https://stackoverflow.com/questions/13674719/easy-solve-tn-tn-1n-by-iteration-method",1,0,0,False,False,False,1638426278.0
r71nbu,hmyooiz,t3_r71nbu,"The vt-x extension means VMs can have their CPU instructions executed by the host CPU directly (if everything is configured right). But it doesn’t address anything else, just the CPU. Both VMware and virtual box can take advantage of vt-x.",3,0,0,False,False,False,1638470104.0
r71nbu,hmys32a,t1_hmyooiz,Good to know! Thanks.,1,0,0,False,False,True,1638471415.0
r6f0od,hmspnuv,t3_r6f0od,"B

Always practice while you learn",31,0,0,False,False,False,1638367935.0
r6f0od,hmsvd34,t3_r6f0od,B for sure,15,0,0,False,False,False,1638370540.0
r6f0od,hmtej9c,t3_r6f0od,"Agree with everyone who said B. In case you haven't seen it and want some resources to learn OS development, this repo is pretty damn cool.

https://github.com/danistefanovic/build-your-own-x#build-your-own-operating-system",11,0,0,False,False,False,1638378386.0
r6f0od,hmsyc99,t3_r6f0od,B is the right choice,8,0,0,False,False,False,1638371831.0
r6f0od,hmt6vp2,t3_r6f0od,Option B. You might not understand a few things in OS without knowing C.,8,0,0,False,False,False,1638375355.0
r6f0od,hmu655u,t3_r6f0od,"You should read about it (you will have to read a ton actually) and practice at the same time. Osdev Wiki should be a good starting point. Also the Tanenbaum Book on operating systems has some exercises you can do while reading iirc.

Writing your own OS is a pretty big project. I did a university course where we wrote our own OS and had a really solid foundation given to us and it was still pretty complicated.",5,0,0,False,False,False,1638389063.0
r6f0od,hmveo11,t3_r6f0od,In my OS class we use Operative Systems in three easy peaces and I really recommend it. Such a joyfull experience.,3,0,0,False,False,False,1638407209.0
r6f0od,hmu302l,t3_r6f0od,"B. It really helps you understand how these things work, also you gain practical experience in low-level programming, which you may use in future.",2,0,0,False,False,False,1638387856.0
r6f0od,hmuzvr0,t3_r6f0od,"If all your after is round robin and multiprocessing, i wouldnt read the whole book. 

Id be flexible and keep your hands dirty, also have a look into Minix 3.  Minix was designed to be an operating system for learning. I think at one point the author was reluctant to expand as it got too big to learn, but its now expanded Minix3 to be a fully fledged OS while maintaining the goal of being an OS to learn on.

[http://www.minix3.org/](http://www.minix3.org/)

&#x200B;

[https://wiki.minix3.org/doku.php?id=www:documentation:start](https://wiki.minix3.org/doku.php?id=www:documentation:start)",2,0,0,False,False,False,1638400657.0
r6f0od,hmv1wjl,t3_r6f0od,"I did option C. I had a summer internship where I worked on bringing up a system with an RTOS, muddled my way through it (plus help from others in the lab), then in the fall I took an Operating Systems class at school and all the pieces fell nicely into place.",2,0,0,False,False,False,1638401602.0
r6f0od,hmwgr99,t3_r6f0od,"I am also trying to write an OS. And option B is the best way to proceed. You understand the concept, implement it, if it doesn't work look where u messed up, and continue

https://wiki.osdev.org/",2,0,0,False,False,False,1638425588.0
r6f0od,hnut7yc,t3_r6f0od,"Everyone is saying B, but in reality if you are going through a good book which contains plenty of exercises then A is certainly a good option.

What you don't want to do is just read a book without doing any sort of practical work in the meantime",2,0,0,False,False,False,1639060594.0
r6eo9a,hmsr80w,t3_r6eo9a,"My research program is largely related to those subjects. No particular language is necessary. Python is generally useful in research because of the large number of libraries available. I personally write code in Python or Java if I'm working with a student (because often it is all they know) or C++ when working by myself (largely because over the years I've developed a large AI/ML library in C++). R and Matlab (or similar products) is also quite useful for doing analysis or data preprocessing. For example, there are good Matlab addons for working with EEG and fMRI. These products (and Python) are also useful for computer vision.

You could also start taking a look at scholarly papers on the subjects of interest, and examine how they did their work. If there is a particular specific work that interests you, then using similar technologies is likely to be useful.",5,0,0,False,False,False,1638368673.0
r6eo9a,hmub04a,t3_r6eo9a,If your using ur for research r studio will make things a lot easier for you,1,0,0,False,False,False,1638390903.0
r6eo9a,hmx8uxt,t3_r6eo9a,"You might be interested in:

https://probmods.org",1,0,0,False,False,False,1638447446.0
r6eo9a,hmsy4gm,t1_hmsr80w,"Thanks for the suggestions, I'll start looking at some papers then !",2,0,0,False,False,True,1638371738.0
r6eo9a,hmxaz4v,t1_hmx8uxt,Seems interesting ! Thanks !,1,0,0,False,False,True,1638448783.0
r6eo9a,hmsznut,t1_hmsy4gm,"If you decide on a more specific area, then I might be able to recommend something. :)  Just reply here, and I'll drop some links if I can.",1,0,0,False,False,False,1638372392.0
r6eo9a,hmt0mci,t1_hmsznut,"I'm mostly interested in human language and cognition so I guess NLP (not sure if cognition is being studied from a computational perspective).

Thanks !",1,0,0,False,False,True,1638372796.0
r6eo9a,hmt155c,t1_hmt0mci,"Yes, although research tends to be more niche, e.g. cognitive models of schizophrenia. Here's a good starting point for some foundation. Good luck and have fun! :)  


https://www.sciencedirect.com/science/article/abs/pii/S136466130600132X",2,0,0,False,False,False,1638373012.0
r639yl,hmqqmgk,t3_r639yl,"This is a complicated question. Computer science students study these topics (usually as part of Operating Systems and Computer Architecture courses) but the deep work at industry-level is generally done by computer engineers or software engineers with significant training or experience in computer engineering.

I’ve heard good things about Nand2Tetris for CPU stuff. Any graduate-level computer architecture textbook will have a ton of information like the kind you’re looking for. We used [this book ](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1?countrycode=US&format=electronic) in my graduate program. I’ve heard the PDF is floating around online. This book explains hard drives, CPU design, RAM, etc. 

For the GPU, you should look into NVIDIA’s CUDS documentation. 

At a slightly higher level, [the OSDev wiki](https://wiki.osdev.org/Main_Page) is a great reference as well.",26,0,0,False,False,False,1638323970.0
r639yl,hmrdk4s,t3_r639yl,[deleted],15,0,0,False,False,False,1638334808.0
r639yl,hmrw273,t3_r639yl,"You're looking for this thing: https://www.bookdepository.com/The-Elements-of-Computing-Systems/9780262539807

Get your hands dirty and *really* find out how this stuff works. It's a really good hobby project.",7,0,0,False,False,False,1638347098.0
r639yl,hms4xu6,t3_r639yl,"> Does this fall under computer science? 

Yes and no.

It did, traditionally. And a lot of Computer Science courses will teach this stuff. But these days it's considered Computer Engineering, but there's a huge cross over between the two, as CE can be seen as a subfield of CS, or a cross over of CS and Electrical Engineering.

Check out /r/ComputerEngineering for more.

Anyway, here's my stock answer for this question:

If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

This will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc. Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board"")",6,0,0,False,False,False,1638354738.0
r639yl,hmrfts3,t3_r639yl,Check out Computer Systems a Programmers Perspective by Bryant or Computer Organization and Design by Patterson and Hennessey.,4,0,0,False,False,False,1638336037.0
r639yl,hmsrlp0,t3_r639yl,https://www.nand2tetris.org/,5,0,0,False,False,False,1638368851.0
r639yl,hmrz1f5,t3_r639yl,there’s an IT professional course by google. check it out,1,0,0,False,False,False,1638349605.0
r639yl,hms05gt,t3_r639yl,"If you’d like, I can send you a link to a pdf (through libgen) of the textbook we’re currently using in my computer architecture class for a more beginner-esque read",1,0,0,False,False,False,1638350574.0
r639yl,hmr04q6,t1_hmqqmgk,"I just want to say, as a computer science major, absolutely don't do computer science for this. 

At least where I go to school, CS is more for software stuff, and only went into details on this for a single class (Operating Systems), and even then, it wasn't super deep.

Computer Engineering (again, at least at my school), goes down into the technicalities of how everything actually works, and how to build it. 

Computer science -> Software Engineer

Computer engineer -> Designing and learning about hardware components and circuits",13,0,0,False,False,False,1638328219.0
r639yl,hmsz3wf,t1_hmqqmgk,"Thanks, very helpful.",2,0,0,False,False,True,1638372155.0
r639yl,hmsz4nr,t1_hmrdk4s,Thank you!,1,0,0,False,False,True,1638372163.0
r639yl,hmsz7v3,t1_hms4xu6,Thank you!,2,0,0,False,False,True,1638372202.0
r639yl,hmt40rr,t1_hmr04q6,Good advice for sure. It was the same way at my school. :),3,0,0,False,False,False,1638374196.0
r639yl,hmz0z0x,t1_hmsz4nr,"When you do this just keep in mind that this video series covers an architecture used for teaching called SAP - ""Simple As Possible"" created by Malvino for teaching, and our actual hardware doesn't operate very much at all like these model systems.

Generally you would be replacing each part of that assemblage with an entire ecosystem of related hardware. It gets very deep, very fast, and the worst part is a lot of it is considered ""secret sauce"" by manufacturers, so you won't get a complete description of what's taking place in something like a developer's reference manual.

That said, a lot of the time those developer's manuals are often your best bet for learning the things you should know about hardware such as ram or your cpu, and are distributed by manufacturers.

Often you can also find information provided by brave souls who have done the dive for you -- https://people.freebsd.org/~lstewart/articles/cpumemory.pdf",1,0,0,False,False,False,1638474848.0
r6qahx,hmv930p,t3_r6qahx,"Having a central processing entity you were talking about (I.e. the proverbial snakes head) for the invading cyborgs is not a stretch when you are already talking about manure level cyborgs.

Also consider strides in quantum computing which means larger numbers being processed many times faster than today’s binary computers.

Or consider organic computers, operating chemically. (Proteins are essentially single purpose chemical computers that occur naturally)

Source: Have degree in film and one in CS",3,0,0,False,False,False,1638404761.0
r6qahx,hmv08wi,t3_r6qahx,"You need to give more details of the process, because as of now, I can even think about believable bulls**t which would explain a slightly more powerful smartphone to be capable of it.",1,0,0,False,False,False,1638400825.0
r6ei7a,hmu3w7r,t3_r6ei7a,"Cisco use a program called Packet Tracer. Its pretty good. Not sure if its used by professionals or not for planning, but its good for training.",3,0,0,False,False,False,1638388205.0
r6ei7a,hmufh8q,t1_hmu3w7r,"I've heard about that one before, but I am curious about more proffesional stuff.",1,0,0,False,False,True,1638392548.0
r6ei7a,hmuzz5r,t1_hmufh8q,Check out GNS3 and Eve-NG.,2,0,0,False,False,False,1638400702.0
r6ei7a,hmv8ob5,t1_hmuzz5r,">Eve-NG

From a little research, it seems for huge scale networks ( like cities or campuses ) OPNET is still superior to those. 

I'm definately going to try GNS3 for small projects though!",1,0,0,False,False,True,1638404583.0
r5yufr,hmpvj7h,t3_r5yufr,"I just want to emphasize, this post is not a tech support question. I already have the fix. I just really want to understand why the fix works.",21,0,0,False,False,True,1638310288.0
r5yufr,hmrjr2m,t3_r5yufr,"There's a **lot** of incorrect information in this thread. Pun intended.
 
Multi-threading was a thing long, long before multi-core CPUs were a thing. Setting a process priority to high and giving it a single-core affinity does **NOT** make it single threaded. This is misunderstanding the concept of process, thread, and core entirely.
 
What you are doing by changing those settings is ensuring that it is the only thing running on that 1 core, on the proviso that nothing *else* is being given above normal or higher priority, and that you never, ever saturate all of your other cores with other processes (which will override that priority). So in essence turning it into a single core machine as far as that software is concerned.
 
Dual- and higher- core processors were very, very new to the consumer space in 2006, with the vast majority of consumer PCs being single core, and the vast majority of consumer software being written in such a way that single core performance was what was prioritised. This doesn't mean that those programs were single-threaded - far from it, the majority were multi-threaded, so the GUI remains responsive while processing work happens in the background. For example, if you're using Internet Explorer, the page can still be loading while you type things into a search bar, you didn't have to wait for that page loading work to be complete before it could respond to your keyboard input.
 
The difference between multiple threads all running on the one core at the 'same time' and multiple cores, is that multiple threads (or processes) running on the same core don't actually run at the same time at all. Through various algorithms they are sliced, diced, and given effectively a time share of that chip's work, one after the other after the other, to fake things running at the same time. Only 1 instruction is every being worked on at the same time, but it could be some from Thread 1, some from Thread 2, some from Thread 3, then back to 1, and so on (the same for processes, it might do something for the game, then the notepad window in the background, then the OS, then the game again, picking up each thread each process has).
 
What multiple cores do is make it so that time sharing is now done across multiple things that can actually execute instructions simultaneously. So if you are dual-core there are 0-2 instructions being processed at any given instant, from up to 2 threads in up to 2 processes. And scaling up to quad-core with 4, etc etc. This means that so long as you never have 1 big-ass thread that wants all the processing power, a dual-core CPU can be twice as fast as a single core, quad-core is 4x faster so long as you have 4 threads that want all that power, etc.
 
The reason the game is crashing when it is running on multiple cores is because the code that it uses to manage its threads (so they don't all try to change the same data, or so that thread 1 that is waiting on some calculations from thread 2 doesn't start too early, etc) was written when there was a physical guarantee that 2 instructions on 2 threads would not happen at the same exact time. When it is on multiple cores, this guarantee disappears, and that thread management fails. There are a bunch of technical reasons this may occur, usually from attempting to do things more quickly (2006 consumer PC hardware was not fantastic by modern standards) and skipping various safety checks.
 
But it is still multi-threaded on 1 core.",17,0,0,False,False,False,1638338323.0
r5yufr,hmq7pot,t3_r5yufr,"Well it's clearly there is something wrong with your code which handles some threads or asynchronous tasks, something could be wrong here.

And since you put it above normal, this will just suck more cpu resources which shouldn't be opt in even in complex game in normal situations. So as someone already mentioned this doesn't consider a fix. And since it freeze at specific moment try to debug it(could take some time) and find how much resources are being used.


So what programming language and frameworks do you use ? And does it also fail at that point when it's not in full screen? What if it's in big screen but not full (95%) does it work?",2,0,0,False,False,False,1638315474.0
r5yufr,hmq6kfy,t3_r5yufr,"Based on the “fix” (which I would consider more of a workaround than a fix), it sounds like your issue is that your program cannot properly handle being run multi-threaded. I don’t know what language you’re using, but the task of ensuring your program runs properly in a multi-threaded environment is generally a nontrivial task. In terms of google-able terms, things like “deadlock” or “race condition” would be good things to read up on.",1,0,0,False,False,False,1638314964.0
r5yufr,hmreawc,t3_r5yufr,"Maybe it is not properly programmed so some variables are accessed by 2 threads at the same time and caused unexpected behavior. Or like u/chrisxfire says, there is a deadlock somewhere ( one thread is waiting for another thread to leave a code section, but there's no other thread in that code section )  


Edit: The fix is kind of a quick solution that they just figured that in that scenario the problem doesn't happen. The only real fix is that they go debug their code, which could be a huge unmanagable spaghetti. In the GameDev industry there's too much spaghetti ( please don't hate me for saying this )",1,0,0,False,False,False,1638335207.0
r5yufr,hmqqx0u,t3_r5yufr,"My initial thought is that at that point of the game a process kicks off that needs to wait for another process to have completed, so when process b gets to a certain point it doesn’t have the expected app state and the app freezes. When you prioritize only one cpu then the game runs “synchronously”, and process b won’t start until the cpu is released after finishing process a",1,0,0,False,False,False,1638324099.0
r5yufr,hmrj6af,t3_r5yufr,"All of these explanations are explaining away why setting cpu affinity might be solving the issue. But I am still curious why does windowed mode not result into game freeze (windowed mode game would still be multi threaded without setting cpu affinity). For that matter, why would increasing process priority be needed if you have already setup cpu affinity (game would be single threaded even if process priority is high/low)",1,0,0,False,False,False,1638337974.0
r5yufr,hmqlphz,t3_r5yufr,"Most likely, there's a bug in the code trying to use multi-threaded operations resulting in a deadlock which presents itself as a frozen or hung application as you're seeing. Setting the application to use a single core prevents it from obtaining multiple threads, thereby working around the bug.

I can't answer why changing the process's affinity to Above Normal is required. I could only speculate.",0,0,0,False,False,False,1638321751.0
r5yufr,hmrwq31,t1_hmrjr2m,"Your comment is giving the impression that single core multi threaded applications have higher guarantees than multi core multi threaded applications. It is actually not so. Any operation which is non-atomic will have same guarantees in single core as well as multi core. Similarly, any operation which is atomic will have same guarantees whether it is in single core or multi core. (Atomicity can be established either by explicit lock or atomic machine instruction). Anyway I am not convinced that multi threading is an issue. 

https://www.reddit.com/r/computerscience/comments/r5yufr/why_might_changing_process_priority_and_forcing_a/hmrj6af/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3",2,0,0,False,False,False,1638347660.0
r5yufr,hn63o63,t1_hmrjr2m,"Thank you for the lengthy explanation.

There is just one thing I want clarification on. Does this mean that any program designed for only single-core processors will try to use multiple cores to execute its threads if there are multiple cores available? I ask this because you said ""...written when there was a physical guarantee that 2 instructions on 2 threads would not happen at the same exact time. When it is on multiple cores, this guarantee disappears, and that thread management fails.""

If so, I have another question. Why do other old PC games I play work just fine on my modern PC? For example, the previous game in the series of *Shiny Days* (*School Days*) does not have similar bugs on my system.",1,0,0,False,False,True,1638600628.0
r5yufr,hmqd0e0,t1_hmq7pot,"I'm sorry if I made this unclear but, this isn't a game I coded. Another company made this game and has not made the source code public so I can't tell what language it was written in. Shiny Days is a remake of the game Summer Days (Wikipedia article linked below)

[https://en.wikipedia.org/wiki/Summer\_Days](https://en.wikipedia.org/wiki/Summer_Days)

I was just asking this question because, as a player and as someone majoring in Computer Science, I'm just very curious as to why this workaround works, I couldn't think of anything. I tend to think a lot about why my software acts the way it does when it shouldn't act that way and how the developers could have programmed it wrong.

[https://jast.freshdesk.com/support/solutions/articles/12000055415-school-days-shiny-days-randomly-freezes-and-crashes-when-trying-to-play](https://jast.freshdesk.com/support/solutions/articles/12000055415-school-days-shiny-days-randomly-freezes-and-crashes-when-trying-to-play)

Above is a link to the company's post on this workaround. All they said is that it helps with stability on some CPUs but, I wasn't sure why.

&#x200B;

Also, no it does not crash at all in windowed mode at this point in the game. I cannot adjust the window size either. It is locked at either a specific size in the windowed mode that I can't change or full screen by the developers, so I can't put it at 95%.

By the way, just so you know Visual Novels are usually a series of cartoons/anime where you make very occasional choices that affect the story of the game. Almost like a choose your own adventure game. The scene where it freezes is when it seems to transition from the first scene of the game to the second scene of the game",1,0,0,False,False,True,1638317844.0
r5yufr,hmqde7r,t1_hmq6kfy,"Thank you, for those terms, I'm definitely going to do some research into those it seems interesting.

By the way, this isn't a game I programmed nor is it open source. It just has this huge bug and this workaround temporarily fixes it. As a generally curious person who is also studying Computer Science, I was just interested in why this workaround works so well.",4,0,0,False,False,True,1638318017.0
r5yufr,hmrwydi,t1_hmrj6af,"Exclusive full screen is going to hit different code to windowed - which gives a very good indication of exactly where in the software the bug is without the inherent nature of window vs full screen needing to have much to do with it at all. Windowed being a way to avoid the code with the bug, single-core operation being a way to avoid the bug causing a crash. As for the priority it is to stop other processes from acting on that CPU core and delaying the game's threads (see my top level comment for why it's not actually single threaded though).",1,0,0,False,False,False,1638347850.0
r5yufr,hmrxrp7,t1_hmrwq31,"I'm not giving an impression of anything, I am outright stating that there are literal physical guarantees that apply to single-core CPUs that do not apply to multi-core CPUs. You cannot have 2 instructions simultaneously operating on a single core CPU by definition, you *can* have 
 n instructions operating simultaneously on an n-core CPU. This is a fundamental truth to reality.
 
At some point the code in the game is relying on that one instruction at a time guarantee (which is not a guarantee that gets written into an RFC, it just 'is') - the developers probably never even knew they were relying on it, because it would never have come up in testing on any single-core machine.
 
This is not the sort of thing that will come up if you are doing everything by the book - it comes up when you use hacks to make things run faster. For example (no idea if this is related to why it's happening, but it would show this behaviour) - using thread priority to ensure you don't get into a race condition to avoid the need to declare and check locks or sleep/wake. Works when those threads are on a single core and the prioritised thread is doing its thing, doesn't work when the deprioritised thread has another core it can run off onto.",3,0,0,False,False,False,1638348528.0
r5yufr,hn64hom,t1_hn63o63,"Most of the time it isn't an issue - going from multi-thread single-core to multi-thread multi-core won't break anything if your code is following the correct rules. Like I said, it comes up due to hacking things to break the rules in ways that don't actually break under your testing environments, but leave your code 'fragile' for changes you *didn't* anticipate. (Or I suppose it could also be bugs you put in by mistake that are masked by single-core operation). Most software written back then aren't relying on that guarantee.",1,0,0,False,False,False,1638601218.0
r5yufr,hmqd1uw,t1_hmqd0e0,"**[Summer Days](https://en.wikipedia.org/wiki/Summer_Days)** 
 
 >Summer Days is an erotic visual novel developed by 0verflow, released on June 23, 2006, for Microsoft Windows and later ported as a DVD game and for the PlayStation Portable (PSP). It is the second installation of School Days line of series, succeeding the visual novel of the same name and preceding  Cross Days. Unlike the previous titles, that exist in the same continuity however, Summer Days is a spin-off of the original story retold from the perspective of Setsuna Kiyoura, a high school student out for summer vacation who finds herself attracted to Makoto Itou, a classmate and fellow patron of a restaurant she eventually comes to work at.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",3,0,0,False,False,False,1638317863.0
r5yufr,hmqetgv,t1_hmqd0e0," the game is released in 2006 lmao so it's very likely that's not 100% compatible for new systems, so no wondering it's buggy. The libraries in directx could be new and the game depends on older versions.

So now there are lots of possibilities.",3,0,0,False,False,False,1638318659.0
r5yufr,hmqh70s,t1_hmqde7r,"Ah, got it, I assumed this was something you were working on yourself.

To give you a little bit more information, running a program single-threaded (what your workaround does) causes each line of the program to happen sequentially, as if you were running through the code by hand. Running multi-threaded allows you to run different chunks of code in parallel, giving you way more power, but it’s a perfect example of “With great power comes great responsibility.” If you don’t make sure that the multiple threads “play nice” with each other, you could run into issues.

A *race condition* is some bug that only expresses itself when the threads get scheduled in a certain way. The first example you’re normally given is

> Imagine a program that creates 2 threads, that we’ll call Ping and Pong. The Ping thread will access a global variable, lets say Score, and increment it by 1, 1000 times, and the Pong thread will access Score and decrement it by 1, also 1000 times.

The logical conclusion is that, at the end of the program, Score would be equal to 0 because we incremented it 1000 times and decremented it 1000 times. However, what’ll most likely happen is that it’ll be left with some non-zero value because the threads didn’t “play nice” with it. This is because you need to first access Score in memory, update it, then replace it in memory.

If the Ping and Pong threads grab the value of the Score at the same time, increment and decrement respectively, and then reassign to Score, whose value actually made it? Does the new value of Score reflect the increment that Ping did, or the decrement that Ping did? The answer is: it’s entirely dependent on how your OS schedules the threads. When you learn about this topic in your studies, you’ll learn about things like “semaphores” or, more generally, “locks” which are mechanism to help make sure your threads “play nice.”

Deadlocking is a concept that comes up once locking is being used. In that Ping Pong example, we can make the threads play safe by using a lock on Score, so Ping and Ping are only allowed to modify the value of Score if they currently hold the lock we associate with Score, which prevents the scenario from earlier where they overwrite each other’s work. Once your program gets more complicated and multiple locks are at play, you may end up with a situation where one thread says “I won’t free up/release/give back my lock on **X** until I get I get a lock on **Y**” while another thread says “I won’t free up/release/give back my lock on **Y** until I get I get a lock on **X**”. That battle over locks causes your program to come to a screeching halt, since neither thread will give up their lock until they get the other, which is why it’s called a deadlock.

Hope your studies go well!",6,0,0,False,False,False,1638319730.0
r5yufr,hmrx8rs,t1_hmrwydi,Yup I agree with your statement about prioritization not affecting threaded behavior. I assumed full screen and window mode of any application would be handled by operating system (some window server to be specific) and not dependent on application. Damn.,1,0,0,False,False,False,1638348092.0
r5yufr,hmrdvj2,t1_hmqd1uw,Nice,3,0,0,False,False,False,1638334975.0
r5yufr,hn63xmx,t1_hmqetgv,"That is a definite possibility. Graphics libraries tend to be finicky in older games. I wonder though why this game has such difficulties but other games around this time and games older than it doesn't have this same bug. It is unlike anything I have ever seen and I play a lot of old games, including the game earlier in the series (*School Days*).",1,0,0,False,False,True,1638600812.0
r5yufr,hmrye73,t1_hmrx8rs,"It's mostly handled by the graphics library being used, but will involve some OS code and some game code as well. It's not as true these days with more modern graphic environments, but back in the day exclusive full screen gave a much 'closer to the metal' access to the GPU, whereas windowed had to go via the OS. Now that isn't so true borderless windowed is becoming the standard instead (and DirectX 12 has removed exclusive full screen completely).",2,0,0,False,False,False,1638349051.0
r6bl5i,hmsbnbs,t3_r6bl5i,"I don't know the answer to your question, but the distances (or weights/costs/whatever) in general TSP aren't necessarily Euclidean. The edge weights can, in principle, be arbitrary, and they don't necessarily even conform to the triangle inequality, so the weight of a direct edge from A to C can even be greater than the sum of the weights from A to B and B to C.

The visualization in your image looks Euclidean, but is that what the weights actually are, or is that just what the visualization makes them look like?",1,0,0,False,False,False,1638359937.0
r6bl5i,hmsq3ip,t3_r6bl5i,"The green line segments are not connected; they are not a path. If you want to compare the lengths of the two paths, just compute the length of each.",1,0,0,False,False,False,1638368145.0
r6bl5i,hmsvpkf,t3_r6bl5i,"Can you elaborate on what the green path ia supposed to be? Because right now it is not really a path.

Concorde is a well established program and well known in TCS and OR communities. It is likely that you are misunderstanding something. 

Also, how large is the instance you are trying to solve?",1,0,0,False,False,False,1638370693.0
r6bl5i,hmsygx4,t1_hmsbnbs,I used the concorde executable which generate random euclidean graph,1,0,0,False,False,True,1638371887.0
r6bl5i,hmsykda,t1_hmsq3ip,"Yes sorry I thought it would be clear that I kept the linking edge, I updated the picture",1,0,0,False,False,True,1638371927.0
r6bl5i,hmsy8mp,t1_hmsvpkf,"The instance was 300 random cities, I changed the picture to better show what I mean by the green path",1,0,0,False,False,True,1638371787.0
r6bl5i,hmt2gmr,t1_hmsy8mp,"Well,  the picture is much more clear now. 

Have you checked if the values are indeed better for your solution? Both solutions could very well have the same optimal value.

If you name your nodes from left to right: A,B,C and D.

Concorde's solution is: BC + AB + AD

Yours is: AC + AB + BD

What are these values?",1,0,0,False,False,False,1638373563.0
r6bl5i,hn36u8t,t1_hmt2gmr,"Unfortunately, concorde don't let you see the adjency matrix as far as I know",1,0,0,False,False,True,1638551362.0
r6bl5i,hn6lklt,t1_hn36u8t,"Can you see the coordinates of each node?

I find it strange that you cannot see the instance or save it somewhere. I have not used concorde though, I only know what it does and can be used for.",1,0,0,False,False,False,1638615372.0
r5rwl8,hmq8lww,t3_r5rwl8,"In theory, the only thing your scenario requires for the client to get a response is that an application be listening to the appropriate network interface and port in the server.

In practice, the application doing the listening is usually a [Reverse Proxy](https://en.wikipedia.org/wiki/Reverse_proxy) which can then route requests to the appropriate resources (applications, static files, other sockets) or respond themselves. You can have site1.com route to one application, site2.com route to another, site2.com/thing route to a third and anything else gets a 404 error. Plus it can handle a lot of other useful stuff like certification keys.

Depending on the application stack, other middleware may exist between the client and the server application. 

In Python for instance, web frameworks usually have their apps served behind a WSGI, a Web Service Gateway Interface (or an ASGI, it's asynchronous successor). One of my services, for instance, was made with Flask and hosted with gunicorn and nginx.",6,0,0,False,False,False,1638315872.0
r5rwl8,hmphe0l,t3_r5rwl8,"It really depends on the server and API in question. Using PHP as an example, the NGINX web server communicates to php-fpm (a separate process that handles PHP interpretation) via [tcp or Unix socket](https://www.nginx.com/resources/wiki/start/topics/examples/phpfcgi/), whereas Apache can link to a dynamic library and then do the interpretation itself.",3,0,0,False,False,False,1638304594.0
r5rwl8,hmqj889,t3_r5rwl8,"In order to remove abstractions you first need to understand the abstractions - they're there to help you understand the entire system. Back before the internet when network communications were more bare boned and direct, sure, you could just go learn the entire thing together, but these days there are so many complex moving parts that you'll never understand it properly that way.
 
First go for the TCP/IP layer model (the OSI reference model will probably help with this). Understanding what packets are and how the protocols work and how each component of the network infrastructure is expected to deal with things needs to be the basis of your understanding.
 
That will let you then seek out implementations of each layer, which will do all the things you describe in different ways. Someone has already mentioned how NginX and Apache differ on parts of the Application layer side, but there will be differences in how the network interface layer works (is it wifi, ethernet, etc), then how those interface packets are reconstructed into network layer (IP) packets, then those are reconstructed into transport layer (TCP, UDP) packets, which are then reconstructed into application layer data, and in the other direction too.",3,0,0,False,False,False,1638320642.0
r5rwl8,hmqfnzp,t3_r5rwl8,"> Data Arrives to the network interface for the machine, the driver for network interface card tells the OS that it has data

Yes

> (through an interrupt?)

Maybe, maybe not, depends on how the people who build the network adapter designed it

> The OS then looks at the IP:Port# and if there is a connected socket listening with that IP and Port

Kinda. There is a lot (!!) more to network routing than this, but let's just say the OS reconstructs the TCP packet and gives that to the socket

> Is the server and the API the same process?

Maybe, maybe not. For enterprise applications you usually have a lot of microprocesses all doing different things in the backend. You have a database (or, even larger, a lot of databases all distributed over different datacenters all around the globe while guaranteeing some form of consistency), handler threads, web worker threads which do HTTP parsing, perhaps a bunch of seperate processes for different endpoints etc. Your API server might be a data center with many different machines, all being coordinated and working together.

For very small application you can have a single monolith.

> and the dotnet framework (or any API framework) is baking in the socket code and acting as a server as well?

Again, depends on the framework. Simpler frameworks (like flask) have a build-in web server, more complicated ones may outsource this to nginx or some dedicated web server.

> is it something I'm completely missing?

Not really, everything you mentioned is being done in practice.",1,0,0,False,False,False,1638319044.0
r54to6,hml0u5s,t3_r54to6,I'm reading a good book called Computer Networking: A Top Down Approach. I haven't gotten very far yet but so far it is easy to follow. I'm not sure if that is what you are looking for though. I have no experience in this area.,12,0,0,False,False,False,1638222677.0
r54to6,hmkrfg4,t3_r54to6,"When you get to a concept or term you don't recognize, if the text you're reading doesn't explain it, just stop and check another resource.

For example:

[https://techterms.com/definition/protocol](https://techterms.com/definition/protocol)

[https://en.wikipedia.org/wiki/Communication\_protocol](https://en.wikipedia.org/wiki/Communication_protocol)

then pick up the original text where you left off.

I really doubt you'll find a complete-enough tome that explains \*everything\* in sufficient detail, the body of information is just too big.",3,0,0,False,False,False,1638218929.0
r54to6,hmnpref,t3_r54to6,"Computer Networks - Book by Andrew S. Tanenbaum.
This one helped me in similar situation.",2,0,0,False,False,False,1638277921.0
r54to6,hmo26mj,t3_r54to6,"Start with the CCNA/CCENT, JNCIA or equivalent in today's books. Then as /u/berrmal64 said, look up the unknowns or acronym soup in wikipedia, take a note if you want to come back and dig to learn more, and then move forward.

What I'm telling you as someone who's been in the networking field professionally for 20+ years is that you cannot decompose this field into 'first principles' as it is too vast. i would suggest to you, instead, to be task oriented. Learn what you need about the field to complete the tasks at hand and then learn more in the next research phase of the next task.

Get a starter cert for a networking vendor, but be aware that all vendor certs are biased by design towards their implementation.

I mean, you could read a selection of a few hundred of the first 2000 IETF RFCs, and you might be successful in learning something, but a good starter book and a few online references is a much better place for a beginner than any deep dive.

To start as an entry level network tech, effectively, you need knowledge an inch deep and a mile wide, not the reciprocal. 

I approach all my tasks this way, and spend a bit of time researching deeper on topics I am not completely familiar with today, as a support engineer for a vendor that makes networking equipment even after 20+ years of working in the business.",2,0,0,False,False,False,1638284098.0
r54to6,hmkt5vc,t3_r54to6,"I’ve been teaching myself using professormesser.com. His Network+ study materials are a good starting point for a total beginner regardless if you plan on going for the cert or not. Of course it won’t teach you everything there is to know since it’s geared towards the CompTIA exam objectives, but you can move on towards more in-depth material once you have those basics.",1,0,0,False,False,False,1638219609.0
r54to6,hmm7pfu,t3_r54to6,If you want much basic and theoretical concepts go and watch ravindrababu ravula computer networks in yt,1,0,0,False,False,False,1638241710.0
r54to6,hmlbj3y,t1_hml0u5s,"I second this. This was the textbook I used for my computer networks class. Honestly did not read too much of it, the lecture slides were sufficient (the authors of the textbook created the slides). Still learned lots tho. There’s also tons of reviews questions, practice problems, and assignments. There’s even a website with interactive problems. I feel like it’s got everything you could ask for.",4,0,0,False,False,False,1638227168.0
r54to6,hmnr1nj,t1_hmnpref,"Came here looking for the comment suggesting the Tanenbaum, cheers!",2,0,0,False,False,False,1638278629.0
r54to6,hmoadq7,t1_hmo26mj,thank you sir appreciate it!,1,0,0,False,False,True,1638287573.0
r54to6,hmn36o6,t1_hmlbj3y,"I also used this textbook for two of my classes, but instead of the slides I ended up basically reading the entire thing on my own because I had a timing conflict with my lectures.

Network topics have a tendency to turn into a bunch of alphabet soup for me with all the protocols and their overlapping acronyms, but there’s a ton of visuals in there (and also included in the corresponding slides, I believe) that really helped with recalling everything later on. Also, the way the information is laid out makes it so you form a progressively bigger picture throughout the semester. Was probably the most digestible textbook I had for those reasons.",1,0,0,False,False,False,1638260477.0
r4y6sf,hmmf75m,t3_r4y6sf,Amazing result! Should this not be on the reddit main page?,3,0,0,False,False,False,1638245185.0
r4y6sf,hmma1gf,t3_r4y6sf,Hell yeah! Randomness can catch these hands.,1,0,0,False,False,False,1638242769.0
r4y6sf,hmmwtkm,t3_r4y6sf,"Yeah I grokked maybe 70% of that. 😅
Seems super cool, am going to give it another read later and try to digest it better.


Thanks for sharing 👍",1,0,0,False,False,False,1638255538.0
r4v7w6,hmj2d2w,t3_r4v7w6,"Disclaimer: not a C# programmer but I do know a thing or two about memory.

I'm not 100% sure what you mean by memory stack, as you've mentioned the same theoretical structure used in two different places: the stack data structure. I'm going to assume you wanna know about the ""stack memory"" used by programs during function calls.

First off: there are different types of memory at your disposal when you write programs:

1. Text : this is where the source code of your program resides. If someone (or something) tries to change it, you'll get an error as it's read-only
2. Data and BSS : these two are used by variables and objects that live during the whole lifetime of  your program. So global and static variables are usually stored there (at least in C and C++)
3. Heap : this is dynamic memory which you can request from your OS for your program. Of course, if there's not enough free memory, you'll get some indication of failure (nullptr in C++ and maybe an exception in higher-level languages)
4. Stack : I assume this is what you wanna know about. Whenever you call a function, your compiler has to allocate some space to be used by the parameters and local variables of that function. Also, you want to store a pointer to place from which you called the function, so that you can return to that place after the function finishes executing. Now, this ""stack"" is fundamentally no different than the user-defined stack data structures created by programmers in languages like C and C++ (some higher-level languages provide stack data structures as part of their standard libraries). The practical difference is just that the memory stack happens to be a useful data structure when dealing with function calls and is used by the compiler when it emits assembly code (this is what C/C++ compilers do, not so sure about C#). You can see PUSH and POP instructions if you disassemble an executable. It's what computer scientists call a LIFO structure (last element that goes in is the first one to come out), which makes perfect sense in the context of function calls since you want to free up space (i.e. pop) used by the variables of the last function you called so that the next function has a fresh memory space to work with, while you also want to make sure to keep the memory used by the functions higher up the call chain untouched.

Why is this important? Couple of important considerations:

1. Stack tends to be faster. I say tends to because I've worked in some environments where the difference in performance between stack and heap is negligible. That being said, in many environments stack seems to be much faster than heap. So prefer it whenever you can.
2. It tends to be more memory-constrained than heap. You can't just shove gigabytes of data into it and expect things to go smoothly. Sometimes it can happen even when there is no obvious indication that you're shoving a lot of data into it. For example, remember when I said the location from which you called the function is saved into the stack? Well, in a more popular scenario when a programmer forgets to put an exit condition in a recursive function (google it if you don't know, it's basically a function ending up calling itself infinite amount of times) then you will get a ""stack overflow"" since all those saved pointers end up filling the stack. You can overflow stack much quicker in embedded systems with limited resources, than on a desktop gaming PC.

That's what I can think of now. I hope more experienced programmers will elaborate more on this issue and maybe even find a fault in my own answer",9,0,0,False,False,False,1638192944.0
r4v7w6,hmjqbt9,t3_r4v7w6,"Stack is an overloaded term. It's both a general data structure and also a specific stack is used as you mention as a part of what is referred to as an ABI. The ABI related usage is the one that is used for function calls. Generally its referred to as the ""call stack"" of a program. Your operating system is also managing and using a stack of this type. For more information on the data structure check the C# documentation related to the Stack class in System.Collections. For more information on the ABI related usage of the term, search for information related to what is called the ""calling conventions"" used by the ABI for your computer's architecture.",3,0,0,False,False,False,1638204039.0
r4v7w6,hmko2sv,t3_r4v7w6,"Basically the stack is where local function variables are stored in most programming languages.   
To be clear

    val x = 5 // not a function variable
    def myFunction(y /* not a function variable */) = {
      val z = 10 // function variable
      return x + y + z
    }

So if you had some code like 

    def myFunction2(n) = {
      val pi = 3.14
      // breakpoint 1
      return n + pi
    }
    def myFunction1(n) = {
      val x = 5
      val y = myFunction2(n)
      val z = x + y
      // breakpoint 2
      return z
    }
    myFunction1(1)

then at `breakpoint1` your stack would look like

    3.14
    5

then at `breakpoint2` it would look like

    9.14
    4.14
    5

notice that at `breakpoint2` our `3.14` is gone. because once `myFunction2` returned, then the stack pointer (pointer to the top of the stack) was moved back to the location it was at before the function was called. thus `3.14` was overwritten with `y` and `z`.

hopefully this makes sense to you, if not I'm sure there are lots of great intro CS / unmanaged language classes online that can explain i more depth.

ps. note we never actually ""pop"" the memory stack, we just move the memory pointer back to where it was before a function was called. when we want to reference a value on the stack it's done by accessing the memory address of the value directly.",1,0,0,False,False,False,1638217585.0
r4v7w6,hmm26uf,t3_r4v7w6,"Others' answers give good context. This may be a bit more direct:

The ""call stack"" is basically like a C# `Stack` of ""stack frames"". The OS sets aside memory for the call stack before a program starts. **A stack frame contains all the information needed to return from a function call and give back control to the caller.** This information is usually just a ""return address"", which is a pointer to the instruction that should be executed after returning.

A stack frame can also store local variables and function arguments. This is different from a C# `Stack`, which only stores one type of value. The compiler will keep track of the size of each function's stack frame, so the return address can still be found by popping that many values off the stack.

Source code is compiled into a list of simple instructions that the CPU can execute. These instructions operate on tiny chunks of memory inside the CPU called registers. Some instructions are loads or stores, which move data between registers and RAM. Some instructions are arithmetic, which just manipulate the contents of registers. Some instructions also do both.

**There are usually dedicated registers for the ""stack pointer"", which is the address of the top of the call stack, and the ""instruction pointer"", which is the address of the next instruction to execute.** The instructions that operate on these dedicated registers are the answer to your question. 

Most CPUs have instructions dedicated to manipulating the stack pointer. A `PUSH %reg` instruction will increment the stack pointer and store the contents of the general-purpose register `%reg` at that address. A `POP %reg` instruction does the reverse. The compiler can use `PUSH` and `POP` instructions in pairs, or manipulate the stack pointer directly, to store as many local variables as each function needs without losing track of the return address.

Most instructions implicitly increment the instruction pointer. A `JUMP` instruction instead explicitly overwrites the instruction pointer, so control transfers to a new location. **A `CALL` instruction pushes the instruction pointer onto the stack before jumping, like a combination of `JUMP label` and `PUSH %ip`. A `RET` instruction does the opposite of `CALL`, which is essentially `POP %ip`. This is how function calls and returns use the stack.**",1,0,0,False,False,False,1638239256.0
r4v7w6,hmj329n,t1_hmj2d2w,"One more thing about heap allocations: They can cause memory fragmentation if you use them a lot in your code. Probably not that big of a problem if you don't care about performance, but still, take care not to litter your code with it too many times.",5,0,0,False,False,False,1638193333.0
r4v7w6,hmjr3w2,t1_hmj2d2w,"The only fault I would bring up is that program text is not source code, it is the machine code of your program, produced by translating source code to machine code. The ""code"" bit is an overloaded term, but in this case refers to executable instructions.",2,0,0,False,False,False,1638204357.0
r4v7w6,hmnmpug,t1_hmj2d2w,"Sorry for the late reply was trying to wrap my head around Big O notation last night 🤦‍♂️ but this is great thank you, this makes it a lot clearer to me and I understand it a lot more now",1,0,0,False,False,True,1638276142.0
r4v7w6,hmjrd3b,t1_hmjr3w2,Oops! Should've clarified that!,1,0,0,False,False,False,1638204461.0
r4v7w6,hmkkjbo,t1_hmjrd3b,Well explained sir. The stack OP is familiar with in C# is a data structure. The memory stack is a an application of that data structure in the way compilers deal with particular type of memory.,1,0,0,False,False,False,1638216147.0
r4xo16,hmjmfsd,t3_r4xo16,"Being more specific with parameters would make it a different method. One thing I think you are confusing is the difference between accepting and requiring.

Your code has the `Animal.speak` method which requires a `LoudSpeaker` parameter and returns a string. This means any subtype of `Animal` must have a method that *accepts* a `LoudSpeaker` and returns a string (or string subtype?).

Being more specific (covariant) with the return type doesn't violate the method contract. If you returned a subtype of string (is that possible with string?) your are still by definition returning a string.

Being more specific with the parameters does violate the method contract. If the `Cat.speak` method *required* a `ScreamingSpeaker` (i.e., the method definition specifies it) then it can no longer *accept* a `LoudSpeaker`. Keep in mind that the  `speak` method can still *accept* a `ScreamingSpeaker` when you call it. If you wanted `Cat` to only take `ScreamingSpeaker` you could detect the type of the passed argument and throw an exception (or return null or whatever) if it is not a `ScreamingSpeaker` but since the method signature did not change that is **not** being covariant with the parameters.

Being less specific (contravariant) with parameters is allowed because it still fulfills the method contract. If `Cat` can take (requires) a more general `Speaker` in it's `speak` method, then it still *accepts* a `LoudSpeaker`.

I'm not sure what you mean on line 37 with ""According to LSP I shouldn't pass in LoudSpeaker here?"" because that is wrong. The `Animal.speak` method *requires* a `LoudSpeaker` which means it can also *accept* a `ScreamingSpeaker`.

Edit: Used OP's example code and fixed mixing up terms. This is why I need coffee in the morning.",3,0,0,False,False,False,1638202418.0
r4xo16,hmk55ut,t1_hmjmfsd,"> If the Cat.speak method required a ScreamingSpeaker (i.e., the method definition specifies it) then it can no longer accept a LoudSpeaker. Keep in mind that the speak method can still accept a ScreamingSpeaker when you call it.

I understand I can pass in anything at runtime. What I'm saying is that Liskovs Substitution Principle dictates (as I understand it) that if the parent class's method (Animal.speak) requires a Loudspeaker, then the child class's method (Cat.speak) can only require a Loudspeaker or it's parent Speaker as a parameter. Regardless of what can be passed in at runtime.

So the methods themselves, because they are in a subtype relationship, must be *contravariant*.

However, any other function, in order to adhere to LSP must be *covariant* in their parameters.

There still seems to me to be a disconnect here. I'm sure I'm still misunderstanding something, I don't think I've found a formal flaw or anything like that but on it's surface it appears that LSP has an implicit contradiction.

Edit: Think about it like this.

Forget about the classes. I'm looking at all the functions in that code whether they are methods or not, and I'm wondering why some functions have to be covariant in their inputs and adhere to LSP and why some (subtype methods) have to be contravariant in order to adhere to LSP.

I'm trying to tease out what implicit assumption I'm making to make it seem like that.

Words are hard, sorry. lol.",0,0,0,False,False,True,1638210001.0
r4xo16,hmk8tvp,t1_hmk55ut,"Don't doubt yourself too much. It's a really good sign that you are questioning principles without immediately thinking you've found something wrong with it.

&#x200B;

>I understand I can pass in anything at runtime. What I'm saying is that Liskovs Substitution Principle dictates (as I understand it) that if the parent class's method (Animal.speak) requires a Loudspeaker, then the child class's method (Cat.speak) can only require a Loudspeaker or it's parent Speaker as a parameter. Regardless of what can be passed in at runtime.  
>  
>So the methods themselves, because they are in a subtype relationship, must be contravariant.

This is nearly correct. The *parameters* of the method *may* be contravariant. The `Cat.speak` method can require a `LoudSpeaker` or any parent-type in the method definition. This allows the method to still accept a `LoudSpeaker` but it may also accept a more general (parent) type.

&#x200B;

>However, any other function, in order to adhere to LSP must be covariant in their parameters.

Not quite sure what you mean here. Are you referring to the parameters that are passed in when the function is called? Taking the function with the definition `speak(LoudSpeaker $speaker)` and calling it with `speak(new ScreamingSpeaker())` is not covariance it's just polymorphism. A `ScreamingSpeaker` *is* a `LoudSpeaker` so it can be taken as a parameter. If `Cat` overrides it with `speak(Speaker $speaker)` (being contravariant), then you can still call it with `speak(new ScreamingSpeaker())` but you could also do `speak(new Speaker())` or `speak(new QuietSpeaker())` where `QuietSpeaker` exteneds `Speaker`.

To help clear a little confusion, the types that a function takes in according to it's definition are generally referred to as **parameters**. The actual values that are passed in are generally referred to as **arguments**.",1,0,0,False,False,False,1638211457.0
r4xo16,hml4xnj,t1_hmk8tvp,"> This is nearly correct. The parameters of the method *may* be contravariant.

Wikipedia and everywhere else states that in order to adhere to LSP, it **must** be contravariant. Meaning that you must allow for either what the parent class expects as a parameter, or something more generic than that.

[Contravariance of method parameter types in the subtype.](https://en.wikipedia.org/wiki/Liskov_substitution_principle)",1,0,0,False,False,True,1638224372.0
r4xo16,hmlfa88,t1_hml4xnj,"That is what I meant by ""may"". It either is the same as the parent class, or it is contravariant and it takes a more generic type.  
  
I suppose some may say that it is contravariant even if it uses the same type but that seems odd to me.",1,0,0,False,False,False,1638228826.0
r4xo16,hmlhk3i,t1_hmlfa88,"But one thing you *can't* do is pass in a more specific type in the subclass's methods. I mean you *could* but you'd be in violation of LSP. And therein lies the issue for me. In one sense, LSP is saying that for subclassing, you have one rule in the method's parameters (contravariance), but in other instances it calls for covariance in function signatures. I'm not sure how to square that.",1,0,0,False,False,True,1638229844.0
r4xo16,hmlnuhy,t1_hmlhk3i,"Passing in a more specific type *is* LSP, not a violation of it.

    signatures:
    Animal.speak(LoudSpeaker) // Base signature
    Cat.speak(Speaker) // Contravariance, no LSP violation
    
    function calls:
    animal.speak(new LoudSpeaker()) // Normal call
    animal.speak(new ScreamingSpeaker()) // ScreamingSpeaker is a subtype of LoudSpeaker, no LSP violation
    cat.speak(new Speaker()) // Allowed because of the contravariance in the signature
    cat.speak(new LoudSpeaker()) // LoudSpeaker is a subtype of Speaker, no LSP violation
    cat.speak(new ScreamingSpeaker()) // ScreamingSpeaker is also a subtype of Speaker, no LSP violation",1,0,0,False,False,False,1638232730.0
r4xo16,hmlvgif,t1_hmlnuhy,"> Passing in a more specific type is LSP, not a violation of it.

I agree. That's how I've always understood it.

However, when it comes to method parameters of a subtype, LSP is pretty explicit that you *should NOT* pass in a more specific subtype than what is required. You can however pass a more generic parent type.

In Wikipedia and everywhere else I've read it says explicitly that LSP requires subtype method parameters to be **contravariant** in their input.

Do you not agree that to be **contravariant** in your input means that you cannot pass a more specific subtype in? I think that's where we're both getting stuck.",1,0,0,False,False,True,1638236200.0
r4xo16,hmlwtt3,t1_hmlvgif,"It does **not** mean that you cannot pass a more specific variant. It means that you cannot **require** (in the method definition) a more specific type.

You have to think of the *use* of a method and the *definition* of a method separately.

>However, when it comes to method parameters of a subtype, LSP is pretty explicit that you should NOT pass in a more specific subtype than what is required. You can however pass a more generic parent type.

LSP is explicit that the method must support *at least* the specific type, but it may support a more general type.

It looks like you are misusing the term ""passing in"". Passing is **only** referring to the *use* of a function. Accepting/requiring/taking refers to the *definition* of a function. Contravariance is referring to what a function can **take** as an input, not what you actually **pass** to the function.",1,0,0,False,False,False,1638236852.0
r4xo16,hmlxnkh,t1_hmlwtt3,"> Contravariance is referring to what a function can take as an input, not what you actually pass to the function.

Right. As I've said before I understand that you can pass in anything you want.

I think we're getting hung up on some of these terms.

How about this. I can just reformulate the question. Why does LSP *require* a subtype's method parameters to be *contravariant*?

And then, why does LSP require that any other function that accepts a type be *covariant*? And why are those two seemingly different?

Also, you are the most patient person in the universe and I offer my many thanks. :)",1,0,0,False,False,True,1638237219.0
r408nu,hmi3lln,t3_r408nu,Oh damn them feels. I’m a CS major and all that tawk about how you’ll get rich working for Big Tech or Silicon Valley really made the field seem off putting. I wish that they taught CS (and other STEM fields) like how they teach philosophy.,3,0,0,False,False,False,1638166818.0
r408nu,hmi5qz6,t1_hmi3lln,Yes yes,3,0,0,False,False,False,1638168307.0
r3rold,hmcfxo6,t3_r3rold,"Some of the simplest solutions include hosting the resume on a GitHub repository, or hosting it on a website linked to from the GitHub profile. If you're set on a cryptographic solution, though, some possibilities include:

* You include your public key on your GitHub profile, and a signed message in your resume, or vice-versa

* You could use some kind of [identity proofs, like what KeyBase does](https://book.keybase.io/guides/proof-integration-guide). This can prove that the same human controls a website, GitHub profile, Twitter account, etc.

Seems over-complicated to me, though, I'd stick with ""my CV is on my website, which is linked to from GitHub, and my CV includes links to both my website and GitHub profile.""",10,0,0,False,False,False,1638062627.0
r3rold,hmdhrj4,t3_r3rold,The employer could simply ask the interviewee to login and show them that they own the github account? But some type of cryptographic signature seems like a good idea.,4,0,0,False,False,False,1638083159.0
r3rold,hmdc5ac,t3_r3rold,"In the case where you want only one way verification and *don't* want to overtly tie your github username to your actual name, the company can just ask the person to add something specific to their bio, comment on an issue on a company repo, etc.",3,0,0,False,False,False,1638079430.0
r3rold,hmdyvz1,t3_r3rold,"I have a plain text file with URIs to my profiles on social media etc (GitHub, Twitter, Facebook…) signed with my PGP key and hosted on my website.",3,0,0,False,False,False,1638096439.0
r3rold,hmew0db,t3_r3rold,"This is pretty much the same problem certificate providers have to solve. They have to check is someone is the owner of a website.

Usually the prove is done by creating a specific file with a content given by the certificate provider on the website. An other way is to set a HTTP header with a value given by the certificate provider. However, the idea is always the same: Provider gives you a secret and the owner has to place it on the website.

To not have to place a secret on the github page for everyone that want to verify the owner. The owner could place a public key. The verifier can than send a challange (random string) to the owner, the owner can than sign the challange with the private key and send it back. If the verifier can verify the signature with the public key from the repository it ownership is proven.",3,0,0,False,False,False,1638115902.0
r3rold,hmcjczr,t1_hmcfxo6,That seems to be a concise explanation. Thank you.,2,0,0,False,False,True,1638064246.0
r3cbfk,hmacs8x,t3_r3cbfk,maybe also ask also on physics/math related subs/forums,8,0,0,False,False,False,1638029945.0
r3cbfk,hmbz32e,t3_r3cbfk,You would probably have better luck in an engineering or physics sub.,8,0,0,False,False,False,1638054905.0
r3cbfk,hm9uvqv,t3_r3cbfk,"[https://www.youtube.com/watch?v=qsYE1wMEMPA](https://www.youtube.com/watch?v=qsYE1wMEMPA)

[https://www.youtube.com/watch?v=uG2mPez44eY](https://www.youtube.com/watch?v=uG2mPez44eY)",3,0,0,False,False,False,1638021044.0
r3cbfk,hmdg911,t3_r3cbfk,Look at some of the MOOC sites!,1,0,0,False,False,False,1638082137.0
r3cbfk,hma4nxr,t1_hm9uvqv,"Thanks for advice, sadly i've ready watched them :D",2,0,0,False,False,True,1638026187.0
r3exb5,hma1zlm,t3_r3exb5,[deleted],1,0,0,False,False,False,1638024879.0
r3exb5,hma5q1s,t3_r3exb5,"There are 2^16 (65536) locations, each storing 16 bits (2 bytes, also called a *word*).  65536x2 (bytes per location) is 128kb.

It's _addressable_ with a 16-bit number, meaning one 16 bit number can represent any of the locations.",1,0,0,False,False,False,1638026694.0
r3exb5,hmdg805,t3_r3exb5,"It’s a common misconception that 128 kb = 128,000 bytes. It’s actually 2^17 = 131,072. This is the same concept that 1 kb does not equal 1,000 bytes, but rather 1024.",1,0,0,False,False,False,1638082118.0
r3exb5,hma30v2,t1_hma1zlm,16 * 65536 = 1048576 bits? which is not 128 kilobits?,1,0,0,False,False,True,1638025398.0
r3exb5,hma88bc,t1_hma5q1s,"Is this correct? I might be having a stroke, because for some reason I don't get this at all, I might've understood something completely wrong because:
65536x2 = 131072 bytes? which is => approx 1048kb (kilobits).

So I've 65,536 locations
If I store 16 bit worth of something into every single location available it should be exactly 1048576 bits. 

This is my thought process and for some reason I don't get this at all. Be it 16 bits or 2 bytes, essentially it should be the same? As in 1 byte is 8 bits and 2 bytes is 16 bits, hence your explanation seems exactly the same as mine but using bytes?",0,0,0,False,False,True,1638027863.0
r3exb5,hma8z15,t1_hma88bc,"Yeah I don't think any of that is wrong (except kb is kilobytes, not kilobits).  There are 2^16 slots, each slot can store a 16-bit number, so that's 1048576 bits.  Kb here is kilobytes, which is the standard for storage (always in bytes, not bits).",2,0,0,False,False,False,1638028205.0
r3exb5,hma9r1w,t1_hma8z15,"Is the material wrong? Because it says 
> This means it can store a total of only 128kb

As in kilobits, not in kilobytes. If the material would have kB in it, I would understand it, but no matter how much I wrestle with the bits and bytes here I cannot see how the memory is only 128 kiloBITS in size.",1,0,0,False,False,True,1638028559.0
r3exb5,hma9wrg,t1_hma9r1w,"Nobody measures storage in bits.  Storage is in gigabytes, megabytes, kilobytes, or bytes.  KB is kilobytes.  The material is correct.  It's kilobytes though, not kilobits.  You can't rely on seeing KB instead of kb.  You may often see gb, mb, or kb...they're all in bytes.",2,0,0,False,False,False,1638028634.0
r3exb5,hmegtql,t1_hma9r1w,"Technically yes, but lots of people don't bother to get the unit symbols for bits and bytes right.",1,0,0,False,False,False,1638108611.0
r3exb5,hmegkd5,t1_hma9wrg,"Technically, OP is correct and the material seems to be a bit sloppy about the unit symbols. Traditionally (and also according to an [IEEE standard](https://en.wikipedia.org/wiki/IEEE_1541-2002)), an uppercase B is used for bytes and a lowercase b is for bits. It's technically wrong to use ""kb"" for kilobytes.

Of course nobody actually bothers to get it correct nowadays, so you're right that you need to kind of figure it out from the context rather than relying on notation actually being correct. However, considering that the page is about rather low-level stuff that involves both bits and bytes, one might reasonably expect the author to actually clearly distinguish between the two. OP's confusion is understandable since they apparently learned the unit symbols right at some point, although it's not much of a stretch to figure out the author of the documentation just didn't bother to get the units right.",2,0,0,False,False,False,1638108473.0
r3exb5,hmgj1p9,t1_hmegtql,"Yeah, I wasn't truly aware of that to be honest and I'm ok with it. I genuinely was just very confused because I literally googled the meaning of ""kb"" and realized it meant kilobits and rest is history. But yeah, I thought that material being rather low-level would've super precise details and didn't consider the possibility of the context at all.   


You live and learn. Now I know a ""bit"" more! :)",1,0,0,False,False,True,1638139827.0
r39ah9,hmambok,t3_r39ah9,"Fetching images from RAM and context switching is expensive.  To do that many times in a single frame while rendering slows everything down.

Atlasing allows you to pack textures into a single file, normally minimizing the amount of empty space around sprites (thusly size required to store images in memory), and store metadata which is useful in the logic and rendering part of your code.  

The GPU can now use one or two textures instead of switching context between multiple textures residing in memory.  It is hard to understate how big of an improvement texture atlases can be to your rendering times (and it is worth doing because it is simple to do).  A good experiment would be to build two like projects rendering a hundred 2D sprites, stationary, but one utilizes an atlas and the other has every sprite in a separate file.  You will see the importance then.

One of the techniques I like, since I don't like having all of my images in one file, is to build atlases on the fly in memory during scene changes in the game.  I'll have all relevant information for the graphics at that point.  Some images that may not repeat a lot I usually won't put into the atlas because it doesn't make a great difference, but for the most part I always have character textures, equipment textures, and scenery textures in there.  

There is a plethora of really in depth information about this and context switching out there if you want more detailed information.",3,0,0,False,False,False,1638034105.0
r39ah9,hmdkt68,t3_r39ah9,"You are ABSOLUTELY right!

One big texture or 20 small textures, should be the same, problem is old OpenGL is fucked.

Basically the trick is that texture unit filling is expensive, for some FUCKED reason they didn't think games would need many textures, and many of us are still paying for this shortsightedness.

The issue is larger actually, GPUs are great once data is resident but their ability to stream data in and out is very unsupportive.

Overall writing games is so hard that packing textures is never that bad but i totally agree that we should talk openly about how this is not something we (as game devs) should really be dealing with.",1,0,0,False,False,False,1638085253.0
r39ah9,hmdohu5,t1_hmdkt68,But isn't opengl an ever updating standard? Shouldn't some new version fix this?,1,0,0,False,False,True,1638088085.0
r39ah9,hmgjl2u,t1_hmdohu5,New versions do fix it but they are less compatible,0,0,0,False,False,False,1638140052.0
r2ye5m,hm7sdr2,t3_r2ye5m,You will get completely different answers based on WHAT is being built. A static we site requires a different stack from a web service than a video game than a video editing program than a social media app. Tech are tools and a good computer scientist simply uses the tools for the jobs they were built for.,24,0,0,False,False,False,1637972552.0
r2ye5m,hm8740j,t3_r2ye5m,"If I was being pedantic (and of course I am because I’m a Computer Scientist) I would point out that this is not something you can have an opinion on. 

It’s a piece of factual information that you may or may not be able to find an accurate answer to. As someone else said it’s almost certainly HTML/JS by the numbers.",12,0,0,False,False,False,1637980014.0
r2ye5m,hm8w63k,t3_r2ye5m,Go to job boards and sort openings by language. See which language has most openings.,2,0,0,False,False,False,1637994192.0
r2ye5m,hm85z8p,t3_r2ye5m,"In sheer volume? HTML, CSS, JavaScript hands down.",8,0,0,False,False,False,1637979442.0
r2ye5m,hm8rjqi,t3_r2ye5m,"For backend, Java and sql",4,0,0,False,False,False,1637991219.0
r2ye5m,hm98t3h,t3_r2ye5m,.NET ecosystem,2,0,0,False,False,False,1638003911.0
r2ye5m,hm8fbmu,t3_r2ye5m,"HTML/CSS/Javascript.

Backend? PHP - legacy languages always dominate volume metrics.

If you love legacy code bases, or want to be in the same position as COBOL developers are now, those are the languages to learn.",0,1,0,False,False,False,1637984266.0
r2ye5m,hm8tmqy,t3_r2ye5m,"I assume that you are asking this question to decide upon what stack to choose to learn. It would have been a lot easier if you had stated a specific niche it would have been easier to answer. Learn a general programming language like Python which you can use anywhere(web, ml, application programming, etc). If it is web, then I recommend frameworks like node.js for an easier learning curve, choose go and/or rust for a steep learning curve but better perf.",1,0,0,False,False,False,1637992515.0
r2ye5m,hma30ju,t3_r2ye5m,"as far as legacy it's xAmp (os of choice, apache for server, mysql for db, and php for code base language) based stacks but I personally don't forsee many new projects being built with this but it is what most of the internet is currently built with. 

Newer stacks that are common would be mean, mern, mevn and conversely fern, fevn, fean stack (f or m for mongodb or firebase dB, express js for server, [angular, react, or vue js for front-end framwork], and node js for server-side code-base.

Ruby on rails had its moment in the sun IMHO and is still quite valid as a framework but you could also argue is not nearly as popular as a few years ago.

In the end it has less to do with what is popular over all since that can vary regionally, and more to do with choosing the right tool for the right job. Generally speaking they all do the same thing, which is provide the devs/engineers a blueprint to fast track the build of an application. It is your job as the engineer however, to figure out which will provide the most useful functions for what you need.

also take any opinion of mine or otherwise, with a grain of salt",1,0,0,False,False,False,1638025394.0
r2ye5m,hmb1tth,t3_r2ye5m,We’ll with cloud migration u can have several stacks to pick and choose from. Now ur apis can be made with anything and consumed together through something like aws lambda or fargate. Ur front end can be mixed as well. Really depends on what most your companies code was built with prior though,1,0,0,False,False,False,1638040619.0
r2ye5m,hm9ozwd,t3_r2ye5m,TCP/IP.,0,0,0,False,False,False,1638017354.0
r2ye5m,hm8jni4,t1_hm85z8p,"That's not a stack, that's just the front end...",16,0,0,False,False,False,1637986622.0
r2ye5m,hm9i502,t1_hm8rjqi,I thought PHP was still more popular,2,0,0,False,False,False,1638011983.0
r2ye5m,hm9d1ll,t1_hm8rjqi,"Really, I didn't know java is still widely used.",1,0,0,False,False,True,1638007557.0
r2ye5m,hmavex1,t1_hm98t3h,Must be a Microsoft guy lol,1,0,0,False,False,False,1638037905.0
r2ye5m,hm8jzw6,t1_hm8fbmu,I have seen more Java/Spring backends judging by global contractor and job opportunities.,8,0,0,False,False,False,1637986816.0
r2ye5m,hm8g91q,t1_hm8fbmu,"Would Personal Home Page ever really be like COBOL? It’s not too different from Java or C in syntax, COBOL is sort of a different beast",1,0,0,False,False,False,1637984762.0
r2ye5m,hmb4lri,t1_hmb1tth,I have never undrstood the serverless architecture. I will have to do research on lambda or fargate…,2,0,0,False,False,True,1638041753.0
r2ye5m,hm8rlvz,t1_hm8jni4,"A Stack is just a complete set of systems that can run independently in its entirety without the need for any extra jazz. Millions of websites are written without the need of a backed that do more than just display text. JavaScript is Turing Complete, so you can put all of the work client-side without issue. Yes, you're right, that's just the front end, but that doesn't exclude it from being a Stack.",7,0,0,False,False,False,1637991256.0
r2ye5m,hm9hici,t1_hm9d1ll,"In enterprise backend systems, Java is by far the dominant language still. Other languages like Golang are getting more popular for sure, but there's still a lot of Java code out there and will continue to be so for the foreseeable future",4,0,0,False,False,False,1638011433.0
r2ye5m,hmacayf,t1_hm9d1ll,"Yeah I've worked at a few large companies. Amazon for example is almost entirely Java. Tried and true, to deliver fast, dont have time to experiment with other tech. Also more resources to reference internally.",2,0,0,False,False,False,1638029727.0
r2ye5m,hmau74v,t1_hm8jzw6,PHP is the [overwhelming](https://w3techs.com/technologies/details/pl-php) most popular backend language.,1,0,0,False,False,False,1638037389.0
r2ye5m,hm8jxxe,t1_hm8g91q,"A decent Python programmer can side-skill in PHP in about a week to be productive, a year to be the equivalent of a senior dev. Totally different to COBOL both in terms of language paradigms and what is done with the language. The reason COBOL devs are in such high demand is because COBOL runs core banking software that is simultaneously very difficult to test and absolutely cannot, ever, ever fuck up.",1,0,0,False,False,False,1637986785.0
r2ye5m,hmbbv9k,t1_hmb4lri,"It’s def worth learning the basics. U can essentially set up any api u want with any architecture to be consumed as a service. So at my job we got node, spring boot, python backend apis and either angular or node front end. I believe some react as well, but not as much",1,0,0,False,False,False,1638044736.0
r2ye5m,hm8tbvk,t1_hm8rlvz,"There is no website in the world that is written without the need for a backend. Some way and some how you have to get that website to the end user, even if that way is walking up to them and handing them a USB stick. That is what a 'stack' is, it includes the web server that is serving those pages even if once they are served they don't need any more info to or from the server.",-10,0,0,False,True,False,1637992321.0
r2ye5m,hm9ev9x,t1_hm8jxxe,I once did pair programming that touched my companies billing code in PHP and it felt really tough and scary. I can’t imagine doing the same but in COBOL for a massive bank.,0,0,0,False,False,False,1638009155.0
r2ye5m,hm8x7y6,t1_hm8tbvk,"Saying you still have to host the site doesn't really go to the OP's original question of ""What's the most popular stack"" because the backend is totally irrelevant for a front end stack. There are millions of websites that don't care how you host them, S3 buckets, GitHub Pages, whatever, so saying that a Stack is ""HTML, CSS, JavaScript, and some kind of hosting solution"" is pointless, just like it's pointless to say that you technically need a browser to interact with websites that have a UI.",7,0,0,False,False,False,1637994896.0
r2ye5m,hm903ft,t1_hm8x7y6,"Except that choice is literally what a stack is. Yes, you can swap out a SQLServer database for MariaDB for Postgres and the site won't give much of a shit, but when someone says 'what stack are you using' your choice of RDBMS is a part of that. Words mean things whether you want them to or not.",-6,0,0,False,True,False,1637996915.0
r2ye5m,hm93her,t1_hm903ft,"So you would have felt better if I said ""HTML, CSS, JavaScript, Static Hosting""? Fine then, that's my answer if it'll make you feel better. You don't need a database to have an application.

How would you answer OP's question? By rejecting its phrasing and saying it's a bad question?",6,0,0,False,False,False,1637999522.0
r2ye5m,hm94h84,t1_hm93her,"It's not a bad question at all, you just answered in a way that is misleading to a newbie who wouldn't know any better. Static hosting is in no way the most common way an application is served, so your answer is wrong in any event. Java EE is the most common in traditional commercial settings, LAMP/MERN elsewhere.",-2,0,0,False,False,False,1638000311.0
r2sjef,hm71zhm,t3_r2sjef,"I suspect part of it would be the fact that while you’re editing video, the editor has it in a format optimized for editing rather than the usual playback-optimized format. When you export it, it converts it into one such (compressed) playback-optimized format, so the output video runs a lot smoother than a video being edited would.",15,0,0,False,False,False,1637959954.0
r2sjef,hm751pp,t3_r2sjef,"Hardware support for playback. H264 format and other codecs are well supported by hardware decoder in most devices. 

A video editor will open the files and read and convert to its internal format, likely a RAW video format. This takes time and a lot of CPU power.",9,0,0,False,False,False,1637961378.0
r2sjef,hm868i8,t3_r2sjef,"The version of the video inside of the editor is separated into parts that make is easier to edit.   The regular version is optimized for size and playback.  When the the video is exported, the whole thing has to be converted from a list of editable files to one long file that is encoded in some regular playback format.   When the editor is playing back a version, it is converting those parts  while you are watching it.",5,0,0,False,False,False,1637979573.0
r2sjef,hm9sa0e,t3_r2sjef,"Video compression codecs work on a delta basis: the next frame’s data is often stored as the differences from the previous frame. For normal playback, you have the previous frame data, get the delta and calculate the next frame, repeat. Plus this job can be offloaded when hardware decoding support is available. For anything else: like non-linear video editing, where you need a preview of the whole video in the timeline and jump around in a non-linear fashion, the whole thing basically needs to be decoded up front.",2,0,0,False,False,False,1638019477.0
r2sjef,hm6myk6,t3_r2sjef,This is better suited for /r/AskReddit,-24,0,0,False,True,False,1637952897.0
r2sjef,hm7pp6p,t1_hm751pp,This. To do complex calculations on each frame the video would be stored in a raw format. If you wanted to preview the changes being made you have to calculate each frame in real time from a raw format.,6,0,0,False,False,False,1637971213.0
r2sjef,hm6nm8s,t1_hm6myk6,"Thanks, I will post it there as well.",-4,0,0,False,False,True,1637953205.0
r2a6yz,hm443re,t3_r2a6yz,"A BIOS software image is flashed with specialized software, and hardware that applies power to the chip and uses a chip-specific protocol to write the image.  The hardware can either be a specialized harness or board, or after installed on a full board (in-circuit programming).  That's how programming software directly onto most microchips works.  Chips themselves need ways of getting software loaded directly to them, so manufacturers provide that.

The BIOS is the software responsible for several tasks in a computer, one of which is loading the bootloader software, which locates and loads the full OS.",62,0,0,False,False,False,1637897580.0
r2a6yz,hm41xce,t3_r2a6yz,"After a CPU powers up, it starts executing instructions starting from a fixed address. Exactly what address, and the power up sequence leading up to, can vary based on the specific hardware. A ROM chip (there a number of kinds) that has the initial 'operating system' on it is wired up so that the memory the CPU starts executing from, corresponds to the address of that chip. 

For the last 25-35ish years, that initial 'operating system' is a small program that generally called a boot loader. It is very small, and able to just enough to to read the operating systems from disk into the memory, and then jump to the start of that memory. 

That's the generic idea of how the hardware works. How'd the text of code to get be compiled code on that ROM chip? Someone typed it into a computer, used a computer to compile it, and a computer to write it to a ROM. 

Of course, if you go back far enough, you'd have a time where you had a computer you wanted to run something, but not a computer to write/compile your program separately on. Initially the programs were translated to their binary representation by hand, and the bits were loaded into a computer manually. Literally set a series of 8 switches to the bit values of the byte you want, then press a button to load it to memory and more on the next byte. 

There has been a long evolution of tech between manual switches and using a fully functional, separate computer. It should be pretty obvious that once you have a computer that will do part of the tedious process for you, you'd use it. You'll find this progressive boot strapping of technology all over engineering. You build tools, so you can build better tools.",13,0,0,False,False,False,1637896418.0
r2a6yz,hm4tkkn,t3_r2a6yz,"When we load the operating system onto our memory for operations, we call that process 'bootstrapping' or 'booting', hence, we are 'booting' up the computer. Booting is done via a 'boot loader' program.  This program is what we are interested in here, the bootloader changes depending on what kind of device we are working with. In modern devices, the boot loading process is sophisticated, so we employ multi-stage bootloaders to be able to boot up a system using chain loading.   
As the name 'chain loading' suggests, your boot loader loads up something and then proceeds to load another boot loader which does the next process. 

Your bootloader firmware is installed together with the BIOS, in non-volatile memory (basically your ROM). This is what you'd call a stage 1 bootloader, it simply starts up, detects your boot device, and then moves to the next stage. (The BIOS does a lot more at the initial stage, but we aren't concerned with it for now).

The Master Boot Record is the first disk block and this is where the real booting process starts. This is the very beginning of your disk partition. The MBR stores the first stage boot loader and the disk partition table.

After the BIOS phase is completed, MBR starts to scan the partition table and loads up the Volume Boot Record, VBR is basically what partitions (and what kind) this particular disk has.   


Just like MBR, VBR's initial segment has what partition type and size the partitions in this particular volume are, followed with the Initial Program Loader (IPL).   
This IPL is our second stage bootloader, which is basically coded to actually load our operating system. (Depending on what kind of operating system you are using, IPL will load up another program that will actually bring the contents of the operating system to the main memory and make it ready for execution).    
Like for example, IPL will load up the NT loader for windows which actually loads up the content of the windows operating system. Or IPL will load up GRUB for Linux.  These would then follow different stages to load up the operating system. 

All in all, it's a complex chaining process that starts from static code and then points you to another location where more sequence of actions are stored. 

(Also most machines now use UEFI over BIOS).  
(Another fun fact, this process can be abused to load up malware or even prevent the completion of the bootloader process. A lot can be done with this since the initial code is actually a static sequence, and if able to tinker with it, the threat actor has a lot of power) 

ps: sorry if this doesn't make sense xD",6,0,0,False,False,False,1637913935.0
r2a6yz,hm3j11z,t3_r2a6yz,The motherboard already includes software: UEFI.,4,0,0,False,False,False,1637886448.0
r2a6yz,hm4vprj,t3_r2a6yz,This is a hard concept to grasp without first understanding BIOS and firmware. I would look up some YouTube videos about them. At the airport right now but when I have some time I’ll come update with some links,5,0,0,False,False,False,1637915628.0
r2a6yz,hm3t7cf,t3_r2a6yz,They burn a small program into Read-only Memory (ROM or EEPROM) that does some initialization and look to read in the OS when the computer is first turned on.,3,0,0,False,False,False,1637891790.0
r2a6yz,hm5gyx9,t3_r2a6yz,"Bootloader (BIOS in PC context, firmware in others) is basically a mini-OS that can read/write disc, it allows for installation of bigger OS.

How do you get a bootloader onto your computer? Depends on the architecture:

* in x86, it comes in it's own chip on your motherboard
* in phones/embedded, it could be the first partition of the main storage disk. To install first time, you take out the disk and write to it with another device. Afterwards, the bootloader can support self update through something like TFTP or reading specific file on a removable disk. (Hence the reason, you can brick the device by bad update)",2,0,0,False,False,False,1637932800.0
r2a6yz,hm70jej,t3_r2a6yz,"The computer has rom (or read only memory) that gets loaded with instructions for how to boot

The data gets loaded on electronically, ie the chip has pins that they use to load data onto it in the factory so it already has the data on it when it gets assembled and sent to you",2,0,0,False,False,False,1637959284.0
r2a6yz,hm45vmz,t3_r2a6yz,We have this post every day.,-8,0,0,False,True,False,1637898544.0
r2a6yz,hm58tdh,t3_r2a6yz,"The best way to understand it, is to DIY with linuxfromscratch. The experience of putting your own OS together is satisfying.",-1,0,0,False,False,False,1637926903.0
r2a6yz,hm44isi,t1_hm443re,"That was a very informative. What is a BIOS software and when you say specialized software do you mean software specifically made to write a program onto the chip in question? Finally, what chip is it written into? The CPU?",8,0,0,False,False,True,1637897807.0
r2a6yz,hm5jvh5,t1_hm4tkkn,This guy has it sus succinct and accurate,1,0,0,False,False,False,1637934549.0
r2a6yz,hm3j6i2,t1_hm3j11z,How does the software get onto the motherboard and on which chip is it located?,5,0,0,False,False,True,1637886529.0
r2a6yz,hm58fm6,t1_hm3j11z,"Bios/UEFI is firmware, not software.",1,0,0,False,False,False,1637926580.0
r2a6yz,hm3upzs,t1_hm3t7cf,">that

Did you mean ""then"", or did you mean ""that does""?


What is the ""initialization""? Does it just mean set up?


Finally, thank you very much.",2,0,0,False,False,True,1637892590.0
r2a6yz,hm73q9e,t1_hm70jej,">The data

Did you mean the ROM data?",1,0,0,False,False,True,1637960765.0
r2a6yz,hm45m5d,t1_hm44isi,"BIOS software (or it's UEFI counterpart) is the software that lives directly on a computer (stands for ""Basic Input/Output System""), not a hard drive like your OS (windows or whatever).  Its usually written by the people who make your motherboard (or logic board or whatever, depending on what kind of device were talking about).  It has many jobs, but a main one is to realize when you turn the computer on and start to load more complex software (like windows or whatever).  Big oversimplification, but that's the idea.

That software is written into ROM (read-only memory) area installed on that board somewhere...not the CPU.  The CPU itself doesn't have a BIOS, it has some other really small software that helps it do it's job (called microcode or firmware usually).  A computer has several different chips/boards/components, each to do a different job, and most of them have some level of firmware that's loaded in some way similar to my first comment (usually developed by the manufacturer).  Even a hard drive has some tiny amount of software on it.

The software that does the loading is special for each exact kind of chip/board/thing, it's usually called programming software but can have any number of names.

The bottom line is that computer components get their first software on them because they were designed to be able to accept programming from the manufacturer, and another component or computer does that programming.",27,0,0,False,False,False,1637898404.0
r2a6yz,hm3qs0a,t1_hm3j6i2,Guy in my dreams 2-4 days ago said there were machines that write onto motherboards and SBCs similar to how we used to burn data to read only DVDs.  I was thinking about this lately which is why I dreamt it,6,0,0,False,False,False,1637890505.0
r2a6yz,hmxsvg8,t1_hm58fm6,Do you think that is a helpful distinction in this context?,1,0,0,False,False,False,1638457622.0
r2a6yz,hm4g56f,t1_hm3upzs,"Yeah, it should be ""that does"", I fixed it now.

As far as this question is concerned, ""initialization"" just means telling the CPU where to find the instructions that load & boot the OS. It used to be done by having the CPU go to the Master Boot Record which would contain the essential info for getting the OS booted (along with some other stuff). This is now considered legacy BIOS and most systems use UEFI (of which I know less about).",2,0,0,False,False,False,1637904561.0
r2e13s,hm4htyu,t3_r2e13s,"Each problem has two bounds. NP is a upper bound: meaning a problem is at most as difficult as NP. NP-hard is a lower bound: meaning a problem is at least as difficult as NP. NP-complete means a problem is both NP and NP-hard. If you want to show a problem is NP-complete, you need to go both directions.",13,0,0,False,False,False,1637905632.0
r2e13s,hm49l2x,t3_r2e13s,"> If X is in NP-Complete (so all NP problems reduce to X), and X reduces to Y, then Y is also NP-complete right?

That reduction just means that X is not harder than Y. It doesn't provide any guarantee like Y being no harder than X. For example, Integer linear programming (which is NP-complete) is easily reducible to mixed-quantifier Presburger arithmetic (which is not in NP).",10,0,0,False,False,False,1637900607.0
r2e13s,hm4wcll,t3_r2e13s,"You need further assumptions. First, when stating that X reduces to Y, it needs to be a *polynomial* time reduction (i.e. problem X can be transformed into a subset of problem Y in polynomial time with respect to the length of the input of problem X). Second, you need to assume (or prove) that Y is an NP problem. 

Recall that an NP-complete problem X is simply:
1. an NP problem
2. a problem where all NP problems can be reduced to X in polynomial time (i.e. X is NP-hard).

Intuitively, assuming Y is in NP and there exists a polynomial reduction from the NP-complete problem X to Y, Y becomes an NP-complete problem as follows:
1. For any NP problem A, reduce this to X in polynomial time. This is possible because X is NP-complete.
2. From X, reduce this problem to Y in polynomial time. This is possible since this is assumed.
3. Thus, any NP problem A can be reduced to Y (which shows that Y is NP-hard) in polynomial time, since the two-step reduction process above takes polynomial time.
4. Finally, we assumed that Y is an NP problem. This shows that Y is in NP and Y is NP-hard, which then shows Y is NP-complete.",3,0,0,False,False,False,1637916145.0
r2e13s,hm585sg,t3_r2e13s,"Not necessarily.

One thing is that from your question, you only mentioned ""X reduces to Y"", but does not specify what kind of reduction it is. This I can already imply, there is no information on how difficult Y is.

To give a fun example (I like to argue by using examples), consider X = sorting an array of n integers. This is in polynomial (in P), and this is trivial to verify. Then, if I were suddenly feeling cute and decide to come up with this Y:

1. Set up an email connection
2. For each integer i, do:
   1. Compose a message; specify that the message should only arrive on the ith day after the sending date; set the receiver to self
   2. Add the message to the Message Array
3. Send everything out in the Message Array

>Listen to the incoming email connection and print out the results. On the 1st day you will receive 1 (if there exists any ""1"" in the original array), on the 2nd day you will receive 2, ... ad infinitum

Then the complexity suddenly becomes much higher, because literally another computer is required to do this job (sending the email back to you). Also, sorting integers (X -(reduce)> Y) suddenly becomes unbounded.

It is important to specify the kind of reduction you are talking about. But let's say you are talking about polynomial reduction. It still doesnt say anything about whether Y is NP-complete or not. If I find some Y which is in P (or more realistically, where Y is in EXPONENTIAL) then I would have disproved your idea.

In fact as others may have pointed out, if someone happened to find some Y such that Y is in P, then they would have solved P = NP and would become famous. It is just that no one have found such algorithm of Y and so I cannot give any examples to this. ""People can only think of NP"" does NOT imply ""there is only NP in this world"".

\-------

Actually in situations like this, it is often reversed: I want to know what kind of problem X is, and so I try my best to reduce X to some ""simplest"" problem Y, and I literally cannot get more simpler than Y. But oh look, Y is e.g. in NP! So unfortunately X will have to be in NP too.

This mindset can be useful if you happen to also want to explore undecidability. In the broader scheme of things, P, NP, EXPONENTIAL, ... all are in ""DECIDABLE"", and for Turing, one of his contribution was that he found a whole bunch of problems that were ""UNDECIDABLE"", i.e., you literally will never get any result if you tell any Turing machine (e.g. a modern computer) to ""go calculate it"". EG, ""does this program terminate?""",1,0,0,False,False,False,1637926344.0
r2e13s,hm75zl0,t3_r2e13s,"I have been a software developer for 5 years. I have literally no idea what this means. I am not ashamed to admit it, but I am curious what this has to do with computer science(besides the obvious mathematical connection) is this an algorithm thing, or is it reference to some type of hardware configuration?",1,0,0,False,False,False,1637961813.0
r2e13s,hm4amel,t1_hm49l2x,"Thank you, the specific example is very helpful. I'm still a bit confused though. Why would we need a guarantee that Y is no harder than X to conclude that Y is NP-Complete? 

My thinking was that all NP problems reduce to X in polynomial time, and X reduces to Y in polynomial time, so all problems in NP reduce to Y in polynomial time, thus Y is NP-Complete. Is there an error there?",5,0,0,False,False,True,1637901217.0
r2e13s,hm4bk79,t1_hm4amel,That means Y is NP-hard. An NP-complete problem is one that's both NP-hard and in NP.,5,0,0,False,False,False,1637901772.0
r27cn1,hm39zas,t3_r27cn1,Honestly just creating a program that operates with these equations would be really cool. I would love to see a project like this,11,0,0,False,False,False,1637881793.0
r27cn1,hm3anqy,t3_r27cn1,"So it appears to make sense! 

I absolutely love this. It reminds me of the sort of stuff I'd doodle on slow nights at work. That said, I'm not gonna choose to check your adder.

In terms of a turing machine, a turing complete system can handle recursive functions, such as ackerman's function. I'm struggling to see how it could be used to perform recursion when there's no actual mechanism to perform a branch (like an if). I'd therefore propose that it's probably not.

I can't think of any way of using this encoding of these gates that wouldn't stop.",9,0,0,False,False,False,1637882145.0
r27cn1,hm4wvud,t3_r27cn1,"That's a cool concept, thank you. Your NOR is wrong though, it should be

1-(x+y-xy) = 1-x-y+xy

With your formula, 0 NOR 1 would be 2.

You've got the same error with XNOR.",3,0,0,False,False,False,1637916579.0
r27cn1,hm4hjki,t3_r27cn1,So 1+1+1 == 43?  What am I missing?,2,0,0,False,False,False,1637905448.0
r27cn1,hm6dge9,t3_r27cn1,"Lol. I remember doing this in first year. If you allow inputs to vary smoothly between one and zero, you end up with basic probability, but you lack an easy way to model dependant variables.",2,0,0,False,False,False,1637948622.0
r27cn1,hm4gi9p,t3_r27cn1,">could we write a Turing machine like this?

Yes! In fact, you can simulate any logic using solely NAND gates as they are functionally complete. An arbitrary amount of NAND gates could be used to implement any possible algorithm based on binary encoding.",2,0,0,False,False,False,1637904793.0
r27cn1,hm64kb4,t1_hm39zas,What would such a program actually do?,2,0,0,False,False,False,1637944719.0
r27cn1,hm3z6i1,t1_hm3anqy,"Yes, this can only encode acyclic graphs, i.e. trees.

Yet even then you can not build a Turing machine out of finitely many bits",5,0,0,False,False,False,1637894959.0
r27cn1,hm5cpfx,t1_hm4wvud,"    x=0,y=1:
    OP:        1-x+y-xy => 1-0+1-0 => 1+1 => 2
    Corrected: 1-x-y+xy => 1-0-1+0 => 1-1 => 0

Edit: Fixed above block, now showcasing the difference between OP's and u/Lornedon's; some signs are flipped.",2,0,0,False,False,False,1637929933.0
r27cn1,hm6yb8s,t1_hm4wvud,Crap I will be correcting this along with a couple of other errors in my sums. Thanks for letting me know,1,0,0,False,False,True,1637958234.0
r27cn1,hm4s28e,t1_hm4hjki,"1+1+1+8\*1\*1+8\*1\*1+8\*1\*1+16\*1\*1\*1A note I could have probably added in was that 7xy doesnt mean 710 or 711 it means multiply 7 by x by y

I am just reckoning this is where you went wrong

Or I just totally messed up my 4 bit adder and honestly it is probably my error",2,0,0,False,False,True,1637912800.0
r27cn1,hm7a5x4,t1_hm6dge9,"But that modeling is the art of it. Really, these constructs can end up being involved in simple machine learning models ... as long as you can correctly fit your data into the [0, 1] continuous range in a meaningful way.",1,0,0,False,False,False,1637963738.0
r27cn1,hm5dkp8,t1_hm4gi9p,"I think turing completeness != functional completeness

You can implement every possible boolean function using NAND gates, but implementing a turing machine is a different thing",5,0,0,False,False,False,1637930546.0
r27cn1,hm4hpfc,t1_hm4gi9p,How is a latch represented in this schema?,3,0,0,False,False,False,1637905552.0
r27cn1,hm6megs,t1_hm64kb4,Everything that can be expressed in binary equations. So everything a normal computer can do.,1,0,0,False,False,False,1637952639.0
r27cn1,hm5e0w6,t1_hm5cpfx,"The formula OP had in the post for NOR was 1-x+y-xy.

They two formulas mentioned in the correcting comment look the same because they are the same. That's why there was an equal sign between them",2,0,0,False,False,False,1637930861.0
r27cn1,hm4ubfy,t1_hm4s28e,"    1+1+1+8*1*1+8*1*1+8*1*1+16*1*1*1 == 1 + 1 + 1 + 8 + 8 + 8 + 16

    == 43

So like I said, 1+ 1 + 1 == 43.  What am I missing?",2,0,0,False,False,False,1637914509.0
r27cn1,hm4sy4e,t1_hm4hpfc,"I did at one point fiddle around with trying to write a latch

I ended up using the answer system written into the calculator 

so Ans + 1 = Ans gives you a value that increments over time incrementing by one",5,0,0,False,False,True,1637913458.0
r27cn1,hm6n2y9,t1_hm6megs,... just slower and more complicated. Got it!,0,0,0,False,False,False,1637952955.0
r27cn1,hm5jex4,t1_hm5e0w6,"Gotcha! I see where I went wrong; the one in the OP is almost the same, just some signs are flipped. Cheers for pointing it out",2,0,0,False,False,False,1637934278.0
r27cn1,hm5cc72,t1_hm4ubfy,"I think the carry bit z should be input as 2 and the result should be 100 (i.e. binary 4). However, that doesn't quite work, and nor do most other test cases:

    x + y + z + 8xy + 8xz + 8yz + 16xyz
    
    x=1,y=1,z=2?
    1 + 1 + 2 + 8 + 16 + 16 + 32
    76 => not binary :(
    
    x=1,y=1,z=0
    1 + 1 + 0 + 8 + 0 + 0 + 0
    10 => 2!
    
    x=1,y=0,z=0
    1 + 0 + 0 + 8 + 0 + 0 + 0
    9 => not binary :(
    x=0,y=1,z=0
    0 + 1 + 0 + 8 + 0 + 0 + 0
    9 => not binary :(
    
    Attempt to treat the carry as 1 or 2, x and y are 0:
    0 + 0 + 1 + 0 + 0 + 0 + 0
    1 => 0 + 0 + 2 != 1
    0 + 0 + 2 + 0 + 0 + 0 + 0
    2 => This works

Must be something a bit off somewhere! Perhaps the next iteration of these equations will sort it out these issues (or we're both reading it wrong!).

EDIT: Realised I messed up my cases where the z was set but x/y were 0; have fixed them now and found the correct value for z is 2. Still doesn't quite work when adding 1 + 1 + 2, sadly",2,0,0,False,False,False,1637929663.0
r27cn1,hm6ygwa,t1_hm4ubfy,I totally messed up and forgot that you must deduct the sixteen not add it,1,0,0,False,False,True,1637958309.0
r27cn1,hm68jtv,t1_hm5cc72,"I think the intention is to make functions which take decimal inputs that look like binary, then emulate logic systems even when they have multiple inputs and outputs.

If we write `1001` in binary, we're talking about `9` in decimal. But here, we want `1001` in *decimal* as a result of a binary system with 4 outputs, using decimal math.

The whole thing doesn't seem particularly useful. Admittedly, I'm having trouble sifting through the rah-rah here, the OPs word salad, and the math errors, but that's my takeaway.

If we think of [a full-adder](https://en.wikipedia.org/wiki/File:Full-adder_logic_diagram.svg), we have two inputs X and Y, plus a carry input Z. (I'm using the OPs input variables, the image I linked uses A, B, and C instead.) It outputs two bits: sum and carry, which you can conveniently think of as a two digit binary number with carry as the MSB and sum as the LSB.

The goal appears to be to write a function that does this all in *decimal*, such that `f(1,1,1)` for example, results in `11` *decimal*. The function would have this truth table: 

X|Y|Z (Carry)|Result
:--|:--|:--|:--
0|0|0|0
0|0|1|1
0|1|0|1
0|1|1|10
1|0|0|1
1|0|1|10
1|1|0|10
1|1|1|11

It's important to remember these functions have very limited domains: `x` and `y` and `z` can be 0 or 1, and absolutely nothing else ... even though the function definition looks like a plain old polynomials over the reals.

The function given for a full adder:

> Heres an adder that takes a carry and 2 bits and outputs the binary in:
> x+y+z+8xy+8xz+8yz+16xyz

is pretty obviously incorrect, as we're finding.

I think we're a lot closer with this (tested):

    x + y - 2xy + z - 2z(x + y - 2xy) + 10(xy + z(x + y - 2xy) - xyz(x + y - 2xy))

which simplifies a bit (untested):

    y + z + 8yz + 10yzx^2(-1 + 2y) +  x(1 + y(8 - 16z) + 8z - 10zy^2)

[This Google Sheets spreadsheet](https://docs.google.com/spreadsheets/d/1PCjgBgT8Dq2L6hh5oB_iGN440q2mbXe4CaQzqQ9D51M/edit?usp=sharing) tests that expression, and it looks a lot better than the OPs original to me.",2,0,0,False,False,False,1637946456.0
r27cn1,hm6zii0,t1_hm68jtv,">The whole thing doesn't seem particularly useful. Admittedly, I'm having trouble sifting through the rah-rah here, the OPs word salad, and the math errors, but that's my takeaway.

I love how you turned my garbage writing in to a food outlet, I should start a business about word salad

You are correct that the aim is to take Denary numbers that look like Binary and add them in Denary to give an output that looks like Binary

As far as math errors goes I believe that I only messed up which way round the + or - signs go

`x+y+z+8xy+8xz+8yz-16xyz` 

Does work and is far more compact then the solution you had, mostly because 0\^2 or 1\^2 are totally meaning less and since xy and z all are ones and zeros you will find that you can just remove them and then cancel down.

I will be correcting my mistakes in an edit",2,0,0,False,False,True,1637958808.0
r27cn1,hm78jzm,t1_hm6zii0,Great! Happy that I could help you find something that works.,2,0,0,False,False,False,1637962996.0
r2rbee,hmhou1r,t3_r2rbee,The art of computer programming there’s 6 volumes I think. Often touted as one of the greatest computer science series ever written.,1,0,0,False,False,False,1638158539.0
r1xcw9,hm1aokc,t3_r1xcw9,"At this time, it is not only not possible, we don't even know if it is possible. Or as I like to say, not only do we not have a path to ASI, we don't even know if such a path exists. AI, as it currently exists, is simply a computational tool (or aide) for certain types of problems.",59,0,0,False,False,False,1637849707.0
r1xcw9,hm1ytep,t3_r1xcw9,"Well the good news is that we are very likely decades away from real artificial intelligence.

What is often called AI by buzzword salesmen is more properly called machine learning, though even that is a but presumptuous.

The way most of it works is by randomly trying out curves on a graph again and again until one or more of the models guesses the right answer. 

That model is then used and sold as AI even though its really just guessing some fancy math equation that would just take a while to do by hand.

The model of the brain probably cant be used to simulate a mind yet, we don't fully understand how neurons work fully let alone the nervous system as a whole.

Also while MRIs can monitor brain activity (local only, cant be done remotely) they are a long way from being able to read thoughts and especially put them there.

Lastly we don't even know the a true general AI is even possible and the definitions even gut a but blurry.

 I prefer Virtual intelligence to describe a system that appears intelligent but isn't true a thinking mind (""agent"" in philosophy); its only AI once it is properly a novel mind, e.g. displays sentience, sapience, self-awareness, consciousness, and intelligence.

Tldr: most of what is called AI really is just not; they are programs that excel at pattern recognition and functional optimization, but they cant actually think and adapt like a real intelligence. Mapping the brain is only one piece to the puzzle and decades more work is needed; As a result planting thoughts is almost certainly not possible at this time.",7,0,0,False,False,False,1637860683.0
r1xcw9,hm5lxmk,t3_r1xcw9,"You have people with PhDs in the comments telling you everything you need to know. But most importantly, whoever got you on this path of worrying about AI, stop listening to them. Take care of yourself.",3,0,0,False,False,False,1637935705.0
r1xcw9,hm23m0n,t3_r1xcw9,"The fuck? You have Schizophrenia. The literal last bullshit you should be worrying yourself with is artificial intelligence. Look, our AI amounts to some calculus and linear algebra -- it's a neat accounting trick for estimating functions. There is zero intelligence involved, and AI is a misnomer title. 

We are so far from general AI that we literally don't even have a vocabulary set to properly even DISCUSS the topic.

Even if we did, creating a ""brain map"" is worthless with regards to creating general AI systems. The fact that a  ""supercomputer"" is doing this ""mapping"" is not relevant to AI: biologists want to know about human biology, they aren't trying to build AI systems with that data.

I can't stress this enough: You know you have Schizophrenia, and you have to already be aware that these types of delusions regarding AI are unhealthy and inappropriate. Don't feed these types of thoughts to the point where you have to reach out to people to confirm your suspicions are incorrect.",11,0,0,False,False,False,1637862664.0
r1xcw9,hms3uhh,t3_r1xcw9,"I suggest learning about computers and programming a bit yourself.

You'll see very very quickly that computers are dumb as bricks and that we're extremely far away from anything remotely resembling agency.",1,0,0,False,False,False,1638353808.0
r1xcw9,hm1b9st,t1_hm1aokc,Ok thank you so much. Do you know if aurora21 is anything like ASI or what even is it?,9,0,0,False,False,True,1637850004.0
r1xcw9,hm1fkc9,t1_hm1aokc,"Thanks for the reply.

Does the human brain itself not prove the possibility of general intelligence? If brains are nothing more than huge information processors, then given enough time(a thousand years is nothing on the cosmic scale) we will eventually be able to mimic it's full architecture digitally, no?",2,0,0,False,False,False,1637852126.0
r1xcw9,hm2o7v3,t1_hm23m0n,Preach brother nice comment,3,0,0,False,False,False,1637871460.0
r1xcw9,hm3t59y,t1_hm23m0n,How is reaching out to informed parties not exactly what we should recommend to individuals with doubts and suspicions -- even doubly so for those with mental disabilities?,4,0,0,False,False,False,1637891760.0
r1xcw9,hm383o1,t1_hm23m0n,Or just dive in with some pkd,1,0,0,False,False,False,1637880855.0
r1xcw9,hm1bily,t1_hm1b9st,"It is just a brand (in a sense) of a supercomputer. The project they did is to map the connections in a brain, which is very complex and requires a lot of computing power. It would be like mapping the connections made by every road, sidewalk, path, railway, etc. But it is just a map. The goal is to be able to understand how different structural connections relate to different conditions. E.g., can we diagnose Alzeimer's earlier by scanning the connections in the brain, hence treat it earlier; thereby, improving outcomes.",24,0,0,False,False,False,1637850130.0
r1xcw9,hm1ggg2,t1_hm1fkc9,"No, it’s not clear at all that biological brains (human or otherwise) can be modeled digitally.  We do know that computers can implement any algorithm, but we don’t know that everything a brain does is algorithmic. And we do also know that certain problems cannot be solved algorithmically (e.g., the halting problem).  Furthermore, for the few things the limited AI of today can do, we know that computers accomplish many of those tasks differently than humans.",8,0,0,False,False,False,1637852569.0
r1xcw9,hm1hejp,t1_hm1fkc9,"What ComputerSystemsProf is accurate. We know that general intelligence is possible; however, as they pointed out computers seem to think differently than us. So this raises some interesting questions as potential paths to general intelligence:

1. How can we make a computer generally intelligent without duplicating human intelligence?
2. How can we make a computer duplicate human general intelligence?

We do not really know if either of those can be done. We can ponder the question, but there is no really clear direction to the goal. Of course, some people have some ideas (general AI is one of my side projects, so I happen to believe it is possible) but to date, none of them have really panned out or again, kind of provided a very clear ""Oooooooh"" moment, that's how we can do it.",4,0,0,False,False,False,1637853025.0
r1xcw9,hm1c6u1,t1_hm1bily,"Ok cool, so it dosent take into account the functions of connections? Could that really be done in 3 years, i heard 30 before?",5,0,0,False,False,True,1637850466.0
r1xcw9,hm1cz57,t1_hm1c6u1,"Maybe, but unlikely.

ASI has been 10 years away for about 60 years. :)  As I said above, there is no known path to ASI right now. Could somebody discover it tomorrow? Yes. Is that likely? No.

Also, the dangers of an ASI are greatly overexaggerated. First, an ASI would have to be hostile. It is not certain that would be the case. So, a hostile ASI could cause a lot of disruption, but it has no way to cross the physical divide, so there are extreme limits to what it could do.",12,0,0,False,False,False,1637850873.0
r1xcw9,hm235in,t1_hm1c6u1,"No. *The* premier whole-animal conectome simulation is [OpenWorm,](https://openworm.org/) which is currently working on simulating a less than 1000 cell animal. 

> OpenWorm aims to build the first comprehensive computational model of the Caenorhabditis elegans (C. elegans), a microscopic roundworm. With less than a thousand cells, it solves basic problems such as feeding, mate-finding and predator avoidance. Despite being extremely well studied in biology, this organism still eludes a deep, principled understanding of its biology. 

Despite the organisms simplicity, we have failed to simulate it. There are about 86 billion cells in the human brain, and they are far more complex than the worm's cells. 

You should only start worrying about ASI when you see, for example, a brainless mouse hooked up to a supercomputer and doing mouse things.",5,0,0,False,False,False,1637862477.0
r1xcw9,hm1jry8,t1_hm1cz57,"> Also, the dangers of an ASI are greatly overexaggerated. 

The dangers of an ASI would be very real, although I agree there is no clear path toward one.

> First, an ASI would have to be hostile. It is not certain that would be the case.

It doesn't need to be hostile per se, any slight misalignment would have drastic consequences. [Accurately aligning advanced AI systems is a difficult unsolved problem.](https://youtu.be/IeWljQw3UgQ)

> So, a hostile ASI could cause a lot of disruption, but it has no way to cross the physical divide, so there are extreme limits to what it could do.

[It's impossible to effectively sandbox an ASI.](https://youtu.be/i8r_yShOixM?t=305) It would be a better manipulator than any human who ever lived. By communicating with its human operator, it impacts the real world and crosses the physical divide. Even our current dumb AI systems are scarily good at manipulating humans (e.g. social networks maximizing engagement).",3,1,0,False,False,False,1637854137.0
r1xcw9,hm39vgc,t1_hm1cz57,"Disagree. The fundamental root of computation is Binary decision making. I’m not sure what the fundamental structure of an official first and functional ASI will be (quantum , etc). However, an ASI likely does not conform to our human concept of ‘hostile’ . Take the anthill scenario for example 

The TRUE danger is if we allow it to control our governments in the name of computational precision. Control our nuclear weapons, etc. Above this, we do not have the ability to even fathom the repercussions as it is outside of our precedented human range of understanding and mental capacity",1,0,0,False,False,False,1637881740.0
r1xcw9,hm1p9f5,t1_hm1jry8,"1. While there is of course no upper bound on the potential danger that can be caused by an ASI, if we look at it through a realistic lens, then the dangers are overexaggerated.
2. Hostile, within the context of AI, means unaligned or incompatible with human desires or needs.
3. RE: AI as a master manipulator. There's no indication that this is necessarily true. Our current AI systems are not really that good at manipulating us. Humans are good at creating systems that use AI as a computational tool to do such manipulation. These are vastly different things.",4,0,0,False,False,False,1637856581.0
r1xcw9,hm47otl,t1_hm39vgc,"You are quite welcome to disagree.

I don't really understand your first paragraph. It does not make much sense to me. I think perhaps you are misunderstanding the term hostile as it applies to AI ethics. As I posted elsewhere, hostile in that context means unaligned or incompatible with human needs or desires. It does not imply maliciousness or anything to do with ants.

As for the second paragraph, this same can be true for any safety-critical piece of software. Flawed software can have serious repercussions whether AI-based or not. As for the last sentence, this is simply fundamentally flawed (in my view anyway) and based on science fiction or pure speculation (usually from non-experts, such as Elon Musk). ASI does not mean that it can do everything, and it certainly does not mean it can do the impossible. It is in fact quite possible to examine ASI in a scholarly way by making reasonable extrapolations based on what we know about AI. Of course, even such work is speculative because we do not really know much about ASI (see my other posts), but at least it is justified by existing literature.

If you're really interested in this, then I'd suggest looking at some works on AI ethics. There are some good works on ASI as well. Nick Bostrom as written some works on the dangers of ASI (which I personally feel are flawed), and then there are a number of good rebuttals to his arguments. So this is a good place to start.

[https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=ai+ethics&btnG=](https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=ai+ethics&btnG=)

tps://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=superintelligence+bostrom&btnG=&oq=Superintelligence+Bostr",8,0,0,False,False,False,1637899530.0
r1xcw9,hmcuson,t1_hm47otl,"Thanks for sending, I’ll give them a look. And yes I misunderstood your application of hostile. Sure, most software has risk and security related concerns and implications, AI based or not

My argument is that the potential caliber of ASI, when applied to certain dimensions or areas, could potentially have indefinite negative repercussions, and we lack the capacity to even forecast such repercussions. Sure, non experts will fear monger with what is built on speculation. So: the dangers of *current* applications of ASI  are more or less benign. 

Please allow me to illustrate my thought process. I believe that there is power and greater understanding with dimensions. Example: our human existence is likely tied to 3 dimensions. We most likely do not live in a 3 dimensional universe, in fact, maybe at least 4  (a reality with more than an x, y, and z axis). I’m guessing that we are likely unable to comprehend the laws of higher dimensions as we are built with / possess the syntax of 3 dimensions (likely. Unless our body is 3 dimensional but our mind is not. I don’t know). Im saying all of this to demonstrate our lack of understanding as humans. We don’t want to create something which exceeds our ability to a highly significant extent, where variables can’t even be assigned as we lack the ability to detect such variables. Moreover, assign critical roles to said technology. So long as most variables are understood and data is established, the technology is ok to deploy. I hope this makes sense",1,0,0,False,False,False,1638069839.0
r1xcw9,hmcv352,t1_hmcuson,And then you have the issue of discovering variables as you deploy the technology into the environment,1,0,0,False,False,False,1638069984.0
r27hds,hm3ayev,t3_r27hds,I would imagine most of the power would be spent storing the entirety of Wikipedia rather than displaying/reading the files,5,0,0,False,False,False,1637882297.0
r27hds,hm5f8cp,t3_r27hds,"For something like this (long term archive?) I would look at technologies known to last a very long time, and work backwards to find the needed power.

I'm not very well versed in hardware, but my first thought is something like a mask ROM, where the data isn't editable, but also is burned into the structure of the chip.  I've got games cartridges with this tech that are 30 and 40 years old with no signs of deterioration at all, I don't think 100 years is a stretch.  I don't think it's very memory sense, and probably not low power either (5v TTL? Idk) but there must be a more modern iteration that is more dense and efficient but similarly reliable, and I don't know if off the shelf, consumer grade 3.3v flash memory on an Arduino will survive that long without data errors. 

For display, you could use something like a 1-line character LCD, like used in simple solar calculators, or even an e-ink display.  I know those are fairly low power, and last decades with care.",1,0,0,False,False,False,1637931668.0
r27hds,hmblflb,t3_r27hds,"For display, eink is likely your best bet. It only needs power to change. It does not need any power once the image is ""set"".

DNB batteries are snake oil for any general usage application. They output extremely low power over an extremely long time. They are not meant for random bursting use like a consumer device. Unless you are planning on making extremely low power, extremely long life, extremely feature limited, devices that do exactly nothing 99.9999% of the time then it's best to forget they even exist.

Pie-in-the-sky theoretical ""wonder technologies"" almost never pan out, and when they do it's almost never anywhere close to the imagined uses of how the mass media reported on it.",1,0,0,False,False,False,1638048818.0
r27hds,hm3biuv,t1_hm3ayev,Doesn't take any energy to store things at rest in a non-volatile medium (aka disk). Shuffling it to and from the disk is what uses energy.,8,0,0,False,False,False,1637882584.0
r27hds,hm3d7vt,t1_hm3ayev,"physically, why does it take more power to store more text?
edit: assuming the text has already been downloaded",2,0,0,False,False,True,1637883438.0
r27hds,hmbm1xy,t1_hmblflb,thanks for the explanation. would it be more practical to use a solar panel?,1,0,0,False,False,True,1638049087.0
r27hds,hm3bpbi,t1_hm3biuv,"Good point, that makes sense",3,0,0,False,False,False,1637882674.0
r2cwfc,hm467w8,t3_r2cwfc,I have never heard the term data width before.,3,0,0,False,False,False,1637898725.0
r2cwfc,hm5biqf,t3_r2cwfc,"I’ve heard of “wide data” vs “tall data” in a data science context(ie looking a tabular data, the relative ratio of rows to columns). Doesn’t really have anything directly to do with bandwidth as far as I know.",1,0,0,False,False,False,1637929048.0
r20mys,hm1za86,t3_r20mys,"You can reach out to the researchers, you can post on stack exchange, you can go learn the math that the paper is using on Khan academy or something.

Those are my best suggestions.",2,0,0,False,False,False,1637860883.0
r20mys,hm5jdh2,t1_hm1za86,"Thank you for the reply. I have a few questions.

Do you think it matters how much I know on the subject before I reach out to the researchers? I want to move on this quickly, but I’m also worried they may not wish to engage if they believe I’m novice in the subject. Not sure if that’s common.

Do you recommend exploring anything on stack exchange in particular? I’ve used stackoverflow plenty, but am not familiar with its cousin.",1,0,0,False,False,True,1637934254.0
r20mys,hmdruai,t1_hm5jdh2,Yes. Eastern Promises is brilliant. Second this. For a long time ago but it didn't run as expected.,1,0,0,False,False,False,1638090767.0
r1lsow,hlzl113,t3_r1lsow,"Most of the time, we talk about worst-case complexity. We also only really care about large data sets. In those scenarios, the highest order function dominates lower ones, ie if a function is say... O(nlog(n) + n\^2) then we can say that it's really O(n\^2).

Assessing complexity is mostly about the relationship between the algorithm and the growth rate of data. An algorithms class shouldn't teach you memorization of the major algorithms in different areas, or the runtimes of those. Instead, it's about learning how to develop and assess algorithms, and the different strategies that led to the development of the major algorithms. It should also teach you how to reduce problems down into simpler ones, or transform them into a similar problem with a known or easier solution.

The overall complexity of your algorithm will depend entirely on the most complicated step, which is your for-loop that will run the sqrt(n) times.",12,0,0,False,False,False,1637809626.0
r1lsow,hlzuxe1,t1_hlzl113,"Makes sense, we take the worst case of all discrete cases and use that. Thanks.",2,0,0,False,False,True,1637814540.0
r1lsow,hm0azxc,t1_hlzuxe1,"Additionally, it is worth noting that asymptotically what you are doing with the 6n + 1 case is improving the best case of the algorithm without reducing the worst case (which is likely not possible in the general case).

This means that without the optimization you would be able to characterize the algorithm as having a complexity of Theta(sqrt(n)), but with the optimization your best case becomes Theta(1), meaning the complexity of the overall algorithm becomes Omega(1) and O(sqrt(n)). This kind of thing is pretty common in algorithms, where you can solve a specific subset of problems really quickly, but cannot solve EVERY possible problem quickly.

Asymptotics can be confusing because they are really about mathematical functions, not just algorithms, but the general logic to follow is that you cannot say you are reducing the runtime of an algorithm unless you do it for all possible inputs.",3,0,0,False,False,False,1637823899.0
r189ka,hlze5en,t3_r189ka,I would love this as well!,3,0,0,False,False,False,1637806499.0
r189ka,hm03r38,t3_r189ka,"Well, writing such a book is dangerous 😂",2,0,0,False,False,False,1637819364.0
r19gul,hlz7i6q,t3_r19gul,"Here you go:

https://www.cs.usfca.edu/\~galles/visualization/",3,0,0,False,False,False,1637803534.0
r0ss7l,hluexve,t3_r0ss7l,"OH, I think I know why. Is rosetta converting apps before running them into code it can run, whereas running linux in a VM is a constant task? Even then, there are still questions.",19,0,0,False,False,True,1637717255.0
r0ss7l,hlv4ieg,t3_r0ss7l,"AOT vs JIT essentially, and Apple have the time and resources needed to make it as performance as possible.

Basically Mac translates the app as much as it can, and then emulates the bits that aren't already translated.",15,0,0,False,False,False,1637729815.0
r0ss7l,hlvdn8x,t3_r0ss7l,"Because an app isn’t a whole OS. An operating system is a kernel, drivers, modules, tons of apps and schedulers and timers all working together.

QEMU or whatever emulator you use has to actually emulate a working x86 processor, reserve contiguous memory etc, not just a subset required to run a single app, but every single instruction in a continuous loop, translate it and run it typically using a smaller instruction set. The x86 instruction set is very complex.

It’s why emulating a Power or MIPS processor on x86 to boot Linux also runs like dog shit even though the design is decades old.

There are people working on getting Linux working natively on Mac, that will save a lot of that translation step.",9,0,0,False,False,False,1637735252.0
r0ss7l,hlx33x1,t3_r0ss7l,"I'd say those are two entirely different kinds of problems.

Running code compiled for one instruction set architecture on a different ISA usually comes at a steep performance cost since perhaps the most straightforward solution -- emulating the other CPU with software -- is slow. I don't know how Rosetta works in detail, but since it's either JIT or AOT (ahead-of-time) compiling the x86-64 machine code to its native ARM code, and the compiled ARM code is then run on the CPU, it can achieve much better performance than outright emulation.

Other than that, an application running through Rosetta is still code that has been built to run on an Apple OS, running on an Apple OS. Apple knows all the system calls and any other macOS APIs that an application can make use of; it's their design after all. Since they (probably?) still have the implementations of those same APIs in their OS, now just running on ARM, Rosetta can just mechanically make whatever modifications are required to the machine code so that the system calls now work on the ARM version, and re-link the application executables with the ARM version of any library ABIs provided by the operating system, and whatever else they need to do. (I don't know if re-linking is what they're doing, and I'm speculating, but the point is that once they've got the translation logic done right and all the corner cases working, it's a mechanical translation. Most of the APIs themselves are probably still identical in content. The implementation of the binary interfaces just differs, and the difference is systematic.)

It's still a nice piece of work, but Apple essentially knows and controls all the pieces of the puzzle. They also had time to prepare.

Supporting a modern operating system on a particular piece of hardware is a different thing. An OS needs to be able to work with the specific hardware on which it's running. This requires not only compiling the code for the correct CPU instruction set, but also being able to properly communicate with any other device-specific hardware such as the GPU (integrated in case of the M1) and the motherboard chipset. A modern operating system also needs to deal with power saving modes for all of that, and so on.

Writing the code to do this, not to mention figuring out how the hardware works exactly if no detailed public documentation is available, is a lot of work that has to be done for each individual component that a machine might have. Also, I don't know what if any documentation regarding these on the M1 is publicly available, but Apple isn't exactly the kind of a company to go out of their way to release technical information they don't need to.

The same is of course fundamentally true about running Linux or Windows on any other random hardware regardless of the CPU architecture. However, the new Apple hardware brings in a lot of new components to the mix, and there probably isn't a lot of public documentation available for non-Apple developers. Also, if it were some random laptop from some random brand not working well with Linux, you just wouldn't have heard about it. You hear about the M1 because it's Apple.

--
--

TL;DR: Rosetta needs to solve running (application) code from one CPU instruction set architecture on a different CPU with a different instruction set; running Linux or Windows on the M1 comes with all the usual problems of supporting random undocumented hardware.",2,0,0,False,False,False,1637772259.0
r0ss7l,hlw3rp3,t3_r0ss7l,"What if I told you that there's an app that let's  you run an application whatever OS you use? if you're having troubles with your emulator, I'd suggest you try [this](https://www.shells.com/l/en-US/)  since it's the closest app that I can think of that can be really beneficial for you.",0,0,0,False,False,False,1637755331.0
r0ss7l,hlza1bx,t3_r0ss7l,"It will get better. 

Rosetta is translating a single application and there will be a ton of optimisation because its translating binaries for Apples own OS.

Proper Linux and hopefully Windows ports will come to ARM making this all less of an issue. Virtualising whole x86 machines will probably improve too but as the ARM versions improve it will probably make sense to emulate those so the CPU architecture is the same.

I always preferred bootcamp/dual boot for real world use, direct hardware access for graphics etc. I’d be happy with that for consumer use. 

I think for now if you have taken the plunge with M1 then you need to be happy with MacOS but options will open up. Apple are always innovators, only someone like them who can do the whole thing from the chip upwards can make it immediately beneficial, which is thanks to their closed system design. They are effectively pushing the industry forward like they did in the 70s and 80s. ‘Build it and they will come.’",1,0,0,False,False,False,1637804659.0
r0ss7l,hlujnzf,t3_r0ss7l,Noteseeee x84 ctm jajajaajajajajajajajaa,-18,0,0,False,True,False,1637719469.0
r0ss7l,hlujw68,t3_r0ss7l,Saleeee wn jajajajaa,-15,0,0,False,True,False,1637719574.0
r0ss7l,hlv5pk6,t1_hluexve,"Rosetta is doing several different things that simply ""run Windows or Linux in a VM on Apple silicon"" can't easily do.

- First, as you say, there is ahead-of-time recompilation of x86 code to ARM code to allow apps to run without being interrupted by a just-in-time recompiler.
- The M1 CPU has custom features for emulating the x86 memory model, which aren't part of ARM normally. Rosetta uses these in its recompilation.
- Dynamically linked system libraries need no translation, so many components are still running natively, not being recompiled.

By contrast, a VM probably can't compile ahead-of-time, and may not even be able to JIT much without choppiness and memory overhead, it has to translate the entire running OS rather than just a single userspace binary, and it may not be able to access certain internals of MacOS or of the M1.",29,0,0,False,False,False,1637730489.0
r0ss7l,hlx2bii,t1_hluexve,"It's called [Rosetta 2 because it's a *translation system*](https://en.wikipedia.org/wiki/Rosetta_Stone) between different processor instructions. Translating a book ahead of time is a lot less of a hit on overall performance than translating each word as you go along. That's why the first launch of a Rosetta-run app is slower, but subsequent runs are much faster.

https://en.wikipedia.org/wiki/Rosetta_(software)",3,0,0,False,False,False,1637771947.0
r0ss7l,hlujsn0,t1_hlujnzf,"X86_64, colloquially known as x84",-14,0,0,False,True,True,1637719529.0
r0ss7l,hlv3vbc,t1_hlujsn0,"Colloquially x64, not x84.",14,0,0,False,False,False,1637729457.0
r0ss7l,hlw14ws,t1_hlujsn0,"105 IQ, huh? 🤔",2,0,0,False,False,False,1637753453.0
r0eadx,hlsnhnl,t3_r0eadx,Cool idea. Do you consider English translation?,3,0,0,False,False,False,1637690834.0
r0eadx,hltjf2w,t1_hlsnhnl,"Yes, in a few days.",2,0,0,False,False,True,1637703352.0
r0eadx,hrrr9xe,t1_hlsnhnl,The website is now in English as well : https://www.codepuzzle.io/en,2,0,0,False,False,True,1641645402.0
qzwb8e,hlotm0b,t3_qzwb8e,I think one month would be very challenging. Do you have an existing data set upon which to build the search?,25,0,0,False,False,False,1637616697.0
qzwb8e,hlotqb6,t3_qzwb8e,"Some terms you can search for: Information Retrieval system, Inverted index, PageRank, Web crawlers",14,0,0,False,False,False,1637616746.0
qzwb8e,hlph3vq,t3_qzwb8e,"Hi we had a course on big data last year. To break down one approach is the following.

1. Get Hadoop set up on a system. If you have a cluster available by your school/university, definitely request access, as it will massively increase what you can do for this project.

2. Once set up, build a map reduce job. This is the most important part. When you work with big amounts of data, you need some way to quickly traverse this data, and filter only those relevant results to be displayed. An example dataset could be  found at https://commoncrawl.org. You can take an entire segment of the entire set if you can get a large cluster. NOTE THIS IS MULTIPLE HUNDREDS OF TERABYTES. Otherwise use the indexer to find a smaller sample dataset. 

3. Now how do you map reduce. The idea is simple: You have several cycles where each element is mapped, filtered and shuffled throughout the entire Hadoop cluster. These operations often can be done in parallel and are really trivial by themselves. Important is to ensure that you bring the data to the point of computation, not the other way around. Network traffic will be a large bottle neck. Instructions on how to do this are available online.

4. You could do many, many things now to optimise this process. Map reduce is by no means the end of big data. But it’s a good start, especially for a tiny project.

If this is too much for a small project consider doing a part of it, or just setting up a tiny Hadoop server with a toy example of the search engine!

Good luck",8,0,0,False,False,False,1637627059.0
qzwb8e,hlp56pt,t3_qzwb8e,I'd set up elasticsearch and build an interface to it,4,0,0,False,False,False,1637621612.0
qzwb8e,hlptjm3,t3_qzwb8e,"A quick idea:

Updating:  
Your engine should be informed about any new title/tag ""t"" entering the ""network"". t gets added to a hashmap.

Retrieving:  
A few waves of search for ""s"" in the hashmap, starting from most important to least.  
Search one:  
look for exact matches of s  
Loop as many times as you think it's necessary:  
generate similar strings to s, s'. Search for s'

Notes:

* every next loop is on s', s'', s''' etc
* if your ""network"" was created first, you gotta search through the entire thing unfortunately and update your map

Edit: You can have an in-between wave between s and s', which uses a dictionary to find similar words, or correct the ones you already searched for.",3,0,0,False,False,False,1637632918.0
qzwb8e,hlpto9w,t3_qzwb8e,"A search engine is a variable beast - there are a whoooooole lot of steps to take, each step of which is technically a search engine, and notably **does not have to operate on websites to be one**. One that worked on your lecture slides for the class would be just as valid.
 
First, you have the most basic 'return by exact match on field'. You have a database, you go through that database and pluck out the matching elements by title, or keyword, or author.
 
Next, you can apply rules to those searches - This AND That AND The Other Thing OR Something Else. Many search engines are sitting here, for example https://www.scopus.com/
 
The step after that is 'content based' searching, which is going to look at the body of the material and look through there, including all your rules and whatnot (this is not difficult for function, but is a nightmare for optimisation).
 
Next after that you have internal, implicit rule generation. That is things like implicit 'stemming' of words, synonyms or close to them, and other basic natural language processing 'pre-processing' steps. There is still no machine learning at this stage, just related steps you would normally do before throwing it into your algorithms. (note: this is probably the step I would recommend for a good 2nd, average 3rd year programming assignment in a data science curriculum, perhaps excluding body content).
 
After that you add machine learning for natural language processing. You start looking at the connections between words, related topic, density of the topic in the elements being found, working out subject/object, etc etc etc. This has no indefinite end and is where Google spends a significant amount of their time and money.
 
But that's just core functional capabilities. You have non-core and non-functional capabilities as well, of which the most important are probably weighting and optimisation. Weighting is being able to take your matches and assign them a weight. Easy at the lower ends of core functionality (how many rules did they match, and how often do they match them?), when you get into the pre-processing you need to look at how much pre-processing was necessary - so if you searched for 'speedily', then the ranks for matches would probably go speedily->speed->fast, for example. And when you're into NLP land it gets much more complex.
 
Optimisation is working out how to sort out your indices, sharding etc etc to support not just searching, but very rapid searching, which is a whole different ballgame and involves a whole lot of different techniques.",3,0,0,False,False,False,1637632975.0
qzwb8e,hlqnw8p,t3_qzwb8e,"There are plenty of options, if you're going to go for a fast 1-month project you might want to go with elasticsearch/BM25 etc, other commenters have covered this so I won't repeat the same.

If this is a passion project and you decide to do more on it (or maybe this is within your scope anyway), Google relies more and more on ML/AI methods in their search to allow you to search with meaning/concepts - Elasticsearch and BM25 will not be able to do any of that for you, they rely more on word matching (which does still work well, but means that you need to choose the correct words). If you're interested in the ML/AI version, you need two 'pillars' - vector similarity search and NLP models (typically transformers like BERT).

It's super fascinating imo and worth looking into, for the NLP models intro I [wrote this](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/), and for the vector similarity search (or 'semantic' search when used with NLP) [I wrote a 'course'](https://jamescalam.medium.com/free-course-on-vector-similarity-search-and-faiss-9b3e91a91384?sk=b5cb406932f62c1fb69eb0944efba92c).

Whichever route you take for your first month, I hope you at some point have the opportunity to explore the AI/ML approach because it is fascinating.",3,0,0,False,False,False,1637649417.0
qzwb8e,hlph71q,t3_qzwb8e,"I wouldn't bother with PageRank or other extra features for a month long project. I would focus on the very core of the idea of a search engine.   
Mainly I would care about implementing BM25 based scoring, inverted index and some method to deal with spelling errors. Finite State Transducers are a great fit here and the one by burntsushi is great for this use case. BM25 scoring is simple enough to implement I think, and I believe burntsushi's library does provide some levenshtein based spell corrector. 

Writing this up should be possible in a month or so in my opinion, I say this because I built the above system in a month or so.",3,0,0,False,False,False,1637627101.0
qzwb8e,hlq410f,t3_qzwb8e,"I have literally just done a search engine data structures assignment.

We didn't build it from scratch, rather we had to complete it in parts and do exactly what the assignment specs had told us to do. For example, using a certain algorithm etc.. 

We used the pageRank method (you can google this) and sorting algorithms to produce search results from key words. We didn't web-crawl or anything like that, rather they provided us with files with url links, and then we had to search those files and access those url links and read the pages.",1,0,0,False,False,False,1637637858.0
qzwb8e,hlsss2h,t3_qzwb8e,"I would just take a look at their first paper on it: http://infolab.stanford.edu/~backrub/google.html

I think you could build a version of that system.  This goes over the various pieces they use and how they use them.",1,0,0,False,False,False,1637692899.0
qzwb8e,hm86go2,t3_qzwb8e,"I would narrow the scope of what your search engine can find like a search engine for plants, or a search engine for  cars, then use Google to build your dataset.",1,0,0,False,False,False,1637979687.0
qzwb8e,hlou0wl,t1_hlotm0b,"So to clarify, this doesn't have to be a search engine that can handle anything I throw at it from halo to food science papers. The goal is just to demonstrate my ability to build such a feature. I think my uni can provide me with datasets but I really want to build my own crawler and parser. Of course the demo will be carefully crafted to show what the search engine can do with the right data set and I'll be completely transparent about it.",11,0,0,False,False,True,1637616865.0
qzwb8e,hlouli1,t1_hlotm0b,"However, if you still thinks so then let me know what you think would be a better approach?",1,0,0,False,False,True,1637617094.0
qzwb8e,hlougfz,t1_hlotqb6,"Currently I am planning to build one using the inverted index strategy. I do want to include page rank as well. My thought process was to crawl the web, build a big enough dataset by parsing the pages I crawled, apply pagerank on that dataset and store it in order of their score. When a query happens, I use an inverted index strategy to query the webpages and display them according to the pagerank score I stored earlier.",3,0,0,False,False,True,1637617037.0
qzwb8e,hlpu50c,t1_hlp56pt,"Correct me if I am wrong, but that is already a search engine right?",2,0,0,False,False,True,1637633181.0
qzwb8e,hlrlpfv,t1_hlptjm3,Are you talking about a vector search technique here?,2,0,0,False,False,True,1637675027.0
qzwb8e,hlpw8v7,t1_hlptjm3,"Besides the pseudo algorithm I'd like to add:  
No it won't be much more difficult than a sophisticated dictionary structure regardless of implementation. Unless if you have to deal with the real internet which will land a lot of complications along the way and require a lot of computational power, memory and money.",1,0,0,False,False,False,1637634165.0
qzwb8e,hlrp8di,t1_hlpto9w,">Next after that you have internal, implicit rule generation. That is things like implicit 'stemming' of words, synonyms or close to them, and other basic natural language processing 'pre-processing' steps. There is still no machine learning at this stage, just related steps you would normally do before throwing it into your algorithms. (note: this is probably the step I would recommend for a good 2nd, average 3rd year programming assignment in a data science curriculum, perhaps excluding body content).

So basically you mean I should search for words using synonyms, but only do so in the title?",1,0,0,False,False,True,1637676810.0
qzwb8e,hlrpn2w,t1_hlpto9w,"If I am building a search engine that doesn't work on the internet, how do I assign a score? I mean websites could be ranked using some page Rank or some other variation of it, but if we are taking something that works on lecture slides or something else, how do I give them a score?",1,0,0,False,False,True,1637677011.0
qzwb8e,hlqvqdp,t1_hlqnw8p,"That is the correct one. You don’t need to learn about databases or hadoop to learn about how google does it. 

BM25 and Transformers (aka BERT* variants) are how Google achieves search results.",2,0,0,False,False,False,1637655533.0
qzwb8e,hmlw8hw,t1_hlsss2h,"Hey, so I took a look at this and didn't find much information on the actual implementation of the data structures. Any ideas on where I can find that? Not expecting the actual code here. They mentioned they have designed the data structures to reduce disk writes but there was no explanation as to how they did it",1,0,0,False,False,True,1638236553.0
qzwb8e,hlounxk,t1_hlou0wl,"If you have the existing data, and it is not too absurdly big it might be possible. There's kind a couple of interesting problems for a project that would be good. For example, building an effective data structure to facilitate the search. The actual search algorithm could be interesting, especially if you incorporate some machine learning to do some recommendations of related material. It is doable, no mistake about that, but it is a pretty big project for a month. I wonder consider using an existing web crawler and focus on data storage and search. Overall, just try to keep the scope manageable. :)",14,0,1,False,False,False,1637617122.0
qzwb8e,hlow57j,t1_hlouli1,"I would focus primarily on the data structure aspects. I think they're interesting enough for a (2nd year I'm guessing) project. Non-trivial for certain, and doable within the time frame.",2,0,0,False,False,False,1637617731.0
qzwb8e,hlpvey5,t1_hlpu50c,Correct. Upon reading your post that's not exactly what you're looking for,3,0,0,False,False,False,1637633773.0
qzwb8e,hlro95g,t1_hlrlpfv,"I made it up, what do you mean by vector search technique? Can I have a link?",1,0,0,False,False,False,1637676332.0
qzwb8e,hltuurp,t1_hlrp8di,"And keywords, if you can get them. If you're doing web pages these are in a meta tag.",1,0,0,False,False,False,1637708060.0
qzwb8e,hltv030,t1_hlrpn2w,Work out some criteria and base it off that. Number of times the search term appears is the easiest.,1,0,0,False,False,False,1637708124.0
qzwb8e,hlovlli,t1_hlounxk,"You're probably right, given how I need to take classes in this month as well, so I can't devote all my time. Plus, I can always build a web crawler later on and incorporate it in this search engine. 
Unfortunately, all machine learning stuff is off the table as I don't know anything about it and I am pretty thorough with my learning, so it'd take me at least 2 months to get a decent grip on it. I do have a course on ml, later on in my degree so I'll probably incorporate some rudimentary ml techniques in this project after that course.",5,0,0,False,False,True,1637617508.0
qzwb8e,hlqydzr,t1_hlpvey5,"Actually, no. The goal isn't to have a search engine, but to build one so I can learn more about it and the decisions that go behind creating something like this; specially when it comes to the storage of dataset to allow quick retrieval. While elastic search would build a search engine, it wouldn't achieve those goals",2,0,0,False,False,True,1637657814.0
qzwb8e,hlrq5xq,t1_hlro95g,"Vector search is [like this](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/), basically you build vectors to represent your 'objects' then compare them to find the most 'similar' vectors (eg those closest geometrically, or with the smallest angle between them).

I do think the hashing approach you made up is similar (although not the same) as [locality sensitive hashing (LSH)](https://www.pinecone.io/learn/locality-sensitive-hashing/), but in that case you're using a hash function specially designed to hash similar items into the same bucket, so when you have a search query you hash it with LSH and identify which other items were hashed to the same bucket.",2,0,0,False,False,False,1637677259.0
qzwb8e,hlrul14,t1_hlro95g,https://youtu.be/mu6ExLCtFsQ,1,0,0,False,False,True,1637679257.0
qzwb8e,hlu3su3,t1_hltv030,"Yep that seems good. Maybe I can even add a review field to it, to simulate the resource bank of a college where all the teachers have submitted their slides and we will be showing the ones that are rated higher by the students at the top.",1,0,0,False,False,True,1637712047.0
qzwb8e,hlp7vum,t1_hlovlli,">Unfortunately, all machine learning stuff is off the table as I don't know anything about it and I am pretty thorough with my learning, so it'd take me at least 2 months to get a decent grip on it. I do have a course on ml, later on in my degree so I'll probably incorporate some rudimentary ml techniques in this project after that course.

When you do decide to have a look at those things, [watch some 3Blue1Brown for the high concept background information](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), and also [have a gander at one of the most commonly used libraries in the field, for the practical information](https://pytorch.org/tutorials/). I've watched the 3Blue1Brown videos and found them as useful as good university lectures even when you are just watching them to slightly increase your understanding and most of the math goes over your head*, and pytorch is a gold standard which is on my list just for the experience even though I don't use python much. If you are comfortable with such personal goals as building a search engine then this is fairly reasonable and probably worth your time.

*It certainly did for me, but I got enough of it to grasp the idea and to know that I could learn it if I took the time, which I certainly will in the very near future.

Edit: I hope it's appropriate to hand out resources like that. I'm sure someone will find them useful.",2,0,0,False,False,False,1637622803.0
qzwb8e,hlrswzg,t1_hlrq5xq,"First of all thank you for the valuable information!These are topics I am very unfamilliar with, but what I presented above can be implemented in any way you like, why not.What I can think of out of this is that if you manage to somehow create a Vector-space with meanings of words, It would make the search engine even greater as it would not be limited to spelling. But it's good to mix it with searches of similar words grammarly, because someone can mispell something that probably doesn't exist in a dictionary e.t.c.

LSH sounds awesome, but won't there be a lot of collisions in your hashmap?",2,0,0,False,False,False,1637678519.0
qzwb8e,hlrlbak,t1_hlp7vum,"Thanks man, I'll definitely take a look at them once I get the time or my ml course starts",2,0,0,False,False,True,1637674825.0
qzwb8e,hlsxihu,t1_hlrswzg,"Yes exactly, there was a [cool example recently](https://youtu.be/7RF03_WQJpQ?t=150) which implemented both BM25 and NLP models for searching through wikipedia on 'what is the capital of the US?', and the BM25 approach comes up with a load of random things that contain 'US', 'capital', etc - whereas the NLP models pull in the right responses.

On LSH yes you can get too many collisions, you have to increase the number of possible buckets until you reach a point where there's not too many in each bucket - you increase the number of buckets by increasing the number of bits (which I think of as being similar to increasing the resolution in photos)",1,0,0,False,False,False,1637694737.0
qzwduc,hlovby3,t3_qzwduc,Build different sorting algorithms. There's a reason they show up so commonly as early assignments in a CS degree. They are very good practice for working with lists and logical thinking.,12,0,0,False,False,False,1637617396.0
qzwduc,hlpd6fw,t3_qzwduc,"Try LeetCode webpage. It's mostly for interview questions, but there are also beginner level stuff and general DS&A problems. For example, implementing your own linked list and then reversing given one, sorting or merging two sorted into one big sorted.",4,0,0,False,False,False,1637625210.0
qzwduc,hlr2eb1,t3_qzwduc,You can try competitive coding on platforms like codeforces and codechef,2,0,0,False,False,False,1637661380.0
qzwduc,hlrhbq5,t3_qzwduc,"Try pepcoding 

Start from the basic problems of DSA 1 

even if you think that you can solve those ""easy"" problems, still put in the effort and solve everything. It'll take you 6-7 months to complete all those problems.",2,0,0,False,False,False,1637672625.0
qzwduc,hlrd1jq,t3_qzwduc,"Try doing a [WAVL Tree](https://en.m.wikipedia.org/wiki/WAVL_tree).

They are somewhat of a halfway point between AVL and red-black trees in terms of performance.

AVL trees have height at most 1.44 * log(n).

Red-black trees have height at most 2 * log(n).

WAVL trees have height at most 1.44 * log(n) if only insertions are performed but can get closer to 2 * log(n) if deletions are performed. It will build the exact same tree as an AVL tree if only insertions are performed.

AVL trees need at most log(n) tree rotations per operation.

Red-black trees need at most a constant number of tree rotations per operation.

WAVL trees need at most log(n) tree rotations per operation, however amortized over time it is a constant number.

Here is the original [paper](http://sidsen.azurewebsites.net/papers/rb-trees-talg.pdf) which details the operations.

As far as the difficulty of implementation I would say they are again somewhere between AVL and red-black trees lol.",1,0,0,False,False,False,1637669983.0
qzwduc,hlrtw88,t3_qzwduc,Solve algo class exercises from other universities. They are usually in depth and require a lot of thinking,1,0,0,False,False,False,1637678954.0
qzwduc,hlovtyt,t1_hlovby3,Are you talking about bubble sort kinda stuff? I have already built almost all of them and I wanted some nice questions that show me how they'd be used in a real world scenario.,2,0,0,False,False,True,1637617604.0
qzwduc,hlrkfa1,t1_hlpd6fw,This is exactly what I am looking for. Should I take one of the paths they offer?,1,0,0,False,False,True,1637674358.0
qzwduc,hlrzv5s,t1_hlrtw88,I am in a uni myself and I do solve the assignments myself. The reason I asked this was I wanted to find individual questions giving me an idea of what can be done with a Ds concept like reversing a list. Leetscode and similar platforms seems good enough.,2,0,0,False,False,True,1637681537.0
qzwduc,hloxc0t,t1_hlovtyt,You could build a basic accounting software.,3,0,0,False,False,False,1637618224.0
r02lfl,hlskagy,t3_r02lfl,Reverse-engineer the problems existing websites solve and come up with your own solution on pencil and paper?,1,0,0,False,False,False,1637689611.0
r02lfl,hltok5n,t3_r02lfl,"The whole point of studying algorithm design and analysis is knowing if they work and how they behave asymptotically without having to test them first.

Do as Dijkstra, and program on paper.",1,0,0,False,False,False,1637705401.0
qzo8bt,hloj44f,t3_qzo8bt,"I learned scalability and software design patterns over a fairly long period of time by osmosis, so I don't really know of any good resources to help you in that regard, but I do want to say something about the OOP hierarchy problem that you mentioned, which was one of the first non-trivial design problems I encountered when learning OOP.

The trick to that particular problem in OOP, as I see it, is in recognizing inheritance as a secondary feature of the paradigm (the primary feature being just polymorphic message passing). My first language was modular and procedural, so I actually had a bit of difficulty learning OOP as a concept (with Java 6) probably due to the fact that I was so used to thinking procedurally. Whatever the reason for my troubles, learning Smalltalk, with which I quickly replaced Java as my main OOP language, was immensely helpful in elucidating for me the core of OOP and thus in helping me to gain an intuition for what complex OOP software should look like from a high level.

All this to say that focusing upon inheritance relationships (aka ""is a"" relationships) tends to result in hierarchies that are difficult to follow or even to formulate in the first place, which is the issue you mention. Instead of using inheritance, try using interfaces, (or whatever ad-hoc class level polymorphism your language provides). This is a key element in an approach to OOP that is often referred to as ""composition over inheritance.""

For a while, try writing OOP programs using no inheritance at all (other than mandatory inheritance, like from Object in Java, ofc). I don't believe that inheritance is always evil, but refusing to use it is a good way to train yourself to not have to, and thus to become more flexible and better able to accurately model the domain of your software.

Alternatively, I guess you could do what I did and just write really shitty software for a while before eventually figuring it out, but your approach, asking for help and resources, seems to me the definitively smarter way to go about it.",4,0,0,False,False,False,1637612465.0
qzo8bt,hlovoo1,t3_qzo8bt,"[Design Patterns](https://en.wikipedia.org/wiki/Design_Patterns) is the canonical reference for the patterns themselves, it's mainly a catalogue, but there are use cases there too from what I recall.  Still, for me, this text and any of a number of videos or tutorials on the subject do little to instill in me the ability to recognize when to use what pattern beyond the more simple ones.  I guess it just takes a lot of practice, and prior to that learning the patterns.",3,0,0,False,False,False,1637617542.0
r07asz,hlu7f0q,t3_r07asz,[Wikipedia](https://en.wikipedia.org/wiki/Parallel_ATA) to the rescue!,1,0,0,False,False,False,1637713728.0
qzkegn,hlmy2nv,t3_qzkegn,IOT hacking by No Starch Press,2,0,0,False,False,False,1637588549.0
qzkegn,hlnc3nl,t3_qzkegn,Building the Internet of Things by Maciej Kranz,2,0,0,False,False,False,1637595184.0
qzkegn,hln2sc5,t1_hlmy2nv,"Ok thanks! 
Does it contain application and importance of different devices?",1,0,0,False,False,True,1637590961.0
qzkegn,hlnjro6,t1_hln2sc5,"Not sure, sorry",2,0,0,False,False,False,1637598351.0
qzkegn,hlnvhae,t1_hlnjro6,Ok thanks... I'll check it out anyway!,1,0,0,False,False,True,1637603041.0
qz9jwv,hlobf17,t3_qz9jwv,"Not an expert, but I do know that kernel programming is very difficult due to the fact that encountering a bug in the kernel means it’ll likely crash to the metal. Plus, recompiling kernels takes a very long time.",1,0,0,False,False,False,1637609361.0
qz023c,hlj81jx,t3_qz023c,"You can find a lot of literature on Google Scholar on any of those subjects.

&#x200B;

For example, [https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=genetic+algorithms&btnG=](https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=genetic+algorithms&btnG=)

&#x200B;

I'd suggest limiting the search to the 1970s, 1980s and 1990s to see the foundational papers. E.g., Holland on Genetic Algorithm.",10,0,0,False,False,False,1637516772.0
qz023c,hljqrzd,t3_qz023c,"Cant help much with genetic algorithms or Bayesian optimization, but for reinforcement learning I strongly suggest  [Sergey Levine's video lectures](https://www.youtube.com/watch?v=JHrlF10v2Og&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc). Going from the basic algorithms and methods used into more difficult problems without shying away from the math involved.",4,0,0,False,False,False,1637524123.0
qz023c,hlme97c,t3_qz023c,Koza wrote a very detailed book or two on genetic programming https://www.goodreads.com/book/show/644125.Genetic_Programming,3,0,0,False,False,False,1637574105.0
qz023c,hlj8xs4,t1_hlj81jx,Thanks a lot 👍,3,0,0,False,False,True,1637517133.0
qz023c,hljq1fv,t1_hlj81jx,"Holland's paper is really good so far (although there is a surprising amount of small typos). I'm just starting it, but it's obvious that this is related to machine learning (talk of hill climbing algorithms in the first page). 

Thank you for posting this and your other comment as well, and thank you to the OP for bringing this up. I'm going to make a week out of this.

Update: [This paper is really good, actually.](http://www2.econ.iastate.edu/tesfatsi/holland.GAIntro.htm) I've been able to reproduce some of the easier parts of it in Kotlin and it is very neat once you see it in action. In a few weeks or months I'll post a link with the full reproduction of the Prisoner's Dilemma example, because that's a very good one which teaches you the fundamentals of building a little framework in which to solve problems in this way. 

It's so good I've decided to order one of John Holland's books (*Signals and Boundaries*). Very neat stuff. Check that paper out if you're in to this stuff. I'm gonna be having a field day with it for a few weeks at least. Definitely a subject I want to study in depth.

Update 2: I've decided to make a little library around this idea for learning. Early stages yet, but I've implemented many of the ideas from the paper. Feel free to take a gander, as I have open sourced it. Just bear in mind that it is early stages and a prototype. It may prove helpful to anyone interested in grasping how Genetic Algorithms work (at least according to that paper): [Genetic Playground](https://github.com/sgibber2018/GeneticPlayground).",2,0,0,False,False,False,1637523826.0
qz023c,hlnigoe,t1_hljqrzd,"Yep, haven't watched them, but also heard these are great from Sergey.

So is the ""RL: An Introduction"" Sutton & Barto book ([here](https://incompleteideas.net/book/RLbook2020.pdf)) or David Silver's (guy behind DeepMind's AlphaGo) [lecture series](https://www.youtube.com/watch?v=2pWv7GOvuf0) \- I used these two and I ended up publishing research in RL!",5,0,0,False,False,False,1637597821.0
qz023c,hljb5pg,t1_hlj8xs4,"No worries. I was in a meeting so my last reply was short.

I'd recommend Holland, Back, Grefstennette as all good authors on GA and evolutionary algorithms. Mitchell is good as well for AI/ML in generall.

A good paper on Bayesian inference. Mitchell covers it as well

[https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1968.tb00722.x](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1968.tb00722.x)

A could of good paper on hyperparameters.

[https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)[https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)

&#x200B;

If you decide to get into optimization, then make sure to read the ""No Free Lunch Theorem."" Very important and generally poorly understood.",4,0,0,False,False,False,1637518035.0
qyv2sd,hlih7cx,t3_qyv2sd,"Is this resource useful? 
https://ifs.host.cs.st-andrews.ac.uk/Books/SE9/WebChapters/PDF/Ch_27_Formal_spec.pdf

You may want to try this question over at 
https://www.reddit.com/r/learnprogramming/",1,0,0,False,False,False,1637505411.0
qyv2sd,hlkoklg,t3_qyv2sd,Check out Agile Systems Engineering by Bruce Douglas. It’s fantastic.,1,0,0,False,False,False,1637538345.0
qyv2sd,hlirgr4,t1_hlih7cx,"Hey, thanks a ton. This helped a lot !",2,0,0,False,False,True,1637509952.0
qyo107,hlh9vz6,t3_qyo107,this is managed by the operating system. the OS retrieves the memory capacity from the underlying hardware via syscalls. it is responsible for allocating memory to running applications.,41,0,0,False,False,False,1637475022.0
qyo107,hlir6vz,t3_qyo107,"Most memory and storage hardware doesn't really have a sense of full or not. There is something at every address. It's up to software to decide if that information is meaningful or not. The operating system breaks that memory up and gives it out (assigns it) to processes or to itself. The new ""owner"" of the memory can write useful stuff into it instead of whatever garbage happened to be there initially. The hardware can help with controlling which processes can and cannot write where.

If you'd like to learn more, you can Google for ""operating system memory allocator"" it's a pretty well defined topic so I bet there's good material out there for more details.",5,0,0,False,False,False,1637509840.0
qyo107,hljavsd,t3_qyo107,How does a hotel know how many of its rooms are full?  It keeps track of them as they are occupied and records that.,4,0,0,False,False,False,1637517924.0
qyo107,hljefd3,t3_qyo107,"This is exactly the kind of question that is answered in an OS class. If your university has one, be sure to take it! If not, may I suggest Operating Systems: 3 Easy Pieces, which is a relatively easier book to read (as far as CS textbooks go), but covers all the main topics (and a few extra ones too)",2,0,0,False,False,False,1637519324.0
qyo107,hljnld8,t3_qyo107,great question !!,1,0,0,False,False,False,1637522854.0
qyo107,hlha3oz,t1_hlh9vz6,"Ah that makes a lot of sense, Cool! And I am assuming the operating system adds to the used memory variable as programs write to memory. I am also assuming it is dependent on priorities, like with Windows you can set the priority of a program and it will allocate more resources to that program if available",11,0,0,False,False,False,1637475168.0
qyo107,hljepaa,t1_hlh9vz6,"Hmm, does the OS retrieve the memory capacity from hardware via syscalls? My understanding is that syscalls are used to call priveleged functions in the OS (such as reading a file, or allocating memory) from functions in the user space, and that the OS keeps track of all the memory that it has allocated in additional data structures, which is how it keeps track of the memory usage. Correct me if I am wrong.",3,0,0,False,False,False,1637519429.0
qyo107,hlhc7vr,t1_hlha3oz,"operating systems can use several memory management models including single contiguous, partitioned, and paged allocation. this wikipedia article is good literature: https://en.wikipedia.org/wiki/Memory_management_%28operating_systems%29?wprov=sfla1

the priority in windows operating systems you are referring to is process priority, also known as ""niceness"" on Unix/Linux-based operating systems. priority/niceness modifies scheduling, which is an allocation of various resources including CPU time.",18,0,0,False,False,False,1637476673.0
qyo107,hljk1z8,t1_hljepaa,"you're correct. my mistake, I just reread my post. the os queries the BIOS or UEFI for the systems memory capacity. it makes memory available to applications in user space and reserves memory for itself in kernel space. user space applications have no visibility to memory reserved in kernel space, though the os makes the capacity (amount) of that memory available to some applications via syscalls.",3,0,0,False,False,False,1637521495.0
qyo107,hlhdhzh,t1_hlhc7vr,Awesome! Thanks for clarifying that and thanks for the help,6,0,0,False,False,False,1637477617.0
qyo107,hljg0bs,t1_hlhc7vr,"Something along the lines of:

There are 4,447 places in memory info can be stored. Program x wants to use locations 1 - 47, thus 4,400 are free?

Except, that changes many thousands of times per second. Programs x, y, and z are all taking memory and letting go as needed.

Am i in the right ball park?",2,0,0,False,False,False,1637519931.0
qyo107,hljkgxl,t1_hljg0bs,you're in the right ballpark. your example is one of contiguous allocation.,2,0,0,False,False,False,1637521651.0
qyaklu,hlepxnw,t3_qyaklu,[CoRecursive: Coding Stories](https://corecursive.com),31,0,0,False,False,False,1637430425.0
qyaklu,hleu2c9,t3_qyaklu,"[Code Newbie](https://www.codenewbie.org/podcast), [Coding Blocks](https://www.codingblocks.net/), [Command Line Heroes](https://www.redhat.com/en/command-line-heroes), [Soft Skills Engineering](https://softskills.audio/)",13,0,0,False,False,False,1637432130.0
qyaklu,hlghna7,t3_qyaklu,"I like Advent of Computing, but it is more of a deep dive into the history of computing.",5,0,0,False,False,False,1637459187.0
qyaklu,hleptmy,t3_qyaklu,"Not compsci but cybsec related darknet diaries are really good, very light in terms of tech but good to wind down with. Malicious life, privacy security and OSINT are very good if you like something I little more technical.",4,0,0,False,False,False,1637430377.0
qyaklu,hlguxyy,t3_qyaklu,[Algorithms + Data Structures = Programs](https://adspthepodcast.com/),5,0,0,False,False,False,1637466025.0
qyaklu,hlf9zf4,t3_qyaklu,Lex Fridman has some really good guests. Grumpy Old Geeks is kinda fun.,14,0,0,False,False,False,1637439014.0
qyaklu,hlfblsq,t3_qyaklu,Wes Bos and Scott Tolinski's Syntax podcast is great for web development stuff. I'm not really into web dev but I still find it entertaining!,2,0,0,False,False,False,1637439742.0
qyaklu,hlgzbs0,t3_qyaklu,Any body know any british coding podcats?,2,0,0,False,False,False,1637468467.0
qyaklu,hlftond,t3_qyaklu,"https://oxide.computer/podcasts

https://www.softwareatscale.dev/?utm_campaign=pub&utm_medium=web&utm_source=copy",1,0,0,False,False,False,1637447917.0
qyaklu,hlhbtux,t3_qyaklu,Real Python and Django Chat are on Spotify.,1,0,0,False,False,False,1637476394.0
qyaklu,hly6zyq,t3_qyaklu,"Its not directly coding, but Darknet Diaries is really great :)",1,0,0,False,False,False,1637787902.0
qyaklu,hlevysh,t1_hlepxnw,"That one is soooo good.  Very in depth, long enough to get pretty technical, not so long that it gets boring.  I've especially liked their ""Apple 2001"" and SQLite shows.",16,0,0,False,False,False,1637432924.0
qyaklu,hm1k173,t1_hly6zyq,What’s it about?,1,0,0,False,False,True,1637854255.0
qyyi89,hliyz6i,t3_qyyi89,If you’re creating new ways to do it and studying the theoretical side it’s computer science. If you’re dealing with implementing and adapting an already understood method to real world constraints then it’s engineering.,9,0,0,False,False,False,1637513043.0
qyyi89,hljo9ef,t3_qyyi89,"That's kinda a misrepresentation of the boundary between science and engineering in tech.

If you're researching the known limits of the field, you're doing science. If you're implementing scientific discoveries in industry considering costs and scalability, you're engineering.

Though it is always still under the umbrella field called Computer Science.",9,0,0,False,False,False,1637523118.0
qyyi89,hlix3j9,t3_qyyi89,One could also argue that it's just a subfield of math,1,0,0,False,False,False,1637512272.0
qys9kx,hlia05u,t3_qys9kx,"I think you've drawn it oddly, which is causing the confusion.

Usually the ""fetch"", ""decode"" ""execute CMD"" is written on the X-axis or time-axis and the sequence of CMDs in the program along the Y-axis.

&#x200B;

See [https://www.sciencedirect.com/topics/computer-science/stage-pipeline](https://www.sciencedirect.com/topics/computer-science/stage-pipeline) for an example (I cannot figure out how to include a picture).",2,0,0,False,False,False,1637501577.0
qys9kx,hlhv1ds,t3_qys9kx,"An easier approach would be to make a line of all the instructions you are going to execute and calculate the time it takes for the last instruction in the line to pass the last stage in the pipeline. (See the image)

[https://i.ibb.co/qMBJ2MV/image.png](https://i.ibb.co/qMBJ2MV/image.png)

Circles are instructions. You have ""n"" of these

Squares are the pipeline with ""x"" stages

So the very last instruction has to travel a distance of (n-1+x) steps

All these take ""y"" nanoseconds each

So the total time should be (n-1+x)\*y nanoseconds",1,0,0,False,False,False,1637491122.0
qys9kx,hlhx3rx,t1_hlhv1ds,"Wait but that equation would mean it would take way longer to complete the program and therefore isn't staged?

I just dont understand what youre saying, could you describe it differently?",0,0,0,False,False,True,1637492745.0
qytazd,hli6urh,t3_qytazd,search subnet questions and you will find them,1,0,0,False,False,False,1637499670.0
qy9ke0,hlejxjn,t3_qy9ke0,"They're electrical, radio or optical pulses so if other pulses are in the same medium they can interfere.",22,0,0,False,False,False,1637427891.0
qy9ke0,hlentlg,t3_qy9ke0,They certainly aren't metaphysical.,10,0,0,False,False,False,1637429529.0
qy9ke0,hlhbr4l,t3_qy9ke0,"They are physical occurrences, of electrical and em wave signals.

But they are not physical bodies. More analogous to sound waves, which are also physical but not objects.

Interference is like if I say something and you can't understand because some other noise is too loud to hear enough. Collision is like if I say something and you don't catch it because someone starts talking to you specifically at the same time, or I interrupted myself half way to say something else.

It's just electrical and electromagnetic signals not sound waves.",6,0,0,False,False,False,1637476339.0
qy9ke0,hlezbst,t3_qy9ke0,My network prof told us today an IP packet (single) is long kilometers,3,0,0,False,False,False,1637434335.0
qy9ke0,hlepqjm,t3_qy9ke0,https://www.phys.uconn.edu/~gibson/Notes/Section5_2/Sec5_2.htm,1,0,0,False,False,False,1637430341.0
qy9ke0,hlipx74,t3_qy9ke0,"Lol, TCP stack will retransmit anything lost, dont worry about lost frames or packets :)",1,0,0,False,False,False,1637509313.0
qy9ke0,hliu377,t3_qy9ke0,Information is just an abstract representation of the motion of physical bodies,1,0,0,False,False,False,1637511027.0
qy9ke0,hlepsbs,t1_hlejxjn,"There's [a whole field of philosophy that deals with the relationship of parts to the whole](https://plato.stanford.edu/entries/mereology/), so I'd imagine they'd have something to say about whether a collection of particles is a physical body.",4,0,0,False,False,False,1637430362.0
qy9ke0,hlepuao,t1_hlentlg,LOL GOT 'EM,2,0,0,False,False,False,1637430384.0
qy9ke0,hlff3sc,t1_hlezbst,It's said that each IP packet is a separate miracle.,3,0,0,False,False,False,1637441314.0
qy9ke0,hlf6yqm,t1_hlepsbs,what is a physical body other than a collection of particles?,2,0,0,False,False,False,1637437662.0
qy9ke0,hlf870y,t1_hlf6yqm,The fact that all physical bodies are collections of particles doesn't imply that all collections of particles are physical bodies. That's just logic. I'm super logical.,3,0,0,False,False,False,1637438203.0
qy9ke0,hlg5yxk,t1_hlf870y,"Hi, super logical. I’m dad.",11,0,0,False,False,False,1637453573.0
qy9ke0,hlgh73v,t1_hlg5yxk,Take it. Take your upvote.,4,0,0,False,False,False,1637458964.0
qykxoh,hlgyz2v,t3_qykxoh,"The memory says eg ""hello, I am 16 GB"" when the bios starts. It is then known. There's no calculation at all.",9,0,0,False,False,False,1637468261.0
qykxoh,hlig6iv,t3_qykxoh,A slightly off topic comment but how did you learn about the computer architecture ? Is there a book / site that is helpful to get started with concepts related to building a computer ?,2,0,0,False,False,False,1637504907.0
qykxoh,hlh0aep,t1_hlgyz2v,"Ah okay kinda makes sense, the reason I thought it calculates the amount is because if you look at older machines you can see the amount of RAM count up until like 65536K for example

Edit: A good example I see is in my old Pentium 4 Machine with 700MB RAM. It counts up rapidly until it sees i think 768MB RAM and on screen says 768MB RAM OK or something similar to that",2,0,0,False,False,False,1637469029.0
qykxoh,hll7x5j,t1_hlig6iv,"I learned from a book called ""But How do it know"" and a series that ben eater made on youtube: [https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU)

He has a lot  of really good videos on the topic that I think you may like",2,0,0,False,False,False,1637547328.0
qykxoh,hlh0nt2,t1_hlh0aep,"It's testing the memory for errors before booting. Modern computers can still do this, but the firmware in many cases is configured to skip the tests by default because they generally aren't thorough, modern users are generally impatient and expect startup to be quick, and there's specialized software for testing memory (e.g. MemTest86).",3,0,0,False,False,False,1637469247.0
qykxoh,hm1qycq,t1_hll7x5j,Apologies for the late reply - But thanks !,1,0,0,False,False,False,1637857330.0
qykxoh,hlh1xfi,t1_hlh0nt2,"Yeah while playing a match of Dead By Daylight I realized that, Thanks for the help! Both of you",1,0,0,False,False,False,1637469998.0
qyiizq,hlg7onb,t3_qyiizq,"Look at the relationships any gate operators relate to a gate symbol you can draw for a circuit diagram.

A cleanish way to draw the diagram would be to have inputs (normally a,b,c,…) along the top with lines coming down and then lines that divert off of that horizontally into the circuit gates. 

So if you have A+B in your truth table then you know you need input for A and input for B and an AND gate.

There are tons of images if you checkout google. Programming Circuit Diagrams",2,0,0,False,False,False,1637454386.0
qyiizq,hlg7uv2,t3_qyiizq,"Well, if the function is multi-argument (more than two), I would go for the Karnaugh map in order to minimize the function.",2,0,0,False,False,False,1637454466.0
qyiizq,hlhmcro,t3_qyiizq,I think circuit simulators should usually have an option to input the desired truth table and then automatically create the circuit. At least that is what I used to use in my university. The tool we used is 'logisim'. Search for it and you should be able to find a download link. I also think the software is free or open source. Hope that helps!,1,0,0,False,False,False,1637484177.0
qyiizq,hlrlm2s,t3_qyiizq,Might be best to first use a KMap and get the simplified equation... then convert that in your head to a circuit diagram.,1,0,0,False,False,False,1637674979.0
qyiizq,hlkg746,t1_hlhmcro,I actually downloaded Logism however I was still curious as to what processes would be involved in converting manually for a deeper comprehension of the full adder. I'm not a computer science student or anything but I feel that it would be very cool knowledge to have. Thanks for replying.,1,0,0,False,False,True,1637534618.0
qyiizq,hlrln1e,t1_hlrlm2s,This is how you would do it by hand,1,0,0,False,False,False,1637674992.0
qyiizq,hlkmg6h,t1_hlkg746,"Well I remember we did make a full adder in uni. I think we somehow first made a half 1 bit adder. Then a full adder. And I also remember the nice thing about logisim is that you could make your own components. So we took the full adder and made it a component with two inputs and two outputs(result and carry?). Not sure if it's called carry, but it is the overflow bit. So having this new component you could make any n-bit adder. For example a 4 bit adder.",1,0,0,False,False,False,1637537394.0
qyiizq,hlkmnhw,t1_hlkg746,"Introduction to the Logical Design of Switching Systems by h.c. torng 

It is an old textbook, but I think it is what you are asking for.  Because it predates all the computer-based methods of doing this, it explains everything from scratch.",1,0,0,False,False,False,1637537485.0
qyiizq,hlkr6in,t1_hlkmnhw,"Damn 1967, that's an old textbook alright. Sounds like it would be useful though, I suspect there's nowhere online that I can view this however considering its age.",1,0,0,False,False,True,1637539527.0
qyiizq,hlkvr20,t1_hlkr6in,"Indeed it is.  Looks like you could get a copy from Abe Books for just a few dollars though.  The edition I am familiar with is the 1966 one.  I suspect the later ones drop the chapter on relay logic design, since by then they were only used in elevators I suspect.  But if you want to build something like a 4-bit relay adder, it's essential.",1,0,0,False,False,False,1637541618.0
qxxaab,hlcl93b,t3_qxxaab,"No, because although I have a B.S. in Computer Science, I am a software engineer.",144,0,0,False,False,False,1637382293.0
qxxaab,hlcyp5j,t3_qxxaab,"Since I have a master's degree in CS, and not a PhD, I just refer to myself as a computer master.",116,0,0,False,False,False,1637391016.0
qxxaab,hlckoca,t3_qxxaab,"Yes, depending on the context. Sometimes I just call myself a scientist or a medical researcher. It depends on the audience. The problem with telling laypeople that I am a computer scientist is they think this means I'm a programmer (and boy do they have a great app idea they want me to build) or a technician (and boy do they want me to fix their PC/laptop). As you may notice in this subreddit, we get a lot of such confusion with numerous tech support and programming posts. There is some overlap (especially with programming) of course, but a lot of differences as well. Anyway, TL;DR, depends on the context.",48,0,0,False,False,False,1637381963.0
qxxaab,hlcna7i,t3_qxxaab,"No, I usually think of scientist as someone who does research as their job. I’m a software engineer.",39,0,0,False,False,False,1637383457.0
qxxaab,hldbbvi,t3_qxxaab,I refer to myself as a code monkey.,33,0,0,False,False,False,1637401677.0
qxxaab,hlcn4ta,t3_qxxaab,"I have yet to conduct my own science outside of perscribed assignments, so no. But I will happily award myself the title once I conduct my own research to answer my own question no matter how trivial the question is.",13,0,0,False,False,False,1637383372.0
qxxaab,hldsmlg,t3_qxxaab,Depends on how badly I'm trying to win the argument,11,0,0,False,False,False,1637414942.0
qxxaab,hlcouxk,t3_qxxaab,I refer to myself as just some dude who sits in front of a computer lol,11,0,0,False,False,False,1637384396.0
qxxaab,hle0azc,t3_qxxaab,I don't know what the hell I am anymore.,10,0,0,False,False,False,1637419149.0
qxxaab,hlddk2s,t3_qxxaab,"No, because I'm not a computer scientist. I refer to myself as a programmer, because I program computers and other devices.",9,0,0,False,False,False,1637403645.0
qxxaab,hlds9od,t3_qxxaab,"Computer scientist by study, software engineer in practice. So yes, but also no",10,0,0,False,False,False,1637414725.0
qxxaab,hldnz53,t3_qxxaab,"No, I call myself Slick Fatsack",7,0,0,False,False,False,1637411959.0
qxxaab,hlcxnxo,t3_qxxaab,"The usual nomenclature is this.

The term computer scientist typically refers to people who have the educational background, which is usually Phd, to carry out of original research or contribute to original research in the field of computer science.

People who therefore do not fit this definition do not usually go by the description of computer scientist.",12,0,0,False,False,False,1637390241.0
qxxaab,hldcl5s,t3_qxxaab,"That might be the degree, but you are only a computer scientist if you study computation.",11,0,0,False,False,False,1637402782.0
qxxaab,hle6lzq,t3_qxxaab,"In my language i dont recall anyone actually saying it. Programmer, tech guy, it, engineer, developer, but never ""scientist""",3,0,0,False,False,False,1637422145.0
qxxaab,hle9drs,t3_qxxaab,"Nah, Depressed usually does the job.",3,0,0,False,False,False,1637423369.0
qxxaab,hldfv2h,t3_qxxaab,"No, just like how traders are to math statisticians I am to computer scientists",2,0,0,False,False,False,1637405628.0
qxxaab,hldnbng,t3_qxxaab,"I've been called a Computer Science Major, Software Engineer and a Developer in my career. I don't care what they call me at work. As long as it's not derogatory.",2,0,0,False,False,False,1637411502.0
qxxaab,hlfe407,t3_qxxaab,I'm a computerer,2,0,0,False,False,False,1637440864.0
qxxaab,hldbknf,t3_qxxaab,I call myself a computer scientist on the grounds that I have the prerequisite training to do research in the field of computer science and I can teach the fundamentals of my field as a teacher (even though I'm not actually a CS teacher). Daily reminder that computer science is a discipline of mathematics.,3,0,0,False,False,False,1637401883.0
qxxaab,hle43ni,t3_qxxaab,"Well, maybe?  Even though i’m now a senior executive i still call myself a software engineer, but when i’m teaching computer science to high school students (through my non-profit), i will sometimes say i’m a computer scientist, especially when knee deep in talking abou measurement of algorithms or complexity classes of problem spaces.

I actually hate the field name ‘computer science’, but dont have one I prefer to replace it with.",1,0,0,False,False,False,1637420986.0
qxxaab,hlekgjn,t3_qxxaab,"Yes absolutely why not, I successfully wrote hello world and can create html in notepad in windows.",1,0,0,False,False,False,1637428110.0
qxxaab,hlddznd,t3_qxxaab,I do! :),1,0,0,False,False,False,1637404035.0
qxxaab,hle8b3y,t3_qxxaab,"Refer to myself as a software developer personally but that also happens to be my current job title, even when it wasn’t though- I still use software dev",1,0,0,False,False,False,1637422904.0
qxxaab,hleaz0o,t3_qxxaab,no just a humble data janitor,1,0,0,False,False,False,1637424060.0
qxxaab,hleb0u6,t3_qxxaab,"No, i have a degree in computer but I'm a systems administrator",1,0,0,False,False,False,1637424082.0
qxxaab,hlegahv,t3_qxxaab,I refer to myself as a senior data engineer,1,0,0,False,False,False,1637426381.0
qxxaab,hlemac3,t3_qxxaab,"No, I refer myself as a developer. Lately with the faaaaaaang stuff the lens on what developers should know has shifted to more to the comp sci frame ignoring so much more of the job.

So yeah, unless your doing research type activities your not a computer scientist (IMO).

It seems that ‘engineer’ is a title that some are using to differentiate between developers and those in between scientists.

In the end it’s just a title.",1,0,0,False,False,False,1637428890.0
qxxaab,hlf3y8b,t3_qxxaab,I have a BS in computer science and I’m an electrical engineer.,1,0,0,False,False,False,1637436334.0
qxxaab,hlfa3e3,t3_qxxaab,"No, i tell people Im a computer engineer so they don’t think all I do is plug in cables and open Microsoft word.",1,0,0,False,False,False,1637439063.0
qxxaab,hlfonth,t3_qxxaab,"It’s applied math using computers.

Third order of Hogwarts.",1,0,0,False,False,False,1637445614.0
qxxaab,hlfty37,t3_qxxaab,I prefer the term IT Lich.,1,0,0,False,False,False,1637448040.0
qxxaab,hlg3p6e,t3_qxxaab,"I do science with and about computers, so yes, computer scientist is a good term.",1,0,0,False,False,False,1637452520.0
qxxaab,hlgjhy6,t3_qxxaab,I have part of a degree in CS and a whole degree in liberal arts. I am a “byte wizard”.,1,0,0,False,False,False,1637460097.0
qxxaab,hle5jv2,t3_qxxaab,Yes.,-1,0,0,False,False,False,1637421658.0
qxxaab,hleok3v,t3_qxxaab,Yes,-1,0,0,False,False,False,1637429840.0
qxxaab,hlduumi,t3_qxxaab,Yes and a games programmer.,0,0,0,False,False,False,1637416235.0
qxxaab,hlcpbbt,t1_hlcl93b,"Yeah, this is me. Im a software engineer, even though my B.S in CS",34,0,0,False,False,False,1637384671.0
qxxaab,hleimju,t1_hlcl93b,"computer alchemist, got it.",31,0,0,False,False,False,1637427358.0
qxxaab,hlem6c0,t1_hlcl93b,Same here. I guess if I was in the research world then computer scientist would be appropriate.,8,0,0,False,False,False,1637428844.0
qxxaab,hlf6s0s,t1_hlcl93b,Same,1,0,0,False,False,False,1637437579.0
qxxaab,hle64t5,t1_hlcyp5j,"I have BS in Computer Science, but did not finish my Master’s degree. So I refer to myself as a computer slave, or on days when I have abandoned human and returned to monke, code monkey. 

https://www.cafepress.com.au/mf/4785794/warning-monkey-coding_sticker?productId=12172404",25,0,0,False,False,False,1637421928.0
qxxaab,hldsla9,t1_hlcyp5j,very cool,10,0,0,False,False,False,1637414920.0
qxxaab,hlekpy9,t1_hlcyp5j,If you get a PhD you would be a computer doctor,16,0,0,False,False,False,1637428221.0
qxxaab,hlfhjp7,t1_hlckoca,What do you actually do if not programming?,1,0,0,False,False,False,1637442404.0
qxxaab,hlcy69i,t1_hlcna7i,Data scientist has entered the chat,22,0,0,False,False,False,1637390616.0
qxxaab,hlefjvn,t1_hldbbvi,I refer to me as my own bad self.,3,0,0,False,False,False,1637426063.0
qxxaab,hlewbu5,t1_hldbbvi,😂😂😂,2,0,0,False,False,False,1637433079.0
qxxaab,hlfow7b,t1_hldsmlg,Or impress a girl,4,0,0,False,False,False,1637445720.0
qxxaab,hle6gcp,t1_hle0azc,I feel you mate...,1,0,0,False,False,False,1637422078.0
qxxaab,hldu6mf,t1_hldfv2h,Do math statisticians generate and analyze statistics about maths?,1,0,0,False,False,False,1637415858.0
qxxaab,hlg3l3x,t1_hldnbng,"Can they call you ""the most beautiful man/woman/else in the office""?",2,0,0,False,False,False,1637452468.0
qxxaab,hldcboq,t1_hldbknf,I don't agree. I think it uses math but it is its own thing.,2,1,0,False,False,False,1637402548.0
qxxaab,hlfn84e,t1_hlfhjp7,"CS subjects generally lays down the fundamentals in programming but that doesn't mean everything in CS is based on programming. It's more like on the creation of a more efficient algorithm, or set of instructions. We can also apply this in robotics, mechatronics, avionics, and other fields.

Basically, CS deals with logic and how, why, and when it can be applied to a computing device. This device is not limited to traditional computers. You can use matchsticks for all I care.",7,0,0,False,False,False,1637444978.0
qxxaab,hlfo2gl,t1_hlfhjp7,"Generally my life goes like this:

1. Get inspired by something.
2. Read the scientific literature about it.
3. Have an idea for a contribution to the literature.
4. Develop it further into a research proposal (data, metrics, analytics, methodology)
5. Gather any necessary data.
6. Preliminary data analysis.
7. Design and develop an algorithm to solve the problem (here lies programming although quite different then industrial software development, which I did for 14 years).
8. Evaluate the algorithm.
9. Fail.
10. Enhance the algorithm or build something entirely new (lots of thinking and some programming)
11. Evaluate the algorithm.
12. Probably still fail.
13. Repeat 10-11 until a breakthrough (literally had a massive research breakthrough yesterday after months of work that will completely change algorithmic inference forever, very exciting!!)
14. Write paper(s).
15. Publish papers(s)
16. Have paper(s) rejected.
17. Revise paper(s).
18. Repeat 16-17 until the paper is accepted.
19. Goto 1. :)

And on the side, mentor students, apply for funding and answer Reddit posts. :)

Note, I'm an applied machine learning researcher. A theoretical computer scientist would not do much if any programming at all.",8,0,0,False,False,False,1637445352.0
qxxaab,hlgafmy,t1_hlfhjp7,This is an easy TLDR I’ve heard before: programming is to a computer scientist as a telescope is to an astronomer. Programming is a tool through which they conduct their research,3,0,0,False,False,False,1637455682.0
qxxaab,hldl7xc,t1_hldcboq,"The basis of all computer science is all about the mathematical concept known as the computation. In anything related to computer science, I expect a trained computer scientist to formally prove the mathematical properties about any given computation. The consequence of being able to formally prove the nature of any computation means that computer science is fundamentally a discipline within mathematics.",6,0,0,False,False,False,1637409938.0
qxxaab,hli5zb1,t1_hlgafmy,That's a very apt description.,2,0,0,False,False,False,1637499120.0
qxxaab,hle913z,t1_hldl7xc,"I’m not disputing what you are saying about the basis of CS being math, but does it matter? You see, the basis of Medicine is  Biology, the basis of Biology is Chemistry, and the basis of Chemistry is Physics. Yet, by current standards, those are considered separate fields of science, with a lot of theory and methods shared among themselves. Do you expect a biologist to prove/explain the nature of her/his work by the formalities of chemical reactions?

The definition of what’s a field or another is highly debatable. A good analogy is to see the human knowledge as a continuous variable, and fields of knowledge being our attempt to discretize it.

Applied Computer Science which IMO is by far the most important part of the field, spreads across all other fields, e.g Computational Biology, Computational Chemistry, and even Computational Sociology. These fields couldn’t care less about proofs btw. So, how do we classify it? Are those subfields, mixed fields? 

By current standards of what constitutes a field, I dispute the idea of CS not being one in its own, but it doesn’t really matter. The goal of every field of scientific endeavor - applied or theoretical - is to further the knowledge of humankind, debating which field your work falls on, is utterly pointless.

Edit_0: typo",4,0,0,False,False,False,1637423217.0
qxxaab,hle6338,t1_hldl7xc,Yeah that makes a lot of sense actually. My mind is changed for sure. I study the damn thing I didn't even consider that haha,1,0,0,False,False,False,1637421906.0
qxxaab,hlg1zwp,t1_hle913z,"I'm going to refer to every medician, biologist, chemist, physicist, and mathematician as ""normal scientist"". This is because every field of endeavour is to further the knowledge of mankind, am I right? I'm being facetious here, I don't actually think this. 

The labels for each discipline of study exists because of the location where the locus of study is centred around. It's not surprising that there are overlapping themes, lessons, and tools that are shared between the distinct disciplines. The main idea is that the discipline of study is large enough that it deserves its own label to distinguish where is the locus of attention; the main idea is that the discipline has enough people thinking about the specific lessons for whatever motivation they decide.

Applied computer science exists upon the foundation set by the study discipline that we call computer science. No matter where you choose to apply the lessons of computer science, the foundational lessons of computer science remain a discipline of thought that exists without regard for application into real world practice. I consider applied CS to chemistry as computational chemistry. Formal proofs are still important within computational chemistry based on lessons learned within CS. 

For example, one lesson of general CS is that we can calculate the runtime cost and the runspace cost of any given computation. We can analyse whether the computation in question is a solution within P-time. We can analyse whether the problem itself is possible to be solved as a P-problem. If we find that the problem is a ""hard problem"", this means we won't be able to completely solve the problem on normal computers. One way to deal with this in the real world is to find a partial solution the problem. Or perhaps we find that the problem is so difficult that it's not actually possible to find a computation that gives a partial solution; we give up on trying to solve the specific problem and try to reformulate the problem into something that is feasible. Your average computer programmer who is untrained in these matters cannot apply this kind of analysis to the problem they are trying to solve; they could probably apply a naive ""hard"" solution because they don't know any better. This is just one lesson that is the consequence of understanding the cost of problems and computations, there are many other useful lessons that are a normal part of the computer science discipline.

I like to distinguish the discipline of computer science and the discipline of computer programming as being not the same discipline; you don't need computer science training to work as a computer programmer. If you consider yourself a trained computer scientist, then I expect to you be able to produce your formal proof that explains the different properties of your computation.",2,0,0,False,False,False,1637451737.0
qys6ai,hlidlcr,t3_qys6ai,"Something like this is actually used, but not to encode more than two states.

It is used in high end fibre optic connections used for internet communication. In a single fibre you not only establish a single connection but establish multiple connections that are multiplexed by different wavelenghts.

I remeber a youtube video from someone that visited a american fibre cable provider where this (remarkable unspectacular) beige box with a throughput of multiple Tb/s, through a single fibre, was shown, where multiple 10Gb/s links were connected and modulated onto different wavelenghts.

However, this only works for transmission. For computation you would need to design a transistor capable of process the different wavelengths parallel. But I doubt that this would improve calculation conpared to just use multiple regular transistors.",3,0,0,False,False,False,1637503555.0
qys6ai,hli5r9t,t3_qys6ai,.... What?,2,0,0,False,False,False,1637498983.0
qys6ai,hliexsy,t3_qys6ai,[See Wavelength-division multiplexing](https://en.wikipedia.org/wiki/Wavelength-division_multiplexing) and [optical computing](https://en.wikipedia.org/wiki/Optical_computing).,1,0,0,False,False,False,1637504270.0
qys6ai,hli7kad,t1_hli5r9t,"So instead of us looking at 0-1 values. 

If we had away of using colours to group values together. 
0 being a white light. When passed through an optic gives a value of a zero. 

When a red light is passed through gives on gives you a value of 00

When a blue light is turned on a value of 1 is passed through the optic.

Green a value of 11.

The colour spectrum is so vast, slight variations of colours could send down different formations of values.

What I was suggesting, by different colours could represent a different number. What I meant was a different colour could represent a different segment or sequence of binary.",0,0,0,False,False,True,1637500108.0
qys6ai,hljd6wy,t1_hli7kad,"The idea of a 0 and 1 at a physical level is an arbitrary line drawn between high and low voltage. Simply speaking, a voltage measurement above this line is a 1 and below the line is a 0. 

To instead encode 3 different values you can have three different voltage levels. You can have however many voltage levels you want. You do not need colors. The reason binary is used is because it's reliable and simple to work with (it's extremely easy to work with boolean algebra, etc...)",1,0,0,False,False,False,1637518853.0
qys6ai,hlia73g,t1_hli7kad,"OR...we could, I don't know, maybe just use some more bits to denote larger numbers?   
I mean, perhaps we could assign 8 bits together in some way, like, say each one is a power of 2? That way the least significant bit could be 2\^0 or just 0 or 1, then the next one could be 2\^1 or just 0 or 2, and so on. Collecting 8 bits together like that could represent integer numbers from 0 to 255! And if we keep going, using more bits, we could really get some very large numbers! Just thinking out loud.",0,0,0,False,False,False,1637501689.0
qyeqvw,hlflhpi,t3_qyeqvw,"You can always just encrypt it with <insert encryption algorithm here> and it won't be immediately obvious how to decrypt it, or where the key is in the code. Less effort is something like Base 64 encoding.",1,0,0,False,False,False,1637444195.0
qyeqvw,hlfofs3,t3_qyeqvw,"You could store them as integers (e.g. 0-25 for A-Z) instead of ASCII. That's basically still a Ceasar shift, but one step better, because it will prevent them from showing up as ASCII strings — someone using the [`strings`](https://linux.die.net/man/1/strings) command won't find them.",1,0,0,False,False,False,1637445513.0
qyeqvw,hlh7fdn,t1_hlflhpi,That <insert encryption algorithm here>  part is what I need help with. Basically deciding which to use since most implementations depend on some larger libraries.,1,0,0,False,False,True,1637473365.0
qyeqvw,hlh7o3d,t1_hlfofs3,"That's a good idea, going off of that idea... let's say I have 2 letters in my password, is there a way to store just one number? For example is there a way to store two unique numbers in one number that can then be taken apart to reliably obtain the two unique numbers back out?",1,0,0,False,False,True,1637473522.0
qyeqvw,hlh806u,t1_hlh7o3d,"Yeah, your numbers are probably 4 or 8 bytes wide. You said you don't have bitwise operators, but if you have division and modulus, you can separate each byte by dividing by 256^(n) then modulo by 256.",1,0,0,False,False,False,1637473743.0
qyeqvw,hliq0wb,t1_hlh806u,"Interesting, could you explain that more to me? For example If I took the number 2000, and did 2000 / 256 I would receive 7.8125. Are you saying there is a way to return to 2000 from 7.8125 using the modulo operation? Forgive me if I am not seeing this right away.",1,0,0,False,False,True,1637509356.0
qyeqvw,hlisjtw,t1_hliq0wb,"I meant integer division, where you floor the result. The number 2000 represents two integer bytes: 7, and 208.

(2000 // 256) % 256 = 7

2000 % 256 = 208

... where ""//"" means floored division, and ""%"" means modulo.

It might be easier to understand in the reverse direction. If we want to pack two integer bytes, 7 and 208, we multiply 7 by 256, then add 208.

7 * 256 + 208 = 2000

If you're familiar with bitwise operations, multiplying by 256 is equivalent to left shifting by 8, which makes room for a new byte in the least-significant position. Likewise, dividing by 256 is equivalent to right shifting by 8. Modulo 256 selects only the least-significant byte (equivalent to bitwise AND 255).

EDIT: Oh, and if your language treats all numbers as floating point, it likely uses ""double precision floats"" meaning you can use 53 bits before losing precision, so you can pack 6 byte-sized numbers (integers between 0-255 inclusive) in one ""number"".",1,0,0,False,False,False,1637510403.0
qxrdbd,hlbfxnt,t3_qxrdbd,"It's called ""self-modifying code"".

It's uncommon because it's really hard to write and there is no benefit to writing such code. Why bother?",102,0,0,False,False,False,1637362686.0
qxrdbd,hlbyr1x,t3_qxrdbd,They are rare because you need to be a genius to write them without causing horrible bugs.,16,0,0,False,False,False,1637370929.0
qxrdbd,hlbi71d,t3_qxrdbd,"Really hard to design and debug. The last time I saw it used was in microprocessor assembly for optimization purposes in a very constrained environment.
I wonder if a modern OS could even unprotect program memory to allow this in the first place, huge security hole if you can overwrite code...",38,0,0,False,False,False,1637363622.0
qxrdbd,hlcprj9,t3_qxrdbd,"I disagree with most of the answers given so far. I think most commenters are thinking of programs written in, say, C++ or assembly, and in the context of those languages it is true that self-modifying code is rare. 

But in many languages the self-modification of code is just a natural part of writing programs. Think about how in Python decorators work by dynamically defining an inner function. Or consider the low-level details of how Python works, how basically everything is a `dict`.

There is a related concept called [homoiconicity](https://en.wikipedia.org/wiki/Homoiconicity). A language is homoiconic if you can manipulate code as if it were data. Lisp and Prolog are the usually cited examples, but for me it was Mathematica that really made the concept click in my brain. In Mathematica, functions are just expressions that have a particular evaluation discipline.  So you might construct a mathematical function, say, and then apply that function to a list of values.  Or you might start with a function and then deconstruct the pieces of the function. For example, your function might be a polynomial, and your program might make decisions based on the degree or coefficients of the polynomial. Seasoned Mathematica programmers don't restrict the dynamic manipulation of code to just mathematical functions. 

So the answer is, people write self-modifying code all the time, but it's such a natural part of the languages in which it is done that it may not *feel* like you are doing anything fancy.",10,0,0,False,False,False,1637384951.0
qxrdbd,hlc5340,t3_qxrdbd,"In a very real sense many, perhaps even most programs will modify themselves all the time - you just need to expand your idea of 'themselves' beyond the base source code. Quite often the source code is a seed for some other thing, and that other thing is what is actually being executed, and will modify itself on the fly a whole lot. Polymorphism in OOP is probably the most approachable example of this.",10,0,0,False,False,False,1637373881.0
qxrdbd,hlcg88p,t3_qxrdbd,"It is called meta-programming and I once tried to write a ""meta-program"" or a program that modifies itself. It was an year ago, and it was JavaScript... so I could never complete writing the program because it was very hard to write it and there are ""relatively"" less resources about meta-programming (particularly js meta-programming) on the internet. 

It has a use case, just like my case, it was a blockchain(kind of) written in node.js(ikr bad choice) and I wanted the stdin value to be the name of a variable which stored an object `Wallet`. My program's workflow was like this: it would accept a value via stdin [using `readline` module of node.js] and then it would use the value inputted by the user as the variable name to store the user wallet in. Please do not hate me for this approach, ik this is a shitty approach but I just used it because I coded it in less time and did not want to set up a database kinda thing.

Edit: if anybody is interested to know how to do the task I wanted to do, I later found the solution, it can be accomplished using promises in JavaScript.",3,0,0,False,False,False,1637379528.0
qxrdbd,hlcj5el,t3_qxrdbd,Self mutating codes are gold mine for malware researchers. They are easy to hide from antiviruses.,8,0,0,False,False,False,1637381120.0
qxrdbd,hlbrvc7,t3_qxrdbd,"If you think of an operating system + all the currently running programs as a single program, then that program is self modifying. It modifies itself each time a program is opened or closed.",6,0,0,False,False,False,1637367816.0
qxrdbd,hlc3d6n,t3_qxrdbd,"It's called meta programing , if language is designed around it it can be viable ,its pretty good in Lisp family of languages specially in Racket",8,0,0,False,False,False,1637373061.0
qxrdbd,hlc5bo3,t3_qxrdbd,"As others have said, if debugging and maintaining someone else's code normally, imagine how it would be with the added difficulty of the code changing every time you opened it on an editor/IDE, any possible optimizations would seldom be worthwhile.

That said, there are some cases where the use of self-modifying code techniques are particularly common, malware development immediately comes to mind.",2,0,0,False,False,False,1637373996.0
qxrdbd,hlcfdsy,t3_qxrdbd,"There’s some use cases discussed in papers in Lisp.

Programs that edit themselves sounds dope. I’ll try it out.

I don't think saying it's useless is demonstrably correct. It's a tool, the fact is that people haven't been able to think on use cases. I know it should be really useful for generative systems. 

We must galaxy brain meme this and eventually someone will figure something out.",1,0,0,False,False,False,1637379081.0
qxrdbd,hlcnsg0,t3_qxrdbd,Do you want Skynet? Because this is how you get Skynet!,1,0,0,False,False,False,1637383756.0
qxrdbd,hlcp91v,t3_qxrdbd,"In many computer architectures (usually of the RISC variety), the instruction cache does not have to be coherent with the data cache. Thus, a self-modifying user-mode program would have to manually manage instruction cache invalidation (by calling the OS). This is a sufficiently difficult undertaking, so in terms of practicality, self-modifying programs may as well be impossible on these architectures.

It's been pointed out in this discussion that when an OS loads a program and executes it, it is itself a self-modifying program. This is correct. It can be generalized; from a strict technical perspective, all stored-program computers are by their nature, self-modifying, unless the program is in ROM and never changes. The OS can be self-modifying because it's much more restricted. It knows where it's loading the program into, and can invalidate the instruction cache accordingly.",1,0,0,False,False,False,1637384633.0
qxrdbd,hlcuxao,t3_qxrdbd,"One archaic use case that hasn't been mentioned is for bootstrap loaders back when ROM was not common and, when available, very small.  The first computer I programmed machine language for would boot by putting a -7 in the X register, 2 in the PC, an input from channel to location 2 in the instruction register, and start the paper tape reader in binary 4 character mode.  On the tape you had:

    2: input to location 10 indexed
    3: increment X and branch to location 2 if X negative
    4: load X with 9
    5: input location 0 indexed
    6: skip if buffer not ready
    7: increment X and branch to 5 (X always negative)
    8: branch to start address
    9: (load address + sign bit)
    10 ... end-of-tape (whatever you want)

Anybody know which machine it was? \[Hint: it wasn't named after a radical student group\]",1,0,0,False,False,False,1637388274.0
qxrdbd,hlcwm1o,t3_qxrdbd,Wouldn't that just be ai?,1,0,0,False,False,False,1637389473.0
qxrdbd,hlcypii,t3_qxrdbd,They aren't? JIT compilation is pretty common our days.,1,0,0,False,False,False,1637391023.0
qxrdbd,hld4wo9,t3_qxrdbd,"Self-modifying code can be used for obfuscation - making a code much harder to analyze and reverse engineer. Can be for malicious purposes or just plain anti-reversing (you don't always want your code to be easily understood, as it implements some algorithm you do not want people to know how it works).",1,0,0,False,False,False,1637396028.0
qxrdbd,hldhlnx,t3_qxrdbd,Would polymorphism count as style modifying to you?,1,0,0,False,False,False,1637407070.0
qxrdbd,hldit2w,t3_qxrdbd,"Yes, they make reference to it in a book called ""The Art of Computer Virus Research and Defense"" (Symantec Press) by Peter Szor. Part 1, Chapter 7, section 7.6, page 269, ""matamorphic viruses"".

> Virus writers try, of course, to implement various new code evolution techniques to make the researcher’s job more difficult. The W32/Apparition virus was the first-known 32-bit virus that did not use polymorphic decryptors to evolve itself in new generations. Rather, the virus carries its source and drops it whenever it can find a compiler installed on the machine. The virus inserts and removes junk code to its source and recompiles itself. In this way, a new generation of the virus will look completely different from previous ones.

If you're interested in this area take a look at a Java library called Java poet.

https://github.com/square/javapoet",1,0,0,False,False,False,1637408048.0
qxrdbd,hle3jfa,t3_qxrdbd,"Well if you’re willing to accept that a computer program = source code + data, then modifying data is akin to changing program behavior. Under that interpretation, this happens all the time",1,0,0,False,False,False,1637420720.0
qxrdbd,hlep0i7,t3_qxrdbd,"I don’t think it’s a stupid question. Here is an example of a self-modifying software which is capable of running compiled C code in Linux, Windows, macOS, and BSD. It sounds like sorcery, but it’s actually an incredibly clever piece of code. 

https://redbean.dev/

http://justine.lol/cosmopolitan/",1,0,0,False,False,False,1637430039.0
qxrdbd,hll1iy5,t3_qxrdbd,It's a pain in compiled languages (which rule serious development),1,0,0,False,False,False,1637544344.0
qxrdbd,hlbyvc9,t3_qxrdbd,"Oh yeah, that’s called Siri",-4,0,0,False,False,False,1637370982.0
qxrdbd,hlcmfsf,t3_qxrdbd,Its name is Madness ...  MADNESS I SAYYY,0,0,0,False,False,False,1637382969.0
qxrdbd,hlbiq7f,t1_hlbfxnt,"Very true. And good luck maintaining someone else's self-modifying code!

OP, here's a [blog](https://blog.osteele.com/2006/04/javascript-memoization/) discussing self-modifying code that is elegant and clever—but isn't exactly transparent, so I'd worry about it in production. For example, can you guess what this is about?

`OSGradients.initialize = {  
  OSGradients.initialize = function() {};  
  ... // initialization  
}`",30,0,0,False,False,False,1637363849.0
qxrdbd,hlchmh1,t1_hlbfxnt,"There's lots of reasons to write self modifying code, such as it being fun, interesting and emergent. I'm sure there's plenty of academic reasons to do so too.",9,0,0,False,False,False,1637380279.0
qxrdbd,hle3der,t1_hlbfxnt,"In addition to being hard to write, it would be hard to deploy and monitor. Not only do you have program state to worry about, but also source code state and whatever kind of reloader state that exists.",2,0,0,False,False,False,1637420644.0
qxrdbd,hldc8uf,t1_hlbfxnt,"I agree with hard to write, but disagree with there being no benefit.  In truth, we simply don't know if there are benefits.  Nobody has written a (non-esoteric) language that forces its use, or even encourages it.

For all we know, it could be the easiest way to create sentient AI, time machines, and replicators.

All that said, it's true there is no _known_ benefit.

As for why to bother?  Perhaps to be the first to make it easier and mainstream.",3,0,0,False,False,False,1637402480.0
qxrdbd,hldehja,t1_hlbi71d,"Yeah, I've written a fair amount of self modifying code, but I work in a highly constrained environment where every single bit counts. If I know that a pile of instructions will never be called again (like initialisation code) well, that's just free real estate.

Edit: even then the best example I can think of was to add a bit of debug code so I could follow the code path. It's not very often I've shipped it in production",5,0,0,False,False,False,1637404464.0
qxrdbd,hlbreoq,t1_hlbi71d,[deleted],2,1,0,False,False,False,1637367610.0
qxrdbd,hlcwjz8,t1_hlcprj9,"mathematica and wolfram lang deserve more love (and to become open source so that it finally starts gaining contributors and traction)

and yeah, people cite lisp bc it was the first one, and the one where it's actually not weird to do this. functional languages do this all the time. it's fun.",3,0,0,False,False,False,1637389430.0
qxrdbd,hlfenrt,t1_hlcprj9,This is an extremely good answer.,2,0,0,False,False,False,1637441113.0
qxrdbd,hlctf73,t1_hlbrvc7,"To extend that, VMs like Java Virtual Machine (JVM) that have Just in time compilation (JIT) enabled, are technically self modifying.",4,0,0,False,False,False,1637387262.0
qxrdbd,hlcfif1,t1_hlc3d6n,Getting downvoted by COBOL programmers.,3,0,0,False,False,False,1637379148.0
qxrdbd,hlcwetp,t1_hlcnsg0,"Skynet runs on lisp then, we've had meta-programming in Lisp since when it was implemented in 1959. 

Crazy stuff.",3,0,0,False,False,False,1637389328.0
qxrdbd,hlehc01,t1_hlbiq7f,You've reminded me of this interesting video from Computerphile (Dr Julian Onions) https://youtu.be/SWU_DgjSwRU,2,0,0,False,False,False,1637426829.0
qxrdbd,hldf63u,t1_hlbiq7f,"Isn't this ""trick"" quite well known for implementing memorization? I don't think it's that bad.

Of course, I still won't use it in production code - in Python, the decorator `functools.cached` implements memoization, and I'm sure there's something similar in JavaScript also.",1,0,0,False,False,False,1637405038.0
qxrdbd,hlci6ou,t1_hlchmh1,"Well, yes, there is value in doing things ""because they are hard"". Unfortunately, that value is not recognized by companies expecting production-ready code, and self-modifying code is hard to reason about or formally verify in general.  


I originally hoped to add some qualification to my answer by linking an ""self-modifying code competition"" or something like that, but that does not seem to exist. The closest thing is the IOCCC, and that is not really about self-modifying code since your code must be cross-platform (I believe).",13,0,0,False,False,False,1637380590.0
qxrdbd,hlcanv5,t1_hlbreoq,"This is not true, you can't write in .text memory mappings in any modern OS. Those are mapped read-execute, without the write permission. You'd need to go out of your way to do it (and some security mechanisms sometimes prevent you from having a memory mapping write-execute, known as W\^X).",15,0,0,False,False,False,1637376620.0
qxrdbd,hlbuj3o,t1_hlbreoq,"That makes sense.  Too much time in low level environments for me, appreciate the confirmation 👍",4,0,0,False,False,False,1637369016.0
qxrdbd,hlczipk,t1_hlbreoq,Who told you this?,1,0,0,False,False,False,1637391648.0
qxrdbd,hlcieii,t1_hlcfif1,"I don't understand it ,was something wrong with my answer ?",2,0,0,False,False,False,1637380706.0
qxrdbd,hld7vff,t1_hlcwetp,"So what you're saying, is we're absolutely safe from human destroying robots?  Because this is the internet, so I'm going to believe you, and use this conversation as source material down the road.",2,0,0,False,False,False,1637398591.0
qxrdbd,hldgrqh,t1_hldf63u,"you mean \`lru\_cache\`? Also, the above code isn't for memoization. At least, there isn't anything about it nearly similar to what I've seen and done with memoization.",1,0,0,False,False,False,1637406390.0
qxrdbd,hlciw37,t1_hlci6ou,"I find it a lot easier to write self modifying code actually. One's mileage may vary. Anyway, I said ""fun, interesting, and emergent"" which has nothing to do with difficulty. There's few things more interesting than watching some self modifying code do something elegant, emergent, and unexpected. I write silly procedural games, not production ready code (for now), so there's neither harm nor foul in it. Academically it must surely have value for people really good at it, taboos notwithstanding. It's the emergence that is fascinating.

In my opinion, making code that is fully referentially transparent and clean is way harder. Just my amateur opinion.

Edit to clarify: nothing against clean code. It's important to be able to do. Just saying I find it harder.",2,0,0,False,False,False,1637380977.0
qxrdbd,hlcfx3i,t1_hlcanv5,"This is why we have a security team at work, they watch out for us 💯 and audit published code.  So much to keep track of.",2,0,0,False,False,False,1637379362.0
qxrdbd,hlclxmm,t1_hlcanv5,Definitely. And RWX sections in memory are a huuuge giveaway that something fishy is going on!,2,0,0,False,False,False,1637382681.0
qxrdbd,hlcivap,t1_hlcieii,"I don’t think so. I think it’s a wonderful answer. We’ve literally had this since the late 50s, and people shied away from it. So weird. A tool is a tool",3,0,0,False,False,False,1637380964.0
qxrdbd,hldbcgm,t1_hld7vff,At least we didn’t get killed by Java Skynet.,1,0,0,False,False,False,1637401691.0
qxrdbd,hleq3ms,t1_hldgrqh,"`cache` is a new decorator (in Python 3.9), which is basically equivalent to `lru_cache(maxsize=None)`. See the `functools` docs: https://docs.python.org/3/library/functools.html

In Python, we can do something similar to the JS code like this:


    class A:
        @property
        def x(self):
            ans = expensive_computation(self)
            del self.x
            return self.x = ans


Of course, this is not a one-liner, but I'm sure a one-liner is also possible 🙂

This code does memoization / lazy loading of some expensive computation. Of course, we don't need to do this manually - the better version is:

    class A:
       @functools.cached_property
       def x(self):
           return expensive_computation(self)

**Edit:** for some reason, the Python cide is not being formatted correctly.",1,0,0,False,False,False,1637430492.0
qxrdbd,hlcrujw,t1_hlciw37,"More power to you, but be careful if you ever have to be responsible for drafting an RCA",4,0,0,False,False,False,1637386249.0
qxrdbd,hlcry1z,t1_hlcivap,"Now those downvoted seems to be gone ,weird !",2,0,0,False,False,False,1637386307.0
qxrdbd,hldlnrx,t1_hldbcgm,Because it couldn't grow past 3B devices,1,0,0,False,False,False,1637410275.0
qxrdbd,hlcse3c,t1_hlcrujw,"I'm not bragging. I am self taught so I got used to what pleased me and would need to train up new habits to be a professional. I value all kinds of coding. 

May I ask what an RCA is? I am not a professional. Just an eager amateur.",1,0,0,False,False,False,1637386589.0
qxrdbd,hlcwft6,t1_hlcry1z,I may have meta-programmed them.,2,0,0,False,False,False,1637389347.0
qxrdbd,hld0bs8,t1_hlcse3c,"Root Cause Analysis. Investigating a problem like a service outage, security vulnerability, equipment malfunction, etc.",7,0,0,False,False,False,1637392283.0
qxrdbd,hle0ue4,t1_hlcse3c,"Yeah, Generally speaking, most development regards building production systems which need to be explainable when things go wrong. We’re already in the midst of a crisis in Machine Learning explainability; 

Self modifying code would be a development nightmare if it causes any problems for a project using that code.

If I were in charge of a build system, I’d scan for self modifying code in its modules and auto fail the build, and tell the devs to use different modules",2,0,0,False,False,False,1637419411.0
qxrdbd,hlezimv,t1_hld0bs8,Many thanks!,1,0,0,False,False,False,1637434415.0
qxrdbd,hlf0596,t1_hle0ue4,"My understanding of machine learning is lean at best, but is it even possible to do something like a neural net without some degree of emergence which is impossible to explain fully? That seems to be one of the reasons it works at all. Not all code can be exactly the sum of its parts. That field is very attractive to me but I'm not quite good enough to get into machine learning as deeply as I would like, yet. Can you explain this crisis in explainability to me, or point me to an article?",1,0,0,False,False,False,1637434689.0
qxrdbd,hlgef69,t1_hlf0596,"I like to outline this crisis as such

Let’s say you build a bridge and after a year or so, it fails, killing and injuring a large number of people. The city needs three things: the families to be compensated, the bridge to be repaired, and justice to be served.

So you get sued, and the city does an investigation to determine what went wrong and why so they can fix it.

If the bridge is built by standard practices, and complying with regulations, they might find that their was a design flaw where something like temperature variations in the particular metals used caused joints to loosen and over time, caused structural failure. They’d be able to determine this because they have records of both the design specs and construction records.

But this analysis might not possible if* the bridge were designed by machine learning because the specs could be too novel to perform a sure analysis.

Result: unclear how to repair the bridge, and now you’re in jail for using designs that weren’t repairable

Case and point: explainability == accountability. If it can’t be explained, then its unaccountable and that is untenable when people’s lives are involved",2,0,0,False,False,False,1637457614.0
qxrdbd,hlggzyq,t1_hlgef69,"That explains exactly why too much unaccountable emergence could be bad in production. Is there a way to make neural nets without it, though? Is it even possible to log every last step in the process? I would like to know more about the math behind machine learning and maybe make my own little neural net some day but I'm not there yet. What research I've done suggests it would be impossible or highly burdensome to make them fully explainable. For systems used in the real world which could have consequences on peoples' lives or legal standing, however, that burden seems justified if there is a way to achieve that emergence while logging everything. But then maybe you wind up needing another neural net just to parse the data!

Edit to clarify position: Just because it's hard doesn't mean it's impossible, and the benefits outweigh the risks. I would rather live in a world where we take a chance on these things.",2,0,0,False,False,False,1637458867.0
qy34w1,hldmyvd,t3_qy34w1,"Viruses don't work by magic. They are programs like any others and just do nefarious things. They don't magically start executing, something has to explicitly start them. Usually this happens without you wanting to, e.g. when opening an infected file, or a malicious website etc.

However a virus executable can just be stored on your hard drive without any issues. Security researchers do it all the time. The key is to never execute it.

So if you have an encrypted virus somewhere, you're safe since you can not execute it, because the file is encrypted. But the encryption really does not matter.

Of course, once you execute the virus, the fact that they were encrypted at some point in time becomes irrelevant.",19,0,1,False,False,False,1637411242.0
qy34w1,hldug6g,t3_qy34w1,"Encryption is not a container. It is a function you apply to the data. Example: If you apply + to the numbers 2 and 3 it is not like you are putting the 2 and the 3 into a box with a + on top of it, instead you are generating a new number (5). Like with encryption you can not reverse the function only knowing that the new data is 5, you need addition information. This additional information is the encryption key. If the 2 was your virus it is no longer there.

If a folder is encrypted you actually have to concatinate all the files in it to generate a single file and encrypt this, or encrypt every file seperate.

To answer your question. An encrypted virus is just meaningless data. All data in an encrypted container is encrypted.",11,0,0,False,False,False,1637416009.0
qy34w1,hldn27o,t3_qy34w1,"Even unencrypted, a virus file just sitting on your hard drive can't do anything until something opens it. It's just zeroes and ones waiting to be executed, not a living creature which runs by itself.",6,0,0,False,False,False,1637411307.0
qy34w1,hle9r5s,t3_qy34w1,"If a virus is encrypted it cannot be executed. It needs to be decrypted first. Similar to how you cannot start software that has been compressed (compression is a type of encryption), you must decompress first. Of course there could be additional malicious software that decrypts the virus. Viruses are just software that do things you don't want.",3,0,0,False,False,False,1637423526.0
qy34w1,hldfkui,t3_qy34w1,If there’s a virus in your computer it all goes out the window,-2,1,0,False,False,False,1637405388.0
qy34w1,hlg2xel,t3_qy34w1,Why is OP replying only to trolls and not any of the serious answers?,0,0,0,False,False,False,1637452161.0
qy34w1,hlluvii,t1_hldmyvd,I thought there were viruses that could do things themselves and that Trojans were the ones that had to be executed by the user? Of course I may be wrong but this is why I thought virtual machines were a thing. I've heard of viruses that can duplicate and spread all over a system. Do they all need to be activated by the user?,0,0,0,False,False,False,1637559169.0
qy34w1,hlluwlr,t1_hldug6g,As I suspected. Thanks for the answer :),0,0,0,False,False,False,1637559188.0
qy34w1,hldfo5u,t1_hldfkui,I never said there was. I just want to know if encryption works both ways.,3,0,0,False,False,False,1637405466.0
qy34w1,hllv50f,t1_hlg2xel,"Just from that comment alone I'm suspecting you yourself are a troll. The ""trolls"" commented first hence why I replied to them first. Then I got busy as other answers started rolling in, now I'm back again and responding to others.",1,0,0,False,False,False,1637559332.0
qy34w1,hloecrs,t1_hlluvii,"No. Computers are not sentient beings. Everything happens because it is explicitly caused by some other thing.  


Viruses typically exploit security issues in programs to trick those programs into executing (e.g. sometimes E-Mail programs automatically attempt to download attachments, trip up and then execute these attachments because someone fucked up while programming them).  


Viruses also don't ""duplicate all over the system"". That's not how any of this works. A virus is a computer program. Its primary goal is to get executed, so that it can do its malicious activitiy. A virus especially wants to get executed after a restart, or when the user deletes the original file. Hence, the virus copies itself to a few new folders, and registers these copies of itself as programs to be run when booting the system.

Trojans typically exploit the user by masquerading as a useful program. They often even perform a normal function besides their malicios activity so that the user does not become suspicious.

&#x200B;

Besides, these denominations are all fuzzy in practice. The names also are. Just because it is called ""Virus"" does not mean that the analogy holds for everything. You would not expect to be able to vaccinate computers, would you?",1,0,0,False,False,False,1637610550.0
qy34w1,hldfu9q,t1_hldfo5u,Not when you have a virus.,-3,0,0,False,False,False,1637405608.0
qy34w1,hldfzbl,t1_hldfu9q,"Ok, if I'm the one who created a virus, but then I put it into an encrypted container and sent it to someone and they cannot decrypt the container, could the virus be a threat to them?",2,0,0,False,False,False,1637405727.0
qy34w1,hldioqa,t1_hldfzbl,"If you mean an encrypted zip i can't see how it would ""escape""",2,0,0,False,False,False,1637407952.0
qy34w1,hldgcr0,t1_hldfzbl,Yes very.,-4,0,0,False,False,False,1637406044.0
qy34w1,hldivsu,t1_hldioqa,"Or a Veracrypt container. Anything with proper encryption (password protection doesn't always mean encryption eg windows lock screen isn't encrypted, bitlocker is)",1,0,0,False,False,False,1637408107.0
qy34w1,hldghrw,t1_hldgcr0,May I ask how though? Encryption as far as I'm aware turns all of the data into a hot mess until it's decrypted. How can anything function in such a state?,0,0,0,False,False,False,1637406161.0
qy34w1,hle7og3,t1_hldivsu,"u/JoJoModding's comment pretty much says it all.

A virus (or any kind of malware) is made of program code.

What separates viruses or malware from any other programs is that their program code 1) attempts to do something malicious, and 2) malware usually tries to include itself in your computer's startup programs or other startup routines so that their code is automatically executed whenever you start your computer.

Malware might also have some other kinds of tricks to make itself harder to detect, for example.

Fundamentally, though, malware is just program code. Program code only does something when it's being run, one way or another; it doesn't do anything by sitting on the disk. In fact, a better way of looking at it might be not to consider the program code as doing something (malicious or otherwise) by itself; rather, when you run a program, the operating system and the computer's CPU take the program code they find in the program file and start doing what the program code says.

If the code on the storage device is encrypted, it's not a question of whether it can ""escape"" from somewhere. The program code that has been encrypted does not make sense as program code, and it doesn't look like program code to the operating system or to the computer: it cannot be executed in the first place, so it cannot be doing anything, inside or outside of that folder. It's just inert bytes.

In that sense it doesn't necessarily make sense to say that encryption ""works both ways"", as it's not as if the virus is alive but being contained within the encrypted container somehow and restricted from accessing the rest of the system by the encryption.

You're correct that if malware code has been encrypted, and it's not being decrypted (or the password simply isn't available), the malware can't possibly be doing something. However, that's not really ""the other way around"": it's exactly the same way that encryption works for any other files. MS Office cannot open an encrypted Word document that hasn't been decrypted; the CPU cannot execute program code that has been encrypted without it being decrypted first.",3,0,0,False,False,False,1637422621.0
qy34w1,hle1wl5,t1_hldivsu,"Yeah, a zip file is encrypted with AES thou :)  
If the computer can't access the virus code how can it run it XD",2,0,0,False,False,False,1637419922.0
qxixkk,hl9pvgr,t3_qxixkk,"Understanding functional programming is important in common languages as well. 

When you pass something by reference to a function, does it modify that object? Is it clear that that object is modified? That's a common source of bugs in C# or Node. 

Additionally, modern frontend frameworks (Redux, rxjs, etc) can be very functional programming. You have a state. That state can't be modified, so everything computing the new state is functional (ish). 

It's not something that's used as a whole all the time, but the concept of ""make it so that nothing is changed behind the scenes without making it very very clear"" is very useful in a lot of situations.",50,0,0,False,False,False,1637338568.0
qxixkk,hlb1zcp,t3_qxixkk,"From a high level perspective, functional languages are nice because they are very deterministic. Immutable data structures and very strict typing allows for a clean and predictable program.",9,0,0,False,False,False,1637356943.0
qxixkk,hlab158,t3_qxixkk,"Functional languages aren't a good fit for everything, but they tend to excel whenever there's a complex domain model and/or a pipeline-like solution to the problem at hand. I wouldn't write a game in Haskell, just like I wouldn't write a compiler in C++. Vice versa, however, would work great.",14,0,0,False,False,False,1637346810.0
qxixkk,hlbmqj9,t3_qxixkk,"It is easier to analyze mathematically (in terms of runtime, correctness, and so on) and some people see it as ""pure"" computer science.  

I remember one of the core Haskell developers as saying, Java has started from a powerful but unsafe place.  Haskell has started from a safe but powerless place.  As time has gone on, Haskell has become increasingly powerful while maintaining safety, and Java has become increasingly safe while maintaining its power.  Both are converging to best-of-both worlds from both directions.  From that perspective if you were more concerned with safety than power, you might prefer a functional and strongly typed language.  

I've heard about recent developments in OCaml about using it (I think through Merlin) to utterly get rid of the layer between it an C.  If this is very successful it offers the promise of a simpler process to deliver an executable, but this has not yet been fully fleshed out.  There are promises about when and if this will happen but ... you know.  ""Promises""

Ultimately, I can't fully understand certain people's love of functional programming other than it being a religion.  I'm much more omnivorous.  I like OO, I like functional, I'll do whatever the fuck I want.",13,0,0,False,False,False,1637365567.0
qxixkk,hl9n7yq,t3_qxixkk,"Here’s a nice little discussion on the topic:

https://stackoverflow.com/questions/36504/why-functional-languages",10,0,0,False,False,False,1637337501.0
qxixkk,hla9k98,t3_qxixkk,">The languages that FP is generally used in are annoying to write software in, as well.

Chances are that you think so because you've mainly used imperative / OO languages. I'd argue that writing Java or C# is *way* more ""annoying"" than Haskell or F#.

>but I don’t understand how it could work for more complex systems

Just try it",19,0,0,False,False,False,1637346263.0
qxixkk,hl9pyvt,t3_qxixkk,Because the grass is always greener on the other side,31,0,0,False,False,False,1637338605.0
qxixkk,hl9zg5o,t3_qxixkk,"For the right situation fp can be really great. I work in data science, and we use languages like haskell or R to do transforms quite a bit. When you care a lot about being to reproduce results  and be aware of all changes happening to data fp is a nice paradigm to be in.

I've heard they can be used for scalable software engineering but I don't know much about that.

&#x200B;

I imagine for a lot of people, solving problems in a functional way is very satisfying and that will bias their opinion a bit even if it not always the most practical aproach.",8,0,0,False,False,False,1637342326.0
qxixkk,hlaz080,t3_qxixkk,SICP does a great job explaining this.,8,0,0,False,False,False,1637355750.0
qxixkk,hlax6ob,t3_qxixkk,"Functional programming is awesome in every regard, including ease of writing and learning.

The problem is you're (likely) coming at it from an Object Oriented viewpoint, and seeing the more complex aspects of Functional programing. It's like a procedural programmer seeing object oriented for the first time. 

To put it simply, it's a known fact that any task you can do in programming, you can do in Procedural, Object Oriented, and Functional.

The biggest way I can highlight the advantage of Functional is this: If you want to break down working code into its most indivisible units that still work, what you're going to be left with varies on which it is. Procedural will likely leave you the whole program. Object-Oriented will frequently leave you monolithic Objects with large supporting libraries. Functional will leave you with... one line. 

And that makes all the world of difference. If you're hopping into some new code, Procedural requires you understand the entire program.   
Object Oriented requires you understand the object, the connected libraries, the state, the variables, the ways output can vary from the object, etc.   
Functional requires you to understand one line. 

Further, Functional is insanely more reusable than Object Oriented since it's so atomic. Anything you've done before, you can treat like a buffet table, grabbing what you need. 

After you've grasped it, coding in functional is faster, cleaner, more effecient, easier to pick up, snappier, and all around more fun to write.",5,0,0,False,False,False,1637355044.0
qxixkk,hlb3cog,t3_qxixkk,"Well, what complex systems have you tried writing in a functional language?

But you have an important point, and as much as I like functional programming, I have to concede it's frequently pretty horrible to have to debug functional code. It's also a lot less straight forward to read and comprehend than some other styles, which matters a lot for maintainability. In most real world scenarios, I think functional programming should be used to complement a more direct, imperative/OO programming style.",5,0,0,False,False,False,1637357491.0
qxixkk,hlazx2a,t3_qxixkk,"I tend to find most people exist within a sub-domain in programming, of which there might be some overlap and they decide that whatever works best for them, must be the one true way.",2,0,0,False,False,False,1637356111.0
qxixkk,hlciedy,t3_qxixkk,"I learn JavaScript as my first programming language and as I was introduced to OOP in JavaScript by MDN and FP in JavaScript by [Academind](https://www.youtube.com/watch?v=aoE-92Ac4zE). I also learnt Java which has one of the best OOP styles.

Personally, I like OOP more be it the standard OOP or prototypal OOP. In JavaScript, I had to write a lot of functions to accomplish a task and considering that JavaScript recommends using camelCase and my problem of failing to think of nice function names, it was difficult. 

When I tried to accomplish the same task in OOP js, it was a lot easier for me primary because code can be organized into objects and nested objects which makes it very easy to name and remember the method names like `bank.depositCash` rather than its FP counterpart.

Now, I mainly use Golang and Rust as my daily drivers so I like FP and you can really use FP in complex systems. I like FP in Golang and Rust primarily because of their module systems, like in Golang, packages contain modules and modules contain functions which makes it really easy to import and use them without any hassle.",2,0,0,False,False,False,1637380705.0
qxixkk,hlatamz,t3_qxixkk,"They're caught up in [The Churn.](http://blog.cleancoder.com/uncle-bob/2016/07/27/TheChurn.html)

----

Functional programming is older than Object-Oriented Programming, yet people are rediscovering it like it's something brand new. Like ""protocol-oriented programming"" or the other ""programming"" styles floating around that are supposed to be better than sliced bread.

Everything is a tool. Some tools are more useful for some tasks than other tools. You wouldn't drive a screw with a hammer, nor would you pound in a nail with a screwdriver.

And sometimes you use one tool wrapped in another because it solves the problem better. For example, I will routinely (in Java) wrap functional programming in singleton objects and anonymous classes declared from protocols; basically I'm using the class and interfaces as a sort of namespace mechanism to isolate internal state of a collection of functions.

But they're just tools, and a good craftsman learns how to use all the tools in the toolkit, and when it's appropriate to use them.",6,0,0,False,False,False,1637353553.0
qxixkk,hlaznvz,t3_qxixkk,"No state, which is the source of many problems, keeping track of state and communicating mutations of state across the system.",1,0,0,False,False,False,1637356009.0
qxixkk,hla7i86,t3_qxixkk,Lambda calculus is also Turing-complete,-3,1,0,False,False,False,1637345473.0
qxixkk,hlaku88,t3_qxixkk,"I don't ""know"" the answer, but I speculate it might be similar to why mathematicians enjoy solving problems. It's kind of like a fine art. When you have it mastered and you can accomplish difficult tasks in a fascinating way, it's really rewarding.",0,0,0,False,False,False,1637350359.0
qxixkk,hlbguj3,t3_qxixkk,"It is true that FP is impractical from the perspective of complex systems, however, what is fascinating about FP is never the engineering applications, but the clarity of understanding that it brings. FP is lesson 101 in programming language theory. We learn to replace design patterns with higher-order functions, to eliminate illegal states with ADT, to improve program composition with abstraction, to prove the correctness of programs with invariant. These things are important enough on their own, furthermore, they also open the door to further study of PL theory.",0,0,0,False,False,False,1637363062.0
qxixkk,hlarf39,t3_qxixkk,"Functional programming can be super useful for certain types of software. For example, we used the functional language OCaml in my compiler construction course in college. I couldn’t imagine how much harder that would have been in an imperative language. The recursive nature of a functional language was quite useful in that context.",1,0,0,False,False,False,1637352839.0
qxixkk,hlb6pox,t3_qxixkk,"All I can say is I enjoy it a lot, I get a greater sense of satisfaction solving problems in FP, it’s made my work in imperative code bases better and the most complex things I’ve ever built have been in Haskell.

Edit to add: FP isn’t a new shiny thing for me though, I first learned it about 20 years ago.",1,0,0,False,False,False,1637358847.0
qxixkk,hlcf46h,t3_qxixkk,"FPP is a toolkit, a set of tools, not a singular tool.  It's easy to overlook this, because FPP programing languages can be limiting at first and seemingly unusually restrictive.

Because FPP is a set of tools, many modern mainstream languages have taken pieces from FPP.  If you're using a popular mainstream programming language today, you're using bits and pieces of FPP already without knowing it.

>Why do some people like it so much and act like it’s the greatest?

Some people like some of the tools FPP provides.  It's ideal to ask them what parts they like to get a better idea.  Odds are the parts they like you've already used and might be able to relate with them, if they don't use convoluted terminology to explain themselves so they are easy to understand.",1,0,0,False,False,False,1637378939.0
qxixkk,hlg8eas,t3_qxixkk,"1. The conceptual simplicity of pure functions allows compilers to perform some elaborate optimizations they otherwise couldn't. E.g. [Elm is blazing-fast and builds small bundles](https://www.freecodecamp.org/news/a-realworld-comparison-of-front-end-frameworks-with-benchmarks-2019-update-4be0d3c78075/), and Haskell performs favourably in benchmarks relative to other high-level languages, sometimes approaching the speed of e.g. C. (YMMV, sufficiently complex Haskell applications tend to have hard-to-debug space leaks owing to ""laziness"", though the community has been moving towards more strictness in I/O, and laziness is not a feature of all functional languages. The community has also been building more understanding of how to fix these issues in recent years.)
2. Fearless refactoring. When people talk about ""functional programming"" these days, they often implicitly include a static ML-family type system. These expressive type systems allow you to encode a lot of information, preventing nonsensical states from being represented in the language. E.g., it's generally impossible to represent an out-of-bounds enum member in Haskell, unlike in C. When adding a new variant of a data type, the compiler will enforce that you handle it in all of your switch statements. Also, impure functions are carefully restricted into the IO type and every type ""inheriting"" from it (i.e. every instance of the MonadIO class). That means that when you're refactoring non-IO code, you can be sure that you haven't introduced an error merely by changing the sequence it's written in.
3. Effective abstraction encouraging uniform interfaces. Historically, software engineering has taken a very unscientific approach to design patterns. But many of them are patterns that are studied by computer scientists and mathematicians. There are arbitrary API differences in many languages between strings, arrays and vectors, but Haskell (specifically via its ML-style type system) allows a high degree of polymorphism for functions acting on these types. Now that I know the common interfaces (the most common are functor, applicative, monad, monoid and traversable), I find myself being able to pick up new libraries very quickly without having to learn their specific, ad-hoc interfaces.",1,0,0,False,False,False,1637454716.0
qxixkk,hlb6cq5,t3_qxixkk,"I like this article to explain some things https://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html

But in a nutshell it makes code easier to reason about, easier to compose and refactor, and less prone to bugs.",0,0,0,False,False,False,1637358700.0
qxixkk,hlc2c6q,t3_qxixkk,"Well it's just plain fun ,not everything has to be 100% practical . If you like pythonic code fp in Haskell at least is like that on steroids . I like it specially for doing problems on code wars",0,0,0,False,False,False,1637372584.0
qxixkk,hla4wwm,t1_hl9pvgr,"But you can write immutable classes in any OO language. It is a very fundamental concept called Value Object in DDD, which is like 20 years old.",3,1,0,False,False,False,1637344451.0
qxixkk,hlaovup,t1_hl9pvgr,I guess that makes sense. I mean even in an OOP language you’d want to minimize side effects. Writing clean and concise code seems to follow some FP principles. Although I still don’t understand how FP languages would work in a complicated engineering systems that needs to do a lot of different things,-1,0,0,False,False,True,1637351877.0
qxixkk,hldomnf,t1_hlab158,This,2,0,0,False,False,False,1637412401.0
qxixkk,hlbnf69,t1_hlbmqj9,Hahaha that’s fair. Honestly I feel like I usually use OOP at a higher level but then within the class I try to adhere to some FP principles when writing classes,3,0,0,False,False,True,1637365859.0
qxixkk,hlbeekv,t1_hla9k98,Every single large system I've ever heard of being written in a functional programming language since event-driven programming became the mainstream has regretted it. The paradigms don't mesh well.,9,0,0,False,False,False,1637362055.0
qxixkk,hlbfu9x,t1_hla9k98,"Yeah trying to follow the state of a mutable object as it's passed around a graph of classes and method calls is annoying. Trying to retrofit error handling in a codebase fill with `throw`, trying to reason about which functions throw exceptions and when is maddening.

Good thing is imperative language designers are more and more understanding the value of FP and adding features. It took Java 19 years to get closures, but hey, progress is progress",2,0,0,False,False,False,1637362647.0
qxixkk,hlcfsgg,t1_hl9zg5o,"You use Haskell?  [intriguing](https://c.tenor.com/CSvXOimoG5kAAAAC/spock-eyebrow.gif)  Does Haskell have dataframes?

(For anyone who is curious, scientific programming has two roots, research and AI which does in fact has a heavy FPP and [literate programming paradigm](https://en.wikipedia.org/wiki/Literate_programming) history, though more LISP than Haskell, and super computer programming in FORTRAN.)",1,0,0,False,False,False,1637379295.0
qxixkk,hlb2rhb,t1_hlaz080,Go back to /g/,-7,0,0,False,True,False,1637357256.0
qxixkk,hlbn851,t1_hlax6ob,"> Functional requires you to understand one line.

Which one line is that?",5,0,0,False,False,False,1637365776.0
qxixkk,hlay0wj,t1_hlatamz,"Okay that’s similar to what I do, I think. Like OOP at the high level, but utilizing some FP principles inside of the classes",1,0,0,False,False,True,1637355370.0
qxixkk,hlb14ct,t1_hlaznvz,Hmm interesting. But don’t many problems in software involve changing a state?,2,0,0,False,False,True,1637356596.0
qxixkk,hlacsz3,t1_hla7i86,Yes..? Why is that worth mentioning?,6,0,0,False,False,False,1637347453.0
qxixkk,hlamh5p,t1_hlaku88,Yeah that makes sense to me. Maybe it’s like mathematicians/physicists look at a problem differently than an engineer would because they apply different principles when solving problems,0,0,0,False,False,True,1637350977.0
qxixkk,hlbmis6,t1_hlbguj3,"Yeah that has been my understanding of it as well. It is good for a lot of theoretical stuff and proving things out, but can be difficult to implement in the real world",1,0,0,False,False,True,1637365474.0
qxixkk,hllsj3e,t1_hlcf46h,What does FPP stand for?,1,0,0,False,False,False,1637557725.0
qxixkk,hlh1kvj,t1_hlg8eas,"Also, /u/WiggWamm, when a program is expressed in pure functions, it's trivial to parallelize it. Most of the speed increases in CPUs in recent years have happened via increasing core count instead of increasing clock speed or register width. Therefore, speedups in program execution time are mostly going to come from improved parallelism and concurrency. Parallelism and concurrency with conventional programming languages is difficult to get right, and takes lots of time. But the fact Haskell has parallelism API that is so simple is an enormous advantage.",2,0,0,False,False,False,1637469790.0
qxixkk,hlacgke,t1_hla4wwm,"Its not about immutability really. People tend to get caught up in that, but the biggest selling point of fp is composability. Functions as truly first-class values is a very powerful feature that allows simple and  robust code reusue. Mutability doesn't compose well, so it's discouraged (or disallowed) in fp. Combine first class functions with rich static typing and expression based control flow, and you've got yourself a language that is phenomenal for writing any deterministic or highly formal software, e.g. compilers, financial systems, theorem proving, verification, AI, etc.",39,0,0,False,False,False,1637347330.0
qxixkk,hlb3yo1,t1_hla4wwm,"> But you can write immutable classes in any OO language.

And that gives you some of the benefits of programming in a functional style. In the same way, you can write functions that act as object methods in C, and that gives you some of the benefits of OO programming.",8,0,0,False,False,False,1637357738.0
qxixkk,hlaqfdk,t1_hla4wwm,"FP is over 60 years old, and the math behind it over 80. I don't know the history of  the value object idea, but in general, FP has been informing other paradigms for a long time. So the ability to easily implement FP ideas in not-primarily-functional languages is often thanks to FP.",10,0,0,False,False,False,1637352461.0
qxixkk,hlarb7v,t1_hla4wwm,I agree with u/raedr7n and want to add that sometimes you can't make the whole thing 100% immutable (eg what if a deep copy takes HUGE amounts of time?). It doesn't mean that functional programming isn't useful. Making things composable (aka reusable) and limiting side effects is always a good programming practice.,2,0,0,False,False,False,1637352798.0
qxixkk,hlbhsf1,t1_hlaovup,"Depending on what the specifics of that complicated engineering system entail, there are FP patterns like monads and sum types that can capture and handle a lot of complexity.",3,0,0,False,False,False,1637363453.0
qxixkk,hlbh5dq,t1_hlbeekv,"Can you name some of those systems? And funnily enough even driven programming works \*very\* well with a functional style (see for example Scott Wlaschin's ""Domain Modeling made functional"").  


Certainly there are a few failed projects but the same is true for (for example) OOP and there's also very succesful projects / companies that use FP succesfully for large scale projects (e.g. the FPGA design system and hardware synthesizer of QBay (and compilers in general), Facebooks spam detection, (sadly) a bunch of crypto shit, Hasura's whole platform, ... or just look at Erlang's (German - the English one doesn't list all the companies and projects) wikipedia article to see how insanely good FP can work)",5,0,0,False,False,False,1637363186.0
qxixkk,hlh1ukj,t1_hlcfsgg,"Haskell doesn't have dataframes natively, but there are libraries implementing it, such as:

[https://hackage.haskell.org/package/Frames](https://hackage.haskell.org/package/Frames)

(I have done no work to evaluate if this library is any good.)

But I suspect dataframes are one of those things that will only ever be productively useful in a dynamically typed language like R or Python.",1,0,0,False,False,False,1637469951.0
qxixkk,hleebcf,t1_hlbn851,The one you want to change.,1,0,0,False,False,False,1637425536.0
qxixkk,hlb1u5e,t1_hlay0wj,"Well, Java kinda thinks everything is an object. And while Swift has the notion of functions and function closures, it seems happiest if you organize things around objects as well. 

I also do a lot of mobile iOS and Android development, and OOP is the primary hammer used in making user interfaces. So there's that.",1,0,0,False,False,False,1637356885.0
qxixkk,hlb3h1t,t1_hlb14ct,"Functional programming forces the programmer to be explicit about where state lives in their program and how that state changes.

This can make things quite a bit easier to deal with than the worst case of OO programming, where state is spread all through a graph of objects, any of which can change arbitrarily at pretty much any time.",1,0,0,False,False,False,1637357541.0
qxixkk,hlc4t1v,t1_hlacsz3,"I suppose it highlights that the expressive power of FP languages (which build upon the lambda calculus) is equivalent to imperative ones, as both are Turing complete?",4,0,0,False,False,False,1637373747.0
qxixkk,hlb6l4y,t1_hlamh5p,"That’s a flawed analogy, because it implies that functional programmers aren’t doing engineering.",2,0,0,False,False,False,1637358796.0
qxixkk,hlm13jk,t1_hllsj3e,Functional programming paradigm.,1,0,0,False,False,False,1637563360.0
qxixkk,hlbn4ak,t1_hlbhsf1,"I guess where I get stuck is state changes. For example, let’s think about a cars computer. It probably needs to know what gear you are in, which is a state. It probably wants to know if you are breaking or accelerating, which  a state can keep track of. Same goes for using your turn signal, headlights, etc. 

How do you get around not using the state for these objects? I get that for math functions, FP is very good at solving them and maybe even can eliminate some complexity. Maybe people using python are using FP without even realizing it. But how do you ignore state for every case?",2,0,0,False,False,True,1637365730.0
qxixkk,hlbl15w,t1_hlbh5dq,Reddit and Yahoo are the classic examples.,4,0,0,False,False,False,1637364829.0
qxixkk,hlbm9hk,t1_hlb6l4y,"Yeah I didn’t mean to imply that. I guess I was just trying to say that engineering principles seem to be focus on modularity and breaking big systems down into smaller subsystems that operate without know what the other systems are doing (kind of like how your car’s engine doesn’t care if your headlight is broken. They operate somewhat independently and the whole system is modular). OOP fits that idea pretty well for the most part, as long as there isn’t too much encapsulation/inheritance",1,0,0,False,False,True,1637365363.0
qxixkk,hlnefhj,t1_hlm13jk,Why not just call it FP?,0,0,0,False,False,False,1637596165.0
qxixkk,hlc484n,t1_hlbn4ak,"You'd probably have some type whose values are all states the car can take on, and then you can have functions that alter this state -- that is, they take in the old state and produce a new one. Such functions can be cascaded (composed) to create more complex functions that alter the state. Ultimately, you can simulate stateful systems with fundamentally stateless components (pure functions). 

If you mean to suggest something more like changing the state of something external to the program, something that will have some effect in the real world, then this is something that is addressed in purely functional programming. Altering external state is a behavior of impure functions (functions like print) which makes it less straightforward to model them as mathematical functions. Purely functional languages like Haskell, Clean, etc. have their own solutions to this problem.",3,0,0,False,False,False,1637373470.0
qxixkk,hlf8gu8,t1_hlbn4ak,"I work on a production Haskell codebase.
Basically, you do have state. Haskell allows you to allocate memory and pass/modify it by reference.

These actions have to occur in an expression with the type IO. But the top level entrypoint, `main`, has the type IO, and so there's no fundamental barrier to using state at the top level.

You use pure functions as the default, since the compiler can so aggressively optimize them with out-of-order execution and fusion. But for things that have to be stateful, you give them the type IO, which effectively just turns those optimizations off and makes the machine execute your function line by line like an imperative program.

There are more elaborate APIs people often wrap IO with, but that's the basic idea.

Edit: the other big reason to use pure functions, beyond compiler optimizations, is that you can eliminate while classes of runtime errors from ever occurring. Pure functions should never throw an exception or close a socket when it shouldn't have, etc.",1,0,0,False,False,False,1637438325.0
qxv7ln,hlcjs43,t3_qxv7ln,"Because to mirror a write means it's not complete until the last disk completes. So you are only ever going to get the performance of your slowest disk.

 Reading, on the other hand,  can be split up between disks. So if you wanted to read 2000 sectors of data, disk one might read sectors 0 - 999 while disk two is reading from sectors 1000-1999. Thus twice the speed.",6,0,0,False,False,False,1637381464.0
qxv7ln,hlcge8t,t3_qxv7ln,"Writing already happens ""in the background"" essentially, because writes are cached at the OS level and/or the disk controller level. What we call the ""write speed"" is how long it takes for the write operation to completely finish.",2,0,0,False,False,False,1637379616.0
qxv7ln,hlehuwr,t3_qxv7ln,Unless I am mistaken RAID 0 does what you are describing,2,0,0,False,False,False,1637427047.0
qxv7ln,hlcc56i,t3_qxv7ln,"This is a really interesting question. Would like to see what others have to say. I think with most modern RAID cards the cache handles this then writes to both and eliminates most performance issues with 2 drive RAID1. I don’t have real world experience with this since I never use RAID1, but… https://arstechnica.com/civis/viewtopic.php?t=1216469",1,0,0,False,False,False,1637377375.0
qxv7ln,hldwb7i,t3_qxv7ln,"A cache could handle this. But if the systems fails and the cache got corupted, you loose the redundancy, which was the reason to use a RAID in the first place.

Modern RAID systems like build into ZFS have various ways of handling reading and writing.",1,0,0,False,False,False,1637417052.0
qxv7ln,hlf8m8f,t3_qxv7ln,"Because if you call sync() you expect the data to be present REGARDLESS of what happens next (power outage, disk failure etc). What happens if your disk fails milliseconds after the call to write to the disk? How long are you willing to wait if the disk IO is 100%? 

You’re basically proposing your data to be written in RAID0 until the disk is free to repair itself. You can simply do that by enabling a writeback cache, that way your writes go to RAM until such time that the disk is available to finish the write.

There are various better ways of improving performance, sacrificing your data safety is generally the wrong proposal.",1,0,0,False,False,False,1637438394.0
qxv7ln,hlemrne,t1_hlehuwr,"Yes it does, but then you lose the redundancy (RAID 0 is striped). What I am asking about is this system I laid out where you achieve that performance, but still have the redundancy. Its such a simple solution that it either has been considered already and there are unknown reasons why it wouldnt work, or its so simple that it hasnt been done.",1,0,0,False,False,True,1637429091.0
qy32gm,hldq62c,t3_qy32gm,"I think you have the basic functionality of the routing table down. What I think you haven’t understood yet is that in networks there is a “routing protocol” which is responsible for figuring out what needs to be in the routing table for each router in order for it all to work. There are lots of different routing protocols from simple to complex (e.g. RIP, OSPF, BGP, etc). But they all are basically applications that run on each router, send packets of information to directly attached neighboring routers, and build up some kind of internal state about what the network “looks like” from which they decide what the routing table should be. If they do this correctly then the network works: packets get forwarded over the correct links to reach the correct destination. However it’s highly non-trivial to do correctly when things in the network start changing. Building such a routing protocol is a classic example of distributed systems.",1,0,0,False,False,False,1637413414.0
qxh10d,hl9bk0m,t3_qxh10d,"Listen, learning C has nothing to do with compiler design, first study compiler design, then try to relate the concept. And you can find on internet the compilation process of a C program.....that is pre processing, compilation , assembly , linking . U can study these steps in details",14,0,0,False,False,False,1637332567.0
qxh10d,hl9ga7q,t3_qxh10d,"I would not try and understand how the higher level abstractions translate to the lower C level. I would instead learn from first principles on how a computer works and build the abstractions up from there. You will learn how a CPU works. How the data bus and registers are used. How memory is laid out and accessed. The call stack and how that works, etc.. This will go a long way in understanding how C sits on top of this and how it's data structures like arrays and structs map to this and understanding how pointers work the way they do and why. Check out these resources:


1. Read [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. Watch [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
5. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
6. Grab a copy of [C programming: A Modern Approach](http://knking.com/books/c2/index.html) and use it as your main course on C.
7. Follow this [Tutorial On Pointers And Arrays In C](https://github.com/jflaherty/ptrtut13)

The first four really help by approaching C from a lower level of abstraction (actually the absolute lowest level and gradually adding layers of abstraction until you are at the C level which, by then is incredibly high!) You can do all four or pick one or two and dive deep. The 5th is a great introduction to computer science with a decent amount of C programming. The sixth is just the best tutorial on C. By far. The seventh is a deep dive into pointers and one of best tutorial on pointers and arrays out there (caveat, it's a little loose with the l-value/r-value definition for simplicity sake I believe.)",11,0,0,False,False,False,1637334643.0
qxh10d,hl9d8p9,t1_hl9bk0m,"Yes , but I want to study it to get more confident that what actually is happening behind the scenes. I tried to study it in this way , like when I am not able to relate any concept I got for searching over internet and its a lot more time taking. Thank you for responding!!!",2,0,0,False,False,True,1637333321.0
qxh10d,hlaalla,t1_hl9ga7q,Ty for this list,3,0,0,False,False,False,1637346649.0
qxh10d,hl9u2o5,t1_hl9ga7q,"Thanks, well that's a big list as I am a final year undergrad and also many topics, I may already have learned. I will study as much I can.",0,0,0,False,False,True,1637340231.0
qxh10d,hl9msnv,t1_hl9d8p9,"Yes, you are doing good work, whenever u in doubt feel free to search, there are multiple courses on YouTube, one I can recommend is neso academy, there others too, but I liked their way of teaching",3,0,0,False,False,False,1637337334.0
qxh10d,hl9qclz,t1_hl9d8p9,[deleted],1,0,0,False,False,False,1637338753.0
qxh10d,hlcwjcy,t1_hlaalla,I'm taking th CS50 class now. It's amazing. Well thought out lectures. Long but fun.,1,0,0,False,False,False,1637389418.0
qxh10d,hla3y60,t1_hl9msnv,Neso academy is fantastic.,1,0,0,False,False,False,1637344076.0
qxh10d,hl9tfr4,t1_hl9qclz,"I referred several books like Let us C , C in depth and also I read from my professor and internet. I also followed KN King for some topics but Ritchie is the book that I read almost complete.",2,0,0,False,False,True,1637339983.0
qxh10d,hl9trih,t1_hl9tfr4,[deleted],3,0,0,False,False,False,1637340112.0
qxh10d,hl9uo2g,t1_hl9trih,"Yeah I started revising C. I can understand most of the things but where I am lacking is , I am not able to related Compiler theory I read on my course to the programming. Like I am getting this or that error what might be the cause in term of compiler and many other things.",-1,0,0,False,False,True,1637340467.0
qxh10d,hl9uyhp,t1_hl9uo2g,[deleted],1,0,0,False,False,False,1637340579.0
qxh10d,hl9vr94,t1_hl9uyhp,"Haven't you read compiler design till now? My intention is when I read CD it was more theoretical and examples used form real programming were very much less. So I want to see that theory happening in real C programming. But I think no one studies in this way, I am the only one ( may be I am thinking wrong ) :), and no one gonna write a book or course just for me. Thanks for your consideration.",-1,0,0,False,False,True,1637340889.0
qx827n,hl7z8i8,t3_qx827n,Not an embedded dev but I'd guess it's probably some type of PROM or EEPROM or some such for the program memory and just SRAM for RAM,29,0,0,False,False,False,1637300282.0
qx827n,hl8d2us,t3_qx827n,"Go look at a Casio teardown. Although it's possible that these days it's all done on a single IC, in which case I imagine it's just some kind of RAM. It's not like it persists between power ons.",3,0,0,False,False,False,1637309848.0
qx827n,hl807k5,t1_hl7z8i8,Yup precisely. I think EPROM would be a better option for ROM though but yeah EEPROM and PROM works too,10,0,0,False,False,False,1637300839.0
qx827n,hl9o7gz,t1_hl807k5,"For a small calculator, the program memory is probably just masked into the chip, and then it uses SRAM cache. For a calculator with slightly more advanced features, it might have EEPROM or even some flash if it's a nice graphing calculator, AFAIK UV-EPROM has almost been entirely phased-out.",5,0,0,False,False,False,1637337900.0
qxl1o1,hla63sv,t3_qxl1o1,"DHTs are a building block to the larger whole. So, simply put, it depends on the system where the DHT is being used.

There is nothing about DHTs in and of themselves that declares how data is stored. It simply defines how keys map to values and how many different nodes can connect/disconnect without causing considerable disruption to the system.

In a real application, one might PUT some data into the DHT and the implementation thereof will decide where to store it and with what actual backend. When one wants to GET the data, the implementation will get it from any available node that has it.

Take BitTorrent as an example. The content is described by a hash, and clients look up the hash. The DHT responds not with the data itself but with IP/Port pairs that the client can connect to in a subsequent request to download the data.

Don't know if this helps or not. Basically, how and where the data is stored is not defined. That's up to the implementation. DHT simply defines the infrastructure of the table and how it can maintain high availability with many nodes.",2,0,0,False,False,False,1637344916.0
qxl1o1,hlaija2,t1_hla63sv,"Got it! Thank you so much, this cleared up a lot of things for me.",1,0,0,False,False,True,1637349514.0
qxl1o1,hlb4xeq,t1_hlaija2,Glad it could help!,2,0,0,False,False,False,1637358129.0
qx22or,hl8474x,t3_qx22or,"The midian of two sorted arrays in time complexity log(n) where n is the sum if lengths is not quite this simple im afraid. Rather then the coding part, thinking about the algorithm needed is harder.",2,0,0,False,False,False,1637303433.0
qx22or,hlsi6c9,t3_qx22or,"What you have come across is the fact that in maths, ""empathy is hard"". This, of course, does not mean that mathematicians are cold people, but that it is very hard to judge whether some problem is easy or hard, especially for someone who does not have the same skills as oneself.

This is because you often require an insight into why something is right, which is hard to come by, and even if you so, the road you went down to find it is likely unique to you.

Thus, the person who created this problem likely thought that this was easier than it actually is, for a variety of reasons.",2,0,0,False,False,False,1637688805.0
qx22or,hl9dafw,t3_qx22or,Number 4 is hard if do it log(m+n),1,0,0,False,False,False,1637333342.0
qwqv67,hl4rnyj,t3_qwqv67,"A concise way I like to look at it is a database does two things:

1. Stores data in some kind of file system (a data store).
2. Has a way to access that data systematically.",4,0,0,False,False,False,1637250401.0
qwqv67,hl4uicn,t3_qwqv67,"I usually recommend Martin Kleppmann's book ""Designing Data-Intensive Applications"". It starts by considering a very simple database (a single file on disk) then builds from there. I'm sure it will help you build intuition.",3,0,0,False,False,False,1637251539.0
qwqv67,hl4nkkc,t3_qwqv67,"I think you're overcomplicating it a bit. Its really just a program that stores data. Nothing special happens to a computer or server when you create a database instance on it. As far as interacting with a database there are usually multiple interfaces. Some will have interactive shells (the two I'm familiar with are postgres and hive) where you can write sql or some other basic commands to manage the data. There are also db browsers which are basically a frontend client for interacting with the database; you can execute sql, see the results of queries, explore the database, ect. This is how most developers would interact with the database. There is also usually some sort of API for a website backend or data pipeline to interact with. 

> Secondly, if you could also briefly talk about the computer science of database management systems?

There's a myriad of different concepts that go into a database, do you have a more specific question on how they work?

> Please, also let me know if I'm thinking about this incorrectly. I am a self taught person with a lot of black boxes.

The biggest tip that I have for this is to try not to overcomplicate like I mentioned before. Just take whatever the concept is in its most simple form and try to build off of that understanding rather than worrying about all of the complexity up front. It just makes understanding a little more approachable.

Hope that helps! 

Edit: Comma Splice, addition",8,0,0,False,False,False,1637248732.0
qwqv67,hlc441e,t3_qwqv67,"The term database is pretty abstract.

It can be implemented in multiple different ways, but essential is a software that stores organized data.

Can store data in memory, on disk, tape, anywhere pretty much.
Can be in form of a library (SQLite) or a standalone software (postgres).

Hope this helps!",1,0,0,False,False,False,1637373415.0
qwqv67,hl631ze,t3_qwqv67,"Von Neumann is one a of the fathers of modern computations and his bigger cs contribution was make a paradigm when all data the same, one part of the data have ""meaning"" and are programs, you could store the data in whatever way you want for example put all in the same place whit index and then say to the machine search here this, most dB engines(all) take what and where you send for example in SQL and optimize and help whit replication and errors but a database is literally a file which have some time of structure like a excel (or the data science alternative csv)",-1,0,0,False,False,False,1637269058.0
qwqv67,hl4v6mh,t1_hl4uicn,this book is really good! you won't regret reading it,2,0,0,False,False,False,1637251807.0
qwqv67,hl7gib0,t1_hl4uicn,"Yup, DDIA ftw",1,0,0,False,False,False,1637290751.0
qwqv67,hl4r7cb,t1_hl4nkkc,This is a great and much appreciated response. I understand. Thank you!,2,0,0,False,False,True,1637250217.0
qwqv67,hm2s8ne,t1_hl631ze,Are these similar or the same amount 0f prestiges,1,0,0,False,False,False,1637873254.0
qw8yg4,hl21oev,t3_qw8yg4,Cool. Was thinking about logic gate with complex number integration. You can take the work from here...,5,0,0,False,False,False,1637194273.0
qw8yg4,hl3xyyr,t3_qw8yg4,Really cool. I just ordered 3. ☺️,5,0,0,False,False,False,1637235360.0
qw8yg4,hl5n2u1,t3_qw8yg4,"""My mom said it's special""

I love this.",2,0,1,False,False,False,1637262758.0
qw8yg4,hl9xqq7,t3_qw8yg4,Just ordered a blue one for my desk at work! Awesome little project. Congrats on the article!,2,0,0,False,False,False,1637341660.0
qw8yg4,hl4twxd,t1_hl3xyyr,Agreed.  I just ordered 2!,2,0,0,False,False,False,1637251301.0
qw8yg4,hl5veh7,t1_hl3xyyr,Thanks! I hope you like them! If there's any problems just message me on here for through the contact page at website.,2,0,0,False,False,True,1637266031.0
qw8yg4,hl5v75b,t1_hl5n2u1,\>.<,1,0,0,False,False,True,1637265952.0
qw8yg4,hla57r7,t1_hl9xqq7,"Thanks, I was really geeked that someone wrote a little something on my project, and thanks for supporting me making more stuff. Hope you love it.",2,0,0,False,False,True,1637344568.0
qw8yg4,hl5vptj,t1_hl4twxd,:) Hope you love them. The new Kits are super sleek. Be careful soldering the micro USB so that theres no shorts. Any questions ask me. Instructions here: https://tsjelectronics.com/instructions,2,0,0,False,False,True,1637266155.0
qwnv1m,hl446od,t3_qwnv1m,"OK, programming sounds like a gothic romance now.",10,0,0,False,False,False,1637239377.0
qwnv1m,hl4cfb0,t3_qwnv1m,"Same thing, different names. Sounds even more dramatic in French: étreinte fatale.",4,0,0,False,False,False,1637243742.0
qwnv1m,hl45x3s,t3_qwnv1m,"Deadlock (which is sometimes called the deadly embrace) is another crippling condition.  
 It occurs when two or more programs are each waiting for the others to   
complete - or even just to produce a data value - before proceeding. ...  
 There's a historic reason why deadlock exists.",2,0,0,False,False,False,1637240364.0
qwnv1m,hl4a6tr,t3_qwnv1m,(deleted),2,0,0,False,False,False,1637242623.0
qwnv1m,hl4x3dj,t1_hl446od,"She looked lustfully at her daemon, wondering what he was thinking. Her hand on his, her breath breathing his breath, her chest heaving in anticipation. She had one of the semaphores, she just needed the other. She slipped her fingers along his iostream and reached for the mutex. He gasped in ecstasy and dropped the semaphore. Finally! But, alas, as she reached for the now-available mutex, she realized that in her haste she had dropped the one already in her other hand.",13,0,0,False,False,False,1637252570.0
qwnv1m,hl4stxw,t1_hl4a6tr,"One common way to avoid deadlock is to have processes acquire resources in the same order.

In the example above: if P1 and P2 always attempt to acquire R1 first and then R2 second, then you can guarantee that the system won’t deadlock.",2,0,0,False,False,False,1637250870.0
qwnv1m,hl4yyjh,t1_hl4x3dj,One day this will become as part of theatrical performance.,4,0,0,False,False,False,1637253313.0
qwnv1m,hl4xgi2,t1_hl4x3dj,OMG.,3,0,0,False,False,False,1637252715.0
qw5g3c,hl106st,t3_qw5g3c,"Doesn't sound like this is exactly what you are looking for, but the best way to accomplish that for me has been to find past exams on the topic with answer keys. Either from my own university or another. If helpful, here are the past finals and midterms for the first OOP class at my old school, [https://exams.ubccsss.org/cs210/](https://exams.ubccsss.org/cs210/).",24,0,0,False,False,False,1637178530.0
qw5g3c,hl173zj,t3_qw5g3c,"Doesn't your school keep copies of old exams on file? 

They are often in library if not online already.",10,0,0,False,False,False,1637181285.0
qw5g3c,hl2uro8,t3_qw5g3c,"Geeks for geeks has hundreds if not thousands of practice interview questions where you can filter by subject, company, difficultly and programing in java, python, c++, inside their own web environment.",6,0,0,False,False,False,1637207702.0
qw5g3c,hl0sx23,t3_qw5g3c,Upvoted cause I want to know the same,11,0,0,False,False,False,1637175627.0
qw5g3c,hl44hf8,t3_qw5g3c,Check out [https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/) . They have quizzes as well as other resources related to all CS subjects.,2,0,0,False,False,False,1637239552.0
qw5g3c,hl1hdjn,t3_qw5g3c,Yeah it’s called the end of chapter textbook questions,2,0,0,False,False,False,1637185484.0
qw5g3c,hl2u04h,t3_qw5g3c,"Quizlet, chegg",1,0,0,False,False,False,1637207321.0
qw5g3c,hl16cth,t1_hl106st,Thank you kind stranger 🙏,2,0,0,False,False,True,1637180985.0
qw5g3c,hl194jk,t1_hl173zj,Only some teachers do that but not often. If there's a website that has exams of every lesson it would be great. Googling past exams is a but chaotic,3,0,0,False,False,True,1637182095.0
qw5g3c,hl1n4nb,t1_hl194jk,I thought at most universities it was an exception when they don't have old exam copies  for undergrad courses. My university regulation required professor to make special request to exclude them from library database.,4,0,0,False,False,False,1637187883.0
qwtqeb,hl50r8j,t3_qwtqeb,"I'm confused. How does the algorithm decide which bits to flip without an RNG? If you know how to flip bits randomly, you've already solved the problem.",10,0,0,False,False,False,1637254016.0
qwtqeb,hl5qbj9,t3_qwtqeb,I think that you want to read this: https://en.m.wikipedia.org/wiki/Hardware_random_number_generator,1,0,0,False,False,False,1637264044.0
qwtqeb,hl60rlx,t3_qwtqeb,New computer have hardware modules who measure temperature an think like that to generate true random number if not you need a seed you could make alot of functions  for different people and burn it like zcoin but the seed still exists,1,0,0,False,False,False,1637268153.0
qwtqeb,hl7gc8p,t3_qwtqeb,Because pseudo random number is a very complicated thing. Math rocks!,1,0,0,False,False,False,1637290674.0
qwtqeb,hl857ri,t3_qwtqeb,"First, theres no reason to shy away from math - its marely a tool.

Second of all, the solutions is impractical - even if you could ""flip a coin"" to determine if a bit would be flipped it would cause problems when using upper/ lower bounds.

For example lets say you have two bits and want a random number between zero and three. You roll both bits and both gets flipped to one, because this is outside of your bound you need to turn that number into the range [0,3] with equal probability. The only way to do that is roll *both* bits again. 

So despite having this ""bit flipper"" black box the solution would still be subpar to simply sampling from semi random thing like tempeture.",1,0,0,False,False,False,1637304114.0
qwtqeb,hl54fb3,t1_hl50r8j,Excellent point. I guess I was thinking in terms of the physical world where you can just flip 8 coins and have a random byte value.,1,0,0,False,False,True,1637255453.0
qwtqeb,hl5ja1e,t1_hl54fb3,"If I understand what you mean, it is to 'physically' randomly change bits. I think that would be against the idea of computers in the first place. If computers randomly changed physical bits we wouldn't be able to predict and program them. 

Anyways I could also be wrong there :D If so I would be very interested in an example/explanation.",2,0,0,False,False,False,1637261250.0
qwtqeb,hl5kgo5,t1_hl5ja1e,"As far as computer science is concerned nothing is random but you can for example take a temperature sensor with 10 bit precision and try to get a 16 bit reading from it, the last 6 bits will essentially be thermal/electrical noise. Then you can treat that as an I/O device and acquire random bits like that.",2,0,1,False,False,False,1637261712.0
qwegv6,hl2oc7x,t3_qwegv6,"Well to start with, it can check whether argument and return types match at compile time.


    int takesAnInt(int x);
    
    int main() {
        char c = takesAnInt(3); // Error, return type isn't a char!
        takesAnInt(""foo""); // Error, argument should be an int!
        return 0;
    }

Those checks aren't possible ahead of time in a dynamically-typed language, because variables don't have definitive types until they're evaluated.",6,0,0,False,False,False,1637204612.0
qwegv6,hl3ks3o,t3_qwegv6,IDEs can basically compile as you go (sort of) and show you many type errors in real time. So if you try assigning a string to an int variable you'll immediately get a red squiggly line telling you how dumb you are. Same thing with argument and return types as said in the other answer. This can be especially helpful when using interfaces and/or inheritance as the the variable you give a function might not be *exactly* the same type as the function requests (could be a subtype). Most IDEs are also pretty good at inferring what function overload your are trying to use too.,1,0,0,False,False,False,1637224497.0
qwegv6,hl3kuw7,t1_hl2oc7x,Depending on your compiler settings that first error would actually just be a warning.,1,0,0,False,False,False,1637224560.0
qwegv6,hl49bym,t1_hl2oc7x,Python solved that with type hints.,1,0,0,False,False,False,1637242181.0
qw5zd4,hl0r4mu,t3_qw5zd4,"Just that the optimal makespan has a time greater than or equal to the longest job, which of course it must. Trivially, if there is only one job, then T\* = t\_1. But no matter what T\* can never be shorter than the longest-running single job.",1,0,0,False,False,False,1637174926.0
qw5zd4,hl0ru6r,t1_hl0r4mu,Okay so why is it max\_i t\_i not just max t\_i?,1,0,0,False,False,True,1637175200.0
qw8tom,hl1pth2,t3_qw8tom,"It’s definitely possible. Like imagine if the python developers decided to build into the interpreter a stage where it runs a static type checking tool like mypy on your code and refuses to execute it if it fails. 

Java might even fit into that description because the compiler requires static typing but only compiles code to byte code which if interpreted by the Java Virtual Machine when you run the program.",3,0,0,False,False,False,1637189020.0
qw8tom,hlc4kkq,t3_qw8tom,"Yes of course, there is typescript for example.
 
You can have a compiled language not strongly typed or an interpreted language that is typed. The two concepts are pretty orthogonal.

Usually interpreted languages are not typed cause their usual main goal is development speed and adding types means you need to type more (no phun intended lol).",1,0,0,False,False,False,1637373634.0
qvgwfj,hkwf1qy,t3_qvgwfj,Keeping it accurate as the code base changes.,74,0,0,False,False,False,1637095654.0
qvgwfj,hkwiwq5,t3_qvgwfj,Waiting until the very end of coding and debug to start.,22,0,0,False,False,False,1637097163.0
qvgwfj,hkxacz7,t3_qvgwfj,Getting (non-great) engineers to understand how important it is.,9,0,0,False,False,False,1637108633.0
qvgwfj,hkxtnqm,t3_qvgwfj,"The hardest thing in writing documentation is somewhat similar to how we try to code the same way in a team using code conventions.

Defining your team's documentation conventions and having everyone on board is freakin' **hard**.

Here's what I've been preaching to my team and those who work with us.

1. Start with high level infrastructure stuff using something like https://c4model.com/ The 2 first levels are usually enough to figure out where I should do a fix or where I should attempt to work.

2. Before coding something, start with writing functional specifications first. IMHO, a good reference for writing no-bullshit-functional-specs is https://www.joelonsoftware.com/2000/10/02/painless-functional-specifications-part-1-why-bother/ 

Its form could be simple sentences and bullet points just like within the article. A flow diagram or a sequence diagram is sometimes more efficient in communicating the overall intent. My preferred tool to draw those diagrams is https://mermaid-js.github.io/mermaid-live-editor Since it's all text, you can even commit your diagrams within your code repository.

3. I usually don't care much about inline comments unless it's really trippy and complicated.

4. I do care about good naming conventions for your classes, methods, functions and variables.",8,0,0,False,False,False,1637117131.0
qvgwfj,hkwrwhe,t3_qvgwfj,"Depends on the project and the end user. In my previous project at work, I developed the entire app and had to also do a large of acquiring and documenting the business process from the customer. So not only did I had to comment and document my code, but I also had to document the business side.

The business side is generally where I hate having to document, usually because the business folks often don't have a consistent process.",5,0,0,False,False,False,1637100755.0
qvgwfj,hkwjxkf,t3_qvgwfj,"For me it's most difficult to find find the right time to write documentation comments. In the first place I think, that the code will change too much while debugging. After I got it working I struggle with the amount of methods which I have to comment. ^^""",13,0,0,False,False,False,1637097567.0
qvgwfj,hkwo7z1,t3_qvgwfj,It can be difficult to strike the correct balance between too little and too much comments.,4,0,0,False,False,False,1637099271.0
qvgwfj,hkx0j8w,t3_qvgwfj,"If you're doing things the correct way, your code is going to be negotiated, and should flow from tests - to the point the test code itself is a significant source of documentation in itself. This helps to solve the most significant problem with documentation, i.e. that it goes stale, because when you can't commit code that doesn't pass your unit tests, then your unit tests inherently must be up to date. And because you negotiate your code with those people who rely on it (larger system architecture or clients) you help to solve the *other* most significant problem with documentation, i.e. that it's only able to be understood by the person who wrote the code at the time they wrote it.",4,0,0,False,False,False,1637104352.0
qvgwfj,hkxeenm,t3_qvgwfj,"I find that writing documentation is a chore as much as designing algorithms and writing software as a chore: it's all equally tedious. For the case of documenting code, I structure my labels and functions together with commentary for the purpose of a ""clear narrative"". My aim is that I can read my code after six months of not seeing the code. Clear code narrative supports bug hunting and feature tweaking.

As for writing user documentation, this is equally important as writing correct code. Every software system is unique even when there are overlapping themes that are common to software titles. I want my users to be able to comprehend the intent of the features I've exposed to them. Powerful software systems are sophisticated by nature, users deserve some explanation about ""why"" and ""how"".

For my perspective, documentation is not optional. I need to communicate to the code writers and to the users about the meaning of my software. This perspective helps me slog through the chore that is writing computer software and communicating the meaning through documentation.",3,0,0,False,False,False,1637110420.0
qvgwfj,hkx2rxf,t3_qvgwfj,The part where I realize no one will ever look at it or do anything with it.,3,0,0,False,False,False,1637105323.0
qvgwfj,hkxanqb,t3_qvgwfj,Where I work we just don’t have enough and the systems are very complex. End up having to dig through code to figure it all out,3,0,0,False,False,False,1637108764.0
qvgwfj,hkz65vv,t3_qvgwfj,Turning on the computer,2,0,0,False,False,False,1637149195.0
qvgwfj,hl0juuz,t3_qvgwfj,"If your document involves APIs, use Postman. Postman automatically generates documentation for you. So, if there are any changes to your APIs, just call it once with the changes on Postman and it will change the documentation for you. In this way, your work will be made much easier and you'll be spending a lot less time on documentation.",2,0,0,False,False,False,1637172096.0
qvgwfj,hkwxi9u,t3_qvgwfj,"The most difficult part is that my boss doesn't value documentation until he's made to, so we aren't encouraged to do our best in preparing for projects with checklists and assessments, comment the code,  or document the project holistically once done. I try my best to get into the habit of documenting my own work, but we're lacking that important push from management to really cement that thinking",1,0,0,False,False,False,1637103056.0
qvgwfj,hkydexy,t3_qvgwfj,Keeping it in one place. Inevitably management will change tools of reorganize everything so then nothing is searchable.,1,0,0,False,False,False,1637127144.0
qvgwfj,hkyenth,t3_qvgwfj,"As I am learning this, it is the SAD part. Not sure if this is even actually used in building a software. Can anyone working in the industry answer this?",1,0,0,False,False,False,1637127893.0
qvgwfj,hkyh1kj,t3_qvgwfj,"I always found it difficult to put my thoughts into words. Like I know why I'm doing certain stuff because most of it is intuition but when I'm asked to explain why I did something it's takes me some time to explain it in words. I don't know, I think it's just a problem I have.",1,0,0,False,False,False,1637129402.0
qvgwfj,hkyw8ua,t3_qvgwfj,"Getting started, especially if large parts of the codebase have outdated or no documentation.",1,0,0,False,False,False,1637141110.0
qvgwfj,hkyxc6i,t3_qvgwfj,writing it after the fact. like when im done coding but only god knows how it works,1,0,0,False,False,False,1637142048.0
qvgwfj,hkzwkby,t3_qvgwfj,Knowing when to do it and knowing when not to. Knowing who the audience will be. Making it be good but also generated.,1,0,0,False,False,False,1637162902.0
qvgwfj,hl08akz,t3_qvgwfj,personally for me writing a documentation is the hardest part of writing a documentarion,1,0,0,False,False,False,1637167610.0
qvgwfj,hkyfdj3,t1_hkwf1qy,"This. I always find docs are amazing at the beginning when the engineer(s) who do the initial implementation do a great job of summing everything up. Then over time more engineers get introduced to the codebase and make changes which affect documented features without being aware of the documentation, creating a drift.",11,0,0,False,False,False,1637128335.0
qvgwfj,hkxnjn1,t1_hkwxi9u,"Yeah but you dont need documentation until you need it so go back and write it all down after something goes wrong, of course.",2,0,0,False,False,False,1637114442.0
qvi861,hkwp9ys,t3_qvi861,"Multi-tasking: multiple different tasks sharing a cpu core. Each task gets a share of cpu time, then the next task. On one given core, only one task can run per cpu cycle. By switching between tasks, we can multitask. Note that with multiple cores, we can have true multitasking.

Parallel processing: one task is subdivided among many cores. Think GPU processing a physics simulation, or DNN inferencing. Some problems are parallelizable, which means they are suited for parallel processing. Hardware accelerators like GPU are used for these types of tasks.",27,0,1,False,False,False,1637099691.0
qvi861,hky3qlg,t3_qvi861,"I'm cooking dinner. And while I'm chopping vegetables, my neighbor knocks on the door to give me a miss-delivered package, so I stop what I'm doing, put down the knife, answer the door, take the package, then return to chopping my vegetables.

In that scenario, I was doing two different tasks (chopping veggies and receiving a package) where at least one task was not completed in its entirety before also doing/finishing another, but only one of those tasks is being worked on at any single moment. This is ""concurrency"" (also sometimes called multi-tasking).

Now, I'm still cooking dinner, but while I'm chopping vegetables I'm also boiling noodles. The noodles are still cooking even though I'm not attending to them. Two tasks are being worked on (chopping, boiling) at exactly the same moment. This is ""parallelism"".",6,0,0,False,False,False,1637121838.0
qvi861,hkwoqgw,t3_qvi861,"Multitasking is more of an operating systems concept that means being able to run multiple individual processes (or programs) at the same time, or so that their executions overlap. The execution of multiple tasks may or may not actually take place at the same moment, and multitasking and what appears to the user as simultaneous execution can be achieved e.g. through rapidly switching between tasks instead.

Concurrency and parallelism are more general concepts, and they can take place in several different ways and in different kinds of contexts. Concurrency and parallelism are relevant to the OS concept of multitasking, but they're really concepts of a different level.

Out of those two, concurrency means being able to execute more than one thread of control so that their executions overlap. The threads can be from the same or from different processes. The execution doesn't necessarily take place at the same actual moment, i.e. it may be that any particular moment in time only one thread of control is being executed.

You could think of multitasking as a form of concurrency on the OS level, although when talking about concurrency, you'd usually be interested in multiple threads of control that have some kinds of dependencies or communication between each other.

Parallelism means actually being able to perform multiple operations at the same moment. Depending on the context (and which level of parallelism we're talking about), the operations can be from the same task, from different threads, or from entirely different tasks. But what separates parallelism from concurrency is that multiple things are actually being done at the same moment.",2,0,0,False,False,False,1637099473.0
qvi861,hkymgs3,t3_qvi861,"in short and to not get confused, just target the core, one core doing multiple tasks is multitasking while many cores doing one task is parallel processing",2,0,0,False,False,False,1637133182.0
qvi861,hkwk7z2,t3_qvi861,"From what I understand, concurrency (multi-tasking) is about responsiveness and parallelization is about doing more work at the cost of more resources.  


If we have 2 tasks: A and B, each takes 5 seconds to complete. If we run the tasks concurrently, the operating system will do, say, half a second of work on A and then switch to do half a second of work on B - switching back and forth until both are completed. This will actually take a little *longer* than just running them end to end. So why use it?  


Well, what if task A is processing a file and task B is checking for user input. If we run A then B, while A is running the computer will become totally unresponsive as it's completely focused on completed job A when computers have other task to consider.  


It we parallelize A and B we need to have a process free to take on the new load of work and to make sure that running these jobs at the same time isn't going to have any side effects. Parallelization at a large scale can cause nightmares with race conditions if you're not careful, but it great for small batch work - if you've got the computing power to spare.",2,0,0,False,False,False,1637097679.0
qvi861,hkws5mn,t1_hkwp9ys,Thank you! I think this would it much more simple and appropriate answer for an A-level question (If such was to come up in mock or exam).,-2,0,0,False,False,True,1637100858.0
qvi861,hkwr20v,t1_hkwoqgw,"Ah, that does make sense when thinking it like that. And from reading your reply as well as u/LowLvlLiving, I believe that I now have a better understanding of it now:

* Multi-tasking is more to do with OS types and its ability to perform multiple task nearly simultaneously.
* Assuming conditions are correct for both concurrency and parallelism. Parallelism will be slightly faster in performing task as it does not require constant switching between two or more jobs.
* Parallelism will also be more effective in processing larger or more tasks due to previous bullet point.

(I hope this correctly summarise the points from both of your replies? Also thank you both for answering my question.)

\#Edit: Second bullet point (new) - parallelism is slightly faster in performing task as it does not require the task to be finished before starting the next task, since a coordinator or job scheduler have already assigned it to another CPU/GPU core.",1,0,0,False,False,True,1637100410.0
qvi861,hkwx7p3,t1_hkwr20v,"That's generally the gist of it.

> Multi-tasking is more to do with OS types and its ability to perform multiple task nearly simultaneously.

Well, yeah. Or so that the OS or its user can switch between multiple applications or programs without having to terminate one of them first. You'll generally want the OS to be able to switch between them quickly and relatively often so that switching between applications is fluent and so that your music doesn't break up and so that a web server can get around to responding to requests quickly. You want it to be fast enough to *look* like the computer can run multiple things at once. Technically, however, it's multitasking whether it's really quick or not.

Multitasking is something you generally take for granted nowadays, as there haven't been any non-multitasking operating systems in mainstream PC use in the last ~25 years. But, well, *somebody* had to implement multitasking, and sometimes it can be useful to know how and why it works, so we still learn it.

> Parallelism will be slightly faster in performing task as it does not require constant switching between two or more jobs.

Depending on the task (or tasks) and the resources you have, it can be faster by any factor. For example, if you have *n* processor cores and your task can be divided into *n* equal parts that can be run (mostly) independently of each other -- or if you have *n* separate tasks you want to run -- you can ideally get a speedup of *n* times over doing those parts or tasks one after the other. In the best case anyway.

You might save something by not having to constantly switch (as the switching itself has a cost), but the real reason parallelism can give speedups is that it allows you to make use of the multiple cores or other parallel resources you have available.

Parallelism is kind of like dividing work between multiple workers and having them actually work and advance tasks at the same time instead of one of them working and another having to wait for the first one to finish before starting their own part.

Some tasks are easy to parallelize, but in case of other tasks it might be harder, so the speedup you can achieve might be smaller or even none.",2,0,0,False,False,False,1637102933.0
qvcvqa,hkydoki,t3_qvcvqa,"I also want to know, I didn't find any books but there are some blogs and YouTube videos on about them. You can check Intel tino core uefi project maybe that will give you some reference!",1,0,0,False,False,False,1637127303.0
qvcvqa,hkye96x,t1_hkydoki,You should ask this in r/embedded because usually embedded engineers are responsible for implementing first stage bootloader( bootloaders are very tied with hardware because bootloaders are responsible for hardware initialization ),2,0,0,False,False,False,1637127646.0
qvcvqa,hl1y4wn,t1_hkye96x,"That's a good one, I did so. Thanks!",1,0,0,False,False,True,1637192658.0
qvcvqa,hl2k9pm,t1_hl1y4wn,"I find a book on UEFI it is a good one ""Beyond BIOS Developing with the Unified Extensible Firmware Interface""",1,0,0,False,False,False,1637202728.0
qunqn2,hkretam,t3_qunqn2,So old they put wood paneling on it,122,0,0,False,False,False,1637006866.0
qunqn2,hkrt3np,t3_qunqn2,you can't trick me this is a hexbug,35,0,0,False,False,False,1637012453.0
qunqn2,hkreod3,t3_qunqn2,The top part looks like an ice cream sandwich,23,0,0,False,False,False,1637006811.0
qunqn2,hkr5evh,t3_qunqn2,"Three other CPU chip designs were produced at about the same time: the Four-Phase Systems AL1, done in 1969; the MP944, completed in 1970 and used in the F-14 Tomcat fighter jet; and the Texas Instruments TMS-0100 chip, announced on 17 September 1971. The MP944 was a collection of six chips forming a single processor unit. The TMS0100 chip was presented as a ""calculator on a chip"" with the original designation TMS1802NC. This chip contains a very primitive CPU and can only be used to implement various simple four-function calculators. It is the precursor of the TMS1000, introduced in 1974, which is considered the first microcontroller—i.e., a computer on a chip containing not only the CPU, but also ROM, RAM, and I/O functions. The MCS-4 family of four chips developed by Intel, of which the 4004 is the CPU or microprocessor, was far more versatile and powerful than the single-chip TMS1000, allowing the creation of a variety of small computers for various applications.

//////////////////////

For more information on little-known history and other 1970s events, consult the 50YearsAgoLive Project, a Twitter program that reports events from exactly 50 years ago as if they’re happening in real time. It is meant to stoke an interest in history by making it accessible to the everyday reader:

[https://twitter.com/50YearsAgoLive](https://twitter.com/50YearsAgoLive)",39,0,0,False,False,True,1637003155.0
qunqn2,hkto1mp,t3_qunqn2,"The 4004 is rather lovely to study and play with, you can read the datasheet and write code for it in raw hex in a matter of hours.  What most people don't understand about the 4004 is it is less a general-purpose CPU and more a ""multichip microcontroller"".  
  
The 8048 microcontrollers and beyond are spiritual successors to the 4004 I understand.  
  
The 8080, then 8086 that all our PCs hark from have their roots in the 8008 which was the first general-purpose single-chip CPU I think.",10,0,0,False,False,False,1637043788.0
qunqn2,hkux1l3,t3_qunqn2,What happened to Intel 0 through 4003?,3,0,0,False,False,False,1637074192.0
qunqn2,hkv8jdk,t3_qunqn2,This is very cool,1,0,0,False,False,False,1637078990.0
qunqn2,hkrq3n6,t1_hkretam,That's how you know it was released in the 70's.,78,0,0,False,False,False,1637011281.0
qunqn2,hkt2wlc,t1_hkretam,Woodgrain grippin,6,0,0,False,False,False,1637032313.0
qunqn2,hkvctdk,t1_hkretam,No seriously. What is that?,2,0,0,False,False,False,1637080692.0
qunqn2,hkv254d,t1_hkrt3np,Came here to share some nostalgia with my fellow 2000’s kids.,5,0,0,False,False,False,1637076374.0
qunqn2,hkv2uko,t1_hkrt3np,That is a hexbug,2,0,0,False,False,False,1637076668.0
qunqn2,hkvvnua,t1_hkvctdk,"It is a cover. There is a silicon wafer in the middle, under it. Fine wires connect from the legs to pads on that wafer.

original chip:

[https://www.semanticscholar.org/paper/The-Intel-4004-Microprocessor%3A-What-Constituted-Aspray/7ba1ae43acd70844b8c00e47c436354339c12bfe/figure/6](https://www.semanticscholar.org/paper/The-Intel-4004-Microprocessor%3A-What-Constituted-Aspray/7ba1ae43acd70844b8c00e47c436354339c12bfe/figure/6)

modern-day recreation similar to old design:

[https://fuentitech.com/diy-silicon-people-build-integrated-circuits-similar-to-intels-4004-cpu/192855/](https://fuentitech.com/diy-silicon-people-build-integrated-circuits-similar-to-intels-4004-cpu/192855/)",3,0,0,False,False,False,1637088010.0
qv9yun,hkv1jkc,t3_qv9yun,"Take some very simple programs, like just one loop or if statement if you don’t understand those and walk through it by hand. Like have a list of what the variables values are and update them every instruction. Also ask yourself why each line of code exists. 

Then after you are done with that run the program in a debugger to compare. If you did it right awesome, try a different harder program, if not figure out why you made the mistake.",6,0,0,False,False,False,1637076127.0
qv9yun,hkw2b8y,t3_qv9yun,"Here are some things that really helped me:
1. Relating the algorithm or concept to something in the physical world (like imaging sorting a deck of cards when doing sorting algos)
2. Physically drawing diagrams and tediously going through every step using examples.
3. Going through each line of code and trying to understand its purpose, dissecting every component of the syntax as steps you would take to complete the problem irl.
4. Practice writing small programs related to what you’re learning, you can make this part fun by involving other stuff you’re interested in (when I was learning object oriented programming in high school I made a drumset program since I’m into that)

Good luck :)",6,0,0,False,False,False,1637090628.0
qv9yun,hkw9jsd,t3_qv9yun,"I found it helpful to put some simple programs into this: [https://pythontutor.com/](https://pythontutor.com/) 

And then step through them. Despite python in the name, it also does a bunch of other languages.",3,0,0,False,False,False,1637093496.0
qv9yun,hky3hkz,t3_qv9yun,"One thing that helps me is running things on a REPL, basically it shows the output of stuff as you write it in real time.

It’s been around as a tech for decades, but a lot of people don’t know it exists. It’s the standard way to do things in Lisp.",2,0,0,False,False,False,1637121716.0
qv9yun,hkyc36k,t3_qv9yun,Give yourself much more time than you think you'll need.,2,0,0,False,False,False,1637126359.0
qv9yun,hl9zimg,t3_qv9yun,"I like to draw out tricky stuff using a flowchart and kinda note out where things are happening and what variables are being handled. I personally think it helps to know what the machine is doing when you tell it something, so maybe think about that as you look at the code. Also, is it your code you’re having trouble with or are you looking at others’ code on stack overflow or an example online or text book?",2,0,0,False,False,False,1637342353.0
qv9yun,hli2n2s,t3_qv9yun,"Just start doing things in the language... Say a sentence for what you want to happen then write the code the does it. Python is awesome for this and an excellent first language for this reason.

    python = list()
    doing = True
    things_to_do = 5
    
    print(""We want to do "" + str(things_to_do) + "" things!"")
    
    while doing == True:
        python.append(""things"")
        if python.count(""things"") == things_to_do:
            print(""Looks like we've done enough things for now."")
            print(""Here are the things in python: "" + str(python))
            exit()
        else:
            print(""Let's do more things."")

Doesn't matter if what you do has a purpose or not, just become fluent with the syntax first.",2,0,0,False,False,False,1637496873.0
qv9yun,hkz7e68,t1_hkw2b8y,Thanks so much!!! I will try implementing these steps into my learning process,1,0,0,False,False,True,1637150049.0
qv9yun,hlddcm3,t1_hl9zimg,It’s mainly other people’s code - especially my professor’s code. A couple people have recommended drawing things out before but I sometimes don’t even know where to start 😅,1,0,0,False,False,True,1637403455.0
qv9yun,hli7f8h,t1_hli2n2s,Thanks so much!,1,0,0,False,False,True,1637500022.0
qv9yun,hles2kn,t1_hlddcm3,Start with main or if it’s just a certain few lines look at the function/method they are in and just draw that part out. Draw.io is a good online tool but I keep a small dry erase board and markers nearby for exactly this and math. Anything to help visualize the ins and outs as they get processed. Good luck with your studies.,1,0,0,False,False,False,1637431304.0
qvhov8,hkwkc3k,t3_qvhov8,"I don't use bit manipulations or many bitwise operations, but my favorite bit of hijinkery is fast inverse square root",7,0,0,False,False,False,1637097724.0
qvhov8,hkwr6cb,t3_qvhov8,The book hackers delight was just packed full of them. Lots of branch free operations. Don’t have a favourite but I do like the trick where you multiple by 1 or 0 to avoid an if statement.,4,0,0,False,False,False,1637100459.0
qvhov8,hkynu0s,t3_qvhov8,"Finding all permutations of a set by looping from 0 to (1<<N)-1 then picking members based on bits.

Binary indexed tree and its variants. 

Finding the highest power of two in a number with (n & ~(n - 1)). 

XOR for finding a unique numbers in a sequence of duplicates. 

Tries with bit operations as edges.

These examples are mostly for fun. There are some bit-based data structures have been useful for me professionally though: hyperloglog, Bloom filters, roaring bitmap.",4,0,0,False,False,False,1637134230.0
qvhov8,hkwpnys,t3_qvhov8,What are some things that you find interesting?,2,0,0,False,False,False,1637099847.0
qvhov8,hkz2i7m,t3_qvhov8,"calling popcount 5 times for a branchless and fast way to OR all the bits

1 or 0 to  all true by sll by 31 and then sra by 31

also theres a ton of fractals that can be done as bit hacking",2,0,0,False,False,False,1637146389.0
qvhov8,hlca3iu,t3_qvhov8,[Hacker's Delight](https://en.wikipedia.org/wiki/Hacker's_Delight) is a book full of various bit flipping tricks as well as other useful low level tricks.,2,0,0,False,False,False,1637376327.0
qvhov8,hkype2p,t1_hkwpnys,"I can’t say I’m using whole lot. Just simple arithmetic, such as int division/multiplication by 2. Albeit I read some where that it’s less performant in those simple cases compared to regular division. 

But I still use them because they are cool and remind me that I gotta learn and use them more. 😁",2,0,0,False,False,True,1637135440.0
qvrimr,hkyg6nu,t3_qvrimr,"A ""directed acyclic graph"" means:

1. A ""graph"", sometimes called a ""network"". You have nodes with edges to other nodes. This could represent a ton of things, from a search tree to Twitter following relationships.

2. ""Directed"", means the edges between nodes have a direction, like ""A -> B"", and not ""A <-> B"".

3. ""Acyclic"" means there aren't any cycles. If you have ""A -> B -> C"", then C can't have edges to A or B, and B can't have edges to A.

You can represent DAGs many ways in memory. If this were a homework assignment in C, we might make a struct for a node, which contains an array of pointers to other nodes. Like a tree, with a variable number of branches at each step. Alternatively, you could represent a DAG as an adjacency list, an adjacency matrix, or just about any other way you'd store a network.",5,0,0,False,False,False,1637128850.0
qvrimr,hkz121g,t3_qvrimr,"Directed Acyclic graph is a directed graph which doesn't have cycles. This is useful if you want to solve something like constrained scheduling, eg. chalk out a sequence of activities where some activities depend on others to complete. So if you hypothetically have a case where job A requires job B to complete and job B requires job A to complete, you have a cyclic Directed graph, and you cannot have a topological order (so you cannot logically draw out a sequence for doing those jobs). On the other hand, if job B required job C and C required A, you have a DAG and you can solve the job ordering problem (A->C->B).

In general, a directed graph can have cycles. A DAG is a special kind of directed graph.

In order to understand memory representation of a DAG, you have to understand memory representation of an ordinary, ""undirected"" graph. An undirected graph is a set of vertices and the edges joining them. Say you have V vertices and E edges between them, so you represent the graph as follows:

* You represent the vertices as an array of length V, having indices from 0 to V-1.
* Each array entry 'arr\[v\] 'points to a (linked) list of vertices 'w' which are connected to the vertex 'v', where v is any integer between 0 and V - 1. The list of vertices is called the Adjacency list, or 'adj'.
* So for example, if vertices v and w are connected, v will have w in its adjacency list and w will have v in its adjacency list.
* When you add an edge to the graph, say add\_edge(a, b), b will be appended to a's adjacency list and a will be appended to b's adjacency list (since in an Undirected Graph, edge a-b is same as b-a --- and *this is where a directed graph differs from an undirected graph*)

Now, in a directed graph, edge a-b means edge a->b, and edge a-b and b-a are not the same. So when you add an edge a-b, you only add 'b' to a's adjacency list, and not 'a' to b's.

Now it is possible to have 3 edges a-b, b-c and c-a in a directed graph, which will cause a directed cycle. That's all.",3,0,0,False,False,False,1637145202.0
qvrimr,hkyfomz,t3_qvrimr,"A DAG could be represented in memory a bunch of different ways. It really depends on how you were going to use it.  As far as I know there is no reference implementation for a DAG in memory , but I could be wrong.",1,0,0,False,False,False,1637128529.0
qvrimr,hkzamtp,t3_qvrimr,Would it be the same as a tree?,1,0,0,False,False,False,1637152148.0
qvrimr,hl0jh8q,t1_hkyg6nu,"Thanks, man!  
Appreciate  it.",1,0,0,False,False,True,1637171951.0
qvrimr,hl0jkit,t1_hkz121g,Wow! Thanks for the detailed response. It made my day.,1,0,0,False,False,True,1637171986.0
qvrimr,hkyh46s,t1_hkyfomz,"Thanks, appreciate your answer. 

Can you mention a few ways in which it could be implemented in memory.",1,0,0,False,False,True,1637129449.0
qvrimr,hl0gay2,t1_hkzamtp,"Tree implies a single root, so while all trees are DAGs, a DAG is a more general class of graph",2,0,0,False,False,False,1637170721.0
qvgo6i,hkwma10,t3_qvgo6i,"Take a look at software defined radios. One possible approach is to find a (relatively) inexpensive SDR with TX / RX capabilities. You can then use open-source software to ""control"" the SDR. A very popular interface for this application is GNURadio. 

Since this is software-defined, then you essentially control what happens at the GUI level, using the SDR as an RF front-end. All of the mod/demod will happen at the Pi itself, as opposed to being processed at the SDR. Speaking from experience, you may find that the Pi is rather underpowered to handle certain SDRs or signal types.",1,0,0,False,False,False,1637098495.0
qvgo6i,hla2cou,t3_qvgo6i,"I don’t know specifically what your set up would be doing but I work with CAN a lot and have experience with TCP/IP and those use a “packet” which is a fancy term for a specific cluster of bits being sent from device to device. The packet is predefined in a standard somewhere and the devices know how to read it or can be told how to read it. There’s some identifying information regarding the source and the destinations and the contents, the data itself, and some CRC data as well. All of that gets framed inside the predefined packet so the devices can recognize where each packet starts and stops. The bits themselves are sent by literally sending specified voltage pulses. Which is beyond my scope I deal more with filling and reading the data after the machine handles the packet.",1,0,0,False,False,False,1637343462.0
qv9g3m,hkuxqz2,t3_qv9g3m,"It will be very difficult to find research papers that are 500 to 600 words. Short conference papers are usually 4-6 pages, and it goes up from there. You might find some published abstracts but they are not so common in CS (they appear more frequently in psychology, medicine, etc.). The only thing I can recommend is to add ""abstract"" to your search terms but I don't think that will really work.

&#x200B;

Also, you may want to check with your teacher because perhaps they meant articles and not research papers. In which case, there should be plenty online.",5,0,0,False,False,False,1637074499.0
qv9g3m,hkxhfdx,t1_hkuxqz2,"Yes, I found some abstract in my research but they are too short and are less than one page... I think I can look over neural network cause it's fashion this time and can just say to my teacher ""Ho, neural network is a kind of data structure"" btw. Idk if it's true or false but I can maybe have a bit more luck in that way ...",1,0,0,False,False,True,1637111763.0
quwl9r,hkuxal8,t3_quwl9r,"Because the application is downloaded using multi-threaded download at the beginning, the speed will suddenly be very high. At the end of the download of resources, multi-threading is not worth it, and the number of threads will be reduced. At the end, it will be a single-threaded download. very slow。",1,0,0,False,False,False,1637074300.0
quwl9r,hla2qgv,t3_quwl9r,Loading/Downloading percentages are sometimes completely arbitrary and do not reflect the actual state of the loading. It just makes the user feel better to have something on the screen to look at. There even studies around this.,1,0,0,False,False,False,1637343613.0
qtrrml,hklhewu,t3_qtrrml,You are mistaking experience with genius.,685,0,2,False,False,False,1636903542.0
qtrrml,hklqzj6,t3_qtrrml,Ok but what editor does he use?,24,0,0,False,False,False,1636907704.0
qtrrml,hklm3ip,t3_qtrrml,"What you mentioned about ""knowing the answer upon being asked"" is actually a skill you can pick up on.   

It's something I've done for basic problems myself and realized they if I keep practicing the art of ""problem solving"" I'll get to do it for harder questions.   

Say `hello world!` as you embark on this world of programming, and good luck to you, buddy!",66,0,0,False,False,False,1636905620.0
qtrrml,hklmx1s,t3_qtrrml,This is funny.,43,0,0,False,False,False,1636905970.0
qtrrml,hklw75l,t3_qtrrml,I had a prof who did this in university. It sure helped me learn more when I saw him actually typing everything and explaining as he went. I wanted him for every class lol,8,0,0,False,False,False,1636909869.0
qtrrml,hkloomj,t3_qtrrml,"If you’re from ucla you know what prof codes on Microsoft word, das.",21,0,0,False,False,False,1636906727.0
qtrrml,hkmogbu,t3_qtrrml,"Using a shell in a terminal instead of clicking around in some GUI is a lot more efficient once you're used to it. The Unix model is incredibly ingenious, with its small tools that do one thing, and that can be stringed together using an efficient language to perform pretty much any task. I don't see the point in using an IDE since I took the time to learn the POSIX shell, and haven't used one, privately or professionally, in seven years.

Highly recommend. It makes using a computer and programming more or less synonymous, again.",10,0,0,False,False,False,1636920579.0
qtrrml,hkltju6,t3_qtrrml,"I would love to be in at least one of your classes with this professor, he sounds amazing.",3,0,0,False,False,False,1636908776.0
qtrrml,hkn7szn,t3_qtrrml,">	He programs everything using the text editor

As opposed to what?

Anyway, good story, made me smile.",3,0,0,False,False,False,1636928129.0
qtrrml,hkn6ukm,t3_qtrrml,"I agree with the comment ""You mistake experience for genius"" because it brings up an important point - You can easily get to the point you see your professor at if you dedicate yourself to learning the tools, methods, and theory behind the practice. Hopefully that motivates you throughout your studies, I wish you luck.",5,0,0,False,False,False,1636927736.0
qtrrml,hknicb5,t3_qtrrml,"Those People are great! 
I also find those people pretty impressive that can control the whole PC just with their keyboard. And not even that, they even do it like twice as fast as i ever could with my mouse.
On top of that, they know hotkeys, no person ever in the history of mankind had heard of.",2,0,0,False,False,False,1636932631.0
qtrrml,hko7038,t3_qtrrml,What university do you go to?,2,0,0,False,False,False,1636943854.0
qtrrml,hkp0qbw,t3_qtrrml,I thought calling someone a Chad was an insult? This seems like a complement,2,0,0,False,False,False,1636960917.0
qtrrml,hkpeo8f,t3_qtrrml,"I do the terminal thing too, it's because standard out and standard error are being sent to the terminal so if it's not logging to a file you can see what's going wrong or why it errors out. Not every application does this, also I almost forgot the #1 reason is because you can ctrl + c out of buggy programs like firefox with 400 tabs open. The kind of ""lag"" so bad that you can't even open the task manager to kill the program",2,0,0,False,False,False,1636972585.0
qtrrml,hkmj5bv,t3_qtrrml,"Learned how to compile and run Java programs through the command line today. Quite cool, cus I felt like Mr. Robot for a sec.",3,0,0,False,False,False,1636918593.0
qtrrml,hkneqo7,t3_qtrrml,"There is a recorded lecture series called “The missing semester of your CS education”

https://missing.csail.mit.edu/

What you describe about live coding, using the terminal, using a text editor, etc. is part of that lecture, and the best professional engineers drive their computers from the command line like this all the time.  It’s why we prefer posix-compliant shells over the dos command line.",5,0,0,False,False,False,1636931061.0
qtrrml,hkmfx28,t3_qtrrml,This because he spent years coding alone in his office and preparing for the lectures.,1,0,0,False,False,False,1636917393.0
qtrrml,hkmuckj,t3_qtrrml,Colored words and red squiggles FTW!!!,1,0,0,False,False,False,1636922836.0
qtrrml,hknip6o,t3_qtrrml,"This isn't abnormal. When teaching intro programming courses, I'd do virtually the same thing. I'd be using a IDE because that is what we teach the students to use, but for programs that size, the eclipse (at the time) was too slow to do anything past color/highlighting before I finish jotting a segment in. 

This is just a matter of experience. Those examples really are trivial to him, and hopefully will be to you eventually.",1,0,0,False,False,False,1636932787.0
qtrrml,hknvhl0,t3_qtrrml,My professor at JuCo was like this. Seriously a legend. I thoroughly enjoyed watching him work. I wish all professors lectured this way. It’s soooooo beneficial to see the process rather than slides and 10 minute recorded videos,1,0,0,False,False,False,1636938568.0
qtrrml,hko0umq,t3_qtrrml,I need this type of profs in my Master degree,1,0,0,False,False,False,1636941051.0
qtrrml,hkovlwy,t3_qtrrml,"I'm  glad that pumped you. Stuff like that really has gotten me into working in terminals, learning vim / Doom Emacs, stop being scared of low level langs, etc. 

This talk absolutely blew my mind when he started coding live: https://www.youtube.com/watch?v=OyfBQmvr2Hc Due is writing at a million miles per hour. 

There's also the Sussman / Ableson MIT course where they code live in the terminal in Lisp AND ON THE BLACKBOARD! This was in the 80s mind you. Pretty neat stuff.",1,0,0,False,False,False,1636957258.0
qtrrml,hkovyhl,t3_qtrrml,Imagine not living in a tty in linux and only ever using CLI.,1,0,0,False,False,False,1636957492.0
qtrrml,hkowc5c,t3_qtrrml,My programming prof just shows us videos from youtube of an inaudible indian guy talking about c programming in turbo c. Most of her presentations of codes has syntax issues which makes the whole class boring.,1,0,0,False,False,False,1636957756.0
qtrrml,hkpbyfi,t3_qtrrml,Girls with autism vs boys with autism,1,0,0,False,False,False,1636970249.0
qtrrml,hkphzez,t3_qtrrml,"Ah, yes. I did that as well, while I was teaching. As I was in the told, that skill is very rare, although I never understood why, and what's so difficult about writing a program that has less than 200 lines or code on the spot and that works as it should from the first time",1,0,0,False,False,False,1636975305.0
qtrrml,hkpk4zh,t3_qtrrml,"There are many talents, but memorization is just one of them. There is also:
- Creativity
- Discipline
- etc.",1,0,0,False,False,False,1636976951.0
qtrrml,hkpmb64,t3_qtrrml,This is the kind of professor I want to be in the future. I don't want to be a boring old man with powerpoint.,1,0,0,False,False,False,1636978469.0
qtrrml,hkpo6kr,t3_qtrrml,probably because he teaches the same class every year??????,1,0,0,False,False,False,1636979684.0
qtrrml,hl5pz36,t3_qtrrml,"I don't know why you guys are dragging him down. I mean yes, writing 20 lines of code for an introductory class is not hard, same goes for opening everything over cli; I can do it and I am in my sophomore year, but this guy managed to inspire one of his students to take interest in what he teaches and get the student to do the assignments himself, without forcing them. That's what the teachers job is. I am in CS because of a teacher like this and I can't thank him enough, ever. You can learn syntax and basic concepts over internet; probably better than they will teach you at the university, but getting inspired by your teacher is a priceless thing.",1,0,0,False,False,False,1637263907.0
qtrrml,hkmh96q,t3_qtrrml,"I usually write dozens lines of code in different files/directories and it often works from the first running

I mean I even see where I will get problems in my code, and where I won't

It's not a magic, it's just a skill and knowledge....",-3,0,0,False,False,False,1636917879.0
qtrrml,hklj5sl,t3_qtrrml,[deleted],-2,0,0,False,False,False,1636904320.0
qtrrml,hkp4erf,t3_qtrrml,"He learned to use the terminal when a Mouse did not exist.  It is what he's learned and only what he uses.  It is his happy space.  He's probably a lunatic and uses VI, arggg. EMACS is better, but why?  Nano does just fine.

Set up your phone, recording his entire lecture. We never had that (when we walked 8 miles uphill in a blizzard across campus, both ways).

Oh, yea he probably still has stacks of cards in his office.  Visit, be impressed.  Build some xxix servers of your choice either as VM's or on Raspberry Pi's.  Either way, you learn to use the terminal.  Do things his way.  You'll learn.  Enjoy.

You want a real CS job.  Here's one that will make you UNREPLACEABLE!  [https://www.popularmechanics.com/space/a17991/voyager-1-voyager-2-retiring-engineer/](https://www.popularmechanics.com/space/a17991/voyager-1-voyager-2-retiring-engineer/)

https://voyager.jpl.nasa.gov/mission/did-you-know/",0,0,0,False,False,False,1636963792.0
qtrrml,hklkuks,t1_hklhewu,"Thats right, most coding done in courses at universities is basic stuff. Writing 20 lines of correct code is not that complicated when you are doing it for several years.",219,0,0,False,False,False,1636905068.0
qtrrml,hkmmvgj,t1_hklhewu,"Also him coding on the spot is a good example. The students should be paying attention to how he’s coding on the spot and his reasoning, ask if he’s not explaining out loud",7,0,0,False,False,False,1636919993.0
qtrrml,hklmb84,t1_hklhewu,There exist experienced geniuses!,17,0,0,False,False,False,1636905710.0
qtrrml,hkmazh2,t1_hklhewu,Agreed. He's just been programming a long time.,4,0,0,False,False,False,1636915598.0
qtrrml,hkohgja,t1_hklhewu,"Exactly. I do this all the time in front of the class, and I can tell you: I am NO GENIUS! 
OK I don't exactly use just bare bones text editors like vim or Notepad (I prefer VS Code), but on occasions I would use vim and compile on the command line (for example to show different compilation levels, compiling with debug info etc) and quickly show the effects to the students. 
We also do a lot of variations of our programming excercises (C/C++) on the spot, and if the students have problems, I will just code them on the fly in front of the class. 
You can do this too. You just need more time and determination.",3,0,0,False,False,False,1636948896.0
qtrrml,hkns8p1,t1_hklhewu,Well this is the first prof that does it that way and he teaches us 4 different languages that are all pretty different from each other. Our profs from the last semesters always prepared their code and often still had to later edit it because they made mistakes. Also my first time seeing a prof just using text editor and the terminal for everything. I think being able to take in all that experience also needs intelligence.,5,0,0,False,False,True,1636937031.0
qtrrml,hklwrdg,t1_hklqzj6,Would be funny it was gedit,28,0,0,False,False,False,1636910095.0
qtrrml,hkm3yip,t1_hklqzj6,Google docs,13,0,0,False,False,False,1636913016.0
qtrrml,hkm5wea,t1_hklqzj6,Excel,7,0,0,False,False,False,1636913714.0
qtrrml,hkn1a2x,t1_hklqzj6,The true Chad editor. Vim,12,0,0,False,False,False,1636925542.0
qtrrml,hkmc4k7,t1_hklqzj6,atom gang rise up,4,0,0,False,False,False,1636916015.0
qtrrml,hkmb5j2,t1_hklqzj6,"The correct editor: neovim.

(Ducks...)",5,0,0,False,False,False,1636915660.0
qtrrml,hknrcr9,t1_hklqzj6,Just the text editor,1,0,0,False,False,True,1636936617.0
qtrrml,hkn6d46,t1_hklmx1s,"What I found funny was the use of 'Chad' as an adjective. I have never seen it used in a postitve way like in this post. I have only seen it used by incels as a way to describe the guys they simultaneously envy/loathe.

I'm not trying to be rude; I'm completely serious. I have never seen someone call someone a 'Chad' in such a positive way.

For context: [Chad, on the incel wiki](https://incels.wiki/w/Chad).

Do normal people really use 'Chad' as an adjective these days? It just seemed so odd in this context.",15,0,0,False,False,False,1636927538.0
qtrrml,hkn2wl9,t1_hkmogbu,"While I do use terminal very heavily, there are certain aspects to what an IDE can provide that the terminal simply cannot - primarily to do with visual indicators. None of them are deal breakers, but particularly when you're dealing with very complex systems (think code that has been allowed to grow too 'Enterprisey' and large web application systems) or developing graphics-heavy applications, they can be a significant benefit.
 
I effectively ban them in my teaching until students are in at least 3rd year though, otherwise they turn into a crutch.",9,0,0,False,False,False,1636926161.0
qtrrml,hkpu6m8,t1_hkn7szn,Real men draw the code in MS paint and rename the file to an .exe.,3,0,0,False,False,False,1636983133.0
qtrrml,hkp71r1,t1_hkn7szn,"""IDE""",1,0,0,False,False,False,1636965968.0
qtrrml,hkpc6cm,t1_hko7038,honestly op would be way better off with his name,1,0,0,False,False,False,1636970440.0
qtrrml,hko5o75,t1_hkneqo7,"Is this understandable for 1st Year CS student like me, I only have a tiny amount of background about programming, so this would really help if I could understand this lecture.",2,0,0,False,False,False,1636943241.0
qtrrml,hkslh1u,t1_hkpo6kr,He doesn't. The program has changed a lot during the years. But he definitely is very experienced. Haven't had a prof yet that is this good at his job.,1,0,0,False,False,True,1637024585.0
qtrrml,hlgp8c5,t1_hl5pz36,"I think it is my fault for not explaining it properly. Reading it again really sounds lame and there are a few things I should have mentioned. It is not an introductory class but an advanced class. Writing 30 lines of code might still not seem that much but the damn speed with which he is able to do it even when students ask for veeeery specific examples. He can also easily switch between the languages with no problems. I would also be able to open programs over the terminal but I've never seen someone not using a mouse at all and controlling every aspect over it.  


It might still not sound that impressive to experienced people but you should really see that guy. I also did some more research and he seems to be the chairman of the programming language faculty as well and he regularly holds international speeches. He definitely is different from the other profs we had until now. So many others already had to google shit during class multiple times.",1,0,0,False,False,True,1637463009.0
qtrrml,hklk42n,t1_hklj5sl,"i agree with your sentiment, but the way you said it was so rude and condescending...",4,0,0,False,False,False,1636904742.0
qtrrml,hkp4fd6,t1_hkp4erf,8 miles is 12.87 km,1,0,0,False,False,False,1636963805.0
qtrrml,hkp4f85,t1_hkp4erf,"Ahoy joelhuebner! Nay bad but me wasn't convinced. Give this a sail:

He learned t' use thar terminal when a Mouse did nay exist.  It be what he's learned n' only what he uses.  It be his grog-filled space.  He's probably a lunatic n' uses VI, arggg. EMACS be better, but why?  Nano does just fine.   

Set up yer phone, recording his entire lecture. Our jolly crew nary had that (when our jolly crew walked 8 miles uphill in a blizzard across campus, both ways).  

Oh, yea he probably still has stacks o' cards in his office.  Visit, be impressed.  Build some xxix servers o' yer choice either as VM's or on Raspberry Pi's.  Either way, ye learn t' use thar terminal.  D' things his way.  You'll learn.  Enjoy.",0,0,0,False,False,False,1636963802.0
qtrrml,hkp4ff5,t1_hkp4erf,8 miles is 6849.71 Obamas. You're welcome.,0,0,0,False,False,False,1636963807.0
qtrrml,hkln41o,t1_hklkuks,"Maybe but I'm pretty ""experienced"" at this point and taken point on a few very large data intensive systems and I think if you pointed me at a random problems without a few days leetcode warmups I'd probably struggle if it wasn't in my domain. If it isn't genius its at minimum someone very skilled.",52,0,0,False,False,False,1636906053.0
qtrrml,hknti47,t1_hklkuks,Well it probably seems impressive to me since it's so different from the other profs. Also I checked and it's more like 35 lines for each example which probably still isn't that impressive for some. But to me it is something new to see someone just program multiple examples with about 35 lines each lesson without using some editor that shows you when you fuck up. Just haven't seen someone do that before,9,0,0,False,False,True,1636937628.0
qtrrml,hkmj762,t1_hklwrdg,Whats wrong with gedit 😡,11,0,0,False,False,False,1636918611.0
qtrrml,hkm3n5i,t1_hklwrdg,inb4 it is regular windows notepad to screw with everybody.,7,0,0,False,False,False,1636912890.0
qtrrml,hkoi731,t1_hkn1a2x,Obviously,4,0,0,False,False,False,1636949276.0
qtrrml,hkov9nx,t1_hkmb5j2,"Nothing wrong with any vim. Neovim looks beautiful when prepped up. 

I'm using ((((DOOM!)))) >:D",2,0,0,False,False,False,1636957032.0
qtrrml,hkoo1ap,t1_hknrcr9,There are a couple of them. Getting to know what the ubiquitous ones are and how to use them could help you in your career later.,6,0,0,False,False,False,1636952507.0
qtrrml,hko0rpn,t1_hkn6d46,"Teenager here: yeah, we use ""chad"" as equivalent to ""worthy of respect and admiration""",15,0,0,False,False,False,1636941015.0
qtrrml,hkp455d,t1_hkn6d46,"normal might be a stretch, but its very common now to use 'Chad' as an adjetive, the internet is weird.",6,0,0,False,False,False,1636963579.0
qtrrml,hknael8,t1_hkn6d46,"Definitely, and , anecdotally at least, more often than not I see it used in a positive way. Though, whether or not the people I am around (mostly other students) can be classified as “normal” is heavily debated.",3,0,0,False,False,False,1636929221.0
qtrrml,hknggmx,t1_hkn6d46,I just thought it was funny that simply being able to code on the spot was such a huge deal to OP.,1,1,0,False,False,False,1636931816.0
qtrrml,hknlckj,t1_hkn6d46,Yeah dude this passed into general slang with some semantic shifts a long time ago lmao,1,0,0,False,False,False,1636933930.0
qtrrml,hknoyz1,t1_hkn6d46,It has become a popular meme,1,0,0,False,False,False,1636935547.0
qtrrml,hkpicuv,t1_hkn6d46,"say chad to a youth and theyll think of that buffed up image of that mascular guy, it was a meme thing. chad = powerful and worthy.",0,0,0,False,False,False,1636975602.0
qtrrml,hkopo33,t1_hkn2wl9,"Personally I don't find them that beneficial for those cases either, but it's a personal preference: I find it easier to spot systemic flaws in an architecture if I have to study it manually than if I can just ""jump around"" in an IDE and not fully suffer the consequences of how the project is structured.

I can see there's a point to them for very verbose languages like Java, just to save some typing, but I'm not sure they provide a gigabyte of value for that.",1,0,0,False,False,False,1636953485.0
qtrrml,hklrusx,t1_hklk42n,What was it?,2,0,0,False,False,False,1636908070.0
qtrrml,hkp4fvc,t1_hkp4f85,8 miles is 12.87 km,1,0,0,False,False,False,1636963817.0
qtrrml,hkp4weo,t1_hkp4f85,"Oh, holey hell!",1,0,0,False,False,False,1636964187.0
qtrrml,hkp4fra,t1_hkp4f85,8 miles is  41133.29 RTX 3090 graphics cards lined up.,0,0,0,False,False,False,1636963814.0
qtrrml,hkp4g3u,t1_hkp4ff5,8 miles is 12.87 km,1,0,0,False,False,False,1636963822.0
qtrrml,hkn8oft,t1_hkln41o,"Yea, but a teacher would be experience at the type of problems students ask. That is his domain. The problems uni students come up are not really that “random”
I am happy that op has a good teacher.",38,0,0,False,False,False,1636928494.0
qtrrml,hklnwry,t1_hkln41o,"It is skill, for sure.",15,0,0,False,False,False,1636906396.0
qtrrml,hkohdtu,t1_hknti47,"So he uses nano or vi? And then he complies using the CLI G++? You guys learning C++?

I had a pretty good professor who was comfortable enough to type out code demos in lecture. It kept the class fun and engaging. I think he used an IDE, though.

I wouldn't mind using a terminal text editor, but I think I gotta have color syntax highlighting, minimum feature....",7,0,0,False,False,False,1636948858.0
qtrrml,hkmzxpr,t1_hkmj762,"Nothing, I use it all the time",3,0,0,False,False,False,1636925035.0
qtrrml,hkpmdz2,t1_hkmj762,Nano is superior.  s/,2,0,0,False,False,False,1636978524.0
qtrrml,hkoppqh,t1_hkmj762,Everything. It isn't Hedit.,3,0,0,False,False,False,1636953513.0
qtrrml,hkpyj01,t1_hkov9nx,Doom nvim is very nice. I was using doom emacs but Doom nvim meets my needs better.,1,0,0,False,False,False,1636985358.0
qtrrml,hkph1by,t1_hkoo1ap,"If it's on Linux, gedit is labelled just ""Text Editor"" at least on the Gnome desktop (kind of like Gnome calls the Nautilus file manager ""Files"", etc.), so it might be that.",2,0,0,False,False,False,1636974554.0
qtrrml,hknr19e,t1_hknggmx,"Well that is probably because the profs I had in the last years always prepared the code and students were still able to find mistakes or criticize it. They also often replied to questions with ""I'll research that once I get home and tell you next lecture"" or would think for a quite long time and experiment a little before getting it right. I mean we are 400 students and a few already are experts at the field and simply need a degree so some of the questions are very specific and rather complicated. I am just amazed how he seems to immediately know the answer and starts typing.",11,0,0,False,False,True,1636936469.0
qtrrml,hklz6am,t1_hklrusx,"just a bunch of unnecessary stuff like ""20 lines of code?? wow!!""",1,0,0,False,False,False,1636911071.0
qtrrml,hkp4gij,t1_hkp4fra,8 miles is 12.87 km,1,0,0,False,False,False,1636963831.0
qtrrml,hkok1cb,t1_hkn8oft,And a teacher would be more experienced than a student who has taken some courses rather than taught.,7,0,0,False,False,False,1636950248.0
qtrrml,hkp0tv0,t1_hkohdtu,"Until I have achieved flow, I want rainbow ide's for training wheels. And even then, i might still want rainbows",1,0,0,False,False,False,1636960992.0
qtrrml,hksktqb,t1_hkohdtu,Pretty sure he uses gcc for compiling. We are learning a few advanced Java methods and the basics of Haskell and Prolog with him this year.,1,0,0,False,False,True,1637024295.0
qtrrml,hkq0fd5,t1_hkpyj01,that's a thing?? :O gonna check it out,1,0,0,False,False,False,1636986279.0
qtrrml,hkslyd8,t1_hkph1by,Yeah exactly. I probably should've explained what I mean better 👍,1,0,0,False,False,True,1637024798.0
qtrrml,hknz2jz,t1_hknr19e,"Congratulations, you are finally getting what you're paying for, a competent professor.",12,0,0,False,False,False,1636940229.0
qttk9c,hkoeqbq,t3_qttk9c,"This is the trend for every single development in the industry.

Probably the most recent large one that maybe goes unnoticed is the migration to the cloud. 10 years ago, I would never imagine not having a server room. Now, the idea of physical machines hosting your application is probably something very foreign to people, and of course it was resisted immensely at first.",12,0,0,False,False,False,1636947536.0
qttk9c,hkmtd5a,t3_qttk9c,See also [The Story of Mel](http://www.jargon.net/jargonfile/t/TheStoryofMel.html).,6,0,0,False,False,False,1636922451.0
qttk9c,hko9aac,t3_qttk9c,"Throughout time, the acceptance and adoption of new technology has always followed this pattern: 

Denial, Anger, Bargaining, Depression, Acceptance. 

Oddly enough, this is also happens to be the five stages of grief. Go figure.",4,0,0,False,False,False,1636944926.0
qttk9c,hkmbk4g,t3_qttk9c,Not relevant but it reminds me of a historian who claimed ancient writing was purposely more complicated than it needed to be because the scribes would benefit.,8,0,0,False,False,False,1636915811.0
qttk9c,hkofh83,t3_qttk9c,"That’s funny. On the other hand the first Lisp compiler and interpreter was implemented by a grad student who was tired of having to write Assembly code.

John McCarthy told his grad student: ""You can't make a computer language out of a notation language."" 

Grad Student: Lmao.",3,0,0,False,False,False,1636947899.0
qttk9c,hkpa4ra,t3_qttk9c,"it's the same stuff programmers (usually the narrow minded or ignorant ones) have been saying for most new language or framework...

This is similar to the mindset that older programmers who come from C/C++/Java backgrounds call younger programmers coming from a web dev, JS, python background ""script kiddies""",2,0,0,False,False,False,1636968653.0
qttk9c,hkqcf8n,t3_qttk9c,Thank you for posting this! Very very interesting,2,0,0,False,False,False,1636991599.0
qttk9c,hkp769t,t1_hko9aac,What stage would you say Bitcoin/Crypto is in currently?,1,0,0,False,False,False,1636966076.0
qttk9c,hkmq6u9,t1_hkmbk4g,You don't need to go that far. Try studying Law. Completely unnecessary verbose just to gatekeep others from understanding it so they remain necessary.,24,0,0,False,False,False,1636921229.0
qttk9c,hkpvg37,t1_hkpa4ra,"scripts are actually awful, its so unnorganized and all over the place. i was literally horrified at some of the spaghetti python code a peer had done (they were only 4 years younger than me, their code was very affective, but i dont think they thought about someone else being able to read and understand it).

JS as a script has its place because web.",1,0,0,False,False,False,1636983795.0
qttk9c,hkpnn9x,t1_hkp769t,"IMHO, bargaining. Some adopters, but not mainstream yet. When it reaches about 45% adoption, we'll start to hear all sorts of sad stories about it. That's when you'll know it's in the depression phase.",1,0,0,False,False,False,1636979351.0
qttk9c,hkn0ink,t1_hkmq6u9,Yeah but my example highlights that we've been avoiding getting replaced for 6000 years.,3,0,0,False,False,False,1636925248.0
qttk9c,hkn7kdw,t1_hkn0ink,Wasn't it Socrates that claimed that writing was bad because people wouldn't learn to memorize? (Which we learned through Plato's writings),5,0,0,False,False,False,1636928030.0
qttk9c,hkpuw9f,t1_hkn7kdw,its ironic that we (mankind) only 'remember' this now because it was written down.,1,0,0,False,False,False,1636983508.0
qu8lzi,hkpjfgv,t3_qu8lzi,[Good video here.](https://youtu.be/0oDAlMwTrLo),2,0,0,False,False,False,1636976426.0
qu8lzi,hkq59h8,t3_qu8lzi,CS50 week 3 :),1,0,0,False,False,False,1636988513.0
qu4c71,hko9khj,t3_qu4c71,"I think the fundamental misunderstanding here is between the difference in read / write operations. A mechanical arm responding to an electrical impulse within the brain specific to the area that lights up when someone tries to move their arm is much easier to read and mimic than it would be write some information to the brain. 

We are beginning to understand how to read impulses from the brain for simple operations. But we have no idea how to write back to it.",5,0,0,False,False,False,1636945061.0
qu4c71,hkordik,t1_hko9khj,">But we have no idea how to write back to it.

We sorta do, conceptually. Education is all about this. As is martial arts, and sports. In fact, anyone who has ever learned anything and gone through the process of creating a repeatable ""function"" (muscle memory* is the phrase I've most often heard) out of something that used to require much more attention during the learning phase is then aware that some kind of self-write operation is a part of daily life and learning new things, beyond just being a passive sensor. If we can map what happens in the brain during other things, then it stands to reason that we could map what happens in the brain when people are intensively learning and training themselves. The idea of insta-grok technology does not seem far fetched to me, but I'm not an expert. I would add the important caveat that the brain is complex and consciousness even more so. It would probably take far longer to perfect and understand how to use such technology than it would to get it going in the first place.

I am not sure I would replace the learning process itself with a Matrix style ""insta-download-to-your-brain"" system, but it would be cool to have more control over the process which we already use when teaching ourselves things. Oftentimes training is like playing the part of both the animal and the trainer at the same time, and even if you're a smart trainer you might still also be a stubborn animal. Some people's muscle memory (and neuroplasticity, for that matter) is less stubborn than others, and easier to tame for this purpose or that. I personally enjoy the process of learning and practice, but I would enjoy having more control over it. Perhaps current Sci Fi depictions only scratch the surface of what cybernetics and biotech could allow, in theory. 

*Muscle Memory is not simply memorization. It is creating repeatable functions which you then use for higher level tasks, like tools in a tool belt.",2,0,0,False,False,False,1636954530.0
qu4c71,hkoryks,t1_hkordik,"One I enjoyed your response.

But if I can, the question isn’t that we don’t understand the general process of repetition leading to acquired skill or knowledge. The question is what the actual underlying physiological process occurring during repetition is. And whether or not we could repeat that. 

I see that as a far stretch from mapping some part of the brain which lights up when a person attempts to lift their arm.

Certainly not impossible, but well beyond our current knowledge of how the brain works. Unless you listen to Elon musk of course. Then we’ll have all of this done in no time",2,0,0,False,False,False,1636954895.0
qu4c71,hkqhzjg,t1_hkordik,Did you just coin the word insta-grok? I love it.,1,0,0,False,False,False,1636993878.0
qu4c71,hkostqd,t1_hkoryks,"I don't think repetition alone teaches a person much. To use the sports and martial arts analogy again: you want to build mental, logical, conditional reflexes for different situations in addition to building the muscle memory of the actions themselves. In general this means applying what you've learned in a variety of situations and getting ""practice"", which is distinct from repetition, although repetition is a byproduct of practice. A surprising amount of, say, Math is about building this kind of thing so that when you see a certain kind of problem the right solution hits you like a reflex. Intuition comes from practice.

I'm not a doctor or an expert programmer (or expert anything), so I'm not sure what the current ""resolution"" limits are for making sense of brain scans. I'm skeptical because it is hard to measure the conscious meaning of even a face expression from a friend, nevermind a gazillion neurons being examined in n-dimensions of data analysis. That said, it seems like a surmountable task with enough effort, given that we have already mapped out so much.

Edited for typo.",1,0,0,False,False,False,1636955442.0
qu4c71,hkr2uwu,t1_hkqhzjg,"I didn't coin the word *grok* (which is from a Heinlein book I've never read and means something like ""to grasp a concept""), but I will happily take credit for *insta-grok* if nobody else has said it before. I'd be shocked if I were the first though, as the word has been around for about as long as microwaving food. 

Glad you enjoyed!",2,0,0,False,False,False,1637002137.0
qu4c71,hkr8f9f,t1_hkr2uwu,"It's from Stranger in a Strange Land which was a counter culture hit in the 60s & so the word grok entered the lexicon. But insta-grok was new to me.

Edit: I just searched & it's already in use as a website name.",2,0,0,False,False,False,1637004350.0
qu4c71,hkr99zy,t1_hkr8f9f,Glad to hear I'm not the first! It would be a good brand name for all kinds of things. Thanks for reminding me which book it was from. I read Starship Troopers as a kid but that's the extent of my Heinlein.,1,0,0,False,False,False,1637004690.0
quec2x,hkrxhy7,t3_quec2x,"I'd say the DQN paper is good to know for games, https://arxiv.org/abs/1312.5602",2,0,0,False,False,False,1637014199.0
quk6my,hkrfgi3,t3_quk6my,"Computer science: the scientific discipline of information processing, includes a lot of theory about for instance computability theory, complexity classes, abstract machine models like finite state machines and turing machines, ... also lots of math

Programming: the process of writing code, i.e. text that is in such a language that a computer is able to understand it

Software engineering/developing (don't know of any difference): the discipline of creating software, this involves programming, but also a LOT more, like organization, planning, designing architecture that fulfills quality standards and allows expanding and changing the software later, testing and QA",7,0,0,False,False,False,1637007124.0
quk6my,hkqurbq,t3_quk6my,"From what I know is computer science is a degree which is useful to software engineering and can help you, but a computer science degree isn't required to get a software engineering job

Software engineering/software developer is a job that targets in building, analyzing, developing new things by Programming.

Programming is a process by making a program with code.

Sorry for bad English and I'm just new to these stuff. You guys can correct me, it would be an honor.",2,0,0,False,False,False,1636998951.0
quk6my,hkqwucd,t3_quk6my,"10 years ago Computer Science was/is a degree that focuses on theories and data structures. Computer engineering focused more on hardware/electronics. These may have changed since I got out of school.

Programmer/software engineer/software developer differences are going to vary from place to place and often are interchangeable. When different generally a engineer focuses more on designing solutions and developers focus more on producing.",2,0,0,False,False,False,1636999771.0
qu426c,hknuz9m,t3_qu426c,"LaTeX is like vim. If you really know how to use it, it's just so much more powerful and easy to use tool, you don't want to touch anything else.",22,0,0,False,False,False,1636938330.0
qu426c,hknzcxp,t3_qu426c,Simple: Word and GDocs equations look like absolute dog shit in comparison.,17,0,0,False,False,False,1636940363.0
qu426c,hknw6nl,t3_qu426c,"Also, and perhaps more importantly: LaTeX lets you write once and use anywhere, which is important if you want to submit your work into a journal that used a different format than you did, and then also submit it to a second different journal that uses a different format than the first one. You don't actually have to do very much.",14,0,0,False,False,False,1636938897.0
qu426c,hko38xx,t3_qu426c,"The primary reason is that in LaTeX you can put in mathematical equations and they print out like mathematical equations. Try doing that (properly) with Word. Second reason, it will print out the way you define it, Word can’t even guarantee your text will be on the same page even if it is in the same versions across platforms.",12,0,0,False,False,False,1636942144.0
qu426c,hkootrh,t3_qu426c,"When you have a large and complex technical document, latex manages figures, tables, citation and equations without  moving your hand from the keyboard, which is very efficient when you know the syntax.",6,0,0,False,False,False,1636952977.0
qu426c,hkr0hgy,t3_qu426c,"As other have mentioned LaTex still does MANY things well that Word/Gdocs does not. And that goes well beyond just equations. I have used it extensively for papers which rarely included equations, but here are the some of the most important factors I found:

* Automatic generation of bibliography with references in paper order. ALL papers in every field have citations/references and numbers range from tens to hundreds. Being able to collect that in a bib file and reference a cite with a simple function is worth its weight in GOLD.

* Separation of content/structure from formatting. When writing in LaTex you write the content with minimal structural markup (like sections/subsections). Almost ALL the formatting like fonts, page layouts, etc are separate and usually GIVEN to you by the journal/conference/publisher so you don’t spend much if any time fiddling with it. For those familiar with it LaTex splits content from style kind of like HTML and CSS.

* Simple ASCII file format that is human readable makes it possible to recover from mistakes/corruption as well as collaborate using revision control systems designed for code. When using a binary format or machine generated XML, this is impossible. If a tool screws something up you are up shit creek and may need to rebuild from scratch. When working with large documents like a thesis or book, this would be a disaster.",6,0,0,False,False,False,1637001208.0
qu426c,hkp04m2,t3_qu426c,"Aside from the features you mentioned, it seems still nothing does justification as well as tex/latex, even after all these years.",3,0,0,False,False,False,1636960470.0
qu426c,hko2mke,t3_qu426c,"I am not a LaTeX user but I usually use Markdown (also a Markup language) to quickly write some text with formatting.
The advantage of using markup language for formatting is that your hands do not need to leave the keyboard to do formatting. If you are familiar with most syntaxes, you can do formating much more efficiently than using GUI to do so. Markdown, for example, let's say I would like to make a quote block or code block. I can simply type '>' or '```' to create one quickly and start inputting what I want. In Microsoft Word, I'll need to perform additional steps to create similar things.
From what I know LaTeX is widely used to make mathematical formulas (correct me if I'm wrong). As I don't know how to use LaTeX, whenever I want to create a mathematical formula, I have to manually add symbols by choosing the list of symbols in the menu. It is definitely inefficient to do so.",2,0,0,False,False,False,1636941862.0
qu426c,hkpa3ws,t3_qu426c,"I think most people answered the actual reasons well, but I’d just like to add it can be fun. I don’t know about y’all, but it feels really satisfying to manually specify the proper formatting and then have it appear. Then again, it can also be incredibly frustrating too.",2,0,0,False,False,False,1636968632.0
qu426c,hknvrfe,t3_qu426c,"Ew, no. I don't want to hand-draw everything in a GUI, are you kidding me?

Just because it's hard for you doesn't mean it's hard for everybody.",-2,1,0,False,False,False,1636938698.0
qu426c,hkqipbj,t3_qu426c,Control,1,0,0,False,False,False,1636994169.0
qu426c,hkz30sz,t3_qu426c,legacy stuff. journals only accept latex. also the lack of competition. latex is dogshit but its all we can use,1,0,0,False,False,False,1637146809.0
qu426c,hknw33r,t1_hknuz9m,"I see, thanks.",3,0,0,False,False,True,1636938850.0
qu426c,hko3pml,t1_hko2mke,Many Markdown environments are LaTeX extended. Wrap with double dollar signs and try it out. $$6x_2 \geq x_1^2$$,1,0,0,False,False,False,1636942356.0
qu426c,hknw17w,t1_hknvrfe,"I never said it was hard, i simply asked why it was used....",9,0,0,False,False,True,1636938824.0
qt4qqy,hkh6t44,t3_qt4qqy,This photo of spaghetti is making me hungry...,57,0,0,False,False,False,1636824081.0
qt4qqy,hkh2gmz,t3_qt4qqy,before her suicide...,26,0,0,False,False,False,1636822241.0
qt4qqy,hkh9f0f,t3_qt4qqy,The amount of “nope” associated with the general lack of labels is off the charts,20,0,0,False,False,False,1636825174.0
qt4qqy,hkh708z,t3_qt4qqy,That is like a scene right out of Star Trek. That's awesome.,14,0,0,False,False,False,1636824164.0
qt4qqy,hkhcc6l,t3_qt4qqy,Beautiful colors. I love how gentle film photography is,5,0,0,False,False,False,1636826413.0
qt4qqy,hkij7yj,t3_qt4qqy,"Ugh that just gives me a headache. Much love to her, for doing something I would dread doing.",2,0,0,False,False,False,1636844752.0
qt4qqy,hkjegjx,t3_qt4qqy,So much better [now](https://www.reddit.com/r/pcmasterrace/comments/5r1cr4/cable_management_from_the_depths_of_hell/)...er...,2,0,0,False,False,False,1636858656.0
qt4qqy,hkl8qwm,t3_qt4qqy,Cable management would like a word with you,2,0,0,False,False,False,1636899441.0
qt4qqy,hkhsdh2,t3_qt4qqy,Imagine getting segfault on this.,3,0,0,False,False,False,1636833313.0
qt4qqy,hkhg9gn,t3_qt4qqy,meanwhile the students in my digital logic class are struggling to make a 4 bit adder,-1,1,0,False,False,False,1636828092.0
qt4qqy,hkif9zq,t3_qt4qqy,Are you joking? Is she from Star Trek?,1,0,0,False,False,False,1636843058.0
qt4qqy,hkk37em,t3_qt4qqy,my uncle did stuff like this,1,0,0,False,False,False,1636871399.0
qt4qqy,hkk66oz,t3_qt4qqy,"I wonder, if connecting lines on the modern circuit board represents the wires on this photos",1,0,0,False,False,False,1636873408.0
qt4qqy,hkkh7ul,t3_qt4qqy,This photo makes you really appreciate PCBs.,1,0,0,False,False,False,1636881391.0
qt4qqy,hkl3ceg,t3_qt4qqy,"I guess this is basically FPGA equivalent?

(also, all that work to colourize, and they left her arm like that? strange choices..)",1,0,0,False,False,False,1636896668.0
qt4qqy,hkl49yr,t3_qt4qqy,r/forbiddensnacks,1,0,0,False,False,False,1636897140.0
qt4qqy,hkkb5we,t3_qt4qqy,What a cool kitchen,-2,0,0,False,False,False,1636876930.0
qt4qqy,hkjxd0l,t3_qt4qqy,[deleted],0,0,0,False,False,False,1636867961.0
qt4qqy,hkjsokj,t1_hkh2gmz,Can you share something more about this? I googled but there aren't much information about her life. She doesn't even have a wiki page :/,6,0,0,False,False,False,1636865521.0
qt4qqy,hkkygvg,t1_hkh9f0f,Imagine trying to run those wires through panels!,3,0,0,False,False,False,1636893905.0
qt4qqy,hkhhqr2,t1_hkhcc6l,"The colors are fake. This photo was black and white, then colorized (poorly). Notice the gray on her arms, and behind her ear.",17,0,0,False,False,False,1636828729.0
qt4qqy,hkkok7m,t1_hkjxd0l,The wires are the source code,1,0,0,False,False,False,1636887028.0
qt4qqy,hklhz4p,t1_hkjsokj,I think it was a joke? I could not imagine troubleshooting a panel like this. You would have better a chance sending it to an Italian bistro for repair with all that fucking spaghetti,2,0,0,False,False,False,1636903796.0
qt4qqy,hklswtr,t1_hkkygvg,The horror,2,0,0,False,False,False,1636908512.0
qt4qqy,hkhi6e5,t1_hkhhqr2,I too would call it quits after tracing all those wires.,13,0,0,False,False,False,1636828919.0
qt4qqy,hkm86pk,t1_hklhz4p,was a joke...,2,0,0,False,False,False,1636914567.0
qt4qqy,hkqnc19,t1_hkhi6e5,Pretty sure ML does that,1,0,0,False,False,False,1636996010.0
qtl1ir,hkw87zo,t3_qtl1ir,"big O is always an approximation.  It is not a measure of how long something takes but rather a measure of how fast the time it takes grows as n grows.

The reason for the approximation is to make the math easier.

Check out below for a more complete explanation of how 

https://www.baeldung.com/cs/fibonacci-computational-complexity",1,0,0,False,False,False,1637092971.0
qtkimi,hknwkyu,t3_qtkimi,"I’ll give some unconventional advice here. Check out Arduino, it’s a dev board based on the atmega328p microcontroller. A microcontroller is essentially a tiny computer, this particular one has 32kb flash memory, 2kb ram, and runs at 20mhz. But they’re simple, especially compared to a modern PC and operating system, much easier to grasp everything that’s going on. Typically you run them without an operating system, your code just goes right on top of the hardware and you have full access to all of the registers, you can set and read the voltages on the pins, you handle interrupts yourself, etc. You learn a lot this way.

Arduino is a good place to start, it mainly just provides a nice interface for flashing code into the microcontroller, but once you get a little more familiar with it you can use the microcontroller on its own on a breadboard, then use a programmer like the atmel-ice and AVRstudio to flash code into it, AVRstudio also has an assembler you can use as well.

After you get familiar with that you can move upto a multicore family of microcontrollers like stm32 and get exposure to concurrency this way.

The reason I recommend this route is I think these topics are best learned in their simplest possible forms and the PC ecosystem is not really the place to find that. I also think computing is best understood bottom-up instead of top-down.

There’s Arduino kits such as [this](https://www.amazon.com/ELEGOO-Project-Tutorial-Controller-Projects/dp/B01D8KOZF4/ref=mp_s_a_1_1_sspa?crid=2RW64OFAM05ZF&keywords=arduino+kit&qid=1636939028&smid=A2WWHQ25ENKVJ1&sprefix=arduino+kit%2Caps%2C127&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEzMkY3OFBNOTNHQUpSJmVuY3J5cHRlZElkPUEwODcwNzExMVRWOEdTTzhPSlRZRiZlbmNyeXB0ZWRBZElkPUExMDAxMzc2M1RTT1dKR0NNR05TQiZ3aWRnZXROYW1lPXNwX3Bob25lX3NlYXJjaF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl) to start with.

You can get the [Arduino IDE](https://www.arduino.cc/en/software) here. It’s C++.

There’s no hello world because it doesn’t have a screen but the equivalent would just be a program to blink an LED.

    void setup()
    {
        pinMode(LED_BUILTIN, OUTPUT);
    }

    void loop()
    {
        digitalWrite(LED_BUILTIN, HIGH);
        delay(1000);
        digitalWrite(LED_BUILTIN, LOW);
        delay(1000);
    }

setup() runs at startup

loop() runs immediately after and is looped forever until power down, there’s no operating system to return to so your program can never end.

pinMode() just sets a pin as output or input, you’re either outputting a voltage on any particular pin or reading it.

LED_BUILTIN is just a constant for the pin connected to an LED on the arduino. Equal to 13 in this case.

digitalWrite() sets the state of a pin configured as an output.

HIGH and LOW are just constants for digital high and digital low, equal to 1 and 0 respectively.

delay() just loops the cpu for a given amount of milliseconds.

You can flash your code to it by hooking it up to a USB port, selecting that port in the Arduino IDE and then hitting upload.

r/electronics is good if you need help with the circuitry aspects.

r/embedded is good if you need help with programming it or the hardware concepts.

[atmega328p datasheet](https://www.sparkfun.com/datasheets/Components/SMD/ATMega328.pdf)

[AVR instruction set](http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf)",3,0,0,False,False,False,1636939083.0
qtkimi,hkljpo9,t3_qtkimi,"""Operating Systems Concepts"" by Silberschatz is probably what you want to start with. 

And of course, simply googling for material on each topic will yield mountains of reading material for you.",2,0,0,False,False,False,1636904562.0
qtkimi,hkohkop,t1_hknwkyu,"That is very cool, and really in a way, a small dream coming true - ever since my cyber days I wanted to utilize microcontrollers - and I didn't even think for a second to get one for these purposes.  


I will def consider getting now, as I get myself a book on the subject",1,0,0,False,False,True,1636948956.0
qtkimi,hklqkf9,t1_hkljpo9,Reading that right now for OS classes. Its a hard class and a struggle sometimes but I feel a lot more confident about programming after reading this. The author explains concepts very well,2,0,0,False,False,False,1636907525.0
qtkimi,hkoh95k,t1_hklqkf9,"Looks great! I will start diving right into it. Its just that googling gets me all over the place..   
But I was looking for something more linear that could help me establish solid grounds. Thanks for the recommendations!",1,0,0,False,False,True,1636948791.0
qtg70j,hkjd4xx,t3_qtg70j,See [One's Compliment](https://en.wikipedia.org/wiki/Ones%27_complement).,1,0,0,False,False,False,1636858056.0
qtg70j,hkk0ho4,t3_qtg70j,"Someone mentioned it in the thread, but using ones complement (inverse of a hex string) will give you the negative number",1,0,0,False,False,False,1636869748.0
qtg70j,hkkd3k6,t3_qtg70j,‘8’ is a hex number that begins with 8; is a multiple of 4; is positive.,1,0,0,False,False,False,1636878327.0
qtg70j,hkktnc9,t3_qtg70j,"In the pure sense; hexadecimal is a number system, so a negative hexadecimal number would be -0x…. 

However what your referring to is probably a byte type, in which 0x80 is 128 which is the start of the negative range.

Unless it’s 1 complement which I’m not familiar with.",1,0,0,False,False,False,1636890691.0
qtg70j,hkpkxiu,t3_qtg70j,"Recall that the sign of a binary number is determined by its most significant bit (number furthest to the left). Every hexadecimal digit is represented as 4 binary bits. For example, Hex digits 0 through 7 in binary are 0000 through 0111. On the other hand, starting at 8 we have 1000 through F is 1111, where the most significant bit is always 1 and which would indicate negative. Therefore, any hex number whose first digit is at least 8 is negative.",1,0,0,False,False,False,1636977517.0
qtg70j,hkjd6c2,t1_hkjd4xx,"**[Ones' complement](https://en.wikipedia.org/wiki/Ones'_complement)** 
 
 >The ones' complement of a binary number is the value obtained by inverting all the bits in the binary representation of the number (swapping 0s and 1s). This mathematical operation is primarily of interest in computer science, where it has varying effects depending on how a specific computer represents numbers. A ones' complement system or ones' complement arithmetic is a system in which negative numbers are represented by the inverse of the binary representations of their corresponding positive numbers. In such a system, a number is negated (converted from positive to negative or vice versa) by computing its ones' complement.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1636858074.0
qtg70j,hkjnf6u,t1_hkjd4xx,"Not sure what the OP's ""and it's a multiple of 4"" is about, but most machines now use 2's complement.  Otherwise, either form uses the high-order bit as sign, so in hex that would be spelled as \[8-F\] in the high-order nibble.",1,0,0,False,False,False,1636862988.0
qtg70j,hkqnzh0,t1_hkkd3k6,"Well, technically it’s negative in 4bit representations, that’s why we precede it with 0s",1,0,0,False,False,False,1636996273.0
qtg70j,hkkybgl,t1_hkjnf6u,"Yeah, I was just referring to that page to show how the MSB can represent the sign bit for any bit-sized integer value, which is essentially what the OP was asking IMHO.",1,0,0,False,False,False,1636893806.0
qtg70j,hklxq9a,t1_hkkybgl,That's what I assumed.  I just thought he might  go down a 1's complement rabbit hole without a little more context.,2,0,0,False,False,False,1636910485.0
qtet0i,hkj6yj2,t3_qtet0i,"To be this seems to get a few things mixed up at once.

O(nk) can be as said on SO an outer loop with n iterations and an inner loop with k. In the case of k = n you get O(n^2 ).

O(n+k) would be on for loop with n iterations followed by a for loop with k iteration, again if k = n then you'd get O(2n) ~ O(n).

I then don't fully understand what you mean with n = 10 and k = 0.

So in the case of O(n^k ) which you could think of k nested for loops with n iterations, if k = 0 it would mean there are no nested loops, in fact no loops at all -> O(1)

In the cubic sense of O(n^2 ) there's no dependency to k. Now if you'd consider O(nk) though you'd have one loop with n iterations and a nested loop with 0 iterations. Now if the second loops doesn't do anything -> O(1).
Might be easier to think of it as the loop with k iteration being the outer loop.

In both these cases if k = 0 the code will not scale with larger inputs and therefore finishes in constant time or O(1). With that said k = 0 to me seems like more a theoretical exercise than anything else.",3,0,1,False,False,False,1636855247.0
qtet0i,hkjmokx,t1_hkj6yj2,"thanks a lot!

&#x200B;

>theoretical

yea it was just an assumption to understand the difference but like you said there is no nested loops if k is 0 so it's clear to me now!😊",1,0,0,False,False,True,1636862633.0
qtapfy,hkjc4kd,t3_qtapfy,"https://ieeexplore.ieee.org/Xplore/home.jsp

Some of it is free. A lot of it is very technical",2,0,0,False,False,False,1636857594.0
qt3e8y,hkgtz8e,t3_qt3e8y,"Computers can understand any base that they are designed to. However, it turns out that representing numbers in a base 2 system is currently electrically optimal.

Computers don’t look at a number and try to understand what value it is. A computer is a combination of digital circuits that apply a set of predetermined transforms on a set of signals, in our case these are binary signals. 

I think the best thing to help your understanding is to look at how some of the simple digital circuits operate. Google around and look at an adder circuit, digital comparator, and multiplier. This should help you understand that computers don’t “know” things, they just operate or compute.

This is a good place to start: https://en.m.wikipedia.org/wiki/Adder_(electronics)",23,0,1,False,False,False,1636818522.0
qt3e8y,hkgsfsu,t3_qt3e8y,I also want to know and look forward a valuable answer. Thanks!,2,0,0,False,False,False,1636817807.0
qt3e8y,hkh0bem,t3_qt3e8y,"Imagine for example, we have two addition problems,. let's say 125+67 and 12+43. We do not do anything special in either case. There is a definitive algorithm on how to add 2 numbers and we just apply it to both problems. We do not have to recognise the numbers to carry out the addition. This is also what a computer does. It just carries out whatever algorithm we tell it to on the values we give it. This is just a basic example, but I hope this gets the idea across.",2,0,0,False,False,False,1636821303.0
qt3e8y,hkjfoye,t3_qt3e8y,Guy there’s 1000 page text books written about this.,1,0,0,False,False,False,1636859252.0
qt3e8y,hkieop2,t3_qt3e8y,"How does the computer know which one is 1, 2, 4 etc? Well that's depending on the prcoessing unit's architecture. It depends on how the words are stored in memory. Usually it's MSB (most significant bit) on the left and LSB (least significant bit) on the right. But it could technically be the otherway around.

So in memory at a random adress 0xCAFE with MSB first it'll look something like this (given this represents an unsigned integer, for other cases check out two's complement for signed or IEEE 754 for floats)


 | 2^3 | 2^2 | 2^1 | 2^0 | at 0xCAFE


Now does the CPU care what this binary represents? The sinplified answer is no. It will take this binary number and feed it to whatever place we tell it to. 
E.g. when we tell it to add 1 it will feed it into the adder and do that. The interpretation of what this binary number is, happens on higher levels, also know as the code we run on the hardware.

Example for that, you know how you can open a binary file in an editor? It just looks like random gibberish and that's because we take 0s and 1s that don't represent text (ASCII or any other encoding) and try to display it as such.",1,0,0,False,False,False,1636842812.0
qt3e8y,hkjirfh,t3_qt3e8y,"Computers do not know which binary values represent which numbers. Computers do one thing: they take an input signal (a string of electrical pulses) and return an output signal. So how does a computer know what to do with the input signal? A computer engineer physically designed the computer to perform *the most fundamental computer actions* the technical name is called the **Instruction Set Architecture opcodes**. A computer programmer is a person who is trained to understand the meaning of the ISA opcodes, programmers will write a computer program that conforms towards the computer opcodes. The computer program they write is also a long string of input signals that the computer will perform.

So you can input any string of binary values into the computer and the computer will perform an action according to the definition of the computer opcodes. This doesn't mean that the computer understands what it is doing, it's just performing computations because it was designed to work that way. So when you enter a number that's supposed to represent a number value, the computer doesn't understand this at all. It is supposed to be the programmer's responsibility to give meaning to the binary string value: it is the programmer who is interpreting and assigning meaning to the binary string value.",1,0,0,False,False,False,1636860742.0
qt3e8y,hkwe6m8,t3_qt3e8y,i suggest going through the nandgame. it really cleared a lot of this stuff up for me.,1,0,0,False,False,False,1637095318.0
qt3e8y,hkgv8m9,t1_hkgtz8e,Username checks out :O,13,0,0,False,False,False,1636819091.0
qt3e8y,hkh4f2q,t1_hkgtz8e,This is wonderfully explained answer.,2,0,0,False,False,False,1636823078.0
qt3e8y,hkhd3s7,t1_hkgtz8e,Thank you\~,2,0,0,False,False,False,1636826734.0
qt3e8y,hkif19x,t1_hkieop2,ffs I can't get that formatting right for the number on mobile sorry for that.,1,0,0,False,False,False,1636842957.0
qsn40p,hkegq7h,t3_qsn40p,"> That said, he then explained us that for that reason (not every state with an input I will end up in a steady state) we need an external synching (a clock) and a memory to make a state steady. Why is that? And why don't we need that for asynchronous circuits?

It's kind of by definition. The use of the clock's edge to trigger input/output latching is what makes a circuit synchronous. Whereas an asynchronous circuit is a ""natural"" electronic one that is always in flux.


So a synchronous circuit is the normal stuff you get in digitial logic - a clock, lots of flip-flops, and then random logics gates between the flips, usually defined by a state machine.

An asynchronous one can be made up of things like [Muller C-Gates](https://en.wikipedia.org/wiki/C-element).


I don't know of a good resource to direct you to for this. Do you know much digital logic? Asynchronous circuits is quite an ""advanced"" topic there, but understanding basic digital circuits and you understand the synchronous case.",5,0,0,False,False,False,1636764192.0
qsn40p,hkegrlh,t3_qsn40p,"First note that synchronous and asynchronous only apply to sequential logic, not combinational logic.

In a synchronous circuit, the inputs to a stage are sampled only when the clock signal goes active.  So the clock needs to be slow enough that all inputs are valid when the clock goes active, otherwise the output may be wrong.

In an asynchronous circuit, the inputs to the stage are sampled either all the time, or when it is known that all inputs are valid.  The former solution can have glitches or race conditions if the circuit is not designed properly.

In short, it's all about how to knowing when your input data is valid.  Either wait long enough, or have something tell you when it is valid, or design your circuit so that transitory invalid inputs don't make a difference.",5,0,0,False,False,False,1636764211.0
qs8dso,hkbghmo,t3_qs8dso,"Computer science is more theoretical, e.g. analyzing and comparing algorithms. Programming is more practical, with specific details about a particular programming language. Sort of like the difference between a Linguistics class and a French class.

The course descriptions should provide more specific details on the two classes.",141,0,0,False,False,False,1636713465.0
qs8dso,hkbgh2d,t3_qs8dso,"Most unis typically get it wrong, and have computer science as 'programming for good students' and programming/software as 'programming for weak students but we still want their money'. So the real definition may not apply here.
 
Computer science is the mathematics discipline that studies computation, which is the storage and manipulation of data. So data structures, algorithms, formal languages/grammars, logic. Programming, or software development/engineering is the *use* of that computation to solve problems in the real world.",106,0,0,False,False,False,1636713451.0
qs8dso,hkbq3so,t3_qs8dso,"Computer science is the what and why, programming is the technically how.",16,0,0,False,False,False,1636720463.0
qs8dso,hkcy0qb,t3_qs8dso,Computer Science is to programming as Physics is to Mechanical engineering,15,0,0,False,False,False,1636740021.0
qs8dso,hkbjg0j,t3_qs8dso,"Others have distinguished the two well but I will say the following.

Studying CS will mold you into a better problem solver. You will be able to analyze problems with analytical reasoning; thus, helping you to become a better programmer. CS is all about learning why and how it works; hence, knowing what happens ""under the hood"" is very beneficial for any programmer. You will have the skill to solve problems efficiently if the need arises and you will have the skill to optimize code as well. With CS knowledge, you will write better maintainable code. 

You can definitely study programming and learn CS along the way as well. So, it's up to you but know that CS is very applicable and very useful to programming.

Best of wishes.",13,0,0,False,False,False,1636715850.0
qs8dso,hkc5hc8,t3_qs8dso,I was expecting a joke. This post disappoints.,10,0,0,False,False,False,1636728299.0
qs8dso,hkbhsse,t3_qs8dso,"In addition to the existing answers:

Roughly the difference between driving and studying the materials science and physics involved in driving.",8,0,0,False,False,False,1636714561.0
qs8dso,hke2o99,t3_qs8dso,"Computer Science is the study of algorithms.  You will learn programming to help you study algorithms.  Essentially, programming languages are a tool and computer science is a means to understand how to use that tool well.",2,0,0,False,False,False,1636757538.0
qs8dso,hkc4743,t3_qs8dso,"Do you go to UT Dallas and are you talking about CS 1200 and CS 1325? Bc then everyone in this comment thread is wrong. One is about what the curriculum teaches and the other is actually learning programming concepts. 

I always tell people to post which courses they’re talking about bc without reading the descriptions no one will know what you’re talking about. People made some pretty wild assumptions.",1,0,0,False,False,False,1636727722.0
qs8dso,hkbub8a,t3_qs8dso,[deleted],1,0,0,False,False,False,1636722875.0
qs8dso,hkbzz3z,t3_qs8dso,"In practical terms, it's like construction vs architecture.",1,0,0,False,False,False,1636725742.0
qs8dso,hkc88pf,t3_qs8dso,Programming is computer science but computer science isn’t necessarily programming,1,0,0,False,False,False,1636729504.0
qs8dso,hkct32d,t3_qs8dso,What's the difference between learning medicine and applying a Band-Aid?,1,0,0,False,False,False,1636738028.0
qs8dso,hkcyipy,t3_qs8dso,"I forget where I heard this analogy but basically like how astronomers study the stars and use a telescope as a tool to do so, computer science and programming can be thought of in the same way. Programming is just a tool we use to study computer science.",1,0,0,False,False,False,1636740226.0
qs8dso,hkd2dp8,t3_qs8dso,I would probably go with CS to understand the *why*; most of my colleges that have taken programming in school have discovered you mostly learn programming on the job.,1,0,0,False,False,False,1636741819.0
qs8dso,hke2qvq,t3_qs8dso,"It is sort of like studying music vs playing the guitar (or any instrument).  


The guitar is a technical skill and you may be able to use it well even without formal music education, but knowing music theory is a new domain. Sure, they interact a lot. Also, there is no much sense in learning musical theory if you don't play any instruments.  
Finally, playing the guitar will probably help you pay the bills, but most of the greatest guitarists know a lot about music theory.",1,0,0,False,False,False,1636757570.0
qs8dso,hke3pfg,t3_qs8dso,"My university has a blanket computer science degree, and several different emphases for specializing. Data science, bioinformatics, software engineering. In addition to some core computer science practices, software engineering at my university covers material related to the development life cycle as well as some practical programming skills like some ides, in depth practice with git, testing and verification, etc. 

I'm just pointing out there can be overlapping material in the labels, and that there's also software engineering.",1,0,0,False,False,False,1636758010.0
qs8dso,hkfbgeh,t3_qs8dso,The same difference as carpentry and architecture. Programming is practical and computer science is more theory.,1,0,0,False,False,False,1636780194.0
qs8dso,hkbgjrh,t1_hkbghmo,Ohh ok thanks!!,21,0,0,False,False,True,1636713518.0
qs8dso,hkbglgo,t1_hkbgh2d,"Ohh I hope they don’t cheat me lol, thank u!!",15,0,0,False,False,True,1636713560.0
qs8dso,hkeewkx,t1_hkbgh2d,"See I thought that's what was going to happen to me as a CS major, but boy was I wrong. Now in in Automata Theory and Advance Algorithms breaking down individual components and I want to quit so bad. But I'm so close to graduating so I can't. I hate theory, always have.",2,0,0,False,False,False,1636763305.0
qs8dso,hkd00kj,t1_hkcy0qb,That's my go-to explanation,1,0,0,False,False,False,1636740838.0
qs8dso,hkbjo7f,t1_hkbjg0j,Thank u so much!!,5,0,0,False,False,True,1636716026.0
qs8dso,hkggvwu,t1_hkc5hc8,You ever heard about the guy who went from VIM to Emacs?,1,0,0,False,False,False,1636811983.0
qs8dso,hkbhys8,t1_hkbhsse,So I should take computer science first before programming?,2,0,0,False,False,True,1636714697.0
qs8dso,hkd50ek,t1_hkbub8a,My schools cs program includes a math minor.,1,0,0,False,False,False,1636742920.0
qs8dso,hkd8xrp,t1_hkc88pf,I don't know about that.  I've some programming that's more similar to Italian pasta.,3,0,0,False,False,False,1636744585.0
qs8dso,hke10cs,t1_hkbgjrh,"I liked this comparison the most. Simply saying, you can learn one language and yet stuck in it threw the end of your life. Or learn the grammar, from scratch, and learn how to gather the appropriate resources, just to learn any other language. What you choose?",5,0,0,False,False,False,1636756782.0
qs8dso,hkelthn,t1_hkeewkx,"Honestly though it’s going to make you a good engineer. If you can write a proof you can write good, tight code. Mathematical fluency will never be a disadvantage",2,0,0,False,False,False,1636766704.0
qs8dso,hkbiy8c,t1_hkbhys8,I'd pick computer science. If you pick programming you'll probably be programming a lot of stuff without knowing why whereas if you do computer science you'll learn why it works and have a better understanding of why you are programming stuff in a certain way when you pick up programming. That's been my experience anyway,13,0,0,False,False,False,1636715468.0
qs8dso,hkbimg9,t1_hkbhys8,"Well, that depends on which trajectory you want.

There is a lot of overlap: you can't learn one without getting a fair idea of the other. If you are going to get into programming, you could in principle start with either one.

However, if you start with computer science and pick up programming, the thing that is going to trip you is all the stuff that is neither CS nor programming. Collaboration, systems analysis, etc.

If you start studying programming, you will possibly get the work-related stuff (there are different kinds of programming courses) and a gist of CS.

If you simply mean that you will study programming, then CS or CS, then programming before you start working, I imagine it's roughly the same.",5,0,0,False,False,False,1636715216.0
qs8dso,hkbweuo,t1_hkbhys8,"If you can, try mixing both. During school I'd suggest to do more of CS, but still you need some practice to really get what all the sciency stuff means",1,0,0,False,False,False,1636723981.0
qs8dso,hke71ex,t1_hkbhys8,"its harder to learn computer science than programming, imo you can learn programming yourself online and if you take computer science you should get enough experience to cover for the other course and obviously you will learn more theory. i dont think taking both is worth it but it depends on what exactly they include",1,0,0,False,False,False,1636759565.0
qs8dso,hkez94c,t1_hkelthn,This !,1,0,0,False,False,False,1636773386.0
qs8dso,hkbj83z,t1_hkbiy8c,Ohh ok I’ve solidified my choice lolol thank u!!!,3,0,0,False,False,True,1636715676.0
qs8dso,hkcki2s,t1_hkbiy8c,"It's not always the case. There was an episode of The Big Bang Theory where the guys broke down by the side of the road. Leonard asks if anyone in the group knew about automobile engines. They were all eager to describe how an internal combustion engine works, then Leonard asks who knows how to fix one; they all fell silent. :-)",2,0,0,False,False,False,1636734576.0
qs8dso,hkbiury,t1_hkbimg9,"Oh I see, thank u so much!",3,0,0,False,False,True,1636715395.0
qsis3v,hkdoxnl,t3_qsis3v,"I'm not sure about the companies you mention renting out time, but supercomputers ('High performance computing', or HPC) are not as rare or difficult to create as you might imagine. Many Universities and companies with heavy R&D (e.g. pharmaceutical companies or engineering firms) own their own supercomputers. They will usually approach a specialist company that helps them design, procure, install and maintain it. Or help them build one in the cloud.

  
In terms of what is run, a workload manager like Slurm is usually used. This allows for different groups of users/ different queues for compute jobs. So this could be say your astronomy department, your biology department and your engineering department. They can submit jobs to a queue, which will run when resources (CPUs, GPUs and memory) become available. Queues can be given different priorities and resource allocations.

  
It would be up to the person/ people in charge of the supercomputer to determine who can submit jobs and what queues can be used.",6,0,0,False,False,False,1636751427.0
qsis3v,hkdojb3,t3_qsis3v,[deleted],-1,0,0,False,False,False,1636751257.0
qsis3v,hkdoz63,t1_hkdojb3,No shit? Interesting. Well thanks for replying. I definitely thought that the cost of buying one outright would not justify the research efforts.  Now I'm gonna fuck off the next two hours and see what the pricing models look like for funsies!,1,0,0,False,False,True,1636751445.0
qsis3v,hkdpaey,t1_hkdoz63,I mean you can get a 500 ish core system for maybe 250,1,0,0,False,False,False,1636751583.0
qsis3v,hkg1oqi,t1_hkdoz63,"If you wanted to really get hands on, it is possible to create a small one of your own with a few Raspberry Pis: https://www.raspberrypi.com/news/supercomputing-with-raspberry-pi-hackspace-41/",1,0,0,False,False,False,1636801258.0
qs8gkb,hkbwwjh,t3_qs8gkb,"You can try to study slowly, take more notes, and then do a comprehensive project for each chapter, use it continuously, and you will become proficient.I think the most important thing in learning is to use it, not to memorize it.",12,0,0,False,False,False,1636724231.0
qs8gkb,hkcv8sw,t3_qs8gkb,"First, I pick books based on whether I see them on multiple “must read lists.” It’s not perfect but Hacker News is generally where I pull book recommendations. 

Second, before buying, check the Table of Contents. In the store or online you can usually see the first few pages. See if the chapter names are totally new or known topics. 

Then, if the books a good fit and has good reviews, I get it. Most books have a “how to read this book” in the intro. So the answer to your question is really “it depends.” Some are only meant to be read front to back. Others have pick-and-choose style. Some have a read the first half in order but then you can pick after that. 

If I’m studying, like for interviews, I take an index card and write down chapters & sections I want to read. 

I usually take notes in the Apple Notes app if it’s mostly text (not a lot of code or special characters) or the GoodNotes app on my iPad with a pencil. You can just use a pencil and notepad for less than $5 too. 

Repeating or refreshing material on an exponential back off is CRUCIAL to actually learning. Look up “how to learn” resources. I know that sounds condescending but learning how to learn is the most important meta-skill you can have.",5,0,0,False,False,False,1636738896.0
qs8gkb,hkc8g01,t3_qs8gkb,If it’s technical that I want to remember I wrote it down even if I won’t ever read those notes again just for the memory boost. But if it’s a story or something I don’t plan to use frequently I just read to understand and don’t write notes. U can always go write notes should I need it,4,0,0,False,False,False,1636729592.0
qs8gkb,hkc2ao2,t3_qs8gkb,"Personally, I partially read them on an on-demand basis, depending on the problem I’m trying to solve. Never really read them from start to finish.",2,0,0,False,False,False,1636726843.0
qs8gkb,hkdwxvs,t3_qs8gkb,I just read it like I would any other book I guess. Start with the first page and finish with the last one; don't go on until you know what you've just read. That's it really.,2,0,0,False,False,False,1636754940.0
qs8gkb,hkdznl0,t3_qs8gkb,"Hi, for me it really depends on the kind of book. For a programming/AI style book I will read it quite swiftly and then start some small projects to play with the software or algorithms. Then, instead of googling when something isn’t working the way it should, read the according section thoroughly. 
Now, for books that focus on the theoretical side, e.g. Algorithms by Sedgewick, I will focus throughout the book and read the import sections more than once next to taking notes and summarising the essentials.",2,0,0,False,False,False,1636756166.0
qs8gkb,hkekjvh,t3_qs8gkb,"Lots of good answers here.

I like to read books that have type in programs that actually run so I do two passes through the book. One pass I type everything in.  One pass I read everything but the code.  I get context from the reading and first hand line by line exposure to the code by typing.  The two passes can happen concurrently for example I might type in chapter 4 in the morning and read it as im going to sleep that night.

This is the best way for me and ive been through 4 books this way and Its fun and I love it.

Good luck",2,0,0,False,False,False,1636766078.0
qs8gkb,hknagf1,t3_qs8gkb,"I read my books like a novel (just reading straight through) and I take the approach of repeating the book multiple times. I am making use of the proverb that **repetition is the key to learning**.

1. First exposure is all about getting exposure to ideas within the book. I don't worry about remembering ideas or the way the ideas connect, I just want exposure to the ideas so I know that they exist.
2. Second reading is about considering how the ideas relate to one another. I don't worry about memorizing anything, I want to focus on a narrative that connects a weave of ideas from beginning to end.
3. Third reading is when I start to write notes if I feel like writing notes. Since I have exposure to the ideas and how they connect, I now have a simple understanding of the book, I can write effective notes about the book. If all I wanted was a simple understanding, three iterations is normally good for me.
4. Any more readings above three is to reinforce the learning that I've already done. Repeating the study material multiple times means you can pick up subtle ideas that are easy to miss when you're faced with a whole book's worth of detail.  The more repetitions you do, the stronger your imprinting for the ideas you already know, the easier it is to detect subtle details that are easy to miss. 

This strategy allowed me to read books that where initially incomprehensible to me. The value of repetition means I can (eventually) absorb ideas that have no meaning to me. I don't always need to understand ""why"" or ""how"", but I can ""know"" precisely what the idea says through the power of repetition.",2,0,0,False,False,False,1636929242.0
qs8gkb,hkegxpj,t3_qs8gkb,Bring that manual in the bathroom! I read when I can! 🚽,1,0,0,False,False,False,1636764295.0
qs8gkb,hkpfg6f,t1_hknagf1,very cool method!,1,0,0,False,False,True,1636973243.0
qsg140,hkctk03,t3_qsg140,"_DNS - Domain Name System_, ergo: no domain = no point in storing random address on DNS server",6,0,0,False,False,False,1636738218.0
qsg140,hkdnn15,t3_qsg140,"You register a name to an ip. So your ip isn’t stored within that specific carrier until it’s associated by registering it. The internet is IP address, and can be navigated as such, a dns resolution just equates a string to an ip.

Your pc won’t be a open network device, you connect to the internet through a router and modem, these devices have their own IP and that is queryable in the network table, which is a ledger of known IPs. Your pc has a local IP within that devices network and not the public network.

Your pc -> router -> modem-> buncha different devices -> target server -> buncha different devices -> request origin modem -> request origin router -> request origin local IP.

The other thing to note here is that the device you request data from might have the same local IP as you, but within a different parent network.",1,0,0,False,False,False,1636750870.0
qsg140,hkfm7ra,t3_qsg140,"Generally speaking your isp who assigns you your IP address will have a DNS entry associated with it. This is not always the case it just usually is. Theres not too much extra information there, but often you can get what city the IP is in and who your ISP is. Either directly or by tracing the names of routers on the way to the IP.

The one for my cell phone right now is:
69-232-153-116.lightspeed.tukrga.sbcglobal.net

(Incidentally I'm pretty sure this IP is shared with a bunch of other folk, and changes since it's mobile, so I'm not terribly concerned with security here, don't share your IP as a general rule)

The actual ""DNS ledger"" is distributed over a number of different servers. To figure out what specific computer has this entry more or less works by reading from right to left. There a few root nameservers that are all synced that can point you to nameservers that control top level domains like .net. Those can point you to nameservers that control domains like sbcglobal.net. It usually ends there but there might be a separate nameserver that controls a specific subdomain like tukrga.

Once you get the specific nameserver, in this case ns2.attdns.com, you can ask it for what ip 
69-232-153-116.lightspeed.tukrga.sbcglobal.net points to.",1,0,0,False,False,False,1636787714.0
qqpf3o,hk1xzzr,t3_qqpf3o,"Document formats aren't a field of research, they're a field of application. There is significant research being done into the component aspects of document storage, display, transmission, translation, etc etc etc, but they won't be published as such because they are useful in many more areas than just documents, and as such it is very unlikely that there is a journal specialising in it (certainly there is no high impact journal, and nothing from ACM or IEEE).
 
Transactions on Graphics would publish things that are relevant to documents in terms of image storage within documents. There are relevant journals for the compsci algorithms to do with text and other data compression (which are not document-specific), there is a growing field of computer history which includes looking at data archival and longevity (including reading old formats of documents), and a sub-field of business anthropology looking at the development of document standards.
 
The application work to bring that research together into some new document standard wouldn't be published in an academic journal, it would be released as a white paper from the company that developed it.",42,0,0,False,False,False,1636541418.0
qqpf3o,hk1or8a,t3_qqpf3o,"I don't think this kind of thing is being worked on at all. Storage is so bonkers cheap that even if all pdfs were cut it half by a new compression algorithm, almost no one would even notice.
  
Things that are possibly being worked on is getting stuff to load faster, but generally the bottleneck in this situation is going to be the CPU/Memory/Storage interfaces so a new algorithm isn't going to help too much.",30,0,0,False,False,False,1636533392.0
qqpf3o,hk2byii,t3_qqpf3o,"One doesn't usually research file formats, those are very much an application. However, people do regularly research into things that might eventually be used in a new file format if companies ever care enough to implement it.

For example, take this (relatively) young new compression algorithm: [ZStandard](https://en.wikipedia.org/wiki/Zstandard?wprov=sfla1)

Maybe this will one day be used in a document file format, maybe it won't. That's not really something for researchers to decide but for companies building those software and products",9,0,0,False,False,False,1636550413.0
qqpf3o,hk2m4d7,t3_qqpf3o,"Something to keep in mind is that file/document formats are almost never about quality/merits/capabilities of the formats. The format people use is determined by what software they, and the people they are sending documents to, have.  

A good example is JPEG. There have been a few formats that give equal or better quality images, use less space, and use about the same resources to decompress/view. Why haven't those formats pushed JPEGs out of existence? Well everyone's web browsers support JPEG and don't support fancy new format X. While it might seem practical to just say, everyone should just update their browsers' to a never version, and then we can switch. Experience has shown that this just isn't the case. And that's not getting into all the backend and image generation software on the server side, or the firmware on physical cameras, and anything like that. 

Everyday file formats aren't a matter of someone building a better mouse trap. Adobe has a lot of power in the document/image sphere because their software is entrenched there. If you can't get Adobe to make their whole software line support a PDF-successor format, that format is dead in the water. 

Put another way, file formats are a matter of economics not computer science. 

PDF files don't use a single specific compression algorithm by the way. They use a pile of them, specialized for the part of the document they are being used for. Compressing vector graphics, fonts, blocks of text, color images, and greyscale images with different algorithms that are tailored to the specific task. 

Note that while introduced in 1993, PDF was a closed, proprietary format until an open standard for it was established in 2008. The number of versions of PDF is around 10-20 I think, it has been updated/changed many times over the years. The most recent update was in December of 2020. While user visible features have certainly been introduced,fill in forms for example were added in version 1.2 in 1996, most updates have been adding encryption and compression algorithms to the multitude that PDF supports. Keep in mind that 1996 form feature is still vastly underutilized.  Being able to view a PDF, and even more so being able to 'print' anything into a PDF, without installing specific software that the user downloaded and installed (as opposed to came with their operating system) are pretty new things. 

In many ways, you could argue that PDF is more of a container format than anything else. Something that's the case for a lot of file formats nowadays. All audio/video file formats and even zip files fall into this category. The standards define a standard way of holding different hunks of data and metadata, with different formats being used by the individual hunks. A new compression algorithm can be added to the PDF standard without throwing away the format or backwards compatibility.

I am curious if there is anything about PDF you find lacking. Of course, file using less space is nice, but that applies to all files. I'm sure that most people, even in CS, don't know the majority of the features that PDF even support. I'd definitely include myself in that category.",8,0,1,False,False,False,1636555195.0
qqpf3o,hk1xa1m,t3_qqpf3o,"> Is there any chance new discoveries are made (e.g. more efficient algorithms to compress documents)

https://en.wikipedia.org/wiki/Zstandard

> Zstandard (or zstd) is a lossless data compression algorithm developed by Yann Collet at Facebook. Zstd is the reference implementation in C.

You mean like this?",6,0,0,False,False,False,1636540828.0
qqpf3o,hk2elq9,t3_qqpf3o,"There are/have been working groups and standards bodies working on document formats. On Computerphile, Prof. Brailsford has presented many videos on his experience working in the field.",3,0,0,False,False,False,1636551731.0
qqpf3o,hk1qii7,t3_qqpf3o,"I agree with what has been said about this being not a tremendous issue given modern storage capabilities.

If you are interested in compression generally, I would recommend investigating signal processing and digital signal processing, probability theory, and information/coding theory (especially coding theory); typically, the actual format of these files is just some compression mechanism (typically one or more lossy encodings, followed by an entropy encoding) dressed up in fancy clothes.",2,0,0,False,False,False,1636534915.0
qqpf3o,hk26r4s,t3_qqpf3o,"I feel like the “cutting edge” research is mostly in other areas: compression, verification, performance, security, etc.",2,0,0,False,False,False,1636547532.0
qqpf3o,hk3oo4j,t3_qqpf3o,"This would be dealing with Encoding/Decoding/Transcoding.

You would wanna focus on studies of Algorithms, Cryptography, Computer Graphics, and Programing Language Theory.

Reasons why? The goal. Making a super detailed, small memory required document format that is also computationally less intense. So the fields above would give you the insight to facilitate that.

Good luck!",1,0,0,False,False,False,1636570419.0
qqpf3o,hk2nuqr,t3_qqpf3o,"http://www.cs.nott.ac.uk/~psadb1/

http://www.eprg.org/research/",1,0,0,False,False,False,1636555939.0
qqpf3o,hk2o51e,t3_qqpf3o,Did you by chance recently ask if open source software is discouraging innovation citing the numerous frustrations associated with PDFs?,1,0,0,False,False,False,1636556061.0
qqpf3o,hk38nat,t3_qqpf3o,"As far as file compression, document formats, pdf's, etc... go, the technology used today has existed for quite a while and I don't think there's a huge amount of research going into it.  However, we are entering the age of Cloud Computing and there is definitely a lot of new technology being developed around streaming compression, distributed documents, document security/e-signatures, and that sort of stuff.",0,0,0,False,False,False,1636564236.0
qqpf3o,hk5j52i,t1_hk1xzzr,Thanks for this. I would like to know more about fields that have to do with text compression algorithms.,1,0,0,False,False,True,1636598028.0
qqpf3o,hk3z1q6,t1_hk2m4d7,"This is a great and informative post, even though it doesn’t directly address the question.

Please enjoy my Reddit-welfare award.",1,0,0,False,False,False,1636574448.0
qqpf3o,hk5lv1m,t1_hk2m4d7,"It wasn't that I found PDF lacking in features. Wavelets are a thing in applied math, and Daubechies wavelets are used in JPEG. This intrigued me, because it's a great example of using something from research to implement a very practical thing related to pictures. Since I deal with PDF documents on a regular basis, and really like the quality of infinite zoomability of crisp documents that use vector graphics, I was wondering if there is a similar application of research level applied math, or algorithms, not for images as in JPEG but for documents. I realise my question was naive, but all the replies (including yours) have been very helpful in clearing up misconceptions. Your comment about PDF requiring different algorithms for different features was particularly helpful; I was under the impression that for a PDF compiled from LaTeX only vector graphics is involved, I was not aware that fonts are handled differently.",1,0,0,False,False,True,1636599247.0
qqpf3o,hk3samw,t1_hk1xa1m,"I have used this before, it is great when you have a lot of similar files. I used it for a very large web crawler storage, where there was lots of repeated data. Although rebuilding a new dictionary was annoying (and heaven forbid you aren't backing it up safely), I could simply make a dictionary file and the average response was like 4kb instead of 60. I actually used it on pickled python response objects, but same idea. AFAIK Facebook uses it to store people's chat histories, which often use similar phrases and speech patterns. It's also high performance. It's great for that kind of stuff

I will say, though (edit: and this is reiterating other comments) formats like PDF are rare. It's not about whether there's a new better standard, it's about adoption. PDF and DOCX were shoved into widespread adoption and that's why they are standard, because they were pushed by Adobe and Microsoft. A better format that nobody uses is worse than a bad format that everyone uses. It takes a long time (or a killer feature) for there to be any significant migration.",3,0,0,False,False,False,1636571825.0
qqpf3o,hk826so,t1_hk5lv1m,"Something to keep in that is that decompression/zooming stuff has to be fast enough to use in vaguely real time because that's what people expect with documents.  Fonts and vector graphics can be scale indefinitely, but raster images you really can't do anything perfect with them. 

There a lot of interesting projects using AI/ML to fill in the missing information on pictures. Some could be used to generate high-resolution images so you can zoom more.",1,0,0,False,False,False,1636650861.0
qqxc02,hk6h40p,t3_qqxc02,"    def step_func(analog, threshold=0.5):
        if analog > threshold:
            return 1
        return 0

The above code is in Python. But it's a very simple algorithm, and can be implemented in any language. You'd have to map that function over your entire wave in a preprocessing step.",3,0,0,False,False,False,1636617879.0
qqxc02,hk3kzdb,t3_qqxc02,"Not sure what you got going on but usually one sets up the sensors, then for each channel an adc, at some sample rate, with some scaling, fft and filters in sw.",2,0,0,False,False,False,1636568997.0
qqxc02,hk39jp9,t3_qqxc02,Noise is usually dominant at higher frequencies. So you can try removing higher frequency waves. Usually Fourier analysis is used for this task. I guess your removing higher than mean method might work better with frequencies separated though I'm not entirely sure.,1,0,0,False,False,False,1636564590.0
qqxc02,hk3jicj,t3_qqxc02,"Are you computing the mean as the data comes in or are you preprocessing it? 

If you’re analyzing data as it comes in your mean will fluctuate and might not give you accurate results.

If you’re preprocessing, this could be very slow as you need to make an initial pass through per run.


What you might consider is seeing what range your sensor outputs and determining what the minimum amplitude is for it to be a 1. Doing this would require you to run the sensor and do some analysis on the data you get.",1,0,0,False,False,False,1636568433.0
qqxc02,hk4lx67,t3_qqxc02,"If you want to turn an LED on only when a certain observation is made in the EEG data stream, you might want to look at reinforcement learning methods. There is a Matlab tutorial with an engineer teaching a DNN to detect him making a high five motion based on accelerometer readings. You might be able to find that one online.",1,0,0,False,False,False,1636583429.0
qqxc02,hk6icwn,t3_qqxc02,"I suggest first to look at your data, see if you can find the noise at specific frequencies (like the top comment says its usually higher frequencies but not always), if so then a simple filter would do.

If the noise and data sits on the same frequncy your problem change based on your snr and how presice you want to be.

The most important is to know your data well!",1,0,0,False,False,False,1636618895.0
qqxc02,hk9a53h,t3_qqxc02,"You are going to want to use a low pass filter before you threshold your ""on/off"" values.

Here is an easy way to do it with scipy. [https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html)",1,0,0,False,False,False,1636668700.0
qqxc02,hla4fbb,t3_qqxc02,"There are a lot of programable senders out there that will do this for you, including raspberry pi devices. You just set the parameters you want and let it do it’s thing. Tweak it as you go until you get what you want. Some of them even support CAN messaging.",1,0,0,False,False,False,1637344261.0
qqxc02,hk4ljih,t1_hk39jp9,Then a simpler low pass filter method might also work.,3,0,0,False,False,False,1636583274.0
qqxc02,hk9amu8,t1_hk9a53h,You could also look into M of N sliding window detection to determine with a good probability that the light should be on. (If the value is greater than the threshold for M out of N ticks) A tick would be some time unit defined by you.,1,0,0,False,False,False,1636668906.0
qqvz15,hk2ojp8,t3_qqvz15,"I had never heard of it; however, looking it up it seems like a legitimate conference. It does not trip any of the red flags for a conference. H indices are ok. Committee sizes are pretty normal. The acceptance rate is a bit high at 30+%. Normally, you should aim for conferences that are closer to 20-25%. But it is not absurd like 70%.",3,0,0,False,False,False,1636556235.0
qqvz15,hk2sbvs,t1_hk2ojp8,Can I please message you personally?,2,0,0,False,False,True,1636557793.0
qqvz15,hk2twvo,t1_hk2sbvs,Sure,1,0,0,False,False,False,1636558438.0
qrlsrw,hk7amy7,t3_qrlsrw,Programming will never be obsolete the languages just become higher level.,51,0,0,False,False,False,1636639014.0
qrlsrw,hk7b90q,t3_qrlsrw,Self teaching for over a year and already prophesying  the end of all things,34,0,0,False,False,False,1636639314.0
qrlsrw,hk7arqg,t3_qrlsrw,"Yeah... I don't think AI is going to be capable of coding in a right way as there is no right way to code, have you seen GH Copilot?",16,0,0,False,False,False,1636639079.0
qrlsrw,hk7ipzw,t3_qrlsrw,[deleted],14,0,0,False,False,False,1636642764.0
qrlsrw,hk7czgm,t3_qrlsrw,"People have been saying that technology will put everyone out of work for something like 200 years.  It was only a couple of years ago that everyone was saying how necessary a universal basic income was as machines were taking over work and when there's nothing to do, it's not realistic to expect everyone to support themselves by working.

Look around.  There is no shortage of jobs, there is a shortage of labour.  I don't see any reason this pattern will not continue.",7,0,0,False,False,False,1636640147.0
qrlsrw,hk7rota,t3_qrlsrw,Have you ever dealt with a customer? An AI wouldn’t be able to deal with that bullshit.,5,0,0,False,False,False,1636646665.0
qrlsrw,hk7yc37,t3_qrlsrw,"Nah. Computers cant figure out scope or what brings value.

AI is also highly overrated. The “AI revolution” is just an explosion of data mining and data storage.
Computers 20 years ago could have done the same shit but data is more availa le.

What data would you mine to make an original program? None.",3,0,0,False,False,False,1636649334.0
qrlsrw,hk7epnm,t3_qrlsrw,"Lol forget programming my dude, the circus could use someone like you",14,0,1,False,False,False,1636640950.0
qrlsrw,hk7oq7i,t3_qrlsrw,Be careful on what you read and who you learn from. Most information online is from people that don't know anything and just write articles about AI taking over programmer's work.,3,0,0,False,False,False,1636645398.0
qrlsrw,hk7cpom,t3_qrlsrw,"I enjoy coding.

And as a result of the course that I did, I realised coding isn't something I'd personally wanna do as a job anyway (well at least in the way it works in most companies), and I'm okay with that.",2,0,0,False,False,False,1636640015.0
qrlsrw,hk7qd0s,t3_qrlsrw,"I mean, if AI is really at a point it can automate programming itself, it's likely it'd have automated away other, manual jobs already by then",2,0,0,False,False,False,1636646091.0
qrlsrw,hk7zzr8,t3_qrlsrw,AI to do development work? That's decades away at least (if it ever happens at all).,2,0,0,False,False,False,1636649983.0
qrlsrw,hk81nvw,t3_qrlsrw,"AI has been threatening to remove devs for 40 years.

I'm not scared.",2,0,0,False,False,False,1636650653.0
qrlsrw,hk8cbxi,t3_qrlsrw,"It's not going to replace programming jobs.

Some kind of an AI thing may be able to produce some code for common cases or tasks. That's something we've seen in recent demonstrations of ""code-writing AIs"". It's not going to be able to produce brand new kinds of code for brand new tasks that don't resemble something they've seen in the material they've been trained with. Even more importantly, any kind of an AI like that is not going to be 100%, and it's not going to be able to tell when its output is wrong or to correct itself. You'll need a programmer to at the very least tell if the code is reasonable or does what it's supposed to. And reading and debugging code can be at least as hard as writing it in the first place.

Good luck just pulling some code out of an AI, plugging it into production and thinking you're done with it, without having someone with expertise involved. That's just not going to happen.

Being able to always get correct code from an AI, or having the AI also understand whether the code actually makes sense, would require artificial intelligence on a much higher level than we're able to even imagine building at the moment. If that ever happens, the entire society will have to be rethought, and programming is almost certainly not going to be one of the first victims.

So, I largely deal by trying to ignore the clickbait titles and senseless articles that proclaim AI is going to replace programmers.",2,0,1,False,False,False,1636654841.0
qrlsrw,hk8f70s,t3_qrlsrw,Buddy McDonald’s is still paying real people to dump fries out of a basket into smaller cardboard baskets. I think we’re quite a long way from AI replacing programmers.,2,0,1,False,False,False,1636655983.0
qrlsrw,hk8gnxc,t3_qrlsrw,"...said no AI researcher, ever. Don't listen to journalists when it comes to computer science. AI research is still in its infancy after 50 years, and struggling to even define what intelligence is. Real world AIs can be rather frightening though, but mainly because they increasingly get entrusted to make real decisions affecting people's lives, despite being so utterly dumb and manipulable.",2,0,0,False,False,False,1636656570.0
qrlsrw,hk7oem4,t3_qrlsrw,"If all goes wrong, I will still code for the fun. I love coding, earning money is just a consequence.",1,0,0,False,False,False,1636645259.0
qrlsrw,hk7s6o4,t3_qrlsrw,Even ai will need more people to do operation,1,0,0,False,False,False,1636646874.0
qrlsrw,hk81zrd,t3_qrlsrw,[deleted],1,0,0,False,False,False,1636650783.0
qrlsrw,hk86dha,t3_qrlsrw,Don’t believe the sensationalist hype.  They’ve been saying the same thing for years.,1,0,0,False,False,False,1636652509.0
qrlsrw,hla58r0,t3_qrlsrw,"Yeah… have you actually dealt with any real AI? The most widely used “advanced” AI still autocorrects “fuck” into “duck”. Sure there are some neat AI’s out there but they are extremely limited in what they do, like recognizing real world objects, or analyzing pictures for certain content. The ability to write functioning code just isn’t in the cards for AI yet, and even then, there will still be humans writing the code for the AI… so not really something I’m gonna worry about.",1,0,0,False,False,False,1637344579.0
qrlsrw,hk7pfl3,t1_hk7amy7,"I'm inclined to agree. Even when they are programming themselves, there will probably always be a market for someone to dig in there and take it further. One of the first things programmers did was automate part of their jobs by inventing compilers. The field seems to be founded on raising the level of interaction through this kind of virtuous cycle.",6,0,0,False,False,False,1636645698.0
qrlsrw,hkclvdt,t1_hk7arqg,Copilot is part of the reason I ended up on this thread.,1,0,0,False,False,False,1636735127.0
qrlsrw,hk7s09h,t1_hk7ipzw,"Basically coders will be made obsolete, engineers will become more important.",8,0,0,False,False,False,1636646799.0
qrlsrw,hkab0eh,t1_hk7czgm,Something like 200 years?  Socrates laments in the Phaedrus that the new invention called “paper” would destroy memory. Obsolescence is an argument as old as time.  Keeps not happening.,2,0,0,False,False,False,1636685881.0
qrlsrw,hk8038t,t1_hk7rota,"Not what we have now, we would need human level AI.",3,0,0,False,False,False,1636650021.0
qrlsrw,hkcm82o,t1_hk81zrd,What class did you take?,1,0,0,False,False,False,1636735269.0
qrlsrw,hk85h0f,t1_hk7s09h,"You are right actually, that's spot on what I was thinking about. !",2,0,0,False,False,False,1636652158.0
qrlsrw,hk85m4y,t1_hk8038t,Which is definitely not a couple years out,3,0,0,False,False,False,1636652216.0
qrlsrw,hk8agsf,t1_hk85m4y,Decades minimum (if it ever happens).,2,0,0,False,False,False,1636654095.0
qr8cpv,hk5b4a6,t3_qr8cpv,Don’t do it. Especially not on Twitter,3,0,0,False,False,False,1636594467.0
qr8cpv,hk52iks,t3_qr8cpv,Twitter SWE is basically JavaScript porn.,2,0,0,False,False,False,1636590604.0
qr8cpv,hk52x4u,t3_qr8cpv,"I follow synsation on Instagram 
Really inspiring. She was a baker then became a web developer",0,0,0,False,False,False,1636590785.0
qr8cpv,hlas7uv,t3_qr8cpv,IAmTimCorey on YouTube is about it. And he pretty much only deals in C# and ASP.NET content,1,0,0,False,False,False,1637353141.0
qr8cpv,hk5bpaa,t1_hk5b4a6,Lol. Who are the biggest SWE influencers to avoid?,-2,0,0,False,False,True,1636594727.0
qr8cpv,hk53x6s,t1_hk52iks,any specific accounts you follow?,0,0,0,False,False,True,1636591233.0
qr8cpv,hk53y7a,t1_hk52x4u,thanks,1,0,0,False,False,True,1636591246.0
qr8cpv,hk5py03,t1_hk5bpaa,"If you’re looking for real content I would suggest signing up for a weekly newsletter on something you’re interested in.

SWE Twitter is essentially the same 5 recycled jokes and low effort content",4,0,0,False,False,False,1636601141.0
qpu9g8,hjw4wbz,t3_qpu9g8,"""Code"" by Charles Petzold is what you're looking for.",94,0,0,False,False,False,1636428588.0
qpu9g8,hjwxsxz,t3_qpu9g8,"That's maybe a really weird way to learn it, but you can try building a Minecraft CPU. I tried it, and from that point on I really realized how CPUs work, what you need the individual parts for, etc. For example, I never really understood what the control unit is used for. (Well I thought I understood it, but I didn't) That was, until I had built my ALU inside Minecraft, and realized ""cool, I have a ALU that can add & multiply things, but how do I set the correct enable bits for the corresponding instruction? Multiplying takes a lot longer than adding, how do I wait longer before executing the next instruction? How do I even get to the next instruction?"". That's what the control unit is for.

All the questions you didn't even think of will answer themselves when you try to build your own CPU.",44,0,0,False,False,False,1636446817.0
qpu9g8,hjx874b,t3_qpu9g8,"Here's my [stock answer](https://www.reddit.com/r/C_Programming/comments/qpa6pp/i_am_a_curious_boy_having_lots_of_interest_in/hjt2xkg/) for this:

If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest)
3. If you have the time and energy, do https://www.nand2tetris.org/

This will let you understand *what* a computer is and how a CPU works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how a compiler works etc. Arguably it will also give you the tools to design all of that, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's computer on plastic prototyping board"")",29,0,2,False,False,False,1636455982.0
qpu9g8,hjwaqms,t3_qpu9g8,Look into https://en.wikipedia.org/wiki/Von_Neumann_architecture,24,0,0,False,False,False,1636431342.0
qpu9g8,hjw4cbo,t3_qpu9g8,Do you know about logic gates? Start there.,7,0,0,False,False,False,1636428330.0
qpu9g8,hjx04cn,t3_qpu9g8,"Check out [https://www.nand2tetris.org/](https://www.nand2tetris.org/) the go from logic gates to cpu to writing assembly and finally create tetris.

Havn't done the whole course yet myself but if you want to explore it further it is worth a look",7,0,0,False,False,False,1636448901.0
qpu9g8,hjxchd8,t3_qpu9g8,Youtuber ben eater [made his own 8 bit computer](https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU) from logic gates and then shows how he programs it. It really helped me understand it all,6,0,0,False,False,False,1636459159.0
qpu9g8,hjxjzeh,t3_qpu9g8,"Somewhat of a simplified answer, but I think this conveys the jist of what you’re asking for.

So first understand what a [Boolean function](https://en.m.wikipedia.org/wiki/Boolean_function) is. Every function of a processor is built as one of those, the mov, ldr, str, etc. will be a function of ones and zeros being input, and translates to a certain series of ones and zeros output. You can build any possible Boolean function you desire with the correct series of transistors like this.

Next you need to understand what a [Multiplexer/Demultiplexer](https://en.m.wikipedia.org/wiki/Multiplexer) is. Essentially, it’s like a type of Boolean function where you give it an address of ones and zeros and it will pick from either a corresponding input or output bus. This is how you “select” things in a computer.

Assembly code correlates almost one-to-one with the machine instructions that gets put into a processor. Each machine instruction is a series of ones and zeros (most commonly 32 or 64 bits, this depends on the specifications of the processor). A certain number of those bits represents the “op code,” which would be your mov, ldr, add, etc. and that specific series of bits would go into one multiplexer to “select” the function that is being performed.

Each of those functions requires other information (ie add needs two values to add together and a register to store them, mov needs a source and a destination register, etc.) which would be passed as arguments. Other bits in the instruction represent those arguments, which would be either your registers or immediate values in your assembly code. Each of those strings of inputs go into a multiplexer that selects the appropriate value.

The largest section of the machine instruction is usually at the end which is the offset. This is needed when accessing data from large memory stores because your memory addresses point to large sections of data, and when you need a specific piece of the data you would specify an offset.

Your assembly then gets translated into these instructions.

Look into Von Neuman architecture and this might put some more of this in context. I hope my explanation at least answers more questions than it creates.",6,0,0,False,False,False,1636463673.0
qpu9g8,hjws2z6,t3_qpu9g8,"It's a vastly simpler example then a modern CPU, but I found it really helped my understanding - https://youtu.be/dXdoim96v5A

Ben Eater's 8-bit CPU defines an 11-bit ""control word"" which is hand-crafted by the chip designer and stored in ROM for each instruction (and each step of each instruction; they have 5). The control word has a number of bits that each correspond to a transistor that enables/disables a piece of functionality in the CPU .

For example, one step of a Load Memory Into Register instruction have an enabled control word bit that tells the Memory module to ""output"" its current value to the bus, and the Register module has its ""input"" bit enabled to read from the bus.

The CPU works its way through a series of control words to move data around in the CPU, or toggle functionality (for example one of the control word bits switches the arithmetic unit between Addition and Subtraction modes).",9,0,0,False,False,False,1636442070.0
qpu9g8,hjxjdc0,t3_qpu9g8,"CPUs aka microprocessors are just made for processing data. They are interfaced with different interfacing IC which does rest of the actual work, like moving data, sending inputs from peripheral devices and sending output to the said peripheral devices. 
For example, you have DMA or Direct Memory Access which allows hardware or peripheral to access the main memory without the need for microprocessor to do anything. 
Microprocessor mainly performs logical and control operations. Like picking data, processing it (doing logical or arithmetical operations like >,<,+,-,/ etc.)
The way they work largely depends on what kind of architecture is being used. Different microprocessors work differently. 

A great way to learn this is to learn how the older processors worked. An 8-bit one would be perfect in my opinion. Something like 8051, 8085 or 8086 are still taught in many places to teach the basics of microprocessors.",3,0,0,False,False,False,1636463332.0
qpu9g8,hjxu3yg,t3_qpu9g8,"1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",3,0,0,False,False,False,1636468676.0
qpu9g8,hjzwuty,t3_qpu9g8,"https://youtube.com/playlist?list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye
Try crash course computer science.",3,0,0,False,False,False,1636498862.0
qpu9g8,hjwx0rk,t3_qpu9g8,"I don't understand why people are so eager to recommend books when you're looking for a relatively simple answer. Start the other way around, watch easily consumable medium and then dive into books for detailed information. 

**I recommend** [**this video by Computerphile**](https://www.youtube.com/watch?v=IAkj32VPcUE) **which covers exactly what you've asked** (and a bit more) although on a somewhat high abstraction level. For further reading, what you're looking for is Fetch-Decode-Execute cycle.",6,0,0,False,False,False,1636446127.0
qpu9g8,hjx0wpu,t3_qpu9g8,"There is a crash course on YouTube on computer science. It explains from ground up all you need to know. No technical knowledge required, you can watch it in your leisure time. https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo",2,0,0,False,False,False,1636449616.0
qpu9g8,hjxkkus,t3_qpu9g8,"There is a module on the CPU called instruction decoder. There is a very simple 4 bits CPU simulation that helps you understand it. ""simulator.io | Sample - 4-Bit CPU"" https://simulator.io/board/AWZpw7Fy3I/2",2,0,0,False,False,False,1636463995.0
qpu9g8,hjxtlvk,t3_qpu9g8,"If you want some in-depth knowledge on how a CPU works and how you build it up from logic gates, I can recommend ""Digital Design and Computer Architecture"" by Harris & Harris.",2,0,0,False,False,False,1636468451.0
qpu9g8,hjyclic,t3_qpu9g8,"This is quite a large topic area and other users have given good resources to start with. for some footnotes essentially your general purpose cpu is made of a few components such as an arithmetic logic unit (ALU), registers (the fastest form of memory), and a control unit (which controls how these pieces communicate. each flop is controlled by your computers clock so each cycle of electricity will cycle through the cpus network of transistors to perform a task.

the ALU as it's name suggests can perform simple arithmetic operations such as addition and subtraction. From these two opperations we can build more complex operations such as multiplication, division, modulo. etc (see ripple adder for a basic circuit bit number addition).

 small segments of data can be saved in the registers which hold data that should be frequently accessed with less frequently accessed data going to ram then hard-drives.


going down a level these components are made of circuits such as a JK flip-flop which can store a single bit of data, D or data flip-flops, timers, and comparators. 

these in turn are made of the low level logic gates we all learned about at somepoint such as your OR's, AND''s, XOR's, etc. 

finally you get down to the transistor a simple off and on switch where several are used in the correct configuration to create the gates.


beyond this you get into theoretical EE territory which is not my wheelhouse. hope this helps and if I made any mistakes let me know as I am a bit rusty on the topic myself.",2,0,0,False,False,False,1636476376.0
qpu9g8,hjzjih2,t3_qpu9g8,"A specific answer to your ""Who translates 'mov'"" is that all CPUs have a specific set of instructions. This is called an instruction set. Different CPUs may have different instruction sets. For example, Intel and ARM CPUs do not have the same instruction set. Therefore a compiled Assembly program may run on an Intel CPU but not on an ARM CPU. Or it may run, but yield different outputs. The instruction sets are basically a giant table of Hexadecimal numbers which correspond to a specific instruction.

But if you want to create an instruction set, you have to program the gate logic for the instructions. Gate logic is basically a series of cycles where each cycle has a set of operations for which gates and busses to open/close. Some instructions take more cycles than others to be completed depending on the CPU architecture and instruction set which is why we have concepts like ""Branchless programming"". Branchless programming is basically writing if/else logic differently to avoid switching branches in the compiled machine code since switching branches can be a heavy instruction and takes more cycles.

A good but small difference between ARM and Intel instruction sets is that an 8-bit integer is signed differently. You know the difference between unsigned and signed integers in programming. Say you have a C program where you declare a variable c:

    char c;

On an Intel CPU or most x64 CPUs, this basically compiles or is equivalent to:

    signed char c;

Since an integer is evaluated as a signed integer by default. But on ARM CPUs this compiles to:

    unsigned char c;

So you have to take that into account when you program for ARM CPUs. For example, if you compile this code below to an ARM instruction set. Based on what I mentioned earlier, what is the problem?

    char abs(char c) {
        return c < 0 ? c * -1 : c;
    }",2,0,0,False,False,False,1636493383.0
qpu9g8,hjx287p,t3_qpu9g8,"[http://www.buthowdoitknow.com/](http://www.buthowdoitknow.com/)  
This book explains CPU in very easy to follow way

[https://www.youtube.com/playlist?list=PLuiLMR-Dpj-3s72aqvmKC5Ik\_d6GB6KOf](https://www.youtube.com/playlist?list=PLuiLMR-Dpj-3s72aqvmKC5Ik_d6GB6KOf)  
This playlist explains concepts and logics you need for building computer in minecraft but they apply to real life too

[https://www.youtube.com/c/BenEater](https://www.youtube.com/c/BenEater)This channel got a lot of videos for low level concepts and hardware",1,0,0,False,False,False,1636450791.0
qpu9g8,hjykt1i,t3_qpu9g8,"[https://ict.iitk.ac.in/wp-content/uploads/CS422-Computer-Architecture-ComputerOrganizationAndDesign5thEdition2014.pdf](https://ict.iitk.ac.in/wp-content/uploads/CS422-Computer-Architecture-ComputerOrganizationAndDesign5thEdition2014.pdf)

read this textbook",1,0,0,False,False,False,1636479615.0
qpu9g8,hjypm88,t3_qpu9g8,"Mods, we get this question like EVERY day. Can we please add something to the sidebar/wiki?",1,0,0,False,False,False,1636481509.0
qpu9g8,hjyzjvo,t3_qpu9g8,Sometimes they channel voltage and sometimes they don't. Duh.,1,0,0,False,False,False,1636485453.0
qpu9g8,hjznc10,t3_qpu9g8,There are lots of useful links. You pretty much need to start with smaller to larger blocks. transistors -> logical gates -> ALU -> registers -> clock. A modern day CPU is an insane piece of engineering and close to magic that it even works.,1,0,0,False,False,False,1636494912.0
qpu9g8,hjzrh16,t3_qpu9g8,Here you go https://youtu.be/sK-49uz3lGg,1,0,0,False,False,False,1636496614.0
qpu9g8,hk01osf,t3_qpu9g8,"Maybe a bit more high level than you want, but after you done with other links posted here check out
https://www.nand2tetris.org/

It deals with this exact question -- how you go from transistors to operating system.",1,0,0,False,False,False,1636500969.0
qpu9g8,hk01y8y,t3_qpu9g8,"Comment for personal archive* this is an amazing thread, also pulling in minecraft, beautifully done!",1,0,0,False,False,False,1636501086.0
qpu9g8,hjw6uxv,t1_hjw4wbz,"thanks, I'll look into it",19,0,0,False,False,True,1636429500.0
qpu9g8,hjwwb08,t1_hjw4wbz,"Great book, reading it before starting my studies really helped me grasp some early concepts.",8,0,0,False,False,False,1636445520.0
qpu9g8,hk05xl1,t1_hjw4wbz,"Seconded, fantastic book and explains exactly what you want to know plus a whole lot more.",2,0,0,False,False,False,1636502860.0
qpu9g8,hjz7mzn,t1_hjwxsxz,"Lol, that's how I've learned how logic circuits work - either by building them from real components or building them in Minecraft. It was kind of eye opening.",7,0,0,False,False,False,1636488674.0
qpu9g8,hjxisli,t1_hjwxsxz,Wait what’s a minecraft cpu,11,0,0,False,False,False,1636463012.0
qpu9g8,hjxn339,t1_hjx874b,"I'm a Senior CS student and even though I took Computer Organization, I'm constantly thinking, ""I get how this works, but HOW does it work a level deeper?"" 

For this reason, thank you for one of the most useful posts I've ever come across.",9,0,0,False,False,False,1636465307.0
qpu9g8,hk0dz03,t1_hjx874b,Commenting to save for later,0,0,0,False,False,False,1636506449.0
qpu9g8,hjw7kgh,t1_hjw4cbo,"I do, for example I know about adder circuits - [https://en.wikipedia.org/wiki/Adder\_(electronics)](https://en.wikipedia.org/wiki/Adder_(electronics))

and coming from the other end I also know about microcode.

So starting with assembly code, say someone writes:

add eax,1

then that gets compiled to binary

when you execute that binary, I would like to know exactly what happens from start to finish. Who interprets the binary code, how does the adder circuitry get involved, who ""puts it in motion"" . It's black magic to me that a string of 1s and 0s can have physical consequences inside a CPU.

I realize this is pretty low level stuff but I never understood it and I'd like to know more about it",1,0,0,False,False,True,1636429833.0
qpu9g8,hjzi8sg,t1_hjx04cn,"I read this book 2 years ago and I think it answers *exactly* what OP is asking. I didn't follow any course, just the actual book.

Seeing how an adder works revealed the mystery of how circuitry can actually do math.

Then seeing how the whole circuit starts off in chaos where any parts (not good with terminology) could be on/off but once electricity has flowed through the whole circuitry it reaches an equilibrium.

And how certain parts (registers) are protected from that chaos and are updated only after that equilibrium state is reached.

>Who translates 'mov' into what it actually does and who knows how to interpret that command ? 

An assembler does this. It does what you think, it sees mov and there is a set binary sequence which corresponds to that. It just sticks that down as the next bytes of the binary file.

The CPU never sees the letters m/o/v, it is fed the first 8 bytes of the binary file. Those bytes are just put into a CPU register. The bits in those bytes set up the initial conditions of the circuitry which will come to some equilibrium. Then it automatically puts the next 8 bytes of that file into the same register (unless there was a jmp command or an interrupt etc.).

For example, the 10th bit might be connected up to a AND gate, so if that bit is a 0, the output of the AND gate will be off. That's just an example of what I mean by initial conditions affecting the circuitry.

I started out giving a very general answer and went into some details so it's a bit inconsistent but I hope I've advocated for the book well enough.",3,0,0,False,False,False,1636492876.0
qpu9g8,hjxk0ls,t1_hjxjzeh,"Desktop version of /u/JDHuff185's links:

 * <https://en.wikipedia.org/wiki/Boolean_function>

 * <https://en.wikipedia.org/wiki/Multiplexer>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",0,0,0,False,False,False,1636463692.0
qpu9g8,hjwta3d,t1_hjws2z6,"I don't know why this bothers me so much but it isn't ""ben eater's 8 bit cpu"", that video series is covering SAP-1, designed by paul malvino for teaching students digital electronics.",5,0,0,False,False,False,1636443025.0
qpu9g8,hjx8cgy,t1_hjwx0rk,"Probably because books are better than videos, especially in terms of information density. And OP is after a topic that is particularly complicated and requires a lot of information.

I would say watching that 11 minute video isn't going to satisfy OP's questions, whereas reading Code will, and it'll also give them a huge amount of other relevant information they never knew to ask for.",4,0,0,False,False,False,1636456103.0
qpu9g8,hjyl08v,t1_hjykt1i,especially chapter 4,1,0,0,False,False,False,1636479692.0
qpu9g8,hjxmmj3,t1_hjxisli,"I haven't played Minecraft in quite a few years but I think the game is pretty advanced in terms of redstone and all the things you can build with it? 

[Here's something I found on it.](https://minecraft.fandom.com/wiki/Tutorials/Redstone_computers) (I only briefly glanced at it so I'm not sure if it's what we're looking for here)",17,0,0,False,False,False,1636465069.0
qpu9g8,hjxoxao,t1_hjxn339,"Did you university not have a course on digital logic?


(Although, thinking about my own course years ago, it's possible only the basic gate stuff was mandatory, and the RTL-level stuff where you mush gates together into a computer might have been optional / for computer engineering students)",9,0,0,False,False,False,1636466226.0
qpu9g8,hjyb3qp,t1_hjxn339,[deleted],3,0,0,False,False,False,1636475784.0
qpu9g8,hjy1hu4,t1_hjxn339,"That is exactly my problem. Let me explain more - I haven't had a chance to review all the great links everyone posted, so maybe they already explain what's missing in my head.

=======

CPUs are made of billions of transistors, assembled in logic gates, assembled in more complicated circuits like the Adder, ALU, etc.

I'm sitting in front of that computer and I write a very basic program, compile it and for the sake of argument, I store it in RAM.

'storing' something in RAM means that at the lowest levels, a transistor/capacitor pair gets activated and one capacitor gets filled with current storing 1 bit (I'm guessing now ?). 

so reading something from memory must mean that an electric current arrives in the ram circuitry that says: give me the status of all capacitors in row 10

Then how would the status (full/empty) of capacitors be communicated back to the CPU and what does the CPU do with that info ? Does the CPU in turn also have transistor/capacitor pairs that get activated in order to 'work' with this information ? 

========

This is the kind of explanation I'm looking for because otherwise I've seen plenty of documents that discuss address space, and lookup functions, adding functions but they never explain how \*those\* work.",2,0,0,False,False,True,1636471873.0
qpu9g8,hjwfq9e,t1_hjw7kgh,"Others have linked tons of helpful documentation, but for me what really clicked was watching Ben Eater build his own CPU. Seeing the bus, registers, and gates physically laid out helped it all make sense, and he explains every detail.

https://youtu.be/dXdoim96v5A",14,0,1,False,False,False,1636433890.0
qpu9g8,hjwamue,t1_hjw7kgh,This is a super complicated topic and someone typing it all out is a big ask - this is documented extensively online though!,3,0,0,False,False,False,1636431291.0
qpu9g8,hjwtgdz,t1_hjwta3d,"Yeah, though it's not precisely SAP-1; there are changes, improvements and optimisations he's made, as well as his own clock module, designing the layout etc. There's a lot he's added that I think qualifies it as his CPU, in the same way that Apple's M1 is based on ARM's design",5,0,0,False,False,False,1636443168.0
qpu9g8,hjxqb9v,t1_hjxmmj3,ok I've never played minecraft before but now I really want to. That is insane,11,0,0,False,False,False,1636466904.0
qpu9g8,hjxvs4c,t1_hjxoxao,"Unfortunately not, at least not that I'm aware of. 

We have a course on ""the fundamental topics of modern computing systems"" which focuses heavily on assembly, but not the deep-level of why it works, just how to get it to work.

We also have another course on assembly that acts as an introduction to CPU architecture (pointers, logic, etc) but that doesn't go in depth on a hardware level. 

Either way, there's *something* missing, it could definitely be that I'm not an engineering student, but some of your links satisfy that itch I've always had.",4,0,0,False,False,False,1636469418.0
qpu9g8,hk1yzql,t1_hjyb3qp,"> My boss just says, don't question how it works. As long as it does.

I think this is bad advice. In my experience the people who question how it works will write better code, and they'll be much more useful in a situation of when stuff goes wrong, because when stuff goes wrong abstractions break down, and if you have no knowledge of those lower foundations you've had your legs swept from underneath you.",2,0,0,False,False,False,1636542205.0
qpu9g8,hk1yuju,t1_hjy1hu4,"> Then how would the status (full/empty) of capacitors be communicated back to the CPU and what does the CPU do with that info ? Does the CPU in turn also have transistor/capacitor pairs that get activated in order to 'work' with this information ? 

The address and data buses are a vital part of a CPU <-> RAM interface. Check out the resources for more :) They'll tell you how such a bus is implemented and how RAM module multiplexors are usually implemented.",1,0,0,False,False,False,1636542091.0
qpu9g8,hjxqe4y,t1_hjwfq9e,That’s awesome! This dude is genius . I don’t remember when was the last time to enjoy someone explaining stuff in an interesting way and to take my attention 100%. Thank you for sharing !,3,0,0,False,False,False,1636466944.0
qpu9g8,hjxvj68,t1_hjwfq9e,Yea this was what I was going to suggest.  Even having it on stand by is good if it doesn't make sense now. His videos got more and more cooler the more I learned.,2,0,0,False,False,False,1636469309.0
qpu9g8,hjxivfo,t1_hjwfq9e,Dude I’m so glad someone mentioned him. Discovered him a couple weeks back. He’s amazing,1,0,0,False,False,False,1636463056.0
qpu9g8,hjwtiwu,t1_hjwtgdz,"Those things were in SAP2 through SAP3... No. It's all in the same reference book, written by Malvino. And people are expected to make changes while building that's what it's for, it's an architecture used to teach.",5,0,0,False,False,False,1636443223.0
qpu9g8,hjy25b4,t1_hjxvs4c,"You should send your University an nasty letter calling them poopy heads.

Digital logic is something that should be touched on in every degree, I think. If only because the basic gate stuff is fun!

Instead you probably had to endure some crap about databases that you could probably have guessed.",6,0,0,False,False,False,1636472145.0
qpl4wt,hjuovnq,t3_qpl4wt,All of them!,22,0,0,False,False,False,1636405832.0
qpl4wt,hjvmmrr,t3_qpl4wt,"The human genome project was only possible with computer science. Sequencing using accurate machines was going too slow and not really working right. Then someone had the idea to use cheap and high throughout machines even though they made tons of mistakes. The trick was to read each genome lots of times on these fast machines and use computers with statistical algorithms to piece it back together and repair errors. This is now the standard way to sequence genomes in general (at least when doing the whole genome rather than looking for specific markers). Genomics is very computational in nature and has had a direct impact on medicine and agriculture (and tons of other stuff).

The reality is that I could probably do this for any scientific or engineering field. Airplane companies only do wind tunnel stuff at the very end because of computers. Drugs are analyzed and candidates chosen using computers. Materials are investigated with simulations and machine learning and such. Astronomy uses massive computations to answer questions and make sense of the massive amounts of data we collect. There are projects to map neurons in brains using computer vision techniques. All of these involve algorithms and math, but they also rely on advances in computer hardware, networks, languages, and systems. Ultimately we do what we do so that other people can use computers for what matters (hopefully).",10,0,0,False,False,False,1636420341.0
qpl4wt,hjvql2k,t3_qpl4wt,"Pretty much every field needs robust solvers for systems of linear equations, least squares problems, eigenvalue problems, and singular value problems. That's computational science (not computer science) in a nutshell.",6,0,0,False,False,False,1636422140.0
qpl4wt,hjuqis6,t3_qpl4wt,The entire field of bioinformatics is a good example,4,0,0,False,False,False,1636406484.0
qpl4wt,hjvohqv,t3_qpl4wt,"I did some computational research as a chemist when I was in my undergrad.  Not much was accomplished by myself.  However, the school I graduated from literally has a super computer to do quantum calculations for chemistry and biology, including a whole department that does research just for computation.

EDIT: I imagine physics and many others use it a lot too for other stuff, but during my time there I only saw biology and chemistry stuff.",3,0,0,False,False,False,1636421191.0
qpl4wt,hjw3zn2,t3_qpl4wt,"I have an entire course named computational physics, which is all about using computer programs to find solutions and different scenarios to physical equations.",3,0,0,False,False,False,1636428171.0
qpl4wt,hjvs1eo,t3_qpl4wt,I'm pretty sure every laboratory uses software to analyze and store data,1,0,0,False,False,False,1636422792.0
qpl4wt,hjupyc8,t3_qpl4wt,The proof for the 4-color theorem was the first one to be done with help from a computer iirc if you're looking for specific examples,1,0,0,False,False,False,1636406262.0
qpl4wt,hjvgcnd,t1_hjuovnq,This. Imagine doing statistics without a computer.,11,0,0,False,False,False,1636417444.0
qpl4wt,hjvn2o8,t1_hjuqis6,"Was just about to suggest this. Only thing is that you need a PhD and some research experience, and additional knowledge of biology.",2,0,0,False,False,False,1636420546.0
qpl4wt,hjvhu12,t1_hjupyc8,It was not the first one to be done with help from a computer - it was the first proof where the calculations involved in the proof essentially *had* to be done on a computer. Computers were assisting in research and proofs long before that.,3,0,0,False,False,False,1636418122.0
qpl4wt,hjwj59i,t1_hjvgcnd,you can have computers without computational science though,2,0,0,False,False,False,1636435896.0
qpl4wt,hjx71tv,t1_hjwj59i,"Well, maybe. But who are the people making those algorithms go brrr (sorry)? What people are responsible for making it feasible to program them, for making it secure etc",1,0,0,False,False,False,1636455027.0
qp5vod,hjrpbuu,t3_qp5vod,"Yeah, that's pretty dense. But if you go one sentence at a time and really think through each of the definitions, you can make sense of it. Reading math literature (which this essentially is) is not like reading a novel. You frequently have to go to prior sentences to remind yourself of what something was defined as being.

What Knuth is actually saying though isn't too complex (though he certainly makes it look like it).

In the first two sentences, he's remarking on the fact that the algorithm defined previously (I'm looking in my copy of Volume One of *The Art of Computer Programming*, though mine is the second edition from 1973) as quadruple (*Q*, *I*, Omega, *f*) is not sufficiently restrictive, as it isn't necessarily *effective* (previously defined as being computable by a pen and paper in finite time).

He then goes on to provide a restriction of the quadruple such that the algorithm *must be* effective. He then remarks about how the restriction he provides could be done in many different ways, specifically noting Turing's notion of effectiveness (the most famous and earliest such example), and Markov's notion given in *The Theory of Algorithms*.

How what he wrote corresponds to what I just described can be tricky to figure out, but if you want to read *The Art of Programming* I suggest you try to get used to it.",63,0,0,False,False,False,1636347120.0
qp5vod,hjt3n8y,t3_qp5vod,"I am a little rusty on the details but I believe he is describing a version of Churchs lambda calculus, one of the earliest ways to define an abstract notion of what a human can compute: https://en.m.wikipedia.org/wiki/Lambda_calculus

Turing came up with an equivalent version that most (including Church) think is more natural and elegant: https://en.m.wikipedia.org/wiki/Turing_machine",5,0,0,False,False,False,1636382575.0
qp5vod,hjtvxiw,t3_qp5vod,"Shouldn’t it be obvious?

The f theta omega lambda = theta N, with respect to x1 … xj such that the compute sequence is within Q",1,0,0,False,False,False,1636394164.0
qp5vod,hk8fohv,t3_qp5vod,"Just in case you are still thinking about this. This website does a wonderful job of explaining what's going on with Knuth's definition of an algorithm and compares it to a Markov Algorithm.
https://www.rudikershaw.com/articles/computationalmethod3",1,0,0,False,False,False,1636656177.0
qp5vod,hjrqs13,t1_hjrpbuu,Thanks so much for the reply. I guess I’ll have to dig into this more and try to diagram it out for myself a bit. I appreciate you taking the time!,9,0,0,False,False,True,1636348033.0
qp5vod,hjsf1oa,t1_hjrpbuu,"Yeah, I sometimes just scribble something when reading, visualizing the definitions. That can help.

Thanks also for the explanation.",2,0,0,False,False,False,1636367723.0
qp5vod,hjt3ojb,t1_hjt3n8y,"**[Lambda calculus](https://en.m.wikipedia.org/wiki/Lambda_calculus)** 
 
 >Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It is a universal model of computation that can be used to simulate any Turing machine. It was introduced by the mathematician Alonzo Church in the 1930s as part of his research into the foundations of mathematics. Lambda calculus consists of constructing lambda terms and performing reduction operations on them.
 
**[Turing machine](https://en.m.wikipedia.org/wiki/Turing_machine)** 
 
 >A Turing machine is a mathematical model of computation that defines an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the model's simplicity, given any computer algorithm, a Turing machine capable of simulating that algorithm's logic can be constructed. The machine operates on an infinite memory tape divided into discrete ""cells"". The machine positions its ""head"" over a cell and ""reads"" or ""scans"" the symbol there.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",7,0,0,False,False,False,1636382592.0
qp5vod,hjuh4da,t1_hjtvxiw,Not helpful but thanks for taking some time anyway,8,0,0,False,False,True,1636402730.0
qp5vod,hjrrgzp,t1_hjrqs13,Yeah sure thing! And definitely have a piece of paper next to you when reading this kind of stuff. You can only keep so much in your head at once.,10,0,0,False,False,False,1636348484.0
qp5vod,hjuh2zs,t1_hjt3ojb,"Thank you so much for taking the time to link those pages for me! I appreciate the response.  You’re absolutely correct as well, the next portion mentions these very briefly.",4,0,0,False,False,True,1636402715.0
qp5vod,hjv58oq,t1_hjuh4da,"FYI this was just a joke comment at how complicated it looks. 

I was mostly just trying to say random variables and pretend I knew what it was saying.",2,0,0,False,False,False,1636412506.0
qp5vod,hjru72d,t1_hjrrgzp,"I've found that it is much easier to keep a lot more in my head when it is something I'm writing, speaking, or coding than it is when reading something that someone else wrote. One of the real quirks of cognition right there, as the difference is massive. One of the things that makes something like that harder to hold in the mind (for the reader, as I have done this plenty as a writer and felt no shame about it until a few weeks later when trying to read what I'd written) is the use of single letter variables and Greek letters rather than words. There are likely a great deal of programs or examples of pseudocode that are easier to read than that, despite being potentially more complex or lengthier. 

That said, the last time I took an algorithms class I did get pretty good at turning math-speak into pseudocode and found the process very rewarding once I got the hang of it. Once you've done it a few times, Wikipedia is your oyster and the sky is the limit for implementing random stuff based on vague or highly complex descriptions in the language of math. 

For someone looking to get better at it, who enjoys math, logic, and coding but lacks a higher math education, would you say that ""The Art of Computer Programming"" is a good and useful read in the 21st century for someone looking to get good enough to contribute to the field in a serious way?",5,0,0,False,False,False,1636350293.0
qp5vod,hjvzklv,t1_hjv58oq,Oh sorry haha! I’m used to seeing the /s on Reddit!,3,0,0,False,False,True,1636426150.0
qp5vod,hjrxtdk,t1_hjru72d,"Totally agreed. I often read something I had written a few weeks ago and become totally confused. But if I write down something someone else wrote I immediately understand it better. And I'm at the point where some Greek letters (alpha, beta, gamma, delta, epsilon, theta, omega) are easy for me to read, but most of them introduce cognitive load that makes understanding more difficult. Which is weird given that they're all just arbitrary symbols.

But I'm not nearly at the point where I can comment on contributing to the field. And I read *The Art of Computer Programming* more out of historical interest than a desire to learn computer science. I can say though that it's definitely not good as an introduction to computer science. Knuth was a younger man when he first wrote them, and younger men almost always try to look smart. If you're not already knowledgeable in the subject much of what he wrote will go over your head (and by design). Doubly so if you lack a math background. What you want is an author who's trying to seem less intelligent than they actually are (Brian Kernighan is a great example, though he hasn't really written about computer science, just computers). For that though I don't have many good recommendations, though one book I always recommend to people trying to learn more about computers is *Code*, by Charles Petzold.",5,0,0,False,False,False,1636352887.0
qp5vod,hjs2sfu,t1_hjrxtdk,"Many thanks! I saw an interview with Kernighan once and was shocked at his easy and humble demeanor. Really chill dude. I admit to wanting to give Knuth a read just to see if I can, though. It's always fun to try and climb those kinds of mountains.",2,0,0,False,False,False,1636356748.0
qp5vod,hjs3ca6,t1_hjs2sfu,"I know, Kernighan is great, and especially for his age! He talks with the energy of a twenty year old, but with the humility and wisdom of someone, well, his age. I hope I can be as sharp as him when I get up there in years.

And I totally relate. It's kind of a sense of pride when you read ""the canon"", the works of The Greats. I'd say go for it if you want to, just don't feel the need to do so right away, and come well prepared.",3,0,0,False,False,False,1636357204.0
qpr8lq,hjw0wvt,t3_qpr8lq,r/netsec r/cybersecurity,2,0,0,False,False,False,1636426763.0
qpjwrs,hjuba6o,t3_qpjwrs,"> I don't see why we don't just use a [NULL] character at the end of every string if it's so much more efficient than the alternative.

It's not ""so much more efficient."" In many cases it is _far less efficient_, for example, when you need to determine the length of the string. If the string is stored along with its length, determining the length is instantaneous. But if it's stored with a null terminator, you need to count every character.

Storing the length also makes it possible to reference substrings from anywhere within a larger string. With null-terminated strings, you can only reference a substring which goes until the end. For example if the string is ""HELLO WORLD"", in C (a null-terminated string language) you could refer to the substring ""WORLD"", by adding 6 to the start address, but you could not refer to the substring ""HELLO"" without modifying the data to replace the space with a null character. In Rust (another systems language, with length-based strings), you can split the string into ""HELLO"" and ""WORLD"", without modifying the ""HELLO WORLD"" string data.

Null-terminated strings are only more efficient by saving a few bytes per string, which rarely matters anymore, because memory capacities have increased from kilobytes to gigabytes.",12,0,0,False,False,False,1636400370.0
qpjwrs,hju8sc6,t3_qpjwrs,"It's more memory efficient, but computationally less efficient.",6,0,0,False,False,False,1636399374.0
qpjwrs,hjvndnd,t1_hjuba6o,"Why not go with both then? If the real struggle these days is in efficiency over storage, why not store the length and a null character, and the compiler could just use whatever is most effective for a specific operation? Or is that overthinking it a little too much?",2,0,0,False,False,False,1636420684.0
qpjwrs,hjvyc4f,t1_hjvndnd,"That does happen, for interoperability's sake. It doesn't save any storage — it uses slightly more, because you store the length _and_ an extra null character. The C++ specification requires a `string` to store its length, but most implementations also add a null terminator, making it easier to send the string to C functions.",2,0,0,False,False,False,1636425596.0
qpjwrs,hjw9ikp,t1_hjvyc4f,"Ah okay, that last part was interesting, I guess I worded it wrong because I already understood that first part, but it's interesting that it exists like that",2,0,0,False,False,False,1636430756.0
qon555,hjny21c,t3_qon555,[deleted],23,0,0,False,False,False,1636287191.0
qon555,hjosyn2,t3_qon555,Silicon Valley is great,17,0,0,False,False,False,1636301878.0
qon555,hjojwch,t3_qon555,"Numbers is a TV crime/investigation TV show that centers around a math professor's insights into problems. Math, physics, CS, etc. are all part of every episode.",11,0,0,False,False,False,1636298243.0
qon555,hjqhzjf,t3_qon555,"Typically crime shows tend to have a character that is their tech/hacking specialist. The first show that came to mind for me was Criminal Minds, there is a character who’s only job is hacking and the show often shows her referencing linux, pinging IP addresses, etc",6,0,0,False,False,False,1636326222.0
qon555,hjor9pb,t3_qon555,Futurama is loaded with CS jokes and references.,9,0,0,False,False,False,1636301201.0
qon555,hjr1q7g,t3_qon555,"The Devs miniseries has references to some actual quantum computing topics, including a random mention of Shor's algorithm IIRC (although that one was just in some chitchat).",3,0,0,False,False,False,1636335183.0
qon555,hjrs7mk,t3_qon555,Halt and Catch Fire. Excellent drama at the backdrop of 4 different eras of tech booms,3,0,0,False,False,False,1636348968.0
qon555,hjrk2ys,t3_qon555,Definitely silicon valley and mr robot!,1,0,0,False,False,False,1636344155.0
qon555,hjrp2v4,t3_qon555,The Imitation Game,1,0,0,False,False,False,1636346967.0
qon555,hjnypju,t1_hjny21c,"Cool! I haven't seen this series yet, but I'll definitely put it on my list",6,0,0,False,False,True,1636287628.0
qon555,hjrg7gp,t1_hjosyn2,Yup! like gilfoyle the most!,3,0,0,False,False,False,1636342144.0
qoyr6v,hjqbxw5,t3_qoyr6v,"i've worked with embedded systems and board bring-up (baremetal or w/ u-boot) is what i do ... reading RAM specs, and programming the memory controller.

1 RAM has many interface, mix and match is a PITA. physical interface. access protocols. then ranks, and banks. (*i shudder as i remember a project, F that sheyt*)

2 you need a memory controller specific to the RAM modules. most controllers i've used are built-in to an ASIC

3 you need to think about battery backup and good power smoothing

4 RAM is very fast, USB is slow. you are at the mercy of USB throughput speeds.

> the fact that this software exist is proof that a ram disk is possible

remember (*or maybe not*) DOS had a RAMDISK.",4,0,0,False,False,False,1636323618.0
qoyr6v,hjrmk9v,t3_qoyr6v,"Check out Gigabyte's i-RAM. It is an ""external"" RAM disk, but it uses the PCI bus and SATA.

https://www.youtube.com/watch?v=bYbCYgYZVT8",2,0,0,False,False,False,1636345501.0
qoyr6v,hjqe0ua,t1_hjqbxw5,"I am indeed too young for DOS 😂 but I do know what you are talking about, thank you for the helpful input. I'll likely ask you and others more questions as I dig deeper into this project. 

Hmm...  I'm looking at cat8 data transfer rates and it looks like we have a max trans. Speed of 40gbs and under Old ddr3 ram I'm seeing Max trans. Of 12.8gbs on the (possibly more common) 1600's. I imagine thats per module, so I'd max out the cat8 or USB gen 3.2 bandwidth at around 3 modules. Assuming I could get an integrated controller to distribute the data properly amongst the modules. Which for now, is good enough for proof of concept... Though eventually I'd like to make that 4 or 6 module capacity for people who have upgraded more than 1 laptop, I currently have 4 sticks of ram from 2 different laptop upgrades I performed. Different sizes, and potentially different generations (one may be ddr4, laptop was almost 10 years newer) I'm liking this idea for a cool tinkerer project. 😁",1,0,0,False,False,True,1636324509.0
qoyr6v,hjr4ajn,t1_hjqe0ua,"cool tinkerer project as it is, but you really need to properly interface (physically and protocol-wise) those RAM properly first (HW and very low level firmware). i don't see any hobby boards that provide slots/HW interfaces for these modules.",2,0,0,False,False,False,1636336395.0
qoyr6v,hjr5gab,t1_hjr4ajn,"I was thinking about the hardware already, I figured I could scavenge or buy some ddr3 dim slots from somewhere, as for the low level programming needed for the firmware, I'd have to find a way to write a custom code, likely in c+ to organize and identify the hardware, and initialize a storage standard like nvme or something to layer on top of the ramdisk software. It's gonna be a pain, but yea. 
It's doable, a 3d printer would be good to make the dimm module housing.
 Mind sharing a source on good learning material to write low_level firmware? I'm familiar with a little bit of python and JavaScript.
 But nothing worth noting, I'm not currently capable of making standalone apps as of yet.",1,0,0,False,False,True,1636336942.0
qoyr6v,hjr5hd7,t1_hjr4ajn,I am open to learning tho.,1,0,0,False,False,True,1636336956.0
qoj0r4,hjndhun,t3_qoj0r4,[deleted],21,0,0,False,False,False,1636268981.0
qoj0r4,hjnn8a4,t3_qoj0r4,"So this is a form of multi-factor authentication. Instead of ""something you know"" (password) + ""something you have"" (fingerprint, phone, etc) it's ""somewhere you are"" (location). Even if the location could be spoofed, an adversary would have no way of knowing the position of the device ahead of time, and a robust lock-out mechanism would guarantee brute forcing it becomes unfeasible.

...or does it? In your threat scenario, your laptop is tied to a physical location. If it's stolen, an adversary would probably notice the GPS sensor and find a way to spoof it so it reports the same location where it was stolen (having them stolen it themselves). So at best, it's not a great advantage.

You could do variations of the above where the GPS coordinates (down to a reasonable number of decimal points, to have precision but be usable) could be used to initialise some variation of [HOTP](https://en.wikipedia.org/wiki/HMAC-based_one-time_password) like TOTP (think Google Authenticator) so that instead of time, the second factor is coordinate-based. Which would make for an interesting Cicada-like game, where the laptop can be unlocked only at different geographical locations... or be given to different people, each of them able to unlock it only in one location only they know. 

I'm fairly certain there are some corner cases whereby the procedure would risk failing open (for example, how to ""prime"" the laptop to unlock in certain locations without someone knowing all the locations ahead of time), but it might make for a good story :)",3,0,0,False,False,False,1636278249.0
qoj0r4,hjnyr0i,t3_qoj0r4,"Firstly, yes. Secondly stop watching too much agents of shield. Thirdly, i fully agree with the answer of u/No_Engineering8506 but would say its safer to base it on position, time of creation and motion, i.e. the device would only be accessible on the earth but only directly beneath the iss (or something)",3,0,0,False,False,False,1636287654.0
qoj0r4,hjnk1w9,t3_qoj0r4,"Assuming a “trusted geolocation device” which does both encryption and decryption based on (location, password) there is still the possibility of an attack by manipulating the satellites. This may result into the access of the encrypted data or worse the inaccessibility of all data encrypted with TGDs.",1,0,0,False,False,False,1636275314.0
qoj0r4,hjnmld6,t3_qoj0r4,"technically yes, but very difficult to make integrally. it might be easier to do it with local network (if this still meets your criteria).",1,0,0,False,False,False,1636277671.0
qoj0r4,hjp088t,t3_qoj0r4,"You might be able to use nearby wifi SSIDs for geolocation to avoid spoofing 🤔 that comes with its own host of problems, like if the wifi around you changes suddenly you can't unlock your device.",1,0,0,False,False,False,1636304760.0
qoj0r4,hk0pvmx,t3_qoj0r4,"Yes, it would simply be another layer attributed with multi-factor authentication methods. How would you determine location? Longitude/latitude can have whatever degree of precision (by degrees, minutes, and seconds) and altitude is harder to measure and identify on a latitude/latitude basis. Your level of precision would influence the length of the brute force process, and unless you had ridiculous sensitivity on location - which would be troublesome - it would become easier to bypass this authentication method.",1,0,0,False,False,False,1636511802.0
qoj0r4,hjnevuy,t1_hjndhun,"I see, so if someone wanted to have a password on their PC but very deep within the encryption have it locked to altitude, that way all you need do is be at the specific height. 

If when entering the password you are not at the altitude required to have the PC unlocked could it then be set to erase all data and then it is set to overheat and then explode inside insuring that all Data is as unrecoverable as possible?

So the altitude is the door and the password is the key. 

Without the door itself the key is useless. 

What do you think?",1,0,0,False,False,True,1636270291.0
qoj0r4,hjnn93j,t1_hjnn8a4,"**[HMAC-based one-time password](https://en.wikipedia.org/wiki/HMAC-based_one-time_password)** 
 
 >HMAC-based one-time password (HOTP) is a one-time password (OTP) algorithm based on hash-based message authentication codes (HMAC). It is a cornerstone of the Initiative for Open Authentication (OATH). HOTP was published as an informational IETF RFC 4226 in December 2005, documenting the algorithm along with a Java implementation. Since then, the algorithm has been adopted by many companies worldwide (see below).
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1636278269.0
qoj0r4,hjnge0q,t1_hjnevuy,[deleted],2,0,0,False,False,False,1636271739.0
qoj0r4,hjnicmp,t1_hjnge0q,"This is mainly for personal curiosity, although any and all knowledge gained could come in useful. 

So things can be tricked and tempered with, what about old school satellites that are used for navigation instead of the internet, if somehow someone managed to connect the PC so it was only accessible when connected to a sat nav and it’s can’t connect to the internet because it’s hardware was removed and the software was also corrupted specifically the software for internet access. If then it could only be unlocked with connection to a sat nav connection and then only when you reached a certain altitude with 10m SQ at around that certain altitude, then you enter the password and then it unlocks safely, if however the altitude is way off or the password is incorrect on the first attempt, it would then erase all data followed by the CPU and the data drive heating up until it is completely destroyed. 

Would this be more secure instead of having it rely on GPS via the internet?",1,0,0,False,False,True,1636273681.0
qoj0r4,hjniv05,t1_hjnicmp,[deleted],2,0,0,False,False,False,1636274185.0
qoj0r4,hjnmepw,t1_hjniv05,and there's the other thing that this is basically just a longer password. Adding a couple characters to the password is going to be better than jumping through hoops like this for anything except an ARG.,2,0,0,False,False,False,1636277507.0
qoqb6q,hjp39dh,t3_qoqb6q,"There is a lot of research. I'm not sure exactly what you are looking for, but if you refine this search you are likely to find it.

https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=non+computable+functions&btnG=&oq=non-comput",2,0,0,False,False,False,1636305950.0
qoqb6q,hjr1o7t,t3_qoqb6q,"Check out Barak’s Intro to Theoretical Computer Science book, available online.

He presents a very explicit concept of uncomputable, and the many functions that fall into that category (including the Halting problem). Highly recommended.",2,0,0,False,False,False,1636335157.0
qoqb6q,hki1xl0,t3_qoqb6q,"Look for papers by Alan Turing, Alonzo Church, Kurt Gödel, Jacques Herbrand, and Andrey Markov Jr.

Generally speaking it comes down to declaring that what computation is are processes that complete in a finite number of steps to reach a result, and showing that some processes cannot be shown to have a finite set of steps that will do so. The ones that cannot be shown to do so are precisely those that are not computational processes. Those algorithms are not computable.For Turing it was showing that some processes can't be encoded as instructions on a machine that can be shown to stop processing when the machine processes the instructions, and there was no set of instructions that could tell you whether the instructions for other programs would result in a machine that stops processing or not. For Church and others not being computable meant there was no way to reduce some logical formulas using recursive processes in such a way that would eventually be considered maximally normalized, and there is no such recursive process that can show whether another formula has a maximally normalized form or not.",2,0,0,False,False,False,1636837382.0
qo5xff,hjkwted,t3_qo5xff,Your terminology is a bit confusing. A set only has unique elements to start with. Give an example of what you want?,15,0,0,False,False,False,1636223675.0
qo5xff,hjlj3sg,t3_qo5xff,"Searching set coverage is NP-hard, and thus has no polynomial time solution. As such a good greedy algorithm is your best bet to find the *exact* optimal solution. If instead you are interested in finding an *approximation* of the optimal solution, then there are many options in terms of [sampling](https://www.mit.edu/~mahabadi/slides/sublinear-sc.pdf) or [linear programming with relaxed constraints](http://theory.stanford.edu/~trevisan/cs261/lecture08.pdf).",2,0,0,False,False,False,1636233520.0
qo5xff,hjm0cv5,t3_qo5xff,"So, you have X sets (sets cannot have the same element in them twice, those are usually called ""bags"") and you get to choose n of them, which n do you choose to maximize the number of elements in the total union of all n sets?

This is definitely related to the set cover problem:  
[https://en.wikipedia.org/wiki/Set\_cover\_problem](https://en.wikipedia.org/wiki/Set_cover_problem)

I'd recommend looking at using a dynamic programming algorithm for this, there may be a simple recursive formulation that, when used with memoization yields a simple, elegant, performant algorithm",2,0,0,False,False,False,1636241349.0
qo5xff,hjmhyg1,t3_qo5xff,"This seems very similar to this Problem:
[Set over](https://en.m.wikipedia.org/wiki/Set_cover_problem). 

This a NP-Complete problem so there is a good chance you wont find a polynomial algorithm for it. 

I thought about this problem a bit now and i dont think you can get something much better than brute force. There might be a dynamic solution though that runs in linear time with respect to the Capacity (the range of possible values in your case). 

If you need further help just ask me^^",2,0,0,False,False,False,1636249623.0
qo5xff,hjmjjlk,t3_qo5xff,"You want to select the set with the most unique numbers?

Find the set with the most numbers, let's call it ""set X"", than count how many unique numbers it has, let's call it ""N count"". Than, go through each set. If the set size is smaller or equal to ""N"", than don't count it. If it's bigger, than count it.

I don't see how you could find the set with the most unique numbers without counting the unique numbers on each set. A good optimization is to count the number for each set that is added or changed, so when you need to find the set, you already know it",1,0,0,False,False,False,1636250389.0
qo5xff,hju4d4m,t3_qo5xff,"Believe this is an NP hard probelm, although I remember that the greedy approach (always choosing the set with the most uncovered items (items which are not already seen) is discussed as an approximation.",1,0,0,False,False,False,1636397594.0
qo5xff,hjkxkct,t1_hjkwted,"Yes, but a want to maximize unique elements among sets. Choose x number of sets that maximize unique elements",2,0,0,False,False,True,1636223994.0
qo5xff,hjky1ub,t1_hjkwted,"Example: I am proggraming something to find (among a list of texts, songs for example) the best n songs to learn in  order to maximize vocab (based or not in a desired vocab list)",2,0,0,False,False,True,1636224208.0
qo5xff,hjn1n3p,t1_hjkwted,">  A set only has unique elements to start with

btw I think that sets can have duplicate elements to start with, but it is useless to have duplicate elements. In other words, it is not a rule that a set cannot have duplicate elements but there is no use of having duplicate elements. You can read more about it [here](https://stackoverflow.com/questions/10011475/can-a-set-have-duplicate-elements)",1,0,0,False,False,False,1636260138.0
qo5xff,hjm0ej6,t1_hjm0cv5,"**[Set cover problem](https://en.wikipedia.org/wiki/Set_cover_problem)** 
 
 >The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. It is one of Karp's 21 NP-complete problems shown to be NP-complete in 1972. It is a problem ""whose study has led to the development of fundamental techniques for the entire field"" of approximation algorithms. Given a set of elements                         {         1         ,         2         ,         .
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",2,0,0,False,False,False,1636241370.0
qo5xff,hjmiiut,t1_hjmhyg1,"Thank you! I'm trying to read about the cover set problem, many people pointed me there and seems to be similar indeed.

I don't know much about math but what you mean by dynamic solution is something related to optimization problems? Maybe it could be solveable through optimization algorithms considering cost function as number of chosen sets or something?",1,0,0,False,False,True,1636249893.0
qo5xff,hjn1as1,t1_hjmjjlk,This won't work. It's possible a union of smaller sets yields more unique elements than the largest set.,1,0,0,False,False,False,1636259945.0
qo5xff,hjkxvl5,t1_hjkxkct,Look into set cover problem and see if you can write your problem like it. If you can then use the best available method for finding set covers?,7,0,0,False,False,False,1636224132.0
qo5xff,hjky7ll,t1_hjkxkct,"What are the input parameters to the algorithm? I think that's what I'm not understanding. 

It almost sounds like you have a minimum subset cover problem on your hands, but that depends on what exactly are the input parameters to the function and what do you want it to output exactly.",3,0,0,False,False,False,1636224278.0
qo5xff,hjn2ovr,t1_hjn1n3p,"A set does not have ""duplicate elements"" means you don't write a single element name twice. E.g., `{1,1,1,1} = {1}` and both sets have cardinality 1. A set is defined by membership rules. An element belongs to a set if it passes those membership rules. If an element passes membership rules for numerous reasons you still only write it once in the set. Z unioned with Z is still Z. Now there does exist a [multiset](https://en.wikipedia.org/wiki/Multiset) which allows for duplicates as you are thinking of them.

Stack Overflow is good place to get help with code but a terrible place to get your maths definitions from. Usually, you can visit [math.stackexchange.com](https://math.stackexchange.com/).",2,0,0,False,False,False,1636260756.0
qo5xff,hjn1kl6,t1_hjmiiut,"They are most likely referring to Dynamic Programming https://en.m.wikipedia.org/wiki/Dynamic_programming
Programming in this sense is referring to filling in a table with solutions as you go along.",2,0,0,False,False,False,1636260097.0
qo5xff,hjn6jsx,t1_hjn1as1,"If a set has 10 unique numbers, than no set with less than 11 numbers can contain more than 10 unique numbers. Simple

Might as well start with the largest set of random numbers, because it's more likely to be the one",1,0,0,False,False,False,1636263310.0
qo5xff,hjl3o10,t1_hjkxvl5,"Thank you. I'll take a look into it. I was familiarize with the existence of optimization problems, but have never seem the term ""discrete optimization"".

[Looks indeed](https://www.youtube.com/watch?v=cjSeHSjPmsk) very similar to what I'm searching for here.

Thanks.",3,0,0,False,False,True,1636226679.0
qo5xff,hjkytbg,t1_hjky7ll,"Simpler version: I have a list of sets and want to find the best n sets that (together) maximize unique elements.  


less simpler version: I have a list of sets and a list of elements and I want to find the best n sets that (together) maximize unique elements that are present in the element list",1,0,0,False,False,True,1636224541.0
qo5xff,hjn2q2j,t1_hjn2ovr,"**[Multiset](https://en.wikipedia.org/wiki/Multiset)** 
 
 >In mathematics, a multiset (or bag, or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The number of instances given for each element is called the multiplicity of that element in the multiset. As a consequence, an infinite number of multisets exist which contain only elements a and b, but vary in the multiplicities of their elements:  The set {a, b} contains only elements a and b, each having multiplicity 1 when {a, b} is seen as a multiset. In the multiset {a, a, b}, the element a has multiplicity 2, and b has multiplicity 1.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",1,0,0,False,False,False,1636260776.0
qo5xff,hjn1m3k,t1_hjn1kl6,"Desktop version of /u/codeIsGood's link: <https://en.wikipedia.org/wiki/Dynamic_programming>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",2,0,0,False,False,False,1636260122.0
qo5xff,hjoj9i8,t1_hjn6jsx,"The algorithm returns a list of sets to choose not a single set to choose.

Example where this doesn't work.

Set 1: {A,B}

Set 2: {C}

Set 3: {D}

 Set 4: {E}

The algorithm would output sets 2, 3, and 4 since the union of sets 2, 3, and 4 has more unique elements than only set 1, yet set 1 has a higher cardinality than any other individual set.",1,0,0,False,False,False,1636297971.0
qo5xff,hjymczv,t1_hjn6jsx,Parkour has been on-point since Origins. It was an accident. They ended up giving up on Yamaha and just sent them to a custom body shop to get it over my leathers,1,0,0,False,False,False,1636480222.0
qo5xff,hjkz860,t1_hjkytbg,"That still doesn't really answer the question. 

Are you given n? Or do you find n? And what do you mean by ""best n sets?""",4,0,0,False,False,False,1636224722.0
qo5xff,hjl8pc0,t1_hjkytbg,"This is set cover, which is NP-Complete for finding the optimal solution: [https://en.wikipedia.org/wiki/Set\_cover\_problem](https://en.wikipedia.org/wiki/Set_cover_problem)

You probably want to just pick a group of sets at random, or to pick K sets from largest to smallest and take their union (this is the greedy approximation algorithm described here: https://en.wikipedia.org/wiki/Set\_cover\_problem#Greedy\_algorithm).",2,0,0,False,False,False,1636228887.0
qo5xff,hjkzlvs,t1_hjkz860,"Parameters:

I) collection of sets

II) n - size of group to consider

III) (Optional) - list of elements

Output: which n sets from collection should be selected in order to maximize unique elements (if parameter III is given, consider only elements from that list) considering all sets from collection.

Is that description more accurate?",1,0,0,False,False,True,1636224887.0
qo5xff,hjlbsik,t1_hjl8pc0,"Thank you. I'll study it. But as far as I can tell, adding the next subset with the most uncovered points each time is not an optimal way to do it in that case. Is that what greedy aprox do? (Don't bother to answer if you don't want to, once I have not completely understood the wiki page yet). tks!",1,0,0,False,False,True,1636230255.0
qo5xff,hjlf9o5,t1_hjl8pc0,"Actually, it's closer to this related problem, which is still NP-Hard: [https://en.wikipedia.org/wiki/Maximum\_coverage\_problem](https://en.wikipedia.org/wiki/Maximum_coverage_problem)  


It has the same sort of greedy approximation algorithm, which chooses next the set with the most uncovered items.",1,0,0,False,False,False,1636231783.0
qo5xff,hjlfqfy,t1_hjl8pc0,"FWIW, you \*can\* solve this exactly for N sets by checking all 2\^N combinations of sets, and seeing which has the largest number of covered items for the fewest chosen amount of sets, but it's an exhaustive exponential search that's only feasible for small N.

You can get an approximate result (which may be good enough) by randomly choosing K < N sets, checking how many items are covered and tracking how large K is, and then iterating repeatedly to keep the solution with the most covered items for the smallest K. This is sort of similar to monte carlo integration.",1,0,0,False,False,False,1636231989.0
qo5xff,hjl1h8c,t1_hjkzlvs,"Yes I understand now. 

This seems like a problem that will generally take exponential time since something like a greedy algorithm wouldn't work here (you can't just pick the largest sets). 

Your best bet is likely to use a backtracking approach, but with some clever pruning to cut down on runtime (stop searching using a specific set if it isn't adding enough elements to the total cover or something like that).",1,0,0,False,False,False,1636225705.0
qo5xff,hjl47ru,t1_hjl1h8c,"Thank you. That pruning idea seems very usefull to be used together with some sort of discrete optimization problem mentioned above.

I'll search more about what you've mentioned. 

Appreciate it.",1,0,0,False,False,True,1636226917.0
qoqmin,hjqj23r,t3_qoqmin,Wow,1,0,0,False,False,False,1636326702.0
qoqmin,hjrmv2v,t3_qoqmin,"finally, no TLE for me on LC questions",1,0,0,False,False,False,1636345670.0
qoqmin,hjokkkb,t3_qoqmin,"The supercomputer, called Jiuzhang 2, can calculate in a single   
millisecond a task that the fastest conventional computer in the world   
would take a mind-numbing 30 trillion years to do..",0,0,0,False,False,True,1636298520.0
qoqmin,hjr2wxe,t1_hjokkkb,According to China,9,0,0,False,False,False,1636335741.0
qoh47t,hjnafhu,t3_qoh47t,"There is a lot of literature on genetic algorithms and schema theorem. I would suggest reading Holland's original work on it.

To give you a somewhat concise answer, the basic principle is that high-quality solutions have elements (building blocks as Holland called them) that are likely to appear in other high-quality solutions. Hence, with genetic algorithm (GA), a population of high-quality solutions is maintained. Two members are selected and their building blocks (genes) are intermixed. By mainly using the building blocks from these solutions it increases the likelihood of finding a better solution than those in the population. However, since it is possible that a key building block is missing from the population and hence cannot be selected by simply intermixing (crossover), a mutation operation is added to give a chance of finding such an element.

Since GA is non-deterministic it cannot guarantee to find the globally optimal solution. However, it is guaranteed to find better solutions over time because only better solutions are preserved in the survival/culling step. The computational cost for finding a better solution generally increases over time.

Note, that GA is not optimal for all search problems and is highly parameter driven (see various papers by Grefstennete and ""No Free Lunch Theorem""). Parameter optimization is required to make it work well for even those problem spaces for which it can search effectively.

GA is ineffective when the underlying hypothesis is not true, i.e., that high-quality solutions are not composed of building blocks of other solutions. Element independence is good for GA, and interdependence can be bad, as can deceptive search spaces. The best way to really know how to search a particular space effectively is to conduct a fitness landscape analysis.",4,0,0,False,False,False,1636266276.0
qoh47t,hjp0ktk,t1_hjnafhu,Great explanation!,2,0,0,False,False,False,1636304901.0
qoj64n,hjnqcc0,t3_qoj64n,(deleted),7,0,0,False,False,False,1636281066.0
qoj64n,hjnnpsz,t3_qoj64n,"It would be possible if we had completely wireless network infrastructure across the whole planet. There's no reason why this wouldn't be theoretically possible (although in practice, the whole network will probably crawl to a halt with current wireless tech, as it comes nowhere near the throughput of fiber optics), it's just that we haven't built this way (for good reasons, probably).",6,0,0,False,False,False,1636278689.0
qoj64n,hjp1pqa,t3_qoj64n,"Technically yes, practically no.",3,0,0,False,False,False,1636305348.0
qoj64n,hjo9osm,t3_qoj64n,Have you been watching what Elon has been doing lately?,1,0,0,False,False,False,1636293705.0
qoj64n,hjqns50,t1_hjnnpsz,"Exactly, there is only so much RF spectrum we can use which must be shared between wireless devices that are near each other. Whereas each cable can do whatever over the whole spectrum they support with little/no interference with other cables and devices.",3,0,0,False,False,False,1636328755.0
qoifaz,hjok49s,t3_qoifaz,[deleted],1,0,0,False,False,False,1636298334.0
qoifaz,hju598p,t3_qoifaz,"If your asking why the first row and first column are zero, I think we can rationalize as if we have zero items, then no matter what weight we have we cannot generate any value, thus the value is zero. Additionally if we have zero available weight, then no matter how many items we have we cannot generate and value. Thus the first row and first column must be all zeros. From a mathematical perspective, zero should be the base case as when considering element number 1, we just take the weight of zero elements as zero if we cannot add the first element to the knapsack ( it’s weight is too high).",1,0,0,False,False,False,1636397950.0
qoifaz,hjoke3x,t1_hjok49s,I mean we initialize the entire array _(each box)_ in the code to 0. Why do we only take zero? Why not another number?,1,0,0,False,False,True,1636298448.0
qnl3xe,hjilk4q,t3_qnl3xe,This is a good question.........,14,0,0,False,False,False,1636176052.0
qnl3xe,hjiqprm,t3_qnl3xe,"I think this is sort of like asking what is the difference between a hammer for a carpenter and a hammer for a blacksmith.  Like, they're both tools, and they both hit things, but it's the what the tool is building that's the different.  The carpenter builds a house, the blacksmith builds more tools.  The Statistician (carpenter) is using ML build some data or interpret it.  But CS is using ML to build more or better tools.

I think as you mentioned that /r/statistics is more concerned about the interpretable models they get out using ML.  And that makes sense.  I think CS is more concerned with the algorithms to make those models.  

There's a ton of overlap, ML is like applied statistics using computer science algorithms.  But, CS tends to focus more on how to efficiently build the tools (so to say).  Which algorithms best fit the application and how to arrange the data so that it can be taught to the model. When working with ML there's more focus on the kinds of regressions, variance and bias from some given training set vs the test sets.  The overall goal in CS ML is to find ways to get the best resolution of the data while limiting the BigO time it takes to do it.

For instance.  A statistician doesn't really need to care too much about how long it takes for an ML to process a model.  If they're just analyzing numbers of some huge data set about global wind trends, or aggregate user data and viewership trends.  There's no pressing need that the ML be able to instantly pop out a prediction in miliseconds given some new input, just that whatever answer it arrives at is correct.  CS will say, okay, it's correct, but now my autodriving car just crashed because it wasn't able to quickly make that prediction.  So CS will want to know, ""how can I get as accurate of a model as I can, that can process a new input as quick as I can?"".

And obviously, all this with a grain of salt.  This is just generalties betweens fields but obviously individual use cases for each person in their specific industry may differ.  Like I said, there is a ton of overlap.",12,0,0,False,False,False,1636179842.0
qnl3xe,hjkw79u,t3_qnl3xe,"Saw this thread when it was posted in the stats reddit. Here's my tongue in cheek summary.

If you do ml in a stats dept you are smarter, better, more principled, more rigorous, and more interested in interpretability.

If you do ml in a cs dept you will make more money.",2,0,0,False,False,False,1636223405.0
qnl3xe,hjk17n1,t3_qnl3xe,One prob focuses more on how mathematically and less dealing with code while the other prob emphasizes more application of techniques,1,0,0,False,False,False,1636210119.0
qnl3xe,hjilvx2,t1_hjilk4q,"If you’re curious about answers I also posted this question to r/statistics and got a fair deal of responses. Paraphrasing, the consensus seems to be that statisticians are less concerned with neural networks and favor interpretable, empirical verified, analytically sound models. CS people by contrast tend to treat it like more of an experimental science.",10,0,0,False,False,True,1636176279.0
qnl3xe,hjilwm7,t1_hjilvx2,"Here's a sneak peek of /r/statistics using the [top posts](https://np.reddit.com/r/statistics/top/?sort=top&t=year) of the year!

\#1: [\[D\] Accused minecraft speedrunner who was caught using statistic responded back with more statistic.](https://np.reddit.com/r/statistics/comments/kiqosv/d_accused_minecraft_speedrunner_who_was_caught/)  
\#2: [\[D\] Very disturbed by the ignorance and complete rejection of valid statistical principles and anti-intellectualism overall.](https://np.reddit.com/r/statistics/comments/k88ifr/d_very_disturbed_by_the_ignorance_and_complete/)  
\#3: [\[E\] The 2nd Edition of An Introduction to Statistical Learning released. Still free. Lots of new topics.](https://np.reddit.com/r/statistics/comments/p0yg74/e_the_2nd_edition_of_an_introduction_to/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/)",2,0,0,False,False,False,1636176293.0
qnj83v,hjgkguf,t3_qnj83v,"No, no relation. it's 'making' a memo, for example if you want to keep a list of solutions",45,0,0,False,False,False,1636143025.0
qnj83v,hjh2cqz,t3_qnj83v,"I have never seen the two used interchangeably.

If I had to guess, I'd say that a lot of people use 'memorize' in place of 'memoize' because the former is a far more common word than the latter and they're making an innocent typo out of habit.",21,0,0,False,False,False,1636149858.0
qnj83v,hjgmmi8,t3_qnj83v,"Memoization is a specific technique where you store the result of an expensive computation so you don't have to redo that calculation everytime you need it.  It's from an old programming method called 'dynamic programming'.  One example that I see a lot is in Rails, where you might do an expensive database query. Instead of doing that query everytime you need it, you can make it an instance variable:   
\`@some\_var ||= DatabaseRecord.where(query\_is\_expensive)\`    
The ||= operator only does the query if that variable isn't already assigned.  

Another example might be if you are doing a loop with an expensive computation every time. You can just save that computation and reference that variable instead of doing the same thing each time.",53,0,0,False,False,False,1636143830.0
qnj83v,hjif2sb,t3_qnj83v,'Never memoize what you can infer' - Albert Einstein,6,0,0,False,False,False,1636172113.0
qnj83v,hjjaiwz,t3_qnj83v,Nope,4,0,0,False,False,False,1636195534.0
qnj83v,hjoh36m,t1_hjgkguf,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",1,0,0,False,False,True,1636297036.0
qnj83v,hjoh3r9,t1_hjh2cqz,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",1,0,0,False,False,True,1636297043.0
qnj83v,hjhk3je,t1_hjgmmi8,"Calling dynamic programming an ""old programming method"" isn't quite right. Fundamentally it is just a technique for designing algorithms with recursion - it's not ""old"" in the sense that it is outdated. It would be like calling calculus an ""old method for computing derivatives"".

I think the term ""programming"" can throw one off - when the term was introduced it was a more mathematical term, compared to now it is a more engineering term. The term ""linear programming"" is a helpful comparison.",46,0,0,False,False,False,1636157164.0
qnj83v,hjgwcg4,t1_hjgmmi8,What is the difference from caching?,8,0,0,False,False,False,1636147519.0
qnj83v,hjiy3dc,t1_hjgmmi8,"I had this thing in OCAML classes. I always thought it come with ""lazy programming"" since my teacher didn't name it and the main subject was lazy programming.",2,0,0,False,False,False,1636185633.0
qnj83v,hjoh4z8,t1_hjgmmi8,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",1,0,0,False,False,True,1636297058.0
qnj83v,hjgnse9,t1_hjgmmi8,Note: my Rails example may not be a strict example of memoization but it's a similar concept: https://en.wikipedia.org/wiki/Memoization,0,0,0,False,False,False,1636144269.0
qnj83v,hjgv7ib,t1_hjgmmi8,"Can do the same thing in Python with Pandas. Run the expensive query once, store it in a dataframe, assign it to a variable and have access to it repeatedly while only running the query once",0,0,0,False,False,False,1636147089.0
qnj83v,hjhlkxq,t1_hjhk3je,Fair. I meant it more in the sense that it’s been around for a few decades but good point.,4,0,0,False,False,False,1636157823.0
qnj83v,hjhf8so,t1_hjgwcg4,"My understanding is that memoization typically refers to saving the result of some computation.

Caching usually refers to saving data locally that lives in some far away location, not necessarily requiring computation. For instance, web browsers cache certain data from servers for later use, CPUs cache data that lives in RAM, etc. 

Though I'm not an expert by any means, I'm sure there could be exceptions.",22,0,0,False,False,False,1636155109.0
qnj83v,hjhpyuu,t1_hjgwcg4,"u/JazzGateIsReal's answer is perfectly acceptable, but I would add that really there is a bit of ambiguity.

Caching is a more general term - memoization is a specific form of caching that particularly helps when computing recursive functions.

For instance, in Python you can implement memoization by using the built-in `functools.lru_cache` as a decorator on a function (LRU means Least Recently Used - which is how it removes items from the cache).

Caching can be done in more general ways, e.g. to minimise calls to a server in a networking context so that the server is free to attend to more pressing requests.",6,0,0,False,False,False,1636159793.0
qnj83v,hji2joh,t1_hjgwcg4,It’s a form of caching. Caching of the results of functional calls.,2,0,0,False,False,False,1636165647.0
qnj83v,hjgz3bz,t1_hjgwcg4,My Rails example could probably be more accurately called caching.  Memoization more strictly defined is used in recursive functions (as I understand it).,1,0,0,False,False,False,1636148576.0
qnj83v,hjjn6te,t1_hjiy3dc,"*OCaml

It's not an acronym anymore.",0,0,0,False,False,False,1636203565.0
qnj83v,hjhoxv0,t1_hjhlkxq,Makes sense - tbh I figured you meant that. I just wanted to make it clear incase your explanation would mislead someone who hadn't heard of dynamic programming but had done some computer programming.,7,0,0,False,False,False,1636159323.0
qnj83v,hjhfnd6,t1_hjhf8so,Your answer really makes sense.,7,0,0,False,False,False,1636155279.0
qnj83v,hjoh7bd,t1_hjhf8so,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",1,0,0,False,False,True,1636297086.0
qnj83v,hjhsi9m,t1_hjhfnd6,[deleted],3,0,0,False,False,False,1636160956.0
qnj83v,hjoh7xr,t1_hjhsi9m,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",1,0,0,False,False,True,1636297093.0
qnduwn,hjfh7lm,t3_qnduwn,"The M1 doesn't have any eDRAM, it mounts LPDDR4 memory on the same package the processor, resulting in a system-in-package (SiP).

This isn't a novel idea in mobile devices and embedded computers; for example, the original Raspberry Pi from 2012 did it, though it was to reduce the size of the system, rather than to improve memory performance. In the server space, Intel's Sapphire Rapids Xeons support HBM on package. The are coming out later this year or earlier next year, the last time I checked.

Outside of CPUs, GPUs have been doing this for a while; as have the NEC SX-Aurora Vector Engines (since 2017) and FPGAs. Other processors needing high-bandwidth, like neural network processors might be doing the same thing, but I don't know.",25,0,0,False,False,False,1636128073.0
qnduwn,hjffwqf,t3_qnduwn,"I don’t know, seems like a great way to limit upgradability … although I’m not sure there’s too much of that to begin with on that platform.  They had something similar a decade ago or so with stacked RAM. Basically soldered on top of the CPU.",4,0,0,False,False,False,1636127563.0
qnduwn,hjfp5jp,t3_qnduwn,I'm not exactly sure about ram but M1 will and is having a huge effect on the hardware industry. check out the new intel cpu's and you'll notice how alder lake also boosts a number of effeciency and performance cores just like the M1 does. In no time w'll see more ARM in the desktop space.,3,0,0,False,False,False,1636131102.0
qnduwn,hjigowe,t3_qnduwn,"More integrators will do it, Intel has something in the pipeline I believe for Core processors.

Won’t be practical for the high end though, but you can already see in Xeon and IBM POWER, they’ll be growing L1-3 cache from MB to GB, merging L2 and L3 with the POWER architecture looking at shared and concurrent cache across chiplets and even across chips.

It will be some interesting years to come. For best results you’ll have on-die RAM and slower external RAM and even slower PMEM backed up by SSD as permanent storage with PCIe NVMe as the slow tier. That can all be integrated as a single access tier with controllers managing the migration of data from RAM to NVMe. That’s where Apple is going by baking all that into the chip and with tight integration in the OS, the SSD is basically the “slow” RAM.",2,0,0,False,False,False,1636173025.0
qnduwn,hjj3k3i,t3_qnduwn,IBM integrates eDRAM inside IBM Power Server processors since Power8 in 2014 as a cache.Apple is soldering ram with the SoC just like the GPU or APU used in PS5 or XB SX|S so it's not the same as eDRAM. If you want to upgrade M1 from 8 GB to 16 GB or M1 Pro from 16 to 32 or M1 Max from 32 to 64 it will take you a lot of time to get the process right assuming that you can use a soldering machine.,2,0,0,False,False,False,1636190128.0
qnduwn,hjgbphp,t3_qnduwn,"It is close to impossible to upgrade any post 2015 mac. But with the introduction of the M1 chips. Damn they are quite powerful, a laptop that is truly worth the title of pro.",1,0,0,False,False,False,1636139744.0
qnduwn,hjib0gm,t3_qnduwn,"If integrating dram provides incomparably greater performance over others (with similar form factor), yes.
Ram modularity is not that important for most of comsumer product, compared to everyday performance and battery life.
Not sure about M1 Pro or M1 Max, but I'm pretty sure that M1 competitors will follow in some time.",1,0,0,False,False,False,1636169928.0
qnduwn,hjfjky6,t1_hjfh7lm,"ohh, that makes more sense, I was misled by diagrams of the layout in Apple's marketing I guess. I know mobile processors have been using the SiP design, its still interesting to see it in more powerful designs despite not being what I thought it was.",6,0,0,False,False,True,1636128976.0
qnduwn,hjg547e,t1_hjffwqf,"To be fair, these systems aren’t meant to be upgraded. Like most other laptops.",2,0,0,False,False,False,1636137259.0
qnduwn,hjjb4yo,t1_hjg547e,Or repaired,1,0,0,False,False,False,1636195984.0
qn56a6,hje11vn,t3_qn56a6,"Yes, although metadata (file system info) will be different. But copying a file results in a precisely identical copy. This is one reason why digital is superior to analog recordings (from a degradation standpoint); digital copies are perfect, but analog copies lose fidelity.",24,0,0,False,False,False,1636097770.0
qn56a6,hjdz7ja,t3_qn56a6,"This entirely depends on the operating system, but I believe that some OSes won't actually copy the file (just pretend to copy it) until something wants to modify it.",9,0,0,False,False,False,1636096136.0
qn56a6,hjehyzd,t1_hje11vn,"And interestingly enough, sometimes analog's degradation qualities can be desirable (see the proliferation of tape and vinyl simulation audio plugins, and big name music engineers who still record to tape).",2,0,0,False,False,False,1636111886.0
qn56a6,hjfs5je,t1_hje11vn,[removed],2,0,0,False,False,False,1636132267.0
qn56a6,hje7eiq,t1_hjdz7ja,"Officially known as ""Copy on Write"".",7,0,0,False,False,False,1636103596.0
qn56a6,hjfjtf9,t1_hjdz7ja,Oh wow that's actually pretty interesting,1,0,0,False,False,True,1636129068.0
qn56a6,hjf5zg6,t1_hjehyzd,"Yes, there’s a healthy lo-fi movement.",2,0,0,False,False,False,1636123664.0
qn56a6,hjeorrv,t1_hjehyzd,"I'm not a sound engineer, but I think any high-end audio tape recording still in existence today would be using DAT (Digital Audio Tape).",1,0,0,False,False,False,1636115892.0
qn56a6,hjhdwmv,t1_hjfs5je,"Yes, and there are plenty of other issues that can degrade digital signals but I only meant to discuss typical/normal operation.",1,0,0,False,False,False,1636154536.0
qn56a6,hjetvpv,t1_hjeorrv,"No, I'm referring to [regular analog tape](https://www.youtube.com/watch?v=D5VHW1J5o0Q).",3,0,0,False,False,False,1636118447.0
qn56a6,hjex3dp,t1_hjetvpv,That was interesting to watch. I had no idea people still used analog tape these days.,1,0,0,False,False,False,1636119928.0
qn56a6,hjf87kz,t1_hjex3dp,"And it's interesting seeing the development of digital software to simulate those ""imperfections"" and subtle characteristics you get from recording to tape, using analog compressors/EQs/preamps, analog synths with variations in the aging components, tube guitar amps, etc.",2,0,0,False,False,False,1636124544.0
qna9yb,hjeprb4,t3_qna9yb,"Because writing simple business logic code isn't the point of computer engineering. Computer engineering focuses on computer architecture, computer organization, systems programming (things like firmware, operating systems, device drivers, etc.), and electronics engineering.",68,0,0,False,False,False,1636116412.0
qna9yb,hjeu1c1,t3_qna9yb,Computer engineering is not software engineering,53,0,0,False,False,False,1636118522.0
qna9yb,hjetdrn,t3_qna9yb,Computer science isn't software engineering.  There are software engineering specific programs but if you just changed the title of a program from computer science to software engineering without changing the nature of the program it'd be misleading.,19,0,0,False,False,False,1636118213.0
qna9yb,hjf0kz0,t3_qna9yb,"> Why do we keep calling CS not CE?

Why do we call Computer Science Computer Science? Because it's Computer Science... There's a massive overlap between CS and CE, but I don't see why we'd rebrand it? If we called CS CE, what would we call CE?

Just because people who get a CS degree go on to be software engineers that doesn't mean they didn't study CS. Just like a lot of Maths majors go on to become financial analysts, doesn't mean we should rename ""Maths"" to be ""finance"".

Personally I did a degree in Computer Engineering but I still become a Software Engineer (although it was CE-adjacent).",16,0,0,False,False,False,1636121464.0
qna9yb,hjerwrp,t3_qna9yb,"Engineering is the use of scientific principles to design and build ""things"". Computer Engineering is an engineering discipline where you learn how to build computer/information systems and solutions on a theoretical (logical) and implementation level as well. As every engineering discipline we are working with models, and since models are an abstractions of the real world (or the system to be built) from a certain view, this is why you need to study differential equations, graph theory, physics, electronics, distributed systems, etc. so that you can model things correctly. 

All those things are taught at the university, but you can learn it also on your own. 

In Europe/Hungary we use the name Computer Engineering, or Computer Science Engineering.

https://www.bme.hu/computer-science-engineering-bsc?language=en

Most people actually won't become engineers. To become an engineer you need to go through an university and get your bsc or msc. They will become programmers.",11,0,0,False,False,False,1636117498.0
qna9yb,hjeyo1e,t3_qna9yb,There's a difference between them. Computer engineering is electrical engineering applied to computers with a bit of computer science.,9,0,0,False,False,False,1636120634.0
qna9yb,hjeywmm,t3_qna9yb,Computer engineering is typically electronics engineering. Computer science can either be actual CS or software engineering,8,0,0,False,False,False,1636120741.0
qna9yb,hjfg1wr,t3_qna9yb,"Some, I think valuable, distinctions:

Computer Science is the study of algorithms, computing, formal languages, recursion, runtime analysis, data structures, operating systems, databases, graphics, sound synthesis, machine learning, embedded systems, and anything related really. Some of which is more math and paper oriented, some of which is more coding and practice oriented. 

Software engineering (SWE) is an application of those skills listed above, some more than others maybe, to build systems that accomplish some goal for a product or project. Being more well versed in CS is the same thing as being better at SWE. CS is the foundation of good, scalable, fast software.

Computer engineering is a non standard term, and to me is a bit ambiguous as to if you’re talking about systems engineering which I think requires some Electrical Engineering coursework as well, or just SWE which is what your question seems to hint. 

To answer your question:
“Why do we keep calling CS not [SWE]”

Just because some people go on to get a PhD in CS and advance the field with new algorithms and tools, doesn’t mean that those who don’t are not studying CS. CS is a whole host of skills, many of which would be the exact same ones you should learn if you wanted to just skip school and go straight to industry to do SWE. Universities, being places that want to attract people to do PhD programs, keep the title CS. Industry, being it’s own entity that can decide any number of labels for its employees has largely chosen SWE as their moniker. Most people don’t really care that they’re different words, but that’s why they’re different. In practicality, they’re the same field with practitioners and theorists, just like medicine has with doctors.",5,0,0,False,False,False,1636127620.0
qna9yb,hjf5h1t,t3_qna9yb,"Computer science engineer here.

I did physics and differential equations.

Only difference between my major and an EE is the extra software classes and computer oriented hardware classes, versus power conversion and power transmission stuff.


I am NOT a software engineer.

We weren't taught how to write software. We were taught to use computers as a system to solve problems.",4,0,0,False,False,False,1636123461.0
qna9yb,hjj7gg2,t3_qna9yb,"Computer Engineering != Software Engineering

if(person.inCS_major()){

   personCanBeSWE  = true;

   personCanBeDataArtitect = true;

   personCanBeGameDev = true;

   personCanBeComputerEngineer = false;

}",5,0,0,False,False,False,1636193219.0
qna9yb,hjfd8ph,t3_qna9yb,"Your question makes little sense. What is precisely the context between computer science versus computer engineering?

If you are referring to a degree program, than the answer is obvious. A degree is named based on the course content and syllabus, not on the eventual career outcome of people receiving education under that program.

Majority of people who receive any degree at undergraduate level and masters level, and even significant number of people who go all the way to Phd level ( or any equivalent qualification) will not have a career as a researcher in that field or equivalent position that requires the epitome of specialization in that field. Therefore, select few physics major become physicist; select few creative writing majors become full time authors of creative literature; select few business majors go on to become a CEO ; and you can expand this argument to many other fields.

So, a computer science is called that because the university offering it believe that the courses and syllabus fall under the field. And, it is common for colleges to differentiate between a computer science and computer engineering course. At my school these are two different majors despite considerable overlap, and at most school the same applies.

If you are referring to how the term CS is used in casual conversation — e.g CS pays well— there can be some semantic ambiguity which by the pragmatics of the language it is very clear anyway. For the very example I have cited, it is clear that speaker is making a comment on specific subset of the field, rather than a statement on the field in its entirety.

Some lay people may not understand the differences and the intersection of different fields but that usually has little to no bearing on the use of terminologies, jargons, and other common lexicon, and it is very easy to convey these differences if and when the situation requires for it.",3,0,0,False,False,False,1636126525.0
qna9yb,hjfspl4,t3_qna9yb,Computer engineering deals with hardware. Computer science deals with theory. Software engineering deals with software and applications. All of them overlap in different degrees.,3,0,0,False,False,False,1636132482.0
qna9yb,hjfznbk,t3_qna9yb,"I know of (non-computing) engineers who are pissed at the term software (etc) engineering because it’s borrowed the terminology without taking the rest of what makes an “engineer” along with it i.e. the notion of chartered status, a regulatory body for professional standards, etc",3,0,0,False,False,False,1636135172.0
qna9yb,hjhlozy,t3_qna9yb,"The names are conflated because CS spawned out of the math department at some schools and out of engineering departments in others. It spans a wide variety of traditional categories, and the names have never really settled.

In my mind SWEs are programmers, the only “real engineers” are hardware/architecture people, and “computer scientists” span a wide range of computing theory to very applied stuff.

Also, my college degree officially says “computer science and computer engineering” even though it was just one major, and my Master’s says “electrical engineering and computer science” even though it was on very abstract CS stuff. Just a matter of what the departments were called, lol.",3,0,0,False,False,False,1636157872.0
qna9yb,hji369w,t3_qna9yb,Op are you 12?,2,0,0,False,False,False,1636165947.0
qna9yb,hjeyj8p,t3_qna9yb,maybe you would like to rephrase the post...,3,0,0,False,False,False,1636120573.0
qna9yb,hjf6zn3,t3_qna9yb,Computer engineering is more about the hardware interaction than software,1,0,0,False,False,False,1636124058.0
qna9yb,hjg43os,t3_qna9yb,I don’t think you know what computer engineering is.,1,0,0,False,False,False,1636136873.0
qna9yb,hjht8fj,t3_qna9yb,Computer science is a branch of mathematics. Computer engineering is a branch of electrical engineering. A computer scientist is a specialized mathematican. While a computer engineer is a specialized electrical engineer.,1,0,0,False,False,False,1636161298.0
qna9yb,hjeq5qz,t1_hjeprb4,"I agree, but I think 90% cs major are pursuing swe now",-58,0,0,False,True,True,1636116620.0
qna9yb,hjev84s,t1_hjeu1c1,"To make things more funny in our system when you do the computer engineering bsc, on the 5th semester there is a specialization for software engineering or infocommunications.",10,0,0,False,False,False,1636119081.0
qna9yb,hjf1zis,t1_hjf0kz0,"Makes sense!
I think most of cs major will become an engineer, that is my point. Also the cs major courses *are* engineering oriented.",-7,0,0,False,True,True,1636122057.0
qna9yb,hjeswl8,t1_hjerwrp,"Lol I agree most of it, except that I think a programmer is a engineer, same",-12,0,0,False,True,True,1636117981.0
qna9yb,hjezpxw,t1_hjeywmm,Oh that makes sense my bad,1,0,0,False,False,True,1636121091.0
qna9yb,hjfk92w,t1_hjfg1wr,"There  are many engineering majors in the universities too, my point is that computer should belong engineering school",1,0,0,False,False,True,1636129233.0
qna9yb,hjfdojc,t1_hjf5h1t,"Now, that's rather interesting. Let me try to remember what kind of classes we had 20 years ago.... (computer science engineer, fault-tolerant systems major)

Analysis (diff equation, fourier, etc.), Linear algebra (equations, matrices, etc.), Physics (2 semester), Electronics (mostly transistors, 1 semester), Digital systems (2 semesters), Graph theory (1sem), Algorithm theory (1sem), Signals and systems(1sem), Controllers (like PID 1 sem), Probabilty theory (1sem), 3D graphics (1sem), Coding technology (error coding, compression, des, rsa, etc.) , Computer architectures (1 sem), Programming in various languages (3 sem.),  Web programming (1 sem), Communication systems (1 sem), Databases (1 sem), Software development methodologies (1 sem) etc. 

And for the major we had stuff like: model based system engineering, fault tolerant systems, system integration, system validation, and various labs.  


How does it compare with what you learned?",2,0,0,False,False,False,1636126697.0
qna9yb,hjfg0yy,t1_hjf5h1t,"If not swe, What position will be then?",1,0,0,False,False,True,1636127609.0
qna9yb,hjezyh6,t1_hjeyj8p,"Yeah sorry I meant it should be an engineering degree not a science degree , well maybe it does not matter",0,0,0,False,False,True,1636121193.0
qna9yb,hjew80m,t1_hjeq5qz,computer engineering is not software engineering,47,0,0,False,False,False,1636119535.0
qna9yb,hjex0xc,t1_hjeq5qz,Is software engineering not just applied computer science.,17,0,0,False,False,False,1636119899.0
qna9yb,hjew3p2,t1_hjeq5qz,"I'm sorry, I don't quite follow. If you agree with me that computer engineering is distinct from computer science, then why does it matter at all whether 90% of CS majors pursue SWE or not? They're still different disciplines.",8,0,0,False,False,False,1636119479.0
qna9yb,hjf64ff,t1_hjeq5qz,That’s cause computer science degrees generally prepare you for that line of work. Computer engineering is more focused on things like embedded software and other things closer to the hardware level,2,0,0,False,False,False,1636123718.0
qna9yb,hjft2g0,t1_hjev84s,Usually first two years of all 3 majors are same. Studying math and CS fundamentals.,3,0,0,False,False,False,1636132623.0
qna9yb,hjf489x,t1_hjf1zis,"I think you're confused about what Computer Engineering is.

If you'd have said: ""software engineering vs computer science"" or ""Why do we keep calling CS not SE?"" then it would have made more sense.

I would draw the spectrum as something like:

          Maths          ..CS..
            Physics  EE  CE  SE",10,0,0,False,False,False,1636122966.0
qna9yb,hjetxfj,t1_hjeswl8,A programmer is a computer engineer the same way as a bricklayer is an architectural engineer.,9,0,0,False,False,False,1636118470.0
qna9yb,hjfl9ib,t1_hjfk92w,That’s not what you asked in your question. My school includes CS in the Science and Engineering department. Even got a school of engineering hoodie. Sounds like you should take this up with your school,2,0,0,False,False,False,1636129633.0
qna9yb,hjj1vtq,t1_hjfk92w,"Most CS programs I’m familiar with are apart of the engineering dept, it’s quite rare to have a separate CS department as far as I know. 

What you do after your degree does not dictate what your degree is called. SWE is the profession that is in most demand that a CS degree qualifies you for. There are also so many paths that range from network architect to systems analyst that boiling the degree down to computer Engineering (which is a field already) does not accurately describe what you learn in a CS degree, especially at the bachelor level.",1,0,0,False,False,False,1636188740.0
qna9yb,hjfijxh,t1_hjfdojc,"Sounds pretty similar, but we do engineering statistics and statics as well.",2,0,0,False,False,False,1636128582.0
qna9yb,hjfisn6,t1_hjfg0yy,"An engineer.

Software engineering is a method of producing software by applying engineering-like techniques.

It's not really engineering.

Edit : Why are you booing me? I'm right.",1,0,0,False,False,False,1636128675.0
qna9yb,hjexpwo,t1_hjew80m,I am saying most of cs majors will be a swe(or want to be),-40,0,0,False,True,True,1636120207.0
qna9yb,hjeye18,t1_hjex0xc,"most of the time, no. Computer science studies the math and algorithms behind a certain problem. CS studies the beautiful world of computer logic whereas swe (\*sighs\*) is not cs because most of the sw engineers just write basic crud code",13,0,0,False,False,False,1636120508.0
qna9yb,hjezavw,t1_hjex0xc,"I think we are pursuing something academically ""clear"" separation but it's never so easy. Like at my former university: it is called computer science engineering. We were taught heavy math, physics, electronics, networks, programming, computer science, engineering modeling, etc. It was really a mixture, and later you could specialize.",2,0,0,False,False,False,1636120914.0
qna9yb,hjev580,t1_hjetxfj,"No I don’t agree, there is a reason why they spend six figures to hire a programmer",-9,0,0,False,True,True,1636119044.0
qna9yb,hjfjy3r,t1_hjfisn6,"Software engineer is an engineer, isn’t?",2,0,0,False,False,True,1636129117.0
qna9yb,hjfth32,t1_hjfisn6,All engineering basically producing products and considered applied science.,2,0,0,False,False,False,1636132781.0
qna9yb,hjfkgm6,t1_hjexpwo,It doesn't change the fact that SWE is not Computer Engineering and vice versa (despite sharing *engineering* in name),16,0,0,False,False,False,1636129316.0
qna9yb,hjf68k4,t1_hjeye18,"You need to be good at CS to be good at software engineering, tho.",9,0,0,False,False,False,1636123763.0
qna9yb,hjf0mtd,t1_hjeye18,"Sounds like it is dependent on where you went to school and how you are approaching the problems you are working on. My school was very much focused on ""the beautiful world of computer logic"" and then how to use it to build software and solve problems with it. As for problems I'm working on, just yesterday I was having a conversation with the architect on my team about how we could multithread a part of our ""basic crud"" application to reduce overall runtime complexity on an endpoint while also avoiding putting more stress on our company mainframes than we absolutely need to.",3,0,0,False,False,False,1636121486.0
qna9yb,hjevb4q,t1_hjev580,It has nothing to do with salary 😂,9,0,0,False,False,False,1636119121.0
qna9yb,hjfmfw3,t1_hjfjy3r,"""Coincidence"" of names",2,0,0,False,False,False,1636130074.0
qna9yb,hjfxjsn,t1_hjfth32,There's a difference between programming and applied science.,1,0,0,False,False,False,1636134349.0
qna9yb,hjf9079,t1_hjf68k4,"Yep, if you aren't good at CS, you won't be as able to understand when applying stuff from CS would be beneficial vs not doing it, as well as, understanding what you should even apply in the first place.",6,0,0,False,False,False,1636124862.0
qna9yb,hjexkca,t1_hjevb4q,"I haven’t heard companies hire programmers..it is same as software engineers, and with current devops culture, even not much difference between a swe and architect",-1,0,0,False,False,True,1636120139.0
qna9yb,hjg8ww1,t1_hjfxjsn,Programming is a general way of implementing algorithms to compute mathematical problems. Hence programming is applied science.,2,0,0,False,False,False,1636138702.0
qna9yb,hjfqcdw,t1_hjf9079,"Agreed. You don’t necessarily need to understand software engineering to be good at CS, but you need to be good at CS to be a good software engineer",3,0,0,False,False,False,1636131564.0
qna9yb,hjeyaxr,t1_hjexkca,"swe is just a formal name for programmers. companies don't hire computer scientists because they don't build anything. computer engineers design and build computers. software engineers design and build software. they are symbiotic, not synonyms",6,0,0,False,False,False,1636120470.0
qna9yb,hjey3oc,t1_hjexkca,"They give it fancy name, but if you are don't have an engineering degree, you are not an engineer. The same way as you are not a doctor if you don't have a doctor degree. It is rather simple. You might do some level of ""engineering"" work, or just code some Rest interface with some business logic.",1,0,0,False,False,False,1636120378.0
qna9yb,hjg9vzk,t1_hjg8ww1,"That's like saying putting together a kit is applied science.

Try writing your own array sorting algorithm and using it at work. Your senior will ask why you didn't just use the default library.",0,0,0,False,False,False,1636139073.0
qna9yb,hjgab3p,t1_hjg9vzk,"At my work we write our algorithms. Your point is invalid. It’s like when a manufacturer company is building a robot should they build their sensors and cameras from scratch too or just buy one from the market? It’s a basic engineering concept, don’t reinvent the wheel.",1,0,0,False,False,False,1636139227.0
qna9yb,hjgakvb,t1_hjgab3p,"I'm not saying that. I'm saying that a lot of programming is done using already been cooked code.

It's a different thing entirely to talk about sensors and physical devices because you still need to design the interface, regardless of which IC you use.",0,0,0,False,False,False,1636139327.0
qna9yb,hjgbam7,t1_hjgakvb,This shows that you really don’t know what you are talking about. If all codes are “cooked” out there I wonder why I’m getting $250k a year. Can’t my company just copy the codes and paste them together and save them millions? Have a nice day Mr. Genius.,1,0,0,False,False,False,1636139591.0
qna9yb,hjgoqfg,t1_hjgakvb,"Same for sw, it is also a design phase",1,0,0,False,False,True,1636144624.0
