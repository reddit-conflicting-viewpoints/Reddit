post_id,title,flair,score,upvote_ratio,subreddit, url,num_comments,body,created
su3ybe,Project ArXiv Citation Network,General,19,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/su3ybe/project_arxiv_citation_network/,0,"Hello everybody, üë®‚Äçüíª

Here is a project I developed about ArXiv Citation Network. I hope you find it useful for those who want to get more involved in the topic of Graph Networks. ü¶æüíª

Link: [https://github.com/dennishnf/project-arxiv-citation-network](https://github.com/dennishnf/project-arxiv-citation-network)

&#x200B;

https://preview.redd.it/ffdypvisw8i81.png?width=2246&format=png&auto=webp&s=50af2f4e1b7cf334ed3e8b96c0d27c6d818a3782",1645039275.0
su6m6s,Why doesn't Karatsuba multiplication break numbers into word size blocks?,,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/su6m6s/why_doesnt_karatsuba_multiplication_break_numbers/,0,"So under the WORD RAM model of computation the word size w is at least log of the input size and arithmetic operations on words takes constant time.  


So rather than dividing an n bit number into bits, why not divide the number into n/w bytes?",1645046068.0
su8vgf,What is your favorite thing about learning computer science? ‚ò∫Ô∏è,Discussion,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/su8vgf/what_is_your_favorite_thing_about_learning/,0,,1645052015.0
su7zf4,looking for answers for excercises in Principles of Model Checking book,Help,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/su7zf4/looking_for_answers_for_excercises_in_principles/,0,"I've been doing some excercises from the book Principles of Model Checking, but can't find any exemple solutions anywhere in the book to check if my answers are correct. Where can I find those?",1645049665.0
st0hsq,Has anyone been stuck on a technical problem and spent say 5 or 6 hours on it?,General,122,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/st0hsq/has_anyone_been_stuck_on_a_technical_problem_and/,68,,1644923044.0
st08lc,How important is C language?,Discussion,61,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/st08lc/how_important_is_c_language/,51,"I have watched some youtube channels talking about different programming languages. The channel ""Computerphile"" made a few episodes about C language. In my university, a lot of senior professors emphasize the historical importance of C language. I belong to the millenial group, so I cannot understand why it is important. Nowadays, some younger professors are teaching newer languages like python. Some famous universities like MIT use python as the learning material.

I have done a little research on C language. As far as I know, C language is like a foundation upon which many other languages were built. Is it necessary for younger people to know C language?",1644922034.0
stb8pj,Whats the optimal method to find the biggest subset of nodes on a graph which arent directly connected?,Help,11,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/stb8pj/whats_the_optimal_method_to_find_the_biggest/,3,"Ive been thinking about this for a while. I was thinking of giving each node a score of how many nodes it would prevent from being added and then doing calculations with that.

But what would be the optimal way to solve this?",1644953674.0
stdryq,Karnaugh maps question,Help,4,0.68,computerscience,https://www.reddit.com/r/computerscience/comments/stdryq/karnaugh_maps_question/,2,"I'm creating a k-map for an fsm, in the fsm I have some binary values that go unused: 110 and 111. When I'm creating the k-map do I write these down as ""don't care"" or 0?

Any help is appreciated :)",1644960245.0
ssv8zr,"If I make a GUI app in Python that connects to the database and builds a billing system, what architecture it is ‚Äì tier-1, tier-2, or tier-3?",Help,5,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/ssv8zr/if_i_make_a_gui_app_in_python_that_connects_to/,11,"If I make a GUI app in Python that connects to the database and builds a billing system, what architecture it is ‚Äì tier-1, tier-2, or tier-3?",1644902294.0
st2hhc,Networking i/o,,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/st2hhc/networking_io/,2,"Hello,

I'm having a great time working through Computer Networks: a top down approach, however one of my big questions is yet to be answered.

I understand how packets move through networks but not quite how a computer physically takes in a package on more of the hardware (I/O) side of things. E.g. when i get a http response from a web server, what part of my computer is responsible for ""accepting"" the packets, where do these packets go (some kind of buffer?). I've searched around and have not yet found anything satisfying; could anyone recommend me some resources to learn more about this.

Thanks!",1644930205.0
ssk9ty,Node2Vec Explained,Article,15,0.89,computerscience,https://towardsdatascience.com/node2vec-explained-db86a319e9ab,0,,1644870008.0
ssursj,Explanation of this MSE plot,General,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/ssursj/explanation_of_this_mse_plot/,1,"Hi guys, would you like to explain this figure, how can I find overfit or underfit from such figure?

&#x200B;

https://preview.redd.it/0l9o7myqgxh81.png?width=1003&format=png&auto=webp&s=216986bde93f16deccc135b5149336bfadb2efc3",1644900700.0
ss0efg,What would you have wanted from a CS class in school that you never got?,Advice,110,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/ss0efg/what_would_you_have_wanted_from_a_cs_class_in/,35,I recently became a temp computers teacher at my school (kids aged 10-14) after some staffing issues. My school admin asked me to take on the class given my personal experience in computer science although I‚Äôm a English teacher now. I have complete creative control over the course material as the admin is just happy to have an adult in the room essentially. I feel like this is a once in a blue moon opportunity to give these kids a really special experience. So if you were a kid 10-14 and you had a computer science class‚Ä¶what would her the most beneficial or coolest thing your teacher could do or let you do?,1644808534.0
ssagax,Do programmers spend a lot of time on setting up a new project folder structure?,,7,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/ssagax/do_programmers_spend_a_lot_of_time_on_setting_up/,6,"When you start a new project, usually you will have some logical project structure. For example, you might want to put all your entities in one folder and common methods in a different folder. You will need a structure that makes managing the project files easier. Do programmers spend a lot of time setting up a new project structure? I do not remember reading this anywhere during my academic years. But recently I personally find that I spend time setting my project structure. Is it a common problem or is it just me?",1644844837.0
sspvav,The story of how was created the Brain-computer interface,,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/sspvav/the_story_of_how_was_created_the_braincomputer/,2,[https://ildarr2016.medium.com/the-story-of-how-was-created-the-brain-computer-interface-ea922172fb78](https://ildarr2016.medium.com/the-story-of-how-was-created-the-brain-computer-interface-ea922172fb78),1644885844.0
srrd8w,Which subreddit has active community for new computer science research papers?,,37,0.89,computerscience,/r/AskReddit/comments/srovil/which_subreddit_has_active_community_for_new/,11,,1644782197.0
srk48q,Predicting OverWatch‚Ñ¢ Match Outcomes with 90% Accuracy,Article,15,0.86,computerscience,https://taven.me/openskill/,0,,1644763467.0
srqmpy,How do you calculate a block-cut tree?,Help,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/srqmpy/how_do_you_calculate_a_blockcut_tree/,1,"Basically, given a graph, how do you split it into a block cut tree? I know how to find cut vertices, but am stumped on finding blocks, at least in a complexity less than O(n^2).

BACKSTORY:

Recently, I was in a programming contest. (It is over now).

The last question was along the lines of 'there is a graph, find  which node, when removed, will make the graph the least connected' (where connectedness is the amount of node pairs who are connected). In the start, there are at most 10000 nodes and 200000  edges, and the graph is initially connected. 

The case for graphs is easy (count how many nodes are at each edge for all nodes in O(n+e), and for each edge add up n(n+1)/2 for each edge, for a total complexity of O(n+e).

Therefore, I realized that a bct would be a good solution for the general case. I am mostly asking this question now because I was unable to find any resource during the competition, and am curious for my own learning.",1644780266.0
sraf70,Want to learn how to implement relational databases,,41,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/sraf70/want_to_learn_how_to_implement_relational/,8,"I did my undergrad in comp sci a couple of decades ago and I‚Äôd like to refresh how relational databases are implemented. I mean how to write a query optimizer/executor, translating sql to relational algebra (not necessarily the parser part), how to use b-trees for storage, etc.

As far as I can tell, I‚Äôd have to get a text book for this. However, most of the text books seem to cover everything from how to design a set of tables for an online store to protecting against sql injections. It‚Äôs hard to tell which books are good. Most such books have terrible reviews because undergrads hate their classes.

I want a resource which seriously covers the actual implementation. Willing to get multiple books if necessary. I‚Äôd love something modern, great if it covers columnar databases, such as monetdb/c-store.

Ideally, I‚Äôd like to implement a basic version a database, the way there are some toy operating systems which are only a few thousand lines of code.

I‚Äôd love any pointers!",1644727787.0
srw89o,"In the 2012 post linked below, N. J. Wildberger criticized a common definition of the term ‚Äúfunction‚Äù. What is it that, according to his post, endangers that definition of ‚Äúfunction‚Äù?",Help,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/srw89o/in_the_2012_post_linked_below_n_j_wildberger/,0,"https://njwildberger.com/2012/10/13/the-problem-of-rigour-in-modern-mathematics/

He discusses the problems with the definition in the ninth paragraph.

What is it that, according to him, endangers the common definition?

Is it the way that one of many phenomena can be meant with the term ‚Äúfunction‚Äù?

Or is it the fact that there are different ways of speaking about the same phenomenon (I mean the phenomenon of a function) that endangers this?

If he means that the latter of these is what endangers the common definition, then ‚Äúdefines the same ‚Äòfunction‚Äô as your program‚Äù, in the last sentence of that ninth paragraph, should be replaced with ‚Äúdefines ‚Äòfunction‚Äô in the same way as your program‚Äù‚Ä¶but I‚Äôm not an expert.)",1644795197.0
srsbj1,Kruskal's Algorithm produces connected component,Help,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/srsbj1/kruskals_algorithm_produces_connected_component/,1,"I am trying to understand the proof of Kruskal Algorithm. I understood that it cann't have a Cycle but it not at all intuitive that its will be Connected. Here's is a Explanation from wikipedia but I am unable to get it

https://preview.redd.it/8vw55p52vnh81.png?width=2464&format=png&auto=webp&s=306e2220c2a58a78ee036fd461352cdd881e5b83

Can anyone please explain it.  
Thanks  


Wikipedia Link -> [https://en.wikipedia.org/wiki/Kruskal%27s\_algorithm#Proof\_of\_correctness](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm#Proof_of_correctness)",1644784638.0
srbdx2,What is the path to self learning software engineering,,4,0.7,computerscience,https://www.reddit.com/r/computerscience/comments/srbdx2/what_is_the_path_to_self_learning_software/,3,"What is the path to go self-taught route of learning computer science and trying to be a software engineer?

&#x200B;

Question is essentially in the title, but I'll expand further here. I am trying to learn how to go learn computer science and eventually become a software developer. I don't care how long it takes, but ideally, I am trying to avoid the bootcamp and as well as going back to school. I'd like to do this for fun, not for a job, so thankfully, I have the time and the leisure to practice as I please. That being said, I've been watching hours upon hours of computer programming language, python and Javascript.   


Aside from this, what is the best way to put into practice what I've learned? For those that go self-taught, what are you supposed to do? Am I just supposed to learn all the languages? How do I build a website?

What githubs are there for walking me through the process? I'd like to so side gigs here and there. Thanks.",1644731140.0
sqt9uf,Cryptographic hashes can't be reversed because information is lost but can it be reversed to some other string of bits that produces the same hash?,Discussion,42,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/sqt9uf/cryptographic_hashes_cant_be_reversed_because/,26,"First of all, I am a total noob in my 1st semester. The intuition I built for hash functions being irreversible was that say I take a string ""ABCD"" I can sum up their ASCII values and get a hash of some sort (266) and I know that you can't take this value and go back to ABCD without brute forcing because their sum does not tell which value it was precisely was like 10 + 18 could also be written as 26 + 2 and therefore, information was lost here but what if instead of trying to reverse it to ABCD, we could just reverse it to some other string of characters that make the same hash for example: √†√† which both have the ASCII values 133 so if we compute it's hash from the simple *adding ascii values function*, we will get the same 266 hashed value i.e., a hash collision even though the information isn't the same. As you all already know that passwords are verified this way by matching input's hash with the stored hash, is it theoretically possible to compute any other string of characters (or bits) without brute forcing from a SHA-256 or even an MD5 hash that although isn't the same as the original set of bits,  produces the same hash. Thanks for fulfilling the curiosity of a young one.",1644677801.0
sr998x,How Do Websites Find Books So Fast?,General,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/sr998x/how_do_websites_find_books_so_fast/,4,"Hello everyone,

So I was just using an mla citation generator for my class paper, and I copy pasted the name into the generator and it literally found the book in less than 2 seconds.

Are they using a known database of books like maybe Google had one or something? Or are they literally searching the internet until they find that book publication?

&#x200B;

Isn't it still crazy fast even if they have a database already? **Unless they use hashing and the books name is sent to a hashfunction which gives the bucket address in the database**

&#x200B;

Thank you for explaining!",1644723847.0
sqwh3u,Trying to understand Grover's algorithm,,5,1.0,computerscience,/r/QuantumComputing/comments/sqwg54/trying_to_understand_grovers_algorithm/,0,,1644686656.0
squ1lg,Questions about full stack development from a higher level,Help,5,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/squ1lg/questions_about_full_stack_development_from_a/,8,"Random questions about full stack development

So let‚Äôs say for our frontend we‚Äôre using a React application, our backend a Flask server running, and a Postgres database.

1. For the longest time I thought that React was a javascript framework that runs on a server (because npm start makes it seem like it‚Äôs running on a server) but apparently that‚Äôs only for local development. Why does local development run on a server and not when you deploy it? What‚Äôs the difference between running npm start versus if you had just static html files and opened those up?

2. I know that there‚Äôs talk of frontend and backend frameworks, but if our Flask server returned HTML and we got rid of our React application, what‚Äôs our frontend tool in this case then? Does that mean any backend framework (like Django or Flask) could also be considered a frontend framework? Since we could have Flask on the ‚Äúfrontend‚Äù returning HTML and a Django server acting as our backend. Or even just Flask all by itself! I guess I‚Äôm still confused about this separation in terms of the technology/frameworks I know of.

3. A bit of an add-on to question 2, but are all frontend frameworks/tools purely Javascript frameworks? Like Angular, Vue, React? I suspect it‚Äôs because only Javascript can run on the browser, in which case is this a good thing to only have one programming language dominate the scene?

4. I‚Äôve deployed web applications before through Heroku, but I‚Äôve never deployed a database. In fact, I‚Äôm not even sure that‚Äôs the correct term. If I were to deploy my React and Flask app, how would Postgres for example be running remotely on a server? Would this be done through Heroku?

5. How does platforms like Heroku make any money at all? I can create a free account and deploy websites for free (granted it‚Äôll have the herokuapp domain), but doesn‚Äôt it cost money for them to have those servers running?

I might have more questions later but that‚Äôs all for now! I‚Äôve been studying CS for 4 years now and I never quite got the answers to these. Any answers would be much appreciated!",1644680023.0
sr0w0y,Any recommendation on learning Cloud concepts,,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/sr0w0y/any_recommendation_on_learning_cloud_concepts/,1," Hello, so to be brief. been in IT for a few years now and have strong network and Linux skills with a bit of sysadmin thrown in. I also have an affinity for FOSS and open-source hardware, So I would love to get into Openstack. The problem is that it's just not common enough for me to transition to it naturally.

So I want to learn the principles behind all clouds platform preferably private oriented ones. that way I can jump from one platform like VMware or Azure to Stack easily without getting lost in Vender specific terminologies & implementations

Do you guys know any place I can look to learn the underlying techs in depth? Only lead I have is Cloud+ using Sybex with Todd's study guide. But not sure if it covers everything; going off my limited understanding on Compute I don't see mentions of NUMA.... and somewhere else on reddit mentions that Certificate barely cover 50% of all features, while going down Azure to AWS is a 90% similarity.",1644698987.0
sq9tve,Sources that describe mechanical logic gates,Advice,31,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/sq9tve/sources_that_describe_mechanical_logic_gates/,14,"I'm working on implementing a mechanical computer to learn about basic binary logic and how they combine to produce machines. I've managed to make all the basic gates except XOR and XNOR using simple rods and pivots, but I'm having... writers? block over trying to come up with a design for these two gates. Maybe I need someone else's input for a paradigm shift. Preferably it should be possible using rods and pivots but I'm open to other forms of rigid or flexure based designs.",1644614177.0
sq0aef,Suggested Resources to Learn how to Write RISCV Emulator?,,20,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/sq0aef/suggested_resources_to_learn_how_to_write_riscv/,4,"In order to gain a deeper understanding of computers and how they work, I wanted to write a RISCV computer system emulator capable of running an operating system. This is a recreation project of a couple of projects that I found on GitHub:

[https://github.com/franzflasch/riscv\_em](https://github.com/franzflasch/riscv_em)

[https://github.com/d0iasm/rvemu](https://github.com/d0iasm/rvemu)

I could just read through the code and try recreating what they have done but I want to be able to do this myself. So I want to know which resources I can use to learn enough about computer systems, their design, and organization in order to build this. I am currently looking into either the Patterson & Hennesy, CS:APP, or Harris books. Any suggestions.",1644588566.0
sqih7z,Nerd Talk - Doug McIlroy & Brian Kernighan,General,1,1.0,computerscience,https://www.youtube.com/watch?v=Xe5ffO6Ouwg,0,,1644638853.0
spnpvh,"Books about CS (Non-Fiction, Fiction, Biographies, ect)",General,62,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/spnpvh/books_about_cs_nonfiction_fiction_biographies_ect/,23,"Looking for books related to computer science **\*Not learning material\*** but more of; biographies of computer scientists, history of cs, even fictional stories related to cs. 

Anyone know of any?",1644546245.0
spwb7n,What‚Äôs your favourite fun YouTube tutorial series?,Discussion,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/spwb7n/whats_your_favourite_fun_youtube_tutorial_series/,3,Tell me your favourite YouTube tutorial series! I‚Äôm looking for one to watch and am open to anything as long as the host keeps it fun and interesting.,1644575220.0
sp7b3q,Prim's Algorithm produces a Spanning Tree,Help,36,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/sp7b3q/prims_algorithm_produces_a_spanning_tree/,2,"I have been studing Algorithms using Stanford Algorithms Course (Coursera)

[https://www.coursera.org/specializations/algorithms](https://www.coursera.org/specializations/algorithms)

I've understood how the concept of Prim's algo works. I have understood that to prove Prim's Algorithm produces MST we have have to prove that it produces a ST(Spanning Tree) But I am very confused with the argument made by the lecturer i.e. Here basically he used Graph's Cut Property specifically **Lonely Cut Property** and argue that **each time we add a edge we are not creating a any cycle**

https://preview.redd.it/69skzwc6i0h81.png?width=2864&format=png&auto=webp&s=b5094441f0f180a9e390eb19d0c73717027e95a8

Here's the excat part of the lecture --> [https://youtu.be/pGUzn3S7bp4?list=PLEAYkSg4uSQ37A6\_NrUnTHEKp6EkAxTMa&t=798](https://youtu.be/pGUzn3S7bp4?list=PLEAYkSg4uSQ37A6_NrUnTHEKp6EkAxTMa&t=798)

Can any anyone please help me to understand Prim's algorithm produces a ST intutively using Cut Property.

P.S. I have already understood other way to proof that i.e. using number of vertex and number of edges (if you have n vertex and you have n-1 edges then it will be a tree)",1644501693.0
spay93,Where to learn Data Structures & Algorithms fundamentals?,,16,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/spay93/where_to_learn_data_structures_algorithms/,5,"I want to get into coding interviews but first I want to learn foundation of Data Structures & Algorithms. Where to learn Data Structures & Algorithms fundamentals? Paid or not. 

Thank you",1644511599.0
soq68k,Was there ever any competition for x86 and will there be in the future?,Discussion,15,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/soq68k/was_there_ever_any_competition_for_x86_and_will/,10,"I was reading this article https://dfarq.homeip.net/why-is-x86-so-popular/ and wanted to hear from somebody who was around then. I figured I could find them here.

What do people think about x86 going forward? Arm is up and coming but I‚Äôm not sure it will ever fully displace x86.",1644446616.0
sob87b,Personally I can only learn stuff by understanding the core building blocks. How can I do so for programming languages without spending years on doing so? E.g. why is everything an object in js? What's behind that design? How do other languages work?,Discussion,67,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sob87b/personally_i_can_only_learn_stuff_by/,17,"What are the pieces I need to learn to wrap my head around this. Right now I'm learning an obscure new language related to cryptocurrencies and I have to say I have no clue why you can return an array but not a hashmap for example (I think you can't). So I realise I'm pretty lost still. Now starting to understand better how memory works and that arrays and linked lists are the basic physical data structures. But I still feel lost about different languages. Why can you do what when?

Is there a good course on fundamental stuff around these things? I always feel like it's a complete blackbox I'm interacting with and all I can is learning it by heart...",1644405516.0
soiieq,Query regarding computer architecture,,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/soiieq/query_regarding_computer_architecture/,2,"Hi, I am currently studying computer architecture at my university. I have learned how data is stored using flip flops on a very lower level, but I am confused how it is actually stored through a software we use in modern day. Using a very basic example, Suppose I open up a calculator on my PC and type a number, this gets stored in my computer's RAM. What's troubling me is **how** is it stored, since you need to provide high and low current to hardware (flip flop) to save data, how does a software signal to send a high or a low signal?? Creating a project in my Logic design course, we did this by using a timer IC and some switches, but since no direct access to hardware is happening rather it's just me clicking buttons on my computer. 

I am trying to grasp a concept, please explain :)

Thanks in advance",1644426488.0
sofw4w,Can aligning a structure field in some other order reduce the padding in the structure?,,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/sofw4w/can_aligning_a_structure_field_in_some_other/,1,"I was writing a document regarding memory alignment and I had described that the padding inserted in a structure can be reduced by aligning its fields in descending order. Consider this struct:
```c
struct foo {
   char i;
   char *j;
   int k;
} bar;
```
The above struct is 24 bytes wide on a 64-bit system, out of which 13 bytes account for padding. Reordering the fields of the struct in descending order of their alignment requirements reduces the size of the structure by 8 bytes.
```
// reordered fields
struct foo {
   char *j;
   int k;
   char i;
} bar;
``` 

I wrote that reordering the fields in some other way except in the descending order of their alignment requirement can never result in a smaller size as compared to reordering them in descending order. I thought about this for some time and came to the conclusion that this is right but then I thought I might as well ask in this sub to confirm. Thanks :D",1644419573.0
soamn6,Theoretical question about Blockchain-ish virtual worlds,,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/soamn6/theoretical_question_about_blockchainish_virtual/,3,"I'm writing a sci-fi story about an mmo type virtual world. This is a pretty common concept in modern sci-fi. But where most of these stories revolve around man-made, server based mmo's, I would like to bypass the looming Gods/Deus ex machina tropes this entails by going for an autonomous AI generated world that is beyond human tampering. From my limited understanding of blockchain technology, it seems that transactions and mining happens peer to peer, and people cannot temper with the process in any way. This would be perfect for my virtual world. Are there any theoretical models or ideas out there for how a blockchain type of technology could generate a persistent virtual world?",1644403310.0
snoihm,Best books for learning how OS works?,Advice,75,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/snoihm/best_books_for_learning_how_os_works/,19,,1644338357.0
so25ut,Book Recommendations and Study Tips,,6,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/so25ut/book_recommendations_and_study_tips/,2,"Hi!

I'm currently on a BSc Hons Computer Science Course, I find concentration a constant struggle when studying and getting into the mood to study I also find difficult, I do find reading enjoyable though and was wondering if people had recommendations for books on the topic which is more focussed on theory and concepts which are enjoyable as well as adequate at training and furthering my aptitude in the subject.",1644373805.0
sn86ck,Claim: No bounded-time algorithm with access to uniform bit randomness can produce uniform ranges except powers of two.,Discussion,31,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/sn86ck/claim_no_boundedtime_algorithm_with_access_to/,34,"Claim:

> No bounded-time algorithm with access to uniform bit randomness can produce uniform ranges except powers of two. 

In plain English , you cannot use coin flips to generate a set of random positive integers within a given range, R whenever R != 2^k 

Original thread :  https://www.reddit.com/r/math/comments/smvbqz/is_there_a_way_to_get_random_numbers_from_any/


I believe this claim is false.  False by counter-example. Below is an algorithm that performs this for R = 9000 


    R=9000 
    total=10
    coinflips='010110010111001111110001100100110011100011101110'
    #
    soup=int(coinflips,2)
    br = len(  format(R,'b') )
    utot=min( (len(coinflips)-1)//br , total )
    for g in range(0,utot):
        print(soup % R)
        soup = soup // R

Your thoughts?",1644286506.0
snfpxr,According to you what topics fall under the intersection between physics and computer science that currently used in the tech industry?,Discussion,8,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/snfpxr/according_to_you_what_topics_fall_under_the/,8,,1644310658.0
sndcd7,Is it possible that a computer destroys itself permanently? I mean software-wise speaking,General,7,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/sndcd7/is_it_possible_that_a_computer_destroys_itself/,19,"Hi I don‚Äôt know anything about computers but this question intrigues me because I‚Äôm investigating about the human brain capacity to take itself to the extent of killing itself by committing suicide. And I know machines ARE NOT a human brain (which is, I assume, tons of times more complex), but I‚Äôm just curious about how such a complex thing (but simpler than the brain) as a computer could destroy itself.

If it can, how would the computer do so? Does it need to be preprogrammed to do it? Or does it need an external posterior intervention (like an alien virus or a code or something generated inside itself). Sorry for my poor vocabulary in this area, I really have no idea.",1644301834.0
sndnjm,Question,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/sndnjm/question/,9,"In this paragraph that I am reading of Structured computer organisation I do not understand what virtual machine means. I do not think its VM as running a VM on a PC I think its something else. Here below is the paragraph: 

""Rather than thinking in terms of translation or interpretation, it is often simpler to imagine the existence of ahypothetical computer or virtual machine whose machine language is L1. Let us call this virtual machine M1 (and let us call the virtual machine corresponding to L0, M0). If such amachine could be constructed cheaplyenough, there would be no needfor having language L0 or machine that executed programs in L0 at all. People could simply write their programs in L1 and have the computer execute them directly. Even if the virtual machine whose language is L1 is too expensive or complicated to construct out of electronic circuits,people can still write programs for it. These programs can either be interpreted or translated by aprogram written in L0 that itself can be directly executed by the existing computer. In other words, people can write programs for virtual machines, just as though they really existed""",1644302909.0
snauwt,Need Clarification on a Symbol,Help,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/snauwt/need_clarification_on_a_symbol/,4,"I get in computer language, Sigma is basically the alphabet it‚Äôs a set of characters, but what does Sigma Squared mean?

Example: Say Sigma = { 0,1,2 } and  
a E Sigma 

Thus, the values of a would be { 0,1,2 }.

However, if a E Sigma Squared, would the values of a just be {0,1,4} or something else?",1644294065.0
sn8ppi,Counting people in an area?,Advice,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/sn8ppi/counting_people_in_an_area/,5,"Is it possible to ""count"" the number of people in an area by tracking their mac addresses?? I'm planning of making this for a project where users can check the crowd density of a specific area inside the university. Thanks in advance!",1644287962.0
smanmb,Thoughts on teaching good programming skills versus computer science?,,64,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/smanmb/thoughts_on_teaching_good_programming_skills/,24,"Just wanted to get other people's thoughts about this. When I went to college, my computer science degree taught a bunch of theory. We learned about Big o notation, a bunch of different algorithms, different types of data structures, and things like that. However, we didn't really learn very many practical skills. For example, we didn't really cover git/svn, debugging tools, proper code management, proper documentation, and things like that.

Were other people's experiences in college the same? Do you agree or disagree with how your courses were?",1644189277.0
sm1pz4,Assistance with IPv4 Classes and Ranges,General,24,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/sm1pz4/assistance_with_ipv4_classes_and_ranges/,27,"Working through some of my networking study material I started heading down the IPv4 rabbit hole over the past week or so. I'm a visual person so I built this table to help me learn the information. As I've looked around websites I have found various different piece of information but this is the most ""right"" answer I could come up with. I had a few questions for everyone:

1) Does all the information look correct. 

2) Is the loopback IP ranges considered part of Class A or are they on their own?

3) I may be completely misunderstanding where the numbers come from but why does Class have has so many more no of hosts per network but Class C has a lot more number of networks. I keep looking at the math but don't understand it.

* I promise this isn't homework, I'm studying for CompTIA exams and started going down the rabbit hole and need some help.

&#x200B;

https://preview.redd.it/tk9tcudvv8g81.jpg?width=773&format=pjpg&auto=webp&s=5782af7d2b321f65cfe4251baa3c1df11c8110c7",1644167291.0
smdequ,Single-board computers for control robot by the mind with shield PiEEG (Open-source brain-computer interface),Article,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/smdequ/singleboard_computers_for_control_robot_by_the/,0,"[https://arxiv.org/abs/2202.01936](https://arxiv.org/abs/2202.01936)

 This paper presents Open-source software and a developed shield board for the Raspberry Pi family of single-board computers that can be used to read EEG signals.",1644196862.0
sm9s5c,Hello! I want to build the computer described in the book: The Elements of Computing Systems: Building a Modern Computer from First Principles.,Help,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/sm9s5c/hello_i_want_to_build_the_computer_described_in/,3,"I've just read the first chapter, and I noticed that it used to exist a web page that had all the resources. Does anyone know if those resources still exist somewhere?

I append some pictures of the book's cover.

[Images of the book and the web page link (which no longer work)](https://imgur.com/a/NPQonyE)

Thanks in advance for your time and attention!",1644187102.0
slcgd1,Algorithm for finding how many times substring occurs in string ? (does not need to be continuous),,43,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/slcgd1/algorithm_for_finding_how_many_times_substring/,20,"Hello,  
I am trying to make algorithm that can count how many times substring occurs in string, but I does not have to be continuous.

For example: `BARBARAR` cointains `BAR` **9** times
```
1: BAR.....
2: BA...R..
3: BA.....R
4: B.....AR
5: B..A...R
6: ...BAR..
7: ...BA .R
8: ...B. AR
9: B...AR..
```

Thanks for help

**EDIT:** I think there is O(n+m) solution using Aho-corasic algorithm  
m = lenght of string  
n = lenght of substring",1644082529.0
slotas,Integrals and Floats: Is that it?,,3,0.62,computerscience,https://www.reddit.com/r/computerscience/comments/slotas/integrals_and_floats_is_that_it/,7,"To represent values, it seems that computer scientists have settled that the only things they need are integral types that represent whole numbers (-1,0,1,2) and floating point numbers (.25, 10.0, Positive infinity, NaN). These come with modifiers for the size of the variable (eg int and long or float and double) and whether they are signed or unsigned (eg uint or int). 


After that, representations of any other mathematical object come from combinations of integrals and floats and the functions defining their uses. For example, a vector is a struct of floats that adds componentwise, can multiply with scalars by multiplying each element, and can dot product with another vector.


Why don‚Äôt computers support more data types? I know that there is specialized hardware for specific operations like FPGAs, but there are types that any computer should be able to use. 


I‚Äôm not an expert in hardware design or machine code, but I think that as an example, a ‚Äúpolynomial‚Äù data type could be used to reduce the size and increase the speed of many programs. 


A polynomial is an expression of the form ‚Äúy = ax^b + cx^d ‚Ä¶‚Äù To represent this with an array of bits, I suggest the following rules. I believe it should be possible to create circuits to support this. 


Starting from the right, add the length of a string of ones times x to the power of the number of 0s to the right of the string. 


As an example, here are the polynomials that 4 bits can represent with this system:


0000: 0

0001: 1

0010: x

0011: 2

0100: x^2

0101: x + 1

0110: 2x

0111: 3

1000: x^3

1001: x^2 + 1

1010: x^2 + x

1011: x+ 2

1100: 2x^2

1101: 2x +1

1110: 3x

1111: 4


It should be possible to create circuits that take an integral or a float and a polynomial and output an integral or float that the polynomial returns when called on that value. It should also be possible to create circuits that add, subtract, multiply, divide, and do function composition on pairs of polynomials. 


The polynomial rule I described above does not support negative coefficients, but that can be fixed with the rule that the coefficient is, where L is the length of the string of ones: ((L%2==0)? -L-1 : L+1)//2. 



0000: 0

0001: 1

0010: x

0011: -1

0100: x^2

0101: x + 1

0110: -x

0111: 2

1000: x^3

1001: x^2 + 1

1010: x^2 + x

1011: x -1

1100: -x^2

1101: -x +1

1110: 2x

1111: -2


This rule extends to larger strings of bits in an intuitive way. 


Variant ideas:
A type that multiplies all values by x to get rid of the constants. 


A type for floating point inputs that uses the inverse coefficient instead of the coefficient to better represent certain important infinite series including the Taylor series expansions of sin(x) and e^x.



I want to hear more data type ideas in the comments! There is so much possibility for creativity; limiting computer scientists to ints and floats is a crime. Are there not enough opcodes or something?",1644119426.0
slsxv7,Measuring Consistency of a Model,,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/slsxv7/measuring_consistency_of_a_model/,1,"Good day, I have an emotion detection model for my thesis, and one of my research problem is determining its consistency. Is there any good way to measure this? I‚Äôve read [What is the measure of consistency in the data?](https://forum.onefourthlabs.com/t/what-is-the-measure-of-consistency-in-the-data/7449) and I‚Äôm not really sure if it would be applicable here.

&#x200B;

By consistency, I mean its consistency in being able to detect correct emotions

&#x200B;

Thanks for the help!",1644140297.0
sl4tdf,Why is natural alignment used in most processor architecture over processor-word alignment?,Discussion,27,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/sl4tdf/why_is_natural_alignment_used_in_most_processor/,5,"Most processor architectures prefer natural alignment as the default alignment requirement but I think that processor-word alignment is a more efficient alignment requirement that saves memory without any performance overhead over natural alignment.

For example, a double has an alignment of 8 according to natural alignment but on 32-bit processors it would have no performance overhead if double had an alignment of 4 and it would have saved memory. This [source](https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf)#3.6.4 states that double has an alignment of 8 on 32-bit processors:
> Align 64-bit data so that its base address is a multiple of eight.

Similar examples can be seen in 64-bit processors, 16 byte sized data type(int128) has an alignment of 16 whereas it could have been beneficial to keep the alignment equal to the size of the size of a processor word(i.e. 8 bytes long in 64-bit processors).

My guess is that this standard of natural alignment was created because when data was read directly from the wire, the machines could default to natural alignment and not have to deal with different alignments of the same data type according to the cpu architecture of the sender of the data.

When all the fields of a data structure are being stored in a single CPU word, they still have padding inside due to natural alignment whereas I do not think that padding is needed when all the fields are stored in a single CPU word because any field of the structure would take the same amount of byte shifts to access it despite of where in the CPU word it is stored(please correct me on this if i am wrong).

For example, consider this struct:
```
struct example {
   char i; // 1 byte
   // 1 byte padding
   short j; // 2 bytes
   int k; // 4 bytes
   char l; // 1 byte
   // 3 bytes trailing padding
} foo;
```
The padding between `foo.i` and `foo.j` is not needed in my opinion because `foo.j` would still need 6 byte shifts to access.

To summarize my question, I want to know that what are the benefits of natural alignment over processor-word based alignment.

I also want to know that whether inserting padding inbetween the cpu word where all your data is stored any better over storing those fields without any padding. Also, does the position of fields in the same CPU word make any difference?

Thank you!!",1644060234.0
sl86jq,I love these old Bell Labs videos,,7,0.9,computerscience,https://www.youtube.com/watch?v=dFZecokdHLo,0,,1644071276.0
skltie,Confusion Between Different Types of Optimization Problems,,42,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/skltie/confusion_between_different_types_of_optimization/,3,"I do not have a background in optimization and I am trying to teach myself more about this topic. I find myself having a lot of trouble understanding the different ""types"" of optimization problems that exist.

For example, I understand the idea of optimizing continuous functions (e.g. y = x\^2) - for example, we could be interested in finding out the value of ""x"" that results in the smallest value of ""y"". I also understand that continuous functions can be optimized subject to some constraints.

However, I find myself very confused when trying to sort through the following types of optimization problems:

* **Discrete Optimization**
* **Integer Optimization**
* **Mixed Integer Optimization**
* **Combinatorial Optimization**

When I think of these problems, the first thing that comes to mind is that they are fundamentally different from optimizing continuous functions. For instance, the inputs of the above list of problems are usually ""categorial"" in nature - this is why I have heard that problems belong to the above list usually require ""gradient free optimization methods"" (e.g. evolutionary algorithms, branch and bound, etc.) , since it is impossible to take the derivatives of the objective functions corresponding to these problems.

For example, if you take problems such as **""Travelling Salesman"" or ""Knapsack Optimization""** (note: I have heard that these problems belong on the above list, but I am not sure), I would visualize the objective function as something like this:

&#x200B;

&#x200B;

https://preview.redd.it/376tdicu9vf81.png?width=1581&format=png&auto=webp&s=d0d764cc25cbbf26b4422dd478a474dc6419e4dd

 

This leads me to the following question:

* Are 4 types of optimizations on the above list effectively the ""same thing""? The way I see it, all 4 types of these problems have ""discrete inputs"" and in a mathematical sense, ""integers"" are always considered as ""discrete"". In all 4 types of problems, we are interested in finding out a ""discrete combination"" of inputs - i.e. ""combinatorial"". Thus, **are 4 types of optimizations on the above list effectively the ""same thing""?**
* I have heard the argument that ""any optimization problem that can be formulated into a linear problem is always convex (because linear objective functions are always convex)"". If we consider continuous optimization problems - we usually say that ""convex optimization problems are easier than non-convex optimization problems"" because non-convex functions can have ""saddle points"" that can result in the optimization algorithm getting stuck in these ""saddle points"". Using this logic, I have seen the objective function of the ""Travelling Salesman Problem"" being written as a linear function and thus the ""Travelling Salesman Problem"" being considered as a convex optimization problem - I have also heard of the ""Travelling Salesman Problem"" is a very difficult problem to solve. **If the ""Travelling Salesman Problem"" is convex and difficult to solve - does this imply that there are non-convex discrete/combinatorial problems that are even more difficult to solve?**
* I have heard the following argument: Discrete/Combinatorial Optimization Problems are more difficult to solve compared to Continuous Optimization Problems. This is apparently because discrete/combinatorial optimization problems involve ""treating the problem as a continuous problem"" to first come up with a solution, and then determine if the solution lies within the feasible region - thus effectively solving two optimization problems in one. **Is this correct?**
* Finally, I have seen both ""Travelling Salesman"" and ""Knapsack Optimization"" being formulated as a linear problem and therefore as convex. **Are there any well known examples of non-convex discrete/combinatorial optimization problems?**

Thanks!",1644002463.0
skp5fm,Equivalent propositions to P=NP that are not complexity theoretical?,,10,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/skp5fm/equivalent_propositions_to_pnp_that_are_not/,2,"Hello all,

So obviously, existence of a polynomial time algorithm for an NP-complete problem is equivalent to P=NP. And there are many other ways where you can talk about a program running in some time, and if such a program exists or doesn't exist determines if P=NP.

I wanted to ask if there are other equivalent formulations of the P=?NP question that doesn't make reference to Turing machines, computation times or whatever. I actually want to ask how much non-complexity/computation theoretical a formulation can one give. I don't know how this could be, except that I think the whole thing is expressible as a statement about proof lengths in formal systems, but am not sure. Would like to hear any and all ideas!

Hope it makes sense.

Thanks.",1644010816.0
sksa9m,"Strengths and Weaknesses of ""Branch and Bound"" Algorithm",,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/sksa9m/strengths_and_weaknesses_of_branch_and_bound/,0,"I am trying to better understand the strengths and weaknesses of the Branch and Bound Algorithm that is often used in Search and Optimization.

For instance, I have heard that Branch and Bound Algorithm can take more time than Evolutionary Algorithms, but in turn also has the ability to provide exact solutions compared to approximate solutions.

Can someone please comment on this - is this correct?

Thanks!",1644018691.0
sk7puv,Behaviour of TCP and UDP traffic on the same network,,23,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/sk7puv/behaviour_of_tcp_and_udp_traffic_on_the_same/,4,"So, as we all know , one of the main advantages that TCP has over UDP is its flow control mechanism, namely it'll reduce its packet ACK buffer if it detects a high number of packet loss, while UDP just doesn't care. 
 
How does this translate to a bottleneck situation where a router is overwhelmed with both TCP and UDP traffic? 

Will the TCP traffic reduce its transmission rate while the UDP traffic will then dominate the network?

e.g. Lets say router has a 1.000 Kbps bandwidth, there is an incoming 800 Kbps  of TCP traffic and 800 Kbps of UDP traffic, how will the network settle?

My intuition says that eventually the TCP traffic will be reduced down to 200 Kbps while the UDP traffic stays at 800Kbps",1643961054.0
skkkmf,Would you for sure get equal distribution of digits coming out of a TRNG?,Discussion,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/skkkmf/would_you_for_sure_get_equal_distribution_of/,6,"If we ran 10,000,000 cycles would we see a million of each digit, if we are just generating digits?",1643999377.0
sk80jv,Help with analysing time complexity of Karatsuba Algorithm.,,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/sk80jv/help_with_analysing_time_complexity_of_karatsuba/,2,"I have done this much so far. I can‚Äôt proceed further. I computed the amount of work required for each step and added them all together. 
The no of steps are log2(n) and the summation of work done on each step forms a Geometric Progression.

https://imgur.com/a/U7OnMTs",1643962179.0
sjn3gy,Estimating the Run Time of the Travelling Salesman Problem,,35,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/,7,"The ""Travelling Salesman Problem"" is a problem where a person has to travel between ""n"" cities - but choose the itinerary such that:

* Each city is visited only once
* The total distance travelled is minimized

I have heard that if a modern computer were the solve this problem using ""brute force"" (i.e. an exact solution) - if there are more than 15 cities, the time taken by the computer will exceed a hundred years!

I am interested in understanding ""how do we estimate the amount of time it will take for a computer to solve the Travelling Salesman Problem (using ""brute force"") as the number of cities increase"". For instance, from the following reference ([https://www.sciencedirect.com/topics/earth-and-planetary-sciences/traveling-salesman-problem](https://www.sciencedirect.com/topics/earth-and-planetary-sciences/traveling-salesman-problem)):

&#x200B;

&#x200B;

https://preview.redd.it/oat1pzbl7nf81.png?width=753&format=png&auto=webp&s=e7342dde923fa64d8acf19874bee51d58ec06862

**My Question:** Is there some formula we can use to estimate the number of time it will take a computer to solve Travelling Salesman using ""brute force""? For example:

* N cities = N! paths
* Each of these N! paths will require ""N"" calculations
* Thus, N \* N calculations would be required for the computer to check all paths and then be certain that the shortest path has been found : If we know the time each calculation takes, perhaps we could estimate the total run time as ""time per calculation \* N\*N! ""
* But I am not sure if this factors in the time to ""store and compare"" calculations.

**Can someone please explain this?**

Thanks!",1643904849.0
skhmpy,Will software engineers become obsolete by 2030?,,0,0.29,computerscience,https://www.reddit.com/r/computerscience/comments/skhmpy/will_software_engineers_become_obsolete_by_2030/,14,With growing technology like AI I fear I will never be able to experience the Field since my purpose with be meaningless. My goal was to innovate and create cutting edge technology but that‚Äôs hard to do when where literally coding our downfall!,1643992282.0
sjasw0,Is learning low-level engineering useful in building distributed systems?,Advice,15,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/sjasw0/is_learning_lowlevel_engineering_useful_in/,5,"Is learning low-level engineering like compiler engineering and other low-level stuff like CPU architecture, operating systems, and assembly any useful while building large-scale distributed systems and backend microservices. I am interested in learning about low-level stuff but I am doubtful whether it would be useful or not. Any examples where low-level knowledge can help in distributed systems would be great.",1643865444.0
sjuhh6,A computer's ability to reason,Help,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/sjuhh6/a_computers_ability_to_reason/,1,"Hello my fellow nerds.

I'm trying to track down an older article/study about a computer's ability to interpret a picture.

The picture was of a child in a cowboy hat with a lasso, a cat, and a broken vase. The computer was trying to determine how the vase was broken and was coming up with some strange conclusions.

Anyone remember that?",1643922709.0
siux7r,What's the point of studying Group Theory ?,Discussion,50,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/siux7r/whats_the_point_of_studying_group_theory/,23,"I'm doing my BTech in Machine Learning ( first semester ) and teachers often stress on getting good grasp on Linear Algebra concepts and I can see little bit where we are using Vector space concept, Matrices etc. But Professor did not say anything about Group Theory. I don't see it anywhere till now. But somehow we are studying it.




The thing is I'm loving Group Theory very much. I want to continue with this book of Joseph A. Gallian but I'm afraid if I'm wasting my time doing unnecessary. I've never grasped any concept of mathematics as quick as Group Theory, maybe it's easy whatever but I really like it. Will it be any beneficial for me ?",1643824133.0
sj5m8j,Any book recommendations on Networking?,,10,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/sj5m8j/any_book_recommendations_on_networking/,9,"I‚Äôve been working for an ISP for almost a year now as tech support (will be moving towards into a software developer position). My new position will mostly be concerned with building serverless applications, not really a ‚Äúnetworking‚Äù position. However I‚Äôd still like to learn more about the fundamentals of networking! Any book suggestions?",1643850357.0
siry8q,Confused in understanding pumping lemma - Theory of automata.,Help,24,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/siry8q/confused_in_understanding_pumping_lemma_theory_of/,5,"I have just started out theory of automata in my university and we are studying pumping lemma. From what I have understood, the lemma states that y != Œª (empty string) and that x y^i z (where i >= 0) so if ""i = 0"", then y^0 = Œª and the string is still accepted.

How do these not contradict each other? If i = 0, y becomes the empty string that is previously stated cannot be empty? Can someone please clear this for me?",1643817117.0
sj2gsb,I found this awesome video about the TSP and how the Concorde solver roughly works. The speaker is one of its creator !,General,4,1.0,computerscience,https://youtu.be/tChnXG6ulyE,0,,1643842180.0
sicshh,I don't understand how to find the Big-O of functions including multiplying multiple 'n'.,Help,50,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sicshh/i_dont_understand_how_to_find_the_bigo_of/,12,"For specifically, I don't understand how to find the Big-O of functions such as: 

log(2\^n) \* log(n\^2) =

n \* sqrt(n) = 

n \^ sqrt(n) = 

Any help or resources to look at would be appreciated.",1643768469.0
sizzx9,Why do we assume the first value is sorted in an insertion sort algorithm?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/sizzx9/why_do_we_assume_the_first_value_is_sorted_in_an/,6,,1643836228.0
siw69v,"Theoretically, can you remotely re-program the system inside those micro/nano computers, so the devices run by them can just perform any tasks or move to specific environments, also upgrade itself as the programmer desires?",Article,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/siw69v/theoretically_can_you_remotely_reprogram_the/,0,"https://www.dailymail.co.uk/sciencetech/article-5875871/Worlds-smallest-computer-revealed-Incredible-image-shows-tiny-machine-dwarfed-grain-rice.html


https://www.makeuseof.com/nanocomputing-can-computers-really-be-microscopic/

> Nanocomputing involves using nanoscale structures to make computing processes. Nanoscale structures like protein and DNA (deoxyribonucleic acid) can be used to produce nanocomputers.
> DNA computing involves using DNA, molecular biology hardware, and biochemistry to perform computing processes instead of the traditional electronic computing which makes use of silicon chips. Information in DNA is represented using a four-character genetic alphabet (A [adenine], G [guanine], C [cytosine], and T [thymine]), instead of the binary numbers (1 and 0) used by traditional electronic computers.
> When applied to separate and non-sequential tasks, the DNA nanocomputer is better than the traditional electronic computer as it can store a larger amount of data in memory and conduct multiple operations at once. DNA nanocomputers are considerably faster than their electronic counterparts.

So according to these articles, micro/nano computers in that sizes are really able to perform any tasks and store datas just like the traditional computers do or even better? And it is indeed possible with our current technologies that people can remotely re-program the system inside those computers, so the devices run by them can just switch to perform different tasks or move to specific environments, or upgrade itself indefinitely as the programmer desires?

Is it possible that they are even able to perform tasks of building/creating a specific object by using the chemicals and substances they collected form the environment like engineering robots do?",1643827073.0
sj47go,How machines understand us?,,0,0.2,computerscience,https://www.reddit.com/r/computerscience/comments/sj47go/how_machines_understand_us/,2,"Can you recommend  a source material on how computers understand what we want from them on most fundamental level of operations? I don't even know how to phrase this question. I have  a vague understanding of machine code, instructions, compilers, interpreters, etc. but I still don't understand what is the ultimate interface between the software and the hardware. Or maybe my question relates more to how circuit boards  and transistors work.",1643846612.0
si23iw,The SRE Handbook via Google,,47,0.95,computerscience,https://sre.google/sre-book/table-of-contents/,4,,1643740737.0
shzu1j,Algorithms where complexities similar to O(n/k * log(n/k)) appear,,18,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/shzu1j/algorithms_where_complexities_similar_to_onk/,7,"I'm looking for any algorithms or data structure where a complexity of a form like O(n/k * log(n/k)) appears. In particular, I want cases where the complexity is normally considered worse than O(n). It looks like nlogn, so surely such a case exists?


One case I know of is the IO-model where we often have algorithms that perform O(N/B * log(N/B)) IO-operations, with B being the block-size of the IO-device. However, in this context, that complexity is usually considered _better_ than O(N) IO-operations.

(Strictly speaking the IO-model complexities are often more complicated, e.g. with an M/B in the base of the logarithm, but that kind of difference is OK for the kind of examples I'm looking for.)",1643735022.0
siaup5,What's something that has not (yet) been done with computers or programming?,Discussion,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/siaup5/whats_something_that_has_not_yet_been_done_with/,8,"Something that you wish to see done that hasn't been done?

Something that you **do not** wish to see done that hasn't been done?",1643763172.0
si9m1h,Computer Architecture: A quantitative approach,Help,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/si9m1h/computer_architecture_a_quantitative_approach/,2,I am going to start reading this book for a class and I was wondering if there are any resources or videos I can take a look at to be able to understand it. I think I lack the background to be able to understand the book. I already read computer organization by the same authors but had trouble with understanding it because I usually like to learn with videos.,1643759837.0
shi03u,US Governors gathered in person for 114th annual Winter Meeting of National Governors Association. 'Governors will commit to strategies for expanding computer science education in public schools',,38,0.86,computerscience,https://www.nga.org/news/press-releases/governors-convene-in-person-for-114th-national-governors-association-winter-meeting/,1,,1643678107.0
sgzipv,"What is the best explanation you've ever read/seen on how computers go from bits to expressing logic. Still don't get it at its core unfortunately ;). And I don't only mean logic gates, I still don't get the big picture even with them.",Advice,64,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sgzipv/what_is_the_best_explanation_youve_ever_readseen/,26,,1643630008.0
sh0rr0,Looking for online resources,Help,22,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/sh0rr0/looking_for_online_resources/,7,Is there any free educational resources that are somewhat equivalent to the computer science bachelor program (like all the course pdfs of a certain program)? I am really interested in learning as much as I can and enrolling in a computer science bachelor program is not an option for me. I'm not interested in degrees or certificarites I just want to learn.,1643634195.0
shath4,Implementation of Count Sort using a Sparse Matrix,Discussion,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/shath4/implementation_of_count_sort_using_a_sparse_matrix/,1,"Hi, The count sort is only feasible for some inputs of 'k' that are not abnormally large since the space complexity increases arbitrarily. Can it be implemented using a sparse Matrix to counter that drawback.

I've tried but can't seem to get it work, can someone provide some insight on this. Thanks",1643659852.0
sgvyvn,Overlaps in research between Computational Geometry and Graph Theory,Discussion,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/sgvyvn/overlaps_in_research_between_computational/,2,"Hi, 

I'm currently a CS undergrad who is very interested in graph algorithms and NP-complete problems. I am going to start my grad school this year to study computational geometry (I only have basic knowledge about this such as polygon triangulation and partitioning.

While looking through some other applications of computational geometry, I noticed that there are problems that I am familiar with in a graph theoretic manner (such as the steiner tree problem and TSP as mentioned in [this 1976 paper](https://dl.acm.org/doi/10.1145/800113.803626) (Some NP-complete geometric problems). 

The school that I am going to mainly focuses on shape matching, casting, and dynamic convex hulls so I cannot see much intersections on Graph Theory here (I believe). 

My question is, are there current overlaps in research between geometry and graph theory (in the CS sense) that is active these days beyond TSP, geometric networks, and Steiner Tree? Thanks!",1643616183.0
shfux4,Do a computer engineer have same software knowledge as a computer science software engineer?,Discussion,0,0.35,computerscience,https://www.reddit.com/r/computerscience/comments/shfux4/do_a_computer_engineer_have_same_software/,24,"Someone who studies computer engineering learns how a computer works in low level. If they know the hardware don‚Äôt they also know what software can be built on it and how?

For me it feels like computer science is an easier part of hole IT, CS, CE, EE field. They only know high level things for the most part. So the computer engineers have the same knowledge as a software engineer and the computer engineer do also have hardware knowledge. When does CS/IT education benefit from CE when the time to study CE and CS (both BSc and MSc) is mostly the same.",1643672113.0
sg4epv,"In technical detail, how does the internet router work?",,41,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/sg4epv/in_technical_detail_how_does_the_internet_router/,10,"I saw a comment on a programming subreddit that casually mentioned that they wrote code for an internet router, and I was mind blown. I decided to look up how they work because i was inspired to work on a project like that, but i didn't get any info. How do internet routers work?",1643530219.0
sfiktu,"Ever since I saw the graphs for the Lorenz attractor and logistic map in my first year CS course, I thought they would look cool as art prints, so I made some using Python and Photoshop!",,311,1.0,computerscience,https://i.redd.it/pg40caf5ume81.jpg,21,,1643464477.0
sf4ab2,1982 'Twin Cities Computer User' Newspaper,,66,0.99,computerscience,https://www.reddit.com/gallery/sewioz,3,,1643413550.0
sf7cyl,I'm confused? I can't tell the difference between a Software Engineer and a Programmer...,,10,0.68,computerscience,https://www.reddit.com/r/computerscience/comments/sf7cyl/im_confused_i_cant_tell_the_difference_between_a/,19,"I want to teach a computer to do things based on what I code it to do. What is this called? Is this a Software Engineer or a Programmer? I am confused and don't know where to go with this. I know you can code apps to pick out what the user wants. I'm doing research and the more I research the more confusing it gets. I don't want to be responsible for what things look like, I just want to do all the back end stuff. I just want to make things work. Make it make sense, please. I want to get a job, but don't want to learn the wrong stuff.",1643422532.0
seq1gi,what is the purpose of the ISA Bridge in the following PCI diagram?,Help,126,0.96,computerscience,https://i.redd.it/n90xtw3cdfe81.jpg,12,,1643374076.0
sf5rtw,How does functional programming deal with memory management?,,9,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/sf5rtw/how_does_functional_programming_deal_with_memory/,8,"In functional programming, we don‚Äôt create mutable data correct? I understand the usefulness of eliminating side effects for debugging, but doesn‚Äôt allocating a whole new data structure for every change gobble up memory? Or does this just depend on the compiler?",1643417759.0
sepvgl,Learn Computational Economics (Free Course),,15,0.86,computerscience,https://simulation.school/p/computational-economics,1,,1643373510.0
secwcr,Do you need a theoretical understanding of all the math stuff for Computer Science?,Advice,101,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/secwcr/do_you_need_a_theoretical_understanding_of_all/,52,"I'm going through Precalculus II: Trigonometry, and I sort of do understand some of the stuff at a conceptual level but I'm kind of worried that I need to literally understand why everything works the way it does for Trigonometry, Calculus, and whatever other math is needed for Computer Science.

Like, I didn't sign up to be a mathematician, I'm fine with not knowing all the theoretical details, but I'm worried that I'm going to actually have to know them if I get into Computer Science.

So, is it a problem? (Note that this isn't about knowing the math - I'm going to learn Calculus and Linear Algebra Ok, that's not the problem, it's more about knowing how it works underneath)",1643328619.0
sf5gj9,How do websockets and round robin LBs work together?,Help,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/sf5gj9/how_do_websockets_and_round_robin_lbs_work/,1,"Round robin LBs typically are TCP layer LBs which merely route incoming requests in RR fashion. Websocket protocol specifically needs that the client keep talking to the specific server with which it has opened the socket. How do client requests get routed to the same server by the TCP LB?
I tried looking it up online but couldn't find many resources",1643416854.0
sf7tjb,programming lang paradigm,,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/sf7tjb/programming_lang_paradigm/,1,"Hi, what website has latest programming lang paradigm? I am thinking to create a new language and want to get some idea from the others. thanks",1643423943.0
seqqpv,Is Software Engineering a sub title to Computer Engineering?,Discussion,2,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/seqqpv/is_software_engineering_a_sub_title_to_computer/,12," Is computer engineering consider top of the line or ""cooler"" than Software engineering? What I mean by this is that computer engineers often design embedded system or the computers/micro controllers and write low level software. After that the software engineers code och develop stuff on hardware that computer engineers made. For me it sounds like this: Computer engineering says to software engineer: ""Your not that good at computer so i'll build it for you and then you could program your software on my hardware with high level programing languages thats easier to understand"". Do you get what I mean, or am I wrong?   


Is there stuff that Software Engineers is able to make that the computer engineers aren't? My understandings is that because a computer engineer have made the computer they know everything about it and how to code and use it. And a software developer only learns how to use the tools that a computer engineer have made and dont need all that technical understanding. Is computer engineering higher prestige than software? Do a software engineer knows better way of making software than a computer engineer? This is my thoughts don't get offended by my thoughts instead describe to me how the job market works, im still a student.",1643376272.0
sed2hd,What are the best research papers/articles etc. for someone interested in zero-knowledge proofs and their practical applications?,General,9,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/sed2hd/what_are_the_best_research_papersarticles_etc_for/,0,,1643329084.0
sdur26,What is a good book for learning algorithms in programming?,Advice,86,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/sdur26/what_is_a_good_book_for_learning_algorithms_in/,29,"Hello there, i am a somewhat intermediate full-stack developer who is 100% self taught without any educational background in CS. but i want to up my game by learning some of the scientific fundamentals of computer programming, and i figured what better subject  to start with than algorithms!? so anyways let me know ur suggestions guys :D",1643276745.0
sdxd4b,Data Structure & Algorithms Book for Beginners,Advice,8,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/sdxd4b/data_structure_algorithms_book_for_beginners/,6,"\[SOLVED\]

Hi all,

I know there are many posts here asking for book recommendations, particularly on this topic.

However I am looking for a recommendation of a good book for beginners with easy to comprehend language and structure. I agree that its a tough topic so all of them will be somewhat technical I am just wondering if anyone knows any that are both good but simple enough to follow!

Many thanks!",1643286803.0
sdh6gc,Why doesnt multi-core processor like dual-core or quad-core perform 2x or 4x times faster?,,62,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sdh6gc/why_doesnt_multicore_processor_like_dualcore_or/,40,"Yes, its true, your program must be written in a way so it could be processed by multiple cores or processed effectively by them.

  
So to this question:  
Why dual/quad isnt 2x/4x faster then single core or why isnt quad 2x faster then dual core processor,

beside saying:

*  Due imperfect software algorithms and implementation. 

Are there any other facts/arguments we can add, like hardware ones, e.g.:

* They all share same resources, memory, bus etc etc

I know that there are some details that might need to be included in this like, brand, microarchitecture, speed etc etc... I am aware of that, its just a general question about multiprocessing.

Feel free to post any link where this might have been answered or it contain some explanation.

Thank you! :))",1643233106.0
se2nq0,"Is there a ""trait theory""?",Discussion,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/se2nq0/is_there_a_trait_theory/,2,"I'm interested in getting into type theory, lambda calculus, etc. but I'm wondering if there's an equivalent model for *traits* rather than types.

For example, in C any non-zero value is True, so a pointer has the ""trait"" of being either true or false depending on whether or not it's null.

A `mod` type in Ada has the ""trait"" of being bounded between two values, and also the ""trait"" of overflowing to stay within those bounds.

This can allow you to express some very complicated things. The trait system in Rust allows you to say ""if I can do X, then I can do Y."" Using that, you can basically have behaviours ""implement themselves"" as long as there's a valid path through the trait implementations.

I'm wondering if there's a theoretical system similar to type theory that uses ""what you can do with something"" as the core idea rather than ""what something is.""",1643301893.0
sd0e5c,All the ways to model and deal with side effects in a program,Help,21,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/sd0e5c/all_the_ways_to_model_and_deal_with_side_effects/,4,"I know about [monads](https://en.wikipedia.org/wiki/Monad_(functional_programming)) for dealing with side effects, by adding the ""world"" as an internal variable which gets passed around. And I know the basics of linear type systems (or more generally [substructural type systems](https://en.wikipedia.org/wiki/Substructural_type_system)), some of the ideas of which are used by Rust. Monads are used in Haskell or perhaps other languages, but these are purely for functional languages as far as I can tell (it is to make the functions pure again). I am not too sure about linear logic/types though.

But what other ways are there of dealing with side effects in terms of type theory, especially regarding to imperative or object oriented programming languages, dealing with pointers perhaps, or memory management, object creation and destruction, accessing and manipulating local/global state/context, etc.. It would be wonderful to paint a full picture of the possibilities the research has uncovered so far, so it is easier to make sense of how to model side effects (especially in imperative programs using type theory).",1643182419.0
scik77,A New Crisis in Mathematics?,Discussion,29,0.97,computerscience,/r/math/comments/scamj7/a_new_crisis_in_mathematics/,0,,1643130626.0
sccyyv,"What's the most efficient way to keep yourself up to date in your field after working deeply focused in the lab all week (while experts in the field are non-stop posting amazing stuff in twitter, papers, podcasts, youtube, books, in-depth news, subreddits etc.)?",Discussion,40,0.96,computerscience,/r/learnmachinelearning/comments/sc2w0e/whats_the_most_efficient_way_to_keep_yourself_up/,5,,1643114971.0
scv94z,"Summary of the KindLang ""proof language"" theory?",Discussion,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/scv94z/summary_of_the_kindlang_proof_language_theory/,0,"Just stumbled across [KindLang](https://github.com/kind-lang/Kind), ""a modern proof language"", which looks amazing. I found some of the [old writing](https://github.com/kind-lang/Kind/tree/f70ed05356f4d6e88f4d28df3d1ddb0a3cad8c94), such as the so-called [FMC Specification](https://gist.github.com/lancejpollard/afa755498c6590c821290dc2b24a588b#file-fmc_specification-md), which describes some of the theory. Some important snippets from it are (bold added):

>Within the field of type theory, mathematical proofs and theorems are represented by functional programs and their associated types. This is called the Curry-Howard correspondence, and is the basis of modern proof assistants such as Agda, Idris, Coq and HOL/Isabelle. Sadly, those are complex software with monolithic implementations that are hard to audit and reason about. We're interested in a simpler alternative: a fully-featured proof assistant that can be turned into a small standard that is easy to analyze and implement independently.  
>  
>Under that perspective, one may be interested on the **Calculus of Constructions** (CoC), a small type theory that **easily fits 1000 lines of code** in a modern programming language. Sadly, that and similar languages **aren't capable of deriving mathematical induction**, an important proof technique without which any non-trivial theorem isn't provable. Moreover, it isn't capable of expressing efficient (constant space and time) pattern-matching, without which functional programming isn't viable. The usual solution to both problems is to supplement CoC with a native datatype system, but this results in the complexity explosion that is seen on the mentioned languages.

The same author made this [Calculus of Constructions repo in JavaScript](https://github.com/VictorTaelin/calculus-of-constructions/blob/5b4ede47dfcc408ce27903f78d29c81bf393a9d3/src/core.js), but it seems to have been replaced by KindLang.

So then it is stated:

>**A simple, clever alternative was proposed by** [Aaron Stump](https://homepage.divms.uiowa.edu/~astump/). With just one additional primitive, the ""**self type**"", **coupled with mutually recursive definitions, one can easily derive induction** for lambda encodings, i.e., a way to represent datatypes with native lambdas, subsuming the need for a separate implementation. This also solves the efficiency problem, as we're able to pattern-match in constant time with certain lambda encodings. The problem with that solution is that providing a semantics and, thus, proving the consistency of the resulting system becomes extremely hard.  
>  
>Aaron Stump **moved on from self types towards a very similar solution based on dependent intersections**. The idea is that, instead of relying on mutual recursion, inductive datatypes can refer to themselves in simplified, erased forms. This results in a comparably small language that is much easier to provide a semantics for, which Aaron Stump called Cedille-Core. In exchange, programming on it is hard. The simple task of defining a datatypes with efficient pattern-matching and recursion becomes a complex task involving non-trivial type-level manipulations and many intermediate proofs. While doable, programming in Cedille-Core without a supplementary language to do all that work for you is not practical.

Then it talks about ""consistency"" being a problem when modeling, which is a bit over my head. Then:

>Computationally, Formality-Core is just the lambda calculus.

Meanwhile, the latest KindLang [THEOREMS](https://github.com/kind-lang/Kind/blob/master/THEOREMS.md) doc describes how to prove things. Haven't read through all of that yet, though.

What I'm wondering is, what is the summary of the theory KindLang is built upon? What I shared above is pieced together.

I found this by landing on the [Calculus of Constructions](https://en.wikipedia.org/wiki/Calculus_of_constructions) (CoC) after looking at the [Lambda Cube](https://en.wikipedia.org/wiki/Lambda_cube). I found that from looking at [HoTT](https://en.wikipedia.org/wiki/Homotopy_type_theory) and [UniMath](https://en.wikipedia.org/wiki/Univalent_foundations), both of which are touted as ""foundations of math"". (UniMath is based on Calculus of Constructions as far as I understand). But as the above snippets show, CoC cannot model induction or recursion I guess, so I wonder how these can serve as foundations. So it looks like KindLang builds upon CoC like UniMath, but adds that ""self type"" that Aaron Stump talks about. *Does it add anything else?*

I was reading about [Cubical Type Theory](http://nlab-pages.s3.us-east-2.amazonaws.com/nlab/show/cubical+type+theory) (doesn't even have a Wikipedia page yet), but it was said it also had some key problems, and then landed on [this paper](https://arxiv.org/pdf/2101.11479.pdf) which states right up front:

>We prove normalization for (univalent, Cartesian) cubical type theory, **closing the last major open problem** in the syntactic meta-theory of cubical type theory.

*Maybe I misread that though.*

So I am wondering, what theory does KindLang use, and how does it relate to Cubical Type Theory, which seems to be the latest edition of Type Theory that is the most expressive.

Trying to figure out what the latest theory is for implementing a proof language like KindLang.",1643165204.0
scntb2,Resource recommendations for imperative algorithms that implement Type Checking and/or Type Inference,Help,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/scntb2/resource_recommendations_for_imperative/,0,"What are some good books, journal articles, or even GitHub repos, which show how to implement algorithms for **type checking** and/or **type inference**? There appear to be several resources with implementations in functional languages like OCaml or Haskell, but I have yet to find anything which could be used to write a JavaScript implementation of type checking and type inference (i.e. algorithms defined in an imperative style). I would like to learn how to implement it in detail.",1643144450.0
sbw98k,How do general compression algorithms approach data redundancies in novel file formats?,,25,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/sbw98k/how_do_general_compression_algorithms_approach/,7,"For example, if one had a source file containing 100 transformed instances of the same 3D model but that information is just vertices within the source file is a model built to recognize that data redundancy? Are algorithms for various types of information like this updated on a regular basis and integrated into the popular compression methods?",1643057763.0
sbxbya,Max number of parallel http requests,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/sbxbya/max_number_of_parallel_http_requests/,3,"Hi all, is there a way to know/calculate how many parallel http requests a computer can send?",1643060451.0
saqw7i,Human Brain Cells From Petri Dishes Learn to Play Pong Faster Than AI,Article,215,0.99,computerscience,https://science-news.co/human-brain-cells-from-petri-dishes-learn-to-play-pong-faster-than-ai/,29,,1642934253.0
sbk9aw,"Do you believe in cryptocurrency like Bitcoin and Ethereum, etc.? Why or why not?",Discussion,0,0.48,computerscience,https://www.reddit.com/r/computerscience/comments/sbk9aw/do_you_believe_in_cryptocurrency_like_bitcoin_and/,34,"I would be asking this question in the crypto sub but that sub honestly just seems like an echo chamber. I myself have limited knowledge about cryptocurrency so I'm willing to learn more about it from some of you. Basically what I know is:

1. It acts as a digital ledger for a given set of programs.

2. There are limited number of Bitcoin and the more there are, the harder it is to mine them.

3. It can decentralise power and all computers acting as mining devices form part as a mini server for mining.

4. With all the talk about how Bitcoin is the future, I'm still skeptical yet to see mast adoption of this system even in the near future.

5. Many of the conversation about crypto (and Bitcoin specifically) seems and looks kinda scammy and I have also noticed many bots on such forums.

Apologies if there are some misinformed arguments here, as I said I myself have tried to understand it but still have a lot of confusion. So, what is your opinion?",1643024277.0
sasbd7,"In Dijkstra's Algorithm, if we assume that for a problem all input graphs are complete, the time complexity will be O(V^2) no matter the implementation, right?",Help,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/sasbd7/in_dijkstras_algorithm_if_we_assume_that_for_a/,3,"For example, the implementation with Fibonacci heap and adjacency list has a time complexity of O(E + V‚Ä¢log(V)). Because we assume the graph is always complete, the time complexity is O(V^2 + V‚Ä¢log(V)), which is O(V^2 ), right?",1642939758.0
sa6sdw,Need recommendations for CS books and courses.,Help,31,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/sa6sdw/need_recommendations_for_cs_books_and_courses/,16,"I will be starting my bachelors in CS in fall 2022.
Before the course begins , what Books/courses can I take/read for introduction on the topic.
Especially please recommend some basic beginner books for Linear algebra , Calculus and Discrete Maths.",1642870813.0
s9enpp,"Started learning ML 2 years, now using GPT-3 to automate CV personalisation for job applications!",General,259,0.99,computerscience,https://gfycat.com/snappysadichthyostega,43,,1642782622.0
s9gnb2,Maths for beginners book/resources recs,Advice,7,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/s9gnb2/maths_for_beginners_bookresources_recs/,11,"Hello, 
I'm planning on studying CS next year, and from what I've heard it's kinda maths heavy and I have forgotten everything since I graduated high school back in 2014, can you guys recommend any books or courses that would help refresh my memory and prepare me for Calculus and linear algebra? 
TIA.",1642787809.0
s95ek1,How are vectors used in Computer Science?,,33,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/s95ek1/how_are_vectors_used_in_computer_science/,8,"I am working on a project and I want to explore how vectors are used in Computer Science.

Some articles/YT links on the same would be v helpful.",1642750830.0
s9ffoc,What would you suggest ACM or OReilly?,,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s9ffoc/what_would_you_suggest_acm_or_oreilly/,4,"I was looking to get a sub on OReilly Online Learning, but I noticed a lot of people suggest Association for Computing Machinery. I'm wondering how these compare for learning purposes and possibly certification, for Computer Science and Software Engineering generally.  


I also noticed that ACM offers OReilly material through their own subscription - what OReilly resources exactly does one get through ACM, is it books, etc. or live content and training as well?  


If you have any suggestions and tips on other similar platforms, please do share, I'd appreciate it a lot.  


Thanks!",1642784616.0
s9c6ve,Are data types self-aligned or aligned according to the memory access granularity of the processor?,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/s9c6ve/are_data_types_selfaligned_or_aligned_according/,1,"I was reading [The Lost Art of Structure Packing](http://www.catb.org/esr/structure-packing/) by ESR(Eric Steven Raymond) and he was written this:
> The jargon for this is that basic C types on x86 and ARM are self-aligned. Pointers, whether 32-bit (4-byte) or 64-bit (8-byte) are self-aligned too.

Now, he says that the data types are self aligned on x86 and ARM processors and pointers are self-aligned on 64-bit architectures too. Self-alignment means that data is stored at a memory address which is a multiple of the size of the data type of the data.

My question is that is data aligned according to its size(self-alignment) or according to the memory access granularity of the processor. There are hardly any cpu architectures which allow data types larger than their memory access granularity, so even it won't make a great difference if some data is self-aligned or aligned according to memory access granularity of the processor.

It would be great if you could specify some cpu architecture that allows data types larger than its memory access granularity. If self-alignment and alignment according to memory access granularity do not practically make a difference at the memory level, won't it still be better to say that data is aligned according to the memory access granularity rather than self-alignment.",1642775959.0
s8eq88,what book(s) would you recommend that could come across as motivational to a programmer? Something about the deep depths of the art of code and philosophy kind of thing?,,79,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/s8eq88/what_books_would_you_recommend_that_could_come/,24,,1642669337.0
s8kxq7,Name of Algorithm,,5,0.74,computerscience,https://www.reddit.com/r/computerscience/comments/s8kxq7/name_of_algorithm/,3,"Is there a name for an algorithm that assigns N outdoor people to N distinct houses such that the total distance they have to walk to their respective houses is minimized? (This is just another wording for assigning boxes to drop-off locations in Sokoban if you heard of that). Or is this an intractable problem where every permutation has to be tested?

Not sure if the Gale Shapley algorithm will work in this case??",1642690952.0
s7yrsa,Why is there nothing between 8bit and 16bit?,,36,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/s7yrsa/why_is_there_nothing_between_8bit_and_16bit/,31,"For example, if i want to change the image specifications in photoshop, i can choose between 8 and 16 bit. But why is there nothing inbetween 8 and 16 bit? Since the difference between those to options is that high, 8 bits are sometimes too less and 16 bits are sometimes too high.",1642621874.0
s7li8v,Why is Internet/transmission speed in Bit and storage capacity/file size in Byte?,,61,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s7li8v/why_is_internettransmission_speed_in_bit_and/,42,"I am always wondering about this. Why Internet/transmission speed is always in Bits (Kbits, Mbits...) and storage capacity/file size is always in Bytes (Kbytes, Mbytes...)? Can't we just have the same unit for both of them (to reduce the confusion of some people)? Is there a reason for the convention like this?",1642581032.0
s82mjq,How did you learn about different protocols,,4,0.64,computerscience,https://www.reddit.com/r/computerscience/comments/s82mjq/how_did_you_learn_about_different_protocols/,4,"I always wonderd, how people can write programs, that interact with the web like its their harddrive, I always get confused what, how an why the different protocols do. My biggest problem is finding a good resource to learn about the stuff, often I find myself on websites, that are glossing over details and leaving out things. So my question is where did you start and where did you learn it ? 

Good book recommendations related to the subject are appreciated",1642631640.0
s7wqyg,Will a RISC like instruction set ever be developed for QC?,Discussion,5,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s7wqyg/will_a_risc_like_instruction_set_ever_be/,0,"Horray for quantum computing picking up more traction for the past few years. I've done a fair bit of reading and tinkered with things like qiskit  


but algorithm implementation on a per qubit level seems a little üò¨

&#x200B;

What does a RISC-like instruction set look like for QC, is it even viable? Are circuit construction implementations here to stay?",1642616748.0
s7h2vc,Can someone explain to me Big O notation like I'm dumb?,Help,50,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s7h2vc/can_someone_explain_to_me_big_o_notation_like_im/,16,I've learnt it today in Java but can't seem to wrap my head around it. How can I learn to use it?,1642565300.0
s8a2k9,Is cellular life itself an example of the P=NP problem?,Discussion,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/s8a2k9/is_cellular_life_itself_an_example_of_the_pnp/,11,"This is a little bit out there, but I think this is an interesting way to think about / apply the question of P=NP. 

Think about cellular life. Biologically, for many organisms (think simple multi cellular; plants, things that don‚Äôt have super complex brains) we understand the structure and function of literally every single atom and cell that they are made of. But we are unable to synthesize these organisms we know so much about ourselves in a lab despite knowing how to perfectly verify nearly every piece of their structure by looking at them. 

Translating to the P=NP problem, the ‚Äúverification‚Äù of a problem can be thought of as examining every single piece of an organism and confirming that it is what we think it is. The ‚Äúsolving‚Äù of the problem would consist of creating such an organism from scratch, from non-living material. Despite knowing how every single piece of a multi cellular organism works and being able to verify them, humans cannot solve them by creating them in a lab. 

What is the only current known way to ‚Äúcreate‚Äù life? By waiting for nature‚Äôs random iteration to try and fail over billions of years and trillions of ‚Äúguess and check‚Äù iterations. If evolution can be thought of as the only way to achieve the ‚Äúsolution‚Äù, until we can figure out a way to synthesize complex multi cellular organisms from scratch, nature‚Äôs answer to the P=NP problem is that verifying a solution does not imply that it can be solved. Only guessed and checked over trillions of years.

This may not make sense but I think it‚Äôs an interesting way to think of how nature sort of answers one of humanity‚Äôs biggest questions.",1642652810.0
s7mb3k,Why is Knapsack NP-complete?,Help,3,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/s7mb3k/why_is_knapsack_npcomplete/,5,"Here's what [wikipedia](https://en.wikipedia.org/wiki/Knapsack_problem) says:

> Even if P‚â†NP, the O(nW) complexity does not contradict the fact that the knapsack problem is NP-complete, since W, unlike n, is not polynomial in the length of the input to the problem.

`W` is the max weight capacity.
`n` is the number of items.

Why is `n` polynomial in the length of the input? 
If the input is binary, `n` is definitely exponential in the bit length.

Also, for problems in NP, we can verify them in polynomial time. However, here for a polynomial time algorithm in the numeric values `n` and `W` for knapsack, we still call knapsack NP-complete because of the length of the input. By the same logic, wouldn't the algorithm to verify a solution to knapsack **not** be polynomial (pseudo-polynomial) - and so, it's not even in NP?  
Could you please clarify this?",1642584461.0
s71762,Composite Images of Sphere into 3d object and more?,,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/s71762/composite_images_of_sphere_into_3d_object_and_more/,3,"I have a question, which I believe is fairly well defined, but I don't have the knowledge to approach it.  If anyone has an idea of a better place that I can ask this question that would be excellent.

The problem :  I'm playing a game called Scam \*cough\* I mean Star Citizen and there are many large 3d worlds to explore but no coordinate system yet.  It does, however, give you the distance in kilometers you are from given hubs on said world as well as the center of the planet itself.

The hair brained scheme:  
Step 1 - take screenshots of the planet from X km away at each pole and then again from a few angles around the equator.  
Step 2 - stitch those together onto a 3d sphere to create a mapped globe  
Step 3 - take screenshots of interesting locations by flying to a certain altitude and looking straight down.  Then try to find software that can look at the screenshot and match it with a given location on the globe

Some problems I'll face:  
\- the screenshot will be a 2d image of a sphere and so will need to be reshaped as it cant just be put on as a skin ( there is a better mathy way of saying this right? something something projection? but im making sense yeah? )  
\- The planet will have lighting and cloud cover which change and could be confusing to any ML Algo.  
\- Mapping locations to the planet might be difficult as the planet may be too 'samey'.  A second option is to record the distance one is from several WP on the planet and use that information to, i believe the word would be, ""triangulate"" the position? yeah? Does that sound right?

This sounds very possible to me but might require some specialized ML magic that I'm not educated on? If anyone has any advice for how to better approach or frame the problem, or potential solutions for software or math that can be used to solve this, I would be stoked :)

In case it's relevant I'm fairly competent with JS and Node

Example Images

https://preview.redd.it/mv9r2kbk9hc81.png?width=898&format=png&auto=webp&s=94718778f56c9751b692874cbab53eb6f5019911

&#x200B;

https://preview.redd.it/ze65vktr3ic81.png?width=987&format=png&auto=webp&s=2f70d344c0756252f0f8f91aadff95052bc3dcbb",1642522761.0
s6v29r,Good resources to learn more about authentication and authorization concepts?,Help,3,1.0,computerscience,/r/webdev/comments/s6ev5f/good_resources_to_learn_more_about_authentication/,0,,1642504022.0
s6msij,Where can I get access to industry journals or scholarly articles free/cheaply?,Help,8,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/s6msij/where_can_i_get_access_to_industry_journals_or/,4,"Having finished a masters a couple years ago,  I miss having access to journals and articles that my university library provided. Is there a free/inexpensive source of similar materials that I may be overlooking?

Thank you!",1642474798.0
s6zxpb,"Creating a scalable intent classifier with Elixir, Python and Tensorflow | Arjan Scherpenisse - ElixirConf EU 2021",,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s6zxpb/creating_a_scalable_intent_classifier_with_elixir/,0,"Arjan Scherpenisse gave an amazing talk with title ""Creating a scalable intent classifier with Elixir, Python and Tensorflow"" for the #ElixirConfEU conference in Warsow.   

Watch the video & learn more about the QnA ninja, a classifier service that recognizes text  https://youtu.be/U8c\_hsWC2jE",1642519300.0
s6mxkp,Need some Cache Memory analogies‚Ä¶preferably to a refridgerator. Thanks :),,9,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/s6mxkp/need_some_cache_memory_analogiespreferably_to_a/,3,,1642475188.0
s67l1c,What is Singular Value Decomposition (SVD)? A 2-minute visual guide. [OC],Discussion,59,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/s67l1c/what_is_singular_value_decomposition_svd_a/,8,"&#x200B;

https://preview.redd.it/3a0ls71hv9c81.png?width=2048&format=png&auto=webp&s=fbf7e51bb04eb3215d08ff139b831a059e1a145a

https://preview.redd.it/lhtxvf3hv9c81.png?width=2048&format=png&auto=webp&s=0f072d9ab91e62c9dcb06b8a0a3ef342333d4105

https://preview.redd.it/6hswrj3hv9c81.png?width=2048&format=png&auto=webp&s=48a76af7b76ac6b4d1150b5e718bc82137ed29a9

https://preview.redd.it/ckrbdg1hv9c81.png?width=2048&format=png&auto=webp&s=8d441f775adde43a065b8e36403ac2cb514de0ef

https://preview.redd.it/zvtlv91hv9c81.png?width=2048&format=png&auto=webp&s=6b948bbe8b0ca2227ad370569f7b1be74efe72bc

https://preview.redd.it/hphsu02hv9c81.png?width=2048&format=png&auto=webp&s=74df09e30462215b4d2c0f203c772ba2b9d11824

https://preview.redd.it/hglrki1hv9c81.png?width=2048&format=png&auto=webp&s=9163169b5eacdafcca7ec3c61e31675f730791f1

üîµ Singular Value Decomposition üîµ

ü™Ñ Remember from the [post on eigenvectors](https://www.reddit.com/r/learnmachinelearning/comments/ruf6eq/what_is_an_eigenvector_a_2minute_visual_guide/): eigenvectors of a matrix are special vectors that only get scaled when the matrix is applied to them. While EVD (eigenvalue decomposition) can only be applied to special matrices (diagonalizable matrices) SVD is a generalization of EVD and can be applied to any rectangular matrix.

üß© Like prime factorization where a number is broken down to its prime factors (simpler pieces = prime numbers in this case); SVD factorizes the matrix into simpler pieces i.e. simpler matrices. This factorization or decomposition comes in handy for many applications some of which I briefly touch upon later.

üè•ü¶¥ Time for (somewhat of) an analogy: You can think of SVD as an x-ray of a matrix. It provides us with simpler pieces that constitute the matrix and by looking at these simpler pieces i.e. simpler matrices we can say a lot about the matrix in question. Things like its (pseudo-) inverse, rank, null-space, range among other things can be easily determined the same way a doctor can say a lot with an x-ray scan of your body.

üî® The Singular Value decomposition breaks the original matrix into 3 pieces: 2 unitary matrices and a rectangular diagonal matrix. You can draw a parallel with the Eigenvalue decomposition equation and see exactly how SVD has a more general form.

üîÑ With the help of the Singular Value decomposition we can explain what happens to an arbitrary vector when a matrix M is applied to it. Since M can be broken down into simpler pieces, we can represent the matrix M by a rotation, followed by scaling (by the singular values) and another rotation. So if you had a circle it would first be rotated then stretched by the singular values (becoming an ellipse) and finally be rotated to end up as a rotated ellipse.

üóúÔ∏è There are numerous applications of SVD. Perhaps you could comment on the ones that I missed but you know about. One of the nicest things you can get is the pseudo-inverse of a matrix without doing crazy matrix inversion. This is because the simple pieces that SVD provides are extremely easy to invert making inversion of the original matrix child's play. SVD is heavily applied in solving numerical problems such as solving linear equations that so frequently pop up in engineering.

üé• Data compression and computing low-rank approximations make it handy for compressing images and videos so you can watch cat videos without buffering. And of course, SVD has also been used for ranking web pages like Google's Pagerank algorithm does so you can find your favorite recipe with a quick search.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1642435873.0
s6ijx1,"""The early days of Unix at Bell Labs"" - Brian Kernighan (LCA 2022 Online)",General,6,0.88,computerscience,/r/unix/comments/s6iiyh/the_early_days_of_unix_at_bell_labs_brian/,0,,1642462826.0
s5vvf5,Initiating a study-group for MIT's Algorithms Course,,60,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s5vvf5/initiating_a_studygroup_for_mits_algorithms_course/,11,"Hello,

I am looking for other students to join me in learning [Erik Demaine's algorithms course](https://stellar.mit.edu/S/course/6/sp15/6.046J/).

The course is mathematically matured and requires a background in discrete math, logic and proof techniques.

If you are already familiar with algorithms, Then you might focus more on advanced lectures. If you lack background in math, Then you might focus more on early lectures. Students coming from non-CS background are highly welcomed, including engineering, physics, and pure-math. Especially if they were interested in computational methods. We hope they contribute to CS students a new perspective about computation.

We hope to initiate a community, Where you can mutually contribute and benefit from others' experiences, skills, and backgrounds. You are welcome to ask questions, share your solutions to be reviewed by others, and even suggest further problems/topics.

We plan to complete two weeks of the course on one month, and have 1 hour meeting per week. There is no obligation to attend all meetings, solve the whole course's problem-set, or to respond to every question posed. Learn and engage with other members at your own suitable pace.

My availability is limited to 2 weeks per month, but other students are welcomed to collaborate at any time.

~~Join by submitting this google form.~~

We started but it is not late to catch up. Through the [group's website](https://mostafatouny.github.io/erik-alg-website) you can see our previous meetings, and Join our community.",1642396803.0
s694uk,"First time posting here, hope this is the right kind of thing to put here. Can someone explain the basics of Bayesian Neural Networks to me? Specifically if they address the problem of overconfidence in AI systems when they encounter information outside of their data sets. Thanks!",,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/s694uk/first_time_posting_here_hope_this_is_the_right/,4,"Bit of background, I come form the social sciences and very little computer science knowledge but I'm researching how the integration of AI will impact my field of study. 

One potentially large problem I've found is that AI can be overconfident when faced with information outside of what it has been taught to recognize. I've read that bayesian neural networks can mitigate this problem but my lack of computer science knowledge making me run into a wall when it comes to understanding exactly why it fixes this problem. 

I'm doing other readings on this topics but I thought I farm this out to Reddit and use this site for more than just watching people argue about literally anything. Thanks for any assistance or sources you can provide!",1642439716.0
s67zoq,Question about the correctnes of an algorithm,,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/s67zoq/question_about_the_correctnes_of_an_algorithm/,4,"**Problem statement:**

>Write a function, which creates a random graph of a certain size as follows. The function takes two parameters. The first parameter is the number of vertices n. The second parameter p (1 >= p >= 0) is the probability that an edge exists between a pair of nodes. In particular, after instantiating a graph with n vertices and 0 edges, go over all possible vertex pairs one by one, and for each such pair, put an edge between the vertices with probability p. The graph representation is an adjacency matrix.

**Proposed algorithm:**

    1- Create a bool matrix (2d nested array) of size n*n initialized with false.
    2- Run a loop over the rows:
      - Run an inner loop over the columns:
        - if row_index != col_index do:
            - curr_p = random() // random() returns a number between 0 and 1 inclusive
            - if curr_p <= p: set matrix[row_index][col_index] = true
              - For an undirected graph, also set matrix[col_index][row_index] = true
    
    **Note:**
    Since we are setting both cells (both directions) in the matrix
    in case of a probability hit, we could potentially set an edge 2 times.
    This doesn't corrupt the correctnes of the probability
    and isn't much additional work. It helps to keep the code clean.
    
    If you want to optimize this solution, you could run the loop
    such that you only visit the lower-left triangle (excluding the diagonal)
    and just mirror the results you get for those cells to the upper-right triangle.

Now, someone told me that without the optimization the algorithm is wrong since it calculates the propability of edge existence twice.  In their words:

>With the current code in the undirected case, the probability that matrix\[0\]\[1\] will be set is the probability that the random number in the iteration with col=0/row=1 or in the iteration with col=1/row=0 is smaller than p. That probability is 1-(1-p)\^2.

&#x200B;

At first I thought they were right, but thinking more about it it occured to me that since the random number generations are independent of each other and since we don't need 2 false (or 2 trues) to set or unset an edge, the probability calculation of the algorithm is correct. If their argument was correct, it would be like saying when you throw a 6 on a 6 sided dice, your chances of getting a 6 next time are less than 1/6. In other words, it looks like the [Gambler's fallacy](https://en.wikipedia.org/wiki/Gambler's_fallacy) on their part.

&#x200B;

I hope I've managed to convey the problem and arguments in full. If not, please ask.

What I'm looking for is feedback on which argument is correct. Thank you very much in advance.",1642436886.0
s5r8t0,Where do you get your latest news and tutorials when you just want to browse what's new on computer science?,,16,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/s5r8t0/where_do_you_get_your_latest_news_and_tutorials/,5,"I am interested in browsing new and exciting areas on computer science like programming, diy instructions on software, and reading informative articles on software or blogs. 

What are some sites every new techie should be on to read such material?",1642382740.0
s5i47n,stupid question,Discussion,42,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/s5i47n/stupid_question/,11,I watched a video on how computers work and how it uses binary instead of base 10. But if computers had a way to use base 10 or like distinguish 10 different types of charges in a transistor or something wouldn‚Äôt that computer be like ultra super fast???,1642357905.0
s5tfpw,Question about Neural Networks and Video Games,,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/s5tfpw/question_about_neural_networks_and_video_games/,7,"I understand that neural networks have been trained to play games like chess pretty effectively. I've also seen some neural networks able to generate photographs based on a text description.

But has anyone tried training an AI to be a video game? In my mind it seems feasible because you could theoretically automate the training by making a bot play an existing game like Doom Eternal and then treat the bot's commands as input and use the resultant game state as the output that the neural network needs to fit to. And then if your network learned the game correctly, a human could give it input with their controller and it would return a close approximation of whatever game state the original game would have given. And then you'd only need a computer powerful enough to run the NN instead of the full game.

I'm curious because I'm 100% certain someone smarter and more experienced than me has thought of this before but Google isn't giving me any good articles on the subject",1642389192.0
s57kne,Making Your Game Go Fast by Asking Windows Nicely,,41,0.87,computerscience,https://www.anthropicstudios.com/2022/01/13/asking-windows-nicely/,6,,1642322533.0
s5swwz,Aren't software and hardware (kind of) the same thing?,General,0,0.28,computerscience,https://www.reddit.com/r/computerscience/comments/s5swwz/arent_software_and_hardware_kind_of_the_same_thing/,4,"I'll try to clarify. A software (like an OS), to exist, needs a hardware (memory). All the software information is stored in the hardware, which means it is a physical thing (like circuits). So, if we define a hardware as ""the associated physical equipment of a computer, directly involved in the performance of data-processing or communications functions"", turns out software is hardware.

The difference is a practical approach, where software is a set of instructions (programs), and hardware is the PC parts (CPU, GPU...).

It is like your brain and your thoughts (technically, thoughts exist inside the brain)

I'm totally a layman on the subject, so don't judge me if I'm saying bullshit lol",1642387620.0
s4k68d,Any essential comp sci books that you wish you read earlier?,,91,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/s4k68d/any_essential_comp_sci_books_that_you_wish_you/,31,"I am a high school senior who has a lot of free time and loves studying comp sci. I have read an introductory Java book, SICP(chaps 1 and 2), and the C programming language or K&R. I am interested in continuing with chap 3 of SICP but I am also looking for another challenge: think comp sci essentials. All suggestions are appreciated.",1642252786.0
s4h4a4,Difference Between an Implementer of C and a C Programmer?,Help,46,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/s4h4a4/difference_between_an_implementer_of_c_and_a_c/,16,"(Brand new to CS so bear with me) I can't find anywhere online so far about what an implementer is and how that is different from a programmer. 

(My question comes from a C language book that talks about how part of the book is meant for programmers, not implementers.)

Thanks!",1642240851.0
s4frba,Are there any effective machine learning methods that aren't copied from nature?,,13,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/s4frba/are_there_any_effective_machine_learning_methods/,6,"Recently have been struck by the fact that the two most powerful machine learning methods, neural networks and genetic algorithms, are partly just copied from nature (in concept at least, obviously was a ton of work by a lot of brilliant people). I guess there is a lot of machine learning that is basically just memorization or math but are there any other deep learning algorithm paradigms out there? Do you think there could be someday?",1642235250.0
s3mbvt,Interesting Computer Science youtubers?,Discussion,116,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/s3mbvt/interesting_computer_science_youtubers/,48,"I have been wanting to find some good videos that I can watch in my free time that are about cool computer science projects so I can learn more about new algorithms, and programs in a more leisure way instead of solely doing projects and reading documentation. 

&#x200B;

I'm interested in most anything related to Python, Data science, or back end development, but I'd really love to learn more about Machine learning algorithms if there are any good series about people working on machine learning algorithms.",1642144705.0
s3wf1q,Nerves - An industrialization journey | Jean Parapaillon | ElixirConf EU 2021,,8,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/s3wf1q/nerves_an_industrialization_journey_jean/,0,"By integrating buildroot, BEAM and erlang/elixir ecosystem into a comprehensive toolkit, Nerves has become a major framework in the IoT field. Watch the video and see Jean Parapaillon  talking about ""Nerves - An industrialization journey""  

https://youtu.be/6MDNm7k2OoI",1642177798.0
s36y35,Novel view tennis from single camera input,,239,0.95,computerscience,https://v.redd.it/v6xlgqq17ib81,9,,1642100761.0
s2qf5f,This book demonstrates an infinite loop in a pretty cool way!,,1288,0.98,computerscience,https://i.redd.it/jx92aw75udb81.jpg,33,,1642047968.0
s3yrrt,How does Concorde solver work ?,,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/s3yrrt/how_does_concorde_solver_work/,5,"I'm looking for some resource clearly explaining how this solver for the tsp works.

I read that it uses the Lin-Kerighan heuristic and the branch and bound algorithm, but I didn't find a full explanation of how it works. The closest thing I read was [in the answers of this post](https://www.reddit.com/r/compsci/comments/8auwm9/how_does_concorde_claim_to_be_a_tsp_solver) , but it's not a full answer.

If you have the knowledge or any link/paper available online or on scihub that explains how it works in detail / in summary, that would be awesome",1642183919.0
s3gau6,What makes cyclic tag systems Turing-complete?,Discussion,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s3gau6/what_makes_cyclic_tag_systems_turingcomplete/,1,"I've been doing research on computational models, but I can't find anything on cyclic tag systems and their proof of universality. I want to know because it's supposedly what rule 110 can emulate (more specifically that's the proof that rule 110 is universal), however the fact that rule 110 can emulate a cyclic tag system means nothing to me if I can't find proof that cyclic tag systems are universal, let alone an implementation of rule 110 running a cyclic tag system. Does anyone know where I can find a cyclic tag system's proof of universality?",1642125936.0
s3241k,Confirmation on if I am understanding hardware/software interaction correctly,Help,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/s3241k/confirmation_on_if_i_am_understanding/,4,"Recently I have begun learning about how computers actually know to execute your programs, and I was hoping somebody could verify if I generally have the idea correct. 

- computers come preset with instructions on how to boot the OS located in the ROM. Once this takes place the OS controls the remainder of operations. 
- when you plug your computer into the wall, electrical signals are sent in clock cycles that run at some frequency all the time, either idle or active. These signals are always looking for data to transfer via the bus to read, but they are essentially always running (this is the part I'm shaky on I think) 
- when you write and compile a program say hello world, you have organized some high level code into what can be translated into machine code and binary. When you executed this is sent to the ram, where the clock frequency (always looking) activates logic gates based on what was sent, and tells you what the output it.
- if you wanted to use an embedded controller to run a motor the same applies where the programmer writes readable code which can be converted to binary, and is sent to the pinouts in such a way that causes some actuation to happen (such as alternate pulses to a stepper motor) 

Am I on the right track here ? 

Thanks 

Also, who sits around and actually designs computer architecture, I feel like that has to be one of the most complicated jobs on the planet. You've got millions of transistors to manage controls? Or is it more abstract than that.",1642087995.0
s33v2n,"If Interpreters are used to run code and are written in another language, won't interpreters need interpreters?",Help,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/s33v2n/if_interpreters_are_used_to_run_code_and_are/,5,"Sorry if my question sounds dumb but I am a beginner to coding and have started with Python. Python is an interpreter-based language and not a compiler based language ( I know the difference between the two) 

As I was going through my online lectures, it is stated that python is interpreted with Cpython, a software written in C. Similarly, they have Jython (written in Java) and PyPy (written in Python). My question is is these interpreters break down the language into low level code like Assembly and machine code, then who helps interpret the software of these interpreters so that they run in the computer? 

Again sorry for the dumb question",1642092742.0
s2ar5c,Just how powerful is an 8Bit CPU?,Discussion,50,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/s2ar5c/just_how_powerful_is_an_8bit_cpu/,16,"By ""8Bit"" i mean a CPU that uses registers that are 8 bits wide and with an ALU that takes two 8 bit binary numbers and performs operations on them, for the sake of this post, lets say the ALU is capable of bit shifting, NANDing and Adding.
So just how capable this CPU will be, Will it be able to run a very simple version of tetris? or pong?",1642006016.0
s1yfl6,Any ideas how hardware is physically-created? There are no comprehensible guides in the internet about the process.,,66,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/s1yfl6/any_ideas_how_hardware_is_physicallycreated_there/,30,"By how is created I do not refer to a general theory like what's CPU/GPU or motherboard, but as if how they are physically designed and produced, when I was getting my degree they explained to us the basics (been years since then) but never the exact process.

I also saw some youtube videos but they seem more of hype inducers and they seem to be divided into scientists wearing anti-dust suits and working on the nano-world and people doing mechanical tasks. I'm looking for a more pragmatic and practical procedure about how hardware is truly created, anyone has any leads?",1641965454.0
s2gdc0,Question: What does 'digital' mean?,,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s2gdc0/question_what_does_digital_mean/,5,"I've gone about my life for a while thinking that even 'digital' things have some physical correspondence. i.e. binary numbers are just surges of voltage and things of the such. My real concern for this idea turned up when I was learning about actuators and learned that to get it to receive some electrical charge it needed to pass through a DAC. Implying that the charge isn't a digital element and my correlation may have been wrong.

Are digital things not physical things? I'm pretty sure that's not true but I have absolutely no idea what to make of the idea now...",1642020128.0
s2nqom,Is there any overlap between computer science and foreign languages ?,,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/s2nqom/is_there_any_overlap_between_computer_science_and/,6,"hey guys, basically the question. and forgive it is a noob question. I'm relatively new to computer science but have really started taking a passion to programming. I also have a deep love for studying for foreign languages. 

Is there any obvious / not so obvious overlap between the two?",1642040102.0
s2ccci,A Guide to Combatting Human-Operated Ransomware: Part 1,,2,0.67,computerscience,https://www.microsoft.com/security/blog/2021/09/20/a-guide-to-combatting-human-operated-ransomware-part-1/,0,,1642009924.0
s28p5d,Love Your Crash Dumps | Micha≈Ç ≈ölaski | ElixirConf EU 2021,,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/s28p5d/love_your_crash_dumps_micha≈Ç_≈õlaski_elixirconf_eu/,0,"Livebook enabled a new level of interactivity with a running BEAM VM. Michal Slaski presented his talk ""Love Your Crash Dumps"" at #ElixirConfEU, describing the techniques for live analysis of VM performance.

Watch the video and learn more: [https://youtu.be/wyjWR731uSU](https://youtu.be/wyjWR731uSU)",1642000729.0
s1gfy8,What is Gradient Descent? A 2-minute visual guide. [OC],Discussion,36,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/s1gfy8/what_is_gradient_descent_a_2minute_visual_guide_oc/,9,"&#x200B;

https://preview.redd.it/qa90r07j13b81.png?width=2048&format=png&auto=webp&s=7373c6bf3bcfec41bf1794f47317b637438027f9

https://preview.redd.it/4w1nhr6j13b81.png?width=2048&format=png&auto=webp&s=770a39f232e15e0e44407d8d5ea3ffa2b504cb12

https://preview.redd.it/zi4hqe7j13b81.png?width=2048&format=png&auto=webp&s=0085316ff44aaed6d78c07dc3c92a06635690472

https://preview.redd.it/ahkxq67j13b81.png?width=2048&format=png&auto=webp&s=42a6545dd07b141e2e2a7ebe92e4e203909107c2

https://preview.redd.it/nxiiny7j13b81.png?width=2048&format=png&auto=webp&s=6e5d77c26e6de57b25a7408a0b5d88f94a2b8ac5

[EDIT: Thank you to u\/antiogu for pointing out the error. The y-intercept should be 2 in my sketch.](https://preview.redd.it/chwmmn3j13b81.png?width=2048&format=png&auto=webp&s=5a5cc5127a2fa7d8bf3811b26e64ec1a77e405eb)

üîµ Gradient descent üîµ

üíæ A more detailed post this time but I wanted to make sure I touch upon some basics first before diving into gradient descent itself. This is mainly so that it is more inclusive and no one feels left behind if they have missed what gradient is and if you already know what it is you get to brush up on the concept.

üèÉ Although a relatively simple optimization algorithm, gradient descent (and its variants) has found an irreplaceable place in the heart of machine learning. This is majorly due to the fact that it has shown itself to be quite handy when optimizing deep neural networks and other models. The models behind the latest advances in ML and computer vision are majorly optimized using gradient descent and its variants like Adam and gradient descent with momentum.

‚õ∞Ô∏è The gradient of a function is a vector that points to the direction of the steepest ascent. The length or the magnitude of this vector gives you the rate of this increase.

üî¶ Time for an analogy: it is nightfall and you are on top of a hill and want to get to the village down low in the valley. Fortunately, you have a trusty flashlight that helps you see the steepest direction locally around you despite the darkness. You take each step in the direction of the steepest descent using the flashlight and reach the village at the bottom fairly quickly.

üìê Gradient descent is an optimization algorithm that iteratively updates the parameters of a function. It uses 3 critical pieces of information: your current position (x\_i), the direction in which you want to step (gradient of f at x\_i), and the size of your step.

üßóThe gradient gives the direction of the steepest ascent but because we need to minimize we reverse the direction by multiplication with -1.

üéÆ This toy example illustrates how gradient descent works in practice. We compute the gradient of the function that needs to be optimized i.e. the differentiation of the function with respect to the parameters. This gradient gives us the information we need about the landscape of the function i.e. the steepest direction where we should move in order to minimize the function. A point to keep in mind: gamma the step size (also called the learning rate) is a hyperparameter.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641917310.0
s20uj1,Adjacency List Representation of a Graph: Time Complexity to get the list of all neighbours/adjacent vertices of a particular vertex,Help,0,0.5,computerscience,/r/AskComputerScience/comments/s20ofa/adjacency_list_representation_of_a_graph_time/,0,,1641974267.0
s1h1gt,Subredditd and youtube channels for staying up to date,Advice,13,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/s1h1gt/subredditd_and_youtube_channels_for_staying_up_to/,2,"Not sure if this is the right subreddit for this, but im looking for a few subredditd and youtube channels for staying up to date and just general knowledge of the software development industry. Can anyone help?",1641918826.0
s14xir,Favorite CS books?,,93,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/s14xir/favorite_cs_books/,49,"Basically title. I've been searching for some good CS books, but can't find much other than 1) the mainstream novels about ""AI"" that are of no use to someone who's studied CS and 2) the tutorial type books titled ""How to \[do something\] with \[insert popular technologies\]"". Looking for recs.",1641877819.0
s0ficm,CS themed graduation party!,,516,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/r89rlr/graduation_party/?utm_source=share&utm_medium=ios_app&utm_name=iossmf,32,,1641804617.0
s0vdjp,Hitting a wall between math and data structures,Help,6,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s0vdjp/hitting_a_wall_between_math_and_data_structures/,5,"Hello everyone,

I've been stuck in the classic problem of applying mathematics in the context of data science. However, I am much stronger in my math-side than in computer science. If you gave me a homework sheet of problems in stats, multivariable calc or linear, I'm happy. If you ask me to apply mathematics for machine learning, I'm drawing blanks.

Has anyone else struggled with this disconnect? If so, were there classes or concepts that helped build that bridge? Is this data structures with extra steps?

Thank you in advance",1641850841.0
s0ihgg,PIEEG: Turn a Raspberry Pi into a Brain-Computer-Interface to measure biosignals. arXiv:2201.02228,,12,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/s0ihgg/pieeg_turn_a_raspberry_pi_into_a/,0," This paper presents an inexpensive, high-precision, but at the same time, easy-to-maintain PIEEG board to convert a RaspberryPI to a Brain-computer interface. This shield allows measuring and processing eight real-time EEG(Electroencephalography) signals. We used the most popular programming languages-C, C++ and Python to read the signals, recorded by the device. The process of reading EEG signals was demonstrated as completely and clearly as possible. This device can be easily used for machine learning enthusiasts to create projects for controlling robots and mechanical limbs using the power of thought. We will post use cases on GitHub (https://github.com/Ildaron/EEGwithRaspberryPI) for controlling a robotic machine, unmanned aerial vehicle, and more just using the power of thought.",1641816059.0
s0jzi8,What are 1s and 0s in computers?,Help,5,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/s0jzi8/what_are_1s_and_0s_in_computers/,21,"I have not learnt much electronics but I do kinda know how the CPU works and how logic gates and all the higher level more abstract things work in a CPU.

I‚Äòm really confused by this, some say 1 and 0 is the state of the transistor but that doesn‚Äôt make sense to me. I‚Äòve also read that it‚Äôs a high vs low voltage, but than isn‚Äôt voltage the electric tension between two points, which points would that be? I have always imagined the 1s and 0s or asserted and deasserted signals as high and low amperage, is that wrong?",1641821166.0
s01ijj,A.I. Debate topics,Help,34,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/s01ijj/ai_debate_topics/,29,"Hello! I'm a high school computer science teacher, and teach a course on computer ethics. One of my units is on A.I. and I want to conclude the unit with student debates on topics in AI. I'm struggling to come up with topic statements however. I know for sure I want one of the topics to be centered on whether A.I. at an advanced level should be afforded the same rights as humans.  


Any other topic statement ideas? Thanks!",1641762273.0
rzqau2,Meaning/purpose of computer science,,60,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rzqau2/meaningpurpose_of_computer_science/,52,"I am an undergrad studying computer science. While I enjoy coding (it is ludic) I can‚Äôt quite grasp its purpose/use in the bigger meaning of life.

Why do you study computer science and how do you find purpose in it?",1641729944.0
rzcexe,Can anyone explain NFTs from computer science perspective without the hype?,,197,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/rzcexe/can_anyone_explain_nfts_from_computer_science/,119,"I searched and searched and still haven't been able to really figure out how NFTs are working behind the scenes. There is sooo much hype. 

Could a developer please explain. I'd like to know - 

1. How are they created? Using which popular frameworks (any code snippets appreciated)
2. Where is the media hosted? My understanding is that NFTs are really a record that your wallet owns this unique hash. But then how is that has linked to an image? And where is this image hosted? Follow up - can't anyone take this image host down and then that link between the hash and the image is lost.

Appreciate any elightenment without the hype. I am tired of 100th video about an influencer explaining NFTs as if they are the next gold with no context.",1641682303.0
s011f6,Why did it take so long for consumer OS's to get UNIX-like features?,Discussion,3,0.62,computerscience,https://www.reddit.com/r/computerscience/comments/s011f6/why_did_it_take_so_long_for_consumer_oss_to_get/,9,"Forgive any and all ignorance as I ask this question please. As I understand it, Linux was the first open source UNIX-like OS, correct? And UNIX itself has been around since 1969!

My question is, if the robust stability and security features like protected memory, preemptive multitasking, secure user accounts/admin stuff and all that have existed for so long... why did Mac OS and Windows both lack any of that robustness until a full decade after Linux was created?

In other words, if we had a way to make an OS that would virtually never crash and had orderly memory management and security and the rest, why did Apple and Microsoft both ship such fragile OS's that could be toppled by a single rogue app and were generally terrible at security and for *SO* long?

Apple even had [it's own UNIX implementation](https://en.wikipedia.org/wiki/A/UX) for the early Mac computers complete with the System 7 GUI laid over top. Anyone care to speculate why THAT wasn't just the Mac OS everyone got? Microsoft also [licensed a UNIX version](https://en.wikipedia.org/wiki/Xenix) from AT&T in the early days. So it's not as if no one knew computers didn't have to suck. Why did it take so long to reach the average PC user?   Thoughts?",1641761027.0
rzc739,Best Way to Learn Programming theory Summarized,Help,40,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/rzc739/best_way_to_learn_programming_theory_summarized/,23,"I am new to coding. I have a math background (up to Diff EQ and LA). I want to learn the basic idea behind all programming languages. The idea of Syntax, how things operate, the different types of programs. Just a review of how programming works.

I know nothing. I need to understand it for my physics major. I learn best by theory and having a foundation of the most basic principles and abstract terms (nothing specific or applied at first).

Do any of you know any good websites or videos that describe the idea of programming? (I would prefer relation to math terms like ‚Äúfunction‚Äù or ‚Äúlinear‚Äù or ‚Äútransformation‚Äù or a set of axioms or system that can be diagramed).

I want to understand the process  and the terms before learning code.",1641681729.0
ryzj9x,Where is database research going these days?,Help,47,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/ryzj9x/where_is_database_research_going_these_days/,10,"Hi!
I'm a data analyst intern who loves math and databases and I'm pretty sure that in the future I want to do something that uses both of these things.
For that reason, I've become very curious about where database research is going these days. I don't have much interest in software development per se but databases are just such mathematical marvels that I can't help but lean towards them.

So, what is currently being researched? User centered design?( i love this and it's very overlooked by companies)
New database paradigms? Graph databases? Etc...?

So what else is there?",1641647438.0
ryeny8,"Does the rise of no code, low code and AI coding tools, like Codex and Copilot, threaten developer jobs?",Advice,129,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/ryeny8/does_the_rise_of_no_code_low_code_and_ai_coding/,129,"A career counsellor said that I should teach math (my other possible career goal) rather than go into software development, since the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years. There is so much hype about this that I thought I'd ask the opinions of those here that know what they're talking about.

&#x200B;

Thank you",1641581316.0
rxuuqm,what would be the most efficient way to have a computer guess a number?,,16,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/rxuuqm/what_would_be_the_most_efficient_way_to_have_a/,25,"say you picked a really large number at random and wanted to find it by repeated guessing, what would be the most efficient way to do that? 

i've been trying (really just out of curiosity) this and so far the fastest outcomes are usually from just incrementing the number one by one until it gets it but i don't feel like that's the most efficient solution. what would be a better way?",1641518891.0
rxjst3,Parametric vs. non-parametric statistics. A 2-minute visual guide. [OC],Discussion,63,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/rxjst3/parametric_vs_nonparametric_statistics_a_2minute/,1,"&#x200B;

https://preview.redd.it/hqkf9w37q3a81.png?width=2048&format=png&auto=webp&s=f11e0afc13e872afd68e6150ed4972853a90c7b8

https://preview.redd.it/t7pbrm37q3a81.png?width=2048&format=png&auto=webp&s=a9895b8c904ce1802376adde7a79bf719c456499

https://preview.redd.it/l4tl8w37q3a81.png?width=2048&format=png&auto=webp&s=4f5c191f19c7b6fdd0a4750c3e6b7d63cfafde8e

https://preview.redd.it/ofi01147q3a81.png?width=2048&format=png&auto=webp&s=daaf40e3fd1a22c2b0cd661944faa3e3f5f7172e

üîµ Parametric and non-parametric statistics üîµ  


üßë‚Äçüíª If you have written code before you have heard of a parameter. It is what a function takes as input to do some computation on and return an output. Similarly, probability distributions have parameters. These define the properties of the distribution.  


üîî In the case of a normal distribution, the mean and standard deviation are its parameters. The mean controls the position of the distribution while the standard deviation controls the spread or ""peakiness"" of the bell curve.  


‚õ∑Ô∏è In the parametric approach, the model consists of a finite set of parameters that characterize it. The big assumption that a parametric model makes is that the model will do well on the task if the underlying distribution from which your data is sampled matches the model. If that is not the case the model and data mismatch will hurt the performance on the task you care about.  


ü§∏ A non-parametric model does not rely on parametric assumptions and is generally more flexible. It is a good choice if you don't have any prior knowledge about what could be a good model that reflects your data distribution. A histogram is a good example that can model points sampled from an arbitrary distribution.  


üßê However, there is no free lunch: you will need enough data points to get a good approximation otherwise the histogram will look nothing like the underlying distribution.  


\---------------------------------------------------------------------------------  


If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641489774.0
rxn4al,Why is the castle clock a computer?,Help,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rxn4al/why_is_the_castle_clock_a_computer/,3,"The castle clock of al-Jazari is said to be the first programmable analog computer. Now I get why it‚Äôs analog and also what makes it programmable (accounting for the length of day and night), but what makes it a computer? It gives the time, does that make all clocks computers?",1641498290.0
rx1mtm,Public key encription,,32,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/rx1mtm/public_key_encription/,30,"I have been trying to wrap my head around it, but i dont get how the publick and private key are combined.

Like, if i only i know the key, and i use it, the receiver wont be able to decript it, or will it?

And if we both have it, it means it was shared before being aplied, and i sent it to the receiver through simetric enciption, thus defeating the purpose of it existing since everyone with the public key (wich i am assuming is public) can decript the message and find what the private key is?...

What am i missing here? I cant seem to find any explanations that made me understand it, if you know any good explanations plz share!

Edit: The problem is that i thought that the public key was  some preexisting key, when in fact it is generated by me at the same time I generate the private one, I share the public one, te receiver uses it to encrypt stuff, and only the private key can decrypt what was encrypted with the public key, and, it is very hard (close to impossible rn) to get the private key by looking at the public one!",1641429660.0
rx5u9h,What is the maximum # of guesses an reasonable Minesweeper player needs to take?,Discussion,10,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/rx5u9h/what_is_the_maximum_of_guesses_an_reasonable/,4,"Not homework, just math.

Suppose there is an arrangement of X mines on an M x N grid. A player enters a sequence of clicks, which each ""flood fill"" outwards until adjacent mines are hit. They make logical deductions/inferences whenever possible, and if they need to guess, they will take any guess that is 50-50 or better. They don't have any intuition - i.e. they won't visually analyze the grid to see if one safe click is ""better"" than another.

What is the upper bound for the number of guesses such a player may need to take?",1641444320.0
rwly7h,Books that Changed My Career as a Software Engineer,,145,0.96,computerscience,https://julianogtz.github.io/my-personal-blog/posts/five-books-that-changed-my-career-as-a-software-engineer,4,,1641386187.0
rwnjr8,What is k-means clustering? A 2-minute visual guide. [OC],Discussion,54,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rwnjr8/what_is_kmeans_clustering_a_2minute_visual_guide/,1,"&#x200B;

https://preview.redd.it/l885cgy6lv981.png?width=2048&format=png&auto=webp&s=67b8b1f40e1ec3a919810758aa8d606dd8b639f9

https://preview.redd.it/mpr2ufy6lv981.png?width=2048&format=png&auto=webp&s=3b5e1f964c06ab3ff7a8de9f66c94073c093c0d1

https://preview.redd.it/grnekly6lv981.png?width=2048&format=png&auto=webp&s=ce49caa99da92e45c02e4325fe138501d66af9ba

üîµ k-means Clustering üîµ

üî± The k-means algorithm divides N data points into K disjoint clusters. It is a well-known unsupervised learning model. This means that the k-means algorithm only requires the data points and does not need the corresponding cluster that each point belongs to as this is what the algorithm figures out.

‚ú® The clusters are found by allocating points in such a way that the total variance of the points within each cluster (intra-cluster variance) is reduced or minimized. Although it iterates quite fast, the k-means algorithm can have varying cluster formations based on the initialization. It has been widely implemented in many software packages. The scikit-learn package for python is the one that I have used most often.

üîÅ The most common method to perform the clustering is iterative. It alternates between two steps: (1) assigning each point to a cluster based on the point's closeness (or distance) to the cluster center and (2) updating the cluster center (called mean or centroid; hence the name k-means one for each of the k clusters) based on all the points that belong to this particular cluster. The iterations are usually performed until the centroids stabilize (converge) between consecutive iterations.

ü§ì K-means is popular in scenarios where the data is known to consist of multiple groups (distributions) but it is unknown which point belongs to which group (cluster). It can be used for data analysis and splitting data that comes from multiple distributions, image segmentation, color quantization among other things.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641391224.0
rwv1vj,"How to aim towards publishing theoretical computer science papers regarding computer/IT systems for space travel, exploration, and survival?",Advice,14,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rwv1vj/how_to_aim_towards_publishing_theoretical/,3,"Hello redditors, I humbly ask for your assistance...

Here's the elaboration: I'm a computer science undergraduate with a relatively decent amount of pre-existing background knowledge in computer systems and some programming, and I have an interest and some supplemental education in astronomy as well as a semi-professional observatory which I use actively. In my future academic career (I intend on pursuing a M.Sc. and Ph.D.) I'd like to focus my research and studies around theoretical systems which could help humans and machines travel, explore, and survive in space. 

The question is... What would actually be the best way to start tackling this in order to one day be worthy and knowledgeable enough to pursue this? There is no subject such as ""astrotechnology"" so I am a bit at a loss. Also, how do ""theoretical"" papers consisting more of science/evidence-backed ideas (rather than experimental results) even get accepted and published by journals???

I would be grateful for any pointers. <3  
P.S. - Not sure if it matters, but I doing a degree in health science alongside computer science. Also - due to where I live and financial constraints - my resources are limited; I have only my mind, my computer, and the internet.",1641411326.0
rx5nj6,Turing Machine Exercises,,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rx5nj6/turing_machine_exercises/,2,"**Brief: looking for more ideas on exercises for TM - share please if you had some in your CS course or seen somewhere.**

Always been fascinated by Turing Machine concept, decided to collect some puzzles to be solved with it on my site. However after adding first three ([these](https://www.codeabbey.com/index/task_list/turing)) feel lack of good ideas:

1st and 2nd are about making copy of a line of characters and incrementing binary value. There could be combinations and variations (copying line of different characters and incrementing ternary, octal, decimal value... or count characters in a line making result a binary value) - but after grasping principle these do not make additional fun (unless I'm missing some whimsical approach).

3rd is about implementing [Rule 30](https://mathworld.wolfram.com/Rule30.html) one-dimensional cellular automaton with TM, I'm somewhat glad with it (since I managed to solve it).

Have in mind something like checking bracket balance or wiping out comments in C++ / Python source. However these seem to be more about general sense of state automaton rather specifically TM...

So would be really glad for any suggestions! Perhaps you have come across something in university or played with TM for personal fun etc... thanks in advance!",1641443710.0
rx4z0z,Is there a name for 256 bytes?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rx4z0z/is_there_a_name_for_256_bytes/,7,"Im wondering if there is a name for 256 bytes, like how ~1000 bytes is a kilobyte, or ~1000000 bytes is a gigabyte?",1641441709.0
rvy218,What is an eigenvector? A 2-minute visual guide. [OC],Discussion,195,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/rvy218/what_is_an_eigenvector_a_2minute_visual_guide_oc/,19,"&#x200B;

https://preview.redd.it/t23amo984p981.png?width=2048&format=png&auto=webp&s=564718c960ea7754ef20081d0374b58902605668

https://preview.redd.it/mcvd1q984p981.png?width=2048&format=png&auto=webp&s=5ef0fcb88fc8af53cd6ba6d21e1789ee8b6cccec

https://preview.redd.it/2pohyo984p981.png?width=2048&format=png&auto=webp&s=cd07e5c504eb15e5e07642c51e0f20c5401433d1

https://preview.redd.it/fn2yxo984p981.png?width=2048&format=png&auto=webp&s=4a263659688b553b9463d5a6df32fc8a9d7220e1

üîµ Eigenvectors üîµ

üöÄ An eigenvector is a special vector associated with a linear transform. It's special in the sense that after applying the said transform it does not change direction but only gets scaled (multiplied by a scalar value) by the eigenvalue.

üî® Each eigenvector comes with a corresponding scalar called the eigenvalue. Breaking a matrix M into its eigenvalues and corresponding eigenvectors is called eigendecomposition.

üî≠ The word ""Eigenvector"" comes from ""eigen"" in German where it means ""its own"". It was originally used to study rigid body motion and in the discovery of principal axes. However, nowadays it has found its way into a wide array of applications from my favorite: principal component analysis, differential equations, and problems in physics and chemistry relating to wave transport and molecular orbitals.

üé≠ Another one of the classical applications is the Eigenfaces project for facial recognition. Eigenfaces decompose a face as a composition of face templates (basis) called eigenfaces. Imagine N eigenfaces E\_1, ..., E\_n when given a new face F it can be written as a composition of each of these N eigenfaces for example: F = (10% of E\_1 + 55% of E\_2 + 35% of E\_3). Each eigenface would represent a meaningful feature in the high-dimensional space of faces.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641312934.0
rwgzqx,"Is there a ""synthetic"" Universal Turing Machine? Where can I find it's rule set?",,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rwgzqx/is_there_a_synthetic_universal_turing_machine/,0,"I've been on the lookout for a UTM description, and I've found a machine with only 2 states and 3 symbols. I thought, how neat, it's so simple! I put the description into a TM simulator, realized I had no idea how to use the rule set, and took to the internet again to figure out why. Turns out, it simulates a 1D CA, which I know to be terribly time and space inefficient, and very hard to understand. It makes me wonder if there's a larger, synthetic ruleset, one with more internal states but only 3 symbols (1, 0, and empty symbol), one that can reasonably be experimented with, one that can be programmed more easily?",1641367089.0
rw99z6,What is the proof that a 2-state Turing Machine with a 3-state alphabet can simulate any arbitrary Turing Machine?,Discussion,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/rw99z6/what_is_the_proof_that_a_2state_turing_machine/,1,"This is a follow-up to my question of how many states and symbols a TM would need to simulate a TM with an arbitrarily large number of states and symbols, that was answered for me, the answer is 2 states and 3 symbols. But I can't find a proof for this, or even better, an example. Also, is there a proof that a 2-state 3-letter TM is the simplest universal TM? Could a 2-state 2-symbol simulate bigger TM's, or could a 1-state 2-symbol TM work?

Edit: I found a [source](https://mathworld.wolfram.com/UniversalTuringMachine.html) for the description of the (2,3) UTM, as well as some larger UTM's. However, the description of the Turing Machine does not prove anything to me, because it's only half the story. I'd also need to know what the tape looks like while it operates to gather anything.

E2: I've now learned that the (2,3) TM is universal because it simulates a specific 1D CA that has already been proven to be universal, however this pushed the question out for me, so I looked up a proof that 1D CA are universal, and I found that it can simulate some other thing that's been proven to be universal, and soon enough the question was put back onto the UTM and why it is universal. Either I'm stupid or the internet is.",1641342841.0
rvq0w4,Have you ever used what you learned in chemistry classes in your computer science career?,,26,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/rvq0w4/have_you_ever_used_what_you_learned_in_chemistry/,31,?,1641286813.0
rw2dl6,What's the difference between a Python ID and a pointer in C++?,Help,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rw2dl6/whats_the_difference_between_a_python_id_and_a/,3,"My current understanding is that Python does not support pointers because it is against the ""Zen of Python"" but it does have id(my\_obj), which returns ""the object's address in memory.""  How is that not a pointer and what is the difference between the Python id() function and a pointer in C++?  Are both not just an integer representing the object's location in memory?",1641324167.0
rvfquw,Why did the Microsoft date bug happen in 2022?,,66,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/rvfquw/why_did_the_microsoft_date_bug_happen_in_2022/,12,"So Apparently Microsoft was trying to store date in a 32 bit signed integer. 

But the max value of that is 2147483647, so why can‚Äôt January 1 2022 be stored?",1641253825.0
rvc4y3,Minecraft to teach software architecture?,Discussion,48,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rvc4y3/minecraft_to_teach_software_architecture/,17,"Hey guys I just thought about teaching Classes, Inheritence, Dependency Injection etc. (you get my point) with Minecraft. Nearly everyone knows the game and a block serves as a great example for a class. It has specific attributes like texture etc. and there are lots of subclasses. 

Additionally you could discuss how those blocks could be stored, which sort of database would be suitable etc. 

What do you think about this?",1641244232.0
rw311s,Memory and pointers: where does the actual memory address come from?,Help,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/rw311s/memory_and_pointers_where_does_the_actual_memory/,4,"When, say, in C++, we say `void* ptr = &my_var`, we set ptr equal to the memory location of my\_var.  Okay.  But where does this memory location number come from, and why/how do we actually access it?  

Say we have our memory as 5000 bytes of data, in 1D.  My understanding is that each byte in memory has its own address, but I don't understand the mechanism for how.   I understand that it's like a ""house address"" and we need it in order to find a value stored at a given location.  But at the most basic level, where are these addresses coming from? The hardware? Or is this just something we are adding to that byte of memory (sort of like metadata in Numpy arrays)? If that's the case, then does this not take up some of the memory? 

E.g. we have a memory location of 13413241 let's say.  Where is this value stored? I would assume that all the memory locations, all these integers, are inherent to the computer we are working on, and thus always exist.  Is this entirely separate from ""memory"" in terms of how big our program is (if they always exist anyways)?  It just seems like we can pull these memory addresses out of thin air.  If I define say `x = 5`, is it always the case that I have also implicitly defined `some_pointer = &x`, and is this now in my program's ""memory""?   Unless the memory location is not ""bundled"" to that 5, I don't see how the pointer ""knows"" the memory location.  Maybe all of this is happening at the hardware level and I'm too focused on the software level.  I haven't found any articles that explain what I'm looking for, only what a pointer is.  Maybe this is more of a question about what the `*` and `&` operators are actually doing.

It might be evident already, but I almost exclusively use Python.  In Python, we can define a variable, which will always be equal to some object, and then we can set that variable equal to other things, namely we are just telling that variable to now ""point"" at a new object: but the previous object still exists in memory, it just isn't referenced.  However, my understanding is that Python doesn't actually have pointers, so my understanding may be tainted pretty heavily by the Python id() function which returns the location in memory, and also by how variables and objects interact in Python.",1641325885.0
rvrtx4,"The frequency that we see in the CPU, is the frequency at which the single bits are read by the CPU?",,1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/rvrtx4/the_frequency_that_we_see_in_the_cpu_is_the/,4,,1641294168.0
rutayn,How does a quantum computer reduce time complexity?,,113,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rutayn/how_does_a_quantum_computer_reduce_time_complexity/,21,"When I studied algorithm, people talked about time complexity and bigO notation. What I want to know is that if given an algorithm whose running time is O(n\^2), if I run the algorithm on a quantum computer, does it necessarily reduce the running time to O(n), that is, the sqare root of n\^2?

And if given an algorithm whose running time is O(n\^3), does a quantum computer reduce it to O(n\^(3/2)), that is, the square root of n\^3 ?",1641185362.0
ruum67,What Every Programmer Should Know About Memory by Ulrich Drepper[PDF],,11,0.93,computerscience,https://akkadia.org/drepper/cpumemory.pdf,5,,1641189641.0
rushx3,How is memory aligned data good for caching?,,16,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/rushx3/how_is_memory_aligned_data_good_for_caching/,12,"I read some articles about memory alignment and while I was reading about the advantages of memory alignment I read about caching. It was not very clearly explained that how memory alignment helps in caching. I would be delighted if someone could explain this to me.

This [Stack Overflow answer](https://stackoverflow.com/a/381368/14951775) mentions that memory alignment is helpful for caching. I know that aligned memory makes the fetching of the data faster because the CPU does not need to shift the bits in two reads and then combine the results in the register. I got to know about this from this [IBM Developer article](https://developer.ibm.com/articles/pa-dalign/).

Thanks!",1641182808.0
rue0sq,"What does it mean for the CPU to ""decode a command""? I mean i get the general idea but how exactly it works?",,48,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rue0sq/what_does_it_mean_for_the_cpu_to_decode_a_command/,20,"Pretty much title, i know that CPU fetches a unit from memory and then it decodes it, but i dont know what exactly that means or how it works",1641143248.0
ruwflx,Question about installation software,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/ruwflx/question_about_installation_software/,8,"Bit of background: I am a Computer Science student and a software developer, have been programming for several years now.

I have been wondering about this for a while now, but how come the Installer of a software is sometimes much much lighter than the software itself? As I understand it, the concept of an installer generally is to compress the necessary files for a software, and then distribute the files in the PC + configure whatever settings required to operate the software (add registry records etc.).

But sometimes the weight difference between the installer and the software is really big, like several GBs big, and for games it's sometimes up to a couple dozen GBs even when the installer is an offline one (as in does not have to download extra data in order to finish).

I don't see how these variations can be achieved using conventional compression, especially when the compressed files are not simple textual data.

Am I missing some key point in how installers work? Is it really just simple compression and decompression? Am I just exaggerating the weight differences? (I really don't have a concrete example of an installer that behaves that way at the moment, but if memory serves me right that was the case with many of them)",1641196518.0
rubju8,Resources for time complexities of common data structures across multiple programming languages,,12,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/rubju8/resources_for_time_complexities_of_common_data/,11,"In **JS**, array.push() is always O(1) since even if need to 'extend' the current array as the reference to the last item is simply removed.

In **Java**, there's no array.push() and O(n) as need to shift all the elements to a the new array.

Is there any resources available that show different time complexities for different programming languages?   


Happy 2022!",1641136358.0
rtu2da,Why is time stored as a signed integer?,,72,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rtu2da/why_is_time_stored_as_a_signed_integer/,18,"Someone was talking about the y2k bug, whrich reminded me of the 2038 bug and I pulled up the Wikipedia page to read up on it again.  I was under the impression that it would flip down to all 0s and move us back to 12/31/69 but I found out I was wrong and that it will just flip the sign when that happens and move us back to 12/13/1901.

But why was time stored as a signed integer anyway?  Was there an expected need to work with negative time?  If it was unsigned wouldn't that buy us a few more decades to procrastinate before needing to fix it?",1641075398.0
ru5qo2,How many internal states are needed in a Turing Machine?,Discussion,10,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/ru5qo2/how_many_internal_states_are_needed_in_a_turing/,12,"I haven't personally experimented with a Turing Machine implementation myself - mainly because building my own implementations makes learning about the concept easier and more fun - but I know enough to know that TM's can simulate TM's, even if at a slower and smaller scale, which makes me wonder.


If a Turing program exists that simulates the entirety of a Turing machine inside a Turing machine, how many internal states would it need to run such a program? Would this number vary depending on the size of the tape's alphabet, or the number of states the simulated Turing Machine can be in?

EDIT: I found the answer, though I don't yet understand it. a 2-state, 3-letter Turing Machine can do this. I'm searching the internet for proof that that's true.",1641113742.0
ru3c37,Computing Knowledge Itself?,,8,0.64,computerscience,https://www.reddit.com/r/computerscience/comments/ru3c37/computing_knowledge_itself/,18,"So before I start I literally had the idea 5 minutes ago and I hope to explore it more thoroughly with you guys by finding possible problems and possible solutions. So I encourage you to have a think before you comment so we all get the maximum benefit. Let‚Äòs get started!

So knowledge is something that people love. Rightly so! Knowledge helps solve all sorts of problems. In fact, it is a very valuable resource in any given arena. However knowledge also has its own statistical topology. There‚Äôs ways in which you can get to the next theory quicker and more fundamentally. So why don‚Äôt we try and teach the AI this!?

We define a computable theoretical topology with a small amount of variables. Mathematics itself would be preferable because it is relatively data minimal. Then we define the axioms of our current understanding of mathematics. Then we let the AI invent knew theories and check it against our current ones. Where am I dreaming?",1641103921.0
ru6jx7,I would like to learn how a database is encrypted.,,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ru6jx7/i_would_like_to_learn_how_a_database_is_encrypted/,1,"Hello, I understand a database containing passwords should be hashed using some sort of key, but I understand that each entry should be encrypted again in some other way. What I can't understand is how that works. I'd love a few pointers to some resources if anyone knows of any. Thanks :).",1641117296.0
rubwjz,How does computers with different systems communicate ?,Help,1,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/rubwjz/how_does_computers_with_different_systems/,5,"how can computers with different systems that have different data type representation communicate properly? what's the concept behind it, any links or articles would be appreciated.",1641137406.0
rtkdm7,Mind reading algorithms,Discussion,66,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rtkdm7/mind_reading_algorithms/,25,"Good day everyone

Recovering sz here, on meds and in therapy

Iv been researching the incidences you hear about where, social media,google or the internet seems to read peoples minds. I think its quite common and alarming.

You often hear folk say they don't mention aloud what their thinking let alone have it in their search history or have any record of it, they simply thought it and received an ad or a search suggestion.

My question is this ;

How is this possible? Iv heard its a predictive algorithm but how does it do it exactly? Is it really an algorithm if its right time after time

Iv heard of confirmation bias as well but i don't believe thats what this is


Any help appreciated

Have a good day",1641047052.0
rtv1bn,Are there other machines exactly as powerful as the Turing Machine?,Discussion,9,0.74,computerscience,https://www.reddit.com/r/computerscience/comments/rtv1bn/are_there_other_machines_exactly_as_powerful_as/,10,"I'm playing around with manually operated mechanical computers (think the abacus, crank calculator, etc.) and I'm wondering if there's other deterministic architectures exactly as powerful as the Turing Machine?

&#x200B;

Edit: I've done some hours of research and discovered an online copy of Stephen Wolfram's ""[A New Kind of Science](https://www.wolframscience.com/nks/)"" that includes some mechanisms that match closely what I was looking for, such as [tag systems](https://www.wolframscience.com/nks/p93--tag-systems/), [register machines](https://www.wolframscience.com/nks/p97--register-machines/), and [substitution systems](https://www.wolframscience.com/nks/p82--substitution-systems/). I don't know if any one of them are Turing complete yet, but something tells me at least some of them are.",1641078155.0
rstlls,Why is RAM called random?,Discussion,173,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rstlls/why_is_ram_called_random/,33,"Good day!

I've been wondering, what's so random about memory?",1640956234.0
rsjxaz,Are there any real-time trackers of Moore's Law?,,23,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rsjxaz/are_there_any_realtime_trackers_of_moores_law/,6,"I'm thinking some sort of website that automatically plots released processors vs their number of transistors or transistors per dollar against time.

It feels obvious enough that either someone's already done it, or it wouldn't work, but all I've been able to find is a [static graph on the Wikipedia page](https://en.wikipedia.org/wiki/Moore%27s_law#/media/File:Moore's_Law_Transistor_Count_1970-2020.png).",1640920271.0
rs9uqq,"This may be a dumb question, but how would one scientifically develop AI?",Discussion,73,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/rs9uqq/this_may_be_a_dumb_question_but_how_would_one/,16,"I‚Äôve been following a YouTube channel called Two Minute Papers for some time now and it‚Äôs awesome stuff. I‚Äôm a computer science major currently in college (senior) and I was wondering how I could get started or have some fun learning and training my own algorithm based AIs?

Also, I have done research online with regards to this but I have not found what I‚Äôm looking for. My real question is, how do AI ‚Äúscientists‚Äù get their start? Are they all geniuses that are self taught or is there actually a major/education route you can take to become one?",1640892280.0
rslk6w,"I know I'm toeing the line between science fiction and reality, but what is the current state of artificial general intelligence? Is there any literature on the subject worth reading?",Discussion,6,0.65,computerscience,https://www.reddit.com/r/computerscience/comments/rslk6w/i_know_im_toeing_the_line_between_science_fiction/,3,,1640925537.0
rs0z7i,What is the BEST way to find which agents are the best with minimum games between them (genetic algorithm),,12,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rs0z7i/what_is_the_best_way_to_find_which_agents_are_the/,9,"Hi, I am trying to make a genetic algorithm to play chess. My problem is with finding the fitness value for each model. Currently, I just pair each agent with a random agent and the winner adds to his score while the loser subtracts from his score. But I find it not very effective because while having a generation of 10 I only play 10 games and it's not enough to evaluate which agents are best. Moreover, I can't match every agent with every other agent since it will take fo ages to complete training. 

What do you think is the best way to know which agents are the best with minimum games between them?",1640868325.0
rr75ns,It would be really interesting to research nature's sorting algorithms to see if there's one better than the ones we've found so far. Does anyone know of any research like that? Also I guess this is Crab insertion sort haha,Discussion,631,0.96,computerscience,https://i.redd.it/gdzbgny17h881.jpg,28,,1640781086.0
rrvks1,Solving Logic Puzzles Using Computers and Brute Force,,18,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rrvks1/solving_logic_puzzles_using_computers_and_brute/,5,"I came across the following logic problem:

&#x200B;

&#x200B;

https://preview.redd.it/hsy6iegqrm881.png?width=1344&format=png&auto=webp&s=583fcf8d3514d80d7abaae1e266f5f704328a990

In this problem, you are required to match the real names of basketball players to their nicknames, and sort the basketball players by their heights. Normally, this problem would require you to manually enumerate different combinations of names-nicknames and names-heights, until there are no contradictions according to the conditions below.

&#x200B;

I was wondering if these kinds of problems can be solved by brute force using programming languages such as R.

&#x200B;

For example, the code below lists every possible combination of basketball players by height:

    
    my_list = c(""Bill"", ""Ernie"", ""Oscar"", ""Sammy"", ""Tony"")
    
    d = permn(my_list)
    
    all_combinations  = as.data.frame(matrix(unlist(d), ncol = 120)) |>
      setNames(paste0(""col"", 1:120))
    
    
    data_frame_version = data.frame(matrix(unlist(d), ncol = length(d))
    
    matrix_version = matrix(unlist(d), ncol = length(d)) 
    
    #first 20 rows of matrix version:
    
         [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]    [,10]   [,11]   [,12]   [,13]   [,14]   [,15]   [,16]   [,17]   [,18]   [,19]  
    [1,] ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Tony""  ""Tony""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Tony""  ""Tony""  ""Sammy"" ""Sammy"" ""Sammy""
    [2,] ""Ernie"" ""Ernie"" ""Ernie"" ""Tony""  ""Bill""  ""Bill""  ""Tony""  ""Ernie"" ""Ernie"" ""Ernie"" ""Sammy"" ""Sammy"" ""Sammy"" ""Tony""  ""Bill""  ""Sammy"" ""Tony""  ""Bill""  ""Bill"" 
    [3,] ""Oscar"" ""Oscar"" ""Tony""  ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Tony""  ""Sammy"" ""Sammy"" ""Ernie"" ""Ernie"" ""Tony""  ""Sammy"" ""Sammy"" ""Bill""  ""Bill""  ""Tony""  ""Ernie""
    [4,] ""Sammy"" ""Tony""  ""Oscar"" ""Oscar"" ""Oscar"" ""Sammy"" ""Sammy"" ""Sammy"" ""Tony""  ""Oscar"" ""Oscar"" ""Tony""  ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Tony"" 
    [5,] ""Tony""  ""Sammy"" ""Sammy"" ""Sammy"" ""Sammy"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Tony""  ""Tony""  ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar""

And this code below records every possible combination of name-nickname:

    
    list.a <- as.list(c(""Bill"", ""Ernie"", ""Oscar"", ""Sammy"", ""Tony""))
    
    list.b <- as.list(c(""Slats"", ""Stretch"", ""Tiny"", ""Tower"", ""Tree""))
    
    result.df <- expand.grid(list.a, list.b)
    result.list <- lapply(apply(result.df, 1, identity), unlist)
    result.list <- result.list[order(sapply(result.list, head, 1))]
    
     head(result.list)
    [[1]]
       Var1    Var2 
     ""Bill"" ""Slats"" 
    
    [[2]]
         Var1      Var2 
       ""Bill"" ""Stretch"" 
    
    [[3]]
      Var1   Var2 
    ""Bill"" ""Tiny"" 
    
    [[4]]
       Var1    Var2 
     ""Bill"" ""Tower"" 
    
    [[5]]
      Var1   Var2 
    ""Bill"" ""Tree"" 
    
    [[6]]
       Var1    Var2 
    ""Ernie"" ""Slats"" 

The way I see it, the two objects (""matrix\_version"" and ""result.list"") should contain the right answer to this logic puzzle - I just don't know how to extract the correct combination from these two objects such that the logical conditions are respected.

Can someone please show me how to do this?

Thanks!",1640848635.0
rrsvuo,"CPU instructions, OS and applications",,19,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rrsvuo/cpu_instructions_os_and_applications/,5,"Hello,

I have a hard time picturing how the CPU instructions, the operating system, and applications work together.

What I mean by that is the following, we have an application that is compiled, now it's converted to CPU instruction (I guess ??), but then this app is using system calls to interact with some I/O devices, how does this system calls go through the OS? And if it's just an instruction that invokes an OS function that makes that system call, what prevents the application from calling the I/O directly or ""hacking"" that function by pretending it's another process ( as an example a process reading data from a socket owned by another process).

Thanks",1640839846.0
rq60d4,Rules of Programming,Advice,168,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rq60d4/rules_of_programming/,45,"Rob Pike's 5 Rules of Programming

&#x200B;

\#Rule 1.

\*You can't tell where a program is going to spend its time.

Bottlenecks occur in surprising places, so don't try to second guess and

put in a speed hack until you've proven that's where the bottleneck is.\*

&#x200B;

\#Rule 2.

\*Measure. Don't tune for speed until you've measured, and even

then don't unless one part of the code overwhelms the rest.\*

&#x200B;

\#Rule 3.

\*Fancy algorithms are slow when n is small, and n is

usually small. Fancy algorithms have big constants. Until you know

that n is frequently going to be big, don't get fancy. (Even if n

does get big, use Rule 2 first.)\*

&#x200B;

\#Rule 4.

\*Fancy algorithms are buggier than simple ones, and

they're much harder to implement. Use simple algorithms as

well as simple data structures.\*

&#x200B;

\#Rule 5.

\*Data dominates. If you've chosen the right data

structures and organized things well, the algorithms will

almost always be self-evident. Data structures, not

algorithms, are central to programming.\*

&#x200B;

\*Pike's rules 1 and 2 restate Tony Hoare's famous maxim

""Premature optimization is the root of all evil."" Ken

Thompson rephrased Pike's rules 3 and 4 as ""When in doubt,

use brute force."". Rules 3 and 4 are instances of the

design philosophy KISS. Rule 5 was previously stated by

Fred Brooks in The Mythical Man-Month. Rule 5 is often

shortened to ""write stupid code that uses smart objects"".\*",1640663682.0
rqysm5,How do we know when real ‚ÄúAI‚Äù is achieved?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rqysm5/how_do_we_know_when_real_ai_is_achieved/,8,"I see all kinds of marketing about machine learning and AI but I don‚Äôt think any of it passes the Turing test. Are there other tests that are used to determine when artificial intelligence is achieved? Maybe I am missing something but I don‚Äôt think AI exists yet, thought?",1640751083.0
rqqd61,Question about storing variables in memory in programming languages,Help,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rqqd61/question_about_storing_variables_in_memory_in/,3,"Lets say i declare int x and x's adress is 1 for example. Since x is an integer it will take adesses 1-4. Then lets say i declare a float and its adresses are 5-8 

How does the computer know that the data type of adress 1-4 is int and 5-8 is float? Does it also have another memory address that points to those 4 memory adresses that says that those are integer and float types? 

I know ints and floats are represented differently in binary but in the end its all 32 bits so the computer could read it however it pleases. 32 bits can be read as an integer or as a completely different float. Thats why i dont know if the computer leaves another address just to say what data types are on certain adresses or does it work differently?",1640727368.0
rqeudi,What are your resources to understand theory behind programs?,,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rqeudi/what_are_your_resources_to_understand_theory/,4,"Hello! I am somewhat new to programming. I am wondering what you're recommendations are for resources (books/YouTube videos/websites etc.) that explain theoretical concepts behind programming and programs. One example of what I mean by ""theoretical concepts"" would be explaining what data structures are, from their smallest components to abstract ideas. 

The goal is to understand what exactly is happening when programming, not just learning which statements and syntax to use for any particular language. 

Thanks a lot!",1640694726.0
rpr7gj,Book recommendations on algorithms related to chess programming?,General,56,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rpr7gj/book_recommendations_on_algorithms_related_to/,14,I‚Äôve recently taken an interest in chess engines.  I‚Äôm looking for seminal books that discuss the algorithms used in modern chess programming. There are lots of algorithm books out there. Recommendations?,1640622649.0
rp30oe,Where does one find out what the latest tech is?,General,68,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rp30oe/where_does_one_find_out_what_the_latest_tech_is/,11,"Hello everyone,

We all know how Computer Science is such a fast moving and evolving field, but where do you even find out what the next big thing is in the field? Youtube/TikTok tech influencers? Blogs? News? 

And say you find out that X, Y and Z are three new things in Computer Science. Does this mean we should learn all of X, Y and Z if they relate to our field of interest?

Thank you!",1640544158.0
rp1tzo,How does the c++ std rotate algorithm impl work?,General,21,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rp1tzo/how_does_the_c_std_rotate_algorithm_impl_work/,7,"Impl in referring to is here:
http://www.cplusplus.com/reference/algorithm/rotate/

I can't find any intuitive explanation online that motivates or proves the correctness of this algorithm.  It doesn't appear to be the ""clever"" triple reverse method for implementing rotate with O(1) extra space.",1640540786.0
rop984,K+1 databases more secure than K?,,14,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rop984/k1_databases_more_secure_than_k/,10,"I'm just now, starting to learn about security as a full stack engineer.

In terms of security, does it make sense to separate the database for user authentication w/ the db that stores user data? My approach for a project I'm doing is to salt and hash raw passwords (unique salt per password) server-side, and compare that hash output with the stored pre-computed hash output, to do user authentication.

What happens after? I feel like if you can protect the integrity of user->pass and therefore user->userData, it doesn't matter whether userAuth db and userData db are separate or not, b/c it would be a pain for a hacker to determine data ownership (assuming data does not hold identifying info). Is there a principle I can follow about separating data/processes into different databases?

Also, are there any resources one would recommend for application-focused security practices? I don't want to be the security guy, I just want to learn some basic high-level understanding.",1640492946.0
rolb3t,Patterns in Prime Factorisations (and an alternative algorithm for prime factor generation),,11,0.79,computerscience,/r/math/comments/rok78o/patterns_in_prime_factorisations_and_an/,4,,1640479238.0
rnyhfs,What happens if a component doesn‚Äôt do its job before the successive clock tick?,General,57,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rnyhfs/what_happens_if_a_component_doesnt_do_its_job/,11,I‚Äôm new to this stuff so maybe my question is not phrased correctly...,1640392031.0
rnt040,Operating systems,,27,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rnt040/operating_systems/,18,"Hello,
Is there any latest research going on in OS? 
I find OS specifically very interesting and wish to do research on it.",1640373904.0
rnvod9,Was the Apple I computer an example of SoC?,General,13,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/rnvod9/was_the_apple_i_computer_an_example_of_soc/,3,,1640382608.0
rnuf9s,Book Recommendation For a Student,Advice,16,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rnuf9s/book_recommendation_for_a_student/,7,"Hi everyone,

I want to read some books related to mathematics and computer science. I am not looking for a book to learn a topic I don't understand; I am just trying to find a book about computer science and/or maths to read to have fun with and to learn something new.

To clarify, I will give an example, although the math is more basic than what I am looking for: The book of numbers by John Conway

Thanks in advance",1640378438.0
ro43co,NeurIPS author qualification,,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/ro43co/neurips_author_qualification/,0,"Who can submit papers to NeurIPS? 

Are there any specific requirements or qualifications for authors?

I looked in their website and couldn't find an answer.

Thanks!",1640413417.0
rnmo0w,Help me understand assembler/assembly language,Help,43,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rnmo0w/help_me_understand_assemblerassembly_language/,10,"So, I know that a compiler takes the code that you write and processes it into machine code. But what's the role of assembler/assembly language here?",1640353595.0
rnt4bg,Introduction to Hash Tables,,7,0.77,computerscience,https://bytethisstore.com/articles/pg/implement-hash-table,0,,1640374275.0
rne5t2,what is the big O for this simple problem?,,64,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rne5t2/what_is_the_big_o_for_this_simple_problem/,86,"public static int recursiveFunction (int n ) {  
   if (n==100) return 606;

   if (n == 99) return 600;

   return recursiveFunction(n+2) -12; 

}

&#x200B;

it seems in the worst case scenario this runs forever so I thought it is O(infinity) but my Prof put O(n) in the exam as the answer. can someone tell me if my answer is acceptable?",1640320743.0
rnc33s,What is your best critique of decentralized public blockchain technology and why it won't play a major role in the future of society?,Discussion,6,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/rnc33s/what_is_your_best_critique_of_decentralized/,18,"I'm doing a personal project where I will steel man the arguments for AND against a decentralized blockchain led future, and then write a formal dialogue between the two perspectives with citations.

I've done a fair bit of research thus far and have found it much easier to find pro-blockchain future articles/resources (this is not surprising to me), but am still looking for more of both. However, I would be really happy with some solid bear arguments because the ones I've found aren't especially convincing. They do more to explain the limitations of blockchains and call out the flaws in maximalist thinking.

Can anyone link some really interesting/compelling content for either side? I would really appreciate it, and will absolutely share my work here when it's done!",1640313890.0
rmtdj4,"A fascinating read about ELF ..take a peek , if it's interesting to you or importantly, have time!",Article,68,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rmtdj4/a_fascinating_read_about_elf_take_a_peek_if_its/,5," Part One: [https://kestrelcomputer.github.io/kestrel/2018/01/29/on-elf](https://kestrelcomputer.github.io/kestrel/2018/01/29/on-elf) 

Part Two: [https://kestrelcomputer.github.io/kestrel/2018/02/01/on-elf-2](https://kestrelcomputer.github.io/kestrel/2018/02/01/on-elf-2)",1640257630.0
rmewjn,Web pages indexing and ethics for search engine results ?,General,16,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rmewjn/web_pages_indexing_and_ethics_for_search_engine/,1,"Recently, I've started looking into search engines and some algorithms that get used for crawling and ranking the pages on certain factors.

I wanted to understand different perspectives on ideal ethics we should adopt while creating something similar of our own. I very much know that the combination of keywords generate different results but my question is how to go about some of these keywords which can be borderline non acceptable. For example, someone recently pointed this out to me. The word voyeur is not shown in search recommendation although some movie with same name recently came out and is available on Prime. So this got me curious and i searched for the actors in the movie with that movie's keywords and results were not according to my expectation to say the least. (search for Sydney Sweeney voyeur and go to images/videos tab and you'll understand my point)

This also leads to another question regarding the most popular search engines that we use everyday...so suppose some particular word is not ""acceptable"" according to societal norms but gets very popular due to certain pop culture catalyst. How does these search engines work around that to handle these cases ?  [Because any 12 year old who uses Amazon prime will see the movie Voyeurs there on the home page and will go to Google to search for actor etc and my good it can be daunting]

P.S. I hope i haven't derailed too much in the description but the title should give away the gist. And sorry for the typos, typing from mobile.",1640207457.0
rm1nxm,is there a book which takes me through the process of how computers and/or programming languages were first invented?,Discussion,102,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/rm1nxm/is_there_a_book_which_takes_me_through_the/,22,"

id love a book thats like a history lesson but also showing the technical details that the founders struggled with and eventually solved. basically a book where, if i was teleported in time and space to the lunch room of the inventors i would understand the conversation they have about what theyre trying to figure out. 

I know it sounds a bit vague since its a very big topic but im interested in both the hardware and the software, like anything from 1930s to 1970s.",1640165632.0
rls9mc,Can nodes in balanced binary search trees contain parent node field?,Help,27,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/rls9mc/can_nodes_in_balanced_binary_search_trees_contain/,9," The background of this question comes from me the fact that in Skiena's book ""The Algorithm Design Manual: Second Edition"" page 85:

https://preview.redd.it/sc6mewa9lz681.png?width=961&format=png&auto=webp&s=893bdc9adb5bd9b79ce1f17cf2fdcb0c63741169

He claims that the delete-minimum operation for the priority queue when implemented with a balanced search tree will have O(log n) time complexity. But, I disagree with that statement IF the nodes in a balanced search tree have a parent node field. Because since we have a pointer to the minimum it takes constant time pointer manipulation to unlink the minimum node and since the new minimum node will be the parent node, setting the pointer to the new minimum also takes constant time. Unless my assumptions on how a balanced search tree is misguided (I assume a balanced search tree is functionally equivalent to a binary tree, except its self-balancing process), then the delete-minimum operation should have O(1) time complexity. Where have I gone wrong?",1640132590.0
rma55n,Question about ALU Design,Help,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/rma55n/question_about_alu_design/,10,"Hi , I am a bit confused about the ways in which an ALU is designed , So and ALU has mainly 2 operations: Arithmetic And Logic , The Arithmetic part is mostly achieved through Adders but what about the logic part?
I saw an online article and they used AND , NOR , XOR gates , but why those gates specifically? , Wouldn't it make more sense to use AND , OR , NOT gates as they are the basic logical gates? I am extremely new to this stuff , Am i missing out on something?",1640194155.0
rlfsh9,How does data structure alignment help in efficient reading of memory?,,49,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/rlfsh9/how_does_data_structure_alignment_help_in/,9,"I know that data structure alignment is done to prevent a data structure being stored across two memory blocks. This was explained by u/AuntieSauce in another post I created regarding [the internal working of structs](https://www.reddit.com/r/computerscience/comments/rj8mre/how_do_structs_work_internally/).
> Now, why does the second rule ensure we don‚Äôt have one element of a struct stretched across two blocks? Here is an example: lets say we have a struct that an 8 byte element and a 1 byte element:
> 
> | 8 bytes | 1 byte
> 
> Now, lets say we want to make an array of structs. Each struct is 8 bytes in size, and arrays are stored contiguously in memory, so we have
>
> 8+1 bytes | 8+1 bytes | 8+1 bytes | etc‚Ä¶
> 
> If our array is large enough we will eventually approach the end of a block. Will a block size ever be divisible by 9? Like, almost definitely not. So you‚Äôll get the end of the block with less than 9 bytes of space left, and the struct will get chopped!

In the Wikipedia article about [data structure alignment](https://en.wikipedia.org/wiki/Data_structure_alignment), it is written that CPU can efficiently read memory when it is data structure aligned.
> The CPU in modern computer hardware performs reads and writes to memory most efficiently when the data is naturally aligned, which generally means that the data's memory address is a multiple of the data size

So my question is that how does data structure alignment actually help in the efficient memory reads by the CPU.

Thanks!",1640096846.0
rm2a30,How is long distance network communication done?,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rm2a30/how_is_long_distance_network_communication_done/,3," I just started learning networking and I was playing with tracert.

One  of the outputs got me wondering. Im in Europe and trying to access New  York server. 

&#x200B;

    Tracing route to 104.21.24.154 over a maximum of 30 hops
    
      1     4 ms     1 ms     3 ms  192.168.0.1
    ...
      5     9 ms     5 ms     6 ms  xe-8-1-3-3180.bar1.Sofia1.Level3.net [212.162.46.177]
      6     *        *       33 ms  ae-2-3216.ear4.Frankfurt1.Level3.net [4.69.210.66]
      7    41 ms    33 ms    36 ms  195.122.183.210
      8    33 ms    30 ms    38 ms  104.21.24.154

 

The 7-th address is in  Germany  and  the 8-th (destination) is in New York. So how is the communication  between the two routers performed?

Are  there routers between the last two points that tracert cannot find  information for or a satellite communication or a really long cable?",1640168240.0
rlgtu5,Concept of Delayed abstraction,Article,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rlgtu5/concept_of_delayed_abstraction/,0,"Hi,

I am trying to understand the concept of delayed abstraction from the following paper at:

[VerX](https://files.sri.inf.ethz.ch/website/papers/sp20-verx.pdf)

     To verify a Past LTL specification of a bundle of contracts C
    we apply abstract interpretation over a symbolic domain.
    We employ predicate abstraction [35] but without the usual
    conversion to boolean programs. Our approach is similar to
    that of Flanagan and Qadeer [32] where two transformers are
    alternated: precise symbolic transformers to handle individual
    commands, and an imprecise transformer to ensure convergence.
    In contrast, classic abstraction applies an imprecise
    transformer at every step. Hence, we call the precise/imprecise
    approach delayed abstraction.

Somebody please guide me.

Zulfi.",1640100001.0
rkr2j4,Andrew Tannenbaum,,70,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/rkr2j4/andrew_tannenbaum/,26,"Hello, I have gone through Tanenbaum's books on OS. They are very informative and interesting. So much that I googled about the author and his. In his wiki I came to know that he has got degrees in physics and astrophysics from MIT and Berkeley. 
I'm curious as to how he got the expertise to write books and do research on OS? He has other books also like Structured Computer Organization, computer networks etc. It seems like he has studied CS in his physics courses. I dig into his career but couldn't find any info related to when and where he learned CS. 

Kindly address me the missing link if anyone knows, as his wiki says he's a established physicist while his books and his career speaks he's a computer scientist.",1640017594.0
rkf6jh,I really want to design a single board computer. I have a lot to learn. Advice of specific resources?,Help,26,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rkf6jh/i_really_want_to_design_a_single_board_computer_i/,17,"I am aware this is not an easy task but I don‚Äôt care. I want to do it. I am an electronic engineer and have experience with electricity, testing and troubleshooting pcbs. I have never designed anything besides simple circuits with lights and switches.. lol. 

Currently studying software development for a bachelors and I‚Äôm comptia a+ certified.

To be able to design an SBC I definitely need to improve in designing pcbs. I need to improve my understanding of computers so I can have a better comprehension of how some of the components on an sbc will go together.

I feel a bit overwhelmed to be honest and wanted to consult you guys so I can define a learning path for me so I can actually start considering designs.

If you have ideas or specific resources that‚Äôd be helpful for me and my intentions I‚Äôd appreciate it.",1639976497.0
rkr18d,Looking for textbook recommendations for algorithm analysis/proofs?,General,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rkr18d/looking_for_textbook_recommendations_for/,3,"Any recommendations? Looking for a book that will go through the analysis of algorithms, giving proofs by induction/contradiction etc. 

Is there well known go-to resource for what I'm looking for? Or any suggestions that you just found to be great in general?

Thanks in advance.",1640017494.0
rk5sms,"Is there a ""perfect"" random event?",Help,42,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rk5sms/is_there_a_perfect_random_event/,18,"I dont mean a coin toss or such thing, as its technically determined as soon as you flip the coin ( gravity, wind etc.), but rather a totally unpredictable event like a programmed coin toss, but I'm not sure wether its actually totally random. (sorry for bad my bad english)",1639948067.0
rkfmao,I'm making a web app and I'm hashing the passwords with a salted bcrypt after that I hash the hashed value with md5 then store it in my DB. Do you guys think this is secure or do you have any recommendations?,Advice,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rkfmao/im_making_a_web_app_and_im_hashing_the_passwords/,23,"My thought is that even if the DB leaked and even if they got the salt by hacking into the server they would still have to deal with the md5 they wouldn't be able to decode it without knowing the original value is because first, they would have the guess the md5 hashed value that bcrypt generated then decode it with the salt.

What do you guys think? I hope that hacking won't be an issue though as my production environment will be locked down pretty well.",1639978028.0
rjvbrs,Looking for programming challenges/projects related to randomization/approximation algorithms.,,61,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rjvbrs/looking_for_programming_challengesprojects/,17,"Hello, I have been reading on randomized/approximative algorithms books, and while Math is aesthetically pleasing, I wish to get my hands dirty with practical coding challenges or projects.

Any recommendations?",1639915018.0
rjwxfl,Why does SSH use symmetric encryption,,19,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rjwxfl/why_does_ssh_use_symmetric_encryption/,5,"Hello everyone! 
I've been reading up on SSH and am trying to figure out why SSH uses symmetric encryption for communication when asymmetric encryption is used for authentication and key exchange.
The only reason I can think of is symmetric cryptography allows for fast encryption and decryption. Is there any other reason?",1639921080.0
rk78ya,Reentrancy Detection Technique by Zeus tool,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rk78ya/reentrancy_detection_technique_by_zeus_tool/,1,"  

I was reading the detection of same function reentrancy by Zeus as discussed in \[[Zeus: reentrancy detection](http://pages.cpsc.ucalgary.ca/~joel.reardon/blockchain/readings/ndss2018_09-1_Kalra_paper.pdf)\]. They have done cloning. I can‚Äôt understand this method. Is this a valid method of testing a SC because we are modifying the SC. Can we modify the SC during testing?

The paper says:

    Reentrancy in Solidity can happen via the call method. send only invokes the default function with limited gas for logging purposes. ZEUS handles same-function reentrancy by first cloning the function under consideration, and inserting a call to the clone before the invocation to call. Fig. 14 shows the patched function for the example in Fig. 2. Note that ZEUS ensures that the patch is done within the same basic block so as to ensure that if the cloned function is called, then the invocation to call is also made. Further, we also assert false before the call code. If the verifier finds a path leading to this assert, it indicates a bug.

Somebody please help me to understand .

Zulfi.",1639952159.0
rj8mre,How do structs work internally?,Help,67,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rj8mre/how_do_structs_work_internally/,35,"How do structs work internally in memory. I know that an instance of a struct is a pointer to the first field of the struct. I also know that all the fields of a struct are contiguous to each other in memory so the memory address of the second field of a struct can be accessed by adding the size of the first field to the memory address address of the first field.

I am failing to understand that how do we access the consequent fields of a struct with just the memory address of the first field. We can do it in arrays by jumping x bits ahead according to the data type of the array, we can only do this in arrays because the values in a certain array have the same data type. My question is that how do we navigate through the fields of a struct by only knowing the memory address of the first field of the struct.

Thanks!",1639837811.0
rj7x0t,Can someone explain how I would calculate the Big-Theta notation for this bit of code please? (Python),,7,0.74,computerscience,https://www.reddit.com/r/computerscience/comments/rj7x0t/can_someone_explain_how_i_would_calculate_the/,7,"    specifiedItem = 'item6'
    lists = [['item1', 'item2'],
                  ['item3', 'item4'],
                  ['item5', 'item6']]
    
    wasItemFound = False
    
    for list in lists:
            for item in list:
                if item == specifiedItem:
                    freeFromIngredient = True
                    break

Please don't just give me the answer, if you could explain how I would calculate it that would be great! Thanks :)",1639835496.0
riwlmc,Brian.W.Kernighan will give a keynote at linux.conf.au 2022,General,48,0.97,computerscience,https://twitter.com/linuxconfau/status/1472020904687198208?s=20,1,,1639792616.0
risxvd,Uptime difference between servers and computers?,,22,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/risxvd/uptime_difference_between_servers_and_computers/,13,"I was curious about the differences in how we treat servers and computers. So for example, if my computer is on for several weeks, it starts to get slow and weird things start happening until I eventually shut down or restart and it comes back up peforming normally. Why is it that servers can stay up for months or even years and not experience the same flakeyness as regular computers. I imagine it has some to do with the OS and some to do with the hardware, but wanted to see if anyone could enlighten me. Thank you.",1639781308.0
rinkr5,How far can a one terabyte file be compressed?,Help,29,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/rinkr5/how_far_can_a_one_terabyte_file_be_compressed/,36,"Does anyone know how far a one terabyte file can be compressed? What‚Äôs the limit of today‚Äôs technology compared to 2000 and 2010? Regarding the compression of a file. 

If one terabyte holds 1,000,000,000,000 bytes, what is the utmost limit of compression?

If data loss will occur, tell me the limit for both. With and without data loss.

Edit: Let‚Äôs say the data is an entire computer full of word files, photos, and videos. I know it‚Äôs basically impossible to state an exact amount of word files, photos, and videos, however, I‚Äôm stating an example. One terabyte of your entire computer. Going off the assumption that your computer is exactly one terabyte of data.

Edit 2: If someone has an exact example, let me know. For example, your own computer. How much would you be capable of compressing? Let me know the beginning size and then the compressed size.",1639765489.0
rihkz0,Software Architecture Readings,,33,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/rihkz0/software_architecture_readings/,7,"Hey all, I'm looking for some software architecture design books or websites that can help me devlop and understand large and scalable projects. It's difficult trying to find readings that aren't aimed at newbies. Really just looking to advance my skills. Java is my primary language if it matters.",1639748149.0
rip3jw,Question: Alan Turing‚Äôs approach to decidability problem,Discussion,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rip3jw/question_alan_turings_approach_to_decidability/,7,"A question: Alan Turing‚Äôs approach to Decidability problem(Can we know beforehand if certain numbers and theorems are calculable and provable) was that if there existed a decidable program D that takes another program as input and decides if it will finish, We can encode the program itself and there should exist a decidability program that decides on the program D. So is Decidability Decidable. Well now my question is can‚Äôt the answer be yes. That would make an infinite chain of decidability programs but that doesn‚Äôt mean it is logically incoherent as suggested by the analogy used that this problem reduces to logical problem that arise from:‚Äúthis statement is false‚Äù.Why is the answer No?",1639769913.0
ri2yda,Question about a possible attack on WEP,,44,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/ri2yda/question_about_a_possible_attack_on_wep/,10,"Hello everyone, I'm learning about the Wired Equivalent Privacy (WEP) protocol. I thought about a possible attack and wanted to know if it is possible:

An adversary intercepts a packet/frame that has the initialisation vector (in plaintext) and the ciphertext. They then replace the ciphertext with some other data, and send it to the intended recipient.

Is it possible to perform this attack without the recipient detecting that this has happened and without finding the value of the secret shared key? 

Thanks in advance!",1639696553.0
ri9bkh,Advent Of Code: My Best Hobby For 2021,Article,6,0.81,computerscience,https://theabbie.github.io/blog/advent-of-code-2021,0,,1639716383.0
ri6duj,Question about the time complexity of a specific algorithm,Help,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ri6duj/question_about_the_time_complexity_of_a_specific/,3,"I was watching a youtube video about coding interviews and after looking in the comments, someone was claiming that [this](https://gyazo.com/fa08cd2babb00bead656e52bf93d0a0c) algorithm had a time complexity of O(n) where n is the size of the array.

I have more of a technical background rather than a CS one so this is my thinking when approaching this:

Because he's taking each integer in the array and brute force testing it against all other integers in that array and then moving onto the next integer if there is no match, the algorithm will take exponentially more time as the array size increases and therefore has a time complexity of O(n^2).

For example, an array with 2 integers will have 4 (2^2) total pairs: 00, 01, 10, 11,

While an array with 4 integers will have 16 (4^2) pairs: 0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111

So which one is it? O(n) or O(n^2)?",1639706958.0
rhll1c,"If a text message held 64 characters, would that equal 64 bytes?",Help,22,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/rhll1c/if_a_text_message_held_64_characters_would_that/,28,"I‚Äôm not sure if this is the right place to ask, however, I‚Äôm gonna ask anyways. I‚Äôm pretty sure one byte equals eight bits. If that‚Äôs correct, am I correct in assuming that one byte equals one character? Are all characters the same amount of bytes? Like, numbers and letters. Example being; 7 compared to H. They‚Äôd both equal one byte? Separately, of course. Not together. 

Also, is a space considered a character byte? 

Lastly, is there a difference between a email message versus a text message? Pertaining to byte size per character. 

If this isn‚Äôt the right place for this question, could someone point me to the correct area? If this is the right area, mind answering these questions?",1639639943.0
rh4amb,"Does the programming language type system spectrum (such as below) exist anywhere in academia? I'm writing my dissertation and would really like to include it somewhere, if anyone knows an academic paper it's in that would be incredible, been searching for hours!",Help,279,0.92,computerscience,https://i.imgur.com/DxhVCU5.png,45,,1639589143.0
rhhyf3,I have questions about the physical nature of files,Discussion,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rhhyf3/i_have_questions_about_the_physical_nature_of/,6,"Will downloading files wear on a computer? 

The processes of downloading files, storing information, sending information, etc.

Does the process destroy wires, chips the memory drive, etc?

Does a file physically imprint itself in such a way that it can never be completely erased?

Is there a field of study that relates to my specific interest?

Thank you",1639627270.0
rhnc1k,[Discrete Maths] Surjective functions,Help,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rhnc1k/discrete_maths_surjective_functions/,3,"So if asked to prove that f(x) = y is surjective, we check if all the elements in the co-domain(y) have an element in the domain(x), so does this mean that we can have an element in x that doesn't map onto any value in y. i.e
for the domain {x1, x2, x3} and the codomain {y1, y2}

f(x1) > y1
f(x2) > y2
f(x3) > blank

Is this Surjective? Since I've heard that when mapping, all elements in x are used, but when we check if a function is surjective we only use elements in y to check for x, so there might be some elements in x that don't map to y",1639647549.0
rgnbmf,I wrote code the stupid way,,136,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rgnbmf/i_wrote_code_the_stupid_way/,33,"I've been working as a software developer for 6 months and during this time I had to write a certain query and did it in the absolute stupidest and unnecessarily complex way that I thought was genius and was very proud of.
Meanwhile a coworker had to solve the same problem and I came across his solution. I was embarrassed by it's simplicity and now I feel like a moron.
Has anybody had similar experiences?",1639533028.0
rh7r2v,Bresenham's Line Drawing Algorithm is... confusing?,Help,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rh7r2v/bresenhams_line_drawing_algorithm_is_confusing/,1,"Forgive me if this is the wrong sub for this, but I'm kinda getting desperate at this point.

Can anyone explain in a dumbed down manner how the following lines from [Bresenham's Line Drawing Algorithm](https://medium.com/geekculture/bresenhams-line-drawing-algorithm-2e0e953901b3) make sense? Essentially, this is used to calculate where the next pixel should be drawn when attempting to draw a line between two points in a 2D space:

>We use dx and dy values to calculate the initial decision variable (d). value of the decision variable is changed on each step.  
>  
>d=2dy-dx  
>  
>Similarly, we need to calculate ‚ñ≥E and ‚ñ≥NE values. After the first initiation, these values are not changed.  
>  
>...

Like, where does the multiplication come from and why is it arbitrarily on dy and not dx? Not only that, most of this method seems to only account for positive increases, and although it does mention them, it doesn't give a proper solution to other cases.

Also, a ""slope error"" doesn't seem to be something google wants to tell me about. So, what's the deal with that?",1639598063.0
rgyayz,System Design Interview ‚Äì An Insider's Guide | Any difference between the book and the online course,Advice,4,1.0,computerscience,/r/csMajors/comments/rguyh0/system_design_interview_an_insiders_guide_any/,0,,1639572303.0
rg53gw,Is there a website or atleast a document that lists common optimizations done by compilers or interpreters?,Discussion,64,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rg53gw/is_there_a_website_or_atleast_a_document_that/,5,When watching tutorials I often notice them sometimes saying one way is better due to compiler optimizations. I know they treat it as a black box for time reasons but it would be nice to have a compilation that lists what these optimizations are and when the compiler does them. Even without looking at the assembly or atleast an explained version of it.,1639479581.0
rfmupc,How should I introduce a kid to programming?,General,134,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/rfmupc/how_should_i_introduce_a_kid_to_programming/,74,"Hello everybody! 

I am a 4th year Computer Science student. My fianc√©‚Äôs 10 year old sister has expressed some interest to me about the work I do and said she wants to learn how to code. 

Does anybody have any resources or advice on how to introduce kids at this age to programming? She‚Äôs interested in games like Roblox. I just feel like the approach I took to start learning might not be appropriate since I started as an adult.",1639421131.0
rf81l6,Viruses- mutation?,,21,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rf81l6/viruses_mutation/,14,So seeing as various types of colds and viruses can autonomously mutate- is it possible for a computer virus to mutate by itself?,1639372760.0
rfovp2,What's the best way to encode SHA-2 Hash?,Help,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/rfovp2/whats_the_best_way_to_encode_sha2_hash/,3,,1639426374.0
rf03ai,Understanding NP Completeness,Advice,54,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/rf03ai/understanding_np_completeness/,15,"Can you share a good website, book or other resources where the ideas related to np complete and np hard complexity classes are explained intuitively?

I read Cormen and Wikipedia but feel like I want something more. 

Thanks.",1639348304.0
reoznh,Do you think neural networks could be used to recognize murderers through the handwriting in the notes they sometimes anonymously send to the police? How?,,34,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/reoznh/do_you_think_neural_networks_could_be_used_to/,18,,1639315566.0
rey8v8,Widespread Exploitation of Critical Remote Code Execution in Apache Log4j,Article,8,0.83,computerscience,https://www.rapid7.com/blog/post/2021/12/10/widespread-exploitation-of-critical-remote-code-execution-in-apache-log4j/,0,,1639343004.0
rfa9t6,Will we ever reach a point were no more malware can be created.,Discussion,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rfa9t6/will_we_ever_reach_a_point_were_no_more_malware/,12,"Will there ever be a point were every possible malware type will be detected by antivirus software, making malware nonexistent?",1639381039.0
reinb8,"Is the ""mod"" in Modular square root same as the other mod (e.g. 5%2 = 1)?",Help,20,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/reinb8/is_the_mod_in_modular_square_root_same_as_the/,5,"Hey, I am not from a computer science background but I do software dev. I was trying to understand VDF and how it works - Verifiable delay function. I came across 'Modular square root' in [https://www.youtube.com/watch?v=\_-feyaZZjEw](https://www.youtube.com/watch?v=_-feyaZZjEw). I was not able to interpret how ""mod"" was being used here. 

Tried this link: [https://www.rieselprime.de/ziki/Modular\_square\_root](https://www.rieselprime.de/ziki/Modular_square_root), its even confusing. 

Any resources or material to understand the background will help. 

Thanks in advance.",1639289343.0
rdc453,Why do computer scientists (and mathematicians) use the worst variable names in proofs and papers?!?,,213,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rdc453/why_do_computer_scientists_and_mathematicians_use/,83,"Sorry if this post doesn‚Äôt quite fit this sub but this has been bothering me for ages and I‚Äôve recently hit a breaking point for my frustration. 

It just pisses me off that there are 26 letters in the alphabet and 325 pairs of letters you can choose and for some reason the pair that we all seem to use is (i,j).  
There are literally no two letters that look more alike and that could get more easily mixed up but for some reason whether it be in code are in my case here in proofs and papers these two letters are always used together.  
Why should I have to use a magnifying glass to read a damn proof from a paper that couldn‚Äôt have the decency to at least use a better font?

And it doesn‚Äôt stop at (i,j). 
We also love to use (m,n), (p,q), (u,v). 

Like I get it, it‚Äôs more clear that the variables are related when they look alike, but it should certainly be more important that people don‚Äôt mix them up???",1639152919.0
rdlhtp,Computer science riddle: monkey and 10 cages,,47,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rdlhtp/computer_science_riddle_monkey_and_10_cages/,25,"There is a monkey and 10 cages.
Each cage has a door and there is a passage between each adjacent cage, not including 1 and 10.

The monkey starts at a random cage. Your goal is to find him. You may open and close as many cage doors as you want, but each time you close a cage the monkey must go through a passages to a different adjacent cage. 

The monkey can enter a previously opened cage (for example, he was in cage 5, you looked inside cage 4 and once you closed it he moved into cage 4).

Example:

Monkey at 6, you opened 8, you closed 8, monkey went from 6 to 5. You opened 4, you closed 4, monkey went from 5 to 4. Repeating.

The goal is to find a method that ensures you'll find the monkey in the least amount of doors opened.

Me and my friends had a go on this one for about 2 hours, we came up with a 17 moves solution that's I'll describe below in the first comment.

The person who told us about it said there is a better solution with less doors to open.
I hope you can help me find it. Good luck!",1639178190.0
rdr5st,Coding path to learning guidance,Help,3,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/rdr5st/coding_path_to_learning_guidance/,5,"Hi, I have 0 experience in coding (went to school for Finance), and my end goal is to learn solidity. I want to start with JavaScript but should I learn about Data Structures and Algorithms first or first learn the basics of JavaScript. I also saw that there are courses of Data Strucutres and Algorithms specific to JavaScript. Any guidance would be helpful.",1639195644.0
rdo7ab,Confusion on adder-subtractor circuits.,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rdo7ab/confusion_on_addersubtractor_circuits/,2,"&#x200B;

https://preview.redd.it/6omanltxet481.png?width=613&format=png&auto=webp&s=14a787509b92bbf1d6fcc17ce86c61b5cad6286a

So in an adder-subtractor circuit the output for addition will be as an unsigned integer while the output for subtraction will be in two's complement form right?  


If so does the circuit not work when the difference is less than minimum two's complement value? For example in two's complement a 4-bit integer can range from -8 to 7 in decimal. But you can input 0000 - 1001 for example. The difference of those two terms would be -9 in decimal which is less than -8 the lowest possible value. So are adder-subtractors unable to work when the difference of the input is less than minimum two's complement value?  


Sorry if this makes no sense. I am very confused.",1639186172.0
rcla2n,So what do computer scientists think about NFTs? Cool tech with real world application? Or just a new way for rich people to launder money?,Discussion,97,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/rcla2n/so_what_do_computer_scientists_think_about_nfts/,83,"Seems like everyone is talking about NFTs in some capacity but I haven't seen a lot of opinions about them from tech literate people, just wondering what the general consensus on them is from a comp sci perspective.",1639067420.0
rcip1t,How do you think should Calculus be taught to Computer Scientists?,Discussion,68,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rcip1t/how_do_you_think_should_calculus_be_taught_to/,32,"Do you think that it's better to teach it with an emphasis on applying calculus to actual problems instead of the conventional way of integrating this ugly monstrous function? If you were to teach it to undergraduates doing CS, then how would you do it?",1639059771.0
rc955j,Time Complexity,,23,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rc955j/time_complexity/,13,"I have this question where it asks what the time complexity of this piece of code is. I know that the solution is O(n), but I can't wrap my head around the ""why"" of it. I was wondering if someone could help explain it to me.

https://preview.redd.it/992i6gk00g481.png?width=1202&format=png&auto=webp&s=887f13656915bea3909b15678ce44e5315b559b1",1639022484.0
rbwnot,How do I get better at coding?,Help,119,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rbwnot/how_do_i_get_better_at_coding/,43,"I'm taking computer science in school and I'll admit, I'm not the best at it and I want to change that, I want to get better at coding but I'm not sure how I should go about it because I've heard that practise is the best way of getting better at it but I'm not sure where I should practise (We're studying python btw)

thanks in advance!",1638985373.0
rc3wsu,"You are tasked with creating a ""library"" computer which can survive societal collapse and remain operational for decades, centuries, or longer. How would you go about it? What kind of hardware and software would it need?",Discussion,37,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rc3wsu/you_are_tasked_with_creating_a_library_computer/,26,"Perhaps this isn't quite the right subreddit. I'm doing some research for a bit of fiction I'm working on. 

Let's say it's built using current or very-near-future tech. The group making it has the resources of a large nation. Their aim is to preserve human knowledge even if we get knocked back into the stone age, and for the device to remain operational for long enough to be used to help rebuild society.

What sort of information would it store? What sort of hardware would be able to store and retrieve data for a very long time? What kind of software would run on it? What can be done to make sure it's stable? What kind of software would be most useful for this purpose? What and how do we teach a user who might not even know how to write? How would we power it?

How long, realistically, could we make such a device remain operational? What kind of peripherals could this device use? (communicate with a paired satellite in a geostationary orbit, antenna on top of a mountain, etc)",1639006059.0
rcfj0u,"Don't Just Track Your Machine Learning Experiments, Version Them - Managing ML Experiments as Code with Git and DVC",Article,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/rcfj0u/dont_just_track_your_machine_learning_experiments/,0,"ML experiments often get split between Git for code and experiment tracking tools for meta-information - because Git can't manage or compare all that experiment meta-information, but it is still better for code.

The following guide explains how to apply DVC for ML experiment versioning that combines experiment tracking and version control: [Don't Just Track Your ML Experiments, Version Them](https://dvc.org/blog/ml-experiment-versioning) - Instead of managing these separately, keep everything in one place and get the benefits of both, like:

* Experiments as code: Track meta-information in the repository and version it like code.
* Versioned reproducibility: Save and restore experiment state, and track changes to only execute what's new.
* Distributed experiments: Organize locally and choose what to share, reusing your existing repo setup.

Experiment versioning treats experiments as code. It saves all metrics, hyperparameters, and artifact information in text files that can be versioned by Git, which becomes a store for experiment meta-information. The article above shows how with DVC tool, you can push experiments just like Git branches, giving you flexibility to share experiment you choose.",1639048353.0
rcj9zu,How does a cpu process information,Help,0,0.44,computerscience,https://www.reddit.com/r/computerscience/comments/rcj9zu/how_does_a_cpu_process_information/,4,"I heard about something turning on and off, I don't know anything more",1639061548.0
rc1j2y,"Difference between data type, structure, schema, model",,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rc1j2y/difference_between_data_type_structure_schema/,4,"What is the difference between data type, structure, schema and model? And what's the relationship between each? Any examples?

New to CS and would love to learn more. Thanks!",1638999111.0
rbz19p,Programming permissions and privileges?,,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rbz19p/programming_permissions_and_privileges/,2,"The previous company I worked at and the current company I work at both have implemented some sort of permission system so that end-users or even different organizations could not access data that they were not allowed to.   


This sort of feature is actually really important when selling a product. Both implementations have also been absolute cluster-fucks, with a ton of spagetti, edge cases, weird states, un-flexible for future feature requests, etc. 

&#x200B;

In both cases I believe this is due to the original implementors, while smart people, were trying to design a complex system from scratch using their own intellect without taking a look at the problem from a foundational or theoretical perspective.   


I might have an opportunity to right some wrongs in the coming year, and I was wondering if there's any material/information out there on how to properly design security/privileges. If i model a proven solution, i think we might end up in a better place.",1638992000.0
rc9jzq,Can AI theoretically solve the halting problem or at least most of it?,,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/rc9jzq/can_ai_theoretically_solve_the_halting_problem_or/,15,"Imagine you have an neural network system, and its a classifier. It takes in some code and spits out whether or not it will halt or not. You could imagine in a simple case that you train the neural network with code that has a while (true) loop. It clearly halts and the AI can report that it halts with some trivial training examples, probably easily, without the need to do an infinite amount of computation.

So theoretically as you expand the sophistication of the AI system and incorporate more training and more examples it should be able to determine whether code halts more and more. It seems obvious, at minimum, that the AI should be able to determine whether a much larger range of code either halts or not without doing any traditional computation. Have there been any cs papers exploring whether AI could expand the reach of solvable verifiable problems traditionally thought to have been out of computational reach e.g. in the np space?",1639023865.0
rawuw6,For the computational scientists and AI guys here,General,101,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rawuw6/for_the_computational_scientists_and_ai_guys_here/,42,Tell us about some cool projects that you've worked on.,1638875623.0
rbng33,Etymology of screen dump,Discussion,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rbng33/etymology_of_screen_dump/,5,"I'm posting here because internet is for once proving useless in answering this question, what is the etymology of screen dump?

Over the years the word, screenshot, has taken over for the use of 'dump', but in my native language the word still carries this legacy meaning. I've got a master in informatics and currently doing my phd in information systems research, but can't for the life of me find the answer. The old-school 70s and 80s professors in my research group, who lived through the nascent computer science days, doesn't have the answer either.  


any old school computer scientists here that might know the reasoning behind this naming? What is the reason for using dump? I've been speculating that it has something to do with how computers back in the day had to dump the memory and not make a screen photograph, but something else. Would love to hear your speculations, or possible answer!",1638955440.0
rb9f0g,Good books on optical/photonic computing?,,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/rb9f0g/good_books_on_opticalphotonic_computing/,2,\^,1638910927.0
rajca8,"I‚Äôm a first year computer science student with a lot of free time, I would like to study topics that aren‚Äôt included in my program that would help me when i look for work, what would you suggest?",,33,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rajca8/im_a_first_year_computer_science_student_with_a/,41,,1638829038.0
ra4e6d,Linear algebra and matrices in computer science,,107,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/ra4e6d/linear_algebra_and_matrices_in_computer_science/,23,"I am really struggling with the first course on linear algebra and matrices and I find it hard to motivate myself, as I don't know many real-life applications to the things I am learning, or am trying to learn.

I know these things have many applications in cs, but since I don't have a clear vision on what I'd like to do in this field yet (so far I've enjoyed web deving and I am interested in audio programming and graphics, although I haven't gotten into them yet), I don't really know why these things are relevant for me.

Any pragmatic examples on why I really should learn this stuff? Thank you.

edit: thanks to everyone who has responded, it is greatly appreciated!",1638787049.0
rawmbc,What are the real life applications of Taylor‚Äôs theorem?,Help,0,0.33,computerscience,/r/maths/comments/rawm02/what_are_the_real_life_applications_of_taylors/,0,,1638874746.0
rb71fe,How long did it take for you to realize that the dynamic programming taught in algorithms is the same as the one in RL?,,0,0.2,computerscience,https://www.reddit.com/r/computerscience/comments/rb71fe/how_long_did_it_take_for_you_to_realize_that_the/,5,,1638904548.0
rahxnv,Formal Automata Theory next steps,General,8,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rahxnv/formal_automata_theory_next_steps/,6,"I just recently finished studying Automata. I found it very interesting and enjoyed the intuitive process, even though I struggled a bit.

I was wondering what topics or subject matters would be best to study next?",1638825302.0
raflir,Question on malware.,,13,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/raflir/question_on_malware/,4," 

How is it that malware can run on computers that dont have programs that run the language installed? or even certain libraries?

Like if i dont have python installed and i get a python virus, how does it run?

or if i did have it, but didnt have the right libraries, how could it function?

sorry if this is a stupid question, i am relatively new.

&#x200B;

I just really dont understand how code can even run without the right stuff installed.",1638819261.0
raadci,Computation tree,Advice,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/raadci/computation_tree/,3,"Was watching presentation by The Guy Steele

[https://www.youtube.com/watch?v=dPK6t7echuA](https://www.youtube.com/watch?v=dPK6t7echuA)

He describes computation as computation tree.

I'm trying to find if there are research (books) that focuses on computation trees specifically?

Take code -> unfold its execution into computation tree 

Found this: [https://en.wikipedia.org/wiki/Computation\_tree\_logic](https://en.wikipedia.org/wiki/Computation_tree_logic)

This is close but I was wondering is there something that applies ""computation tree"" in compiler/JIT/interpreter optimization...?

Thanks!",1638805950.0
ram49s,Everything's a Provider?,Advice,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/ram49s/everythings_a_provider/,4,"I showed a junior engineer that you can reduce dependencies by using inheritance and only using the parts of the interface you need.

For example,

class Circle : Shape, UIElement {}

And later, if all you need is shape properties, then use a Shape ref to point to a circle, don't require circles.

But instead, he's now broken *everything* down into single field classes/structures. E.g.

class NameProvider { string name; }

class AddressProvider { string address; }

class AgeProvider { int age; }

class TitleProvider { string title; }

class RankProvider { int rank; }

class HeightProvider { int height; }

...

class Thing : NameProvider, AddressProvider, AgeProvider, TitleProvider, RankProvider, HeightProvider, ...

I find this level of breaking things ridiculous, and it's become impossible to work with his code. It takes forever to figure out what anything is and where it is defined. But he's in love with this new approach and won't compromise. Is there a formal reason to not break things out so much? Or better yet, is there a test for when you've broken things out the right amount?",1638837044.0
rabwtu,Does it make sense to study something from 2007 and will it really make me a better engineer?,,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/rabwtu/does_it_make_sense_to_study_something_from_2007/,4,"Hi. I'm a self-taught developer who studied CS50 and full-stack web development and is solving coding challenges daily. I'm trying to become a better developer by deeply understanding the fundamentals of CS but I'm not really sure if I'll become a better developer by doing so. I came across that website: [https://teachyourselfcs.com/](https://teachyourselfcs.com/)

And I found it has old books and 2007 MIT courses. I want to know from more experienced developers out there if it is worth it to study these old heavy textbooks and courses? I'm not trying to learn the theoretical side of computer science or something I'm just trying to become a better developer who can provide better value for businesses.",1638809925.0
r9bx1d,Computer Networking Basics Every Developer Should Know,,252,0.97,computerscience,https://iximiuz.com/en/posts/computer-networking-101/,7,,1638695607.0
r9qvnk,How do I lead a project?,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/r9qvnk/how_do_i_lead_a_project/,8,"Alright, to give some background on myself, I'm a junior in High School who has been learning to code and programming for the past 2 years in just one language. Anyways, this year, I decided to join my school's CS club where they said they were taking applicants for team leads. I didn't really know what a team lead was and they left it pretty vague so I decided to sign up because I felt like why not and that this would probably be a good opportunity for me to learn some leadership skills. This being said, I really didn't know what I was getting into. For one thing, I assumed that we would be having team meetings during the club at lunch on Friday when we usually meet, however, in reality I'm supposed to schedule meetings on my own. Anyways, this isn't really too much of an issue for me however, my largest issue is the fact that 9 people signed up to be in my group, most of them arguably better programmers than me, and others, quite beginner. This is an issue for me because I've never actually coded something not by myself so I really have no idea how to divide up the work into 9 parts given how interrelated everything is in the project. In the same sense, I really have no idea how often I should meet with my group. I feel like the project is fairly simple and I'm afraid if we meet even just an hour a week we'd probably finish within a month. If you want more details on what the project is, feel free to dm me, however, if anyone could give me advice on how I should proceed given how lost I am that would be great.",1638742267.0
r95hk3,How do computers render perfect circles given things like SVG images?,Discussion,17,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/r95hk3/how_do_computers_render_perfect_circles_given/,10,"I don't know if this is the right place to ask but I couldn't think of any other subs that might be appropriate. Basically, how does the computer take an svg image, and create a perfect circle that is perfectly circular no matter how ""zoomed in"" you are. It seems like its either a lot less complicated than I'm assuming or a lot more complicated than I'm assuming, but it seemed like an interesting topic.",1638671737.0
r8rl8f,What Part of Computer Science Does Computer Graphics Fall Under?,,77,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r8rl8f/what_part_of_computer_science_does_computer/,21,"I'm really interested in creating software like game-engines or 3d modeling software, mainly i like to deal with the graphical aspects of the software like reflections or lighting. What would be the name of the course that i will study?",1638630602.0
r8z2ei,"Does publishing an app on the Apple store constitute as ""commercialization""?",,6,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r8z2ei/does_publishing_an_app_on_the_apple_store/,8,"I'm working on an iOS application for my portfolio. I'm a frontend guy so I'm using an open API as the backend. It's free to use but strictly mentions that the data must not be ""commercialized"". 

1. Does this mean I can only use the API as a personal project, and not publish? 

2. Or does it mean I can publish my application - I just can't monetize it?",1638651761.0
r8upnl,How did early computers understand input?,Discussion,4,0.65,computerscience,https://www.reddit.com/r/computerscience/comments/r8upnl/how_did_early_computers_understand_input/,8,"I know they used punch cards and ASCII, but without programming languages like we have nowadays how did the computers understand what was on the cards? How can a machine understand input without a language allowing it to do so? It would be like talking to a rock and expecting it to do something. I‚Äôm really curious about how this worked, how the first communication with machines began. Can‚Äôt seem to find an answer on google",1638639539.0
r8f02f,How does firmware get updated?,Discussion,27,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/r8f02f/how_does_firmware_get_updated/,16,"So if firmware is on an ROM chip soldered to the motherboard, this should mean it is impossible to change what is on the chip because it's read only. But firmware updates happen over the internet, so how is that possible?",1638583707.0
r8mrol,Are pointers useless in the same scope?,Discussion,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r8mrol/are_pointers_useless_in_the_same_scope/,8,"Pointers are obviously very useful when they point to values across function calls but are they actually useful while pointing to values in the same scope. For example if a variable `age` stores the value `114`, I can update the the value of `age` by creating a pointer to that memory address, but why would I do that? I can simply change the value of `age` directly because I have access to it as I am changing from the same scope.

A reason I could think of for creating a pointer to a variable in the same scope is that it might be useful to have pointers point to a variable in the same scope because it would make the naming easy and referencing to the value in huge codebases would be made easier, but I do not think that this is a good use case. 

The main purpose of pointers is to stop the wastage of memory by copying of the values stored in a variable to another variable but if we are creating pointers, we are of course storing them in a variable which is taking memory so we are back to square one. This might not be the case where variables store a lot of data, this is true for variables that store less data than the memory address itself.

My question is that is there any use for creating pointers that point to variables in the same scope or they are never used like that. Stating any examples would be very helpful!",1638613113.0
r80r4b,Any good resources or books for learning Lambda Calculus?,,51,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/r80r4b/any_good_resources_or_books_for_learning_lambda/,11,"As in title, I would appreciate any reccomendation.",1638543014.0
r81i62,Any good resources to learn Boolean Algebra?,Help,20,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r81i62/any_good_resources_to_learn_boolean_algebra/,11,I've my exams in a few days and I need to learn about Boolean algebras and logic gates. Can you guys please help me with some resources?,1638545028.0
r7x6t1,What is the difference between AI specialists and computational scientists?,,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r7x6t1/what_is_the_difference_between_ai_specialists_and/,2,I'm currently an UG student in Comp sci and want to PG in a field that contributes more to scientific research. The fields I've found that do this are AI and computational science. They seem pretty close to me so what's the difference between them and which is better wrt to my goal?,1638531200.0
r7gaav,Halting Problem & Quantum Entanglement,,49,0.93,computerscience,https://www.youtube.com/watch?v=2H8629BCbkM,2,,1638476316.0
r7nt4h,Help with Asymptotic Growth,Advice,14,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/r7nt4h/help_with_asymptotic_growth/,5,"EDIT: For people asking the same question as me, I have found an INCREDIBLY useful video that explains Asymptotic Growth and Asymptotic Bounding. Please watch this here: [https://youtu.be/0oDAlMwTrLo](https://youtu.be/0oDAlMwTrLo)

Hello. This post is very simple: I am struggling to understand Asymptotic Growth. It just seems like this pointless silly abstract mathematical concept that doesn't even make sense when its applied to things like computational complexity. I can write down definitions of what it means for f to equal O(g), but I don't understand the problem, I don't understand how this is the solution, and I don't understand how the solution couldn't be something else. Basically, I need a resource that can really comprehensively teach what this thing is in a way that makes sense for it to exist.

&#x200B;

RANT-- Skip to TL;DR if uninterested

Like if you already have a function that perfectly describes your algorithms time complexity, what's the point in simplifying it in a weird way that breaks algebra rules? If you're trying to compare it to another algorithm, then why not just... compare it to another algorithm? Grab two functions, from two different algorithms, come up with some reasonable input sizes, and evaluate. Simple as that. But then there's this best case worst case thing? And I don't get how what comes out of the definitions is the best or worst case... The way that O(n) was defined to me just made it seem like any arbitrary function that was larger than f(n) could suffice in defining the worst case scenario...

&#x200B;

TL;DR please give me resources on Asymptotic Growth that go deeper into explaining why its a thing and how its actually useful",1638497963.0
r7fkp9,Official client verification,,17,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/r7fkp9/official_client_verification/,9,Is there a way I can have a client send something like a hash of it's executable to confirm the client has not been modified to a server?,1638474405.0
r6vpdh,Encrypted dead man's switch,,87,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/r6vpdh/encrypted_dead_mans_switch/,51,"I want to make a dead man's switch to release all my code publicly and my master password to my family, but I want it to be encrypted beforehand, and have a low chance of being cracked or destroyed before deployment, I don't have anyone trusted to maintain it and I might end up in a psyc ward for long periods of time

any way set up a dead man's switch with these conditions?",1638410229.0
r70ia7,Algorithm Time Complexity,Help,14,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/r70ia7/algorithm_time_complexity/,3,"I was given a function T(N) = 5N + T(N-1)

My understanding is the algorithm will always have a Big O time complexity of of O(N) because there isn't any multiplication to increase N's power, but the answer that was expected from me was O(N\^2). Even in words, I could say that the function calls itself 5N times plus another N minus 1 times, so the time complexity should still be linear with respect to N inputs. Why is this O(N\^2)?",1638424964.0
r71nbu,VMs and Executing Machine Code,General,8,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/r71nbu/vms_and_executing_machine_code/,2,"Sometimes when trying to run a VM, you need to enable virtualization in the BIOS. I always presumed this meant that the hypervisor was sending machine code directly to the CPU, which in turn interacts with all the peripherals for the VM, and that the host OS was ""allowing"" the VM's machine code to pass by.

However, I just watched [this video](https://youtu.be/wX75Z-4MEoM) where he says that the hypervisor (or at least a type 2 hypervisor) is making a virtual representation of the hardware. When I use something like VMware (type 1) desktop, is it actually interacting with bare metal? Whereas something like Virtual Box (type 2) goes through the host kernel?  Any insight is appreciated.",1638429089.0
r6f0od,Learning OS after learning C,Discussion,54,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/r6f0od/learning_os_after_learning_c/,12,"Hey guys I wanted to know. If someone knows C and wanted to get into the world of operating systems and know all the round robin, multiprocessing stuff should they:
a) Read an entire book on OS first, before getting their hands dirty with real code 
                                                 OR
b)Should read books, alongside getting their hands dirty while following the documentation of the OS they have chosen.
I wanna know you all thoughts and opinions.",1638366123.0
r6eo9a,Getting into computational modelling ?,,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/r6eo9a/getting_into_computational_modelling/,8,"Hi guys, I'm an undergrad cognitive psychology student and recently I came across the existence of the use of computational models to study phenomena.

Since I've always been pretty keen on computer science I wanted to dive more into it but I couldn't find much material on the web about what knowledge you need to start developing these programs.

I'm particularly interested in computational linguistics and computational biology.

Do you guys have any suggestions on what mathematical and CS knowledge is needed in order to have a solid background for starting to build these models ? I have some programming skills but only with Python and Java and i don't know what language is used for models (C++ maybe?)

I've already asked some professors but i still have some doubts about it.",1638365054.0
r639yl,Resources to learn about the inner-workings of a computer at a technical level?,,52,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/r639yl/resources_to_learn_about_the_innerworkings_of_a/,14,"Hi all, IT professional here. Recently I have become interested in learning more about how the components inside computers work. For example, I know what CPU, GPU, RAM, etc are...but how do they actually operate?  I'm talking at an extremely technical level. Does this fall under computer science? Thanks all.",1638323120.0
r6qahx,I'm a hard sci-fi writer looking to write about cyborgs that edit their RNA with the help of nanites. How do i find the processing power to do this effectively?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/r6qahx/im_a_hard_scifi_writer_looking_to_write_about/,2,"I'm fully aware that controlling the many variables that go into genetics is a difficult task. Previously i had the computers that controlled the nanites linked to a massive, planet-wide supercomputer, but realized this connection would be impossible to maintain on earth (the cyborgs are also aliens). Is there a way I can fit the needed processing power into a small package? ",1638395644.0
r6ei7a,Network simulator,,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/r6ei7a/network_simulator/,4,"In Uni are currently using Modeler of riverbed, which was created by opnet in case you know it. I basically hate that tool to the core, it makes our lives misserable! ..mostly due to the lack of documentation.  
It is basically a tool where you can create Logical or Physical networks to simulate, with many choices for mediums, settings, protocols, profiles e.t.c.  


 I was wondering, what are the similar alternatives out there for IT in big companies?",1638364521.0
r5yufr,Why might changing process priority and forcing a single CPU thread to focus on a program improve the stability of that program?,,57,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/r5yufr/why_might_changing_process_priority_and_forcing_a/,22,"This is kind of a specific question but, I formulated it in the most general way possible so anyone can benefit from any answers.

I have a very lightweight visual novel video game called *Shiny Days*. For some reason, the game runs just fine with minimal bugs in windowed mode. However, when in fullscreen mode, the game will freeze at a very specific moment in the game every time without fail.

The fix for this endorsed by the community is to change the process priority to ""Above Normal"" and change the processor affinity to only ""CPU 0"" or ""CPU 1."" This is to force the program to only use a single thread.

&#x200B;

The question I am asking here is, why does that fix work? Why would increasing process priority and limiting the program to a single thread help anything? I know what the fix does but, I don't get why it stops the freezing of the program in such an effective way. What do you guys think is going on here? I'm curious to hear any insights computer science experts would have to say on WHY this fix actually works.",1638310213.0
r6bl5i,Does Concorde algorithm really give an optimal solution to the TSP ?,,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r6bl5i/does_concorde_algorithm_really_give_an_optimal/,9,"Recently, I played around with Concorde algorithm [available here](https://www.math.uwaterloo.ca/tsp/concorde/downloads/downloads.htm).

It should return an optimal solution, however some choice he made in his optimal path are counterintuitive.

For example, I would have thought that green path would be more optimized than the red one proposed by Concord :

[Red path is suboptimal ?](https://preview.redd.it/el0vbb9s6y281.png?width=325&format=png&auto=webp&s=6aac58adabee8a54644470bf4127535cc306115d)

&#x200B;

Is this caused by some rounding approximation ?",1638353634.0
r5rwl8,How does a web server talk with an API at a low level.,Help,26,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/r5rwl8/how_does_a_web_server_talk_with_an_api_at_a_low/,4,"Hello, I'm trying to get a deeper understanding of networking, specially low level network programming. I've been reading about sockets and how to write server programs using socket libraries like winsock or socket.h, but I am kind of confused about how the server then talks with other programs. I think the best way to explain it, is for me to lay out what I THINK is happening when a simple GET request hits an HTTP server.

Assume a client and server socket are already connected using TCP.

Data Arrives to the network interface for the machine, the driver for network interface card tells the OS that it has data (through an interrupt?). The OS then looks at the IP:Port# and if there is a connected socket listening with that IP and Port, it then (through the socket library) passes the data to the process that is the ""server"". The server has the payload with the HTTP headers, metadata, body, etc. Now lets say I have written an API using C# using the webapi framework in dotnet. Does the server open a socket with the process running the API in order to pass the HTTP request info to the correct endpoint? Is the server and the API the same process, and the dotnet framework (or any API framework) is baking in the socket code and acting as a server as well? or is it something I'm completely missing?

&#x200B;

Let me know if this makes sense at all. Please correct and incorrect assumptions or missing steps in my walkthrough. I'm trying to remove as much abstraction as possible and understand how something like this works at a low level.

Thanks for your time!",1638291958.0
r5w1xe,"Advice on studying virtualization (material, learning path, ...)",,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r5w1xe/advice_on_studying_virtualization_material/,2,"Hi guys, 

This semester I am taking a course named ""virtualization techniques"". It is very interesting and also daunting to me when it involves a huge amount of low-level understanding of the system. I don't have a good background in this and I think I can get some good advice from Redditors :D Like good textbooks to read, basic articles to grasp the idea, ... I think it would be very helpful to refresh my computer architecture knowledge, but it would be nice if I could get a list of worth-studying contents :D As of now, I am trying my best but kinda lost in the lecture (I feel the professor is quite bad at eli5)

Don't tell me to quit, I picked this course because I found it fun to learn, and also I intentionally spent this semester studying shit I have almost zero knowledge of. I don't care about my grade at all.

Thanks in advance!",1638302695.0
r54to6,(HELP) Learning Networking,,27,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/r54to6/help_learning_networking/,11,"Hey guys, i just wanted to see if any of you know any good resource because so far i have read many books but none of them helped me (Head first, Cisco , Comptia)  i know those are good books but i need something that explains EVERYTHING, im a complete beginner.. for example they start talking about protocols without explaining what  a protocol is and a lot of things like that they just say words without explaining

i need a book or course that gives a deep introduction.",1638217686.0
r4y6sf,Researchers Defeat Randomness to Create Ideal Code,Article,38,0.93,computerscience,https://www.quantamagazine.org/researchers-defeat-randomness-to-create-ideal-code-20211124/,3,,1638199555.0
r4v7w6,Operations of a memory stack and how it is used to implement function calls on a computer,,29,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r4v7w6/operations_of_a_memory_stack_and_how_it_is_used/,10,"Hi everyone, I have recently been doing some personal research on data structures and algorithms, one of the things I want to look into is a memory stack. 
I have already written about how a Stack data structure works in C# (what is does, what the operations are and some examples) but I also need to talk about a memory stack.

Would anyone be able to explain to me in simple terms what a memory stack is and if there is any different between a Stack in C# and a memory stack and also how it is used to implement function calls in a computer?

Everything I have found online is either too complicated or is just telling me you can Push and Pop like you can in the C# stack so I don‚Äôt understand the difference.
Thanks :)",1638190442.0
r4xo16,Liskovs Substitution Principle Question,,5,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r4xo16/liskovs_substitution_principle_question/,10,"My question is about why LSP requires contravariance in the parameters of the subtype's inherited methods.

And more to that, if following LSP means that I can pass in a subtype wherever a parent type is expected, then why couldn't the inherited methods of the subtype be covariant in the parameters?

Does the fact that you can't be more specific when passing parameters to a subtype's inherited methods itself violate LSP?

Edit: Thanks to u/Probablynotabadguy I think I understand this.

If you have a type T and a subtype S, then for all intents and purposes, S IS a T.

And so of course the substitution portion holds that if you have a function `foo(T)` that you should be able to pass in an S - because we just said an S IS a T, right?

That part I understood. However, because anything provable of type T should be provable for type S, that includes it's methods. And so the contravariance of the subtypes methods is precisely because it's trying to preserve the provability of the subtype relationship.


My apologies, I know that's probably a lot of the same words.

[Code](https://carbon.now.sh/?bg=rgba%28171%2C+184%2C+195%2C+1%29&t=vscode&wt=none&l=text%2Fx-php&ds=true&dsyoff=42px&dsblur=68px&wc=true&wa=true&pv=56px&ph=56px&ln=false&fl=1&fm=Hack&fs=14px&lh=133%25&si=false&es=2x&wm=false&code=class%2520Speaker%2520%257B%250A%2509public%2520function%2520speak%28string%2520%2524word%29%2520%257B%250A%2509%2509return%2520%2524word%253B%250A%2509%257D%250A%257D%250A%250Aclass%2520LoudSpeaker%2520extends%2520Speaker%2520%257B%250A%2520%2509public%2520function%2520speak%28string%2520%2524word%29%2520%257B%250A%2509%2509return%2520strtoupper%28%2524word%29%253B%250A%2509%257D%250A%257D%250A%250Aclass%2520ScreamingSpeaker%2520extends%2520LoudSpeaker%2520%257B%250A%2520%2509public%2520function%2520speak%28string%2520%2524word%29%2520%257B%250A%2509%2509return%2520strtoupper%28%2524word%29%2520.%2520%27%21%21%21%27%253B%250A%2509%257D%250A%257D%250A%250Aclass%2520Animal%2520%257B%250A%2520%2520%2520%2520public%2520function%2520speak%28LoudSpeaker%2520%2524speaker%29%253A%2520string%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520return%2520%2524speaker-%253Espeak%28self%253A%253Aclass%29%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Aclass%2520Cat%2520extends%2520Animal%2520%257B%250A%2520%2520%2520%2520%250A%2520%2520%2520%2520%252F%252F%2520Why%2520am%2520I%2520not%2520allowed%2520to%2520accept%2520ScreamingSpeaker%2520here%253F%250A%2520%2520%2520%2520%252F%252F%2520According%2520to%2520LSP%2520I%2520can%2520only%2520accept%2520either%2520LoudSpeaker%2520or%250A%2520%2520%2520%2520%252F%252F%2520it%27s%2520parent%2520class%2520Speaker.%250A%2520%2520%2520%2520public%2520function%2520speak%28LoudSpeaker%2520%2524speaker%29%253A%2520string%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520return%2520%2524speaker-%253Espeak%28self%253A%253Aclass%29%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Afunction%2520makeAnimalSpeak%28Animal%2520%2524animal%29%2520%257B%250A%250A%2520%2520%2520%2520%252F%252F%2520According%2520to%2520LSP%2520I%2520shouldn%27t%2520pass%2520in%2520LoudSpeaker%2520here%253F%250A%2509return%2520%2524animal-%253Espeak%28new%2520Speaker%29%253B%250A%257D%250A%250AmakeAnimalSpeak%28new%2520Cat%29)",1638198036.0
r408nu,"Computer Science was always supposed to be taught to everyone, and it wasn‚Äôt about getting a job: A historical perspective",,12,0.94,computerscience,https://computinged.wordpress.com/2021/11/26/computer-science-was-always-supposed-to-be-taught-to-everyone-but-not-about-getting-a-job-a-historical-perspective/,2,,1638090600.0
r3rold,Creating hash based verification of links.,,18,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/r3rold/creating_hash_based_verification_of_links/,6,"Hi geeks,

How do we verify whether a link or a github profile that was provided in Resume belongs to the same person as the Resume? I am thinking of having a hash based addressing to solve such problem. Any leads? 

&#x200B;

For detailed explanation: Let's say in my resume I put [https://github.com/randomuser](https://github.com/randomuser) and no one knows If I am the random user and github user names are strange most times. What can be a possible solution to the problem?",1638060862.0
r3cbfk,Can you recommend any free courses/lecutres on fluid simulation?,Advice,25,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/r3cbfk/can_you_recommend_any_free_courseslecutres_on/,5,"Basically, question is in the title  
UPD: found a great book on the topic:  Fluid Simulation for Computer Graphics by  Robert Bridson ",1638014898.0
r3exb5,LC-3 memory size question,,12,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/r3exb5/lc3_memory_size_question/,10,"https://justinmeiners.github.io/lc3-vm/index.html#1:3

> The LC-3 has 65,536 memory locations (the maximum that is addressable by a 16-bit unsigned integer 2^16), each of which stores a 16-bit value. This means it can store a total of only 128kb, which is a lot smaller than you may be used to! In our program, this memory will be stored in a simple array

Can someone help me to understand why and how the size of the memory is 128kb (kilobits)? I really might be this dense, but I don't get it.

Sorry in advance if not correct sub, I thought this would be the closest one. This is not my home-work, I'm old dude just doing this on my freetime.",1638024227.0
r39ah9,Why do we use texture atlases?,,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/r39ah9/why_do_we_use_texture_atlases/,4,"If it's all in ram already then it shouldn't add any additional time to skip around for textures, right? And even if it's some buffer shit then isn't that openGLs job?",1638002216.0
r2ye5m,Most Used Software Engineering Stack,Discussion,20,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/r2ye5m/most_used_software_engineering_stack/,33,In your opinion what is the most widesbread stack used in software development feild today? I‚Äôm cs student and am trying to get an idea of how people build software in the real world.,1637965523.0
r2sjef,How a video player plays the video file instantly but not a video editor?,,23,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/r2sjef/how_a_video_player_plays_the_video_file_instantly/,7,"When we open a video in any video player, it opens it almost instantly and plays in full resolution with 30fps. We can also skip to a particular section (time) and player instantly jumps both frame and audio to that section.

But when we open it in video editor, it does operation like caching frames or drop frame rate while playing inside editor. Also when we zoom a particular length inside editor it takes time to make thumbnails for that section while in video player it resizes to resolution of windows in real time.

It seems video player can decode the video file instantly but not video editor.

So whats different between how a video player handles a file and how a video editor handles a file. Also why it takes 5 minutes to export (write) the video while it can read it instantly.

I'm not a computer science student. Sorry if question is poorly phased.

Thanks.",1637948259.0
r2a6yz,How do operating systems get loaded onto computers?,,73,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/r2a6yz/how_do_operating_systems_get_loaded_onto_computers/,20,"When there are the components of a computer assembled into an otherwise perfectly functional PC, how is the operating system loaded onto the computer? It's not like they can code the computer by just typing on it because with no software it can't interpret the text and it wouldn't even know how to make the text because Unicode is software too.",1637885515.0
r2e13s,"If an NP-complete problem X reduces to Y, is Y also NP-complete?",Help,24,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r2e13s/if_an_npcomplete_problem_x_reduces_to_y_is_y_also/,7,"My only exposure to NP is a very well written stack overflow answer and I want to see if I understand it. If X is in NP-Complete (so all NP problems reduce to X), and X reduces to Y, then Y is also NP-complete right? Or is it not transitive?",1637898299.0
r27cn1,Mathematical Logic Gates,Discussion,62,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/r27cn1/mathematical_logic_gates/,26,"Any letter here is used to represent a binary input (1 or 0) and then you just follow the basic of principles of maths, oh and work in **DENARY.** The idea is that you could punch one these formula into a standard school calculator without changing any settings and have it working.

Heres the basic ones:

1. NOT = 1-x
2. AND = xy
3. NAND 1-xy
4. OR x+y-xy
5. NOR 1-x-y+xy
6. XOR x+y-2xy
7. XNOR 1-x-y+2xy

Heres an adder that takes a carry and 2 bits and outputs the binary in:

* x+y+z+8xy+8xz+8yz-16xyz

Sorry I didn't create a half adder as my main source for logic gates was a video series by Sebastian Lague so I just followed that along .

Anyway just for fun and to prove how utterly useless this whole thing is here is an untested 4 bit adder:

z+a+b-2ab-2az-2bz+4abz+10((ab+az+bz-2abz)+c+d-2c(ab+az+bz-2abz)-2d(ab+az+bz-2abz)-2cd+4cd(ab+az+bz-2abz))+100(e+f+(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef-2e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)-2f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)+4ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+1000(g+h+(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2gh-2g(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2h(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+4gh(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)))+10000(gh+g(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+h(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2gh(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)))

z is a carry value (if you want to string together multiple adders) and a adds to b c adds to d etc

Heres an interesting question, how far can we take this? Could we write a Turing Machine like this?

Mistakes have now been corrected Thanks everyone who pointed them out and suggested solutions",1637876920.0
r2rbee,book recommendation,Advice,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/r2rbee/book_recommendation/,1,"hi
i studied computer science and worked in security for 20 years, but now I have changed and work in communications ( datacenters architecture, Ironport, bluecoat, Cisco, F5, infoblock, dns, firewalls, mpls, wan, etc) does anybody know of a good book to refresh my knowledge in communications in general. not too basic as i have good background but a good digest. 
thanks",1637944740.0
r1xcw9,Artificial super intelligence (ASI),Help,44,0.74,computerscience,https://www.reddit.com/r/computerscience/comments/r1xcw9/artificial_super_intelligence_asi/,22,"Good day everybody,insight here (worried)

1.The supercomputer aurora21 is nearly finished and been used to map the human brain/connectome, they say it could only take three years to map it

Source:https://www.pbs.org/wgbh/nova/article/brain-mapping-supercomputer/

2. Im also worried about artificial super intelligence and artificial general intelligence already been used


My delusions are now furthered thinking Aurora21 and ASI already exists and are been used to read/implant thoughts (and making people hear voices)

Can someone in the know tell me this isn't possible or the details on how it works/or doesn't

I dont know anything about computers so im turning to you for insight again

Again,on meds,in therapy. Just want to know your insights which i struggle with due to schizophrenia",1637849376.0
r27hds,Lowest wattage to read text file,Discussion,9,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/r27hds/lowest_wattage_to_read_text_file/,7,"There‚Äôs work being done on a diamond nuclear battery that can last hundreds of years, but it‚Äôs only 100 microwatts. How many of these would you need to read a basic text file? It could be as simple as having an LED light up above a set of printed alphanumerical stickers, but the easier it is to read, the better. The idea here is to be able to download Wikipedia and access it at any time without recharging.",1637877306.0
r2cwfc,Quick Vocabulary Clarification,Help,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r2cwfc/quick_vocabulary_clarification/,2,"Are ""Data Width"" and ""Bandwidth"" interchangeable phrases? This assignment I've been tasked with uses the term ""data *width*"" in the context of data transmission and I don't think that it's really commonplace. I'm just curious if it has any other probable/possible meanings. Thanks.",1637894489.0
r20mys,Where to start as a dev who wants to adapt a CS paper to code?,Advice,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r20mys/where_to_start_as_a_dev_who_wants_to_adapt_a_cs/,3,"As the title says, I want to find out if I can implement an algorithm described in a research paper, or one research paper out of a specific family of research. I think a great achievement would be to understand a more basic paper in this family of research, implement a basic working form of the algorithms presented, and have them working together.

The parts of any of these papers I am unfamiliar with is the more complex math and symbols, and how these different described algorithms should fit together. Most of my relevant experience is as a software engineer, if that matters at all.

What are the steps I can take to move towards this goal, as someone unfamiliar with the research and language. Who can I reach out to?",1637858536.0
r1lsow,Analyze Algorithm Complexity which has Distinct Scenarios,,15,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r1lsow/analyze_algorithm_complexity_which_has_distinct/,3,"I have an algorithm which will check if any given number is prime. I've included it at the bottom of the post. It makes use of a few optimizations, mainly the 6n+-1 optimization, where numbers which are not modulus of 6 + 1 or 5 are not considered prime.

I can analyze the complexity of the algorithm to be ``O(sqrt(n))`` if n % 6 === (1 or 5) and ``O(1)`` otherwise, but I am not quite sure how to assess the 'overall' complexity.

My first though is to average them out, such as: ``(4 * O(1) + 2 * O(sqrt(n)))/6``, but I believe that would still equal ``O(sqrt(n))``. Is that correct, or have I taken the wrong approach?


```javascript
/**
 * Check if a number is prime
 * 1. Check if number is a base case
 * 2. If not, check if num is of the form 6n +- 1
 * 3. If it is, then call helper method
 */
function isPrime(num) {
    //handle special base cases
    switch (num) {
        case 1:
            return false;
        case 2:
        case 3:
            return true;
    }

    //primes are never even (except for 2)
    if (num % 2 === 0) {
        return false;
    }

    //determine if num is of the form 6n - 1 or 6n + 1
    const sixMod = num % 6;
    if (sixMod !== 1 && sixMod !== 5) {
        return false;
    }

    return _determineIsPrime(num);
}

/**
 * Helper method to determine if a number is prime
 * If the ""isPrime"" method is inconclusive, this will be called
 */
function _determineIsPrime(num) {
    //optimize by only searching up to sqrt of num
    const limit = Math.sqrt(num);

    //iterate over numbers
    //return false if any divides
    for (let i=3; i<=limit; i+=2) {
        if (num % i === 0) {
            return false;
        }
    }

    //if nothing was divisible, return true
    return true;
}
```",1637807749.0
r1rdzl,Edsger Dijkstra's Turing Award Speech - Part 1 of 8,General,4,0.84,computerscience,https://www.linkedin.com/posts/unixbhaskar_edsger-dijkstras-turing-award-speech-part-activity-6869541920759156736-C2GQ,0,,1637826714.0
r189ka,Do you know any good pop-sci books on the topic of NSA backdoors in Encryption Algorithms?,,25,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r189ka/do_you_know_any_good_popsci_books_on_the_topic_of/,2,"I've seen mention of the supposedly backdoored  Dual\_EC\_DRBG  a few times.

I'm interested in reading more about the background and known facts, any other relevant cloak and dagger shenanigans would be appreciated also.

I am not a mathematician, so I'm looking for a pop-sci book, or something digestible for someone technologically literate but non-expert.

Books are preferred, but good quality articles also appreciated.

Thanks

&#x200B;

EDIT: Any good books on encryption and randomness would be great actually, too.",1637769198.0
r19gul,Tree/Graph Diagram program,,7,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r19gul/treegraph_diagram_program/,1,"Friend is asking me if I know any program where you can drag and drop nodes in order to visualize Trees/Graphs.  
Additionally, is there any program that could diagram your code ? ( like graphviz, but for algorithms instead of structures  )",1637772506.0
r0ss7l,"Why does Rosetta run x84 apps at basically native speed, but running Linux on an M1 is borked as hell?",Discussion,51,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/r0ss7l/why_does_rosetta_run_x84_apps_at_basically_native/,15,"Why? Isnt rosetta essentially a CPU emulator? How can it run apps at normal speed, but running Windows or Linux (the real versions, not the ""M1"" versions) is unusable? Dont even know if this is the correct sub to ask.",1637717136.0
r199w0,"Publishing, The Choice and The Luck",Article,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r199w0/publishing_the_choice_and_the_luck/,0,Publishing research results is Computer Science is a process with no simple solutions. In this blog post I try to present some common strategies and learn from recent insights into the random effects that are present in the review process. [https://cacm.acm.org/blogs/blog-cacm/256994-publishing-the-choice-and-the-luck/fulltext](https://cacm.acm.org/blogs/blog-cacm/256994-publishing-the-choice-and-the-luck/fulltext),1637771971.0
r0eadx,"Parsons puzzles (create, save, modify, delete puzzles)",Discussion,24,1.0,computerscience,https://www.codepuzzle.io,3,,1637677285.0
qzwb8e,Any advice on building a search engine?,Help,70,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qzwb8e/any_advice_on_building_a_search_engine/,37,"So I have a DS course and they want a project that deals with big data. I am fascinated by Google and want to know how it works so I thought it would be a good idea to build a toy version of Google to learn more.

Any resources or advice would be appreciated as my Google search mostly yields stuff that relies heavily on libraries or talks about the front end only.

Let's get a few things out of the way:
1) I am not trying to drive google out of business. Don't bother explaining how they have large team or billions of dollars so my search engine wouldn't be as good. It's not meant to be.
2) I haven't chosen this project yet so let me know if you think it would be too difficult; considering I have a month to do it. 
3) I have not been asked me to do this, so you would not be doing my homework if you give some advice.",1637616378.0
qzwduc,How do I practice my DS Algo concepts?,General,21,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qzwduc/how_do_i_practice_my_ds_algo_concepts/,10,"Before you ask me to create project, I am asking for questions to practice the individual concepts like linked lists, stacks, sorting algos etc. so I can get comfortable with their implementation and the various problems they can solve, before moving on to building projects and integrating them together.",1637616562.0
r02lfl,Resource for practicing algorithms without programming? (migraine sufferer),Help,7,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/r02lfl/resource_for_practicing_algorithms_without/,2,"Does there exist a resource for exercises of the form ""design a program with the following specifications in time/space O(f(n))""?

I am aware of competitive programming websites but i can't currently program due to migraines. I can't practice off these websites because often the best solution is some exponential time algorithm which i will have no way of knowing whether it will work without implementing it.

CLRS is a good resource for problems but there aren't enough to regularly practice on. I would like a resource preferably of the computer contest flavor but where i can practice on pen and paper rather than programming it.

edit: why is this getting downvoted",1637634523.0
qzo8bt,Resources or practice problems for software architecture or design patterns,Advice,25,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qzo8bt/resources_or_practice_problems_for_software/,2,"I am not sure if software architecture or design patterns are the right terms. But I am looking for resources to read up on or practice problems for ways to approach coding problems and implement solutions in an optimal way. Like how to break down a problem in smaller chunks and modularize it. Or even when you should use procedural vs functional vs oop approach.

The reason is that for smaller projects I tend to just do what works, which is manageable, but as it gets bigger I quickly get spaghetti code. Then I start to have analysis paralysis on how to solve certain problems, because it feels like there are so many different ways. I want to practice scalability so that I can keep my projects manageable. Likewise, something that bothers me is that a lot of basic online examples (particularly in OOP) give very trivial problems (e.g. squares being a type of shape, or cats being animals), which is easy to visualize, but the problems I am dealing the objects are very ambiguous (for my feeling). To the point where it is even the question if objects are relevant, with the exception of being able to separate code/data from each other.",1637595660.0
r07asz,How do data cables index files for large hard drives?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/r07asz/how_do_data_cables_index_files_for_large_hard/,2,"Specifically with PATA data cables, I've been looking through their specifications and noticed they have 16 data lines but only 3 address lines. Hard drives that support PATA seem to be able to support hundreds of gigabytes, but directly addressing even just 4 GB would require 4 bytes. How does a computer send data to the hard drive about which data it wants read/written?

My hypothesis would be that either it has something to do with paging and lookup tables, or the data lines are repurposed to tell the microcontroller within the hard drive the area in memory to be accessed, or the computer tells the hard drive which block to start at and it copies over the entire block, one word at a time, or the 3 address lines control the read/write head directly and reading the correct block is timing based, similar to how DVI doesn't use coordinates for pixels.",1637650582.0
qzkegn,Books on IOT,Advice,19,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/qzkegn/books_on_iot/,5,"I recently got interested in IoT. My background is computer software and programming. My interest has always been on the different mediums for  communication between human and computers. 
I would appreciate if you can suggest books that covers the basic concepts of IoT and their application on communication and collecting data.",1637584284.0
qzkz6g,"2nd to Last member of the ""Traitorous Eight"" passed away. Rest in peace Dr. Last.",,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qzkz6g/2nd_to_last_member_of_the_traitorous_eight_passed/,0,"The New York Times: Jay Last, One of the Rebels Who Founded Silicon Valley, Dies at 92.
https://www.nytimes.com/2021/11/20/technology/jay-last-dead.html",1637586259.0
qz9jwv,"Why are errors ""close"" to the kernel so irreparable?",General,7,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/qz9jwv/why_are_errors_close_to_the_kernel_so_irreparable/,1,I have so many questions to ask about kernels and this is one of them.,1637544058.0
qz023c,Genetic Algorithms| Bayesian Optimization | Reinforcement Learning,Help,29,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qz023c/genetic_algorithms_bayesian_optimization/,7,"Hello all,

I'm interested in learning more about Genetic algorithms and Bayesian optimization in the context of Hyperparameter tuning in Machine Learning and Operations Research. Not interested in medium articles, I want to dive and understand the Math. I am also intested to get a good introduction to Reinforcement Learning.

Could you suggest good books/ pedagogical articles about these three subjects?",1637516416.0
qyv2sd,Good introductory book on formal specifications?,Help,24,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qyv2sd/good_introductory_book_on_formal_specifications/,3,"I am looking for a good introductory book on formal specifications. This is a new field for me so I haven't the slightest clue where to begin. Any help is greatly appreciated.

Thanks",1637501642.0
qz4cwb,Genetic programming: tree GP vs. linear GP,,6,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qz4cwb/genetic_programming_tree_gp_vs_linear_gp/,0,"I'm writing an algorithm to predict global average temperatures. I read an academic paper that used tree GP to solve the same problem. I already have a linear GP algorithm I wrote for something else that I'd like to recycle, but I'd also like to get the best performance possible out of the temperature algorithm.

Is there an advantage to using tree GP vs. linear GP and vise versa? Are there certain types of problems that are better suited to one representation over the other?",1637528545.0
qyo107,How does a computer know how full memory is? (Should have asked in my previous post),Discussion,57,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qyo107/how_does_a_computer_know_how_full_memory_is/,13,"Okay, Another memory related question, I wish I thought of this with my previous post to save a bit of time, but anyway. Another thing I have thought of is how does a computer calculate how full Memory is? Is it fully software based, or does hardware handle it? If its hardware based is it in the CPU Memory controller, or the RAM Sticks themselves?",1637472565.0
qyaklu,Does anyone know of any good podcasts that cover computer science or programming topics?,General,121,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qyaklu/does_anyone_know_of_any_good_podcasts_that_cover/,13,"Basically what the title says. There are podcasts about space that aerospace engineers can listen to, for example, so I was wondering if anyone knows of any comp sci podcasts",1637429966.0
qyyi89,Is computer vision is a part of engineering or computer science,Discussion,2,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/qyyi89/is_computer_vision_is_a_part_of_engineering_or/,3,"I wonder if computer vision consider for computer science or engineering or both
As my knowledge that Computer vision is  An Ai but many ppl tell mostly for engineering than science",1637512069.0
qys9kx,Exam question format: Consider an x-stage pipelined CPU. How much time does it take to execute n CPU instructions where each stage is y nanoseconds.,,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/qys9kx/exam_question_format_consider_an_xstage_pipelined/,3,"So I've been doing some past papers recently and this format or something amounting to the same thing keeps cropping up.

The only problem is, if your stages dont divide neatly into your instructions and leaves a remainder.

For example a 3 stage piplined CPU with 8 instructions, will initilise as such:

You subtract the triangular number of 3 from the instructions: 8 - 6 = 2

And that is how many instructions will be carried out while the program is operating at full efficiency, so you can divide the 2 by the number of stages you're operating at any given time so 2 / 3 = ...

Oh... rem. 2?

Trying to visualise what happens this is my best guess at the answer:

 [https://imgur.com/WyVTjCN](https://imgur.com/WyVTjCN)

As you can see you would just have a wind down stage which last 1 cycle? But that doesnt make sense, because on the x axis the third stage of one of the instructions doesn't get carried out?

I think I just fundamentally dont understand.

Thanks a million in advance for any assistance c:",1637490467.0
qytazd,Where to find practical exercises about IPv4 addresses?,,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/qytazd/where_to_find_practical_exercises_about_ipv4/,1,"I'd like to train/practice on IPv4 tasks, such as:

\- what number of addresses there in this address and mask?

\- which range does this address belong to?

\- calculate this, find out that

Where to find such? Thanks!",1637494991.0
qy5tvy,French supercomputer Adastra with AMD EPYC Genoa and Instinct MI250X to become one of the most powerful in Europe,Article,27,0.93,computerscience,https://www.ryzencpu.com/2021/11/french-supercomputer-adastra-with-amd.html,0,,1637415696.0
qy9ke0,"If data packets can collide and can be affected by interference, does that mean they are physical bodies?",Discussion,12,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qy9ke0/if_data_packets_can_collide_and_can_be_affected/,14,,1637427093.0
qykxoh,How would a computer calculate how much RAM it has to work with?,Discussion,3,0.64,computerscience,https://www.reddit.com/r/computerscience/comments/qykxoh/how_would_a_computer_calculate_how_much_ram_it/,7,"Hi! I am designing a computer just for fun, I learned the basics and what not, and I am building one in Logisim (I know, not the best software, but works for now) And it is a decently complicated CPU and stuff. (Still trying to work out how the stack works, and a couple of other things before it can technically be deemed a proper computer) And this thought crossed my mind: ""How do computers calculate how many bytes of Memory it has?"" I have a working prototype of the whole computer and am working on a simple BIOS for it. And while I was making a prototype of the BIOS I thought of ways the computer detects memory and how much there is, but cant think of any. I tried looking it up but cant find any good explanations. Anyone know how this is done? And my apologies if I picked the wrong flair and if this breaks any rules, I have read them and it doesn't seem to break any. Any help is appreciated!

Edit: Thanks to raedr7n and NamelessVegetable for the help! Found out how it works",1637461704.0
qyiizq,How exactly would you go about converting a truth table into a circuit diagram?,Help,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qyiizq/how_exactly_would_you_go_about_converting_a_truth/,10,,1637453711.0
qxxaab,Do you guys refer to yourself as computer scientists,General,80,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/qxxaab/do_you_guys_refer_to_yourself_as_computer/,62,,1637380818.0
qylubl,"Build a simple ad hoc network, with small computers.",,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qylubl/build_a_simple_ad_hoc_network_with_small_computers/,0,"Hi, I programmed a small computer based on von Neumann architecture with a few set of instructions on binary and now I'm asking to design and implement an ad hoc network with some of this emulated computers, so, do you have any advice or documentation for this purpose? Thanks",1637464746.0
qys6ai,"To get passed binary input, do we need to drop conventional sourcing? Fibre optic?",Discussion,0,0.2,computerscience,https://www.reddit.com/r/computerscience/comments/qys6ai/to_get_passed_binary_input_do_we_need_to_drop/,6,"So I am very new to coding, so apologies if this is completely impossible or even incorrect.

But I do believe that current systems run off CPUs measuring the amount of 0s and 1s through voltage.

However if we were to change this process to fibre optic where the process is still assigned to an on or off process.

However if we introduced colour into fibre optics we could expand the possibilities. For example if white was 0-1 blue 1-2 yellow 2-3 and so on so forth we could expand the possibilities.

We can't expand so far, on the 0 being off and 1 being on, so to introduce colours into fibre optic we could essentially assign colours a number to achieve different out comes.

We know fibre is faster than conventional powered connections. So this could increase performance also.",1637490058.0
qyeqvw,Recommendations for a basic high-level language encryption / obfuscation algorithm?,,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qyeqvw/recommendations_for_a_basic_highlevel_language/,7,"Hi I am making a game with a high-level scripting language that has pretty minimal support for bitwise-manipulation (if any). Think something like Lua. And I am making a game where the game has a secret message that is defined as a string literal, ""secret message"". This script can be scraped by data miners, string literals are not obfuscated at all, so if they got a hold of of the sourcecode, they would be able to read ""secret message"". What is a basic obfuscation algorithm I can use here to store the string literal in a scrambled-up format so that dataminers will have a harder time interpreting this message? I'm sort of looking for something one step above a caesar cipher. Any algorithms come to mind?",1637442185.0
qxrdbd,"May be a stupid question: Why are computer programs that modify themselves so uncommon? I can't really think of a use case, but does the phenomenon have a name?",,72,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qxrdbd/may_be_a_stupid_question_why_are_computer/,62,,1637361444.0
qy34w1,Does Encryption work both ways?,Discussion,7,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/qy34w1/does_encryption_work_both_ways/,19,"Here's an example. If a folder is encrypted, it's obviously protected from outsiders trying to get in. But let's say there was a virus inside of an encrypted container, would it be able to escape this container or does the encryption keep it ""imprisoned""? My understanding is that encryption garbles all of the data so surely protection DOES work both ways?",1637405141.0
qxixkk,Why are some people so excited about functional programming?,Discussion,58,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/qxixkk/why_are_some_people_so_excited_about_functional/,60,"It seems like FP can be good at certain things, but I don‚Äôt understand how it could work for more complex systems. The languages that FP is generally used in are annoying to write software in, as well. 

Why do some people like it so much and act like it‚Äôs the greatest?",1637336708.0
qxv7ln,"Why cant a RAID 1 (mirroring) have twice the write speed of a single drive, the way it has twice the read speed?",Discussion,6,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/qxv7ln/why_cant_a_raid_1_mirroring_have_twice_the_write/,7,"I understand how it currently works, and subsequently why it cannot write twice the speed. My question more specifically, is why doesnt it work a different way where that is possible. For example, if there is a 100mb file, instead of writing the 100mb to both drives simultaneously, why doesnt it write 50MB (different halves of the data) at the same time, and then virtually treat it as one complete file as it then continues to write the full file on both drives in the background after done? Does writing 1 bit take up 1 bit of potential reading or something? Even then, the background writing can resume after reading is done at a later time if that is the reason. The only downside I see to this is that you risk losing that particular file if one drive fails before the completed write takes place, but that seems like a minor sacrifice for the improved performance.",1637373882.0
qy32gm,Why does the routing table system work?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qy32gm/why_does_the_routing_table_system_work/,1,"Hi all, I‚Äôve recently been looking a bit into networking and trying to understand it a bit more but routing tables have me a bit stuck. I think that I understand how they work by I don‚Äôt have the faintest idea why.

So as I understand it, a packet comes into a router and the destination IP address is AND‚Äôed with each of the subnet masks in the table and the result of each of these is compared with the destination IP address of each entry in the table. If there are any matches between the result of this AND calculation and destination IP addresses then the packet is forwarded to this destination. If there are no matches the packet goes to the default destination and if there is more than one matches then the packet goes to the destination with the highest subnet mask.

My question is, why does this work? How does this form of selecting destinations make sure packets make it to their destination. How does this stop data packets going round in circles and taking ages to make it to their destination.

I hope that I‚Äôve been clear, but please let me know if I should clarify anything.

Thank you so much for any help here :)",1637404847.0
qxh10d,Learning C again,,14,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/qxh10d/learning_c_again/,11,I have learned C previously from Dennis Ritchie book. But as now I have learned the concepts of Compiler Design  ( and other theoretical CS subjects ) I again want to learn C but with some compiler design concepts( and or other CS concepts ). Like when I am doing something I want to know whole thing behind it relating to compiler design concepts. I don't know whether I have asked clearly or not. But if you get it please tell the resources for it. I want to know whole process that follows running a code.,1637331102.0
qx827n,What sort of memory is used on a simple Casio calculator or any calculator in general ???,Discussion,43,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qx827n/what_sort_of_memory_is_used_on_a_simple_casio/,4,,1637295608.0
qxl1o1,How is data stored in a DHT?,Help,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qxl1o1/how_is_data_stored_in_a_dht/,3,"I‚Äôve been learning about DHTs (distributed hash tables), and I understand that data is distributed across many nodes (or peers in p2p networks). I can‚Äôt seem to find any information on how data is stored in each node though. Is data stored in blob stores in each node? Or SQL databases? Am I understanding this correctly?",1637342787.0
qx22or,Are LeetCode questions poorly grouped in their difficulty tiers?,Discussion,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qx22or/are_leetcode_questions_poorly_grouped_in_their/,3,"I have done my fair share of LeetCode and cannot help but notice that ""easy"" questions are sometimes quite difficult. Medium questions are generally more difficult but many of them just use slightly more advanced data structures and the actual concept behind the question is very straightforward. 

I had never even attempted a hard question since many mediums rack my brain for an hour or two, most of the time I need to look solutions up because there is a new approach to learn. I tried my first ""hard"" question the other day and I swear it was mis-labeled and could definitely be placed in ""easy""

I just solved this graph-ish question and it was labeled as easy but I had to use an iterative breadth-first search approach? That seems slightly more advanced than inverting a binary tree or reversing a string.

The hard question I am referencing is [Median of Two Sorted Arrays](https://leetcode.com/problems/median-of-two-sorted-arrays/).

This was my solution for the ""easy"" question, IMO far more complex than the median of two arrays.

    class Solution {
        public int[][] floodFill(int[][] image, int sr, int sc, int newColor) {
            int row = image.length, col = image[0].length; // STORE GRAPH BOUNDS
            int target = image[sr][sc];                    // STORE VALUE WE ARE AFTER
            Queue<int[]> traverse = new LinkedList<>();
            boolean[][] visited = new boolean[row][col];   // VISITED NODES
            int[][] directions = new int[][]{
                {0,1},{1,0},{0,-1},{-1,0}                  // TRAVERSAL DIRECTIONS
            };       
            int[] tmp = {sr,sc};
            traverse.offer(tmp);                           // PUT START NODE IN TRAVERSAL QUEUE
            
            while(traverse.size()>0){
                int[] node = traverse.poll();              // GRAB NEXT NODE
                int m = node[0], n = node[1];
                visited[m][n] = true;                      // VISITED = TRUE
                image[m][n] = -1;                          // SET VALUE TO CHANGE COLOR
                
                for(int[] dir : directions){
                    int a=m+dir[0]; int b=n+dir[1];
                    
                    if(   a<0 || a>=row
                       || b<0 || b>=col
                       || visited[a][b]==true
                       || image[a][b] != target )          // DISQUALIFY NODE FOR TRAVERSAL
                        continue;
                    else{
                        int[] add = {a,b};
                        traverse.offer(add);              // SATISFIES CONDITIONS PUT IN TRAVERSE QUEUE
                    }
                }
            }
            
            for(int i = 0; i < image.length; i++){
                for(int j = 0; j < image[0].length; j++){
                    if(image[i][j]==-1)
                        image[i][j]=newColor;
                }
            }
            
            return image;
        }
    }",1637276565.0
qwqv67,Can someone help me understand the computer science of what exactly is a database?,Help,3,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qwqv67/can_someone_help_me_understand_the_computer/,9,"So this topic that might be a little closer to computer science, but could someone explain to me what happens on a computer/server when a database is created? Like a high-level overview of the computer science behind the creation of a database and it's properties(how to interact with it)? 

Secondly, if you could also briefly talk about the computer science of database management systems?

Please, also let me know if I'm thinking about this incorrectly. I am a self taught person with a lot of black boxes.",1637245557.0
qw8yg4,A tech journalist just wrote a small article about a project I did. I'm geeked to the max.,Article,73,0.98,computerscience,https://www.hackster.io/news/tyler-jacobs-logic-gate-display-is-a-desk-toy-for-learning-the-basic-building-blocks-of-computing-38ed6ae6cdb6,10,,1637183137.0
qwnv1m,Deadlock vs Deadly Embrace,,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/qwnv1m/deadlock_vs_deadly_embrace/,8,What's the difference between a deadlock and deadly embrace in concurrent programming?,1637235350.0
qw5g3c,Are there any good websites with quizzes where you can check your knowledge on a certain topic and see if you are ready for exams?,Help,89,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qw5g3c/are_there_any_good_websites_with_quizzes_where/,10,"Currently studying Object Oriented Programming which is a course of the 3rd semester in my uni. Im pretty sure I have a good understanding of the concepts but I'd like to confirm that somewhere somehow. There are many times where I wish I had something like that in courses like data bases, networks, data structures. Any help is appreciated!",1637173180.0
qwtqeb,RNG algorithm that just flips bits randomly,Discussion,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/qwtqeb/rng_algorithm_that_just_flips_bits_randomly/,8,"I was thinking about RNG algorithms. And most of them are pretty complicated, using math. Middle-square, Mersenne Twister, etc.

But has anybody come up with an algorithm that just randomly flips binary bits of a variable you want to randomize? What would be the advantages and drawbacks of this method? I guess it would depend on the implementation. 

Anyway, I just wanted to get opinions of people smarter than me.",1637253610.0
qwegv6,IDE Features - Statically vs. Dynamically Typed Languages,,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qwegv6/ide_features_statically_vs_dynamically_typed/,4,"I am trying to find what additional features IDEs provide for statically typed languages that it doesn't for dynamically typed ones for a project. I've tried researching it online, but couldn't come up with anything clear. So, I would appreciate if someone could help with this.",1637199526.0
qw5zd4,How to read the optimal makespan,,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qw5zd4/how_to_read_the_optimal_makespan/,2,"Hello I'm having trouble reading the optimal makespan of job scheduling algorithm. In particular, what does it mean for max to have index i here?

https://preview.redd.it/pir2dkccb7081.png?width=988&format=png&auto=webp&s=05a4c7ad5b716ad7d71419862a04dd142512bd69",1637174685.0
qw8tom,"Is possible too have static typed, interpreted languea",,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qw8tom/is_possible_too_have_static_typed_interpreted/,2,"Type checking insted of hints, and force you to use types",1637182707.0
qvgwfj,What's the most difficult part of writing documentation for you guys?,Discussion,75,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qvgwfj/whats_the_most_difficult_part_of_writing/,23,"Recently started working on a large project which requires I document my code.

Feel free to share which part about writing documentation (writing comments, official documentation, things like that) you find the most annoying or difficult.",1637093532.0
qvi861,What is the different between multi-tasking and parallel processing?,Help,29,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qvi861/what_is_the_different_between_multitasking_and/,8,"Hi,

While revising for my mock, I have unexpectedly became confused about the difference between multi-tasking and parallel processing.

At first, I though multi-tasking is the simultaneous processing of multiple tasks, while parallel processing is the simultaneous processing of the same task but broken down in chunks.

However, after reading about SIMD and MIMD, I realised that parallel processing could also process multiple task at the same time, just like multi-tasking.

Thanks in advance :)",1637097069.0
qvcvqa,"Books on bios, uefi, smbios and acpi?",Help,18,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qvcvqa/books_on_bios_uefi_smbios_and_acpi/,4,"I would like to learn a bit more about firmwares and their interactions with operating systems, especially on everyday computers.

After reading, I would like to have a good overview of what types of firmwares are common on the market, how they interact with with the OS and how they are structured. Ideally without reading all the specs, I just want go get an overview at first :) 

Any recommendations?",1637083224.0
qunqn2,"50 Years Ago Today (November 15, 1971) Intel releases the world's first microprocessor, the Intel 4004.",,716,0.99,computerscience,https://i.redd.it/opv4njiy4tz71.jpg,14,,1637003033.0
qv9yun,Does anyone have tips for understanding code?,Help,16,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qv9yun/does_anyone_have_tips_for_understanding_code/,11,I just started studying computer science and have trouble understanding code. We've started with java and python and even though I have some experience with java I still have a hard time reading and understanding the code. Every time I look for tutorials they always go directly into coding and creating an algorithm (which is understandable - learning by doing) but I'd like to understand how to learn how to read and understand an algorithm first. Does anyone know a good platform on how to learn exactly that?,1637075641.0
qvhov8,Bit Manipulations You Like,,6,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/qvhov8/bit_manipulations_you_like/,8,"Bit manipulations is one of the most interesting subjects I ever studied. It really blows my mind.

I'm wondering if you use those occasionally, and if so what are your favorite tricks?",1637095647.0
qvrimr,What is a DAG (Directed Acyclic Graph) and how are they represented in memory,,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/qvrimr/what_is_a_dag_directed_acyclic_graph_and_how_are/,8,"I have an interview tomorrow. 

This question was recently asked to one of my friend who interviewed at the same company. 

I searched really hard but couldn't find the answer to the latter part of the question. 


Even for the first part, I need a simple answer and not a wiki kinda definition. 

Any help is appreciated.",1637125263.0
qvgo6i,How does the communication between devices with cables work?,Help,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qvgo6i/how_does_the_communication_between_devices_with/,2,"I would like to get a Raspberry Pi in the near future and a receiver and transmitter for electromagnetic waves. I have to connect the receiver and the transmitter to the Raspberry Pi with cables, but now I'm wondering how exactly this works, for example how the transmitter knows what data I want to send?",1637092938.0
qv9g3m,Can't find recent research paper over data structure,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qv9g3m/cant_find_recent_research_paper_over_data/,2,"Hi everybody, for my enflish class, I need to read 7 articles over data structure (it was imposed to me) with a restriction of 500/600 words and  should be published after may 2021. I don't know where to search, the most part of articles that I found on internet is over neural structure in the brain or is too old or too long (+-4500 words). Thank you for reading.",1637074201.0
qv3ej2,quantum computing,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qv3ej2/quantum_computing/,0,"Watching the xponential rise of technological development, I havent heard about quantum computing for quite a while. What is the progress atm? And are just the big companies working on it? Wondering if there are small startups as well?",1637052222.0
qulcc1,Display and Keyboard (DSKY) used on the apollo spacecrafts,General,7,0.82,computerscience,https://i.redd.it/2q09n3wslsz71.jpg,0,,1636996589.0
quwl9r,"Why do applications always download so fast in the start, say the first 99%, and the last 1% take so long?",Discussion,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/quwl9r/why_do_applications_always_download_so_fast_in/,2,,1637028143.0
qtrrml,My Programming prof is an absolute Chad,,515,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qtrrml/my_programming_prof_is_an_absolute_chad/,96,"He programs everything using the text editor and he doesn't even prepare the code before the lectures. He just quickly programs it all on the spot in front of us. And not just a few lines but often around 20 for each example. And even if a student asks him to show a very specific example that guy just immediately knows what to do and starts typing without mistakes. He also opens every document and application over the terminal instead of clicking on it. 

&#x200B;

He honestly inspires me to actually take my assignments seriously. It seems like he always immediately knows the solution to any coding task in his head. That guy must be a genius.",1636902666.0
qttk9c,Early programmers were reluctant to switch from binary programming to an assembly language,,71,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/qttk9c/early_programmers_were_reluctant_to_switch_from/,14,"I'm currently reading ""The Art of Doing Science and Engineering: Learning to Learn"" by Richard Hamming. It's a great book about computer history and programming in general.

One thing that strikes me is the omnipresent status quo bias.  Early programmers were reluctant to switch from binary programming to an assembly language:

https://preview.redd.it/vos58v5f9lz71.jpg?width=2787&format=pjpg&auto=webp&s=a555d1663ead77991c4e2f1cceeb825d8caa774d

The same happened during the transition from assembly to FORTRAN, one of the first high-level, compiled programming languages:

https://preview.redd.it/gkxkw4ji9lz71.jpg?width=2910&format=pjpg&auto=webp&s=7a76bc3ea7597597d8fc9cc562785a5e3c28b3b8

Have you experienced a similar thing when new frameworks, tools, or languages were introduced?

&#x200B;

>Source: [https://twitter.com/krebs\_adrian/status/1459909375427059715](https://twitter.com/krebs_adrian/status/1459909375427059715)

&#x200B;",1636907791.0
qu8lzi,Good resources to learn about time complexity?,Help,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qu8lzi/good_resources_to_learn_about_time_complexity/,2,Does anyone have a good resource to learn about time complexity? I am struggling with the material and can't wrap my hear around anything besides O(1) and O(n).,1636952514.0
qu4c71,I've always loved the concept of instant learning and I want to study it more.,General,7,0.74,computerscience,https://www.reddit.com/r/computerscience/comments/qu4c71/ive_always_loved_the_concept_of_instant_learning/,8,"One of the things that fascinated me the most in the Cyberpunk genre, was the ability to have instant learning, such as when Neo learns how to fight in Matrix, or when in Neuromancer you had those chips in the back of your ear, that made you a instant fluent Chinese speaker. Knowing the existence of biocomputing, and those biomechanical arms that responds to brain impulses, I started asking myself if there wouldn't be any papers that could help me grasp an idea of real life applications between computing, the human body and the learning process... Is there any book or paper recommendation where I could follow?",1636938467.0
quec2x,Can you recomend papers about reinforcement learning applied to games?,,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/quec2x/can_you_recomend_papers_about_reinforcement/,1,Can you recomend papers about reinforcement learning applied to games?,1636975603.0
quk6my,what‚Äôs the difference?,Help,0,0.29,computerscience,https://www.reddit.com/r/computerscience/comments/quk6my/whats_the_difference/,3,"are there any differences between computer science, programming, software engineering, and software developing? i‚Äôve been using google and doing my own research, but each website i look at gives me a different answer and leaves me more confused",1636993510.0
qu426c,Why is LaTeX still used academically?,Discussion,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qu426c/why_is_latex_still_used_academically/,15,"I can understand the usage of it for the time it was created, but surely in recent years with the introduction of many other word processors IE Word and Gdocs, it would be made redundant? The inclusion of both equation and formatting tools in Gdocs is really good, so why do academics still use LaTeX?",1636937585.0
qt4qqy,(1964) Engineer Karen Leadlay working on the analog computers in the space division of General Dynamics,,587,0.99,computerscience,https://i.redd.it/c1lkr0t5s8z71.jpg,26,,1636821044.0
qtl1ir,Approximating T(n-2) and T(n-1),Help,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qtl1ir/approximating_tn2_and_tn1/,1,"Based on what I've read to get the closed form of the Recursive Fibonacci Algorithm, you have to approximate `T(n-2)` to `T(n-1)` from `T(n) = T(n-2) + T(n-1) + c` and vice versa to eventually reach O(2^(n)).

The thing that's confusing me is **why** we should approximate it. The only thing I know so far is that T(n-2) will take lesser time than T(n-1), but I'm just not sure of the mathematical explanation behind approximating both times to make them somewhat equal.",1636876550.0
qtkimi,"Best books to understand Computer Architecture, threads, concurrency, processes and everything else that goes under the hood",,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qtkimi/best_books_to_understand_computer_architecture/,5,"Hi!   
I'm looking get a strong grip on the inner workings of CPUs, threading, concurrency, instructions , SIMDs and etc..  


Right now I have a shallow understanding of the matter, Im looking for a new intro book into the whole thing, nothing very very technical - as I am starting to program with RUST and get to grips with Assembly language.  


Basically, how does programming gets executed on the physical level, and how it all comes together to create these fascinating machines. Thank you!",1636874256.0
qtg8ok,Is there a formula to calculate the magnitude of precision loss with respect to the magnitude of a number presented by a float?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qtg8ok/is_there_a_formula_to_calculate_the_magnitude_of/,0,"We know that a float will lose precision as it's value increases. But is there a way to calculate when floats will be come, for example, at least one whole unit off? 

For example this algorithm would tell me that numbers of magnitude of 10\^9 will likely be off by a magnitude of 10\^-2.",1636857763.0
qtg70j,"I heard that if the first digit of a hexidecimal number is 8,9,A,B,C,D,E or F (and it‚Äôs a multiple of 4) then it‚Äôs negative. Is this true? And if so, why?",Help,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qtg70j/i_heard_that_if_the_first_digit_of_a_hexidecimal/,10,,1636857609.0
qtet0i,What does K mean in O(nk) or O(n+k),Discussion,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qtet0i/what_does_k_mean_in_onk_or_onk/,2,"**So for the context:**

If i'm assuming right the ""n"" stands for input or size of the array and when one says O(n) it means the statment will take ""**n""** number of times to complete it....

but what does ""**k""** mean?

[here](https://stackoverflow.com/q/27301287/5630533) op said this ‚¨áÔ∏è

&#x200B;

[https:\/\/stackoverflow.com\/q\/27301287\/5630533](https://preview.redd.it/jmrx4r07ogz71.png?width=1272&format=png&auto=webp&s=fdc3bd65cf25418a5eaa893c1f28f4fcf0ffd352)

Does that mean ""**k""** is just an another (secondary) notation for ""**n""**

&#x200B;

&#x200B;

[sorry, couldn't find the original link](https://preview.redd.it/0kxd4sr6mgz71.jpg?width=1010&format=pjpg&auto=webp&s=16283f1b1acb8e318c710fae010e53fccac5aebc)

&#x200B;

Referencing the above table, how much would be the difference between cubic and polynomial if n is 10 and k is 0(can we even assume 0 for ""**k**"" value)?",1636852787.0
qtapfy,Any goods websites/newsletters for IT/CS news and research papers?,Article,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qtapfy/any_goods_websitesnewsletters_for_itcs_news_and/,1,"As the title suggests, as an undergraduate CS student, I'd like to really improve my knowledge on the ongoing research, hardware improvement, Linux news, security, etc. by reading more articles. I was wondering if some fellow redditors had anything to share?

I may also consider creating an RSS stream.",1636839302.0
qt3e8y,How do computers know which binary values represent which numbers?,,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qt3e8y/how_do_computers_know_which_binary_values/,11,"In computer science, we learn that computers only understand base 2, essentially binary. But how do computers actually know which numbers are which? For example, the number 00000011. How does the computer know the the 1st '1'(from the right to left) represents 1(in denary), and the second '1' represents 2, which adds up to 3? I'm hoping for a low level explanation. Thank you!",1636816896.0
qsn40p,What is the difference between synchronous and asynchronous sequential circuits?,Help,31,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qsn40p/what_is_the_difference_between_synchronous_and/,3,"Hi everyone! First of all, I'm sorry if this post can be a little bit confusing, I'm trying to gather all I have understood so far to make it as clear as possible.

So, I am attending a Logical Networks class, and we started some days ago talking about sequential circuits. Today, our professor introduced us to synchronous and asynchronous circuits, but not in the same way I found online.

He introduced them by saying this:
Basically an asynchronous circuit is a circuit that whatever the state and the input (q1,I) you consider, will always end up in a steady state (qn, I), where if you keep I as an input you'll keep staying in the qn state.

And, contrary to this, he told us that synchronous circuits are circuits where not all states and inputs have this property.

So what does that specifically mean? I struggle to understand the concept, as some of the example he gave us for synchronous circuits (one of them was a machine that was able to recognise if there was a 101 sequence in the input) seemed to have the steady state property I talked about before, so I think I might be missing something.

That said, he then explained us that for that reason (not every state with an input I will end up in a steady state) we need an external synching (a clock) and a memory to make a state steady. Why is that? And why don't we need that for asynchronous circuits?

Lastly, he talked about how some of these machines need just an impulse or a longer signal to work (and for example an asynchronous machine cannot recognise a '00' input as it operates when an input changes, so it can only tell that the last input was a (for example) 1 and this one is a 0). How do you tell that? 

Hope this was clear, thank you for everyone who chooses to spend their time to help me understand :)

Oh and, don't take my words for something so accurate of what our professor told us, since I am trying to rephrase these things from what i have understood.",1636757693.0
qs8dso,What‚Äôs the difference between programming and computer science?,Help,86,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qs8dso/whats_the_difference_between_programming_and/,38,I‚Äôm going to take introductory classes at my uni and there‚Äôs two diff options,1636712906.0
qsis3v,"How are supercomputers utilized and how does one present a usecase for the machine(not that I want to, just curious)?",,16,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qsis3v/how_are_supercomputers_utilized_and_how_does_one/,4,"I've been in a development capacity at my job for about 4 years. I work on the bulk of the SQL and c# interactions on our flagship products. So I have a pretty good understanding of what a supercomputer is and what they are good at. (Not saying my job gives me that knowledge, just saying I work in the industry and pay pretty close attention to these topics for fun) So, what I'm most curious about are the NVIDIA, INTEL, IBM, etc companies that build these things and how they decide what application the machine is used for. Like radio telescopes, for example, I know universities, companies, really any organization can apply for dedicated time with the telescopes. Is it similar to that? Can someone submit a proof of concept to these corporations and then get approved or denied? Or do all projects that use the computers just come from their internal teams? I recently saw a future growth model of earth coming out NVIDIA's supercomputer. It showed weather patterns, climate shifts, all sorts of really cool things. So I just kind of got to thinking about the origins of these projects and the process. I checked the rules out and I think this is an okay question for this sub, if not, I can post it elsewhere. Thanks!",1636745237.0
qs8gkb,How do you read your books?,,12,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qs8gkb/how_do_you_read_your_books/,12,"Cs has a lot of great material, so i thought of figuring out a strategy to learn it better, do you read a book like a novel for the first time, and then take notes? How often do you repeat your material etc",1636713262.0
qsg140,Are personal IP addresses stored on DNS?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qsg140/are_personal_ip_addresses_stored_on_dns/,4,"I have a very superficial understanding of how DNS works. If you type ""[google.com](https://google.com)"" into the browser input, the computer asks a server in the DNS for google's IP address, the server either answers or asks another server for help, and eventually tells your computer where to go. 

So in my mind, these servers have a sort of ledger like ""[google.com](https://google.com) : [216.58.223.255](https://216.58.223.255) "" 

But what about my own public IP, for example? Is that stored in there as well? If so, what information is associated with it? In other words, how the server be asked to look it up?",1636737568.0
qqpf3o,What field of CS deals with cutting-edge research in creating better document formats like pdf?,,55,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/qqpf3o/what_field_of_cs_deals_with_cuttingedge_research/,19,"In particular, is there a journal analogous to ACM Transactions on Graphics (which publishes cutting-edge discoveries in the field of computer graphics), which publishes articles related to more portable, robust, efficient file formats for viewable documents like pdfs? 

Is there any chance new discoveries are made (e.g. more efficient algorithms to compress documents), or is this only a purview of companies like Adobe? I hope I'm making sense.",1636531399.0
qqxc02,Can someone point me in the direction of some sort of algorithm that converts noisy analog data into binary,Help,9,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qqxc02/can_someone_point_me_in_the_direction_of_some/,10,"I am working with a cheap EEG sensor that outputs attention levels. This data has some noise (fluctuations). I want to turn a LED on and off using this data. I am currently considering taking a mean of x values and if the mean is higher than a threshold, it's a 1 else 0. This is kind of naive and I was wondering if there are better algorithms out there for such problems?",1636559365.0
qqvz15,Information on a Conference,,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qqvz15/information_on_a_conference/,3,"Does anyone know if the SocProS conference is a respectable conference?
I know that the submitted papers are published by Springer.",1636555474.0
qrlsrw,Everyone learning programming/computer science how do you deal with the fact that AI may be making you obsolete in a few years as you are busting your ass learning technologies and tools that may turn out to be almost useless?,,0,0.18,computerscience,https://www.reddit.com/r/computerscience/comments/qrlsrw/everyone_learning_programmingcomputer_science_how/,28,"I‚Äôve been self teaching over a year, typical front end shit, but I do a lot or reading from programmers about AI tech replacing the future of coding jobs-makes me almost wish I chose a field that is not stable like plumbing or health.",1636638817.0
qr8cpv,SWE influencers/pages you follow on social media?,General,0,0.29,computerscience,https://www.reddit.com/r/computerscience/comments/qr8cpv/swe_influencerspages_you_follow_on_social_media/,8,"Who are the some social media influencers/pages (could be a meme page even) for SWE? Or maybe some of your favorite influencers for SWE that you follow?

Could be IG/TikTok/Youtube/LinkedIn/etc.

Thanks!",1636590290.0
qqpsqd,A terrible schema from a clueless programmer,,0,0.5,computerscience,https://rachelbythebay.com/w/2021/11/06/sql/,0,,1636533030.0
qpu9g8,How do CPUs actually work ?,General,139,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qpu9g8/how_do_cpus_actually_work/,54,"I'm curious as to how CPUs actually communicate with the outside world.

I know it's electric currents flowing through transistors at the lowest level.

I know you can run compiled programs.

But where is the transition layer from one to the other ?

For example, Intel creates a CPU and I write a program for it, let's say in Assembly. I write the command: mov eax, \[ebx\] .

Who translates 'mov' into what it actually does and who knows how to interpret that command ? Is there a dictionary somewhere that says mov in binary is 0011010010011 (made up obv) and when you encounter that string of binary code you perform this function ?

Even more, how can this be hardwired onto a CPU ? Is mov a physical circuit that gets called whenever that command string is encountered ?

Sorry for the avalanche of questions but this particular topic is very hazy to me. I can't picture how a written command can direct electric current down in the CPU to do certain things.",1636426967.0
qpl4wt,Impact of computational science in research?,,22,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/qpl4wt/impact_of_computational_science_in_research/,13,Can anyone provide me with some examples of the impact computational science has had on science and research? How many advancements have been made with direct help from CS in thr past decade?,1636399760.0
qp5vod,"Can someone help better explain what‚Äôs going here? (From the book, ‚ÄúThe Art of Computer Programming‚Äù by Donald Knuth",,116,0.97,computerscience,https://i.redd.it/2dkaibv8tay71.jpg,16,,1636345321.0
qpr8lq,Trying to read server memory bytes via postgresql vulnerability in Kali-Linux VM,,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qpr8lq/trying_to_read_server_memory_bytes_via_postgresql/,1,"
Basically I found this vulnerability and I want to perform it on my system. I installed this virtual machine (64-bit for VirtualBox):    https://www.kali.org/get-kali/#kali-virtual-machines      and here is the vulnerability: https://security.snyk.io/vuln/SNYK-DEBIAN11-POSTGRESQL13-1292197         I‚Äôm really confused how is even do this, so far I know how to start postgresql and I can create a table in a database. If anyone could assist me in anyway I‚Äôd really appreciate it.",1636417260.0
qpjwrs,Question about the debate of how to end a string?,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qpjwrs/question_about_the_debate_of_how_to_end_a_string/,5,"I am an IT student, but [I'm watching this video](https://youtu.be/0fw5Cyh21TE) for fun and he explains the way developers can end a string (if that's the proper verbage). He explains a couple of different ways, the first being that the developer tells the computer how many characters they're looking for. He also explains that this is not efficient use of memory and then explains the second method, where developers would simply stick a \[NULL\] character at the end of the string. He also says that there are debates as to which method is better to use, but to someone with a pretty solid education with computers, I don't see why we don't just use a \[NULL\] character at the end of every string if it's so much more efficient than the alternative.",1636396422.0
qosr3u,Standardized Methods for Monte Carlo Markov Chain (MCMC) Sampling Using Computer Language,,11,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/qosr3u/standardized_methods_for_monte_carlo_markov_chain/,0,"Hello!

I have been trying to research standard ways to perform sampling (MCMC) high density regions of multidimensional probability distributions using computer programs. As far as I can find, there do not seem to be any standardized ways of performing this sampling - and often requires the user to translate the mathematical algorithm into computer code.

I tried to attempt the implement this algorithm over : [https://stackoverflow.com/questions/69849254/r-standard-method-for-mcmc-monte-carlo-markov-chain-sampling](https://stackoverflow.com/questions/69849254/r-standard-method-for-mcmc-monte-carlo-markov-chain-sampling)

Can someone please take a look at it and let me know if this is correct?

Thanks!",1636304620.0
qon555,Computer Science in pop culture,,22,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/qon555/computer_science_in_pop_culture/,12,"Hello, people. English is not my first language, so, sorry for any grammatical mistake.

&#x200B;

Have you ever saw any tv show, or movie, or video game, with any direct reference to a subject from computer science?

&#x200B;

I see a lot of references to Artificial Intelligence and Hacking, but few to other areas of CS. I'm curious to see if there are references to other areas as well.",1636286480.0
qoyr6v,External version of RamDisk,Discussion,3,0.64,computerscience,https://www.reddit.com/r/computerscience/comments/qoyr6v/external_version_of_ramdisk/,6,"I've been thinking about starting a project to repurpose old RAM from previously upgraded laptops and desktops. I'd like to see if I can create an enclosure for holding multiple RAM chips to be connected to a computer via USB or thunderbolt, and configuring them as an external disk. Using software similar or even the very same as this below. [http://www.radeonramdisk.com/software_downloads.php](http://www.radeonramdisk.com/software_downloads.php)

Can anyone point out the burdens and obstacles that I may have to cope with and overcome in order to make a idea like this a reality. 

Note: If your going to tell me this isn't possible please don't bother, I need constructive ideas here only, the fact that this software exist is proof that a ram disk is possible, I just want to make it external.",1636321907.0
qoj0r4,Is it possible to encrypt an SSD/HDD/USB or even a whole PC based on geographical location?,,17,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/qoj0r4/is_it_possible_to_encrypt_an_ssdhddusb_or_even_a/,10,"So for instance if I wanted to encrypt one of the above or multiple based on a specific location so it would only be unlocked if the password was entered while at a specific location such as let‚Äôs say altitude, so the PC would have to be at a certain height to then be able to be unlocked with the password, is this possible?

Because if it is, this would mean that essentially you can completely insure that even if they got the password right it still would not work, because they are not at the location which would be required. 

I‚Äôve tried to find out by doing a quick internet search but no results show. 

Any information would be helpful. 

(This is just for my personal curiosity.)",1636267434.0
qoqb6q,Non-computable processes or functions,,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qoqb6q/noncomputable_processes_or_functions/,3,"Are there any resources on what are non-computable processes or functions and how do they look like. 

I am trying to search for them, but more often than not, something on the topic of Consciousness or Roger Penrose comes up.

I want to understand what is not a computation and I'm pretty sure there is a huge amount of research on such a topic.

Thanks.",1636297414.0
qo5xff,What algorithm could I use to find optimal selection of sets to maximize unique elements,,36,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/qo5xff/what_algorithm_could_i_use_to_find_optimal/,31,"Hello there!

Could someone tell me which algorithm should I use in order to do the following:

I have X groups with different number of elements in them and I want to find the best combination of n chosen groups in order to maximize the number of unique elements (alternative version maybe even choosing which elements).

Should I use some optimization algorithm or something or is it easier than that?

Thanks in advance!

**Edit (to add maybe a more accurate description of what I'm searching for):**

Parameters:

I) collection of sets

II) n - size of group to consider

III) (Optional) - list of elements

Output: which n sets from collection should be selected in order to maximize unique elements (if parameter III is given, consider only elements from that list) considering all sets from collection.

**An interesting case I think worth mentioning** (made me wonder it might be optimization problem and not a brute force that would check for set with more unique elements and go next for  set with more unique elements disconsidering previous added):

Set 1: {A,B,C,D,E,F}

Set 2: {A,B,C,G,H}

Set 3: {D,E,F,I,J}

for n = 2, optimal collection here would be sets 2 and 3.",1636222460.0
qoqmin,"China's New Quantum Computer Has 1 Million Times the Power of Google's, the world fastest.",,0,0.41,computerscience,https://interestingengineering.com/chinas-new-quantum-computer-has-1-million-times-the-power-of-googles,4,,1636298416.0
qoh47t,Theoretical Proofs for Evolutionary Computing,,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qoh47t/theoretical_proofs_for_evolutionary_computing/,2,"Has anyone ever heard of ""evolutionary computing"" and ""genetic algorithms""? They are computer algorithms used in optimization that were inspired by phenomena occurring in nature.

I posted a detailed question on stackoverflow relating to their theoretical justifications : [https://math.stackexchange.com/questions/4295279/does-the-following-computer-science-optimization-theorem-have-a-proof](https://math.stackexchange.com/questions/4295279/does-the-following-computer-science-optimization-theorem-have-a-proof)

Can someone please take a look at it? 

Thanks!",1636259294.0
qoj64n,Can the internet exist without wired data transfer?,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qoj64n/can_the_internet_exist_without_wired_data_transfer/,5,"I know how Bluetooth works. It is completely wireless. In the case of the internet, let's say we are talking about Google servers. They have an IP address and some information stored in them. My PC has an IP address. Can it somehow get the information from the Google servers without any wired communication between intermediary devices?",1636268103.0
qoifaz,Why do we initialize the table to 0 in the knapsack problem at the start?,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qoifaz/why_do_we_initialize_the_table_to_0_in_the/,2,"Video: https://youtu.be/PLJHuErj-Tw
Why do we initialize the table to 0 in the knapsack problem at the start?",1636264749.0
qnl3xe,How does the ML research done in Stats departments differ from the ML research done in CS departments? What areas are empathized more/less in either one?[D],,52,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/qnl3xe/how_does_the_ml_research_done_in_stats/,6,,1636147706.0
qnj83v,"Is ""memoization"" also called ""memorization"" in computer programming?",Discussion,50,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/qnj83v/is_memoization_also_called_memorization_in/,25,"Is ""memoization"" also called ""memorization"" in computer programming?",1636142182.0
qnduwn,Is embedded DRAM the future?,Discussion,41,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qnduwn/is_embedded_dram_the_future/,10,"With the Applr M1 processor using integrated DRAM on the SoC, with affords faster memory access, do you think this will be adopted by other cpu companies? Is it likely x86 cpus will be manufactured with embedded dram in the next few years?",1636126949.0
qn56a6,"Question in regards to stored memory. Was curious about this for a while, if I were to copy paste a photo or just a folder or a notepad document, when I make a duplicate, is it the exact same data down to the T with every 0 and 1 being the same? Are they truly identical in this manner?",General,25,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qn56a6/question_in_regards_to_stored_memory_was_curious/,13,,1636094567.0
qna9yb,computer engineering vs computer science,,8,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/qna9yb/computer_engineering_vs_computer_science/,63,Most of people will become engineers tbh. Few people will invent a new algorithm or prove a computing theory. Most people will deal with writing simple CRUD code in certain business. Why do we keep calling CS not CE?,1636116142.0
qmvvyw,Whats the highest level programming language you can program an operating system in?,Discussion,17,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qmvvyw/whats_the_highest_level_programming_language_you/,12,"Is the go-to language C? or has it been upped? and do you think this will change in the future? or does it even matter what language is used? Can we build an OS from Python?

I guess I'm asking programming languages that best communicate with modern hardware architectures",1636063345.0
qn6kqf,What happens when you open a connection?,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qn6kqf/what_happens_when_you_open_a_connection/,4,"I‚Äôm curious what actually happens when I open a connection from a client to something like a backend or database. Like, what is a connection actually? I‚Äôve been struggling finding out how to understand the nitty gritty details of how a connection actually works. Thanks",1636101113.0
qm9wc6,Travelling Salesman of world map visualisation,,83,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qm9wc6/travelling_salesman_of_world_map_visualisation/,13,"&#x200B;

[This uses the simulated annealing algorithm - source code in comments](https://i.redd.it/9fjlbchahhx71.gif)",1635990296.0
qmuxrz,Do all compilers output the same machine language?,Help,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qmuxrz/do_all_compilers_output_the_same_machine_language/,10,"Are all machine laguages the same on a single computer architecture even when written in different programming languages?

Like is the instruction of if (x==2) then print(x) the same machine language for C and python?",1636060799.0
qmil8h,"Been reading Computer organization and design, can't understand a sentence",Help,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/qmil8h/been_reading_computer_organization_and_design/,2,"As the title says, i've read some chapters of a book called **Computer Organization and Design: The Hardware/Software Interface,** finished the part about variables, registers and memory interactions. But this sentence completely staggers me, can someone elaborate/explain(english is my 2nd language  )  


The sentence:  


**Many times a program will use a constant in an operation‚Äîfor example, incrementing an index to point to the next element of an array**",1636025659.0
qmtz3q,OS Installation Sequence,Help,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/qmtz3q/os_installation_sequence/,1,"When you install an operating system from a USB. Am I correct that it installs it from the USB then copies itself over to the harddrive? Or does it copy itself to the harddrive first then install the operating system from there?

Second question is, when the OS is copies over to the harddrive is that piece of hardware essentially your whole computer? Like if I swapped out two laptops harddrives from one to another, would they both just boot up as if they were switched?",1636058232.0
qm0yhw,How are emails sent if they are stored as hashes in a database?,,24,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qm0yhw/how_are_emails_sent_if_they_are_stored_as_hashes/,16,"Let's say I have a newsletter people can sign up to. When a user signs up, I hash their email and store it in the database. What I don't understand is how I would then use the hash (I no longer have access to the actual email as I've hashed it) to send my newsletter to them.

Also sorry if this is the wrong sub, I'm not entirely sure where else to post this. Thanks!",1635964423.0
qm0bpt,Thoughts on analog AI accelerators?,Discussion,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qm0bpt/thoughts_on_analog_ai_accelerators/,4,"It's almost time for me to select a master's program within Computer Engineering/Computer Science. I want to do something valuable and so I've been thinking of going into high performance computing and doing research on applying analog computing to AI hardware. 

Does anyone have any similar experiences or thoughts on the subject?

Edit: For those that have never heard about the concept before, [here's](https://www.youtube.com/watch?v=owe9cPEdm7k) a good introductory video",1635962672.0
qlvaqy,How does one go from working on console programs to making things like apps/games/etc?,,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qlvaqy/how_does_one_go_from_working_on_console_programs/,7,I am a second year CS major and the language I have the most experience with is C++. How do I begin making stuff with visuals? Are there any online courses that can help me?,1635948582.0
qlzztd,transfer protocol rdt2.1 related question,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qlzztd/transfer_protocol_rdt21_related_question/,5,"Does 1 sequence bit really suffice for a reliable transfer?  
Are there still no scenarios of endless loops or corrupted( and accepted ) packages?  
I'm confused because my intuition tells me it reduces the risk of failure but does not eliminate it.

Edit: Adding reference images.

https://preview.redd.it/z6pubzgr9fx71.png?width=1205&format=png&auto=webp&s=6d4ca9f2d379d36304471d96cbd78b4e15b36bc9

&#x200B;

https://preview.redd.it/re3n52iu9fx71.png?width=1200&format=png&auto=webp&s=cd4941c5aa72cd683339176285d23a1f96aedfbd",1635961753.0
qlj8jr,Looking for an obscure language.,Advice,10,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qlj8jr/looking_for_an_obscure_language/,1,"A few years ago I worked on a project with a friend of mine and I remember we used a language based off futurama, I believe the language was some kind of fork of Ruby. Not sure though. Can't seem to find it anywhere, anyone know what that could be? Thanks! (It was a web app if that is of any use!)",1635901899.0
ql9rw5,CLRS,Discussion,38,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/ql9rw5/clrs/,16,"Any tips about needed math or study methods that can help me to finish this book faster, and get as much as possible of the content? And for those who've already read, what is your opinion about the book?

https://preview.redd.it/oeq7bem5y7x71.jpg?width=1536&format=pjpg&auto=webp&s=95f151988914e7e222476c7d7e76e6882760d47d",1635875109.0
qkkwwt,Why is the following code O(n)?,,45,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qkkwwt/why_is_the_following_code_on/,23,"    def f(n):
        c = 0
        for i in range(n):
            j = i 
            while j < n:
                  j = 2*j
                  c = c + 1

 

My professor tells me this is O(n), but his explanation did not make sense to me. The way I see it, the complexity boils down to log(n) + log(n-1) + log(n-2) + ... log(n-n) and I don't see how that is asymptotically n rather than nlog(n).",1635792943.0
qkc8u6,Build an 8-bit retro computer powered by a Z80 !,General,90,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/qkc8u6/build_an_8bit_retro_computer_powered_by_a_z80/,10,"Hello computer sciences lovers,

Let me join your community by presenting to you a project I've always wanted to make (or simply have) since my young age but never tried until now. I am currently designing an¬†**8-bit computer**¬†with Zilog¬†**Z80 CPU**.¬†**It is not an emulation, and not powered by MCU**. I hope that it will be both entertaining and educational: ideally it would be used to learn¬†**hardware, programming in assembly, Basic and even play 2D games**¬†etc.

[Zeal 8 bit computer prototype](https://preview.redd.it/aedi2s0xyyw71.jpg?width=1080&format=pjpg&auto=webp&s=13bafe0aa61808c9389aaaf5f6a41cecc72259f7)

**Now the key features already working:**

¬∑ Native OS fully written in Z80 assembly

¬∑ ROM and RAM support with banking (both internal and external)

¬∑ Support external extension card (for adding RAM, ROM, Flash, EEPROM, and so on...)

¬∑ PS/2 keyboard support (targeting a full 104-key keyboards support)

¬∑ 16 GPIO pins (some used by the system)

¬∑ Software I2C

¬∑ Software UART

¬∑ VGA graphics support (powered by an¬†**FPGA**)

¬∑ Sound support (powered by the FPGA)

Here is my video showing the design process:

[https://www.youtube.com/watch?v=n\_eEDAQWMdY&t=25s](https://www.youtube.com/watch?v=n_eEDAQWMdY&t=25s)

Feel free to give me your feedback or remarks",1635767382.0
qjz6f7,Any Really Good Computer Science or Coding Channels on YT?,Advice,147,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qjz6f7/any_really_good_computer_science_or_coding/,55,"Any good YouTube channels for new people learning coding and coding fundamentals. I watch lots of math videos on YT and if anyone where to recommend me for math channels I would say 1blue3brown, Veritasium (sometimes). I was wondering If anyone knows any good channels that doesn't sticky teach how to learn a certain langue step by step but more deep understandings and good advice that I will keep back in my head as I keep learning to code. Interesting topics as well, like those math channels. Thanks",1635715720.0
qk9vxt,Why Embedded Software Development is Harder,,3,1.0,computerscience,https://beza1e1.tuxen.de/embedded.html,1,,1635757149.0
qk5g35,How do I get machine learning to be gamified and interesting?,Help,3,0.62,computerscience,https://www.reddit.com/r/computerscience/comments/qk5g35/how_do_i_get_machine_learning_to_be_gamified_and/,1,Are there any websites like hackthebox for Machine learning which make it fun and challenging? I want to constantly do projects and get my hands dirty in it while having fun.,1635737407.0
qjk3eb,What is this data structure called?,,113,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/qjk3eb/what_is_this_data_structure_called/,35,"Imagine you have a List or Array like this:


`[1,1,1,1,9,9,9,9,9,5,5,5,6,6,7,7,7,7,7,7,7,0,0,0,12,12]`

Then it may be beneficial to display it like this, e.g. for compression purposes:

`[(1,4),(9,5),(5,3),(6,2),(7,7),(0,3),(12,2)]`

or even like this:

`[1,4,9,5,5,3,6,2,7,7,0,3,12,2]`

On the left side of the tuple you put the element on the right side how many times it occurs in a row. What is this structure called?",1635663569.0
qjrkb9,Is compression just a way to express the same data in a much much simpler/cheaper way?,General,5,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/qjrkb9/is_compression_just_a_way_to_express_the_same/,3,Title says it all.,1635693250.0
qjwzwd,Can someone help understand this algorithm?,Help,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qjwzwd/can_someone_help_understand_this_algorithm/,1," 

    M ‚Üê ‚àÖ        ‚ñ∑ Initialize with an empty matching P ‚Üê getAugmentingPath(G,M) 
    while P is not empty do 
    M ‚Üê M ‚äï P 
    P ‚Üê getAugmentingPath(G, M)

 

This is an algorithm for maximum  matching. however, what I don't understand is that before the iteration  loop starts,

 what exactly is P getting intialized with? 

Since M is set  to nil and we are passing that as a parameter.

Also can someone give an example output of what kind of return value we expect from the getAugmentingPath function?

&#x200B;

For example:

&#x200B;

https://preview.redd.it/0ma28i369uw71.png?width=457&format=png&auto=webp&s=37067d91a4322c56a61fe503e397675ffa32ace2

What would be the output of the augmentingPath function in the first few iterations? Someone please help as I can not understand this at all.",1635709068.0
qjntyw,Chinese Researchers Built a Quantum Supercomputer 10 Million Times Faster Than Previous Systems,,3,0.62,computerscience,https://science-news.co/chinese-researchers-built-a-quantum-supercomputer-10-million-times-faster-than-previous-systems/,5,,1635680652.0
qj9666,Feel a bit incomplete on insertion sort,,17,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/qj9666/feel_a_bit_incomplete_on_insertion_sort/,11,"Was reading the CLRS book section regarding insertion sort, and it gives the following code for insertion sort:

    INSERTION-SORT(A)
    for j = 2 to A.length{
        key = A[j]
        i = j-1
        while i > 0 and A[i] > key{
            A[i+1] = A[i]
            i = i-1
        }
    A[i+1] = key
    }

CLRS gives the first loop invariant saying that A\[1..j-1\] are in sorted order.  But I'm not entirely convinced that's the case from a mathematical standpoint.  Intuitively looking at an example it is pretty clear that A\[1...j-1\] is in sorted order, but I'm wondering what could be the loop invariant for the inner while loop, since I'd assume its why the loop invariant CLRS gave holds.  Not the most useful question from a practical standpoint but I really like math and am curious what the loop invariant is for the inner loop.  Also not sure why CLRS would leave it out.

EDIT: formatting",1635623589.0
qjk2pi,How do you even solve programs?,Discussion,2,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qjk2pi/how_do_you_even_solve_programs/,3,"I have recently passed my algorithm course where I learned about 15 algorithms. 

My professor has suggested me to solve problems in online judges. Now that's where the problem begins.

When I was in the course, the processor would provide exercise related to a topic he had just taught. So every time exercises were given, I knew which algorithm it is relevant to. But now when I pick a random problem from an online judge I'm lost. I don't know what algorithm might it be relevant to, I can't write a single line.

I was kind of ahead of my class in some of the programming courses but now I'm losing interest in programming because I can't identify the algorithms needed to solve problems. Should I give up programming(which will be a disaster)? 

Any suggestion on getting out of this situation? Sometimes people say ""keep practicing"" but it's not helping. How can I even keep practicing if I have to look up the answer of every problem I pick?",1635663479.0
qibyr4,Why the development of brand new operating systems has stagnated in the last 20 years?,Discussion,114,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qibyr4/why_the_development_of_brand_new_operating/,59,"Almost every OS we use today was conceived and it's development started in the 80's or the 90's and since the 2000's no significant new OS's pop-ed up. Obviously the major OS's were developed and upgraded further while new technologies were incorporated in them, but yet again those OS's are based on 90's concepts and technologies. So why no brand new OS's were created since then? Were those OS's designed to be future-proof? For example was Linux/Unix so advanced that it could support every breakthrough in computer science with just minor updates ,or nowadays every company/organisation has figured out that it's not worth to write something new from scratch?",1635511404.0
qi8v4v,How did large & detailed games like God of War store game files in an 8 MB memory card in play station 2?,,24,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/qi8v4v/how_did_large_detailed_games_like_god_of_war/,16,"Found my Play station 2 recently and remembered it had a 8 MB memory card for all my games. God of War used the most space in the memory card with 550 KB. How could such detailed game files could be stored in an 8 MB chip (other than the CD) but now everything needs 50-100 GB, for only slight improvement in graphics?",1635499469.0
qiq0jd,What software process model uses component-based development?,,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qiq0jd/what_software_process_model_uses_componentbased/,1,What software process model uses component-based development? What software process model can be used for developing a new software using some components that already exists?,1635553461.0
qiu1wc,Programming Languages are overrated,,0,0.37,computerscience,https://www.reddit.com/r/computerscience/comments/qiu1wc/programming_languages_are_overrated/,12,"I have been thinking about this from a long-long time and thought of sharing this here. Programming Languages are given a lot of importance than they deserve in the programmer community. I have seen a lot of people flexing that they know x number of languages which is incredibly shameful. Most of the programmers(90%+) would always give programming languages a lot of importance. Programming languages do not matter much. 

The core concepts of programming matter a lot more than programming languages but still a lot of people want to learn more and more programming languages. Senior software engineers know this that programming languages don't matter as you can learn a programming language thoroughly well in a night if you know the concepts well. By concepts I mean all the things that are applied while making a product(e.g. a web app), if you know compiler design, it would be incredibly easy to grasp a language, that is where concepts pay off.

This problem's cause is bad platforms and bootcamps but I would not name them. They teach the programming language and nothing else. That does not pay off as you might get a freelance job for 10$/hour but you won't be able to land a good 100k+ job at a good company. And I won't even like to call you a programmer if you just know the programming language.

You actually get the taste of core concepts when you go to get a CS degree from a formal college. Yea, I hear you saying that ""Duh, I can't go to college"", then learn it yourself. I myself am 14, started to program seriously since 13 and I can't go to college too. So I started learning the concepts myself. I do not know a lot of programming languages and I just know JavaScript that too not to a great level but I have come to learn system design and databases pretty well and I can make a web app easily. 

Programming Languages are just a way to implementing our programming thoughts, it is just a tool. Just like english is a language to express our thoughts about philosophy, science, etc. Do tell me your thoughts about this, I would love to read them. I have been writing and encouraging this on other platforms to like YT and dev.to.",1635568580.0
qidftd,Is there a most recursive function?,,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/qidftd/is_there_a_most_recursive_function/,5,And doesn't make sense to measure the recursivity of a function?,1635515916.0
qhohv1,AI consumes a lot of energy. Hackers could make it consume more.,,19,0.85,computerscience,https://www.technologyreview.com/2021/05/06/1024654/ai-energy-hack-adversarial-attack/?utm_source=Facebook&utm_medium=tr_social&utm_campaign=site_visitor.unpaid.engagement&fbclid=IwAR2jjhiI2CfI2qT0-IAyulThAM2cr5PiZPy1DJRGNJ6EQFBjOKnNCx3ejrg,4,,1635431906.0
qhhz7s,Is there an order of operations for Boolean logic?,,8,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/qhhz7s/is_there_an_order_of_operations_for_boolean_logic/,6,"Like how mathematicians have the convention that ""2 + 4 x 2"" means ""2 + (4 x 2)"" and not ""(2 + 4) x 2"". If I had ""A and B or C"" does that mean ""(A and B) or C"" or ""A and (B or C)""? Does it change with NOR, NAND, XOR, NOT etc?",1635408068.0
qhcmwq,Question About LinkedList,,17,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/qhcmwq/question_about_linkedlist/,7,"Does the head node store a value, or only the address?",1635387514.0
qgzhy7,Processing Order of Operations,,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qgzhy7/processing_order_of_operations/,2,"From an IDE‚Äôs perspective, does and expression with an equation, that requires order of operations, take longer to process than code that has the order written separately to its own defined register?
e=y‚Äô+(y‚Äô-y)+x‚Äô+(x‚Äô-x),
Vs.
ey=y‚Äô-y,
ex=x‚Äô-x,
e=y‚Äô+ey+x‚Äô+ex

E: Carriage return did not function; insert commas.",1635349133.0
qgoxsp,I have a question about predictive text engines,Help,7,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qgoxsp/i_have_a_question_about_predictive_text_engines/,5,"What exactly are they? I know that they're AI's that take information and create new text similar to it, but if I wanted to code one in Python, how would I do it? Some help would be appreciated.",1635310700.0
qgbxhk,Minimal motivation on partly known studying topics,Discussion,35,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/qgbxhk/minimal_motivation_on_partly_known_studying_topics/,9,"As an computer science student with an programming apprenticeship done, some topics seem very hard to get motivated on, since I know half of the lectures but am not an Expert. 

Where does one take the motivation from to do for example Javascript / Html exercises, when you already worked 4 years in Web development?",1635270486.0
qgc7oj,First Search Algorithm,Discussion,11,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/qgc7oj/first_search_algorithm/,6,Does anyone know the name of the first break-through search algorithm for classical computing was? I‚Äôve done some research on it but nothing seems very definite. Any ideas?,1635271256.0
qfwbyw,what math do I study for AI,,76,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/qfwbyw/what_math_do_i_study_for_ai/,21,"I haven't touched math in 12 years. I am starting from the bottom at basic algebra.

Can anyone recommend a path starting from algebra to be successful in the AI field?

I am not looking for a job in tech, I'm doing this for me because AI fascinates me. So time is no obstacle for me, I could spend the next 5 years learning all this math If have to. 

Thank you for any help yall can give!",1635214522.0
qg6bvz,Tesla Dojo Whitepaper,Discussion,3,1.0,computerscience,https://tesla-cdn.thron.com/static/SBY4B9_tesla-dojo-technology_OPNZ0M.pdf?xseo=&response-content-disposition=inline%3Bfilename%3D%22tesla-dojo-technology.pdf%22,0,,1635254670.0
qfhy4b,What makes an algorithm 'good'?,Help,75,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qfhy4b/what_makes_an_algorithm_good/,33,"Hi all

In an effort to became a better programmer I wanted to check what actually makes a given algorithm 'good'. e.g. quicksort is considered a good algorithm - is that only because of average-case performance? 

Is there a community-approved checklist or something like that when it comes to algorithm evaluation? I tried looking on my own, but the deeper I dig the more questions I have instead of answers.

P.S. If you know any papers or articles that go in depth about the topic that would be great",1635172110.0
qfwfbn,Are power demands the main reason Raspberry Pi doesn‚Äôt incorporate a better CPU?,Discussion,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qfwfbn/are_power_demands_the_main_reason_raspberry_pi/,7,"Of course form factor is a big deal for the raspberry pi. Desktop CPUs really aren‚Äôt very big and for a slightly larger raspberry pi it could have it. The only restriction on that is the power demand. Newer CPUs consume 65W whereas the RPi4 consumes 4-5W. The heat dissipation required as a result of the power demand could be an issue without added cooling.

Is there another reason why a better CPU isn‚Äôt used in a raspberry pi?",1635214851.0
qg1ucj,Post Office Horizon Scandal - Computerphile,Discussion,0,0.5,computerscience,https://www.youtube.com/watch?v=hBJm9ZYqL10,0,,1635236773.0
qg1nhc,Would an adaptive processor ever become a thing?,,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/qg1nhc/would_an_adaptive_processor_ever_become_a_thing/,2,"Ok, so, knowing that AI is a series of logic gates, I theorize that programming is going to go extinct, and be replaced with AI training.

Instead of writing a program, the program writes itself over time, based on your needs. The logic gates assemble themselves after getting a base neutral network from a trainer.

Building off of that theory, processors can be much more like the human brain, taking input data, and learning to route the output to the appropriate place, while learning what output is to be expected.

In this scenario, a single piece of hardware could theoretically become whatever it was needed to be, in the same way software defined radio works. The processor would be a hardware neural net, with variable values. External hardware would automatically train the net when it was plugged into a processor port.

Are there any known concepts like this?",1635235825.0
qfnew0,Personal Project Impediment,Help,7,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/qfnew0/personal_project_impediment/,4,I have a lot of ideas for personal projects I would like to work on. The problem I keep running into is that I need data from various places that either don't offer it up or using their APIs are extremely expensive (i.e. various MLS APIs). What is the best approach to obtaining data you need? How do you guys get data for your personal projects?,1635187433.0
qfah2t,Youtube channels to get in touch in basic computer terminologies,,43,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/qfah2t/youtube_channels_to_get_in_touch_in_basic/,11,,1635142580.0
qftnw9,what does model and design mean in Computers science/software engineering?,,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qftnw9/what_does_model_and_design_mean_in_computers/,2,"I read this in some book, "" Instead of focusing first on programming, we can learn how to model and design. We can also learn how to use different modeling and design approaches together and in context to determine what a system will do and how it will do it. ""

What does model and design mean computer science and software engineering in general?",1635205690.0
qfgg3g,What is the Algorithmic Complexity of Euclidean Method of GCD Calculating?,Discussion,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qfgg3g/what_is_the_algorithmic_complexity_of_euclidean/,1,"Hi folks. In short, the Euclidean method of calculating GCD says that;

    suppose a>= b and rem = remainder, gcd(a, b) = gcd(rem(a,b), b)

Implementing this method is also easy. However, I do wonder what its algorithmic complexity is. Or does it exist in terms of known notations? I know some basic ideas behind categorizing algorithms such as recursivity, loops, binary searches, etc, but I couldn't categorize that. Thanks in advance.",1635167560.0
qfdf0t,"Prof. Niklaus Wirth, 1984 ACM Turing Award Recipient",General,3,1.0,computerscience,https://www.youtube.com/watch?v=SUgrS_KbSI8,0,,1635156360.0
qfhaf7,Purpose of OR gates in bit-shifter,,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/qfhaf7/purpose_of_or_gates_in_bitshifter/,4,"I can't find the answer anywhere to this question, but I've been looking into the circuit diagram for a bit-shifter and I can't figure out what the purpose of the OR gates are. I've tried putting in sample values for the inputs and working it through for left and right shift, with and without the OR gates at the bottom and I get the same values with or without the OR gates. If that's the case, then why are they in the circuit?

You can see a diagram of the circuit I'm talking about here: [https://www.101computing.net/binary-shifters-using-logic-gates/](https://www.101computing.net/binary-shifters-using-logic-gates/)",1635170157.0
qeyub5,What is a an example of OSI layers in daily life?,,12,0.74,computerscience,https://www.reddit.com/r/computerscience/comments/qeyub5/what_is_a_an_example_of_osi_layers_in_daily_life/,10,I understand sending and receiving letters is similiar but I'm just curious if there is a similiar process in daily life ?,1635102571.0
qebtrg,What algorithms every one should know?,,196,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qebtrg/what_algorithms_every_one_should_know/,45,,1635017726.0
qewt85,Counting the number of unique syntax trees of a grammar,Help,1,0.55,computerscience,https://www.reddit.com/r/computerscience/comments/qewt85/counting_the_number_of_unique_syntax_trees_of_a/,3,"Lets say we have a arbitrary grammar for which we would like to know how many syntax trees does it generate.

We know that there are two always two derivations and 2021 leafs.

How to start?",1635096645.0
qebyt6,What is THE stack?,,34,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/qebyt6/what_is_the_stack/,17,"Hello,

I understand what a stack is as a data structure; it's like a stack of plates at a buffet, where you only have access to the top plate. 

However, people seem to talk about THE stack of a computer, and the stack overflowing. For example, I think I've heard that it's bad to write recursive functions, because it can cause the stack to overflow.

Can someone please explain what this is?

Thanks!",1635018169.0
qe1vmc,"Difference between ai,ml,dl & ds(data science)",,15,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qe1vmc/difference_between_aimldl_dsdata_science/,11,"Hi guys, I wanted to know a brief difference among these four fields in computer science.so please any kind of link to the difference or giving a brief difference in this post will be very helpful. Thanks in advance",1634982628.0
qebwa6,Happy Saturday! I was wondering how a social media feed is usually programmed?,Advice,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qebwa6/happy_saturday_i_was_wondering_how_a_social_media/,6,Is it a data base that uses the observer design pattern to refresh and post new entries into the database? That's what I'm thinking in my head but I'm not quite sure. Just trying to learn on Xamarin got a good bit of experience with C#. Thank you so much for your time and I'm sorry if this isn't the proper way to ask.,1635017950.0
qe36q2,"For generations capable of facing the development of technology and information security, the UAE launches a training program to enhance youth skills in cybersecurity",Article,3,0.6,computerscience,https://i.redd.it/toff45alr6v71.jpg,1,,1634988793.0
qdgm1o,"Understanding algorithms and data structures, but not being able to implement them?",,88,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qdgm1o/understanding_algorithms_and_data_structures_but/,45,"Just a bit of background information: I'm currently in high school, and I'm taking a course about algorithms on Coursera. I do have previous programming experience.

I'm able to understand the concept behind algorithms and why and how they work, how efficient they are etc... 

However, when I try to implement or code those algorithms, I get stuck. I know that to solve this problem I should practice more, and I do try, but for some reason, I just can't seem to ""translate"" the algorithm into code. 

This is really affecting me cause I really enjoy computer science in general, and I understand the concepts, but I just can't seem to find a way to transfer my thoughts into code, and it kinda discourages me. However, I'm not gonna give up anytime soon. 

What can I do to solve this problem? Any advice is greatly appreciated! Thank you so much :)

Sorry if this post doesn't belong here, I'm not sure where to post it.",1634908201.0
qdmby0,"Website to build logic gates from simpler gates, allowing you to 'unlock' and use these gates to build more complex one",Help,2,0.61,computerscience,https://www.reddit.com/r/computerscience/comments/qdmby0/website_to_build_logic_gates_from_simpler_gates/,3,"Hi all, I remember using a site that allowed you to wire logic gates to construct more advanced ones - it was really helpful for me but I can't remember the name of it for the life of me.

I am almost certain that you only started out with a NOT gate and I think it there was an AND gate. I think you could choose the number of inputs to create the gate and had to wire them to enable the truth table to operate in the correct way, allowing you to use the gate - for example, after an AND and NOT gate you you could make an NAND, OR gate, then an XOR ... 

If anyone can knows the name of that website I would be really grateful !",1634925063.0
qd6bj4,Donating computers to schools in Central America. Recs for educational centered software (computer coding for kids maybe).,Advice,45,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qd6bj4/donating_computers_to_schools_in_central_america/,9," I'm donating a couple of computers to an elementary school in El Salvador and Mexico. I was hoping to also pack them with useful educational software that the kids can utilize (typing games, English learning games, match games, etc.) Anyone have good recommendations for this? I'm anticipating that the place won't have internet connection, so I want to make sure they can use them without an internet connection.",1634866249.0
qdkg3r,"I would like to start a study on quantum computer (not asking for help in the subject itself) but I don't thing I have a strong base in this topic, what studies and topic should I practice and study before moving to the main topic",Help,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qdkg3r/i_would_like_to_start_a_study_on_quantum_computer/,3,,1634919659.0
qczvme,Why is While(1) an endless loop,,24,0.65,computerscience,https://www.reddit.com/r/computerscience/comments/qczvme/why_is_while1_an_endless_loop/,54,Hello everybody may I ask on why is while(1) an endless loop?,1634845790.0
qd407r,How does the use of a register make this feedback logic-gate diagram work?,,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qd407r/how_does_the_use_of_a_register_make_this_feedback/,3,"Here is what we want:

    s = 0;
    for(int i = 0; i < n; i++){    // n could be any number
        s = s + x
       }

With this digital logic diagram, it is impossible:

&#x200B;

[No Register](https://preview.redd.it/tgc9butftvu71.png?width=896&format=png&auto=webp&s=3cf5c0936ec8756ae3c9e1285fb547847396c77c)

The above digital logic diagram is a logical circuit with feedback. The combination logic unit is an accumulator (with the '+' on it).

Using this diagram Suppose we feed a 1 into the accumulator (assuming we are working with 2 bits). This is the progression.

1. We begin with the circuit above. With the inputs of the accumulator being x and y

&#x200B;

https://preview.redd.it/cr2u5whwwvu71.png?width=773&format=png&auto=webp&s=d4cbec178fb49ba146c26cd85ad83d3bfcec4fd8

2. We feed 01 (assuming we are working with two bits of data) into x

&#x200B;

https://preview.redd.it/zyt951m3xvu71.png?width=1920&format=png&auto=webp&s=c240fa17f3aee1ad53f10b1fc384ba629f61b54e

3. 01 is fed into the accumulator, and the output is 01 because we added nothing to 01

https://preview.redd.it/sdiqo8raxvu71.png?width=1920&format=png&auto=webp&s=98cb3a624bb7348ea4509f7bf696a226934597ec

3, We begin the feedback. x still remains 01 because it had not been updated. y is now 01 because we fed the output into y. Sowe feed 01 and 01 into the accumulator

&#x200B;

https://preview.redd.it/8w46jzyjxvu71.png?width=1920&format=png&auto=webp&s=50a5fbfe425070f23902e4f13beaf10f15f96efb

4.  We add 01 and 01 into the accumulator. So we add 01 + 01, which is 10

&#x200B;

https://preview.redd.it/9v0v51hxxvu71.png?width=1920&format=png&auto=webp&s=13d98e3cb662622328b9ec5482a35feb84374a5b

5. 10 is the output. We then want to feed 10 into the accumulator.

&#x200B;

https://preview.redd.it/snx0coz4yvu71.png?width=1920&format=png&auto=webp&s=8ebf6207677c63ce40ca61487280edabc5f12c7e

6. x still remains 01 because nothing has updated it. y is the previous output of x and y.

7. (this continues infinitely)

So I understand that this loops forever and thus, an incomplete solution.

This problem is completed with the addition of a register.

&#x200B;

https://preview.redd.it/egp8l95nyvu71.png?width=1182&format=png&auto=webp&s=0e542594e08b8266e250600bb9b86404b4d395e8

This fixes the problem. To preface, I am familiar with registers on a software level and had an internship doing reverse engineering so I conceptually understand how registers work. In this example, how does a register help? How does it add the control? How does it make it so that s is initialized to 0?

I would appreciate this a lot.

EDIT: Solved

Summary as to why the register

I understand registers on a software level. I've interacted with them a lot when I had an internship doing reverse engineering. But, I've wanted to up my knowledge so I'm resisting the digital logic implementations of them. I digress, but it was to illustrate I have somewhat of an idea of what I'm doing.

The reset signal from the register is sent indicates the end of the loop.

As for the initialization, the load signal indicates to replace the state of S. In other words, initialize S.

So, to recap:

The register does the following ( slight over simplification):

1. CLK = State Change
2. LD = Initializes
3. Reset = Control",1634858345.0
qcxk48,Why I cant run a program in Hard Disk,,6,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/qcxk48/why_i_cant_run_a_program_in_hard_disk/,7,"I have searched in on google. The reason they say is 

1. It is too slow.
2. It is not byte addressable.

Lets keep that slow factor aside , why cant they make it byte addressable?",1634839200.0
