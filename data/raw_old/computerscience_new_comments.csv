post_id,comment,comment_id,parent_id,created,is_submitter
skltie,"1) yes they are basically the same things. The differences are subtle and mostly unimportant. The exception being that mixed integer programs allow some continuous variables. 

3) this is often true, but not why they are hard. The first optimization problem is basically trivial and all the difficulty is in going from one to the other. 

2,4) you seem to be mixing up convex objective function and convex constraints. 

Linear functions are convex, but integer programs do not have convex feasible regions because convex combinations of integers aren’t generally integers. 

The TSP has a linear objective and non-convex constraints. We can certainly make harder problems by replacing our linear objective with any other kind of function. 

For instance you provably can’t do better than n! if I ask you to guess my favourite tour without any other information. More tractable is to add a submodular function to the objective.",hvlyn6o,t3_skltie,1644007104.0,False
sklhet,What language is your code in?,hvm3b74,t3_sklhet,1644008878.0,False
sklhet,It can be in any of the common languages.,hvm5e7x,t1_hvm3b74,1644009674.0,True
sklhet,What kind of chip? Is this an embedded device with ROM attached?,hvm3sh3,t3_sklhet,1644009063.0,False
sklhet,I have no idea. The usb inputs are coming from a computer mouse. The direction the mouse is moving determines which way the motor rotates.,hvm5sqa,t1_hvm3sh3,1644009827.0,True
sklhet,You're going to have to give us more information about the board for us to really help. Is this a raspberry pi?,hvm7143,t1_hvm5sqa,1644010293.0,False
sklhet,Probably with a wire. Maybe electricity,hvm3vp2,t3_sklhet,1644009097.0,False
sklhet,Probably. Maybe.,hvm5uaw,t1_hvm3vp2,1644009844.0,True
sklhet,"You’ve got to give more info:

Programming Language? OS? Protocol? Chip type?",hvm603q,t1_hvm5uaw,1644009903.0,False
sklhet,"It can be in any language (I haven’t written it yet). I know nothing about chip types.       

The input is a computer mouse. The program is going to take the direction the mouse is moving and then rotate the two motors.",hvm6s0g,t1_hvm603q,1644010196.0,True
skkubm,"Not sure what you mean by mid-level. We don't really have a standard way to rank universities. Additionally, there's no such thing as a ""good school"". Don't assume CS is good at a school because it is well known. Try to research specific schools online and evaluate the program if possible. You can even ask on the school specific subreddits. My favorite way to evaluate schools is to look at their course material. A good school will publish course material online for free.

Washington would be a good state to look at, given that Seattle is a big tech hub. UW has an amazing CSE program, but it is extremely competitive. My suggestion is not to attend UW without also getting into your major - else you may spend 1-2 years without a major. Western Washington University has a good cybersecurity program as well. I'm not sure about WSU but they're a party school.",hvlp9qp,t3_skkubm,1644003577.0,False
skkubm,"I wonder, is there a state university that isn’t a party school?",hvm1yna,t1_hvlp9qp,1644008368.0,False
skkubm,"I guess in my experience anything \*SU will have a lot of partying, while the U\* schools are more serious. But that might just be anecdote",hvm2lcq,t1_hvm1yna,1644008605.0,False
skkubm,Thank you so much,hvlu54i,t1_hvlp9qp,1644005414.0,True
skkubm,"The University of Illinois at Champaign-Urbana is considered quite good for CS. The region has a low cost of living, but tuition for non-Illinois residents might be higher; I'm not sure.",hvly7t4,t3_skkubm,1644006945.0,False
skkubm,"I will search, thank you",hvm076s,t1_hvly7t4,1644007698.0,True
skkubm,https://www.collegefactual.com/majors/computer-information-sciences/computer-science/rankings/best-value/far-western-us/,hvm6wt4,t3_skkubm,1644010247.0,False
skkkmf,"From my understanding, *which is just a 2 hour lecture on discrete probability*, that the distribution of each digit should only *approach* 1/10 (1/D) of the total number of cycles (N) as there are only 10 different digits (D). I'm also assuming that TRNG stands for True Random Number Generator as in a theoretical truly random output from a uniformly distributed set of single digits (if you mean like a hardware random number generator I have no clue:P).

&#x200B;

In the 10 million cycles case, I think it is likely that each digit is close to a million (over or under a little) rather than exactly one million each. For example, even if we had a truly random coin and flipped it twice, there would only be total of one head and one tail half of the time (with the other halves been 2 heads or 2 tails) and it would be equally likely for any of those possibilities. To apply it to your scenario, it means that even if the TRNG of the digits was perfect, its entirely possible that each digit isn't at a million exactly but as we keep doing more cycles, each digit approaches 1/10 of the total number of cycles. Hope this helps but I'm not completely confident about it.",hvlmy6r,t3_skkkmf,1644002705.0,False
skkkmf,"Thanks man, this was quite helpful! 

And damn this was dense!",hvlnydr,t1_hvlmy6r,1644003082.0,True
skkkmf,"Quite the _opposite_ actually: such a generator as you describe is provably _not_ random.

Random in this context means essentially that you can't predict the next value by looking at the prior values. But if you ran such a generator 1M times, and got a hundred thousand 1s, 2s, ... 8s but only 99,999 9's? Then the ""equal distribution of digits"" property means that the next output _has_ to be 9. I used the last outputs to predict, with 100% confidence no less, what the next output is. Thus, not random!",hvlwg2m,t3_skkkmf,1644006279.0,False
skkkmf,"Also, if you handed me something that you claimed was an RNG and said you got equal numbers of each digit after 10M runs, I would say ""BS, no way that's random"" because the odds of that happening are roughly [one in a _thousand billion billion billion_](https://www.wolframalpha.com/input/?i=%2810%5E7%29%21%2F%28%28%2810%5E6%29%21%29%5E10%29+%2F+10%5E10%5E7)",hvlykc4,t1_hvlwg2m,1644007074.0,False
skjr2f,"As a Web science researcher that has been published multiple times in, e.g., The Web Conference, to me Blockchains is one of those kind of overhyped buzzwords that everyone seems to think will be the next big thing. Even in the scientific community a few years ago, it felt like you only had to mention the word in your papers to get accepted.

That is not to say, that the core concepts of blockchains do not have their merits, but to think that ""the next Web"" will be solely based on blockchains is akin to saying that the next big mode of mass transportation will be the Hyperloop.

With developments in the areas of AI and machine learning, I honestly think it is much more likely that the next Web will be geared much more towards machine-readability of the data published on the Web. This is also what the creator of the World Wide Web, Tim Berners-Lee envisioned when he, together with Tim Hendler, and Ora Lassila, described the evolution of the Web into what they coined as the Semantic Web or Web 3.0 (yes I know the terms are confusing); they outlined a Web of Data where data entities link to other data entities rather than Websites linking to other websites. Together with enabling data providers to define ontologies, machines are able to navigate this data much more efficiently than what is on the Web today. Because of this reason, big companies like Google, Facebook, Amazon, etc., already use some of the technology on their sites; for instance, when you do a Google search, the infobox you see on the right is created by traversing their own Knowledge Graph to find the corresponding information, and the company behind Wikipedia published their own collaborative knowledge graph called Wikidata.

Now, in the Web community, we see this as a good way of bringing the vast amounts of knowledge on the Web to the people in a readily accessible manner. We are making a lot of progress towards these goals; however, there are some big challenges involved in this, such as data availability, data correctness, and so on, which is where decentralization and blockchains might actually fit in and helped solve the problems. For instance, last year I published a paper on using blockchain-like chains of updates to knowledge graphs to enable people to collaboratively ensuring that information and knowledge available is up-to-date and factual. Another exciting project in this area is the [Solid](https://solidproject.org/) project. Decentralization of the Semantic Web is a rapidly growing scientific community where Tim Berners-Lee is also somewhat still active. I have participated in conferences and workshops with him in the past on the subject.

tl;dr Blockchains are only a small part of what the next Web will be, and I think it is much more likely it will be geared towards machine-readability that be solely based on blockchains.",hvlt8yp,t3_skjr2f,1644005072.0,False
skhmpy,"Nah, chill. GitHub CoPilot didn't bring any major change in the world, nor did Web 3. These are just some hypes created by old geezers and the so-called experts in the industry who think sharing shit content on Twitter will make them superior.

PHP is still used, so is COBOL. Keep developing new SaaS products which help people, you will never have to fear then.

Edit: Grammar",hvkw2ui,t3_skhmpy,1643992810.0,False
skhmpy,remindme! 8 years,hvlw52p,t1_hvkw2ui,1644006166.0,False
skhmpy,"I will be messaging you in 8 years on [**2030-02-04 20:22:46 UTC**](http://www.wolframalpha.com/input/?i=2030-02-04%2020:22:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/skhmpy/will_software_engineers_become_obsolete_by_2030/hvlw52p/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fskhmpy%2Fwill_software_engineers_become_obsolete_by_2030%2Fhvlw52p%2F%5D%0A%0ARemindMe%21%202030-02-04%2020%3A22%3A46%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20skhmpy)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hvlw8yz,t1_hvlw52p,1644006205.0,False
skhmpy,"Coding is dead. No-one does real programming (machine code) anymore. They all use these ""programming languages"" that basically do all the work for you so any dummy can write code. 

/joke",hvl0yri,t3_skhmpy,1643994595.0,False
skhmpy,AI is unlikely to replace software development altogether anytime soon.,hvkvimz,t3_skhmpy,1643992603.0,False
skhmpy,"No…we’re nowhere near becoming an endangered career field. Don’t buy the hype, we have a long way to go before Halo-style Cortana A.I.’s are running around solving all our problems. You should continue to pursue your goal(s). 

There are many exciting fields on the cutting edge of computing that you can get into (quantum computing, computer vision, edge computing, federated learning, etc.). You only live one life, enjoy your time to be a geek!",hvkxuk6,t3_skhmpy,1643993459.0,False
skhmpy,The jobs innovating and creating cutting edge technology on novel problems won't be going anywhere. If the models get REALLY good There might be slightly less demand for code monkeys to build generic e commerce backends much the same as HTML developers died off with more abstraction. Training a model to crunch leetcode problems doesn't actually replace anything substantial.,hvkwmmf,t3_skhmpy,1643993010.0,False
skhmpy,"Software hardly ever brings any “cutting edge technology”. It enables new technologies, but is hardly ever the new technology.

Also, AI will never* replace software engineers. Think about embedded systems, control systems, flight software, smart homes, automotive, etc. AI cannot even adequately replace a simple PID controller, let alone creating a full software application from scratch.. 

This post makes me think you don’t understand technology at all.

*as long as computers, processors and FPGAs exist in a similar form as they do today, then the amount of software engineers will only ever need to increase.",hvl1y1k,t3_skhmpy,1643994952.0,False
skhmpy,No.,hvl2v27,t3_skhmpy,1643995285.0,False
skhmpy,Yes. /s,hvlcfd6,t3_skhmpy,1643998774.0,False
skhmpy,"It will change. 

Machine Learning subs have recently started to complain that they can't write a better data model than those provided by some fantastic libraries where 10 lines of code replaces months of work. This is because someone smart sat down and did it for everyone else. We will always need someone smart but possibly not as many people in this field. Machine learning does what it was designed to, and majority will become users that create value not builders that create opportunity for value.

Website programming is required for information publishing (static or streaming), sales, data collection, and social. There are many easy ways to get your website out there with templates for all of these technologies. Past what templates offer you might need a developer, but it's mostly content creators that are needed once a template becomes good enough. I would argue it's not the development that is the bottle neck it is the design.  Even social media template sites exist. You will still need smart folks developing templates, but less as value creators settle for packaged products, rather than seeking out something new from value enablers.

Tool programming. I don't see this going away any time soon. Finance and logistics tools are just not capable of being perfect and handling every situation, as are content creation tools.

Gaming.. you can argue that engines can grow to a point of perfection where once again art and content will rule.

But we are about 20 years out until we reach peak programming, in my opinion, so still a very valuable and valid career.",hvleo66,t3_skhmpy,1643999606.0,False
sk80jv,"You are actually pretty much right, you just omitted a factor of ""n"" from your final equation. It should be this:

&#x200B;

n \* 2 ((3/2)\^log\_2(n) - 1)

&#x200B;

Ignore the constants and simplify to get:

&#x200B;

n\*(n\^log\_2(3) / n) = n\^log\_2(3)

&#x200B;

You should also check out the master theorem as it provides an easier method to solve these problems: https://www.geeksforgeeks.org/advanced-master-theorem-for-divide-and-conquer-recurrences/",hvlf8d9,t3_sk80jv,1643999814.0,False
sk80jv,"Oh man come on, how did I miss that. Thank You for the comment.",hvljcjt,t1_hvlf8d9,1644001342.0,True
sk7puv,"Hello, the router in this situation does not really factor (minus the reason for congestion)

Simplified, The TCP client would potentially decrease the TCP window size if congestion is an issue thereby reducing flow of the traffic from the server. Software that uses UDP usually does have mechanisms to manage ""flow""  as well, this is just handled at different layers of the OSI model (Application).",hvjfk9q,t3_sk7puv,1643964347.0,False
sk7puv,Then could a malicious/malfunctioning application throttle TCP traffic by flooding a network with UDP packets?,hvjpazq,t1_hvjfk9q,1643972107.0,True
sk7puv,"Yes, that's called a ""UDP flood"". It's a form of Denial of Service (DoS).",hvjsllc,t1_hvjpazq,1643974545.0,False
sk7puv,"I disagree, as a network engineer by trade, with the previous poster. Your question is why QoS methods exist for managing these situations. You couldn't operate the internet if UDP just wins all the bandwidth because TCP throttles.

You don't need QoS until the transmission line is saturated. Once there, QoS provides a way to equalize the playing field between traffics based on an operators policy. QoS methods exist at L2, L2.5(MPLS) and L3 as well as L4 re:TCP to provide various techniques for handling these situations in aggregate network traffic streams.

See 

https://en.wikipedia.org/wiki/Quality_of_service

Network QoS overview (its a PDF) https://archive.nanog.org/meetings/nanog36/presentations/sathiamurthi.pdf

802.11 to DSCP L3 marking 

https://tools.ietf.org/id/draft-ietf-tsvwg-ieee-802-11-05.html

Look at the section on pcp marking at L2

https://en.wikipedia.org/wiki/IEEE_802.1Q

Class of Service definitions

https://www.omnitron-systems.com/carrier-ethernet-learning-center/carrier-ethernet-2-0-multi-cos

https://www.mef.net/Assets/Technical_Specifications/PDF/MEF_23.pdf",hvkm1vb,t3_sk7puv,1643989080.0,False
sjuhh6,"Nada but you could recreate that with ~100 lines of python

1. Extract labels from an image using AWS Rekognition (or opencv). https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html

2. Pass the labels to a GTP3 endpoint. https://gpt3demo.com/apps/openai-gpt-3-playground

3. lol at results",hvi1h09,t3_sjuhh6,1643937378.0,False
sjn3gy,"Most of the time, you just run a simulation with a reduced data set or reduced number of rounds and extrapolate from there. Compute time is going to remain (mostly) linear for conventional systems. That means if it takes X time to compute and compare 1 round, it will take n*X time to compute n rounds.

FWIW, you don't even need to keep the distance traveled and paths for all rounds, you are only comparing to the best so far, so the compare time doesn't need to grow (linearly or non-linearly) either. Set the best so far to the first round/route you compute, then see how long it takes to compute and compare the next route/round. It should be roughly linear all the way through the maximum combination of possible paths.",hvfqkvw,t3_sjn3gy,1643905504.0,False
sjn3gy,"They are just estimating using some average computer specs and plugging in the numbers into the asymptomatic run time. Let's say an average PC can run 1 billion instructions a second, you then take the number of estimated instructions and divide it by the number of instructions per second, that gives you the number of seconds your program takes to run. You then convert that into number of hours, days, years, etc.

N! Is a huge run time. 69! ~ 171122452428141311372468338881272839092270544893520369393648040923257279754140647424000000000000000

If a program needed to run this many instructions it would take probably until the heat death of the universe to do with our current technology.",hvg1911,t3_sjn3gy,1643909370.0,False
sjn3gy,"I'd also like to point out, almost no one actually tries to calculate the true running time of a general algorithm mathematically. This is for a few reasons. For one, computers are not all the same and have different performance specs. Another reason is that no modern computer only runs a single program at a time. They concurrently run many processes and thus you can't get an accurate true run time because of the uncertainty of the computer's run-time conditions.

To combat these things, theoretical computer scientists estimate the run time of algorithms via asymptotic run time. This is the familiar Big-O notation (although there are others such as little-o, Big-Omega, etc.). But most of the time, people just want an estimate of how the algorithm's run-time grows **with respect to the algorithm's input.** In the case of the Traveling Salesman problem, the input is the number of cities or towns, and it's asymptotic running time is O(N!), which people usually just equate to ""exponential"" time. This problem is considered a HARD, or in cs terms, an NP-Complete problem (For the decision problem version) .

If you truly want to know the actual running time of your algorithm on a specific machine, you typically implement it and run it a few million times to get the average run time.",hvg46m5,t1_hvg1911,1643910421.0,False
sjn3gy,This dives right into the border between what computer science is and what software development is.,hvihz7o,t1_hvg46m5,1643944701.0,False
sjn3gy,RemindMe! 5 Days \[ [https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating\_the\_run\_time\_of\_the\_travelling/](https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/) \],hvg1v6d,t3_sjn3gy,1643909593.0,False
sjn3gy,"I will be messaging you in 5 days on [**2022-02-08 17:33:13 UTC**](http://www.wolframalpha.com/input/?i=2022-02-08%2017:33:13%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/hvg1v6d/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fsjn3gy%2Festimating_the_run_time_of_the_travelling%2Fhvg1v6d%2F%5D%0A%0ARemindMe%21%202022-02-08%2017%3A33%3A13%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20sjn3gy)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hvg3co1,t1_hvg1v6d,1643910124.0,False
sjn3gy,RemindMe! 5 Days [ https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/ ],hvjaubq,t1_hvg3co1,1643960816.0,False
sjasw0,"Of course it’s useful. You should know how hardware resources work on a single node before you consider a cluster. You wont use everything you learn (low level assembly comes to mind) but you will use some of it and understanding things like context switching, I/O overhead, concurrency issues, memory management, etc. will give you a better perspective on more complex distributed architectures. Imagine trying to effectively utilize a load balancer without understanding how resources are being allocated. That’s a recipe for disaster.

Now, you asked for examples so I’ll give you a personal one. I work as a data scientist. Our enterprise cloud data platform has to deliver data products (visualizations, reports, etc.) to consumers. One technical way that we measure user experience on the platform is by evaluating the performance of our Power BI capacity instance. If the utilization/load is too high then our end users will encounter delays and the overall user experience will decline/suffer. Monitoring these hardware resources and identifying which resources are being stressed helps us decide how to optimize the delivery of data products to the platform and it helps us protect against potential slowdowns. If my team and I had no understanding of computer organization, computer architecture, and operating systems we wouldn’t know how to make intelligent decisions about operating efficiently.",hve63y3,t3_sjasw0,1643873883.0,False
sjasw0,"Any benefit? Yes. Immediate benefit to the things you do to get started and build 'ok' distributed systems? Not really.
 
And there are big differences in what parts will be useful and what parts aren't, because even though 100% of them are abstracted, some of them will directly influence decisions you make. CPU architecture, operating systems, and most importantly the memory model and memory architecture (in detail) will be **critical** for writing performant scaled distributed systems. Compiler engineering and assembler you are very likely to never, ever use - though admittedly assembler can be useful for learning how the memory model works properly, and there are some parallels between the way compilers work and the way declarative languages or libraries often used in distributed systems work.",hveiva2,t3_sjasw0,1643883604.0,False
sjasw0,"If you asking this question, you shall probably first write a simple backend service which will run in production, and not thinking about designing large scale distributed systems.",hve0p2d,t3_sjasw0,1643870253.0,False
sjasw0,Thanks for the reply. I have created some backend services know a little about them. I asked this question because I would be creating large-scale distributed systems in the near future and wanted to know whether low-level engineering and computer organization knowledge would have any benefit.,hve6uoy,t1_hve0p2d,1643874407.0,True
sjasw0,It’s not going to help you with implementing this future project but it’ll widen your perspective about what’s going on under the hood. If you have the time go learn it. Personally I’d spend more time on the topics and technologies you primarily deal with and learn the low level stuff on the side when time permits.,hve7ivl,t1_hve6uoy,1643874887.0,False
sj5m8j,"Communications Networks by Leon-Garcia and Widjaja is excellent for things like ARQ and Layer 2. For TCP/IP, read the relevant RFCs to buttress TCP/IP Illustrated by Stevens; throw in UNIX Network Programming if you're feeling randy. You'll then want Kleinrock's two volumes on queuing theory. Finally, read the ip-\*(8) man pages covering iproute2, and the section 7 man pages on tcp, udp, ip, and arp.",hvd1cp2,t3_sj5m8j,1643853189.0,False
sj5m8j,"Whoa I'm interested in networking research, but most texts I've read have been more on a economics/math (optimization) or probability side, I'll have to check it out for a higher level view. If you want a more math-heavy variant I would recommend [https://www.amazon.com/Communication-Networks-Introduction-Synthesis-Lectures/dp/1627058877](https://www.amazon.com/Communication-Networks-Introduction-Synthesis-Lectures/dp/1627058877) which is great albeit mathy as heck haha",hve7vvp,t1_hvd1cp2,1643875152.0,False
sj5m8j,Read some RFCs and also the networking book by Andrew Tannaebaum is a great read. Also the Network Warrior published by O'Reilly is a great read.,hve49ez,t3_sj5m8j,1643872603.0,False
sj5m8j,"Berkeley's http://cs168.io/ uses Computer Networking: A Top-Down Approach, 7th edition by Jim Kurose and Keith Ross.",hve7j6y,t3_sj5m8j,1643874894.0,False
sj5m8j,"USC’s CSCI 353 also uses Computer Networking : A Top-Down Approach, I’m a big fan",hvfkafu,t1_hve7j6y,1643903158.0,False
sj5m8j,"BeeJs Book on Networking or something like that. Been ages, I have it somewhere on my PC.",hvdayov,t3_sj5m8j,1643857188.0,False
sj5m8j,http://www.tcpipguide.com/free/t_toc.htm,hvf5atf,t3_sj5m8j,1643897040.0,False
sj5m8j,Professor Messer on YouTube has the whole CompTIA Network+ course for free. Very high quality (free) content.,hvfho9k,t3_sj5m8j,1643902154.0,False
sj47go,Check out this playlist I've been meaning to watch for over a year now https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo,hvcpjcc,t3_sj47go,1643848256.0,False
sj47go,I will. Tnx.,hvel12u,t1_hvcpjcc,1643885254.0,True
sizzx9,"I'm not sure if I understand what you're asking, but if it helps an array of size 1 is always automatically sorted.",hvc07ny,t3_sizzx9,1643838274.0,False
sizzx9,"Yeah, my mistake was assuming that this was unique to insertion sort because of the way my instructor was stressing it. Appreciate it man",hvc0rf2,t1_hvc07ny,1643838472.0,True
sizzx9,"It took me 1 minute of googling to know it doesn't assume that. In step 2, it checks array[1] with its predecessor, which is array[0]",hvbv92a,t3_sizzx9,1643836502.0,False
sizzx9,"[https://www.programiz.com/dsa/insertion-sort](https://www.programiz.com/dsa/insertion-sort)

&#x200B;

>The first element in the array is assumed to be sorted. Take the second element and store it separately in key.

It does assume that. After some reading I found that other algo's like selection sort also assume the first value to be sorted. The difference is selection sort compares the value in question to the other values in the unsorted list first whereas insertion immediately compares it to the values in the sorted list. My confusion was thinking that counting the first value as immediately sorted was unique to insertion sort but that was wrong.",hvbzax3,t1_hvbv92a,1643837946.0,True
sizzx9,"Nope; but they do express is in a really clumsy way, and I can see where the confusion came from.  They probably meant that the first element is assumed to be sorted because the second is picked as the ""key"" and the first has no predecessor, but the second element is still compared with the first to reorder them if necessary, and so on. The geeksforgeeks article explains it better imo",hvcarge,t1_hvbzax3,1643842247.0,False
sizzx9,"The algorithm doesn't assume anything about the desired position of the first element in the final sorted array.
However, the first loop starts from the second element because the first element 1) has no preceding elements to compare it with and 2) if we view it as an array of length 1, it's sorted. Why would anyone care to call a single value array a sorted one? Just to highlight how the first step of the algorithm is no different from the rest of the iterations where the left part of the array is sorted.",hvc6k5q,t3_sizzx9,1643840637.0,False
siux7r,"Mathematics often has surprising applications in computer science. You might come across some problem, realize that it can be represented as some kind of finite simple group or whatever and suddenly theorems from group theory make your life easy.

Apart from that, a good grasp at some mathematics is always beneficial, no matter through what you approach it.",hvaznp5,t3_siux7r,1643824737.0,False
siux7r,"Totally agree. There is a matematician/computer scientist named Petter Graff. When he is hired to analyze systems to make them more efficient and reduce the cost, he looks for monoids in the application-domain. If or when he recognise a monoid, he can reduce the problem significantly and such an architecture often involves a streaming platform. I don't have the details, but look him up.

EDIT: name of the mathematician/computer scientist",hvb8pg2,t1_hvaznp5,1643828085.0,False
siux7r,"I find this interesting, but my quick searches (the name, ""monoids"") haven't found anything.  Do you have any references?",hvbdzqm,t1_hvb8pg2,1643830047.0,False
siux7r,"A monoid is a semi-group with an identity element. I should  have wrote: ""..looking for a monoid..""

Google \`monoid\` an you will find a lot of resources, e.g. here: [https://mathworld.wolfram.com/Monoid.html](https://mathworld.wolfram.com/Monoid.html)

EDIT: semi-groups have a binary operator, so removed redundancy.",hvbfuml,t1_hvbdzqm,1643830761.0,False
siux7r,"I'm familiar with monoids and semi-groups, I meant specifically anything to do with Peter Graff, or the applications you described.  Thanks though.",hvbhf6s,t1_hvbfuml,1643831366.0,False
siux7r,"Ah, I see! He held a course in 2019 where he described a transactions system which had grown over its proportion, s.t. every transaction cost .4 USD. (PayPal). By ""out-streaming"" the old hog of a system which demanded more and more servers, he would employ abstract algebra when modelling the new stream-based architecture.

 I also spelled his name incorrect. It is Petter Graff.

Here are some links: 

[https://pettergraff.blogspot.com](https://pettergraff.blogspot.com)

[https://www.linkedin.com/in/pgraff/](https://www.linkedin.com/in/pgraff/)

I cannot speak on his behalf, but like any adept computer scientist, I am almost certain that he will reply if someone reaches out inquiring for his concepts of interest.",hvbjqvl,t1_hvbhf6s,1643832256.0,False
siux7r,"That helps, thanks very much!",hvblpul,t1_hvbjqvl,1643833026.0,False
siux7r,Anything you love reading/learning is never a waste of time.,hvbdaio,t3_siux7r,1643829787.0,False
siux7r,Its the basis of lots of cryptography. Eg the diffie-hellman key exchange uses finite cyclic groups,hve5noi,t3_siux7r,1643873568.0,False
siux7r,"I've read some machine learning articles and there are  concepts which i couldn't grasp until I start reading about abstract algebra. I recall one article emphasised on ""optimizing over a smooth manifold"", hence I had to investigate and ran into AA. I also learned that a vector space is an Abelian group and by doing linear algebra, I was learning just a small subset of such an interesting subject. 

The book ""Deep Learning"" (Goodfellow et. al, 2015 (with the strange creature on the cover)) he talks about topology and how e.g. classification can be done with points in space which seem quite randomly placed and how he manages to separate (or dichotomise) them with a straight line (If I remember correctly.) I am not familiar with this field, but I see this mentioned a lot in the context of ML/DL. I believe it is crucial if you wanna be a researcher. 

To get more motivation, I suggest you check out Socratica on ""modern algebra"" on YouTube).",hvbsngi,t3_siux7r,1643835580.0,False
siux7r,Right I had the same experience with measure theory.,hve5saa,t1_hvbsngi,1643873657.0,False
siux7r,"Not CS but group theory is pretty darn essential in chemistry, from predicting molecular shape and reactivity to predicting how molecules will interact with light.",hvc7nym,t3_siux7r,1643841058.0,False
siux7r,"Even in CS. Theoretical CS uses, I believe, every area of Math. I am not well-rehearsed in Group Theory, but I know a little bit about groups, fields, rings, and their applications in this area called Error-Correcting Codes.",hvdt8h9,t1_hvc7nym,1643865837.0,False
siux7r,"Whoops.... I meant to say... ""This comment is not about CS but group theory...""",hvdw1gz,t1_hvdt8h9,1643867435.0,False
siux7r,Oh.. my bad,hvdxxjt,t1_hvdw1gz,1643868543.0,False
siux7r,"There are some wonderful connections to group theory in machine learning, mostly around finding symmetries and invariances",hvenpgl,t3_siux7r,1643887210.0,False
siux7r,"You could say that Group Theory is the basis (or one of them) in several different concepts, such as OOP (as in heritance hierarchies and relations between objects) and it also helps to understand database structures and how to build queries. You could apply that same thinking into other data sets and structures in general.

I think it generally helps you structuring abstract thinking of any elements/objects/points, and thus helps you to design better solutions.",hvc1l06,t3_siux7r,1643838771.0,False
siux7r,You can't study any math without group theory. Almost all math is based on definitions derived from group theory.,hve6g19,t3_siux7r,1643874123.0,False
siux7r,"It depends on what do you want to do. It likely will not see much use if you are just planning to become a programmer, but if you want to be a computer scientist you will find use for it no matter which field you are in.

In general, algebraic structures like group, semiring, lattice; spaces (which in my mind are also algebraic structures) like vector space, topological space, measurable space, and Banach space; and to be more abstract, category theory, are all incredibly useful in my area.

To give you some example, programs naturally have a semiring structure, where the addition is given by non-deterministic choice, and multiplication given by composition. If you add an iteration operation on it, you will get [Kleene Algebra](https://en.wikipedia.org/wiki/Kleene_algebra): an equational theory that is vastly useful not just in program semantics and verification (https://mamouras.web.rice.edu/other/phd-thesis-2015.pdf), but also in abstract reduction system (https://www.researchgate.net/publication/220118377_Abstract_abstract_reduction).

There are extensions to it to verify different kind of systems, some of them can be found in https://mamouras.web.rice.edu/other/phd-thesis-2015.pdf; and
 
- there are ways to reason about concurrency: https://link.springer.com/chapter/10.1007/978-3-642-04081-8_27; 
- network applications: https://www.cs.cornell.edu/~kozen/Papers/NetKAT-APLAS.pdf; 
- correctness of programs: https://www.cs.cornell.edu/~kozen/Papers/typedHoare.pdf; 
- incorrectness of program: https://dl.acm.org/doi/10.1145/3498690; http://link.springer.com/content/pdf/10.1007%2F978-3-030-88701-8_20.pdf

and many more.

And there are also related systems that are based on lattices, the most notable one is [Quantle](https://en.wikipedia.org/wiki/Quantale), or sometimes called Kleene Algebra à la Conway. It is a stricter notion of Kleene Algebra. And many lattice based systems are mentioned in [here](https://core.ac.uk/download/pdf/35095875.pdf)

For spaces, they are necessary to reason about probabilistic programs. For example, the famous [Kozen 81](https://www.cs.cornell.edu/~kozen/Papers/ProbSem.pdf) asserts that all probabilistic programs needs to live in a Banach space for a reasonable semantics. Later, Fredrik and Dexter follows this up with [this paper](https://www.cs.cornell.edu/~kozen/Papers/ProbSemPOPL.pdf), which uses ordered Banach spaces to formulate the higher order semantics of probabilistic programs. Topological space, given its simplicity, are still surprisingly essential in program reasoning, [this paper](https://www.cs.cornell.edu/~praveenk/papers/cantor-scott.pdf) demonstrates that choosing the right topological space for the measurable space have important practical impact on program reasoning and semantics. 

All in all, learn all the math you want, they will be useful some day if you want to do computer science.",hvfwsnr,t3_siux7r,1643907755.0,False
siux7r,Which book are you reading? I might check it out too,hvj8ud4,t3_siux7r,1643959406.0,False
siux7r,Contemporary Abstract Algebra by Joseph A. Gallian..... it's easy to understand for non-brilliant students like me 😅,hvjclyu,t1_hvj8ud4,1643962104.0,True
siux7r,Pretty sure the current fast algorithm for graph isomorphism uses some group theory.,hvdv82p,t3_siux7r,1643866968.0,False
siux7r,"Not exactly ML application, but you can see monoids mentioned below. Twitter had a library called algebird open sourced few years back. Roughly the idea was to express data summarisation operations as monoids. If that was done, you would get parallel processing for free on large datasets. The interesting part here was if a given operation couldn’t be expressed as a monoid (for example finding quantiles) then you would try to find a probabilistic data structure which would let you express the approximate operation as monoid . You could then use it with algebird. This was essentially trading some accuracy to gain parallel processing.",hvfb8vg,t3_siux7r,1643899599.0,False
siux7r,Do you have something better you know you should be doing? If not why not give it a go and learn a subject you like? Studying group theory has many applications in theoretical CS and at the very least will build your mathematical maturity that will help you with anything rigrous you might want to do later. Life's too short to not do things that we like :),hvfg6ls,t3_siux7r,1643901580.0,False
siux7r, Many blockchain applications use Group Theory,hvfpdy7,t3_siux7r,1643905070.0,False
siry8q,If i=0 then y^i is empty. This does not mean y is empty.,hvafx3o,t3_siry8q,1643817548.0,False
siry8q,"ok got it, thanks",hvaj08z,t1_hvafx3o,1643818701.0,True
siry8q,Good luck with the course. It is one of the most interesting (but mathematical and rigorous) topics of CS.,hve0g48,t3_siry8q,1643870098.0,False
siry8q,Yes it’s a lot of maths but quite interesting! Thanks,hve1lwv,t1_hve0g48,1643870841.0,True
siry8q,Oh man pumping lemma was the worst when I took a class dedicated to T of A,hvcbl8v,t3_siry8q,1643842568.0,False
sicshh,log(2^n ) * log(n^2 ) = n log(2) * 2 log(n) = O(n log(n)) because log(2) and 2 are constants,hv7zq38,t3_sicshh,1643768663.0,False
sicshh,"Adding to this

n * sqrt(n) = n * n^0.5 = n^1 * n^0.5 = n^1.5",hv9km4y,t1_hv7zq38,1643803214.0,False
sicshh,"A resource on logarithmic properties: https://www.cuemath.com/algebra/properties-of-logarithms/

A resource on multiplying fractional exponents: https://www.cuemath.com/algebra/fractional-exponents/

Big-O notation removes constants like factors of 2 or log(2) as winniethezoo commented. Otherwise, simplifying the formulas is mostly math.",hv8ksv2,t3_sicshh,1643778141.0,False
sicshh,I've been programming or developing or engineering since 2007 and have a bachelor's degree and at no time was I ever taught this.,hvarttb,t3_sicshh,1643821891.0,False
sicshh,"It's ""just"" algebra and a bit of real one-variable calculus.

Whether you write ""n\*n"" or ""n\^2"" does not (really) change the ""number of 'n's"" in your function, as that is an ill-defined concept anyways.

Also, not every ""big-O function"" (whatever that means) must be written with ""one n"" (whatever that means"".  


Saying that f(x) is in O(n \^ sqrt(n)) is perfectly valid.",hv9fvr7,t3_sicshh,1643799954.0,False
sicshh,Is that equivalent to O(n\^1.5) ?,hvauwp5,t1_hv9fvr7,1643823008.0,False
sicshh,"No, as sqrt(n) ≠ 1.5 in general",hvaza9k,t1_hvauwp5,1643824601.0,False
sicshh,>n * sqrt(n) = n * n^0.5 = n^1 * n^0.5 = n^1.5,hvb4x5j,t1_hvaza9k,1643826691.0,False
sicshh,"True, but n \* sqrt(n) ≠ n \^ sqrt(n)",hvbke2n,t1_hvb4x5j,1643832506.0,False
sicshh,"true, sorry. we probably had the same confusion though",hvbpt1k,t1_hvbke2n,1643834550.0,False
sicshh,yes,hvb4v4m,t1_hvauwp5,1643826670.0,False
sicshh,"log(2\^n) -> nlog(2) log(n\^2) -> 2logn

&#x200B;

2logn \* nlog2 is 2nlogn which asymptotically is nlogn",hvicymn,t3_sicshh,1643942458.0,False
siaup5,"| **Something that you wish to see done that hasn't been done?**

Technological singularity

**| Something that you don’t wish to see done that hasn't been done?**

Technological singularity",hv7npk4,t3_siaup5,1643763688.0,False
siaup5,Yes This!,hv88gta,t1_hv7npk4,1643772366.0,False
siaup5,"Human level AI, complete with emotion and sentience.

Both want it, and don't want it.",hv9mtzh,t3_siaup5,1643804584.0,False
siaup5,"It’s basically making humans predecessors. Noble, but dangerous.",hv9rqg8,t1_hv9mtzh,1643807325.0,False
siaup5,Solve Chess (Draughts has been done),hv87p4s,t3_siaup5,1643772031.0,False
siaup5,What do you mean by solve? Do you mean create a method to solve every conceivable game?,hv8csgx,t1_hv87p4s,1643774276.0,True
siaup5,That's what [solving a game](https://en.wikipedia.org/wiki/Solved_game) generally means.,hvbpoim,t1_hv8csgx,1643834503.0,False
siaup5,Sentient artificial intelligence.,hv9rohu,t3_siaup5,1643807297.0,False
si9m1h,What parts did you get stuck on?,hv7n9xk,t3_si9m1h,1643763503.0,False
si9m1h,I haven't started reading it yet but I read that you need prior knowledge to be able to udneratand it so i am wondering what the topics I need to be familiar with are.,hv7v95y,t1_hv7n9xk,1643766810.0,True
si23iw,Seems like work to read this.,hv822yz,t3_si23iw,1643769633.0,False
si23iw,Read it at work and call it professional development!,hv82tah,t1_hv822yz,1643769940.0,False
si23iw,dont read it lol,hvahs34,t1_hv822yz,1643818244.0,True
si23iw,"I honestly like this whole make more things public thing companies are doing, like GitLab publicizing its marketing handbook. Whether we like or hate google's products, we can all learn a thing or two from their engineering know-how :D

Great find op!",hv8285n,t3_si23iw,1643769692.0,False
si23iw,Thanks boss!,hvahtr8,t1_hv8285n,1643818261.0,True
shzu1j,"That's O(nlogn)

Edit: to give details: n/b*log(n/b)=nlogn*(1/b)-logb so it's in O(nlogn) for constant b. İf b is an input or parameter don't know but some distributed programming algorithms are the only place I can think of like you said",hv5ummi,t3_shzu1j,1643738675.0,False
shzu1j,b is not constant - it's a parameter,hvecxrc,t1_hv5ummi,1643878934.0,True
shzu1j,I feel like you’ve pry thought of this but — merge sort.,hv6ma5z,t3_shzu1j,1643748953.0,False
shzu1j,[deleted],hv778d3,t3_shzu1j,1643756844.0,False
shzu1j,"**[Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl's_law)** 
 
 >In computer architecture, Amdahl's law (or Amdahl's argument) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. Specifically, it states that ""the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used"". It is named after computer scientist Gene Amdahl, and was presented at the AFIPS Spring Joint Computer Conference in 1967. Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hv77a0h,t1_hv778d3,1643756863.0,False
shzu1j,Binary search?,hv8ksnv,t3_shzu1j,1643778138.0,False
shzu1j,"The closer K is to 0, the worse it is compared to O(n) given a large enough input.  It's just not really worse by an appreciable amount.  But, again, the size of the input is the determining factor here.

Sorting algorithms that use comparison are a good place to start, as the best case for those is O(nlogn)",hv9pdcs,t3_shzu1j,1643806059.0,False
shfux4,"I double majored in CE and CS. 

CE is much more focused on the hardware, you only take a few intro level programming courses. There are a lot of courses much more focused on low level things like hardware design, solving circuits, signal processing, and microcontrollers.

CS is more software and algorithms focused. You work with more programming languages and your homework tends to involve actually building software rather than just design. Higher level classes get into things like AI, computer security, advanced algorithms, programming language structure, and some really gnarly pieces of software like compilers.

Overall there isn't nearly as much overlap as I thought there would be when I first decided to double major",hv2lp1k,t3_shfux4,1643677830.0,False
shfux4,"In your opinion, would you recommend a double major in CE and CS to other people? I've been thinking of double majoring between the 2 majors for quite some time now.",hv2sbnz,t1_hv2lp1k,1643680752.0,False
shfux4,"That would depend on your goals and motivations. If you're just trying to maximize your job potential, I don't think it's super beneficial to double major. 

But for me it was very beneficial because I didn't really know the difference between the subjects and I wouldn't have known what I wanted to do with my career if I didn't try them both. I started out as just a normal CE major and after my first year I added the 2nd CS major. And over time I grew to like CS better and that's what I got my masters' degree in and now I'm a full stack engineer. 

So I'd say if you know for sure what you want to do between the two, just major in that because the other major won't help a lot across career tracks. But, if you aren't really sure exactly what you want to do and you think you can handle the extra workload, it's a great way to explore both subjects and see which suits you best.",hv34au3,t1_hv2sbnz,1643685998.0,False
shfux4,"How is this post different from [your previous one](https://www.reddit.com/r/computerscience/comments/seqqpv/is_software_engineering_a_sub_title_to_computer/)?

>If they know the hardware don’t they also know what software can be built on it and how?

u/Henrique_FB already provided quite good analogy:

>You are basically saying that a guy who makes paper can make any mathematical equation on said paper. Since he built the paper he knows how to write anything on it perfectly.

And here's mine:

>They decide it as much as car designers decide where a car can drive, with how much baggage and for what purpose.  
>^(Audi Q7 wasn't designed for agricultural work, yet I've seen it done)

And now I thought about another one: _You are saying that any mechanic is better driver than a professional race driver_  
What can also address:

>They only know high level thinks for the most part. 

Professional driver doesn't need to know how every aspect of the engine works - they only need the general idea.  
Car mechanic doesn't even need a driver license!

>So the computer engineers have the same knowledge as a software engineer

No, they don't. They usually lack - tremble please - the higher level stuff and mathematical abstraction.

>For me it feels like computer science is an easier part of hole IT, CS, CE, EE field. 

~~Are you studying CE and have some insecurities?~~ Why?

---

Also, where the idea that ""low level is better than high level"" even comes from?  
Assembly knowledge will do you jack sh\*t when you are dealing with SQL database.",hv2cb5o,t3_shfux4,1643673549.0,False
shfux4,"Quite honestly at this point it just feels like the guy is a stuck up computer engeneering student.

At least it doest seem like an honest question to me. I envy you for having the patiance to answer these.",hv2dl6b,t1_hv2cb5o,1643674104.0,False
shfux4,"Nope, I've just checked their profile. Looks like OP just romanticizes the concept of embedded development.",hv2hrfb,t1_hv2dl6b,1643676061.0,False
shfux4,I’m actually a CS student. I found that my earlier post where a little harsh so I wanted to try again. I can’t seem to find a good answer on the internet. I’m currently finding hardware and embedd interesting and I’m trying understand if I should change path or not. Im sorry for being “weard” I’m stuck in my mind.,hv3t1ev,t1_hv2hrfb,1643699989.0,True
shfux4,"Do a physicist have the same electronics knowledge as a computer engineer?

Someone who studies physics learns how atoms work in low level. If they know the atoms, don’t they also know what pieces can be built on it and how?

For me it feels like computer engineering is an easier part of physics. They only know high level things for the most part. So the physicists have the same knowledge as an engineer and the physicist do also have knowledge of the universe and everything it contains. When does computer engineering education benefit from physics when the time to study physics and CE (both BSc and MSc) is mostly the same.",hv2oh1s,t3_shfux4,1643679060.0,False
shfux4,"I’m understand I’m wrong. Can you give me an example of subjects when software engineers have more knowledge than computer engineers. Algorithms, security, database?",hv3tnb5,t1_hv2oh1s,1643700422.0,True
shfux4,All of those,hv4q5la,t1_hv3tnb5,1643722888.0,False
shfux4,So it’s not common for a computer engineer to know database and security?,hv5mcai,t1_hv4q5la,1643735687.0,True
shfux4,"No

Add to that important topics in the CS/software industry like concurrency, distributed computing, AI.

There’s a difference between learning SQL (which many CE/EE do) and knowing when and why you should prefer a bitmap index instead of a B-tree.

There’s a difference between learning to use Spark/Hadoop and knowing why the task/job won’t gain any speed up because of the specific class of problem you are trying to parallelize.

I’ve worked with EE and CE people with masters degree and they struggled to wrap their heads around certain concepts that are (should be) trivial to CS people. Also, because their code/design decisions didn’t take those core concepts into account and often times that meant trouble to the business.

Unlike your comment, I’m not saying one profession is better than the other. The difference is that you rarely see CS people dabbling into CE/EE roles while the opposite is quite common. That’s because the job market is hot for CS roles not the other way around.

So, from a purely job market perspective. Choose wisely...",hv5u9py,t1_hv5mcai,1643738545.0,False
shfux4,"The trick is that each university has it's own approach to computer engineering. They or somewhere between electrical engineering and computer science somewhere.  Whether it is in the middle, or almost CS but using  engineering as a glue, etc. is really up to the university.

Example:

https://www.vik.bme.hu/en/education/programs/",hv7818j,t1_hv5mcai,1643757171.0,False
shfux4,"You are right that there is some overlap, but maybe not as much as you think. This is a CS grad perspective, so take my CE, and EE thoughts with a grain of salt.

CS focuses on things like Data Structures, Algorithms, Programming Language Theory, etc.

CE focuses on more EE types of things like designing circuits, low level programming like assembly, and how the specifics of a computer work.

I don’t think it is fair to say that CS is the easy IT degree. It may not be as Mathematically challenging as CE or EE, but it is a challenging discipline.",hv2d642,t3_shfux4,1643673918.0,False
shfux4,"> It may not be as Mathematically challenging as CE or EE

But CS is a derivative of mathematics. And CE is EE and CS combined.",hv2gzb6,t1_hv2d642,1643675693.0,False
shfux4,"I don’t think I used anything beyond Calculus 1 & 2 in undergrad CS. I know machine learning uses a lot of Linear Algebra, but I can’t remember using much math. Things we did use math for: Big O, CPU throughput, disk read speed/time, I can’t think of anything else. All of that, if I’m remembering correctly, didn’t use any math beyond Algebra. I only used Calculus in my Calculus and Physics courses.",hv4hk5b,t1_hv2gzb6,1643718350.0,False
shfux4,Haven’t you taken mathematical logic or computability/complexity classes?,hv5hqfj,t1_hv4hk5b,1643733998.0,False
shfux4,"We did have a Discrete Math course that I had forgotten to mention. It touched on logic, but was mostly focused on doing basic calculations on binary numbers, add, subtract, convert to hex and back, convert to decimal and back, and 2’s complement.",hv82vau,t1_hv5hqfj,1643769964.0,False
shfux4,"The word _algorithm_ is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī.  
Did you know that algorithms are math?

Have you heard about Turing Machine? The _mathematical_ model which is considered a symbolic birth of the field of Computer Science?",hv5uamf,t1_hv4hk5b,1643738555.0,False
shfux4,"True, though my Algorithms course didn’t focus on math, it focused on learning to apply established algorithms to solve problems. I swear it felt like we spent 2/3 of the course talking about hash tables lol

As for the Turing machine, I feel like it was briefly mentioned in my Introduction to Computer Science Theory course. That course focused mainly on formal languages, pumping lemma, and finite state machines.",hv83tmv,t1_hv5uamf,1643770366.0,False
shfux4,">Algorithms course (...)  
>(...) briefly mentioned in my Introduction to Computer Science Theory course. That course (...)

Did you know that Computer Science isn't just a name for a glorified school curriculum?

>it focused on learning to apply established algorithms to solve problems.

And? How does that not fit ""algorithms are math""?

>I swear it felt like we spent 2/3 of the course talking about hash tables lol

So... math.

>That course focused mainly on formal languages, pumping lemma, and finite state machines.

All of those? Math.

&nbsp;

Basically, everything what falls under Theoretical Computer Science falls also under Mathematics.",hv8u5dy,t1_hv83tmv,1643783542.0,False
shfux4,"Okay, dude. Whatever. I was speaking from my experience in my undergrad program. As far as algorithms go, they may have been derived/invented using math, but all I did was call functions, no math.",hv9jrds,t1_hv8u5dy,1643802654.0,False
shfux4,"As someone having an msc in CSE (computer science engineering) we learned all what you described, including AI and 3d graphics and the circuit design, assembly programming as well.",hv7797p,t1_hv2d642,1643756854.0,False
shfux4,"No,  they don't.",hv2wj8p,t3_shfux4,1643682561.0,False
shi03u,"At the very least if students are coming into Computer Science knowing how to program in JS or Python, then higher education can focus more on the theoretical stuff. Granted this is the US so poor schools won't get this education",hv2y0yz,t3_shi03u,1643683205.0,False
shath4,"I don’t have a proof for this, but my gut feeling says it can’t be done.

The reason is that count sort isn’t really a sort at all, it’s a cheat that lets you recreate the original sequence in order without doing a single comparison, by indexing the values in a bit vector.

As you know, this comes at a memory cost equivalent to the largest number in the sequence (or the difference between the largest and smallest). Any optimization of this would either need to project this range onto a smaller range losslessly or abuse some regular distribution property of the original sequence. Intuitively this makes it impossible to do for arbitrary input sequences. There could still be optimizations possible for special classes of inputs, but I don’t think you can do it for arbitrary inputs.

More to your point, all the ways I know of to efficiently represent sparse matrices/arrays end up losing the one property of bit vectors that count sort relies on: O(1) “ordered insert”.",hv1qp9p,t3_shath4,1643664827.0,False
sh0rr0,https://github.com/ossu/computer-science,huznz1s,t3_sh0rr0,1643635639.0,False
sh0rr0,Thanks a lot!,huzspgn,t1_huznz1s,1643637941.0,True
sh0rr0,Have a look at Teach Yourself CS: https://teachyourselfcs.com/,hv0jt4e,t3_sh0rr0,1643648870.0,False
sh0rr0,"There are many resources available online. You can check out [freecodecamp.org](https://freecodecamp.org) and [geeksforgeeks.org](https://geeksforgeeks.org). There are also several youtube channels available, you can check out some of them and then go with the one you find the best. You can also check out GitHub repositories as they also have a collection of good resources. There are paid courses too, but I would recommend going with the free ones initially because they are equally amazing. After you get some basic knowledge you can consider the paid ones if you want to.",hv0oslt,t3_sh0rr0,1643650695.0,False
sh0rr0,"If you want to do frontend, my university teachers literally taught off of w3schools (🤌🏻)",hv1p6we,t3_sh0rr0,1643664248.0,False
sh0rr0,There's the Open Logic Project if you want to learn about that (and basic Computability Theory),hv26754,t3_sh0rr0,1643670961.0,False
sh0rr0,https://GitHub.com/qvault/curriculum,hv34xkh,t3_sh0rr0,1643686290.0,False
sgzipv,"Its not really a two mins explanation but rather a whole course. 

I suggest looking into nand to tetris, while i didnt play it myself i heard only good things about it",huzl6j3,t3_sgzipv,1643634159.0,False
sgzipv,"https://nandgame.com


Nand To Tetris is a course but this is a game made from it that may help.",huzwog4,t1_huzl6j3,1643639731.0,False
sgzipv,I love this!,hv0i6k0,t1_huzwog4,1643648270.0,False
sgzipv,"It's a course actually, not a game, and I came here to recommend it as well! I'm doing it right now and can vouch for it. The nature of the course is starting with NAND gates and a rough explanation of bits and how and why gates work, and then through the course you build other logic gates, and then the internal chips of a cpu, and then a whole cpu, and ram, until you've built essentially an entire computer and OS that you can use to, for example, play tetris, and you've built it completely from scratch.

But to answer OP's question, I'm not really sure what step he's looking for between bits and logic gates...

Maybe electrical/electronics engineering? But as far as computer science is concerned, there is nothing between knowing what a bit is (literally just a binary value determined by power or lack thereof), and logic gates. If I have an And gate, the output is determined by whether or not both bits are ""true"" or ""on"". HOW that gate determines it is outside the realm of computer science, because it's engineering based, but I know it's got to do with transistors so that'd be a good place to start; maybe google exactly how logic gates are engineered.

Hopefully this answers your question or sets you on the right track, I wish I could help more!",huzml2e,t1_huzl6j3,1643634916.0,False
sgzipv,"> I suggest looking into nand to tetris, while i didnt play it myself i heard only good things about it

It's intended as a third year capstone course that reiterates on everything student have learnt and puts them to use, rather than as a course that teaches you these things as it goes.

i.e. it might be a bit much for OP",hv2e7ul,t1_huzl6j3,1643674384.0,False
sgzipv,"Get the book: Code by Charles Petzold. 

This helped bridge the gap between understanding TTL, and doing the large data path hardware stuff at uni.",huzvwgt,t3_sgzipv,1643639389.0,False
sgzipv,"^ I second this, great book",hv0x1ow,t1_huzvwgt,1643653728.0,False
sgzipv,"This is the definite correct answer

It’s the book that strips away the magic of how a computer works",hv486mt,t1_huzvwgt,1643711834.0,False
sgzipv,"I actually study at the university where Noam (from Nand2Tetris) teaches, and it's a mandatory course for us in CS.
It's a beautiful course that get's you all the way from basic Nand gates, through all logical gates, then to RAM storage, CPU functionality (though they don't really go into how the clock operates), then into giving commands to the CPU using Binary, then to translating a low level language into binary commands, then a high level language into a low level language, building a basic operating system and eventually writing programs for that operating system, for example a game like Tetris (hence from NAND to Tetris).

There is really a lot to explain to answer your question (how do nand gates turn into binary functions that do logic, how does a stack operate to allow namespaces and classes, how the screen is drawn etc.). But this course will surely clear most of it up. Noam and Shimon did a great job with this.",hv08ozz,t3_sgzipv,1643644663.0,False
sgzipv,Watch [Ben Eater's videos](https://youtube.com/c/BenEater). He has some really good explanations from basic logic all the way to a full computer.,hv00zgl,t3_sgzipv,1643641570.0,False
sgzipv,[Crash Course Computer Science](https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) might be helpful.,hv1d2f9,t3_sgzipv,1643659681.0,False
sgzipv,"Nand2Tetris is great, and will explain what you are looking for. Another great book that will explain this to you is [The Secret Life of Programs](https://www.amazon.com/Secret-Life-Programs-Understand-Computers/dp/1593279701/ref=sr_1_1?crid=13U1MBSSCZ842&keywords=the+secret+life+of+programs&qid=1643639632&s=books&sprefix=the+secret+life+of+prog%2Cstripbooks%2C88&sr=1-1)",huzwkbv,t3_sgzipv,1643639680.0,False
sgzipv,"""nand2tetris"" has already been mentioned and I would start there.
If you want to play with FPGA stuff to build hardware in software, have a look at https://www.amazon.com/Digital-Design-Computer-Architecture-Harris/dp/0123944244 as a supplementary resource.",hv0fnza,t3_sgzipv,1643647338.0,False
sgzipv,Computer Organisation and Design RISC-V Edition. Read the first few chapters and all your questions will be answered.,hv1j1pb,t3_sgzipv,1643661934.0,False
sgzipv,"It takes years of hard work to understand that. As another comment mentions you should try the nand to tetris course, it is a good course but it still won't teach you everything in detail. What I'd recommend is that you learn C and assembly(if you don't already know them) and then learn about systems programming and eventually slide into electrical engineering. That is the way to understand how computers go from bits to expressing logic in a somewhat detailed way.

If you wanna learn in a concise way, put some months and completely dig into the subject and search about what you do not understand, you will definitely find useful links to blogs, articles, and books that will teach you the stuff.",huznvfx,t3_sgzipv,1643635587.0,False
sgzipv,"You ever seen one of those pictures that’s a face, but when you zoom in it’s made up of faces? Computer logic is like that 7 times with the smallest face being binary. Look up the 7 layers… I forget the term but there are 7 layers of logic that are all the same thing they just each concern themselves with an aspect of all that goes into modern technology",hv1iqc4,t3_sgzipv,1643661817.0,False
sgzipv,I really like this explanation. It still boggles my mind thinking about recursion in Verilog,hv2dumg,t1_hv1iqc4,1643674222.0,False
sgzipv,https://youtu.be/Zz7mcHUNWjE,huzzx0k,t3_sgzipv,1643641126.0,False
sgzipv,"ASCII, and binary to twos complement. And digital logic.",hv00np4,t3_sgzipv,1643641436.0,False
sgzipv,"My stock answer is:
If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

This will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc, though it won't go into any detail of how common OS are implemented or how to implement your own (see /r/osdev for that). Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board"")",hv2eere,t3_sgzipv,1643674469.0,False
sgzipv,"As far as the philosophy is concerned, there's a functor between electrical circuits and logic, such that you can translate a logical model into circuitry. We simply exploit the fact that electrical circuitry behaves logically. And because logical constructions can compose and scale well, we've end up with modern computers capable of supporting vast logical models.

The big trick is that physically, electromagnetism and material physics allow us to create electrical circuits, which we can model logic on. You can use other physical systems to model logic, for example fluid computing. So computers are the result of this weird interplay between math and physics, where we can use a physical system to model an abstract mathematical one.",hv2o635,t3_sgzipv,1643678927.0,False
sgzipv,"In my advanced digital system design class we made a full cpu out of logic gates. It took a whole semester but it was really enlightening. This is what I got out of it in the most concise way possible.

Using only 5 gates you can get a 1 bit full adder. Input two 1 bit numbers and it’ll give you a result and a carry. The carry output from one can go into the input of another to make the adder able to handle 2 bit numbers. You can cascade as far as you want but modern computers use 32 or 64 bit architectures. Doing the twos complement with some more gates allows you to subtract two numbers. So we call the adder and twos complement circuit the arithmetic logic unit. Most real ALUs are super complicated but the result is the same. A digital circuit that takes two inputs, an operation code (add/subtract/AND/OR/SHIFT/ETC), and outputs a result. This is the calculator part of our calculator with a to do list.

Using more gates, you can make an SR NOR latch circuit that can hold data. Cascading these latch circuits like with the adder gives you registers. Usually a CPU has 32 registers built in and these are made with latches since they have to hold numbers. Each register holds a 32 or 64 bit number depending on the architecture. The ALU can only take input from the CPU registers so numbers that need to be crunched have to be moved from system memory to a register first.

The way this is all done is by a 32 or 64 bit number that is called an instruction. Which bit means what is defied by the instruction set, and usually they specify an operation, destination, source, and operand. Your destination, source, and operand comes from registers. 

add $1 $2 $3 
//add the numbers in register 2 and 3, and put it in register 1 
OP[31:24], DEST[23:16], SRC[15:8], OPERAND[7:0]

ld $7 300
//put the value in memory address 300 into register 7

These aren’t exactly correct but the idea is there. 
Finally you have a clock and a program counter that goes through memory one line at a time and executes the instructions. These instructions can jump to other spots in memory so you can create conditional jump statements to get your if statements. Above assembly, it’s all languages and compilers. 

Hope that cleared some of it up.",hv3s1nw,t3_sgzipv,1643699285.0,False
sgzipv,"Back in Uni when I had a course that used the concept of Turing Machines a lot at some point it made click

The simplest Turing Machine has 1 Belt with a row of 3 different characters on it, either 0, 1, or a Blank meaning nothing, the head can read one Character at a time and you have a table that dictates how the machine will behave. The machine has a state it starts in and for each state the behavior is defined depending on what it reads on the belt. So e.g in State Nr.1 if it reads a 1 it will change it to a 0 move the belt one character to the left and change to state 2.

This machine seems quite simple, but now imagine you have a different machine that has an alphabet of 0,1,2,Blank. Even though it has more letters in theory you can still convert any Programm it runs into a Programm that our simple machine can run by converting the characters on the belt into a unique combinations of 1 and 0s, so eg 1 becomes 01, 0 will be 00 and 2 will be 11, then you ad a step into the instructions for reading a sequence and then treating it accordingly.

Since this works with 1 extra letter it will work with N extra letters meaning our simple TM can simulate any Turing machine regardless of its alphabet with its basic alphabet.

Same goes for machines that have multiple heads or belts, you just need to add logic that uses sections of the belt that store positions of simulated heads or belts. So now since we know that our simple machine can work the same way as a complex machine, we don’t have to prove it for any machine and don’t have to worry about it, and can work with complex machines to solve more complex problems knowing that the simple machine can do it aswell

And with computers and logic gates and bits it’s analog. We have bits that are our binary alphabet, and we have logic gates that generate output depending on inputs. We know we can muse bits to simulate the decimal system, and we know we can combine our simple logic gates to things like adders that can do math. So while keeping this in mind no matter how complex it gets, it will all boil down to bits beeing combined with logic gates.",hv3tpqv,t3_sgzipv,1643700472.0,False
sgzipv,CMU CS 15-213,hv3unt9,t3_sgzipv,1643701166.0,False
sgzipv,"The book ""But how do it know"" does an excellent job at explaining it imo",hv44hrz,t3_sgzipv,1643708917.0,False
sgzipv,take a digital logic design course,hv4cj3e,t3_sgzipv,1643715061.0,False
sgvyvn,"Geometry is a mesh of points and connected lines. A graph is a set of nodes and edges. Same diff

That’s why eg image size optimization is internally removing pixels based on shortest path",hv2jhwg,t3_sgvyvn,1643676861.0,False
sgh0h8,"The reasons you put them together is that you can not really seperate them. For example, you typically implement Dijkstra's Algorithm using some form of priority queue. The algorithm for finding MSTs is faster with a union-find data structure. You need data structures for your algorithms and algorithms for your data structures. Arguably both are flip sides of the same coin.",huxnl6c,t3_sgh0h8,1643592782.0,False
sgh0h8,"Check out leetcode.com

People use it for hacking interview which tend to be advanced DSA topics",hv2n3wr,t3_sgh0h8,1643678456.0,False
sg4epv,An entire textbook couldn't cover all the details.,huu84fw,t3_sg4epv,1643540323.0,False
sg4epv,"There is so much to cover to answer that…

You have to remember that when we started this that you could not buy “a router” — we literally wrote code to move packets from one interface to the other.  All the routing was done by hosts - big computers with multiple users - and IMPs - specialized computers built to forward packets. 

I actually laughed when Cisco started, because building routers out of software was so commonplace and so frequently done that I couldn’t fathom why you would pay money for one… 🤣

Go look for the source for a piece of software called “routed” (pronounced “route-dee”, for “route daemon”). That’s readily available and has not only the packet forwarding logic but the routing protocol logic for the major routing protocols.",hutvrqb,t3_sg4epv,1643530742.0,False
sg4epv,"I’m gonna try to make this as simple as possible, 

a router receives level 2 electrical signals from a “local area network” that are sent from physical “local style” addresses/devices. This can be thought of as addresses that are “below” the router that are managed by the router. The router then prepares these electrical signal IE packets of data, and converts from the “local style” addresses to “logical addresses” IE: “IP addresses”. The router takes care of knowing which IP corresponding to a “physical address” it manages below it. When it receives signals back from levels above the router itself.

The router then sends its own electrical signals up to “level 3” which uses a new protocol called IP protocol to send electrical signals through this “higher level” network.

You should look up models of how this stuff works and research deeper on your own though. I’ve glossed over many detail and or maybe gotten some details wrong. Look up “OSI model”

The stuff that handles what to do with the electrical signals is basically a driver that is a program coded to handle it",huvjnlx,t3_sg4epv,1643563885.0,False
sg4epv,It would be fun to also ask this question on r/explainlikeimfive,huug7cm,t3_sg4epv,1643546133.0,False
sg4epv,Was just thinking that,huuvulr,t1_huug7cm,1643554346.0,False
sg4epv,"At a very high level - you're receiving internet packets (believe in IPv4 or IPv6 form) and then handing them on to the correct next step according to a list of rules you've got.

Imagine like you've got a massive pile of addressed envelopes, how would you go about delivering them?

Either there address is local enough to you that you can pop through letterbox or envelope is taken to a closer point and then the method starts again.",huvkkby,t3_sg4epv,1643564231.0,False
sg4epv,"You could start by learning how to program programmable switches using p4. It will give you some idea of what a router does (receiving packets and then deciding what to do with it depending on some bits). Like others have already mentioned, this will only scratch the surface but might be a good place to start.

Some links that might help 

[https://github.com/p4lang/tutorials](https://github.com/p4lang/tutorials)

&#x200B;

https://www.youtube.com/channel/UCOQAFkDKucJWr-KafdJsdIQ",huvjzg9,t3_sg4epv,1643564012.0,False
sg4epv,As others mentioned this is a huge topic but if you want to look at the code to one here is a link. https://forum.dd-wrt.com/phpBB2/,huvhjai,t3_sg4epv,1643563083.0,False
sg4epv,"ELI5 - There’s multiple isolated clusters that know how to communicate internally. Attached to those clusters is an endpoint that can traverse traffic cross-cluster

ELI10 - Clusters have unique numbers for there internal workstations (endpoints) and external receiver. You can target look up that number through a registry (eg dns or bgp)

ELI15 each cluster can internal host more clusters. Routing policies mimic the same lookup mechanism that we explained at Eli 5. Policies also filtering/forwarding and mutation traffic as it crosses between endpoints 

ELI 20. - The traffic is a stream of packets that use a protocol encoding to denote order and application specific data. You can layer protocols (see osi model) decoupling hardware, software, and routing designs 

Eli 25 - Packets route from the external endpoints to internal endpoints through the OS network interface. This construct uses bit masking to map the port to receiving application. This is a fancy way of saying it looks up a unique number and sends to an internal endpoint.. semantically like when you were 5",huzpuih,t3_sg4epv,1643636577.0,False
sfiktu,Can you share the Python code you used to generate these?,hurn5m3,t3_sfiktu,1643490880.0,False
sfiktu,I love these. Please upload the actual images and dm me with them. I feel like you achieved those dark academia vibes with a subject not typically represented,huq7kxh,t3_sfiktu,1643469794.0,False
sfiktu,Thank you for the kind words! DM'd!,hurcpky,t1_huq7kxh,1643486522.0,True
sfiktu,"DM me too if possible. 
And yes you nailed the dark academia vibes!",hurynlf,t1_hurcpky,1643495777.0,False
sfiktu,Thanks! DM'd.,huvnegb,t1_hurynlf,1643565312.0,True
sfiktu,Me too please? My mom majored in math in college and she would LOVE these.,huvnzdd,t1_huvnegb,1643565536.0,False
sfiktu,That's so sweet. DM'd!,huvpjzf,t1_huvnzdd,1643566132.0,True
sfiktu,If possible could you dm them to me too? They look fantastic,husjiq7,t1_hurcpky,1643505054.0,False
sfiktu,Thank you! DM'd.,huvnfy3,t1_husjiq7,1643565328.0,True
sfiktu,could you DM them to me as well and/or share the python code? thank you :) they look super cool!,husoaq1,t1_hurcpky,1643507171.0,False
sfiktu,Thanks! DM'd.,huvngta,t1_husoaq1,1643565338.0,True
sfiktu,Could you also DM me the images :),husyrmm,t1_hurcpky,1643511911.0,False
sfiktu,Done!,huvnhnc,t1_husyrmm,1643565347.0,True
sfiktu,These are awesome.  Nice work,hurpp70,t3_sfiktu,1643491946.0,False
sfiktu,Thank you!!,huvnim2,t1_hurpp70,1643565357.0,True
sfiktu,"Man, can you please share the code of Python?
Or share this images, man I’ll frame this, is beautiful asf",husrnii,t3_sfiktu,1643508680.0,False
sfiktu,Thank you so much!! DM'd.,huvnkg3,t1_husrnii,1643565376.0,True
sfiktu,"I would love to get the images too, they are gorgeous. Great job!",hux4i5j,t3_sfiktu,1643585183.0,False
sfiktu,I’d also love to have these on my wall! Been looking for science that’s presented in way that could be hung on my wall - this is it!,huywygz,t3_sfiktu,1643616340.0,False
sf7tjb,"Could you rephrase the question? There haven't been any notable programming paradigms developed recently, except maybe type-driven/type-oriented (like in Idris 2), and even that has strong roots in old theorem proving techniques.",huokv73,t3_sf7tjb,1643431485.0,False
sf7cyl,"I honestly wouldn’t focus so much on the titles, these can be really confusing as you have discovered. Focus on what a team is doing and what experience you need and that tells more or less the kind of work you’ll be expected to do. 

Programmer and software engineer if we pick apart the actual words would seem to mean that software engineers are more in the business of solving software problems in new ways. This can be creating or utilizing mechanisms to write solutions or algorithms creatively. Programmers may be more basic. But the lines are so blurred and I may very well be wrong on my intuition there.",huo2nzc,t3_sf7cyl,1643422718.0,False
sf7cyl,I have the same intuition 👍,huo5tti,t1_huo2nzc,1643424161.0,False
sf7cyl,"hmm, ok. I'm trying to learn how to code, and learned how to do a little bit of it through Python. But I don't know what I can do with it except to tell a computer what to do. I just know that I enjoy playing around on Python, but want to translate that into a job. Maybe I'm going through information overload.",huo6nie,t1_huo2nzc,1643424540.0,True
sf7cyl,Python is awesome! All software engineering / computer science is is learning more ways of telling the computer what to do. What clever sets of instructions you can give to accomplish tasks,huo7aqw,t1_huo6nie,1643424837.0,False
sf7cyl,"You won’t find a job without a degree without a pretty substantial portfolio and some personal projects. Maybe master python (OOP, argument passing, polymorphism, recursion, etc) and then get familiar with database like SQL. You’ll want to make sure your portfolio stands out as you’ll be applying for the same jobs CS grads will , and most jobs filter to toss applications from people with no degree",huoj6kz,t1_huo6nie,1643430602.0,False
sf7cyl,">But I don't know what I can do with it except to tell a computer what to do.

Theoretically, pretty much anything you can imagine - but you need to build a solid foundation to stand on first.

>Maybe I'm going through information overload.

Very likely, yes. I don't know how much you know, but my general advice to you is to take it a bit slower and try to make sense of each step along the way, staring with the very basics.",huoolo6,t1_huo6nie,1643433552.0,False
sf7cyl,"Slightly different words for the same thing. Usually ""software engineer"" is the formal job title, ""programming"" is what you're doing. Informally, we call people who program ""programmers"".",huo2wlc,t3_sf7cyl,1643422826.0,False
sf7cyl,"Gotcha, that makes a bit more sense.",huo6uh6,t1_huo2wlc,1643424630.0,True
sf7cyl,"* **Computer programming** is the process of performing a particular computation (accomplishing a specific computing task). You can program a website, a game, a database, a robot etc. (notice that programming doesn't always result in creating a software).
* **Software development** is making software, regardless of abstraction level, used technologies or complexity level. Creating simple calculator in Assembly for ZX Spectrum is just as software development as is creating Facebook front- and back-end using React for modern web.
* **Software engineering** is:
  * ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"" ~ IEEE
  * ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"" ~ Merriam-Webster
  * ""an engineering discipline that is concerned with all aspects of software production"" ~ Ian Sommerville",huo4vc0,t3_sf7cyl,1643423725.0,False
sf7cyl,">I want to teach a computer to do things based on what I code it to do. What is this called?

It is called programming unless you want to do it well, in which case it is called software engineering.  If you want to learn how and why either works, and find ways to improve upon that, it's called computer science.",huo8ye5,t3_sf7cyl,1643425605.0,False
sf7cyl,"Focus less on job titles and more on your actual set of skills, that is what will matter the most",huod236,t3_sf7cyl,1643427541.0,False
sf7cyl,"Software engineering is working with a team to design, build and launch code. You'll potentially work with product managers, designers, QA engineers and sales in the process. I've been a software engineer at a few companies including Amazon and spend less than half my day actually writing code and the other half meeting with my team.

Programming is focused around just the building part of software development. It can often be outsourced or contracted out.",huoqvrs,t3_sf7cyl,1643434898.0,False
sf7cyl,"It gets weirder because in Canada Engineer is a protected job title like Doctor.

So unless you have your Bachelors of Engineering you're not a Software Engineer.

Yet by American definition I am.",hupq8ln,t3_sf7cyl,1643460859.0,False
sf7cyl,"A programmer is anyone who writes a programme, while engineer is someone who develops an abstract idea and implements it with the best tools/people.",hups8fz,t3_sf7cyl,1643462094.0,False
sf7cyl,"Lol.

They are interchangeable. 

Software engineer, programmer, coder, Member of Technical Staff, Software developer, Software Development Engineer, Software programmer, software specialist, they're all the same thing.

Relax.",huocaid,t3_sf7cyl,1643427178.0,False
sf7cyl," Software engineer is someone with an engineering degree in the IT domain.

Everyone else might call him or herself whatever they want, like programmer, coder, etc. but not engineer.

The difference I have seen is the mindset: measure twice cut once. Everyone I worked with having an engineering degree  never jumped to coding. They always started with some level of abstraction and worked on the solution until it was correct on a logical level. They start working on the code only when this problem refinement was done, and the code is always clear, logical, and mostly works immediately. 

Also the ability to work on different abstraction levels if needed.",huow3d5,t3_sf7cyl,1643438154.0,False
sf7cyl,The titles are designed by someone in hr with no understanding of the role. They are describing the job title in a different way to prevent you from comparing salaries in the industry and collecting the market rate.,husdwew,t3_sf7cyl,1643502518.0,False
sf7cyl,"They’re all the same pretty much, just some advice, if you want to avoid a long and painful road of finding a job in tech , I’d recommend in starting a bachelors in SWE or CS. Otherwise you’ll be competing with those with degrees for this stuff and entry level is stupid competitive even for new grads",huoihnf,t3_sf7cyl,1643430242.0,False
sf7cyl,"A software engineer is a programmer is a developer is a coder. It’s all the same thing. What matters more is how adept you are with what technologies, what levels of abstraction are you comfortable operating within, how independently can you operate without guidance while still producing quality work, and how well you can integrate your work with the work of others in a way that improves the overall quality of the design.",huq7f0a,t3_sf7cyl,1643469723.0,False
sf5rtw,"Depending on the language, there are specific techniques to make memory allocation and garbage collection efficient. From partially shared data structures to generational garbage collectors, different functional languages address these concerns in ways that, sometimes, even end up being adopted by imperative language compilers too. In particular, pure functional languages have compilers that exploit the lack of side effects to more aggressively garbage collect. Other specific techniques such as deforestation (for example, when an anamorphism is followed by a catamorphism) and lazy (non-strict) execution can be leveraged to reduce the overall memory use and paralellize garbage collection.",huo10bv,t3_sf5rtw,1643421952.0,False
sf5rtw,"That makes sense! I can see how in a language that enforces pure functions you could garbage collect pretty aggressively as you don’t need to worry about a bunch of dependencies like in the jvm, you only need to worry about if a value is directly accessed again. I would imagine this would be much harder to pull off in a multi paradigm language however.",huo26l3,t1_huo10bv,1643422496.0,True
sf5rtw,"Exactly. However, generational garbage collectors, originally developed for Haskell, found their way into modern JVMs too, since they offer a number of advantages, like very parallelizable executions. Of course, in Java, dependencies make it more difficult to be too aggressive. There is a very good paper from Simon Peyton Jones on the generational garbage collector and some of these memory optimizations in pure non-strict Haskell.",huo2mvg,t1_huo26l3,1643422704.0,False
sf5rtw,That’s very interesting thank you!,huog6dd,t1_huo2mvg,1643429062.0,True
sf5rtw,"Don’t discount partially shared data structures too. 

If P is a linked list, and Q is the same linked list with one more item prepended to it (this is a common operation) then Q’s next pointer just points to P. Likewise if you create a new list R which is list P with the first few items lopped off, well, you can have R point to the middle of list P. Nobody is “allocating a whole new data structure.”

If somebody has a tree, and they want a new tree with a different value in a leaf node, you don’t have to allocate a whole new tree, you just have to allocate a whole new set of nodes on the path from the leaf up to the root (log n). If nobody mutates data, then we can share entire branches of the tree that didn’t change in that operation. 

Arrays are probably the biggest hiccup. Yes to truly simulate an array you’d have to allocate a whole new structure on each update. Various alternate strategies include not using an array (would a hash work instead?), having a special mutable array carve-out to make the language “less pure but more practical”, or sacrificing some aspect of arrayness such as: sacrificing O(1) lookups, secretly implement the array as a tree, and accept O(log n) look ups and/or updates instead. Log n access times??!? remember, a balanced binary tree can reference a million leaves in 20 comparisons… it feels like constant time access.",huplf73,t3_sf5rtw,1643457541.0,False
sf5rtw,"I’ll have to look more into that. I’d imagine that mapping certain functions to certain paths of a tree could get pretty complex with enough different “versions” of the tree or say something even more messy like an undirected graph. Is the differentiation usually accomplished with metadata stored about each node?

I can imagine replacing arrays with separate chaining hash tables could work as they can still get O(1) lookup.",hur1o7q,t1_huplf73,1643482037.0,True
sf5rtw,"Differentiation is usually accomplished by creating new nodes, not storing metadata about each node. Some of the pictures in this section might help: https://en.m.wikipedia.org/wiki/Persistent_data_structure#Trees",hurzkc3,t1_hur1o7q,1643496170.0,False
sf5rtw,Helpful resource thank you!,hutejr0,t1_hurzkc3,1643519531.0,True
sf5gj9,"A TCP load balancer will send a NEW TCP to the next server in a round robin manner, but once the connection has been established it just stays with that server. There is state that tracks the connection to route the packets for that TCP connection to the same server. This is the same as normal HTTP except that websockets typically keep the connection open for longer whereas HTTP connections for web pages or API requests are typically much shorter lived. This means that a naive RR LB may be less effective for Websockets unless it keeps track of how many connections a particular server still has open and tries to send new connections to servers with fewer connections.",huop9c6,t3_sf5gj9,1643433934.0,False
sf4ab2,Imma try some of those BBS numbers on page 8,huo3ipp,t3_sf4ab2,1643423108.0,False
sf4ab2,Really cool find. Thanks for sharing :),huorpr8,t3_sf4ab2,1643435402.0,False
sf4ab2,Glad you like it. The ads alone are gold!!,huos489,t1_huorpr8,1643435646.0,True
seqqpv,"A few things,

People already replied about how none is truly a subset of the other - which is true, but a lot of computer engineering students (and some EE as well) ends up with purely software jobs. And some cs students ends up in a more hardware/CE jobs. 

Its more likely for a CE to get into CE jobs as they took much more relevant courses (advanced computer arch, semiconductors etc) but those are mostly electives for cs in most places - so they could get into jobs requiring those course as well. 

CE jobs include to build the micro-architecture that runs a specific instruction set. Deciding the actual instruction set is done by very senior designers where the degree does not matter at all.

Its not programming in assembly but rather describing how a hardware component should behave - understand and study its limitation regarding temperature, voltage, frequency etc. 

Oh and 
>  Is computer engineering higher prestige than software? 

Prestige should not be a factor since you finished high school.",hul08jf,t3_seqqpv,1643378756.0,False
seqqpv,"Few definitions/descriptions:

  * **Software development** is making software, regardless of abstraction level, used technologies or complexity level. Creating simple calculator in Assembly for ZX Spectrum is just as software development as is creating Facebook front- and back-end using React for modern web.
  * **Software engineering** is:
    * ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"" ~ IEEE
    * ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"" ~ Merriam-Webster
    * ""an engineering discipline that is concerned with all aspects of software production"" ~ Ian Sommerville
  * **Computer Science** is the study of algorithms, computation and information. It spans a range from theoretical studies to practical issues of implementing computational systems in hardware and software.
  * **Computer Engineering** is a branch of engineering that integrates several fields of Computer Science and Electronic Engineering required to develop computer hardware and software.

As one can see, all of them overlap, but none is fully a subset of another",hukymcf,t3_seqqpv,1643378019.0,False
seqqpv,"I have an MSc degree in computer (science) engineering and have 20+ years of experience in the field.

In the past we had mathematicians and electrical engineering. Both area started to use more and more computers therefore two title was born, from the math side programmer-mathematician (a mathematician who writes computer software), and from the electrical engineer specialized in low-voltage current computers . The programmer mathematician brought in the pot the math parts, then electrical engineer brought in the engineering methods and principles. Now, it really depends on the history of the school.

If the school created it's program as an offshoot of the math department it will be called computer science, and they will teach you programming from the point when you already have a blinking cursor. If the school however created it's program from electrical engineering (like mine) you will start with the hardware, and start gaining skill from there.

There is a big part which will be common like programming languages, networks, algorithm theory/data structures/math, cryptography, 3d graphics, etc. so if you are doing a regular programmer job, it does not matter which direction you went.

However, there are differences as well. In computer engineering you will have some digital systems/electronics/embedded development/assembly/etc. courses, system design (like SysML), modeling and model validation, fault tolerant systems and maybe some specialized math courses as well (signals and systems for example).

What I see the biggest difference is the mindset. The engineering way is a very multi-disciplinary way which is based on the well established engineering principles (theory, model, validate, etc.). It uses things from lots of disciplines and combines them to solve a problem in a computer domain. For me the computer science looks more like programming, but on steroids.",hulcrdo,t3_seqqpv,1643383880.0,False
seqqpv,"In general terms computer engineers design computer *hardware* while software engineers provide the software.  There is some crossover in the areas of firmware, device drivers, etc.",hukxt19,t3_seqqpv,1643377644.0,False
seqqpv,Okay my understanding is that computer engineers decide what can be written on the machine because they make the architecture?,hukypql,t1_hukxt19,1643378062.0,True
seqqpv,"They decide it as much as car designers decide where a car can drive, with how much baggage and for what purpose.

^(Audi Q7 wasn't designed for agricultural work, yet I've seen it done)",hukz99b,t1_hukypql,1643378310.0,False
seqqpv,"Computer Engineers take hardware and software courses. Software engineers only take software courses. From a general point of view, computer engineers know how the computer will work from start to finish and produce hardware and software accordingly. Software engineers, on the other hand, abstract the hardware and can focus more. But I must underline that we have undergraduate education here. Undergraduate departments of universities are for training academician candidates. If they want to educate software developers, they can open a vocational course. I think software engineering is in this category.",hul8yqf,t3_seqqpv,1643382410.0,False
seqqpv,"You are basically saying that a guy who makes paper can make any mathematical equasion on said paper. Since he built the paper he knows how to write anything on it perfectly.

There is MUCH more to computers then you imagine. And knowing how to build one does not make you good at writing stuff inside it.",huln16p,t3_seqqpv,1643387706.0,False
seqqpv,"Prestige? I might be reading into this and if I am ignore the rest...

It's like you're trying to compare the two for clout. Sounds kinda douchebagy and shouldn't be how you decide an education or career path. If you study computer engineering you're not smarter or better than a software engineer, hopefully you're interested in hardware. Both have their roles.

Hell, you could be a garbage man, and that should be ok too and nothing to look down on if that's your calling. If the tone I'm picking up is accurate, this is going to make for real difficulty in the team fit department....",hul4t6t,t3_seqqpv,1643380723.0,False
seqqpv,"Yep I know it sounds like that, sorry. I’m currently studying BSc web development/software engineering but I’m also into hardware and embedded systems so I don’t know which path to choose. So I’m starting to think that hardware is cooler cause that’s the core and that’s why I should choose embedded systems. But I also like software…",hul5k8m,t1_hul4t6t,1643381038.0,True
seqqpv,"It's not about what's more cool in a prestige sense, unless you're rich already, it's about what you like, can stand doing, can do well at, and can find a job in.

Sure your degree might give you a bump depending on where you got it, or what its in, but that's only because a subset of people with the douchebag mindset made it through the team fit and personality test as a false positive.

Having done interviews for a large biotech firm, I cant tell you how many smart, but obnoxious people we've declined to go forward with. It makes bad team fit if wonderboy talks shit on the TPM, the test engineer, and the support personnel. You wouldn't want to do their jobs: it takes precision and exhausting levels of effort, and they too deserve respect.

This is in the general life advice category now: if you're really smart and gifted and are making great decisions, that's awesome. Good for you. It does not make you better than anyone else. If you're a genius, you didn't choose your genetics, or your luck, so you don't choose to be smart. Be humble. Also, be fair, plenty of people don't want to dedicate their lives to a profession, and that should be honored too, or they maybe have different callings. Many people arent as smart, or gifted, or didn't have the best start, and we shouldn't think poorly of them. Others are smart but uninterested in making money.This kind of ""I'm smart I deserve to look down on others"" attitude I see has lead to the world we live in now. It's a sad place where majority live in precarity, and have hard lives.

I thought VLIW was really cool in embedded.

I don't work in computer engineering, why not find someone who does and ask them about the profession, and how they like it etc?",hul8rwz,t1_hul5k8m,1643382335.0,False
seqqpv,Computer engineering is the physical part software engineering is the programming part,humx8b2,t3_seqqpv,1643405027.0,False
seq1gi,I hope that’s from a history class.,hukwpgn,t3_seq1gi,1643377122.0,False
seq1gi,"Sometimes I really miss college, this isn't one of those times",hulbcez,t3_seq1gi,1643383340.0,False
seq1gi,I’m missing it rn,hunmbpw,t1_hulbcez,1643415299.0,False
seq1gi,"That's mid-late 1990s PC architecture.   


ISA was the standard bus on the original IBM PC and XT. It hung around until the late 1990s for legacy devices with low bandwidth requirements, like modems and sound cards. This diagram shows that the entire ISA bus could be run as a single PCI device via the ISA bridge.   


Starting in the late 1990s, many of these devices could be integrated into the motherboard or emulated by the CPU and the ISA bus was dropped.  


https://en.wikipedia.org/wiki/Industry\_Standard\_Architecture",hulh5yz,t3_seq1gi,1643385526.0,False
seq1gi,"Legacy connection, or ""backwards compatability""",hukqztf,t3_seq1gi,1643374230.0,False
seq1gi,“The PCI has a bridge to the ISA bus so that the ISA controllers and their devices can still be used” This is and explanation from my lecture but the answer does seems convincing to me. Wyt?,hukrgop,t1_hukqztf,1643374482.0,True
seq1gi,"What are you confused about? ISA is a legacy bus from the 1980's, modern CPUs don't have direct attached ISA buses, because that's a waste of space.

Instead a direct attached PCI bus can provide a bridge and proxy legacy ISA for backwards compatibility of old expansion cards.

It seems silly now, but during the transition period it would have been a big deal because having multiple weird niche expansion cards was a lot more common and users wouldn't want to have to replace them all just to replace their mobo/CPU.

Edit: This is mostly relevant in industrial/scientific systems. They're usually the mostly likely to have extremely weird old hardware that they need to support decades after everyone else has moved on.",hul3k3m,t1_hukrgop,1643380197.0,False
seq1gi,"Are you confused about the PCI bridge too? Modern systems use PCIe, which *is not PCI*, and for systems with old PCI slots they use a PCI bridge. The bridge of either type is just that, a way to bridge an old standard to the newer one that the main system uses.

On a whim, are you confusing ISA (the slot/expansion bus) with ISA (the Instruction Set Architecture of the processor)? They are completely separate things.",hum3hy5,t1_hukrgop,1643393772.0,False
seq1gi,"IT's so the CPU can pull information from and put information on the devices hooked to the ISA bus. Technically other things on the bus could ""see"" the signals too.",humksl8,t3_seq1gi,1643400254.0,False
seq1gi,Erase it from the diagram and look carefully at the graph again. Can all of the components still communicate with the CPU? What functionality might be lost?,humblyr,t3_seq1gi,1643396790.0,False
seq1gi,"To talk to the sound, printer and modem controller (in the diagram at least).",hunob48,t3_seq1gi,1643416169.0,False
seq1gi,"A bridge allows one network to communicate with another. It is like having a group of english speakers and a group of spanish speakers that cannot communicate. The bridge would be the bilingual person translating between the two groups. Each group uses a different protocol to communicate, so without the bridge, they don't recognize or understand each other.",huohpq7,t3_seq1gi,1643429842.0,False
sepvgl,thanks,hun5viv,t3_sepvgl,1643408434.0,False
secwcr,"Some of the answers here are bullshit. At the end of the day you need the math for the topics you study. Computer science is not one monolithic topic, there are lots of branches and subjects within it. You definitely don't need to know trigonometry if you want to write a compiler or a database..

Having a strong mathematical foundation will help generally, but honestly, a lot of math will only apply to specific areas. If you want to work in graphics, game development, physics simulations, etc, then trigonometry is extremely important, as is linear algebra.

If you want to work in machine learning, or AI, then you need linear algebra, but also statistics, and probably some calculus, especially for deep learning.

If you want to focus on compilers, or programming languages and semantics, then logic and set theory is the most important.

If you want to be an expert in security or cryptography then you need to understand number theory, prime numbers, etc.

I would say for ""general"" computer science, the most important mathematics is logic (often called discrete mathematics) & algebra. Beyond that it's really up to what you do within CS.",huiswen,t3_secwcr,1643333201.0,False
secwcr,"For the vast majority of working professionals, even in fields where the mathematics is relevant, the important part is usually a good grasp of concepts. You definitely need calculus for a lot of topics in Computer Science, but mostly what that means is that you recognize an integration problem when you see one, that you know that the the derivative of a function is 0 at an extrema, etc. 

I have a PhD in Machine Learning, and if you dropped me into a Calculus II exam this morning, I might not crack double digits. I simply don't remember enough technical mathematics to be able to start from ""what is the indefinite integral of this reasonably complicated function f(x)"" to getting a correct answer. What I can do is see a real problem in my field and understand that what I need to do to get past that problem is formulate and solve the right integral. From there, there are loads of tools available to me that don't require me to be able to pass the undergraduate exam on the topic.

That said, to be able to get to that point in my field, I did at some point have to know how to pass the Calculus II exam, so if it's really something that you can't get past, it can still be a very real brick wall.",hul5km9,t1_huiswen,1643381043.0,False
secwcr,"For my degree linear algebra, calculus, and statistics were all required.",huiz9cd,t1_huiswen,1643335922.0,False
secwcr,"They were required for my degree, as well, but I never need linear algebra or calc to do my job(s). For statistics all I've ever really needed are the basics that I learned in high school - mean, median, standard deviation, and I only use those when discussing the performance of an algorithm.",huj5c5n,t1_huiz9cd,1643338568.0,False
secwcr,">If you want to focus on compilers, or programming languages and semantics, then logic and set theory is the most important.

Hot take: If you are unable to understand high school algebra you are going to have a very hard time with stuff like Galois connections. The formal reasoning is the same everywhere.",hukmgo7,t1_huiswen,1643371535.0,False
secwcr,"By discrete maths and algebra for general compsci do you mean abstract algebra? (Groups, rings, modules, category theory etc)",hukavyl,t1_huiswen,1643363053.0,False
secwcr,i would agree with this answer in a software development sub but in a computer *science* sub?,hujjz6g,t1_huiswen,1643345385.0,False
secwcr,"I actually majored in Math and CS, so I think I have a good perspective. I honestly used very little of the math for my CS. My CS was split into two, theory and application. The theory was mostly about proving properties of algorithms, like correctness or complexity, or things like denotational semantics, proving properties of languages, and lastly topics like abstract machines. It was very theoretical but not very math heavy. The applications side was the same, learned about CPU architecture, programming languages, compilers, data structures, computer vision, databases.. more I don’t remember, again not a whole lot of math to be honest. I would say I rarely used math beyond high school level during my CS. There are some exceptions, like I mentioned. For computer vision it’s very important to understand linear algebra and trigonometry, but that’s quite a niche topic.",hujkv9n,t1_hujjz6g,1643345843.0,False
secwcr,"I think you're limiting your definition of math in saying you used very little math.. 

To me, 'proving properties of algorithms, like correctness or complexity, or things like denotational semantics, proving properties of languages, and lastly topics like abstract machines', are literally math. You don't have to be familiar with a huge number of mathematical objects, but you're applying mathematical reasoning and using math's tools.

If you're finding bounds on functions (deriving algo complexity), or using induction to prove correctness, you're doing undergraduate-level math.",hujn81b,t1_hujkv9n,1643347089.0,False
secwcr,"Yeh fair enough, getting the definition right is important, in math and on Reddit!",hujq94t,t1_hujn81b,1643348777.0,False
secwcr,"There are a great many software developers on this sub who think they are computer scientists. Likely because their university called their degree or major computer science, but that doesn't mean they aren't still wrong, either.",hujw4dq,t1_hujjz6g,1643352342.0,False
secwcr,"You definitely need to know things that require trigonometry to learn (or at least are approached via trig even if they don't strictly require it) in order to write a good, performant compiler or database.",huiz4wg,t1_huiswen,1643335868.0,False
secwcr,What do you need trigonometry for in a compiler?,huj55ni,t1_huiz4wg,1643338488.0,False
secwcr,"Not exactly what you asked but, I mean, the math behind compiler shit is so difficult (think about language automata, grammars, parsers, optimizations) that I'll go out on a limb and say that if you can't understand trigonometry don't even try to write a compiler.

Sometimes, especially in undergrad, learning math is useful to teach you mathematical thinking, whatever is the subject.",hujyztx,t1_huj55ni,1643354230.0,False
secwcr,Not what I said. Read it again.,huj6flh,t1_huj55ni,1643339044.0,False
secwcr,What do you need to know that requires trigonometry to learn in a compiler?,huj9o8u,t1_huj6flh,1643340482.0,False
secwcr,"Calculus (actual, not lambda) is usually built from pieces including trig (hence the name of the OP's class) and is used extensively in optimisation. You can write a compiler without it, but you can't write a good, performant compiler.",hujay87,t1_huj9o8u,1643341052.0,False
secwcr,How many people write databases and compilers? A tiny minority of devs,huj4io2,t1_huiz4wg,1643338209.0,False
secwcr,Most devs aren't computer scientists.,huj6hbs,t1_huj4io2,1643339065.0,False
secwcr,It will depend on what problems you want to solve. Most computer programming solutions will be mathematical in nature. Data Structures and Algorithms or heavily based on mathematical theory.,huikdxe,t3_secwcr,1643329623.0,False
secwcr,"Answers here already cover most of it: if you want to be a programmer, understanding the math you use isn't exactly necessary. If you want to stay in computer science beyond an introductory level, you'll do well to actually understand what the math you use *means*. 

Computer Science (and many other STEM fields) are fields that train you to think. Understanding ""why"" something is the correct answer is arguably more important than knowing the answer itself.

But this also goes for any STEM based field, it's not really enough to just memorize steps like you're following a recipe; that will only get you so far. A cookbook is really useful for a burgeoning home cook but if the aspiring chef doesn't ever think about why types of flavors work well together or why a particular ingredient was used in one recipe but not the other... then they'll really only ever be able to cook the things in the book. It will be much more difficult to create dishes of their own.

Any area of study that you pursue, if you don't understand the basics for *why* something is useful to the problem you're solving, then generalizing what you know to slightly different problems will be that much more challenging.",huj0xtr,t3_secwcr,1643336648.0,False
secwcr,"Computer Science, as an academic field, is very mathematical.

Most people who graduate with CS degrees work in jobs that do not require a lot of math.",huiofaw,t3_secwcr,1643331305.0,False
secwcr,Remember that computer science does not equal programmer. Computer science is pretty much math but with computers.,huisibf,t3_secwcr,1643333034.0,False
secwcr,"""Computer science"" and ""programming"" are two different things. You don't actually need much math for programming except for in certain fields (e.g., programming a game physics engine).",huiqxy1,t3_secwcr,1643332365.0,False
secwcr,"It really depends on what you mean by ""understanding the underlying workings of it"". And it also depends on your specialization.

For CS in general, it helps to have a feeling for maths, but it's not required to be an absolute math whizz. However, understanding the logic behind math will help you a lot with algorithms, automata and basic programming components such as recursion.

For cryptography, group theory is pretty important, and it is also the more mathematical part of CS. For programming, or web/network security, it is less of a focus.

You can absolutely get through CS without understanding all the specifics of calculus or trig (afaik they don't go that deep into it either), but you shouldn't despise it. A lot of the fun of computer science is figuring out how things work exactly and precisely, modeling it and translating it into language that is unambiguous. You can see how it is similar to maths in that way.",huirrc0,t3_secwcr,1643332714.0,False
secwcr,"Everything is tough the first time you learn it. Calculus will get easier (algebra is actually more important). The first day of calc we did limit proofs and I was so lost. Years later delta epsilon proofs seem trivial. Just attend the lecture, and then if you are interested, do more research. Linear algebra is definitely your friend.

Edit... memorize the unit circle, very helpful.",huj98iw,t3_secwcr,1643340289.0,False
secwcr,"I am sorry that you have to go through all this because of degree fetishism, even for seemingly unrelated jobs like CRUD applications. But university should not give you automatic job expectation. You should ideally be there for science, not to have a guarantee to land a webdev job.",huka7nc,t3_secwcr,1643362508.0,False
secwcr,Computer science is literally mathematics. You did sign up to be a mathematician.,huik11r,t3_secwcr,1643329475.0,False
secwcr,Is this a joke?,huj4ca4,t1_huik11r,1643338132.0,False
secwcr,"You must not even understand what computer science is to ask such a question... Computer science is not a synonym for programming or software engineering, it is a branch of mathematics that focuses on the theory of computation, efficiency, etc.",huj85rd,t1_huj4ca4,1643339816.0,False
secwcr,Exactly,huj9dmh,t1_huj85rd,1643340351.0,False
secwcr,"Not to be a jerk about the question, but I don't understand why the math component of cs is such a concern. 

I feel like this is a recurring post at least monthly",hundw9k,t3_secwcr,1643411722.0,False
secwcr,Math to some extent is important In comp sci but to fully understand computer science you by no means have to be a math expert. I had the same question when i started my degree in computer science and the answer that was given to me was that I'd need to learn the math that was relevant to certain aspect of the field but unless you go into a job that requires advanced mathematics you probably won't require anything beyond calc 2 or 3. I'm no expert though and others probably have different experiences in the field.,huik9s1,t3_secwcr,1643329575.0,False
secwcr,">be a math expert

Good thing those math credits weren't counted towards my major average....lol",huj5vre,t1_huik9s1,1643338809.0,False
secwcr,"Depends on the uni. Some CS programs are more math heavy 

In real life, only a small percentage of devs use heavy math",huj4akg,t3_secwcr,1643338110.0,False
secwcr,"\>  I need to literally understand why   
everything works the way it does for Trigonometry, Calculus, and   
whatever other math is needed for Computer Science.

What specifically do you mean by this? Are you asking about understanding what it's built of? Or like formal proofs?",hulhts2,t3_secwcr,1643385773.0,False
secwcr,"You actually did sign up to be a mathematician. Computer Science is a sub field of math.

Almost all of computer science is math, you just don’t think it is. Discrete math, combinatorics, category theory, are all things you will use on the regular when writing code, but it’s very different math than what you are thinking. 

And no you don’t need to know calculus generally.

Math is much much bigger than just algebra/calculus/geometry",hujyjjk,t3_secwcr,1643353921.0,False
secwcr,">Like, I didn't sign up to be a mathematician

On a tangent, CS itself is a runaway branch of Mathematics. It (or at least it's theoretical basis, which all that hardware just implements) was spawned literally by Mathematicians.",hujkfpn,t3_secwcr,1643345618.0,False
secwcr,"So, will I have to explain how math works at the fundamental level? I don't think even Computer Scientists know everything that there is to know about math.",hujljfp,t1_hujkfpn,1643346196.0,True
secwcr,">So, will I have to explain how math works at the fundamental level?

You'll have to know enough math to be able to understand the concepts in a particular sub-field that's employing that math.

**Example** : Lets say you're solving a recurrence relation by substitution (useful for finding time complexity of recursive algorithms).

You need to be able to recognize an arithmetic/geometric progression when you see one and know how to find sum of such progressions.

Stuff like that.

On a different note:

1. Being able to apply math and understanding how it works aren't two separate things. To do the former, you need the latter.
2. If your college demands you take a math course, you'll be taking it and understanding the content, one way or another.
3. There are topics in Discrete Maths (commonly taught in CS programmes) that do explore how certain bits of Math work at a fundamental level (eg. formal definition of what a Boolean Algebra is in terms of sets and lattices).

...

If I had to sum it all up, if you're going to do CS, it's not very productive in the long run trying to create a border between what math you can skip and what you can ""comfortably bear"". Things will become unnecessarily complicated.  

Just go ahead and master whatever bit of math they throw at you. Get comfortable with it.",hujmvik,t1_hujljfp,1643346905.0,False
secwcr,"Yeah, I mean I'm up to the task; really, I'm just trying to gauge the expectation.",hujn15i,t1_hujmvik,1643346989.0,True
secwcr,"> I don't think even Computer Scientists know everything that there is to know about math.

No one knows everything there is to know. There's just so much out there no one person can know even half of it.",hulibh7,t1_hujljfp,1643385961.0,False
secwcr,"Not a single person knows all there is to know about mathematics. It's a huge domain and computer science is only a subset of a subset.

You don't need to worry about perfect recall as long as you know enough to derive one idea from another. Professors will forget a lot of the basics through their time, and what you're really supposed learn in a university maths degree is to ""mathematical literacy"". It's like learning to read, you don't need to read all books or memorise all words, just how to parse what's in front of you, and use context to ask the right questions.

If you want to be a software engineer, you really only need to know the bare minimum for whatever field you're working in. The ability to reliably recognise the rough outline of a problem and to Google around for a solution will probably get you into the top 20% of developers.",hulcegm,t1_hujljfp,1643383745.0,False
secwcr,"I've been a code monkey for 40 years now; for the past 20 I've worked as a back-end developer for high-performance computing clusters. I've never needed math more complicated than boolean algebra to do my job.

That said, there are areas of computing where you **will** need more math - crypto, for example,  or image or signal processing. Beyond that, you need to be able to think logically and have a firm understanding of how your code and your runtime environment work.",huj50nk,t3_secwcr,1643338427.0,False
secwcr,Really depends on what field you go into.,huk9bdo,t3_secwcr,1643361792.0,False
secwcr,"If you want to get through school with a degree in CS then yes you need to know a lot of math. Math details the language of logic and is what the actual science of computers is. If you want to be a developer who writes application code then no you do not need to know the theory that in depth, but it is harder to get a job as a developer without the degree and you might face some inequality in the workforce for not having one.

Yeah you are gonna have to truly understand the base of it all, depending on your country and school, but if proofs is in your curriculum then you gotta really understand it",hukzwc0,t3_secwcr,1643378604.0,False
secwcr,"probably not, but to pass your next test maybe.",hul8syl,t3_secwcr,1643382347.0,False
secwcr,"It’s not necessarily about any specific topics in Mathematics, but the general aptitude and confidence to grasp a given mathematical concept - is what’s required of a good computer scientist. 

I might not have read about modular theorems before, but if it ever comes up as a background for something real, I must be capable of picking it up swiftly. This general ability and state of the mind is what’s truly needed.",hulkps4,t3_secwcr,1643386852.0,False
secwcr,"You don’t really need to understand every nook and cranny, just the general concept.

I’m a CS student & as well a math minor in my second semester. Majority of my home work is based on general calc & trig for math & just typical coding problems (I’m a first year so I know it’s nothing as of now) 

Math in general for CS just basically helps you, it’s just your decision if you want to spend a lot of time into it.",hulokkx,t3_secwcr,1643388270.0,False
secwcr,Calculus is very valuable for CS.,hum9da2,t3_secwcr,1643395954.0,False
secwcr,"From what all of my academic advisors have told me is that math is something that might apply to your career, but the bigger emphasis (at least at my school) is the problem solving skills you learn is the main benefit. Learning how to solve a problem using concepts you know is super beneficial. 


Im in calc 1 right now and they care far more about the why of the theorem opposed to the computational skills (grades care about computational skills, calc as a subject cares more about the why)",humsez1,t3_secwcr,1643403168.0,False
secwcr,Being good at maths makes you a better programmer imo cause it allows you think about ways to solve problems faster,hune1ek,t3_secwcr,1643411781.0,False
se2nq0,Stuff like that all fall under type theory I think.,huh08gr,t3_se2nq0,1643308252.0,False
se2nq0,You’re looking for type theory and effect systems.,huhavuu,t3_se2nq0,1643312112.0,False
sdxd4b,Freebie  http://www.cl72.org/110dataAlgo/Algorithms%20%20%20Data%20Structures%20=%20Programs%20\[Wirth%201976-02\].pdf,hufl15i,t3_sdxd4b,1643287967.0,False
sdxd4b,"Thank you for your reply! I have access to O'Reilly Learning through my institution so if there is also any paid books they're likely on there.

I will check this out anyway!",huflewu,t1_hufl15i,1643288178.0,True
sdxd4b,A Common-Sense Guide to Data Structures and Algorithms. Perfectly suited for beginners.,hufncu1,t3_sdxd4b,1643289209.0,False
sdxd4b,">A Common-Sense Guide to Data Structures and Algorithms

Thank you! Having a quick flick through this looks exactly what I want - will mark this thread as solved!",hufq5jc,t1_hufncu1,1643290577.0,True
sdxd4b,"If you’re a visual learner, grokking algorithms (the actual book not the educative.io course) is really nice for beginners",hufsr43,t3_sdxd4b,1643291791.0,False
sdxd4b,Algorithms by sedgewick and wayne hands down. Still one of my faves.,huigqi5,t3_sdxd4b,1643328098.0,False
sdur26,The Algorithm Design Manual by Steven Skiena. He’s a professor at State University of New York and hes posted all his lectures online. https://www3.cs.stonybrook.edu/~skiena/373/videos/,hufwzma,t3_sdur26,1643293671.0,False
sdur26,Grokking algorithms is also a good book.,huf7mam,t3_sdur26,1643278836.0,False
sdur26,I'm in the same exact situation as you're and I really find the book A Common-Sense Guide to Data Structures and Algorithms By Jay Wengrow to be really really good.,huf9bk8,t3_sdur26,1643280191.0,False
sdur26,Seconded this book. Read this OP.,hufgq7w,t1_huf9bk8,1643285429.0,False
sdur26,"hey i have a question, should i learn algorithms first or data structures?",hufbok0,t1_huf9bk8,1643281983.0,True
sdur26,"I found it easier to go through the basic structures first, then basic analysis then basic algorithms",huffadu,t1_hufbok0,1643284488.0,False
sdur26,"Just go in order of complexity, start with simple DS, then simple algos, then more complex DS, then more complex algos, etc.",hug9anp,t1_hufbok0,1643298572.0,False
sdur26,"You start with the most basic algorithms, then most basic data structures and then more advanced basically simultaneously",hufc2he,t1_hufbok0,1643282267.0,False
sdur26,"thanks! :D oh and btw, for data structures... do u recommend the introduction to data structures book?",hufdec9,t1_hufc2he,1643283219.0,True
sdur26,"Almost all, if not all, books covering algorithms also cover data structures",hufvkzg,t1_hufdec9,1643293060.0,False
sdur26,The Art of War  - sun Tzu,hugxiwg,t3_sdur26,1643307300.0,False
sdur26,Grokking algorithms is a great intro,hufjcx6,t3_sdur26,1643287028.0,False
sdur26,"I found Sedgewick to be approachable for self study and a decent intro to the topic.  He has versions of his book out with code examples in many popular languages such as ""Agorithms in C"" or ""in Java"" or whatever.",hugv7p2,t3_sdur26,1643306473.0,False
sdur26,Intro to algorithms 3rd edition,huhdung,t3_sdur26,1643313196.0,False
sdur26,"""The Design & Analysis of Algorithms"" 3rd edition by Levitin, Anany is a good algo book that is language agnostic. Used currently by my prof in an Algo class I'm taking (BS in CS).",huhroqp,t3_sdur26,1643318306.0,False
sdur26,CLRS.,huf5bhm,t3_sdur26,1643276963.0,False
sdur26,"To clarify for people who might not get it:

Introduction to Algorithms, by Cormen, Leierson, Rivest and Stein.",hufnocx,t1_huf5bhm,1643289366.0,False
sdur26,"I see this infatuation with CLRS being promoted everywhere and touted as the be-all-end-all book to (self-)study for the subject. I honestly believe that this does more harm than good. I tried to self-study under CLRS before I started college. It made me scared of algorithms. I went back to it a few times during my undergraduate years, but I still never found it to be all that pleasant of a read. 

I still have this book on my bookshelf. I do go through it from time to time, and by this point, I'm familiar enough with the subject area that I no longer find it as daunting as I did during undergrad. However, for a self-learner, I truly believe that the road to understanding how to understand this book is a decade-long endeavor. For this reason alone, I don't think it is a good recommendation to push onto people as a self-study text.

----------------

I feel like the reason for the hype of the book is because of existing hype. This book was great when it first came out because it was one of the first comprehensive self-contained textbooks that generalizes several modern algorithmic patterns. It made the runtime analyses less of a combinatorial mindfuck and relegated it as almost a secondary citizen in comparison to the intuition and the soundness proofs. It deserves a definitive place in the history of modern algorithmics. By comparison, it is fairly accessible and acted like a great cross-disciplinary introduction to the systematic study of algorithms. Pick any modern Algorithm Design text: Skiena, Kleinberg, Sedgwick, and go through its table of contents. Their sections are almost all laid out in the same order as CLRS.

This is why CLRS is impressive, it is so comprehensive that even after over a quarter of a century, new textbooks in the field still follow the same overall structure that it first pioneered in the early 90s.

However, it's not an easy read. I don't just mean that this is a dense technical brick. 

It lacks a certain coherence. Some of the problems emphasize intuition, others emphasize rigorous proofs of time complexity, and still others emphasize a non-intuitive leap of logic required to prove the correctness of these algorithms. How do you show the correctness of greedy algorithms? Beats me. Within specific chapters, the problems and examples presented seem to jump all over the place as well. Even within the same example, you trampoline around from intuitive constructions onto awkward case analyses of some (unintuitive) abstract representation of the problem. There really isn't a unifying thesis for each chapter (or even each problem it discusses). Of course, there aren't any algo-design texts out there that have solved this problem, but I've definitely gone through a few other textbooks that are significantly more coherent and understandable. (In fact, take any two of Skiena, Kleinberg, Sedgwick, Erickson, and Dasgupta and you'll have a more readable presentation of everything covered in CLRS).

----------------------

Look, I don't want to knock on CLRS for the sake of knocking on it, but I really don't think that this is the textbook to push onto people looking to study algorithms on their own. 

I took both of my undergrad and graduate courses in algorithms with Kleinberg (both the textbooks and the lecturer) and the one thing that I am most grateful for in my second attempt to understand this material is the clarity of the presentation. If it weren't for the fact that I had a fantastic lecturer, it would have taken me many more trials before I would have been able to grasp the topic. This despite my general view that Kleinberg's textbook is much more difficult to get through than CLRS.

Unfortunately, CLRS - on its own - does not stand out particularly well when it comes to clarity of presentation.

----------------------

First - texts focused on **designing algorithms**:

I can also talk about the (non-CLRS) textbooks that were popular when I was in undergrad: Ericksons, Skiena, Kleinberg, Sedgwick, (and Dasgupta).

My favorite of the batch is **Sedgwick's Algorithms**, it's the most accessible of the batch, and takes a very grounded approach to help build up your intuition for the subject. For many of us, rereading it after more rigorous training may feel like this is almost too introductory, but that's what makes it an especially great guide to learn from on your own. My favorite part are the runnable examples that help to (literally) visualize these algorithms you're studying.

**Skiena's Algorithm Design Manual** is definitely the funnest read of the batch. The presentation on the actual algorithms themselves is great, it's got a light-hearted expository style like K+T, with a good dose of visual intuition like Sedgwick. However, the best part of the text is the pacing. It's the only textbook so far where I didn't feel fatigued after reading more than one or two sections at a time. This is because Skiena sprinkles small doses of breathers throughout the text; every 10-20 pages of text (mod exercises), he would add some fun anecdotes (""war stories"") related to the material. These are surprisingly interesting and engaging for a textbook on algorithm design, and it definitely helps to pace the book in a way that few other textbooks have really thought to do. 

On the flip-side, unlike CLRS, K+T, or Erickson, the ""Manual"" isn't nearly as comprehensive in the types of material covered. In terms of the core theoretical algo-design foundations, we get through the basic DS, search, graph traversal, and some dynamic programming + approximation algorithms. However, instead of drilling deeper into these topics and, for e.g., complexity + tractability, the ""Manual"" pivots in the second half towards more practical applications. Many of these aren't really what you'd take away from from a typical algo-design undergrad course, but they're nevertheless very insightful, and significantly funner to read and learn about.

I haven't gone through all of **Erickson's textbook** yet, though this post motivated me to reread through a few chapters (Recursion, Dynamic Programming, Greedy Algorithms, and the two chapters on Max Flow/Min-Cut and applications of network flow). It's really very well written and well presented. In fact, I would even go as far as saying that it's genuinely fun to read through the chapters! I think it still suffers from some the same problem that many other algorithms books suffer, but it's definitely one of the most readable and accessible books on the market. The problems and algorithms presented are fairly standard (and covered without the depths of its denser counterparts). 

It feels a bit ""dated"" in the sense that it's still an exact (more-or-less) subset of all of the other Algorithms texts out there. For example, the use of Tower of Hanoi to illustrate recursion; Fibonacci #s, edit distance, and substring partitioning for dynamic programming. Gale Shapely's solution to stable matching, maximal interval scheduling, and huffman coding for greedy algorithms. These problems more or less encompass the entirety of these chapters as well. That said, that's not really a weakness. There hasn't been any significant breakthroughs that can/should be covered in an introductory monograph on Algorithm design. However, I'm impressed at the depth this book goes into for network-flow type reductions; until recently, this wasn't a topic indulged in much depth outside of Kleinberg & Tardos's Algorithm Design. (the bonus chapters also sound like fun extensions)

At the same time, I think my main gripe is that the text is still very heavily focused on presenting why a construction or design of a solution is the right one. This is obviously important, but it doesn't quite impart you with the intuition to design algorithms more generally. For example, why did we use this functional recurrence for edit distance and not look at other subproblems? How did we magically think about looking at the finish time for optimal interval scheduling? Why didn't we look at other types of network constructions for Baseball Elimination? That said, the section on stable matching does go down a false start first before giving the G&S construction. Of course, this is a shared problem that almost all textbooks share, nevertheless, I would have loved if there was a textbook that delves more into why seemingly intuitive algorithms don't work.",hujqz3z,t1_huf5bhm,1643349193.0,False
sdur26,"
--------------------

Next, onto textbooks about **analysis of algorithms**:

I love playing around with generating functions, and **Sedgwick's Analysis of Algorithms** spends a significant portion of the text on how to do these very novel and creative runtime analysis with tools like generating functions. He also published another text on Analytic Combinatorics, which is presented as a series of monographs of his own personal research interests. It's surprisingly accessible, very visual and intuitive, and leaves you with a feeling that you can count just about everything, or at least you can encode these problems as real-valued functionals and look for their singularities to understand how to count them. 

Next is **Kleinberg's Algorithm Design**, but again I'm biased because I was his student for algo. In contrast to CLRS, the best part of Algorithm Design was the exposition and the style. It's still very technical and theoretically inclined, but Kleinberg + Tardos put a lot of effort to make the text more readable. The other distinguishing feature of Algorithm Design is its inclusion of some more exotic algorithms that you usually don't find in a first course on algo design. 

Both K+T are algorithmic game theorists, and their influence in these and econometric algorithms shows through in their text. Most algo design texts mention reductions to max-flow/min-cut. Algorithm Design however spends significantly more of its tree-budget on network-flow type algorithms, both in the Network Flow chapter, as well as throughout in Greedy Algorithms, Approximation Algorithms, and in the discussion of complexity and tractability. His (in)famous course (CS 4820) on algorithm design also spends about a third of the semester on Network Flow back when I took the undergrad course.

Otherwise, I've never read through any significant chunk of Dasgupta. I hear good things about it, it's definitely the smallest text of the bunch.

Beyond these, the field has become significantly more popular since I've left school, so I don't doubt that other textbooks / resources have started to overtake these.",hujqzgu,t1_hujqz3z,1643349199.0,False
sdur26,"The 4th Edition is planned to  be released in March this year ""A comprehensive update of the leading algorithms text, with new material on matchings in bipartite graphs, online algorithms, machine learning, and other topics. """,huk0hw5,t1_huf5bhm,1643355250.0,False
sdur26,"Algorithms with C by Kyle Loudon. It is a very well written book that takes you from basics. The only annoying thing is the comments in the code waste so much space. Other than that, it is a very good book.",huh7xsl,t3_sdur26,1643311029.0,False
sdur26,"https://teachyourselfcs.com

This is a great website that points you to various resources for teaching yourself CS concepts.",huilgti,t3_sdur26,1643330075.0,False
sdur26,"You want lectures and other materials by Leiserson and/or Skiena. Leiserson taught most people writing on the subject, including Skiena. Both have lectures available on youtube, and have authored books together.",hugit8w,t3_sdur26,1643302096.0,False
sdur26,"algorithms to live by, it's not a book for CS education but very fun and useful for beginners",huh9kaj,t3_sdur26,1643311624.0,False
sdur26,Not a book but something to use as a companion piece for whatever book you end up going through. https://www.geeksforgeeks.org/fundamentals-of-algorithms/ helped me out so much.,huhctk5,t3_sdur26,1643312822.0,False
sdur26,I'm using algorithm design by kleinberg and tardos right now and it's great.,huhep3x,t3_sdur26,1643313506.0,False
sdur26,"I haven't gone through it but I picked up  ""Algorithms In A Nutshell""  by George T. Heineman, Gary Pollice, and Stanley Selkow.",huhf58k,t3_sdur26,1643313668.0,False
sdur26,"Algorithm Design Manual by Skiena

A bit old school from a language point of view, but taught me a lot.",hui5m5m,t3_sdur26,1643323574.0,False
sdur26,I don’t compute! What is this education background being equated to not being self taught?? There are very few professors that can explain things where you don’t have to teach yourself! 😂😂😂,huiem9p,t3_sdur26,1643327212.0,False
sdh6gc,[Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl%27s_law),hucp2h3,t3_sdh6gc,1643233361.0,False
sdh6gc,"You should definitely read the linked details but to give a quick and intuitive description of Amdahl's law:

Imagine you're baking a cake. More people can help, but there's a limit. One person can make the frosting while you're mixing the batter. That splits pretty easily and makes sense to do.

Theoretically you could have different people measure each ingredient out or one person per egg to crack but these steps are so quick you'd probably spend more time coordinating with each other than actually just doing the work yourself. Technically can be done in parallel but probably not worth it.

Then you need to actually bake the cake. I don't care how many people you have, you can't put the cake in until the batter is mixed. It doesn't matter how many ovens, the cake won't cook any faster. These steps fundamentally can't be parallelized.

So having a few people (cores) helped some phases (making the batter and frosting), but having more didn't do much (hard to split work when making the batter), while some steps take the same time no matter what (baking). The same principle applies to programs. Some things are easily parallelized, some things are hard to parallelize, and some things simply can't be parallelized.",hud0yh3,t1_hucp2h3,1643237822.0,False
sdh6gc,Is there a method to help identify tasks that benefit from parallelization and tasks that won’t?,hudwy4h,t1_hud0yh3,1643250984.0,False
sdh6gc,"The easiest tasks to parallelize are what we call ""embarrassingly parallel"". 

For example: if your task is purely functional (output depends only on the input and produces the same output for the same input every time) and you need to perform this task on many different inputs, then you can do them all at the same time since no single task like this depends on another one.",hueacnm,t1_hudwy4h,1643256943.0,False
sdh6gc,"That's basically what GPUs do. It's called number crunching. Calculating the color of each pixel on the screen in parallel. Also calculating weights of neural networks in parallel if you use them for AI training. Basically almost everything that makes heavy use of matrix operations can be parallelized like that and hence run on a GPU, thousands of such operations simultaneously.",huez8n5,t1_hueacnm,1643272096.0,False
sdh6gc,"Simple tasks that don't do I/O and have all the state they need available quickly can be blazingly fast.

For instance, that's all the GPU really is: just processes a bunch of fairly simple stuff (floating point math for graphics) at a level your CPU can't dream of. 

It's fairly intuitive to see what will benefit and what won't at the code level, but often harder to design the distributed system to enable more of those opportunities.",hueeaih,t1_hudwy4h,1643258877.0,False
sdh6gc,College campuses use Amdahl’s law to prevent students from filing complaints by filling the process with multiple layers of slow administrative positions. Fun fact.,huer27u,t1_hud0yh3,1643266204.0,False
sdh6gc,Really? :),hurbayi,t1_huer27u,1643485940.0,True
sdh6gc,Thank you kind mister for this good teacher like explanation :),hur90pf,t1_hud0yh3,1643485026.0,True
sdh6gc,Thank you for this!,hur8ue8,t1_hucp2h3,1643484958.0,True
sdh6gc,"Thoughts.

1 Woman can have 1 baby in 9 months

9 Women can have  9 babies in 9 months

9 Women cannot have 1 baby in 1 month

Some problems are linear",hud9t12,t3_sdh6gc,1643241376.0,False
sdh6gc,"> 9 Women cannot have 1 baby in 1 month. 
  
That right there is the *crucial* bit.  
 
Natural Human reproduction is a process that typically requires a woman and 9 months gestation for success. You can’t split the work and you can’t really speed it up.",huhwd1k,t1_hud9t12,1643320038.0,False
sdh6gc,"Understood, thanks Cyher! :))",hurb96s,t1_hud9t12,1643485920.0,True
sdh6gc,"The main reason can be summarized as ""communication overhead"". Cores share the same memory buses, some caches and same infrastructure on the system.

Sometimes cores need to pass data between themselves to execute the algorithm, and this can be expensive in terms of bandwidth and time (in processor scale).

This incurs some communication penalties as the programs scale up. Sometimes memory bandwidth falls short, sometimes caches need to be flushed and refilled much more, etc., hence the scaling up is not perfect or linear.

Source: I develop multicore software.",hucpkyz,t3_sdh6gc,1643233544.0,False
sdh6gc,Thank you for sharing your knowledge and experience man! :))),hurv477,t1_hucpkyz,1643494240.0,True
sdh6gc,You're most welcome. I'm glad it helped.,hurvj9e,t1_hurv477,1643494418.0,False
sdh6gc,">That's basically what GPUs do. It's called number crunching. Calculating the color of each pixel on the screen in parallel. Also calculating weights of neural networks in parallel if you use them for AI training. Basically almost everything that makes heavy use of matrix operations can be parallelized like that and hence run on a GPU, thousands of such operations simultaneously.

Thank you for your input good sir! :)))",hur9ki0,t1_hucpkyz,1643485242.0,True
sdh6gc,Multi and parallel processing are still subject to the law of diminishing returns. [Amdahl's Law](https://en.m.wikipedia.org/wiki/Amdahl's_law) may be what you're looking for from a theoretical perspective.,huctrhw,t3_sdh6gc,1643235073.0,False
sdh6gc,"Will take some look into this, thanks Music man! :))",hurazvs,t1_huctrhw,1643485816.0,True
sdh6gc,"Besides what has been said already: Not all problems can be expressed in a way that allows parallelization like in signal processing. Often computations are dependent on previous results. This is not necessarily due to ""imperfect algorithms"". It's in the nature of the problem.",hucz7zz,t3_sdh6gc,1643237152.0,False
sdh6gc,"Its something that has been said on wiki, but yes you are right, computation can be rather dependent on previous results :) thanks Wayne!",hurb75n,t1_hucz7zz,1643485897.0,True
sdh6gc,"Multi-core CPUs ""do"" perform N times better for right kind of tasks. Two examples are :

* Neural networks or any kind of Matrix multiplication . These tasks can scale up to thousands of cores on GPU.  Similarly, performs scales with number of cores. 
* Financial simulations like Monte Carlo",hug07r9,t3_sdh6gc,1643295019.0,False
sdh6gc,Thank you for this info! :),huray5v,t1_hug07r9,1643485797.0,True
sdh6gc,The same reason it doesn't take 4.5 months for two pregnant women to make one baby.,hud00l3,t3_sdh6gc,1643237456.0,False
sdh6gc,"Understood, thanks! :)",hurav8d,t1_hud00l3,1643485764.0,True
sdh6gc,"One of the primary limitations on x86_64 processors right now is the complexity of the instruction set.  Processing the machine code before the cpu truly begins doing math on it is a pita on x86_64 processors, regardless how many cores you have.  This is one of the key 'secrets' to the M1 processor's speed, because it's instruction set does not have this issue.",hucyskf,t3_sdh6gc,1643236984.0,False
sdh6gc,"> Processing the machine code before the cpu truly begins doing math on it is a pita on x86_64 processors

Could you expand on this a bit? I think you're referring to instruction pipelining, but it's hard to tell.",hud6cb1,t1_hucyskf,1643239961.0,False
sdh6gc,There are a lot of articles online about it.  Here's the top google hit: https://debugger.medium.com/why-is-apples-m1-chip-so-fast-3262b158cba2,huhkqu2,t1_hud6cb1,1643315738.0,False
sdh6gc,"You have just explained CISC vs RISC :)

ARM (eg M1) is basically RISC",huda3ve,t1_hucyskf,1643241501.0,False
sdh6gc,"Instruction set complexity has nothing to do with why 4 cores aren't 4 times faster than 1 core.  Each core has its own decoder, scheduler, registers, etc.

It does impact clock speed and instruction level parallelism, which is a separate thing.",hueuou9,t1_hucyskf,1643268698.0,False
sdh6gc,Work versus Span,hudaegh,t3_sdh6gc,1643241621.0,False
sdh6gc,"You might want to take a look at BeOS and Haiku OS. ""Pervasive Multithreading"" was built in at the OS level. If anyone is going to run into limits of multi-core or multithread processing, it would be them.",hudt9lx,t3_sdh6gc,1643249456.0,False
sdh6gc,Thanks for info! :),hurakjo,t1_hudt9lx,1643485643.0,True
sdh6gc,"There’s overhead in creating a multithreaded program. Creating threads is taxing, dividing tasks is taxing, and combining results is taxing. It’s the same reason two people won’t sort a bowl of M&Ms twice as fast as one, work needs to be divided and combined.",hue89k8,t3_sdh6gc,1643255946.0,False
sdh6gc,Thanks man for your input! :)),huranhi,t1_hue89k8,1643485676.0,True
sdh6gc,"This doesnt answer the question OP, but may help you understand the issue in more detail. Current Personal Computers have identical / nearly identical cores so it shouldnt matter if a process is being run on which core (i dont know if something like core#0 takes initial boot, its too detailed for my level of hardware knowledge)

There was a computer architecture that was designed to have programs run in parallel using a different architecture and that was the CellBE (Cell Broadband Engine). There was the main computer (which had hyperthreading) known as SPE (Synergistic Processing Elements) and there were 8 or so SPU( Synergistic Processing Units) which had a different instruction set to the SPE and there was a lot of space on the chip to allow SPU's to communicate with the SPE as well as each other. Theoretically, one SPU could decompress, pass to next which decrypts, then pass onto another which does rendering.  

The CellBE was in the PS3 and i understand while it performed well, it was a pain to code for.  

https://en.wikipedia.org/wiki/Cell\_(microprocessor)",huer1ma,t3_sdh6gc,1643266193.0,False
sdh6gc,Thank you Scott! :),hura1ym,t1_huer1ma,1643485434.0,True
sdh6gc,"It’s complicated, but the thing you mentioned first is a major cause and there are some inmediately related snags.
 
—
  
To elaborate: *If* CPU1’s task has to be completed *first* before CPU2 can do it’s thing, then you’re right back to square one and might as well use a single
processor and process. There’s going to be a lot of waiting involved otherwise.
 
Basically you have to be able to divide up the work into either *order-independent* computations or entirely separate tasks. This applies to all kinds of stuff at a fundamental level.  
  
Combining all the separate computations at the end can also force you back onto a single CPU or result in resource contention which I mention further down.
 
—  
  
In addition, if your code *blocks* on I/O transfers, then  the CPUs/Cores can suffer from *contention* over any *shared resources*. 
 
This basic issue applies to main RAM, data caching, storage, communication buses, etc.  
 
It’s also critically important not to have *collisions* or issues with flushing buffers, etc. You have failed before you really started if “CPU A was supposed to do Task A and CPU B was supposed to take Result A and peform Task B” and CPU B checked for the results before CPU A had finished… 
  
P.S. 
  
Imagine 50 people trying to dig a whole with 25 shovels, put all the dirt into 4 big piles and *not* run into each other or waste a lot of time….",huhvcbo,t3_sdh6gc,1643319657.0,False
sdh6gc,"Thank you for your input, this is some interesting info! :)",huraitb,t1_huhvcbo,1643485624.0,True
sdh6gc,Thank you all for your interesting and knowledgeable input here! :)) <3,hurbgel,t3_sdh6gc,1643486002.0,True
sd0e5c,"Edit: Sorry, I did not read through the question. You want these in the context of ""type theory"". 

1. Comonad has been encoded in [Haskell](https://hackage.haskell.org/package/comonad), so is [coeffect](https://hackage.haskell.org/package/effect-monad-0.8.1.0/docs/Control-Coeffect.html). Given one of the guy behind coeffect, Tomas Petricek, is fantastic at F#, I am surprised I cannot find a encoding of coeffect in f#
2. Algebraic effect and handler can be added in a type system. There are languages like [eff](https://www.eff-lang.org/) and [koka](https://koka-lang.github.io/koka/doc/book.html). I have not found any research article on a ""practical type system"" (like system F or Hindley-Milner) enriched with algebraic effect (I suspect you can find some publication related to Koka), but I have found [dependent type with algebraic effect](https://dl.acm.org/doi/abs/10.1145/2500365.2500581).
3. ATS tutorial provides lots of cool use of linear and dependent type to guarantee program safety, especially for heap manipulation. Unlike Haskell, ATS does not separate effectful and pure computation. 
This is for good reason, as ATS aims to be a fast language, it prides itself to do safe low level memory access. Separating effectful fragment and non-effectful makes these efficient codes hard to incorporate into larger programs. 

------


Effect system is a huge research area. It is not my research area, so I probably will say something wrong here. I am just listing some research topics that are related to effects. You can look into them, and hopefully find more useful research on their references. 

You already know the monadic effect, however there are some effects that can be more effectively modeled as comonad. some reference on comonad as a model of computation: https://www.sciencedirect.com/science/article/pii/S1571066108003435

There are notions of coeffect, and I am not sure if they are the same as comonadic effect. This seems like an interesting paper unifying coeffect and effect: https://dl.acm.org/doi/abs/10.1145/3022670.2951939 I am sure you can find some reference of coeffect in there. 

For research on monad, coeffect, and general categorical semantics, I would recommend to look into the work of [Tomas Petricek](http://tomasp.net/) (coeffect), Tarmo Uustalu (categorical semantics), and Marco Gaboardi (graded monad).

There is also what called algebraic effect (in the sense of algebraic data type, things defined without quantifier, or as an initial algebra). There is also a notion of [monad algebra](https://bartoszmilewski.com/2017/03/14/algebras-for-monads/), that to me seems to be slightly related to handler, but I am not sure.  

I recommend you to look into the work of [Matija Pretnar](https://matija.pretnar.info/) and [Andrej Bauer](http://www.andrej.com/) on this line of work.

There is also some less categorical approach where they focus on specific effects. One of the most popular one is reading and writing from memory, since it is one of the more theory rich effects.

This is reeeally not my area, so my ramble will get even less stable. 

I think this publication might be relevant here? https://dl.acm.org/doi/abs/10.1145/2535838.2535869 I heard logical relation is very commonly used in this line of research to prove effectful program equivalence.

Also as you have mentioned that linear system can encode some safety properties of effect (for example no read before write etc.), but I have not found any relevant research on that, but you can find some [tutorials on ATS](http://www.ats-lang.org/Documents.html#EFF2ATSPROGEX) that covers topics like this. ATS is a language with linear type, and it uses linear type a lot to guarantee some security properties of effects.",hublxuj,t3_sd0e5c,1643219446.0,False
sd0e5c,Check out the Flix language for some interesting research in polymorphic effect types.,hubbjhk,t3_sd0e5c,1643215791.0,False
sd0e5c,"Not sure about cutting edge research. But for everyday otj development, it's all about limiting side effects via decoupled, modular design.",hubi28u,t3_sd0e5c,1643218094.0,False
sd0e5c,"Imperative programs without side effects do not really make sense in my opinion. What is a statement, if not something that affects the current state? If so, the formal semantics require a current state, and that is what having side effects means.

On the contrary, functional languages can be given semantics that do not need external state, by merely rewriting one term into another until you (eventually maybe) reach a normal form.",hubr402,t3_sd0e5c,1643221278.0,False
sccyyv,"The people who are loudest on social media are by no means the experts in their field. Working deeply focused all day, every day also is not a good way to keep up with what other people are doing. You might want to take a few hours a day to do this. (If you're doign deeply focused work in a lab, that would usually mean you eventually produce research original enough to present at a conference. Attending them usually gives a good overview what others are doing).",hu68zf8,t3_sccyyv,1643129761.0,False
sccyyv,"I approach this from a university academic perspective, so you may have different goals and access to peers.
 
The best thing ever are reading groups. Each discipline and subdiscipline should have a reading group, made up of 2 'layers' of people - 1 layer that is people who are actively in that space, or at least closely tangential, and then a bigger layer which is just anyone interested. For subdisciplines these are usually pretty small, for the larger disciplines these might include the whole school and beyond into other faculties and industry. Each <period>, depending on how many people, interest, publishing rates etc, as well as tied to each conference and major serial, you have people divvying up the papers that were released in that period to the internal layer to read. They then give a snap summary (1-3 sentences max) of those papers to a mailing list, with short recommendations, i.e. from 'this is garbage' to 'you must read this'. Then the 'you must read this' papers are read (or at least skimmed) by the outer layer who are interested, and everyone comes together to discuss them. If they're particularly good papers they get shunted up from the subdiscipline group to the main discipline for everyone to read.
 
This cuts the required publication reading to keep absolutely abreast of what is going on in the whole of computer science to about 1% of what it would otherwise be - because 90% of the papers that are published are crap, either wrong, or just rehashed old stuff, and 9% are hyper-niche interest only.
 
Note that (in an academic setting at least) you don't need to follow twitter, podcasts, youtube, news, or subreddits. They fall into 3 categories - people doing work that has been published elsewhere, people doing work that isn't worth publishing, or accounts run by university marketing departments trying to boost their institution's profile and citation rates. I'd still recommend following them, but they are far more of a 'I am bored and am looking for something to read/watch while I am eating/pooping/travelling/supposed to be sleeping' thing than anything to be religiously consuming.",hu7szqs,t3_sccyyv,1643150901.0,False
sccyyv,I think the most efficient way is to follow them on social platforms.,hu5edob,t3_sccyyv,1643117077.0,False
sccyyv,"Why do you need to be so highly efficient in remaining up to date in your field? Are the older technologies of your field so atrocious that the updated technologies destroy the meaningful existence of the older technologies?

As for me, I strongly believe in understanding the fundamental of your field. The fundamentals rarely change so it's perfectly feasible to focus only on the fundamentals to gain as much insight as you can. When you understand the fundamentals, you should be able to pick up on the meaning of any new technologies (that build upon the fundamentals) that happen to come up over time.",hu5oa5o,t3_sccyyv,1643121679.0,False
sccyyv,wtf why would people downvote this? i totally agree,hu7l5t1,t1_hu5oa5o,1643147761.0,False
sbxbya,"Truly parallel? The amount of cores and CPUs. Concurrent? The amount of threads, cores, and CPUs",hu2pkfr,t3_sbxbya,1643060889.0,False
sbxbya,"Concurrent connections can go even higher than this using non-blocking IO where a single thread can manage N connections. Practically the limit is likely going to be the max number of connections your OS allows at once, but this is tunable with something like linux. If your computer only has one IP address you’ll run out of TCP ports once you start to approach 2^16 connections. But even this can be worked around by using multiple IP addresses. At that point you are only really limited by network bandwidth and memory.

There are special programs build to simulate large numbers of connections per second for scalability testing of things like web servers. Suggest looking at those if this is something you are interested in.",hu2r0x3,t1_hu2pkfr,1643061442.0,False
sbxbya,"That's a good point! If we're looking at asynchronous operations, it's a huge amount",hu2y6un,t1_hu2r0x3,1643064220.0,False
sbw98k,"Most generic compression algorithms don't assume any specific file format.  They look for byte-level redundancies and work to eliminate repeated information.

I'm not aware of any general-purpose compression algorithm that will identify the more semantic redundancies you've described.  I'd argue that if you're aware of this kind of structure, you should encode it explicitly in the format.",hu2kut2,t3_sbw98k,1643059121.0,False
sbw98k,"You have described a domain specific problem, and compression method devised for the specific domain will always beat the general algorithms.",hu3eru4,t3_sbw98k,1643074095.0,False
sbw98k,"If we lived in a world where storage space or network bandwidth was more of a bottleneck than they are, it is likely we would see 'plugin' compression algorithms used with a general algorithm layered on top of known file type specific algorithms. But that isn't the case - storage and bandwidth is continuously cheaper, and implementation complexity is probably the most significant limiting factor. The implementation complexity necessary to recognise that that transformation has occurred would be prohibitive (not to mention the actual computation required to recognise and compress it would be a significant cost).
 
It's at the point now where not even text is treated differently to random binary data for compression. While you could theoretically compress something more if you knew about things like stemming words, it's not worth it given the gains are so small vs just looking at common bit ranges.",hu3xwal,t3_sbw98k,1643082549.0,False
sbw98k,"I don't completely agree with you. Having plenty of storage is not an excuse to waste resources. 

Also, network bandwidth is still small in some part of the word, and in places where it's huge the data being transmitted is also ever increasing, so you need compression anyway (even with the fastest network connection we have nowadays, it would be impossible to stream a 4K video without compression). 

Finally, there are applications where compression is still relevant, satellites communication or small embedded devices come in mind.",hu4hqyh,t1_hu3xwal,1643093702.0,False
sbw98k,"Nobody ever said it was an excuse to waste resources, it's a factor in the never-ending balancing act between different compromises. The cheapness of storage and bandwidth means that the value gained from marginally increased compression is tiny, so any cost greater than tiny means it isn't going to be worth it.
 
You need to read what is being written more closely, because the rest of your post is a complete strawman that has exactly nothing to do with what is being discussed. I'm not saying compression isn't useful or used, I'm saying there is no push for specialisation inside general compression algorithms - which is objectively true, because it isn't happening.
 
Video is compressed with a specialised algorithm, not a general one. Satellite communication typically uses *less* compression than you think, because the issue with satellites is latency, not bandwidth, and the processing capability of a satellite tends to be the bottleneck. There will be plenty of compression before and after the satellite is involved, but again there's no value in going after marginal gains there, either.
 
Embedded devices I'm not even sure what you're getting at. As you scale things down processing power drops exponentially faster than storage or network bandwidth. You can cheaply get the same network bandwidth as a full sized desktop on an Arduino micro, and multiple terabytes of flash memory.",hu4j79b,t1_hu4hqyh,1643094705.0,False
sbw98k,"Satellites and embedded systems were just two examples were I have worked on architectures for compression, in which the processing power was out weighting bandwidth. 

You are right though, I missed the point that you were making about specialized compression!",hu4m6r7,t1_hu4j79b,1643096835.0,False
sbw98k,Segmentation into neighbourhoods and then deltas from neighbour avgs.,hu4sb65,t3_sbw98k,1643101638.0,False
sbk9aw,"My three main issues with crypto are..  
1) It fluctuates in value too much to be used as currency.  
2) In most countries, traditional investments are insured against theft. Theft of crypto isn't.  
3) Much publicised environmental cost of mining.",hu0dzzq,t3_sbk9aw,1643026699.0,False
sbk9aw,"> 2) In most countries, traditional investments are insured against theft. Theft of crypto isn't.

How can cryptocurrency owned by a person be stolen? Cryptocurrency is very secure, right?

> 3) Much publicised environmental cost of mining.

I feel like this argument is a moot point because everything we do has a high carbon footprint on the environment, including watching YouTube. I think moving away from non renewable energy with a healthy mix of renewable and nuclear energy would solve this problem, because regardless of whether we use Bitcoin or not, I think block-chain technology is here to stay.",hu0g8vm,t1_hu0dzzq,1643028103.0,True
sbk9aw,">How can cryptocurrency owned by a person be stolen?

I'm not sure this statement is the slam-dunk you think it is. Just because you own something doesn't mean it's yours forever. Someone owns the Mona Lisa, but it can still be stolen.

There's plenty of malware out there that sits quietly undetected on peoples machines until it detects the user copying their private key to the clipboard, at which point it uploads to the hacker. [There's also been plenty of crypto-exchange hacks over the years.](https://crystalblockchain.com/articles/the-10-biggest-crypto-exchange-hacks-in-history/)

There's also the danger of accidental loss. If my house burns down with my laptop, password manager, and any notebooks containing investment or banking credentials are destroyed, I can still eventually prove my identity and get access to my assets again. [If you lose your private key, you're fucked.](https://www.bbc.co.uk/news/uk-wales-55658942)",hu0n9ic,t1_hu0g8vm,1643031926.0,False
sbk9aw,"Your correct.

One of the oft touted positives of decentralisation is that it empowers the poor to get out of the grip of the powerful.

For me; this is actually really wrong; it transfers the burden of security to the individual, removes any kind of safety net.",hu0xnrf,t1_hu0n9ic,1643036679.0,False
sbk9aw,"I see, apologies if I seemed a bit arrogant since I'm very new at understanding this technology. Thank you for taking the time to explain it to me.",hu0nyjz,t1_hu0n9ic,1643032272.0,True
sbk9aw,"No need to apologise, and sorry if I came across as rude. Just please be very wary of crypto. For every person you read about who retired with millions in the bank at 25 after investing in crypto, there are probably 50 people whose investment went down, or who lost all their crypto through scams, but are too embarrassed to talk openly about it.",hu0one3,t1_hu0nyjz,1643032612.0,False
sbk9aw,"The extreme lack of authority makes it so any mistake is permanent, if someone steals my credit I get every cent reimbursed, if someone steals my wallet I’m absolutely fucked, there’s just no real point from the consumer side to go crypto over fiat",hu3jb4o,t1_hu0g8vm,1643076030.0,False
sbk9aw,"Hmm, I never considered that part. You're right.",hu4k65b,t1_hu3jb4o,1643095388.0,True
sbk9aw,"> How can cryptocurrency owned by a person be stolen? Cryptocurrency is very secure, right?

What if the person hands it away himself :-) ? Yes, you heard it right, that is what crypto theft is. The blockchain is very secure but most people using cryptocurrency are not tech junkies like us, they do not know how to secure their crypto wallet, they either end up giving away their private key to some hacker because of some social engineering or fall into other scams. Also, the cryptocurrency exchanges can have bugs, thus causing uh-huh crypto theft. 

u/rzlmmfia meant that traditional investments are secure from this kind of theft, which the ""common man"" is unaware of, although traditional investments are also not safe, god forbid but somebody can break into your house and steal valuables. I think that there are some benefits of having a centralized authority as the middleman which I would describe in my answer to the main question.",hu0ouri,t1_hu0g8vm,1643032713.0,False
sbk9aw,"From a CS perspective, I believe Blockchain is a very clever and interesting technology, using cryptography not to protect information, but instead to build concensus among trustless parties.

From a social perspective, I believe platforms like Ethereum could revolutionize human interaction at a global scale and disrupt virtually any industry despite entrenched central authorities and gatekeepers.

From a political perspective, I believe deeply in the promise of Bitcoin as a hedge against the irresponsible and reckless behavior of the Federal Reserve, printing the US dollar into valueless oblivion. And also as a hedge against the ""too big to fail"" private banks and their cronies in the SEC and other civil regulatory agencies whose collective incompetence caused the disaster in 2008 which robbed so much wealth from so many people.

However, from a pragmatic perspective I recognize that, in reality, the crypto community thus far has produced little more than new-age digital Ponzi schemes and other various forms of fraud and grift. All while building a culture around itself which has its collective head so far up its own ass that it can't recognize or resolve any of the issues that are undermining it's potential.

Finally, from an emotional perspective, I got caught up in the Elon-fueled mania and am the not-so-proud owner of 29,290.53 Dogecoin. So what the fuck do I know?",hu0xzhn,t3_sbk9aw,1643036818.0,False
sbk9aw,Your answer seems like the most balanced opinion on this comment section.,hu4kcgg,t1_hu0xzhn,1643095511.0,True
sbk9aw,"Believe?

Eh; the technology is interesting but generally there no actual good use case that needs a distributed ledger. We all use regular money just fine; it’s worked for thousands of years.

A lot of money and a lot of smart peoples time has been invested , and yet we’ve still only got crypto coins. So this amazing technology is basically useless.

Cryptocurrency looks very much like a fancy Ponzi scheme currently; the fact the supply of money is increasing at a fixed interval, means the higher demand for coins, the greater value each coin is, and this basically rewards those with existing high holdings. So they are incentivised to attract more people in, which is easier enough if the returns are there. But for those later to the party, you get less of the pot, and potentially don’t get the returns; pretty much a Ponzi scheme.

Unless you’re blinded by the tech; or think it’s so cool, or are an anarchist at heart; you generally think it’s useless.

However, I wish I’d bought some at the start and become a multi-millionaire.",hu0zfxp,t3_sbk9aw,1643037426.0,False
sbk9aw,Except that is not true in some countries where currency is not reliable (e.g. hyper inflation).,hu1jduj,t1_hu0zfxp,1643045073.0,False
sbk9aw,"You're right, but I think the much more fluctuations of crypto's value can be a very big problem for it's adoption.",hu4k8u7,t1_hu1jduj,1643095441.0,True
sbk9aw,"I have not invested in crypto. 

I personally don't believe in Bitcoin, and other crypto currencies. I do believe in their underlying technology. They have huge potentials. It can really be the new phase of internet. 

None of these crypto currencies have any underlying value, the value of a stock is determined by the company, bonds by the govt stability and currency value, etc. Crypto currency as such does not have any underlying value which can make it a valuable investment.",hu0eiqi,t3_sbk9aw,1643027033.0,False
sbk9aw,"> None of these crypto currencies have any underlying value, the value of a stock is determined by the company, bonds by the govt stability and currency value, etc. Crypto currency as such does not have any underlying value which can make it a valuable investment.

Hmm, I kind of agree and disagree with this statement. If we think about it, no currency really has any intrinsic value, the only ones which do are services and goods that are produced. I, however, do get your point. Bitcoin as a currency has yet to see mass adoption in the global economy and I personally don't see how it can happen as it's value fluctuates so much.",hu0gj11,t1_hu0eiqi,1643028272.0,True
sbk9aw,"Currency is backed and guaranteed by a government. Its stability is directly tied to the stability of that government. The only way a major currency would lose value is if the government collapses. You see that instability in countries that are very unstable, like Venezuela. That's not really an issue for the Euro or USD though. 

It used to be backed by gold, but modern economics doesn't care about that.",hu0pnbh,t1_hu0gj11,1643033094.0,False
sbk9aw,"It has been too widely adopted for how early and rough around the edges it is.  Finance bros are using is as a speculative investment is ruining its potential to be a useful currency.  Finance bros combined with its energy usage issues and the fact that a rough new technology is being presented to the general public as the next big thing is placing a stigma on it that will be very hard to over come.  It could've one day been something great and improve the world.  Maybe it still will, but that will require a major rebranding, and people to smooth out the edges despite the current taint attached to it.",hu0nnkn,t3_sbk9aw,1643032120.0,False
sbk9aw,I agree with you. What would you say are the current limitations and problems that Blockchain technologies face?,hu0o51x,t1_hu0nnkn,1643032363.0,True
sbk9aw,"Technically it’s an interesting but extremely inefficient solution to a problem that doesn’t really exist. 

Economically it has way too many flaws as a currency for me to see it being mainstream, but I could see it always having a niche status. 

I don’t really “believe” in it, no.",hu1rjpu,t3_sbk9aw,1643048062.0,False
sbk9aw,I agree with you.,hu4jyrz,t1_hu1rjpu,1643095245.0,True
sbk9aw,[deleted],hu0cvr0,t3_sbk9aw,1643025959.0,False
sbk9aw,"> You shouldn't apologize, people are becoming rich than getting therapy that why they are unhappy. They are not ready to share how they got into and planning to gift their next 50 generations Bitcoin as inheritance.

I'm not sure I quite understand what you are saying here.",hu0gcu3,t1_hu0cvr0,1643028168.0,True
sbk9aw,[deleted],hu0jsnr,t1_hu0gcu3,1643030128.0,False
sbk9aw,I think that can be said about the whole state of affairs happening all around the world. Not a very profound statement lol.,hu0ntbq,t1_hu0jsnr,1643032200.0,True
sbk9aw,Please don’t post stuff like this here.,hu0fltx,t3_sbk9aw,1643027714.0,False
sbk9aw,Why not?,hu0fyoh,t1_hu0fltx,1643027932.0,True
sbk9aw,What is the name of the subreddit? What does this have to do with computer science?,hu0gdh1,t1_hu0fyoh,1643028179.0,False
sbk9aw,"Uhm, not sure how to say this, but Blockchain technology is a topic within the field of computer science lol",hu0o9df,t1_hu0gdh1,1643032422.0,True
sbk9aw,"You didn't ask about blockchain technology / algorithms. You asked about the consumer / social side of crypto, i.e. about whether or not it is a scam, whether or not it will be adopted, etc. And you asked if this subreddit ""believes in crypto."" 

You're not asking about CS. That's why the post has no upvotes. 

\> I would be asking this question in the crypto sub but that sub

You talked yourself out of doing the right thing - go talk about this there.",hu0trwx,t1_hu0o9df,1643034996.0,False
sbk9aw,"Should we not discuss the social and political ramifications of CS? Science and technology do not exist in a vacuum.

If there existed an r/eugenics sub, I would hope it would contain healthy debate on the ethics of such scientific pursuits. Admittedly, this is an extreme example, but the point is that scientists and engineers should always be considering the implications of their work in a broader context outside their narrow scientific field. I believe that is what OP is going for here.",hu13p4n,t1_hu0trwx,1643039131.0,False
sbk9aw,"\> Should we not discuss the social and political ramifications of CS?

oh my god, that's not what this person is asking about. The post was ""hey crypto bros, wanna talk about crypto??""",hu1tlfe,t1_hu13p4n,1643048818.0,False
sbk9aw,Not really. I was asking people's opinion on what they thought about cryptocurrencies.,hu4kfmv,t1_hu1tlfe,1643095575.0,True
sbk9aw,Ignore this idiot. It's people like these that reddit becomes a meme because now you can't even ask anything. I also made a post recently and I only see pokemons like that. I wonder if they have a life of their own...,hufwpu9,t1_hu4kfmv,1643293552.0,False
sbk9aw,"Viewing cryptocurrency as a simple currency is a bit outdated, as it has evolved beyond that. While “currency” aspect does have its place, the technology is now becoming more of a decentralized state machine, with nodes forming consensus as to the state of execution, rather than merely the number of coins in a wallet.  A new trustless economic platform is being created and while I tend to think it won’t be quite as revolutionary as the internet was, this is the more important technologies being developed right now.",hu2ckzx,t3_sbk9aw,1643055992.0,False
sbk9aw,"But don't all transactions happen through like some few companies? I think that's a big part about that. Plus, there is a limited number of transactions you can do through bitcoin in a given time period, which is way less than normal money. I think these are some of the main factors hindering cryptocurrency's mainstream adoption.",hu4k4gy,t1_hu2ckzx,1643095356.0,True
sa6sdw,"In my experience it is not worth trying study the courses material just the summer before. You never end doing that much progress and you will literally have to spend time again reviewing it. I think it's better just to do some fun projects that get you more in touch with programing ( I did exactly this and that was my conclusion). Just as an idea, of course I don't want to discourage you from study. Good luck with that!",htrijml,t3_sa6sdw,1642871627.0,False
sa6sdw,Depends how much he knows. Might be worth making sure you understand the underlying concepts before taking calc. A lot of people struggle because they could use work on trig or algebra,htrj2uo,t1_htrijml,1642871834.0,False
sa6sdw,"Yeah right, if u aren't comfortable with the academical stuff it is worth to care of it",htrnyoz,t1_htrj2uo,1642873733.0,False
sa6sdw,I am an international student from different study background so U might consider me a beginner or a a little above it.,htrll7p,t1_htrj2uo,1642872803.0,True
sa6sdw,"Thanks appreciate it , Building projects is particularly I love to do more than just studying stuff and also loved ur idea.

Can you recommend some basic-mediocre level projects I can do and also a place to learn this kinda stuff?",htrldny,t1_htrijml,1642872722.0,True
sa6sdw,ya i agree. you will learn these topics anyways. just spend the time learning something else,htrppx2,t1_htrijml,1642874421.0,False
sa6sdw,"Cs50 is a good intro course

On one of the pinned threads there’s an entire CS degree outlined with free courses I’ll edit and post the link momentarily (actually just go to the learn programming sub reddit)

Khanaceademy and freecodecamp are good too",htri4es,t3_sa6sdw,1642871464.0,False
sa6sdw,"r/learnprogramming sub reddit right?

cs50 by harvard if I am not wrong?

Also thanks :)",htrlw0r,t1_htri4es,1642872919.0,True
sa6sdw,"Yes cs50 is Harvard you can find it on edx.com,
The lectures are by David j Milan and he’s charismatic af so it’s enjoyable

And that’s the sub, I just can’t link to them on my phone

Maybe precalc to make sure your up to speed on your maths,",htrm4j9,t1_htrlw0r,1642873011.0,False
sa6sdw,"Rosen, K: Discrete Mathematics and Its Applications

[Calculus Course on YT](https://youtube.com/playlist?list=PLl-gb0E4MII1ml6mys-RXoQ0O3GfwBPVM)

[Codecademy](https://www.codecademy.com) for programming: start with either Python, C++, or Java


Books on programming (more in-depth theory):
Problem Solving with C++, Walter Savitch

Introduction to Java Programming and Data Structures, Y. Daniel Liang",hts19of,t3_sa6sdw,1642878900.0,False
sa6sdw,Already studying C++ in high school so The book will be really helpful:) thanks.,htuc07z,t1_hts19of,1642913611.0,True
sa6sdw,"There are complete syllabus’s on GitHub if you haven’t looked at your complete CS syllabus from school. They should be able to give a set of topics to use as a foundation for your CS knowledge. If you have your course syllabus you may be able to research the books in advance, though particular classes may change their textbook from year to year so I wouldn’t get anything head of schedule as the textbook may change by the time you take the class. If you want to study ahead there are plenty of good vendors online that offer the textbooks for the class, and I believe some free websites.",hu06don,t3_sa6sdw,1643021142.0,False
sa6sdw,I think it’s better to do leisure learning rather than just getting into details. That will help more IMO when you actually take those classes just search topics you like and see what video lectures you can find.,htsw6px,t3_sa6sdw,1642891434.0,False
sa6sdw,"If you have access to the materials of the courses you will be taking (syllabus, bibliography, etc), I'd suggest that you use them as guidelines. Nevetherless, here are my suggestions:

\- Discrete Math: this [playlist](https://www.youtube.com/playlist?list=PLl-gb0E4MII28GykmtuBXNUNoej-vY5Rz) follows some sections from the book *Discrete Mathematics and Its Applications*, by Kenneth Rosen. You could follow the video lectures and read the corresponding section in the book. I'd suggest that you go up to video 30 (seems a lot, but they are not long); these cover Logic, basic Set Theory and proof techniques, which will give you a nice head start for when you begin your course.

\- Linear Algebra: I think the best resource you could ever want to study this subject is Strang's MIT course [18.06SC](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/). This version is organized specifically for self-study (it has exercises with solutions, lecture notes, etc). I'd suggest that you go through Unit I; this will be more than enough to give you a good start in the subject before your course.

\- Calculus: some may have better suggestions than mine, but I think Khan Academy's [Precalculus](https://www.khanacademy.org/math/precalculus) course would let you well prepared for your first encounter with Calculus. Unlike the other subjects, Calculus requires that you are knowledgeable in algebra, trigonometry, etc. So if you have gaps in these prerequisites, I'd urge you to prioritize studying them instead of the other subjects I listed. LA and DM will be covered in your bachelors, but high school math certainly won't.

\- Programming: last, but not least, [CS50](https://cs50.harvard.edu/x/2022/) from Harvard makes a nice introduction to programming for someone entering a CS major. This course does NOT focus on a particular programming language; instead, it teaches the basics of many programming languages (C, Python, Javascript) and covers basic algorithms and data structures.

Hope I could help. Best of luck! ;)",htt5rgc,t3_sa6sdw,1642895377.0,False
sa6sdw,"Just WOW! for this explanation.
Thanks a lot :)",htubchd,t1_htt5rgc,1642913288.0,True
sasbd7,Pretty sure that’s right,htwzued,t3_sasbd7,1642965641.0,False
sasbd7,Not always O(V\^2). If use min-priority queue it drops down to O(V+V log(V)) .,htyot0u,t3_sasbd7,1642989662.0,False
sasbd7,Are you sure that's not O((E + V) log(E))? Making it O(V^2 log(E)) in complete graphs?,hu06upo,t1_htyot0u,1643021520.0,True
saqw7i,This will come with all sorts of ethical dilemmas in the future. Where does human consciousness actually come from? It's in the petri dish playing pong.,htvz4mj,t3_saqw7i,1642951302.0,False
saqw7i,Computational neuroscience will solve consciousness,htx2088,t1_htvz4mj,1642966498.0,False
saqw7i,Maybe but definitely not in the near future.,htxmypx,t1_htx2088,1642974548.0,False
saqw7i,Why do you think so? Also how near is near?,htxw3gc,t1_htxmypx,1642978048.0,False
saqw7i,"Because the amount of computational power needed to simulate anything close to a normal brain simply isn't there yet.

Edit: And from a biology standpoint we don't know nearly enough about how the brain works to realistically simulate it.

Edits2: Just to elaborate I am refering to a complete simulation down to molecular level",hu0i4ul,t1_htxw3gc,1643029201.0,False
saqw7i,"That's quite a big assumption (that we need to simulate it at the molecular level).

And possibly we might see some big leap with quantum computers, for which simulating quantum systems like molecules is a piece of cake. 

So, I would say it's better to say: we don't know. Maybe, maybe not. That's how things are, those are the paths and let's see! (Or contribute if we are in the area :p)",hu98hsf,t1_hu0i4ul,1643173107.0,False
saqw7i,"Yeah it is indeed. My line of though is that aince we don't know which ""level of abstraction"" (to use our lingo) let's say ""generate"" consciousness, it would be interesting to try and simulate it at different levels.",huh8n35,t1_hu98hsf,1643311286.0,False
saqw7i,Maybe we dont want it to.,htyvx2t,t1_htx2088,1642992534.0,False
saqw7i,5 years,htzlptx,t1_htx2088,1643005267.0,False
saqw7i,Can't beat the good old noodle.,htvhek9,t3_saqw7i,1642942041.0,False
saqw7i,But how do they reward the cells with some sort of loss function? Afaik the article doesn't discuss this.,htxe5mq,t3_saqw7i,1642971202.0,False
saqw7i,"Another article referencing the same paper was posted on r/science a while ago, as far as I can remember these neuron cells react well/ ""like"" predictable signals and ""hate"" unpredictable signals, so when ever they got something wrong they were fed random noise and when they got something right they got a simple periodic signal",htzte1a,t1_htxe5mq,1643010621.0,False
saqw7i,Sex,htzu7ps,t1_htxe5mq,1643011246.0,False
saqw7i,"This will be the AI that actually makes a real difference, if true. I've been watching this space for 30 years, and have always been dismissive. This is the first time I've been impressed.",htvmhp9,t3_saqw7i,1642945140.0,False
saqw7i,It does not take an hour to train an AI to play pong.,htvvvku,t3_saqw7i,1642949865.0,False
saqw7i,"It's based on hardware-speed independent 'time' - based on the number of rallies occurring at a fixed rate, rather than actual time. 5000 rallies for computer-based AI vs 15 rallies for this brain cell version.
 
However, they also state that the brain cells know how to play quickly, but suck at it and would lose vs a standard AI. Which seems very human.
 
Bottom line is the pre-print itself isn't focusing on the speed vs computer AI, it is about the increased performance of human neurons vs mouse neurons, and the potential for future development.",htxhzo2,t1_htvvvku,1642972660.0,False
saqw7i,It doesn’t take a billion dollar company and billions of simulations to teach a teenager to drive a car.,htwchkf,t1_htvvvku,1642956657.0,False
saqw7i,"The difference is that you can ctrl-c, ctrl-v that software.",htxj6bj,t1_htwchkf,1642973100.0,False
saqw7i,that is scary,htx0wau,t3_saqw7i,1642966060.0,False
saqw7i,"This will not spread in mainstream because Elon musk don't post on twitter about this, and don't have a video on youtube discussing about this research",htxduwv,t3_saqw7i,1642971091.0,False
saqw7i,Wow this is.. quite honestly very jarring,hudq370,t3_saqw7i,1643248138.0,False
saqw7i,Completely unethical. I hope they keep the scientists involved in a dark room and force them to play Pong for the rest of eternity.,htxu80g,t3_saqw7i,1642977364.0,False
saqw7i,This is as unethical as a human being born. People dont seem to view the lack of natal-consent as an issue.,htywbad,t1_htxu80g,1642992686.0,False
saqw7i,"Hypothetically, imagine a neural net of the size of a human brain to solve a complex task. If we do not fully understand how brains work, how will we understand what this lab brain will go through? Imagine it lives in hell? Imagine there are pain neurons that are arbitrarily firing and the lab brain is conscious about it.",htzywty,t1_htywbad,1643014958.0,False
saqw7i,"What if our existance is a living hell and we are concious of it, what if life is pain?",hu2v3t2,t1_htzywty,1643063014.0,False
saqw7i,Could be.. but if we can't prove what a neural network perceives in the slightest sense we shouldn't create it in the first place. We have a good estimate of what a baby feels because we see it laugh and we try to give it a good life. We couldn't say that about a pong playing neural net because we can't see inside a black box yet.,hu2wd18,t1_hu2v3t2,1643063502.0,False
saqw7i,"You can always just kill yourself though, unlike these Frankenstein creations.

&#x200B;

If you are going to make such a poor comparison then I will gently remind you that birthing a child for the purpose of forcing them into to slavery is also widely considered unethical.",hu1tue8,t1_htywbad,1643048910.0,False
saqw7i,"Agreed, while it is the consensus that natal-consent is unimporant I hold the uncommon belief that maybe its not super cool.

&#x200B;

Thing is, all human soceities have and continue to practise slavery in some form (modern days is placed on people in developing nations).

&#x200B;

The question remains, how are we going to do work?  
Is it more ethical to coerce full bodied people into it?",hu2vqg7,t1_hu1tue8,1643063258.0,False
saqw7i,"The obvious alternative is to not use slaves and have people with free determination cooperate by exchanging their labour for goods and services? 

Slavery is neither inherent nor necessary  for survival as you so heavily imply. Small societies do not own slaves. Slavery is merely a side effect of despotism.

Anyway in response to your question(s):

1) how are we going to do work? Sort of like what we have now? Why do we need a bunch of laboratory abominations to compute anything when we have perfectly capable non-biological computers.

2) Is it more ethical to coerce full bodied people into it? I know this is a loaded question and completely ignores the obvious fact that the world's and humanity's survival does not depend on the the computational tasks of biological brains in jars, but the answer here is yes, because at least those full-bodied people have some capability to end their own lives by their own free will.",hu32uz5,t1_hu2vqg7,1643066092.0,False
s9gnb2,"Cs is not that math heavy tbh. It’s just calculus , linear algebra and discrete. Linear algebra and discrete are more important than anything. I would advise you to pick a language most probably the one used in the university course work. It will help you a lot to finish projects on time.",htmsicy,t3_s9gnb2,1642790689.0,False
s9gnb2,I definitely agree but just want to add in a caveat: If you want to go towards a data science pathway I would not slack off with the calculus.,htrt0im,t1_htmsicy,1642875686.0,False
s9gnb2,"Thanks, I don't plan on slacking off with any subject tbh. This is why I'm asking for resources, I want to do well.",htvzy0o,t1_htrt0im,1642951651.0,True
s9gnb2,"> Cs is not that math heavy tbh

I would argue about that but nvm",htq2hcs,t1_htmsicy,1642843375.0,False
s9gnb2,"I meant to say for undergrad CS. grad and Beyond , especially theoritical CS is different story",htqm6at,t1_htq2hcs,1642856994.0,False
s9gnb2,"Thanks for the reply! I'm actually learning Python and JavaScript and it's going pretty well so far, but I'm really concerned about the maths since I've forgotten pretty much everything😬",htmtmsx,t1_htmsicy,1642791135.0,True
s9gnb2,"Don’t worry! Universities basically start from calculus and linear.
Also note that python will most probably not be used in 90% courses. Java and C++ are most used for universities. JavaScript is barely touched and python is only relevant for ML courses. 

EDIT : I will recommend going over courses requirements for more info",htmvuj1,t1_htmtmsx,1642791964.0,False
s9gnb2,"Oh wow, thank you so much. The course requirements are a bit unclear since I'm an international student so I don't really know what a (Canadian) hs student learns on average, so I'm trying to get a general idea so that when I delve into details it'll be a bit easier.",htmxweo,t1_htmvuj1,1642792716.0,True
s9gnb2,What university? I might be able to help,htmyyps,t1_htmxweo,1642793104.0,False
s9gnb2,Memorial University of Newfoundland.,htmzhex,t1_htmyyps,1642793295.0,True
s9gnb2,"It wouldn't hurt to get a program for practicing coding in a fun way. I made a graphical, no-fail language that is fun like Minecraft. You can email me at Cameron.flotow@gmail.com.

I also left a link to the FB page I just set up.

[CIRKETZ fb page](https://www.facebook.com/groups/471007034699523/?ref=share)",hu4f4th,t3_s9gnb2,1643091981.0,False
s9ffoc,An ACM membership is less costly and as far I can tell is the same as the regular OReilly online subscription. The periodicals and other content/benefits you get from the ACM membership are excellent.,htnejwf,t3_s9ffoc,1642798731.0,False
s9ffoc,"Thanks for the info mate  
What periodicals and benefits are there currently? is it this https://www.acm.org/membership/membership-benefits",hu4b9xk,t1_htnejwf,1643089627.0,True
s9ffoc,"That’s it. The ACM digital library, Communications of the ACM, and their various newsletters are really nice.",hu5g0yu,t1_hu4b9xk,1643117907.0,False
s9ffoc,I’d suggest O’Reilly. I probably own fifty of them. I like the consistency and structure of their books regardless of the author. The material is great both for learning and as a reference on the job.,htpmjm9,t3_s9ffoc,1642832748.0,False
s9enpp,"Very new to machine learning and Comp Sci. Started learning about ML when I was supposed to be doing my dissertation (on some things called D-Modules and Grothendieck's Crystals) but Alpha Go was busy changing the world.

Properly started learning ML properly 20 months ago (first commit on this repo is on 22nd of April 2020) and this is what I've been up to! Was lucky enough to get onto the GPT-3 beta and I've built a tool to automatically generate resumes based on some notes you enter. Has taken me forever to get it working but finally am at a first properly working version.

The idea is you'll only have to write your resume once (or not really at all), and then you can give it a job spec and it'll automatically tailor it for you.

It is still v much WIP but would love to hear your feedback!

EEDIT: the website is [joinrhubarb.com](https://joinrhubarb.com) :)",htm6sgd,t3_s9enpp,1642782726.0,True
s9enpp,I would legit pay for this tool.,htqb3gf,t1_htm6sgd,1642849683.0,False
s9enpp,"Wow, that's so kind, thank you! I've made it totally free and I have no plan to charge - you can create an account on the website I chucked up :)

(EDIT: it's [joinrhubarb.com](https://joinrhubarb.com) \_",htqbm1o,t1_htqb3gf,1642850051.0,True
s9enpp,Maybe set up somewhere for donations.,htqf3tr,t1_htqbm1o,1642852497.0,False
s9enpp,Agreed. Get us your Patreon link!,htqfy26,t1_htqf3tr,1642853080.0,False
s9enpp,"That's a really kind thing to say but honestly, I haven't done it to make money - the aim is to have enough people using it that I can make the ml models incredible (not just getting past ats but when recruiters read your resume, regardless of your experience, they are wowed and the world becomes a little bit less about presentation of skills and more about your actual skills and how you would fit into a company), so with all that being said, if you can share with your friends and to your networks, that would be worth so much more than any donations :)",htqh8cn,t1_htqfy26,1642853955.0,True
s9enpp,Happy to spread the word :),htqi209,t1_htqh8cn,1642854497.0,False
s9enpp,"That's very kind, thank you!",htqierg,t1_htqi209,1642854724.0,True
s9enpp,You are amazing! Thank you so much!,htqfw88,t1_htqbm1o,1642853044.0,False
s9enpp,"Thank you! It is still quite basic in the ML functionality and planning on doing a lot more so if you have any thoughts or feedback please let me know

(Edit: I've set up a discord so if you do have any thoughts, please message me on there! [https://discord.gg/VmG75yrb](https://discord.gg/VmG75yrb) )",htqgsi8,t1_htqfw88,1642853662.0,True
s9enpp,I've only heard of GPT–3 from Tom Scott videos.,htmb5wa,t3_s9enpp,1642784327.0,False
s9enpp,It's pretty cool - has billions of parameters and is supposedly the most sophisticated language based machine learning out there,htmbzy5,t1_htmb5wa,1642784634.0,True
s9enpp,Oooh. Sounds good.,htmceuq,t1_htmbzy5,1642784786.0,False
s9enpp,Definitely worth checking out if you're interested in machine learning or ai :),htmcqkn,t1_htmceuq,1642784906.0,True
s9enpp,Is GPT-3 open source?,htxi7f1,t1_htmcqkn,1642972742.0,False
s9enpp,"You would think, given that the company that created it is called OpenAI, that the answer would be yes. The answer is that the model architecture and instructions on how to train an instance \_are\_ open source, but good luck actually training a model from scratch without millions of dollars and some pretty sweet hardware. Basically the code is open source but the data is not and the data is the valuable bit so the answer is no :(",htxj9yp,t1_htxi7f1,1642973137.0,True
s9enpp,But can I train the model without a lot of data?,htxm2ol,t1_htxj9yp,1642974199.0,False
s9enpp,You can find pretrained models on huggingface and use those,htxu563,t1_htxm2ol,1642977335.0,True
s9enpp,"That’s awesome! Would you be willing to share some of your experience operationalizing this? I studied Computer Vision (CLIP) but I haven’t found much in the way of taking something from its categorizations to a finished product.

Do you have an API that ingests and predicts, then a UI on top to display the results? It looks really well done.

Are you running it locally or in the cloud?",htmfp6g,t3_s9enpp,1642785992.0,False
s9enpp,"A bit of background on how i built it:

The backend is a massive Django instance which is basically a JSON API (we use DRF to make this easier), which talks to a postgres DB and a redis cluster.

We also use celery for long running tasks (e.g. initially tailoring a resume or beat tasks like sending onboarding reminder emails which I'm setting up now).For the frontend, it's again pretty simple: NextJS with typescript (love typescript) and tailwind for css.

I also use headlessui, the component library from the tailwind team, which has been really helpful in places. The marketing site ([https://www.joinrhubarb.com](https://www.joinrhubarb.com)) is also a NextJS site, I think it's so good for these sorts of things.

The bert instances are all fastapi (not sure if I would use this again) with pytorch for inference. I deploy these on elastic beanstalk (which is also where everything else is deployed) and while it works great for everything else I worry that we're overpaying for some massive ec2 instances we don't need.

Last is the chrome extension which is also react/typescript but like... kinda hacked together with a custom webpack config which needs improvement. We will soon have firefox/safari extensions but it's quite annoying/painful to do and deploying to the stores means we need to go through approval processes which is annoying.

Oh we also have some random lambdas for backend jobs and we use posthog for analytics which I cannot recommend enough, its really so so good.",htmfy8t,t1_htmfp6g,1642786084.0,True
s9enpp,u/pursuitofsadness \- happy to go into more detail on any of these bits if useful :),htmg0k0,t1_htmfy8t,1642786107.0,True
s9enpp,"That’s fantastic! I really appreciate sharing all the components. I’m gonna go through the process and see what using it is like asap!

Once you understood the basics of GPT-3 how fast do you think it was to operationalize this? What did it cost to train the model? Was it easier or harder to do than you expected?

What made fastapi a bad tool to use in your deployment? I’ve generally heard positive things.

With these hyper-scale foundational models I’ve heard the volume of fine tuning data required to get improvement on a more specific corpus isn’t huge (I think it’s called one-shot learning?), was that your experience?

Is it learning in real-time or are the weights updated on a schedule?


And finally, so I’m not a total leech here:

It’s not a lot, but I’m a Product Manager and I would be happy to give you my take on your sign up process and some use takeaways?",htmibsu,t1_htmg0k0,1642786956.0,False
s9enpp,Have DM'd you - happy to provide more detail and would appreciate any feedback you have on what I have made.,htmjqtd,t1_htmibsu,1642787464.0,True
s9enpp,Wow! Very cool. I'm doing a similar thing on overleaf with my premade java code to change which projects on my cv to highlight. Personalised CV matter,htn3thl,t3_s9enpp,1642794854.0,False
s9enpp,That's really cool! Sounds similar to what i've been doing -  Have DM'd,htq3m3z,t1_htn3thl,1642844202.0,True
s9enpp,"lol, nice!!  I love it.",htnhde0,t3_s9enpp,1642799739.0,False
s9enpp,Thanks!,htnyc6x,t1_htnhde0,1642805998.0,True
s9enpp,"Nice, now your AI can talk to their AI",htq2d3b,t3_s9enpp,1642843286.0,False
s9enpp,Now I just need to put in on the blockchain and twitter will go crazy for it lol,htq3v53,t1_htq2d3b,1642844385.0,True
s9enpp,Maybe Mint the code as NFT,htq4s5s,t1_htq3v53,1642845060.0,False
s9enpp,Could call it Resume Chimp or something lol,htq6a2q,t1_htq4s5s,1642846158.0,True
s9enpp,"Love it, looks and sounds like a really awesome tool. Wonder how long it took you to build and if you worked on it with anyone else. Cheers!",hto58v2,t3_s9enpp,1642808702.0,False
s9enpp,Took about 7ish months and worked on with a friend :),htq3syq,t1_hto58v2,1642844340.0,True
s9enpp,"(Although, I did 90% of the dev and he just helped me with bits and pieces)",htq3tuv,t1_htq3syq,1642844359.0,True
s9enpp,Great job:),htqlmbm,t3_s9enpp,1642856646.0,False
s9enpp,Thank you!,htr4i16,t1_htqlmbm,1642865953.0,True
s9enpp,I’ll be using it . Thanks,htrgduw,t1_htr4i16,1642870790.0,False
s9enpp,"Publishing the source code of this would be awesome, not only advertising for your website but also to improve what you did",htrlnox,t3_s9enpp,1642872829.0,False
s9enpp,"I love it, but I cannot go past the work experience section (as I don't have any). I can't save an empty field or skip it.",htsm21k,t3_s9enpp,1642887351.0,False
s9enpp,"Ahh yes I made it a required field but currently working to make it skippable. The codebase is a bit of a mess so it is taking a little while. If you have any other suggestions, please let me know!

(EDIT: I just set up a discord so feel free to suggest any features or issues in there - [https://discord.gg/VmG75yrb](https://discord.gg/VmG75yrb) )",htvccto,t1_htsm21k,1642938372.0,True
s9enpp,Thank you bro :),htvrbu1,t1_htvccto,1642947694.0,False
s9enpp,Can this tool works in portuguese?,htxep4q,t3_s9enpp,1642971409.0,False
s9enpp,"Definitely adding portuguese (and other languages) is on the roadmap but I would say its not likely in the next \~6 months because unfortunately I don't speak portuguese so would need to pay for translations and don't have the money at the moment :( sorry I don't have a better answer for you, localisation is definitely something we want to do!",htxh6fc,t1_htxep4q,1642972342.0,True
s9c6ve,"> My question is that is data aligned according to its size(self-alignment) or according to the memory access granularity of the processor.

If ""memory access granularity"" refers to the granularity of memory accesses as treated by the memory consistency model, then the answer to your question is according to data type size.

> It would be great if you could specify some cpu architecture that allows data types larger than its memory access granularity.

The Cray NV-1 and NV-2 architectures (implemented by the X1 and X2 supercomputers of the 2000s) had a 32-bit access granularity, but was a clean-sheet 64-bit architecture.",htrm8do,t3_s9c6ve,1642873051.0,False
s95ek1,"Vectors are very general and generic mathematical object with many different representations. In CS, they're most commonly seen as an ordered list which can be acted on with some basic operations like dot product, elementwise operations , etc. 

Just to give a list of examples at the top of my head:

* **Vector Clocks** \-- vectors clocks are used to enforce a causal order of events in a broadcasting system. [https://en.wikipedia.org/wiki/Vector\_clock](https://en.wikipedia.org/wiki/Vector_clock)
* **Graphics** \--  Vectors are used to represent points, direction and lines. Transformations can then be seen as operations on vectors. [https://en.wikipedia.org/wiki/Polygon\_mesh](https://en.wikipedia.org/wiki/Polygon_mesh)
* **Vector Processors** \-- Rather than the CPU working on one data item at a time, it can work on a vector of items at once. This is used in designing supercomputers. [https://en.wikipedia.org/wiki/Vector\_processor](https://en.wikipedia.org/wiki/Vector_processor)
* **Machine Learning** \-- in many parts of machine learning, the objects which are selected are feature vectors. https://en.wikipedia.org/wiki/Feature\_(machine\_learning)",htkvoco,t3_s95ek1,1642758103.0,False
s95ek1,"There's the vector the combination of magnitude and direction, which is used in linear algebra, physics, engineering. There's vector the 1 dimensional array, which is used in a whole bunch of ways. There's vector the disease transmission method, which comes up in cybersecurity.
 
Your question really isn't answerable, there isn't going to be an article or youtube video covering what you are asking. Work on your question more to make it less vague and come back.",htkr1mh,t3_s95ek1,1642754482.0,False
s95ek1,"Computer graphics, physics simulations/some CFD techniques are the ones I have off the top of my head. It’s used in game design and missile guidance too.

Edit:
https://en.wikipedia.org/wiki/Guidance,_navigation,_and_control

https://en.m.wikipedia.org/wiki/3D_projection

These are some introductory Wikipedia articles about topics where vectors end up being used with some frequency.",htkqy95,t3_s95ek1,1642754409.0,False
s95ek1,Quaternions!!!,htleud3,t1_htkqy95,1642771192.0,False
s95ek1,Look into computer graphics.,htkmvkp,t3_s95ek1,1642751377.0,False
s95ek1,import numpy as np,htkmuyf,t3_s95ek1,1642751365.0,False
s95ek1,"Graphics, machine learning, many areas. Plenty of algorithms use it for various things, there are vectors involved in PageRank, the algorithm search engines like Google use.",htmxgha,t3_s95ek1,1642792555.0,False
s95ek1,"used it a lot in computer vision, graphs & mathematically computation software like matlab",htsqaa8,t3_s95ek1,1642889070.0,False
s8kxq7,"Gale-Shapley could fail because it's not optimizing a global quantity like the sum of the distances. Take this case where you want to connect lower case and upper case letters and the distance between them is the literal distance in the figure:

A---a-B---b-C---c

Gale-Shapley will give you a stable matching so it will connect a and B (because if a is not connected to B, then a and B will ditch their partners and get together) and b and C. So that leaves A and c to get connected which is a huge distance. Clearly connecting A and a and B and b and C and c is shorter than just the A to c distance.

What you're looking for is obviously well studied since it's such a fundamental question. I don't know much about it, but here's a link suggesting that there is an efficient algorithm for it even if the distance of each pair could be chosen arbitrarily (and a super-efficient randomized algorithm!): https://en.wikipedia.org/wiki/Assignment_problem#Balanced_assignment",htiujzg,t3_s8kxq7,1642720180.0,False
s8kxq7,"Yes, Gale-Shapley will work. Model the people and houses as proposers (men/women in the original algorithm) and the sorted distance to each of the other as their list of preferences. Running the algorithm will give you a stable matching, which means no other pairing would have a greater total preference (lower distance).

Edit: one thing to note, the stable matching will minimize the distance for the proposer, so model the problem accordingly.",hthun7u,t3_s8kxq7,1642705556.0,False
s8kxq7,It's just maximum bipartite matching,htugh9g,t3_s8kxq7,1642915839.0,False
s8eq88,"""Master's of Doom""- David Kushner, also ""Hackers Heroes of the computer revolution"" - Steven Levy, really motivated me for like a month or 2 after reading them. But motivation is overrated, what you really want is discipline. If you're able to do something when you don't feel like it you've already out did the average person.",htfs44z,t3_s8eq88,1642669835.0,False
s8eq88,"“Masters of Doom”! I think DOOM is one of the best case studies in CS/SE, that every aspiring student or seasoned practitioner should analyze because what John Carmack produced, was really incredible. He is, probably, my biggest inspiration, that made me transition from just focusing on 3D graphics as an art form, to wanting to explore computational systems further.",htggsz3,t1_htfs44z,1642686369.0,False
s8eq88,he is a true programming cowboy,hti1m1s,t1_htggsz3,1642708156.0,False
s8eq88,"Bornstrom's thesis on why we most probably are living in a simulation, following the same line of thought a programmer in a sufficiently advanced civilization is god (quite literally).",htk6nms,t1_htfs44z,1642741381.0,False
s8eq88,Idk just read wikipedia of von neumann.,htgccl7,t3_s8eq88,1642684129.0,False
s8eq88,It's crazy to me how somebody can be that intelligent.. like the difference between von Neumann and your average CS graduate is probably like the difference between your average CS graduate and a 12 year old,htj1o8d,t1_htgccl7,1642723115.0,False
s8eq88,Godel Escher Bach by Hofstadter might do the trick.,htghdte,t3_s8eq88,1642686642.0,False
s8eq88,"The Mythical Man-Month

Overview: https://en.wikipedia.org/wiki/The_Mythical_Man-Month",hti5kva,t3_s8eq88,1642710002.0,False
s8eq88,"Hmm, these aren't necessarily the deepest but I think you would like these.

Just for Fun: The Story of an Accidental Revolutionary by Linus Torvalds

Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age by Michael A. Hiltzik

Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Tim Berners-Lee",hti62oc,t3_s8eq88,1642710237.0,False
s8eq88,"Geoffrey James, The Tao of Programming (1986)

Jon Bentley, Programming Pearls (1986)

Jon Bentley, More Programming Pearls (1988)

Jon Bentley, Programming Pearls 2/e (1999)

Gene Kim, The Unicorn Project (2019)",htie32c,t3_s8eq88,1642713182.0,False
s8eq88,"Haven't read The Unicorn Project, but The Phoenix Project was worth a read imo",htiewua,t1_htie32c,1642713482.0,False
s8eq88,"Loved Phoenix Project! A little more DevOps-y, but also we’ll worth reading for coders, IMHO.",htjvst8,t1_htiewua,1642736147.0,False
s8eq88,"the 1981 Pulitzer Prize winner ""Soul of a New Machine"" by Tracy Kidder, the 1995 novel ""Microserfs"" by Douglas Coupland, ""Neuromancer"" and the short story ""Burning Chrome"" by William Gibson, the aside beginning with ""The programmer, like the poet"" in Fred Brooke's Mythical Man-Month, ""A Mathematician's Apology"" by GH Hardy, Wigner's essay ""the unreasonable effectiveness of mathematics in the natural sciences"", Dijkstra's ""a Discipline of Programming"", all the footnotes from SICP and any HAKMEM you can find, aaronson's ""the ghost in the quantum machine"", and The Mentor's ""Conscience of a Hacker"" from Phrack issue 7.",htjhdyt,t3_s8eq88,1642729837.0,False
s8eq88,CODE,hthajdv,t3_s8eq88,1642698280.0,False
s8eq88,Pragmatic programmer is good choice.,htga5q7,t3_s8eq88,1642682923.0,False
s8eq88,Algorithms to live by is a good one. Not motivational but gives hope that everything we learn has a real life use other than putting food on our table,htgp4ef,t3_s8eq88,1642690104.0,False
s8eq88,"* A bit old now, but Dust or Magic is interesting from a game design point of view https://www.amazon.com/Dust-Magic-Bob-Hughes/dp/0201360713 
* Similar vintage, Are your lights on? more about the analysis side of things than actual coding https://www.amazon.com/Are-Your-Lights-Figure-Problem/dp/0932633161
* greenspun's Web Publishing https://www.amazon.com/Philip-Alexs-Guide-Web-Publishing/dp/1558605347
* Programmers at work, an older classic https://www.amazon.com/Programmers-Work-Interviews-Computer-Industry/dp/1556152116",hthffda,t3_s8eq88,1642700028.0,False
s8eq88,[The Cathedral and the Bazaar](https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar) is a classic philosophy of code book that every programmer should read at some point.,hti70ew,t3_s8eq88,1642710600.0,False
s8eq88,Pragmatic Programmer is a pretty nice read,htkf9xz,t3_s8eq88,1642746200.0,False
s8eq88,I’m reading [The Making of Prince of Persia](https://www.jordanmechner.com/store/the-making-of-prince-of-persia/) by Jordan Mechner. I love it.,htfrw17,t3_s8eq88,1642669654.0,False
s8eq88,Dreaming In Code,htiytgt,t3_s8eq88,1642721936.0,False
s8eq88,The hungry hungry caterpillar teaches you how so many different parts of a whole go into making the program.,htkainf,t3_s8eq88,1642743452.0,False
s8eq88,Wow this post is getting some action! Thanks everyone for your suggestions.,htmp5lh,t3_s8eq88,1642789424.0,True
s8eq88,Thanks for this post! I’ve been in search of something similar,hu4ljau,t3_s8eq88,1643096375.0,False
s8a2k9,"It sounds like you are just describing “life” as an NP problem (one where finding answers is hard but verifying it is much simpler). I kinda see the parallel you are drawing there. Don’t see how this is related at all to P=NP as clearly nature doesn’t solves the NP problem in P time, instead it takes forever randomly brute forcing it massively in parallel. If anything it’s a testament that P!=NP as even eons of evolution hasn’t found a better way.",htf4kvu,t3_s8a2k9,1642654409.0,False
s8a2k9,That’s what I’m saying. Nature shows that P != NP,htfdgxc,t1_htf4kvu,1642659332.0,True
s8a2k9,"No this is not a prove. Let me reformulate your proof. We have no fast algorithm to make life. There is a nature algorithm but this one is terribly slow. Therefore there can not be a fast algorithm to make life.

The problem with your proof is that just because we have not found a fast algorithm does not imply that it does not exist. If you would like to proof P!=NP. You would have to proof that the existence of such an algorithm is impossible. This type of proof has been used in the halting problem.",htggp0x,t1_htfdgxc,1642686316.0,False
s8a2k9,"To be able to prove P=NP we would have to reduce a really hard problem into an easier problem. But first we need a formal solution that solves that hard problem. I appreciate your train of thought and it’s interesting to think more deeply. However, biology is not my speciality.

Using your example, first we would have to prove or disprove mathematically that there is an algorithm that can create life (I’ll leave this to the computational biologists) that is tractable or non-polynomial time. [There is evidence](https://www.pnas.org/content/118/49/e2112672118) that we (an AI) can design life using larger building blocks, but having one piece of evidence is not enough to satisfy this type of proof. We’d need something that can be replicated and reproduced empirically and to my knowledge we’re not quite there with synthetic life. In 20 years who knows what we’ll have. (Also please, someone with a better biological background correct me.)",htf6js2,t3_s8a2k9,1642655441.0,False
s8a2k9,"My understanding is that we don’t have that algorithm right now, so as it stands right now, nature shows that P != NP",htfdkky,t1_htf6js2,1642659395.0,True
s8a2k9,"For me, an analogous argument based off evidence isn’t enough to conclude that P=NP or P!=NP.

A counter example I could offer is that, according to the physicists, the universe’s age is on the order of 10^10 and the heat death is theorized to occur on the order of 10^100. Assuming that 10^100 is the time limit for life to form, there’s still a _lot_ of time left in the universe for life to do it’s thing (not even accounting for all the possible chances at life near other stars in the first 10^10 years) in tractable or non-polynomial time.

Perhaps if we had a perfect theory that combined physics, chemistry, biology, and computation into a model descriptive enough to prove or disprove either way. But from my knowledge we aren’t there (… yet!)",htfguv2,t1_htfdkky,1642661490.0,False
s8a2k9,"""show"" in the mathematical terminology means the same as ""prove"" (like in mathematical prove). So that is clearly not the case here.

Still I like your idea, it is an interesting thought.",htmvy7v,t1_htfdkky,1642792002.0,False
s8a2k9,">Biologically, for many organisms (think simple multi cellular; plants, things that don’t have super complex brains) we understand the structure and function of literally every single atom and cell that they are made of.

Name one such organism.",htfvg25,t3_s8a2k9,1642672531.0,False
s8a2k9,Many small Protozoa and very simple life forms. The main grey area in our knowledge is brain functions. There are organisms that are simple enough that we basically understand how each piece of them works.,htfz074,t1_htfvg25,1642675347.0,True
s8a2k9,"Are you sure that we can't design ""artificial"" life? 

[Here is one example of a company which grows artificial meat and designs artificial yeast for beer, using designer cells.](https://en.wikipedia.org/wiki/Ginkgo_Bioworks)

I think we could grow our own plants and design our own seeds from scratch on a CAD in the near future, if it isn't already the case somewhere at Monsanto or something. And that's just the tip of that awesome iceberg.

Edit: To clarify, I think we can probably make whatever we want. I am skeptical of limits of any kind. We can probably design whole organisms in a Frankenstein kind of way. I think that's more likely than not. People find strange comfort in these arbitrary limits on what we can and can't do, and sometimes I'm not sure they are reasonable ones. If we threw trillions of dollars at companies like the one I linked above and other such companies and researchers, then who knows.",hthtaq1,t3_s8a2k9,1642705060.0,False
s8a2k9,"Wonderful thought, thanks for sharing!",htf62fu,t3_s8a2k9,1642655190.0,False
s82mjq,Cisco Packet Tracer? Helped me in understanding networking and was used in school.,hte4ro0,t3_s82mjq,1642638109.0,False
s82mjq,"Protocols are defined and written out explicitly. For instance, UDP is defined as [RFC 768](https://datatracker.ietf.org/doc/html/rfc768)

You just search “<protocol name> protocol paper” and it’ll come up",htexy11,t3_s82mjq,1642651153.0,False
s82mjq,Professormesser.com network + study videos. All free on YouTube,htk0lb5,t3_s82mjq,1642738367.0,False
s7yrsa,"CPUs are designed to operate on bytes, not individual bits.
8bit=1 byte, 
 16bit=2bytes. 
There is no way to operate on half of a byte or any other fraction.

Interestingly, this limitation doesn't actually matter. Compression algorithms will use huffman coding. If you somehow made an image that only used 12 bits and the rest was filled with rezos, compression algorithm could take care of that and find some optimal way to store such images any way.",htd2pzm,t3_s7yrsa,1642623247.0,False
s7yrsa,"> There is no way to operate on half of a byte or any other fraction.

Fun fact, half of a byte is called a nibble.  You'll only ever see it in assembly programming, and even then it's incredibly rare to bump into.",htdvoyk,t1_htd2pzm,1642634210.0,False
s7yrsa,"And COBOL, IIRC. Signed numerics store the negative/positive in the low/right nibble of the lowest digit. Is it sad that this takes up space in my brain?",hte53ff,t1_htdvoyk,1642638254.0,False
s7yrsa,and trivia games/shows. I feel like I've seen it on a number of them.,hte98dt,t1_hte53ff,1642640114.0,False
s7yrsa,It uses 4 bits to determine whether a number is negative? Why....,htf4wji,t1_hte53ff,1642654578.0,False
s7yrsa,"Disclaimer: Not a COBOL developer. I’ve just had to read a fair amount in a migration project.

From what I gather, a lot of the persisted data from COBOL is fixed width. Numerics being right justified. If the number gets big enough, you risk loosing the +/- if held on left side. By “packing” the sign with the smallest digit you guarantee knowing whether the numeric is negative or positive. Also ensures you can carry the same digit span.",htgnvpr,t1_htf4wji,1642689580.0,False
s7yrsa,"Almost certainly binary coded decimal.  Each nibble is used to represent one decimal digit.  Yes, this wastes 6/16 of the available storage space but it's trivial to display as a decimal number.  Many CPUs (including x86/x87) have instructions for doing math on BCD numbers, though I imagine these days the use of them must be vanishingly rare.

Once you're using a nibble for each decimal digit, a nibble for the sign bit also makes some sort of sense.",htg28cz,t1_htf4wji,1642677777.0,False
s7yrsa,"As I've mentioned in another comment, the x86 platform still has instructions for doing maths on binary coded decimals, where each decimal digit is represented by a nibble.  COBOL is the only language I've seen that supports it though.",htg2jq0,t1_htdvoyk,1642678001.0,False
s7yrsa,">rezos

Is this the CEO of azamon?",htgdpou,t1_htd2pzm,1642684842.0,False
s7yrsa,There is one computer I know of that uses 16 bits but one is for parity so there's 15 usable bits. It's the Apollo Guidance Computer.,htgwja1,t1_htd2pzm,1642693065.0,False
s7yrsa,"> There is no way to operate on half of a byte or any other fraction

You can certainly operate on arbitrary bit-sized integers, it's just less efficient than those same operations on the size and alignment that the CPU was designed for.

The folks at adobe probably decided image processing at arbitrary bit depths was just not worth the effort- nobody really needs it.  Users would be better served exporting to their desired bit depth if they had a use-case that *really* called for it.",htf4zma,t1_htd2pzm,1642654623.0,False
s7yrsa,"In a sense, tradition.

Earlier computers were designed around different bit lengths: [4 bit architectures like the early 4 bit microprocessors](https://en.wikipedia.org/wiki/4-bit_computing), early PDP computers (like the PDP-8) had a [12 bit architecture](https://en.wikipedia.org/wiki/12-bit_computing), some early mainframes used [36 bit architectures](https://en.wikipedia.org/wiki/36-bit_computing)--and early versions of the LISP programming language were built around those architectures. (LISP uses a two-pointer cell with some control bits--and with a 16-bit address system, 36 bits works very well for a two-pointer cell with 4 control bits.)

But eventually we seemed to settle on 8 bits because it was a power of 2 and because the ASCII character set would fit inside. And eventually we settled on powers of 8 bits (8 bit, 16 bit, 32 bit, 64 bit) for backwards compatibility.

But there is absolutely nothing magic about 8-bits outside of tradition.",htdl2ky,t3_s7yrsa,1642629948.0,False
s7yrsa,"A slight clarification (this was a good write up of the history, not disagreeing with the parent comment at all), though the choice of 8-bits wasn’t magical, once 8-bits was chosen there is “magic” in terms of memory alignment and speed of operations, which is why you see formats aligned on powers of two boundaries.",htdytoy,t1_htdl2ky,1642635510.0,False
s7yrsa,"Memory alignment and speed of operations stuff happen because you can vectorize the base memory storage unit using multiples of that base memory storage unit at the same time--not because the base memory storage unit is a byte with 8 bits.

Meaning you gain a lot with a 16-bit processor over an 8-bit processor because it can operate on two bytes at a time rather than one byte at a time--not because the byte happens to be 8 bits.

Now it turns out 8 bits worked well in earlier microprocessor designs because it was twice 4 bits--the Z-80 CPU, for example, actually had a 4-bit ALU, and would perform things like addition operations by carrying out the operation on the bottom 4 bits first, then again on the top 4 bits. (It's one reason why the Z-80 had the 'half-carry' flag in the flag registers, though the 'half-carry' flag could then be used for the [DAA (Decimal Adjust Accumulator)](https://stackoverflow.com/questions/8119577/z80-daa-instruction) instruction to perform [BCD](https://en.wikipedia.org/wiki/Binary-coded_decimal) addition and subtraction.)

But now that modern CPUs perform vectorized math using multi-byte words, the advantage of doing things like performing 8-bit arithmetic using a 4-bit ALU is lost. There is no reason why an ALU can't be 9 bits wide or 23 bits wide, except that we have gotten so used to 8-bit bytes we seem to think there's no other way.

----

Edit to add: As a side note, the reason why the PDP-10 was 36-bits wide 'byte' was that it was twice the PDP-9's 18-bit wide 'byte', and was able to vectorize operations by operating on two 'bytes' at a time. (In older parlance each addressable unit of memory was referred to as a 'word', even though later parlance came to use the word 'word' to mean 2 8-bit bytes.)",hteq4cf,t1_htdytoy,1642647611.0,False
s7yrsa,"To be clear (maybe my comment wasn't), I wasn't dismissing your original premise (8-bits was not a magical number, it just worked out well) but once we chose 8-bits, there's a reason you see in memory structures aligned on 8-bit or other powers of 2, like the the original questions graphics format.",htf39bd,t1_hteq4cf,1642653746.0,False
s7yrsa,"Absolutely some good things come from 8 bits--but a lot of the things we think of as intrinsically 8 bits in nature really aren't. 

Like 8-bit color in a color file. Some higher resolution monitors are capable of displaying more than 8-bit color--but the most I've ever seen is 10-bit colors; that is, [10 bits per RGB channel.](https://www.bouncecolor.com/blogs/news/10-bit-monitor)

When building graphics software, however, it's easier in this era of large hard disks and gigabytes of memory standard, it's easier and faster--because of the way we vectorize operations across multiple words--to use 16-bit color representations, rather than attempt to bit-shuffle 10 bits into a 30-bit representation.

Were we still using the PDP-8 architecture of 12-bit bytes, however, we'd probably be talking about the naturalness of using 12 bits as the limit of human vision, and talking about how it's natural to represent color as 3 12-bit bytes. There may even be people bragging about their 12-bit color monitors (though they were really only 10, and using dithering for the other 2 bits).

Color graphics in particular has always been a compromise between what the hardware can do and what the eye can perceive. And 8 bit color channels did not solve these compromises; your eye can perceive a 256 color gradient on a monitor.",htgnzr8,t1_htf39bd,1642689627.0,False
s7yrsa,"I think the other answers here are excellent, but wanted to point out that sometimes we do still break with tradition. HDR video, for example, is typically 10-bit per color.",htg46ge,t3_s7yrsa,1642679159.0,False
s7yrsa,"It often is useful to make the word size a power of two. For example, a bit multiplexer will be able to cleanly select a bit out of such a register.

Further, once 8 bit CPUs became a thing, memory being byte-addressable became a standard. If you now design a new CPU which does not use a word that is a multiple of 8 bits large, you either need completely new memory or are going to waste bits. Both is inefficient, so new CPUs are adapted towards existing RAM, and we have stuck with 1 byte being the basic unit of memory ever since.",htd6sso,t3_s7yrsa,1642624757.0,False
s7yrsa,"There are color formats between 8 and 16 bits:

https://en.m.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats

Most modern architectures have a memory bus 2^n bytes wide, so many data formats these days will use as much of that as possible. Compact representations these days are much less important than memory bandwidth and latency.

The real question is why a byte is 8 bits wide. Believe it or not, it’s mostly just consensus. 8 bits was the right size at the right time, neither too big for a memory bus nor too small to be useless. For example, you can fit both the upper and lower Latin alphabet in 7 bits with an extra bit as a check bit (ASCII), so it’s a convenient size.",hted1ei,t3_s7yrsa,1642641820.0,False
s7yrsa,"Desktop version of /u/bargle0's link: <https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hted32w,t1_hted1ei,1642641841.0,False
s7yrsa,"In computing because of bits everything is POWER OF 2, so get used to 2, 4, 8, 16, 32, 64, 128... 1024, 2048...",htdx4be,t3_s7yrsa,1642634796.0,False
s7yrsa,Pretty much everything in computers boils down to boolean operations. This is either true or false; 0 or 1. The technical reason is that either electricity is going through or it is not. Since that only gives you two options everything else tends to involve powers of 2.,htdoqvg,t3_s7yrsa,1642631383.0,False
s7yrsa,x2,htene1e,t3_s7yrsa,1642646424.0,False
s7yrsa,Much easier to write the logic for word sizes that are sections of each other. Be much harder to make a 13bit machine run compatibly with 8bit logic.,htf4rdb,t3_s7yrsa,1642654502.0,False
s7yrsa,"ok, but why is it like that? 8 bit and 16 bit just represent how many possible color combinations per channel there are.",huhwlet,t1_htf4rdb,1643320124.0,True
s7yrsa,"So they represent how the storage is chopped up. Your different variables, like assigning a color to a pixel is part of a system of standardizations. We decided on word size (8bit) now people wrote programs to manage that data storage schema but making the first few bits represent commands that the hardware would interpret as actions to take. So a system of programs (operating system) was eventually developed to manage the addition and removal of sequences of words that formed the actions on a cpu (programs). Because of this standardization we had to agree on how things would be read so we can make generalized logic that can do things on every system.

So eventually we get to the application layer (layer 7) which is a composite of general logical “applications” that live below it (layers 1-6). At this point the application is running on 6 layers of logical framework that all expect layer 1 (hardware) to be exactly those word sizes so that it can perform the millions of actions needed to generate the amount of changes in a system to output such complex functionalities. The color is a pixel isn’t just coloring the pixel, it’s the management of data storage in a way that can manage the color of the pixel in such a robust way that the hundreds of apps and services that are offered today can easily manage the changing of that pixels color in a very very very specific way. 

When you code you generally don’t say what a pixels color is you code what the background color is, or the text buttons color is, or you create an array that contains bits configured in what the program expects as rgb format or whatever makes sense and then you tell the cpu/gpu how each word is chopped up because all the other programs running right now require that the data system be chopped up into exactly 8 or 16 or 32 bits.

Now that’s the reason for exact word sizes the binary expansion as in 8bits to 16bits is because now legacy systems can still chop the database up into 8bit sizes without breaking the systems that run on 16 bits. You just treat one word as two words within that applications allocated storage. 

TLDR: all other programs expect a standard word size, so new applications leverage capabilities built on top of those standards. We go from 8 to 16 because 8 bit programs can run still by treating each storage word as two words without running over a single word size allocation, so legacy programs can always run.

This is a simplification because there are also timings and physics stuff the hardware of the earlier times were hard coded into some paradigms, but the reading and organization of information lives eternally within the binary expansion paradigm",huj8us5,t1_huhwlet,1643340122.0,False
s7yrsa,All thing happens in bytes! So a byte is equivalent to 8 bits. 2 byte is 16 bits. If there was anything inbetween 8 n 16 bits it could be in decimals maybe.,htf9d3q,t3_s7yrsa,1642656970.0,False
s7yrsa,Binary means base two. 8 and 16 are powers of two.,htfj07t,t3_s7yrsa,1642662978.0,False
s7yrsa,Take the binary log of 8 and 16. Does that answer your question?,htfs4hc,t3_s7yrsa,1642669842.0,False
s7yrsa,"No it doesn’t, i don‘t get why in photoshop for example, you can only choose between 8 and 16 bit. Since options in between are possible. 8 bit are 256 possible colors per channel. And 16 bit are 65 536 colors per channel. So the difference is huge.",huhw912,t1_htfs4hc,1643319996.0,True
s7yrsa,"I thought your question was about word size and not a particular software. Anyways, historically, number that are powers of two made internal math easier. I gave you CSesque answer.",huhwyuy,t1_huhw912,1643320265.0,False
s7mb3k,"To write down the input, you need to write down all n numbers, which takes space proportional to n. But for W, you only need to write down log W bits to represent the number W. This means that the input size is log W.

Technically you could force the input format to write the number W in unary (W bits long), and you’d have a modified knapsack problem that’s solvable in P. Nobody actually writes down inputs in that way for obvious reasons.",htau3gl,t3_s7mb3k,1642586950.0,False
s7mb3k,"> But for W, you only need to write down log W bits to represent the number W. This means that the input size is log W.

Right. 

So, Knapsack is still in NP as `W` can be exponentially large wrt `n`. Correct?",htaupe4,t1_htau3gl,1642587429.0,True
s7mb3k,"W can be exponentially large relative to the input size. A problem is in NP if its solution can be verified in polynomial time relative to the input size. 

The input size here is n log W, since we need n numbers, no more than W bits each.",htauyt3,t1_htaupe4,1642587635.0,False
s7li8v,"Educated guess, so I may be wrong, but I believe it is because really old modems used [Bauds](https://en.wikipedia.org/wiki/Baud) per second, which are equivalent to bits per second for binary systems.

Seeing as it was common to use the baud rate for dial-up modems, when they first introduced broadband, ethernet, etc., they just continued to use bits per second.

AFAIK, files were always organized into chunks, since if you accessed them bit by bit, you'd need a lot more storage to keep track of all the bits than the space of the drive itself. For example, standard ext file systems on linux are split into 4 kb chunks (by default, you can change that), so creating a file with 0 bytes will still take 4 kb. It also makes sense with 8-bit ASCII encoding to say that a file containing the string `hello` takes 5 bytes, and not 40 bits.

> Can't we just have the same unit for both of them (to reduce the confusion of some people)?

Probably too late now. You have decades of documentation using other standards... Why change now?",htau2oq,t3_s7li8v,1642586933.0,False
s7li8v,"**[Baud](https://en.wikipedia.org/wiki/Baud)** 
 
 >In telecommunication and electronics, baud (; symbol: Bd) is a common unit of measurement of symbol rate, which is one of the components that determine the speed of communication over a data channel. It is the unit for symbol rate or modulation rate in symbols per second or pulses per second. It is the number of distinct symbol changes (signaling events) made to the transmission medium per second in a digitally modulated signal or a bd rate line code. Baud is related to gross bit rate, which can be expressed in bits per second.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",htau3ws,t1_htau2oq,1642586959.0,False
s7li8v,"This is the most correct answer. Transmission just cares about how fast can I send data and there isn’t a set number of 8 bit bytes I have to send. And remember that historically bytes were hardware dependent and of varying length. Also, transmission speeds aren’t just for your network but also used for inside your computer for other transmissions like the bus speed.",htbkpv9,t1_htau3ws,1642602667.0,False
s7li8v,"Bytes have been commonly 8-bits for a very long time (since the 1970s at least), though, even if it wasn’t codified as a standard yet.  
  
The reason it was ambiguous and hardware dependent before was that a *byte* was defined as “the number of bits that were used to encode a *single character*”.",htcn6i9,t1_htbkpv9,1642617505.0,False
s7li8v,"I've wondered for quite a while, that's really informative",hudqloz,t1_htau2oq,1643248349.0,False
s7li8v,"It's mostly historical. 

Even today, most internet traffic is serialised over a single wire. So that wire sends bit after bit after bit. Sure, it packets them up into bytes/packets/whatever, but the fact is that it's a 1-bit serialisation.

However most memory modules and disks operate in terms of bytes, usually burst-reading a row or sector at a time.

Both of those descriptions aren't 100% true in all cases, but are the most common situations for both formats. (e.g. there are very few parallel communication cables/protocols, but they do exist)",htbmcbr,t3_s7li8v,1642603366.0,False
s7li8v,"It might be a marketing strategy, but it might be the resolution perhaps? You transmit bits and you store a byte. Storing a bit doesent makes much sense, but it could be that the marketing strat works on me haha",htao19s,t3_s7li8v,1642582118.0,False
s7li8v,I had a professor in school tell us it was marketing lol,htb68o1,t1_htao19s,1642595479.0,False
s7li8v,"Practically speaking, you transmit packets, which contain an amount of data you'd generally measure in bytes. It's marketing.",htbignx,t1_htao19s,1642601682.0,False
s7li8v,"> Practically speaking, you transmit packets, which contain an amount of data you'd generally measure in bytes.

Nowadays, sure. However, modems didn't just appear out of the blue. They are the product of years of evolution. Data used to be transmitted in different formats prior to ASCII, such as the [Baudot code](https://en.wikipedia.org/wiki/Baudot_code).

> It's marketing.

It''s a legacy of history.

- Does it benefit telecom companies? Of course.
- Are they happy that it inflates their speed numbers? Hell yeah.
- Is marketing the reason things have been counted in bauds and bits for 6+ decades? **No.**

Actually, one ""byte"" has historically had different bit values. 

> The size of the byte has historically been hardware-dependent and no definitive standards existed that mandated the size. Sizes from 1 to 48 bits have been used.[4][5][6][7] The six-bit character code was an often-used implementation in early encoding systems, and computers using six-bit and nine-bit bytes were common in the 1960s. These systems often had memory words of 12, 18, 24, 30, 36, 48, or 60 bits, corresponding to 2, 3, 4, 5, 6, 8, or 10 six-bit bytes. In this era, bit groupings in the instruction stream were often referred to as syllables[a] or slab, before the term byte became common.

https://en.wikipedia.org/wiki/Byte",htbs9ug,t1_htbignx,1642605810.0,False
s7li8v,"Each of those packets needs to have bit level error correction, and each bit is the smallest resolution (and the biggest) that needs to be considered. When stored and accessed by the CPU, we usually only use byte level precision (indexing by bits would mean we could only address a very small space and isn’t practical for anything).

So no, it isn’t just marketing, and it isn’t just a relic of old times. The problems from that relic era are still alive and well today.",htcafw1,t1_htbignx,1642612805.0,False
s7li8v,"When you receive data you are not just getting the bytes that wind up in the file. You are also getting a bunch of other bits that belong to the frame/packet that the data comes in.

If they changed it to bytes/sec that might be confusing/misleading because people may make the leap from ""bits in"" to file download size, which would not be correct.

I agree with some of the others though. There is at least some historical component to it as well.",htcebuq,t3_s7li8v,1642614233.0,False
s7li8v,Marketing: 1 gigabit/sec sounds better than 125 megabyte/sec.,htc4nwd,t3_s7li8v,1642610648.0,False
s7li8v,"125 MB/sec is actually an absurdly fast rate of transfer  when you consider the physical distance traveled and the sheer quantity of intermediate hardware.  
 
That’s almost as fast as the fastest of IDE hard drives ever and they operated *locally* and transmitted data in *parallel*.",htcl0t1,t1_htc4nwd,1642616717.0,False
s7li8v,"Oh yeah, I agree. And with minor exceptions here and there, the data is almost always perfectly intact with all those trillions of transfers going back and forth every few seconds",htcmkzs,t1_htcl0t1,1642617288.0,False
s7li8v,"They are measuring different things.

When you download a file of 1024 bytes, you expect it to take up 1024 bytes in disk.

Now suppose your internet connection is 8,192 bits per second (=1024 * 8).  You might expect your file to take one second to download but you would be wrong.  Every network connection has overheads and the rate that you can transfer useful data is always less than the line speed. Overheads are framing, headers, packet encapsulation and so on. But if you tell people they can get 10 MBps they expect to download 10MB in a second, while they'll actually get about 850-900kBps depending on the exact networking technology.

Another reason is that people building networking gear care a lot about the bit rate to figure out carrier frequencies and so on and these technologies often originated in telephony rather than packet data, where you really do care more about bits than bytes.",htbeim9,t3_s7li8v,1642599859.0,False
s7li8v,"This is a weirdly misleading comment.

>They are measuring different things.

They measure the amount of data in both cases.

>When you download a file of 1024 bytes, you expect it to take up 1024 bytes in disk.

You may expect that, but you would be wrong on most today's disks. The allocation units where the files are stored have some minimal size, e.g. 4KB. So the amount of useful information stored on the disk is lower as well (can be 4096x lower).

But even if there was a different interpretation for these two cases of measuring amounts of data... How exactly would help using one unit for one case and this unit * 8 for the other one?",htdflgs,t1_htbeim9,1642627946.0,False
s7li8v,Error correction encodings play a role too.,htcm7o7,t1_htbeim9,1642617152.0,False
s7li8v,"its a marketing strategy
edit: people hardly ever notice the difference between Kb and KB or dont know and companies try to exploit that to show bigger values",htanqpy,t3_s7li8v,1642581889.0,False
s7li8v,Fun fact: a kilobyte doesn't actually mean 1000 bytes. It means 1024 bytes... IIRC power of 2.,htats0b,t1_htanqpy,1642586700.0,False
s7li8v,"> Fun fact: a kilobyte doesn't actually mean 1000 bytes. It means 1024 bytes... IIRC power of 2.

[Theoretically wrong](https://en.wikipedia.org/wiki/Kilobyte). A **kibi**byte is 1024 bytes according to SI and the IEC.

In practice, people misfortune words all the time, so it's hard to be certain.",htauik5,t1_htats0b,1642587281.0,False
s7li8v,">In some areas of information technology, particularly in reference to solid-state memory capacity, kilobyte instead typically refers to 1024 (210) bytes.

""kibibyte"" was an attempt to resolve the confusion of ""kilobyte"" having 2 meanings but despite working in the computer industry for nearly 40 years I have literally never heard anyone ever say it nor use it in a piece of tech writing.",htb4m7s,t1_htauik5,1642594485.0,False
s7li8v,"> ""kibibyte"" was an attempt to resolve the confusion of ""kilobyte"" having 2 meanings but despite working in the computer industry for nearly 40 years I have literally never heard anyone ever say it nor use it in a piece of tech writing.

Indeed. I did mention there is a disconnect between the theory and practice. Still, it does exist. `dd` uses both.

    > dd if=/dev/random of=66k bs=65536 count=1
    1+0 records in
    1+0 records out
    65536 bytes (66 kB, 64 KiB) copied, 0.0034109 s, 19.2 MB/s

    > ls -lh 66k # gnu's ls -lh defaults to KiB
    -rw-r--r-- 1 u g  64K Jan 19 14:18 66k
    > ls -l --si 66k  # but it does have an option to use SI units
    -rw-r--r-- 1 u g 66k Jan 19 14:18 66k

I guess Linux can't change that in core utils to not break backwards compatibility.

> I have literally never heard anyone ever say it nor use it in a piece of tech writing.

I mean, I did write it in a comment on a tech science subreddit and linked to an article that covers it, so you are also misusing the word literally :)",htb5tl5,t1_htb4m7s,1642595228.0,False
s7li8v,"He didn't hear you talking, and he probably didn't click the link, so his literally is likely to be accurate.",htc9ce9,t1_htb5tl5,1642612398.0,False
s7li8v,"That’s just confusing though and I doubt most people read *KiB* as *kibibyte*. I believe the changeover in Linux to that is relatively recent in any case (last ten years?)
 
In computer parlance, *kilobyte* is abbreviated as **KB** whereas *kilobit* is shortened to **kb**. To try to use ‘kB’ is just confusing.",htcoaia,t1_htb5tl5,1642617908.0,False
s7li8v,"> That’s just confusing though

Pretty clear to me, but I have been working with computers a long time. Other tools also display both (e.g. `fdisk`), while others let you define which units to use (aforementioned `ls`, but also `free`).

It's just one of those things you can easily google if you care to understand it, I guess... And something most users don't even have to care about, really.",htcqk7d,t1_htcoaia,1642618738.0,False
s7li8v,"What I’m really getting at is that, afaik, ‘kB’ doesn’t really mean anything. If people want to say 1000 bytes, that’s how they say it, at least on this side of the world.  
  
I don’t think most people vontinuously check the man pages for updates. And if the standard output works for them, they aren’t likely to seek an alternative.",htcqwjw,t1_htcqk7d,1642618865.0,False
s7li8v,"**[Kilobyte](https://en.wikipedia.org/wiki/Kilobyte)** 
 
 >The kilobyte is a multiple of the unit byte for digital information. The International System of Units (SI) defines the prefix kilo as 1000 (103); per this definition, one kilobyte is 1000 bytes. The internationally recommended unit symbol for the kilobyte is kB. In some areas of information technology, particularly in reference to solid-state memory capacity, kilobyte instead typically refers to 1024 (210) bytes.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",htaujky,t1_htauik5,1642587303.0,False
s7li8v,"In practice though, people mostly use *kilobyte* differently when talking about computers. The US also doesn’t uniformly use SI units anyway.",htcnzda,t1_htauik5,1642617795.0,False
s7li8v,"> In practice though, people mostly use kilobyte differently when talking about computer.

Again, yes, I have mentioned that.

However, this is /r/computerscience, not R/everydayaveragejoecomputing. In science, it is important to be precise. There is a clear distinction, and it is important to be aware of it, and understand it. If you design a hard drive, it's important to provide your customers accurate information regarding its capacity.

For every day usage/speech, if I download a Linux ISO and it's 1.5 gigs, I don't really care whether it's GB or GiB.",htcozy5,t1_htcnzda,1642618169.0,False
s7li8v,"This has nothing to do with computer science or ‘the average joe’, it’s about how humans use *language*.  
 
It was established a long time ago that a kilobyte (KB) was equal to 1024 bytes. Trying to force change has only resulted in more confusion.
  
Also, SI is not totally universal, no matter how much it’s creators wanted it to be. The UK eventually transitioned, albeit slowly, and the US never fully switched over. What’s more is that *bits* are conceptual and insubstantial, not liquids/solids/or gases.  
 
We also have an imperial system in common use that’s got a standardized conversion to metric, so there’s really no incentive for actual usage to change.
  
P.S.  
https://usma.org/a-chronology-of-the-metric-system",htcptqv,t1_htcozy5,1642618468.0,False
s7li8v,"> It was established a long time ago that a kilobyte (KB) was equal to 1024 bytes. Trying to force change has only resulted in more confusion.

Established by whom? It was never formally established until the the IEC (of which the US is a full member) published their standard in 1999. Hard drives used 10^n for 50 years. A one-terabyte hard drive is 0.91TiB.

Also, `kB` is the preferred symbol, FYI.",htcv2ov,t1_htcptqv,1642620417.0,False
s7li8v,"Established in *common usage*. 
  
What some standards body says is irrelevant if it doesn’t have universal (or near universal acceptance). 
 
Honestly I think people are even less concerned with the *exact* capacity of hard drives. They’ve grown so large that most people don’t even worry about filling them up. Heck you can get 4 TB capacity practicaly off the shelf.",htcwby2,t1_htcv2ov,1642620882.0,False
s7li8v,"> Established in common usage.

Except it very much is not, since it means different things to different people. A 16 GB stick of RAM has a different amount of bytes than a 16 GB HD.

Standardization is important for industries. It doesn't matter if people misuse language, but it matters that if you purchase a hard drive, you can see that it conforms to the specifications.

> What some standards body says is irrelevant if it doesn’t have universal (or near universal acceptance).

I'd say it's the opposite. It doesn't matter that your Average Joe misuses the terms because he doesn't understand them. It is far more important to be able to look at the output of a system monitoring command and be able to adequately interpret it. When tools allow you to choose between SI and binary (KiB, ... PiB), you can get accurate information.

Anybody who thinks that kB vs KiB is too confusing might not be particularly suited for computer science.",htcxw0n,t1_htcwby2,1642621459.0,False
s7li8v,"What you don’t seem to understand is that KB or KiB is irrelevant so long as we all agree that it’s 1024 bytes. The misbehavior of for-profit corporations is utterly irrelevant.  
  
There is absolutely no reason that computing needs to care about SI at all.",htcyorr,t1_htcxw0n,1642621755.0,False
s7li8v,"I guess one of those things that started as bits cause data flow was so slow people couldn't use bytes in units, and then as speeds increased changing the understanding from bits to bytes would be an industrial problem, hence bits per second is now a standard.

Why bytes for storage? Well data storage started as bytes to begin with iirc, so it stayed as industrial standard.",htanu46,t3_s7li8v,1642581961.0,False
s7li8v,"Forgot to add, all data communication protocols are bits per second. QPI, PCI/E, RAM and so on, not just network.",htanxzu,t1_htanu46,1642582047.0,False
s7li8v,"Back then transmission speeds were very low. 9 kilo bits, or maybe slower were the norm at some time. Advertisement schemes aside, it would be less practical to express these speeds in terms of bytes.",htbemxw,t3_s7li8v,1642599917.0,False
s7li8v,"> 9 kilo bits, or maybe slower were the norm at some time.

9 kbps was in the 1980s. The earliest modems were like 300 bits per second lol",htbk626,t1_htbemxw,1642602429.0,False
s7li8v,"It has to do with how transmission of data works. The signal on a single wire can only have one measured state at a time.  
  
And once you realize that errors can happen and implement *error correction* then 1-bit/1-byte of transmitted signal transitions is no longer equivalent to 1-bit/1-byte of actual data.
  
There’s probably some deliberate fudge factor at the level of an Internet Service Provider (ISP) as far as making it sound *faster*.",htckb6u,t3_s7li8v,1642616454.0,False
s7li8v,I don't really know but I would guess it has something to do with older internet speeds being much slower and storage capacity growing at a significantly faster rate in comparison.,htanu0n,t3_s7li8v,1642581959.0,False
s7li8v,"Its because they aren’t actually equal to each other which is a pretty common misconception. 

One byte is equal to 8 bits however one bit is so incredibly small its impossible to have one bit in the 21st century.

Edit: i should also mention that kilobits, megabits, etc. do exist. Gigabit probably the being mentioned the most. But one gigabyte isnt equivalent to one gigabit. As far as im aware its a marketing scheme as to why we dont just call them the same thing. If your isp says we offer gigabit internet most people assume they are getting “gigabyte” internet and don’t even think about there being a difference",htbc70y,t3_s7li8v,1642598721.0,False
s7h2vc,"Big O notation tells you how much slower a piece of a program gets as the input gets larger. For example:

* Returning the first element of an array takes a constant amount of time. If you double the length of an array, it still takes the same amount of time. That code does not scale with input size.

* Summing all the elements of an array takes a linear amount of time with regard to the length of the array. Double the length, the code takes roughly twice as long. Make the array a hundred times longer, code takes a hundred times as long.

* Searching for an element in a sorted array? If you're using a binary search, every time the array length is doubled you need to add one extra comparison. Performance _does_ get worse as the array length increases, but less than linearly.

The ""notation"" of Big O notation is just concise shorthand for describing the above patterns. O(1) for constant time, O(n) for linear time (where `n` is the length of the array), O(log n) for logarithmic time, etc.

The focus of Big O notation is on looking at the biggest trend of an algorithm. We don't care whether an algorithm takes 200 steps or 201 steps, we just care about how it scales as input size changes. Therefore, we only look at the slowest part of an algorithm. O(n + 1) simplifies to O(n), because as the array size gets millions of times longer the O(1) is quite insignificant.

Outside of coursework you will very rarely, if ever, need to do a formal proof to demonstrate the Big O of an algorithm. Instead, you're being taught Big O notation now so that you can easily reason about algorithms and data structures in the future. ""Oh, I should use a hash table here instead of a tree, because random element access is O(1) instead of O(log n)."" It's a convenient tool for quickly reasoning about which of two algorithms or data structures will perform better, or for reasoning about your own code and where the bottlenecks are.

Edit: fixed typo",ht9znq5,t3_s7h2vc,1642566573.0,False
s7h2vc,"I wish my textbooks read this clearly. I’m not op, but this helped me a lot thanks",htdizum,t1_ht9znq5,1642629177.0,False
s7h2vc,Thank you! That’s very kind :),htdzku7,t1_htdizum,1642635828.0,False
s7h2vc,"Amazing, saved. Thank you",htcl6n2,t1_ht9znq5,1642616776.0,False
s7h2vc,You’re very welcome!,htdzih2,t1_htcl6n2,1642635800.0,False
s7h2vc,"This was so helpful to me too, despite not being the OP! I actually feel like I somewhat understand Big O Notation now!",htigj4u,t1_ht9znq5,1642714056.0,False
s7h2vc,"It's used to notate the upper bound of how slow a function, `f(n)`, grows with respect to `n`. 

For example, consider the following function (in Python): 

```
def f(n): 
    print(""Hello World"") 
``` 

No matter what `n` is, this function will always run in *constant* time and is independent of the input `n`. We notate this bound with O(1) (1 meaning ""constant""). 

Now, consider this function: 

```
def f(n): 
    for i in range(n): 
        print(i)
```

Now, this function is dependent on `n`. If `n` is 1, it will print 1 line. If `n` is 9000, it will print 9000 lines. We say that the growth of this function is *linear*, and use the notation O(n) to upper bound its growth. 

There's more nuances: like the difference between upper bounding, lower bounding, and the theta one; how do you choose the best upper bound (can't we just say everything is upper bounded by n^n ?); and, how do we calculate the best equation for different functions. But, that's not for dummies.",htad019,t3_s7h2vc,1642574097.0,False
s7h2vc,"I recommend you check out the book [*Grokking Algorithms*](https://www.manning.com/books/grokking-algorithms).

It covers common algorithms and Big O in very approachable terms.

[Here is their ~~explanation~~ *comparison* of common Big O runtimes](https://imgur.com/a/WzTcnAV).

Edit: `s/explanation/comparison/`",htaocl9,t3_s7h2vc,1642582364.0,False
s7h2vc,"**Big O is the language that we use for talking about how long an algorithm takes to run.** \[This is a simplified definition\]

When talking about the running time of an algorithm, we normally use 3 notations Omega Ω, Theta Θ  and Oh O.Those 3 notations represent the following bounds of the running time of an algorithms.

Omega : Lower BoundTheta    : Tight BoundOh         : Least Upper Bound

This dude explains the concept much more better than I can.[https://www.youtube.com/watch?v=f\_IaKCB7Zo8&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y&index=5](https://www.youtube.com/watch?v=f_IaKCB7Zo8&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y&index=5)",htav6jh,t3_s7h2vc,1642587800.0,False
s7h2vc,"Big O is a way to simplify away insignificant terms.

Let's say you have an algorithm that reads an array of length n and does something to it in **n^3 + 5 n^2 - 22 n + 17** steps. As n gets big, the only thing we care about is the **n^3**, because it'll be so much bigger than the other terms, so we say it takes **O(n^(3))** steps.

So it allows us to just deal with the important factors, and hand-wave away the smaller, less significant terms. This is really useful in algorithms where sometimes it takes 2n^2 + 19n + 85 steps and sometimes it takes n^2 + n steps. These cases still run in a similar amount of time, but analyzing the algorithm exactly would be an absolute pain, so it's much easier to do the math if we say the algorithm is just O(n^(2)).",htbrwim,t3_s7h2vc,1642605661.0,False
s7h2vc,"A alg is O(some function of n)  

Means that the steps the algorithm will take are no more than f(n) if n is the size of the input.",htbsaw6,t3_s7h2vc,1642605822.0,False
s7h2vc,"In layman terms it is how fast your program does something in worst case scenario (a lot of input) and in best case scenario (not a lot of input) 

It basically says how efficient is your algorithm meaning the steps your program takes to achieve a certain thing.

Look on YouTube for a video called sorting algorithm visual and you will see unsorted bars in the video and then they get sorted with different algorithms and the ones that sort faster have a better big o Notation.

There are algorithm that are so good that it does not matter how many million inputs you have they do them at the same speed almost and there are bad ones that become slower with more Input",htbxbj4,t3_s7h2vc,1642607801.0,False
s7h2vc,"There are n numbers in an array. You want to find the biggest number. You start from the beginning, checking each. 

The biggest number can be at the beginning, in which case that would take you 1 checks. But it can also be in the middle, which would take n/2 checks. And the worst case scenario, it can be the last number you check, which would take n tries to reach. Big O in this case would be O(n) because it denotes the worst possible case of runtime",htc3u09,t3_s7h2vc,1642610331.0,False
s7h2vc,"Suppose you are doing a task. And the task is such that no matter how you do it, the task will gets completed within 24 hrs. So, here you can complete that task in 2hrs, 6hrs,12 hrs and it won't take more than 24hrs. So, you can say that Big O of ""task"" is 24 hrs.",htc9avk,t3_s7h2vc,1642612382.0,False
s7h2vc,"\--- The Story ---

Say we both have created algorithms to solve some problem.

And suppose it turns out that although your algorithm takes longer (I'm making the assumption that yours takes longer only because you asked me to assume you are dumb).

Now the question is how much longer your program takes to run than mine. Maybe on some input your program took 5 times as long as mine to run. That's useful information, but to more thoroughly compare our algorithms we have to see how it performs when we have large inputs.

There are two types of scenarios that come to mind:

\- Scenario type 1: Your program takes at most 10 times as long as mine to run, regardless of how large the input is.

\- Scenario type 2: I can't put a bound on how long your program takes compared to mine, because as we take larger inputs the ratio between the time your program takes and the time my program takes just keeps growing larger.

These cases are quite useful to differentiate between. They're saying something we should deem very important about the efficiency of our algorithms. Instead of starting off running our algorithms and computing the ratio between our running times for various inputs, we should first ask whether there would be a maximum ratio at all.

If there is no ratio (because it keeps growing), then we say that your algorithm's running time is NOT O(my algorithm's running time).

If there is a maximum ratio, then we say that your algorithm's running time is O(my algorithm's running time).

\--- Usage ---

If you're trying to create an algorithm, instead of computing exactly the number of steps it takes, it is better to get an estimate just using O() notation.

Let's take bubble sort with an array of size n. It can take many passes through the array to finish sorting, but not more than n passes. So overall it is taking around n\^2 steps. Do I care about whether it is 0.5n\^2 or 5n\^2? Yes, but not yet! Firstly it is definitely not taking more than 20 steps every time it moves between elements. So it takes at most 20n\^2 steps. The ratio between the number of steps and n\^2 is never larger than 20, so the running time of bubble sort is O(n\^2).

Before we pin down bubble sort's number of steps, let us first see if we can get an algorithm that takes O(n\^1.5) steps, or let's even see if bubble sort actually takes O(n\^1.5) steps. That would be a better guarantee. This is the larger, more important question to pursue.

You can prove that bubble sort does NOT actually run in O(n\^1.5) steps. This means that if somebody comes up with a complicated algorithm that takes 10000 n\^1.5 steps, that will still be better than bubble sort (because if bubble sort actually takes only 10000 n\^1.5 steps we would say it runs in O(n\^1.5) steps).

Figure out order first, worry about exact ratios later.",hteoe7o,t3_s7h2vc,1642646861.0,False
s7h2vc,It’s just a bunch of nonsense brogrammers ask at interviews.,ht9x9wd,t3_s7h2vc,1642565403.0,False
s71762,"I meta language, but don't machine learn, so commenting for visibility.",ht7o7v8,t3_s71762,1642532514.0,False
s71762,"It's all math that you'll have to learn converting 2d into 3d and I believe you will need to produce x,y,z coordinates from your mapping then you can create a 3d object.

Just focus on creating a sphere first based on your images.",ht90mk6,t3_s71762,1642550920.0,False
s71762,"I appreciate that ""it's all math"" and I'm attempting to do just this, but the point of my question is that I do not understand the fundamentals involved.  I'm currently re-learning linear algebra in the hopes that it will demystify some of what I'm seeing.  I'd hoped that there existed a tried and true method for doing this as it seems such a straightforward task.",hth5eck,t1_ht90mk6,1642696406.0,True
s6mxkp,"The better analogy would be you a window cleaner/construction worker who is high up on building cleaning/constructing something.

Now you need to use things like hammers, brushes, cleaner fluid etc. When you are hanging there midway through a building, the tools you have access to fastest are the ones you carry in your hand. You dont need any extra time to use them. Your hands are like CPU registers. Fastest but limited space, your hands can only hold so many tools.

Say if you come across a difficult to clean spot that your current tools cannot be used for. Then you can carry some less frequently required but still important tools in your pockets. You will need some time to use it because you will have to empty your registers(hand) and take out the required tool from your pocket and you may need to search which pocket you kept it or you just remember (associative memory) which pocket carries which tool. This is the CPU cache (level 1). Your pocket too cannot hold too many tools so you may keep some on a backpack/toolbox that you carry with you, you will need more time again to take out the tool from there because you have to open and close the toolbox and replace something in your hand. But a toolbox allows you to carry a bit more than you your pockets. This is still cache but level 2 cache, slower than level 1 (pockets) but have more memory. A cpu can have L3 cache as well. (For this we would neee to talk about multiple workers (cores) sharing a common big tool box).

Next say you need some thing that is used even less frequently that you generally dont bother taking with you on person, however it still happens enough that you need to use it atleast a couple times per building, so instead of taking it with you on person you take it in your platform that you use to go up on, in this way if you need the tool you dont have to go all the way to the ground to get it, this is like RAM, much slower than cache you had to get down a bit to reach your hanging platform to reach it swap out some things and then carry it back up. Remember for registers and cache you didnt have to climb down. But is still pretty fast. And have way way more capacity than cache (toolbox or pockets).

Then sometimes it maybe so that you didnt prep well and left somthing in the garage/workshop below. But if you need something like that you MUST come all the way down take the thing and go back up. This is your disk, you have a huge space (workshops on the ground can be as big as you want) but you have to come all the way and go back. So it is very slow compared to RAM but can hold much more stuff.

And lastly say your company had a sudden emergency and now you are are legally required to wear a special suit to protect you (maybe they heard about asbestos in the building you were cleaning or something). But here is the thing, since this is something sudden even your company was not prepared for it, so you must drive to your company HQ get the suit from there and come back. This is hugely slow but also massively big, like once you have to freedom to go anywhere the whole world can be your storage. This is the internet and you getting the suit is just downloading.",ht57qbu,t3_s6mxkp,1642485699.0,False
s6mxkp,Stuff you access a lot or that you accessed most recently gets a spot in the door shelves for quick and easy access. Infrequently used data gets packed into the back of the fridge or in the veggie bins.,ht4ro87,t3_s6mxkp,1642477493.0,False
s6mxkp,"Shipping and transport is a more fitting analogy. Cargo ships are slow and cumbersome, but they can move huge volumes of goods across the world. Upon arrival though, goods from one ship needs to be distributed across many cities. It'd be hugely disadvantageous to use ships for distribution from city to city because of how big and slow they are.

Therefore, that task would fall to the next in the shipping volume hierarchy: 18 wheeler trucks. They may have 1/10,000th the volume of a ship, but they're much more nimble. Highways are accessible to them, allowing them to go between cities.

Once the goods arrive at a warehouse or a shipping facility at some city, even smaller trucks or vans are required for delivering goods to individuals because imagine waiting for an 18 wheeler to navigate through your neighborhood.

At each level in the hierarchy, you're sacrificing volume to be more nimble and accessible, which is analogous to bandwidth vs latency. L1 cache has the lowest bandwidth but also the lowest latency. Which means it's capable of moving small pieces of data to registers, and doesn't make the CPU wait very long (a few clock cycles). Whereas the L3 has the highest bandwidth and the highest latency, probably dozens of clock cycles. It's good at loading in big chunks of data from the RAM, and parts of it to L2 but that operation takes say dozens of clock cycles.

The point of having this hierarchy at all is that RAM is actually very slow compared to CPUs. CPUs have hit GHz a bit over 2 decades ago, which means that some operations can be completed in 1ns or less. However, RAM requires many nanoseconds to be read from and written to even today, and if the CPU waited for it all the time then it'd be irrelevant to have faster clock speeds. The cache hierarchy is an attempt to bypass this limitation of RAM. Since L3 is bigger than L2 and L2 is bigger than L1, you can think of L3 as basically a slice of the contents in RAM, L2 as a slice of L3, and L1 as a slice of L2. This arrangement basically arms the CPU with data that a running program might need in the near future so that you don't incur the penalty of waiting on the RAM for every little memory access, only every now and then. This only works because RAM is read in parallel and that it has the bandwidth to feed the L3 cache, but not the latency to feed the CPU's registers.

This arrangement also causes things like data accesses in arrays to be faster than structures like linked lists because in an array, data elements are directly adjacent to one another, which provides greater probabilities that the subsequent elements are already loaded in the cache. Whereas with linked lists, this node might be in the cache, but the other nodes may be randomly scattered throughout memory, and each node access would then require waiting on the RAM",ht67cp1,t3_s6mxkp,1642511324.0,False
s6msij,"sci hub. combination of google scholar and sci hub is all you need tbh

&#x200B;

edit :  [sci hub](https://sci-hub.mksa.top/)",ht4mdzt,t3_s6msij,1642475215.0,False
s6msij,https://scholar.google.com,ht4mi0o,t3_s6msij,1642475263.0,False
s6msij,"[arxiv.org](https://arxiv.org) just in case, but sci hub is the holiest",ht620h7,t3_s6msij,1642508084.0,False
s6msij,"I normally try with Google Scholar first, then one or all of the following: [z-lib](https://z-lib.org/) for books and some articles, [sci-hub](https://sci-hub.se/), [libgen](https://libgen.fun/scimag/index.html) when sci-hub fails.",ht6zenr,t3_s6msij,1642523376.0,False
s694uk,">     I've read that bayesian neural networks can mitigate this problem

Do you base it on some paper? My understanding of BNNs is that they are used for some kind of regularizations to prevent overfitting. If I understood your problem correctly then this problem while close its still not the same as your problem and ill explain.

In most methods, we assume our data came from some probability distribution and we wish to ""say smart things"" about said distribution from only those samples.

Overfitting essentially means the algorithm does not understand the overall distribution from the samples, and think they came from a much more specified distribution. This problem is often very hard to deal with and some sophisticated methods are used. 

But on the other hand, the ability to understand *different* distributions through the data is something else. For example, if i teach my network to classify animals but gives it only dogs, what it does when shown cat is unpredictable - it might classify as animal because it look somewhat similar to the dog (the distribution we sampled the data from) or it might think its not from the distribution and classify as not an animal. 

Those two problems while similar are different, thats from my understanding of things.",ht2w0ml,t3_s694uk,1642449716.0,False
s694uk,"Thanks for the response, hadn't head about overfitting before and while its not what I'm looking at it certainly has similarities. 

The problem I'm looking at is creation of false positives due to the system not understanding what it is seeing. Going back to the reading that suggested that BBNs could assist with that (The Alignment Problem by Brain Christian), the idea was that an ensemble of modals using Bayesian uncertainty would likely disagree with each other when faced with anything far from the data with which they were trained. 

We can use this degree of consensus or lack of consensus to indicate something about how comfortable we should feel accepting the models guess. We can represent uncertainty, in other words, as *dissent*.

This would be particular useful if a false positive could have dire consequences or in a situation were the available training data was limited or couldn't possibly take into account the complexity of the situation where the AI would be operating. 

Thanks again for the response!",ht41jc4,t1_ht2w0ml,1642466483.0,True
s694uk,"Using an *ensamble* of bnns is actually an interesting thought, i havent considered it.

In general without actually using things its very hard to accuratly guess if something would work in deep learning algorithms, so I wouldnt put all of my eggs on it.

Your problem essentially means that using one NN would lead it to have a distribution over the results that has very low varience but could be wrong. What Brain is saying (if i inderstood correctly) is that the distribution itself changes greatly between each training proceedure and while each module seperatly remains with low varience the addition of all modules will get high varience thus indicate that the net does not understand the input.

Thats to my understanding as i havent been working with bnns much.",ht5f01k,t1_ht41jc4,1642490430.0,False
s694uk,Yeah that's basically the case I believe.  I appreciate the chance to bounce ideas back and forth. Helped it make sense in my head,ht7hgw9,t1_ht5f01k,1642530024.0,True
s67zoq,"Without the ""optimization"" the code would not be correct.

To prove lets look at the simple case of n=2 and p=0.5

There is now two rolls of the dice, one for index(1,2) and the other for (2,1)

If one of the rolls win (with the probability of 0.5) there would be an edge between the vertecies.

The probability of at least one wins is one less the probability of both lose which is (because the rolls are independent) the multiplication between each p, which ends up in 0.75 for an edge unlike the promised 0.5.

You can make the code clean in the ""optimized method"" or to allow the roll to make true values in the array false, but this would implicitally only consider half the rolls in double the runtime.",ht20ppf,t3_s67zoq,1642437870.0,False
s67zoq,"Oh, right. I missed the part where I need to set a true to false if the second roll is false. Thank you so much.",ht2jd6n,t1_ht20ppf,1642444938.0,True
s67zoq,"I think the algorithm would be correct if it also explicitly the edge to false when curr_p > p. In that case the existence of an edge between each pair of vertices would still be set twice but only the latter time would matter.

Without that, the second time those same row and column indices are hit can only *increase* the probability of there being an edge, so it's not the same as evaluating the probability only once for each row and column index pair.

So, basically what u/MyCreativeAltName said.",ht2hrik,t3_s67zoq,1642444331.0,False
s67zoq,Thank you so much. This makes so much sense.,ht2ji6p,t1_ht2hrik,1642444990.0,True
s67l1c,"Nice explanation!

>Perhaps  you could comment on the ones that I missed but you know about.

SVD can be used to compress the wavefunction |Ψ⟩ of a quantum mechanical system expressed in the [Matrix Product State](https://en.wikipedia.org/wiki/Matrix_product_state) formalism (or tensor networks, more generally). Loosely speaking, SVD is used to [split](https://en.wikipedia.org/wiki/Schmidt_decomposition) a system into two parts, say U and V, which gives |Ψ⟩ = |U⟩S|V\^T⟩. Here the singular values in S encode the quantum entanglement between U and V.

The compression is done by discarding small (< 1e-10, say) singular values in S (together with corresponding columns/rows in |U⟩ and |V\^T⟩). This effectively removes unimportant entanglement information from |Ψ⟩. [Repeating the procedure](https://arxiv.org/abs/1008.3477) on all possible partitions of the system, creates a low-rank approximation of |Ψ⟩ which can be accurate under a few (but important) circumstances.

Since the data required to describe a quantum systems of N particles would otherwise scale exponentially (e.g. 2\^N for spin-1/2), this SVD procedure is key to pushing the limit when simulating large quantum systems on classical computers.",ht27k3z,t3_s67l1c,1642440487.0,False
s67l1c,"**[Schmidt decomposition](https://en.wikipedia.org/wiki/Schmidt_decomposition)** 
 
 >In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces. It has numerous applications in quantum information theory, for example in entanglement characterization and in state purification, and plasticity.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",ht27mob,t1_ht27k3z,1642440514.0,False
s67l1c,Good bot !,ht2zxrs,t1_ht27mob,1642451241.0,True
s67l1c,Works well with image compression too.,ht2o2eu,t1_ht27k3z,1642446716.0,False
s67l1c,"Yes, exactly. 💯",ht2zvz6,t1_ht2o2eu,1642451221.0,True
s67l1c,Thanks for sharing! The compression part you mention is similar to what one would do to pick principal components that corresponds to the largest singular values when performing PCA for data analysis/visualization of higher dimensional data.,ht2lwrr,t1_ht27k3z,1642445898.0,True
s67l1c,I had an exam today and was looking for SVD for 1h . What were the odds that you would publish this 2 and a half hour after it ? Lol,ht39gek,t3_s67l1c,1642454928.0,False
s67l1c,😮👀 oh damn. If only I knew.,ht3bysv,t1_ht39gek,1642455899.0,True
s5vvf5,When are you going to start?,ht0c5br,t3_s5vvf5,1642401213.0,False
s5vvf5,An email will be sent within 4 days. Thank you for the note.,ht0c9nu,t1_ht0c5br,1642401293.0,True
s5vvf5,"What will be the medium for communications? Discord group? Slack? Two weeks of the course is equivalent to how many lectures?

Personally, I will be self-studying Design and Analysis of Algorithms on the first semester of the year. My goal is to be able to tackle advanced topics, in particular integer linear programming, by the second semester. However, I didn't plan to follow this MIT course because, as far as I'm aware, it's a second course in algorithms that requires some background I still lack. I just selected some lectures that I found useful. Are you aware that there is a previous algorithms course (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/) to the one you pointed?

Anyways, I will be studying the CLRS book, which is used in the course, but I will start with earlier chapters that are not covered (in fact, they are assumed as known by the student...). Hence, I'm not sure if I'd be able to keep pace with you. Depending on the dynamics of the community, it'd be a pleasure to be part of your endeavor.",ht6l1no,t3_s5vvf5,1642517764.0,False
s5vvf5,"> What will be the medium for communications

Discord and maybe Zoom.

> Two weeks of the course is equivalent to how many lectures?

You can see the course's schedule through its link on my post.

> Are you aware that there is a previous algorithms course

Yes I am. Thank you for the note.

> it's a second course in algorithms that requires some background I still lack

You might watch the lectures even if your comprehension isn't 100%, Then pick-up an easier reference and exercises related to the lecture you didn't fully comprehend.

The main requirement is flexibility and capability to interact and learn from others.",ht72ncc,t1_ht6l1no,1642524596.0,True
s5vvf5,"**NOTE** As seen [here](https://imgur.com/a/hHMmfjc), Someone sent a wrong email address. If it is you, Please submit another form.",htfui99,t3_s5vvf5,1642671774.0,True
s5vvf5,signed up - hope I'm not too late!,hv1ayof,t3_s5vvf5,1643658894.0,False
s5vvf5,No you are not,hv3971h,t1_hv1ayof,1643688300.0,True
s5tfpw,"What's your question exactly? Are you saying you want to play a human to be able to ""play"" the Neural Net instead of the actual game?",ht01pww,t3_s5tfpw,1642395181.0,False
s5tfpw,"I'm wondering if anyone's trained a neural network to simulate/predict the output that a regular video game would give, so you only need to run the NN instead of the full game. I think you understand correctly. The net would be outputting its best guess of what the full game would have returned.",ht037jq,t1_ht01pww,1642395987.0,True
s5tfpw,So you essentially want an ai that can…. Play games? It would be interesting in theory I suppose but training an ai on so many different games that it could theoretically predict the outcome of a new game would be a very complex task I feel,ht07h5m,t1_ht037jq,1642398373.0,False
s5tfpw,"I think OP meant that the neural network would act in stead of the game, and would basically attempt to react to the player's inputs as a game would, without actually going through the actual exact algorithms of the game. So, rather than replacing the human player with an AI in the game/player combination, you'd replace the game by having some kind of a neural network that produces a stochastically generated game-like result and behaviour, e.g. by learning that games tend to pan a view to the left when the player presses the left turn key.

Or at least that's what I got from the comment you replied to.

So maybe something like https://www.youtube.com/watch?v=atcKO15YVD8 but with full game behaviour, including some kind of graphics generation etc.?",ht2x92f,t1_ht07h5m,1642450196.0,False
s5tfpw,"So I think you might have a slight misunderstanding of how Neural Networks function. Well to begin with, the type of network you'd use to build a reinforcement learning model (which you'd train to learn something like chess) is vastly different from one which would generate pixel data (which in your scenario could be a model which predicts the next frame of a game based on input)

For the first type, there's almost never any visual data being passed to the network. The input is in the form of an array (data type could vary but let's just assume this for now) which summarises the current state of the game; and the output could be something like a simple integer which would dictate the model's next move. There's then a reward function which evaluates how good or bad the output move is, which can then be used to tweak the model's parameters for the next iteration of the model's training. This sort of net could definitely be played against in a video game, but not really in the manner you're describing. Think of it as being able to train an AI adversary within the game, rather than creating the entire game itself. The entire game engine, visual assets etc. would still be required to interpret the model's output and put them in the game. This sort of thing has already been done with chess and a few other games. Now one additional thing since you've mentioned Doom Eternal; the more complex the mechanics of the game, the harder it's going to be to setup a good enough model to play it. Afaik we're definitely not at a point where we can create a model to be a player of a game like Doom Eternal.

Now for the second type of model, one where you'd predict the next frame of a game based on a starting frame and any input. This sort of model is theoretically possible today, but it's going to be almost impossible to meaningfully train it to function as a game. This is more in the realm of procedurally generating frames. You wouldn't be able to program any rules/mechanics/levels of any sort into the game to begin with, the network will merely come up with what it thinks should come next based on what it's seen before during training. There's also the problem that this sort of process might work for a few frames but will soon fall into chaos; this is because after a few frames, the original output frames will then need to serve as input, and the general entropy from the unpredictability of the output will be magnified exponentially. Also as a bit of a footnote, a game like Doom Eternal is not going to be feasible for this sort of approach; all of the problems I've highlighted will be exacerbated proportional to the complexity of a single frame of the game.",ht07s8r,t1_ht037jq,1642398553.0,False
s5tfpw,"That... would be kind of scary. Imagine you just train a NN on the top video games, somehow pass in levers to randomize level design, graphics, and gameplay according to what's available in different games, then you get unlimited NN-powered video game experiences, where you only need to polish and do voice-over/direction. I'm really curious now, is this viable?",ht0539y,t3_s5tfpw,1642397012.0,False
s5tfpw,I think TTS tech is looking really promising. If it keeps improving at the current pace we might not have much need for voice actors in 10-ish years.,ht05x5t,t1_ht0539y,1642397489.0,True
s5swwz,"I think by this logic you could say all computer development is physics, because it either pertains to the materials the computer is made out of, or the electricity running through those materials. That's not a _wrong_ classification, but it's probably not the most helpful, either.

We usually distinguish software and hardware _because_ of the practical differences in skill sets. Most software engineers don't have a deep understanding of computer architecture and microprocessor design. Maybe they've had no exposure past one class in undergrad with a few diagrams about the ""five stage pipeline"" and writing a bit of assembly. Similarly, most computer systems engineers don't have the same depth of software understanding as a software engineer or computer scientist, and haven't taken classes on operating systems, compiler/interpreter design, computer networking, and so on.

To return to your brain and thoughts example, neuroscience, psychology, and therapy all overlap in that they deal with your brain and thoughts, which are certainly interconnected. Nevertheless, those three fields are distinct and have a lot of divergent expertise.",hszwq0d,t3_s5swwz,1642392641.0,False
s5swwz,That was by far the best explanation I saw. Thank you so much for clarifying this question,hszy9ot,t1_hszwq0d,1642393391.0,True
s5swwz,"The false assumption lies in this statement: ""All the software information is stored in the hardware, which means it is a physical thing"".

While running a program, it is stored in hardware. But software can also be sent over a network, written down on a piece of paper, or exist only in the head of the programmer. That's why it is soft.",ht0fvw4,t3_s5swwz,1642403696.0,False
s5swwz,the software information is stored as a physical thing but specifically it's as a electrical charge. the configuration of that charge is easily changeable the physical hardware is not.,hup9zj4,t3_s5swwz,1643448589.0,False
s5r8t0,Youtube recommendations mainly. MIT Tech Review is pretty great too to keep track.,hszhzqw,t3_s5r8t0,1642386130.0,False
s5r8t0,"Domo Origato, yeah youtube is great",hszjax1,t1_hszhzqw,1642386701.0,True
s5r8t0,"Hacker News, KDnuggets, IEEE Spectrum, Communications of the ACM, Quanta",hszt0jk,t3_s5r8t0,1642390908.0,False
s5r8t0,"ACM's digital library: https://dl.acm.org/

IEEE's digital library: https://ieeexplore.ieee.org/Xplore/guesthome.jsp",ht08fh0,t3_s5r8t0,1642398925.0,False
s5r8t0,"Hey there!  
This is the  weekly tech news teller me and my friend created you can see the main sources of information for this newsteller that are used on the main page listed. ( these webpages themselves are very informational for news and etc.)  
[https://techteller.org/](https://techteller.org/)  


Hope it will help.  
Thanks.",ht0bihr,t3_s5r8t0,1642400803.0,False
s5i47n,"It’s not a stupid question. If you had a component like a transistor but which could output 10 distinct voltage levels — let’s call it a TENsistor — and that output would in turn serve as an input to another tensistor, etc.. you could represent 10^8 states with 8 signal traces.

It wouldn’t be 8 “bits”, but maybe we could call them 8 “dits”. An 8 dit system would then be able to process a 10^8 value (100M) in a single instruction (I’m oversimplifying here but the principle holds). A binary computer would need a 27-bit architecture to achieve the same.

(Again, really oversimplifying here)

So if you had tensistors and were able to lay them down on silicon as densely as in a modern CPU; and if your CPU had the same number of tensistors as the binary CPU had transistors, then yes. It would be much faster.

Of course, you could absolutely design a ‘tensistor’ out of binary logic gates, but it would end up being orders of magnitude larger and more complex than the damn clever NAND based logic in a modern CPU, and so your new architecture would end up in practice being way slower that way.

And going the analog route, where you actually distinguish different voltage levels inside every component — I think we’d be looking at another couple orders of magnitude in size and complexity to do that.",hsxt11m,t3_s5i47n,1642361852.0,False
s5i47n,"I would like to add that the speedup using dits instead of bit is ""just"" a constant factor. The possible representable states grow faster with the exponent than with the base (for large enought numbers). Meaning it is more important how many dits/bits you have than how many states a single of them can hold.

As long as you can build the tensistors just n times larger you would speed things up. If you instead need something like n^2 or even 2^n times the space, tensistors would be worse than transistors (regarding computing capacity per space).

So it is just more efficient to simple increase the amount of bits than to mess around with dits.

PS: I like the word dits 😀",hsyeh0b,t1_hsxt11m,1642370034.0,False
s5i47n,"Yep, good point!",hsyes8a,t1_hsyeh0b,1642370154.0,False
s5i47n,"Take a look at [settling time](https://en.wikipedia.org/wiki/Settling_time). When a wire changes voltage levels it doesn't immediately change to the new voltage; it overshoots and wiggles a bit before it settles down to the intended value. In a binary system there is a lot of room for error, since the wire only needs to be above some voltage or below some voltage. When the wire changes values, it will be high or low pretty quickly, and you don't need to wait long for it to settle into a value that's close enough.

If there are 10 possible values, the voltage will need to be much more precise, so the system will need to wait longer for it to settle on the precise level. This will make all the electronics much slower. I'm a software guy, so I can't say precisely, but an electrical engineer could probably quantify exactly how much slower things would be.",hszi7yc,t3_s5i47n,1642386225.0,False
s5i47n,"Other posters have answered the question well. Just for fun I'll mention that storage (as opposed to logic) actually does use higher number of states. Many SSDs use 2 or 3 bits per cell (4 or 8 distinct charges). It's really tricky to get right and slower, but it's much more space efficient.",ht05206,t3_s5i47n,1642396993.0,False
s5i47n,"It's hard to represent 10 different states. These states could sort of blur together, and distinction between them would be a lot harder. 1/0 (on/off) makes it so that there is no ambiguity- it is easy for a computer to tell the difference.",hsy6nwz,t3_s5i47n,1642366982.0,False
s5i47n,"It's not that it's impossible. It is very possible.

In fact, the first programmable, electronic, general-purpose digital computer - famous [ENIAC](https://en.wikipedia.org/wiki/ENIAC) - was operating on decimal numbers!

But we don't see such machines anymore. Think, why?",hsxsodo,t3_s5i47n,1642361722.0,False
s5i47n,We have also made [ternary computers.](https://en.wikipedia.org/wiki/Ternary_computer),ht062in,t1_hsxsodo,1642397574.0,False
s5i47n,Is that because of potential signal inaccuracy? I genuinely don't know why.,ht0iom8,t1_hsxsodo,1642405656.0,False
s5i47n,I don’t get your point. Why do you think it would be faster?,hsxkc7q,t3_s5i47n,1642358627.0,False
s5i47n,"No, I would expect about 3 times as fast, since with 3 bits you already have 2^3 = 8 possible states, which would be as good as having a base 8 computer. 
Also, during a calculation you are not interested in the base 10 representation. So it's not that useful.
Additionally, computers also do computations on for example pointers, so not all computations are on (base 10) numbers.
Short answer: no, I don't think it will be a lot faster (if it's even faster at all)",ht06llw,t3_s5i47n,1642397874.0,False
s57kne,[removed],hsy9ubx,t3_s57kne,1642368224.0,False
s57kne,[removed],hsym6ff,t1_hsy9ubx,1642373065.0,False
s57kne,"Thanks for posting to /r/computerscience! Unfortunately, your submission has been removed for the following reason(s):

* **Rule 2:** Please keep posts and comments civil.



If you feel like your post was removed in error, please [message the moderators](https://reddit.com/message/compose?to=/r/computerscience).",hsza1zr,t1_hsy9ubx,1642382717.0,False
s57kne,"This is so cool, man",hsxnqgk,t3_s57kne,1642359880.0,False
s4k68d,"Here are two college textbooks that I highly recommend. They're college textbooks, so it means they're very expensive, but it also means there's lot of reasonably priced used copies out there.

Introduction to Algorithms, i.e. ""the CLRS big book of algorithms""  
[https://mitpress.mit.edu/books/introduction-algorithms-third-edition](https://mitpress.mit.edu/books/introduction-algorithms-third-edition)

Artificial Intelligence: A Modern Approach, i.e. ""AIMA"" or ""Russell and Norvig""  
[http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)  


Books about programming language go out of date quickly. These books focus on the procedures themselves (independent of any particular language), so the material stays relevant for a long time. Some of the material in these books is 70+ years old and still studied because they remain relevant today, even though computers and programming languages have changes dramatically since then.",hssfla4,t3_s4k68d,1642268301.0,False
s4k68d,"It’s not a book but I wish someone had encouraged me to read research papers and technical documents sooner.

RFCs are often networking focused but a lot of what we do in CS has direct applications there and can be abstracted/generalized with graph theory. There are algorithms, data structures, topological suggestions, and even some coding best practices.

It’s great to go to those documents because I think people come into computer science with a lot of preconceived notions about how logic and computer systems work. It helps me to see what the actual thought leaders saw as the pressing industry issues, and helped me to see the standard for state of the art solutions.

Stuff like that fleshes out the fundamental understandings your books will teach you - while giving you a sense of the breadth of the field.",hss4syd,t3_s4k68d,1642264103.0,False
s4k68d,What is a RFC?,hst5q0v,t1_hss4syd,1642278464.0,False
s4k68d,"An RFC is a “Request For Comment”, which is like a formal statement of a proposed system, protocol, algorithm or otherwise solution to a technical problem. 

It is intended to be an opportunity for a community of engineers to gather around a proposed solution and improve it, or otherwise reference the agreed standards of a given solution.

The most famous set of RFCs are by the IETF (Internet Engineering Task Force): https://www.ietf.org/standards/

Read through some - start with something you recognize. Gateways are a good starting point in my opinion, or RFC1918. I think you’ll find them accessible given a little familiarity.

The really exciting thing about computer science for me is this quote from Steve Jobs:

“Everything was made up by people that were no smarter than you”",hst6mka,t1_hst5q0v,1642278827.0,False
s4k68d,I enjoy reading RFCs a LOT! I usually read them and then try to implement the technique discussed in the RFCs. I usually read networking RFCs,hsvgp91,t1_hst6mka,1642316353.0,False
s4k68d,"I'm in a different field and I agree with the research papers suggestion. 

One surefire way to do it is to look at textbook authors' and contributors' journal articles. Search Google Scholar for their names and download some that are free. This way you don't easily stumble accidentally on journal articles that have bad writing/wrong methods. Same could work for referenced articles on Wikipedia pages: search for more work by the same author(s).

Once you have some downloaded, read a few paragraphs from each and see how much you understand. If it's 60% or more, awesome, read that one. If it's less, discard and grab another. You could try to look up terms but usually there are whole complex ideas and esoteric methods behind a bunch of the terms. Also the writing could just be an overly esoteric style. Find an article first that conveys meanings well to you before you get frustrated trying to look terms up.",hsvxybf,t1_hss4syd,1642329617.0,False
s4k68d,I think it is a good idea to continue reading SICP. If you like math you can try reading an intro discrete math book.,hsslabi,t3_s4k68d,1642270500.0,False
s4k68d,"There are a buncha people who say SICP as a whole is outdated. I have an easy time understanding and disagreeing with that perspective. However, I have also seen some claim that SICP chapters 1, 2, and 3 are liquid gold, while chapters 4 and 5 are lacking comparatively. How do chapters 4 and 5 compare to the earlier chapters?",hswscx2,t1_hsslabi,1642347684.0,True
s4k68d,https://www.reddit.com/r/computerscience/comments/s14xir/comment/hs6fo0x/?utm_source=share&utm_medium=web2x&context=3,hss4zf0,t3_s4k68d,1642264174.0,False
s4k68d,"Go through all the books/courses in this website: [https://teachyourselfcs.com/](https://teachyourselfcs.com/)

I went through most of them during uni, and can attest that they are all fantastic textbooks.",hstp77k,t3_s4k68d,1642286442.0,False
s4k68d,"Hunt, Thomas. The Pragmatic Programmer

Robert Martin. Clean Code

Robert Martin. Clean Architecture

Martin Fowler. Refactoring

These will get you past the stage of hacking together something that works, to the point where you at least know what well-designed code is supposed to look like.

After that, you're ready for

Eric Evans. Domain Driven Design

Kent Beck. Test Driven Design

Gang of Four. Design Patterns

Joshua Kerievsky. Refactoring to Patterns",hssf8h6,t3_s4k68d,1642268166.0,False
s4k68d,"I should point out that literally *none* of these books are computer science books. They are **software development/engineering** books. Very important field, and where I spend most of my time even though my job title has compsci in it, but it's an important distinction for the newbie to recognise that these are not compsci.
 
Computer science books would be things like
 
* any 1 of several algorithms & data structures books  
* The Art of Computer Programming  
* Types and Programming Languages  
* Compilers: Principles, Techniques, and Tools  
* Introduction to Automata Theory, Languages, And Computation  
* Computational Complexity: A Modern Approach
 
However the 2 most important OP already has, K&R + SICP.",hstlib0,t1_hssf8h6,1642284927.0,False
s4k68d,"Meh... I know this perspective and I think it's from the way how courses are labeled. They put SE on one side and everything else on the other. A bit like programming and mathematics in the 60s and 70s. This way of separation rly hurts my brain...

I can see connections between Uncle Bob's SOLID principles and Dijkstras thoughts in the humble programmer. I see the connection between engineering methods and HCI and to design and to psychology to constructivism to hylomorphism to Alexanders adaptive morphogenesis to digitalisation of analogue signals to approximation in mathematics. In what bucked shall I put these thoughts? SE or CS?

I rly prefer the perspective: Software engineering and software development are specialised fields in Computer Science that exist alongside other fields like algorithm or machine learning. Everything is connected and further more it's connected to domains that are clearly out of the field of computer science.",hsu4iyh,t1_hstlib0,1642292816.0,False
s4k68d,"It's not a perspective, it's a reality. Software developers/engineers use computer science, but that doesn't mean that they aren't different things. Connections between things don't mean there is no difference between things - there is a connection between my finger and my wrist, but they are distinct. Software development/engineering is *not* a specialised field in computer science, it is an interdisciplinary field that includes (some of) computer science, and also includes a bunch of other things. This is objective fact, it's not something multiple people can have different correct opinions on.",hsu59s8,t1_hsu4iyh,1642293133.0,False
s4k68d,"Yes sure connections don't mean there are no differences. But that's not the point: You think of wrists and fingers. I think of limbs, wrist and fingers.

And an objective fact, the reality in relation to a human concept... Something like this doesn't exist. That's why I speak of perspective and don't tell anyone what is right or wrong. Every single human concept is just a conceptual model based on our personal mental model. If I ask you to draw me past and future down on a paper you may draw a line from left to right. Someone else in the world will draw a line from right to left and another from top to bottom. Which conceptual model is right?",hsu9zcz,t1_hsu59s8,1642295113.0,False
s4k68d,"If you throw out the meanings of any words, you lose the ability to communicate. You can either speak English with the rest of us where the words computer science and software development mean computer science and software development, or you can speak whatever language you are speaking where they aren't different things - but you can't expect to be able to communicate with anyone if you run off and use your own definitions.",hsubyjz,t1_hsu9zcz,1642295964.0,False
s4k68d,"Exactly! 😊 It's about the meaning! Very good.

""The spoken words are the signs of ideas in the soul and the written words are the signs of spoken words. Just as the written signs are not the same for all people, so the words are not the same for all people."" -Aristoteles, Peri hermeneias

Semiotik? Model theory? Stachowiak? 
Words are only sounds, without a human interpretation they are indeed meaningless. But here, in a discussion about what is computer science, we have to think in different levels of abstraction, a different granularity when looking at the details, the relation and the dynamics with which a concept is connected. And in a context like this, the subject of computer science, I think that one can expect more than a linear simplified form of classification to answer this question.

Does anyone object when someone asks software engineering questions here? Why are you working in a professional title but with a software engineering job? It is not simply black and white, even if that is easier and more convenient.

It is very natural and human to create taxonomies and this is the basis for what is ""true"" for us. In the end, however, it remains only a simplification of reality, a way for our human mind to grasp the potentially infinite complexity of reality, which always remains an incomplete representation. And because we see this image as true, things that say otherwise are false, an attack on our world order that makes us quite aggressive because we want to stand up for what we think is right. This leads us to exclude those who think differently, for example by talking about you and us and automatically implying that you are not part of us. Someone who says he has a scientific background knows that this kind of exclusion has troubled so many of the people we honour today, throughout their lives, and only because people insisted that their concepts were not the one of ""us"".

In the end, however, it remains what it is, a simplification, not a truth. And that's why I prefer the term perspective. And I am very glad that we are having this discussion at this level ""where"" I am from.",hswk4ou,t1_hsubyjz,1642344015.0,False
s4k68d,">Why are you working in a professional title but with a software engineering job
 
Because people who aren't in the computer science or software development fields don't understand the distinction, yet write the job titles for those fields. Same reason why programmers get asked to fix hardware.
 
This is not a philosophical question, it is a question of what the community of practice has defined as different things over decades of communication. Again, you can choose to run off and use your own definitions, but you self-exclude yourself from that community and generally make yourself a nuisance when you impose your incorrect definitions upon that community.",hswkzp4,t1_hswk4ou,1642344425.0,False
s4k68d,"> Because people who aren't in the computer science or software development fields don't understand the distinction, yet write the job titles for those fields. Same reason why programmers get asked to fix hardware.

So we can agree that in general, in the non-professional environment, these issues are not differentiated. We are ""computer people"". But the perspective that I follow and that we also try to build our curriculum around and make first-year students understand what their computer science degree consists of is a variety of disciplines in the field of computer science. Even if we exclude SE from this consideration. With a computer science degree, will what you do simply be computer science? No, it is the specialisations that we have chosen. Visualistic, machine learning, digital signal processing, etc. are all directions in computer science that are so broad in themselves that we consider them as a separate field. To unite all this under the term computer science is just as inappropriate as asking a programmer to repair hardware. 

This is not only the case with CS, but also with SE, CE and all other categories that were defined in the 90s. This classification is the result of a knowledge-based education where I dont say its right or wrong. It treats computing as a meta-discipline a collection of disciplines having a central focus of computing. But even this classification now recognises that these disciplines are intertwined. In particular, SE and CS have a massive overlap in the theoretical part of ""application technology"", ""software development"" and ""systems infrastructure"". The ""community"" has been classifying CS for 10-15 years in such a way that almost all aspects of CS are relevant to SE. And rightly you will say that someone who has spent the same time focusing on more theoretical content will probably be more competent in this field than someone who has spent time in both theoretical and application.

And that is also the reason why I do not like this classification. It is simpler but loses important details that are relevant for example in a job profile. I prefer a classification based on competences. Here, knowledge, skills and disposition are considered in the context of computer science without a separation into disciplines. And we have already defined in 2017 which elements of disposition, cognitive skills and computing knowledge are useable for this classification. The resulting profiles are much more complex but much better suited to describe people or activities.",hsx2q7g,t1_hswkzp4,1642351895.0,False
s4k68d,">we can agree that in general, in the non-professional environment, these issues are not differentiated
 
Someone who isn't a chemist can't easily differentiate between a bottle full of H2O and a bottle full of H2O2. You going to drink both?
 
You are teaching your students incorrectly. A history major learns much more than just history. That doesn't make those other things 'history' just because they are taught to a student in a history major.
 
You can prefer whatever you like, but you are incorrect, and you are just going to confuse the people you speak with until they realise you are speaking a pretend language.",hsx4djw,t1_hsx2q7g,1642352548.0,False
s4k68d,Well I didn't talked about what they learn and what not and that's the point. You lack the ability to listen and even comprehend what I am talking about. So: You are right I am wrong and I'll take these perspectives to a place where they can't hurt you.,hsxcas1,t1_hsx4djw,1642355577.0,False
s4k68d,"As a collector of CS books, thank you, this just added a few to my to-buy list",hst02yz,t1_hssf8h6,1642276214.0,False
s4k68d,Code by Petzold. It was required reading where I went to college but I regret not reading it long before that. It's not a long book at all but should absolutely be required reading everywhere.,hstqyxf,t3_s4k68d,1642287165.0,False
s4k68d,"1. The Nature of Computation by Cristopher Moore and Stephan Mertens
2. Concrete Mathematics by Don Knuth et al
3. Algorithms a Creative Approach by Udi Manber

But please finish SICP first. Makes everything look a lot easier and more interesting.",hsuzxom,t3_s4k68d,1642306524.0,False
s4k68d,"https://doc.lagout.org/Others/Data%20Structures/Data%20Structures%20and%20Network%20Algorithms%20%5BTarjan%201987-01-01%5D.pdf

This was used as the text for a graph algorithm class I took in graduate school I think it feels like a really intuitive and interesting way to describe the subject. Personally, I really like the way Tarjin writes and introduces algorithms. This doesn’t teach you C or Java, but something much more fundamental. Give it a look if you want a bit of a challenge!

Tarjin for me is the Knuth of graph algorithms.",hsvdm6k,t3_s4k68d,1642314291.0,False
s4k68d,Cracking the Coding Interview. Recommended to me by my professor and really helps you prepare for interviews and real world problems!,hssilmj,t3_s4k68d,1642269469.0,False
s4k68d,Pragmatic programmer.,hsth7dx,t3_s4k68d,1642283167.0,False
s4k68d,studying UML would be helpful since different types of diagrams can be very helpful in planning and communicating software systems among technical and non-technical people alike,hssx7py,t3_s4k68d,1642275089.0,False
s4k68d,I do not know a single person that used UML more than once a year after getting their degree.,hsudst9,t1_hssx7py,1642296743.0,False
s4k68d,"At certain levels of the planning process diagrams are definitely useful.  UX workflows, multi-system interactions, data flows, etc.  I find they help with business to technical specification as a bit of common ground between analysts and engineers.",hsv43n3,t1_hsudst9,1642308693.0,False
s4k68d,hey ppl do you know Doctor Strange?,hu1xlz8,t1_hsudst9,1643050307.0,False
s4h4a4,Just a guess but perhaps when the authors refer to ‘implementers of C’ they’re talking about people making compilers of the language?,hsqz9o2,t3_s4h4a4,1642241220.0,False
s4h4a4,That would be my guess as well.,hssdcs5,t1_hsqz9o2,1642267435.0,False
s4h4a4,Yeah makes sense,hsu0ix3,t1_hsqz9o2,1642291124.0,True
s4h4a4,"What I think they're saying is:
- implementer of C = someone who creates parts of the C language itself or an extension to it.
- C programmer = someone who uses the C language to develop for example an application.",hsqzjxr,t3_s4h4a4,1642241434.0,False
s4h4a4,"One makes C, other makes with C",hsqzp0a,t3_s4h4a4,1642241545.0,False
s4h4a4,"Implementer most likely to be referred to someone who's creating the C libraries, improving compilers etc 
Programmer is the person using the language/consuming the libraries created by implementer to develop something say a game.",hsr6kap,t3_s4h4a4,1642246688.0,False
s4h4a4,Yeah,hsu0m67,t1_hsr6kap,1642291162.0,True
s4h4a4,"Implementer refers to one who makes the ""idea"" of C language into reality: writes the compiler, runtime library etc.

Programmer is one who writes programs in the C language and depends on the implementation of it (compiler, runtime etc.).

The ""idea"" of a language can be considered to exist as described in standards and definitions before there exists an implementation of it although some people start hacking on an implementation before there is a formal definition of it (syntax etc.)

Implementer also has to consider how the description of language will apply in real world in a real computer architecture.",hsraj1i,t3_s4h4a4,1642249501.0,False
s4h4a4,That’s helpful thanks,hsu0wc8,t1_hsraj1i,1642291278.0,True
s4h4a4,The implementer is the person writing the compiler or the standard library,hsranpy,t3_s4h4a4,1642249590.0,False
s4h4a4,"An implementer of axes is a smith, a “programmer” of axes is a lumberjack.",hss41c5,t3_s4h4a4,1642263796.0,False
s4h4a4,That’s a good analogy,hsu0qd2,t1_hss41c5,1642291211.0,True
s4h4a4,Amogus,hss80tu,t3_s4h4a4,1642265355.0,False
s4h4a4,"C implementer: makes compilers or implements the standard library for C

C programmer: people who program in C",hsspkvj,t3_s4h4a4,1642272158.0,False
s4h4a4,"Implementers are engineers who write the C compiler from the official C spec. There are many versions: c99, c11, etc. A spec is a document that defines the standards of the language. The grammar, the rules, etc. These rules aren’t always strict and some of it is left to the implementers to decide. 

The C programmer consumes the implementers work and writes programs with the help of routines it provides in a shape of header files.",ht111tk,t3_s4h4a4,1642419983.0,False
s4h4a4,Yeah makes sense and that is very helpful. I understand that an engineer can can create and implement versions of C that are tailor-made for the OS or compiler they are writing.,hta4oan,t1_ht111tk,1642569187.0,True
s4frba,"Yes, a vast majority of them",hsrqr59,t3_s4frba,1642258169.0,False
s4frba,"I'm not sure I entirely agree with the assumption that any other well-known machine learning method outside of deep learning isn't ""effective"".

Deep learning is pretty much by definition based on artificial neural networks. So, 1/1 of the base methods used for deep learning are, in some sense, inspired by nature (at least originally).

However, for smaller amounts of data or for data with more linear relationships between variables, things like support vector machines may still be completely reasonable. If the phenomenon being modeled is linear or close to linear, a linear model might even well perform better than a nonlinear one such as an ANN, at least if you don't have a crapton of training data. (edit: and nearly all of these other methods are *not* inspired by nature.)

Also, as far as genetic algorithms go, are they actually one of the ""two most powerful"" machine learning methods? I may be a bit out of the loop, but genetic algorithms used to be one of those methods that sound appealing and may get a lot of hype but which often don't necessarily perform that well, or at least not necessarily better than other simpler and less computationally intense methods do.

I've seen some of that hype for GAs appearing again e.g. on Reddit in recent years but I'm not sure that attention is actually warranted. Again, I may be out of the loop, so perhaps GAs have actually enjoyed some kind of a significant rise in real performance along with neural networks recently. Since I'm not aware of that actually happening, though, I'm inclined to think of it as mostly hype, and I wouldn't count GAs as another data point towards effective machine learning being naturally inspired. I'd be happy to be shown wrong by someone in the actual know.

You might want to ask whether there are other methods beside ANNs that could be used for a layered approach that integrates some kind of feature learning, kind of similarly to deep learning. That's a good question, to which I don't know the answer. I'd just phrase it a bit differently because despite the (warranted) attention and hype, deep learning probably isn't the only kind of effective machine learning.",hsrtfwa,t3_s4frba,1642259383.0,False
s4frba,"Maybe do some research into statistics? As a field they do a LOT of data analysis and predication that’s not based around neural networks or genetics algorithms. As far as I know anyway, I’m not a statistician so anyone actually versed in statistics feel free to correct what I’ve said.",hssy8sf,t3_s4frba,1642275486.0,False
s4frba,"Right now DL looks nothing like whats done in nature.

DL are NN that use more then one hidden layer - if you assume automatically that all NN are ""copied from nature"" then of course all DL falls into this category.

Every single ML algorithm is basecally just math, you cant dismiss it for being this way too, math is an extremely powerfull tool.

On top of my head though, a lot of RL algorithm are not at all infulenced from nature, for example DQN.",hsqwvs9,t3_s4frba,1642239423.0,False
s4frba,All algorithms are memorization and/or math. Machine learning leans towards statistics tho. You can’t compute anything (or have an intelligent system) without math.,hss047h,t3_s4frba,1642262217.0,False
s4frba,"Here's a simple method that gets used all the time, and has no connection to nature: linear regression",hstpdfc,t3_s4frba,1642286514.0,False
s3yrrt,"The Concorde solver uses the cutting-plane method, iteratively solving linear programming relaxations of the TSP. The interface shows the solver's progress at the end of each major iteration of cutting planes by coloring the edges according to their current LP values.

The Graphical User Interface implements in addition to the optimal Concorde solver the following edge generating algorithms:

* Delaunay Triangulation
* Minimum Spanning Tree
* Various Nearest Neighbor Set generators

The Concorde interface can read and write graphs from and to files in various formats. (Note: Input graphs must have greater than 10 nodes.) Users may add, delete, and move a graph's vertices interactively. The interface consists of two display panes. One is used to show the graph, the other to print textual information about the graph, the algorithm's progress and results.",ht3swrg,t3_s3yrrt,1642462780.0,False
s3yrrt,"Thanks, I'll need to dig into the cutting plane method",ht5am8a,t1_ht3swrg,1642487489.0,True
s3yrrt,I found this video which explains that for the TSP. It's very well explained : https://youtu.be/8yajCJKezZQ,hv1mse1,t1_ht5am8a,1643663336.0,True
s3yrrt,[https://youtu.be/tChnXG6ulyE](https://youtu.be/tChnXG6ulyE) this one too,hvcawbv,t1_hv1mse1,1643842299.0,True
s3yrrt,I think the introduction of [this paper](https://dl.acm.org/doi/pdf/10.1145/3071178.3071304) may be a good start,hsoh7zc,t3_s3yrrt,1642194642.0,True
s3mbvt,Computerphile,hsm7y5o,t3_s3mbvt,1642160395.0,False
s3mbvt,Sebastian Lague,hsmpulv,t3_s3mbvt,1642169963.0,False
s3mbvt,Sebastian Lague is a GOD,hsnpvli,t1_hsmpulv,1642184056.0,False
s3mbvt,"absolutely. his explanations are superb, calming voice, and incredible talent and work ethic. Most importantly, he does really cool shit that inspires me",hsoo39j,t1_hsmpulv,1642197312.0,False
s3mbvt,I just followed your recommendation and man… that was amazing. Definitely subscribing to this channel.,hsp23rg,t1_hsmpulv,1642203066.0,False
s3mbvt,Absolute favourite of mine. Glad you enjoyed him too.,hsp44d5,t1_hsp23rg,1642203922.0,False
s3mbvt,Seconding this one. Sebastian Lague is amazing. His projects are visually very interesting and tackle some fascinating challenges.,hsotvol,t1_hsmpulv,1642199640.0,False
s3mbvt,"One of my absolute favorites. His coding adventure videos are all awesome. His voice is so soothing, I often put his videos on at night/in the morning.",hsqpebs,t1_hsmpulv,1642233722.0,False
s3mbvt,Ben Eater all the way,hsmzg23,t3_s3mbvt,1642173907.0,False
s3mbvt,interesting and hardcore,hsnw3v7,t1_hsmzg23,1642186505.0,False
s3mbvt,I second this,hsn3jg1,t1_hsmzg23,1642175530.0,False
s3mbvt,Happy cakes,hsn3lup,t1_hsn3jg1,1642175555.0,False
s3mbvt,That guy is a legend,hsorp2v,t1_hsmzg23,1642198754.0,False
s3mbvt,Computerphile.,hsm7ygp,t3_s3mbvt,1642160400.0,False
s3mbvt,"[BackToBackSWE ](https://youtube.com/c/BackToBackSWE) is a good channel for algorithms

[Reducible ](https://youtube.com/c/Reducible) is another great channel similar to [3Blue1Brown](https://youtube.com/c/3blue1brown) but specifically for Computer Science. It explores CS topics in a much deeper way with excellent visuals and explanations. 3Blue1Brown does have a few videos on [Neural Networks.](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)",hsnfzr1,t3_s3mbvt,1642180318.0,False
s3mbvt,Two minute papers and mCoding make great videos around those topics.,hsm6i08,t3_s3mbvt,1642159378.0,False
s3mbvt,Michael Reeves is def someone who inspired me to take up computer science. But he’s more of a person to watch to show the fun/humorous side of computer coding.,hslxrll,t3_s3mbvt,1642152682.0,False
s3mbvt,Humorous...ha. He puts alot of effort to make his videos funny,hsn2aq1,t1_hslxrll,1642175042.0,False
s3mbvt,"Algorithms: https://youtube.com/c/UndefinedBehavior

ML/AI (more fun to watch): https://youtube.com/c/CodeBullet

Applied Math (i.e. cryptography, neural networks, FFTs, etc): https://youtube.com/c/3blue1brown",hsndewz,t3_s3mbvt,1642179330.0,False
s3mbvt,Network Chuck!,hsn6wkt,t3_s3mbvt,1642176834.0,False
s3mbvt,"Steven Skiena (channel name)

Computer Science (channel name)",hsom0kh,t3_s3mbvt,1642196494.0,False
s3mbvt,3Blue1Brown if you like a mix of math and cs,hsp8imr,t3_s3mbvt,1642205852.0,False
s3mbvt,"[Jacob Sorber](https://www.youtube.com/c/JacobSorber) is superb for any low level programming interests (embedded systems, C, etc) or just general data structure/algorithms.",hsoa37z,t3_s3mbvt,1642191898.0,False
s3mbvt,">Sebastian Lague

Hey, i never previously found or stumbled on this guy in my search for computer science/programming Youtube folks and creators. Checked him out - and what his content to be a real gem. Especially his data structures with C. I was looking to polish / refresh my data structure fundamentals and exactly in C ! What a finding, thanks!",ht5nbq2,t1_hsoa37z,1642496779.0,False
s3mbvt,Jdh,hsnddeu,t3_s3mbvt,1642179314.0,False
s3mbvt,"You might like some of these:

Fireship
Brad Traversy
Tom Scott
Computerphile
Neural Nine
Corey Schafer
Distrotube
Jabrils",hspb92g,t3_s3mbvt,1642207069.0,False
s3mbvt,"Check out Sentdex. He has tons of videos about anything Python and has shiftet towards an ML/AI type of channel lately. He recently released a book where he wrote many neural network algorithms from scratch in Python/numpy to teach the concepts.

I haven't read it or seen the most recent videos because I'm more into computing systems, but it's a super cool channel.

Another cool one is Anthony Sottile's channel. He makes a lot of open source developer tools in Python. He streams and uploads to YouTube afterwards. He maintains stuff like pre-commit, pytest, flake8, tox and some other cool popular projects.",hso3j02,t3_s3mbvt,1642189388.0,False
s3mbvt,TwoMinutePapers,hso5ty3,t3_s3mbvt,1642190258.0,False
s3mbvt,"* [https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1\_aw](https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1_aw)
* [https://www.youtube.com/user/Computerphile](https://www.youtube.com/user/Computerphile)
* https://www.youtube.com/c/Fireship
* [https://www.youtube.com/c/mitocw/playlists](https://www.youtube.com/c/mitocw/playlists) (This one is a lot of different fields. Basically you can just do a search of ""MIT machine leaning"" or ""Stanford machine learning""",hsm8a82,t3_s3mbvt,1642160626.0,False
s3mbvt,"You may be interested in this awesome project too : [https://www.youtube.com/watch?v=rPkMoFJNcLA](https://www.youtube.com/watch?v=rPkMoFJNcLA)

It's not really machine learning but more about how can evolution make individuals better. It's my favorite ML related project",hsnoimy,t3_s3mbvt,1642183543.0,False
s3mbvt,Engineering Man,hsoptbx,t3_s3mbvt,1642197994.0,False
s3mbvt,"Some fun, interesting ones I didn't see mentioned (but these are pretty specific to their applications) are [ThinMatrix](https://www.youtube.com/c/ThinMatrix) and [BPS.space](https://www.youtube.com/channel/UCILl8ozWuxnFYXIe2svjHhg)",hsotyjz,t3_s3mbvt,1642199673.0,False
s3mbvt,"While his focus might be more on the philosophy side of things, Mark Jago is a trained computer scientist, and his videos are mostly about formal logic, with the occasional foray into CS. 

https://m.youtube.com/c/AtticPhilosophy/videos

Plus he’s got a cool aesthetic",hsp45yl,t3_s3mbvt,1642203941.0,False
s3mbvt,"[Abdul Bari](https://youtube.com/channel/UCZCFT11CWBi3MHNlGf019nw). I've watched and rewatched his content many times. I wish I knew about him when I was taking my algos class in college. He was making many of his algorithms videos right around that time, too.",hsp5tsf,t3_s3mbvt,1642204658.0,False
s3mbvt,i heard his content is just basic and not for intermediate/advanced programmers,hsqigft,t1_hsp5tsf,1642228894.0,False
s3mbvt,"Maybe his non-algo related content. I haven't watched any of it. But he has many videos walking through algorithms that I learned about in my more advanced algorithms course in college.

The first couple of algo videos are fundamentals but there are over 30 in his algorithms playlist.",hsqom73,t1_hsqigft,1642233153.0,False
s3mbvt,"3Blue1Brown has some computer sciency stuff, has tons of math as well.",hspdoaa,t3_s3mbvt,1642208156.0,False
s3mbvt,RemindMe! 1 day,hspr097,t3_s3mbvt,1642214260.0,False
s3mbvt,"I will be messaging you in 1 day on [**2022-01-16 02:37:40 UTC**](http://www.wolframalpha.com/input/?i=2022-01-16%2002:37:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/s3mbvt/interesting_computer_science_youtubers/hspr097/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fs3mbvt%2Finteresting_computer_science_youtubers%2Fhspr097%2F%5D%0A%0ARemindMe%21%202022-01-16%2002%3A37%3A40%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20s3mbvt)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hspr20l,t1_hspr097,1642214283.0,False
s3mbvt,“Stat Quest with Josh Starmer” for Data Science and Machine Learning. You will not regret it.,hspvrta,t3_s3mbvt,1642216536.0,False
s3mbvt,Fireship. His videos are relatively short and very info packed.,hspz4ek,t3_s3mbvt,1642218161.0,False
s3mbvt,Tren Black,hsq3ipu,t3_s3mbvt,1642220367.0,False
s3mbvt,Joma tech,hsq6ge4,t3_s3mbvt,1642221927.0,False
s3mbvt,"Jordan Harrod makes interesting videos about machine learning and AI ethics, and about her experience as a PhD student",hsqcari,t3_s3mbvt,1642225101.0,False
s3mbvt,"- Spanning Tree
- Ben Eater
- Computerphile
- 3Blue1Brown
- AlphaPhoenix",hsqewgt,t3_s3mbvt,1642226629.0,False
s3mbvt,Carl Herold?,hsqgtj9,t3_s3mbvt,1642227827.0,False
s3mbvt,Forest Knight,htwn83r,t3_s3mbvt,1642960758.0,False
s3gau6,Cocke and Minsky's construction of a universal Turing machine within a tag system can be found [here.](https://dl.acm.org/doi/10.1145/321203.321206) And  [here is](http://www.complex-systems.com/pdf/15-1-1.pdf) Matthew Cook's proof that a cyclic tag system can emulate Cocke and Minsky's tag system.,hskrj9n,t3_s3gau6,1642128098.0,False
s36y35,"Oh come on, you can't just show something this cool and *not* link a paper or article or at least the name of the tech.",hsjngrf,t3_s36y35,1642111582.0,False
s36y35,"I agree, so I searched “tennis ball tracking software” on Google…

https://en.m.wikipedia.org/wiki/Hawk-Eye#:~:text=Hawk%2DEye%20is%20a%20computer,path%20as%20a%20moving%20image.",hsjuxon,t1_hsjngrf,1642114447.0,False
s36y35,The article says this system works with six cameras which is way less impressive than the singular camera input mentioned in title.,hsjwfch,t1_hsjuxon,1642115023.0,False
s36y35,"Not at all lol.  It's pretty sweet.  The title is just flat out bullshit.  How are you going to tell where an object is in 3D space with only a single input at a single point?

https://en.wikipedia.org/wiki/Triangulation_(computer_vision)

EDIT: Hmm, this is actually an interesting thread:

https://movies.stackexchange.com/questions/572/why-do-you-need-6-points-to-define-a-location-in-3-dimensional-space",hskmll2,t1_hsjwfch,1642125956.0,False
s36y35,">How are you going to tell where an object is in 3D space with only a single input at a single point?

Yeah, that's what made me think ""this is really amazing"".",hslua78,t1_hskmll2,1642150004.0,False
s36y35,"**[Hawk-Eye](https://en.m.wikipedia.org/wiki/Hawk-Eye#:~:text=Hawk-Eye is a computer,path as a moving image)** 
 
 >Hawk-Eye is a computer vision system used in numerous sports such as cricket, tennis, Gaelic football, badminton, hurling, rugby union, association football and volleyball, to visually track the trajectory of the ball and display a profile of its statistically most likely path as a moving image. The onscreen representation of the trajectory results is called Shot Spot. The Sony-owned Hawk-Eye system was developed in the United Kingdom by Paul Hawkins. The system was originally implemented in 2001 for television purposes in cricket.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hsjuzaz,t1_hsjuxon,1642114464.0,False
s36y35,"Desktop version of /u/cassidysvacay's link: <https://en.wikipedia.org/wiki/Hawk-Eye>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hsjuzj8,t1_hsjuxon,1642114467.0,False
s36y35,">VcSv

Based on a masters thesis at Cambridge by Jack Davis, founder of [Loci.AI](https://Loci.AI). He has now raised VC funding to eventually build a photorealistic neural rendering engine.",hsp3lrw,t1_hsjngrf,1642203701.0,True
s36y35,Motion sickness version.,hskm8ry,t3_s36y35,1642125805.0,False
s33v2n,"As you said, the main Python interpreter is program called CPython.

Its source code is written in C - a compiled language.  
A compiled source code is turned into machine code.  
Machine code is executed (""interpreted"") by hardware.

In case of Jython - it's written in Java then compiled to byte code executed by JVM.  
Java Virtual Machine is also usually written in C and/or C++.  
Then again, source code get compiled, turns into machine code, which is executed by CPU.

In case of PyPy: written in RPython (special subset of Python).  
Then it's translated into a form of byte code, then to C and you know the rest.",hsiprv0,t3_s33v2n,1642098932.0,False
s33v2n,Yup! Thanks God Bless!,hsiua5d,t1_hsiprv0,1642100612.0,True
s33v2n,"That’s a really insightful question and I first want to say congratulations on finding that problem. Seeing inconsistencies like that’s really important for learning and a well written question (like this) is never a dumb question. 

The answer is not every language is interpreted. Some languages (like C) are compiled which means transformed into the raw machine code that can be run on the hardware. No interpreter needed once compiled. 

However this does pose the challenge of how the compilers were written. For example, the c compiler is written in c so how did it get compiled? That was done through boot strapping. First the most fundamental part of the compiler was written in assembly. This let you compile an extremely reduced C program. Then the next layer of the compiler was written in that reduced C and then compiled to allow a slightly less reduced C to be compiled. So on until the entire language could be compiled.",hsiqegg,t3_s33v2n,1642099166.0,False
s33v2n,"Out of all the platforms that I have posed this question and lost my brain cells thinking about this question over and over, your answer in layman terms is the best I could understand.

I understand the case now for interpreters written in languages that are compiled (Cpython written in C). Just to complete my understanding of this, what if the interpreter is written in the same language as to be interpreted eg. PyPy (interpreter written in Python for Python)? are such interpreters wired to in-built compilers?

Update: Just checked that the PyPy is written in RPython which is translated into C. So I guess the process for this is Your code in python => PyPy (RPython) => C files (which are compiled)

Thank you so much for your answer. God Bless! I can go to sleep now peacefully.",hsislyq,t1_hsiqegg,1642099984.0,True
s33v2n,">Your code in python => PyPy (RPython) => C files (which are compiled)

No, no, no. Your Python code is never turned into C code during interpretation.

PyPy interprets your regular Python, not RPython.  
It's the program PyPy - the interpreter itself - that was written in RPython, which got turned into C to be compiled into machine code.

Python language doesn't care what interpreter was written in.",hsj6v6n,t1_hsislyq,1642105303.0,False
s3241k,"A few things, 

The power supply will simply give your computer power to work. It does not dictate the clock freq or anything like that.

What exactly a computer does is very complicated, but a simple abstraction is a von neumann architecture. This is not at all whats happening behind the hood but thats what the hardwere wants you to see.

The tldr of von neumann architecture is that for every instruction (the ones the compiler breaks the high level program into) the cpu goes through five steps:

Fetch, where the cpu gets the instruction. Decode is where the cpu understand the cmmand. Execute is where the computation happen. Memory is where you read from the memory (ram) to get data you need for the instruction and write-back is where the results are stored in the registers or memory.

Your code does not go to the ram, but your cpu reads and write to the ram in order of executing instructions.

Writing hardwere (and checking it) is not an easy task at all. This is done by splitting the actual hardwere into many separate parts that should work indepentently and combining it all.

An hardwere engineer wont usually deal with transistors but would write in what looks like code (hdl) and use basic building blocks to create the whole piece.

Hoped it answered your questions",hsi09b6,t3_s3241k,1642089501.0,False
s3241k,"Does the clock frequency dictate the ""fetch"" component? 

Im just confused on how your computer knows to read the binary. Like when I type out my code and run it, how does the cpu know to read it. Is it always looking?",hsi0r86,t1_hsi09b6,1642089692.0,True
s3241k,"The cpu has whats called a program counter (or pc). This is a register that stores the address of the the next command it should fetch. One instruction is called branch - this instruction would change your program counter and therefore change the code you will run (for example if statement might translate into conditional branch).

Essentially (its a bit more complicated, but thats the jist of it) when you run a program the pc would point to the location of the first instruction the compiler made from your code. When the pc is there, all it do is just do the usual 5 steps until theres nothing to run from your program and it does some sort of branch to where it was before it ran your program.

Its pretty complicated so what ive said is not 100% accurate, but its close enough.

Also, under von neumann architecture we assume that the clock define all of the stages, which means all five stages should take the same time to run.",hsi3cv5,t1_hsi0r86,1642090687.0,False
s3241k,"> Also, who sits around and actually designs computer architecture, I feel like that has to be one of the most complicated jobs on the planet. You've got millions of transistors to manage controls? Or is it more abstract than that.

Computer scientists and engineers design computer architectures, if you're using the term *computer architecture* to mean *computer architecture* (aka instruction set architecture [ISA]). The design of actual computer hardware is done by computer and electronics engineers. These are the folks who figure out how a computer architecture is implemented to meet the performance, power, size, and cost goals. They'll make heavy use of abstraction and hierarchical design to manage the complexity in modern computers.",hslendn,t3_s3241k,1642139424.0,False
s2qf5f,Those who don't understand infinite loop definition won't understand this infinite loop definition.,hsg84h1,t3_s2qf5f,1642050481.0,False
s2qf5f,you may not understand it but you will feel it,hshccuo,t1_hsg84h1,1642078916.0,False
s2qf5f,I think technically that's infinite recursion rather than an infinite loop,hsg84ys,t3_s2qf5f,1642050488.0,False
s2qf5f,Recursive infinite loop,hsgbtj1,t1_hsg84ys,1642052526.0,False
s2qf5f,"whats the difference, beginner here",huoglo1,t1_hsgbtj1,1643429278.0,False
s2qf5f,Semantics. Recursion can create an infinite loop.,huomoh6,t1_huoglo1,1643432470.0,False
s2qf5f,Not really. It looks like a go-to.,hshomq2,t1_hsg84ys,1642084846.0,False
s2qf5f,"Kind of a moot point, with the exception that recursive function calls will eventually cause a stack overflow.",hsgdoir,t1_hsg84ys,1642053636.0,False
s2qf5f,Believe that can be avoided with tail end recursion,hsge09f,t1_hsgdoir,1642053838.0,False
s2qf5f,"There's no difference between infinite recursion and infinite iteration, though many programming languages will expose a difference in terms of a stack overflow. There are also those that don't.",hsl4gci,t1_hsg84ys,1642134043.0,False
s2qf5f,Which book? I think there must be more cool examples like this...,hsgaybh,t3_s2qf5f,1642052033.0,False
s2qf5f,seems to be a Latex book .. \infty,hsgbp26,t1_hsgaybh,1642052454.0,False
s2qf5f,"This was pretty common in programming books in the 80s and 90s (when I last read physical programming books). I assume it's still pretty common today.

Google does something similar when your search for teens related to infinite loops.",hsgiyka,t1_hsgaybh,1642057110.0,False
s2qf5f,"> teens related to infinite loops

Uhhmm... .teens?",hsgpz6w,t1_hsgiyka,1642062367.0,False
s2qf5f,"""terms"" sorry",hsgqt5d,t1_hsgpz6w,1642063021.0,False
s2qf5f,"Now do ""deadlock"".",hsg87eo,t3_s2qf5f,1642050525.0,False
s2qf5f,you may need 2 other philosophers,hshchh7,t1_hsg87eo,1642078986.0,False
s2qf5f,"This is just a GOTO with a dead end; there's no motivation to go back to 'Inifinite loop' in the index text. The Batman version does a better job: [https://wiki.secretgeek.net/unbounded-recursion](https://wiki.secretgeek.net/unbounded-recursion).  
Edit - Fixed the link after a long weekend.",hsgxb7k,t3_s2qf5f,1642068344.0,False
s2qf5f,page not found,hshm1z2,t1_hsgxb7k,1642083705.0,False
s2qf5f,"Why was your weekend long, huh?",hv2zgxz,t1_hsgxb7k,1643683830.0,False
s2qf5f,Got stuck in some wierd iteration thing of course!,hv6pxnt,t1_hv2zgxz,1643750292.0,False
s2qf5f,This often happens with recursion also,hshf2uz,t3_s2qf5f,1642080360.0,False
s2qf5f,"I dont get it, May someone explain pls? Am a newbie in CC",hshw08a,t3_s2qf5f,1642087872.0,False
s2qf5f,"When you search for “infinite loop” it tells you to look on page 252. So you search for “infinite loop” on the page and it tells you to go to page 252, where you search for “infinite loop” on the page and it tells you to go to page 252…. Thus an infinite loop of searching.",hsjdug3,t1_hshw08a,1642107970.0,False
s2qf5f,I stared at this trying to get it for so long I got a migraine and had to stop.,hsh6aiv,t3_s2qf5f,1642075186.0,False
s2qf5f,"delimiter, 47",hshbx8b,t3_s2qf5f,1642078670.0,False
s2qf5f,may i ask what book is it ?,hshpi74,t3_s2qf5f,1642085218.0,False
s2qf5f,"Also, why goto is bad",hshvx56,t3_s2qf5f,1642087839.0,False
s2qf5f,Genius!,hsi0fez,t3_s2qf5f,1642089566.0,False
s2qf5f,Are they trying to make my brain crash?,hsifx7n,t3_s2qf5f,1642095330.0,False
s2qf5f,Lol!,hsj55wd,t3_s2qf5f,1642104687.0,False
s2qf5f,What book is this?,hskvztp,t3_s2qf5f,1642130082.0,False
s2qf5f,"this is an *infinite recursion* technically speaking, not an infinite loop",httwly2,t3_s2qf5f,1642906785.0,False
s2nqom,"From my limited knowledge, programming languages themselves tend to be English. Now, there's nothing stopping you from doing basic manipulation to ""translate"" it into another (spoken) language. Plus functions, variables, etc. aren't limited to English",hsfmzni,t3_s2nqom,1642040478.0,False
s2nqom,"An AI which assists a non-native person during email/essay writing process, or just a good freakin' translator. You can do these things if you love both cs and foreign languages! :D

There are also so many things could be done. For example, extended search algorithm which would translate the given query into other languages and give much more results in other languages (which could be translated to preferred language after) rather than just in one you used. It would make searching things so much more productive and effortless.

Sorry for such a messy comment, I'm not really good at editing. Which makes me think of an algorithm which corrects texts in several languages using their traditional punctuation and spelling rules (i think it exists already, but still)

So yeah, there are still lots of things to be done, lots of possible projects and opportunities! (Even though most of the ones stated are already implemented in our everyday lives, but there is still a lot of fish to catch)",hsfs088,t3_s2nqom,1642042671.0,False
s2nqom,"Theory-wise I think there's an overlap in that if you've developed skills to learn languages then those same skills will help you learn computer languages.


Job-wise there are a lot of technical writing jobs in CS where they need people who can write in multiple languages.",hsfzwpo,t3_s2nqom,1642046318.0,False
s2nqom,"I don't know a lot about it but I believe ""context-free grammar"" is an overlapping idea in linguistics as well as programming language/compiler design and maybe computer science in general",hsg4f0l,t3_s2nqom,1642048553.0,False
s2nqom,"On the contrary, look up esoteric programming languages. It’s purely experimental though.",ht127hr,t3_s2nqom,1642420837.0,False
s2nqom,"Yes, the theory behind programming languages that give us syntax, semantics, and “grammar” is a base concept in this space. They are majorly based off of a concept called context free grammars(CFG). CFGs can be though of as the building blocks of of how to make sentences, strings in CS. Research into what these are and how they work and you’ll see obvious parallels as to how base theories in CS and mathematics are present in formal language.",hu4eirp,t3_s2nqom,1643091597.0,False
s2gdc0,"""digital"" comes from ""digit"" and means it can be expressed by a digit or it is countable, you can express it as a digit, you can point in it. It's the counterpart of ""analog"" which is not countable but continuous, you can't point on it, it is not an exact digit.
Real things can be digital, like the apple on a tree, or they can be analogous, like air or like water.",hsedg3a,t3_s2gdc0,1642021335.0,False
s2gdc0,"To be digital is to be discrete, where any digital information has only a limited number of discrete values. For electronics, voltage is continuous but we have selected certain voltages to represent digital information, and that digital information can only be one of two voltages, a “zero” or a “one.” It is discrete, even though voltage as a concept of physics is continuous",hsei01i,t3_s2gdc0,1642022990.0,False
s2gdc0,"Great question. I could be wrong but I believe digital means representing numbers with digits. So instead of measuring a voltage by seeing how much its current deflects a magnet, for example, you could measure it by convering it to binary digits with a ADC. 

Now how a ADC represents measurable quantities with digits is a question I don't feel qualified enough to answer.

For more information on how analog signals are measured digitally, I would look into Series-Approximation Registers (SARs).",hsedp6c,t3_s2gdc0,1642021427.0,False
s2gdc0,"Digital comes from a latin ( I think) word  meaning finger, as in technology at your fingertip.",hsjwy11,t3_s2gdc0,1642115223.0,False
s2ar5c,"All CPU sizes are turing complete and computationally universal (so long as they atleast contain conditional branch)

The bus size is mostly just about performance, even very small devices like the 8bit gameboy could access large memory banks by using bank shifting.

There are compilers which can convert any program to execute on the z80 (such as in the 8bit gameboy)",hsd9n83,t3_s2ar5c,1642006474.0,False
s2ar5c,"Wow, I didn't know that, Thanks for answering!",hsda1j7,t1_hsd9n83,1642006624.0,True
s2ar5c,"It's very important not to conflate bit width in one aspect with bit width in another. If an 8-bit CPU didn't have way to deal with data types larger than 8 bits then they wouldn't be able to count over 256. Instruction size is not the same as bus size is not the same as data type size is not the same as memory address size, yet all are expressed in bits.",hsdtlsb,t1_hsda1j7,1642013881.0,False
s2ar5c,"The CPU won't be able to operate on more than one byte at a time, but multiple bytes can be combined to represent larger values. Operating on those will just take multiple instructions.",hseoay2,t1_hsdtlsb,1642025356.0,False
s2ar5c,"And while it probably goes without saying, working those larger values comes with a **significant** performance penalty.

Even just adding two 16-bit numbers turns into:

* Add LSB bytes.
* Test for overflow/carry.
* Add carry to one of the MSB bytes.
* Add MSB bytes.

That second step is multiple instructions so you have 1 8-bit ADD instruction turning into an easy 5+ for a single 16-bit ADD. Can always make up for it via clock speed but back in the 8-bit computing days we didn't have Arduino 16MHz speeds and long calculations like that ended up just being avoided for performance reasons.",hsflc22,t1_hseoay2,1642039768.0,False
s2ar5c,You never heard of add-with-carry instructions? They've been standard for a long time...,hsfyyll,t1_hsflc22,1642045867.0,False
s2ar5c,Don't you still have to check the carry flag and add it to the next byte before adding the next significant bytes?,hshir39,t1_hsfyyll,1642082171.0,False
s2ar5c,"It's built into the instruction. For example, on the 8080 the ADD instruction will ignore the carry bit but set it if the addition generates a carry. ""ADD D"" basically does A += D. The ADC instruction will add the carry bit. ""ADC D"" does A += D + carry.",hsi2a61,t1_hshir39,1642090277.0,False
s2ar5c,i salute your knowledge,hsfn89p,t1_hsd9n83,1642040582.0,False
s2ar5c,"If you’re interested in poking around, and don’t want to learn any specific assembler, the cc65 compiler can compile C code into 6502 compatible executables for many vintage 8-bit systems, such as the Commodore 64. You can write little examples like what you’re talking about and see what happens. Speaking from experience, there’s a huge hit when you go from 8-bit integer math to 4 byte long int. Not only is it slower, but it adds a lot of extra code, which takes up program size and ram when the program is loaded. 

For example, I created a modern-like command line interface for the Commodore 64 called ChiCLI. 

I added a simple math function, so you could do some quick calculations on the command line. Nothing fancy at all, but like 11 lines of code turns into 472 extra bytes of machine code! Maybe that doesn’t seem like a lot, but in this case it’s 1% of my approx max 50K program! All the code is about 4 klocs, so 11 lines is about 0.27% of the code base. So 0.27% of the code turns into 1% of the entire program! 

The whole thing is here: 
https://github.com/chironb/ChiCLI

Here’s the code I’m referring to: 

// ********************************************************************************
		// = COMMAND / MATHS COMMAND 0--> This takes up 472 bytes!!!
		// ********************************************************************************
		} else if ( user_input_command_string[0] == '=' ) {

			long int answer, first_number, last_number;
			sscanf(user_input_arg1_string, ""%li"", &first_number);
			sscanf(user_input_arg3_string, ""%li"", &last_number);
			switch (user_input_arg2_string[0]) {
				case '+' : answer = first_number + last_number; break;
				case '-' : answer = first_number - last_number; break;
				case '*' : answer = first_number * last_number; break;
				case '/' : answer = first_number / last_number; break;
			  default  : puts(""?""); break;
			};//end-switch
			printf(""  = %li\n"", answer);",hsdxgfd,t3_s2ar5c,1642015333.0,False
s2ar5c,"8 bit CPUs like the intel 8080 were used to create arcade machines without even having bit shifting instructions independent of the accumulator register. Such machines had shift registers added as additional pieces of hardware made of a few ICs, http://searle.x10host.com/spaceInvaders/BlockDiagram.jpg",hse4p9h,t3_s2ar5c,1642018105.0,False
s2ar5c,"I don't know about tetris and pong in particular but some comments saying that it would be Turing complete are not telling the full story.

For Turing completeness it would need to be able to access memory in ways that are not usually done. For instance if its memory access is only through saying ""I want the memory at so-and-so address"", then it is not Turing complete. It would be severely crippled and even old 8-bit computers effectively use a 16-bit address register to get past this and have a little more power (while still not being Turing complete).

The way it would be Turing complete is if it could also say ""Shift the memory by one address left/right"". This way by running this command repeatedly it could access memory addresses that it otherwise wouldn't have been able to. (If you've seen a depiction of a true Turing machine this is how the machine accesses the tape.)",hsfmpyw,t3_s2ar5c,1642040363.0,False
s2ar5c,Thanks for this explanation. I was hoping to read someone explain what is meant by 8-bit computers being Turing complete.,hsfsi7j,t1_hsfmpyw,1642042892.0,False
s2ar5c,"A normal 8-bit CPU is turing complete with instructions for conditional branching as mentioned in one of the posts above. 

By turing complete we mean that it can run whatever the f\*\*\* you want really. Bitcoin mining, witcher 3, reddit etc. Now, how fast these things will run is a whole other question though.",hse5iph,t3_s2ar5c,1642018417.0,False
s2ar5c,"I like to think of this like being a carpenter, of a craft worker. A 64 bit cpu can handle all the tools and a whole range of sizes quite easily. However you can, if you needed to, do the same work and effort with a few simple tools. 

It would take a lot longer and it would be way more complicated to use the simple tools, but it can be done.

An 8bit (and even a 4 bit) cpu can perform the same “work” as a 64 bit cpu, but its implementation and execution would be much more complicated",hsfcu51,t3_s2ar5c,1642036000.0,False
s2ar5c,"[https://www.youtube.com/watch?v=ifXr7LORNCo](https://www.youtube.com/watch?v=ifXr7LORNCo)

&#x200B;

10 best 8bit games",hsdc5nq,t3_s2ar5c,1642007415.0,False
s1yfl6,This guy did fab in a home lab and has a great writeup on the physical process: http://sam.zeloof.xyz/second-ic/,hsbgnwm,t3_s1yfl6,1641968899.0,False
s1yfl6,"Thank you, that was a fascinating read. Some people are too smart",hsbi5tv,t1_hsbgnwm,1641969890.0,False
s1yfl6,How smart this is should be evaluated long term. It is highly probable you would be fast tracking yourself to stage iv cancer in such a home lab.,hse5qpb,t1_hsbi5tv,1642018500.0,False
s1yfl6,Utterly fascinating.,hsbxtb3,t1_hsbgnwm,1641981911.0,True
s1yfl6,"Instead of working, i too am being fascinated by his process. Great link /u/timeforscience",hscvfii,t1_hsbxtb3,1642000996.0,False
s1yfl6,This is incredible. Thank you for posting this,hsd07yt,t1_hsbgnwm,1642002884.0,False
s1yfl6,I literally just wanted to post him the moment I read the title.,hsda8xj,t1_hsbgnwm,1642006702.0,False
s1yfl6,"Edit: sorry for the formatting I am writing on phone. By hardware I guess you mean chips. Well like someone said the design and production/fabrication are two different things.
ARM designs the chips but does not produce the physical chips. You can learn design online, and with degrees. We use computers to design as well. We ""program"" our logical requirements like need 5 ALUs here, have a bus there following the infinity fabric protocol etc in something like verilog which them gives us a schematic with respect to logic gates.
The final implementation and production of the chips are actually in the domains of semiconductor electronics, nano tech, chemical engg and material science. The main technique used is known as photolithography. There is only ONE company in the world that makes the Machines to do photolithography and all the fabs buy from it.
The general basic process follows: (this is outdated now)


- You get your raw silicon

- process them into ingots (cylinders)

- cut them into thin round wafers

- Polish them

- You then use the output of the verilog design to make a mask.
A mask a generally a flat face of something that can block light, and you then cut the mask into the shape of a circuit (2D) to allow some spaces in between where the wires/transistors would be.


- you place the mask on top of the silicon wafers

- You use that mask and use high powered light to melt away the some of the silicon that is under the gaps of the mask.

- in the newly created spaces you add some impurities like gallium or aluminium through electron vapour diffusion(not sure if that was the correct name I am forgetting some stuff) or some other techniques to dope the silicon so that it is more or less conductive.


- what this does is creates transistors PNP or NPN FETs(field effect transistors) at the wafer level.

- then it is layered with some other layers depending on the fab.

- there can be layers of masks and multi layer designs to improve efficiency. 

- a single silicon wafer will have many cpus/chips.


- there will always be chips that will have defects in them because we are dealing with such a small scale. Those chips either are either discarded or more commonly sold as lower tier products with the bad/defective circuits burned off.


- The use of light is also the reason why crossing the 1nm barrier would be so difficult apart from electron tunneling.

I hope this somewhat explains it.
Today we have even more packing density with FINfet and 3D stacking.
We would probably see chips with inbuilt cooling pipes at the transistor level soon.",hsbhqnf,t3_s1yfl6,1641969611.0,False
s1yfl6,">photolithography.

I did not even remember such term existing, I absolutely thank you for this!",hsbwpn4,t1_hsbhqnf,1641980988.0,True
s1yfl6,Can you recommend any good resources to learn design online?,hsbxzvt,t1_hsbhqnf,1641982062.0,False
s1yfl6,"That will depend how good you are already with digital electronics and digital logic.

If you are a beginner then you should probably do a course on digital logic first and then move onto computer architecture and organization, microprocessor internals (x86 and arm).
There are lots of completely free college level playlists from the likes of MIT opencourseware. 
For digital logic you WILL need to do a lot of problems. Because problems are the way that you actually engage with something.
Once you are done and have a good grasp on the basics there are lots of nice udemy courses that goes for very low prices during sales, I am talking like $10 but are full of content. Take a single or at Max two courses and not more and focus on digesting them well.
The courses you should be looking out for generally are courses that have VLSI, VHDL, verilog in their teaching content.
For a first course any top rated course with many hours of video would do.
By the time you are done with that you'll know what you want to do next. For getting industry jobs, you can maybe start with FPGA. Because it is something you can actually own and experiment on and upload the code to GitHub like any other software project. And the concepts that you are going to need to program an FPGA overlaps a bit with verilog. You can even use C++.",hsbyzk8,t1_hsbxzvt,1641982898.0,False
s1yfl6,"Huge +1, and you can actually run all that stuff in simulators, the actually fabbing part is more of an industrial process, very hard to get into it as a hobby.

If you want to do anything there as a hobby, you can try to build small “computers” with transistor-transistor logic, check Ben Eater’s channel on youtube.",hsc1icu,t1_hsbyzk8,1641984927.0,False
s1yfl6,"To give you some idea of the scale of things, my computer systems engineering degree included a semester-long course in first year that began with you knowing nothing about digital logic and ended with small groups working together to do a gate-level design of an 8-bit CPU in VHDL.  We were given an instruction set specification and a minimum amount of memory it had to support (64k IIRC) but otherwise everything about it was up to us.  This was circa-2000 and the CPU we were designing was roughly Z80 sort of technology.

We thought we were super cool with our pipelined execution and paged memory layout giving us 16MB addressable memory (hardware was not memory mapped into data memory in this design).

We were allowed to assume 1ns per gate; our design was able to run at 16MHz.  Good times.

Anyway, point is that it's possible to get your head around enough knowledge to do a basic CPU design in a few months of two lectures a week, some reading and some experimenting.",hsclhjf,t1_hsbyzk8,1641996775.0,False
s1yfl6,"Woah, cooling pipes at the transistor level. It sounds like they will run cooler. When do you think it will be used in common products like cpu and gpu chips?",hsc4vy3,t1_hsbhqnf,1641987469.0,False
s1yfl6,"There’s 2 main processes:

Photolithography is used to create semiconductors and chips. This is an optical and chemical process which will imprint certain doping patterns onto semiconducting material along with copper interconnects. This is essentially a tiny circuit, you can make transistors, diodes, resistors and some very limited values of capacitors and inductors in this process. Conductive pins which reach outside the chip casing provide access to certain points in this circuit. Production falls under physics, chemistry and solid state physics, design falls under electrical engineering, these usually aren’t done by the same company.

Printed circuit board (PCB) fabrication is used to create finished boards with all their components including chips, resistors, transformers, connectors, capacitors, inductors etc etc. Similar to semiconductors this also uses photolithography to create the copper traces on the board, along with a host of various CNC machines for drilling holes, cutting edges, placing components. There’s various high volume ways of soldering the components to their pads such as wave soldering or reflow ovens. Production falls under mechanical engineering and a little bit of chemistry, design falls under electrical engineering, again usually not done by the same company but larger companies will spin their own PCBs because a PCB fab house is much cheaper to run than a semiconductor fab.

Other kinds of components all have their own specialized manufacturing method, for example a USB connector is a cleverly folder piece of sheet aluminum. A capacitor is two conductive plates separated with a layer of ceramic insulator. A transformer is enameled copper wire wound around a ferrite core. A crystal oscillator is a finely cut piece of quartz with two metal plates on each side. Etc etc.

Once you have assembled boards the rest isn’t too complicated, they’ll be screwed into some kind of plastic or metal enclosure, various forms of cables will provide connections to other boards, external power sources and I/O devices.",hsbwnbp,t3_s1yfl6,1641980935.0,False
s1yfl6,"This humbles me as a software engineer haha, and I thought my job was complicated!",hsbxylg,t1_hsbwnbp,1641982031.0,True
s1yfl6,You should probably ask r/ComputerEngineering or a similar engineering subreddit.,hsbc98g,t3_s1yfl6,1641966161.0,False
s1yfl6,"I'm a software engineer with some cyber security /penetration testing background, I haven't touched anything CPU related for almost half a decade never had to all this seems absolutely amazing to me.",hsbxwsp,t1_hsbc98g,1641981989.0,True
s1yfl6,"I’m a materials scientist, though, I work with polymers more than silicon. A lot of people here already mentioned aspects of the process, but I know many people who go into Si chip manufacturing. Maybe you could ask r/materials if you want more answers regarding lithography and vapor deposition. It’s pretty cool.",hsbz1e6,t3_s1yfl6,1641982940.0,False
s1yfl6,"I knew that computer architecture / engineer would be a labyrinth but I never expected that just the creation and production of a simple CPU requires literally a dozen ++ of different sciences to be done, can't even imagine how hard it will be to comprehend how a high level language interacts with the metal and controls a software.

If you're doing this you're absolutely amazing.",hsbzb9j,t1_hsbz1e6,1641983162.0,True
s1yfl6,"Haha I actually study computational materials science so I use code to study polymers, but there is definitely a lot overlap between sciences these days.

As an aside, a cpu is incredibly complex even though it seems like such a simple thing these days! Pretty incredible.",hsc2iac,t1_hsbzb9j,1641985697.0,False
s1yfl6,"Design and production are very distinct. Production is a mostly automated process that involves creating silicon wafers and burning circuit patterns into ‘em. The design process involves creating circuit diagrams and software simulations, AFAIK. They also use software to optimize the layout of the circuit, so that minimal space is used on the chip.

EDIT: I’m surprised you can’t find any information on these processes. There’re tons of documentaries and series detailing the creation of computer parts. Maybe try going to your nearest university and finding resources there if your internet searches come up dry? Most of what I know of the process comes from my computer organization class in college so it’s not a bad place for that purpose. :P",hsbf38j,t3_s1yfl6,1641967881.0,False
s1yfl6,"I refer to information about the physical creation then production, which I already know they are separate processes, apparently it makes sense I can't find them cause there's only one company in the world that creates the tools for it.",hsbwry0,t1_hsbf38j,1641981040.0,True
s1yfl6,Small correction. Its only one company that makes the cutting edge nanometer photolithography machine. For PCB (the likes of motherboards) there are other options. You can make PCBs on your own in a homelab!,hsbzbtn,t1_hsbwry0,1641983175.0,False
s1yfl6,I'm looking specifically for the way a nanometer-photoligraphy machine interacts with the rest of the computer architecture and how it is done when it comes to its physical substance of the computer its self.,hsbzj9o,t1_hsbzbtn,1641983346.0,True
s1yfl6,"Hmm, the main working concepts are not very hard to grasp. Its the details and the fact that we are talking nanometer scale that makes it hard from an engineering standpoint. And that will take a whole career to understand tha complete nuance of.
But honestly from the perspective of CS it really doesnt matter how the logic is implemented physically. CS starts at the logic level. If tomorrow we change from semiconductors to say carbon nanotubes provided we have the same TTL layer (transistor transistor logic) we can run the same programs on it even though the underlying implementation is completely different. Thats the beauty of abstraction and the reason you dont need to be an electronics expert to design chips. Because chip design and verilog generally abstracts out the transistor layer.
If you know your digital logic. All you need to know to understand the basics is how photolithography can be used to make things like transistors, diodes, resistors. Once you got your logic gates everything else comes by copy pasting.
So what you REALLY want to understand is at the semiconductor level how we make transistors. And for that if you are already in uni you could probably take a digital circuit design class or semiconductor electronics class. The info we get at the undergraduate level is enough to see how this can be done on a smaller and smaller scale. Now how to make it smaller?? Thats a whole different ball park and comes under the domain of nano tech, material science and physics.",hsc0a7e,t1_hsbzj9o,1641983950.0,False
s1yfl6,"I’d research into the “abstraction” mentioned here: it’s called the von-neumann architecture, and basically everything in the world runs on it.",hsc1u65,t1_hsc0a7e,1641985183.0,False
s1yfl6,What company ?,hsc2efg,t1_hsbwry0,1641985614.0,False
s1yfl6,"Silicon, silicon, silicon",hscu0n6,t3_s1yfl6,1642000423.0,False
s1yfl6,"This person reverse engineers (mostly vintage) hardware as a hobby and their blog is one of the most insightful on the matter, including the tiny clever design tricks: http://www.righto.com/

On the less serious hand, this article describes the process of building a cpu with colorful illustration: https://blog.robertelder.org/how-to-make-a-cpu/",hsdg95q,t3_s1yfl6,1642008938.0,False
s1h1gt,"LinkedIn, TL;DR, console, Linus tech tips, Some Ordinary Gamers, simpli learn, fire ship, network chuck.",hs8j8oq,t3_s1h1gt,1641924518.0,False
s1h1gt,https://youtube.com/c/CodingTech,hs9ojp2,t3_s1h1gt,1641939666.0,False
s1gfy8,This is awesome and I love the art. I'm definitely going to print these and post them in my lab.,hsbnrpe,t3_s1gfy8,1641973905.0,False
s1gfy8,Thank you! I'll take that. 😄 So your lab is related to CS/mathematics?,hsbp6xv,t1_hsbnrpe,1641974991.0,True
s1gfy8,"Yup! :)

We work in Infosecurity and cybersecurity. I was very surprised by how much ML is used in our field.",hsbryat,t1_hsbp6xv,1641977149.0,False
s1gfy8,Interesting! Is the ML being used for some form of anomaly detection?,hsbt2db,t1_hsbryat,1641978025.0,True
s1gfy8,"Yes! However for our specific subfield and group, no (but there is a different lab here that is doing some form of anomaly detection). I can't really say what exactly we are working on since it might be easy to find me irl lol. 

But to give you an idea, our features usually consist of assembly code, which is then vectorized, and then we apply some traditional ML algorithm - typically clustering (k means is popular since it is simple :P ). But we currently are exploring different and better techniques to improve for our next project",hsbyp2d,t1_hsbt2db,1641982650.0,False
s1gfy8,"Yes, of course.

Usually, with more complex models the trade-off is between performance and explainability. Sounds exciting! It is always nice to go from more classic approaches to something more modern and see how it affects the project and the performance. Also, the nice thing here is that you will now already have a good baseline to compare the ""better techniques"" for your next project.",hsbz5b6,t1_hsbyp2d,1641983027.0,True
s1gfy8,Amazing,hsbsnvs,t3_s1gfy8,1641977704.0,False
s1gfy8,Thank you!,hsglxf6,t1_hsbsnvs,1642059246.0,True
s1gfy8,EDIT: Thank you to u/antiogu for pointing out the error. The y-intercept should be 2 in my sketch on the last panel.,hsbpx69,t3_s1gfy8,1641975560.0,True
s14xir,"* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **The Art of Computer Programming** (TAOCP) by _Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to Automata Theory, Languages, and Computation** by _Hopcroft, Ullman_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Discrete Mathematics** by _Ross, Wright_
* **Introduction to Graph Theory** by _Wilson_
* **Software Engineering** by _Sommerville_
* **Design Patterns: Elements of Reusable Object-Oriented Software**
* **Design Patterns Explained: A New Perspective on Object Oriented Design**
* **Fundamentals of Database Systems** by _Elmasri, Navathe_
* **Numerical analysis** by _Kincaid, Cheney_
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **Artificial Intelligence: A Modern Approach** (AIMA) by _Russell, Norvig_
* **Compilers: Principles, Techniques, and Tools** (Dragon Book) by _Aho, Lam, Sethi, Ullman_
* **The C++ Programming Language** by _Stroustrup_
* [**Beej's Guide to Network Programming**](http://www.beej.us/guide/bgnet/)
* [**Modern C**](https://modernc.gforge.inria.fr/) by _Gustedt_
* [**x86-64 Assembly Language Programming with Ubuntu**](http://www.egr.unlv.edu/~ed/assembly64.pdf) by _Jorgensen_
* **Effective Modern C++** by _Meyers_",hs6fo0x,t3_s14xir,1641884352.0,False
s14xir,Oh wow that’s a lot of good books only a few of which I recognise. Did you read them all?,hs6ohn8,t1_hs6fo0x,1641890918.0,False
s14xir,"Not from cover to cover, of course, but yes, I've read at least fragments of each.

Some of them I'd read for work, some for university, some for sheer curiosity; I liked most of them much enough to even buy my own copy (although some are too expensive and TAOCP is work in progress).",hs8inwr,t1_hs6ohn8,1641924304.0,False
s14xir,"> TAOCP is work in progress
 
Is it though? Game of Thrones could be finished and given a sequel series with several spinoffs and that last book still won't be here.",hs8zuw3,t1_hs8inwr,1641930591.0,False
s14xir,"The latest errata to TAOCP was from 2021-12-24, ergo it's being worked on.

And even if it takes years then what? You suggesting we won't see it at all?  
JWST was supposed to launch in 2006, it got delayed (by lot), but we still got it.",hs9mht8,t1_hs8zuw3,1641938883.0,False
s14xir,"Errata hardly counts as continuing a work in progress, it just means a reader sent in a correction. The difference between JWST and TAOCP is that there was no time limit on JWST - there is on TAOCP. When Knuth (who is 84 years old) is gone, that's it.
 
And that's even taking 4b as the conclusion, when the current plan has 4b, 4c, 4d, 5, and perhaps 6&7 also.",hs9njbk,t1_hs9mht8,1641939284.0,False
s14xir,Is there really such time limit? Robert Jordan died in 2007.  The Wheel of Time still got finished in 2013.,hs9pab9,t1_hs9njbk,1641939949.0,False
s14xir,"Fiction is a different beast to non-fiction. Depending on the licence Knuth has with his publisher there may well be something called TAOCP that gets published, but if it's someone else writing it you might as well just grab any other book on the topics and slap that label on it.",hs9q6p7,t1_hs9pab9,1641940296.0,False
s14xir,Man ... you the MVP. You even wrote your own book and shit AND you're a PhD. Damn.,hs766kj,t1_hs6fo0x,1641904272.0,False
s14xir,"Haha, I've just noticed how similar _Jorgensen_ and _Jorengarenar_ look like.",hs8itdq,t1_hs766kj,1641924361.0,False
s14xir,"Wait, so you're not him?",hs9akcg,t1_hs8itdq,1641934518.0,False
s14xir,"No, I'm not. Unfortunately, I'm far from even dreaming about PhD",hs9izu3,t1_hs9akcg,1641937576.0,False
s14xir,"Ah, well. Awesome comment/post anyways.",hs9j9mw,t1_hs9izu3,1641937677.0,False
s14xir,"Hi, 

Is it possible for you to also suggest the reading order (of at least a few books) as well?",hs7likp,t1_hs6fo0x,1641911856.0,False
s14xir,"Surprisingly hard question, you did ask.  
For scientific literature there rarely is order of reading. You read chapter from one, few definitions from another, fill the blanks from third.  
And I've also already forgotten how it is to be new to all of that, so my judgement ~~may~~ is screwed by already knowing things a beginner normally wouldn't have a clue about. But I'll try.

For programming, I could suggest **SCIP**... alongside **K&R**? The first teaches more about programming in general and the second is about C - a base for most languages we use today.  
Then I'd move to C++ and **Clean Code**.

**Concrete Mathematics** (sometimes also called a ""volume 0 of TAOCP"") is the entrance to math here. Then **Discrete Mathematics**.  
Plus you need something for algebra and calculus (unfortunately, the ones I know don't have an English release).

The two books on automata and computation (by *Hopcroft & Ulmman* and by *Sipser*) I've read simultaneously.",hs8zkdb,t1_hs7likp,1641930484.0,False
s14xir,Thank you!,hscc3jf,t1_hs8zkdb,1641992072.0,False
s14xir,"Most of them are good books but I don't think Sipser's Theory of Computation is a good book, particularly for beginners. The explanations are somewhat lacking. There are better books on automata and computation than that one.

Edit: The Algorithm Design Handbook by Skiena is an excellent introductory algorithm book with good explanations and real-world examples.",hs8or77,t1_hs6fo0x,1641926515.0,False
s14xir,"> Design Patterns: Elements of Reusable Object-Oriented Software

Also occasionally referred to as ""Gang of Four""

Also would like to recommend **Types and Programming Languages** by Benjamin Pierce",hsanngo,t1_hs6fo0x,1641954355.0,False
s14xir,"Technically, it is not the book itself, but its authors who are referred to by that name",hsbfcwh,t1_hsanngo,1641968054.0,False
s14xir,"I learned the most about computation from reading Leslie Lamport. Something about his style is simpler than a lot of CS authors. I love this paper:

[Computation and State Machines](https://lamport.azurewebsites.net/pubs/state-machine.pdf) 

It basically has everything in one place, though it is a little dense if you aren’t on practice with a lot of discrete math. The first two chapters in [Specifying Systems](https://lamport.azurewebsites.net/tla/book-02-08-08.pdf) give the best introduction to the subject. The book is great too, again focusing on what computation is at its core vs. talking about specific technologies.

Another book that has been eye opening for me is [Concrete Semantics](http://concrete-semantics.org). It’s about programming language implementation with a proof assistant, but it also focuses on what exactly a programming language is and how its semantics is not a fuzzy, informal thing, but a very concrete definition that we can write down and refer to. Programming languages are the foundation of CS. You can’t create or analyze an algorithm without a language for expressing it.

This last one I’m torn with recommending, because it doesn’t have the same readable style as the previous authors, but it’s also pretty much the best one-stop shop for the most modern view of CS: [Formal Reasoning About Programs](http://adam.chlipala.net/frap/frap_book.pdf). I still have to do double and triple takes while reading this book, but it covers so many important topics in one place that it’s invaluable. What I do is, when I don’t understand a section, I at least use it to search for other papers / books on the topic before coming back to it. Which might be the intention of the book anyway. But I mean, the definition of computer science should literally be agreed to be “formal reasoning about programs.” So this book contains to many relevant topics in one place to avoid.",hs731a3,t3_s14xir,1641902261.0,False
s14xir,"Wasn’t expecting to see “Concrete Semantics” get a mention. For formal methods, I’d also add “Handbook of Practical Logic and Automated Reasoning“ by John Harrison.

Boolos and Jeffrey’s “ Computability and Logic” was written by philosophers, for philosophy students, but it’s rigorous and expansive enough that CS students should be able to get something out of it. Geoffrey Hunter's “Metalogic” is also excellent, and much more accessible.",hs7wvgx,t1_hs731a3,1641916343.0,False
s14xir,"Re Concrete Semantics - that book just changed my view on programming language theory. I was never particularly interested in it, but got there because of my interest in verification - at some point, you have to verify real code, which requires a definition of its semantics.

And sure there are other books on PL semantics, but the focus on using a proof assistant is also a big plus in my book.",hsaph3p,t1_hs7wvgx,1641955126.0,False
s14xir,sorry got to ask what is the role name of someone who does that stuff?,hs9acjh,t1_hs731a3,1641934440.0,False
s14xir,"Well, you can use this stuff in any role, because these books just teach you how to think about programs. Like I’m a “regular” software engineer, but I use TLA+ (Leslie Lamport’s logic and toolkit) to analyze models of things that I’m building all the time. Amazon and Elasticsearch also have been known to use TLA+ too. And, I use the concepts in all of these books when reading code every day.

Anyone involved in software verification uses things like this on an even deeper level. Like anyone that works on airplane software, or even some people who make [formally verified operating systems](https://sel4.systems). These roles are less common though.",hs9wug3,t1_hs9acjh,1641942911.0,False
s14xir,"Thanks. I wanted to ask one more thing, are they related to compilers? The stuff you mentioned",hs9ycvv,t1_hs9wug3,1641943529.0,False
s14xir,"Yea, the book Concrete Semantics is mostly about implementing a compiler (for a very simple language and a very simple target machine). The way this particular book goes about that is unique, in that it goes over how to prove that the compiler is correct in a proof assistant. This is something that isn't done in practice almost at all, but is very interesting in terms of understanding what a compiler is actually doing.

Coming up with a correctness statement is the best way to show that you really understand something.",hsawj9y,t1_hs9ycvv,1641958187.0,False
s14xir,"I’m a huge fan of [Code: The Hidden Language of Computer Hardware and Software](https://en.m.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software). It’s a little older at this point, but the foundation it lays is amazing. It builds from simple hardware relays, to a CPU, all the way up to an OS, and everything in between. Truly a hidden gem.",hs7372r,t3_s14xir,1641902371.0,False
s14xir,"Desktop version of /u/HiImLary's link: <https://en.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hs73854,t1_hs7372r,1641902390.0,False
s14xir,"The Little Schemer

The Elements of Computing Systems",hs6ogcx,t3_s14xir,1641890889.0,False
s14xir,"I recommend ""Models of Computation: Exploring the Power of Computing"" by John E. Savage. I much prefer it over the Sipser.

For a beginner, maybe the CLRS algorithms text or something that makes you learn C/assembler really well.",hs7kkaf,t3_s14xir,1641911450.0,False
s14xir,Is it better than Sipser and how? I had Sipser bookmarked as an intro must read for computational complexity,hsaqoz7,t1_hs7kkaf,1641955643.0,False
s14xir,"Savage's text is just more comprehensive. It covers circuit complexity, satisfiability, and random-access machine models in great depth in addition to all the things in Sipser.

In terms of having something elegant and self-contained for an intro course - or if you want to work through the whole thing - then Sipser is more suitable. It's better as a primer.

You'll see what I mean if you skim the pdf's.",hsatrad,t1_hsaqoz7,1641956968.0,False
s14xir,"I liked the Andrew Tanenbaum's books on OS and computer architecture. 
Digital logic by Morris mano
Computer architecture by Morris mano
Algorithms by Robert Sedgwick",hs6a614,t3_s14xir,1641880885.0,False
s14xir,"Godel, Escher, Bach by Douglas Hofstadter. If you're keen on a different perspective.",hs6k2w6,t3_s14xir,1641887494.0,False
s14xir,"Mathematics and Computation by Avi Wigderson, Algorithms by Jeff Erickson",hs7ihvg,t3_s14xir,1641910551.0,False
s14xir,"Introduction to Linear Algebra by Gilbert Strang: [https://math.mit.edu/\~gs/linearalgebra/](https://math.mit.edu/~gs/linearalgebra/)

Artificial Intelligence: A Modern Approach by Russell and Norvig: [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)",hs89frn,t3_s14xir,1641920974.0,False
s14xir,"I recently got Mastering Regular Expressions by Jeffrey Freidl, seems pretty good in so far. It’s not really tutorialey, I’d say it’s more concept. Though it shows examples in a few different languages",hs8kwas,t3_s14xir,1641925121.0,False
s14xir,Algorithms to live by,hsab8zv,t3_s14xir,1641948983.0,False
s14xir,"This is one of the books that hyped me up when I got my first CS job.  

*Algorithms To Live By” was neat and made me think “woah, this stuff really is cool”

*Thinking, Fast and Slow* is another one. It’s subject is actually psychology, and has nothing to do with CS, but it was recommended by a CS professor and I’ve seen it come up in the CS subs a few times. I like that it aims to compare the two systems of thought and explains the roles each one play in our day to day decision making and problem solving. The things I learned from that book really stuck with me and my career has directly benefited from reading it.",hsblwzh,t1_hsab8zv,1641972543.0,False
s14xir,"Designing data intensive applications by Martin kleppman 

Systems performance by Brendan Gregg 

Fundamentals of software architecture an engineering approach by mark Richards and Neal ford 

Design patterns: elements of reusable object oriented software by gamma,helm, Johnson, vlissides

The art of immutable architecture by Michael Perry is also a new one I’m enjoying",hsd6q3n,t3_s14xir,1642005376.0,False
s14xir,"A little late to this party, but here's a list I personally loved. My background is a bit varied - PLT, Numerical Analysis/Scientific Computing, and random interests I picked up throughout undergrad and beyond.

**Algorithms**

Building intuitions for how to compute efficiently.

The book that everyone else immediately recommends here is CLRS' ""Introduction to Algorithms"". I break from this tradition a bit in that, this was the one text that scarred me the most, mainly because I was wholly unprepared for it when I first went in.

First Course: these texts are geared for a first exposure to the construction + building up intuitions for algorithms.

* Grokking Algorithms - Aditya Bhargava
* The Algorithm Design Manual - Steven Skiena
* Algorithms - Robert Sedgewick
* Algorithms - Jeff Erickson

Analysis of: these texts are a bit more advanced and focus on proving correctness or properties of algorithms

* An Introduction to the Analysis of Algorithms - Robert Sedgewick
* Algorithm Design - Kleinberg & Tardos
* The Design and Analysis of Algorithms - Dexter Kozen

Reference ""brick"": I've always had a love-hate relationship with this one

* Introduction to Algorithms - Cormen, Leiserson, Rivest, Stein^(not a favorite, but obligated to mention)

--------------------------

**Programming Language Theory**

This is my specialization in undergrad - how to reason about program semantics, how to analyze programs, and how to give type to things. I would recommend at least some algebra before tackling these. Some of the semantics texts may touch on algebraic topology results (e.g. Scott + Domain theory), but the poset / limit constructions are mostly self-contained.

Program Analysis: how to analyze programs

* Principles of Program Analysis - Flemming Nielson
* Principles of Abstract Interpretation - Patrick Cousot
* Advanced Compiler Design and Implementation - Steven Muchnick^(it pretends that it's a compiler text, but it's a static analysis text)
* Data Flow Analysis: Theory and Practice - Sanyal, Sathe, Khedker

Semantics: how to specify + calculate with the semantics of programs (what does it mean?)

* Types and Programming Languages - Benjamin Pierce
* The Formal Semantics of Programming Languages: An Introduction - Glynn Winskel
* Foundations for Programming Languages - John C. Mitchell
* Semantics With Applications: An Appetizer - Flemming Nielson
* Formal Languages and Compilation - Morzenti, Breveglieri, Reghizzi

Types: type semantics, & how to encode properties/contracts/proofs into dependent languages like Coq & Idris

* Software Foundations - Benjamin C. Pierce
* Proofs and Types - Jean-Yves Girard
* Lectures on the Curry-Howard Isomorphism - Sorensen & Urzyczyn
* Type driven development with Idris - Edwin Brady

--------------------------

**Systems / Compilers**

Compilers: how to write a compiler / interpreter

* Engineering a Compiler - Keith Cooper
* Modern Compiler Implementation in Java - Appel & Palsberg
* Compilers: Principles, Techniques, and Tools - Aho, Lam, Sethi, Ullman^(not a favorite, but obligated to mention)
* LLVM Cookbook - Pandey & Sarda

Systems: how computer systems are architected, how operating systems are made up, concurrency in action

* The Elements of Computing Systems: Building a Modern Computer from First Principles - Nisan & Schocken
* Computer Organization and Design RISC-V Edition - John L. Hennessy
* Linux Pocket Guide - Daniel J. Barrett
* Understanding the Linux Kernel - Bovet, Cesati
* The Little Book of Semaphores - Allen B. Downey
* Java Concurrency in Practice - Brian Goetz

--------------------------

**Numerical Analysis / Mathematical Modelling**

Prereqs: linear algebra + diff eq

* Introduction to Linear Algebra - Gilbert Strang
* Linear Algebra and its Applications - Gilbert Strang
* Elementary Differential Equations & Boundary Value Problems - Boyce & DiPrima
* Terrell's Notes on Differential Equations - Robert Terrell

First Course: introduction to numerical analysis + scientific computing

* Introduction to Scientific Computing - Gene H. Golub
* Applied Numerical Methods: With MATLAB for Engineers and Scientists - Steven C. Chapra
* A First Course in Numerical Methods - Greif & Ascher

Second Course (grad): more advanced methods

* Matrix Computations - Gene H. Golub
* A first course in optimization - Charles Byrne
* Numerical Linear Algebra - Nick Trefethen
* Applied Numerical Linear Algebra - James Demmel

Applied:

* An Introduction to Mathematical Modeling - Ed Bender
* Scientific Visualization: Python & Matplotlib - Nicolas Rougier

--------------------------

**Programming**

General languages: great PL manifestos

* The C Programming Language - Kernighan & Ritchie
* A Tour of C++ - Bjarne Stroustrup
* Hack and HHVM - Owen Yamauchi
* Programming in Lua - Roberto Ierusalimschy

Functional PL / Richard Bird: a world where you can do calculations with programs, and learn some category theory along the way

* Learn you a Haskell - Miran Lipovaca
* Real World OCaml - Yaron Minsky
* Introduction to Functional Programming - Bird & Wadler
* Pearls of Functional Programming - Richard Bird

--------------------------

**Learning to read Donald Knuth's TAOCP**

It took me most of undergrad and several years afterwards to work through small parts of the fascicles of volume 4. It led me down a giant detour of analytic + enumerative combinatorics, with complex analysis being the backbone of several of these asymptotic enumeration problems.

Step 1: Learning Discrete Mathematics

* A Course in Discrete Structures - Pass & Tseng
* Discrete mathematics and its applications - Kenneth H. Rosen
* Concrete Mathematics: A Foundation for Computer Science - Donald Knuth
* Concrete Math Companion - Kenneth E. Iverson

Step 2: Enumerative combinatorics

* Visual Complex Analysis - Tristan Needham
* Generatingfunctionology - Herbert Wilf
* Analytic Combinatorics - Flajolet & Sedgewick
* Introduction to the Theory of Species of Structures - Bergeron, Labelle, Leroux

Step 3:

* ???

Step 4: Profit

* The Art of Computer Programming - Donald Knuth^(not a favorite, but obligated to mention)

--------------------------

**Others**

Some other books (in order of ease) that I remember

* LaTeX - Leslie Lamport
* Discrete differential geometry: an applied introduction - Keenan Crane
* Nonlinear dynamics & chaos - Steven Strogatz
* A first course in network theory - Estrada & Knight
* Galois' Dream - Michio Kuga
* Ideals, Varieties, and Algorithms - Cox, O'Shea, Little
* Putnam and Beyond - Titu Andreescu

--------------------------",hsqvmqw,t3_s14xir,1642238446.0,False
s14xir,[deleted],hs7gt3j,t3_s14xir,1641909792.0,False
s14xir,I agree. The authors seem so smug through their writing. The book really isn’t the masterpiece people hype it up to be.,hs8nf0z,t1_hs7gt3j,1641926032.0,False
s14xir,"Clean Code

Also Cracking the Coding Interview (if you haven't read it already). I just started reading it late last night and couldn't put it down.",hs6xkxh,t3_s14xir,1641898309.0,False
s14xir,Should I start learning in my spare time or wait until ny University starts CS subjects for us in 2nd year?,hs7a06r,t3_s14xir,1641906450.0,False
s14xir,Start,hsb3lh5,t1_hs7a06r,1641961493.0,False
s14xir,What about modern CS Books?,hs9a1p2,t3_s14xir,1641934332.0,False
s14xir,C++ Concurrency In Action - Anthony Williams,hsa4oi8,t3_s14xir,1641946190.0,False
s14xir,"- The Annotated Turing
- A Mind At Play (biography of Claude Shannon)
- Assorted Papers on Fun and Games, Knuth
- And ther is an upcoming biography on von Neumann available for pre-order",hsambyl,t3_s14xir,1641953789.0,False
s14xir,GEB?,hsc2c60,t3_s14xir,1641985565.0,False
s0vdjp,The math you need for data structures is discrete math. Set theory and graphs.,hs70hb4,t3_s0vdjp,1641900483.0,False
s0vdjp,"What – more specifically – are you having problems with?

Without knowing the above, have you studied much probability theory? This (and linear algebra which you mentioned) makeup an incredibly large amount of machine learning, which often comes as a not-too-convoluted application of these subjects.",hs63wcr,t3_s0vdjp,1641877489.0,False
s0vdjp,"I have not studied probability theory yet

My issue is how I haven’t yet been able to actually “apply” any of these mathematic subjects in computer science yet, and idk when I should be learning to do so",hs7vqrn,t1_hs63wcr,1641915916.0,True
s0vdjp,"Grab Matlab and try to tweak a few pictures, you’ll be able to see linear algebra in action almost immediately lol.",hsoqhun,t1_hs7vqrn,1642198267.0,False
s0vdjp,"I like this one, thanks fren",hsoytoh,t1_hsoqhun,1642201690.0,True
s0jzi8,"A few concepts:

Voltage: voltage is the electrical analog of pressure. It has a value for each point in 3D space, these values can change as functions of time. The voltage at a single point has no meaning on its own, it the difference of voltages between two points which has any meaning. 0v is usually defined as some arbitrary reference point which all other voltages will be compared to.

Node: a node is a continuous path of conductive material, for example a wire. It may branch off in multiple directions. All points on the same node are at the same voltage.

Power rails: two special nodes within the CPU would be the ground rail and the power rail. The ground rail would be the node which is defined as 0v. The power rail has some positive voltage such as 5v or 3.3v. These come from the power supply which will maintain the voltage difference between them. The voltages on these two rails are what defines “logic high” and “logic low”. These are distributed all throughout the CPU.

Transistor: a transistor is a 3 terminal device, meaning there are 3 pins which can each connect to different nodes. A voltage on the 1st pin controls whether the other 2 pins are electrically connected or not.

Logic gate: logic gates are special arrangements of transistors which perform logic operations on the voltages coming from their inputs, and set the voltages of their outputs accordingly. For example a not gate can be built with 2 transistors, one which will connect the output node to the positive power rail when the input node is low, and one which will connect the output node to the ground rail when the input node is high.

So to answer your question, 1s and 0s are voltages on wires. These voltages are set by electrically connecting the wires to the positive or negative terminals of the power supply. These connections are made with transistors, other signals on different nodes tell the transistors when to open or close. Logic is created with special arrangements of transistors.",hs2xtap,t3_s0jzi8,1641832730.0,False
s0jzi8,"Thank you very much.

So would you agree that it is wrong to say that the cpu represents 1s and 0s with turned on and turned off transistors? And that it’s also wrong to say that 1 means there is electrical current and 0 means there is no current?

Edit: Also is it correct to say that a logically high (1) wire has a high voltage relative to ground? And a logically low (0) wire has basically no voltage relative to ground?",hs3h834,t1_hs2xtap,1641839761.0,True
s0jzi8,Yes you have it now.,hs4h0vz,t1_hs3h834,1641852901.0,False
s0jzi8,"Your edit is exactly correct.

However no current actually has to flow, it’s just a static voltage difference. With the exception of during the time when a wire is changing voltage, at the physics level charge is the source of voltage, so to change the voltage on a wire a small amount of charge must flow onto the wire. However this is an unwanted but physically unavoidable property of the wire. So at the beginning of a clock cycle if a wire is changing voltage then a small current flows onto the wire through a transistor, but during the bulk of the clock cycle there are no currents flowing.",hs4uk4w,t1_hs3h834,1641858378.0,False
s0jzi8,Oh that makes perfectly sense thank you!,hs69dos,t1_hs4uk4w,1641880421.0,True
s0jzi8,"At the lowest level yes. The reason for 1,0 is transistor in open or closed.

Transistor level is pretty far into EE, so you wont find as much good answers here, but ill try, ill say some wrong things for simplicity sake - it is very complicated when you look at the low level.

We discuss in an open/closed or 1/0 due to the phyisical properties of the trasistor, control signal (or gate voltage) above some theashold (that depends on the doping and process) you can allow voltage and below it you can not.

So assuming your threashold is 0.7V, having a gate voltage of 0.2V would be the same as 0. This lets us discuss in a resulutions of 0 and 1.

A single transistor is not a ""bit"" though, but a combination of transistors (and other linear element) in the end forms some sort of memory element. This element might be an input for a different transistor and so on, so its important to talk about that in an 0/q abstraction as well.

Having a base 3 processor can work however, using a very carefully tuned transistors and logic levels - but its completely usless due to the immense amount of drawbacks.",hs4juqn,t1_hs3h834,1641853992.0,False
s0jzi8,"1s and 0s are just an abstraction humans use to do calculations. In terms of a transistor 1/ON means current can flow through the transistor. 0/OFF means current can not flow through the transistor. 
Now if you lined up a few transistors in various states like:
ON OFF OFF ON OFF ON OFF ON
that doesn't mean much to a human. So let's assign them 1s and 0s, that would look like: 
10010101
Now we have numbers. But that still doesn't mean much to an average person so let's convert them from base 2 (binary numbers) to base 10 ( 0-9 number system we learn in grade school).
I won't go into a full lesson on calculating binary but 10010101 = 149.
So we just used a little abstraction to make a group of 8 transitors represent the number 149!
Now if we keep adding transistors (bits) we can make all kinds of numbers to do all kinds of calculations with.
Hope that makes sense.",hs2fnzq,t3_s0jzi8,1641825715.0,False
s0jzi8,Yes but where in an actual circuit would you line up transistors like that to represent a binary number? This doesn’t really make sense to me. Sequential logic (memory circuits) sure don’t work this way. And I don’t think combinational logic does either,hs2ga3m,t1_hs2fnzq,1641825965.0,True
s0jzi8,"> where in an actual circuit would you line up transistors like that to represent a binary number?

In the CPU, they aren't ""lined up"" (deserialized), they're ""sideways"" (serialized). So a whole, say, 8 bit operand is operated on at once.",hs2iacg,t1_hs2ga3m,1641826781.0,False
s0jzi8,"Okay but for example with an ALU you have way more transistors than bits in the operands, and the transistors don’t really all represent a bit do they?",hs2imzf,t1_hs2iacg,1641826925.0,True
s0jzi8,"> Okay but for example with an ALU you have way more transistors than bits in the operands, and the transistors don’t really all represent a bit do they?

Well no, because by themselves transistors don't do anything, not even store values. To do that, you need to combine transistors with each other (and resistors, diodes, capacitors, ..). To do this you build gates and latches that are represented as single black boxes, but are made up of multiple components. With these abstracted components you can then build logic circuits. [Here's](https://www.cs.bu.edu/~best/courses/modules/Transistors2Gates/) a website I googled that shows how to build gates from transistors (this is already a bit abstracted because if you want to build f.e. an AND gate from transistors you need resistors, too). [This](https://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) is a book that builds a computer from first principles.",hs32sam,t1_hs2imzf,1641834531.0,False
s0jzi8,"Yes I get that but my point was just, someone said that we can assign bits to the transistors and that the computer represents 1s and 0s with on and off transistors. And I still think that transistors aren’t representing any numbers, the numbers are some electricity or voltage or whatever going through a wire.

Also to be pedantic transistors obviously do do something on their own but you know that",hs3geyi,t1_hs32sam,1641839467.0,True
s0jzi8,"> Yes I get that but my point was just, someone said that we can assign bits to the transistors and that the computer represents 1s and 0s with on and off transistors. 

So without going *too* far, there's levels of abstraction of knowledge. This is a point I think Carl Sagan once famously made when he discussed the answer to the question ""why is ice slippery?"" You can look at a computer at a very low level, for example the level of voltage and current changes in transistors. That is what you have chosen to do, but that decision is arbitrary. It's probably the least abstract level you are comfortable with, but there's no reason to not go deeper and talk about Maxwell and field theory and perhaps quantum mechanics. I don't know, I'm not a physicist. But yes, you are correct when you say that ""transistors represent 1s and 0s"" is false on this level of resolution. But it's a *useful* abstraction. If you actually look at a processor (or any integrated circuits), and you keep zooming in, you eventually end up with transistors so tiny you could fit thousands of them into the cross-section of a human hair. And of course those transistors aren't just connected in series, they are instead configured in latches and gates, in multiplexers, and so on. What ""transistor"" in colloquial speech *means* is really dependent on what you are considering. When somebody says ""transistors represent 1s and 0s"", what they kinda mean is that there are tightly integrated MOSFETs (and other elements) on a chip that are arranged into latches that store state as ""current"" or ""no current"", which can be manipulated by tightly integrated MOSFETs (and other elements) on a chip that are arranged into logic gates and the like. And that's all incorrect and just a different level of abstraction (because what on earth is a ""transistor"" if not a human abstraction over what happens if certain materials are put in very specific configurations within very specific electro-magnetic fields, and what is a ""material"", and so on), so we just say ""transistor"".

> And I still think that transistors aren’t representing any numbers, the numbers are some electricity or voltage or whatever going through a wire.

That's functionally the same thing. They are ""representing"" a bit in that current going through a electro-magnetic field projecting from a wire is understood to encode ""1"", and no such current encodes ""0"". 

I actually don't know what else to say, because I don't think you actually have an issue with that, or at least I hope so. Because otherwise, if you are tasked with giving somebody 2 apples, you would have to burst into tears because how can apples be represented by a number and what is counting anyway?",hs3pka8,t1_hs3geyi,1641842772.0,False
s0jzi8,"Thank you very much for that answer!

> I actually don't know what else to say, because I don't think you actually have an issue with that, or at least I hope so. Because otherwise, if you are tasked with giving somebody 2 apples, you would have to burst into tears because how can apples be represented by a number and what is counting anyway?

Lol. Don’t worry I‘m gonna stop asking now. I just want to understand this stuff at an actual physical level",hs3ufdr,t1_hs3pka8,1641844556.0,True
s0jzi8,"Deleted this original reply cuz QuietLikeScience summed it up way better than me first. 

That said,  If you're interested I highly recommend watching the Crash Course Computer Science series on YouTube.  The first few episodes covers this stuff and may shed some light for you.",hs3rp51,t1_hs3geyi,1641843552.0,False
s0jzi8,"Thanks, I‘ve seen those, they are pretty well made however they don’t go as in depth as I want to understand it lol",hs3ulk9,t1_hs3rp51,1641844620.0,True
s0jzi8,"> isn’t voltage the electric tension between two points, which points would that be?

Personally, I like to think of voltage using the [waterfall analogy.](https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Supplemental_Modules_(Analytical_Chemistry\)/Electrochemistry/Voltage_Amperage_and_Resistance_Basics#:~:text=If%20we%20draw%20an%20analogy%20to%20a%20waterfall%2C%20the%20voltage%20would%20represent%20the%20height%20of%20the%20waterfall%3A%20the%20higher%20it%20is%2C%20the%20more%20potential%20energy%20the%20water%20has%20by%20virtue%20of%20its%20distance%20from%20the%20bottom%20of%20the%20falls%2C%20and%20the%20more%20energy%20it%20will%20possess%20as%20it%20hits%20the%20bottom.)

> I have always imagined the 1s and 0s or asserted and deasserted signals as high and low amperage, is that wrong?

Amps and volts are different. Amps are the amount of water going over the waterfall, volts are the height of the waterfall (the potential energy). When the external clock ""drives"" the CPU, it alternates voltages. Electrical engineers look at [eye diagrams](https://en.wikipedia.org/wiki/Eye_pattern) to assess the quality of binary signals. 

> I‘m really confused by this, some say 1 and 0 is the state of the transistor but that doesn’t make sense to me.

You should watch [this video](https://youtu.be/Hi7rK0hZnfc) on sequential logic. However, digital logic design is an art unto itself.",hs2g20w,t3_s0jzi8,1641825874.0,False
s0jzi8,"What is meant by „power“ in that eye pattern? And I understand what voltage and amperage is, but which one is interpreted as 1 and 0?

Edit: Also those combinational logic circuits don’t really use „transistor is on“ as 1 and „transistor off“ as 0 as far as I understand",hs2gkp5,t1_hs2g20w,1641826084.0,True
s0jzi8,"Here are some excellent resources

[Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)

[Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)

Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share)

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",hs4psv2,t3_s0jzi8,1641856372.0,False
s0jzi8,I am reading Code now and I wish I would have read it 20 years ago when it came out. Such a wonderful book. It explains exactly what OP wants to know.,hs6a30g,t1_hs4psv2,1641880836.0,False
s0jzi8,1s and 0s can be anything that is detectable in multiple states by a single detector.,hsgb7ob,t3_s0jzi8,1642052177.0,False
s0ficm,"This is amazing lol, especially the raspberry pi!",hs1ilik,t3_s0ficm,1641805836.0,False
s0ficm,"Well, Mabrook, I wish them Success in Deen and Dunia as written on the floppy disc 💾",hs1wdbe,t3_s0ficm,1641815905.0,False
s0ficm,"Ayooo, spot the Muslim. Thanks! I’ll let my brother know.",hs1zxdk,t1_hs1wdbe,1641818076.0,True
s0ficm,Love it!   Congrats on the graduation.  These are the best kind of cookies a CS graduate could enjoy .,hs1of4u,t3_s0ficm,1641810255.0,False
s0ficm,This is so wholesome <3,hs1qcii,t3_s0ficm,1641811703.0,False
s0ficm,"Ah, my favorite aspect of CS: uwu",hs2tqmp,t3_s0ficm,1641831250.0,False
s0ficm,"Hahaha. Yeah thats that’s because my brother ironically uses that phrase, so much now that it’s unironic……",hs520rd,t1_hs2tqmp,1641861536.0,True
s0ficm,"This is adorable!!! Congrats to your bro for graduating, this is awesome.",hs3xxlj,t3_s0ficm,1641845847.0,False
s0ficm,"All really cool, but I do have one question. What is yay taco? Maybe I’m totally forgetting something. Once again great stuff!!",hs3tne6,t3_s0ficm,1641844274.0,False
s0ficm,Hahah someone finally noticed. It’s my brothers nick name. Taco. And congrats taco was too hard to pipe. Same with tacos graduation. So yay taco it is,hs51lsu,t1_hs3tne6,1641861361.0,True
s0ficm,Awesome thanks for the response!!,hs56nu3,t1_hs51lsu,1641863474.0,False
s0ficm,yay Taco? I’m graduating with a CS degree in May and I’ve never heard of this xD can someone educate me?,hs3xkdq,t3_s0ficm,1641845711.0,False
s0ficm,Hahah just my brothers nick name. No crazy new code here,hs51pop,t1_hs3xkdq,1641861407.0,True
s0ficm,Damn i thought the @ was a Debian logo... Really good job on everything! It looks great!,hs4qk17,t3_s0ficm,1641856685.0,False
s0ficm,"That’s amazing, how come I wasn’t invited though. 😂",hs2pcdu,t3_s0ficm,1641829593.0,False
s0ficm,Adorable!!! And congratulations 🎊🍾🎉,hs46e6x,t3_s0ficm,1641848961.0,False
s0ficm,That is so awesome 😆❤️,hs4yk0m,t3_s0ficm,1641860084.0,False
s0ficm,"Awesome, you're a good brother, I'm sure he'll be happy",hs1pznd,t3_s0ficm,1641811439.0,False
s0ficm,"I’m a sister, a brother could never. 
My brother got me a dowel, bleach, and a sympathy card when I graduated nursing school. Hahahah each had a meaning.",hs1zst1,t1_hs1pznd,1641818002.0,True
s0ficm,"Oh sorry about that, wish I had a sister like you! Have fun when you celebrate the graduation, cheers.",hs20u1y,t1_hs1zst1,1641818598.0,False
s0ficm,Thanks!,hs21mka,t1_hs20u1y,1641819032.0,True
s0ficm,This is cursed,hs24sgi,t3_s0ficm,1641820717.0,False
s0ficm,What now why would you say that??,hs2hiev,t1_hs24sgi,1641826461.0,True
s0ficm,It's cute but I would hate it,hs2ihan,t1_hs2hiev,1641826859.0,False
s0ficm,Hahah I mean I could see that. Because I was an outsider tryna make it CS themed….so it’s like super cheesy.,hs2oo7k,t1_hs2ihan,1641829336.0,True
s0ficm,"Hey, I’m from CS and I like it. Your brother is lucky 😀😀",hs39ejw,t1_hs2oo7k,1641836936.0,False
s0ficm,arraytest[3] on the yellow floppy disk is cursed af,hs4y8al,t1_hs24sgi,1641859943.0,False
s0ficm,I was wondering if anyone else saw that,hs67fwx,t1_hs4y8al,1641879332.0,False
s0ficm,send the location :D,hs3uyvt,t3_s0ficm,1641844755.0,False
s0ficm,It passed 😅😅😅,hs51mzk,t1_hs3uyvt,1641861376.0,True
s0ficm,"I was expecting computer science symbols to be like asymptomatic notation like Big Omega and boolean or bitwise operators, but very thoughtful nonetheless.",hs2p9c6,t3_s0ficm,1641829560.0,False
s01ijj,Mass surveillance and facial recognition,hryszds,t3_s01ijj,1641762565.0,False
s01ijj,"Several US law enforcement offices have been accused of using facial recognition with false matches as criminal evidence. Many of these algorithms have a noticeable racial bias.

https://www.nytimes.com/2020/01/12/technology/facial-recognition-police.amp.html",hs1pjq1,t1_hryszds,1641811110.0,False
s01ijj,Can an advanced AI be considered someone's property?,hryvu9c,t3_s01ijj,1641763613.0,False
s01ijj,"This just reminded me of the Star Trek TNG episode where Data is on trial and the questions is does he have rights as a person, or is he the property of Starfleet.  


Maybe I'll show that episode to my students at the start of the debate unit. lol",hryxqox,t1_hryvu9c,1641764282.0,True
s01ijj,"If you ask a student to multiply two three-digit numbers, most people would agree that they have to ""think"" to produce an outcome. However, if you write a simple program for a computer to multiply two three-digit numbers, most people would argue against the computer doing any sort of ""thinking"".

So what can we use to define ""thinking"" (i.e. ""intelligence"") if it isn't based solely upon the outcome?",hrzjrdu,t3_s01ijj,1641772328.0,False
s01ijj,I disagree.  Humans would just follow the algorithm they were taught in school.  Thinking would be required if you were to explain the algorithm or your own algorithm.,hrznjr1,t1_hrzjrdu,1641773754.0,False
s01ijj,"Should decisions of an AI be explainable/interpretable in life or death situations (e.g. medical/military)? What about if it makes better decisions than humans?

How would you know for sure when AI has consciousness? When does it acquire the same rights as humans or animals?

Should AI in social media that tries to feed you maximally addictive content be restricted?

Should people whose jobs are automated by AI have a right to get universal basic income?",hryybso,t3_s01ijj,1641764488.0,False
s01ijj,"I major in computer science and for the computer ethics course I took we debated whether it was ethical to do mass surveillance of students social media posts to track for concerning behavior that could lead to something tragic such as a school shooting (we had learned that many school shooters had posted on social media concerning content). If you are interested in learning more feel free to check out this article on one of the PhD students of the professor who taught my computer ethics course.

https://engineering.lehigh.edu/news/article/online-privacy-algorithms-we-trust",hrz1s4r,t3_s01ijj,1641765716.0,False
s01ijj,"What is the meaning of intelligence? What specific character does an ""advanced"" Aritificial Intelligence supposed to have such that it would be considered on the same level as a thinking human?",hrz1qau,t3_s01ijj,1641765697.0,False
s01ijj,"If you could have your English essay graded by a human or by an AI that has been shown to give correct results with reasonable accuracy, which should you choose?

If a car accident results in a death, does it matter whether the car at fault was a human or an AI? How can such a situation be handled in terms of holding the correct party responsible for damages?",hrz33xr,t3_s01ijj,1641766192.0,False
s01ijj,"Who is liable if AI makes a mistake in medicine / self driving cars?

Is the developer liable? Data collector? Etc.",hrzbyjg,t3_s01ijj,1641769410.0,False
s01ijj,"How do we ensure that our AI reflects human ethics? How do we make sure that algorithms do not reflect patterns such as racism or sexism in terms of recommendations or referrals? (Financial scoring, resume review, etc)

Is it possible for AI to become equal to or better than people at tasks that people perform? If so, how should society address this issue? Is it possible for AI to take over ALL jobs?

What are the ethical differences between AI and natural intelligence? Are there any intrinsic differences between AI and natural intelligence and why? How can laws reflect this?",hrzfvcz,t3_s01ijj,1641770871.0,False
s01ijj,"Define Artificial

Define Intelligence

Define Computer 

Define Human",hrz6lq2,t3_s01ijj,1641767447.0,False
s01ijj,Human and computer have solid scientific definitions.,hrzn53o,t1_hrz6lq2,1641773602.0,False
s01ijj,"Computer used to refer to a person computing (i.e. with a pen and paper). Human has a species definition, but this is not the only definition - though perhaps 'person' could be used here instead.
 
These aren't good debate topic statements though, they are questions. A topic statement gives a position that can be argued for and against.",hs1e7fm,t1_hrzn53o,1641802599.0,False
s01ijj,"I think it's still an interesting activity to write down and analyse the implications and contradictions between these definitions

Corolary arguments which arise could be ""to what extent can a human/ person bequeath their personality/ goals/ rights to a machine/ computer""",hs1mwqy,t1_hrzn53o,1641809123.0,False
s01ijj,"You haven’t given us the objective you’re hoping to achieve. If it’s simply, “Get them to think about these things.” I don’t think you’d need to ask Reddit; a simple Google search would do fine.

It truly depends on what you’ve taught your course as most debates seem to tend towards settling into an argument to find consensus over definitions. 

What have you defined to them as artificial intelligence? And what do you define as “advanced?” What characteristics or traits would an “advanced AI” have from your perspective?

Russell and Norvig define “artificial intelligence” as acting rationally, but that seems to omit other human characteristics such as empathy, selflessness, generosity, etc.

Overall, what are you hoping to achieve with debate topics?",hrzxrxq,t3_s01ijj,1641777740.0,False
s01ijj,"Let's say we live in a world where most work is done and managed by AI. AI controls machines/processes to mine for physical resources, build products with them, and handle logistics. All work is done by AI, except for government/ethical/ high-level decision-making stuff.

How should the benefits of that system be distributed (profit, resource wealth, product ownership)? Who ""owns"" the software assets (including resource production and product creation? Who owns ""data""? Do tech people become the new elites?

This can parallel lots of similar debates around say land property rights, the right to catch your own rainwater, the right to privacy/data, etc. in the realm of ""ownership"".

Also, if AI are treated as pets, how should we deal with ""animal abuse"" (hacking ddos stuff)? Should it fall under the same realm as cyberlaw, or is there some kind of human/animal rights situation happening?",hrz1qeg,t3_s01ijj,1641765698.0,False
s01ijj,"What is thought without emotion?   


this could be expanded to how much of human decision making is done with instincts and how much is logic, and whether that's a helpful for us in our dealings with each other, or whether an AI would be better suited for things like governance/law etc. because it's not burdened with emotions OR are those emotions the thing that keep this from falling apart completely? 

I've always wondered how much we're thinking based on what we know to be true and how much is based on an unknowable predictive engine we've developed over evolutionary time (clearly, dreaming has some element of simulation to it; could dreams not be training scenarios?). Seems like you could almost explain the anti-vax phenomenon as an expression of emotional/gut thinking vs logic and data.

Lots of directions to take this",hrz35hw,t3_s01ijj,1641766207.0,False
s01ijj,"The 2021 Reith Lectures were on the advancement if A.I. There is plenty to go on with those if you are in the UK and have a TV licence.
[BBC Reith Lectures page](https://www.bbc.co.uk/programmes/b00729d9/episodes/player)",hrzhkjc,t3_s01ijj,1641771513.0,False
s01ijj,"My major field of publication is in computational creativity: AI that exhibit behavior that would otherwise be deemed as creative in humans. (Art, music, jokes, architecture design, etc.)

If you check ICCC, you can find a good amount of philosophy on which to base topic. One HS-level topic may be along the lines of, ""Is creative AI a threat to human artistic expression/culture?"" (I.e. if an AI can potentially learn to optimize and make the best music, jokes, and paintings, is there room for humans? What if we get pushed out of the space? If the billboard hits are all written by machines, will there be a major fault in human cultural progression?) (Kind of like how the world champion of Go lost to Google's AlphaGo and quit the game because there was no point in competing anymore.)",hs0s6w1,t3_s01ijj,1641790332.0,False
s01ijj,"Lots for AI driving. Whether giving up your right to drive is good? It would eliminate more casualties but the future casualties would be random instead of bad drivers. Also, how do you value life if a schoolbus is going to hit another car. Should the other car swerve killing itself to save the children or should the car do its best to save itself?",hs0xu9y,t3_s01ijj,1641792761.0,False
s01ijj,"This is all my perspective.

Same rights as humans? depends.

Intelligence of solving problems doesn't necessarily emotional intelligence if the A. I don't have what we considered ""will"" then why would we give rights?",hs0yn7b,t3_s01ijj,1641793165.0,False
s01ijj,"If personal or sensitive data (like, say, surveillance video of private moments in a home) are only screened for illegal activity by an AI, should this be considered an invasion of privacy? (Consider the number of cameras in the home already, like laptop cameras and phone cameras and the like, which theoretically could be recording but then ignoring the data. At what point does this become an invasion of privacy? When the data is processed? When it is screened by a computer? I mean, clearly when a human gets involved it is an invasion of privacy, but if only machines see the data?)",hs1029u,t3_s01ijj,1641793901.0,False
s01ijj,"Resolved: Humanity must prioritize the development of super-intelligent AI so that it can be used to solve world problems (hunger, global warming, economic inequality, etc.).

Resolved: A super intelligent AI would make for a better government leader than a republic.

Resolved: Jobs involving repetitive and mechanical tasks should be replaced by AI/robotic systems without regard to unemployment.

Resolved: A minimum wage should be established for robot workers in order to collect taxes and fund universal basic income.",hs11w8u,t3_s01ijj,1641794894.0,False
s01ijj,"* Virtual assistant giving advice, especially when we don't yet have technology to teach it which advice is allowed and which is not (e.g. Alexa telling the child to stick fingers into electric outlet)
* Impersonating yourself with an AI to teachers/colleagues/friends/relatives without telling them, e.g. voicemail bots
* Online presence-based background checks (e.g. refused a job or school/college admission due to online posts)
* Creating and publishing deepfakes of real people—joke causing unintended consequences, revenge, dirty political competition or propaganda
* Teaching machine learning to synthesize infinite paintings in the painter's unique style, rendering the painter unneeded and uncompensated

To OP: try to avoid banal daydreamer fantasies like human-level AIs? We don't know if it will take another 2000 years to get to that stage. There are real problems banging on the door.",hs1c5p1,t3_s01ijj,1641801180.0,False
s01ijj,A.I. taking out more jobs than any technology in history within the next century.,hs1qqkj,t3_s01ijj,1641811990.0,False
s01ijj,"If any justice system decides to use AI: what safeguards, if any, should there be to protect against that AI discriminating against certain groups (ie male/female, black/white)?",hs390v9,t3_s01ijj,1641836799.0,False
s01ijj,Can a service provider collect store and sell an arbitrarily large amount of generally unspecified personalized data in payment for a finite service provided?,hsgjejl,t3_s01ijj,1642057426.0,False
s011f6,"You know Linux was/is also evolving and wasn't as it is today from the beginning?  
Nowadays, legions of programmers (many from big corporations) contribute to Linux.

Few years ago I wouldn't even dream of not having to dual-boot for gaming.  
A decade ago I've been just entertaining a thought of running Linux as primary OS.  
Two decades ago Linux was basically only for enthusiasts.  
When GNU/Linux has its release, it was supposed to be only temporary until GNU finish their own kernel.  
When Linux was born, it was just a summer project of a young student.

Linux wasn't so good at the beginning.

And neither is/was Unix for majority of users. Unix was complex system for academia and commerce (because there wasn't any other type of consumers at the time!). CS students have problems with command line, what do you expect from your average Joe from decades ago?

^(Also you may not know, but while Linux is only Unix-like, macOS **is** Unix by blood)",hrz0fc1,t3_s011f6,1641765232.0,False
s011f6,"Sure, I understand all that.

Mac OS X and beyond are based on FreeBSD UNIX, yes, but that didn't come around until 2001 or so.

Classic Mac OS 9 and earlier were not UNIX based. It didn't have protected memory or preemptive multitasking or even multiple user accounts, it was very basic and it could crash easily.

# My point is that if they found a way to make a GUI in 1983 and UNIX existed in 1969, why did it take until 2000 for a consumer OS with protected memory and UNIX-like stability to be available?  That's my REAL question.",hshwi8w,t1_hrz0fc1,1642088070.0,True
s011f6,"Well, I am just another Redditor with his own (stinky ;) ) opinion.


In the case of PC operating systems (mostly Mac OS and MS-DOS and early versions of Windows) This mostly had to do with design constraints with the hardware that they had to work with at the time. MS-DOS was a derivative of CP/M written for the Intel 8086 and the original Mac OS was written for the 68000. Neither chip had transistors that performed the functions of an MMU (memory management unit) and both operating systems were written with such hardware limitations in mind. They were also written to function in extremely small amount of memory. 

The hardware that both platforms ran on (680x0 series of chips for Mac OS and 80x86 for PCs) did evolve to include an MMU (68020 and 80286/80386) but by then the software ecosystems were very intrenched and new versions of Mac OS/MS-DOS still had to support those older chips for a time.

Contrast that with early versions of UNIX which ran on Mini-Computers which were extremely expensive for the day. Those machines had the hardware that implemented/supported memory protection. With that support in place it was much easier to build a multitasking system with protected memory.

So in short, those features are easier to build when you have hardware to help you with it. Early PC-Class CPU's lacked the hardware to easily support them. Technically I think you could make it work without such hardware if you are determined enough but I would imagine may deciding to wait for MMU support to be standard on CPUs before implementing it.


Well there is my dirty stinky opinion answer :)

EDIT: I decided to google around and I figured this link would be of interest to the OP

https://retrocomputing.stackexchange.com/questions/7740/why-was-preemptive-multitasking-so-slow-in-coming-to-consumer-oss",hs0xu78,t3_s011f6,1641792760.0,False
s011f6,"Excellent, this is a super helpful response. I really appreciate that!",hshwsub,t1_hs0xu78,1642088186.0,True
s011f6,"The first version of Windows NT (3.1) was released in 1993, less than 2 years after the earliest version of Linux. Windows NT 3.1 has everything you described. NT was largely based on the architecture of DEC VMS, and designed by members from the same team.",hs0y3fg,t3_s011f6,1641792889.0,False
s011f6,"Right, but Windows NT was not a consumer OS.

My point is if UNIX existed in 1969 and the GUI existed in 1983, why did it take until 2001 or so for us to have a consumer OS with the protected memory and stability of UNIX AND a GUI?",hshwp4h,t1_hs0y3fg,1642088146.0,True
s011f6,"Sorry, but NT was a consumer OS. It just wasn't the most common one yet. Literally everyone running windows today is running some version of NT. As for protected memory, Unix did not originally include that. Memory protection came about when MMUs were added to processors in the late 70s.",hshzumo,t1_hshwp4h,1642089350.0,False
s011f6,"Maybe I misused the term ""consumer"". What I meant was, prior to Windows XP, the overwhelming majority of Windows users were not running an NT based OS. Windows NT and Windows 2000 were not for regular users, lets not pretend they were mainstream in remotely the same way that the others were. Win 2K didn't even have USB support in the beginning. Most people weren't running it. That was my point.",hsqkcld,t1_hshzumo,1642230175.0,True
s011f6,Following the post,hryw4qu,t3_s011f6,1641763718.0,False
rzqau2,"As someone who studied cognitive science, computer science is RIPE with meaning. Theories of computation have deep implications for how we understand the mind and it’s functions (computational theory of mind). Computer science also has major implications on the field of mathematics. Godel with the help of Turings work on computation (early computer science) have proven that there are mathematical truths which are NOT provable. Some crazy ass shit. Not sure if you’d learn any of this in a CS class. I learned all of this in philosophy classes. Try and compliment CS with Philosophy and I promise you will find the field ripe with meaning.",hrxwzsp,t3_rzqau2,1641751311.0,False
rzqau2,"I'm a big fan of the work of Douglas Hoffstadter - he's written quite beautifully on the implications of Kurt Godels proof for computation and consciousness (GEB / I am a Strange Loop).

I studied some Bert Russell and Godel during my Maths Bsc. Im currently looking to start a master's in computer science this year as I'd like to learn more about how recursion and ""Godel loopiness"" is implemented to make computers. 

What I find interesting about Godel and Turings theorems is that they seem to suggest that computation and logical experiments will always be at the 'cutting edge' of human knowledge (well at least that's my very amateur interpretation and expression. I'm looking forward to developing a more sophisticated appreciation)",hrz3sac,t1_hrxwzsp,1641766433.0,False
rzqau2,"The lines between ""cognitive science"" ""computer science"" ""mathematics"" ""language"" and ""philosophy"" blur quick 

It's a very meaningful field of study",hrz4xdl,t1_hrz3sac,1641766838.0,False
rzqau2,"I'm a CS undergraduate too, I want to pursue a Computational Psychology or Philosophical masters after my degree. But I find myself utterly confused when I go looking online. Can you recommend something/give a bit of guidance?",hs0a0iw,t1_hrxwzsp,1641782672.0,False
rzqau2,"I'm not too sure what guidance you're looking for and i'm certainly not qualified to give much out. I was a pretty mediocre maths student (got a 2:1 from a Russel Group uni in the UK 5 years ago) and haven't formally studied any Computer Science, Philosophy, Psychology ... Courses. Apologies if this is unsolicited

As I mentioned above, I really enjoyed reading I Am a Strange Loop. I found Hoffstadters point of view really informative on how mathematics and the philosophy of logic can explain human cognition. Would recommend that as an entry-level book without much need for any maths or CS background. Not a massive long read (400 pages). Also has a good audiobook version on Audible - although the hard copy has some illustrations which are handy. 

For more mathematical philosophy I'd recommend just searching for things about; 

1. David Hilberts program for axiomatic mathematics, 

2. Bertrand Russel/Alfred Whiteheads theory of types (and it's criticism)

3. Kurt Godels incompleteness theorem (Nagel and Newman have a nice short book on this) and 

4. Turings theorems on computable numbers.

I don't think I have any books I'd suggest for these which are good introductions (although hoffstadter mentions all of these)  but they all have nice wiki pages 

I consider these 4 developments of philosophy from  20s/30s game changing. Turing and Church went on to use these developments to theorise modern computing and Artificial intelligence.",hs34eqm,t1_hs0a0iw,1641835121.0,False
rzqau2,"I might actually be interested in Cognitive Science but when I looked it up online cognitive science seems to be more about psychology, neuroscience, a bit of philosophy, but actually very little Computer Science or Mathematics in it… 
So I don’t know about it…",hs1lrys,t1_hrxwzsp,1641808260.0,True
rzqau2,"Yeah, cognitive science is a subject that is very hard to pin down. Its defined differently by many different people. Broadly, it is the study of the mind, artificial or real. You can study the mind in many different ways: neuroscience, philosophy, psychology, anthropology and so on. One of the more interesting properties of the mind are its computational properties. That's where AI, computer science, and mathematics come in. So you really can't have cognitive science w/o cs and math.

Its also worth mentioning that the foundations of neural networks were inspired by psychology. Also, Turing/other early computer scientists were primarily interested in computation with how it relates to the mind. But this is some really rigorous stuff that won't appear on the front page of a google search. You really just have to dive deeply into it.

I'd recommend studying godel's incompleteness theorem if you really like math and computation. 

Try reading Godel Escher Bach too!",hs8cbih,t1_hs1lrys,1641922015.0,False
rzqau2,"I feel like I'm in the same boat my friend

Although I suspect there is plenty of maths involved at its root. If you're built for it - you can find maths anywhere 🧀🧀🧀",hs34pgi,t1_hs1lrys,1641835228.0,False
rzqau2,"Sounds like you should be taking philosophy but realistically no job serves a purpose other than income, and if you're lucky, entertainment. Some people are just wired in such a way that they enjoy this subject more than others, and usually its to do with the fact that computing is very logical and simple, but very powerful.",hrwyqga,t3_rzqau2,1641737617.0,False
rzqau2,Guess I’m weird. I actually enjoy this,hryeiub,t1_hrwyqga,1641757482.0,False
rzqau2,Because it's fun..? Not really sure what you're talking about.,hrws7m5,t3_rzqau2,1641734201.0,False
rzqau2,There is no meaning and no purpose...,hrwovdp,t3_rzqau2,1641732228.0,False
rzqau2,lol same,hry6e1v,t1_hrwovdp,1641754625.0,False
rzqau2,Sure there is.  There may not be any inherent meaning but meaning can most definitely be created.,hrzo5tz,t1_hrwovdp,1641773987.0,False
rzqau2,that'll last you like 2 seconds before your brain starts flooding itself with things it needs to think about.,hs00zgo,t1_hrwovdp,1641779039.0,False
rzqau2,"There is no meaning or purpose to computer science, except what you want to attach to it.

In that sense, it's the same as everything else in life.

You can use your knowledge of computer science to earn yourself a decent income. And/or you can use it to make the world a better place for all. Nothing wrong with either and it's all up to you.",hrxh1h4,t3_rzqau2,1641745338.0,False
rzqau2,The purpose of computer science is problem solving and developing your problem solving skills.,hrxl34h,t3_rzqau2,1641746868.0,False
rzqau2,"I believe that's Engineering, not just Computer Science.",hs1x0s2,t1_hrxl34h,1641816304.0,False
rzqau2,"This is a discussion I was having with one of my mates that I studied computer science with at university. There's a massive misconception in our industry that CompSci == coding, when they are nowhere near the same thing. Coding is the practical application of writing software, whereas CompSci is the academic study of computers. That's why they teach aspects such as hardware, networking, logic etc. Lots of people go into CompSci thinking their going to learn how to code, when they're really not going to.

To answer your question about purpose is therefore a two part answer:  
\- Coding - the purpose of writing software is to a) automate processes to make people's lives easier/quick, b) to further humanity (see uses in medicine, climate science and humanitarian work etc), and c) most importantly, to make money

\- Computer science - the same as every science and academic study area, to learn more about the area and to further it. Computer Scientists study the world as it is to find more efficient ways to process data, calculate results, and generally just ""do stuff better"".",hryo0jb,t3_rzqau2,1641760826.0,False
rzqau2,"What was your meaning/purpose in enrolling in a CS degree program? If you can't answer that question with an answer more meaningful than to make money then maybe you should figure out what you want to do with your life that will give it meaning before you waste anymore time, energy, and possibly a lot of money.",hrx0pdy,t3_rzqau2,1641738576.0,False
rzqau2,"Why? Making money is a legit reason to study CS.

A career doesn't need to be the thing to supply your life meaning.",hs00xsb,t1_hrx0pdy,1641779021.0,False
rzqau2,OP is specifically looking for purpose in CS as a career path. My point was he should have already done so before choosing CS as a field of study.  As to the broader reason for choosing a career I always advise you should never sell yourself short. If you have the capacity to study CS and be successful in that career path then you have the capacity to do what ever you want. Do what you love. It's sad to see someone spend a third or more of their life doing something that does not add meaning or purpose to their life. Life is short and unpredictable. Live life without regrets.,hs03vcv,t1_hs00xsb,1641780198.0,False
rzqau2,"""should have already done so before choosing CS""

Why? Meaning, purpose, and passion (do what you love) can come later. Some people don't know what they are passionate about in college. Sometimes you only develop those traits as you go deeper into a topic. It almost has to come later because how meaningful is your work really going to be if you barely have any experience in the field? 

Maybe you try, but never find passion or meaning in a CS career (or hell, any career). But if you can work in a CS career you can save and invest a ton of money early and not need to work a third or more of your life. Then you can go do what you love and actually be able to fund it.",hs0i285,t1_hs03vcv,1641785988.0,False
rzqau2,"You learn how to use computers. Computers help automate everything.

If you don’t understand everything that we need to do in life to survive/grow etc that is another topic.",hrwsoif,t3_rzqau2,1641734468.0,False
rzqau2,">You learn how to use computers.

Computer science is not about how to use computers.",hrxovwk,t1_hrwsoif,1641748307.0,False
rzqau2,"I mean he’s literally asking about CS on the most basic level of life etc.

If how computers are built/function, the logic, foundation etc doesn’t all unlock using a computer then I don’t know what does",hrxp79r,t1_hrxovwk,1641748427.0,False
rzqau2,"He didn't say what you're using them for. Computer science is very much how to use computers to do things.

It's not ""how to use power point"" but it's still using computers.",hrxx0gx,t1_hrxovwk,1641751317.0,False
rzqau2,"Using computers is general enough to cover everything, not sure how that makes it wrong considering the question being absolutely general",hryfkgv,t1_hrxx0gx,1641757854.0,False
rzqau2,"Theoretical computer science is basically the development of theories on computing (logic gates, programming, data management etc.) - it’s basically trying to come up with new ways of doing things so that programmers (computer engineers) or hardware engineers can implement your theories and presumably benefit society with more efficient ways of doing things (algorithms) in this realm.

It’s downstream from things like pure logic, cryptography and mathematics but not quite down to systems administration or business logic programming.",hrx65dl,t3_rzqau2,1641741025.0,False
rzqau2,"There is no purpose and, even worse, there's no future in computer science. Sounds crazy, right? Technology IS the future... if, and only if, there's power... and a supply chain that can develop it but the real hole in the basket of the future of this lifestyle we've created is the need for reliable grid power in a time where the weather is getting predictably worse. 

If you're keen on data, you could get into climate modeling. Plenty of meaning to be found in that, but probably only employment in the insurance industry (if you're after the money). 

All your life you've been told that coding is a necessary skill when the people telling you that didn't think to make the power infrastructure well enough that it can be relied on in the future. 

The secret to all of this is that the only meaning/purpose anyone's life has is to consume resources as quickly as possible, to generate income, to maintain a standard of living that's fundamentally unsustainable for even 1/10th of our population. Your lifestyle is intentionally unsustainable because that's where the wealth is generated; in the burning of resources. 

Now, go spend some money and the rest of the afternoon wondering what the meaning of any of this is beyond acquiring more/better and why that's worth your short lifespan, especially considering that most resources can only be used once and that your carbon footprint will be changing conditions on this planet for a thousand years after you die. This is the machine we built. This is what you've spent your life studying to be a part of. Now it's up to you to choose something else that doesn't make the world worse just so you can have nice things.

Even the people down voting this have to admit that this cannot continue and will either change or run out of the resources it needs to sustain this level of growth. It's basic math. there isn't enough for us to live this way, so we either stop and do something else or we wait for it to break and have nothing else to fall back on. Those are the only options. This ship is literally sinking into the ocean (check out the update on the Thwaites Glacier and notice the trend that every time they check it, it's further along in its collapse than they expect)",hryzoo6,t3_rzqau2,1641764969.0,False
rzqau2,"I love this question. I do like coding but I think I like ""computer science"" more. Computer Science for me has to do with coming up with ways to COMPUTE mathematical functions and that for me, at least, is the meaning. When you program a program that can differentiate an equation, you know you're making something that has and will help in the human evolution at least in the mathematical sense. If you get what I mean-",hry3zd7,t3_rzqau2,1641753786.0,False
rzqau2,Why do you assume there is one?,hrybmbl,t3_rzqau2,1641756451.0,False
rzqau2,"I type the words, the things happening, the direct deposits hit.",hryi6l5,t3_rzqau2,1641758783.0,False
rzqau2,"You can make anything you want, and there’s more tools available to do so than at any point yet. Go make the world better! Build a company, a tool, a game, something.",hrysk3k,t3_rzqau2,1641762414.0,False
rzqau2,"The ""meaning of life"" is too abstract a concept, it should be posted on a philosophy sub.

As for CS' ""purpose,"" you should ask yourself how the ""man made"" fields of study came to be. For instance, was math invented or discovered? The general consensus seems to be that it is both; math is the quantification of naturally occurring phenomenon, but requires the invention of a proof (or something along these lines).

In the same vein, electrical engineering is the demonstration of man's mastery of a natural phenomenon (the flow of electrons). Modern computer science has grown out of electrical engineering by combining the ideas of information theory with electrical engineering. 

Now we are seeing the next evolution of this cycle; as computers become fast enough, we see blooming fields like data science and machine learning, that add more abstraction layers on top of CS that allow humans to model complex adaptive systems and gain further insights into the world around us. Basically, as new technologies are discovered, we (humans) gain greater insights into the world around us and gain progressively more tools to model those systems and look past their face value.

So there isn't really a ""purpose"" beyond providing humanity further insights into the world around us, and increasing our collective understanding. This isn't a ""meaning of life"" but the natural human inclination towards adventure and discovery is a pretty noble pursuit imo.",hrz9nt4,t3_rzqau2,1641768561.0,False
rzqau2,"For the purpose and meaning of it all, please see: [https://youtu.be/pneBKFjxInQ](https://youtu.be/pneBKFjxInQ)",hrxn36d,t3_rzqau2,1641747626.0,False
rzqau2,"CE student, here. I look at ourselves as a sort of digital version of old-style craftsmen. I love to have the possibility to wake up with an idea and to be able to create it from scratch. That is for what concerns the hobby. About the job career, instead, I can tell you that IT is one of the most potential fields nowadays.",hrxqobu,t3_rzqau2,1641748977.0,False
rzqau2,The meaning of life is to procreate so the human race can live on. Computers help in this ongoing struggle by delivering porn (knowledge) and dating apps (action).,hry086l,t3_rzqau2,1641752472.0,False
rzqau2,"I can't help but enjoy and attempt to answer such a question! I expect you'll get a lot of interesting answers.

It's pretty simple for me: I like video games (playing and making them), I like robots and I am a huge Sci Fi nerd. Studying CS for me is just studying what some of my favorite things have in common.

For those with a philosophical bent (of which I'm one), the fact that you can simulate things with computers is kind of a big deal. You make models in almost every domain. It's one of the most accessible ways to start tackling big, interesting questions. 

Zooming out further, computers are the latest in a long line of metacognitive tools. A ""Tablet"" is a Clay Tablet's spiritual successor, quite literally and deliberately. Personally, I feel that the tools we interact with are a big part of the human experience and it's neat to be at the fore-front of things. People making the first stone tools might have felt similarly empowered. I like to think that we are still at the front of the computer revolution and that there is a lot of miniaturizing and normalizing left to do. I'd like to ride my bike out into the woods, turn on my AR device, and suddenly have my home office all around me including access to the internet and haptic feedback, and that kind of thing. Right now being a computer person is often like being a person of two worlds (the person at the computer, and the person away from the computer) but I think the technology's logical extreme is a paradigm where you can do computing anywhere and any time, with few restrictions, combining the mobility of ""away from computer"" with the power of ""at the computer"" and applying this combination to every day life. It is really thanks to smartphones and tablets that things are moving in that direction. What is the next step? It's hard not to get excited about the frontier of metacognition. People must have been similarly excited about paper and ink, scrolls and notepads. 

Personally, I want to see what happens when we are applying these fundamentals not only to computers, but to cells. Cells are in theory programmable and certain principles of computing carry over. The body and mind hint at all kinds of systems and properties we could be using for other things. What will a cybernetic future with biotech look like? I really want to know. I think it could be really awesome.",hrypkoq,t3_rzqau2,1641761373.0,False
rzqau2,"Because I enjoy it, you should do or study what you enjoy. And you can help people! Make their life easier by automation repatitve and boring task :)",hrytn61,t3_rzqau2,1641762803.0,False
rzqau2,"While in school, one of my professors told me that Computer Science is ""the study of what can be computed"".",hrzkwk8,t3_rzqau2,1641772754.0,False
rzqau2,"It got me a job as a software developer and nearly doubled my income. Prior to this, I was a dance teacher - a career I was passionate about, until I realized I’d never be able to afford health care or save for retirement. I still dance, now just as a hobby.",hrztmd5,t3_rzqau2,1641776058.0,False
rzqau2,"Computer science is problem solving, you’ll learn how to apply mathematics, logic, and other concepts to solve complex/abstract problems that people working solutions to!",hs02rj4,t3_rzqau2,1641779750.0,False
rzqau2,"imo cs strengthens your problem solving skills. when you get good at problem solving, you can choose what problems you wish to solve. none of them ever have to deal with cs.",hs0w1ao,t3_rzqau2,1641791936.0,False
rzqau2,The way ive always interpreted and explained CS is as an umbrella term. It holds a ton of different concepts in it; everything from cyber security to mathematical theoreticals. Id say a majority of CS majors in college thought it was just programming until they did more research and realized it was so much more.,hs14ep3,t3_rzqau2,1641796339.0,False
rzqau2,"It sounds like need some Albert Camus, if you're not already aware of him.",hs14qiy,t3_rzqau2,1641796532.0,False
rzqau2,You can be useful to the world in a meaningful way using computer science since everything is digital nowadays. With tech you can help humanity propel towards its future.,hs1gyyd,t3_rzqau2,1641804613.0,False
rzqau2,"There is no meaning in life. Well, other then whatever meaning you give it. 

Some people choose religion as it's comfortable for them, some find other things like helping others, understanding the world, getting the most out of the 80 or so years one has (also I'm not saying these things are mutually exclusive)

I like CS, it's interesting. But I also use it as a vessel to get a job, or do other stuff.

 I'm mainly interested in game design and development but I also try to learn how game engines work, or make rudimentary ones in my own just to learn how they are engineered, the structure, the math, the complexity. 

At the end of the day, things only having as much meaning as you give to them.",hs1ha36,t3_rzqau2,1641804847.0,False
rzqau2,"For me computer science is part of mathematics. It helps us to describe the world we live in, it is a powerful tool for other sciences as well such as physics etc., where many calculations are done thanks to computers and so the whole field of computer science for me is about “automating” tasks and describing world around us using different perspective, similarly as other parts of science do. (Yes I know the debate on wether mathematics and compsci are actually a science or not, but that’s not my point here).",hs1hb0o,t3_rzqau2,1641804866.0,False
rzqau2,">I can’t quite grasp its purpose/use in the bigger meaning of life.

Theoretical CS concerns what kinds of mathematical operations can feasibly be realized in physical systems. Hard science traditionally understands physical systems as realizing mathematical functions, so discoveries in CS constrain what is feasible across the whole domain of the hard sciences.

On the other hand, CS programs were created by the computing industry to lower the costs of hiring programmers and executing computations. The field is based on very abstract mathematics with the wild implications described above, but largely designed to route students away from that stuff except as needed in business applications.",hs32161,t3_rzqau2,1641834264.0,False
rzqau2,"Computer science is the study of computers. 


For example:


- what can be automated using computers and how to do it as effectively as possible

- how to connect computers, servers, embedded systems, iot devices and mobilephones into something greater than the sum of their parts.

- making meaningful and valuable conclusions from big data, for advertisers and city planners for example.

- how to ""bring to life"" the gadgets and devices created by those in the information technology industry.



As for the purpose in life question, that is retarded. There is no objective purpose in life other than any subjective purpose you come up with.

Which is to say, why does anyone do anything? For the sheer boredom of course, to distract themselves of the fact that there truly is no meaning.




Also because they find it fun. Why does everything need to have a purpose or meaning?",hrwulw0,t3_rzqau2,1641735527.0,False
rzqau2,Computer science is not the study of computers.,hrx482x,t1_hrwulw0,1641740191.0,False
rzqau2,You're wasting your education.,hrxehec,t3_rzqau2,1641744345.0,False
rzcexe,"I agree with you that there is a lot of hype around NFTs and cryptocurrency in general. Many influencers are promoting NFTs and cryptocurrency and saying that it is a pyramid scheme without any knowledge of the subject of cryptography and distributed systems(blockchains).

Three entities are significant in the NFT world:
- File Hosting Service (IPFS, Google Drive, AWS S3, Dropbox)
- Smart Contracts on blockchains (Ethereum smart contracts using Solidity, Solana smart contracts using Rust, etc)
- NFT Marketplace (OpenSea, etc; they are the platforms that let the bidders bid and they glue together the above-mentioned entities to create their platform)

So let's imagine you are bidding for a cat NFT on OpenSea and finally buy that NFT from the creator after proposing the highest bid. All this bidding game is played on the platform. When you buy the NFT, the code inside a smart contract is triggered which adds a transaction into the blockchain. The transaction serves as proof that you have ""bought"" the particular NFT and are the legitimate owner of the same.

In reality, you do not ""own"" the NFT, you just own the link to the NFT image which is stored on another storage that may be centralized like Google Drive or decentralized like IPFS. If Google Drive decides to remove the particular image from their servers(which is unlikely but assume), you would get a 404 error.

I assume that you know about pointers in languages like C, Rust or Go. So I would explain the above phenomena with context to pointers. Imagine you have a variable `age` and you have a pointer to that variable `elonAge`.
```
int age = 60;
int *elonAge = &age;
```
Let's assume you have the access rights for `elonAge`, you have access rights to `age` but that doesn't mean that you cannot have any more pointers to `age`. You can have another pointer to that same variable called `muskAge`.
```
int age = 60;
int *elonAge = &age;
int *muskAge= &age;
```
You can read the data stored in `age` by dereferencing `elonAge` as well as `muskAge`.

Likewise, if you own a link to an image on the interwebs that doesn't mean that nobody else can't have access to that NFT image. A person named Geoffrey(forgive me if I misspelled the name) recently showed how this works by creating the [NFT bay](https://thenftbay.org) which is the NFT version of The Pirate Bay. 

He demonstrated that he can access the NFT *images* to which the owners have a link by downloading all the NFTs and torrenting them. I heard his talk in a podcast with Coffeezilla iirc. The NFT Bay is now banned in most countries though.

NFTs are created by hosting your artwork on a file hosting server and then hosting the link to the image on a blockchain via the logic written in a smart contract.

Edit: fixed formatting",hrxb9pq,t3_rzcexe,1641743093.0,False
rzcexe,Thank you! For explaining it really well!,hrxoo0n,t1_hrxb9pq,1641748224.0,True
rzcexe,"Suppose you're married. Everybody in the world is banging your wife, but you can't do anything about it. But you have the marriage certificate, right? That's the NFT.",hrvdg8p,t3_rzcexe,1641701185.0,False
rzcexe,Genius.,hrvgshu,t1_hrvdg8p,1641702784.0,False
rzcexe,"Ah, I wish I had a free award to give you today.",hrwt131,t1_hrvdg8p,1641734663.0,False
rzcexe,Sick world,hrxcqq3,t1_hrvdg8p,1641743679.0,False
rzcexe,A gentleman and a scholar,hryvn8y,t1_hrvdg8p,1641763542.0,False
rzcexe,[deleted],hrvgs9o,t1_hrvdg8p,1641702781.0,False
rzcexe,I guess that joke hit a little too close to home for some people,hrvr86j,t1_hrvgs9o,1641708415.0,False
rzcexe,"My apology for the ignorant joke. I think I will be a silent reader for a while on this sub to reflect and try making up for it 🙇‍♂

I also forgot that this sub is educational.",hrwlntc,t1_hrvr86j,1641730171.0,False
rzcexe,"To clarify, NFT's are not the image. NFT's are the hash which links to the image.

The images are then hosted on standard web hosting platforms. If those web platforms take the image down, then your NFT will no longer load the image. If the NFT links to a copyrighted material and there's a DCMA claim, then your NFT will no longer link to the image.

NFT's are really just a claim in the blockchain to own a piece. Anybody can right click and download the piece. Anybody could then do whatever they want with the image. Whether that claim to own the piece actually has value is in the ye of the beholder.  


The reason you don't actually put the image on the blockchain is it's prohibitively expensive. Each transaction of the blockchain has a cost, and storing an image is *way* to expensive as compared to just a link to the image.",hrukmf2,t3_rzcexe,1641688619.0,False
rzcexe,"So what is the point? It seems all hype and woo-woo bullshit to me, pushed mainly by people with a stake in getting rich off of it.

Why would anyone want to buy one? It's not like owning real art. So it's just...hype?",hrvgvzi,t1_hrukmf2,1641702833.0,False
rzcexe,"> So it’s just hype

Yup. It’s just people trying to ride the crypto wave by pushing this as a new way to collect art.",hrvn06m,t1_hrvgvzi,1641706043.0,False
rzcexe,"Ok. Cool. That was my take,  but wasn't sure if I just didn't get it.",hrvnkmt,t1_hrvn06m,1641706354.0,False
rzcexe,"I want to add that there are financial implications of NFTs as well. They're very hard to tax and with the hype around them, their prices inflate a lot. That means wealthy people can invest in NFT, drive prices up (Elon musk's dogecoin to the moon wasn't directly NFT related but still  a good example of crypto market inflation) then sell go turn a profit on something that can't be taxed. 

Of course it's less stable than holding money in a bank or traditional stocks, but since legislators don't really understand them you get away with more.",hrxcewl,t1_hrvnkmt,1641743554.0,False
rzcexe,"Generally, people dont understand what the NFT is. They just see the image and think that's the NFT.",hrvybwg,t1_hrvgvzi,1641712816.0,False
rzcexe,"When people explain NFTs with urls to images it kind of muddies the purpose of an NFT.  It's just independently verifiable digital ownership of something.  The ability to download that something is dependent on there being someone who can distribute it to you, but that doesn't have to be one single source.  Let's say you buy an NFT song from a music artist.  It's not a URL, it's just a code of some sort.  You might be able to download the song from Amazon music, iTunes, Bandcamp, or any number of other distributers if they have a system in place to verify ownership using your NFT.  And you could give/sell that NFT to someone else if you wanted to.  Obviously this is extremely beneficial to the consumer and not the distributer so it will probably never happen, but that doesn't mean the idea is pointless.

Unfortunately NFTs are pretty much only associated with digital art at this point and used to either make money or launder it.  All the interesting use cases seem to be things that are unlikely to happen because they mess with the bottom line of established services.",hrvnlk2,t1_hrvgvzi,1641706368.0,False
rzcexe,"To be technically correct, it's independently verifiable ownership of a few bytes of data stored on the blockchain, usually a URL. The only guarantee you get is that each NFT belongs to exactly one wallet. Here are some guarantees it can't make, however:

* That the person controlling the associated wallet legally owns the thing the NFT links to
* That any specific person controls the associated wallet
* That the data stored in the NFT actually represents something useful (Many of them are only meaningful in the context of a centralised website or service)
* That URLs in NFTs will always link to the same thing (or anything at all, in fact - many NFTs already link to dead pages)
* That trades involving this NFT weren't done through scams or hacks
* That there aren't multiple NFTs linking to the same thing, each belonging to a different person

If every step involved in making a transaction on the internet is a chain, then crypto is about making one link in that chain as strong as it possibly can be. Crypto people will tell you they have the strongest chain ever created and back it up by just describing that one link over and over again and trying their hardest to ignore anyone who points out that the rest of the chain is made of paper and wishful thinking",hrwme1v,t1_hrvnlk2,1641730669.0,False
rzcexe,"Some good points, but also I don't see anyone claiming that NFT represent legal ownership, or that people can't have their wallets hacked/scammed.

>That there aren't multiple NFTs linking to the same thing, each belonging to a different person

Yep and this is specifically a problem with NFTs for collectibles.  You have to trust that the people selling the NFTs won't just decide to sell more of that supposedly unique or rare item.  This wouldn't be a problem in the NFT music example I used.",hrx935w,t1_hrwme1v,1641742228.0,False
rzcexe,"> I don't see anyone claiming that NFT represent legal ownership, or that people can't have their wallets hacked/scammed.

No, they tend not to talk about this part",hrxi3ry,t1_hrx935w,1641745728.0,False
rzcexe,"It’s not really ownership either, because ownership is a legal concept and afaik no major country has ownership laws that recognize NFTs. Neither ownership laws for physical items nor intellectual property laws really fit to NFTs.",hs0x850,t1_hrvnlk2,1641792464.0,False
rzcexe,"Right, the laws haven't really caught up yet.",hs0yb0i,t1_hs0x850,1641792994.0,False
rzcexe,So if the NFT proves your ownership can you then license the song or image or whatever and make money off it? Can you sue someone who is infringing on your ownership?,hrxezm4,t1_hrvnlk2,1641744544.0,False
rzcexe,"No. It could be one part of a very large system we could develop to allow that, but that system doesn't exist today (well, it sort of does, it just works better and doesn't need crypto or blockchains to work), and NFTs themselves can't solve more than a very small amount of the problem on their own regardless.

Let's say you want the exclusive rights to publish a book. Imagine the US copyright office were to print out your registered copyright and mail it to you. The laws of physics prevent another person from having that exact same printed copy as you, and NFTs do the same for the digital version. The Copyright office could turn around tomorrow and print a second copy of the exact same document and mail it to someone else though, in which case it would be really hard to prove which one of you actually owned the rights to that book or whatever. And in the NFT world, we can just create a second token to the same URL and certify that someone else owns that one. The system prevents two people from having the same ""paper"" claiming ownership. It doesn't prevent two people from having different ""papers"" each claiming ownership to the same thing.

And that's just one problem. A second problem is that no one really recognizes any of this stuff as meaning anything. So maybe you are the only person who owns an NFT that supposedly represents that book. I can just sell copies of the book myself anyway, and there's no mechanism for you to stop me. No government or police agency cares about NFTs. 

You can solve these problems. You can empower some centralized broker to track and enforce everything. But then what's the point of the crypto aspect? If you have a trusted third party, just turn the mining farm off and be done with it. We've been doing all this stuff without crypto for hundreds of years. The US Copyright Office does in fact exist today, and unlike anything in NFTs, there are enforcement systems in place.",hrxn0ew,t1_hrxezm4,1641747596.0,False
rzcexe,"I think that's incorrect though. 
[Non-Fungible](https://nerdschalk.com/what-does-non-fungible-mean/) is a term that has been used in property law for many years, so in order for an **NFT** to be an **Non-Fungible Token**, it cannot by definition be duplicated. 

In your example of creating another token to link to the same URL as the one printed the day before is a really poor use case. I mean its barely worth linking a QR code to a URL. Because **NFTs** are unique objects, the only use cases that justifies their creation is representing or being those unique stores of perceived value - *in some cases an NFT actually endows a once Fungible object with Non-Fungibility*. 

This is why a lot of the use cases are in the arts and entertainment domains but *there are others*...I promise there are. 

What you mention though does make me think about the encoding of an **NFT**, particularly its association with a real-world object. What convinces the guy who just bought an expensive NFT, that another token won't be made the next day to link to his new asset (represented by his token)? 

The fact that the blockchain is immutable, cannot be altered in any way and if someone were to try linking another token to the asset, it would result in some kind of error (probably logical) because there'd already be an association registered on the [distributed ledger](https://www.techopedia.com/definition/30246/blockchain) with whatever that asset was. In the cases where an NFT only represents a fraction of an asset, those details would already be coded into the ledger even prior to all of the tokens being *minted*. I don't know though, this is an assumption from my mental model I've yet to verify.

I like art, art is cool and I think it's necessary for society but the use cases that excite me are the ones in which actually, an NFT can now come to represent a **title deed** or someones **last will and testament** and negate the ever present risk of the deeds building burning down or military coup resulting in a total rewrite of relevant country's land ownership.

Plus...I think the primary purpose of the blockchain and its offshoots is to enable peer to peer transactions on an industrial scale without that 3rd party, that almost always adds nothing to the value/utility of the transaction except for a fee and other things. I get that they were/are necessary for providing **the trust variable** to a deal between strangers, but over the years they have gotten a bit too ""big for their boots"", stretching their original mandate and becoming more of a pervasive force so some change in that regard would be nice (namely decentralization which I approximate as more power to the people, and stronger links between them).",hrytiej,t1_hrxn0ew,1641762753.0,False
rzcexe,"There's no error because the asset isn't the NFT or the URL. The asset is the thing hosted at the URL, and I can duplicate the shit out of that, host out at my own URL, and have an NFT showing I own mine.",hryv2mf,t1_hrytiej,1641763330.0,False
rzcexe,"In that example I don't mean you own the rights to do whatever you want with it.  You own an authorized digital copy of the song that you can download and listen to.

But in the case of using NFTs to own a license, there would need to be laws in place to protect that, if it isn't already covered.",hrxgvyu,t1_hrxezm4,1641745278.0,False
rzcexe,"I'm still confused. If the NFT doesn't prove that you own rights to the underlying thing, what do you own? What's to stop additional perfect digital copies of the exact same thing from getting ""authorized"" and sold?",hrzf1a8,t1_hrxgvyu,1641770554.0,False
rzcexe,"So take NBA Top Shot for example.  They take a highlight of an NBA player in a game and mint that moment.  But they don't just mint one NFT for it, they mint thousands.  And then they put them in highlight packs(think trading cards) and people buy the packs.  There will be thousands of owners of that one NBA highlight.  Each owner has a unique token that verifies that they do own a copy of it.  The even though every token is unique, they are still identifiable as being linked to that one NBA highlight.  The NBA Top Shots marketplace can verify this and allow people to sell the NFTs they own.

The common misconception is that since the NFT is unique, then the thing it represents ownership of is only going tied to that NFT.  In reality, the minter of the NFT you are buying might promise that it's the only NFT being minted for that thing, but they could just lie and mint more of them.  But then they sever any trust they have with their audience and effectively kill their own brand.  And there are may be protocols in place in NFT marketplaces to prevent that(idk I've never used one), but that's not inherit to all NFTs.",hrzps4l,t1_hrzf1a8,1641774613.0,False
rzcexe,That makes more sense. Thanks for explaining it. I realize I don't think I ever want to buy one though lol.,hs07s7v,t1_hrzps4l,1641781775.0,False
rzcexe,Also a related sports memorabilia anecdote I thought of. I had been saving all my baseball and basketball cards from the 90s for the last 30 years through 4 states and about a dozen moves because I thought they would become more valuable over time. I just gave away the whole lot to a collector because they weren't really worth the time to go through them and try to sell cards for a buck or two. I have a feeling NFTs are going to end up in the same dustbin of history. But I'm often wrong about things like this so they could go the other way too.,hs9uadn,t1_hrzps4l,1641941879.0,False
rzcexe,"Some expensive art sits in warehouses and gets traded without anyone looking at it. It's all for speculation, and money laundering.",hrwbwqd,t1_hrvgvzi,1641722787.0,False
rzcexe,"NFTs can also broadly describe anything that has a sense of uniqueness. For example, some games use NFTs to represent in game objects so players can buy/obtain those NFTs and trade them with other players.",hrvobpy,t1_hrvgvzi,1641706765.0,False
rzcexe,"Which is generally pretty pointless. Remember, NFTs aren't just bits that say you own something. They're part of a blockchain system, and the whole point of a blockchain is decentralized control. 

If you have a trusted third party, you don't need crypto, blockchains, or NFTs even in the best of cases. It's the world's stupidest way to implement

    update owners set owner_id=9876543 where item_id=1234567

in the third party's system. And what sort of practical game do you expect to support players trading rights to items in a way that it's important that there be no trusted third party. The entire world is running on a trusted third party system. If you don't trust the game server, nothing matters anyway.

The only way you're really going to use this is just to say you did. It's basically performance art to make a game that uses NFTs to track ownership.",hrxpc0v,t1_hrvobpy,1641748476.0,False
rzcexe,"Perhaps a game item isn't the best example. ¯\_(ツ)_/¯
I'm a huge crypto nerd and I like the space a lot. So I'm biased. I see a lot of utility in NFTs though not really as jpgs or game items. I think those sorts of NFT projects are fun but they're really just digital status symbols at best and Ponzi schemes at worst.

There's certainly NFTs with utility that I find very interesting.

One project I've been looking into utilizing is the Superfluid Protocol which allows you to create an NFT that serves as a landing point for a stream of money. You can create a stream of funds (say the stream sends .000001 ETH per second or something) between address A and address B. If address B is an NFT, you can trade that NFT to another person and they will begin collecting that stream of ETH every second.",hrxuni4,t1_hrxpc0v,1641750443.0,False
rzcexe,I don’t understand why would you want to generate such streams of money? Are you thinking  of such things like bond payments?,hryrxe2,t1_hrxuni4,1641762195.0,False
rzcexe,"There's tons of different use cases. One example that superfluid protocol has on their monorepo is the NFT Billboard. They have an ad space on a website. Initially this ad is blank. A user visiting this website sees a ""See your ad here by initiating a stream!"" You click on the button and initiate a stream. You have that ad there until another user comes along and opens a stream at a higher rate towards the ad space. If another user opens a stream that is at a higher rate than yours, yours automatically stops and your ad stops showing.

They also have some other examples for auctions, lotteries and call options utilizing streams but I haven't explored them too much.",hrz8ada,t1_hryrxe2,1641768065.0,False
rzcexe,"The only important value is perceived value

Also money laundering",hrwiylv,t1_hrvgvzi,1641728240.0,False
rzcexe,">It's not like owning real art.

How so? Why do people own original art in the digital age of perfect replicas? What's the intrinsic value of an original piece of art over a print/copy?",hrycmh4,t1_hrvgvzi,1641756805.0,False
rzcexe,"I'd say you're overlooking ways in which it is quite similar to the art world. The original Mona Lisa is deemed to be of immense value by society, but now consider a copy which no average person could tell was different from the Mona Lisa. This is inexpensive to make, and would probably sell for a couple hundred dollars. Based on this, we can see that tens of millions of dollars of the Mona Lisa's monetary value aren't contained in how it looks. Rather, they're contained in that it is THE original. Using NFTs, one can create a similar ""THE original"" property for a piece of digital art. Namely, they issue a single token for the data of their art. From then on, the owner will have cryptographic-blockchain proof that they are the sole owner of the original token.

Now, considering that NFTs are like art, that makes it so they are a very natural space for bullshit and hype. However, this is just the same phenomenon that takes place in the modern art world. A painted red canvas is worthless, UNLESS it was painted by this one famous person, for example. I personally think that valuing something for being THE original, rather than how it looks, is irrational, both in NFT space, and in real-art space. However it isn't like NFTs are all built on nothing and will pass as a phase. There is just as much merit in valuing NFTs (and therefore reason to expect them to continue to be valuable) as there is in valuing original pieces of art. One can further distinguish whether the NFT has artistic value (ie the apes vs some world renowned digital artist's painting). This is just equivalent to the distinction between a collectible card and a piece of art.

Lastly, it's true that much of paintings' value (but very little proportionally in the case of the immensely valuable ones) is from looking cool. NFTs don't contain any of this value, whereas physical art bundles together the two types of value. This is a meaningful distinction. However, to reject the value of NFTs is to reject the second type of value art has. I reject both as a rational thing, but am willing to accept that society will consistently value the latter, making investment in it feasible.",hryduem,t1_hrvgvzi,1641757243.0,False
rzcexe,"I'm somewhat new to the computer science field but my understanding is that NFTs (or smart contracts in general) use a different back end. Instead of hosting on a back end service provider like AWS, they use IPFS (Interplanetary file system) which is similar to a file sharing service like bittorrent. The idea being that as long as 1 person on that network is hosting that file you'll always be able to retrieve it using your digital claim to the NFT address essentially.

But yes, the prices are 100% speculation with some hype sprinkled on top",hrwscun,t1_hrvgvzi,1641734283.0,False
rzcexe,">put the image on the blockchain

There is more than one blockchain, isn't it ?

Afaik, every crypto currency uses its own blockchain. Which one is used for NFT ?",hrwf5k3,t1_hrukmf2,1641725328.0,False
rzcexe,Ethereum,hrwmhlp,t1_hrwf5k3,1641730733.0,False
rzcexe,"Every cryptocurrency does not use it's own blockchain network. There's a lot of blockchain networks though - Ethereum, Polygon, Solana, Avalanche, Fantom, just to name a few. And some of them aren't even true ""blockchains"".

You can host NFTs on any of the above networks and many more.",hrz8usw,t1_hrwf5k3,1641768274.0,False
rzcexe,"I'll add many people see nfts as solely images or hashes but that's definitely not the piece with the most use cases.

- NFTs have different types and aren't all just meant for images",hryg3fk,t1_hrukmf2,1641758042.0,False
rzcexe,This is such nonsense. I honestly cannot wait for interest rates to rise to wash away all the BS and zombie companies doing absolutely useless work.,hrw5oi8,t1_hrukmf2,1641718041.0,False
rzcexe,"What about the comparison with game skins? Csgo, people open chests and sometimes you can open an ultra rare skin worth a lot of money.

The skin itself is owned and hosted by the company valve, so what does the owner of the skin actually own? Only the 'link' to the skin for their use.

What brings that Csgo skin value?

I think based on game cosmetics existing and being worth money in our current environment, it is proof that there is some use case for NFTs.

Whether that's true for real life applications to things like art collection and celebrity autographs remains to be seen, but I think it's a really ignorant thing to make a general statement like 'NFTs are a scam' because you don't actually 'own' anything because it lives on a domain outside of your control.

The same theory on nfts being a scam could also be applied to money and online banking. Inb4 people start screaming at me that the world economy is going to collapse because everything is fake.",hrwqde1,t1_hrukmf2,1641733134.0,False
rzcexe,"Ok, sure, you could have an NFT that links to a game skin, but ultimately CS:GO could still decide if it wants to let you use the skin in game - which defeats the purpose of the decentralization if a centralized authority could have the final say in the end. In a NFT game item platform you wouldn’t have ownership of your items in a meaningful way.",hrwz5kk,t1_hrwqde1,1641737824.0,False
rzcexe,"> The skin itself is owned and hosted by the company valve, so what does the owner of the skin actually own? Only the 'link' to the skin for their use.

The skin would not exist without Valve, and ownership is maintained at a significantly lower cost (both for the company, and in terms of electricity/compute usage). It is literally impossible for other people to use your skins without you giving it to them, whereas an NFT tells everyone that you ""own"" some link, but does nothing to actually enforce the practical things we usually care about with ownership.

> What brings that Csgo skin value?

The skin itself is valuable.

> I think based on game cosmetics existing and being worth money in our current environment, it is proof that there is some use case for NFTs.

People have bought into all sorts of bubbles in the past that ended up worthless, or into financial schemes that ended up being scams. Why shouldn't NFTs be the same?

> The same theory on nfts being a scam could also be applied to money and online banking. Inb4 people start screaming at me that the world economy is going to collapse because everything is fake.

NFTs aren't a scam because people realize that we made up society ourselves, NFTs are a scam because people make big promises beyond what the actual technology is doing for you. You have to buy into lies or be stupid to think NFTs are worth your money, or be [one of the people actually making money off of the scam](https://www.bloomberg.com/news/articles/2021-12-06/small-group-is-reaping-most-of-the-gains-on-nfts-study-shows).",hrx0vpu,t1_hrwqde1,1641738660.0,False
rzcexe,"Skins have no value and are only bought by children with no concept of money and ""whales""  people who are incredibly rich and $5000 is the same to them as a penny to me, or people hopelessly addicted to the game who bankrupt themselves on it.  Its not every players buys $30 of skins, it's 2% of players spend $300,000 on skins. They are not a viable business model.  Europe is legislating them out of legality, and hopefully he rest of the world follows them.",hrx6q02,t1_hrwqde1,1641741268.0,False
rzcexe,"Are you suggesting that a twenty and two fives is worth less if it came from a rich person with questionable taste than if I got it from ""normal"" people? 

$30 is $30. You seem to be ranting against the concept that the market value of something is a concept that exists, and things are actually worth what you feel they should be worth instead. Not sure what to say to that.",hrxqlbi,t1_hrx6q02,1641748946.0,False
rzcexe,"No im suggesting that 

>They are not a viable business model.  Europe is legislating them out of legality, and hopefully he rest of the world follows them.",hry7jkd,t1_hrxqlbi,1641755029.0,False
rzcexe,"See [my other comment](https://www.reddit.com/r/computerscience/comments/rzcexe/can_anyone_explain_nfts_from_computer_science/hrxpc0v/). 

Digital bits that represent exclusive ownership of digital assets absolutely have value. But nothing in your example benefits from a blockchain. Valve can and should just store the fact that you own that very rare skin through a normal database record like any other system.",hrxpymr,t1_hrwqde1,1641748713.0,False
rzcexe,"Never said it was a scam. I said 

> whether that claim has value is in the eye of the beholder

When I buy a CSGO skin though, nobody can use that skin. That's why people buy them, to show them off in-game.

When I buy an NFT, somebody can just right click and download the image. Sure, it's not the NFT. How many people know that, how many will verify I don't own the digital rights to the image? A tiny fraction.",hry06kh,t1_hrwqde1,1641752457.0,False
rzcexe,"I said the idea of applying NFTs to things like art collection is questionable to me, but just as you can buy a replica of a Monet painting and it is worthless, there is an idea behind the value of an NFT that is derived by participating parties willingly participating in the system.

I am not saying I agree with the idea, but what I mean is that I think I can understand why people are buying into it and I can see the potential for success.

When I originally got into bitcoin many many years ago, people kept laughing at me and nobody took me seriously when I tried to get friends to buy bitcoin.

The concept of having purely digital money was absurd, 'like how do you know someone won't just change the bits in your wallet and then you have nothing?'

It was a poor understanding of abstract values like what really made the cash in their wallets able to give them purchasing power moreso than the idea of blockchain itself, so.... Yea, what I wanted to point out in my first comment was that it's too early to say anything bad about NFTs. The jury is still out, but if there is any area blockchain could be applied, it's areas like art and creative content where value is completely immeasurable.

Would you pay millions of dollars for some paint / carbon molecules on a canvas? It sounds absurd, but some people do.",hs0l5gr,t1_hry06kh,1641787291.0,False
rzcexe,Usually the NFTs are stored on IPFS instead webservers so nobody can take it down,hrwbtl1,t1_hrukmf2,1641722719.0,False
rzcexe,Usually? I’m pretty sure that’s the exception to the rule.,hrwmgw7,t1_hrwbtl1,1641730720.0,False
rzcexe,"I couldn't tell you the percentage. All I know is there are a fair amount still hosted on traditional media sites given the amount of NFT's that ""disappear"".",hry19ly,t1_hrwbtl1,1641752836.0,False
rzcexe,Interesting to even know that putting an image on blockchain is possible. I did not know this.,hrxork6,t1_hrukmf2,1641748261.0,True
rzcexe,"You can put _anything_ on a blockchain. It's just a matter of cost and time, so some things just aren't _practical_ to put on a blockchain.",hryh4ly,t1_hrxork6,1641758408.0,False
rzcexe,I think there’s a maximum block size which limits what you could put on.,hrysha3,t1_hryh4ly,1641762387.0,False
rzcexe,You can always create a blockchain with a larger block size though. It's just a matter of practicality.,hryy728,t1_hrysha3,1641764442.0,False
rzcexe,"Sure but that’s the practical aspect of it. The size of the whole chain needs to be a practical size since anyone who wants to verify it needs to hold the whole damn thing.

There’s limits on how big the block size could be, due to the fact we are limited in the real world.",hryylrx,t1_hryy728,1641764587.0,False
rzcexe,Which is why the actual images are never stored on the Blockchain.,hrz4ydc,t1_hrysha3,1641766848.0,False
rzcexe,"> The reason you don't actually put the image on the blockchain is it's prohibitively expensive. Each transaction of the blockchain has a cost, and storing an image is way to expensive as compared to just a link to the image.

Math/TCS guy here :) i've been looking more into blockchain's it seems like some of them don't have the same transaction's costs as Ethereum/Bitcoin such as Cardano. Honestly instead of NFT's if it where custom-created trading card's put on the blockchain I could see it it working",hrycj9g,t1_hrukmf2,1641756773.0,False
rzcexe,"A dumb question, but if ur NFT was took off by hosting platform, does that mean ur money just disappeare too?",hs1ob5y,t1_hrukmf2,1641810175.0,False
rzcexe,There is no hosting platform for the NFT. It's stored on whatever blockchain. Are you referring to the image?,hs5nfam,t1_hs1ob5y,1641870292.0,False
rzcexe,yes that's what I mean. sorry for my stupidity,hs69qfz,t1_hs5nfam,1641880630.0,False
rzcexe,"I'd assume that it losses value, though it depends on whether buyers think they're buying image or understand what they're actually buying.",hs6f7mi,t1_hs69qfz,1641884047.0,False
rzcexe,Thanks!,hs79spp,t1_hs6f7mi,1641906340.0,False
rzcexe,You are buying a hash of an image combined with a URL to a web server of the image. That’s it. They load the hash to the blockchain for ownership record.,hrutfqe,t3_rzcexe,1641692342.0,False
rzcexe,"Minus the “hash of an image” bit.

Here’s a guide: https://docs.alchemy.com/alchemy/tutorials/how-to-create-an-nft",hrv7qzw,t1_hrutfqe,1641698589.0,False
rzcexe,"If they don't use a hash of the linked content, is there anything preventing someone from creating a new NFT of an existing NFT's image reuploaded?",hrvm0i5,t1_hrv7qzw,1641705503.0,False
rzcexe,"There’s literally nothing stopping you from doing exactly that. There’s also nothing stopping the server that hosts the image from changing said image, deleting said image, breaking the link, or shutting down. 
An NFT is just a url that lives in the blockchain",hrvy1gu,t1_hrvm0i5,1641712623.0,False
rzcexe,There are NFTs of unhashable things as well. Like that Banksy painting that got burned after minting.,hrwc2ql,t1_hrvy1gu,1641722916.0,False
rzcexe,"I swore they used a hash to create an ID. Oh well, just use Opensea and upload your stuff.",hrv9lx6,t1_hrv7qzw,1641699429.0,False
rzcexe,"Lmao why not actually hash the image? Also, if it's just on ropsten, that's even worse.",hrx2e3p,t1_hrv7qzw,1641739367.0,False
rzcexe,Images are just scratching the surface of what you can do with NFT’s.,hrwy6cw,t1_hrutfqe,1641737337.0,False
rzcexe,"I also have a question.

The price of the NFTs are defined by supply and demand. If a lot of people buy a specific stock, for example, there's a lot of demand for that stock and the price will automatically go up. That's because people are buying it.

But, if NFTs are unique, how exactly the supply and demand are defined?",hrvqhnu,t3_rzcexe,1641707990.0,False
rzcexe,"The same as with traditional unique items, like art pieces or unique stamps or John Lennon's glasses etc. Supply and demand still applies, but supply is 1. Therefore the price will become whatever the highest bidder is willing to pay.",hrw9pzc,t1_hrvqhnu,1641721072.0,False
rzcexe,"That is precisely the ""non fungible"" part. Stocks are not unique. It makes no difference which particular share you haber in a company.",hrx8rje,t1_hrw9pzc,1641742098.0,False
rzcexe,For most projects there’s like 5000 pieces that are randomly generated. Some are more rare than others because of traits being more rare.,hrxwwg0,t1_hrvqhnu,1641751276.0,False
rzcexe,"The technical part of it is very similar to the idea of cryptocurrency (and works on the same technology of storing a ""ledger"" in a decentralized, online, data structure).  
The thing that distinguishes NFT from cryptocurrencies is that in NFT we give (completely theoretical) value to each individual ""link"" in the chain, whereas in cryptocurrency we give value to a numerical amount of ""links"" like we would with regular currency.

There is no real authority for NFT market shoppers to claim ownership of digital data, that ownership is not technically recognized by any institution in the world (AFAIK) and so basically people who participate in that market just choose to acknowledge that ownership (I believe that mostly because the media decided to play along out of greed, and failed to inform people that it's pretty much make believe, it's like issuing a certificate of authenticity for a molecule of oxygen, sure you could probably capture it, and you might even be able to prove that the tube you keep it in actually holds that same molecule, but as long as no country acknowledges your ability to own said molecule, it's meaningless.

Now, when talking about cryptocurrencies (say Bitcoin) there is usually some form of computational power required to operate the chain, store it's data and deliver it, so if you join in that effort you are essentially rewarded with some fraction of the currency (at least nowadays). The original idea, as I understand it, was a formula (take for example the calculation of pi), that was very computationally intensive, and if you joined in, you had a chance to be the computer that actually got the next number of pi in the calculation - this was called ""Mining"". That number was unique (in terms of it's position in the ""chain"") so when you got the credit for it, you could actually say that your bitcoin is bitcoin number XXX.

When talking about NFT's, pretty much anyone can choose to create a new NFT out of any unique identifier of digital data (specifically hash value), that unique value is stored on the chain, and so when it's ownership transfers it is registered on that chain - that chain of ownership is what the economy of NFTs is based on, you want to own something (as intangible as it is) that was owned by someone else important, or that has some digital history to it, and with blockchain that history, and connection to the original hash value is verifiable",hrwuoth,t3_rzcexe,1641735571.0,False
rzcexe,NFT’s most basic and pointless aspect is a art photo the real use would be proof of ownership for a car or a house or a concert ticket,hrvoajn,t3_rzcexe,1641706747.0,False
rzcexe,Thank you for this. Everyone gets so caught up in hating nfts bc of the shitty apes and I think its stopping them from realizing there's actual use cases for them out there,hrxktoz,t1_hrvoajn,1641746767.0,False
rzcexe,"So what’s the use case?  Let’s say there is a blockchain out there where I own an NFT that “points” to a car.  Does it point to the VIN number or something else?  

Now let’s say someone steals my car.  Now what?  Do I call the police?  If so, how has the addition of a blockchain improved this situation in any meaningful way?  

So let’s say the officer asks for my title to verify that I actually own the vehicle.  Now I fire up meta mask and show the officer my NFT.  That officer laughs his ass off and leaves.

Of course you may be thinking that in this hypothetical world that we’re talking about, the legal system recognizes NFTs as as legally binding ownership of arbitrary assets and investigates the theft and ends up finding my car and returning it to me.  What advantages did having my ownership residing on the blockchain give me that we don’t currently have?",hs1ymw9,t1_hrxktoz,1641817298.0,False
rzcexe,Is there any reason that all has to be in a decentralized system like blockchain? My local government has a big database of car registrations/ownership that seems to work fine. I just don’t see what problem these blockchain solutions are actually solving,hrxymhu,t1_hrvoajn,1641751904.0,False
rzcexe,"Ever lost your car title? It’s a pain and cost money to replace NFT solves this. 

Ever seen a company get hacked with ransom ware and not be able to anything? NFT data can’t be hacked. 

Ever lost your concert tickets NFT solves that too. 

Decentralized is always better no matter what the application is.",hry26dx,t1_hrxymhu,1641753158.0,False
rzcexe,"NFT doesn’t solve it; because you loose you title for your car you can apply to get another one. You lose the private key to the NFT; you’re ducked.

Same with a concert ticket, NFT doesn’t help.

Decentralisation moves the burden of security from companies or governments with resources, to individuals without resources or knowledge . Not only that, it removes any ability to fix issues. You’ve lost your key to NFT, Bitcoin, you’re shit out of luck.  Someone breaks into your wallet somehow, boom everything is gone. 

Decentralisation is always worse, no matter the application.

Also, NFT can’t be hacked! Come on! Security is always a shifting window; no Cryptographic process is 100% secure; which is why key lengths increase every so often.",hryv6qh,t1_hry26dx,1641763373.0,False
rzcexe,Umm did you actually say the blockchain can be hacked? 🤦‍♂️,hryya9m,t1_hryv6qh,1641764473.0,False
rzcexe,"Yep. Everything can be hacked.

If you think something is 100% secure and always will be - oh sweet summer child.

There’s a great story in either applied cryptography or secrets and lies. It’s about the DES algorithm; this was developed by IBM, but was sent to the NSA. They sent it back with some modifications, but nobody knew *why* they were needed. Years later, a researcher published  new form of attack - which it transpired that DES in its original form was more suspectable to. The NSA had modified it because *it already knew about* the attack.

The question is; what do you think the NSA knows about the algorithms utilised in the current implementations? Who knows, but I’m sure it’s more than the outside world.

The network is *secure* as long as enough nodes aren’t compromised; if enough nodes are compromised then the network is foo barred. The network has to be large enough to reduce the possibility that a single attacker could compromise enough of it. We know that computers can be hacked (it was one of your reasons for liking NFTs) so these nodes could be compromised. It would take a huge amount of resources to comprise enough nodes - and this isn’t worth it. But it’s possible.

However the likely attack is attacking the end user, stealing the wallet and all it contains. What’s brilliant about this; it’s it’s totally your fault and all your money/possessions are now irreversibly lost. 

Like I say nothing is 100% secure. To claim otherwise is just false.",hrz1vzg,t1_hryya9m,1641765754.0,False
rzcexe,If you think the blockchain can be hacked you must not understand how it works 😂,hrz22pb,t1_hrz1vzg,1641765821.0,False
rzcexe,"That’s weird; having read the original white paper, and done a few courses on it, pretty sure I understand it.

Unlike yourself who providing absolutely no substance to your claims. Just single sentences and emojis.

The network is secure only if enough nodes reach consensus on which is the correct chain; the security of the network is based on it being infeasible to compromise enough nodes; or indeed to expensive to flood the network with new nodes.

The security of it; completely depends on trusting that enough people are honest. 

So yes it can be hacked, but doing so would require so many resources that really only a government could do it. Although potentially this would cause a fork, followed by a complete loss of trust in the network.",hrz4smj,t1_hrz22pb,1641766791.0,False
rzcexe,😂 yeah take over the ETH network you only need a small amount of computers from what I’ve read,hrzmymc,t1_hrz4smj,1641773535.0,False
rzcexe,Isn’t there a legal aspect that needs to be in play for NFTs to actually enforce ownership over physical goods?,hsgbqyx,t1_hrvoajn,1642052484.0,False
rzcexe,Is there a decentralized place yet to store the image the hash represents?,hrvv596,t3_rzcexe,1641710740.0,False
rzcexe,You could put it on IPFS: [https://ipfs.io/](https://ipfs.io/).,hrwkpcc,t1_hrvv596,1641729489.0,False
rzcexe,Yeah. Look into IPFS. https://IPFS.io,hrwkw4r,t1_hrvv596,1641729625.0,False
rzcexe,"NFT means non-fungible token, meaning any type of crypto token that is unique/distinguishable. Add opposed to fungible tokens which are completely interchangeable - you can't tell one bitcoin apart from another.

The use of NFTs to hold links to JPEGs is just one possible use of the technology, and not as particularly imaginative one. In future, NFTs could be used to represent things like property deeds, event tickets and much more.",hrxb5sq,t3_rzcexe,1641743049.0,False
rzcexe,"NFTs are a type of smart contract. They let you buy and sell ownership of an asset (physical or digital) using the blockchain. The first owner creates an NFT and issues a licence that states whoever owns the NFT owns the thing. The code for the NFT then uses the blockchain  to ensure that only one person can be the owner at any time.

They're overhyped at the moment, but so was the Internet in its early days (dot-com boom) and nobody would deny now the Internet is important. The basic idea - buying and selling stuff other than cryptocurrency over the blockchain - is a good one.

The code for an NFT can be very simple - you can write one in 14 lines. Have a state that keeps track of the owner. Provide an API that transfers ownership if buyer and seller both authorize the transaction, and answers the question ""Who is the owner?"" at any time. The difficult part - keeping this all synchronized globally - is delegated to the blockchain.

For a good, relatively hype-free overview, here's the page on NFTs on Ethereum.org:

https://ethereum.org/en/nft

And for the gory details, here's the spec:

https://ethereum.org/en/developers/docs/standards/tokens/erc-721/

https://eips.ethereum.org/EIPS/eip-721",hruxtwc,t3_rzcexe,1641694234.0,False
rzcexe,"I would recommend learning at least a little bit of the science/cryptography of/in/for blockchain/hashing if you're into CS, but not actually buying these. 

Just invest in businesses that will generate money (with good causes), or your home, or your life, or just be a philanthropist if you're f-ing rich, rather than buying these elitist BS.

Sorry, for ranting. I just don't support NFTs.",hrvizms,t1_hruxtwc,1641703898.0,False
rzcexe,"I don’t necessarily think it’s any different than owning a skin on a video game in principle. I don’t really know a ton about them and I likely will never buy one, but you buy a skin to show off that you own the cool looking skin the same way someone buys an nft to show off owning a cool looking image.",hrvl72j,t1_hrvizms,1641705061.0,False
rzcexe,[deleted],hrvptvn,t1_hrvl72j,1641707612.0,False
rzcexe,"Yeah but it’s about owning the skin not just using it. A lot of games you can use the skin client side but nobody sees that you have it but people still buy them. 

  I am definitely more towards people can do what they want with their money. Sure there’s things they could do with their money that are more valuable to me, but I have no position to tell them that they should make that choice",hrvqzhp,t1_hrvptvn,1641708276.0,False
rzcexe,[deleted],hrvstef,t1_hrvqzhp,1641709340.0,False
rzcexe,"Yeah for sure i feel you, i’m talking more of like as principle for me. I get where you’re coming from on your perspective though it’s definitely just as valid",hrvzxdm,t1_hrvstef,1641713909.0,False
rzcexe,"100% agree. It has become so hard to learn about the CS side of NFT with all the hype. None of the top results on google come up with any CS info these days. It is astonishing how the world took cryptography topic and turned it into a such a big marketing hype.

None of my cryptography focused friends even want to talk about cryptocurriences or NFTs. They are pissed of at what it's become lol.",hrxp248,t1_hrvizms,1641748372.0,True
rzcexe,"I would just say don't let other people spoil something for you. If others want to look and talk about the pretty window frame, let them talk about the pretty frame. If you know there's something more to it, don't let your ego pull you away from moving closer, cupping your hands around your eyes (to block out the glare from all the flash photography the others are doing) and look into that window bro. There's some real s*** inside. 
And also it might actually be a plan to get people blockchain weary, by emphasizing the big-finance aspect enough so that's all that people see, having them ignore its potential to disrupt the status quo and fundamentally change the balance of power.  
Some people love the status quo. Bankers like being bankers and being depended on. Don't know if I'm making sense. It's 00:32 here, I should probably go to sleep now.",hrz6zzk,t1_hrxp248,1641767592.0,False
rzcexe,Thank you for the links!,hrxpaf0,t1_hruxtwc,1641748460.0,True
rzcexe,Key value paid loaded with a GUID and whatever the digital object you just bought was.,hrvz1sz,t3_rzcexe,1641713303.0,False
rzcexe,it's a fraud,hrw9g8l,t3_rzcexe,1641720868.0,False
rzcexe,From what I have gleaned the hash is taken including the image content itself (not just a link). Therefore the image can be hosted anywhere or move to a new location and since the content is the same the “link” is still intact. For small images I think they can be inserted into the blockchain itself so many parties can store it. However this seems to cost transaction fees so may be prohibitively expensive for images of non-trivial size.,hru88yf,t3_rzcexe,1641683494.0,False
rzcexe,The first nft was a number.,hrvfvac,t3_rzcexe,1641702331.0,False
rzcexe,"I recently took a blockchains course and from my understanding NFT's are powered by smart contracts mostly on the ethereum network. Most smart contracts are written using Solidity but they can really be written in any language. There's an online editor called Remix which you can use to write in Solidity. The smart contract is placed on the blockchain where it is immutable; however, there are ways for your NFT to be stolen. 

remix.ethereum.org",hruxxcb,t3_rzcexe,1641694276.0,False
rzcexe,"You got 1 or 2 CS related explanations, the rest are basically garbage hype/anti-hype.  Good try OP.",hry3xso,t3_rzcexe,1641753771.0,False
rzcexe,"Could someone post an example of where I could find the URL to an NFT within the NFT Contract?

For example, this is an [NTF page on OpenSea](https://opensea.io/assets/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d/8322) which costs 97 eth (or, $307,000). I can expand the Details to find the [Contract Address](https://etherscan.io/address/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d#code) or [Metadata](https://ipfs.io/ipfs/QmeSjSinHpPnmXmspMjwiXyN6zS4E9zccariGR3jxcaWtq/8322), or I could right-click on the image and [open in a new tab](https://lh3.googleusercontent.com/qtXorUoQU99DUrPCj694omQp_8_SZNfP4WPSE5LhjXPP9MvzA0-Y9ZjVQWDwgdrc2Otr5PfLp0pZCLHWiYgGJY6g3UHqflOt9K1RIw=w600) (which is simply served by a CDN).

The Metadata page lists an ""image"" key as containing the value `ipfs://QmXffx3vPcYVTUyQzxYWjkAnePQ4LuPB5vpzdDiqHqxcnm`. So, is that ""image"" metadata element a pointer to a URL with the transport language as ipfs:// ? If so, what is ipfs:// and how does that relate to an HTTP server that hosts the image?

And if that ipfs:// pointer indicates a location of the content, how is that tied into the owner of the NFT?

Edits: I'm recovering from covid and I can't seem to type NFT correctly each time (sometimes it comes out as NTF, other times it's NFL)",hry8rs6,t3_rzcexe,1641755448.0,False
rzcexe,"Not an answer, but a great [write up](https://moxie.org/2022/01/07/web3-first-impressions.html) by moxie (creator of Signal) exploring NFTs and web3. He demonstrates how the image you bid on is not necessarily the image you get.",hry9qe2,t3_rzcexe,1641755786.0,False
rzcexe,"I do think NFTs are overhyped, but I also think many are missing the point. The potential value of an NFT is basically the price difference between original art and a replica that is *artistically* identical.

Why do people buy original art? There is more than one reason: supporting an artist, money laundering, or simply prestige are a few reasons.

It's intellectually inconsistent in my opinion to think NFTs are worthless but original art / artifacts should be worth more than replicas.",hryf3j1,t3_rzcexe,1641757684.0,False
rzcexe,"The idea and the possibilities are cool, but the fact that image is not saved on blockchain is the most ludicrous thing ever. It does have good use cases though. (e.g. Tickets, patents, legal documents, in-game assets, helping artists collect royalties from usages)",hrwad64,t3_rzcexe,1641721584.0,False
rzcexe,Why would saving the image on the blockchain matter?,hrwlrsp,t1_hrwad64,1641730250.0,False
rzcexe,"Because when just the url saved on the blockchain, if the image is taken down, you can say bye to your art piece. Some systems also save the hash, but hashes are not 1-to-1 functions. Same hash could be generated by multiple images, albeit not easy to do so.",hrwma6o,t1_hrwlrsp,1641730596.0,False
rzc739,"I would start with first principles.

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)",hrurxml,t3_rzc739,1641691696.0,False
rzc739,"A Crash Course in Computer Science is amazing! I can't believe how much information it conveys in those short, fun to watch and easy to understand videos. Great work.",hrw2h47,t1_hrurxml,1641715684.0,False
rzc739,"Lambda Calculus - Computerphile https://www.youtube.com/watch?v=eis11j_iGMs

The fundamental concept of programming (writing computations) can be described in terms of Lambda Calculus. There are actually numerous models of computing (and programming) that describe the meaning of computing. Lambda Calculus is very useful because it is a model that requires very little in its fundamentals, and yet the little it does have is capable to describe ""everything"".",hru6303,t3_rzc739,1641682595.0,False
rzc739,"Thank you. I know who Alan Turing in and I think the inventor of Lambda Calculus.

Is there anywhere that has terms that would help me understand?

(I super hate applied things. I have a theory oriented mind so I need to to understand the terms and what they mean first. I don’t pick things up if they are not explicitly
 stated—blame the autism. )",hrukw05,t1_hru6303,1641688731.0,True
rzc739,">Thank you. I know who Alan Turing in and I think the inventor of Lambda Calculus.

 FYI, Alonzo Church came up with Lambda Calculus; Alan Turing came up with State Machines.  Since Lambda Calculus is meant to be stateless, they are different approaches to the theory of computation.   
As a Mathematician, I would also recommend looking into Automata, which is the theory behind state machines and grammar.",hruzugj,t1_hrukw05,1641695108.0,False
rzc739,"I have heard of Alfonso Lorenzo is. He was American. I meant I know who Alan Turing is and who I think the inventor of Lamba Calculus is. I knew they are separate people . I just wasn’t sure if he was the right guy. I also know who Claude Shannon is. (My partner has his CS BS and almost got his math minor so they talk a lot about these guys. We’re just speaking of Charles Babbage yesterday).

Are you an applied mathematician? Or do you study theory? I actually want to take partial Diff EQ and complex analysis. I like math.",hryjlg9,t1_hruzugj,1641759277.0,True
rzc739,I have a BS in IT and a BS in Math(emphasis in cryptology)   In grad school for CS I had to take a couple of prereqs including Automata.   Automata is more math and the theoretical side of computing.  I quite enjoyed it.,hs3g73b,t1_hryjlg9,1641839389.0,False
rzc739,"The best way is to take a compilers class if this is what you want to learn. But if you want an intuition of how to “program” you need to just write more programs, and practice.",hrunmpo,t3_rzc739,1641689884.0,False
rzc739,Not good at learning by application. I prefer to get a gist of the theory. And understand the terms in the least general manor possible. Like how I learned algebra before trig or learned the parts speech and what they mean when learning grammar syntax.,hruoizq,t1_hrunmpo,1641690246.0,True
rzc739,"You could search youtube for what you are looking, but all I can say is the best way is to by doing. Programming is a very practical field, theory will only get you so far - and even after reading it may not be clear how something works until you do it yourself. 

Pick up some algorithmic topics in Math and code them up, make a differential equation solver - or whatever. You get the idea",hruu0hr,t1_hruoizq,1641692588.0,False
rzc739,"I need to use this to do Fourier transforms. So yes. Differential equations is what I’ll be working with. 

Do you know Mathematica?",hruxrf2,t1_hruu0hr,1641694205.0,True
rzc739,"I’m not familiar with Mathematica - but here’s what I can suggest. I’m not sure which language you will be using but SciPy and NumPy are two python libraries that have an api for computing fourier transforms. 

If i were you I’d implement a fourier transform from scratch, and bench mark it against an already implemented version from say like numpy and scipy - that should be good enough to give you an intuition of programming.

my point basically is…. no amount of learning before hand, or theory is enough to make you feel prepared or give you a good idea of programming. u need to dive in head first and start doing it",hrv6tf8,t1_hruxrf2,1641698170.0,False
rzc739,I’ll give it a go. I just want to familiarize myself with terms and a few ideas before hand. I will look into what everyone has posted.,hryju2l,t1_hrv6tf8,1641759359.0,True
rzc739,This!,hrx65pl,t1_hrunmpo,1641741029.0,False
rzc739,"You may like the math for programmers book (https://www.manning.com/books/math-for-programmers). It's aimed at programmers trying to learn math, but maybe you could use it to learn programming in a familiar context.",hrw9iv7,t3_rzc739,1641720923.0,False
rzc739,"You should learn how compilers are made. Then you really get it. Grammars, Lexers, Code optimization, ...",hrx62ag,t3_rzc739,1641740989.0,False
rzc739,Those sound like terms I would like to understand. Thank you.,hryhwma,t1_hrx62ag,1641758683.0,True
rzc739,[https://teachyourselfcs.com/](https://teachyourselfcs.com/) this is a good resource for everything CS,hrwgcp2,t3_rzc739,1641726257.0,False
rzc739,I’m seeing a lot of info on how to program which I don’t really believe would be “programming theory”. Equally there isn’t really such a thing that I’ve ever heard of. You’re best bet would likely be anything on compilers. The closest to theory would be what is usually classified as “Formal Languages/Automata Theory” this would be a very theoretical/mathematical approach towards how we define a programming language/computer model. You would probably specifically be looking at context free grammars (CFG),hry54xc,t3_rzc739,1641754189.0,False
rzc739,Sicp?,hrvfnd4,t3_rzc739,1641702226.0,False
rzc739,"What you’re looking for isn’t usually taught to people who don’t already know how to program pretty well. There are classes on programming language theory but they’re typically at least senior level and the way they’re typically taught relies on knowledge someone new to programming wouldn’t have. That said Programming Language Pragmatics by Michael L. Scott is really good, and I believe online PDF copies exist. So I’d find out what language you’re going to be using and which concepts would be relevant, because a lot of things in the book won’t be relevant to what you’ll need and would be very confusing.",hrybmql,t3_rzc739,1641756456.0,False
rzc739,"It sounds like you and I are in a similar enough situation and frame of mind at the moment. I am starting to learn programming at the moment as well and share a similar learning style. I’ve concluded that introducing theory, first principles, and abstract terms is introduced best through a language taught as an introduction to computer science. Graphing calculators integrate Python so I have chosen to start with Python because of that.

Here are the books I’ve ordered:

• Python Programming: An Introduction To Computer Science – 3rd Edition – John Zelle

• Learn Python3 The Hard Way: A Very Simple Introduction To The Terrifyingly Beautiful World Of Computers and Code – Zed A. Shaw 

• Learn More Python3 The Hard Way: The Next Step For New Python Programmers – 1st Edition – Zed A. Shaw

• Python Crash Course: A Hands-On, Project Based Introduction To Programming – 2nd Edition – Eric Matthes 

• Head First Python: A Brain-Friendly Guide – 2nd Edition – Paul Barry 

• Python Pocket Reference: Python In Your Pocket – 5th Edition – Mark Lutz

• Automate The Boring Stuff With Python: Practical Programming For Total Beginners – 2nd Edition – Al Sweigart 

• Django For Beginners: Build Websites With Python and Django – William S. Vincent",hrvt8w4,t3_rzc739,1641709588.0,False
rzc739,"Thank you. I am not sure what languages physicists and mathematicians use. I hear Python is good. I am not a big fan of programming, but I need it for my major.

I am much more interested in maths, but for an analysis course, I will also need programming. I think Python is good for maths/physics. I will check those out.",hryik5e,t1_hrvt8w4,1641758914.0,True
ryzj9x,"One very niche part of databases I researched last year as part of a databases courses I took was the use of metaheuristic-based optimization algorithms to enhance searches, with the heuristic selection component modeled after animal behavior.  A lot of these are modeled on animals that hunt/scavenge/travel in swarms, like sea creatures or flies, and on average some of them perform more searches faster than traditional or heuristic searches.  Whale Optimization (2016) is one example, but there are many, and more are still being researched.
 https://en.m.wikiversity.org/wiki/Whale_Optimization_Algorithm",hrs0rgs,t3_ryzj9x,1641650895.0,False
ryzj9x,Blew my mind,hrsod7q,t1_hrs0rgs,1641661226.0,False
ryzj9x,Mindblowing! I never thought biology and physics could be used to optimize databases!,hrwgupg,t1_hrs0rgs,1641726645.0,True
ryzj9x,"You can browse different papers and articles online in Google Scholar, the Papers We Love GitHub repo, and various journals.  Here are January 2022 articles on the topic of databases from [arXiv](https://arxiv.org/list/cs.DB/current)",hrrwl9x,t3_ryzj9x,1641648661.0,False
ryzj9x,Encrypted search is interesting.,hrs99aq,t3_ryzj9x,1641654939.0,False
ryzj9x,"This will be about distributed systems, but  from what I see, there's no great divide between these worlds, distsys people concentrate on databases a lot. I see some ML stuff around tuning and some around SRE, but it's mostly around consensus and distributed transactional systems.",hrssml2,t3_ryzj9x,1641662892.0,False
ryzj9x,I recently took a course in blockchain algorithms so we talked a lot about distributed systems and made a lot of connections to concepts from databases such as three phase commits. It's not necessarily databases but there are a lot of similarities and the different applications might interest you.,hruwkzi,t3_ryzj9x,1641693696.0,False
ryzj9x,Something to do with a boy's cod?,hru96me,t3_ryzj9x,1641683884.0,False
ryzj9x,Self driving databases are a current topic of active research.,hruv9kh,t3_ryzj9x,1641693125.0,False
ryzj9x,"This is actually my fav because it's a way to mathematize and automate databases, which are already very.beautiful structures",hskdceo,t1_hruv9kh,1642121978.0,True
ryeny8,There's probably a higher chance teaching math will become automated before programming ever does.,hrodpi7,t3_ryeny8,1641583540.0,False
ryeny8,"Hah, that's actually a great point!",hrotih6,t1_hrodpi7,1641589413.0,False
ryeny8,Or really every other job.,hrpctlm,t1_hrodpi7,1641596821.0,False
ryeny8,"Once programming is automated we either do not need any jobs, or we, do not need any jobs.",hrqul08,t1_hrodpi7,1641621350.0,False
ryeny8,"> A career counsellor said that I should teach math (my other possible career goal) rather than go into software development, since the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years.

Hear me and heed my words very carefully: your counselor is a *fucking idiot*. This cannot be understated. I'm nearly speechless. Computers CANNOT THINK. And no matter how clever our algorithms are going to get, they will only be able to produce work within the confines of said algorithm. That means innovation will still come from humans, because it cannot come from machines.

And in order to create software, someone has to tell the machines precisely what is desired. It almost sounds like... Programming... Business people and non-engineers with no idea how computers work or the nuances of computer science and software will never be able to capture the requirements and edge cases of the thing they desire. People are also very bad at knowing what they actually want. This takes professionals to do this work.

Our industry, our field, is quite, quite safe from being automated into obsolescence. The only way your career counselor can possibly be correct is if we hit the singularity in that time frame, where humans develop synthetic life, it grows exponentially, and we hand off society as a whole to it.",hro9vqi,t3_ryeny8,1641582135.0,False
ryeny8,"God, I hate when people hear some buzzwords and then start making sweeping predictions about what is going to happen in the future, especially when they are your average person and not some high-up, specialized, and/or industry leading person. Who does this counselor think will be making these no-code platforms? Who does this person think will be using these no-code platforms even if they take off? In my experience there are many jobs out there where having math and computer science skills would always be an advantage over your average person even if the job doesn't good super in depth with either skill.",hroe2wc,t1_hro9vqi,1641583677.0,False
ryeny8,You just don't get it...  You gotta use AI to DeFi your blockchain if you ever want your NFT's to go meta.  Do you even cyber?,hroeyc6,t1_hroe2wc,1641583999.0,False
ryeny8,"Automation always leads to a new classification of work.  Robot repair, AI repair, code fixer, quantum devices, etc;",hrplkmy,t1_hroe2wc,1641600370.0,False
ryeny8,You want to know a job bound into obsolescence: career counseling!,hrokoxe,t1_hro9vqi,1641586123.0,False
ryeny8,Preach,hrocurm,t1_hro9vqi,1641583227.0,False
ryeny8,"It really ignores that as capabilities grow, demands grow. When we got railroads did all the hauling get done by 5 machines instead of 500 ox carts? No, we decided we needed to haul more stuff further.

When we got more advanced programming languages all the programmers didn't get fired, we decided we could benefit from the more complex software that was now possible.",hrqv9eb,t1_hro9vqi,1641621750.0,False
ryeny8,"By that definition humans also cannot think, as every piece of abstract thought you hold are basically recombinations of observations. Just try to imagine a colour that does not exist, a sound you have never heard before, etc. Our creativity is an abstraction that recombines atomic concepts into newer, bigger scale objects. This is why you can think of a new animal, song or word, which is not all that different from the progress of state of the art NLP and GAN, just at an elementary level.

That being said, the idea of code being automated relatively soon is absurd for so many reasons if you consider the sheer complexity. We can however already synthesize code based on queries, and there are many smart code completion toolings. I'd say the role of developer is simply changing and perhaps becoming even more important. Just look at how critical infra and devops engineers are. Software engineering hasn't been a basic coding job any more for decades.",hrqvejh,t1_hro9vqi,1641621835.0,False
ryeny8,"> By that definition humans also cannot think, as every piece of abstract thought you hold are basically recombinations of observations.

This is reductionist bullshit.",hrryyel,t1_hrqvejh,1641649954.0,False
ryeny8,">Just try to imagine a colour that does not exist, a sound you have never heard before, etc.

People have created new colours e.g. blackest black.

As for imagining a sound you've not heard before, can I introduce you to the concept of music composition? Humans have been creating new sounds for thousands and thousands of years.",hrr8e8c,t1_hrqvejh,1641630905.0,False
ryeny8,Just now completed watching the Matrix film it draws lot of parallels with this notion.,hrs7k2r,t1_hrqvejh,1641654177.0,False
ryeny8,Thank you for taking time to write this to put a slap across a face-,hroz9dm,t1_hro9vqi,1641591584.0,False
ryeny8,"Not quite. You can automate large swaths of the dev landscape away with current tech. It is shortsighted to assume your job is ""safe"". 

When robots are coding that won't stop me from writing code (which may be better and more innovative than what a robot writes, or not). But it may mean that the employment landscape looks different, and it may mean a different relationship between programmer and device at the highest levels of industry.

Edit: more: Fast-forward to UBI, economy 2.0, and full automation, and perhaps only a few very good programmers are working at the highest levels of industry, and most of the rest is automated, except for the millions of open source coders out there who would all of a sudden find that they have the time and the money to make pretty much anything (including better robots). It would be a real renaissance if you could get past the passing of the traditional economy.",hroh705,t1_hro9vqi,1641584826.0,False
ryeny8,"AI writing code is not that different to compilers writing code. Both are tools that make developers an order of magnitude - or more - more productive. If compilers aren't a threat to your job, neither are AI coding tools.",hroyt95,t1_hroh705,1641591414.0,False
ryeny8,"That's a good point. Compilers did not kill jobs, they *added* jobs. It's just that those jobs were not in 1s and 0s, but in higher level languages. AI which can write code (and to an extent even program and bugfix itself) would surely add jobs, but they would be AI-directing jobs rather than coding jobs. Developers might multiply in number while coders shrink. 

As someone who loves to code, that would not stop me from doing so, but I can't help but wonder how much deeper the development process could be if it were taking place in a Star Trek Holodeck with an AI on-hand, as could be done in the not-too-distant future with no-code or low-code AI-driven AR and/or VR IDEs. That's almost totally possible. It is just a matter of cost, adoption, and production.",hrp21qt,t1_hroyt95,1641592630.0,False
ryeny8,People have had the same worries since the invention of a machine to weave fabric (look up the original luddites if you're interested). It's never led either to the paradise where all the machines do all the work for us or the horror of mass unemployment.,hrp9e0l,t1_hrp21qt,1641595465.0,False
ryeny8,"Horror of mass employment, or having humanity's needs met without needing an exploited underclass.",hrr9o78,t1_hrp9e0l,1641631900.0,False
ryeny8,"Keep looking for that Communist utopia. Hint: it never works, because people are people.

The original luddites were more realistic about human nature and mass-unemployment was definitely their worry.",hrrdcj9,t1_hrr9o78,1641634789.0,False
ryeny8,RemindMe! 1000 years,hrov6hu,t1_hroh705,1641590044.0,False
ryeny8,"Architects didn't go away because CAD software was created. Developer jobs will just change, not be eliminated.",hrqvbsn,t1_hroh705,1641621789.0,False
ryeny8,"> Hear me and heed my words very carefully: your counselor is a fucking idiot. This cannot be understated. I'm nearly speechless. Computers CANNOT THINK. 

To add on further anyone remember the dumpster fire that was [Github Coploit](https://www.fast.ai/2021/07/19/copilot/)",hrptqf9,t1_hro9vqi,1641603819.0,False
ryeny8,I wouldn't say dumpster fire. It's useful occasionally when you hold its hand.,hrqaqhv,t1_hrptqf9,1641611259.0,False
ryeny8,True however github coploit ran into an issue with [plagrism](https://twitter.com/MalwareJake/status/1411351168643706886?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1411351168643706886%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.plagiarismtoday.com%2F2021%2F07%2F08%2Fgithub-copilot-and-the-copyright-around-ai%2F) and it revealed security secrets.,hrqg56e,t1_hrqaqhv,1641613748.0,False
ryeny8,Yeah I heard about that. Hopefully they get that fixed. It feels sketchy using it knowing that it could pop out copyrighted material on a whim.,hrqgal8,t1_hrqg56e,1641613818.0,False
ryeny8,"Sure,  version 1 has some issues to work out,  however the idea itself is fantastic and version 2 is going to have some great improvements.",hrrphtj,t1_hrptqf9,1641644181.0,False
ryeny8,"This though. I’m also shocked at your counselors lack or knowledge, despite acting like they know a lot on the subject. Was coming here to say literally exactly this glad to see someone beat me to it :)",hrs501o,t1_hro9vqi,1641652980.0,False
ryeny8,Wish I had a free award from reddit to give it to you. 💯,hrt38fd,t1_hro9vqi,1641667020.0,False
ryeny8,"I don't know a lot about it, but people said the same thing about visual basic. And developer jobs have continued to rise.",hro91y4,t3_ryeny8,1641581833.0,False
ryeny8,"People are making hyperbolic statements about some fields and exaggarate. So not only is it wrong for that particular field, it does not apply to many others.

Things like VB were used (broadly speaking) in desktop and business logic. COBOL was used for business logic and there were hyperboles about that. Then web happened and a lot of business logic changed to online, always available and to support large numbers of users. I wonder what next change would be..

Then there's all the other fields like embedded devices and IOT which have been and are huge deal: it is not as visible, but they are everywhere and used in staggering numbers of devices. That isn't going to disappear.

Then there's different fields like graphics work: 3D rendering has changed from fixed pipelines to programmable ones and there are tons of shaders used in things like games to simulate reflections (physical based rendering, PBR) and ray-tracing is seeing improvements, although real-time ray-tracing still does not handle everything you would want to (too much computing required).

So, people only have narrow view on what they themselves are using and new cases appear regularly that shift the requirements into different directions. Question isn't about if programmers needed but what kind of skills they will need. Many would not recommend COBOL at this point but that is still used in financial software and people who know it are still being hired to maintain old stuff.",hrof0ha,t1_hro91y4,1641584022.0,False
ryeny8,"We have had no-code and low code options for 20+ years. Check out https://old.reddit.com/r/sysadmin/ for the horror stories of important business processes that have been built around Access Databases and Excel Spreadsheets, ask them about Visual Basic and Foxpro, both low code options from the previous century that still haunt the corporate world. 

No-code, low-code falls apart as soon as there is not a widget that does what you need them to do. If you need to do anything hard it becomes a real programming effort, the harder it is the closer to the machine you need to get.",hrodb3t,t3_ryeny8,1641583391.0,False
ryeny8,"And this kind of thing has been said for decades about all kinds of jobs that still aren't really very automated. Welding was going to be done only by machines 20+ years ago and there are still plenty of welding jobs, plus welders still run the machines that have automated some of these jobs.",hrrmkii,t1_hrodb3t,1641642065.0,False
ryeny8,"No code is someone else's code that you don't have control over. 

Machine learning code generation is just that, code generation. It saves you a step from having to go to google stuff and copy-paste from stack overflow. Code is not software. In fact, there are some estimates I read that coding is only about 10% of a job of a software developer. Even if it becomes fully automated/obsolete (and it won't), you'll still have the other 90%. And it's not like all the code already written will magically become self-supporting.  


I bet that career counsellor types with just two index fingers and has to look at a keyboard and mouth the letters while typing.",hrof1b7,t3_ryeny8,1641584030.0,False
ryeny8,The best developer I ever knew typed with two fingers with no more then 10 WPM. I sit in the same row and have never seen her hit backspace or delete.,hrppv0j,t1_hrof1b7,1641602163.0,False
ryeny8,"Ctrl+A, Space",hrre29m,t1_hrppv0j,1641635362.0,False
ryeny8,The other 90% is browsing reddit,hrqxkdk,t1_hrof1b7,1641623176.0,False
ryeny8,Lol,hrodj3v,t3_ryeny8,1641583473.0,False
ryeny8,"As someone who knows both dev and AI/NLP, i echo the words of someone else here: your counselor is a fucking idiot",hrocske,t3_ryeny8,1641583205.0,False
ryeny8,I think you should tell your counselor to move into programming because in 10-15 years he will be the one jobless.,hromffp,t3_ryeny8,1641586777.0,False
ryeny8,"You can create a website or an app nowadays without coding much but to create a system which is highly scalable and available, you'll still need good programmers and engineers. That's not something a machine or non-technical person can do.",hrol0oj,t3_ryeny8,1641586246.0,False
ryeny8,"The people automating jobs away cannot be automated away.

I NEVER found my counselors useful, they literally are counselors and not X profession/career you actually want to get into. That should always be a major red flag on taking their advice on anything.

The number one most important thing about who's advice you listen to is this: Find the people that have/do what you want and go and ask them how to get it/do it.",hronxru,t3_ryeny8,1641587340.0,False
ryeny8,"If it happens, you can always change career to teach math. But its unlikely to. Machine learning requires a lot of developers right now and is likely to do so for the near and medium future.",hro9tvy,t3_ryeny8,1641582116.0,False
ryeny8,"Your counselor is an idiot, full stop. The need for programmers is only accelerating. I’ve been at this since I was 14 coding in my bedroom dreaming about what could be in the future (thanks TI-99, Sinclair 1000, C-64, TRS-80, and IBM AT. You were all good friends). I’m 52 now and we are only scratching the surface. I still dream about what could be in the future.

For a smart kid from poor to lower middle-class background, this career is the surest and fastest way to a solid 6 figure income in flyover country. Factory automation, agriculture automation, websites, mobile apps, VR games, warehouse management, CRM, ERP, hospital EMRs, air quality analysis, self driving cars, wearable tech, robotics, AR systems for military, quantum computing, smart homes, I could go on and on, but its just going to get more and more intertwined with our lives. Now it is still just an addition to our lives, in 30 years it will be fully intertwined. 

Feel free to msg me privately and I will be happy to mentor you through your college applications.",hrotv2d,t3_ryeny8,1641589545.0,False
ryeny8,"> I should teach math (my other possible career goal) rather than go into software development

Have you looked into how much teachers get paid? Ha!

> the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years

They were saying the same thing about outsourcing in the 2000’s and look how that has played out. Also, did CAD replace engineers in other fields? Lol.",hro9uof,t3_ryeny8,1641582124.0,False
ryeny8,"People who think these kind of things threaten developer jobs don't know about AI & development (or the difference). There's nearly zero overlap. Developing an AI for a task requires AI specialists, has high resource costs, and is significantly more pricey than hiring developers; also it only does pattern matching, so anything that doesn't fall in that purview requires a developer which is... mostly everything. They are both very different use cases.

Developers aren't going anywhere anytime soon.",hroqnw6,t3_ryeny8,1641588357.0,False
ryeny8,"The consensus, a resounding no, your career opportunities are not under threat.",hrofhc7,t3_ryeny8,1641584193.0,False
ryeny8,"I fully agree with the other comments here, but thought I'd provide a little bit of evidence as to how important software developers are when working with low-code platforms.

I was applying for new jobs about a year ago, and noticed that major corporations like Thales use a low-code platform called Mendix, which seems to be a major player in the industry. Thales is currently hiring Mendix developers for multiple positions, such as [this advertisement for a technical lead](https://thales.wd3.myworkdayjobs.com/en-US/Careers/job/Crawley/Mendix-Developer_R0114396-1). Note the requirements for the position: Multiple years of experience, experience with all levels of software stacks, experience in programming languages, experience in security...

These are all skills that non-developers do not have and most likely will not learn. Even if low-code platforms actually take off (and that's a big if), software developers will be the ones writing the code (or the not-code, or whatever).",hrot1n8,t3_ryeny8,1641589238.0,False
ryeny8,"I suppose your counselor is not aware of ""The halting problem""",hrovixx,t3_ryeny8,1641590176.0,False
ryeny8,Terrible advice on your counselor’s part.,hrofbg3,t3_ryeny8,1641584133.0,False
ryeny8,"That’s a very foolish position for your counselor IMO.

Computer science and computer programing are only going to increase in value.

Who is going to program the AI to code? A computer programmer. 

Understanding the computer world is going to be increasingly central to everything. And your knowledge as a computer programmer will adapt to the changes if your interest is serious. 

Honestly, teaching math might not be a job in 10 - 15 years because through computer programs getting better and AI, there in all probability won’t be a need for teachers in the current sense.

Your counselor is clueless.",hroh1x0,t3_ryeny8,1641584773.0,False
ryeny8,"LOL No. 

But it does add a whole bunch of new projects for the developers to work on. 

Software engineering will literally be the last job where machine can completely replace human counterpart. It might still happen some time in the distant future. And once that happens, it’s game over for the entire civilization, not just developers. 

We got time.",hrow31h,t3_ryeny8,1641590384.0,False
ryeny8,We are living in a simulation,hrpisqs,t1_hrow31h,1641599237.0,False
ryeny8,"Your career counsellor has no fucking idea about software development. This is like saying don't become an architect don't become a carpenter don't become a roofer because we have drills today!

I would even report this guy. Developers are in high demand and telling people who are interested in this field to not go this way is the complete opposite what he should do.",hrrdmvr,t3_ryeny8,1641635018.0,False
ryeny8,"Honey, I’m gonna put this in a very simple example.

Tell me any virtual assistant (Google Assistant, Siri, Alexa, Cortana) that you have used and never made 1 mistake.

Computers are nowhere near ready to replace developers. Computers cannot find flaws on there own yet. They can only find what they are instructed/deeply trained to find. Humans on the other hand can use knowledge they possess and apply it to anything they see fit. Computers cannot.

Your consular is a prime example of why applications are made toddler safe.",hroe8td,t3_ryeny8,1641583737.0,False
ryeny8,Tell me any developer that doesn't make any mistake :),hrrsa3z,t1_hroe8td,1641646063.0,False
ryeny8,"Bad / entry level crud developers ? Yes

Complex custom solutions / enterprise development ? No",hroprvg,t3_ryeny8,1641588026.0,False
ryeny8,Your career counselor is a clown,hrp5xhk,t3_ryeny8,1641594119.0,False
ryeny8,"Aaahhhh, I love how confident some people are in making predictions of industries they've probably never touched.

(I'm talking about the counsellor, not OP)",hrqdqm8,t3_ryeny8,1641612633.0,False
ryeny8,Your counselor is a moron,hrqr33b,t3_ryeny8,1641619363.0,False
ryeny8,"No... developers will need to troubleshoot and fix why all the ""no-code"" shit isn't working",hrr5l7g,t3_ryeny8,1641628783.0,False
ryeny8,Your counsellor is a downright moron. Period.,hrreqrt,t3_ryeny8,1641635914.0,False
ryeny8,Do you want to make the mistake of your life? Cuz that's how you do it,hrrrxrn,t3_ryeny8,1641645840.0,False
ryeny8,"*Do you want to make*

*The mistake of your life? Cuz*

*That's how you do it*

\- lookintothefuturem8

---

^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.](https://www.reddit.com/r/haikusbot/)

^(Opt out of replies: ""haikusbot opt out"" | Delete my comment: ""haikusbot delete"")",hrrryl2,t1_hrrrxrn,1641645855.0,False
ryeny8,Your career counselor should look for another job.,hrp2swl,t3_ryeny8,1641592919.0,False
ryeny8,"You should look at /r/programmerhumor sometime. There's often a screenshot of the absolute 10/10 shite that Copilot can produce. It can however sometimes give you something useful, but it still needs a hint of what you want it to write. Someone had to think about what needs to be done and more importantly *how*.

I doubt we'd be able to make an AI that can properly make a beautiful and fast frontend with good UX or make a sensible and proper DB design for a backend environment. Even if we're able to do that in the next, I don't know, 40 years it also has to be cheaper to use than paying a team of developers .",hroerj1,t3_ryeny8,1641583930.0,False
ryeny8,"We have a low code platform from Outsystems, all those guys do is ask us SAP folks to build a Odata service for them to consume the data. They can't do shit on their own.",hropha1,t3_ryeny8,1641587915.0,False
ryeny8,"As many have already stated, I'd ignore your career counselor. Low code/no code solutions automate something that should be automated, the easy stuff that we shouldn't be working on because it's easy. It's just a new abstraction layer. This is great, because it means we can focus on the hard stuff that hasn't been solved yet.

I will caveat, there is a set of software developers who may lose their jobs because there are folks who are paid to write code that really should be automated, but isn't for some reason (usually ""enterprise"" inefficiency.) And, some of those coders lack real problem solving skills and won't be able to transition to more meaty problems.

If you have a strong interest in math and code, you should find plenty of opportunity to work on problems that haven't been solved. That's not going away until, as someone pointed out, we reach the singularity, and then we're all out of work :)

P.S. Thanks for asking this here, it's a really interesting topic, and a good wake up call that people are buying a bit too much in to the hype of low code/no code.",hrot0qt,t3_ryeny8,1641589229.0,False
ryeny8,who watches the watchmen?,hrotxj9,t3_ryeny8,1641589571.0,False
ryeny8,"If anything, developers will be managers of robots, but not without jobs. Your advisor sounds like an idiot.",hroyfy2,t3_ryeny8,1641591275.0,False
ryeny8,"In order to automate a programmers job, first we need our clients to accurately describe what they want.  I have yet once in my career experienced this.",hrp32rh,t3_ryeny8,1641593024.0,False
ryeny8,Your career counselor is a wrong,hrp5y67,t3_ryeny8,1641594127.0,False
ryeny8,"There will be developer jobs for decades to come, and maybe even until the singularity.",hrp81o5,t3_ryeny8,1641594943.0,False
ryeny8,"Only to a point.
When electrical and mechanical computer systems, they took away the jobs of ""computer"" people. But they created jobs in operating and programming these computers. When programming could be written in the memory of the machine, it took away the need to dig through wires and switch them around. When compilers were created, it took away the job of writing in assembly. But it created jobs in creating those compilers, one for every machine, one for every language. When these compilers can be made automatically, if they ever do, there will still be jobs in writing programs; there already were. Even if we create meta-compilers, compilers that take in entirely human language and compile that into a common programming language, which is then compiled into assembly, there will still be the creative process above it all. And I can say with almost certainty that a computer will never design a game from scratch.",hrp8orx,t3_ryeny8,1641595193.0,False
ryeny8,"Maybe I am naive regarding how far AI coding can go, but I work on a firewalling application, and when I receive a requirement along the lines of ""There's this type of traffic X which we are failing to identify quickly enough and make a drop decision  before various NIC/system buffers get shot and we start dropping everything"", I feel like no matter how clever the AI there's still a role for a dev--type human in there somewhere.


Predicting the future (especially tech) is hard though, so 🤷",hrpb430,t3_ryeny8,1641596140.0,False
ryeny8,"No, that’s kinda like saying “ugnert invent fire, stop messing with wheel science already figured out” low code no code solutions are tools that have been coded to allow someone to do something without understanding how to code. They do not create new capabilities, that still needs to be coded, and the integration and expansion of these capabilities will need to be coded.",hrpby5s,t3_ryeny8,1641596473.0,False
ryeny8,"When I started university, OOP and code generators were all the rage, and people were hyping that UML, automatic diagram to code generators, etc. gonna end most of the coding. Also, while AI winter wasn't officially over, it's rumored that AI will be able to code, and will be much better at it (w.r.t. humans) in 10 years.

**Result:** >!It didn't happen like that.!<",hrpc7n7,t3_ryeny8,1641596577.0,False
ryeny8,"The day a software engineers job is automated is the day that literally nobody will have a job anymore. 

Even with these tools - someone's gotta design the systems behind it. AI can adapt but it has to have the foundation to do so first.",hrpcqiu,t3_ryeny8,1641596787.0,False
ryeny8,"Well some peoples jobs, but it would be balanced out by more jobs being created. 

Thats the thing about software, every time something gets easier, it just means that there are more things you can now build on top of it.

Until(if) we reach the singularity, no software is ever going to absolete the developer.",hrpcwh0,t3_ryeny8,1641596852.0,False
ryeny8,We are still very short on skilled software developers.,hrpkf7v,t3_ryeny8,1641599901.0,False
ryeny8,"I develop a low code plugin and I totally agree with everyone else here. We need more developers and our best customers are still developers even if just about anyone could build a CRM without code on our platform.

Our users that aren't advanced only get closer to being developers as they use our platform. AI can't do much more than what we've done ourselves over 12 years of listening to customer support tickets and reacting to what people actually tell us they want.",hrplnu0,t3_ryeny8,1641600406.0,False
ryeny8,"If you work in IT or development, you will know it couldn’t be further from the truth.

Seeing first hand all of the issues that are fundamentally related to these kind of tools - tools that aim to abstract difficult computational tasks - makes it easier to just intuitively understand why its silly to suggest that programming jobs will be fewer in 15 years.

Such tools usually only work well in typical and expected scenarios and there is always some kind of trade off - less efficiency, less control, less interoperability, less portability, less capability, lack of underlying understanding, higher costs, security concerns… the list goes on. 

 Programming jobs are more likely to continue to increase than anything else.

You should still consider your options though - because teaching Math sounds like a cool choice too.",hrpyhpo,t3_ryeny8,1641605858.0,False
ryeny8,"Well, if you're gonna lose your job (as you counsellor said) in 10-15 years make sure you earn so much money that you can start with anything in the future without any hassle.

Don't take advice from someone who isn't in the field i.e. The counsellor.",hrpzpyi,t3_ryeny8,1641606388.0,False
ryeny8,Nah.,hrpzvt9,t3_ryeny8,1641606460.0,False
ryeny8,"Good developers are always trying to put themselves out of a job, in a sense.",hrq2w8u,t3_ryeny8,1641607782.0,False
ryeny8,"AI coding will sure save time from programmers, but you still need to write out what you want to do. The AI is unaware of more abstract concepts than objects or lists. You still need to think on a high level about what you want to do. So no.",hrq3ube,t3_ryeny8,1641608197.0,False
ryeny8,My lead told me react and angular devs will be replaced by AI that can build UI.,hrq462m,t3_ryeny8,1641608338.0,False
ryeny8,Can you please show them this thread,hrq5a4u,t3_ryeny8,1641608825.0,False
ryeny8,"Ha, hahaha, hahaha no.  Those tools, if they help at all, will just make existing developers faster.  And they probably won’t even drop the number of new devs being added in the coming years.

I think that laymen forget that programming isn’t just syntax.  Look at scratch, which could be seen as a “low code” platform.  Ask the average business goon to look at a bright beautiful low code platform and see if they can tell whether a loop will halt or continue forever.  Programming is about thinking algorithmically, not about arcane symbols.",hrq65om,t3_ryeny8,1641609215.0,False
ryeny8,"Why would anyone want to go to class and watch you teach when they can pay for an online recording video of the ""best teacher in the world"" from somewhere in Australia for instance lol",hrqbm8a,t3_ryeny8,1641611661.0,False
ryeny8,Need a developer to make pasta primavera out of the spaghetti those tools churn out.,hrqfa7h,t3_ryeny8,1641613349.0,False
ryeny8,"Yes, shitty developers will have their jobs threatened.  Good developers will be alright.",hrqi9df,t3_ryeny8,1641614757.0,False
ryeny8,I have a neighbor who spouts this bullshit constantly then apologizes to me like I even give a fuck. It’s whackadoo… if a computer does something then someone programmed it to do so.,hrqjp9i,t3_ryeny8,1641615478.0,False
ryeny8,no,hrqlmqr,t3_ryeny8,1641616473.0,False
ryeny8,"I'm in QA, and I promise you anyone who thinks a computer can write code has never read a story's acceptance criteria written by a customer.",hrqobez,t3_ryeny8,1641617871.0,False
ryeny8,"If you think it will, then you don't know what programming is or AI !",hrqoge4,t3_ryeny8,1641617944.0,False
ryeny8,"Producing code is a very small part of software engineering. Most of the work of a software project consists of deciding how to handle all possible types of inputs and failure modes that a system could encounter, in order to make that system serve specific goals.

In [Programming as Theory Building](http://pages.cs.wisc.edu/~remzi/Naur.pdf), Peter Naur argues that a team's understanding of a program is their true product.

When code comes from a third party (such as an AI), it is **harder**, not easier, to review the code and make sure that it does what is wanted without introducing any unacceptable risks. This is like the difference between building a new house, and renovating an old house without knowing whether it contains any lead or asbestos or radon.",hrqpzt2,t3_ryeny8,1641618771.0,False
ryeny8,Can I use copilot for basic python ?,hrqufic,t3_ryeny8,1641621257.0,False
ryeny8,"Red queen hypothesis applies here.

As the tools grow more capable and complex the tasks become more complex and demanding. What a developer once did with 300 lines of assembly can be done in 5 lines of C++ or 2 lines of python. But they don't want that anymore. They want you to use python to set up a neural network in Keras, in the time that programmer got to write 300 lines of assembly. But now what your doing would represent 100,000s of lines of assembly.

So if we someday have a tool like the holodeck in star trek where you can say ""make me a program that simulates all the behaviors or Sherlock Holmes"" the programmers will be doing something that is still a lot of work given those tools.",hrqux5l,t3_ryeny8,1641621551.0,False
ryeny8,"For Gov jobs, maybe. For the rest, no. But be prepared to relearn everything on a regular basis.",hrqxvfq,t3_ryeny8,1641623375.0,False
ryeny8,"In the future, you will have to be extraordinary in your field to sustain. AI is gonna replace average programmers and average math teachers too. You will have to be extraordinary to sustain in the highly-competitive world. Do what you want to do, but be the best at it.",hrqyged,t3_ryeny8,1641623752.0,False
ryeny8,"1. Those things do not create program, they write code. They have no concept of why. It can't make decisions, talk to clients or handle anything else then code.

2. If an ai tool like this can replace your entire job right now, then what the hell are you doing?",hrr5w0y,t3_ryeny8,1641629003.0,False
ryeny8,"Have prefabricated houses affected builders jobs? No. If you need some technological shit doing right, you need to hire a coder. The rest is scam. You can tell your mentor to keep studying math. People who say that kind of stuff, never have coded entrepise applications, and there are a lot of them. Learn to code, the rest is shit. I am 30, I have been coding since I was 12. I am an entrepreneur now, and It is super hard to find good programmers to work with, so PLEASE keep studying how to code.",hrragrb,t3_ryeny8,1641632523.0,False
ryeny8,"[Relevant CommitStrip](https://www.commitstrip.com/en/2016/08/25/a-very-comprehensive-and-precise-spec/).
No matter how abstract the specification is, you will always need someone to work on it. And even if you don't stay up to date, there will also be businesses with old tech. I hear COBOL developers are doing fine.",hrrcqgx,t3_ryeny8,1641634301.0,False
ryeny8,"Like some people have already mentioned, just throwing the word ""machine learning and artificial intelligence"" does nothing. We're far far away (it won't be an exaggeration to say about a century) from having something remotely close to Jarvis, Friday or Edith. So no, AI won't be the one who would be making dev jobs obsolete.

Secondly, it seems like you haven't worked at scale. The low code and no code tools only scale so much or provide exactly the features you're looking for. Sure if you're making a majorly static application then yeah those jobs are ALREADY gone. But other than that we're doing as great as ever.",hrrcs6h,t3_ryeny8,1641634340.0,False
ryeny8,No we will just be stuck in bug reviewing hell,hrrl100,t3_ryeny8,1641640885.0,False
ryeny8,"I wouldn't worry till the Terminators start reproducing, then we're screwed anyway.",hrrmnyi,t3_ryeny8,1641642136.0,False
ryeny8,Happy Cake Day jforrest1980! You are never too old to set another goal or to dream a new dream.,hrrmofv,t1_hrrmnyi,1641642147.0,False
ryeny8,"We've had no code tools for a long time my friend, they only create more work for us, and open new doors.

Also, your counsellor is ignorant. We already automate everything and we just do more as a result, not less",hrrn1jz,t3_ryeny8,1641642419.0,False
ryeny8,Computer science is about a lot more than just writing code. It would harm some jobs probably. If you’re still in school might I suggest gearing your education towards AI itself?,hrry21u,t3_ryeny8,1641649471.0,False
ryeny8,"You’re career counselor is an idiot. Find a new one.

Secondly, computers are stupid until people make them smart. No code had to be coded by someone and there will always be something new that needs created via code",hrs96np,t3_ryeny8,1641654905.0,False
ryeny8,"I was just in the process of ordering something from an online shop created by squarespace when the whole thing seizured, lost my order, and then dumped me onto a different product screen with no way to get back to my cart.

Don't worry, software engineers aren't going anywhere.",hrse93k,t3_ryeny8,1641657105.0,False
ryeny8,Who builds the low code /no code tools?,hrsnl5p,t3_ryeny8,1641660917.0,False
ryeny8,"10-15 years? We’re going from “some specialized NLP algorithms can produce a simple Python function given a detailed description of what it does, with a reasonable chance that it won’t be correct” to “literally developers aren’t necessary anymore” in half the time it took to get from Java 1 to Java 8? Yeah I think you’re safe",hrsqzzf,t3_ryeny8,1641662257.0,False
ryeny8,My college counselor told me not to major in computer science and computer engineering because there were too many people going into the field and there would never be enough jobs out there for all of them.  That was 2003-04. Don’t listen to those idiots and do what you enjoy.,hrsrs51,t3_ryeny8,1641662559.0,False
ryeny8,"Very likely not the case and even if, it will definitely create new jobs in a similar field where you already have atleast some experience and you can always learn new stuff",hrst4v2,t3_ryeny8,1641663090.0,False
ryeny8,Another comment said “listen to me carefully: your counselor is *a fucking idiot*.” I couldn’t have said it better myself.,hrsutnb,t3_ryeny8,1641663747.0,False
ryeny8,"The AI can only write a valid program if someone wrote the test to prove it.  AI is science, science only let's us observe what we think is true.  It can't discover and assert true by itself.",hrszkpw,t3_ryeny8,1641665585.0,False
ryeny8,"Before that happens, career counselling is going to get automated. Hope your counsellor is preparing for a new role- maybe math?",hrt0pix,t3_ryeny8,1641666024.0,False
ryeny8,I’m sure the career counselor is an expert in tech.. that’s why they’re a counselor rather than working in any industry.,hrvkull,t3_ryeny8,1641704875.0,False
ryeny8,AI technology is getting too much hype if the general public believes that there will be not need for human-generated code 10-15 years from now.,hrwgemi,t3_ryeny8,1641726297.0,False
ryeny8,Your career counselor is a fucking idiot,hrp5vti,t3_ryeny8,1641594101.0,False
ryeny8,Your career counselor is a wrong. SWE isn’t going anywhere.,hrp60v5,t3_ryeny8,1641594156.0,False
ryeny8,Your career counselor is a wrong. SWE isn’t going anywhere and it’ll always pay more,hrp62qe,t3_ryeny8,1641594176.0,False
rxuuqm,"depends on what information the guesser receives after every guess. if you only tell if their guess is right or wrong then the best approach is just start at 1 and increment it by 1 for every guess. time complexity O(n). 

if after every guess you tell the guesser if their guess was lower, higher or equal to the number then you can use binary search. time complexity O(logn).",hrktu0c,t3_rxuuqm,1641520595.0,False
rxuuqm,"That is the most efficient solution. With no other information, the best you can do is just arbitrary guesses (assuming you never guess the same thing twice). It's pretty easy to prove, actually; you should give it a shot.

Anyway, since there's no algorithm improvement to be had, it comes down to maximizing the number of guesses you can perform per second. That kind of optimization can be somewhat tricky and involved, but I encourage you to look at utilizing parallelism and gpu computation to solve this problem, along with optimizations of the sort used in GNU `yes`. Unroll the living shit out of some loops, lol.",hrktr3o,t3_rxuuqm,1641520561.0,False
rxuuqm,How do you prove something like that?,hrn10se,t1_hrktr3o,1641565636.0,False
rxuuqm,You assume theres some algorithm that sloves the problem in less guesses and show you can pick a result that the algorithm wouldnt guess.,hrnmpd9,t1_hrn10se,1641573852.0,False
rxuuqm,"less guesses being o(n)?, how can you tell what the algorithm wont guess without knowing how it works?",hro3930,t1_hrnmpd9,1641579745.0,False
rxuuqm,"Read about the ""Adversary arguments"". It's a technique to validate the lower bound of an algorithm in the worst case.

In the Fundamentals of Algorithms (*Brassard & Bratley*) chapter **12.3**, you can find a very good explanation. The PDF for the book is easy to find on google.",hro873i,t1_hro3930,1641581522.0,False
rxuuqm,"You assume theres k guesses where k is less then n (n being the biggest allowed number, such a number must exist just like someone said about uniform over the naturals). After running the algorithm we can denote its guesses as A= {a_i} from i=1 to k. Of course each guess is from 1 to n.

Because n is bigger then k, there exist a number b below n that is not in A. Therefore if the randomly selected number is b, the algorithm wouldnt work.

Its a bit too rigourous, but the point stands.",hrogutu,t1_hro3930,1641584700.0,False
rxuuqm,Just learned this in discrete math (proof by contradiction) and am shocked to actually see it being used,hroqkt1,t1_hrogutu,1641588326.0,False
rxuuqm,"haha, those kind of proofs are very common.",hrou42u,t1_hroqkt1,1641589640.0,False
rxuuqm,About 3 pages of discrete kinda math,hrph4g5,t1_hrn10se,1641598554.0,False
rxuuqm,"Can it ask additional question like ""_is it greater than X_""?",hrktlff,t3_rxuuqm,1641520497.0,False
rxuuqm,There is no uniform distribution on the naturals so no random distributions are particularly natural. Without more information on this particular distribution the answer to your question is ‘it depends’,hrlpx30,t3_rxuuqm,1641535463.0,False
rxuuqm,"Hmm. Well technically if you use multiple guess points that focus at a different starting point you can try to get luckier… but statistically it would be the same time complexity 0(n). Let me explain.

So you explained how you you incremented a number one by one so I assume you start at zero. Well in that loop you probably have something like “while the answer is not equal to guess then increase guess by one” starting at zero.

Well what if at the same time you ran a second guess inside the while loop. And this guess starts from the highest digit you allowed the random number to be chosen from and starts moving one down every iteration. So now you have two variables guessing in the same iteration of the loop. One goes 0,1,2…. While the other goes something like 1000,999,998,997….. 

Now while it sounds like it would make it faster it doesn’t because the two points aren’t being used at the same time; however, I think maybe you could get luckier that way. Just sounds plausible even thought statistically it’s not.",hrlkmrm,t3_rxuuqm,1641532572.0,False
rxuuqm,I had actually tried that and it was a few seconds faster on average but sometimes it would take up to like 8 seconds longer if you were unlucky,hrmzmp4,t1_hrlkmrm,1641565048.0,True
rxuuqm,"being faster on average is only likely due to loop unrolling - since you're making 2 guesses per iteration, there is half the overhead for looping",hrny3og,t1_hrmzmp4,1641577916.0,False
rxuuqm,"While I was reading your post, I thought about set theory. There are infinitely many intergers. If you consider such statement as ""the number is between 10 and 10000"", according to your method, you need at most 10000-10+1= 9991 tries. Each time you try, you have a probability of 1/ 9991 to guess the right number. The statement is the information needed to shrink the size of all possible combinations. It is possible to guess correctly by doing it once, twice, three times....., or in the worst scinario, 9991 times.",hrmbg37,t3_rxuuqm,1641550586.0,False
rxuuqm,"The problem of making it any better than O(n) seems to be it is simply random. I see this as you are given a function to check equality with the chosen number, and don’t have access to the number. Therefore it is completely random and unless there is something known about how the guesses are made that’s simply it. Maybe, just maybe, if it’s a bunch of numbers Inputed by people around the world, you can do some ml to find tendencies of what numbers are most likely to be chosen by a person and run those first, worst case would still be O(n) though. Also you can maybe speed it up a bit like that but there is no way to get O(log(n))",hrmvvnb,t3_rxuuqm,1641563370.0,False
rxuuqm,"Increments of one is likely the best - considering anything else would omit certain possibilities and could lower the probability of guessing the number in the shortest amount of time. 

With that being said, you could try to improve the amount of time it takes by trying all likely probabilities first. For example, if the number is a friends 4 digit passcode, it's more likely to end in 5, 0, or some other number that may hold significance to the creator of this number you're trying to guess. 

Write down all factors that could influence what number you're trying to guess. Then, have the computer guess all - more likely numbers first, and increments of one from there. 

You could have it running several algorithms at once. For example (2,4,6,8,10) (5, 10, 15, 20) and several other more-likely-pattern-based-increments. At the same time, you could have the computer trying series of numbers that may hold significance to the original numbers creation.",hrnoyp8,t3_rxuuqm,1641574659.0,False
rxuuqm,"Give the computer a gun, and you will tell it. /s

Otherwise, pick a random number weighted according to the distribution of how you picked the number.",hrokq03,t3_rxuuqm,1641586135.0,False
rxuuqm,Update: I used multithreading and used 4 threads to speed up the guessing. 2 start on the low end and and the other 2 start at the high end. The pairs of 2 are staggered 1 apart and go up 2 each time and together they go through every number in that direction. Going from both directions also helps because the number is more likely to be on one side than in the middle,hromtdv,t3_rxuuqm,1641586920.0,True
rxuuqm,"If I remember correctly, the chance of choosing the correct number that is within a given range is related to the square of that range's size? So if your number is between 0 and 100 you're very likely to guess it within 10,000 (100 squared) tries. I don't remember how the likelihood is actually calculated though.",hrljt0d,t3_rxuuqm,1641532145.0,False
rxuuqm,"If there are only n options (101 in your example) then you don't need more than n tries to guess it obviously, unless you try already attempted numbers again. So what you are saying is not true.",hrn54v5,t1_hrljt0d,1641567314.0,False
rxuuqm,"I meant random tries, not iterative. As in, how many die rolls would it take to roll a 6? Not necessarily just six, because you aren't just iterating from 1 upward.",hrn5de7,t1_hrn54v5,1641567407.0,False
rxuuqm,"Ok. You are talking about the probability of randomly picking some specific element out of a set with n elements.
That's 1/n.
So where does the quadratic part come in?",hrnl1bw,t1_hrn5de7,1641573255.0,False
rxuuqm,Use a quantum computer.,hrn1h8t,t3_rxuuqm,1641565828.0,False
rxn4al,"The clock could show you if it is day or night throughout the year. This was done by programming the length of the days into the clock (afaik with water).

It is a maschine that calculates (among other things) the length of the day based on an input. It has a ""storage"" unit and a ""logic"" unit.",hrm4kgk,t3_rxn4al,1641545279.0,False
rxn4al,"All of those things are true about an hour glass by simply adding and removing sand. A person calculates the length of day and adjusts the input information, water, to provide the desired result the program is always the same though",hsgmqch,t1_hrm4kgk,1642059847.0,False
rxn4al,"A clock can generally be modeled as a cyclical finite state machine with a single input. This is useful in some finite state machine decompositions. Many would say a computer must consistently apply a function over an ensemble of possible inputs so a fully determinate machine like a trivial clock might not qualify. But the castle clock isn't that kind of trivial clock, and it does have many possible inputs, so it's hard to object to calling it a computer.",hrnoxg4,t3_rxn4al,1641574646.0,False
rxjst3,Using tools from real analysis how would you rigorously set this up ?,hrqhxol,t3_rxjst3,1641614600.0,False
rx5u9h,"Hi there! Curious, interesting question - but seemingly it may need some clarifications before being approached.

How do we deal with games in which player failed (hit mine) before clearing the field? Just dispense, don't regard at all?

By the ""least upper bound"" you mean in a case of some specially constructed arrangement (to reach such bound), not randomly placed? This seems to depend greatly on relation between number of mines and field size, right?",hrgab6n,t3_rx5u9h,1641444970.0,False
rx5u9h,"Yup to both questions. We only consider a sequence of guesses that could result in a win, but require some cumulatively amazing luck.

And you are a mastermind and specifically constructed the grid yourself to make your unluckiest players feel as miserable as possible. I also sense the assignment of mines to squares is important, but no clue which arrangement would be most devilish",hrgb80m,t1_hrgab6n,1641445427.0,True
rx5u9h,"I don't know the answer. But in my playing experience, the true 50:50 are a lot more common near the edges. Also, very important factor is mine density (which is given by you already). I believe that linear increase in mine density results in exponential growth in true 50:50. But this is just my opinion.

Given this and the answer you gave to RodionGork, I think, for general case you need to find the least mine requiring and compatible side by side with itself true 50:50 and spam it all over the plane. An example of this would be a 2 x N grid with N amount of mines, where each collumn has 1 mine in it:

|x|x|3|(3 or x)|...|2|
|:-|:-|:-|:-|:-|:-|
|2|3|x|(the other than above)|...|x|

This pattern has N-1 (considering you know first click is not a bomb) true guesses with 50% mine density. I doubt there can be more guess heavy pattern, as I mentioned before, edges are breeding ground for true 50:50's.",hri0a7o,t3_rx5u9h,1641482243.0,False
rx5u9h,"If you have a wall of mines in the 3rd row from an edge, then pretty much all the cells in the first two rows need guessing. Then you can probably just partition the grid every 3 rows and design a grid so that half of the mines need guessing (half constitutes the ""3rd row wall"" and the other half needs guessing). This will probably work when X can go all the way up to ~2MN/3",hrkutl3,t3_rx5u9h,1641521010.0,False
rx5nj6,I have an exercise for PDA if you are interested in that.,hrh3dvn,t3_rx5nj6,1641464349.0,False
rx5nj6,"Hm, I'm so noob so that it seems to be first time I hear about PDA (push down automaton?) - so would eagerly study whatever you share!",hrh9eo0,t1_hrh3dvn,1641468782.0,True
rx4z0z,2 kibibit,hrg71mi,t3_rx4z0z,1641443251.0,False
rx4z0z,On 8-bit machines we'd call 256 bytes a page. Not really required anymore so I'm fairly sure it's fallen out of use.,hrgzxwn,t3_rx4z0z,1641461659.0,False
rx4z0z,"Wow, this is cool info, I trust you but could i get a source on that so i can justify using the term myself?",hsd956g,t1_hrgzxwn,1642006289.0,False
rx4z0z,"If you're talking about modern systems you'd best ignore my vintage waffle and refer to this:
https://en.wikipedia.org/wiki/Page_(computer_memory)

If dealing with old 8-bit systems like the 6502 then it's surprisingly difficult to find anything direct. Mentions of zero-pages (first 256 bytes of memory) or page-boundaries (crossing from one 256 byte chunk or memory to another) here:
https://en.wikipedia.org/wiki/MOS_Technology_6502#Bugs_and_quirks

Maybe I just distilled the idea of pages in my own head?!",hsdzqwk,t1_hsd956g,1642016242.0,False
rx4z0z,"If we are talking about naming data size units, then the following may also be interesting to you:

* **1 kilobyte** is precisely **1000 bytes**, while **1024 bytes** is **1 kibibyte**
* **1 megabyte** is precisely **1000000 bytes**, while **1048576 bytes** = **1024 kibibyte** = **1 mebibyte**
* **1 gigabyte** is precisely **1000000000 bytes**, while **1073741824 bytes** is **1 gibibyte**

Check out [this table on Wikipedia](https://en.wikipedia.org/wiki/Byte#Multiple-byte_units)",hrhpco3,t3_rx4z0z,1641477676.0,False
rx4z0z,Chom p,hrgz81i,t3_rx4z0z,1641461093.0,False
rx4z0z,quarterkilo,hroku79,t3_rx4z0z,1641586178.0,False
rx1mtm,"Let’s say messages are times on a clock. I want people to be able to send me times, but I don’t want other people to know what those times are.

I know that if you add 12 hours to a time, it remains the same, it does a full 360 degree rotation around the clock.

So I’m going to pick two numbers, x and y, such that x+y=12. If I take any time, and add x, then add y to it, it will be the same time.

So I tell everybody what x is, that’s my public key.

When they want to send me a time m, they first add x to it and then send me m+x. Let’s pretend subtraction is extremely difficult, infeasible for anybody to do. So somebody listening into our communications will know m+x, but they don’t know m because they have a hard time subtracting x from m+x.

When I receive this message, instead of trying to subtract x, I’ll just add y instead, which gives me m+x+y=m+12=m, I now have the original unencrypted message. y is my private key which is a secret only I know.

Nobody else can do this because in order to find out what y is, they would need to compute y=12-x, and remember subtraction is difficult.

This is essentially how public key encryption works, a public/private key pair are generated to have the mathematical property that application of both, yields the original message. It also needs to be extremely difficult to compute one from the other.

In the case of RSA this would be based on modular exponentiation and prime factorization.

Modular exponentiation means m^x mod n, for some choices of n, we can find two numbers e and d, such that m^ed = m mod n, for any m. e and d are like x and y, e will be the public key which the sender will use to compute m^e mod n, which is incredibly hard to undo. d will be my private key, which I will use to compute (m^e )^d = m mod n, this is like completing the full circle around the clock and arriving back at the original message. All public key encryption is based on these “circular” type operations, a full cycle takes you back to the original message, this is broken down into two steps, one for the sender and one for decryption, the sender makes a partial cycle, which is incredibly hard to undo, the receiver however knows some secret to complete the cycle.

In order to find d from the public knowledge, which is e and n, you need to prime factor n. However n is a very large multiple of two prime numbers, n = pq. It’s over 1000 digits long, and this makes it infeasible to compute the private key from the public key as there is no quick algorithm for factoring numbers.",hrfpzgg,t3_rx1mtm,1641433129.0,False
rx1mtm,"That sums it up real good, thank you!",hrfr8hm,t1_hrfpzgg,1641434161.0,True
rx1mtm,"You can also use this description to work backwards to see how signing things works. We each have an e and a d. e is our public key, d is our private key. To send me a message only I can read, you compute m^e mod n using my public key e. Now only I can read it because only I have the right d to use in (m^e)^d mod n.

To sign a message, you instead compute m^d mod n with **your** private key d. Now the only way anyone can read the message is if they have the right e to compute (m^d)^e mod n, and that choice of e has to be your public key. We can all get your public key, so the message is readable to anyone, but only you could have produced the plain text.",hrhuml5,t1_hrfr8hm,1641479952.0,False
rx1mtm,"It’s not clear what you mean by 
> and I sent it to the receiver through symmetric encryption

Public and private keys are used in the context of assymetric encryption. What is encrypted with the public key can only be decrypted with the private key.",hrfj9i4,t3_rx1mtm,1641430316.0,False
rx1mtm,If they don't know the private key how do they ow do they encript it so that only my private key can decrypt?,hrfl3m8,t1_hrfj9i4,1641431063.0,True
rx1mtm,With your public key,hrfos2i,t1_hrfl3m8,1641432573.0,False
rx1mtm,"Let’s give an example:

You want to send me a document secretly over a public network. I send you my public key but a spy also reads it. 

You encrypt the document on your computer with my public key. Then you send the encrypted document over the network. The spy steals a copy of the encrypted document.

The spy has the encrypted document and the public key. But he needs the private key to decrypt it and I never sent that one out so he cannot decrypt. 

I receive the encrypted document and only I can decrypt it since I am the only person to have the private key that matches the public key.",hrfpwr8,t1_hrfl3m8,1641433081.0,False
rx1mtm,"Where where you 2h ago XD I get it, i thought the public key was something that already existed, and not something that I create at the same time I create the private one...",hrfqzr4,t1_hrfpwr8,1641433934.0,True
rx1mtm,"I like to think of the ""public key"" as a lock you can put on data, and the ""private key"" as the key to that lock",hrgx34m,t3_rx1mtm,1641459400.0,False
rx1mtm,You create key pairs. A private key to decrypt and a public key to encrypt. You exchange public keys with someone to send encrypted messages between yourselves. You use that person public key to encrypt a message and send it to them. The only thing that can unencrypt that message is that person associated private key. They do the same thing sending you a message by using your public key to encrypt the message and you use your private key to decrypt it.,hrfkrvw,t3_rx1mtm,1641430929.0,False
rx1mtm,How do they encript in a way that only my private key can decrypt?,hrfkzys,t1_hrfkrvw,1641431021.0,True
rx1mtm,They use your public key. The only thing that can decrypt a message encrypted with your public key is your private key,hrflk3v,t1_hrfkzys,1641431246.0,False
rx1mtm,Can't everyone else use the public key to also find the private key?,hrfln0n,t1_hrflk3v,1641431280.0,True
rx1mtm,"No. The key pairs are asymmetric. The public key can be openly distributed without compromising your private key.

The generation of such key pairs depends on cryptographic algorithms which are based on mathematical problems termed one-way functions.

https://en.m.wikipedia.org/wiki/One-way_function",hrfnksu,t1_hrfln0n,1641432083.0,False
rx1mtm,Yeah that's what I was having problems with... Thank you,hrfo77r,t1_hrfnksu,1641432336.0,True
rx1mtm,"Yes, but it’s very difficult to do that. It takes a certain large number of calculations, so it is practically impossible to find someone’s private key by using their public key. More specifically, for RSA encryption, you would have to efficiently perform [integer factorization](https://en.m.wikipedia.org/wiki/Integer_factorization).",hrfo61z,t1_hrfln0n,1641432323.0,False
rx1mtm,"I got it! Out of curiosity though, is there already a solution for when computers get more powerful and we can efficiently perform integer factorization?",hrfol2r,t1_hrfo61z,1641432493.0,True
rx1mtm,"The algorithm already exists! It's just that the the time needed by the fastest supercomputers we have today to break it would be... well, the universe itself won't last that long.

And since Moore's law is slowing down, i.e. there is a limit of how much faster computers can get, then we can fairly confidently say classical computers won't ever be able to defeat such encryption.

But there are now quantum computers, which are especially good at such kind of tasks, thus posing a serious threat to all of the cryptography.",hrgx16x,t1_hrfol2r,1641459354.0,False
rx1mtm,https://en.m.wikipedia.org/wiki/Public-key_cryptography,hrfn4ey,t1_hrfkzys,1641431896.0,False
rx1mtm,by using your public key,hrfl4wm,t1_hrfkzys,1641431077.0,False
rx1mtm,That way can't everyone else do the same?,hrfla0d,t1_hrfl4wm,1641431133.0,True
rx1mtm,ok so they encrypt the message to you with your public key right? well you use your private key (THAT ONLY YOU KNOW) to decrypt it! Who cares if people can send you encrypted messages? only you can decrypt it!,hrfmpyk,t1_hrfla0d,1641431729.0,False
rx1mtm,"Oohh i get it now, they are both generated by me right, and I share the public one, that is the one used by the receiver...
I don't know how they are generated but that is work for later... Thank you",hrfo28w,t1_hrfmpyk,1641432281.0,True
rx1mtm,"Yep exactly, it's called assymetric key encryption for that reason. Different keys are used for encryption and decryption. Symmetric key encryption on the other hand uses the same key for both.",hrgz5eu,t1_hrfo28w,1641461036.0,False
rx1mtm,"Check out GPG, https://en.m.wikipedia.org/wiki/GNU_Privacy_Guard",hrfovz1,t1_hrfo28w,1641432618.0,False
rx1mtm,I will!,hrfr4ob,t1_hrfovz1,1641434062.0,True
rx1mtm,are you trying to learn PGP encryption?,hrfpj6u,t1_hrfo28w,1641432881.0,False
rx1mtm,"I am not, i just needed to understand how public key encryption worked, the rest is just curiosity",hrfr06m,t1_hrfpj6u,1641433946.0,True
rx1mtm,The encryption and decryption are asymmetric. Meaning they are one way functions. Someone with your public key can only encrypt a message. Someone with your private key can only decrypt the message encrypted with your public key. This is why you keep your private key private and secure. You usually have a passphrase associated with it for added security.,hrfmue9,t1_hrfla0d,1641431780.0,False
rx1mtm,"You keep your private key,  well...private.",hrflo70,t1_hrfla0d,1641431293.0,False
rwv1vj,"They get published in the same way as any other paper via the peer review process. Theoretical papers are assessed based on the quality of the proof (usually more than one) associated with the theory, the underlying evidence of the theory, how well it is explained, the significance/impact, etc.

You might want to look at theoretical engineering journals and theoretical computer science journals to get an understanding of how such papers are written.  


International Journal of Industrial Engineering

Engineering Science and Tecnology

Research in Engineering Design

Systems Engineering - Theory and Practice

Engineering Science and Technology

SIAM Review

Foundations and Trends in Theoretical Computer Science",hria1et,t3_rwv1vj,1641485985.0,False
rwv1vj,"There are also a few conferences dedicated to ""space"" systems, see for example the NASA formal method conferences, https://easychair.org/cfp/nfm-2022

You can take a look at work published there to get an idea.",hrixzij,t1_hria1et,1641494728.0,False
rwnjr8,[deleted],hrcxlic,t3_rwnjr8,1641394056.0,False
rwnjr8,"I would recommend using a library such as sklearn if possible. [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

  
They have parameters to control the initialization and also the number of different runs.

Also checkout this: [https://en.wikipedia.org/wiki/K-means%2B%2B](https://en.wikipedia.org/wiki/K-means%2B%2B)",hrdybc3,t1_hrcxlic,1641407775.0,True
rwly7h,I'd make this a top-6 and add any clean coding book by Uncle Bob.,hrd8i4q,t3_rwly7h,1641398423.0,False
rwly7h,"Thanks for sharing. 

Will definitely come in handy for aspiring software developers like me :-)",hrd1el6,t3_rwly7h,1641395628.0,False
rwly7h,"I like “The effective engineer”

Thanks for the list, definitely going to do some more reading this year 🙂",hrfng63,t3_rwly7h,1641432030.0,False
rwly7h,"Grabbed the Passionate Programmer after reading your post!  I'm only a year or two into the industry and am having many questions, a lot of self-reflection, and fork in the road moments, so hopefully that book will provide some sort of benefit, thanks!",hrhe4e2,t3_rwly7h,1641471822.0,False
rw99z6,https://en.wikipedia.org/wiki/Wolfram's_2-state_3-symbol_Turing_machine,hrakk3s,t3_rw99z6,1641346039.0,False
rw311s,"When using pointers, you are mostly operating on virtual memory space provided by your operating system, which takes care of managing memory for each process. Internally, mechanisms like paging are used to divide the space into equal sized pages which can be put to swap space and reloaded from disk when needed. The MMU takes care of translating virtual addresses to physical memory addresses. It’s actually a lot more complicated than that, but your operating system knows exactly which process allocated which parts of memory and keeps track of everything including memory boundaries and more.",hr983ax,t3_rw311s,1641326894.0,False
rw311s,"Theres a few mechanisms in place, all are way too complicated to write on a single reddit post.

In a nutshell, the **OS** gives each process the **illusion** it has a big and clean memory space. Which means a few processes can reach into the address 0x1234 and store data there. 

All of this illusion (called effective addresses) is being handeled at the os. Then a combined effort of the os and the harware to translate this effective address into real phyiscal address in the dram.

This has nothing to do with any perticular progremming languege, you can take advantages of it to gain some speed up - but for most applications its not needed.",hr988u7,t3_rw311s,1641326953.0,False
rw311s,https://www.freecodecamp.org/news/understand-your-programs-memory-92431fa8c6b/,hr9jsxl,t3_rw311s,1641331338.0,False
rw311s,"It depends.

If `my_var` is on the stack, then its address increments/decrements (depending on the CPU architecture) as new variables are allocated. The CPU has a register to track the latest stack location in memory (called a stack pointer), and the compiler knows how much to add/remove from this register to reserve space in the memory for new variables.

For example, if I create two local variables (local variables get created on the stack), then their addresses should differ by the size of those variables, possibly rounded up to the nearest word. `long` is equivalent to one word or its multiple on most OS so there shouldn't be any rounding in this example:

    #include <iostream>
    
    int main() {
        long on_stack1;
        long on_stack2;
    
        std::cout << ""Address of on_stack1: "" << &on_stack1 << std::endl;
        std::cout << ""Address of on_stack2: "" << &on_stack2 << std::endl;
        std::cout << ""The difference in the address should be "" << sizeof(long) << "" bytes"" << std::endl;
    
        return 0;
    }

The value of the stack pointer is initially set by the OS when the program starts.

If, on the other hand, `my_var` is on the heap (e.g., created with `new`), then the address is dynamically allocated by the memory allocator (which comes with the C/C++ library for the OS) so how it allocates the memory depends on the allocator's algorithm. To avoid the heap's memory allocator accidentally using the memory used by the stack, the allocator tries to reserve heap memory on the ""opposite"" end of the memory address from the stack as far away from it as it can. You can see a typical memory layout here: [https://www.geeksforgeeks.org/memory-layout-of-c-program/](https://www.geeksforgeeks.org/memory-layout-of-c-program/)

    #include <iostream>
    
    int main() {
        long on_stack;
        long* on_heap = new long;
    
        std::cout << ""Address of on_stack: "" << &on_stack1 << std::endl;
        std::cout << ""Address of on_heap: "" << on_heap << std::endl;
        std::cout << ""The address of on_heap should be far from on_stack."" << std::endl;
    
        return 0;
    }

If you use a lot of memory it is possible for the stack to crash into the heap. And it is possible for a memory allocator to fail to allocate a large chunk of memory even if there are many small pieces of memory due to memory fragmentation. A part of the memory allocator algorithm's goal is to minimize such memory fragmentation.

Also, if `my_var` is a global variable or a constant, the compiler reserves a space for it within the binary so that when the program is loaded into the memory the variable is already reserved an address. Its address can be calculated as the starting address of the program + offset to the variable in the binary.

    #include <iostream>
    
    const char* on_global = ""You should see this string in the binary"";
    
    int main() {
        std::cout << ""Address of a constant: "" << (void*)on_global << std::endl;
        std::cout << ""Address of a global variable: "" << &on_global << std::endl;
    
        return 0;
    }

As others have pointed out, modern OSes, with the assistance of modern CPU architectures, use virtual memory so the addresses printed out by the above programs are not the memory's physical addresses. There exists a memory mapping mechanism to use the memory more efficiently and protect one program's memory from each other. I like these videos for understanding virtual memory: [https://www.youtube.com/watch?v=qcBIvnQt0Bw&ab\_channel=DavidBlack-Schaffer](https://www.youtube.com/watch?v=qcBIvnQt0Bw&ab_channel=DavidBlack-Schaffer)",hrjhmdt,t3_rw311s,1641501927.0,False
rw2dl6,A pointer is directly a memory address. Most high level languages don't allow you to work directly with them at all or at least try to prevent it. A python id is a unique integer identifier but is NOT a memory address.,hr97jwh,t3_rw2dl6,1641326688.0,False
rw2dl6,"Theres quite a bit difference.

For example you cant do any pointer arythmetic explicitly in python, with or without the usage of id.

More generally, you cant reliably infer an id of one object based on the id of another object even if you know how the code looks ljke.

I havent found a place to use this function to be honest, i assume python use it mostly under the hood.",hr96fn7,t3_rw2dl6,1641326255.0,False
rw2dl6,"I have onky used id to store objects in a dictionnary, it gives a hashable key that is unique, even if two objects have the same representation at a some point.

You could also use it to check if two objects are the same. Not only the same representation, but exactly the same memory, if you change one, both change. But the ""is"" do this and is more readble.

So it can be useful, but in quite specific places, and not for any low-level pointer operations.",hrbslmf,t1_hr96fn7,1641368162.0,False
rvy218,"Good work, you put in the effort and know you’ve not only taught others but made sure you know it yourself.",hr8no35,t3_rvy218,1641319035.0,False
rvy218,"Yes, ai find it an effective way to go back to the basics.",hr8siun,t1_hr8no35,1641320922.0,True
rvy218,"I've been thinking I needed a refresher on my linear algebra lately, thanks for posting this.",hr90cpt,t3_rvy218,1641323914.0,False
rvy218,You're welcome! Will post more linear algebra posts so keep an eye out 😏👀,hr96m79,t1_hr90cpt,1641326326.0,True
rvy218,Looking forward to them!,hr96zj4,t1_hr96m79,1641326470.0,False
rvy218,Fantastic! 🙌🔜,hr97ezm,t1_hr96zj4,1641326635.0,True
rvy218,"Next up : Cayley-Hamilton Theorem and calculating A^(k) with it.

""*A square matrix satisfies it's own characteristic equation*""",hr98zkp,t3_rvy218,1641327238.0,False
rvy218,Love the A^k calculation 🤩,hr9fuv1,t1_hr98zkp,1641329841.0,True
rvy218,"Thanks, I hate it",hr9azri,t3_rvy218,1641328003.0,False
rvy218,😛 you're welcome 😁,hr9fyri,t1_hr9azri,1641329882.0,True
rvy218,Imma save this for later and look out for more. Thank you!,hr983pq,t3_rvy218,1641326898.0,False
rvy218,🧘‍♂️👌,hr9fx1n,t1_hr983pq,1641329864.0,True
rvy218,"Oh my days .. eigenvectors and eigenvalues!! 

Quite interesting topic especially in machine learning.",hr9ut6y,t3_rvy218,1641335589.0,False
rvy218,"Yeah, it is one of my favorite topics from linear algebra. :) hope you got something out of it.",hr9vvli,t1_hr9ut6y,1641336007.0,True
rvy218,Yes I did .. saved the visuals also .. thank you.,hr9x0sz,t1_hr9vvli,1641336459.0,False
rvy218,Enjoy reading it. Looking forward to your next post!,hragcu3,t3_rvy218,1641344279.0,False
rvy218,Thanks for the encouragement! 💯,hrbsf8p,t1_hragcu3,1641368038.0,True
rvy218,Don't forget page rank!,hrc6qf0,t3_rvy218,1641378867.0,False
rvy218,Oh wow! I let that one slip under the radar.,hrc6wj1,t1_hrc6qf0,1641378995.0,True
rvrtx4,"No, not really.

A single clock cycle is the shortest period of time within which you expect anything to happen within the CPU. It's the resolution of time for the CPU, in a sense.

It takes time, however brief, for electrical signals to propagate within the CPU. It takes time for transistors to switch.

When the CPU runs, say, an instruction to add two integers together, it does that by passing current through a logic circuit that produces the correct output bits from the input bits. The output is not instantaneous. Rather, the CPU would check the output at the next clock cycle (or possibly after a set number of clock cycles).

Disclaimer: I'm not an electrical or electronics engineer. If someone knows better and I said something wrong, go ahead and correct.",hr7by93,t3_rvrtx4,1641297441.0,False
rvrtx4,"Moreover, OP should look into how different CPU architectures handle complexity, because 2.5Ghz in CISC is different compared to RISC and different layouts, number of cores, type of parallel processing, semaphores etc. impact CPU speed differently. Which is why when buying a CPU you should always look at benchmarks rather than specs alone",hr874c9,t1_hr7by93,1641312577.0,False
rvrtx4,Read [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code),hr8g71l,t3_rvrtx4,1641316163.0,False
rvrtx4,"No. The short, hand-wavy reason why most CPUs have a clock frequency is because they are synchronous circuits. These use a periodic signal, the clock, to coordinate their operation. The opposite of synchronous circuits would be asynchronous circuits, where there isn't a clock signal coordinating activities, but a multitude of control signals that implement some sort of protocol for sequencing events.

Regarding the frequency at which bits move around in a CPU; what is quoted as the CPU's clock frequency has some relationship to the rate at which bits move around, but that's missing the point of why there is a clock in the first place (which isn't to describe the rate at which bits move around). The reason why there's some relationship is because there will be parts of the CPU where data being moved to different parts, or processed, at the same rate as the clock frequency. This isn't a hard requirement; it's a result of there being a clock signal. There will be parts where data moves at a lower rate than the quoted clock rate; and possibly at a high rate in others.",hr8j26c,t3_rvrtx4,1641317270.0,False
rvq0w4,"Yes. I know how overvolting and overheating affects computers in chemical level, and what to expect.

If you gonna write scientific software for chemistry, you gonna need a lot of fundamental stuff while programming.",hr761bd,t3_rvq0w4,1641293199.0,False
rvq0w4,Nope.,hr73gdb,t3_rvq0w4,1641291171.0,False
rvq0w4,Some of the phys chem shows up in reversible computing and things like Landauer’s limit.,hr7y5vb,t3_rvq0w4,1641308931.0,False
rvq0w4,"Nope.

But: Math a lot, physics kind of, biology even, and all the language classes.",hr7oe8t,t3_rvq0w4,1641304497.0,False
rvq0w4,How did you implement the languages in your code?,hr9cci8,t1_hr7oe8t,1641328517.0,False
rvq0w4,Knowing several languages with very different syntax helps when you get into functional programming or other declarative languages; it’s also nice to know some linguistics when you are tasked with coding UI strings for an international application; or some phonetics when implementing a “fuzzy match” algorithm for a search engine in a language not supported by existing Soundex implementations.,hr9fadq,t1_hr9cci8,1641329626.0,False
rvq0w4,NLP,hr9gfrf,t1_hr9cci8,1641330060.0,False
rvq0w4,"Lots of my colleagues do, although computational chemistry is kinda cheating.",hr7xukj,t3_rvq0w4,1641308800.0,False
rvq0w4,"I've written software to control robotic arms used in chemical test apparatus, if that counts?
 
There's an entire field of medical technology that is the intersection of biology (including chemistry) and computer science if that's something you're interested in. Has a bunch of equivalent names and subfields like digital health, health technology, e-medicine, e-health, telehealth, bioinformatics, many others.",hr844co,t3_rvq0w4,1641311377.0,False
rvq0w4,"All the time, but I’m in a niche field for it.",hr86ppf,t3_rvq0w4,1641312415.0,False
rvq0w4,"Tried, but",hr7hvse,t3_rvq0w4,1641301086.0,False
rvq0w4,"Chemistry is where you’re most likely to learn things like the ideal gas law and Newton’s law of cooling. Both can be relevant in any situation where you have something generating heat that needs to be dissipated, like a CPU.",hr897ia,t3_rvq0w4,1641313413.0,False
rvq0w4,Only when sharing knowledge tidbits at the coffee machine.,hr8bdfq,t3_rvq0w4,1641314268.0,False
rvq0w4,"Yes. Inorganic chemistry is important to hardware development. 

If you're looking to do higher level development, then it's less important, although there are computational chemists.",hr8iiyi,t3_rvq0w4,1641317065.0,False
rvq0w4,"yes, i turn coffee into code",hr94h8n,t3_rvq0w4,1641325502.0,False
rvq0w4,"Bioinformatics graduate here. The only thing I used was in the context of Chemical and Biological weapons used in terrorism and that was because I was a DoD software engineer contractor....

Nothing else. Big waste of time and organic chemistry suffering.",hr95avk,t3_rvq0w4,1641325818.0,False
rvq0w4,"I’m pretty sure I tested out of chemistry 1 and only had to take chem 2. Was a bit more relevant for the EE part of my EE/CS degree. 

Other than that, no. I think I used Coca-cola to clean off the car battery in my shitty Camaro to get to work…that’s about it.",hr9c9kq,t3_rvq0w4,1641328486.0,False
rvq0w4,Never again. Lol,hr9smwc,t3_rvq0w4,1641334744.0,False
rvq0w4,"I avoided all the chemistry classes I could. Really depends what you want to go into, most probably don't need it but there are always those specialized areas where it's needed.",hraksse,t3_rvq0w4,1641346143.0,False
rvq0w4,I do in computer engineering. But I'm in an odd niche researching chemical reaction networks and genetic circuits.,hrax5to,t3_rvq0w4,1641351342.0,False
rvq0w4,"Also, in same vein have y'all ever used discreet math, calculus in your career?",hr75mpb,t3_rvq0w4,1641292886.0,False
rvq0w4,"Indeed, it's nice skill to have when building stuff related to ML/DL, as well as in other CS areas, like cryptography and computer graphics (probably also others that i never worked with and as such aren't aware).",hr79b4f,t1_hr75mpb,1641295613.0,False
rvq0w4,"Discrete maths yes (quite a broad field though) but calculus no. Still glad I learned calculus though, definitely gives you a broader understanding of maths, which indirectly will improve your programming.",hr7yfty,t1_hr75mpb,1641309047.0,False
rvq0w4,All the time.,hr7yu7r,t1_hr75mpb,1641309219.0,False
rvq0w4,My whole Ph.D. and the post-grad studies build upon these and how to calculate that stuff faster.,hr762o4,t1_hr75mpb,1641293228.0,False
rvq0w4,"Only the smart ones need DMath, Calc, Combinatorics, and Linear",hr86jwe,t1_hr75mpb,1641312350.0,False
rvq0w4,Computers are discrete math machines. Programming languages are discrete math languages.,hr8ffhk,t1_hr75mpb,1641315860.0,False
rvq0w4,Yes actually.,hr8orw4,t1_hr75mpb,1641319461.0,False
rvq0w4,Absolutely,hr9burh,t1_hr75mpb,1641328330.0,False
rvq0w4,"Even if you do not use the math itself, the method of thinking is the same",hr7vhva,t1_hr75mpb,1641307784.0,False
rvfquw,"It wasn't a problem with storing a date, but a version number which was beginning with date.

So MS has such versioning convention: `YYMMDDxxxx` (where `x` are not important for us here).

So lets create a smallest number for 2022 which fits this convention: `2201010000`  
As you can see, it's bigger than `2147483647`",hr56orc,t3_rvfquw,1641254619.0,False
rvfquw,they should of just used long long,hr6p6sq,t1_hr56orc,1641280215.0,False
rvfquw,or unsigned int,hr70qzb,t1_hr6p6sq,1641289017.0,False
rvfquw,What about the time before Christ? Can't forget about the dinasours,hr755vl,t1_hr70qzb,1641292519.0,False
rvfquw,I didn't know the dinosaurs were fewer than 2022 years before Christ...,hr7jaws,t1_hr755vl,1641301879.0,False
rvfquw,what about 1999,hr7kvbf,t1_hr755vl,1641302720.0,False
rvfquw,Why not string,hr73x49,t1_hr6p6sq,1641291545.0,False
rvfquw,"They were using the version number to check if the version installed is less than than the latest patch available, so they would have had to convert it to a numeric format anyway for comparison since string < string would produce unreliable results depending on the platform.

They could have used a long (64 bit number), unsigned integer, or done a bit of bit-shifting to more efficiently pack the date and version number into 31 bits (apparently one bit was reserved for something else).",hr8e3eh,t1_hr73x49,1641315333.0,False
rvfquw,"> Why did the Microsoft date bug happen in 2022?

Apparently, their test suite sucks.",hr6eoao,t3_rvfquw,1641273796.0,False
rvfquw,How many tens of thousands of employees do you have to hire before someone writes a unit test these days?,hr6jwii,t1_hr6eoao,1641276748.0,False
rvfquw,Would be good to just run the suite with a date a year in the future... they could have caught this a year ago instead of testing in production.,hr6llgq,t1_hr6jwii,1641277811.0,False
rvfquw,Did you read any of the actual coverage of the story? Genuine question; every article I read explained quite clearly what the problem was...,hr7nkqy,t3_rvfquw,1641304103.0,False
rvc4y3,"I mean Minecraft is a great example of programs in general, even being able to compare the same app written in two different languages, Java and Bedrock editions.",hr4j6yz,t3_rvc4y3,1641245256.0,False
rvc4y3,"I think most of you misunderstood me haha. I don’t mean that I want to teach within minecraft, but how the program itself could be programmed. Like which classes, dependencies etc. you would need to program Minecraft",hr5bfl3,t3_rvc4y3,1641256592.0,True
rvc4y3,"I like the classic ""dog and dat and both subclasses from animals"" approach, but you sure can explain OOP using Minecraft.",hr5h72g,t1_hr5bfl3,1641259013.0,False
rvc4y3,"Dog and cat being an animal is one of the worst introductions to OOP that is responsible for so much misunderstanding of what inheritance is to the point it could be given a trophy for worst teaching analogy of all time.
 
First, even if we are leaving composition over inheritance to later, by introducing it that way the misunderstanding you invite causes repeated breaches of the Liskov substitutability principle, because students learn to model their code based on the taxonomy in the real world, which is completely wrong.

Inheritance is not an 'is a' relationship from the real world, that is a gross oversimplification by people who didn't know what inheritance was for. The classic counterexample is square and rectangle - [see this talk by Bob Martin](https://www.youtube.com/watch?v=zHiWqnTWsn4&t=4430s).
 
Rather than modelling the real world behaviour of things in classes, it is far better to approach classes for what they actually are - collections of **software behaviour**. That way you never fall into the square-rectangle trap, but you still arrive at the shape-square and shape-rectangle behaviour (which is also in that talk, it's a good one to watch start to finish).
 
While you could do that with dog and cat, you've gone and abstracted software needs by swapping in 'real world' needs like walk and speak, which makes it really confusing when students are then writing their own code based on software needs.",hr5yyzs,t1_hr5h72g,1641266455.0,False
rvc4y3,"I got your point and the presentation you linked is pretty good too, but I don't think it would a good approach to introduce OOP directly with software behavior. At this stage, people barely know how to write a ""real life"" software.

Also while SOLID is desirable while implementing OOP, they are distinct topics and should be teached separately. For the core concepts of OOP, the real life examples should work fine. When teaching SOLID you can then revisit this examples and show why they doesn't work under SOLID, just like the guy in the video did.",hr79iza,t1_hr5yyzs,1641295774.0,False
rvc4y3,"You don't teach SOLID as a 'thing' on day 1, but you absolutely need to exemplify it and make it a habit in your examples and discussions from day 1. Otherwise you are creating bad habits and assumptions that never need to exist.
 
There are 3ish schools of thought on when they should learn OOP at all, objects-first, objects-early, objects-late. If they are objects-late, then you can give them whatever you like, because they can craft some decent procedural code and you can build on those concepts. Remember that there was a point in time where *everybody* learning OOP was in this category.
 
If they are objects-early, then they already know enough that you can draw out objects and a class structure motivated by the behaviours they already know about. A Zork clone is more than sufficient, which you can learn enough programming to create (procedurally) in a day or 2 boot camp. Rooms, exits, items in the rooms, etc. Easy peasy, and you can even show them the flaws in creating an inheritance structure based on is-a real world relationships when you do so, by comparing your 'what common behaviours do we want to bundle' vs 'let's model a maze in real world concepts' versions. It doesn't need to be 'real life' code, it needs to be real motivations from the software, even if that software is trivial.
 
If they are objects-first, then the best thing to do is give them a scaffold with a canvas object and do shapes. You can do this with BlueJ in the first hour they are at a keyboard.
 
If you are teaching them any other way, then what you have almost certainly been blinded by is the **syntax** of OOP, which is both the easiest and stupidest thing to teach a newbie student. Yes, they'll need to know the syntax of whatever language, they'll need to learn how access modifiers work, they'll need to understand scope, etc etc etc. But that is all crap they can learn by breaking things along the way, and is a distraction from the more useful starting concepts. If they cut and paste boilerplate for a semester but can successfully grow a useful hierarchy out of some software requirements or procedural pseudocode they are light years ahead of someone who has memorised the syntax but will inherit from Rectangle.",hr7cozo,t1_hr79iza,1641297928.0,False
rvc4y3,"Totally agree that an actual real world programming example is much better than the classic dog, cat, animal teaching.",hr64zzf,t1_hr5yyzs,1641269112.0,False
rvc4y3,What? Nah you should build a red stone computer and teach programming using that /s,hr802eb,t1_hr5bfl3,1641309730.0,False
rvc4y3,"Great idea, for more advanced units you could even install computer craft which allows you to write LUA code to control little robots and other blocks",hr4ohyi,t3_rvc4y3,1641247292.0,False
rvc4y3,I’m not sure dependency injection is used in Minecraft?,hr59n7w,t3_rvc4y3,1641255847.0,False
rvc4y3,You really think minecraft a game that literally sold for a billion dollars and is now run by Microsoft doesn't have aby dependency injection? For some reason I really doubt that,hrx8n11,t1_hr59n7w,1641742046.0,False
rvc4y3,"Dependency injection isn’t useful in all scenarios. As I understand it, it’s less common in game development than it is say, enterprise backend development

Them being Microsoft and having lots of money and programming experience doesn’t mean they’re going to use a certain pattern every single time.",hrxza2r,t1_hrx8n11,1641752140.0,False
rvc4y3,Dependency injection at a basic level is greatly useful for making classes testable and reusable. Surely it's riddled throughout minrcrafts code base.,hry710p,t1_hrxza2r,1641754849.0,False
rvc4y3,"In my opinion Minecraft redstone(which can really be thought of as a massive, 3d state machine) is a great way to teach computer architecture, but not necessarily higher level softer design. You can use it to construct simple computers that reflect the same high level digital logic in real computers. I can honestly say that my understanding of the intricacies of a computer has really been solidified after building a redstone computer",hr58nrj,t3_rvc4y3,1641255440.0,False
rvc4y3,They have an entire curriculum and separate game that teaches coding [here](https://education.minecraft.net/en-us/get-started),hr4ynso,t3_rvc4y3,1641251321.0,False
rvc4y3,"Any game is going to be perfectly fine to teach with - if you're teaching at a professional level where you need to grade it's going to be hard with such a well-covered piece of software to know what is your students' work and cut and pasted without understanding from the internet, so for that reason I make sure to keep it well away from any assessments and stick with lesser known games if needed.",hr5zd7j,t3_rvc4y3,1641266624.0,False
rvc4y3,just teach logic in highschool,hr5o0j5,t3_rvc4y3,1641261860.0,False
ruwflx,"An installer is usually called a software installation package which (normally) includes a setup wizard to walk through which features should be added and to set up initial settings.

It's important to point out its usually a package, because it's packed up- a lot of data is compressed and in the case of online installers much of the data is omitted in favor of a downloading protocol for the data from an online server. Think Steam download, but for a single game or software. I would say this is the most common method for modern installers and why they can be smaller than a hundred megabytes.

For some, it is a fully packed software (offline installers), but these are often very large and tell the software how to decompress and reassemble itself.

A lot of libraries can be omitted because they come pre-installed or previously were and require separate downloads (think C++/.NET redistributables) which are shared between many apps.

Games usually only package the engine and boiler plate code and basic art (in the case of installers), then patches and more complex art assets are downloaded online as updates. That's why a lot of games will pitch a fit if you try to run it for the first time offline. Software in general usually includes an updater for patches as the installer package usually just includes the base code.

Specific components of an installer could be: decompression tool, downloader, updater, setup wizard, validater (making sure all common libraries/dependencies are there), along with a few others.",hr28k43,t3_ruwflx,1641209641.0,False
ruwflx,So basically the size difference comes from the compression algorithms that were used and from omitting certain libraries and having .NET / C++ package manager install them if they were not previously installed? (In the case for offline installers),hr2a00m,t1_hr28k43,1641210663.0,True
ruwflx,"Yes exactly, the runtime redistributables are usually handled by Windows though.",hr2be6l,t1_hr2a00m,1641211637.0,False
ruwflx,"The installer is a dam wall, you hook up to it and it releases the dam water (the application) the installer is a completely separate program to manage the installation of a much larger complex app.",hr1umh3,t3_ruwflx,1641198715.0,False
ruwflx,"In your analogy, where does the water come from? And how come that dam is so much smaller than the water?",hr1w3pg,t1_hr1umh3,1641199878.0,True
ruwflx,"The water comes from a server and the damn is a networking/file configuration tool that connects with the host to open a file stream (connect the pipe to the damn wall) then the application starts pouring through, the installer handles keeping the connection open and
Managing the download progress. The installer just manages the flow of data, but the amount of data behind it is semi arbitrary, you just would t build an installer to install small, simple files.",hr1wduc,t1_hr1w3pg,1641200100.0,False
ruwflx,"I was asking about offline installers that have a significant size difference, they have to store the software inside the installer itself, take for example the old days when the installer was stored on a physical CD that was up to 700mb and still the software could go up to 5gb or even more, but this also happens with more modern installers",hr1wt3k,t1_hr1wduc,1641200440.0,True
ruwflx,"OIC yeah the compression ratios are nuts, I’m with ya there",hr33s6g,t1_hr1wt3k,1641225965.0,False
ruum67,"This might be some core info for machine architecture / systems people, but I have to disagree that everyone needs to know this stuff. This seems far more intricate than anyone would reasonably need to learn in depth for a college CS degree, and with more and more people going into higher level software engineering, all the stuff here just seems more and more niche.

This is great stuff for someone looking to specialize into hardware, but for your average programmer? This is way too much.",hr3oi50,t3_ruum67,1641233794.0,False
ruum67,"I agree that the title could have been more appropriate. I think Ulrich Drepper got inspiration from the title of another paper that u/Cull_The_Meek mentioned titled ""What Every Programmer Should Know About Floating-Point Arithmetic"". Although I think that it should be a great read for all programmers because it just clears up a lot of confusion that you have while working with high-level langauges, but sure it isn't for an average programmer.

It seems like a good read to me coz I spend most of my time writing low level langs and fiddling with databases",hrgjrwy,t1_hr3oi50,1641450143.0,True
ruum67,Awesome! I once read “What Every Programmer Should Know About Floating-Point Arithmetic” and it was an eye opener for me. Saving this for my reading.,hr1sa1a,t3_ruum67,1641196909.0,False
ruum67,"> I once read “What Every Programmer Should Know About Floating-Point Arithmetic”

[This](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) one?",hr1vrzs,t1_hr1sa1a,1641199621.0,True
ruum67,Yup! It’s mentioned in the introduction of the article OP posted as well.,hr28rj6,t1_hr1vrzs,1641209785.0,False
rutayn,"Quantum computers don’t just magically run classical algorithms faster than classical computers. It’s more like the quantum computer can do NEW special operations in a single time step that a classical computer would take MANY time steps to emulate. Some problems can be made dramatically faster if a NEW algorithm is invented that uses these special NEW operations.

An analogy would be like comparing a computer that can only do one add in a time step with one that can do a multiplication in a time step. If you are trying to multiply a number by n the adding machine might do it on O(n) by adding in a loop, but the machine that can multiply directly is just O(1).",hr1fgul,t3_rutayn,1641188514.0,False
rutayn,What are these “new special operations”?,hr3254j,t1_hr1fgul,1641225291.0,False
rutayn,"There are Quantum-specific logic gates like the Hadamard gate, CNOT, and many others that do operations on one or multiple qbits and manipulate the state of the qbit before it is observed.",hr3e7cx,t1_hr3254j,1641229992.0,False
rutayn,Layperson here. So does this double the logical output of all the execution units in a circuit? Instead of having q and qbar we could halve the footprint of chips? And qutrits could theoretically triple the logical output of a given execution unit?,hr3tvdt,t1_hr3e7cx,1641235751.0,False
rutayn,"It’s not so simple as “double” or “triple”. At a high level (simplified) the quantum computer lets us setup “complex” quantum states, and combine them together in certain ways so the intermediate results have complex entanglement and setup a kind of interference so the answer falls out at the end. It is not easy to explain as these new quantum operations are HIGHLY NON INTUITIVE. If you want to dive deeper you could look at [the Wikipedia entry on quantum computation language](https://en.m.wikipedia.org/wiki/Quantum_Computation_Language) to start getting and idea of what these new operations are and how they could be used. However I suspect that very few programmers in the future will be building these quantum algorithms directly and will mostly use higher level libraries which deal with the quantum particulars and have “standard” classical APIs. In other words it won’t be that far off from how GPUs can be used to “accelerate” some kinds of computations today, but aren’t used for the general purpose part of programs (which CPUs are excellent at already).",hr4cjd6,t1_hr3tvdt,1641242689.0,False
rutayn,"Interesting, thanks, giving it a read. I guess I was sort of imagining an extension to ""normal"" ISAs but that's clearly not the case. The possibility of a distinct set of programmers dealing with quantum functions is very interesting though.",hr4kaha,t1_hr4cjd6,1641245678.0,False
rutayn,"Probably the first few waves of professional quantum computer programmers will be those who already have backgrounds in quantum physics, mathematics and the like.",hr585fs,t1_hr4kaha,1641255234.0,False
rutayn,Probably? Better asking whether there will ever be any other quantum programmers!,hr59hrd,t1_hr585fs,1641255783.0,False
rutayn,Pretty much lol. Until quantum operations can be captured by a more convenient interface. But that's unlikely give the level of sophistication required.,hr59tq8,t1_hr59hrd,1641255920.0,False
rutayn,"> does this double the logical output 

What do you mean by this? Like double the number of booleans outputted by a given circuit?

> we could halve the footprint of chips

The physical requirements for quantum computers are entirely different from those of regular ones. Modern quantum systems have tens or dozens of bits, with some cutting edge ones having hundreds. While it's extremely likely that they will improve in a similar manner to the regular computers we are used to, it is definitely much too early to say that a quantum chip will have a smaller physical footprint than a a somehow comparable silicon chip. We are at the ""room sized computer"" stage, and the machines are cooled to near absolute 0 with refrigerated gasses.",hr4ft5o,t1_hr3tvdt,1641243945.0,False
rutayn,"> Like double the number of booleans outputted by a given circuit?

Yeah, basically. Like I'm seeing the [quantum logic gates with more outputs than a ""normal"" truth table.](https://en.wikipedia.org/wiki/Quantum_logic_gates](https://en.wikipedia.org/wiki/Quantum_logic_gate) I may not know enough linear algebra to wrap my head around it though.

I get your second paragraph though, I was silly not to think of that. Obviously traditional fabrication techniques won't work with quantum information. [I thought this was a pretty good explainer on the types of qubits.](https://www.youtube.com/watch?v=-5fKVn1GR9Y)",hr4jpo0,t1_hr4ft5o,1641245455.0,False
rutayn,"So you need to be careful here: there isn't some quantum magic that is adding bits or something like that. Every gate that outputs 2 qubits has 2 qubits as an input. The difference between a qubit and a regular bit is that regular bits are *always* only 0 or 1, and a qubit is a 0 or 1 *only when observed*. However, before a qubit is observed, when it is in superposition, it has some probability to collapse to a 0 or 1 on observation (and this probability can be 100% a 1 or 100% a 0). When gates operate on a qubit, they operate on its state in superposition; they change the probability that the qubit will collapse to one of a 0 or a 1. This lets engineers and scientists design rather creative algorithms involving linear algebra, which can often run faster than the classical algorithms used to solve similar problems.

If you'd like a clear intro, see https://www.youtube.com/watch?v=F_Riqjdh2oM, but be warned! It's all linear algebra.",hr4wzgc,t1_hr4jpo0,1641250643.0,False
rutayn,"Yes, quantum computers are very strange things that perform operations that are hard to understand and utilize.

Off topic, heavy tangent: I understand it was chosen for illustrative purposes, but I have to point something out, in case someone would end up thinking that implementing multiplication by adding in a loop is the way to go on platforms that lack multiplication opcodes. 🙂

Given A • B = C, the proper way to do it is by first decomposing one of the factors, say A, into a sum of powers of two. Then multiply B by each decomposed component of A, accumulating the result into C. Because the components are all factors of two, each multiplication ends up being a simple bit shift, which makes the algorithm perform in the order of O(log n).

If that's not fast enough (and on platforms that lack multiplication opcodes it frequently isn't), you can for example approximate the result of a multiplication by sacrificing memory for some lookup tables, and make use of the fact that log (A • B) = log A + log B.",hr4h5tu,t1_hr1fgul,1641244472.0,False
rutayn,Lol. I was expecting someone to call out that there are better algorithms than O(n) to do multiplication using only adding. I decided to leave the O(n) just to keep the explanation simpler and more understandable. You are of course correct. Thanks for keeping redit up to standards!,hr4jidx,t1_hr4h5tu,1641245378.0,False
rutayn,Great explication 👍🏼,hr25tup,t1_hr1fgul,1641207581.0,False
rutayn,"As far as I know, it doesn't necessarily reduce time complexity. A quantum algorithm has to be specially designed to leverage the benefits of the quantum computer.",hr1c56p,t3_rutayn,1641186680.0,False
rutayn,"[Shor's algorithm](https://en.m.wikipedia.org/wiki/Shor%27s_algorithm) shows how a quantum algorithm can solve a problem in faster time than a classical algorithm, the problem being find the prime factors of a given integer.",hr1fn80,t3_rutayn,1641188615.0,False
rutayn,"Quantum computers have characteristics like [entanglement](https://en.wikipedia.org/wiki/Quantum_entanglement) and [reversibility](https://www.linkedin.com/pulse/computational-reversibility-quantum-computing-sa%C5%A1a-savi%C4%87) .

This creates a computational paradigm that is different than your classical computer and therefore requires different algorithms and so far can only solve certain problems.",hr2xlyz,t3_rutayn,1641223392.0,False
rutayn,"u/OdinGuru’s answer is excellent! I’d like to add, if you’re interested in learning about quantum computing, I have found https://quantum.country to be an excellent introductory resource. It describes quantum operations as shortcuts. We can use these quantum operations to design new algorithms which can solve the same problems we already face in computing, sometimes utilizing the built-in shortcuts to result in faster runtimes. The key is sometimes. Algorithms have to be discovered which use these shortcuts to their full potential.",hr4rlou,t3_rutayn,1641248508.0,False
rutayn,[deleted],hr1vbxu,t3_rutayn,1641199270.0,False
rutayn,That's a completely wrong understanding of QC. Follow your own advice.,hr2blnn,t1_hr1vbxu,1641211781.0,False
rushx3,In relation to caching the article is just stating that an unaligned access may overlap TWO cache lines rather than one and that two cache loads is slower and eats up more cache space that a single load.,hr1e7sw,t3_rushx3,1641187815.0,False
rushx3,Eureka! Now I get it. Thanks a ton!,hr1ei6w,t1_hr1e7sw,1641187978.0,True
rushx3,"could you please share the link you're referring to?
I could explain it based on their perspective",hr16brx,t3_rushx3,1641183774.0,False
rushx3,I updated the post with the appropriate links.,hr17lg5,t1_hr16brx,1641184375.0,True
rushx3,"so the thing is CPUs do not access memory like we though they do
they do not have granular access to memory exactly like we have thought all this while

a 8bit CPU will have 1 byte of granular access
for 16byte it can load 2bytes at a time

now consider both these CPUs want to load want to read something at address 0x0001-0x0004 the 8bit CPU will be able to read the address directly from 0x0001-0x0004 onto the memory
but in case of the 16bit CPU it will first have to read 0x0000-0x0001
then perform a shift operation to set the actual starting point
for us it seems like a very nominal problem but that one extra cycle costs a lot, especially when there millions of these operations

now coming back to the real question, if the memory is unaligned you will tend to have more these shifting operations because you'd be loading more number of uneven memory addresses and then shift compared to placing it once and then shifting on from there

in case it is aligned you save on the reading and then shifting operation

carefully read the IBM link and observe the diagrams it will give you a better clarity

https://developer.ibm.com/articles/pa-dalign/",hr1dpvo,t1_hr17lg5,1641187536.0,False
rushx3,"yeah, that is what the IBM developer article explains. A lot of cpu cycles are saved when memory is aligned which is why aligned memory access is faster than unaligned memory access(also illustrated in the IBM developer article). Could you find any explanations on how aligned memory is good for caching?",hr1e8od,t1_hr1dpvo,1641187829.0,True
rushx3,it just reduces CPU instructions needed when the memory is aligned that's how the reads would become faster as less cycles are wasted to read the memory,hr1eko5,t1_hr1e8od,1641188016.0,False
rushx3,"I kinda get it now. If data is unaligned, and the CPU caches a cache line, there is a possibility that the data would be chopped off and wouldn't be cached properly. Because only a portion of the data is cached, the CPU might then cache another cache line containing the other part of the data which wastes cache and also it wastes cpu cycles which are required to shift and combine the portions of the data.",hr1f66i,t1_hr1eko5,1641188348.0,True
rushx3,yes absolutely,hr1g38q,t1_hr1f66i,1641188871.0,False
rushx3,I don't have any articles or resources to back this up but from what I've heard is that when a memory address in RAM is accessed it loads a whole block of it in to cache (L1/L2 cache maybe?). So then if the next instructions are accessing a memory address in that block it will pull it from the cache instead of RAM.,hr1c8xf,t3_rushx3,1641186735.0,False
rushx3,no this is something different I almost forgot about this concept read about it in my comment,hr1cmmz,t1_hr1c8xf,1641186940.0,False
rushx3,"> I've heard is that when a memory address in RAM is accessed it loads a whole block of it in to cache (L1/L2 cache maybe?).

Afaik, on subsequent reads of the same block of memory, a [cache line](https://stackoverflow.com/questions/52890824/cache-line-format-layout) which is usually 64 bytes, is cached. I got to know about these cache lines in this [youtube video](https://youtu.be/247cXLkYt2M?t=274) about data structure optimization but it does not explain why memory alignment is beneficial for caching.",hr1deg6,t1_hr1c8xf,1641187361.0,True
rue0sq,"Let’s say we have an 8 bit data width CPU, with 16 bit instructions.

It has 16x 8bit general purpose registers.

There are 2x (16 to 1) 8bit multiplexers with the 16x 8bit inputs being the values in the registers. They each have 4 control bits to select which register values end up on the output.

The 2 8bit outputs are fed to an ALU which can do 4 operations: add, sub, and, or. The operation is selected with 2 control bits.

The 8bit output is fed back to the 16 registers, each have a control bit which if enabled, will latch the 8bit value into the register at the end of the clock cycle. These 16 control bits are the output of a 4 to 16 decoder, which is controlled by 4 select bits.

So our CPU can choose any 2 registers from our 16, perform one of 4 operations, and then save the result to any of the 16 registers.

It needs 4 bits to select the 1st operand, 4 bits to select the 2nd, 2 bits to select an operation, and another 4 bits to select which register to save the result in.

So you might have an instruction format which looks like this:

00xxaaaabbbbcccc

Where xx are the 2 bits which select the operation the ALU will perform.

aaaa are the 4 bits which select the register of operand 1.

bbbb are the 4 bits which select the register of operand 2.

cccc are the 4 bits which select which register to save the result in.

When the CPU sees this instruction it will route the relevant control bits to the multiplexers, ALU, and decoder to perform the specified instruction. This is what is meant by decoding the instruction. This is a simple instruction set but real ones are much more complex and require more logic to derive the relevant control bits from any given instruction. The area of the CPU which will do this is called the control unit. It uses the instructions to derive control bits which will control the flow and processing of data in the other components of the CPU such as registers, the ALU, memory, various multiplexers and decoders, bus access, etc.",hqyhh8j,t3_rue0sq,1641145316.0,False
rue0sq,"Don’t think I could have explained any better. This is a good quality textbook-level response. Did you write CLRS ;)

Could you also clarify the third sentence? Are you saying 2 8 bit multiplexers logically function as a 16:1?",hqzz88x,t1_hqyhh8j,1641165600.0,False
rue0sq,"2 separate 16:1 multiplexers, which have 8 bit wide inputs and outputs. So a total of 2x16x8 inputs, 2x1x8 outputs, and 2x4 select lines. Always confusing to talk about multiplexers because there’s a lot of independent bit widths to specify haha.",hr08rij,t1_hqzz88x,1641169413.0,False
rue0sq,"It's been a while since I actually looked at digital multiplexers in detail; could I ask how an 8 bit multiplexer is 16:1? Wouldn't it be able to take 256 different combinations to select? Sorry, I thought I understood this, but maybe I need to brush up myself :P Or are you saying there are 4 control bits for each one, meaning 2\^4 remaining combinations on the last 4 bits which would make more sense. Lastly, you're awesome! Is your background more in CS or CE/EE?",hr0y5b5,t1_hr08rij,1641180067.0,False
rue0sq,"8bits isn’t referring to the number of select lines but the size of each individual input. Most multiplexers are just 1bit but sometimes you want to switch not single bits, but entire busses. To give an example:

A 1bit (4:1) multiplexer, needs 2 select lines:

    In0 - 0
    In1 - 1
    In2 - 1
    In3 - 0

    S0 - 1
    S1 - 0

    Out - 1

With the select lines set to 01 it gives output In1 which is 1

A 3bit (4:1) multiplexer, still needs 2 select lines:

    In0 - 101
    In1 - 000
    In2 - 011
    In3 - 110

    S0 - 0
    S1 - 1

    Out - 011

With the select lines set to 10 it gives output In2 which is 011

You can build a nbit (X:1) multiplexer by taking n x 1bit (X:1) multiplexers and tying their select lines together.

My background is EE but working in embedded engineering which is sort of the half way point between EE and CS haha.",hr4fu28,t1_hr0y5b5,1641243955.0,False
rue0sq,You might find Ben Eater's [video](https://www.youtube.com/watch?v=dXdoim96v5A) exploring his 8-bit computer's control logic useful if you want a visual understanding of /u/No_Engineering8506's reply,hqzpehx,t3_rue0sq,1641161753.0,False
rue0sq,"Well in general the CPU have something called a control unit. The control unit configures the cpu into different states (This could be adding values, putting things in memory or setting registers). When you decode a command, you're telling the control unit what state the cpu should be in.",hqyf1an,t3_rue0sq,1641144386.0,False
rue0sq,"Pretty much decoding refers to taking an instruction (or “command”) and breaking it apart depending on what kind of instruction it is, then sending these pieces to the other parts of the processor that executes the instruction. The control unit (which others have mentioned) is the component that checks a certain parts of the instruction, and depending on what values it finds in those parts it will send the correct parts to the correct components.

For example (and  I just made this up) if you want to add 5 and 2, and the control unit is set up to add when the first four indices of the instruction equals 2 (or 0010 in binary) then a 16 bit instruction for that could be: 

0010 000101 000010

The control unit would look at the first four values, see a ‘2’, and from there it knows to take the next 6 values and pass them in as the first operand to the ALU, and the last 6 values as the other operand to the ALU. In reality it’s more complicated as it also needs to know where to save the result and other operations might need even more information, but this is an easy way to visualize it.",hqzelm6,t3_rue0sq,1641157376.0,False
rue0sq,"If you want an expanded example you can think of the commands as gates/pathways that are controlled by switches. Each CPU command has a bitset that corresponds to it. Those gates open when the certain bits are set (through ands/nots) and then the data you're using with the CPU command is sent through that path.

It doesn't really ""decode"" it as much as the bits split up with circuits and sent in chunks. The command code is one such chunk and the data (more specifically the register number being used) is another one. Flags can also be used.

Now take that analogy and compact it, the gates overlap and the logic behind the gates can be simplified using ors an boolean simplification. Our CPUs are massively simplified to be as compact as possible. Programmable CPUs also exist (as in the command sets can be changed), but that is a lot more complicated.

Actual decoding/encoding is done to help us read it, but it's not at all necessary for the CPU function. The decoding you're referring to is more of an analogy for how the data splitting is done and how it's interpreted by the system.",hqzyn6k,t3_rue0sq,1641165371.0,False
rue0sq,"This is at least one college-level course. Binary instructions and data are fed into a hierarchy of semiconductor logic gates using transistors and other electrical components. I would recommend a textbook ""Logic and Computer Design Fundamentals"" by Mano, Kime, and Martin. It's kind-of a rough technical read, but it will get you started. 

I could write a whole essay, but you're not going to ""get it"" until you sit down and work-out some truth tables.",hqzyybm,t3_rue0sq,1641165494.0,False
rue0sq,"You got a lot of great responses here OP, but if you want a simplified TLDR, assume you have 16 bit storage (and processor of the same size) you can imagine 4 of those bits being used to encode an instruction (so 2^4=16 instructions) and then the next 4 bits are an argument for the instruction, such as an absolute value, a memory cell, or a pointer to a memory location, next 4 bits are second argument (same as before), and the last 4 bits are where to store the result. In practice it’s a tad more complex but this is the important intuition to develop imo.",hr0193o,t3_rue0sq,1641166396.0,False
rue0sq,"You got some really awesome responses here, I will try to give an answer that requires less CS knowledge to understand, and perhaps it will help someone out there (though if you really want to understand it look at other comments here!)

So basically, a CPU works by reading some list of ""commands"" in Binary code (lines of 0s and 1s).

These commands vary from OS to OS but they can basically be summed up to 3 categories: ""basic mathematical operations"" and ""memory operations"" and ""Jump operations""*.

The memory operations will be things like write num x to an address y in the RAM, or pull up the number in a RAM address and save it to the stack (a dedicated place on the ram, that mathematical operations will take their inputs from).

The mathematical operations will take numerical inputs from the RAM (from the stack) and replace them (on the stack) with the operation's result.

The ""Jump operations"" are taking us back to the first ""list of commands"" for the CPU, as we are all aware, in programming two of the most powerful tools we have are ""loops"" and ""conditions"", these make the difference between flicking switches manually and actually responding to various types of inputs, and being able to make varying computations with the same function - The Jump operations are able to check whether a condition applies (typically ""is the last stack item ==\<\> to 0"" and if that condition applies, the CPU will ""jump"" to a certain command in the list and start pulling the commands from that line to achieve conditional programming and loop iterations.

That is basically how a CPU operates. Hope it helps someone :)",hr1vxxn,t3_rue0sq,1641199753.0,False
rue0sq,"Logic gates and other tiny shit. Gates flip open gates flip closed, shit gets decoded from java or cpp or another high level language to machine code like move eax to blah blah blah then pop and push and eventually it all ends up to 1's and 0's. Or so I've heard.",hqyo04x,t3_rue0sq,1641147772.0,False
rue0sq,"no, thats encoding ""commands"", op asked aboud decoding them",hqz59os,t1_hqyo04x,1641154037.0,False
rue0sq,Oh shit just put it in reverse then.,hqzdtv5,t1_hqz59os,1641157108.0,False
rue0sq,"gates don’t actually flip at all, their electrical output is altered depending on the input voltage. pop/push are actually implementation-dependent iirc, some just add and subtract from program counter.",hr00fnx,t1_hqyo04x,1641166073.0,False
rue0sq,"„˙ǝƃɐʇloʌ ʇnduı ǝɥʇ uo ƃuıpuǝdǝp pǝɹǝʇlɐ sı ʇndʇno lɐɔıɹʇɔǝlǝ ɹıǝɥʇ 'llɐ ʇɐ dılɟ ʎllɐnʇɔɐ ʇ,uop sǝʇɐƃ„",hr00gu1,t1_hr00fnx,1641166085.0,False
rue0sq,Good bot,hr8g6no,t1_hr00gu1,1641316159.0,False
rue0sq,"Thank you, bobthebuilder747, for voting on Upside_Down-Bot.

This bot wants to find the best and worst bots on Reddit. [You can view results here](https://botrank.pastimes.eu/).

***

^(Even if I don't reply to your comment, I'm still listening for votes. Check the webpage to see if your vote registered!)",hr8g7qf,t1_hr8g6no,1641316171.0,False
rubwjz,"Could you give an example of what you mean? Often when computers/programs use different data types or file formats they just can’t communicate. To get around this, programmers pick standard formats to use so other programs can understand the output. 

Try to open a word document in notepad and see this failure in action.",hqxzhyz,t3_rubwjz,1641138110.0,False
rubwjz,">how can computers with different systems that have different data type representation communicate properly?

The same way countries with different monetary systems are able to trade goods: they agree on a common system of exchange.",hqy86il,t3_rubwjz,1641141738.0,False
rubwjz,">a common system of exchange

AKA. a **protocol** for communication and behaviour",hr0xjgv,t1_hqy86il,1641179799.0,False
rubwjz,"It's called ""serialization""/""deserialization"". Anything that transmits or stores data has to do it. This can be very simple and fast (but not very portable) or more complicated and slow (but more portable).

At one extreme, you might just save your whole memory image to disk. It would be pretty fast, and you wouldn't need to do anything to your data. For this to work, you probably would need the exact same version of the exact same program on the exact same operating system (and probably a little help from the OS).

At another extreme you could use a text based format like JSON or XML. Everyone knows how to store and transmit text, and standard formats mean it's easy to write libraries that convert them to whatever internal representation your program/language requires. Unfortunately, this process is pretty slow (imagine converting millions of ints into a variable number of characters). It also wastes space. But the up shot is that it's super portable. Even humans can read and understand it!

There are compromises as well. Protobuf is a tool from Google that helps you define how things get serialized and communicated. Two programs have to use the same protobuf definition and version, but they can be written in different languages and run on different OS's. It's still kind of slow, but it's more efficient than JSON. Python has something called ""pickle"" that does this, but it only works in python.

Serialization/deserialization is a huge problem in distributed systems. It's a significant fraction of all computer cycles at Google (look up ""the data center tax""). It can really slow down distributed frameworks and overall is a pain in everyone's butt.",hqydxle,t3_rubwjz,1641143968.0,False
rubwjz,"https://afteracademy.com/blog/what-are-protocols-and-what-are-the-key-elements-of-protocols

This seems like a reasonable summary",hr1klvp,t3_rubwjz,1641191619.0,False
rubju8,"You would need to look at the reference implementations and libraries for each language individually. C++'s STL has a lot of these kinds of rules, such as containers needing to return their size in O(1).",hqxwosd,t3_rubju8,1641136836.0,False
rubju8,Yeah pretty much.,hqyg5y3,t1_hqxwosd,1641144816.0,False
rubju8,I also like finding [cheat sheets](https://github.com/gibsjose/cpp-cheat-sheet/blob/master/Data%20Structures%20and%20Algorithms.md#10-data-structures) for this,hqzoj9k,t1_hqxwosd,1641161418.0,False
rubju8,"> You would need to look at the reference implementations and libraries for each language individually. C++'s STL has a lot of these kinds of rules, such as containers needing to return their size in O(1).

What are some of rule's that C++ has for their reference implementations ? Some of the feature's being in some questionable performance debt from the new abstraction's being introduced",hr0ewue,t1_hqxwosd,1641171956.0,False
rubju8,"In java you rarely use array, but a list implementation (arraylist, linkedlist, etc.). An add to arraylist is o(1).   


[https://www.programcreek.com/2013/03/arraylist-vs-linkedlist-vs-vector/](https://www.programcreek.com/2013/03/arraylist-vs-linkedlist-vs-vector/)",hqy5191,t3_rubju8,1641140464.0,False
rubju8,"Also, the model behind time complexity does not take account of the modern processor architecture (caching, etc.) and this is the reason when any engineer worth it money will say: only after an actual measurement on a certain architecture will I tell you anything about an algorithm's performance.   


So there is a huge, real life difference between a theoretical algorithm and a concrete implementation of that algorithm on a certain architecture. Also nowadays you can run stuff on multiple cores as well, which makes things more tricky. Also in java (and js as well) you need to think about the GC.  


[https://stackoverflow.com/questions/10656471/performance-differences-between-arraylist-and-linkedlist](https://stackoverflow.com/questions/10656471/performance-differences-between-arraylist-and-linkedlist)",hqy67sp,t3_rubju8,1641140950.0,False
rubju8,"Caching and parallelization affect runtime, not time complexity.",hr0uvm1,t1_hqy67sp,1641178637.0,False
rubju8,And this is why time complexity as a mathematical formula has limited usage when talking about real world performance.,hr1gmf5,t1_hr0uvm1,1641189176.0,False
rubju8,Not sure if there is an existing resource for these quirks. You'll probably just need to look into the implementations of common structures in the languages you're curious about.,hqzh35y,t3_rubju8,1641158435.0,False
rubju8,"> In JS, array.push() is always O(1) since even if need to 'extend' the current array as the reference to the last item is simply removed.

Sometimes array.push requires a copy even in JS. The operation is **amortized** O(1), but it's useful to make sure you're comfortable with why that is.

Once you've got a decent understanding of common data structures it's pretty easy to recognize which ones are used in a given language. At that point it should be pretty easy to accurately guess what the complexity of each operation is.",hqzta2x,t3_rubju8,1641163255.0,False
ru6jx7,"> I understand a database containing passwords should be hashed using some sort of key

Keep in mind that [cryptographic hashing and encryption](https://www.ssl2buy.com/wiki/wp-content/uploads/2015/12/hashing-vs-encryption.png) are two different things. 

You aren't hashing the whole database, just the password. Passwords are salted and then put through an execution unit in the CPU that does [this](https://emn178.github.io/online-tools/sha256.html) to them, so that if there is a data breach or a rogue sys admin, there aren't a bunch of plaintext passwords floating around.

> but I understand that each entry should be encrypted again in some other way.

Depends. [Some implementations encrypt as the information is written to disk, others don't.](https://en.wikipedia.org/wiki/Disk_encryption#Transparent_encryption) The database is on the disk. Encryption and hashing can be done at the hardware level; there could be, for instance, an [AES execution unit](https://en.wikipedia.org/wiki/AES_instruction_set) in the CPU before it's sent over the wires (maybe the SATA bus?) to be written to disk.",hr5d36f,t3_ru6jx7,1641257286.0,False
ru5qo2,"This sounds like [Complexity Theory](https://en.wikipedia.org/wiki/Computational_complexity_theory). You might consider connections to the more general question ""for which problems can we describe limits on the time, space, (or other resources such as number of states) needed by any Turing machine which solves them""",hqx5oww,t3_ru5qo2,1641119085.0,False
ru5qo2,"**[Computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)** 
 
 >Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm. A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hqx5ptn,t1_hqx5oww,1641119106.0,False
ru5qo2,"To answer your specific question, it can be shown that 2 states is ""sufficient"" for any kind of Turing machine you want to make. The more interesting tradeoffs are in time (number of steps) and space (number of visited tape cells).

EDIT: I was wrong, I confused the alphabet size with the number of states. They are related in the sense that a machine with fewer states may require more alphabet symbols to be universal, and vice versa. But here's a universal machine with 2 states and alphabet size 3: [Wolfram's (2,3) Machine](https://en.wikipedia.org/wiki/Wolfram%27s_2-state_3-symbol_Turing_machine)",hqx6x8l,t3_ru5qo2,1641120106.0,False
ru5qo2,"Hang on. A ""Turing Machine"" can be a one bit system?",hqxkx7g,t1_hqx6x8l,1641130619.0,False
ru5qo2,"The tape must be infinite, of course.",hra6ief,t1_hqxkx7g,1641340239.0,False
ru5qo2,"I think the alphabet (tape characters) can be of size two to compute anything (I.e binary), but I don’t think all computations can be done with two states…",hqyhgo7,t1_hqx6x8l,1641145310.0,False
ru5qo2,See my edit. 2 state UTM can exist,hqzghkm,t1_hqyhgo7,1641158051.0,False
ru5qo2,"A 2 state TM can exist, but it can't do everything. A larger alphabet can help out, but with a single tape, a 2-state machine can't do that much.


I've made a program on a 2 state 3 letter turing machine that simply increments a binary number until it hits 255, where the machine crashes. Not much more can be done with that kind of machine.",hr9uiw0,t1_hqzghkm,1641335476.0,True
ru5qo2,Everything can be done with that machine. That's what makes it universal. It may take more patience than you have.,hra06vy,t1_hr9uiw0,1641337713.0,False
ru5qo2,"If it can, can you show me an example of it emulating, say a 16 state 16 letter machine?",hra0ddo,t1_hra06vy,1641337787.0,True
ru5qo2,"No, because it would take more patience than I have. But it's possible. Please read the article I linked.",hra5j2b,t1_hra0ddo,1641339847.0,False
ru5qo2,"A Turing machine has a finite number of states in its CPU. However, the states are not small in number. A real computer consists of registers which can store values (fixed number of bits). A universal Turing machine can be constructed using one tape and having only two internal states.",ht2eu4x,t3_ru5qo2,1642443233.0,False
ru3c37,"I have read or listened or talked about this like some months or an year ago but ah I can't really remember where I read/listened/talked about it. What I do remember is that an AI was given all the laws of classical and quantum physics and it was given the task to discover new laws. My guess is that I read about this in either of these two books:
- A Mathematical Universe by Max Tegmark
- Life 3.0 by Max Tegmark

I probably have physical copies of both the books, so I'll give both of them a look and respond back.",hqwte9o,t3_ru3c37,1641109157.0,False
ru3c37,"After some googling, I found this [article](https://www.technologyreview.com/2018/11/01/1895/an-ai-physicist-can-derive-the-natural-laws-of-imagined-universes/) which talks briefly about this [research paper](https://arxiv.org/abs/1810.10525) by Max Tegmark and Tailin Wu from MIT.

Also I looked up Life 3.0 and well it is a pretty heavy book so I could not find much about the AI discovering the new laws and axioms except two topics that I thought might help. I read about AI creating laws for the society and a topic titled ""Physics: The Origin of Goals"". I am sure that the paper mentioned earlier would definitely be helpful.",hqwva81,t1_hqwte9o,1641110603.0,False
ru3c37,This is insane. Guess I know who to talk to when I visit Big Tech in Summer! I also had the same Ideas about morality before I read about this. Basically you define a goal and then the moralistic stance just revolves around the goal.,hqwwb70,t1_hqwva81,1641111401.0,True
ru3c37,I think the best goal to be honest is just the pursuit of knowledge itself. I know it sounds crazy but it’s a cause I‘m willing to die for when I think about it. Even If we can’t pass all the great filters of the universe maybe we can at least pass on the ability to more advanced forms of life. Our feeling of being human is restricted to knowledge 2.0 something that is probably not capable of understanding the universe.,hqwx0dh,t1_hqwva81,1641111952.0,True
ru3c37,I actually have Knowledge 3.0 as well (I‘m a general intelligence btw hahahahaha). I‘ll have a look too. Omw to creating a discord.,hqwtrx3,t1_hqwte9o,1641109443.0,True
ru3c37,I’ve read life 3.0 and would highly recommend it to anyone interested in this topic. Whilst I can’t 100% remember if it touches on OP’s specific example it definitely at least explores the idea,hqy4w1w,t1_hqwte9o,1641140405.0,False
ru3c37,"The problem I can see with this is the fact that even if the AI finds anything, there's no experimental confirmation involved. 

At most it'll be a 'string-theory'-like idea, that isn't and can't be substantiated. Only you'd get probably millions of them.

Then you'd need to create AI to be able to test (or come up with methods of testing) all of those hypotheses xD",hqxfaiy,t1_hqwte9o,1641126815.0,False
ru3c37,"There are two ways to approach this idea.

* You could try to use computers to verify proofs of theorems, and maybe even generate new proofs of new theorems. There has been some progress on this, but it turns out that mathematical proofs tend to leave out ""obvious"" steps that need to be written out in painstaking detail for computers to understand. Additionally, this approach doesn't help with non-mathematical things, such as understanding language. Relevant link: [https://www.quantamagazine.org/lean-computer-program-confirms-peter-scholze-proof-20210728/](https://www.quantamagazine.org/lean-computer-program-confirms-peter-scholze-proof-20210728/)
* We can also try to ""understand"" the world, with all its complexities and ambiguities, in an ad-hoc way, learning what's what. In the last decade, there's been an explosion in this thing called ""deep learning"", which is a method in AI. This has been pretty successful and has been used commercially. However, due to its ad-hoc nature, it's hard if not impossible for a human what the computer is ""thinking"", making progress in this field kind of slow and full of trial-and-error (except that ""throwing more hardware into the problem"" seems to work surprisingly well).",hqxp92c,t3_ru3c37,1641133116.0,False
ru3c37,I was thinking more of a principia mathematica approach with the dataset inputing a set of axioms and the output being a closely related result (mabye only needing one or two lines of logic). I assume we can then better know what the AI is doing.,hr2vr9e,t1_hqxp92c,1641222586.0,True
ru3c37,"What's the difference between that and the first approach I mentioned?

Also, AI is much, much harder than you think.",hr2wz6i,t1_hr2vr9e,1641223124.0,False
ru3c37,"The difference is that I‘m guiding the AI in a certain direction. The loss function then becomes trivial. I’ve programmed machine learning algorithms from the ground up, I know I’m getting into a lifelong journey. I think it’s fundamentally however it’s the thing most worth pursuing if we ever want to fully understand mathematics and physics.",hr3057e,t1_hr2wz6i,1641224466.0,True
ru3c37,"I don't quite understand what you're saying. Is this what you're looking for?

>A formidable open challenge in the field asks how much proof-making can actually be automated: Can a system generate an interesting conjecture and prove it in a way that people understand? A slew of recent advances from labs around the world suggests ways that artificial intelligence tools may answer that question. Josef Urban at the Czech Institute of Informatics, Robotics and Cybernetics in Prague is exploring a variety of approaches that use machine learning to boost the efficiency and performance of existing provers. In July, his group reported a set of original conjectures and proofs generated and verified by machines. And in June, a group at Google Research led by Christian Szegedy posted recent results from efforts to harness the strengths of natural language processing to make computer proofs more human-seeming in structure and explanation.

https://www.quantamagazine.org/how-close-are-computers-to-automating-mathematical-reasoning-20200827/",hr33qcm,t1_hr3057e,1641225944.0,False
ru3c37,"Point 1 :

What you displayed here is what Einstein referred to when he used the word ""imagination"". I call it ""inspiration"".

It's at the root of almost all of our startling discoveries. And I don't think an AI will be able to replicate that in a long, long, long while. .

Point 2 :

Because of Point 1, what your model at best could perform is fetch us the ""closure set"" of all mathematical concepts that apply due the concepts and first principles already known to us. For eg. (and I don't know how our present AI models work so forgive me if I'm incorrect about this example) but think about imaginary numbers.

If humans hadn't invented the concept of imaginary numbers as a mathematical tool, what I think is that your AI could've only fetched us knowledge applicable to Real Numbers.",hqxiz4d,t3_ru3c37,1641129389.0,False
ru3c37,Have you heard of constructor theory? Maybe we can rearrange the axioms by finding the counterfactuals. For example sqr(-1) doesn’t exist is the counterfactual to sqr(-1) = i . Also even if it only does that I‘m very happy.,hr2v94s,t1_hqxiz4d,1641222360.0,True
ru3c37,">Have you heard of constructor theory? 

Nope. Enlighten me.",hr2zskv,t1_hr2v94s,1641224318.0,False
ru3c37,It basically just questions axioms of physics by assuming the opposite is true.,hr30p7j,t1_hr2zskv,1641224697.0,True
ru3c37,"Ok, so I like your general train of thought, but please allow me to humor you with this angle, namely that our “pool” of knowledge is always essentially what we hold in our collective RAM/working memory for the explicit purpose of executing a task.

Therefore energy must actually be invested by ourselves to how we organize/operate etc. Therefore what you are proposing is totally natural, and in many ways one could argue is already being done although in its only just getting started in its potential. My main point is that we as humans are essentially neural processing nets that guide energy allocation to support ourselves. For example if we need to come up with a specific protein to create a biological outcome for a process we tool AI to that end. 

What you are suggesting, correct me if I am wrong is to direct AI to building a general knowledge pool. The problem is that is too broad.

The most interesting area I see is actually the creation of an object oriented learning architecture that capitalizes everyone’s unique interest/energy level in that interest to dynamically transmit information and organize ourselves at the same time.

With such a system our neural nets actually can function in symbiosis with AI where we both can help each other perform at our collective maximum with us guiding systemic output towards to meeting our current systemic demand while building an exponentially more capable system, itself based on an exponentially growing base of actionable knowledge.",hrdquba,t3_ru3c37,1641405121.0,False
ru3c37,How would the system determine when a theory has been discovered so it can proceeded to check it,hsggjg2,t3_ru3c37,1642055469.0,False
rtv1bn,"Like _lambda calculus_ or _Markov algorithm_?

^(I hope you don't think of TM as of an actual, physical machinery?)",hqv54vy,t3_rtv1bn,1641079151.0,False
rtv1bn,"I suppose they both count, however Turing's model of computation seems the simplest of the three to understand, and I realize I may have not asked the same thing I was wondering. Is there a type of *mechanical computer,* as powerful as the turing machine, that can be implemented in the same way as something like [this](https://youtu.be/vo8izCKHiF0)?
Sorry for any confusion. English *is* my first language, but I'm just really poor at articulating something correctly the first time.",hqv6t1m,t1_hqv54vy,1641079857.0,True
rtv1bn,"Ah, Turing-complete mechanical computer then? Like [Babbage's Analytical Engine](https://www.youtube.com/watch?v=5rtKoKFGFSM)?",hqveuii,t1_hqv6t1m,1641083347.0,False
rtv1bn,"A 'TM' like that with a fixed-size tape has no special power, it's a finite-state machine. Trivially, consider an _n_-state FSM which has inputs for every possible transformation on _n_ states. Clearly, such a machine has a ridiculous number of inputs, but it can be used to simulate any other _n_-state FSM by attaching a little circuit in front that performs a simple mapping from the inputs of the target machine to the inputs of the ""universal"" FSM which effect the same transitions. That little circuit is like a program. The 'TM' here is like one where the program takes a single input, the turn of the crank.

Such a ""universal"" FSM can be reduced with semigroup methods to be quite compact; this (_n_+1)-state transition table (Hartmanis and Stearns 1966, p. 193) can simulate any _n_-state machine, by mapping inputs of the target machine to sequences of inputs for the table:

state | 0 | 1
---------|----------|----------
1 | 2 | _n_-1
2 | 3 | 1
⋮ | ⋮ | ⋮
_k_ | _k_+1 | _k_-1
⋮ | ⋮ | ⋮
_n_-1 | _n_ | _n_-2
_n_ | 1 | _s_
_s_ | 1 | 1

Draw out the exact table for some small _n_ > 3, try inputs 0, 1, 00, 01, ... and you may be able to see how it works. I can imagine a mechanical realization of this device which the margin of this textarea is too small to contain.",hqw384f,t1_hqv6t1m,1641094309.0,False
rtv1bn,"Yes, as others have mentioned, since the universe is finite, you will never be able to build a real Turing machine",hqyojbe,t1_hqv6t1m,1641147963.0,False
rtv1bn,">I hope you don't think of TM as of an actual, physical machinery?


There are at least three physical implementations of a turing machine that I've seen, one partly electronic, one partly electrical, and one made entirely out of wood and metal. There's a link I posted on one of my comments to see the wooden one. Of course, it is only a 3-state 3-letter alphabet turing machine with a finite tape roll, far less powerful than an infinite-state 2-letter alphabet one with an infinite tape.",hr4rs9n,t1_hqv54vy,1641248580.0,True
rtv1bn,"Yes, of course. But many people\* make a mistake of thinking that TM is exactly such physical implementation.

The difference is, in case of ""normal"" machines (ATM, fax machine, washing machine etc.), it's the implementation what matters.  
In case of TM, such literal implementations are mostly toys made for fun; it's the concept that matters.

&nbsp;

^\* ^(what baffles me, you can find some of them even amongst 2nd year CS students)",hr54ofe,t1_hr4rs9n,1641253788.0,False
rtv1bn,isn't it technically not a turning machine if the tape is finite but a linear bounded automata?,hupc038,t1_hr4rs9n,1643450197.0,False
rtv1bn,"John Conway's ""Game of Life"" is not exactly mechanical (or easy to physically automate), but it is easy to play by hand, with grid paper and beads. It is much simpler than a Turing machine, but equally powerful.",hqwmsmj,t3_rtv1bn,1641104624.0,False
rtv1bn,"Any other computational model exactly as powerful as the Turing Machine IS a Turing Machine (and even if it apparently looks different, you could express it's functioning in terms of the usual Turing Machine concept with finite control and one or more read/write tape(s)).

Understand that Turing Machine is a theoretical model of computation.

Specifically, it represents a logical set of computational possibilities. The setup with finite control and one or more (unbounded) read/write tapes can perform all of those computational tasks.

Any Machine/Model that can perform all of the computational possibilities in aforementioned set and no more, is an implementation of Turing Machine.",hqw9pdz,t3_rtv1bn,1641097433.0,False
rtu2da,"Negative time means any date prior to 1970 - which at the time Unix time was developed was very important, because literally everyone programming with it was born prior to 1970. This is becoming less and less important as people die, but is still required. So if you take as assumed that time is stored in an integer of some kind and time starts counting from 1970, it is necessary so you can store dates before then. You could go with unsigned and start from 1901, but the reasons to *not* do that have already been covered.
 
The question of why time is stored as an integer of some kind is mostly to do with sorting and > < comparison in general, which is the most common date/time operations that exist by a mile. These operations must be as rapid as humanly possible, and so date/time storage is optimised for them, hence a single integer count (so you don't have to go looking for the year variable, then the month variable, etc etc).
 
The question of why time starts at 1970 was because at that gives enough time to cover preeeeeetty much everybody's birthdays (not all, but enough that those rare people just got to be born in 1901, there weren't many 70+ programmers back then, nor even employees or customers of that age), while simultaneously being long enough that it seemed like it would get sorted out before we got there (which is very likely to turn out true).",hqw4i5b,t3_rtu2da,1641094912.0,False
rtu2da,"Just to extend this and say that perhaps the most common operation done on the time stamp is the simple increment that happens once a second. And that's very easy to do with a single integer. Like you say, I'd we store it as a multipart number then we have to worry about roll over.",hqx0eba,t1_hqw4i5b,1641114682.0,False
rtu2da,"While it's something that happens, it's not a major issue as that rollover operation occurring just once a second is not a big deal. The issue is not really 'the time now', because that generally happens in one spot per machine (and these days aren't even counted in memory at all, they're tracked by an external clock and updated as needed). Previously stored time is where the efficiency is needed, because that is when you won't have 1 operation per second, you'll want billions.",hqx1jv6,t1_hqx0eba,1641115632.0,False
rtu2da,"Forgive me for the tiny, mostly useless contribution. But the internet as we know it will eventually be older than every living thing, assuming we live for another few thousand years at the least.",hqwzfiv,t1_hqw4i5b,1641113899.0,False
rtu2da,We can hope! :-),hqxop59,t1_hqwzfiv,1641132811.0,False
rtu2da,Because you might want to store a time in the past,hqux5xg,t3_rtu2da,1641075739.0,False
rtu2da,Does this mean there is a limit on how far back the time can go by storing it as a signed integer?,hqy146a,t1_hqux5xg,1641138814.0,False
rtu2da,"What I’ve heard is that when the first Unix systems used the epoch, the compiler (maybe the architecture but that would be weird) didn’t differentiate between signed and unsigned ints. So it is maintained as that type for backwards compatibility, and because a bigger number wasn’t thought to be needed.",hqv6xnd,t3_rtu2da,1641079911.0,False
rtu2da,"It’s convenient to store time using unix time; it’s zone agnostic; it’s generally simple to perform arithmetic on. 

And when many of these systems were created 2038 was a long way off. 

Much like only storing 2 digits of the date; the smallest viable size was chosen because storage wasn’t cheap at the time and when you’ve got millions of records a wasted 2 bytes can add up.",hqv33yl,t3_rtu2da,1641078280.0,False
rtu2da,"Unsigned integer needs different (often unexpected) logic for arithmetic operations due to the fact that it can easily underflow at zero. So for arithmetic types, they are avoided. For example:

* People often do date calculations
  * `daysUntil = refDate - today`
  * `daysSince = today - refDate`
* `for (int i = x; ... ; i--)`, instead of checking for `i >= 0` you need to also check for underflow which is `i <= x`.",hqvrlwa,t3_rtu2da,1641089005.0,False
rtu2da,Oh wow I never knew that could be an issue!  I always just assumed that an unsigned integer sorta just bottomed out at 0.  I feel like these high level languages have coddled me and hid the weird reality of the circuits from me this whole time.,hqvsltr,t1_hqvrlwa,1641089457.0,True
rtu2da,"> unsigned integer sorta just bottomed out at 0

If you try to assign a signed value(less that 0) to an unsigned integer, weird things happen which are implementation-specific. For example, if you assign a negative value to an unsigned int and compile with gcc, this is what would be assigned to the variable:
```
#include <stdio.h>

int main() {
    unsigned int illegalUint = -3;
    printf(""%u"", illegalUint);
}
```
the above code returns:
```
4294967293
```

Certain new languages like Go and Rust(i assume), do not allow signed integer assignments to unsigned integer variables.

> I feel like these high level languages have coddled me and hid the weird reality of the circuits from me this whole time.

I can kinda resonate with you, JavaScript was my daily driver like 5-6 months ago, I just did a lot of node.js and well when I started learning Go, I learned so much about memory and the CPU that I couldn't ever have learnt while using JS.",hqwfcig,t1_hqvsltr,1641100327.0,False
rtu2da,"I, after nearly 40 years of working with computers, built my own Z80 on a breadboard (am building, actually, I’m adding stuff to it) and have been programming it in assembly since I’ll need the assembly code to add devices to CBIOS.  It’s been an amazing experience and I’ve learned so much. Everyone who does this professionally should take on a project like this.",hqxd8ze,t1_hqvsltr,1641125271.0,False
rtu2da,"Because according to Einstein, negative time *could* possibly happen, so we built our computer architecture to deal with it in case it happens!",hqw6zrn,t3_rtu2da,1641096093.0,False
rtu2da,"Do you mean ""the past""?",hqwygup,t1_hqw6zrn,1641113127.0,False
rtu2da,[deleted],hqv1jkp,t3_rtu2da,1641077612.0,False
rtu2da,"The unix standard (most frequently) represents time as a 32 bit signed integer, ranging from -2,147,483,648 to 2,147,483,647.
If you changed it to an unsigned 32 bit integer, you can now store values from 0 to 4,294,967,295. Because you have freed a bit from storing the positive or negative state of the number.
When the number of bits used is expected to be standard, it makes sense to wonder why we don't maximize how high we can count with those bits, and would in fact change the 2038 problem to a 2106 problem.",hqv5rrp,t1_hqv1jkp,1641079417.0,False
rtu2da,"> would in fact change the 2038 problem to a 2106 problem

Yeah, that is why shifting to 64-bit architecture is the best option by far. Btw I wonder that when will we encounter a similar problem with int64.",hqwfl5p,t1_hqv5rrp,1641100458.0,False
rtu2da,"> I wonder that when will we encounter a similar problem with int64.

If only we could calculate this somehow, e.g. simply by finding the maximum value it can hold and adding it to the start date! :)",hqx0is3,t1_hqwfl5p,1641114786.0,False
rtkdm7,"Turns out, individual people aren't that unique.  A classic (literally, textbook) example is if 10,000,000 other people searched beer, then bought beer and diapers, when you search beer, or even just linger over an ad for beer long enough to indicate it caught your attention, you're going to get ads for beer and diapers, even though you didn't search diapers.  That's an incredibly simple example, in reality the algorithms look at many more variables than that, looking for patterns.  Maybe people who by beer and milk get ads for diapers and people who buy beer but no milk get ads for protein powder.  There is a bit of confirmation bias to it as well - you notice the times the algorithm was correct, and you don't notice when it wasn't, because that just looks like ""random ads"".

Other times, it's as simple as, for example, Google seeing what kind of products you get spam email for, and showing you ads for things bought by people who got those same emails, or showing you ads for things one of your family members or friends was searching for (it knows your ""friends"" by doing things like matching up who has each other's numbers or emails in their contacts list, when you give it permission to ""manage contacts"" or whatever.)

Another thing that happens is these people were getting those ads all along, but simply never noticed.  Once they do think of that thing, they start noticing the ads; basically the same as how you never notice a certain kind of car until it's brought to your attention somehow (an ad, a friend buys one, etc) then all off a sudden you see them everywhere you look.  The world didn't change, your brain-vision-attention filter did.",hqt4qqc,t3_rtkdm7,1641048007.0,False
rtkdm7,">Turns out, individual people aren't that unique.

[You're all individuals!](https://youtu.be/KHbzSif78qQ?t=31)",hqtsn7m,t1_hqt4qqc,1641059049.0,False
rtkdm7,Yes! We are all different! (I'm not..),hqtuoot,t1_hqtsn7m,1641059891.0,False
rtkdm7,"> just linger over an ad

Ok so they are also using facial recognition software to detect anomalys or emotional reactions to things?",hqt8u5j,t1_hqt4qqc,1641050186.0,True
rtkdm7,No. Linger as in mouse over it or have it on your screen for longer than necessary.,hqtdseq,t1_hqt8u5j,1641052605.0,False
rtkdm7,How to the log data like that?,hqtqi89,t1_hqtdseq,1641058161.0,False
rtkdm7,"Your browser knows where your cursor is and what is currently being rendered. A website can just log that information (so long as its happening on its own site, see Same Origin Policy).",hqtu0fr,t1_hqtqi89,1641059615.0,False
rtkdm7,"To be more exact, it is the script (javascript) that is running that does the tracking of where the cursor is. Browser itself just provides the information to the script and the script decides what to do with it. 

This is the problem with allowing shady code from untrusted sources to be run in on your browser since they get access to a lot of behavior-related information and sometimes some private information as well.

Unfortunately, a lot of the scripts are necessary these days for even basic functionality in the web so disabling it is not that easy.",hqtvc3u,t1_hqtu0fr,1641060159.0,False
rtkdm7,"No, I meant if you're scrolling down a news feed and briefly stop scrolling, they just detect the pause and assume you were interested in whatever was on screen.",hqtl9ch,t1_hqt8u5j,1641055942.0,False
rtkdm7,">linger over an ad

Which means that my cats' demands for attention control the advertisements I see.",hqtrlek,t1_hqtl9ch,1641058618.0,False
rtkdm7,"It isn't straight up mind reading but rather pattern recognition. People are more predictable than we want to admit we are and most people do pretty specific things in specific situations. What big tech has is a fuck ton of data that shows that if you like X you're extremely likely to also like Y.

Let's say for example Google figures out that you like heavy metal. It isn't wrong to like heavy metal. Some people like heavy metal and some people don't. Then let's say that they figure out that 96% of people who like heavy metal also like cheap beer. What Google knows is that betting that you like cheap beer if you like heavy metal is a safe bet as there's only a 4% chance that they're wrong. So what they then do is work with advertisers who want to sell cheap beer and say ""metal fans like cheap beer, target them."" That isn't individual mind reading; that's just recognizing patterns in human behavior. If you like heavy metal then chances are you're in the 96% that also likes cheap beer. To an individual this might look like mind reading as you get targeted advertisement but it really isn't mind reading. What happens is Google analyzes your patterns, connects them to overall population patterns, and figures out what you like and what you tend to search for. They'll also notice things like people who enjoy heavy metal are extremely unlikely to search for, say, Brittney Spears and are thus not going to recommend her to metal fans.

What they use is machine learning which chews through obscene amounts of data and finds the patterns. It genuinely is just computers doing math and pattern recognition then figuring out ""people who do X are highly likely to also do Y."" There are also certain patterns that most people show in their searches when they're going through certain things or are about to do certain things that are also recognizable.

The other thing is that big tech tracks what you search for and knows what you do and don't like. Yes this is actually kind of creepy and I don't like that they do it but right now it's legal so they aren't going to stop. Even then though most of it is mundane; we all have our favorite foods, income levels, and demographic information that affects how we behave. So if I like cheap tacos Google probably knows I like cheap tacos. So if I go to Google Maps and search for ""restaurants"" it's going to give me recommendations that are appropriate to my preferences and income level. If I'm broke as fuck and like cheap tacos it is less likely to show me the most expensive steakhouse in town first. That's not mind reading that's just the machine looking at the numbers and making recommendations based on the patterns it knows.

This also isn't new. Supermarkets and department stores have been analyzing customer patterns longer than computers have been around and have noticed weirdly specific behavioral patterns in humanity. The only difference is that computers are doing it rather than people at Google. One downright weird thing that people do when going into stores is we turn right first. Not everybody does it but the vast majority of people when they walk into a store look and turn right. Why? Who the fuck knows? However there's a weird exception; the Japanese go left.",hqt6f4q,t3_rtkdm7,1641048916.0,False
rtkdm7,And most Brits.. we go left to cos theres typically cheap beer right to the left of a supermarket entrance. Coincidence? I think not,hqtmbc2,t1_hqt6f4q,1641056389.0,False
rtkdm7,"Thank you so much for your considerate responses, very kind 🙂",hqt86pr,t3_rtkdm7,1641049849.0,True
rtkdm7,"You cannot read thoughts without hooking up your body as an input to a machine that could interpret it's thoughts.
The algorithms in Social Media / Google / etc. Are based on the profile they have gathered about you (that is linked to your Social Media account / IP address / MAC address) and compared with the behaviour of accounts that are vaguely similar to you in various aspects (mostly relating to shopping habits).

As was mentioned before, humans are not that different from one another, and age groups with vaguely similar interests will often search for similar things at similar situations, and if you take into consideration the fact you are most likely to notice something when it's really extraordinary (like getting suggested something you only thought about for a second and never searched for, a minute after you thought it), you get the feeling that someone is literally reading your mind.

You are getting algorithmic suggestions all of the time, chances are if the algorithm is good enough, it will suggest something that is anecdotal to your recent actions (since other vaguely similar people searched for it right after behaving in a similar way to you). But when it's suggesting something irrelevant you simply won't notice.",hr1xx3l,t3_rtkdm7,1641201307.0,False
rtkdm7,"There are special algorithms, called artificial neural networks. Those mimic the way your brain behaves, and are trained to recognise patterns in a dataset. Big tech coorperations train these, to predict what the users will do.",hqtpj2h,t3_rtkdm7,1641057745.0,False
rtkdm7,Fascinating. Can you go into a bit more detail? How many neurons are they based on?,hqtqv9d,t1_hqtpj2h,1641058316.0,True
rtkdm7,"They are based on mathematical equations, and the Neurons can be specified by the developer. There are different types of Neurons, unlike the human brain, to handle different tasks like image recognition or generation",hqtsnjf,t1_hqtqv9d,1641059053.0,False
rtkdm7,"Could this be the start of ASI? If the developers are in charge of the number of neurons, couldnt they make one with 86billion the same as a human brain ala mind control! Im freaking out 😬",hqtz4u3,t1_hqtsnjf,1641061706.0,True
rtkdm7,"Adding more and more neurons does not necessarily mean ""real"" intelligence will emerge.

A rough analogy: if you throw a 1,000 toddlers in a room, that doesn't mean the group of toddlers can solve 1,000x (or even 2x) harder math problems. They probably can solve more problems collectively than individually, but their capacity is still limited by their own toddler brains.

Likely, there is new stuff that needs to be invented to develop something we would call intelligence.

If you are interested in this stuff, I would highly recommend the book *Superintelligence: Paths, Dangers, Strategies*. It goes into a lot of the interesting concepts behind AI / how we can classify intelligence.",hqu72pp,t1_hqtz4u3,1641064938.0,False
rtkdm7,"The ""neurons"" in our artificial neural networks are very simplified models of biological neurons in a human brain. They don't function the same way and they aren't connected to each other in the same way. They don't learn the same way and they aren't seeing the same kind of experiences and data to learn from. It's sort of like a child's model of the solar system compared to the actual solar system. It's a useful model of computation -- not a physically accurate simulation of human brains. So even if you keep adding them in, there's no guarantee you're ever going to reach human level thought.

That said, suppose you built a perfectly accurate brain simulation able to do everything a human could do. On the one hand, you'll be as famous as Newton and Einstein. It's a 10,000 year discovery in terms of significance. On the other hand, creating something the works like a human brain can be done by nearly anyone given nine-months to build it and a decade or two to train it. Just making new human brains isn't especially frightening. Every infant doesn't grow up to become some sort of terrifying eater of worlds just because they can think as well as a human.",hqv2exs,t1_hqtz4u3,1641077984.0,False
rtkdm7,"Theoretically, that is possible, but practically the computational power is required, it is impossible, to build something of that scale, yet.",hqu15ob,t1_hqtz4u3,1641062532.0,False
rtkdm7,"Stop using reddit and the internet to feed delusions. I suspect you're the user Insight_7407 and god knows how many other accounts.

Stop the bullshit, and don't lie to yourself. You're seeking out this type of information and interactions intentionally, and you can easily choose not to.

https://old.reddit.com/r/computerscience/comments/r1xcw9/artificial_super_intelligence_asi/",hqwlvf7,t1_hqtz4u3,1641104054.0,False
rtkdm7,"No your misunderstanding me, this is just a thread about technology that dose exist of which im trying to understand from people who know about it..if i was delusional this process wouldnt be very helpful. I have a right to post what i want",hqx9bqi,t1_hqwlvf7,1641122101.0,True
rtkdm7,"If someone figured out how to read minds, do you think the first thing they would do is try to sell you stuff? And if they did, wouldn't they stop advertising things you don't want?",hqvn6yw,t3_rtkdm7,1641087066.0,False
rstlls,"It's not ""random"" as in roll a dice random.

It's random as in ""arbitrary"". Any position you like can be accessed in the same time / speed / effort.

This is as opposed to stacks, tapes, hard drives and such.

In a stack it's fast and easy to access the top (data point) of it quickly. You don't know what's below it until pop the top to the next below data point and you read it, and so forth.

In a tape or a hard drive, the ""read head"" goes through the data sequentially (literally in a mechanical motion), so that data near the read head is faster accessible than data way before or after it, since the read head needs to seek to that position first to access the data.

RAM has no mechanical or moving components, it's based on electrical current and signals being transmitted.",hqoqcw7,t3_rstlls,1640962406.0,False
rstlls,Nice reference to clear the subject,hqprlp8,t1_hqoqcw7,1640977957.0,False
rstlls,"Yeah, you almost have to think of it as “randomly-accessible memory” rather than “serially-accessible memory” for HDDs or tape drives. Noting that access can either mean read or wrote or both. 

Of course, an SSD nowadays would also be a form of randomly-accessible memory, so the modern usage of RAM to refer to volatile system memory doesn’t really make the same literal distinction as it previously did",hqr2h44,t1_hqoqcw7,1640998727.0,False
rstlls,"Even previously, a ROM chip was randomly accessible too; functionally ROM was no different to RAM when reading.",hqsgd3v,t1_hqr2h44,1641029075.0,False
rstlls,"Yeah people assume ROM and RAM are opposites, when actually they’re entirely unrelated other than both referring to memory

You can have ROM RAM, RAM that is not ROM, and ROM that is not RAM - they describe independent properties of the memory modules",hqszd29,t1_hqsgd3v,1641044891.0,False
rstlls,"While this is correct let me add  that (as an architectural detail of DRAM) you can address data stored sequentially faster due to ""burst mode"". This means that different RAM technology, despite the name, may be a little less random when it comes to time / speed / effort.",hqr8r8b,t1_hqoqcw7,1641001720.0,False
rstlls,"To be correct. This is not a property of all DRAM. DRAM just means its volatile. Burst mode capable ram is called BEDO-RAM, or BEDO-DRAM if it is volatile.",hqsh37t,t1_hqr8r8b,1641029721.0,False
rstlls,"Using addresses you can access any random memory location when you need it. You can't do that with a tape-based memory, for instance. To access the end of a tape you would need to spool through the whole thing, you can't just access any location.",hqog358,t3_rstlls,1640957084.0,False
rstlls,"Exactly, unlike ram which is directly indexed via an address pointer. So you can access memory locations in O(1), unlike the tape like he said, which would be at very minimum O(n) because you have to unspool it until you find your address.",hqov05x,t1_hqog358,1640964534.0,False
rstlls,"Does no one read the other answers? Or just assume none of the first 20 people answered it, and write out the same thing everyone else said?",hqoy7q2,t3_rstlls,1640965931.0,False
rstlls,"It's random because it's describing the way you access memory addresses - **randomly**. Prior to this type of memory, people used some sort of tapes, where you had to sequentially start from lowest address and then go further. With random access you don't need to start from lowest address, you can start at **any** address.",hqog5s2,t3_rstlls,1640957127.0,False
rstlls,"Random as opposed to serial access.

Magnetic tape was used in early computers not only store instructions but to write results.  Core memory and, eventually hard drives were random access by contrast.",hqogd8m,t3_rstlls,1640957247.0,False
rstlls,"RAM stands for Random Access Memory. Random access means that you can access any part of the memory at any time, essentially in ""random"" order.

This is in contrast to serially accessed memory in which data can only be accessed sequentially in the order that it is physically present on the medium. You can't ""jump"" to different parts of the data without ""scrolling"" through the bits in the middle.",hqolpb4,t3_rstlls,1640960118.0,False
rstlls,"From Wiki.

A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such as hard disks, CD-RWs, DVD-RWs and the older magnetic tapes and drum memory), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.",hqog0az,t3_rstlls,1640957039.0,False
rstlls,"Random is meant to characterize it as different from sequential.

In the old days, storage was on tape. This meant the tape had to be wound to the right location, so generally that type of memory is called sequential because you read/write from beginning to end. Random in this case simply means you can read/write from wherever you want without having to wind any tape.",hqommuq,t3_rstlls,1640960588.0,False
rstlls,"It's Random because you can randomly ask from any memory location and based on the information provided like index it can calculate where that memory would be.

Unlike non random where you need to check one by one each memory block to find if that's what you need.

---

**An analogy**

Array is a DS that provides random access you can just say that what `arr[i]`. You can't say that in LinkedList you need to traverse Linked List to reach to `ith` Node.",hqp86ay,t3_rstlls,1640970061.0,False
rstlls,"Additionally to the previous comments, the random access component is also relevant in the context of data structures. So for instance, stacks and linked lists don't allow random access to elements, but arrays do.",hqpky7j,t3_rstlls,1640975226.0,False
rstlls,"The second word indicated by the acronym is the clue: ACCESS.

It means you can access (read or write) any memory location at any time, just by driving the address.",hqpumfz,t3_rstlls,1640979208.0,False
rstlls,"Random access means you can access arbitrary memory locations in any order. An alternative would be sequential access in which memory could only be accessed in order and to get to a memory location at a given address you would have to go through every location whose address comes before it in order.

This isn't just a hardware thing either. There are also data structures that have these access characteristics. The simplest examples of this would be an array or hash table which has random access, a singly linked list which is sequential access only, and then a doubly linked list which has bidirectional sequential access.",hqq55ln,t3_rstlls,1640983642.0,False
rstlls,If only we could refactor technical jargon the way we do code. Arbitrary access memory just makes so much more sense.,hqroiwn,t3_rstlls,1641009652.0,False
rstlls,Ram does not stand for random it stands for Random Access Memory. Which means you can access every address of memory randomly as in it does not matter when you address which memory slot,hqqfwdx,t3_rstlls,1640988318.0,False
rstlls,"Most answers here are indicating that the ""Random"" refers to where memory is accessed - this is only partly correct.

Think about the acronym of RAM in contrast to ROM: Read Only Memory. ROM doesn't necessarily require sequential access any more than RAM does. The only difference betwee RAM and ROM is that ROM cannot be written-to, but *RAM can be written to at any time, randomly*. There is one time you write to ROM, which is the last time, but reading from ROM can be random locations as well - for example, accessing a specific file from a DVD.

The ""random"" of RAM means abritrary read or write requests at any time, not merely accessing random locations.",hqq94s3,t3_rstlls,1640985340.0,False
rstlls,access to it,hqofezp,t3_rstlls,1640956697.0,False
rstlls,Darude Sandstorm.,hqox1lh,t1_hqofezp,1640965428.0,False
rstlls,"random here just means ""not in sequence""",hqosb2r,t3_rstlls,1640963318.0,False
rstlls,Imagine a list of memory addresses sent to u when u need one verse calling a specific one,hqou33i,t3_rstlls,1640964124.0,False
rstlls,"The user is the random part, not the memory itself",hqoujia,t3_rstlls,1640964327.0,False
rstlls,"Because the CPU can directly access data at any random memory address on the RAM.

This is unlike, say, tape memory, where you'd have to go sequentially through locations on the tape, as the tape is rolled to put the right location on it under the read head, to access any particular location on the tape.

Compared to this, the CPU just has to put the address of the location on the RAM it wants to access on the address bus and (simultaneously) the read/write signal on the control bus (as well as output data on the data bus, if it's a memory write operation) and that it's. Transaction will be immediately performed on the desired place on the RAM.

The decoders on the RAM card will take the data from the address bus and trigger the appropriate memory location for operation instantly.",hqovtdk,t3_rstlls,1640964892.0,False
rstlls,It's about the time it takes; it''s constant access time for any random memory location.,hqpb5va,t3_rstlls,1640971271.0,False
rstlls,RAM stands for Random Access Memory,hqpzmit,t3_rstlls,1640981274.0,False
rstlls,"RAM is called ""random access"" because any storage location can be accessed directly.  In addition to hard disk, floppy disk, and CD-ROM storage, another important form of storage is read-only memory (ROM), a more expensive kind of memory that retains data even when the computer is turned off.",ht2f7ne,t3_rstlls,1642443372.0,False
rstlls,"Oh god, the number of people in this thread who don't know the difference between ""random"" and ""arbitrary""...@",hqprnf7,t3_rstlls,1640977978.0,False
rstlls,Perhaps you could explain the difference?,hqsgq5g,t1_hqprnf7,1641029396.0,False
rslk6w,"What's your background? Do you already have a strong background with AI (ML, stats, etc)?",hqowuu4,t3_rslk6w,1640965349.0,False
rslk6w,I'd say I have a very theoretical grasp on ML and a solid understanding of statistics.,hqp0g0o,t1_hqowuu4,1640966877.0,True
rslk6w,"I'm assuming you meant 'a strong grasp on the theory', and not 'theoretically I know it exists'! :)

I seem to remember Deepmind publishing work where they advocated for reinforcement learning with reward being all that's necessary for that leap to be made, or something along those lines.

If my memory is right, that would probably be a great place to start for a literature review because Deepmind garners a lot of attention so there's likely to be a reasonable amount of published content referencing it.",hqp2r9f,t1_hqp0g0o,1640967845.0,False
rsjxaz,"New processors don't come out very often, it'd be a very boring graph to watch. The effort is would take to make something that scrapes for the actual info you want, make sure it's correct, and then displays it nicely is probably 100x more work than just updating the graph each time.  
  
It's kinda like asking if there's a realtime tracker of car fuel efficiencies (MPG). There's not really a use to spend money on it and it's not something that would be ""fun"" to make so... it doesn't get built.",hqn9xc6,t3_rsjxaz,1640927222.0,False
rsjxaz,"Not useful today but Intel has committed to breaking Moore’s Law this decade (after all, it’s only an economics law)",hqnol3m,t3_rsjxaz,1640936347.0,False
rsjxaz,"By beating it, or slacking off?",hqns8dj,t1_hqnol3m,1640939059.0,True
rsjxaz,"If you go by their last 10 years of production, slacking off",hqp6us5,t1_hqns8dj,1640969519.0,False
rsjxaz,Not fast enough for real time to be doable or noticeable,hqn9zz8,t3_rsjxaz,1640927262.0,False
rsjxaz,"I’m gonna be honest, at the rate shit is being developed id recommend putting effort into something else that’s similar.

Not saying it’s a bad idea, but it would’ve been helpful a few years ago, as it would be obsolete pretty soon. But if you wanna give yourself a challenge then I’d say go ahead and do it.",hqnawdi,t3_rsjxaz,1640927756.0,False
rs9uqq,"I studied Computer Vision as an undergraduate, I aided in some novel research using CLIP - a model developed by OpenAI. Our research went on to be awarded NSF grants and will soon be deployed in a production environment for live testing.

There’s never been a better time to get into AI, in my opinion. We are really on the cusp of it becoming generally accessible to people through the magic of State of the Art billion hyper parameter models (think GPT-3). It will only get more and more accessible, and if people can keep finding specific use cases of the general algorithms, or some fine tuned descendant, than we are in for a very exciting time as computer scientists. 

DNNs revolutionized the space and in my mind are one of the strongest arguments against another AI winter. There is an enormous amount of greenfield opportunity for research in this space. (Not that it isn’t usually fascinating work)

If I were you I would decide what kind of problem you want to solve - is it something that requires pattern recognition in structured or unstructured data? If you don’t know what that means go find out. Learn why the distinction matters.

Do you want to do computer vision? Audiovisual? Discrete or streaming data categorizations?

Find a problem and learn what the current algorithm is that provides the best solution. Learn the algorithm, read the papers, lean heavily on any professor or grad student who will help you learn. If you can try to take an elective alongside a professor you respect.

TLDR: it’s a huge field, and your question is sort of like saying you’d like to be a specialized doctor, but you don’t mind what kind. As you explore you’ll seek your interests and be able to do some amazing things.

Don’t be disheartened, it will seem like nonsense and too hard - as CS’s we are highly competitive and feel like it should all come easy. NOTHING WE DO HERE IS EASY. It only ever seems that way in hind sight.

For some, AI is the first part of coding that actually challenges their mind. Let that be an indicator of your growth, not a source of insecurity.",hql4ror,t3_rs9uqq,1640893908.0,False
rs9uqq,"Well said and certainly eloquent. I totally agree that the time is NOW, hence why I’ve been finding these videos and highlights on AI so interesting. I’m going to follow your ideologies here a bit and see where it takes me. 

I’ve always been in love with finding new solutions. And AI research is on the frontier of finding these new solutions. I’m excited to see where it takes me, but I am certainly feeling insecure just due to my lack of knowledge. But like you said, I’ll change my mindset to make that a ladder for my own growth instead of quick sand. thank you 🤝",hql5pq8,t1_hql4ror,1640894278.0,True
rs9uqq,"Your school doesn't offer a course on AI / ML ?

Also, you may be interested in reading _Artificial Intelligence: A Modern Approach_ by Russell and Norvig as it is considered the standard textbook on the topic.",hql2fbb,t3_rs9uqq,1640893006.0,False
rs9uqq,"Good point. It’s a small liberal arts college and it does actually offer one ML course within the CS major. Although due to the timings of how classes work there I wasn’t able to ever take it. 

On the other hand the book is a fantastic suggestion. Looking into it now, thank you",hql2p5c,t1_hql2fbb,1640893110.0,True
rs9uqq,"+1 for Russel and Norvig, I took a class that used it and it's really quite good.",hqnei4b,t1_hql2p5c,1640929793.0,False
rs9uqq,Yeah that book is really good.,hqlrp8l,t1_hql2fbb,1640902973.0,False
rs9uqq,Start by learning basic regression and go from there,hqnmfnc,t3_rs9uqq,1640934816.0,False
rs9uqq,"Most of the actual developments involve the maths of how the model works (When it comes to Deep Learning models anyway - as people have pointed out, AI is quite a vague term).

AI research is a very experimental field that relies on demonstrating empirical results. As with most fields, advancements follow on from previous advancements. Many highly impactful AI papers are just refinements of previous developments. For example, the difference between a ""traditional"" GAN and a Wasserstein GAN (wGAN):

[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661) (Original GAN paper)

[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875) (wGAN paper)

It essentially uses the same architecture, or at least is basically architecture agnostic when it comes to layers, etc. The important thing was the switch from using minimax Loss which is reliant on cross-entropy between probability distributions, to using the Wasserstein distance instead. This practically speaking solved significant problems like mode collapse.

Also, Deep Learning itself can be kind of counter-intuitive, for example:

[https://www.pnas.org/content/117/48/30033](https://www.pnas.org/content/117/48/30033)

If you're asking how people come up with these things in the first place - I can't really give you an answer. All research really starts with an intuition. Knowing a decent amount of maths is probably a good start, at least as much as allows you to be able to research topics on your own and gain an understanding of them. However, you don't have to be a mathematician - colleagues of mine come from all sorts of backgrounds: Biomedical Engineering, Software Engineering, CompSci, Business, etc.

If you're coming from a CompSci/Software Engineering background, it will be quite easy for you to jump in and start running state-of-the-art models. Here's a Colab notebook you can try out:

[https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/\_downloads/torchvision\_finetuning\_instance\_segmentation.ipynb](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb)",hqlojr4,t3_rs9uqq,1640901702.0,False
rs9uqq,"Hey thanks for this, it’s a more complex response so I have to read through it but I appreciate the response ! Helps me get some direction",hqlv5fg,t1_hqlojr4,1640904353.0,True
rs9uqq,I am doing my PhD in AI right now and started about three years ago with courses on it from Andrew Ng. Go visit deeplearning.ai and start those courses and interactive sessions 😊,hqo4kj6,t3_rs9uqq,1640948987.0,False
rs9uqq,[deleted],hql3fgr,t3_rs9uqq,1640893390.0,False
rs9uqq,That’s helpful. I was missing the part about grad school. Since i’m only going to graduate with a BA in Comp Sci and I didn’t think about grad school it makes sense that’s where you would learn how to write scientific docs. Now I have to decide if that’s what I want to do in the future… I’m certainly no genius though 😂,hql3ugg,t1_hql3fgr,1640893550.0,True
rs9uqq,[deleted],hql54ie,t1_hql3ugg,1640894047.0,False
rs9uqq,"No worries, this gives me enough I go to look into it on my own. Cant thank you enough 🤝",hql8szm,t1_hql54ie,1640895506.0,True
rs9uqq,"I own a textbook teaching fundamental AI technology. I spent time implementing the technology as a computer application. As part of my study in AI technology, I learned game programming so that I could write AI applications that would display something cool to watch.",hqm7j4c,t3_rs9uqq,1640909436.0,False
rs9uqq,"It starts as simple as it can get, a simple choice-making model, such as a couple of match boxes with colored beads, where every state in a game of hexapawn is represented by one matchbox. The beads inside the boxes represent the potential choices the model can make.


You start playing, and everytime it is the model's turn, you pick the box that represents the current state of the board. You shake the box and randomly pull a bead out. This bead should have a meaningful choice associated with it, and you perform this such choice.


By the end of the game, if you won, you remove the bead that represented the model's last move, which means it can no longer make that bad move, which necessarily, at least near the end of the game, makes the model slightly better at the game than before.


After several games, you will notice the machine winning more, because the choices it made before, the ones that lost the early games, can't be performed anymore.


At least, that's my understanding. This is attributed to Vsauce2. Soon, you get into understanding the different types of reward models and choice models, which are sort of plug and play. Some have a mathematical definition, such as a fitness function or a neural network that makes the choices instead of having a box for every possible state. There's also a few different ways to iterate the model, such as directly manipulating the model, genetically evolving the model, or just randomly changing everything about the model over and over until something happens to work.

Granted, I am not super experienced in the field, as I haven't personally been able to experiment with machine learning, but I've been looking into all sorts of computer science subfields.",hqx0wh4,t3_rs9uqq,1641115092.0,False
rs9uqq,"There's a couple of Ai Libraries (Tensorflow, Pytorch, Scikit learn). Just start by reading the guides on one of them (I recomment Scikit learn as it's targetted towards beginners), and Google anything you don't know.

There's no one path to learn ai, I've found that it's primarily just looking around and seeing what you find.",hr58pfb,t3_rs9uqq,1641255459.0,False
rs9uqq,"I got a PhD in Econ and did a dissertation in computational agent based economics.

""AI"" is such a broad term and used inconsistently. A\* pathfinding used to be considered AI.

Some are talking about logistic regression to build decision trees.

Some are talking about genetic algorithms.

I use behavioral trees in my hobby game project.

Here is the general rule of thumb: don't focus on methods. Methods are for people on the left side of the dunning krueger curve. You limiter is always your data. If you are making a game you probably don't have it set up to collect game data and adjust AI behavior on that so you just go with creative methods of building AI like a behavioral tree that you build by play testing.

AI is overused and oversold. Most of what the explosion in the recent years is just logistic regression and some algorithms that adjust the models in real time.

A lot of people into programming turn tail and run when people start talking about ROC curves and Gini statistics.

If you are doing actual data analysis you have to be careful about just tossing data in some off the shelf python library. Depending on what type of data it is, its very easy to make junk conclusions. In economics the big mistake everyone makes is including GDP in models when it should be GDP growth rates.",hqlcb8z,t3_rs9uqq,1640896893.0,False
rs0z7i,"Building a binary tree where the nodes are the agents should work.

After building the nodes (lets assume numner of agent n is in a form of 2**k for some k) you will go in post-order traversal on the tree where the value of each would be the winner between both of his childs.

If we assume each game takes O(m) operations then finding the root would take O(nm) operations.

A very important note that this is an approximation for the best, as usually there is not a single best (agent a beats b, b beats c and c beats a for example) so if your simulation is fast i would try to simulate a bunch of the trees with each agent recive a mark based on how far he ends up from the root and see how much the data flucuate with more simulations.",hqjjdr4,t3_rs0z7i,1640870058.0,False
rs0z7i,"I think that might work, but after each round, the top X% are selected to breed the next generation while also participating in the next round. So I already know how strong they were in the previous generation, so I was thinking maybe I can cut the number of games in the following generations because I know how strong some of the agents are. Maybe you can help me find a way?",hqntva4,t1_hqjjdr4,1640940324.0,True
rs0z7i,"Hmm that's not trivial and I dont think theres an optimal solution for you.

On top of my head theres three options, all has their drawbacks and advantages over the other methods. 

One option would be saying that the ranking of last generation is somewhat noisy, in this case you could just treat your old agents just like new ones with the difference of the ranking would be:

r_new(iteration i) = \gamma r_new(iteration i - 1) + (1-\gamma) r_old(iteration i)

With gamma being a parameter of your choice between 0 and 1. 

This method would not decrease the number of evaluations but it would lower your ranking variance which would help your algorithm to converge.

The second method would be building your match tree in such way new agents would always be against old agents.

You can do that by rebuilding your match tree after every round and enforcing new agents would always be against old ones. Eventually you would need to pair old ones against old ones and you would simply take the better agents from last iteration without simulating a match.

This would yield low amount of simulations as you wont simulate old agents against each other, and it would work in the case where all new agents are better then the old ones (after a round the entire match tree would be only new agents), but the drawback being you might lose out new good agent quickly when he would face the best old agent in the first round - not something you want.

The last method comes to mind is somewhat hard to explain. its similar to the second method but instead of using all of the old agents in the first round, the first round you would use the worst half of your agents and all of the new agents. In the second round you would insert the worst half of the remaining agents and so on.

This would work similarly in terms of amount of matches to the 2nd option - this is better for cases where the new generation is not that different then the old one.

That's a fun exercise and those were just methods from the top of my head - there could be better options then i stated.",hqny403,t1_hqntva4,1640943740.0,False
rs0z7i,Is this the same problem ELO solves?,hql8ycf,t3_rs0z7i,1640895564.0,False
rs0z7i,"Kind of, but I want the agents to play as few games as possible, and elo ratings take a lot of games to be accurate.",hqlezx6,t1_hql8ycf,1640897945.0,True
rs0z7i,"Both answers were perfect. I have been interested in different types or agent modelling, so that was helpful.",hqm1fy7,t3_rs0z7i,1640906933.0,False
rs0z7i,"To me, that type of model, while it sounds slow, seems like it could find the best player in a batch much more accurately than giving them all a single-number score.",hqx1396,t3_rs0z7i,1641115246.0,False
rs0z7i,"Hi, this is really interesting. How does one implement a game so that agents can play chess? Like a specific language? I know this is a noob question.",hqjzmx8,t3_rs0z7i,1640877976.0,False
rs0z7i,"What is the question? How to implement chess or how to implement the agents?

For the agents, I used a monte carlo search tree where the final positions are evaluated with a neural network

For implementing the game, I used the python chess library, if you want to implement it yourself there is a lot of material on the topic",hqleo42,t1_hqjzmx8,1640897813.0,True
rrvks1,"There is a way to solve them programmatically, and yes, it is a kind-of brute force, but a common technique is to not write the 'brute force' piece yourself, but to rely on a tool (sometimes a library in a programming language, sometimes a first-flass language in it's own right) called a ""SAT Solver""

[https://cse.buffalo.edu/\~erdem/cse331/support/sat-solver/index.html](https://cse.buffalo.edu/~erdem/cse331/support/sat-solver/index.html)

Here's a blog entry closer to the problem type your are trying to solve:

[https://sabhijit.medium.com/logic-puzzles-and-sat-solvers-a-match-made-in-heaven-5e0a7a64c04b](https://sabhijit.medium.com/logic-puzzles-and-sat-solvers-a-match-made-in-heaven-5e0a7a64c04b)

Here's a paper specifically about the ""logic grid"" type of puzzles you see here

[https://freuder.files.wordpress.com/2019/09/challenge-ucc-submission.pdf](https://freuder.files.wordpress.com/2019/09/challenge-ucc-submission.pdf)

Languages like prolog and icon are specifically geared towards solving this kind of problem in a declarative style.",hqkdhle,t3_rrvks1,1640883542.0,False
rrvks1,"Link for better formatting: [https://stackoverflow.com/questions/70527987/solving-logic-puzzles-using-r](https://stackoverflow.com/questions/70527987/solving-logic-puzzles-using-r) 

Thanks!",hqiqd91,t3_rrvks1,1640848651.0,True
rrvks1,Take a look at Logical Programming and SAT,hql0m9c,t3_rrvks1,1640892312.0,False
rrvks1,Consider implementing a DPLL solver. You can use a variety of heuristics to perform substantially better than random in practice.,hqlvd13,t3_rrvks1,1640904438.0,False
rrvks1,"This is a job for quantum computers!
I want a quantum computer so bad...",hqx17cm,t3_rrvks1,1641115341.0,False
rrsvuo,"Applications have states of process, like waiting, running, and so on.  You double click gmail to check your messages, while gmail is loading you check your Facebook page, but it's dead, so next you access Instagram.  Now you go back to check your gmail, this is known as Context Switch.  First you opened gmail, the scheduler will grab gmail app and start loading it, but you wanted to see who is on Facebook, the gmail app will go to a wait state and placed in the queue, then Facebook will go active, then you checked Instagram, so Facebook goes to the wait state and placed in queue and Instagram goes active. Then you go back and check your emails.  Now Instagram goes to the wait state while gmail goes to the active state.  
  No matter how it's worded, the CPU can only execute one app at a time.  It can load a program in each core while executing the main program, but it can only run one app at a time, but it will switch to another app in queue if it sees the current app go to a wait state, like waiting for user input.  It can switch between thousands of apps in seconds. 

So you have a scheduler that handles the removal of the running process from the CPU and the selection of another process on the basis of a particular strategy.

The OS scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system;

Schedulers main task is to select the jobs to be submitted into the system and to decide which process to run.


CPU scheduler main objective is to increase system performance, also selects a process among the processes that are ready to execute and allocates CPU to one of them.

Short-term schedulers, also known as dispatchers, make the decision of which process to execute next. Short-term schedulers are faster than long-term schedulers.


Medium-term scheduling  removes the processes from the memory. The medium-term scheduler is in-charge of handling the swapped out-processes
A suspended processes cannot make any progress towards completion. In this condition, to remove the process from memory and make space for other processes, the suspended process is moved to the secondary storage. This process is called swapping, and the process is said to be swapped out or rolled out. Swapping may be necessary to improve the process mix.


A context switch is the mechanism to store and restore the state or context of a CPU in Process Control block so that a process execution can be resumed from the same point at a later time.

When the scheduler switches the CPU from executing one process to execute another, the state from the current running process is stored into the process control block. After this, the state for the process to run next is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing.

This is the basics of a CPU running applications.  There is another task in there I cannot recall at this moment.",hqihlvj,t3_rrsvuo,1640843091.0,False
rrsvuo,I applaud you taking the time to write all of this out.,hqij16y,t1_hqihlvj,1640843906.0,False
rrsvuo,"The answer of u/DevilDawg93 is about the OS scheduling, but misses the part about the interaction with I/O devices. To answer your question: there are multiple levels of security in which a CPU can be, mainly the **privileged mode** in which the software can do anything, and the **user mode**, in which the CPU forbids access to the areas of the memory not explicitly allowed, access to I/O devices, use of some instructions, etc.

How it works: when the OS starts booting, it executes in privileged mode. It can organise the memory of your computer whenever it wants, and use some areas in the memory (the RAM) to write some important tables, in which you will find the pages tables (which indicates the regions of the memory on which a process can read and/or write). The OS sets some important registers of the CPU: one is pointing to the **interruption** handler, a function in the OS that can manage interruptions, another to a page table.

When the OS switches to a process, it writes in the special register the pointer to the pages table of the process, switch to user mode, and then jump to the code of the process, letting this process do its stuff. Because the process is in user mode, it can't do anything dangerous (requiring the privileged mode)... but can ask the OS to do it. It can trigger an interrupt (a system call), which will call a function of the OS. The CPU switch to **privileged mode** when it happens, and this is safe because only the OS could have written in the register referencing the interrupt handler. So there is no way (outside a security breach) for the user process to write in another program memory or use the I/O directly, the restriction is built in the CPU itself. The application **must** ask the OS to do it itself, and then the operating system takes the time to check if the application is allowed to do so, or not.

Of course this is a bit simplified view of how it works, modern CPU are very complex, but it still works according to this schema. I should add that switching from one process to another (as described by u/DevilDawg93) works the same: in privileged mode, the OS can configure the hardware clock to send a signal (for example every X µs). Each time the signal is sent, an interrupt is triggered, and so the CPU starts executing OS code in privileged mode. It is up to the OS to save the process state somewhere, set the CPU registers (for the pages table of the next application), load this next process state, switch to user mode and start executing it \[this is the context switch\].",hqjoe0s,t3_rrsvuo,1640872773.0,False
rrsvuo,"Well stated u/webalorn! 

Happy Cake Day!",hql2nr0,t1_hqjoe0s,1640893096.0,False
rrsvuo,"Thanks a lot guys, it’s very clear to me now !",hqkihf4,t3_rrsvuo,1640885423.0,True
rr75ns,"Rule #5 bans joke submissions, but in this case I think I will make an exception since in the title we have a genuine question about research on sorting algorithms.",hqexeo5,t3_rr75ns,1640789165.0,False
rr75ns,much appreciated! 🤗,hqp2my9,t1_hqexeo5,1640967795.0,True
rr75ns,"The phenomenon described depends on levels of serotonin and octopamine secretion where serotonin levels are high in dominant crabs and octopamine levels are high in submissive crabs. So, the submissive crabs tend to make sure the coast is clear before laying claim to anything.
If by chance no dominant crab shows up during the eight hours of waiting and shows up later after, then the submissive crab either gives up the loot or challenges the obviously dominant crab which could be brutal for either or both of them with the winner and the loser getting higher levels of serotonin and octopamine respectively.

Totally unrelated but👍",hqey5kd,t3_rr75ns,1640789506.0,False
rr75ns,appreciate the fun fact!,hr01kky,t1_hqey5kd,1641166521.0,True
rr75ns,"You can see it in action here:
https://youtu.be/f1dnocPQXDQ",hqexc1l,t3_rr75ns,1640789131.0,False
rr75ns,I dont think waiting for up to 8 hours would be a good starting point :),hqejw8s,t3_rr75ns,1640781898.0,False
rr75ns,Maybe they're using [SleepSort](https://www.quora.com/What-is-sleep-sort)?,hqesbxq,t1_hqejw8s,1640786707.0,False
rr75ns,It's good enough for crypto people,hqezvfd,t1_hqejw8s,1640790283.0,False
rr75ns,"I think we already have a lower asymptotic bound on the time complexity of sorting algorithms at O(n logn):

If we only permit comparisons and swaps between two elements, then every sorting algorithm can be represented by a binary tree where each node is a comparison, with only two possible outcomes (assuming there is a total order on all elements).

Any correct sorting algorithm must have a different set of swaps for each permutation of an n-length input, so there must be n! leaves, so the height is lg(n!), Which is Omega(n logn)

Of courses, there is still possible research around real-world applications",hqew0s2,t3_rr75ns,1640788512.0,False
rr75ns,">I think we already have a lower asymptotic bound on the time complexity of sorting algorithms at O(n logn):

Just want to clarify that this is only for comparative sorting. There are faster sorts for some specific cases, such as a Radix sort.",hqgnm2i,t1_hqew0s2,1640813733.0,False
rr75ns,How does the time change when it is the elements of the list comparing themselves? This allows for every element to be compared to one other simultaneously. Wouldn't crabsort then be O(n)?,hqf2s0c,t1_hqew0s2,1640791562.0,False
rr75ns,"I prefer using Computational complexity over Time complexity as it represents the relationship between the number of data elements and the amount of operations that need to be executed, and the growth of the relationship. 

It's not exactly time, so much as how many steps do I have to complete (which indirectly is time). Even if you're doing comparisons for each individual in parallel as it relates to others, you would still have n individuals doing n observations, for o(n^2) computational complexity.",hqgprgk,t1_hqf2s0c,1640814569.0,False
rr75ns,"The answer depends on your computational model. Assuming you work on a single tape touring machine, your sorting algorithm would be in O(n²) - you sort n elements whereas each element has to compare itself with up to all elements already in the list. On a PRAM it is in O(n²) as well since you multiply the number of cores by the number of instructions each core has to compute.",hqf5rch,t1_hqf2s0c,1640792843.0,False
rr75ns,"The Turing Omnibus mentions a linear time sorting algorithm called ""spaghetti sort."" Grab spaghetti with different lengths, put it in your hand, and tamp down gently on the bottom until they're all even. Repeatedly pick out the longest stand by putting your hand on top and picking the first one it hits. Since both these hands operations are constant time, you get a linear sorting algorithm!!

Let's do a quick analysis to see where this algorithm, and any ""natural"" algorithms in real life, fall short:

- hands are slow. Where a computer might pick something from memory in a few microseconds, it takes dozens of milliseconds to pick out spaghetti from your hand. That's a hard time limit, since physics prevents the spaghetti from being moved too quickly lest it break.
- there are practical bounds on the length of spaghetti.
- it's hard to hold more than, say, 100 spaghetti strands. Practical sorting will require millions of items to sort.
- in order to make this transferable to and from a computer, the computer has to output spaghetti of certain lengths, or cut spaghetti to a certain length. Beyond the precision required, it's just slow. It might be faster reading the spaghetti, although that may require a computer vision algorithm or some sort of spaghetti index.

So, even though it is a linear time sorting algorithm, practical problems abound around the interface and scaling. I think you'll find those problems anywhere you attempt to use a natural algorithm.

To wit: even if you digitized this algorithm, you'd just be doing insertion sort: the hand moving down from above would be replaced with a physics engine determining interactions between all n spaghetti strands, which is n operations n times for a n^2 runtime.",hqf0e7a,t3_rr75ns,1640790513.0,False
rr75ns,What you described is essentially just an oracle that returns the maximum of a list in constant time.,hqgcslz,t1_hqf0e7a,1640809574.0,False
rr75ns,"I think the selection step actually should take linear time in the max height, since if you're actually applying a uniform procedure, or if you built a machine to enact this process, it must start at least at the max height and potentially sweep all the way down through the range of heights. Consider the case of one 6"" spaghett and ten million spaghetts of negligible height; the average distance each sweep is almost 6"" and distance is time.",hqhnkvh,t1_hqgcslz,1640828756.0,False
rr75ns,"Not sure I completely get what you’re saying. I’m personally imagining some spaghetti standing upright in a bunch on a table. There is then a flat surface from above weighing down. The flat surface repeatedly selects the point of contact with the set of spaghetti, which is just a maximum element, and removes it from the clump (note the surface is already touching the max so it doesn’t need to perform an entire sweep to extract it). It only makes one sweep in total, so it takes O(n + h) comparison and extraction operations where n is the number of noodles and h is the maximum height of a noodle. This is better than radix which uses O(kn) operations where k is the number of digits.",hqhoa7d,t1_hqhnkvh,1640829060.0,False
rr75ns,"If the tallest one is in the middle of the bunch, you have to lift the plate back up to pick it out.",hqhoy2g,t1_hqhoa7d,1640829347.0,False
rr75ns,Why? I think our visualizations is different. Did you read my description?,hqhp2zi,t1_hqhoy2g,1640829408.0,False
rr75ns,Yes. How does a flat surface remove the spaghett it hits? Going back to a person with two hands- one hand holds the bunch together vertically. The other hand sweeps down to hit the tallest and then picks it out. That means potentially sliding it vertically out of the middle.,hqhppp8,t1_hqhp2zi,1640829697.0,False
rr75ns,"Left hand is on the bottom. Spaghettis align flat on the left hand. Right hand is on top pressing down. It is constantly in contact w the max. It extracts that noodle by repeatedly letting the one in contact fall. No sweeping needed to remove the max, as the right hand is just constantly pressing down from gravity.",hqhqjov,t1_hqhppp8,1640830079.0,False
rr75ns,"How does the noodle in contact fall through the left hand? To ensure heights remain comparable they have to be flush at bottom, against the hand or preferably the table.
And in any case, once we grant they must slide out that provides a new worst case: they are all MAXHEIGHT to within a negligible distance. Then to get free each one must slide ~MAXHEIGHT inches and we are linear in max height again.",hqhr2nx,t1_hqhqjov,1640830310.0,False
rr75ns,"The left hand can be a grid surface that can open up the grid point corresponding to the grid point that the right hand is contact with. Also yes, the linear in h is unavoidable (its also not new, thats what the h is in my previous comment), but it is O(h + n) rather than O(hn) as you describe, which is much worse. Note it is O(n+h) instead of O(nh) because the noodles are sliding concurrently, the reason it is so effective is because it can take advantage of this huge parallelism. A lot of “biological algorithms” operate on this principle: go read on Adelman’s work on solving a non-trivial TSP instance through biological algorithms. O(n + h) is better than radix sort while O(nh) is not, and that is important as radix sort is sort of the “standard” for non-comparison sorts. 

Also you are thinking too much into the hand analogy, they can easily be held in place by some easily implementable mechanical process, or just don’t use noodles lol. The point is that there are physical sorting algorithms that use a number of operations linear in every parameter, e.g. O(n + h) instead of O(nh), which are enabled due to parallelism scaling with the size of the problem, which is clearly not a property of problems solved in computers. That is what the not-really-active field of biological algorithms tries to take advantage of.",hqhrexn,t1_hqhr2nx,1640830458.0,False
rr75ns,The book Algorithms to Live By talks a lot about interesting cases like this!,hqf2h69,t3_rr75ns,1640791431.0,False
rr75ns,thank you!,hqp04x2,t1_hqf2h69,1640966747.0,True
rr75ns,You could easily crop this image without the giant text claiming people that provide a service to people to rent a place to live as being awful people.,hqgm0w9,t3_rr75ns,1640813124.0,False
rr75ns,!RemindMe 24h,hqhkbzk,t3_rr75ns,1640827321.0,False
rr75ns,"I will be messaging you in 1 day on [**2021-12-31 01:22:01 UTC**](http://www.wolframalpha.com/input/?i=2021-12-31%2001:22:01%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/rr75ns/it_would_be_really_interesting_to_research/hqhkbzk/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Frr75ns%2Fit_would_be_really_interesting_to_research%2Fhqhkbzk%2F%5D%0A%0ARemindMe%21%202021-12-31%2001%3A22%3A01%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20rr75ns)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hqhkfw6,t1_hqhkbzk,1640827369.0,False
rqysm5,Sexbot,hqdkhst,t3_rqysm5,1640755907.0,False
rqysm5,"Turing defined it in his 1950 paper, ""Computing Machinery and Intelligence."" Basically if AI can pass the Turing test, they are effectively as intelligent as anything else.",hqdbkt1,t3_rqysm5,1640751162.0,False
rqysm5,"There have been chat bots which can almost pass it for decades. More interestingly, there are many arenas of life in which people must stop for a moment to think ""is that a robot?"" without knowing for certain. Video game bots (including chess bots), forum (e.g. Reddit) bots, and chat (customer service) bots are all pretty obvious examples but there are less obvious ones too. How long can you drive behind a self driving car on a rainy day before you realize it's autonomous (driving is in many ways communication)? I have had one or two automated robo calls to my phone which took me an embarrassing amount of time to realize were adaptive recordings of some kind (like phone trees, which are also getting really good). There are many games in which it is not obvious that an opponent is a bot.

Basically, I think we are already at the point where the average person is occasionally going to be fooled by a bot impersonating a human. We have long been accustomed to occasionally losing to them at games, although even in victory they are rarely mistaken for human. This is in contrast with the fact that we don't have the kind of artificial general intelligence most people think of when they imagine something passing the Turing test. When it comes to our human ecosystem and the bots (which are not yet ""true"" AIs) which inhabit it, there's already a lot of them out there doing very complex and borderline things (like high frequency trading) which place them in a sort of mixed arena with humans. The expectations of the 1950s are only so useful at the end of the day.

I think it is probable that there is no ""magic line"" after which a thing is ""intelligent"" or sapient or AGI or whatever. We are unimpressed by our current level of accomplishment, but what if we were time travelers from a hundred years ago? Without the finer points of context to help with the distinction, there are a heck of a lot of bots or automatic systems which could easily be mistaken for thinking and willful things. I think that as more and more systems are put in place to make AIs seem intelligent, that line will get crossed without fanfare or anyone having a reason to ""declare"" it crossed. Some day we will all be having cogent, deep, complex, back-and-forth, intelligent conversations with our phones or desktops or cars, and people will still be wondering ""is AGI here yet? What about the Turing test?"". Perhaps instead of a fine line we can observe the stepping across of, we should think of it as a large and fuzzy zone, with us in the middle of it somewhere. People will always be able to point to a seemingly intelligent program and say ""aha, that's just mechanistic"" but at a certain point you are making very philosophical arguments about the meaning of emergence through mechanism by attempting to disprove the alleged intelligence of a program. At some point it will just be easier to assume a thing is sapient until proven otherwise, and I am okay with that future. I think it'll be pretty neat.",hqe2p0v,t1_hqdbkt1,1640768748.0,False
rqysm5,"You're missing the point of Turing's paper. If you can't tell the difference, it doesn't matter if it's ""true AI"" or not. That is the line. So far I still know when to hang up my phone from a robocall. So far I'm not ready start giving my toaster or candlesticks ""rights.""

The difference between writing and chess is the fact that you think in words, not chess moves. That's why it's the only important barrier. Turing thought this all out, trust me. He offers rebuttals at the end of his paper for some of your claims.",hqg6g6s,t1_hqe2p0v,1640807122.0,False
rqysm5,"I was supporting that point, not arguing with it. I think a lot of people are missing all the important points by focusing on this idea of a ""true"" AI when, as you say, that was not really the point of the Turing test. 

>So far I still know when to hang up my phone from a robocall.

Me too, most of the time, but it's getting to the point where I have
on several occasions been unsure if it would be okay to hang up or not
because the robocall was so convincing (on one occasion even replying to me like one of those ""speak to me"" phone trees). It has gotten me thinking about what a future world would be like in which you had to take for granted that sapient processes were all around you, in any given object that could be programmed. In such a world you might adopt a policy of politeness towards objects that would seem highly unusual (to put it bluntly) to someone from, say, Victorian Europe. I already say ""please"" to my Google Voice just because it's healthy to build those reflexes when using your communications muscles, so how much more interesting and nuanced in a few more years when the world is full of systems which are ""close enough to intelligent"" to make you think about your social reflexes when dealing with a machine?

>So far I'm not ready start giving my toaster or candlesticks ""rights.""

I'm not convinced it will ever ask for them, honestly, no matter how far these things go. You may not want to give your candlestick rights, but at what point do you begin to offer it politeness or consideration of some kind, if ever? Or at what point do you get mean to it? In a world full of somewhat sentient or sapient devices which don't want or need ""rights"", it will say a lot about a person how they treat those things. Not for the sake of the device, but for the person's social reflexes.

>The difference between writing and chess is the fact that you think in words, not chess moves. That's why it's the only important barrier. Turing thought this all out, trust me. He offers rebuttals at the end of his paper for some of your claims.

Unless one is just typing randomly then there is logic, movement and order to writing. This is why we can describe computer programs using words, and also chess games for that matter. The comparative cognitive load between traditionally ""cerebral"" activities is probably an area that is prone to a lot of preconceived prejudices, so I hesitate to compare them. You can take a chat bot so far in a conversation sometimes because of how effective projection and cold reading are. To have a deep, cogent, considered, insightful, top notch conversation is so difficult that a lot of humans go through life without ever being more than very bad at it (I'm not great at it myself). When you consider how easy it is to misunderstand, fail to practice active listening, insert a preconception or prejudice, or otherwise mess up a conversation's context wildly between two human beings (and how these things are the norm rather than the exception for all but the most basic conversations between two people much of the time) I think we should consider that in some years most smartphones will be better conversationalists than most people. Even then, we will still probably be arguing about the Turing Test (only then it will be about how ""real people aren't that good at conversation"", in the same way that people accuse video game bots at times). I think it will be just another wake-up call for people who thought that the species was ""defined"" by a specific quality (in this case intelligent communication). Although not all of Turing's rebuttals have aged equally well, I'm not arguing with the paper. I also agree that this ""magic line"" of intelligence is over-rated or does not exist. We will have programs that are ""sapient enough"" long before the argument is settled, philosophically, I think.",hqgje5s,t1_hqg6g6s,1640812110.0,False
rqysm5,"I didn't have to read it, but since I've graded standardized English exams by the thousands, I could tell you're probably human. I'd be willing to bet on it. Congratulations.

Wake me up from Delillo when AI starts betting money. I mean, ""AI"" has owned chess for over a decade and now go, and they still can't make a living like a McDonald's worker fresh out of high school can? Shit, the McDonald's near me is advertising $18/hr. That's like one cheeseburger every 20 seconds.",hqhv5l7,t1_hqgje5s,1640832116.0,False
rqysm5,"every time this question getsasked, someone close to the answer has to come on reddit to downvote the question. This delays the solution.",hqdlq5s,t3_rqysm5,1640756619.0,False
rqysm5,"I don't believe machines will ever truly pass a turing test perfectly until they have a perfect grasp on what it means to be a human. More specifically, what human they are pretending to be.",hqx1ib4,t3_rqysm5,1641115596.0,False
rqqd61,"The computer doesn't know about types. If you do floating point arithmetic, the computer assumes whatever you're operating on is a floating point number; same for integers. The compiler and/or runtime system generally has to keep track of the type of each variable and prevent the developer from doing anything nonsensical. If you're writing assembly, the assembler typically won't enforce any type checking and this responsibility then mostly falls on the programmer.",hqbzit3,t3_rqqd61,1640729572.0,False
rqqd61,"Fun fact: Some language (like C) have ways to reference the same memory location by different data types. You could store a 32 bit unsigned integer, and then read it back as a 32 bit floating point number. And yes, there are legitimate reasons you may want to do that. But it all boils down to the programmer **choosing** to refer to some variable as a specific type. (Or sometimes the language will automatically select the type based on what operation the programmer decided to apply to it.)",hqd21ff,t3_rqqd61,1640746582.0,False
rqqd61,"The data type is not stored with the variable, the data type is stored in a sort of table of contents. Most of the time, when a file is deleted, the file itself is still there, it's just that its index in the file system is removed.",hqx1p7u,t3_rqqd61,1641115759.0,False
rqeudi,Which language do you have experience with?,hqa84y4,t3_rqeudi,1640703903.0,False
rqeudi,Python and a bit of JavaScript,hqee586,t1_hqa84y4,1640777841.0,True
rqeudi,Personally I liked the book Introduction to the Theory of Computation by Michael Sipser as an introduction to the ideas behind computer science.,hqadybv,t3_rqeudi,1640706408.0,False
rqeudi,Thanks! I'll give it a look,hqee6dm,t1_hqadybv,1640777864.0,True
rq60d4,Rule 6. Format your code and text into maintainable and easily readable form. Apply to Reddit posts.,hq8jcrb,t3_rq60d4,1640664315.0,False
rq60d4,"Disagree. If it was hard to write, it should be hard to read.",hqal7s1,t1_hq8jcrb,1640709390.0,False
rq60d4,Lol,hqaljhv,t1_hqal7s1,1640709522.0,False
rq60d4,"If I had to suffer, others have to suffer too? Is this what you mean?",hqbbc1r,t1_hqal7s1,1640719727.0,False
rq60d4,"Yes, but they have to suffer in a different way.",hqbdqq3,t1_hqbbc1r,1640720687.0,False
rq60d4,Sounds petty tbh,hqbnbfb,t1_hqbdqq3,1640724560.0,False
rq60d4,"It's not; if it's hard to read, then hopefully it scares the interns off.",hqbpppw,t1_hqbnbfb,1640725543.0,False
rq60d4,[deleted],hq8mfwp,t1_hq8jcrb,1640665815.0,False
rq60d4,"The real Rule #7: There are plenty of rules to programming for a real purpose, get creative on your own time.",hq8qyia,t1_hq8mfwp,1640668201.0,False
rq60d4,[deleted],hq8sp4l,t1_hq8qyia,1640669185.0,False
rq60d4,"Do you believe that problem solving requires you to have literally no limits? There are rules. There are areas where there are many options within the rules, but there are rules.",hq8t8rv,t1_hq8sp4l,1640669501.0,False
rq60d4,[deleted],hq8zy09,t1_hq8t8rv,1640673693.0,False
rq60d4,"He said, having presented nothing but anecdotes himself...",hq90is9,t1_hq8zy09,1640674098.0,False
rq60d4,[deleted],hq916fk,t1_hq90is9,1640674555.0,False
rq60d4,"That's... not what straw man means. You might have been thinking ad hominem?
 
You've presented no arguments that require countering.",hq91h8a,t1_hq916fk,1640674767.0,False
rq60d4,[deleted],hq928ny,t1_hq91h8a,1640675318.0,False
rq60d4,"I didn't edit any comment - you edited this one though. That little * you see indicates edits, are you having some kind of psych episode?",hq92qvf,t1_hq928ny,1640675686.0,False
rq60d4,Rob Pike on his 6 Rules of Programming - [https://twitter.com/rob\_pike/status/998681790037442561](https://twitter.com/rob_pike/status/998681790037442561),hq9ejqb,t3_rq60d4,1640685077.0,False
rq60d4,"Rule 5 restates Torvald's, ['good programmers worry about data structures](https://softwareengineering.stackexchange.com/questions/163185/torvalds-quote-about-good-programmer)'.",hq8mxyf,t3_rq60d4,1640666071.0,False
rq60d4,"Rob Pike wrote this first in 19**8**7. Linus would have been 18 at the time and didn't start work on Linux until 1991.

https://twitter.com/rob_pike/status/998681791417409536

https://en.wikipedia.org/wiki/History_of_Linux#The_creation_of_Linux",hq9e7mf,t1_hq8mxyf,1640684796.0,False
rq60d4,"**History of Linux** 
 
 [The creation of Linux](https://en.wikipedia.org/wiki/History_of_Linux#The_creation_of_Linux) 
 
 >In 1991, while studying computer science at University of Helsinki, Linus Torvalds began a project that later became the Linux kernel. He wrote the program specifically for the hardware he was using and independent of an operating system because he wanted to use the functions of his new PC with an 80386 processor. Development was done on MINIX using the GNU C Compiler. As Torvalds wrote in his book Just for Fun, he eventually ended up writing an operating system kernel.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hq9e8mx,t1_hq9e7mf,1640684821.0,False
rq60d4,[deleted],hqa4juy,t1_hq8mxyf,1640702229.0,False
rq60d4,Excellent. You get all the disadvantages of excess complexity combined with all the disadvantages of unpredictable performance and inability to scale.,hqbz6x8,t1_hqa4juy,1640729434.0,False
rq60d4,"Ironically, this is what systems programmers are always complaining that frontend developers are doing: using inefficient but simple algorithms until necessary to change.",hqa4g2h,t3_rq60d4,1640702180.0,False
rq60d4,Really? The most common complaint I hear is the opposite - that frontend development is usually ridiculously overengineered for example like introducing a mountain of 3rd party libraries to make trivial systems.,hqbgoe9,t1_hqa4g2h,1640721869.0,False
rq60d4,"The problem is that people don’t have a consistent definition of that word. Isn’t overoptimizing a form of overengineering? So then “not optimizing” by deferring processing to third party libs couldn’t possibly be overengineering. Except that it is, somehow.

Besides, while dependency explosion is a real problem, that’s mostly a fault of the ecosystem and not the developer. Typically all you need is about 5 direct dependencies before the number of node modules is in the thousands. This isn’t the fault of the engineer and has very little to do with their development practices.",hqbne1p,t1_hqbgoe9,1640724590.0,False
rq60d4,">  deferring processing to third party libs couldn’t possibly be overengineering. Except that it is, somehow.

That's a false dichotomy.

You're assuming the only two choices are implementing something complex yourself or pulling in a complex library. There's also the option of doing something simple, or, even better, leaving out that element entirely.",hqbzlyr,t1_hqbne1p,1640729610.0,False
rq60d4,"I know it sounds like that’s what I am saying. But my real point is that outsourcing processing of a thing to avoid overengineering, should not itself be overengineering, yet it is. In other words, the opposite of an action should not be another flavor of the action itself.

Just because your bundle size is large doesn’t mean you overengineered something. It may have been way simpler to use a library than to build it yourself. It’s kind of like saying using a car to get to the other side of the state is overly engineered because you could’ve walked it, or you could have used a bicycle.",hqc08ol,t1_hqbzlyr,1640729876.0,False
rq60d4,"If you pay really close attention you'll notice, that nowhere did I argue against using 3rd party libraries.

I used dependency of *too many libraries* as *an example* of overengineering. This has nothing to do with the notion of moving the processing in-house somehow making it not overengineering.",hqc1t7r,t1_hqc08ol,1640730527.0,False
rq60d4,"> There's also the option of doing something simple, or, even better, leaving out that element entirely.

There's *sometimes* that option. But generally, this is not what people are complaining about in frontend development. They aren't complaining that $APP does too many things, and that they wish it just had less functionality.",hqc3ojh,t1_hqc1t7r,1640731312.0,False
rq60d4,I think you meant to reply to /u/Tai9ch,hqc4fo2,t1_hqc3ojh,1640731629.0,False
rq60d4,"Yes, sorry I pulled a switcharoo on you by deleting my (poor) first draft.",hqc7v6z,t1_hqc4fo2,1640733084.0,False
rq60d4,No problem. :-),hqc9gfd,t1_hqc7v6z,1640733774.0,False
rq60d4,"Which word? Optimization? Neither OP nor your first reply nor my reply mentioned that word even once, so I'm not entirely sure why you introduce this concept.

Overengineering isn't at all the same. Optimization is figuring out how to do the least amount of work to accomplish a task. Overengineering is introduction of an unacceptable amount of accidental complexity. Optimization can be overengineering, if the optimization isn't necessary, but these concepts have nothing in common besides this.

I agree though that there is a lot of confusion and misunderstanding because technical words are misunderstood, misused and abused all the time. There are a lot of blinds leading the blinds in this industry.

I partially agree about your comment on dependency explosion. It's partly an ecosystem issue, but also partly a cultural issue. I will not fault the engineer of the dependency explosion as a whole, but I refuse to absolve the engineer of the responsibility of their choice of libraries for their project.",hqbrazk,t1_hqbne1p,1640726200.0,False
rq60d4,"I think I made a mistake by going down this road of talking about bundle size. It's really different from the main complaint I have heard, which is that the algorithms are inefficient and as a result, web apps and electron apps etc. use way more cycles/battery power than they need to. My retort to that is that of course, that's the only reason you're using the program -- because it could be built faster than we used to build software. And it's done faster now because we have the power of computationally inefficient tools like ES6 spreads and dynamic hashtables forming immutable datastructures, which make programming itself more efficient of a (human) process than something like dirty flags and mutable state/pubsub insanity.

I disagree on definitions for overengineering, optimization, engineering. IMO optimization and engineering are basically the same thing; you have a system of constraints, as well as a system of preferences, and are trying to produce a solution that solves all of the constraints while honoring the preferences as much as possible. You can engineer for certain things (our company has no money, it needs to be entirely libre software) or others (our company has infinite money, spend as much as possible to get it done quickly), just as you can optimize for either memory or speed. Over-engineering is a strange word because in e.g. bridge design it basically means ""to make something stronger."" To make something stronger in computer engineering is to make it simpler and easier to manipulate, which I feel is really not what people mean by ""overengineering.""

Typically when people complain that an Electron app is overengineered, they don't mean it in the traditional sense of the word, which would be to say it has too much functionality, too much capability. They are cajoling it to mean ""they spent too much engineer time"" on this, I guess, which to me is paradoxical since the whole point of bringing in React, Angular, etc. is to save time, and it often does.",hqc4vim,t1_hqbrazk,1640731816.0,False
rq60d4,"It's alright. :-)

I kind of agree with your definitions. Sort of.

Optimization is a loan word from the field of mathematics, which - loosely speaking - means to maximize or minimize a function given a set of constraints.

I'd rather describe engineering as the practice of designing a product while constantly assessing and choosing trade-offs between various considerations - quality metrics like application speed, size, features and correctness, but also ""softer"" considerations like development time, maintainability and customer requests. What you describe as optimizations I'd rather describe as trade-offs, because that's really what they are - what do we want and what is the cost? These trade-offs are what an engineer must make all the time and try to balance everything acceptably.

I fully agree that overengineering is an odd word. We generally agree that engineering is a good thing, so it's odd overengineering became a pejorative for a particular, disliked strategy of trade-offs.

Anyway, to get back to the central point of our conversation. I come from a system programmers perspective, and I too find it odd that web apps are criticized for algorithm efficiency. Web apps tend to use a lot of cycles, but I don't think the algorithms are the culprits here - I think the cycles may be eaten either in the abstraction layers or by the interpreter itself. This is just pure conjecture though, because I'm not knowledgable enough about web dev to confidently make an informed judgment about it.",hqc94ix,t1_hqc4vim,1640733630.0,False
rq60d4,"I had a thought, and I have a conjecture on how the meaning of overengineering got warped.

If we consider the classic meaning, where it means something is made unusually well (like a bridge meticulously constructed to be even better than with usual engineering), then the parallel meaning to software engineering could be that overengineering a piece of software to execute extraordinarily well (high speed, low memory usage, etc.). In other words, a highly optimized piece of software. Optimization makes the source code incredibly complicated, so overengineered software (classic meaning) is much more complicated than it probably needs to be. I can imagine this notion of needless complication of the source code over time coloured the meaning of overengineering, so it became more associated with too much complication rather than building something exceptionally well.",hqeeedd,t1_hqc4vim,1640778031.0,False
rq60d4,Don’t agree with #5: fixation on data structure can lead to lack of flexibility. Programmers should use data abstraction instead. Also not data but functional specification should drive program design,hq9omtz,t3_rq60d4,1640693048.0,False
rq60d4,"Data driven design tends to be cleaner, simpler and easier to reason about, because you can avoid complicated data conversion in your data abstractions. This also saves you from potential correctness and performance issues in those data abstractions.

Flexibility isn't really an issue, because the code is much more straightforward when it has a conservative amount of abstractions. Therefore it's usually not as bad as you might think to change the data structure to something else. On the other hand heavily abstracted code tends to be so complicated that it's easier to just put yet another abstraction on top of it all, compounding the existing issues.",hqbjej0,t1_hq9omtz,1640722954.0,False
rq60d4,"«Data driven design ...» — does data determine functionality of program or vice versa functionality determines the data necessary for its implementation? What does user need? Functionality or internal program’s data structures?

«Flexibility isn't really an issue, ... when it has a conservative amount of abstractions» — correlates with my statement about using data abstraction",hqbvqdu,t1_hqbjej0,1640728016.0,False
rq60d4,"In data driven design data structure (i.e. data layout in memory) determines how you process the data. This has not anything to do with user needs or application features per se - it has to do with how one goes about implementing these features.

No, it does not correlate - it contradicts. You claim in response to rule #5 that the programmer should use data abstractions instead of fixating on data structures. When you as a rule abstract the data and disregard its actual structure, then it will unavoidably lead to unnecessary data abstractions. When you have more data abstractions than necessary, you do not have a conservative amount of data abstractions.",hqbytmu,t1_hqbvqdu,1640729281.0,False
rq60d4,"«In data driven design data structure (i.e. data layout in memory) determines how you process the data»
— I didn’t ask what “data driven” means, I ask how DDD can prevail since programs are usually written to satisfy some user needs, not to serve any data structure itself — and primary need of user is functionality but not pure data.

«No, it does not correlate - it contradicts» — no, it doesn’t contradicts because you said it yourself that data abstraction solves the issue of potential loss of flexibility.

«When you as a rule abstract the data and disregard its actual structure, then it will unavoidably lead to unnecessary data abstractions ... more data abstractions than necessary» — you cannot have more data abstractions then necessary if you design consequentially — because abstraction means eliminating inessential details for given step of design, and new abstractions appear only when you begin concretizing existing ones and realize that they require some new entities — which automatically makes those new entities necessary 🤷‍♂️",hqpk98s,t1_hqbytmu,1640974946.0,False
rq60d4,"In DDD, user needs determine the data structures. The data structures determine the algorithms. Example: if the user needs fast undo/redo functionality, then a doubly linked list might an appropriate data structure, and therefore the code is tailored to linked list functionality.

It absolutely contradicts, and I thoroughly explained how.

Regarding flexiblity, I said that the percieved inflexibility of DDD is overestimated. The reason is DDD code is generally speaking relatively easy to change.

I agree with what you said about using data abstractions when the need arises in an organic manner. This isn't against DDD. In fact, this is the *only* way you should make data abstractions in DDD - not making data abstractions as a rule of thumb, whether really needed or not.",hqt4agn,t1_hqpk98s,1641047762.0,False
rq60d4,"I suspect that was written within the context of ""thinking about performance"". Don't choose a particular data structure just because you want to use a certain algorithm. In general I agree with you, but the data ultimately does have some structure on a disk/in memory which matters.",hq9rlsf,t1_hq9omtz,1640695091.0,False
rq60d4,I agree with you too) but according to rule #2 we must to measure existing data before optimizing it and according to rule #4 we should prefer the simplest alg. and the simplest data for the first impl.) and abstraction is the easiest way of such simplification to my opinion,hq9t80e,t1_hq9rlsf,1640696134.0,False
rq60d4,"Also ""good enough is often better than perfect""",hqawyhl,t3_rq60d4,1640714040.0,False
rq60d4,"Good enough is never better than perfect. I get these sorts of rules are supposed to be pithy, and I get what they're trying to hint at, but they're always phrased in such asinine ways that it's hard to take them seriously.",hqbzw98,t1_hqawyhl,1640729730.0,False
rq60d4,"Reminds me The zen of Python.

Beautiful is better than ugly.

Explicit is better than implicit.

Simple is better than complex.

Complex is better than complicated.

Flat is better than nested.

Sparse is better than dense.

Readability counts.

Special cases aren't special enough to break the rules.

Although practicality beats purity.

Errors should never pass silently.

Unless explicitly silenced.

In the face of ambiguity, refuse the temptation to guess.

There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.

Now is better than never.
Although never is often better than *right* now.

If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.

Namespaces are one honking great idea -- let's do more of those!",hqb5jzu,t3_rq60d4,1640717439.0,False
rq60d4,How exactly am I supposed to write smart objects using only stupid code?,hqb8b06,t3_rq60d4,1640718522.0,False
rq60d4,"When you have picked appropriate data structures, then you code usually does not need to be particularly sophisticated.",hqc2f90,t1_hqb8b06,1640730784.0,False
rq60d4,"rule 6 

unexecuted code should not cause you to write more executed code. 

(class hierarchy/data structure and types should be fitted to your code and not the other way around)",hqnotxz,t3_rq60d4,1640936528.0,False
rpr7gj,"I would definitely recommend Algorithms in a Nutshell by George Heineman. The book gave me the most in-depth look into all algorithms and has a couple chapters focused solely on chess. 

If you want to check the book out before purchasing it you can do that here: https://archive.org/search.php?query=Algorithms%20in%20a%20nutshell",hq69ayn,t3_rpr7gj,1640628890.0,False
rpr7gj,"This isn’t a book, but I found the chessprogramming wiki to be very comprehensive!",hq6ns3f,t3_rpr7gj,1640634812.0,False
rpr7gj,https://www.chessprogramming.org/Main\_Page,hq7vvzr,t1_hq6ns3f,1640653591.0,False
rpr7gj,"The most important algorithm to know for these kinds of (two player turn based) games is the minimax algorithm: https://en.m.wikipedia.org/wiki/Minimax

There are lots of variants for optimizing performance but this is the starting point. Coming up with a good heuristic function to use is an important part, and can be approached lots of ways (ML, hand coding, etc)",hq7tayq,t3_rpr7gj,1640652427.0,False
rpr7gj,"**[Minimax](https://en.m.wikipedia.org/wiki/Minimax)** 
 
 >Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. When dealing with gains, it is referred to as ""maximin""—to maximize the minimum gain. Originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hq7tcod,t1_hq7tayq,1640652449.0,False
rpr7gj,"A good chess engine work using reinforcement learning algorithms. 

Depending on your knowledge you can read for example AlphaZero's paper  [here](https://www.science.org/doi/full/10.1126/science.aar6404) but its a hard read without knowing the prerequisites.",hq6hbfd,t3_rpr7gj,1640632144.0,False
rpr7gj,"Not a book as such, but Stockfish is an open source chess engine.  They have a page for people getting involved, including coding. https://stockfishchess.org/get-involved/

And they have a discussion forum. https://groups.google.com/g/fishcooking",hq761k5,t3_rpr7gj,1640642343.0,False
rpr7gj,"Stockfish wouldn't be a good example for a beginner as the engine employs machine learning and hardware acceleration if I recall correctly. Stockfish is an extremely advanced chess engine, hence why it has the highest *[ELO rating](https://en.wikipedia.org/wiki/Elo_rating_system)* amongst other engines. It would serve as a good example for a developer to work up to, but not start off with. You are essentially handing them the keys to a Ferrari when they don't even have their temps yet as well for throwing a box of parts at them shortly before telling them it's an airplane. OP does not want a box of parts, they want a blueprint to assimilate so that they can later on assemble their own airplane from their own box of parts. I cannot recommend this enough, do not refer beginners to major codebases. Doing so will cause them to spend more time trying to learn and navigate the codebase in lieu of the subject itself.",hq81tzc,t1_hq761k5,1640656262.0,False
rpr7gj,"Check this out: https://wiki.cs.pdx.edu//minichess/

This is from one of my profs at Portland State, and it's fantastic. It's a great intro into the algorithms you're interested in.",hq6yzwp,t3_rpr7gj,1640639423.0,False
rpr7gj,Toledo NanoChess by Oscar Toledo Gutierrez. I guess it's not entirely modern as Stockfish or Alpha.,hq78mml,t3_rpr7gj,1640643423.0,False
rpr7gj,This is handy  [https://www.chessprogramming.org/Main\_Page](https://www.chessprogramming.org/Main_Page),hq7vswd,t3_rpr7gj,1640653551.0,False
rpr7gj,"I know you asked for books (for which you should look at the other comments) but I would read on minimax with α-β pruning and Monte-Carlo Tree Search. The former powers DeepBlue, the latter powers AlphaZero (I think?)",hq8jbhr,t3_rpr7gj,1640664298.0,False
rpr7gj,"Norvig's very influential AI book, while not specifically about chess, uses plenty of example from chess. Worth studying in depth.",hq9a6mg,t3_rpr7gj,1640681470.0,False
rpr7gj,Me too! f,hq60rg0,t3_rpr7gj,1640625439.0,False
rp30oe,"If you're an academic then writing papers, attending conferences and reviewing papers will likely keep you up to date.

Apart from that, keeping up with the latest trends is not that important. The ""hot new things"" often grow out or favor in a few years. Sure, if you can spin your research as being tangentially related to this they may sound better, but apart from that new developments in some area often have no relevance on your particular area.",hq24exx,t3_rp30oe,1640547437.0,False
rp30oe,"Prediction is hard, especially if it's about the future.

X, Y, and Z will usually be the tip of the iceberg. Every day you will read/hear about dozens of things that are the latest and the greatest. It's hard to upfront know what is and what isn't going to pan out. One approach is to just pick one and work at it if it appeals to you.",hq24nax,t3_rp30oe,1640547539.0,False
rp30oe,[Hacker News](https://news.ycombinator.com/),hq2708e,t3_rp30oe,1640548565.0,False
rp30oe,"I think it’s easy to be dismissive of latest tech and chasing the new hotness. However, developing a sense of what is signal in the noise, what is worth using some time to examine and potentially adopt in your toolkit as it evolves is one of the greatest skills you can develop as an engineer. This is a industry always on the move and you can move with it or stand still. You know what happens to those engineers who refuse to evolve. 

To answer your question more specifically, company engineering blogs have become great resources. Find companies you admire and see what they are doing to solve their problems.",hq3k46b,t3_rp30oe,1640572088.0,False
rp30oe,"I have a few YouTube channels I go to to see what’s new in tech. Some cover big things like new releases of stuff like laptops and smart phones, some cover pen testing and new exploits, and others are just people far smarter than I’ll ever be giving me their opinion on different things. I like YT a lot better because it gives more information, so if I am interested in something I can refine my search better when I go to look it up. Ticktock and YT clips never give me enough info and I always have to google it after each thing I watch just to find out it’s not that interesting. As far as learning it. I usually like to take in as much as I can just so I know the name and a very broad understanding of what it is. Not enough to actually have a conversation about but enough that if I hear it again I can be like, yea that’s a new browser, or that’s a new block chain scam. However, if I find it interesting or if it frequently comes up in different discussions I will dig a bit deeper and learn more about it.",hq3p1pd,t3_rp30oe,1640574469.0,False
rp30oe,Quantamagazine. Nature. Try two minute papers on youtube for ML related things.,hq4os7t,t3_rp30oe,1640597530.0,False
rp30oe,"Theres several layers to it. You got the high level hypey stuff like crypto, nft, web3 stuff you can reach about on tech crunch or hacker news or wired. You then have the different language level libraries and design patterns and frameworks you can catch up on by googling ""best software engineering"" blogs. And then you have system design on big tech company websites.",hq2ew4f,t3_rp30oe,1640552740.0,False
rp30oe,Google News Feed app It'll figure out what tech you're interested in. Click on the appropriate links and over time you'll get a personalized tech news feed,hq4notr,t3_rp30oe,1640596607.0,False
rp30oe,I'm a software engineer and I honestly find out about the latest stuff when our apprentices say they want to use it.,hq52r9u,t3_rp30oe,1640608774.0,False
rp30oe,"It is NOT fast moving.

It takes years for a new language to mature to usability. The time from when I first heard of Java to usable Java was about five years. If you hopped on Swift the year Apple announced it you rewrote your app at least four or five times if it still works today. 

I would also argue we do not make much real progress, we just shift things around like hem lengths on skirts. We cast off perfectly good stuff for no good reason in favor of new half baked things that are not yet as good all the time.",hq5gz6w,t3_rp30oe,1640616873.0,False
rp30oe,"Forums and blogs and stuff, YouTube channels as well probably
And reading papers, but you find the papers on forums

As well as annual conferences if you have a lot of spare time 

Honestly if you’re involved in your specific field (ie frontend, embedded etc) and you’re in all the subreddits and what not, you should be able to stay more or less up to date",hq5vxal,t3_rp30oe,1640623459.0,False
rp1tzo,[cpprefererence.com](https://en.cppreference.com/w/cpp/algorithm/rotate) Usualy have a possible implementation (as in this case) and is genarally bettwr than cplusplus.com,hq1qubv,t3_rp1tzo,1640541704.0,False
rp1tzo,I find this implementation similarly perplexing but is at least commented,hq1t9kk,t1_hq1qubv,1640542760.0,True
rp1tzo,"First of all the element at middle has to go to first. Where does the element that was at first go? That's unclear, but we'll get there. Let's store it at middle for now. Then the element after middle has to go to the element after first. So let's swap those in the same way. We continue doing this swapping the elements first+k and middle+k. The correctness we can prove during this is that the element that is swapped into the first+k is now at the correct location.

This stops either when first+k becomes middle, or when middle+k becomes end.

\- In the first case, the elements that were swapped into the locations between middle and middle+k are not necessarily in the right location. They should be the last k elements, and all the elements after middle+k were supposed to shift left k times. So we're basically left with rotating the portion between middle and end, with middle+k as the new middle.

\- You can analyze the second case similarly. It's a bit messy to describe this case but it's easy to see what is needed to be done if you draw the list, note what got swapped, and how it needs to be.

You can verify that both cases are just instances of rotate, and that the code given has the correct first, middle and last in order to do the required rotate.",hq34xd8,t3_rp1tzo,1640564761.0,False
rp1tzo,"I have an intuition why it is correct, but this intuition works by understanding the algorithm as an recursive algorithm.

Here's the set up: we want to rotate the array leftwards, such that the element that used to be at the `middle` position is now at the start (i.e. position `first`)

Note that if `middle=first`, we are done.

Otherwise, we will now swap the first k elements with the k elements starting at `middle`. k is at least one, and is determined by `min(middle-first, last-middle)`. For example, let's say we have an array with numbers 0..9, and we want to pull 3 to the front:`0 1 2 3 4 5 6 7 8 9` becomes `3 4 5 0 1 2 6 7 8 9` after this initial swap step. `k=3` as `3=min(3,7) = min(3-0,10-3) = min(middle-first, last-middle)`. `first` points at position `3` (i.e. `middle`), `next` points at `6`.

Another example: Same array, but `middle=7`:

`0 1 2 3 4 5 6 7 8 9` becomes `7 8 9 3 4 5 6 0 1 2`, as `k=3` again, but this time it's because `last-middle=3`, which is the limiting term. `first` points at `3`, `next` at 10 (i.e. `last`)

Essentially, k is chosen such that at least one of the `if`s will become true after k iterations of the loop.

So, we have now done this step. Note that the first `k` elements are sorted, and `k>1`. Now, the magic is that the remaining array can be brought into correct shape by recursion.

In the first case, we have `0 1 2 6 7 8 9` remaining, which can be corrected by rotating such that the 6 is pulled to the front. In the other case, we must rotate `3 4 5 6 0 1 2` such that `0` is pulled to the front.

&#x200B;

The first case corresponds to `if (first==middle) middle=next;`, the last case to `if (next==last) next=middle;`. In the first case, our new `middle` will be `next` and we do recursion. In the latter case, `next` is reset to `middle`. Notice that in either case, the variables are now arranged as if we had done a recursive call, with `next==middle` holding.

As for run-time analysis, `first <= middle <= next <= last` always holds, and `first` continuously grows while `last` remains constant, thus we must eventually exit the loop, after at most `last-first` steps.",hq2vnqb,t3_rp1tzo,1640560492.0,False
rp1tzo,"bro this is great, really nice explanation thank you. this was an interesting read",hq3v5pt,t1_hq2vnqb,1640577506.0,False
rp1tzo,"1. rotate(a, a + k, a + n) = reverse(a, a + k) + reverse(a + k, a + n) + reverse(a, a + n)
2. Let's fix some index i0 and for i0 let's make a 1-shift for indexes i0, (i0 + k) % n, (i0 + 2k) % n, ..., i0. It's easy to do with O(1) memory. Let's do this action for i0 = 0..gcd(n, k)-1.",hq1wad1,t3_rp1tzo,1640544031.0,False
rp1tzo,"Sorry, didn't read the question",hq1whoy,t1_hq1wad1,1640544117.0,False
rop984,"If we're talking practically speaking, auth0 or some other provider is the most secure solution.

I'm not much help on the theoretical side of your question, though I haven't been at a company which separates application data from user authenticating data. Doing so seems to run along the lines of security through obfuscation rather than a sound security principle.

I Am Not A Security Professional, Just A Security Interested Developer.",hpzx3md,t3_rop984,1640497411.0,False
rop984,"> If we're talking practically speaking, auth0 or some other provider is the most secure solution.

auth0 has had critical vulnerabilities in the past.

The downside to using third party solutions is that they are often targeted by more actors, since any potential exploit can give you access to many more targets (e.g. the recent log4j CVE).

Of course, getting security right is very difficult, so you should probably not roll your own, but you also shouldn't trust others blindly.",hq0nkpg,t1_hpzx3md,1640520276.0,False
rop984,"Yea, that was my gut feeling too. I don't really know how databases get hacked (technique-wise), and my layman guess was that even if one db can get hacked, hacking the second doesn't become any easier.. but I should probably research more about vulnerabilities first, before building my mental models. Thank you!",hq04pom,t1_hpzx3md,1640503084.0,True
rop984,"DB's get hacked in a myriad of ways - from SQL injection to compromised credentials to unintentionally exposed databases.

I'd put something far simpler and more common like SQL injection or ensuring credentials are secure above a circuitous theoretical attack requiring separating user credentials from application data.

Your goal is _always_ to avoid being the lowest common denominator, because no system is invulnerable. Hackers are (generally) lazy and looking for a quick pay day. Don't be the easy target, avoid most (though not all) issues.",hq3j9g0,t1_hq04pom,1640571674.0,False
rop984,"[https://attack.mitre.org/](https://attack.mitre.org/)  


this may be a useful resource for you. I don't think it's exhaustive, but ""MITRE ATT&CK® is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations."" (first sentence on the landing page of the website) Going over the titles and skimming through some of the technique groups/individual techniques could help you build an idea of what you need to protect against",hq7b0no,t1_hq04pom,1640644442.0,False
rop984,"Infosec architect here. Rule 1 of application security, never ever roll your own security unless you absolutely have to. Rule 2 if you think you absolutely have to you are probably wrong. 


Some kind of OIDC system is probably best. but you're more or less right, it doesn't really matter if they're separate so long as you do good salted hashes",hq1l3pi,t3_rop984,1640539146.0,False
rop984,"Probably not, the real security advantages could come from how you treat the databases. Like can you not grant some applications access , or somehow make the databases more separated where if you have access to one you don’t to the other.

If practice I wouldn’t do this without other reasons why I want them separated. For example microservices and these are different teams.",hq056dm,t3_rop984,1640503465.0,False
rop984,"My personal thing is remember Murphy’s law when it comes to security. If anything can go wrong, it’ll go wrong. Apart from security, performance could be affected if both of them are the same. You could use a hybrid design where you store user data in some db that is fast at handling aggregate operations (if you need that) and a db which gives kinda O(1) lookup for username->hashed password. If you have a billion users, storing everything in one db would have performance effects. What if the user doesn’t be access to all user data, just needs to login and do something? There’s also the layer of scalability where you sometimes don’t need to scale the user data db while you need to scale user auth db.",hpzxzed,t3_rop984,1640498012.0,False
rop984,"Ok, I see. To clarify more, my (personal) project is basically recreating Google Sheets, and I can't say for sure, but my intuition is that their implementation is to have a unique db (or group of db's? distributed db's?) for each of read-only/read&comment/read&comment&write, so they can optimize each db for the use-case. 

I was curious if there is maybe a security reason (aside from performance) for separating a read-only db from a read&write db, but now that you mention it, I should probably care a lot more about performance.

I'm gonna search up resources on performance and databases in general now. So much to learn... but thank you!",hq040kg,t1_hpzxzed,1640502509.0,True
rop984,"Keep in mind the context you are looking at. Google has a single set of user auth/basic info and lots and lots of different applications. So it makes sense to separate the user's per application data. Makes it easy to add/remove applications over time. 

Security-wise, it likely doesn't matter (unless you handle the auth database very differently the other), but there are a good engineering reason to separate them.",hq0zxlz,t1_hq040kg,1640528863.0,False
rolb3t,"Trial division is O(sqrt(n)) exactly because smallest prime factor is <=sqrt(n). If you want to generate all primes up to n by trial division, it would be O(n\*sqrt(n)). There is of course Sieve method that can go all the way down to O(n).

Yours - idk. Trying to save all prime factors is at least O(nlogn) and that's already worse than Sieve.",hq03998,t3_rolb3t,1640501905.0,False
rolb3t,"I see, thank you!",hq0p11s,t1_hq03998,1640521491.0,True
rolb3t,"The r/math post has been removed, so I will copy and paste it here:

After watching a YouTube video on prime factorisations, I became interested in the patterns that appear when you factorise the natural numbers in order (from n = 2 onwards).

I think it's really interesting how these patterns appear. For example, when 2 is the base (which happens for every other n; as every other n is even, so has 2 as a factor), the powers follow this pattern:

1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5...

Edit: the powers really follow this pattern: 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, 4... (I'd forgotten the case of 2\^0) and the pattern above is just for powers > 0, but the following logic still holds.

This pattern appears to be such that you have a '1' at every (1-based) index other than a multiple of 2 (so 1, 3, 5, 7, 9... and so on). But then if you remove all the '1's from this list, you end up with a new list: 2, 3, 2, 4, 2, 3, 2, 5... Interestingly, this new list follows the same pattern, just now with '2's at every index other than a multiple of 2. This process carries on and on as you keep removing numbers.

What's even more interesting is that you see a similar pattern for higher bases. When 3 is the base (which happens every 3 n, again for the same reason as before), the powers follow this pattern:

1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 4...

This pattern now appears to be such that you have a '1' at every index other than a multiple of 3 (so 1, 2, 4, 5, 7, 8... and so on). But again, if you remove all the '1's from this list, you end up with a new list: 2, 2, 3, 2, 2, 3, 2, 2, 4... And again, this new list follows the same pattern, just now with '2's at every index other than a multiple of 3, and so on...

protocat-112ocat-112uestion is - what is going on here? Has anyone got an intuitive explanation for why these patterns appear?

To try and understand this a bit more (and since I am more computer scientist than mathematician) I looked up and found an algorithm for generating prime numbers. I then implemented it in C# (which I have called 'current algorithm'). I've added some comments with what I think is going on, but generally I understand it. The algorithm iteratively factors out every prime from the target number. Put this in a simple loop counting up from 2, and you can easily generate a table (or dictionary in this case) of all prime factors for n >= 2. What is the time complexity of this algorithm? I had a go at trying to figure it out and arrived at O(n\^3), but the while loop confused me somewhat...

What I wanted to do however was generate the prime factors in a different way. If I could figure out the pattern of the bases and powers, I could instead 'populate' each number with its factors looping though the factors (a base and a power) instead of the target number, starting with 2\^1, then 2\^2, 2\^3... then 3\^1, 3\^2, 3\^3... and so on. I came up with this algorithm (which I have called 'new algorithm'), which as pseudo code is as follows:

    dict = an empty dictionary with keys from 2 to SOME_LARGE_NUMBER
    FOR (base = 2 to base = SOME_LARGE_NUMBER):
      IF (dict[base] already has entries):
        // base is not prime
        SKIP to next base
      FOR (power = 1 to log_base(SOME_LARGE_NUMBER)):
        FOR (extender = 0 to base - 2):
          FOR (n = basepower + (extender * basepower) to SOME_LARGE_NUMBER),             
      INCREMENTING n by basepower + 1:
            dict[base].Add(base, power)

I managed to figure this out basically through trial and error. The core part of this algorithm is the final for loop. For base = 2, power = 1, extender is only 0 and this gives n = 2, 6, 10, 14, 18, 22.... For base = 3, power = 2, extender will be 0 then 1, giving n\_0 = 9, 36, 63... and n\_1 = 18, 45, 72 (which combined give us all the numbers which have 3\^2 as a factor). Again, my question here is why does this work? I am especially confused about my 'extender' code, which is, as I see it, a fudge to make it work. Also, what is the time complexity of my algorithm? I believe this runs in O(n\^3\*log(n)), but I may be wrong... I've only ever looked at the concept of time complexity for simple examples...

It's worth noting that the runtime of my algorithm seems to be much slower than the 'current' algorithm for finding prime factors when SOME\_LARGE\_NUMBER is indeed large (like 100,000). Also, I've not figured out how to optimise it for only looking for a single n, which I'm not even sure is possible, as the n is in the inner-most nested for loop, which is dependent on the outer loops.

However, it is considerably faster for a smaller value of SOME\_LARGE\_NUMBER, like 1000.

Both of these values may of course be affected by my specific implementation of the algorithms.

I'd be grateful if someone could explain or clarify any of the questions I've raised, or even just add any further info, discussion or relevant reading about this, as I'd love to understand more about what I am seeing.",hq0zltu,t3_rolb3t,1640528679.0,True
rolb3t,"I responded to your /r/math post but I don’t think you understood my comment. Try seeing what happens for n=10 because its the one where people are most familiar with the divisibility rule for n^k.

Start from 0 and increment by 1 in steps. Every 10 steps it’ll look like xxxx0, which is a multiple of 10. Every 100 steps it’ll look like xxx00, which is a multiple of 10^2. Every 1000 steps it’ll look like xx000 which is a multiple of 10^3, and so on.

Now, transfer over this logic to n-nary numbers, since in base n, a number of the form xxx0^k has a n-adic valuation of k.

All of this exactly explains all your observations. Your operation of “removing all the 1’s” and seeing the same pattern with every number incremented by 1 is just you recursing on a new instance of your observation where everything is scaled by a factor n, i.e. increment in steps of n instead and chop off the least significant digit of your n-nary number

Essentially, you are just witnessing “fizzbuzz” with the set {1, n, n^2 ,…} with words {0, 1, 2, …} and operation of “max” instead of the usual fizzbuzz on the set {3, 5} with the words {“fizz”, “buzz”} and operation “concatenation”.",hq1lqdj,t1_hq0zltu,1640539422.0,False
rnyhfs,"A processor can be thought of as a state machine. It’s registers constitute its state. It’s ALU and various other combinatory logic blocks make up its transition table between states.

Each clock cycle the values in the registers flow through the combinatory logic where operations are performed on them and they are transformed, ultimately ending up back at the registers. At the end of the clock cycle they are latched into the registers and the whole thing happens again the next clock cycle.

So if some signals in some section of the combinatory logic have not finished propagating through the circuit by the time data is latched into the various registers, then perhaps incorrect data will be latched into the registers and the processor will malfunction and behave in unexpected ways.",hpvkmz2,t3_rnyhfs,1640399692.0,False
rnyhfs,"I think this is the most sound answer. In the most basal sense, if data is not operated on in the order it's supposed to, or certain operations are ""missed"", you can end up with errors of varying severities in whatever you're doing.",hpwi7lo,t1_hpvkmz2,1640424799.0,False
rnyhfs,"In most cases, the output of the circuit will only be read as 0 or 1 by driver/software, and whatever that output is at the expected clock cycle will be treated as hardware response. The driver has to handle that hardware response. 

* Say the component is a network card, the driver needs to handle lack of response from the network. 
* Say the component is a HD, the driver will detect that your HD cannot provide an appropriate USB/SATA/IDE response, thus is not working.

Most devices don't have its own clock chip, so won't be operating at unexpected frequency.",hpvldtj,t3_rnyhfs,1640400168.0,False
rnyhfs,"Tons will go wrong, such as flip flop metastability. It is something that should not happen ever, this is actually so important that design tools check for that without ever asking. To get rid of that constraint, people either use several clock ""domains"" which are related to one another for sync, or sometimes asynchronous logic circuits.",hpwdln1,t3_rnyhfs,1640420422.0,False
rnyhfs,"Component? What component? Component of what?  
Job? What kind of job? Where it performs the task? What for?

A more specific scenario would be helpful for answering the question",hpvakjx,t3_rnyhfs,1640393383.0,False
rnyhfs,Its obvious to me he is asking about digital design inside a processor.,hpyr4h8,t1_hpvakjx,1640474112.0,False
rnyhfs,"To put it simply. It’s based on whether or not the component was designed to function in between clock-ticks.

An example of this would be an I/O function from your H/SDD to your processor/RAM. The transferring takes multiple cycles, and as such the CPU runs into an Interrupt Request (IRQ).

If the component was designed to work based on the tick of the clock. Generally the program will loop infinitely on that instruction, until it’s able to execute that command, or you/OS kill the program for hanging (as that’s what it would appear to you), if it’s a Kernel/OS memory space program, you’re getting a segment fault.",hpy5w6h,t3_rnyhfs,1640463340.0,False
rnyhfs,"Wow, engineers now a days just aren’t familiar with computer science. It’s absolutely ridiculous!!",hq0mo34,t3_rnyhfs,1640519481.0,False
rnyhfs,"In which context is this? What kind of component, software?

If you are talking about pre-emptive multi-tasking OS, then the running task is switched as the time slice is used up. State is saved until there is another slice of execution time for it.",hpwnhpq,t3_rnyhfs,1640429987.0,False
rnyhfs,Front end? Need to be a lot more specific,hpvl337,t3_rnyhfs,1640399979.0,False
rnyhfs,Real-time operating systems (RTOS') might be something to look at - they're specifically designed to do their job precisely on time,hpvne5n,t3_rnyhfs,1640401443.0,False
rnvod9,[deleted],hpusafy,t3_rnvod9,1640383439.0,False
rnvod9,"I see. 
I have one more similar question to ask but I need to send an image so would it be a problem if I dmed you?",hputyrc,t1_hpusafy,1640384281.0,True
rnvod9,Quite the opposite actually.,hpxhdt2,t3_rnvod9,1640450625.0,False
rnvod9,Nope. The M1 chip is.,hpxfubp,t3_rnvod9,1640449822.0,False
rnuf9s,"1 - Practical Discrete Mathematics: Discover math principles that fuel algorithms for computer science and machine learning with Python
- Author: Ryan T. White, Archana Tikayat Ray

2 - Fundamentals of discrete math for computer science: a problem-solving primer
- Author: Jenkyns, Tom A., Stephenson, Benjamin David

3 - Good Math: A Geek's Guide to the Beauty of Numbers, Logic, and Computation
- Author: Mark C. Chu-Carroll

4 - Sets, Logic and Maths for Computing
- Author: David Makinson

5 - Discrete mathematics for computer science: (a bit of) the math that computer scientists need to know
- Author: Liben-Nowell

6 - Math Prerequisites for Quantum Computing
- Author: R. Kumar",hpuy59w,t3_rnuf9s,1640386414.0,False
rnuf9s,I love [this one](https://www.amazon.com/-/es/K-Dewdney/dp/0805071660). The book cover a lot of important topics in computer science and does it in such a way that is in the middle of divulgation and technical reading. Have fun!,hpurqpz,t3_rnuf9s,1640383166.0,False
rnuf9s,The problem with a Turing Omnibus is you never tell if it’ll ever stop…. /s,hpv3itj,t1_hpurqpz,1640389303.0,False
rnuf9s,"If I had to recommend 10 then the list would be (in no particular order):

* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **The Art of Computer Programming** by _Donald Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **Code: The Hidden Language of Computer Hardware and Software** by _Charles Petzold_
* **Software Engineering** by _Sommerville_",hpumzcj,t3_rnuf9s,1640380787.0,False
rnuf9s,Humble Pi by Matt Parker is a great bedtime read. Can't guarantee that you would become a subject-matter expert after reading it but can surely guarantee that it would spark an interest in you for learning more. Enjoy reading!,hpwn2gp,t3_rnuf9s,1640429568.0,False
rnuf9s,"I think a lot of good ones were already given, but I'm missing **Elements of Computing Systems**

You get to build a computer and programming language from scratch using only NAND gates. The book provides various simulators and instructions to accomplish this. Gives so much insight into how stuff works and is a ton of fun.",hq0btb8,t3_rnuf9s,1640509450.0,False
rnuf9s,"1. Probability and Computing - Michael Mitzenmacher
This book discusses a normal Discrete Math class on discrete probability for the first two chapters and then discusses more advanced results later on. Besides the theory, some applications such as algorithms are also featured to aid in learning. 

2. The Cauchy-Schwarz Masterclass - J. Michael Steele
A must-read if you're into Theoretical Computer Science. I enjoy the ""conversational"" writing style of this book which I found to not be boring and dry. It discusses some interesting results on mathematical inequalities (which I find to be more difficult than equalities, personally).",hq1qzb9,t3_rnuf9s,1640541765.0,False
rnt040,You have to be way more specific than that. Cyber security of Operating Systems? Programming/development? Patching? Quality assurance? There's so many different aspects of operating systems that all have different research and careers.,hpu974r,t3_rnt040,1640374122.0,False
rnt040,"I mainly got interested when I learned how OS does memory management, techniques like paging, the virtual memory concept,maintaining the page table, the entire process of generating the logical address then converting finally into physical address to look for the appropriate frame in the main memory to fetch the desired data. 
Is there any scope of doing research in this memory management? I searched about it but couldn't find much info on it's research. I may not have searched in the appropriate place. So came here for some suggestions.",hpw7t4y,t1_hpu974r,1640415367.0,True
rnt040,"Sounds like you want/are looking for computational resource management with a mix of OS engineering. In terms of research, I'd recommend using sites like Jstor where you can not only look at research, but read/analyze empirical studies, dissertations, and peer reviewed articles on such topics. Also a great way to potentially make connections in the comp science realm. With that said, with the way quantum is approaching, I would heavily suggest also learning cyber security and defense of OS systems and OS architecture. Virtualization is a very popular topic right now as well, you'll be able to find an abundance of info on that.",hpw8h24,t1_hpw7t4y,1640415920.0,False
rnt040,You can check out papers from SOSP to get an idea of research going around OS!,hpvanln,t3_rnt040,1640393434.0,False
rnt040,"Here’s a great thread I found a while back, https://www.reddit.com/r/learnprogramming/comments/5v1c16/why_does_the_cover_of_the_operating_system/?utm_source=share&utm_medium=ios_app&utm_name=iossmf

It’s great to see other people interested in OS Design, it’s not very popular compared to other fields.",hpudp1z,t3_rnt040,1640376257.0,False
rnt040,"I don't know about research exactly, but the field is definitely still looking for changes. RTOS specifically is a very important topic and constantly improving. New OS designs like Zephyr trying to make certain protocol stacks like Bluetooth more sane. Or some out there stuff like Redox OS which ignores POSIX design and just goes for adding interesting stuff like ""everything is a URL"". Which sort of also borrows from Plan9's 9P which everyone interested in OS design should check out.",hpvq3j3,t3_rnt040,1640403139.0,False
rnt040,You’re in luck. OS is a whole field with research constantly going on in its many sub fields. It’s definitely one of the most interesting CS topics.,hpvpz7k,t3_rnt040,1640403064.0,False
rnt040,"I'm mainly interested in OS memory management topic. 
The various techniques that have been developed to handle critical memory segments. Techniques like paging, virtual memory, segmentation, relocation etc. 
I wish to study this area in more details than bachelor level, and contribute to it in future",hpw8090,t1_hpvpz7k,1640415534.0,True
rnt040,"I remember when I took the first real OS class I loved the memory and storage management stuff too. Just to let you know, there’s lots of other places where you can do the same stuff and embed that work. I’d check out some open source emulators or even just regular graphics programming API’s like OpenGL or Vulkan. In Vulkan there’s no default memory allocator so you actually have the option to write an implementation of malloc for a real application.",hpx4tmf,t1_hpw8090,1640443610.0,False
rnt040,"Operating Systems Design and Implementation https://www.amazon.com/dp/0131429388/ref=cm_sw_r_apan_glt_fabc_4TZMCZ9JD572DPY9C296

The bible. It's a great book, and very easy to get in to.",hpvv5c3,t3_rnt040,1640406334.0,False
rnt040,"Beep. Boop. I'm a robot.
Here's a copy of 

###[The Bible](https://snewd.com/ebooks/the-king-james-bible/)

Was I a good bot? | [info](https://www.reddit.com/user/Reddit-Book-Bot/) | [More Books](https://old.reddit.com/user/Reddit-Book-Bot/comments/i15x1d/full_list_of_books_and_commands/)",hpvv5yj,t1_hpvv5c3,1640406346.0,False
rnt040,No,hpwhc8j,t1_hpvv5yj,1640423946.0,False
rnt040,"Check out Genode. It's what my OS Prof suggested.

Also seL4, mathematically proven secure OS, if I understood correctly.",hpx2wlj,t3_rnt040,1640442404.0,False
rnt040,Yes.,hpv86rz,t3_rnt040,1640391962.0,False
rnt040,"I had a course on Operating Systems recently. It was pretty intense, the professor discussed many research developments in areas of Multi core scaled OS, Virtualization, Scheduling Algorithms (mainly, CFS), File systems, Virtual memory.",hpvw3y5,t3_rnt040,1640406957.0,False
rnt040,Is the course available online?,hpw874m,t1_hpvw3y5,1640415694.0,True
rnt040,"Sorry, it was an in-person class.",hpx5wn9,t1_hpw874m,1640444277.0,False
rnt040,Consider whonix,hpx9im1,t3_rnt040,1640446404.0,False
rnmo0w,"The role of assembly language is simply that it’s easier to read opcodes like ‘mov eax, 1234h’ than a sequence of hex bytes or an even longer sequence of ones and zeros",hpt4fc3,t3_rnmo0w,1640354577.0,False
rnmo0w,"And to add onto what u/jddddddddddd has said, this readability allows programmers to understand compiler optimizations. That is, being able to read assembly code allows programmers to see how the compiler has optimized the high level code written by high level language.

Not only that, it makes it easy for the programmer to make his/her own optimizations by writing assembly code.

You can think of assembly language as an abstraction to machine language because it's very very difficult and completely ineffective for programmers to understand compiler optimizations/read/write optimizations in machine code. Hence, we have assembly language to give an easier time for programmers.",hpt7jib,t3_rnmo0w,1640356271.0,False
rnmo0w,"It's perhaps better to explain through what machine code is.

Assuming an instruction on MIPS-architecture, that adds two values placed in registers 1 and 2 and places the result in register 6:  000000 00001 00010 00110 00000 100000

This is not very legible for humans so instead human would write assembly like: add $2 $1 $6

Here opcode for add is more legible and operands are clearly readable. Compilers could produce machine code directly, but the assembler to make machine code is usually separate program in the compiler toolchain and higher-level compiler feeds intermediate code (assembly) to the assembler. This means that higher-level code can be more independent of the actual target architecture.

Assemblers like GNU assembler can target many different architectures as well.",hptr42l,t3_rnmo0w,1640365722.0,False
rnmo0w,"I'd like to add that assembly language commands are 1 to 1 with machine code. That is, for every command in assembly language, the machine is executing one instruction. This is much different than high-level languages which require many operations per line (printing to the console in python, for instance).",hptsdef,t3_rnmo0w,1640366304.0,False
rnmo0w," At first computers were programmed by putting numbers on punched cards. Folks like John von Neumann programmed this way. It was made easier by the design of the instructions. This carried over to way later, even in microprocessors like the 8086 where it's convenient that the instructions can be split up into groups using octal instruction numbers. So patterns in the numbers themselves stick out like sore thumbs and it becomes easier to think directly about programs using the machine code.

A great abstraction came along with the advent of what are essentially assemblers and linkers. Then you could get away with thinking about things at a higher level and the tools would translate to the appropriate numbers for the processor. At this point CS was really off to the races.",hputwt8,t3_rnmo0w,1640384254.0,False
rnmo0w,I think reading an architecture book would help. For me personally trying to understand opcodes and how the memory worked without having the underlying architecture knowledge was just too challenging.,hptwg9n,t3_rnmo0w,1640368179.0,False
rnmo0w,"Having a solid understanding about assembly can help you with things like debugging and reverse engineering, or using disassemblers. Been many years but I've written some patches/cracks for old apps before.    
Idk about the communities anymore but I favored MASM32 last times I was writing code in it.",hpumih0,t3_rnmo0w,1640380554.0,False
rnmo0w,They are essentially one and the same. Assembly code is the human readable form of machine language.,hpvgkty,t3_rnmo0w,1640397114.0,False
rnmo0w,I had to use assembly in school to understand how the ram works and learn how things like stack and pointers are used,hpvj0hs,t3_rnmo0w,1640398662.0,False
rnmo0w,Take for example the assembly language Java. It reads lines of code and spews out 0s and 1s. Quite fascinating!!,hq0mtwc,t3_rnmo0w,1640519623.0,False
rne5t2,"There is quite a bit of misinformation about asymptotic notation in this thread demn.

 We say f(n)=O(g(n)) if there exist c, n_0 constant such that:

f(n) <= c * g(n) **for all n>n_0**.

Due to the algorithm wont stop for n > 100 we conclude that no such n_0 exist for every g. 

Its a really badly thought out question and it push misunderstanding of the math behind the bigO notation, as the runtime of your algorithm is not O(g(n)) for every g.",hptkf5o,t3_rne5t2,1640362628.0,False
rne5t2,"Yeah this is a poorly designed question imo. If n > 100, this will indeed run forever.",hprvlqt,t3_rne5t2,1640321781.0,False
rne5t2,Are you sure? Isn’t an int 32 bits? I.e. it will wrap around at 2^31 to -2^31,hps3iia,t1_hprvlqt,1640326547.0,False
rne5t2,"Problems of asymptotics are not constrained by the representation of numbers in a given language. If that were the case, almost any runtime problem would be O(1) since there would be a constant upper bound to what integer the function could accept.",hps7vho,t1_hps3iia,1640329605.0,False
rne5t2,"Huh, so I guess P = NP after all.",hpsh7x6,t1_hps7vho,1640337050.0,False
rne5t2,Any NP problem is in P if you put a hard upper bound on the size of the problem lol,hptvh11,t1_hpsh7x6,1640367735.0,False
rne5t2,Yeah that was the joke,hptvqfi,t1_hptvh11,1640367853.0,False
rne5t2,[deleted],hpuenzp,t1_hps7vho,1640376723.0,False
rne5t2,Depends on how you look at it. Typically we would say that addition in terms of bits it’s logn since as a number grows linearly the number of bits needed to represent it grows logarithmically. However in this type of problem it is normally assumed that addition is a constant operation.,hpuggc2,t1_hpuenzp,1640377583.0,False
rne5t2,[deleted],hpugvd5,t1_hpuggc2,1640377787.0,False
rne5t2,"This is starting to get pretty in the weeds but I suppose it illustrates that asymptotic analysis has nothing explicitly to do with physical computers, it only describes mathematical functions.",hpum8ux,t1_hpugvd5,1640380418.0,False
rne5t2,Great response to a good question.,hptnd33,t1_hps7vho,1640363988.0,False
rne5t2,If the algorithm depends on the bit length of a number then it is exponential in the input size. Just like Knapsack,hptfya6,t1_hps3iia,1640360516.0,False
rne5t2,so O(inf) should be acceptable right? I feel like they did not realize it could run forever,hprxsvx,t1_hprvlqt,1640323046.0,True
rne5t2,"In my opinion, yes. Especially if the prof didn't give any clarifications about the input domain either.",hprz03m,t1_hprxsvx,1640323749.0,False
rne5t2,"It is acceptable but expect to not be awarded marks if you didn't write your assumption. That is, O(inf) is possible when n > 100. Have a look at the analysis I did below.",hps2fyo,t1_hprxsvx,1640325864.0,False
rne5t2,"You always assume that n is going to infinity in these sorts of problems, there is no need to bound n",hps7xxw,t1_hps2fyo,1640329657.0,False
rne5t2,"You have to bound n because answers differ based on how n is bounded. There is no single answer since n can be bounded.

Without bounding n, there will just be a single answer and this is not very analytical.",hps8kv5,t1_hps7xxw,1640330140.0,False
rne5t2,"No, when asking for the runtime of a problem the bound is implicitly set as n going to infinity. That means that small inputs (in this case 100 for example) are not relevant to the runtime at all. Sure you could bound the input, but that defeats the purpose of this sort of analysis.",hps8unc,t1_hps8kv5,1640330340.0,False
rne5t2,"When analyzing an algorithm, you need to consider both the best case and the worst case. You don't just consider the worst case scenario.

And n = 100 gives you the best case and hence it is definitely relevant to the question.

When analyzing an algorithm, you need to be explicit. This is also tested in interviews. It simply makes you a better problem solver/analytical reasoner/Computer Scientist.",hps9hzd,t1_hps8unc,1640330833.0,False
rne5t2,"Sorry, but this is not quite correct, the best case of this algorithm is still infinite because you still need to analyze as n goes to infinity.

If determining the best case was as simple as picking a value for n, every function would have a best case of Theta(1).

Also this person asked for an O bound, they did not say anything about best or worst case.",hps9ql1,t1_hps9hzd,1640331017.0,False
rne5t2,"I think I get what you are saying but bounding the input helps you to analyze the algorithm deeper as I have shown with my analysis. 

As you can see, I have considered the input that goes to infinity but I have said when best case happens and when worst case happens; that is, when n is a particular value.",hpsb453,t1_hps9ql1,1640332081.0,False
rne5t2,"Please see my response below, but saying the best case happens when n is a particular value doesn't make sense. Asymptotics just doesn't quite work that way.",hpsbnp1,t1_hpsb453,1640332502.0,False
rne5t2,"Would you say the following is wrong?:

**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that n > 100. That is, n can take any number greater than 100.**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.",hpsasg2,t1_hps9ql1,1640331828.0,False
rne5t2,"In broad strokes no, in details yes.

The first two cases do not really make sense to analyze from an asymptotic perspective since they concern small values of n. Asymptotics is all about growth as n approaches infinity. And as I have said in my other responses, you really cannot select a value of n when it comes to asyptotics so the reasoning inside those two parts is misleading. I think you are focusing a lot of specific cases in your responses, but again, we are always talking about general cases here.",hpsbzj6,t1_hpsasg2,1640332763.0,False
rne5t2,"I see. I think I understand my confusion now.

All the algorithms I have analyzed needed an input size. As in, it wasn't a particular integer. It was something like n = len(array). And so it was intuitive to analyze the algorithm as n reaches infinity.

But when n is an integer (as in, not a size of anything), I immediately went to bound n.

So, you are saying that for any algorithm with any kind of input, we should always be looking at its run time as n reaches infinity right?

So, then for OP's question, the best and worst case is O(inf) when n reaches infinity?",hpsd0hz,t1_hpsbzj6,1640333582.0,False
rne5t2,"Yes, but as you can see there are a lot of subtleties on the way to that answer",hpsd70s,t1_hpsd0hz,1640333726.0,False
rne5t2,"Right, what do you mean by ""subtleties""?",hpsdezx,t1_hpsd70s,1640333907.0,False
rne5t2,I mean everything in we just talked over in this huge chain of comments,hpsdj4m,t1_hpsdezx,1640333999.0,False
rne5t2,"Sorry, still don't get what you mean. Can you clarify please.",hpse2ok,t1_hpsdj4m,1640334444.0,False
rne5t2,"The whole you can’t bound n thing. I think as humans we are inclined to want to give absolute values to things, but this sort of thing requires you to think in terms of growth",hpsea3m,t1_hpse2ok,1640334616.0,False
rne5t2,Right I get what you mean now.,hpsg2g2,t1_hpsea3m,1640336074.0,False
rne5t2,"But best case tend to usually happen when the input is a certain value isn't it?

As in, best case = O(1) when n = 100 or n = 99 for instance.

Another example is if we are looking at binary search. Say there is a list that goes from 0 to 100 and I want to check if 50 is in the list. (assume 50 is in the list).

And if I did binary search on that with 50 as my input, best case would be O(1) when mid = 50.

Worst case would be if the number can't be found in the list. That is O(log(n)) where n is the number of elements in the list.

""If determining the best case was as simply as picking a value for n, every function would have a best case of Theta(1)."" But this is why we consider best case and worst case.",hpsaedl,t1_hps9ql1,1640331525.0,False
rne5t2,"Not quite. Consider your binary search example. Yes the best case is Theta(1) which happens when when we get our number on the first try. However, in this case n is the length of the array we are searching, and it doesn't matter how big n gets, we can still find the number we want in constant time. In order words, in your example, you do not need to fix n to a specific value to get the best case. However, in the above example you would have to do so, therefore these are not the same type of problem. In general asymptotics is all about rate of growth, so fixing n to any one value makes no sense, we care only about how the runtime of our function changes as n grows.

Also I do not understand your response to my last line. Let me give you an example of what I mean by ""If determining the best case was as simply as picking a value for n, every function would have a best case of Theta(1)."" Consider scanning through an array. How long does that take in the best case? Well theta(n) of course, because the best and the worse case of scanning through an array are no different. However, if you take your approach and allow yourself to fix values of n, you could just say the best case of scanning an array is Theta(1), because the size of the array could be 1. Again, this defeats the point of asymptotics because now you are looking at one specific type of array rather than an array in the general case.

One last note: ""But best case tend to usually happen when the input is a certain value isn't it?"" Not really. I think it would be more accurate to say that ""best case tends to happen when something in the data causes a short circuit"", which again is what is happening your binary search example.",hpsbagy,t1_hpsaedl,1640332215.0,False
rne5t2,"Right, I totally get what you mean.  

Yes, we need to consider the runtime of the algorithm as input size increases or as input size goes to infinity. 

So, even if an algorithm is asking for a simple integer, we always need to consider the runtime of the algorithm as n goes to infinity? Because in OP's question, the algorithm is asking for an integer and not a size of anything. 

""best case tends to happen when something in the data causes a short circuit"". Right, I never thought of it this way. This makes more sense.",hpscoe3,t1_hpsbagy,1640333311.0,False
rne5t2,"Yes, n always has to go to infinity. For example

void func(int[] ar) {

int n = ar.length;

for (int i = 0; i < n; i++) {}

} 

Has a runtime of n in both best and worst case.

Something like:

void func2(int[] ar) {

int n = ar.length;

for (int i = 0; i < n; i++) {

if (ar[i] == 0) {
break;
}

}

}

Has a best case of Theta(1) since no matter how long the array is, we break if the first element is 0.",hpsd37o,t1_hpscoe3,1640333642.0,False
rne5t2,"Right. 

So, for your second example, I can say something like the following:

Best case = O(1) when first element in the array is 0. 

Is that right?

So, basically, when talking about best and worst cases, don't involve the input size. Check what makes the algorithm end early (if it does) and that's your best case. Is this correct?",hpsdc4b,t1_hpsd37o,1640333843.0,False
rne5t2,"Yes the best case is constant. However you are still taking input size into account, it is just the thing that breaks the algorithm early is not related to the input size, it's related to something about the data or some probabilistic element of the algorithm.",hpsdgdi,t1_hpsdc4b,1640333937.0,False
rne5t2,"""However you are still taking input size into account"". Can you kindly point me where?

I just said Best case = O(1) when first element in the array is 0. I am not talking about input size here?",hpsdxo1,t1_hpsdgdi,1640334328.0,False
rne5t2,"Maybe that phrasing is bad, we are still considering it to be infinite, even though that is not what we are basing the best case on",hpseep2,t1_hpsdxo1,1640334721.0,False
rne5t2,"Sorry I am still a little confused. 

So, is it wrong to say ""Best case = O(1) when first element in the array is 0""?",hpsg4ps,t1_hpseep2,1640336128.0,False
rne5t2,"No, that is correct",hpttny7,t1_hpsg4ps,1640366901.0,False
rne5t2,Thanks for the clarification.,hpuukgp,t1_hpttny7,1640384580.0,False
rne5t2,[deleted],hptcaw9,t1_hps9hzd,1640358732.0,False
rne5t2,"Yes, it is usually the worst case that is considered when talking about big O but for a complete analysis, we do also consider the best case. And not just that, the average case too. 

As in, it's completely correct to consider the best case as well because it gives a better overall analysis.",hpuux6u,t1_hptcaw9,1640384759.0,False
rne5t2,[deleted],hpuzpxn,t1_hpuux6u,1640387248.0,False
rne5t2,"Right but like I said, when talking about the Big O or when analyzing an algorithm in general, we don't just talk about the worst case. The best case is also considered for a complete analysis and that's what I am trying to say in my original comment.",hpv18d8,t1_hpuzpxn,1640388053.0,False
rne5t2,[deleted],hpv39f2,t1_hpv18d8,1640389160.0,False
rne5t2,"Yeah and I am agreeing the fact that Big O is about worst case but what I am saying is that the best case is also considered when talking about Big O too because it gives a complete analysis of the algorithm. That's what I have been saying in my original comment as well.

I am sure you have seen books and other resources mentioning about the best cases when talking about Big O (when analyzing an algorithm's runtime) and that's because it's important to consider it for a complete analysis.",hpv46ug,t1_hpv39f2,1640389677.0,False
rne5t2,"You don't see books and other resources saying, ""oh yeah, let's not talk about the best case because that's not what Big O is about"". Good books and other resources mention it because it's important for a complete analysis of an algorithm.

Again, I understand that Big O is about considering the worst case but when doing a complete analysis of the algorithm's runtime, you consider the best case as well and that's what I am trying to say. 

You clearly see this on CS books/other resources.

So, I don't get why you think that's wrong.",hpv537s,t1_hpv39f2,1640390179.0,False
rne5t2,"If you bound n and express the complexity in terms of n then trivially every terminating algorithm will have complexity O(1). It really doesn’t make sense to bound n, you just have to look at the definition of O notation to see that.

It makes sense to do case distinctions for algorithms if they behave differently for different inputs but that’s a different thing than straight up bounding the input size.

Also of course it can make sense to bound n and look at properties of your algorithm for bounded n but that’s not the same as asymptotic analysis.",hpsj54p,t1_hps8kv5,1640338688.0,False
rne5t2,"""It makes sense to do case distinctions for algorithms if they behave differently for different inputs but that’s a different thing than straight up bounding the input size."" Can you please give me an example of such an algorithm?

I think the confusion I am having is that OP's algorithm expects integers as input. And those are any integers as input. And depending on the integers given, you can have different big O time complexities. 

Hence, why I made restrictions to n in my analysis. 

So, if we are not supposed to restrict n, then that just means the best and worst case is O(inf) as n approaches infinity right?",hpso54s,t1_hpsj54p,1640342984.0,False
rne5t2,"OPs algorithm is actually one example where doing a case distinction is sensible to do because depending on what n is the runtime is O(n) or the algorithm does not terminate. Though tbh having a nonterminating algorithm as an example for asymptotic analysis is just straight up stupid by OPs prof. 

Fundamentally asymptotic analysis is about how an algorithm behaves as the input size goes to infinity/how it scales with the input size. For example if you have an algorithm in O(n^2 ) then you don't know how quickly it will run for n = 1000 but you can estimate that the time it will take for n = 2000 will be roughly 4 times the time it takes for n = 1000.

>So, if we are not supposed to restrict n, then that just means the best and worst case is O(inf) as n approaches infinity right?

You can make a case distinction for n but that's not the same as bounding n. If you bound n then runtime will be O(1) for any terminating algorithm. However a case distinction without bounding n can give you some better insights.

Consider for example this contrived algorithm:

    f(n):
    if(n = 1): return 1
    else if(n is a power of 2): return 2 * f(n / 2)
    else if(n - 1 is a power of 2): return 2 * f(n - 2)
    else return 2 * f(n - 1)

The asymptotic complexity of this function is O(n). However we can make a restricted analysis and say that we only consider n = power of 2 as input. Then the asymptotic complexity of this function for this restriction is O(logn). However that is not the same as bounding n. If we bound n by for example n <= 10^100 then the algorithm's runtime is O(1).",hpsreuw,t1_hpso54s,1640345719.0,False
rne5t2,"""OPs algorithm is actually one example where doing a case distinction is sensible to do because depending on what n is the runtime is O(n) or the algorithm does not terminate.""

But this is exactly what I have been trying to say. That is, depending on how we restrict n, there will be different big O time complexities. Or am I not understanding something?

Ok, I think I get what you mean with the example you have given.",hpszo7y,t1_hpsreuw,1640351769.0,False
rne5t2,">But this is exactly what I have been trying to say. That is, depending on how we restrict n, there will be different big O time complexities. Or am I not understanding something?

I think this whole discussion boils down to the distinction between bounding and doing a case distinction.

OPs example is really not a good one for this distinction since there the necessary case distinction actually corresponds to bounding n. When I replied to your comment I was more talking about asymptotic analysis in general rather than OPs example.

A case distinction make bounds but in practice that is rarely useful. What I'm mainly referring to is case distinctions like these (considering an input n): n is a prime number; n is a power of 2; n is even or things like that. In all of these cases n is restricted but can still grow arbitrarily large. And n being able to grow arbitrarily large is the important thing since otherwise asymptotic analysis doesn't make much sense.

So bounding can be seen as a form of a case distinction, but that's rarely ever the kind of case distinction we want to make for asymptotic analysis (except for completely stupid examples like the algorithm OP posted).",hpt1a2o,t1_hpszo7y,1640352748.0,False
rne5t2,"""What I'm mainly referring to is case distinctions like these (considering an input n): n is a prime number; n is a power of 2; n is even or things like that."" Right, so, like certain properties of n.

""So bounding can be seen as a form of a case distinction, but that's rarely ever the kind of case distinction we want to make for asymptotic analysis"".

Yeah, so, basically with any asymptotic analysis with algorithms, you will not bound the input because we care about how fast the algorithm is as input approaches infinity.

Thanks for the clarifications.

Edit: so, what's a case distinction for OP's algorithm?",hpt5e5o,t1_hpt1a2o,1640355114.0,False
rne5t2,"**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

**So, is the above a case distinction?** 

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**So, is the above a case distinction?** 

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

**So, is the above a case distinction?** 

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**So, is the above a case distinction?** 

**Let's assume that n > 100. That is, n can take any number greater than 100.**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.

**So, is the above a case distinction?**",hpszwqw,t1_hpsreuw,1640351917.0,False
rne5t2,"No that's wrong. Look, this is just an upside down recursive function. Instead of the base case being at the bottom of the number line, it's above. Any input <= 100 has a defined value.

Well alright. So we have something like a Fibonacci sequence, except it's just one recursive call instead of two. So the calls are one to one, so therefore there is a LINEAR relationship between the value of the argument and the number of calls.",hps6l5e,t1_hprxsvx,1640328655.0,False
rne5t2,"Generally yes. Technically speaking you can't say O(inf) because it does not make sense to apply an O bound to inf, but the runtime of this function (both best and worst case as well) is infinite.",hps8c4d,t1_hprxsvx,1640329958.0,False
rne5t2,"When analyzing an algorithm, you need to consider both the best case and the worst case. You don't just consider the worst case scenario.

And n = 100 gives you the best case and hence it is definitely relevant to the question.

When analyzing an algorithm, you need to be explicit. This is also tested in interviews. It simply makes you a better problem solver/analytical reasoner/Computer Scientist.",hpsnvpt,t1_hprxsvx,1640342763.0,False
rne5t2,"My first thought was O(n) since it would just count up to 100, but then I noticed that n is a parameter that could start > 100 so you're right that it could be non-terminating. 

Did the test ask specifically for recursiveFunction(1) or some other constantv argument? Or is there a wrapping function that starts it with a specific number?",hprvnlg,t3_rne5t2,1640321809.0,False
rne5t2,"the only instruction we got was the following, 

For the given algorithms, specify what is the time complexity of the algorithms using big-O notation. You do not need to write your computations. \[8 points\]

&#x200B;

and this was one of the parts, what I typed in the description was exactly what we got, nothing else

&#x200B;

I wrote O(n) initially then realized it was infinite, I never encountered this problem so I thought O(inf) might be it. any more thoughts? I lost quite a bit there",hprwihl,t1_hprvnlg,1640322295.0,True
rne5t2,Did you ask the professor? Maybe they just had a mistake in the test.,hprwwuz,t1_hprwihl,1640322526.0,False
rne5t2,"Professor is stubborn. No reply to emails. You can only send a remark request by tonight and say why  the TA made a mistake in your marking. In fact the prof does not even want an explanation just which part you want remarked 

I think I will just send in a remark even if it may annoy the prof",hprx4z7,t1_hprwwuz,1640322656.0,True
rne5t2,I would do it. His question doesn’t have a well-defined answer unless you specify the O function by breaking it out over the domain of n.,hpu1w8a,t1_hprx4z7,1640370697.0,False
rne5t2,"It would be O(n) under a certain assumption though. It wouldn't be O(n) for every case. If you are interested, you can have a look at the analysis I did below.",hps42hm,t1_hprvnlg,1640326911.0,False
rne5t2,"I think while the person authored the problem did not cover input restrictions, the intention of the question is most obviously to ask the time complexity on inputs that halt, which is O(n). 

Your answer is technically correct, but the professor is trying to test your understanding on recursion, so O(infty) looks like a gotcha answer.",hps4s09,t3_rne5t2,1640327390.0,False
rne5t2,"It's O(n) for when it finishes.

There is no such thing as O(infinity)",hptgpvv,t3_rne5t2,1640360885.0,False
rne5t2,"**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that n > 100. That is, n can take any number greater than 100.**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.

Edit: Edited my assumptions.",hprxku7,t3_rne5t2,1640322913.0,False
rne5t2,"fwiw, you don't need to restrict your analysis of the O(n) scenario to when n >= -100. Your logic holds if n can take on *any* negative value.",hprz79k,t1_hprxku7,1640323863.0,False
rne5t2,"Yes, you are completely correct. I was focusing too much on the worst case scenario; that is when n = -100 or when n = -99 and hence why I had that restriction. 

But yes, n can take on any negative value. Thanks for pointing it out. 

I am going to edit the analysis.",hprzl1n,t1_hprz79k,1640324086.0,False
rne5t2,[deleted],hprzx32,t1_hprz79k,1640324286.0,False
rne5t2,"If n > 100, it'll run forever.",hps07mf,t1_hprzx32,1640324469.0,False
rne5t2,"Oh crap, I was right before. lol. Need to edit again.",hps1c9e,t1_hps07mf,1640325163.0,False
rne5t2,"Thanks for the detailed write up. hmm, so you think even though the worst case scenario is O(inf), we should ignore this and put O(n)?",hprya91,t1_hprxku7,1640323329.0,True
rne5t2,"No, your answer of O(inf) is correct under that assumption only. So, when you are writing your answer, it is very important to say what you are assuming.

So, if you wrote O(inf) and say this is the Big O under that assumption, then you deserve some marks because you were being analytical about the problem. It shows you understand the problem.

However, if you did forget to write your assumption, usually Profs won't reward you marks. That's usually the case.

As I have said above, I am presuming that your Prof has achieved O(n) under a certain assumption. That is when -inf < n <= 100. O(n) is possible when n = -100 or when n = -99.

Edit: Did Prof provide any reasoning behind his answer? You should definitely ask for the reasoning.",hpryq9i,t1_hprya91,1640323589.0,False
rne5t2,"To build on /u/Classymuch's answer, the reason why collegiate educators are rather stubborn about these things is because in both research and industry you're expected to read between the lines to understand what a (broken) program is doing. So they tend to purposely give you vague questions to see how you handle ambiguity, especially under timed conditions to see how comfortable or prepared you are with the aforementioned task. (Asking questions during exams is always an option!)

For instance, anyone who has PR'd someone else's code before will immediately notice the recursive call's +2 increment and the coincidental 2 base case conditions associated with the aforementioned increment for the constraint n <= 100, and perhaps take steps to repair the code for all integers instead of just n <= 100 with O(|n|). The initiative to take apart and repair a program is what separates a weak programmer from an intermediate one.

To defend my point, job interviews will occasionally ask you to debug or identify a program, and from that discussion, they can see how experienced you are with programming while also seeing what clever tricks you can come up with to simplify or refactor the code. Also, Amazon's leadership principles include ""Dealing with Ambiguity"" as a virtue, and for a interview with Amazon you'll need to prepare past experiences to implicitly demonstrate your ability to deal with ambiguity on problems far more complicated than this problem.

Hope this helps open your mind to these types of problems in future exams! Time permitting, never underestimate the complexity of a problem shrouded by ambiguity.",hps0bnf,t1_hprya91,1640324537.0,False
rne5t2,"I just finished my first year in CS and the University I go to has a strong focus/emphasis on algorithmic analysis and they are always about giving vague questions.

This obviously makes internal/external Uni assessments challenging but at the end of the day, it teaches you to be great Computer Scientists/problem solvers in general and great to see that it helps with interviews too.",hps19s6,t1_hps0bnf,1640325121.0,False
rne5t2,"Excellent, keep pushing forward!",hps1lkd,t1_hps19s6,1640325327.0,False
rne5t2,Ask your Prof for the reasoning of O(n) because it's very important to know how he achieved O(n).,hprzav4,t1_hprya91,1640323921.0,False
rne5t2,"OP, have a look at my analysis again. I had to correct my assumptions. Now they are correct.",hps06be,t1_hprya91,1640324446.0,False
rne5t2,"Hey OP, have a look at my analysis for the second time. Sorry, had to edit my assumptions again. It's correct now.",hps1os1,t1_hprya91,1640325385.0,False
rne5t2,"Nope, it's O(n) because the worst case is it will terminate with stack overflow (not using tail recursion), and integers are able to address the entire memory space to the largest integer is proportional to the size of memory available to the program. Therefore time complexity is O(n).",hpujej2,t3_rne5t2,1640379016.0,False
rne5t2,"Well for numbers <= 100 it will be finite if you don’t blow up the stack. Above that it just go on forever, but below that you will have no more than 100 passes.",hpv596o,t3_rne5t2,1640390272.0,False
rne5t2,[deleted],hprvfwx,t3_rne5t2,1640321690.0,False
rne5t2,"Thank you for your thoughts,

Im not really just arguing I’m trying to understand why the solution is like this. We are allowed to ask for a remark because thé TA is the one who marked this not the prof, the remark is done by the prof.

Instead of just gojng in full guns blazing I thought I’d ask here first to not waste the prof’s time.

Do you think my answer is acceptable?

Based on if I’m right my letter grade changes dramatically too as I’m on the borderline between a A and B, so I thought it’s worth asking",hprwsz9,t1_hprvfwx,1640322465.0,True
rne5t2,"""I’m on the borderline between a A and B"" I swing my vote then, this is indeed a time to argue!

I extended my answer a bit so go ahead and reread for my perspective.

Overall I think its a really terrible question but i dont think he is wrong to classify it as N.

Unfortunately most tasks don't grow at some smooth rate, and indeed most algorithms have inputs for which they fail.

IMHO this task is likely one where you were expected to apply certain static analysis type reasoning, I.E. the function does or doesn't have some property (such as loops or in this case recursive bifurcations)

I generally consider school a waste of time so my opinion is likely tainted but i do think this question proves little about your knowledge and certainly teaches you even less.

I do not think you will win this one with him, better to look elsewhere for your last mark or two, best luck and keep in mind that a B is a damn good mark, and that your boss will never look at the grades you got in school, he just wants know you can actually do the task he needs done.

Best luck! have fun!",hprxu3w,t1_hprwsz9,1640323066.0,False
rne5t2,[deleted],hprx0ti,t1_hprvfwx,1640322589.0,False
rne5t2,"This is the first time I got a question like this, I think leaving the uni is a bit dramatic but thanks for the suggestion. Like you said sometimes you just get 'bad bosses'.

Yes I agree it is a poorly designed question and maybe whoever made it did not realize that it can run forever. In this case however, how do you even represent that answer? you said O(inf) is a bad answer, but in this case isn't it the only possible answer?",hpry5wo,t1_hprx0ti,1640323258.0,True
rne5t2,"Yeah bad bosses are best dealt with by finding a good boss.

Infinity isn't really in the domain of the big-O notation.

As i say big O is about the growth of cost rather than the exact value of it.

Given the basic structure of the program (no loops or bifurcation) it is O(n)",hps7li8,t1_hpry5wo,1640329401.0,False
rne5t2,"O(inf) is not a bad answer by the way. It's an answer that's definitely correct under the correct assumption. That is when n > 100. Have a look at the analysis I did above. 

I hope it helps you.",hps0jzw,t1_hpry5wo,1640324679.0,False
rne5t2,"For n>100 you'll get most likely a stack overflow. I'd consider that termination, too.
This, in general, O(n).",hps9u2w,t3_rne5t2,1640331094.0,False
rne5t2,O notation is math. Adding arbitrary hardware restrictions is besides the point.,hpth9d5,t1_hps9u2w,1640361137.0,False
rne5t2,[deleted],hps8jp9,t3_rne5t2,1640330116.0,False
rne5t2,With big O you usually don't take things like stack size into account. Especially because with this function the compiler usually removes the recursion,hpsjs93,t1_hps8jp9,1640339238.0,False
rnc33s,"If you have enough money for mining rigs, you can take over all bitcoins with a 51% attack. Nowadays, most of the global mining hashrate is done by a few individuals. If they work together, they could take over the network.
Bitcoin was never meant to be a productive system. Satoshi mentioned that in their paper.
Also, it's not democratic. Only a handful of people own most of all bitcoins.",hpsaenv,t3_rnc33s,1640331532.0,False
rnc33s,"The number of practical use cases for blockchains is somewhat limited. A lot of blockchain applications in industry are created by people (encouraged by VCs) who see blockchain as a hammer, and all problems as a nail. Instead of looking for innovative ways to solve problems, they look for innovative ways to use blockchain. In lots of these cases, the problem could be solved much cheaper and simpler by using something other than a blockchain, like a distributed database. See [this](https://eprint.iacr.org/2017/375.pdf) for some limitations (and some benefits) of blockchains.  

Blockchains aren’t even that great at solving the problem they were invented for: currency transfer. The bitcoin network can only handle around 7 transactions per second, while networks like Visa and Mastercard can process tens of thousands per second, all while using orders of magnitude less energy than bitcoin. 

And the energy use of cryptocurrencies can’t be overlooked either: the bitcoin network uses the energy of a mid-sized country, most of which comes from non-renewables. Lots of blockchain fans will say that this can be fixed with proof-of-stake or other consensus mechanisms, but this hasn’t really happened yet (and I’m skeptical that it ever will for major blockchains). Proof-of-stake is also a pretty bad consensus method if your goal is decentralization, as it just rewards the richest, accelerating inequality in your currency. 

Cryptocurrencies also offer little in the way of security compared to traditional banking/credit card systems. If someone steals my credit card and goes on a shopping spree, I can report it to my bank and they’ll close the card and issue charge backs. On the other hand, if someone steals my wallet key and transfers all my cryptocurrency to their account, absolutely nothing can be done to get the money back. It’s gone for good. There are far too many ways for key storage to go wrong for the average person to risk storing a significant amount of money in cryptocurrency. 

Overall, for many applications, there are just better ways to do whatever the blockchain is trying to do. Blockchains are interesting technology, and they can be useful in certain instances, but they’re not broadly useful enough for a “decentralized blockchain led future.”",hps6ghz,t3_rnc33s,1640328564.0,False
rnc33s,"Thanks for this! A lot of interesting points you made. I will definitely check out that paper you cited.

Two things I want to push back on slightly""

1) Is it true that fixing the energy consumption problem with PoS hasn't really happened yet? I know Bitcoin won't ever change, and that Ethereum is taking a really long time to change, but basically every other new and promising ""3rd generation"" blockchain is (Cardano, Algorand, Solana, for ex). I know these chains have their own issues that may stem from the inherent downsides of PoS, but I did want to bring up that they are far more energy efficient than BYC and the current implementation of ETH. 

2) Addressing your point about PoS rewarding the richest. I have considered this and it makes sense on a theoretical level, but for example Cardano has introduced some interesting ways to push back on this (for example, the k parameter which caps the amount of interest a stake pool can earn, which incentivizes decentralization of staked ADA. Have you looked into this at all/have any thoughts? 

Finally, there is a cool game theoretic feature to PoS where in order to attack the network, you would need to control >50% of the networks tokens, but then you are just destroying value of an economy of which you control the majority of. This is in contrast to Bitcoin where the ""resource"" used to assure trust comes from outside the system and thus you don't need to own any Bitcoin to tank the network. 

&#x200B;

Thanks for your reply. Cheers!",hq5lg8y,t1_hps6ghz,1640618947.0,True
rnc33s,"It’s true that there are lots of new PoS blockchains, but separate blockchains can’t fix the energy consumption of the big PoW blockchains like BTC or ETH (1.0). PoW blockchains have to be deprecated or drastically shrink. Etherium may be able to do this, but shrinking bitcoin would require a sea change in the blockchain industry, as my understanding is that almost all cryptocurrencies today rise and fall on average with the price of bitcoin. 

>	for example Cardano has introduced some interesting ways to push back on this (for example, the k parameter which caps the amount of interest a stake pool can earn, which incentivizes decentralization of staked ADA.

I’m not familiar with Cardano and hadn’t heard of this, thanks for mentioning it. Do you know what would prevent someone with a large stake pool from splitting it into smaller pools to circumvent the limitations put on large pools?

My overall criticism of these types of features designed to prevent centralization is that if they work as intended, enormous miners/stakers (the kind that prop up most blockchains) would be incentivized to not use that blockchain and move their resources to a more centralization-friendly blockchain that rewards them more.",hq72n1f,t1_hq5lg8y,1640640884.0,False
rnc33s,"> Do you know what would prevent someone with a large stake pool from splitting it into smaller pools to circumvent the limitations put on large pools?

So I think you're right in that there is no inherent mechanism to prevent that. I think what the k parameter is meant to do is to incentive people who delegate their ADA to a stake pool to choose another one instead of the stake pool that is aggregating all of the staked resources.",hq758gu,t1_hq72n1f,1640642003.0,True
rnc33s,Terrible for the environment,hpsc3l6,t3_rnc33s,1640332852.0,False
rnc33s,"I’ll expand on this…

Blockchain uses “Proof of Work”, in which someone needs to do work just to prove that they’ve done it.  In particular, the way it works in blockchain is that multiple parties race to see who can complete the proof of work first.  So there ends up being an incentive (usually financial, as in cryptocurrencies) to do more work than others.

Work requires energy.  Not only is that a basic law of physics, but computational work specifically requires electricity.  The production and distribution of that electricity has environmental impacts, primarily (though not exclusively) in the form of carbon emissions, which contribute to climate change.

But is the amount of electricity consumed actually significant in the grand scheme of things?  Well, the bitcoin system itself uses about 121 terawatt-hours (TWh) per year.  That’s about the same as the entire country of Argentina, more than the Netherlands or UAE, and nearly as much as Norway.  For a sense of scale, researchers at Cambridge estimated that about 3-6 million distinct users used bitcoin in 2017.  Norway has a population of a bit over 5 million, so to a very rough approximation, each bitcoin user is consuming as much energy just to use bitcoin as each Norwegian is consuming for their entire lifestyle.  (Caveats:  The other countries are more populace, to varying degrees, so the same cannot be said for them.  Also, it is the bitcoin miners, not every user, who are consuming the energy, so I’m sort of amortizing that expense across all users.)  Or to look at it another way, the energy required to implement a single bitcoin transaction is enough to power an average U.S. household for 24 days.  (And that’s not to mine a whole bitcoin; that’s just for one transaction.)  Compared to an alternative financial system, you can perform 750,000 Visa card swipes for the energy of just one bitcoin transaction.  And all of this is only for bitcoin, not the sum total of all blockchain systems.  So yes, the energy consumption is very significant.

And all of this is fundamental to the concept of proof-of-work.  So it’s not just bitcoin, but all blockchain systems that use proof-of-work will have this problem.  And again, none of this is productive work; it’s just work for the sake of proving that you have done the work.

All of that is just the energy cost, though.  As people build large compute clusters and hardware accelerators (or just use graphics cards), etc., there is a lot of electronic hardware being produced only for the sake of performing this proof-of-work (e.g., mining bitcoins), and there is a significant environmental impact to mining all the minerals required to produce those electronic components.  (Not to mention the social and economic impacts of the way they are mined and manufactured..)",hpycrk0,t1_hpsc3l6,1640466894.0,False
rnc33s,This,hpsewtk,t1_hpsc3l6,1640335131.0,False
rnc33s,"Internet connection is required for any verification of transactions (unless you want to download the entire ledger). This doesn't sound like a problem in 2021, but there are still very many cases where internet connectivity is not available.",hpsh3ni,t3_rnc33s,1640336948.0,False
rnc33s,"1. Anything done by a decentralized system can be done more cost-effectively by a centralized system. This is because a naively decentralized system will require n² connections between its agents while a naively centralized system (a star basically) will have n connections between its agents. This still hold true for more optimized systems (mesh networks vs hierarchies). I'm confident that there is a rigorous demonstration of this, I'll let you look for it.
2. It is true that centralized systems require more trust than decentralized systems, but it would be wrong to think that decentralized systems don't require trust at all. At the very least you have to trust the developers that their code actually implements a fair and robust consensus algorithm. Also, the control of the system's future is concentrated in the hands of those developers.
3. Any real life application of a decentralized system will require as much real life trust as its counterpart centralized system, because you still need to trust the real life agent to deliver on its real life guarantee.

What follows is that the only use cases where decentralized systems may have an edge are those where non physical assets are exchanged between agents that don't trust one another.

For example, Ubisoft recent utilization of NFTs for in-game assets is dumb as hell because it could be easily centralized in Ubisoft's servers. Going decentralized doesn't remove the need for trust because you still have to trust Ubisoft to actually give meaning to those NFTs in-game.

The same thing will apply for any kind of corporation controlled metaverse, even if the assets are in a blockchain, you still have to trust the corporation servers for rendering those assets in the way that was promised when you bought that asset.",hpt3hj6,t3_rnc33s,1640354042.0,False
rnc33s,"There's some good applications for it, Things like decentralized patent systems or smart contracts etc. But i doubt any of this ""web3"" stuff is going to happen. Some things just don't benefit from decentralization",hptxgko,t3_rnc33s,1640368639.0,False
rnc33s,Define “future”. I’d wager that the vast majority of people in today’s world have no idea what the blockchain is or how it works. The knowledge hurdles that collective society would have to overcome to adopt public blockchain tech into the  real world would take a long time.,hprix97,t3_rnc33s,1640315282.0,False
rnc33s,The vast majority of people in todays world also don’t understand how the internet works. The vast majority of people don’t need to understand something to adopt it.,hps0c1r,t1_hprix97,1640324544.0,False
rnc33s,"Block chain is a solution to a problem nobody has.

Essentially the problem that block chain solves is ""How can we have secure and distributed records that everybody agrees on?""

And all three of those are already solved non issues. Security? We have RSA and key exchange webs. Distributed? The internet. And that actually scales. Universal agreement? Bruh, what's even the point? What's the point of agreeing on what a bunch of ones and zeros are if people can be free to interpret the meaning or weight of them? Consensus really only comes through power, and power is really only achieved by governments.

Bitcoin was a once in a lifetime exception. Everybody wanted an alternate currency and it was first, but it's essentially a ponzi scheme where the only people who benefit are the first movers.",hpsbz92,t3_rnc33s,1640332757.0,False
rnc33s,Lack of central control,hprgwt6,t3_rnc33s,1640314299.0,False
rnc33s,"In the world we live in, it is really difficult to have a decentralization-led future. We can't decentralize everything at least in the near future. I am not a critic of the power consumption of mining cryptocurrencies because the Proof of work consensus algorithm can be replaced with more efficient and decentralized ones in the future(not Proof of stake).

Blockchains solve a problem that doesn't really exist. People are criticizing the big tech companies for using their data to earn money. Most of the people with this critic are members of the herd. Meta and Google are using utilizing your data to sell you ads to make money, and I don't get it what is wrong in that. They have to make money so that their service keeps on functioning.

We need a centralized authority at some point or the other. Suppose you are buying real estate from a person in exchange for ETH. You send the other person 10k ETH on the Ethereum network and then the person denies to transfer the real estate ownership to you. What will you do? You will have to go to law enforcement, which is a centralized authority.

You can only have decentralized in the world of blockchains(that too partially). The whole system breaks when tokens of the blockchain world have to be used to do something in the real world. People are all hyped around decentralization. Decentralization has more demerits than merits.",hpsnmej,t3_rnc33s,1640342539.0,False
rnc33s,"Ever heard what an escrow is?

Too many newbies how want to look intelligent in muh BloCkChAiN",hq9vqsd,t1_hpsnmej,1640697667.0,False
rmtdj4,As a suplement I strongly recommend this post: https://www.muppetlabs.com/~breadbox/software/tiny/teensy.html,hppf77p,t3_rmtdj4,1640281076.0,False
rmtdj4,That was an incredible read!,hpq7qvm,t1_hppf77p,1640293223.0,False
rmtdj4,Thanks.,hpqp4to,t1_hppf77p,1640300943.0,True
rmtdj4,"If this is too much text for you, you can also watch this video as an alternative or complement.

[https://www.youtube.com/watch?v=CVg7CYVV3KI](https://www.youtube.com/watch?v=CVg7CYVV3KI)",hpokxuz,t3_rmtdj4,1640267665.0,False
rmtdj4,Nice :),hpq0a6f,t3_rmtdj4,1640289978.0,False
rmewjn,I would guess search engine could put a nsfw tag for certain key words to adjust weight,hplx9o7,t3_rmewjn,1640209741.0,False
rma55n,"I don't recall ever hearing that the logical operations are mainly composed of AND, XOR, and NOR. Generally at the gate level, complex designs are mainly built from NAND or NOR gates because they are ""universal"", i.e. you can build any logic design you want with enough NAND gates. Using a single gate type has some benefits when it comes to the physical layout and fabrication of your design, but the fact that your logic will be implemented in this way can be ignored for simple cases.

It's also worth noting that you don't always design logic at the gate level. Things like multiplexers, priority encoders, queues, etc. will make it easier to design complex logic structures.",hpmjuqi,t3_rma55n,1640219823.0,False
rma55n,"I’ve heard about NOR and NAND being universal, but it’s it really all that common to use them exclusively? I’d imagine using just those gates would end up adding many extra gates than if you were to use the whole variety. Wouldn’t it be cheaper to use less gates? Or at this low of a level is it truly a negligible cost?",hpn2aa4,t1_hpmjuqi,1640228667.0,False
rma55n,"Not necessarily correct, it depends on the technology you use to implement those gates, and the purposes of design. I remember that generally, the nand have smaller area in cmos technology and faster response time. In digital world, we only know 0, 1 but in real world, it takes time to change from 0 to 1and stable at 1 and vice versa. It's also easier to layout efficiently a design from same blocks rather than multiple shapes and sizes",hpniwh8,t1_hpn2aa4,1640240046.0,False
rma55n,Try building one yourself. There's a great book called The Elements of Computing Systems which just got a new edition.,hpq70xw,t3_rma55n,1640292909.0,False
rma55n,"An ALU can be a multiplexer with, say, 2 select bits, 4 inputs to select from (add, subtract, and, or) and one output. The add input comes from the adder (and is selected when the select signal is, say, 00), the subtract input (select 01) comes from the adder as well but one of the inputs to that adder will be changed to 2s complement beforehand, the and input (select signal 10) comes from an and gate with a and b as inputs, and the or input (select signal 11) comes from an or gate with a and b as inputs. 

If the design is based on the ttl logic family, you would see and & or gates. On the other hand, cmos logic prefers nand & nor gates, and with some minor changes to the circuit (bubble pushing for example), achieve the same end result.",hqtnqmc,t3_rma55n,1641056991.0,False
rma55n,"This is a great answer, thanks for writing this up!",hqto0sn,t1_hqtnqmc,1641057111.0,True
rma55n,"I think I read in a book some time ago that all of the not gates like NAND, NOR take less transistors to build. That could be part of the reason",hsd2wly,t3_rma55n,1642003923.0,False
rma55n,"AND, NOR, and XOR gates have much faster “processing” times than AND, XOR, and NOR. This is because of how the gates are actually implemented on the hardware.",hpnex9n,t3_rma55n,1640237531.0,False
rma55n,It's all just turtles all the way down but some of these use more turtles than others.,hpq6rdk,t1_hpnex9n,1640292793.0,False
rma55n,Following for the answer,hplsy3f,t3_rma55n,1640207973.0,False
rm2a30,There are giant bundles of fibre optic cables running through the Atlantic Ocean connecting Europe and North America.,hpjtgy3,t3_rm2a30,1640176910.0,False
rm2a30,"It’s like a funnel, there is a big single point of access to jump across contents so you will ping your local area, then the dns host in that region which communicates to a layer above it to be sent across the ocean. That’s not the exact path but the hops will go from general to specific then specific to general",hpkgowc,t3_rm2a30,1640188391.0,False
rm2a30,Tracert works by pinging and sending packets specially time live packets. These packets then die or drop when there’s no response. The packets used are ICMP packets which then respond back to the sender of the ping. This resides in layer 3. In short tracert maps out routers and connections to that host and is used for network maintenance to check that routers or servers are live and working through the network.,hpjnu9r,t3_rm2a30,1640173125.0,False
rm1nxm,"It's not about one machine / language, but *Code: The Hidden Language of Computer Hardware and Software* by Charles Petzold is really good for a broader understanding.",hpjm9nq,t3_rm1nxm,1640171943.0,False
rm1nxm,One of the best books I've read.,hpjt80g,t1_hpjm9nq,1640176748.0,False
rm1nxm,"Yeah gotta agree here, great read if you want to broaden your K&U of the subject",hpjnuea,t1_hpjm9nq,1640173128.0,False
rm1nxm,As I read every chapter my appreciation and gratitude increased.,hpk1cob,t1_hpjm9nq,1640181444.0,False
rm1nxm,"Awesome, just ordered",hpk261t,t1_hpjm9nq,1640181858.0,False
rm1nxm,"I love this book and can't recommend it enough, but I don't feel like it's an in-depth history lesson of computing. It does mention important figures and technologies, but doesn't get into the nity-grity and instead is focused on the ""why"" of programming languages.

Still, amazing book. Read it in a few of nights because of how fun it was.",hpk4htn,t1_hpjm9nq,1640182994.0,False
rm1nxm,I second this.,hpk9zkx,t1_hpjm9nq,1640185509.0,False
rm1nxm,This book is required reading for every computer nerd of every stripe. Came here to recommend it.,hprv2c0,t1_hpjm9nq,1640321477.0,False
rm1nxm,"Code: The Hidden Language of Computer Hardware and Software https://g.co/kgs/JTNx9r

This book talks about the origins of “code”, going from Morse code through to binary, etc.",hpjm9s6,t3_rm1nxm,1640171946.0,False
rm1nxm,I think you'll like this much more than a book if you want only an overview of computer history - [Youtube - Crash Course (Computer Science)](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo),hpk7r8a,t3_rm1nxm,1640184500.0,False
rm1nxm,I’m interested in this too!,hpjm6c2,t3_rm1nxm,1640171871.0,False
rm1nxm,Rage inside the machine by Robert Elliot Smith. A really good book mainly about the biases we build into our systems but covers a large part of computation history going 1200 to Babbage and industrial revolution to training fighter pilots,hpjneio,t3_rm1nxm,1640172803.0,False
rm1nxm,"[History of Computing ](https://en.m.wikipedia.org/wiki/History_of_computing)

Read this then check out the references and external links section for deeper dives.",hpjqq6t,t3_rm1nxm,1640175139.0,False
rm1nxm,"**[History of computing](https://en.m.wikipedia.org/wiki/History_of_computing)** 
 
 >The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hpjqrym,t1_hpjqq6t,1640175172.0,False
rm1nxm,"FWIW it’s a complex, multifaceted history that involves a lot of mathematicians, people interested in logic, those who just want to “compute” in the sense of basic calculations centered around all kinds of things, etc. Eventually you’d get back to abacuses, star charts, etc.
  
So you’re unlikely to find a *single* comprehensive book.  
  
You might want to start with **digital computers** and bypass the mechanical/analog stages of computing history. 
  
At the same time you might want to read a book or two on Alan Turing, since the concept of a *Turing Machine* is essential to modern computing.",hpkf0lz,t3_rm1nxm,1640187688.0,False
rm1nxm,"I'm no expert in the history of computing, but the invention of programming languages and modern computers wasn't something that was done in terms of a ""straight linear process."" Instead, many scientists and engineers worked together to create automation technology for mathematical constructs. Think of the evolution of the computer akin to the evolution of Homo Sapiens. A chimp didn't just give birth to a human. Instead, through a long period of time and many small changes (along with other factors), chimps reproduced enough times to get to the human (__PLEASE NOTE: I AM NOT A BIOLOGIST AND THIS IS A REALLY OVERSIMPLIFIED VERSION OF BIOLOGY__). Likewise, through many many inventors and scientists (in)directly working together trying to solve scientific/engineering problems related to their time period, the technology of modern computing came to be.

I would recommend a firm foundation in CS; try the books ""[How Computers Work](https://www.amazon.com/How-Computers-Work-Evolution-Technology/dp/078974984X)"" or ""[Computer Science Illuminated](https://www.amazon.com/Computer-Science-Illuminated-Nell-Dale/dp/1284155617).""

Then you can look history books of computer scientists s.a. Charles Babbage, Alan Turing, Ada Lovelace, etc. Unfortunately, I am not familiar with the history of CS as much as I should be, so I can't help you here :-(",hpm6b87,t3_rm1nxm,1640213598.0,False
rm1nxm,Turing’s Cathedral or Darwin Among the Machines both by George Dyson,hpjr5xq,t3_rm1nxm,1640175433.0,False
rm1nxm,The innovators by Walter isaccson,hpkmz0i,t3_rm1nxm,1640190980.0,False
rm1nxm,I think it's called the innovators by Walter isaacson,hplnu36,t3_rm1nxm,1640205875.0,False
rm1nxm,The Innovators by Walter Isaacson is a great book.,hpmf8zy,t3_rm1nxm,1640217660.0,False
rm1nxm,There’s a Grace Hopper memoir that has a ton of history in it.,hpn1g27,t3_rm1nxm,1640228264.0,False
rm1nxm,The Cryptonomicon is a fun novel that touches on this subject a bit. But I came here to recommend Bertrand Russell’s type system for logic as an early example and influence on Turning’s contributions to the field.,hpnw15g,t3_rm1nxm,1640249810.0,False
rls9mc,">since the new minimum node will be the parent node, setting the pointer to the new minimum also takes constant time.

I believe the issue here is that there is often more work to be done after removing, in order to rebalance the tree.

If you look at the first example here:

https://www.javatpoint.com/balanced-binary-search-tree

If you simply remove the minimum element, your tree is no longer balanced. And it is not as simple as simply re-assigning the parent. In the case of the example, you would have to reassign at least two other elements.",hpibugc,t3_rls9mc,1640141345.0,False
rls9mc,"Okay yea that's where my error r
Lies. I made the assumption that deleting from a balanced bst given a pointer to the node would take constant time. Guess it makes sense that balanced bst would do some more work to balanced tree after a deletion. Thanks!",hpkvq4v,t1_hpibugc,1640194520.0,True
rls9mc,"Uhh. Huh? You have to search for the node to delete...
That's why it's o(log(n)) because worst case you find the node as a leaf node which is the height of the bst number of iterations... 

Only way for the node to delete be o(1) is if your bst is just one node.",hpi36fm,t3_rls9mc,1640137453.0,False
rls9mc,"Not the OP, but the passage in the book mentions storing a pointer to the minimum value. Then it mentions that because of this finding the minimum value is O(1), but deleting that same value is O(log(n)).",hpi7v8k,t1_hpi36fm,1640139579.0,False
rls9mc,"You have to reorganize the bst after deleting a node and that entails traversing down the right most child or left most child of the deleted node iirc.

So if you're deleting the root node, you'll still have to traverse the height of the bst to reorganize the bst.",hpidgl7,t1_hpi7v8k,1640142074.0,False
rls9mc,U didn't read the question well. You don't need to search for the node to be deleted if you keep track of the minimum node in a separate variable of its own.,hpkw1je,t1_hpi36fm,1640194649.0,True
rls9mc,"You still have to rebalance the bst after deletion of the node as I've explained already. 

You have to traverse the height of the tree to replace the node being deleted.",hpm4u55,t1_hpkw1je,1640212952.0,False
rls9mc,"I'm sorry.

The parents didn't realize the cost at the time.

There are multiple solutions to this, I really hope you find yours.  Memoization or Parallization, or whatever.

There may still be a cost in memory.  Best of luck to you.",hpibclr,t3_rls9mc,1640141133.0,False
rls9mc,"He's talking about a priority queue (i.e. min heap), not a balanced BST. Finding and removing the top node from a minheap is O(1) (it's the root). However, maintaining the heap invariant (aka finding the new min) requires looking at its direct children and bubbling up the nodes to fill in the gap, which is O(log(N)). O(1) is dominated by O(log(N)).",hpik8fu,t3_rls9mc,1640145317.0,False
rlfsh9,"Structure alignment tries it's best to align structures to fall within certain boundaries. Specifically memory words. Most address schemes are aligned on word boundaries so if you don't align to them you have to have an (another) offset to find the start of a structure/ field, which takes more instructions and time. You also have caches to deal with, which always read in a certain amount of words, typically called a cache line with is typically measured in memory blocks. The same thing happens if you don't align fields in your structure to at least word boundaries. It can be very expensive if you go across cache line boundaries for a field, especially if one of your lookups results in a cache miss.

 Now like you said, a lot of structures are too big to fit in a cache line. But it still helps to align their fields to a word boundary because you still don't want to also have to figure out the offset within the word. This is why structs in c/c++ will pad fields typically to boundaries of the highest native type. It just makes book keeping easier/faster.

Us humans operate the same way. If you are counting by 4s it's much easier to count by 4 all the time than to count by 4 to a certain point then change to counting by ones.",hpfibxk,t3_rlfsh9,1640098774.0,False
rlfsh9,"Caching is a huge factor of why you dont want your data striped across memory. The ability to fetch data from the cache is often times order(s) of magnitude faster than fetching from memory, which is in turn order(s) of magnitude faster than fetching from a hard drive (HDDs at least). So reducing the number of times you have to go get data from somewhere else, the better.

If you are dealing with very large datasets, where you can’t fit all of the data into the cache, then knowing how data is read from memory/disk becomes more important. For example, execution times while looping over large 2D arrays will vary widely depending on which array is iterated in the outer loop, and which array is iterated in the inner loop.",hpfwyvm,t3_rlfsh9,1640104961.0,False
rlfsh9,"> For example, execution times while looping over large 2D arrays will vary widely depending on which array is iterated in the outer loop, and which array is iterated in the inner loop.

Yup, I took an operating systems course and one of our assignments involved refactoring slow code to optimize iterating over a 2D array. Just changing the outer and inner loops of array access decreased the time of the program from ~30 seconds to about 10 seconds.",hphyn8v,t1_hpfwyvm,1640135372.0,False
rlfsh9,"> Caching is a huge factor of why you dont want your data striped across memory

Why is data striped across memory blocks not good for caching?",hqdx0l5,t1_hpfwyvm,1640764266.0,True
rlfsh9,"At a very basic level, scattered/striped data = more reads/writes performed.

I was going to type out a long explanation but im sure there is better, more verbose documentation out there. 

Like this! https://youtu.be/247cXLkYt2M",hqe3u1i,t1_hqdx0l5,1640769668.0,False
rlfsh9,"That video was a great watch. I always thought that there was not very informational content on YouTube, but well this video defies my belief. I did not fully understand the caching and stuff in the video but it of course gave me a starting point for learning. Thanks a ton!",hqezu1z,t1_hqe3u1i,1640790266.0,True
rlfsh9,"I had a question. I understood that data stored contiguously in memory is great for prefetching and caching, but how is memory aligned data good for caching?",hqigj4q,t1_hqe3u1i,1640842486.0,True
rlfsh9,"I think you have missed something in your example.

If you have a struct with an 8byte field (e.g. a 64-bit integer) and a one byte field (e.g. an 8-bit integer) then on CPUs which have alignment restrictions the compile will add padding at the end of the struct so that when multiple structures are put in an array all fields can be accessed efficiently.

Usually the worst case for alignment restrictions is “natural” alignment which means in this case that the 8byte field must be 8byte aligned. The compiler would then create a struct with { 8 byte field, 1 byte field, 7 bytes of padding }. This ensures that the size of the struct is an even multiple of 8 so if you create an array of them the 8byte field alway lands on a even multiple of 8 byte offset from the start of the array.

Edit: adding explanation for why alignment makes things more efficient.

What is not obvious is WHY aligning fields makes things more efficient. Adding padding to structs “wastes” memory, so it actually makes things LESS cache efficient in general.

At a surface level the answer is that the actual binary CPU instructions have these alignment restrictions built in, so if the compiler wants to access a field it can do it with only a single instruction if it knows the field is aligned. If the compiler know the field is not aligned, then it must compile the unaligned access into multiple instructions that access smaller fields and then more instructions to combine these small access back together into a register with the whole value. Clearly one instruction vs multiple is more “efficient”.

Going deeper one may ask WHY would some CPU instruction sets have this kind of alignment restrictions? Not all CPUs have these restrictions, so why do some? The answer is that these restrictions make the HW simpler, and simpler HW takes less power/room leaving more left over for enhancing performance in general. For example on how aligned access make HW simpler: no access will cross a cache line or page boundary. This means that any aligned load/store maps to exactly one cache line and exactly one page and will never be split accross two of either. HW can be built with this simplification in mind.",hpghkr4,t3_rlfsh9,1640113046.0,False
rlfsh9,"I have no idea what a data structure is, but I’m eager to find out. I’m taking thr FCC JavasScript course.",hpi7g6w,t3_rlfsh9,1640139388.0,False
rkr2j4,"Tanenbaum is 77. When he was getting a degree CS was not really a thing you studied as a standalone degree. I don't know if there were any CS faculties at the time, but most CS people from that era majored in something like Mathematics or Physics. Consider that he got his Physics BSc in 1965, that's room-sized computer era.


EDIT: To add some more perspective, consider that ANSI C (or C89), the oldest version of C probably anyone here has used, was ratified as an ANSI standard in 1989. Think about that. The absolute **oldest** version of C likely all of us have come into contact with came **24 years** after Tanenbaum got his physics degree.",hpb9qot,t3_rkr2j4,1640019118.0,False
rkr2j4,"To be fair, C was standardized in 1989 but was created in the early 70s.

Still came after his physics degree though LOL",hpdt2ns,t1_hpb9qot,1640058671.0,False
rkr2j4,"People were writing in C long before it was standardized. The K&R book was first published in 1978, so C was already fairly mature by then.",hpdv6dj,t1_hpb9qot,1640059692.0,False
rkr2j4,">wiki says he's a established physicist while his books and his career speaks he's a computer scientist.

But the Wikipedia article devotes only _one_ sentence to physics? Everything else is about Computer Science",hpb9bnr,t3_rkr2j4,1640018949.0,False
rkr2j4,Wikipedia is generally very slanted towards computing topics.,hpblwd0,t1_hpb9bnr,1640024057.0,False
rkr2j4,"True, but Tannenbaum himself barely talks about his physics experience on his site or CV. He talks almost entirely about his computing and OS research.",hpcbw1f,t1_hpblwd0,1640034754.0,False
rkr2j4,"Yes it's true. His phd thesis is also on A Study of the Five Minute Oscillations, Supergranulation, and Related Phenomena in the Solar Atmosphere. 

I don't think this requires much core CS knowledge like OS etc. Man this guy did hardcore physics and computer science together.",hpefkyn,t1_hpcbw1f,1640072509.0,True
rkr2j4,"Now, watch this: 

Mathematician turned into computer scientist:

https://en.wikipedia.org/wiki/Barbara\_Liskov",hpeon9g,t1_hpefkyn,1640080018.0,False
rkr2j4,"There is no surprise in it. Computer Science Engineering as a separate discipline was founded at my former university somewhere between 1995-1997, before that people learned Electrical Engineering, and part of it programming and related disciplines. So, a Computer Science Engineering was an electrical engineer specialized to computer programming, the software part for a couple of years. The other direction was the programmer mathematician, which was taught at another University, where a programmer mathematician was a mathematician specialized in programming.",hpbtk3f,t3_rkr2j4,1640027147.0,False
rkr2j4,"Yes but his research is also on core physics stuff. 
His thesis was:  A Study of the Five Minute Oscillations, Supergranulation, and Related Phenomena in the Solar Atmosphere. 
And I don't think it needs much core CS knowledge like OS ,computer architecture etc. 🤔
He must have gotten very interested in the emerging CS field and decided to invest lots of time in learning it. 🙂",hpegbe1,t1_hpbtk3f,1640073080.0,True
rkr2j4,"Yes, agree, I think this is the way he went. TBH I think research wise it is more useful to have a math or physic degree.",hpek746,t1_hpegbe1,1640076242.0,False
rkr2j4,"Once you realise that your degree is supposed to teach you only one skill : Learning how to learn, that is the point you achieve ultimate freedom. You can learn anything you want after that and with suitable access to facilities or not you can research in any domain you like.

There are countless examples.",hpdwqwx,t3_rkr2j4,1640060489.0,False
rkr2j4,"As others have stated, CS as a study on its own really wasn't a thing yet. The first CS programs only really started becoming a thing in the 60s when he first graduated. Instead, what we consider CS today would have fallen under mathematics, or electrical/computer engineering. And a lot of early programmers would have been in those fields or others like physics due to needing computers to solve certain equations. Back then learning to program would just be reading a manual that came with the giant IBM or whatever, not some actual formal education.

If you want to grasp how someone coming from just math/physics was able to then also have such insight into the more complex CS before it was a standard thing, then I would recommend reading The Art of Computer Programming. It's the densest literature you can read in CS, but it shows how mathematics is the basis to create the major CS topics of today. You can really see the natural progression of the author, Donald Knuth, from mathematician to computer scientist.",hpbglr9,t3_rkr2j4,1640021923.0,False
rkr2j4,"I teach CS at university and I'm self-taught with no CS degrees. It's not like you have to get a CS degree to study CS -- true of any subject really. You just need the motivation, dedication, and understanding of how to teach yourself stuff.

CS itself is a highly interdisciplinary field. You can tell from the amount of technical terms and jargon in CS that's been taken (and bastardized) from other fields like philosophy, math, linguistics, and so on. So, often you can pivot from another discipline, using the expertise you have there to start in some related topics in CS. For example, I have a background in formal logic and that's where I started teaching myself about CS.",hpbn8yd,t3_rkr2j4,1640024598.0,False
rkr2j4,100 times this.,hpcb3ua,t1_hpbn8yd,1640034427.0,False
rkr2j4,Any degrees besides CS?,hpbyzof,t1_hpbn8yd,1640029373.0,False
rkr2j4,"When you're talking about degrees that are reasonably common among people working in industry in ""CS-like"" fields, the heavy hitters are CS (obviously), electrical engineering, computer engineering, physics, and mathematics. But you'll see working engineers with all sorts of backgrounds. It's one of the easier fields to be self-taught in to a degree that lets you do most technical work that employs graduates.

For professors, could be anything. A PhD isn't like a BS where you get a degree that intends to certify that you know the core material for a given field. A PhD intends to certify that you know how to do research. If you have a PhD in Folklore and you decide you want to be a computer science professor, you just need to publish in Computer Science conferences and journals for a while and then you're a computer scientist. However you acquaint yourself with the knowledge needed to get your papers accepted is irrelevant. Universities don't require a CS PhD to be a CS professor (or any other field). You need a PhD in literally anything and then you need people in the field you want to join to say, ""That person is awesome at my field. I've read their work and it's great"".",hpc4wi4,t1_hpbyzof,1640031836.0,False
rkr2j4,My degrees are in a humanities and I taught and did research in that field for several years.,hpcc1xo,t1_hpbyzof,1640034822.0,False
rkr2j4,He's more than that too. Started https://electoral-vote.com roughly 20 years ago. So you can add Poli-Sci as another interest.,hpc0vf3,t3_rkr2j4,1640030168.0,False
rkr2j4,"> I'm curious as to how he got the expertise to write books and do research on OS? 

A question you should ask about any academic! His [CV](https://www.cs.vu.nl/~ast/home/cv.pdf) is here, feel free to read it :)",hpcbtlv,t3_rkr2j4,1640034726.0,False
rkr2j4,Thanks 🙂,hpe9xmc,t1_hpcbtlv,1640068335.0,True
rkr2j4,"\[Edit: The post below is about Knuth, not Tannenbaum. Totally misread the question\]

I was lucky enough to attend one of his lectures. He is an incredibly humble individual, who has the ability to narrow his focus to be laser-like, and just do one thing and learn it entirely. I believe that's how he became who he is - by focusing on one thing, and not getting distracted.

During the lecture he was saying he stopped reading email ~~in the late 90s~~ \*since January 1st, 1990 (source: [https://www-cs-faculty.stanford.edu/\~knuth/email.html](https://www-cs-faculty.stanford.edu/~knuth/email.html)) and never went back. I doubt he has a smart phone that pings away his concentration every few seconds, or several meetings scheduled throughout the day.

He just picks up a subject, goes deep into it, and comes out the other end as a master and having created something useful out of it.

I'm sure there's plenty of subjects he approached where he could not produce something, so we might see only a selection of the successful ones. Still, what he can do is impressive.",hpf9ykb,t3_rkr2j4,1640094797.0,False
rkr2j4,[deleted],hqbd58m,t1_hpf9ykb,1640720445.0,False
rkr2j4,"According to [him](https://www-cs-faculty.stanford.edu/~knuth/email.html),   


>if you want to write to me about any topic, please use good ol' snail mail and send a letter to the following address:\[...\]  
I have a wonderful secretary who looks at the incoming postal mail and separates out anything that she knows I've been looking forward to seeing urgently. Everything else goes into a buffer storage area, which I empty periodically.  
My secretary also prints out all nonspam email messages addressed to \[...\]  so that I can reply with written comments when I have a chance.",hqc1s01,t1_hqbd58m,1640730513.0,False
rkr2j4,[deleted],hrvnvxq,t1_hqc1s01,1641706526.0,False
rkr2j4,"Wow, what a blunder. Whops.",hs1l1lr,t1_hrvnvxq,1641807703.0,False
rkr2j4,[deleted],hs3fnz0,t1_hs1l1lr,1641839195.0,False
rkr2j4,"Sorry for being the cause of ""I've read it on the internet so it must be true"" :D",hs6smmq,t1_hs3fnz0,1641894292.0,False
rkr2j4,"It seems that your comment contains 1 or more links that are hard to tap for mobile users. 
I will extend those so they're easier for our sausage fingers to click!


[Here is link number 1 - Previous text ""him""](https://www-cs-faculty.stanford.edu/%7Eknuth/email.html)



----
^Please ^PM ^[\/u\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20hqc1t9n)",hqc1t9n,t1_hqc1s01,1640730528.0,False
rkr18d,"The classic book in the area is Introduction to Algorithms by Cormen, et al.",hpbcm27,t3_rkr18d,1640020300.0,False
rkr18d,Thanks!,hpbdh2f,t1_hpbcm27,1640020654.0,True
rkr18d,"Do you mean correctness proofs? If so I can recommend Software Foundations volume 3 “Verified Functional Algorithms”. The book is available for free online. I recommend going through at least the first volume first, though.",hrb4j0m,t3_rkr18d,1641354626.0,False
rkfmao,"Don't use MD5 at all. Just use bcrypt.

The only time I'd hash something multiple times is in cases such as a zero knowledge password manager where you don't want the server to ever have the plaintext password, so you'd hash it on the client side once and then again on the server side -- but that's a very niche case and not what you need to do.

If you want additional security you can increase the rounds that bcrypt performs. This is the ""cost"" input to bcrypt. The default is typically 10 for most implementations which means 2^10 internal rounds. Using 13 or so is reasonable.",hpab3cv,t3_rkfmao,1640001693.0,False
rkfmao,I'm using md5 so that I don't need to store the byte data in the dB I'm salting it with 15 rounds then hashing that hash again with md5 so I can just use a var char to check the users password I just hash the users input the same way it was created and see if the hashes are the same.,hpahpu8,t1_hpab3cv,1640006005.0,True
rkfmao,It’s still unclear why you MD5 hashing would help anything here.,hpb1ok2,t1_hpahpu8,1640015730.0,False
rkfmao,Its purely just for db storage,hpbnary,t1_hpb1ok2,1640024618.0,True
rkfmao,"But it doesn’t add much security, if any, and adds an extra step later…  
  
Plus any MD5 hash collisions could make it possible to get into someone else’s account or even guess passwords (if the salt is the same each time).",hpbr264,t1_hpbnary,1640026132.0,False
rkfmao,"You can encode the bytes from bcrypt into text using base64, or even hexadecimal.",hpbg9ui,t1_hpahpu8,1640021789.0,False
rkfmao,"The output of bcrypt already contains the salt. You don't need to store anything else. If you were using something like PBKDF2, then you'd need to store the salt either in a separate column, or encode the digest + salt into some kind of hex or base64 string for storage.

e.g. bcrypt will output something like:  `$2a$10$N9qo8uLOickgx2ZMRZoMyeIjZAgcfl7p92ldGxad68LJZdL17lhWy` that has a salt of `N9qo8uLOickgx2ZMRZoMye` \-- this whole thing can be stored just like any other string in your database (e.g. in a varchar column)

Just do `digest = bcrypt(plaintextPassword)`, and save that digest to the database. The language you're using will have something like `hash.check(plaintextPasswd, storedDigest)` for comparing the stored digest against the plaintext password a user enters when trying to log in",hpcdjmr,t1_hpahpu8,1640035448.0,False
rkfmao,"You asked this [yesterday in /r/django](https://www.reddit.com/r/django/comments/rkf0a8/im_making_a_web_app_and_im_hashing_the_passwords/) but deleted your question when people responded negatively, and you're not getting the response you *need to hear* here, so I'll repeat my answer here:

This makes no sense. None.

You've stated elsewhere here that...

> I'm mainly doing it this way so I dont have to use blobs in my database with byte type data and can just use varchar with hexadecimal. 

 This is a solved problem, bcrypt hashes are **strings** that have the form `$2a$10$N9qo8uLOickgx2ZMRZoMyeIjZAgcfl7p92ldGxad68LJZdL17lhWy` and your hashes should be persisted in that format. If your implementation is producing binary output, you should use a different implementation. If you've implemented this yourself, **don't**. Use an off-the-shelf option. **Don't** run your binary data through MD5 just to get a hexadecimal representation. You'll produce something that is completely unportable, and you'll have no way to go back and *make* it portable as your original passwords will be lost after going through this Frankenstein hashing process. EVERY BCrypt implementation in any language can accept a hash in the form `$2a$...` and test it. Your hashes will be useless outside of your own narrow bit of code.

You've also said you're sending `md5(bcrypt(salt + password))` to the database. Where are you saving the salt?? You *need* that salt in order to compute `bcrypt(salt + candidate_password)` the next time the user attempts to log in. If you hash it with MD5, it's gone, and you can't use your stored hash for anything. Storing BCrypt in the form `$2a$...` takes care of this for you, storing the salt as part of the encoded hash.

Your question contains this troubling line:

> ... if they got the salt by hacking into the server ...

This implies to me that you're using **one salt** for all hashes, stored as a single secret. **That's not a salt, and this isn't BCrypt**. BCrypt uses a *per-user salt*, stored along side the output hash. Decorating your hash with a single application-wide secret is a [cryptographic **pepper**][1], which is not part of BCrypt and not useful. It's also unclear to me how you can be using a single pepper and still have BCrypt's cost factor at play.

Finally, MD5 is old and broken, and has **no business** being considered for use *anywhere*, but especially not in any thing related to security or password hashing. MD5 adds absolutely no security here, only complexity and harm, and it's about the worst possible way you could convert raw bytes to a hex encoding.

[1]: https://en.wikipedia.org/wiki/Pepper_(cryptography)",hpc1ye4,t3_rkfmao,1640030613.0,False
rkfmao,"If it’s md5( bcrypt( password + salt)) then yes

If it’s md5( password + bcrypt(salt)) then no. 

I don’t know what bcrypt is off the top of my head but if it’s a hashing algorithm on par with sha256 or 512 then it’s good. If it’s an encryption (that you can decrypt) then probably not.


Out of curiosity, why aren’t you just using the bcrypt value assuming it’s good. Also why not use a high sha rather then the md5. That would make it more secure one way or the other.",hp9jyvb,t3_rkfmao,1639980205.0,False
rkfmao,"SHA-512 is a cryptographic hash while bcrypt is a password hash or PBKDF (password-based key derivation function).  
  
SHA-512 has been designed to be fast. You don't want any delays when validating a signature, for instance. There is no reason for generic cryptographic hashes to be slow.  
  
bcrypt on the other hand is a password hash that performs key strengthening on the input. Basically, it does this by slowing down the calculation so that attackers will have to spend more resources to find the input by brute-forcing or dictionary attacks. The idea is that although the legit users - you in this case - will also be slowed down, they are only slowed down once per password. However, the attackers are slowed down for each try. The legit user is of course much more likely to input the right password first.  
  
Furthermore, bcrypt also contains a salt as input, which can be used to avert rainbow table attacks.

&#x200B;

I'm mainly doing it this way so I dont have to use blobs in my database with byte type data and can just use varchar with hexadecimal. 

&#x200B;

And yes it is md5( bcrypt( password + salt))",hp9kn2l,t1_hp9jyvb,1639980650.0,True
rkfmao,"From what you’ve said this seems perfectly secure. I’m a little concerned that you may be reducing the output space if md5 has a smaller number of bits output then bcrypt it may be more likely to have collisions. Although this isn’t likely a real security risk, more of an academic view of it. 

I would have base64 encoded the output to just be dealing with text rather then md5ing it. Although that’s just a personal preference thing. (Possibly some performance increase as that’s probs faster then md5, but again so minimal it’s academic)

Salted and hashed with a good algorithm is the best way I know to store passwords. If anyone knows of a better I’d be super curious to learn as well.",hp9la0w,t1_hp9kn2l,1639981082.0,False
rkfmao,"Yeah, I don't think an attacker would ever really be able to get the original value of the password out at least. But if anyone else has any more input id be happy to hear it.",hp9lu4q,t1_hp9la0w,1639981456.0,True
rkfmao,"I agree with hourglass492 that if your concern is database storage, you should convert the bcrypt output to a text format such as by base64 encoding. Running your bcrypt output through md5 is reducing your output space for no real benefit.",hpbm6e2,t1_hp9lu4q,1640024170.0,False
rkfmao,Thanks ill just convert it to base 64,hpbml44,t1_hpbm6e2,1640024334.0,True
rkfmao,"Don't do that. BCrypt hashes are stored in the form `$2a$...`, if your implementation is not producing that format, don't use that implementation: https://en.wikipedia.org/wiki/Bcrypt#Description",hpc3nc6,t1_hpbml44,1640031313.0,False
rkfmao,"Instead of MD5 im just going to convert it to base64 as the purpose of the md5 was mainly just for db storage. 

&#x200B;

Instead of MD5 I'm just going to convert it to base64 as the purpose of the md5 was mainly just for DB storage.",hpbmq33,t1_hp9kn2l,1640024387.0,True
rkfmao,[deleted],hp9isll,t3_rkfmao,1639979438.0,False
rkfmao,"Its not MD5 though its bcrypt(password, salt) then I take the bcrypt hash and do md5(bcrypt hash) then send that to the db",hp9jqtu,t1_hp9isll,1639980056.0,True
rkfmao,"I don't think you can use md5( bcrypt( password + salt)) like that, otherwise you wouldn't be able to check if password is correct even if you know it. You have to save salt in plain text format or encrypted by AES. Istead of md5+bcrypt I would suggest using Argon2, with it you can increase or deacrease how long it takes to check plain text password against encrypted one.",hp9o1yf,t3_rkfmao,1639983024.0,False
rkfmao,I don't need to check if the password is correct I can just hash the user's input again and see if it's the same.,hpagnv2,t1_hp9o1yf,1640005377.0,True
rkfmao,Not without the salt you can't.,hpc3oxr,t1_hpagnv2,1640031332.0,False
rkfmao,"Tangential :   
Have you evaluated available libraries for the same operation ?  Depending on the ecosystem (say Java or Python) there might already be libraries to implement password storage + validation +  anti-brute-force (like detect attempts across load balanced instances)",hp9sezt,t3_rkfmao,1639986325.0,False
rkfmao,"Yikes. Don't do this.

Please read this: https://crackstation.net/hashing-security.htm

Don't do this. Take the time to really learn cryptography before you start jamming hashes together willy nilly.",hp9stf6,t3_rkfmao,1639986642.0,False
rkfmao,That article is actually exactly what I'm doing it's impossible for me to decrypt so I take the users input hash it with bcrypt then hash it with md5 and see if the hash is the same.,hpahhsk,t1_hp9stf6,1640005873.0,True
rkf6jh,"Hmm maybe this is a little higher level than what you want, but Ben Eater on youtube has a series where he builds a 65c02 based computer from scratch using a 6502 microprocessor.",hp9kxq7,t3_rkf6jh,1639980849.0,False
rkf6jh,He also has a 8-bit computer from scratch series and kit.,hpb3jxz,t1_hp9kxq7,1640016534.0,False
rkf6jh,Second for Ben Eater - he has some really fantastic content!,hpe5tw2,t1_hp9kxq7,1640065649.0,False
rkf6jh,https://www.amazon.com/8088-Project-Book-Robert-Grossblatt/dp/0830602712,hp9ohgn,t3_rkf6jh,1639983339.0,False
rkf6jh,"What about building something around the raspberry pi RP2040? designed for this purpose, A lot simpler than a full blown ARM64/GOU SoC, I imagine, and with luck there’s a community around it.",hp9vnc5,t3_rkf6jh,1639988955.0,False
rkf6jh,[here you go](https://www.nand2tetris.org/)  . though you need to learn bit of EE for practical purrposes. Buy Andre Lamothe's course from udemy( cheaply available). And for theory of EE search for JIM PYTEL on youtube and start with his DC electronics part 1 playlist.  Best wishes.,hp9qzny,t3_rkf6jh,1639985210.0,False
rkf6jh,"[stack computers](https://users.ece.cmu.edu/~koopman/stack_computers/index.html) 

[hdl resource](https://hdlbits.01xz.net/wiki/Main_Page) 

In stack machines chapter 3.2 there is a basic design with instruction specs that can be implemented in verilog for a fpga without too much difficulty. 

in terms of installing a os on the computer I'm not sure how honestly [os dev](https://wiki.osdev.org/Expanded_Main_Page) might have some resources I imagine that unless you replicate a existing chip design you'd have to edit(make?) the back end of a compiler to use a existing os (am unsure) os dev might have resources and their reading list is pretty good regardless. 

I'm working on something similar (I think) it *might* be easier to first make a small interpreter that serves as the os like forth that's what I'm looking into but haven't tried yet. 

also you could maybe *maybe* use that system to make a tiny vm to boot up a operating system using a vm that uses another computers instruction set as its optcode. 

you're more qualified than I am. I'm just a hobbyist I don't know if that's what you're looking for. I just remember looking to do what sounds similar running into a lot of dead ends and wasted time hope those help.

is there any particular fact about computers you feel you understand particularly poorly?",hpa6bpm,t3_rkf6jh,1639997987.0,False
rkf6jh,"Hello there, I actually have developed some small microprocessor platforms in the past and am putting one together in 2022 for a medical device I am designing. 

My advice to you is to start small. Look at the minimum components necessary to run a fairly simple microprocessor and move up from there. For example, maybe start with a simple Atmel chip (e.g. 328) or even a TI chip like the MSP-430. 

If you have no patience for that, and want to jump right in, I recommend looking at the Atom series x86 platform. The minnow board project is open and you can get right into customising your own platform: [https://www.minnowboard.org](https://www.minnowboard.org)",hr13yt3,t3_rkf6jh,1641182683.0,False
rkf6jh,[deleted],hp9fq5m,t3_rkf6jh,1639977536.0,False
rkf6jh,"Essentially like a raspberry pi.

I’m fine using an existing operating system. Developing my own operating system and SBC I feel like would become unrealistic for one individual. Or at least it is not in my scope of interest at this time.",hp9in93,t1_hp9fq5m,1639979343.0,True
rkf6jh,"I recommend building something compatible with a beagle bone or raspberry pi operating system (ie clone the hardware).

Otherwise, if this is your first or one of your first boards, you may want to consider an easier project first. I have done this exact thing as an electrical engineer with a few years of experience and it was still a challenge to me. Took about 200 hours of work, and needed impedance control (see advanced pcb building techniques). The board has about 350 individual components.

Osd has some good ressources on how to build a beagle bone clone using the osd3358. Read the tutorials!

Best of luck and feel free to pm me if you have any questions",hpakxty,t3_rkf6jh,1640007815.0,False
rkf6jh,"Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)

Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

[Ben Eater's Build a 6502 computer](https://eater.net/6502)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",hpb31ln,t3_rkf6jh,1640016317.0,False
rkf6jh,"Start small.  Design a simple 8bit ALU first and understand how they work, then add a verilog memory module, peripherals, etc.",hpnbe70,t3_rkf6jh,1640235445.0,False
rkf6jh,"> I am an electronic engineer

A student? Or an actually employed EE? I ask because I'm wondering who employs EEs these days to design things with nothing but lights and switches! :)

Do you want a modern SBC, e.g. with an Arm, or are you happy with some random 8bit thing?",hpadsoi,t3_rkf6jh,1640003569.0,False
rkf6jh,"Electronic engineers don’t do much designing. I do testing, program ics, build based on schematics, troubleshoot boards and test systems, design fixtures and some other stuff.

Electrical engineers do design.

Modern SBC is my aim.",hpcbn6o,t1_hpadsoi,1640034650.0,True
rkf6jh,"> Electronic engineers don’t do much designing.

The ones I work with do! Their job is to basically make SBCs in various forms for us software engineers to program :) I don't know if this is a different country thing (UK here) but what you describe tends to lean towards an electronic test engineer.",hpcdbfy,t1_hpcbn6o,1640035351.0,False
rkf6jh,So what do your electrical engineers do then?,hpcgh7f,t1_hpcdbfy,1640036680.0,True
rkf6jh,"Where I work they're not employed, but from what I know in other places they work on bigger things, power systems and stuff?

https://uk.indeed.com/jobs?q=Electrical%20Engineer&l=London%2C%20Greater%20London&vjk=f05819b0247e61d5&advn=2755904151741368

https://www.reed.co.uk/jobs/electrical-engineer-jobs-in-london",hpcwjhc,t1_hpcgh7f,1640043678.0,False
rk78ya,"Hi,

I found some idea about this problem at:[Ethereum.stackExchange](https://ethereum.stackexchange.com/questions/116919/detection-of-same-function-reentrancy-vulnerability).

Zulfi.",hpfj7e7,t3_rk78ya,1640099163.0,True
rk5sms,"Check this out: https://en.m.wikipedia.org/wiki/Hardware_random_number_generator

Basically by measuring certain things at the quantum scale you can get ""true"" random",hp7qc2w,t3_rk5sms,1639948307.0,False
rk5sms,"Up to your confidence in certain quantum theories over others.

Randomness is part of standard model (though even there there’s some nuance), but observationally equivalent theories exist that don’t invoke it.

Of course, lack of information still makes event effectively “random” for those measurements — but at that point you might have an easier time just leaning on known chaotic dynamics — to get deterministic, but unpredictable events that match some distribution of interest.",hp8eylt,t1_hp7qc2w,1639959039.0,False
rk5sms,"I'm fairly sure that Bell's Theorem invalidates hidden variable interpretations of quantum mechanics, no?",hp8h1bn,t1_hp8eylt,1639960003.0,False
rk5sms,"Bell’s theorem only invalidates *local* hidden variable theories.  Where *local* references the theorized particle in question.

You can still have hidden variable theories they just have to have ways for particles to communicate without being next-to/on-top-of each other.

De Broglie’s proposed [pilot wave theory](https://en.m.wikipedia.org/wiki/Pilot_wave_theory) would be an example.",hp8ivnj,t1_hp8h1bn,1639960860.0,False
rk5sms,"Desktop version of /u/OphioukhosUnbound's link: <https://en.wikipedia.org/wiki/Pilot_wave_theory>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hp8iwwk,t1_hp8ivnj,1639960878.0,False
rk5sms,Thanks for the info!,hp8ktf5,t1_hp8ivnj,1639961807.0,False
rk5sms,[Good explanation on the matter](https://www.youtube.com/watch?v=ytyjgIyegDI).,hp8pue9,t1_hp8h1bn,1639964262.0,False
rk5sms,On a quantum scale? So like if I flip a coin and get a coffee flavored duck made out of sand? Would that be considered true random or are we still talking heads or tails here?,hp90w4r,t1_hp7qc2w,1639969682.0,False
rk5sms,"We're still talking heads/tails really.  In the end we must measure something, it's just a question of whether or not that thing we're measuring is deterministic or not.",hpbzjn9,t1_hp90w4r,1640029634.0,False
rk5sms,"Some of the best entropy sources we have are probably background radiation or radioactive decay.

You can get truly random data from https://www.random.org/",hp7s3kc,t3_rk5sms,1639949043.0,False
rk5sms,"In every thread about random numbers, I feel compelled to mention [this.](https://blog.cloudflare.com/lavarand-in-production-the-nitty-gritty-technical-details/)",hp9ft5h,t3_rk5sms,1639977585.0,False
rk5sms,"In theory yes. In practice it is very difficult to get it right, esp for security purposes. For various reasons, e.g.  bias, influenceability, measurement errors, degradation. And it also very difficult to assess that something is actually random.",hp86wms,t3_rk5sms,1639955431.0,False
rk5sms,"You can, yes, a lot of algorithms use minute heat changes in the CPU or background radiation from space (same stuff that causes static in radios, that is radiation in the form of radio waves) since both are relatively inexpensive computations and close to true random.

However, usually you don't want a completely random arrangement as many times people won't see it as random.

For example, in a truly random system, you will see runs of the same number especially in binary systems like flipping coins. The macro results (distribution) are random, not necessarily the micro results you see in short samples.

For this reason, most algorithms and programs employ ""pseudo-random"" generators which appear to be more sporadic to us humans.",hp8mac0,t3_rk5sms,1639962526.0,False
rk5sms,"The answer depends upon the context of the question. If you are asking this question to know whether you can implement this in your application or not, well you can using TRNGs(true random number generators) or HRNGs(hardware random number generators) like the other comments mention.

But if you are asking your question on a theoretical level then this answer is for you. Nothing can be random according to [Laplace's demon](https://en.wikipedia.org/wiki/Laplace%27s_demon). It is a hypothesis because for determining relatively ""random"" events(although there are no random events in the known universe according to Laplace's demon) you would need a lot of computation and that often falls into the [Computability Theory](https://en.wikipedia.org/wiki/Computability_theory).

I wrote a paper about your question titled ""Can something be truly random"" but I later never published it. But I'll discuss some ideas about the paper here. So Laplace's demon is actually kinda proved by the fact that we can now calculate the result of a coin toss before the coin actually lands on the ground.

We can generate random numbers using nature and we can generate pseudo-random numbers using computers. Randomness of something completely depends on the patterns and predictability of the data. The predictability of data depends on the information conveyed by the data. Information is retrieved from the order of the data. If the data is redundant, meaning that it has no patterns but a pattern of uniformity, it conveys no information because it has no order. This relation can be stated as follows:
Randomness ∝ predictability ∝ information ∝ order ∝ regularity of data

We are not yet sure whether Laplace's demon holds in the world of quantum mechanics because ah things of the macroscopic world usually break in the microscopic world. 

Laplace’s theorem also collapses when we apply the second law of thermodynamics. According to Laplace’s theorem, nothing can be random and everything is predictable, which means that the information about the universe doesn’t increase because there is no randomness being created whatsover. This completely goes against the 2nd law of thermodynamics which states that the entropy of a physics system always increases with time.

I am sorry if my comment went a little off-topic but I hope you learnt something.
Randomness∝predictability∝information∝order∝regularityofdata",hp9o73w,t3_rk5sms,1639983126.0,False
rk5sms,You could do it Schrodinger cats style and take a sample of background radiation/neutron collisions and use that as your seed.,hp9h0ri,t3_rk5sms,1639978326.0,False
rk5sms,"I’m no computer scientist but when I think of this I envision chaotic systems. Where the amount of information is exceedingly high and every vector for each informative quanta has it’s own unique internal system for objective emergence or expression.
In a system like that, we would get caught up in the definition of “perfect”. If by perfect you mean not being able to mathematically trace the origin then no system can truly be perfect. Even the universe (if we had the ability to track each event through or outside of time) would be perfect. It would be predictable and not necessarily random.
I think the perfection you’re looking for would come from the vector quality of each quanta of information and it’s ability to be predicted or traced. A perfectly random event would emerge as an unexplainable mystery.",hp9v1mk,t3_rk5sms,1639988456.0,False
rk5sms,yes at the really small scale things can be truly nondeterministic. x86 CPUs use thermal fluctuation data for rdseed instruction.,hpf8vv4,t3_rk5sms,1640094248.0,False
rk5sms,A random number which is prime,hp8e73f,t3_rk5sms,1639958682.0,False
rjwxfl,"Since the keys were securely exchanged via asymmetric encryption, why does it matter? A handshake/exchange happens once, the portion of with the symmetric encryption needs to be fast as it can happen many times.",hp6ehhu,t3_rjwxfl,1639928521.0,False
rjwxfl,"You'll find almost all network protocols such as HTTPS only use asymmetric encryption for the handshake. The same goes for file encryption, typically a symmetric key is used to encrypt the files, then the symmetric key is encrypted using asymmetric encryption, that way you get the speed of symmetric with the security of asymmetric.",hp80xsg,t3_rjwxfl,1639952801.0,False
rjwxfl,"Yeah. Symmetric cryptography is much, much faster than asymmetric cryptography. There's no other reason.",hp6385q,t3_rjwxfl,1639922994.0,False
rjwxfl,It's much faster and just as secure.,hp9ga38,t3_rjwxfl,1639977872.0,False
rjwxfl,"With asymmetric encryption you don’t decode the message, only verify and sign it. You don’t ever want to exchange the private key, only the public key. You could for example sign a message with your asymmetric key and encrypt it with the key you exchanged based on the same key so the receiver can decode the message, verify it hasn’t been tampered with, without the receiver being able to copy your signature.

If you were to use asymmetric encryption for encoding/decoding messages, both sides need to know the private key.",hp9bbns,t3_rjwxfl,1639974960.0,False
rjvbrs,Write a ray tracer from scratch and try to implement the rendering equation via monte carlo,hp5ufoc,t3_rjvbrs,1639917414.0,False
rjvbrs,Thank you for your recommendation. Would you provide me with supportive resources?,hp5ulyk,t1_hp5ufoc,1639917538.0,True
rjvbrs,"1. Ray tracing in a weekend
2. Rendering equation in Wikipedia
3. scholar.google.com for more

I'm assuming you're experienced in C/C++",hp5usjh,t1_hp5ulyk,1639917669.0,False
rjvbrs,"Implementing the AltaVista MinHash alg could be fun with a web crawler of some sort. Or anything with universal hashing, really.

Bloom filters for malicious URLs has some example implementations out there you could probably build on.

Per Wikipedia, there’s also a use for Karger’s algorithm/Min Cut in segmentation-based object categorization scenarios like image compression.",hp6z1xw,t3_rjvbrs,1639937193.0,False
rjvbrs,Thank you for your contribution,hp97kh3,t1_hp6z1xw,1639972995.0,True
rjvbrs,"You can try bots programming on CodinGame, it's a great way to use those kind of algorithms, especially for reinforcement learning.",hp8kmqg,t3_rjvbrs,1639961712.0,False
rjvbrs,Thanks 😊. I am going to check it out,hp97er8,t1_hp8kmqg,1639972914.0,True
rjvbrs,Do you have experience with music software coding,hp7krrx,t3_rjvbrs,1639946026.0,False
rjvbrs,"No, but I would stop be interested in listening from you",hp97chr,t1_hp7krrx,1639972881.0,True
rjvbrs,Would you think you would be able to make a program that finely tunes audio files frequency/pitch? Say as the standard for music is 440hz you wanna wanna pitch it down to 432hz. It’s possible now but not 100% accurate so I would like to do that without any limits on the numbers and then program it so I can save it,hplguyu,t1_hp97chr,1640203050.0,False
rjvbrs,You could do something with Fourier transform.,hp8ojg1,t3_rjvbrs,1639963626.0,False
rjvbrs,What kind specifically of a project or challenge?,hp97hj2,t1_hp8ojg1,1639972954.0,True
rjvbrs,Lossy compression,hp98p0d,t3_rjvbrs,1639973556.0,False
rjvbrs,"Try the advent of code.  It's a challenge that happens every year; it's language independent; and every one gets their own problem set. 

https://adventofcode.com/",hp9mjxp,t3_rjvbrs,1639981955.0,False
rjvbrs,Thank you so much. I going to add it on my puzzles list,hpejrpp,t1_hp9mjxp,1640075885.0,True
rjvbrs,"Just out of curiosity, what books did you read? Perhaps I can point you to something you're already familiar with.",hq5aotq,t3_rjvbrs,1640613626.0,False
rjvbrs,My favorite book of all time is [Algorithm Design by Jon Kleinberg and Éva Tardos](https://ict.iitk.ac.in/wp-content/uploads/CS345-Algorithms-II-Algorithm-Design-by-Jon-Kleinberg-Eva-Tardos.pdf),hqdh795,t1_hq5aotq,1640754092.0,True
rj8mre,"the compiler knows the type of the fields, so it knows how many bytes each field takes up and thus how many bytes it needs to skip to get to the next field.",hp1x93o,t3_rj8mre,1639839471.0,False
rj8mre,"> How do structs work internally in memory.

To answer that we need to pick an implementation, as this is implementation defined, it's not something covered by the C spec 

> I know that an instance of a struct is a pointer to the first field of the struct.

Sorry, but this isn't true!


    struct example { 
        int i;
        int j;
    } an_example;

    an_example.j = 0;


There were no pointers involved there.


> I also know that all the fields of a struct are contiguous to each other in memory so the memory address of the second field of a struct can be accessed by adding the size of the first field to the memory address address of the first field.

Again this ain't true. This struct, on gcc x86, defies what you say:

    struct example { 
        char c;
        int i;
    } an_example;

    // Then print (&an_example + offsetof(an_example.i)) Vs (&an_example.c + sizeof(an_example.c))

This is due to padding. For GCC look up the ""packed attribute""

> am failing to understand that how do we access the consequent fields of a struct with just the memory address of the first field.

The compiler knows the size of each member of the struct, and therefore knows where each other field is. So everything you write `an_example.i` the compiler knows to translate that into a specific memory offset .  Look up the `offsetof` macro.


You should try out compiler explorer at godbolt.org",hp2oabh,t3_rj8mre,1639851690.0,False
rj8mre,"> I know that an instance of a struct is a pointer to the first field of the struct.
> Sorry, but this isn't true!

Yeah I was wrong there. The instance of a struct is rather a reference variable to the first field of the struct. Again this might be implementation specific(I am using Golang which is similar to C is a lot of aspects) but when I print the memory address of the struct instance and the memory address of the first field of the same struct instance, they both are the same. 

> The compiler knows the size of each member of the struct, and therefore knows where each other field is.

So it also knows the amount of padding applied by data alignment? Because if it wouldn't know the amount of padding, then it won't be able to locate the data. Just confirming.",hp5aas1,t1_hp2oabh,1639900679.0,True
rj8mre,"> So it also knows the amount of padding applied by data alignment? Because if it wouldn't know the amount of padding, then it won't be able to locate the data. Just confirming.

It's doing the padding, so of course it knows it :)

The bigger program is the programmer/program can't usually find it out in a ""legal"" manner.",hp6afzt,t1_hp5aas1,1639926648.0,False
rj8mre,"Ah it all starts to make sense now. I thank you and all the others who helped me. This kind of computer science stuff which talks about the internal workings of memory, etc. really intrigue me. What field should I study to learn a little bit more about this kinda stuff? My guesses are that I would need to study assembly or compiler design or operating systems to get a taste of this type of stuff.",hp6b6i7,t1_hp6afzt,1639926999.0,True
rj8mre,"Yeah, all of those :)

A book I recommended the other day is [Crafting Interpreters](http://craftinginterpreters.com/). It's very easy to read, unlike some of the classic compiler books. It's also free online.",hp6hkiw,t1_hp6b6i7,1639929892.0,False
rj8mre,"Struct elements are contiguous in memory, so you could theoretically add the size of the first element to the address of the first element and get the second element, but often times there is padding between elements so you have to account for that as well. There are two general rules when counting structs and padding

1) each element must start at a memory address (relative to the first element) that is divisible by the size of the given element. Example:

Char (1 byte) | 7 bytes of padding | pointer (8 bytes)

2) the total size of the struct must be divisible by the size of the largest element in the struct. Add padding at the end of the struct to achieve this. Example:

Pointer (8 bytes) | int (4 bytes) | 4 bytes of padding",hp1xr9j,t3_rj8mre,1639839727.0,False
rj8mre,What is achieved by implementing the second rule?,hp236vr,t1_hp1xr9j,1639842366.0,True
rj8mre,"Someone can feel free to correct me on this/add to it, but it’s my understand it has to do with ensuring that one struct, or better yet any element within a struct, does not end up being stored across two memory blocks",hp25wst,t1_hp236vr,1639843618.0,False
rj8mre,"I am having a hard time understanding how the two rules work. Suppose we have a struct definition called `employee` with fields `firstName`, `lastName` and `age` of data types `string`, `string` and `int`, respectively. Suppose that we create an instance of the struct `employee` and store it in a variable `monica`. The values of the struct would be as follows: `firstName=""Monica""`, `lastName=""Smith""`, `age=33`.

Before I knew these two rules, I would visualize the memory locations of a struct like [this](https://imgur.com/a/JUJgudK). But according to the first rule, the different between the `n-1`th field's first bit and first bit of `n`th element must be divisible by the size of the `n`th element.

So according to that, we might not need the padding in the lastName field because its value(""Smith"") just occupies 5 bytes of data and 5 is divisible by the size of the next field(int-1byte). I strongly think that I am wrong here and the base size of a data type never changes. Please clarify this.

Also, how will the second rule help to ensure that any element of a struct does not end up being stored across two memory blocks.",hp2ahfx,t1_hp25wst,1639845671.0,True
rj8mre,"You make everything more complicated by bringing up strings :) Note that in \`C\`, \`string\` does not exist.

Strings (and other advanced datatype) are not first-class types because they can be of arbitrary size. The string """" takes 1 byte, ""abcd"" takes 5 bytes and so on (don't forget the null byte).

Thus, if you want to put a string into a struct, you either

* say that the string is at most x bytes long. Then you have an array of chars of size x, which has the alignment properties of a char, i.e. alignment 1, which just means no restrictions
* put in a pointer to the string, which then resides somewhere else in memory. Now your struct contains a pointer, with its specific size and alignment properties.

In general, types have a specific size and a specific alignment requirement. For integers and pointers, those are equal. For structs, the size of the sum of the size of its members, while its alignment usually is the largest alignment of any of its members.  


For arrays of type T of length n, they similarly have size ""n \* size of T"", but their alignment is still just that of the type T.",hp2fnny,t1_hp2ahfx,1639847948.0,False
rj8mre,"> For integers and pointers, those are equal.

How is the alignment for pointers and integers equal? The size of an integer is 4 bytes whereas the size of a pointer is 8 bytes, so they have different alignments.",hq4hn37,t1_hp2fnny,1640591641.0,True
rj8mre,"that's not what I meant. I meant that if the size is 4, the alignment also is for these types. If the size is 8, the alignment also is.

Unlike arrays, which can have size 1000 and alignment 1.",hq5076m,t1_hq4hn37,1640606908.0,False
rj8mre,"First let me provide some clarification on the first rule. It’s not exactly the n and n-1 relationship you described. Rather, I like to treat the beginning of the struct as a 0 point, and then ensure that each element begins at an “index” relative to the 0 point that is divisible by the size of the given element. For example, if we had a struct that had an 8 byte element, a 2 byte element, and a 4 byte element, it would look like this:

8 bytes | 2 bytes | 2 bytes of padding | 4 bytes

Note we have 2 bytes of inner padding, because our 4-byte element is now separated from our zero point by 12 bytes (without padding, it would be 10), which is divisible by 4. 

So now let’s revisit how we’re representing a string. Rather than storing the string literal, character by character, a string really just stores the address of a character array (where the array lives gets complicated and can vary, but it can be on the heap, in static initialized, static uninitialized, or perhaps even the stack), where the array represents the string. This way every string occupies 8 byes (since it is a char pointer). 

Knowing that, firstName and lastName are now both 8 byte addresses (char pointers)! Also, remember the age is a int and is 4 bytes, even tho the value is 33. So, here is our raw struct 

firstName | lastName |    age
8 bytes     | 8 bytes    | 4 bytes

Do we need any padding? The first element looks good, it’s just 8 byes. The second element is also 8 bytes, and the difference between the zero point is 8—all good. Our third element, age, seems to be good as well, it’s separated by 16 bytes from our zero point, which is divisive by 4. Now do we need any padding on the end? 

Total struct size = 8 + 8 + 4 = 20

20 is not divisible by 8! So we need 4 bytes of padding, to get our total struct size up to 24, which is divisible by 8. Here is our final struct:

firstName | lastName |    age    | padding
8 bytes     | 8 bytes    | 4 bytes | 4 bytes 

Total struct size = 24

Now, why does the second rule ensure we don’t have one element of a struct stretched across two blocks? Here is an example: lets say we have a struct that an 8 byte element and a 1 byte element:

|       8 bytes       | 1 byte

Now, lets say we want to make an array of structs. Each struct is 8 bytes in size, and arrays are stored contiguously in memory, so we have 

8+1 bytes | 8+1 bytes | 8+1 bytes | etc…


If our array is large enough we will eventually approach the end of a block. Will a block size ever be divisible by 9? Like, almost definitely not. So you’ll get the end of the block with less than 9 bytes of space left, and the struct will get chopped!",hp2gang,t1_hp2ahfx,1639848228.0,False
rj8mre,"By 'block' do you mean physical segments of memory? A memory block generally means a contiguous block of memory. How large can a 'block' of memory be?

PS: sorry for asking this question 8 days after your comment",hq13by7,t1_hp2gang,1640530760.0,True
rj8mre,"Basically you need to be able to uniformly step through the struct fields with limited knowledge of the contents of a field. All data is a byte sequence with a pre-defined interpretation.
  
E.g. C-strings are terminated with a *null byte*.  
  
M,o,n,i,c,a,\0,? 
 
\0 is the null byte, ? is an undefined byte value which could be any valid byte as it is not part of the string. 
 
A number like 33 would be stored differently than the string “33”.  
  
3,3,\0,? 
^ string version  
  
00000000 00000000 00000000 00100001  
^ 33 as a 32-bit/4-byte integer",hp2dbi2,t1_hp2ahfx,1639846920.0,False
rj8mre,"In general, data often needs to be aligned properly. A 64-bit pointer takes 8 bytes, and it should be placed at an address divisible by 8. If you try to do otherwise, you have a ""misaligned"" value, and reading such a value may be slow, or the processor might just not support it and your program will crash.

&#x200B;

Now, if you have such an 8-byte-sized value in your struct, you add padding to ensure that it's offset within the struct (i.e. what you have to add to the pointer to the first element to get a pointer to the element you want) is divisible by 8.

However, we actually wanted to have its actual address, i.e. the value given by (address of the first member + offset) to be divisible by 8. To do this, we additionally require that (address of the first member) is divisble by 8. This means that our struct now has an alignment of 8.",hp2f7tm,t1_hp236vr,1639847758.0,False
rj8mre,"> To do this, we additionally require that (address of the first member) is divisble by 8.

Could you please explain why?",hpfpnt8,t1_hp2f7tm,1640101953.0,True
rj8mre,"It's just basic divisbility rules.

You want (x+y) to be divisble by 8. We already require x is divisble by 8. What could we additionally require so that the whole sum becomes divisble by 8?",hpg8sga,t1_hpfpnt8,1640109605.0,False
rj8mre,"> What could we additionally require so that the whole sum becomes divisble by 8?

We want `y` to be divisible by 8. I get it now, thanks a ton!",hpjmfw3,t1_hpg8sga,1640172075.0,True
rj8mre,"Suppose we have a struct like this:
```
struct dog {
   int age; // 0x00 to 0x03
   // 0x04 to 0x07 - padding
   person* owner; // 0x08 to 0x015
   int score; // 0x16 to 0x19
   int lifespan; // 0x20 to 0x24
} bruno;
```
The first three fields are stored at a memory address divisible by 8, whereas `lifespan`'s memory address is not divisible by 8, then how can we say that the struct has an alignment of 8?",hq8y5rf,t1_hp2f7tm,1640672502.0,True
rj8mre,The struct will only ever be placed at addresses divisible by 8. This does not mean that all of its members also will.,hq9bwne,t1_hq8y5rf,1640682890.0,False
rj8mre,"It's called ""memory alignment"". The simplest answer is because the CPUs demand it be that way 

E.g. early arm could only load and store on 32bit boundaries. But an x86 could do 1byte increments. 

It also allows for more efficient indexing operations, but at the expense of padding. You can usually choose the trade offs in your compilers option flags",hp2t6dq,t1_hp236vr,1639853817.0,False
rj8mre,Isn't this implementation defined?,hp2sx0p,t1_hp1xr9j,1639853704.0,False
rj8mre,"It works exactly like with arrays, by using the address of the first element and an offset. The individual types don't need to have the same size, since the compiler knows them at compile time.",hp1v6nu,t3_rj8mre,1639838408.0,False
rj8mre,"Strict fields are *not necessarily* contiguous in memory, they might be packed to ensure the fields align on word boundaries.

That said, if you have the type you have all the information you need for field offsets with pointer arithmetic.  

If you have a pointer p to a field f of a struct s, you can calculate the relative offset from s to f by casting 0 (literally 0) to a pointer of type s, accessing field f, then apply & to that. Something like: `&(((struct s *)0)->f)`. You can then subtract this from any pointer to a field f to get its parent s.

This might seem horrid but it’s how linked lists are implemented in the Linux kernel and has the advantage (over a traditional linked list implementation) that you don’t need to cast the list payload every access, which is error prone.",hp5b9js,t3_rj8mre,1639901460.0,False
rj8mre,"A well thought out and carefully constructed comment.

&#x200B;

It's very clear that you wield C in a professional capacity!",hp6xtb0,t1_hp5b9js,1639936707.0,False
rj8mre,You could look it up in memory,hp1ua1c,t3_rj8mre,1639837932.0,False
rj8mre,I can't look it in memory that how does the compiler navigate through the struct.,hp1uwgw,t1_hp1ua1c,1639838259.0,True
rj8mre,You literally gave the compiler the definition of the struct.  It knows the size of each member and the padding needed to byte align.,hp1x30m,t1_hp1uwgw,1639839385.0,False
rj8mre,"If what the other commenter says is true, you could get an intPtr to the struct, then increment it by something like 
     
    sizeOf(typeOf(myStruct.firstElement)))",hp1vo1m,t1_hp1uwgw,1639838662.0,False
rj8mre,"Ah okay, I kinda get it now. So the compiler navigates through the struct by just adding the size of the first field to the memory address of the first field. Is my interpretation right? just confirming.",hp1x7nv,t1_hp1vo1m,1639839450.0,True
rj8mre,"Not exactly, as that does not take padding into account. The compiler determines the layout of the struct. Based on this, it knows the offset of every member from the start of the struct. Computing the address of each field becomes (p + n) where p is the address of the struct, and n is the offset of that field from the start of the struct. n is known at compile time and is a constant offset.",hp3ej4n,t1_hp1x7nv,1639863584.0,False
rj8mre,No I’m afraid not because there might be padding between that element and the next one.,hp5bhv2,t1_hp1x7nv,1639901650.0,False
rj8mre,"At some level, yes, that has to be it. As far as the implementation details they probably have a bunch of optimizations that would make it complicated to parse.",hp1yuqf,t1_hp1x7nv,1639840279.0,False
rj7x0t,"In order of showing this function is big theta of f(n) [where n is the number of items], you want to show its both O(f(n)) and Omega(f(n)).

Now in order of finding the needed f(n) its mostly comes down to understand what the code does. After you figure what f is (using simply your intution) you need to prove both O(f(n)) and Omega(f(n)).

Lets denote the number of operatios of your function as g(n), you now need to find constants n_0 and n_1, c_0 and c_1 such that:

g(n) < c_0 f(n) for n> n_0 and

g(n) > c_1 f(n) for n > n_1.

And by that the proof is done. Sorry if my formating is bad, im on phone",hp1qrl1,t3_rj7x0t,1639836037.0,False
rj7x0t,"Amazing, thanks dude! Really appreciate the help :D",hp6rjsf,t1_hp1qrl1,1639934142.0,True
rj7x0t,"It's hard to answer this question without knowing what you currently know about big-O, big-Omega and big-Theta notation.

First, you should identify what the ""problem size"" is in this context. It looks like the input of your algorithm consists of a list of lists. Candidates for the problem size here could be:

* The total number of items.
* The number of lists in the `lists` object.
* Both of the above.

The first option is probably the most applicable here, so let's say `n = total number of items`.
For your sample input, `n = 6`.

To find the worst case runtime of this algorithm to be Θ(f(n)), you want to show that a worst case instance of the algorithm will:

* complete in **at most** `constant * f(n)` steps (which shows the algorithm runs in O(f(n)) time), *and*
* requires **at least** `constant * f(n)` steps to complete (which shows the algorithm runs in Ω(f(n)) time).

The `constant`s in the above two conditions need not be the same constant, but they must both be independent of `n`.

So first, you should consider what is a worst case instance for your algorithm. Hint: you've already given a worst case instance for `n = 6`. Then you should prove the two statements for some function `f`.

To determine the function `f `, ask yourself this: if the number of items were to double, how much longer would this code run? And then, can you think of a function `f(n)` with this property?",hp232lx,t3_rj7x0t,1639842310.0,False
rj7x0t,"this is a linear search of a 2d array. if the array is m by n then on average you will search through half the list to find it which would be m\*n/2 which is just theta(m\*n).

&#x200B;

also if you want to shit your pants with the technical details then you can guarantee that there is some constant to multiply m\*n by that will be asymptotically less and some constant that would be asymptotically greater. you can guarantee this is true because i said so and i offer a 30 day money back guarantee.",hp5noj7,t3_rj7x0t,1639911970.0,False
rj7x0t,Theta(1) because it only performs a constant number of operations.,hp3eifx,t3_rj7x0t,1639863574.0,False
rj7x0t,lmao dude obviously he meant if you scale by the list size,hp5nhdv,t1_hp3eifx,1639911806.0,False
rj7x0t,"If I asked you how many operations your code makes before it finishes, what answer would you give? Just curious, and it might reveal the best path towards understanding the answer to the question you gave.",hp4a0nw,t3_rj7x0t,1639879187.0,False
riwlmc,That’s super cool!,hp47tm0,t3_riwlmc,1639878091.0,False
risxvd,"Server software is specifically developed and tested for long uptime (because that is a relevant use case). Other software, for example League of Legends, is not tested for long uptime - if the Client crashes after 24h, nobody really cares (at most, they put in a 12h restart warning/force).",hozlh61,t3_risxvd,1639786416.0,False
risxvd,If you were to use a server as a desktop you would run into the same issues.,hozj8re,t3_risxvd,1639785415.0,False
risxvd,"Server hardware runs to varying degrees:

* climate controlled environment
* multiple power feeds, battery backup, generators
* error correcting memory
* redundant drives with redundant drive controllers
* multiple high volume fans
* professional maintenance
* designed for purpose software stack
* active monitoring of component health
* solid metal chassis properly grounded

Laptop: plastic foldy boy that's filled with dog hair and viruses",hp0wr8r,t3_risxvd,1639812772.0,False
risxvd,especially when you see those repair videos with like cockroaches coming out the vents it’s no surprise they don’t run well,hp1n50x,t1_hp0wr8r,1639833912.0,False
risxvd,"Non-server software often has memory leaks.  Web browsers are especially notorious for leaking memory.  Fixing memory leaks is tough, so often companies don't put resources towards fixing them.   Far easier to have your users restart the app, which is not an option on servers.",hp00koq,t3_risxvd,1639793428.0,False
risxvd,Servers have to be rebooted more often than you think. But high availability configurations make it passible to reboot a single server without the service going down.,hp14jsd,t3_risxvd,1639819263.0,False
risxvd,"A server not need rebooted except for upgrade the kernel or change some failed hardware.

Nobody care if a server is running for months or years without reboot.

Even those of us who have servers at home avoid reboots because we like see high uptimes. 

We never reboot the servers in my company, except for kernel upgrade.

PS:i known its possible upgrade the kernel without reboot",hp1do1i,t1_hp14jsd,1639827073.0,False
risxvd,"Just to add to what’s already been said, it *is* possible to keep a desktop computer running for weeks or months without noticeable performance degradation - is all about what OS and software you run on it.

For example, my last three Macbooks would run 6-9 months between reboots, and my work Windows laptop is the first Windows machine I’ve owned which could almost match that, probably due to memory management improvements in Windows 10.",hp124if,t3_risxvd,1639817177.0,False
risxvd,"I usually reboot my desktop computer maybe once a quarter on average, though I should probably do it more often. Old habits, I guess. (Running Linux, Ubuntu to be specific.)

Server uptime was more of a thing sysadmins bragged about in the past. Today if someone says a server's been going without a reboot for two years I just think ""yikes, gotta be some unpatched security holes in that kernel"". Most server software supports running across multiple instances today, so usually you can take servers down for maintenance/upgrades without anyone noticing.",hp1dkpm,t3_risxvd,1639826999.0,False
risxvd,"software in computers at less tested and  leaks more memory

Software in servers its better done and reviewed.

what is a memory leak?

well programs request to kernel memory space, when finish they release it and the kernel can offer that space again.

if a app crash or the app code forget release the memory.. we would hava a memory leak. Kernel not going to offer that memory because assume it in use by other app.",hp1dycz,t3_risxvd,1639827306.0,False
risxvd,"It's the software, the server's softwares are so much efficient in terms of memory, but a normal pc is not, piling up along the way. If you know what you are doing you can get decent up time without being lagging. I was able to run my pc for 12 days straight with little to no loss in performance.",hozieyj,t3_risxvd,1639785039.0,False
risxvd,It's all your electron apps slowly consuming your available memory (and memory leaks and other bugs in your desktop software that aren't present in server software),hoztsot,t3_risxvd,1639790242.0,False
risxvd,It's pretty much entirely the software.,hp09fb6,t3_risxvd,1639797933.0,False
rip3jw,"What is your question, precisely?",hoytzez,t3_rip3jw,1639774772.0,False
rip3jw,sees like only problem of the imaginary program P that can solve halting problem is self reference( i think that is where loops come from in my original post).{can’t we just say program P will solve halting problem for all programs except itself* .what is wrong in saying that instead of just saying since loops can’t exist this program won’t work outside self-referencing as well and no such program exists.,hoyvglr,t1_hoytzez,1639775363.0,True
rip3jw,"Well I think the idea was to solve the halting problem, and no program can solve it completely, hard time understanding your point here.",hoyz3y0,t1_hoyvglr,1639776848.0,False
rip3jw,"I know, I don’t quite know how to put it. my assertion is that,as said above, the only proof given is refusal of any halting solution is that it breaks down in self referencing. How does that say anything about literally infinite other programs that our supposed solution will work on. I mean all we have done is refute a very specific program. And we still aren’t sure about others. So can’t we say there may or may not be a partial solution( partial because self reference is not permitted) instead of declaring outrightly that nothing of the sort exists.",hoz0n9k,t1_hoyz3y0,1639777473.0,True
rip3jw,"It says that your program will never be 100% correct.

More precisely, it raises the question of how your attempt at solving the halting problem would cope when fed itself.",hoz6uqx,t1_hoz0n9k,1639780038.0,False
rip3jw,">  I mean all we have done is refute a very specific program. And we still aren’t sure about others.

Sure. The proof does not identify any specific program that every attempt at halting analysis will get wrong. There is no such program.

We *know* there are partial solutions. It's easy to write an analyzer that gives the right answer for some programs, though it will give the wrong answer (or no answer) for other programs. You can also pick any program you like and write an analyzer that gives the right answer for that specific program (either `print(""yes"")` or `print(""no"")` will be a suitable answer, though it might take you a lot of thinking to figure out which one).",hozouxl,t1_hoz0n9k,1639787956.0,False
rip3jw,"I've thought about the same thing, and I actually believe that's the case.

We do know that if you don't allow recursion in any way, then it's completely decidable.  Thus, it's something about recursion that causes the issue.

However, recursion is _not_ sufficient to cause undecidability.  It's necessary, but some recursive programs are decidable.

My theory that has yet to be disproven:  if we disallow the known program that causes undecidability, it becomes decidable.  It can't decide on itself, but it can on anything else.",hp02zii,t1_hoyvglr,1639794582.0,False
rinkr5,">If data loss will occur, tell me the limit for both. With and without data loss.

I suspect the answer to your polite question regarding lossy compression is that you compress 1TB to one eighth of a byte.

Always happy to help!",hoy78ym,t3_rinkr5,1639765800.0,False
rinkr5,i’m sure it would still be readable after the decompress,hp17zvh,t1_hoy78ym,1639822262.0,False
rinkr5,"If it weren’t decompressible, then it wouldn’t be compression. That would just be straight up data loss.",hp29zac,t1_hp17zvh,1639845445.0,False
rinkr5,maybe that’s were the world of data compression is heading just straight up data loss,hp2aeri,t1_hp29zac,1639845639.0,False
rinkr5,Compression depends on the contents. A 1TB of 0 could arguably be encoded as 1 bit of 0 and 40 bits to store length.,hoy7eyn,t3_rinkr5,1639765863.0,False
rinkr5,Or 1 bit with a zero and always assume a 1TB length,hozkf2h,t1_hoy7eyn,1639785940.0,False
rinkr5,Or 0 bits if your compression algorithm assumes an empty file to be exactly 1 TB of 0's.,hozze1s,t1_hozkf2h,1639792868.0,False
rinkr5,This guy algorithms.,hp08y8p,t1_hozze1s,1639797683.0,False
rinkr5,This guy assumes.,hp0fal1,t1_hozze1s,1639801120.0,False
rinkr5,"Depends on the data. If it's a random stream of zeroes and ones, it cannot be compressed at all. If it's a terabyte of all zeroes, it could be compressed by 99.9%.

Compression looks for patterns and duplication, so if there are no patterns, there is nothing to compress. Different techniques are used to compress text, images, videos, and audio streams.",hoy80su,t3_rinkr5,1639766092.0,False
rinkr5,"Everything has a pattern even if the file itself is the pattern, but a “unique” file cannot be compressed much at all. What’s important is whether a pattern **repeats**.",hp29bsy,t1_hoy80su,1639845157.0,False
rinkr5,Depends on the data in it.,hoy7gfj,t3_rinkr5,1639765878.0,False
rinkr5,"This. Generally, data compression algorithms rely on knowledge of what is being stored. For example, in images, in order to make the JPEG format, there was a bit of research done on quantizing photos in the frequency/cosine domain. They use a different quantization coefficients for each frequency in an 8\*8 block as it makes the image look similar enough even though the MSE  compared to the original image actually increases.",hozhfd0,t1_hoy7gfj,1639784591.0,False
rinkr5,"Every data can be compressed to a single bit, that just holds the information if it is the compressed data or not. All other information than has to be in the decoder. That was always the case, in 2000 as well as in 2010.

For more information you have to get into information theory.",hoyih8t,t3_rinkr5,1639770144.0,False
rinkr5,"While I'm certainly partial to CS theory, and information theory and compressibility are definitely interesting, I think your answer goes a bit too far into the technically correct but almost entirely unhelpful territory.

OP's question doesn't really give enough information to give the answer they're asking for but information theory is hardly going to be helpful to a layperson unless they're willing to go down an entire rabbit hole to answer their question.

So, this answer kind of gets both an upvote and a downvote, mentally.",hoyytwd,t1_hoyih8t,1639776736.0,False
rinkr5,"Thanks. You managed to perfectly verbalize my intension while writing my comment.

Edit: For lossless compression: Compression is very well studied. So there is no practical differences between the algorithms now, 10 or 20 years ago. We basically already got the optimal solution. By how much you can compress data depends entirely on the self-information or entropy of the data. This is well defined and gives an lower bound to the compressed data size. Modern (>= year 2000) algorithm achieve near optimal compression for ""real world"" data.",hoz5gud,t1_hoyytwd,1639779458.0,False
rinkr5,I wonder why you think that compression is solved. There is still a lot of research going on in the field. Just because the theory given a specific information source is done this doesn't mean that the application is straight forward. The lossless compression theorem really is only the starting point. Applying this to the vast probability spaces of for example image data relevant to humans is the difficult part.,hp1q583,t1_hoz5gud,1639835688.0,False
rinkr5,"Regarding images: JPEG2000 was (as the name suggests) developed in the year 2000. It was developed to be the successor of JPEG, but we still use JPEG. There are many improvements, but they just are not requested, because the already available algorithm are good enough for all use cases and the improvemnts are not big enough.",hpg3wc9,t1_hp1q583,1640107712.0,False
rinkr5,"It really, really depends on what you're compressing. It's impossible to give a number without some kind of an idea of what the data are, and what the shares of different kinds of files and formats are.

Photos and videos can hardly be compressed further using lossless, general-purpose compression. That's because most common image and video formats already employ compression, often both [lossy](https://en.wikipedia.org/wiki/Lossy_compression) and [lossless](https://en.wikipedia.org/wiki/Lossless_compression) compression, and applying compression on top of compression generally doesn't give you much. There might be some slack to be picked up by state-of-the-art lossless compression but not a lot. I'd expect almost no gain from compressing already compressed image or video files.

Word documents and other documents from the modern Office suite are also already compressed, and while the compression used in the file format may not be bleeding-edge and you can probably compress it a little more with a better compression algorithm, it's probably not going to be that much.

Most video games that can take a lot of space also come with the majority of their assets compressed nowadays.

Plain text compresses fairly well, but few people have plain text files taking a lot of disk space. Uncompressed image files may also compress well, depending on the image, but few image formats that are commonly used today are uncompressed.

All in all, if there's a lot to be gained by compressing the data, it's probably already compressed.

With that said, my main backup drive that doesn't include operating system or program files, and excludes most video files, has ~130 GB compressed into ~103 GB.

If you're asking with a more practical matter in mind, why not give your practical scenario instead?",hoyukfc,t3_rinkr5,1639775005.0,False
rinkr5,"Theoretically, not depending on the content, nearly 0: https://github.com/ajeetdsouza/pifs",hozk09n,t3_rinkr5,1639785757.0,False
rinkr5,[Shannon's source coding theorem](https://en.m.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) has the answers you seek.,hp0bkje,t3_rinkr5,1639799079.0,False
rinkr5,"What is in the file?  All zeros... the compression will be awesome.  

If it is images or movies, then not so much",hp0joef,t3_rinkr5,1639803658.0,False
rinkr5,"I'm not an expert on data compression but I believe it depends on the contents of the file. How I remember compressions algorithms working is you look for repeating sequences in the file and then substitute those for smaller sequences and have some sort of table to record the switches.

So it your file is 000**111**000**111***0101* you can break that up into 000, **111**, *0101* and say

000 -> 0

111 -> 1

0101 -> 10

So your new file would be: 010110 (plus the overhead for the table)

So if your entire 1 TB file was all 0s you could compress the shit out of it whereas the less repetitions the less it could be compressed.",hoy832a,t3_rinkr5,1639766115.0,False
rinkr5,"Your scheme is broken.  
  
000**111**000**111***0101*  
  
would convert to: 010110, but

010110  
  
would convert to: 000111000111111000, because you don’t know that “10” isn’t a “1” followed by a “0”.  
  
The issue is that you haven’t specified that a “1” cannot be followed by a second “1” and you cannot distinguish a sequence like this:  
  
10  
  
Is it 111000 or 0101 ?  
 
——  
  
A better system might use two bits of starting data
  
00, 01, 10, 11  
  
You’d still have to be careful about anything with an odd number of bits and come up with a way to encode the original data in fewer bits.",hp28q1w,t1_hoy832a,1639844890.0,False
rinkr5,You're right. It's not an actual algorithm; it's to convey the idea of substituting smaller bits for larger sequence of bits,hp2ku83,t1_hp28q1w,1639850205.0,False
rinkr5,Entropy,hoyxg5f,t3_rinkr5,1639776175.0,False
rinkr5,Depends on how random the sequence of bits are. If I had to guess the bound would be log(n) as in the case of a file that stores all 0 bits we could just write the number of bytes.,hp03gmo,t3_rinkr5,1639794827.0,False
rinkr5,"The lossless compression limit depends on the randomness of bits in your data. Let me explain what I mean by randomness of bits. Something is random if no patterns cannot be found in the data. For example, `11111111` is not at all random, it has a pattern which is ""every bit is 1"".

> If the data is redundant, meaning that it has no patterns but a pattern of uniformity, it conveys no information because it has no order. This relation can be stated as follows:
Randomness ∝ predictability ∝ information ∝ order ∝ regularity of data

^ An extract from a paper I wrote about randomness but never published it.

If you had photos with all the pixels of same values. You could compress it to about `1/resolution + bits required to store the length of resolution` size.

Compression is just representing a relatively large amount of data in a concise form. For example `11111111` will be compressed as ""1 for 0100 bits"". If it is a random(explaining what is random, would make the comment humongous so leave it out) pattern, you cannot compress it because you cannot express something random in some other form because you do not know anything about its pattern.

So, now answering all your questions one by one:

> Does anyone know how far a one terabyte file can be compressed? What’s the limit of today’s technology compared to 2000 and 2010?

Not much really because the compression algorithms which are the norms haven't changed much since 2000s, we still use JPEG widely which was originally developed in the late 1980s. We are still using H264 which was originally developed in 2003. H265 was developed in 2013. So compression hasn't changed much in the last 10-20 years.

You can compress an image without loss of data to about 10% with JPEG encoding, but again this is based on the randomness of the pixel values.

> If one terabyte holds 1,000,000,000,000 bytes, what is the utmost limit of compression?

The utmost limit of compression will depend on the data in that 1 TB of data as explained earlier in the comment.

> If data loss will occur, tell me the limit for both. With and without data loss

Well, its in your hands whether data loss will occur or not. You can compress an image to like 50% its size with loss of data but usually the loss of data is not traded for high compressibility. After a compression constant `k`, the compression is directly proportional to the loss of data.",hp0kfoj,t3_rinkr5,1639804104.0,False
rinkr5,"Have you ever heard of 42.zip?

It is a zip bomb, that compressed is only 42kB in size. Fully extracted it is 4.5PB",hp0v372,t3_rinkr5,1639811438.0,False
rinkr5,"Yikes.  
  
Pretty sure those are special *synthetic* sequences that look like a near infinite set of files, though…",hp2aoyr,t1_hp0v372,1639845764.0,False
rinkr5,"It depends on the algorithm you use.  
  
The simplest forms of compression work best on data with lots of *repeated* byte sequences, whereas more sophisticated ones may look to identify patterns that can be converted into a base point and a mathematical function.",hp27tpe,t3_rinkr5,1639844485.0,False
rinkr5,"Image and video encodings already have compression built-in. It's not optimal since people don't want to wait two seconds to decompress one second worth of video, so you can probably do a bit better. If you have an equal number of txt, jpg, and mp4 files, the videos would dominate the storage.",hoynxsl,t3_rinkr5,1639772336.0,False
rinkr5,"Strictly speaking that’s *encoding* rather than *compression*, but it does reduce the storage space needed.  
  
You can potentially stlill compress the result, also.",hp2aw45,t1_hoynxsl,1639845852.0,False
rinkr5,"For music, lossless compression is about 50%.  Lossy can do more depending on how much you want to notice the compression. Video is likely similar. Text based compression depends totally on the text. If a lot of repeated character patterns exist, you can achieve a lot of lossless compression. Random characters get very little compression.",hozg1oe,t3_rinkr5,1639783950.0,False
rinkr5,"1000000000000 of the same content

log2(1000000000000) < 40

so you need 40 bits of multiplier and 1 bit of the data.

or up to 99.9999999959%  for single time compressing.",hozy7ez,t3_rinkr5,1639792308.0,False
rinkr5,"http://mattmahoney.net/dc/

Probably the best resource on the web I've seen for this.",hp12k6u,t3_rinkr5,1639817548.0,False
rihkz0,"Before you dive into software architecture, I'd recommend working on becoming a great software developer first. The book gets a lot of slack, but for young developers I still recommend Clean Code by Bob Martin. Once you've completed (or better yet, while!) some introductory background reading, start implementing the principles discussed in a language you're comfortable with. Small, simple examples with easy to understand conventions will go a long way. Next, take those principles and use them in an existing architecture - MVC is a great one for Java.

From there, it's good to start working on how you organize your software. For this I like to recommend A Philosophy of Software Design (2018) by John Ousterhout for a supplemental approach to the ""historical knowledge"" as well as studying design patterns. [Game Programming Patterns](https://gameprogrammingpatterns.com/contents.html) provides a great overview of some of the most common design patterns you'll see in software through an application that's very approachable through most - game design. Design patterns are great to know because if you can employ and identify them properly, it will be easier for others to understand what your code is doing and vice versa. This is also the point where you start incorporating those principles you learned above into larger projects. 

Finally, to answer your exact question of ""how to understand large and scalable projects,"" [The Architecture of Open Source Applications](http://aosabook.org/en/index.html) is what you're looking for. Practice taking a problem, designing a solution for it, and comparing against what the author implemented. But work on the above first. 

This is a lot of information, so save this post and come back to it. Software development is a lifelong and ever changing endeavour, so enjoy the journey!",hoxgzd8,t3_rihkz0,1639755753.0,False
rihkz0,Awesome! Thank you for the detailed response. I've been doing a lot of JFX work at my job recently so I've been interested in building my own software on free time and the hardest thing I'm finding is how to start and not have to restart later on due to design. MVC I'm already familiar with bur always interested in diving deeper and learning more! Thanks again.,hoxhdpv,t1_hoxgzd8,1639755908.0,True
rihkz0,"I don't want to sound trite or demeaning but architects  are made from experience and mentorship. there are lots of books about architecture and you should read many of them. Ultimately architecture is balancing tradeoffs, technological, resources and political.  Sometimes it's winning hearts and minds, sometimes it's hearing cats , sometimes it's leading brilliant people who are honestly probably smarter than you.",hoxohf9,t3_rihkz0,1639758635.0,False
rihkz0,"Fundamentals of Software Architecture
Software Architecture: The Hard Parts

By Neal Ford and Mark Richards.  And their bi-weekly conversation at https://www.developertoarchitect.com/foundations-friday-forum.html 

Solid advice in those two books.  And echoing the thought that architecture is a skill developed by experience, they hold a twice-yearly “Kata contest” where they present an architectural problem and have teams compete over the course of several weeks to produce and document the best architecture.  I’m biased, because I’m one of 5 judges for that contest.",hoxv985,t3_rihkz0,1639761191.0,False
rihkz0,"This is very interesting! I actually have both these books on my reading list and can't wait to get around to them. 

Do you mind giving more details on how the Kata is judged? I'd be interesting in taking part.",hozding,t1_hoxv985,1639782843.0,False
rihkz0,Honestly you don't learn those from books. You learn from experience... Just actually do projects from smaller ones to bigger ones.,hox9bdw,t3_rihkz0,1639752643.0,False
rihkz0,"Learn to design in UML and maintain documentation for your software, free code camp has an excellent UML intro on YouTube.",hoxu64d,t3_rihkz0,1639760779.0,False
ri6duj,"O(n^2), as you suspected, assuming no ordering on the array's elements. That's quadratic time; exponential should be O(2^n) or similar. Check the difference for n = 10, 20, 30.

If the array is ordered, I think that a O(n) algorithm is possible: map b_i = target - a_i, reverse b, then compare a and b side-by-side until the elements match.",hov5oyf,t3_ri6duj,1639708494.0,False
ri6duj,"Oh, yea sorry I meant quadratic, not exponential.

That makes sense. In this case, the array wasn't ordered.",hov6o73,t1_hov5oyf,1639708933.0,True
ri6duj,"The commenter on YouTube might have been tricked into thinking it's in O(n) because the inner loop runs only for the remainder of the array after the *i*th element rather than always going through the entire array, and that remainder gets shorter for every run of the outer loop.

However, the inner loop still makes n-1 iterations on the first iteration of the outer loop (when i=0), and one iteration on its last run (when i=len(arr)-1), leading to an average of circa n/2 iterations of the inner loop per iteration of the outer loop. Since the outer loop is executed n times, that leads to a total of approximately n * n/2 iterations of the inner loop, which is in O( n^2 ), not in O(n).

That's in the worst case, of course; if matching elements are found, the algorithm stops before going through all of that.

There's probably an off-by-one or some other inaccuracy somewhere in the above, but the big picture should still be right.",howp4yq,t3_ri6duj,1639742595.0,False
ri2yda,"Yes, it's trivial. The vulnerability is well over a decade old.",houjfs2,t3_ri2yda,1639698486.0,False
ri2yda,Could you please elaborate how? I have seen references to this online but I can't find a clear explanation of how the ciphertexts can be modified/swapped.,houjv9a,t1_houjfs2,1639698675.0,True
ri2yda,Well it's not swapped per se. You de-auth attack and collect the IVs by the thousands. it's been a while so I apologize going off memory. Like I said old attack. Check out a tool call Air-crack-ng if it's still around.,houm13o,t1_houjv9a,1639699638.0,False
ri2yda,[deleted],houq2oj,t1_houm13o,1639701488.0,False
ri2yda,[deleted],houqc5m,t1_houq2oj,1639701608.0,False
ri2yda,Yep WEP is trivially cracked. SSL also has it weaknesses in older versions but as long as the TLS is higher than 1.0 and SSL is higher than 3.0  it's pretty pretty good shape :),houry45,t1_houqc5m,1639702329.0,False
ri2yda,"Here's a good pdf with all the information you need to understand why WEP is broken. 

https://www.opus1.com/www/whitepapers/whatswrongwithwep.pdf",hovw5vv,t1_houjv9a,1639721710.0,False
ri2yda,Fond memories of moving into a new flat and borrowing neighbour’s WEP-protected WiFi until my broadband was activated,hoydzmd,t3_ri2yda,1639768380.0,False
ri2yda,There was a lot of hub bub about that song but I didn't really find it that vulgar.,hovvzyn,t3_ri2yda,1639721604.0,False
ri2yda,You do realise WEP is shit/old/insecure and hasn't been recommended for over a decade now?,howf8gs,t3_ri2yda,1639735542.0,False
ri2yda,"Yes, I'm learning about all the different wireless security protocols",howfh6x,t1_howf8gs,1639735733.0,True
ri2yda,"Ah, fair enough.",howi8sz,t1_howfh6x,1639737834.0,False
rhnc1k,Every element in the domain of a function must map to something in the function’s range. The is basically the definition of domain. If there was an element x that didn’t have a defined value f(x) then by definition the domain of f actually cannot contain x.,hori60v,t3_rhnc1k,1639649250.0,False
rhnc1k,thanks,horj4ds,t1_hori60v,1639650044.0,True
rhnc1k,"To expand a bit of this if a given function `f` is defined for some subset of `X`, let's say `X'` (so it is a a function `f: X' -> Y`) then in regards to `X` it is also called a ""partial function"". The partiallity comes exactly from the fact that it does not cover all of `X`. However it still obeys the rest of the laws if you look at functions as relations.

Surjectivity is not partiality however. Let's reset and say that `f: X -> Y` is a function covering all of `X`. Surjectivity asks ""is there an `x:X` for every `y:Y` such that `f(x)=y`""

More succinct: [;\forall y \in Y \exists x \in X f(x)=y;] 

Surjectivity can be expanded to partial funciton, this is normally done by disregarding the partiallity (so the cases where `f` doesn't map to anything) and then asking if that restricted function is surjective. In your example, if `Y = {y1, y2}` then even the partial function is surjective.",hos9g6l,t1_horj4ds,1639665691.0,False
rhll1c,"How many bytes a character has depends on the character table you use. Some examples: ASCII (1byte per char), UTF-8, UTF-16, the thing that microsoft uses.

Of course, a space is a character itself. Also a backspace or enter is.

Any you are right, for all praktical cases 1 byte equals 8 bit.",hor7nbs,t3_rhll1c,1639640716.0,False
rhll1c,"Out of curiosity, do you know what character table is used within text messages? Also, do you know what character table is used within email messages? I’m trying to understand if both of these messages carry the same data if they both sent the same message. Let’s say that you messaged, “Hello there!” You emailed and texted this exact message. Would they both be sending the exact same byte size? If not, what’s the difference. 

Also, do spaces count as a character and or byte? 

Lastly, you said enter counts as a byte? Meaning, if you were to send a message that was 36 bytes and the limit was 36, you wouldn’t be able to send the message because the enter counts as a byte?",hor805a,t1_hor7nbs,1639640980.0,True
rhll1c,"Email usually uses UTF-8. Like 99% of all web applications. I would guess that most messenger also uses UTF-8, maybe also UTF-16 scince this supports for example chinese character.",horbeki,t1_hor805a,1639643631.0,False
rhll1c,"Just to clarify, UTF-8 and UTF-16 (and UTF-32) are all different ways of encoding the same characters and all sort the same set of characters (eg Chinese). For most common use cases, UTF-8 encodes a message in the fewest bytes out of them but requires the most processing to handle non-ASCII characters. UTF-32 required the last processing but uses four times as much memory as UTF-8 for ASCII characters.",hotzndh,t1_horbeki,1639690188.0,False
rhll1c,"To answer the last part of your question a bit more in depth.

If you are using a common messenger, you typically press enter to send the message, and if you want a new line you have to press shift+enter. In a email, you can just press enter to get a new line and have to klick on a button to send the email.

This tells us, that a messenger captures the enter key and uses it as a ""command"" to send the text, while the email programm captures the enter key and uses it to append a new line to the text.

So the same key can do different things on different programms. This is because the programm, and not the keyboard, decides what to do with the pressed key. Because of this you also can easily switch between different keyboard layouts without replacing the keyboard.

To answer the question, the enter is not appended to the message. The reciever knows that you send the text because he recieves the text, so there is no need to put an extra indicator at the end of the message.",horj9tq,t1_hor805a,1639650167.0,False
rhll1c,"Edit: There is more to transmitting a message than just the text. What metadata is send highly depends on the way you send it. So I would guess that a email  transmitts more data compared to a messenger.

You can see the data transmitted for an email in almost any email desktop client. There should be a option to ""view source code"".

Edit2: Space count as character. Also newline, tab and all other, so called ""printable characters"". Besides those, there are also ""non-printable caharacters"" like enter or backspace you could theoretically send to someone.

Edit3: If you send a message you do not send the enter. The programm does not append it to the message, the enter is catched by the programm and than the message is send. But you wount be able to append a new line to the message if no bytes where left.",horid8j,t1_hor805a,1639649418.0,False
rhll1c,"There is more to sending an email? Well, let’s say that both the email message and text message contain the exact amount of bytes. In this scenario, I’ll say both contain 36 bytes. If that’s true and the transmission signal adds a certain length of extra data, how great would the difference be? If you know the question, that’d help a lot. 

Also, people are saying you can change the encoding for an email. How would I go about choosing an encoding that is strictly one byte per character? Or is that already the preset encoding software? As for the email, let’s say I’m using the google email or hotmail. 

Space counts as a character? I see. That makes sense. Does a space contain one byte like every other character? Or since it is blank, is it lowered to lesser bits? 

Enter doesn’t count count as a byte? How about as data? Since it’s caught by the program and then sent out, surely it contains wavelength data of a sort. Any idea of the specific amount?

If you create a new line, does the combination of shift+enter equal two bytes if you are utilizing a encoding that equals 8 bits per character. Thus one byte a character. For practical reasons, let’s say that there was only one space skipped on the last line, meaning, it would be 3 bytes now. Right? Or does skipping a line not take enter+shift as a double character command? Merely just a single command since it is utilizing a single command to create a single character. If you know what I mean. That’s for the phone data.

Pertaining to the computer, you said I’d only have to click enter to skip a line. Does the enter count as a byte if you are creating a new line?",hotc918,t1_horid8j,1639681029.0,True
rhll1c,"It is possible to write a message in English that only uses one bye per character. The ASCII, Latin-1 and UTF-8 encodings will all do this. It is not possible to write a message in Chinese that uses one byte per character (assuming an 8-bit byte). This is because one bye can represent 256 Disney numbers. So you can encodes the English alphabet by saying A=65, B=66, C=67 and so on. Then a=97, b=98, c=99 and so on. But there are more than 256 Chinese characters and so it will necessarily take more than one bye per character to encode a message in Chinese.

There are two characters related to new lines in common encodings, called Line Feed and Carriage Return (LF and CR).  These names come from mechanical printers which had to be told separately to move the paper up a line (LF) and to move the print head back to the start of the line (CR).  There are several conventions on how these characters are used to construct new lines in different bits of software. Some use only a CR at the end of each line.  Some use only a LF.  Some use both CR+LF.  CR and LF are both part of the ASCII character set and have values 10 and 13.

Every character in the ASCII art takes the same number of bits to represent.

Note that some older email clients used 7-bit ASCII to represent emails. Since you can represent 32 non-printing characters, upper and lowercase letters, numbers and a selection of punctuation in 128 characters, you only need 7 bits to assign a unique number to reach character. Back when sending bits was expensive, it made sense to only send seven bits for each character.",hou2bxw,t1_hotc918,1639691247.0,False
rhll1c,"It is safe to assume that a byte is 8 bits, though in the past that wasn't always the case.

How a character is represented in data depends entirely on its *encoding.*

SMS text messages use either a 7 bit or 16 bit encoding, so either one or two bytes per character.

Email messages can be sent in HTML format, which permits any coding the sender and receiver can both handle.  For example, UTF-8 is one to four bytes per character.",hordd9v,t3_rhll1c,1639645215.0,False
rhll1c,"That wasn't always the case? If that’s true, how many bits used to be a byte? Any idea? Also, do you know what year a byte become 8 bits? 

So, the character H and the Character 7 would equal the same amount of bytes? Which is one or two depending on the encoding? 

SMS is either one or two bytes per character? That’s helpful. That is the biggest question of mine. I’ll have to see if I can find a definitive answer to that. 

An email message can contain a single byte per character? That’s 100% possible? Secondly, does clicking the enter button count as a byte? Let’s say 36 bytes is the max. You type a message with 36 bytes and then click enter, but it doesn’t send because the enter counts as a byte? Is that a thing or no?",hore9kn,t1_hordd9v,1639645975.0,True
rhll1c,"> So, the character H and the Character 7 would equal the same amount of bytes? Which is one or two depending on the encoding?

Some encodings are fixed-length, i.e. all characters are encoded with the same number of bits. This is how many common 8-bit text encodings worked in the past: every character was exactly 8 bits. That also set an obvious limit on the maximum number of unique characters that could be represented, as there are 2^8 = 256 unique combinations of 8 bits that are possible.

Other encodings are variable-length, and a single character can take one or more bytes to represent. The most common text encoding on the web is UTF-8 where every character takes between 1 and 4 bytes. The most common characters in English text (the ones that are included in ASCII, which is an old character encoding standard) take up 8 bits, or one byte. This would include the English alphabet, digits 0 to 9, and a number of other characters, but not many non-English letters. This allows the encoding to be compatible with the old ASCII standard. There are more than a million other characters that are possible in UTF-8, but they can take up two to four bytes per character.

So, even when using a variable-length encoding, H and 7 are still likely to take up the same number of bytes, but H, ü and 国 can take up different numbers of bytes.

I don't know about the text encoding used in SMS specifically, but as u/CarlGustav2 said, it seems like there are two possible encodings, one of which is a fixed-length 7-bit encoding, with the other one being a fixed-length 16-bit one.

Edit: clarified choice of words",host0b2,t1_hore9kn,1639673588.0,False
rhll1c,"> An email message can contain a single byte per character? That’s 100% possible?

Yes, if it's using a fixed-length 8-bit (or 7-bit) encoding. A 7-bit encoding would allow basic English text but no international characters; an 8-bit encoding such as [ISO 8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) would allow some non-English characters but the set of characters would depend on the encoding, as no 8-bit encoding can have enough unique combinations of 8 bits to represent letters used in all languages. (Some languages such as Chinese or Japanese of course have thousands of characters all by themselves, so they wouldn't fit in *any* 8-bit encoding even on their own.)

If you want to know whether an email message consisting of 100 written characters can actually fit in 100 bytes, it's worth noting that email messages also include various control information in so-called headers, including the sender and recipient, the text encoding used, and various other things, so a full email message is actually going to take up more space than that.",hosvs05,t1_hore9kn,1639674660.0,False
rhll1c,"It might help to understand why we need different encodings. Note that a byte (8 bits) can represent 2^8 = 256 different characters. Now this is clearly enough to represent the English language. The most common 1-byte encoding is usually ASCII. Just Google ""ASCII table"" to see the encoding. On the other hand, what if you want to represent almost all languages? You will need more bits to make that work. If you instead use a two byte encoding then you can represent 2^16 = 65,536 characters. Now this is enough to represent pretty much all characters you'll need globally. 

So essentially, if the application you're using allows characters other than English then it's probably using 2-byte encoding. Single byte encoding was mostly used back before the whole world had internet and every language needed to be encoded. 

For your last question, yes, enter is going to be a newline character (or possibly two characters if you're on windows). To you when you click enter it looks like nothing there, but realize that the reason your cursor moves to the next line is because the text editor is showing you the message being typed, and the only way anything changes on the screen is if a new character is entered. This character has a special encoding that the text editor understands, and in turn it pushes the cursor to the next line when it sees this character.",hosc7td,t1_hore9kn,1639666885.0,False
rhll1c,"I could technically set an email message to utilize ASCll? That’s correct, right? 

If enter counts as a byte, does the enter still count if it is clicked to send a message? Similar to the send button. From other comments, people says it is more of a message catcher that sends the data off. Meaning, it isn’t a byte. I haven’t gotten replies if it is still data compressed into the sent message though.",hotfvmk,t1_hosc7td,1639682460.0,True
rhll1c,"It sounds like you have a specific use case in mind. Perhaps if you explained why you want to send emails and texts at 1 byte per char, we might be able to help you better, rather than all he possible byte/character encoding permutations?",hoxcvq0,t3_rhll1c,1639754114.0,False
rhll1c,"Sms messages tend to use GSM 7 bit encoding (https://docs.huihoo.com/symbian/s60-5th-edition-cpp-developers-library-v2.1/GUID-35228542-8C95-4849-A73F-2B4F082F0C44/sdk/doc_source/guide/System-Libraries-subsystem-guide/CharacterConversion/SMSEncodingConverters/SMSEncodingTypes.html), although they can vary.

Emails are even more variable, you can set what encoding you want, but UTF8 is typical as a default. Spaces are indeed characters.

The amount of bytes per character will vary by compression.  In a plain text doc, one character will equate to 1 byte, but in a pdf, one byte will give you something like 3 characters.

Which is to say as so many things in programming,  for all your questions the real answer is it depends, there is not a single correct response.",hordmlp,t3_rhll1c,1639645431.0,False
rhll1c,"So, an IPHONE would utilize GSM 7 bit encoding? Do all phones utilizing GSM 7? Does GSM contain 7 bits per character? I know 8 bits is a byte, meaning, each character would be slightly shorter than the typical byte, right? Are all the characters set at a certain byte/bit length? Like one byte or 7 bits per character? 

For an email, how would you set a certain encoding? For example, how would you choose an encoding that is strictly one byte per character? Where is the option or setting to enable such a thing? Is the default one byte per character? What is typically the default and how many bytes per character is the average? 

A plain document, a character equals one byte? Though, a pdf would equal three characters? Is that one percent true? If it is, couldn’t you technically compress a message into a pdf and send it as such?",hotewbb,t1_hordmlp,1639682070.0,True
rhll1c,"Your assuming a character takes up 1 byte - while it’s the case a lot of times, a lot of newer encodings are a lot larger, like utf-16 and 32, which take up 2 or 4 bytes respectively",hosenhe,t3_rhll1c,1639667905.0,False
rhll1c,"Which encodings take up one byte per character nowadays then? It seems limiting to prevent one character from sending as a single byte. If a character was sent as a single byte, you’d be able to send more data due to the limit of text being higher.",hotf8db,t1_hosenhe,1639682203.0,True
rhll1c,"Ascii, utf-8. Yes certain encodings work better if you aren’t going to use certain characters",hou3atx,t1_hotf8db,1639691633.0,False
rhll1c,"If I were to only use basic characters, which encoding should be utilized? Strictly one byte per every one character? Is this encoding available via email format or texting format? 

If newer phones aren’t capable, how about older phones? The flip phones and what not? 

As for email, is it capable? 

Any idea how many bytes an emoji is? Not that it matters, however, I am curious.",hou521f,t1_hou3atx,1639692345.0,True
rhll1c,"Actually UTF-8 is variable width that uses 1 to 4 bytes per character, depending on the character.",hounm2f,t1_hou3atx,1639700361.0,False
rhll1c,"What encodings are still 100% fixed. Also, which encodings are fixed at 8 bits per character? 

For UTF, how would you compress and keep the byte size at 8 bits per character, strictly. Which encoding keeps everything set at 8 bits per character? The ASCll encoding takes up 8 bits and or one byte per character? That’s 100% preset? It’s impossible to increase the byte size per character if you were utilizing ASCll? 

Let’s say you were sending the message via electromagnetic waves as a radio wave, if the wave became energized due to an outside influence, would the data become larger or more compressed? Or would nothing happen? Or if it was too powerful, would it just not send due to it acting as an EMP/jammer? 

While utilizing UTF-8, it’s impossible to compress anything to one byte? It’s all set between 2 to 4 bytes? One byte seems limiting, however, doesn’t that allow for larger messages to be sent if the message limit is a certain number. It seems more useful than the higher grade encodings.",hotdncj,t3_rhll1c,1639681577.0,True
rhll1c,"ASCll, Latin-1, and UTF-8 are capable of utilizing one byte per character? For all characters, strictly? Aside from non-English characters, correct? Spacing, punctuation, and other sorts of symbols would still be one byte though, right? 

Emojis don’t really matter, however, I might as well ask anyways. How many bytes per emoji? 

Older email clients used to send 7-bit? That means it would be a little less than 8-bit, right? Meaning, it is 1 bit less thus it isn’t a byte unless you send another character? If this is correct, what email clients were those? Any idea? 

Also, how many bits and bytes are a character in newer email format? Is it possible to have one byte per character when writing an email? 

When you click the send button, does the send signal become added data on a wavelength?",hou49yj,t3_rhll1c,1639692024.0,True
rhll1c,Be aware that email transmission includes message headers that are not part of the message that included details such as routing information. You can read more about message headers [here](https://jkorpela.fi/headers.html),houq5qf,t3_rhll1c,1639701526.0,False
rhll1c,look up an ascii table. they have the 8 bits in all forms and shows the 256 corresponding characters,hov4o2v,t3_rhll1c,1639708036.0,False
rhll1c,In addition to what everybody else said have a look at [https://en.wikipedia.org/wiki/Quoted-printable](https://en.wikipedia.org/wiki/Quoted-printable). It encodes 8bit characters into 7bit characters but needs more of them. It is still used for email. Maybe not often but every email program needs to be able to interpret it.,howc2oi,t3_rhll1c,1639733052.0,False
rhll1c,"**[Quoted-printable](https://en.wikipedia.org/wiki/Quoted-printable)** 
 
 >Quoted-Printable, or QP encoding, is a binary-to-text encoding system using printable ASCII characters (alphanumeric and the equals sign =) to transmit 8-bit data over a 7-bit data path or, generally, over a medium which is not 8-bit clean. Historically, because of the wide range of systems and protocols that could be used to transfer messages, e-mail was often assumed to be non-8-bit-clean – however, modern SMTP servers are in most cases 8-bit clean and support 8BITMIME extension. It can also be used with data that contains non-permitted octets or line lengths exceeding SMTP limits. It is defined as a MIME content transfer encoding for use in e-mail.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",howc3kr,t1_howc2oi,1639733073.0,False
rhhyf3,"The short answer is downloading files does not wear out a computer, the wires, memory, etc. After all, normal internet browsing downloads dozens or even hundreds of files per page.

The caveat is that solid state memory devices (SSD, or tablets, phones etc) can wear out, but it would take extreme usage over a period of time to trigger it.  By extreme usage, I mean something like downloading your 1000GB gdrive/onedrive, and as soon as it finished, deleting your local copy and repeating.

&#x200B;

I'm sure others will provide more details/corrections.",hoqtzx5,t3_rhhyf3,1639631809.0,False
rhhyf3,"At the most basic level, files are stored as bits, which are represented by states of flip flops. Everything above that is an abstraction. If you download a file that somehow runs itself and contains code to run a heavy workload and turns off the fans, it can destroy your computer. Not instantly, but overtime. A file can imprint itself but that depends on what kind of device it is stored on. It isn’t a property of file, rather the property of the device. For eg: PROM is a programmable ROM that can only be programmed once, while EPROM is Erasable PROM, so shine a UV light and the data disappears. EEPROM is electical EPROM, so data can be erased by passing an electric current. Electrical engineering has a lot of things like this. Especially hardware design and verification. Cool stuff.",hora8h7,t3_rhhyf3,1639642718.0,False
rhhyf3,Thank you,horwno3,t1_hora8h7,1639659388.0,True
rhhyf3,"Transferring or processing data doesn't cause any meaningful wear. Writing the data on a storage device may cause wear but usually not enough to matter in most cases.

Electronics, such as the wires or the computer's CPU or RAM memory, don't really suffer physical wear from processing data. Heavy use of components such as the CPU can cause the component to heat up, and repeated heating and cooling cycles may technically shorten the lifespan of the component. However, that would typically only really make a difference on a time span of decades. That's much longer than you're going to be using a CPU or any other computer component in any everyday use.

Downloading files also doesn't really cause heavy load on the CPU or other components in a computer, so downloading data probably wouldn't make even that kind of a difference.

As for storage devices, that depends on the technology used in the storage drive. SSDs do have a limit on the number of times each memory cell can be written to, so frequently writing a lot of new data on an SSD does theoretically shorten its lifespan. How much data you have on an SSD doesn't matter; what matters is how many gigabytes of new data you keep writing.

However, you can typically overwrite each cell on an SSD thousands of times. As long as there's free memory space available, the SSD also automatically tries to spread the data writes evenly over the free physical flash memory so that the wear wouldn't all be on the same cells even if you keep overwriting the same files. You probably can't wear out a modern SSD with any kind of normal use even if you download or otherwise write lots of large files on it. SSD wear might matter in heavy constant use, such as perhaps in a data center, but it doesn't really matter in desktop use.

Mechanical hard drives are a little different. They are, well, mechanical, and they suffer physical wear over time. A hard disk drive has spinning disks that rotate at thousands of RPM, and data are stored by altering magnetic fields on the disk. Mechanical parts such as bearings can wear out over time. However, most of the wear would probably come simply from having the hard disk drive powered on and the disks spinning, or from powering the drive on and off often, rather than from writing lots of data on it.

So, the TL;DR answer is roughly that *downloading* (as in transferring data from the Internet in the first place) doesn't cause wear; writing the files on the disk (as you usually do) may cause some wear, but not enough to matter in most cases.",hos7qu5,t3_rhhyf3,1639664933.0,False
rhhyf3,"I concur. The downloading itself does not really wear out anything. The act of persisting this data is what causes wear.

You can have a functioning computer even without any writable medium. You can even boot linux from an optical disk and from then on everything that needs storage is stored in RAM.",howvegc,t1_hos7qu5,1639746162.0,False
rhhyf3,"One more thing to add: the servers that you download from need to serve the files. That requires processing power (to a smaller extend on the downleading side, too) which is measurable is excess heat. This will (over long periods of time) also cause damage on the hardware.",hozkqrw,t3_rhhyf3,1639786083.0,False
rh7r2v,"For illustration purpose it starts with only 0-45 degree slope and leaves the detail to you to deal with other cases (flip signs/flip dxdy/etc.)

the coefficients in 2dy-dx are chosen because we're making a decision between picking (1,1) or (1,0) as the offset to the next pixel. We're really just comparing dy/dx direction to (1,0.5) and see if we should go above that (1,1) or below that (1,0). But 0.5 isn't integer, so we scale and compute 2dy-dx instead.",hos88f7,t3_rh7r2v,1639665155.0,False
rh4amb,Isnt C++ is more strongly typed than C ?,hoo7w0f,t3_rh4amb,1639591550.0,False
rh4amb,"I don't know whether it's more strongly typed, but C++ is infinitely more dynamically typed than C.

C does not have dynamic types at all. The type of everything is determined at compile time.

In C++, you have polymorphism.",hooywmy,t1_hoo7w0f,1639601977.0,False
rh4amb,"C++ is a static strongly typed language at its core. 
But since it is the swiss army knife of programming languages, it does support a typed dynamic dispatch (runtime polymorphism)  that is more of dynamic typing feature. 
C language doesnt offer( afaik) any dynamic dispatch mechanism in the language. But since it is THE portable assembler, we can implement any dispatch scheme using the language. But the language still stays statically typed, though compiler is very forgiving when it comes to enforcing types ( weak typing)",hop0z4u,t1_hooywmy,1639602776.0,False
rh4amb,Isn't void* a passe-partout in C/C++ ? Like I can give void* as param to a function then pass a int* cast as void*,horxl27,t1_hooywmy,1639659902.0,False
rh4amb,"But then a ""conversion"" explicitly happens, which is what converts the value to a different type.

C's syntax just allows this conversion to happen without additional syntax. The conversion is explicit in the language's semantics. The conversion is explicit in the type-annotated AST.

All statically typed languages have conversions between types, casts are nothing special.",hos0h8r,t1_horxl27,1639661448.0,False
rh4amb,"Yes, you can write C++ such that it's very strongly typed. But also, it inherits the exact same features that C has that make it weakly typed. 

I guess this graph is a kind of lowest-common-denominator kind of thing.",hoosgls,t1_hoo7w0f,1639599478.0,False
rh4amb,Maybe they mean like with inheritance you can cast an object as it's parent? Or type casting functions (not sure if C-structs offer that)?,hooshf8,t1_hoo7w0f,1639599487.0,False
rh4amb,"Is the reason C++ is classified as ""Weak"" typing because you can, for instance, read the bytes of an `int` as a string? In which case can you not do that in Java as well?",hooc5f2,t3_rh4amb,1639593216.0,False
rh4amb,"In Java an ```int``` is a primitive whereas a ```String``` is an object. However you could have an ```Integer``` and a ```String``` where both are sub-types of ```Object```.  
  
You could convert an object into bytes, but you can't just pretend you have bytes. Whereas with C on the other hand, types are more of wrapper around bytes where an ""int"" is 4 bytes/32-bits and an interpretation of them and a C-string is a null-terminated sequence of bytes (last byte is '\0') which will be be interpreted as belonging to a set of characters.",hop5cnb,t1_hooc5f2,1639604464.0,False
rh4amb,"No, you can't in Java. At least not accidentally. And if you do so on purpose, you do not have undefined behavior.",hooyqr7,t1_hooc5f2,1639601913.0,False
rh4amb,"I think the closest thing you will find in ""serious computer science"" is the lambda cube https://en.m.wikipedia.org/wiki/Lambda_cube

I personally have not seen much theoretical study on totality of a real world programming language, as they are way to complex to reason about. And with such a complex system it is very hard to define the notion of ""stronger type systems"".

Not to mention many system has unique type systems like linear type, refinement type, semantics type, gradual type, and cubical type, which are hard to characterize into a structure.

Lambda cube is one of the more comprehensive summary of type systems inspired by intuitionistic logic.",hop85p6,t3_rh4amb,1639605557.0,False
rh4amb,"I think this is the answer I've been looking for, thankyou very much king",hopj5vd,t1_hop85p6,1639610104.0,True
rh4amb,Why the heck is F# weaker than C#? The whole point of F# is the strong type system.,hooexq6,t3_rh4amb,1639594287.0,False
rh4amb,"To be fair this is just the first example I found, probably not the best",hoohtib,t1_hooexq6,1639595399.0,True
rh4amb,I think these are only placed in the correct quadrant,hoprjvf,t1_hooexq6,1639613803.0,False
rh4amb,You are missing an axis: explicit to inferred.,hop38hs,t3_rh4amb,1639603637.0,False
rh4amb,Could someone explain me why c++ is weakly typed?,hopd9pb,t3_rh4amb,1639607633.0,False
rh4amb,Things are very often implicitly converted (thing passing an int to a function that uses a float and vice versa),hopj3wg,t1_hopd9pb,1639610081.0,True
rh4amb,"Thats inherited from C, in fact C++ disables some of the  implicit conversions from C (eg ""string literal"" to char*). So it doesn't make sense for C++ to be more weakly typed.",hoq7qvw,t1_hopj3wg,1639621109.0,False
rh4amb,"Thanks, i didn’t know that being weakly typed is it about implicit conversions, do explicit conversion have anything to do with it as well?",horo10c,t1_hopj3wg,1639653915.0,False
rh4amb,"So where did you find the graphic? Why would it have to in a paper/book? You can also cite non-traditional sources, especially in CS",hoo3w7m,t3_rh4amb,1639590025.0,False
rh4amb,"I suppose it would be fine to cite where I found it, I was just wondering if there was any academic paper which I could cite instead (better)",hooc5mu,t1_hoo3w7m,1639593219.0,True
rh4amb,"Academic papers for software engineering concepts like this are usually garbage, you will find better and more reliable sources in company white papers and textbooks. The academic publishing for compsci is very good, but *software engineering* academia is dominated by people who couldn't hack it in industry *or* rigorous compsci research, and pump out very low quality papers.",hopsgts,t1_hooc5mu,1639614214.0,False
rh4amb,"I think this topic falls more in the realm of programming language theory than software engineering. And theory of programming languages is a rich area of academic research; heck, you could reasonably argue that academia has driven a lot of the innovation in programming languages.",hoqc5tm,t1_hopsgts,1639623047.0,False
rh4amb,"> Why would it have to in a paper/book?

I guess the general idea is that a paper or book would have put much more thought into the graph than random blog, and so it'd be more ""accurate"" and sourced etc.",hoosn1a,t1_hoo3w7m,1639599547.0,False
rh4amb,Ask over at r/programminglanguages,hoozvs7,t3_rh4amb,1639602355.0,False
rh4amb,"This ""infographic"" would seem to be a summation of opinion rather than researched fact - many languages have been plopped in places that make no sense (but then the axes are also seemingly arbitrary words) - I wonder whose agenda it suits?",hopiqoe,t3_rh4amb,1639609924.0,False
rh4amb,OP makes thread to ask for academic papers.  People instead critique the example given.   lol,hoqlet5,t1_hopiqoe,1639627290.0,False
rh4amb,"I think this could actually be proven with some simple code where implicit conversions could be done as tests (could definitely have more tests), as for the metrics I'm not really sure how you would lean more to one side.",hopv6z4,t1_hopiqoe,1639615464.0,False
rh4amb,They misspelled Haskell lol,hordnc0,t3_rh4amb,1639645448.0,False
rh4amb,im confused as to why C++ is towards weak instead of strong,hos0r15,t3_rh4amb,1639661586.0,False
rh4amb,how the fuck is C more weakly typed than python?,hoolprs,t3_rh4amb,1639596891.0,False
rh4amb,"It all wholly depends on how you define ""strong"" and ""weak"" and how you rank languages against each other. Maybe strength refers to different relative rankings of type safety or perhaps memory safety, or maybe how strict the static or dynamic type checking rules are. Python, for instance, has pretty strong runtime type checking rules. It's very often going to throw an error rather than try implicit conversion.",hoos0jq,t1_hoolprs,1639599304.0,False
rh4amb,"Yeah by my understanding, it's considered ""strong/dynamic"" because each object x has a unique well-defined type type(x) at runtime that doesn't get implicitly converted to other types. Contrast this to C, where there's automatic integer promotion between distinct types, array/pointer conversion, and the ability to cast any kind of pointer to any other kind of pointer; there's a lot of ways to treat one type of data as if it were another. At the extreme, the weakest type system would be no types at all, like some assembly code where everything is just machine words/bytes.

Python is also contrasted with something like JS, which will do whatever it can to produce a result (adding ints to strings and the like), even if the oparands are nonsensical.",hopdx8b,t1_hoos0jq,1639607903.0,False
rh4amb,Which is why a diagram like this is useless without an explanatory text.,hop55am,t1_hoos0jq,1639604384.0,False
rh4amb,Agreed,hop6cph,t1_hop55am,1639604851.0,False
rh4amb,"Because it clearly is. You can do whatever you want with a value in C. C is nearly the perfect example of a statically, weakly typed language.",hoptu6q,t1_hoolprs,1639614842.0,False
rh4amb,"OP have you studied type systems? https://en.wikipedia.org/wiki/Type_system  That field has to quantify how strongly typed a language is for it to work, so there is probably something floating around in the neck of the woods you're looking for.  There may be some papers on the topic.

edit: https://en.wikipedia.org/wiki/Comparison_of_programming_languages_by_type_system  This was found in the type system link above, so there is clearly a path to this topic from type system.",hor1e33,t3_rh4amb,1639636262.0,False
rh4amb,"I have the perfect solution for this. Hope I’m not too late:

[ignore the highlights ](https://imgur.com/a/8QKi1VJ)

While that photo isn’t specifically what you’re after the whole paper will leave nothing uncertain: 

[Programming Paradigms for Dummies](https://www.info.ucl.ac.be/~pvr/VanRoyChapter.pdf)

The name is misleading its most definitely **not** for dummies",hor4f2x,t3_rh4amb,1639638346.0,False
rh4amb,Make sure to include Go. I hear only good things about the operating systems minus being developed by Google. But who cares! It's a great language.,hor6mhw,t3_rh4amb,1639639946.0,False
rh4amb,[removed],hop5nq9,t3_rh4amb,1639604584.0,False
rh4amb,Sorry what?,hop6v84,t1_hop5nq9,1639605051.0,True
rh4amb,A je to iz prosojnic od funkcijskega programiranja profesorja Bosnica?,hop7qa2,t3_rh4amb,1639605388.0,False
rh4amb,scala FTW!,hopz3ry,t3_rh4amb,1639617245.0,False
rgnbmf,">Has anybody had similar experiences?

Yep!

You're not a moron. A moron would be finding excuses for why their solution was ""better"" somehow.",hol9r4q,t3_rgnbmf,1639533311.0,False
rgnbmf,The chad above is right . Sometimes we block our thinking due to performance anxiety or a belief that we need to prove ourself.,holuv8w,t1_hol9r4q,1639543347.0,False
rgnbmf,The alpha male above is right. The need to prove ourselves can often make ourselves feel less.,homs89m,t1_holuv8w,1639567655.0,False
rgnbmf,The person experiencing imposter syndrome above is right. We always learn by our failures and peers.,homyddx,t1_homs89m,1639571767.0,False
rgnbmf,"The person in denial above is right. It's always better to keep an open mindset rather than a closed one, since an open mindset will always help you grow",hon4pyd,t1_homyddx,1639575311.0,False
rgnbmf,"The wise mountaintop guru above is right. An open mind lets in fresh thoughts, like a open window lets in fresh air and sunlight. Stretching the metaphor, all allow you to grow.",honbf9c,t1_hon4pyd,1639578545.0,False
rgnbmf,Hehe.,honc0j8,t1_honbf9c,1639578816.0,False
rgnbmf,We’ll that puts an end to that,hongbrz,t1_honc0j8,1639580717.0,False
rgnbmf,"It's virtually impossible to write dumber code than an experienced developer has come across in production at some point or another.

If it works and people can read it and it doesn't require adding dumbass dependencies, ship it!",holec26,t3_rgnbmf,1639535384.0,False
rgnbmf,"Just by the fact that you recognize that your coworker's solution is a valid but a simpler solution to your problem, indicates that you're definitely not a moron. You learn something from this experience.
6 months is arguably not a lot of time to gather experience yet. I've been doing this for more than 5 years, and I sometimes still do stupid mistakes (though not at the extent and frequency as when I was still starting out). But the most important thing is you learn. You have to always learn.",holzpit,t3_rgnbmf,1639546055.0,False
rgnbmf,Everyday and always. The best part is when you stumble upon code you wrote years ago and wonder 'how the hell did I even come up w/ this???'. Never gets old.,hom03f5,t3_rgnbmf,1639546280.0,False
rgnbmf,"I've been writing software professionally for close to 20 years. I experience what you describe every week, on average.",homr8dr,t3_rgnbmf,1639566898.0,False
rgnbmf,"All good. You are learning!  


Next time when you need to solve the same problem you know how to do it.  


Generally: When ever you think your code is smart/genius... delete it, it's garbage!

Smart code is bad code. If you think it's smart now you'll have a very hard time understanding in a month from now, your stupid colleagues will have an even harder time. Don't write smart code, write simple code!",holnuea,t3_rgnbmf,1639539754.0,False
rgnbmf,"Excellent advice, I second this. Good code often looks surprisingly simple. Code by newbies is often very complicated.",holpdxx,t1_holnuea,1639540500.0,False
rgnbmf,"Hmm, I honestly have encountered a lot of senior code that is very hard to understand and I actually prefer newbie code because it's usually less complex and easier to reason about. 

Well... I might be eternal newbie, but that's another discussion. Just wanted to give another view point aswell!",honmhoj,t1_holpdxx,1639583231.0,False
rgnbmf,"My first reaction is that the people who wrote the code you saw still have things to learn. If code is hard to understand, that is a problem with the code.

A worthwhile place to learn more is the book ""Clean Code"" by Robert Martin (""Uncle Bob"").  Especially note the ""SOLID"" acronym; there is a chapter for each part of this. If you can use these ideas, you will be producing more easily-understood code.",honuq4f,t1_honmhoj,1639586462.0,False
rgnbmf,"Senior code I was referring to is mostly about doing something simple through multiple abstractions. It seems SOLID, DRY, KISS and all those acronyms but it lacks simplicity.  
It's hard to discuss with just words and I should show you an example, but I think you might've encountered code like that as well? One could say, it is over-engineered. I think over-engineering is a bit subjective and it might be just bad code etc.",hora5nt,t1_honuq4f,1639642655.0,False
rgnbmf,Indeed. What may earn style brownie points within a single line may not be good for the whole project.,homif8o,t1_holnuea,1639559508.0,False
rgnbmf,"I spend weeks deleting complex code from a project (to get it working), then I went on to implement a more complex but better algorithm only to discover that the ""junk"" I threw out was that algorithm (or large parts of it, project didn't work at all when I got it, so no clue).

In retroperspective, I stand by this way (no way I could have fixed/tested that complex algorithm). But I still felt dumb. Especially my commit comments: ""Removed useless Class X"" then later ""Implemented Class X"" (Class names come from science/math paper)

&#x200B;

Looking further back - before University, nobody told me that ""If"" is bad (performance wise), so I stacked 8 If clauses with masterworks of boolean math - in 3 Loops, to be executed each frame. Now I know why I only got 30 FPS and couldn't increase resolution at all. ...

Or that time we had a practical course in university - some Java Full Stack thing (a frontend for some research facebook clone (with complex relation networks), intended for actual use - Java, never again). Took 80% of our time to even get it running (because competence is rare we had 2 people with real Java experiance and only 3 others (inkl- me!) even had the competence to install Virtual Box ... ). When we put our (shitty) frontend to the real backend we realised that data queries literally took \~93 seconds. 93 SECONDS!  (everybody got good to very good grades - and the project (and backend creator) got scrapped (or redone)) We felt really dumb. Had we tried that earlier, we could have saved a lot of stress.",holck4r,t3_rgnbmf,1639534584.0,False
rgnbmf,[deleted],housdil,t1_holck4r,1639702524.0,False
rgnbmf,"You mean the Java project?:The problem was that the backend was designed without regard for performance: Connections (friendships, family etc) were entities with properties, which themselfs each could have connections (infinitly recursing!), and Cathegories, nested etc. Pretty sure even the simplest request resolved to dozen or hundrets of SQL queries and MBs of data. (and, of course the backend, frontend,frontend database and backend database were each on different servers (also not in RAM or on SSD))

I wouldn't complain about the servers (much) we used (2013?) Apache, Tomcat and Postgres, plus JSPs /Servlets combined with RichFaces, some Servlet Server (or what that was called) and one other thing I forgot, and of course, we had to use some universal identity/login service (which was difficult). We spend most time getting to Hello world, and second most time ""exploiting"" our projecft components/writing data leaks because nobody (on our team) could figure out how the JSPs / servlets should share presistent, user linked objecfts... One other highlight was figuring out that my code wasn't buggy - RichFaces had a bug (somebody copy&pasted a set function, but forgot to chage the setted property). Project was way to difficult as a first practical team project.",howtcqe,t1_housdil,1639745063.0,False
rgnbmf,"When I have the pleasure of teaching new joiners in my team how to write programs I tell them this:

* First, it has to **do the thing**
* And better is when **others can understand** how it does the thing
* For bonus points, it should do the thing **efficiently**

Sounds like you may have skipped the second step. But at least you recognised that. Don’t feel bad. This is all part of the never-ending learning path.",homg3h1,t3_rgnbmf,1639557569.0,False
rgnbmf,Can you explain what the difference was?,holn0nd,t3_rgnbmf,1639539361.0,False
rgnbmf,"Don't be embarrassed; it's called learning. As time goes on, you will be learning how to do all kinds of things better.

Seeing how experienced people solve problems you have attacked is always worthwhile.

Along these lines, it would be worth your time to find and read ""Design Patterns"", a classic book by ""the gang of 4"" (it has 4 authors). They show how to attack several other issues that come up all the time.",holovbl,t3_rgnbmf,1639540245.0,False
rgnbmf,"I think there is merit to trying to figure things out yourself rather than immediately falling back to reference material like Stack Overflow. I did this a lot in my CS classes in university, and I think it sets you up to be a better problem solver. A strength to develop though is to know when to go look something up, or at least do some follow up after coming up with something to make sure that you've solved something in the best way possible before submitting it for code review/etc.",hols4tz,t3_rgnbmf,1639541895.0,False
rgnbmf,You'll continue to experience this in retrospect when you look at code you wrote a year ago.,homls0l,t3_rgnbmf,1639562396.0,False
rgnbmf,"yep don't worry it's ok and healthy to feel like this, keep learning and get better


ie: i just refactored a chunk of my old code because i could not understand it fast enough and it was doing stuff redundantely and with wrong var names",hon1iqa,t3_rgnbmf,1639573601.0,False
rgnbmf,">Has anybody had similar experiences?

All day every day (I'm a business owner in IT) And I've been tinkering with code for well over 25 years now, don't beat yourself up over it.   


There will be someone that looks at your code and think ""Why didn't I come up with this"" as well and please reframe critisizing yourself into *""Not the best possible solution for the given problem""* or *""I may have....""*  
I drove myself into a **University needed to intervene** situation due to seeing the code people rattled out during a internship at a big tech company, which made me question my abilities which is a fate I wouldn't wish upon you.",hon3gv8,t3_rgnbmf,1639574662.0,False
rgnbmf,">Has anybody had similar experiences?

Yes, it's called progress",hoob0ux,t3_rgnbmf,1639592777.0,False
rgnbmf,This is called lack of experience. It's normal. And if it works it is okay.,hon2hlu,t3_rgnbmf,1639574136.0,False
rgnbmf,"I've been in this line of work for almost 3 decades. Just last month I wrote a complicated if/else contraption. Upon finishing it and testing it, I realized I could simplify the whole stupid thing down to one conditional. Yes, I am an idiot, too. Welcome to the club!  It's a big club.",honaurk,t3_rgnbmf,1639578290.0,False
rgnbmf,All the time,honwhmn,t3_rgnbmf,1639587162.0,False
rgnbmf,Bruh at least your not getting easy questions wrong on Algo expert i feel braindead,hpjqjwu,t3_rgnbmf,1640175016.0,False
rg53gw,"The book ""Engineering a compiler"" by Keith Cooper and Linda Torczon has about 250 Pages on different optimizations, I can highly recommend it.   
You might get it used cheap or it's in a local library.   


or ... there are places on the internet where one can get Pdfs for free apparently.",hoi8v1i,t3_rg53gw,1639487227.0,False
rg53gw,"[https://llvm.org/docs/Passes.html](https://llvm.org/docs/Passes.html) might be a good start.  


Passes to look for are mem2reg, sccp, the loop- passes and anything in the transform category, actually.",hohzdwh,t3_rg53gw,1639481168.0,False
rg53gw,https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html,hoiq4gp,t3_rg53gw,1639495523.0,False
rg53gw,"The Dragon Books

[https://en.wikipedia.org/wiki/Compilers:\_Principles,\_Techniques,\_and\_Tools](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools)

I like the green dragon, but then I am old.",hojo0xy,t3_rg53gw,1639508833.0,False
rg53gw,Not exactly the same but [http://godbolt.org](http://godbolt.org) is pretty cool!,hojra7t,t3_rg53gw,1639510107.0,False
rfovp2,"IDK the ""*best*"" way, but printable characters are definitely a start...",hof8snc,t3_rfovp2,1639427467.0,False
rfovp2,Raw bytes -> hex string is my go-to.,hofd5e8,t3_rfovp2,1639429303.0,False
rfmupc,"I have no experience with this so I'm just spitballing. Depends on how intelligent she is and what kind of computer experience I suppose:

* Scratch would let you make it as easy as possible. Colorful drag and drop tiles to build functions, would introduce the really basic concepts of control flow and all
* You could play with Visual Studio a bit. Dragging and dropping buttons and showing how to do something simple like make a popup window with text or add two numbers
* Python is pretty easy to start with too for obvious reasons, maybe that would make sense to start with for some basics

If you haven't Googled in depth already, there are probably some more specialized languages or ""IDEs"" out there targeted at a young audience",hoetxuo,t3_rfmupc,1639421467.0,False
rfmupc,"One thing I would say about Scratch is that while it looks colourful/easy to learn/fun for kids, there are so many code blocks and so many things the learner has to understand. It also takes a lot of time to look for the right blocks and to place them for something to work.

And I am saying the above from experience. I learned Scratch for the first time in a programming class and failed miserably. Just not enough time to experiment all the blocks and didn't really get how to fit certain blocks into places. And it was really time consuming.

But if you have time in your hands, then give Scratch a try. If she is struggling, definitely let her try Python because it's much easier to learn and make programs in my opinion. You can make a lot more interesting and fun programs with Python and I bet she would like that.",hog26l4,t1_hoetxuo,1639439847.0,False
rfmupc,"Good point. I hadn't considered the potential learning curve for OP as well, sounds like that one might be a time investment",hogut5l,t1_hog26l4,1639453343.0,False
rfmupc,Intelligence is a built skill. Like you learn how to learn better and learn well.  It's not like a fixed thing.,hofexj2,t1_hoetxuo,1639430027.0,False
rfmupc,"Fair enough, not trying to split hairs on that. Just meant that some kids are going to pick it up faster or have a little more under their belt already than others.",hofre66,t1_hofexj2,1639435195.0,False
rfmupc,[deleted],hoewexx,t3_rfmupc,1639422451.0,False
rfmupc,"So, you hate the child",hoh91nv,t1_hoewexx,1639460870.0,False
rfmupc,"As a father of two children ages 10 and 6 where both kids like programming (or ""computer math coding"" as my 6 year-old calls it), I would focus on [Computational Thinking](https://www.bbc.co.uk/bitesize/guides/zp92mp3/revision/1):

1. Decomposition
2. Pattern Recognition
3. Abstraction
4. Algorithms

I would stick with using Scratch (my kids use/know this).

Example:

There's a maze (2D-array) where the student has to navigate a character from from start to finish.

a) They can hard-code the sequence of steps required. (Sequencing concept)

b) They can be forced to use loops to reduce the number of instructions used. (Looping concept)

c) They can use basic decisions (if-then-else expressions) to create a simple algorithm.


 Keep the concepts simple and light. Making success incremental while slightly increasing the difficulty between tasks is key. That keeps the kiddos motivated.",hoezfir,t3_rfmupc,1639423669.0,False
rfmupc,"I started with Scratch when I was around 10 and I’ve been interested ever since. 

https://scratch.mit.edu/",hoex8xv,t3_rfmupc,1639422788.0,False
rfmupc,I started getting my interest in computer science and programming when I was 13. I don't see why not to introduce her. I'm 17yo rn studying Game Development at college and I'm glad I started young and found my passion / future job lol,hoev4s1,t3_rfmupc,1639421942.0,False
rfmupc,"I was introduced to code around that same time and I learned through Scratch. That was my favorite program. But, I had a teacher tell me how to do stuff. There aren't built-in lessons in Scratch. If you want to teach her and be more hands-on, I'd use Scratch. If you want something that teaches her how to code in a way that's similar to Khan Academy, I recommend [Code.org](https://Code.org) or [CodeHS.com](https://CodeHS.com)",hoex8xb,t3_rfmupc,1639422788.0,False
rfmupc,I was an elementary school child when I learned BASIC. I would say BASIC or Python.,hoeys1h,t3_rfmupc,1639423408.0,False
rfmupc,I learned programming BASIC on my dads C64.,hohtx82,t1_hoeys1h,1639476724.0,False
rfmupc,It's too bad modern computers aren't accessible like the old days.,hohywn2,t1_hohtx82,1639480807.0,False
rfmupc,"Eh, I disagree. I think it's more easy nowadays. Especially with higher level languages like python or JavaScript. Remember that you had to type line number yourself  for goto statements?",hoi1bwc,t1_hohywn2,1639482574.0,False
rfmupc,"Figure out the fastest way she can make a game in Roblox and start with that. Ideally if she can do something super simple on an hour and half or less that would be great. Then slowly build from there. If each lesson ends with her game being a little better that’ll probably help a lot with motivation and excitement. 

Overall focus on something she can show off and be excited about. Then sneak in the less fun stuff as you go.",hoeudy9,t3_rfmupc,1639421645.0,False
rfmupc,Start her with [Scratch](https://scratch.mit.edu/parents/),hoexcb9,t3_rfmupc,1639422826.0,False
rfmupc,Already mentioned a bunch of times so just adding to the dog pile here: Scratch got me hooked as well. The drag and drop code blocks and animations made it fun and simple.,hof9atw,t3_rfmupc,1639427680.0,False
rfmupc,"This might sound nutty but hear me out. If you have an Apple device, download the OCaml compiler and teach them ML! I don’t even mean that in a joking way.

Functional programming is actually so much more straight forward and that Mac/iOS compiler is incredibly simple and easy to use.

Sure they won’t be able to do a ton but it’s not like Scratch is really reaching them anything more useful and with ML they will only need to learn like three keywords and the rest just all falls into place.",hoeyfjm,t3_rfmupc,1639423269.0,False
rfmupc,"I don't know any Ocaml (and you're probably more experienced than me at this, since I'm not that experienced a programmer), so I can't say for sure, but based on the criteria you gave, do you think something like Scheme using DrRacket might be easier?

The syntax is dead simple, arguably moreso than Ocaml, and because it's run using an interpreter, it's very easy to see the results of your code quickly. It's also typeless, which means there's one dimension fewer to wrap your head around.

That being said, as someone who's trying to learn how to program by learning C and Scheme side-by-side, I don't know if I'd say functional programming is more straightforward. It's about equal, I would say, although this is based on my rough intuition; I understand imperative programming better than functional because I find C to be more fun to program in, but if I put the effort towards Scheme that I do towards C, I think they'd be about equal.

Also, I realize your comment is about a month old; sorry for necroing.",hrqlffr,t1_hoeyfjm,1641616370.0,False
rfmupc,"Speaking from how I got into CS, I'd say Web Development.

I remember that's how I started when I was a kid. Working with HTML and CSS was extremely simple and provided instant gratification. I remember getting so excited over stupid little things like setting a background image or making text a different color just by typing some letters.

The best part was how much it gradually snowballed. I was forced to consistently learn more and more as my ideas grew. Eventually had to learn stuff like PHP and JS which really came in handy for getting started with programming languages in college.

But web development never felt like work or studying. As a kid, I always considered it more of a art medium rather than a technical skill.",hofb0tz,t3_rfmupc,1639428418.0,False
rfmupc,"BASIC is how I got started, at around that age. See if you can get her interested in something like a turtle drawing library, those are usually quite fun for kids.",hof2qre,t3_rfmupc,1639424998.0,False
rfmupc,Raspberry pi has a lot of learning PDF materials for young children,hof44fi,t3_rfmupc,1639425545.0,False
rfmupc,"**Autonauts** is a cute game that let's you build an automated village. It's very kid friendly and has scratch-like building blocks. The game is super rewarding early on, but it becomes more redundant to progress later. Regardless, it's a good introduction to basic concepts while being entertaining.",hof77ra,t3_rfmupc,1639426804.0,False
rfmupc,"Roblox scripting might actually be a good place to start. You can make a lot of different games in Roblox, and you can start really simple.",hof9tkc,t3_rfmupc,1639427902.0,False
rfmupc,"I got my start around that age by messing around with qbasic, typing in examples from a qbasic book that my dad had. 

The act of rote typing in examples from the book was enough to drill in the syntax and basic concepts. I remember my dad sitting and teaching me about if/then/else, and maybe about for loops. Then I was left to my own devices and started making changes to the examples that I had typed in. I also spent quite a bit of time trying to get things to work due to making typos, which also helped me learn. I seem to remember making lots of Mad Libs type things where you'd be prompted for words, then it would print out silly stuff incorporating those words.

I would recommend setting them up with some environment in a modern language. Something with at least console IO capabilities, or set them up with a skeleton project in a more GUI oriented environment with a ""turtle"" like drawing API, show them the ropes, then see what they come up with. I would lean towards javascript in a browser, or javascript run via node just because it's ubiquitous these days.

I also know kids get into programming via Minecraft these days, so that's another option if she's into stuff like Roblox.",hofmsno,t3_rfmupc,1639433274.0,False
rfmupc,you could try CodeCombat.  it kinda turns coding into a platform-esque video game and makes it fun!  that's how I started out,hofwp5w,t3_rfmupc,1639437451.0,False
rfmupc,"The C Programming Language by Kernighan and Ritchie.

But seriously I would figure out what makes her interested in programming and find something that matches that. If she wants to make games, she should start on that path.

Educational languages can feel boring and fake. Unlike a lot of people here I don't think there is anything wrong with starting out with what people consider a ""hard language"". There are no hard languages, only hard problems. ""Hard languages"" can be more exciting and motivating because you feel like you are doing something real. And if you are self teaching you can go at your own pace anyway.",hoh8mvx,t3_rfmupc,1639460613.0,False
rfmupc,Try her on a few tutorials for different things to see what draws her attention more. Scratch and Code Combat are good ones. I think No Starch Press has some kids programming books for things like python and scratch.,hohfgqe,t3_rfmupc,1639465137.0,False
rfmupc,"Three things come to mind.

1) The Logo programming language, my first memory of writing a program was a turtle moving around a monitor forwards, back in varying distances and rotating 5/10 degrees at a time and repeating n times to create a star 

2) Raspberry pi has some programming languages built in that can interact with actual hardware easily and there is a programming language scratch 3 where programming is done by dragging blocks that are linked to things like button press events and the pi could be hooked up to hardware light a set of traffic lights / pedestrian crossing to introduce something real-world.

3) a game called pingus is a clone of lemmings and is a game with learning thrown in, you have to ""program"" to get the penguins through the level, eg by deploying blocker penguins to hold up them while the builder/digger pengins make the path ahead (pengins will walk straight of a cliff if they unsupervised). One downside to this is that you might have to explode pengins to complete the level and that may not go down well.",hohyj0e,t3_rfmupc,1639480520.0,False
rfmupc,I was wondering when I was going to see someone mention LOGO.  Turtle graphics or bust. :-),hoil2ys,t1_hohyj0e,1639493352.0,False
rfmupc,"You could try these activities through Girls Who Code 

https://girlswhocode.com/programs/code-at-home",hoi8tn2,t3_rfmupc,1639487205.0,False
rfmupc,"100% agree with Scratch. Minecraft redstone is an introduction to logic similar to playing Roblox. Look for camps like Code Ninjas, may not give you the chance to teach her yourself but you can bond talking over what she did there and she gets to try a curriculum meant for introducing kids to programming.",hoexkpw,t3_rfmupc,1639422920.0,False
rfmupc,Roblox,hofpqww,t3_rfmupc,1639434506.0,False
rfmupc,Yes. Do it with Pico8. ittle make it seem less boring.,hofcmle,t3_rfmupc,1639429093.0,False
rfmupc,"I walked a young cousin though making a chess game (no graphics, just terminal). She was a little older but I thought it went well.",hofl0x4,t3_rfmupc,1639432534.0,False
rfmupc,"I would suggest python. Meanwhile do more math problems, after all computer is all about math",hofpxzj,t3_rfmupc,1639434589.0,False
rfmupc,"Look up “STEM robots” on Amazon. They’re robots that you build with cameras and a controller to make them drive and such, but they require some beginner level programming. Great way for kids to learn programming while maintaining some fun interest.",hofxpvn,t3_rfmupc,1639437889.0,False
rfmupc,Use whatever interests/motivates her as a jumping-off point.,hofyc1m,t3_rfmupc,1639438154.0,False
rfmupc,Funny prime numbers games,hofzbq8,t3_rfmupc,1639438588.0,False
rfmupc,"Thats how my first year CS Teacher treats us, the students.",hofzefa,t1_hofzbq8,1639438621.0,False
rfmupc,"Oh, and yet... i know nothing. Nada at all.",hofzh9p,t1_hofzefa,1639438657.0,False
rfmupc,Scratch possibly,hog4eeu,t3_rfmupc,1639441236.0,False
rfmupc,"I think it would be best to start her off with games or online coding websites meant for kids (scratch). If you try to teach her coding basics for python, etc. without anything fun she might get bored pretty fast. I used cmu cs academy as a freshman and the beginning lessons were very easy but that could wait until she’s a little older.",hog6gxh,t3_rfmupc,1639442239.0,False
rfmupc,"Python got me hooked at age 12 because of how easy to use it is. Typing isn’t super strict, and it’s relatively intuitive compared to other languages.",hogfnja,t3_rfmupc,1639446459.0,False
rfmupc,"She could probably learn LUA. Since you can code a lot of things in LUA using Roblox's system, Roblox Studio.",hogho69,t3_rfmupc,1639447371.0,False
rfmupc,"It would really depend on what type of learning she takes to more, like if she gains more with note on definition of things (initialization, loop, data type, or whatever other common place things), or if she gains more from some goal oriented for some set of things (list all even number from 0 to input value). One thing I could suggest for a more goal oriented approach is codingame (codingame.com), since there is a lot of programming languages that could be used, and it has various ""puzzles"" that are solved producing some expected output. The first ""onboarding"" puzzle has a while loop that takes input automatically, and you need to determine the proper output statement per iteration. For some puzzles, there is some graphics to show achieving the goal. For that first puzzle, you have a ship in the center and enemies are flying to you.

You don't have to make an account with the site (it is a free account, if I am remembering right), but you can start without having an account. The account just allows you to save settings and progress.

I think the main item when teaching a younger kid is just determining what keeps them engaged with the topic rather than just what you try to tell/teach them. I hope that helps some.",hogm87x,t3_rfmupc,1639449412.0,False
rfmupc,Maybe begin very basic to make her realize that it requires that tedious attention to detail. Not a focus on the big picture but the small non-graphical game would be great to start. Maybe something like 'Hangman' or 'Tic-Tac-Toe' something you can help her with and she can grasp the concepts of what is happening - and why.,hogpm00,t3_rfmupc,1639450923.0,False
rfmupc,[I suggest these](https://imagilabs.com/?utm_source=partners&utm_medium=direct&utm_campaign=spectra-hackathon). You don't have to buy the keychains. They have learning labs [here](https://www.notion.so/imagilabs/imagi-Learning-Center-5afe3d51d30645849f2738c9b5eb1154) where she can code designs,hogqcvg,t3_rfmupc,1639451261.0,False
rfmupc,"Scratch and khan academy have some pretty cool free programs. I learned the basics of JS with khan academy, there are lessons you can do",hogqwoa,t3_rfmupc,1639451515.0,False
rfmupc,"I don't know if anybody has mentioned it yet, but my gateway into programming was through the Arduino ecosystem. 

I had tried learning to code through various online platforms over the years but the ramp up to doing anything useful was a long road. 

Arduino gives you tangible things to associate your code with so you can really get a feel for what your logic is doing. 

But that was just my experience.",hogrz0a,t3_rfmupc,1639452012.0,False
rfmupc,Lego used to have an embedded controller that you could program with a scratch like language to have the motors go forward/reverse etc. It is what got me into programming in 4th - 5th grade.,hogt62d,t3_rfmupc,1639452569.0,False
rfmupc,"Depending on what she’s interested in, CodePen might be a different option.  It’s fun for making art with code and adding animations and such",hogvsl3,t3_rfmupc,1639453810.0,False
rfmupc,"I don’t see Kode with Klossy mentioned, but it’s an organization specifically focused on helping girls learn to code. They have tons of resources, events, and online and local communities.

 https://www.kodewithklossy.com/",hogwr7z,t3_rfmupc,1639454266.0,False
rfmupc,Web dev is a good entry point. Or setup node. Have 'em work on a simple if else console game. That's a super fun thing. Let's them be creative.,hoh5iw3,t3_rfmupc,1639458764.0,False
rfmupc,"- https://www.computercraft.info/

- https://store.steampowered.com/app/370360/TIS100/

- Just dive head first.",hohb70i,t3_rfmupc,1639462231.0,False
rfmupc,"I think KhanAcademy has some introductory courses to Computer Science that are very easy to grasp, even for children.",hohli3p,t3_rfmupc,1639469650.0,False
rfmupc,"Consider buying a Micro:bit ([https://microbit.org/](https://microbit.org/)) or 2 - a fabulous little engine/chip that you can program directly, sense buttons, shake, sounds, builtin radio communication (if you have 2) etc and it can be a wearable. It has a huge collection of add-ons, lights, electronics, robots etc, codes using blocks (looks like scratch), as well as python (via Mu editor) and javascript, lets you control lights, has a powerful sim ([https://makecode.microbit.org/](https://makecode.microbit.org/)) and is a brilliant entry level physical computing platform that is gender agnostic. We use them with students as young as 8, right up to 15/16 year olds",hohnjae,t3_rfmupc,1639471287.0,False
rfmupc,"there is a HUGE developer community, code share network and heaps of documentation, examples, forums and communities that have competitions, share ideas, showcase cool stuff",hohnxwg,t1_hohnjae,1639471621.0,False
rfmupc,you can prototype real solutions that you can hold in your hand and actually use - the micro:bit is one of the first devices that 5 mins out of the box you can make something that does something you control - really fabulous tech and cheap,hoho39m,t1_hohnjae,1639471742.0,False
rfmupc,terrific gateway drug to other platforms (like rasberry pi and arduino),hoho5ru,t1_hohnjae,1639471797.0,False
rfmupc,"Also consider Minecraft Education (from Microsoft) - it has really powerful programming engine (it is not just blockstacking in cyberspace, or killing creepers, you can do amazing scripting things and there are heaps of resources/tutorials, how tos) to script in-game action (like writing scripts to build complex stuff using sequence, selection, iteration and modularisation - the fundamentals of coding in every language.",hohnv5p,t3_rfmupc,1639471558.0,False
rfmupc,"if the child is already into minecraft, then programming in minecraft adds a real edge that builds on existing skills",hohnzo9,t1_hohnv5p,1639471662.0,False
rfmupc,Scratch it a great one!,hohvcs7,t3_rfmupc,1639477945.0,False
rfmupc,"What all the cool kids use these days - Scratch.  https://scratch.mit.edu

What us old people used as kids - LOGO.  [https://turtleacademy.com](https://turtleacademy.com)

If she was older I'd say start her on something like Free Code Camp - https://www.freecodecamp.org",hoillsk,t3_rfmupc,1639493582.0,False
rfmupc,"Scratch, maybe?",hoimmyl,t3_rfmupc,1639494032.0,False
rfmupc,Coding train on YouTube is a fun way to start,hoj2ee5,t3_rfmupc,1639500417.0,False
rfmupc,Better get her a leetcode account and make her homepage the Blind 75,hoj8i77,t3_rfmupc,1639502762.0,False
rfmupc,"There are a lot of different ways, some are better and some worse. Unless some shows a really serious interest in going deep, I would start with simpler things.  
 
One very important detail is to work out, with her, what is exciting/interesting and use that as a guide for how to teach/explore. I would encourage her to put off messing with Roblox for at least a year or two.
 
**Scratch** is a neat tool/language and is a decent way to get kids interested these days, especially if they just want to make simple games quickly. You can go totally drag and drop if you want to avoid typing code, for instance. If you want go go that route I would recommend making an effort to provide some groundwork in conditionals and logic.  
   
I think **BASIC** was an excellent idea in the past and still has meriy. However it’s essential today to work with a variant that can provide the same ease with math, graphics, sound, etc on a modern computer that historical examples did with vintage microcomputers. *Basic-256* seems interesting, but it might be a little complex for a 10 year old. 
  
If you are willing to take a more hands on approach and help out with anything that’s she struggles too much with the then **Love2D** (uses Lua) or **Processing** (Java-based) might be of interest.",hojb0ze,t3_rfmupc,1639503746.0,False
rfmupc,scratch,hol7tbv,t3_rfmupc,1639532439.0,False
rfmupc,HTML,hq9xct6,t3_rfmupc,1640698581.0,False
rfmupc,Math.,hof0oz7,t3_rfmupc,1639424175.0,False
rfmupc,Swift Playgrounds if you have an iPad or a Mac.,hoev7jy,t3_rfmupc,1639421973.0,False
rfa9t6,"There will always be malware, even if a new tech came out that was hard to infect, it will only be a matter of time before it will have targeted malware.",hodcbu5,t3_rfa9t6,1639397421.0,False
rfa9t6,Windows itself is malware by definition. So no,hocxc2l,t3_rfa9t6,1639385218.0,False
rfa9t6,Edgy,hofhuu7,t1_hocxc2l,1639431227.0,False
rfa9t6,How you figure that?,hoet2u1,t1_hocxc2l,1639421128.0,False
rfa9t6,"Well to name a few:
1. Loads of Crapware and advertisments right inside startmenu.
1. Per User Unique id for targetted advertisments.
1. Forced updates which restarts a system in the middle of work.
1. Incessant shoving of upgrades from one version to another.
1. Difficulty to change default webbrowser.

Windows 7 was last good version of Windows all after it can be classified as adware which are also Malware. So yes present day windows are malware.",hoh5o8o,t1_hoet2u1,1639458849.0,False
rfa9t6,Edge lord,hpf57ua,t1_hocxc2l,1640092236.0,False
rfa9t6,"I would guess no. Maybe it will be harder and harder to make, but it’ll probably never be impossible. An important thing to realize is malware does normal things for bad purposes. For example, you need the ability to encrypt files or send them over the internet in normal programs. However, who and when this happens to determines if it’s malware or a great product.",hoeuruh,t3_rfa9t6,1639421799.0,False
rfa9t6,"No, because the sorts of things that make a program malware are necessary for legitimate programs to be able to do as well.",hof4d90,t3_rfa9t6,1639425644.0,False
rfa9t6,"Even if you could give an unambiguous definition of ""bad"" behavior by a program, whether an arbitrary program has such behavior is undecidable.",hog3kma,t3_rfa9t6,1639440758.0,False
rfa9t6,"No. It's not generally possible for antivirus software (or any software) to tell for certain whether a particular program behaves in a given malware-ish way.

Antivirus software works by either identifying individual programs or pieces of code as specific malware, or by using so-called heuristics for telling whether an unidentified program seems to have malware-like behaviour. The former is limited by the requirement to specifically identify each particular piece of malware, so it always needs to play catch-up; the latter is not nearly 100% accurate, and it can't really be.

Even if you could unambiguously define which kind of behaviour means that an unidentified program is malware, it's not generally possible to have an antivirus algorithm that would unerringly tell if another program behaves that way. That's partially limited by our ability to design such an algorithm, of course, but it's also something that's been proven as mathematically impossible to do with absolute accuracy.",hojjpu9,t3_rfa9t6,1639507142.0,False
rfa9t6,"No, malware prevention will continue to improve; while malware itself will become better at countering malware prevention.",hqcx8gr,t3_rfa9t6,1640744382.0,False
rfa9t6,"No, there will never be a point where malware won't be made. People will always be finding out new ways to exploit stuff because there's always ways to break things. Nothing is truly bulletproof (invincible).",ht2j55m,t3_rfa9t6,1642444854.0,False
rf81l6,"It depends on what you mean by “mutate”. Viruses can be written such that they change their own encoding when they spread to a new system— search for “polymorphic virus”. This is not the same sense of “mutation” that we would use when talking about biological evolution, though. Computer viruses are a lot more fragile than meatspace ones, and once the exploit they target is patched they can’t dynamically discover a new one. This means they don’t “adapt” or “evolve” autonomously.",hocjzr0,t3_rf81l6,1639375217.0,False
rf81l6,"Upvote for ""meatspace""",hocq9l0,t1_hocjzr0,1639379548.0,False
rf81l6,"Mutations analogous to meatspace viruses are more like manual updates by adversaries. Adding new features, new evasion, different impact, etc...",hoewin9,t1_hocjzr0,1639422492.0,False
rf81l6,Some computer viruses can introduce slight variations into their program code when they replicate in order to prevent virus scanners from being able to lock onto a signature.  I suppose this is not unlike biological viruses slightly mutating proteins in their capsid that allows them to evade antibodies.,hocnwcz,t3_rf81l6,1639377827.0,False
rf81l6,"in general, no. it is technically possible i suppose but ""autonomous mutation"" is definitely not ubiquitous in computer viruses like it is in biological ones.",hoci1ws,t3_rf81l6,1639373995.0,False
rf81l6,But I suppose a new one would have to be made by the author,hoci5ox,t1_hoci1ws,1639374061.0,True
rf81l6,"Sure - but then it wouldn't be autonomous, right?",hocijqm,t1_hoci5ox,1639374305.0,False
rf81l6,Right,hocim9h,t1_hocijqm,1639374349.0,True
rf81l6,"It would technically be possible to have the virus randomly alter its code independently (although the code that causes it to do that in the first place would of course need to have been written by a programmer). Genetic programming and evolutionary programming are approaches for automatically generating new or altered programs using random mutation and crossover, although I don't think they involve self-mutation.

The problem, especially if the machine code were randomly mutated as just bits and bytes, would be that the vast majority of the mutations would be nonfunctional or nonviable, or not even valid programs. While this may be true of biological viruses as well -- although I'm not an expert and don't know if that's the case -- the sheer volume of biological virus particles even in a single biological host might make that less of an issue if *some* of them end up working out. You won't have a billion or trillion virus processes running on the computer, though, so while the number of potential hosts might theoretically be in the millions, the vast majority of the mutated offspring being nonviable might practically make it a no-go. You probably need quite a volume for completely random mutations to turn out useful.

Of course actual self-replicating computer viruses are probably fairly rare nowadays anyway, and most malware aren't technically viruses.",hof3t60,t1_hoci5ox,1639425419.0,False
rf81l6,If programmed to yes.  Particularly if engineered with AI assistance,hocghhz,t3_rf81l6,1639373051.0,False
rf81l6,"I mean... only becuase they share the same name doesn't mean they have anything else in common.   


We use the name ""virus"" to describe a type of malicious software but besides that there is no comparison to a biological virus and its properties. But ofcourse you can design a computer virus to adapt and mutate.   


But still this question seems to be based on a strange assumption that there is a correlation other than the name...",hoeo8e5,t3_rf81l6,1639419201.0,False
rf81l6,"I think the other commenters already covered the good answers but if the concept interests you, check out [Coding Machines](https://www.teamten.com/lawrence/writings/coding-machines/). It's a >!fictional!< blogpost about a few developers that find such a virus while troubleshooting a seemingly innocuous compiler bug.",hoevc3w,t3_rf81l6,1639422022.0,False
rf81l6,"You could create a virus that adds entropy but that would likely just aid in detection since it is better for code to have a function, even entropic code. Usually viruses are updated over time after being detected.",holbb8f,t3_rf81l6,1639534017.0,False
rf81l6,"Yes it possible, viruses can attach itself to a system resource which then spawns off to different services or resources and spawns and spawns. WannaCry is a perfect example of a mutated virus",hocj0yt,t3_rf81l6,1639374602.0,False
rf03ai,Another one is Sipser's book. Great book overall.,hob96fa,t3_rf03ai,1639352518.0,False
rf03ai,"I second this. Though many algorithm books discuss this topic, I believe studying formal languages is the best way to get an intuition for the complexity classes.",hobaz8l,t1_hob96fa,1639353331.0,False
rf03ai,+1 for Sipser's book. It's quite good.,hoctu2f,t1_hob96fa,1639382313.0,False
rf03ai,"Some of the resources I used when I was learning complexity classes:

1. [Abdul Bari's take on the topic](https://www.youtube.com/watch?v=e2cF8a5aAhE). Definitely my favorite explanation of it. 
2. [This stackoverflow post.](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)
3. [Video by up and atom](https://www.youtube.com/watch?v=EHp4FPyajKQ)
4. [Another one by hackerdashery](https://www.youtube.com/watch?v=YX40hbAHx3s)
5. [This high level overview article by MIT news](https://news.mit.edu/2009/explainer-pnp)",hob7kxv,t3_rf03ai,1639351801.0,False
rf03ai,"At this point, I feel like I've learned more from Abdul Bari than my entire CS department combined.",hoci9ep,t1_hob7kxv,1639374125.0,False
rf03ai,Thanks!,hob80ac,t1_hob7kxv,1639351994.0,True
rf03ai,"Note, Wikipedia is a terrible resource for learning any technical thing. It is a great resource when you need to refresh yourself on a thing you once knew, or are looking for extra info.",hoc2cc0,t3_rf03ai,1639365811.0,False
rf03ai,Strongly agree. I thought I had this concept sorted out in my undergrad (Cormen). But I now see how there are many holes in my understanding.,hoc2vlv,t1_hoc2cc0,1639366058.0,True
rf03ai,"Algorithms Design

https://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358/",hobt2zl,t3_rf03ai,1639361590.0,False
rf03ai,"Not positive on a resource since I haven't touched this stuff since school, but I'd start with looking at a few reductions to get a sense of how problems relate to each other. Some simple ones might be longest path, Hamilton cycles, and degree-constrained spanning trees. The main idea is that there are mappings between these problems that keep things small (polynomial).

Maybe check out Karp's stuff or Gary and Johnson's stuff to understand the reductions. Once you're a bit comfortable with these ideas then you can get into the nitty gritty of non-deterministic Turing machines and the reduction to general satisfiability.",hob939w,t3_rf03ai,1639352477.0,False
rf03ai,"In case no one has given you a high level idea yet:

A problem X is NP-complete if it is NP-hard and in NP.

NP: A problem you can solve in non deterministic polynomial time; I like to draw the analogy that if you had computer that can run in parallel in multiple dimensions where each dimension tries a different result, it can find a solution within polynomial time ;-). A typical test is to prove that you have an algorithm to check that a result is a solution to the problem in polynomial time.

NP-hard: You can reduce a problem Y that is also NP-hard to X, so if you solve the X, you neccessarily solve that other NP-hard problem. That means X is just as hard as Y.",hobdjk2,t3_rf03ai,1639354497.0,False
rf03ai,"For NP, your definition has a slight mistake. NP includes those problems you can *solve* in non-deterministic polynomial time but can be verified in polynomial time. We don’t know how to build a non-deterministic machine, so your current wording suggests we can’t verify solutions in NP with our current technology.",hof9sel,t1_hobdjk2,1639427888.0,False
rf03ai,Thanks! I corrected it. I meant to say solve as per my analogy below.,hofa1o1,t1_hof9sel,1639427997.0,False
rf03ai,"I learned all about Turing Machines and NP Completeness from one book: Computers and Intractability: a Guide to the Theory of NP-Completeness. They provide the intuition and hard math for everything you might need involving NP and entering the polynomial hierarchy. You might also be interested in reading Karp's original paper on 21 NP problems, just to see what the first reductions looked like in practice. They're both around 50 years old, but still very readable and accessible.",hodx98k,t3_rf03ai,1639408353.0,False
rf03ai,"Gödel, Escher, Bach: an Eternal Golden Braid a book by Douglas Hofstadter.",hocr6lg,t3_rf03ai,1639380242.0,False
reoznh,"No, because they are barely good enough to translate handwriting to text right now. 

Also police do not keep handwriting databases.",ho8u8ad,t3_reoznh,1639316766.0,False
reoznh,"> police do not keep handwriting databases
 
[They do, actually](https://www.fbi.gov/services/laboratory/scientific-analysis/questioned-documents). If you are a researcher in a respected institution you can be granted (very closely overseen) access to their database for the purposes of AI research. AI is already used for this purpose.",ho9ylu9,t1_ho8u8ad,1639333643.0,False
reoznh,"That isn't even remotely the same ML task. Translating handwritten text to characters is entirely different from identifying an author from handwritten text. I'd venture to guess you could even identify authors from digitally written texts, let a lone handwritten text (which has at least an order of magnitude more of information). There are so many idiosyncrasies connected to written communication. Missing i dots or t crosses, frequency of use of ellipses, capitalization, style of the `a`, etc. There is an infinite amount of variability. Given a large enough corpus, it is definitely doable. It is so doable, I would be surprise if there isn't a standard commercialization of it already.

The fields of study is called: https://en.wikipedia.org/wiki/Stylometry",ho9yrvi,t1_ho8u8ad,1639333708.0,False
reoznh,"""handwriting databases"" made me laugh",ho9f6yx,t1_ho8u8ad,1639325954.0,False
reoznh,They do keep the notes though! Because they're considered evidence,ho9sc5w,t1_ho8u8ad,1639331217.0,False
reoznh,Computers aren't any better at guessing than humans. It is just a way to remove the blame from people when the guesses turn out to be bad. I'm sure law enforcement will love it.,ho9vi4b,t3_reoznh,1639332451.0,False
reoznh,"Machine learning chess AIs would like to have a word with you. If they are that capable of pattern recognition already, and better at it than humans, I see no reason why this pattern (handwriting) would be a terribly difficult leap. Computers are REALLY good at pattern recognition. So with a sufficient dataset and training, sure seems reasonable. I'm a dev, and I've written several ml training curriculum for my ml ai for my chess like game.",hoc21rq,t1_ho9vi4b,1639365676.0,False
reoznh,A just society would not convict someone on such flaky evidence.,hoc2hjf,t1_hoc21rq,1639365878.0,False
reoznh,"It's not societies decision to convict or not, it's the grand jury's, and certainly takes more evidence than 1 handwriting match.",hoc2n1f,t1_hoc2hjf,1639365948.0,False
reoznh,"Grand juries determine whether to prosecute (indict) someone, not convict them. Regular juries (or judges in a bench trial) convict people. And that's only in the US, normal countries have normal criminal investigations, none of this grand jury indictment nonsense.",hod91r2,t1_hoc2n1f,1639395048.0,False
reoznh,"In theory, sure. There's not enough data to train on, though, and getting it would be essentially impossible.",ho8yymr,t3_reoznh,1639319182.0,False
reoznh,Maybe as a tool to recognize certain pattern in the handwriting for a expert on this field. But I think ANN is a overkill for this purpose. I can think of a kind of template matching combined with a certain threshold.,hoa79r2,t3_reoznh,1639336935.0,False
reoznh,"Neural Nets can do any learnable task, it has an infinite hypothesis space. This is just identifying similarity between handwriting and there are tons of stuff on this out there already. 
https://arxiv.org/pdf/1606.06472.pdf 
here is a paper that does just that,",hoa8g8q,t3_reoznh,1639337386.0,False
reoznh,No they can identify psychopathic styles of writing or possible pick traits of identity but it can’t prove that someone is a murder verse just a psychopath,hoa6z7u,t3_reoznh,1639336828.0,False
reoznh,"I don't know if a neural network is required. Similarity analysis can be done with a variety of techniques.

But you could use a neural network. Depending of what you want to achieve you 
a) train the neural network to identify if two given hand writings are from the same author,
or b) identify if a given handwriting originates from a specific person.

For a) you just have to collect pairs of two handwritings from the same person (maybe couple days/months/years apart) and train the network on those pairs (and of course pairs of not matching handwritings for the negative case).

For b) you would need to collect handwritings the persons you want to identify and train the network to match the writing and the person. Than you can let the network predict the author of a new handwriting, given the person was in the training set.",hoa5vk7,t3_reoznh,1639336414.0,False
reoznh,Maybe dust the note for fingerprints. Or collect DNA from the note. The post mark on the envelope the note came in.,hobvuts,t3_reoznh,1639362832.0,False
reoznh,"No, because my handwriting changes constantly. I don't know how people keep a consistent writing style. They might as well have their own font tbh. I don't even know cursive so my signature is basically a few letters have fancy lines, and then it's also not consistent.",hod60cv,t3_reoznh,1639392655.0,False
reoznh,"I don't know about neural networks.... but people like do this for a living.  

You can always type what you want to say into a typewriter and then trace the output onto a piece of paper. Undetectable at that point.",hpbg6br,t3_reoznh,1640021749.0,False
reinb8,It's just regular mod. I think if you try and think of some examples it should be pretty clear. Consider 4 mod 5. Both 2\^2 and 3\^2 are congruent to 4 mod 5 so 2 and 3 are the modular square roots.,ho7wdf6,t3_reinb8,1639290878.0,False
reinb8,"Hey, thanks. That's a good simplification for me to begin with.",ho7wow0,t1_ho7wdf6,1639291097.0,True
reinb8,"I dont think this matters here, but sometimes there is a difference between “%” and modulo and the difference is how negative numbers are handled.  % quite often represents “remainder” and can get negative numbers, whereas if you want to stay in positives you might convert remainder to modulo by adding absolute value of m from n%m). So  -7 % 3 = -2 which is a remainder, and modulo would be -2 + 3 = 1. Fix me if I am wrong, might be talking nonsense haha",ho8c282,t1_ho7wdf6,1639303394.0,False
reinb8,"You might be right! I'm a math student so I was thinking about this concept from a pure math background, which is a world where you just say -7, -1 and 2 are all the same mod 3 and leave it at that. however I could see how there could be cases where you want to be more specific in computer science.",ho8d0rx,t1_ho8c282,1639304218.0,False
reinb8,"The meaning of ""mod"" depends on the authority of the source because the meaning of `%` differs in programming languages. In languages like JavaScript and Java `%` means remainder whereas in languages like Ruby and C# it means modulus. There is a difference between remainder and modulus which you can read about [here](https://dev.to/hamiecod/remainder-vs-modulus-3mc8).

[This math stack exchange question](https://math.stackexchange.com/questions/633160/modular-arithmetic-find-the-square-root/633174) might help you. I understood what is modular square root but ah I don't really know VDF so it would be better if you read the stack exchange answer as compared to my interpretation.",ho8a8dv,t3_reinb8,1639301825.0,False
rdr5st,"Learn the basics of a coding language (I don’t know js but I’m sure it’s a fine language with many pros and cons). Figure out how to solve fuzz buzz type challenges. Once you got those down, learn data structures by reading the theory then implementing them in your chosen language. Don’t just use the built in implementation if it exists, actually build it. Then learn algorithms the same way. I would guess that this task would take high 100’s of hours to lower 1000’s of hours. 

Non technical suggestions:
Have fun with it, if you get side tracked in a project or learning that’s a good thing. It means your having fun. 

Learn to google thing well and stack overflow is your friend. 

Find a supporting community to get you through the sucky times while learning and debugging code because there will be those times

Good luck",ho318j8,t3_rdr5st,1639198366.0,False
rdr5st,Thank you. Do you recommend learning data structures and algorithms through a book or online courses?,ho32rae,t1_ho318j8,1639199192.0,True
rdr5st,"I took a university course so I don’t know how much my experience will help. However I would recommend using the free resource on YouTube and a text (you can probably find one for free online). Follow the textbook as your lesson plan. Do your reading, then try and find a YouTube video if you don’t understand the topic. If you still don’t understand the topic write out a post as detailed as possible about what you do and don’t understand and ask here.

That is pretty similar to a university course with assigned readings, lectures and office hours if you need help. 

I just want to emphasize if you ask questions online, the more work you put into your question the more people will want to help you and the better help they’ll be able to give.",ho33t0y,t1_ho32rae,1639199776.0,False
rdr5st,Just do as many projects as you can. Comp sci/data anything is in such high demand if you have a couple personal projects you cna fully explain they’ll take you like no other. Get dat 100k son!,ho3d9r4,t3_rdr5st,1639205574.0,False
rdr5st,"I would focus on the programming language first.

Based on the gained know how I'd suggest you practice / play around with different data types, algorithms, etc.

You might want to use blogs, leetcode and / or ask concrete questions on some problems you encounter by the means of stackoverflow or reddit.

After some time and with some experience you might start to consolidate your knowledge by reading some theoretical CS sources.",ho3f6fa,t3_rdr5st,1639206944.0,False
rdo7ab,"I think the mistake is assuming that it is unsigned when it is adding and 2’s complement when subtracting. That is possible, but the computer will rely on other things telling it that. You could look at it as always being 2’s complement and the computer uses other logic (either in software or hardware) to determine if the overflow will cause a bug.",ho31ogs,t3_rdo7ab,1639198605.0,False
rdo7ab,"what twos complement overflows then it wraps. some processors will throw an error or raise a flag, and on others you have to write code yourself to check for it. 

7 + 1 = 0111 + 0001 = 1000 = -8

\-8 -1 = 1000 + 1111 = ~~1~~0111 = 7

[https://en.wikipedia.org/wiki/Overflow\_flag](https://en.wikipedia.org/wiki/Overflow_flag)

on x86 JO is ""jump if overflow"" using the overflow flag

[https://stackoverflow.com/questions/48619934/mips-overflow-detection-printing-the-result](https://stackoverflow.com/questions/48619934/mips-overflow-detection-printing-the-result)

this is a page about detecting overflow in mips (which offers no hardware detection of overflow)",hodle15,t3_rdo7ab,1639402772.0,False
rdlhtp,"Whats wrong with 1234567899, or am I missing something?",ho1zl62,t3_rdlhtp,1639180816.0,False
rdlhtp,"Sounds like monkey could be at 5, you open and close 4, then monkey goes to 4 and you open 5... And you missed it",ho200pf,t1_ho1zl62,1639181002.0,False
rdlhtp,"Let's assume the monkey started at cage 2, after you closed door 1 he went to cage 1 and now he is behind you and can just go 12121212 and you'll never catch him.",ho1zx8i,t1_ho1zl62,1639180960.0,True
rdlhtp,"Ok, I think I was presuming the monkey couldn't jump into a cage that you had just opened.

But I'm still not sure I understand why your solution (23456789987654322) *does* catch the monkey. If the monkey is at 1, you start at 2 and start moving up the cages. Meanwhile the monkey moves from cage to cage in the lower numbers. You get to 9, then start moving back down again, and then at some point you get to one-cage to the right of the monkey, open it, monkey isn't there, and in the next move the monkey moves into the cage (say, 4), and you continue moving down the cages whilst the monkey moves up and you still don't catch him..

Or am I still missing something?",ho21a23,t1_ho1zx8i,1639181561.0,False
rdlhtp,"The monkey can only move to the adjacent cage, so if it starts in 1: at the moment you open cage 9, it must be in a cage with even index. By choosing the 9 again, you know the monkey moved into an odd cage, so it cannot be adjacent to you. As you go back downwards, the monkey must also move away and will never be adjacent and cannot escape to the higher cages.",ho4nzxl,t1_ho21a23,1639236389.0,False
rdlhtp,">so if it starts in 1: at the moment you open cage 9, it must be in a cage with even index.

Ok, so when you open cage 9 for the first time, lets say the monkey is in even-numbered-cage 6.

>By choosing the 9 again, you know the monkey moved into an odd cage

Ok again, so when you open cage 9 for the second time, lets say the monkey moves from 6 to an odd-numbered cage, as you say, so lets say 7

>As you go back downwards, the monkey must also move away and will never be adjacent and cannot escape to the higher cages.

Wait what? The next move from OPs example was to open cage 8. No monkey in cage 8, so close the door. Monkey moves from 7 to 8. OP then continues moving down to 7 and so on, missing the monkey.

Sorry for dragging this out, I'm just not sure I understand the problem..",ho4rv8s,t1_ho4nzxl,1639238096.0,False
rdlhtp,">Ok, so when you open cage 9 for the first time, lets say the monkey is in even-numbered-cage 6.

>Ok again, so when you open cage 9 for the second time, lets say the monkey moves from 6 to an odd-numbered cage, as you say, so lets say 7

Sorry for the misunderstanding. I meant it moved into an odd cage after we opened 9 for the first time, so at the moment we are opening 9 again, it is already in an odd cage.

Step by step from this point:

* Monkey can be in 2, 4, 6, or 8
* We open 9, monkey moves to 1, 3, 5, 7, or 9
* We open 9 again, monkey is not there (so he is in 1, 3, 5, or 7), he can move to 2, 4, 6, or 8
* We open 8, monkey is not there (so he is in 2, 4, 6), he can move to 1, 3, 5 or 7
* ...
* We open 3, he is not there, so we know he is in 1 and has to move to 2, so we open 2 and find him",ho53iyb,t1_ho4rv8s,1639243048.0,False
rdlhtp,"I'm confused about the problem description as well but I think in your example if the monkey was in 2 to start and you opened 1, when you close 1 it could move to 1 and then you'd be checking empty cages for the rest.",ho20whb,t1_ho1zl62,1639181393.0,False
rdlhtp,Try to use extended pigeonhole principle,ho262s5,t3_rdlhtp,1639183676.0,False
rdlhtp,"I forget all the formal terms and rules for mathematical proofs so apologies in advance for the mistakes.

  
Fact 1: After even number of moves monkey will be on same parity (odd/even) as it started, after odd number of moves it will be on different parity.

  
Start at door 2, go up 1 door at a time (2, 3, 4,...)  


Fact 2, starting from the left and working your way up one door at a time, if monkey starts on same parity door you start on monkey will be caught at worst when you get to n - 1 door where n is the number of doors.

  
Proof by induction

  
base case 3 doors n = 3

  
proof by exhaustion

If you start on door 2 monkey must be on 2, you open door 2 and catch him

  
If you start at 1 monkey must be on 1 or 3. If on 1 you will catch on first check. After first check monkey must move to 2. So on second check you will catch it. 3 - 1 = 2

  
case k + 1 (4 doors) n = 4  
proof by exhaustion

If you start at door 1 monkey and monkey starts on door 1 you will catch it on first check.

If you start on door 1 and monkey starts on door 3 you miss first check, if it moves to door 2 you will catch on second check as shown in base case. If monkey moves to door 4 on second move you will miss on checking door 2. But on 3rd move it must move to door 3, you will check door 3 and catch it.

If you start on 2 and monkey starts on 2 you will catch it on first check.

If you start on 2 and monkey starts on 4 you will miss first check, it must move to 3 and you will catch on second check.

  
By induction if monkey starts on same parity as you you'll catch it at worst on the n - 1 door. Fact 2 is true.

  
So starting on door two and working your way up, if you get to the n - 1 door (in our case 9) and there is no monkey, you know that the monkey couldn't have started on an even # door or else we would have caught it according to Fact 2. So this means the monkey started on an odd number door. After checking doors 2-9 (8 moves) the monkey must currently be on an odd numbered door according to Fact 1.  
We know the monkey must be on an odd door, so using symmetry we know if we start on an odd door (9 in this case) and go back down in same manner (9, 8, 7,...) and we know that we will catch the monkey at worst at door 2. So for 10 doors 2345678998765432 will at worst catch monkey on the last door. Just drop the last 2 off your solution.",ho50t2b,t3_rdlhtp,1639241909.0,False
rdlhtp,"The solutions we found is:
23456789987654322",ho1tlqy,t3_rdlhtp,1639178234.0,True
rdlhtp,"I think that the solution of 234...(n-1)(n-1)...432 is enough, so in your example with n=10 it would be 2345678998765432. 

You already explained in a comment that if the monkeys starts in an even numbered cage, we will catch him in the first ""half of the run"". I cannot proof this assumption but it seems to be true. I tested this with numbers up to n=6 and I could not find a counter example. I know this is not a proof, but I firmly believe this assumption is true.
Following this assumption, this means the second half of the run is only entered, If the monkey starts at an odd numbered cage. The two times we open cage at position (n-1) imply, that beginning from the second half of the run, every time we open an even numbered cage, the monkey is in an even numbered cage. Same goes for odd positions during the second ""half"", if we open an odd numbered cage, the monkey is in an odd numbered cage. So the monkey is not able to cross us.
Thus the opening of cage 2 two times in the end is unnecessary, opening cage 2 one time in the end is enough: if we open cage 3 during the second half, the monkey is in cage 1. Monkey then has to move to cage 2, and we open cage 2 afterwards.

So all in all, the 234...(n-1)(n-1)...432 approach gives us 2*(n-2) cages to be opened at maximum, which would be 16 for n=10. Please correct me, if I did a mistake anywhere!",ho42gku,t1_ho1tlqy,1639224667.0,False
rdlhtp,"Absolutely right! Thanks
I wonder if there is an even better solution that is based on other mechanism, but it seems pretty optimal.",ho4306r,t1_ho42gku,1639225044.0,True
rdlhtp,[deleted],ho2xknw,t1_ho1tlqy,1639196467.0,False
rdlhtp,I used 1-10 and not 0-9,ho3hsl3,t1_ho2xknw,1639208918.0,True
rdlhtp,"So the reason this solution works is this:
If the monkey starts on an even numbered cage (6 for example), you'll catch him in the first half of the run, because Any time you're opening an even numbered cage he will be in an even numbered cage and same for odd numbers. He can't cross you and thus you'll catch him.

If the monkeys starts in an odd numbered cage, the doubled 9 in the middle will make it so that you will open an odd numbered cage at the same time as the monkey will be in one. After that, when you'll go down the cages each even numbered cage you open, the monkey will be in an even numbered cage (same for odd numbers) and you'll catch him.

You can try it with 4 or 6 cages just to get the concept but it works the same for 10.and above.

I'll try to make an animation of this or something later.
If there are more questions about the riddle ask in the comments.",ho24jn7,t1_ho1tlqy,1639182997.0,True
rdlhtp,"Your post says: 

>the monkey can go to a previously opened cage

But your comment says: 

>he can't go to a cage you opened.

Isn't this a discrepancy?",ho2dle2,t1_ho24jn7,1639187057.0,False
rdlhtp,"Right, sorry about that.
Updated it, I hope now it's more understandable",ho3i3w5,t1_ho2dle2,1639209149.0,True
rdlhtp,I'm assuming which passage the monkey goes through is random?,ho219av,t3_rdlhtp,1639181552.0,False
rdlhtp,"Well yes but you should consider every move he can make, and find a solution that solves every move of him.
You can assume he'll know what door you are going to open next and try to find the best move for himself based on that knowledge.",ho254gg,t1_ho219av,1639183256.0,True
rdlhtp,"The solution depends on the number of cages. I'm using 0 indexing.

The idea is as follows. Guess 0..1...2....n-1. If the monkey was initially on an even index, we will always win in this first iteration. This holds for the general case where - if the monkey is on our right, and we guess an even index while the monkey is on an even index, we will always find it by linearly scanning.

So our goal now is to somehow ensure this happens.

With an even number of cages, if we guess 0...1...2...n-1, and we don't find the monkey, then it must have started on an odd indexed cage. After an even number of guesses, the monkey will be on some random even indexed cage, so if we guessed cage 0 twice ( to force the monkey to go odd -> even ). After that second guess, we've now guessed an even cage whilst the monkey was on an even index, so we know we will win.

Similarly, with an odd number of cages, we can do a linear scan once, but we only guess cage 0 once as well, as after an odd number off guesses, our orginally-odd-monkey would be on an odd cage, so if we guess 0 after n-1, we will once again gaurentee that we guess an even index while the monkey is on an even index as well.

This problem fucked me up because I initially thought there was a way to solve it with (a natural) recurrance.

For example, I tried to solve the cases n=1...4 by proving you can guess index 1 twice to gain information, ie that the monkey must be on the right as it couldn't be on 0.",ho5f20s,t3_rdlhtp,1639247847.0,False
rdlhtp,"Solution: 2, 4, 6, 8, 10, 1, 3, 5, 7, 9

This is a variation of the rabbit hole problem. If you play with the problem, you'll realize that the possible location of [the monkey oscillates between even and odd numbered cages](https://i.imgur.com/pQcCT5t.png).

The problem can be solved in linear time by guessing over all even cages first. Then all odd cages next. (or vice versa)

Here is sample code you can play around with using the rabbit hole problem (with a 100 holes). You'll notice that you'll always find the rabbit no matter how many times you run it.

    from random import randint
    
    mn = 0
    mx = 99
    
    def jump(rabbit):
      if rabbit == mn:
        return mn + 1
      
      if rabbit == mx:
        return mx - 1
      
      if randint(0, 1) == 0:
        return rabbit + 1
    
      return rabbit - 1
    
    
    def guess(rabbit, hole):
      if rabbit == hole:
        return True, rabbit
      
      return False, jump(rabbit)
    
    
    def main():
      rabbit = randint(mn, mx)
      caught = False
    
      for hole in range(mn, mx + 1, 2):
        caught, rabbit = guess(rabbit, hole)
        if caught:
          break
      
      if not caught:
        for hole in range(mn + 1, mx + 1, 2):
          caught, rabbit = guess(rabbit, hole)
          if caught:
            break
      
      if caught:
        print(f""Found rabbit in hole: {hole}"")
      else:
        print(""Did not catch the rabbit. :-("")
      
    
    
    if __name__ == '__main__':
      main()",hobgdjy,t3_rdlhtp,1639355781.0,False
rdlhtp,"This solution is easily proven wrong.

Assume monkey is in cage 6

G = your guess (the cage you open), M = the cage the monkey moves to

G2, M5 (you open cage 2, monkey moves to 5)

G4, M4  (the monkey moves to 4 after you close the cage

G 6, M5

G8, M6

G10, M7

G1, M6

G3, M7

G5, M6

G7, M7

G9, M6",hojqxpf,t1_hobgdjy,1639509973.0,False
rdlhtp,"Simplest, easiest solution: 
Open all the cage doors, without closing them. The monkey only moves when you close the door. This method will ensure the monkey is found within ten cages. 

Other solution: 
Stick your head into the cage and look down the passage. Assuming the passages are straight (given the cages are adjacent), you will have found the monkey. This method only takes opening one cage, and is therefore the ideal method. Of course, this approach may result in you getting your face scratched at or your head stuck in a cage. However, the problem only specified that the monkey needed to be found, not that the finder needed to emerge from the ordeal unharmed, and therefore this solution is completely functional. Q.E.D.",hor2qyy,t3_rdlhtp,1639637183.0,False
rdlhtp,"If there are n cages, the solution is:

1234....(n-1) n n (n-1) ....4321

The reason this works:

The distance function between the cage we look at and the cage the monkey is in is non-increasing in this case. However, parity of the distance function does not change as we go through 123...(n-1)n

So, if the parity was even when we checked cage 1, then we catched the monkey.

If the parity was old, we will catch him in n (n-1) ... 3 2 1.",ho29z1k,t3_rdlhtp,1639185426.0,False
rdc453,"You have inspired me. From now on I’m gonna use combinations of the letter i, j and l for my variables.",ho03t0t,t3_rdc453,1639153476.0,False
rdc453,"string aO0D = ""go fuck yourself"";",ho087d7,t1_ho03t0t,1639155217.0,False
rdc453,You called?,ho3jll6,t1_ho087d7,1639210291.0,False
rdc453,"main(){

int i = 0;

while(i=0){

cout << aO0D;

}

}",ho27gsr,t1_ho087d7,1639184298.0,False
rdc453,Lmao,ho04c2v,t1_ho03t0t,1639153686.0,False
rdc453,r/foundsatan,ho06z9c,t1_ho03t0t,1639154736.0,False
rdc453,"Here's a sneak peek of /r/foundsatan using the [top posts](https://np.reddit.com/r/foundsatan/top/?sort=top&t=year) of the year!

\#1: [â€™](https://i.redd.it/8u9uo2ekove61.jpg) | [18 comments](https://np.reddit.com/r/foundsatan/comments/la4j28/â/)  
\#2: [I guess i won't go hiking for a while](https://i.redd.it/op9bc53nzpb61.jpg) | [42 comments](https://np.reddit.com/r/foundsatan/comments/kylt43/i_guess_i_wont_go_hiking_for_a_while/)  
\#3: [Airpods](https://i.redd.it/7n0d5klmf4z61.jpg) | [18 comments](https://np.reddit.com/r/foundsatan/comments/nce4k9/airpods/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[Source](https://github.com/ghnr/sneakpeekbot)",ho070hx,t1_ho06z9c,1639154749.0,False
rdc453,"It’s from math notation where i,j refer to rows and columns. There’s no real reason it continues except that’s how people come to learn the notation and people stick with what they know",ho04gtz,t3_rdc453,1639153739.0,False
rdc453,"At the very least you should be using `ii` and `jj` in your code - if only to make the editor search function work usefully with them. Even then, lots of code quality tools will fail you for use of a 2-letter variable name.",ho3fm0d,t1_ho04gtz,1639207272.0,False
rdc453,U can’t use the same thing because there’s no differentiating,ho5a6me,t1_ho3fm0d,1639245829.0,False
rdc453,I’m aware that’s where it comes from but it’s dumb that it ever started that way.,ho0gqsm,t1_ho04gtz,1639158587.0,True
rdc453,"It's not. Index notation like `a_{i, j}` was a huge innovation, and massively helps readability. If you have doubts, consider reading Gauss:

https://www.gutenberg.org/files/36856/36856-pdf.pdf

Here, instead of noting related variables by, say `P_1, P_2, P_3`, Gauss uses separate letters of the alphabet `P, Q, R`. It may seem trivial, but once you have many examples of that going at the same time, it really affects comprehensibility. You also run out of letters pretty damn fast.

You may argue that `i` and `j` are poorly chosen, but they fit nicely in small type as a subscript, and allow packing in the type of dense information needed in communicating mathematics. I've never had any trouble distinguishing them in my own hand, and they stand out just fine in typewritten text.

You may argue that we shouldn't use single letters in the first place, and in some cases you may be correct. But in many others something like `a_{first index, second index}` just adds a ton of clutter, and I think you would find it to be a worse choice in practice.",ho0myfa,t1_ho0gqsm,1639161015.0,False
rdc453,"Bruv, he didn’t mean the invention of notation— he means the use of i,j in it.",ho3b1j5,t1_ho0myfa,1639204113.0,False
rdc453,I concur,ho30s4s,t1_ho0myfa,1639198126.0,False
rdc453,It started back when things were hand written and it was much more easy to distinguish between the two letters. This is why we need to invent a monospaced Papyrus font to use for all future coding. Thank you for coming to my TED talk.,ho0m7u5,t1_ho0gqsm,1639160728.0,False
rdc453,"Monospace is overrated. I've been coding in a quasi-proportional font for a few years now. Iosevka Aile Code. quasi- cause most chars are the same width, but space and i etc are half width.

https://typeof.net/Iosevka/",ho0tn92,t1_ho0m7u5,1639163691.0,False
rdc453,I've been coding in wingdings for 20 years and that's clearly superior.,ho0wfaa,t1_ho0tn92,1639164810.0,False
rdc453,"Why write a post asking why, if you already knew the answer?",ho22qqo,t1_ho0gqsm,1639182201.0,False
rdc453,"The worst is when a student/professor, or anyone really, writes handwritten m and n in sloppy cursive.",ho099ii,t3_rdc453,1639155639.0,False
rdc453,"> The worst is [...] cursive.

Agreed. :D",ho0zjm8,t1_ho099ii,1639166062.0,False
rdc453,"""minimum"" in cursive is especially bad",ho2f41i,t1_ho099ii,1639187741.0,False
rdc453,"Pull up a chair, young'ens, and hear about the old timey language FORTRAN.  Back in FORTRAN, variables were implicitly typed based on the first letter of their name.  Start your variable with letters between 'i' and 'n' and it would be an integer. Outside that range, it would be a float (if I remember correctly).  Why 'i' and 'n' ?  Because some clever language designer decided 'integer' thus the 'i' and 'n'.  Using i and j as loop variables started from there (in the 60s and 70s, probably) and has continued on ever since.",ho0m2bd,t3_rdc453,1639160668.0,False
rdc453,"FORTRAN still used today, by the way.  Is very important in numerical computing. Because of some quirks in the language design, FORTRAN compilers can generate some of the fastest code (faster than C).  https://en.wikipedia.org/wiki/Fortran#Science\_and\_engineering",ho0noyi,t1_ho0m2bd,1639161313.0,False
rdc453,Neat edge case.,ho0wn6q,t1_ho0noyi,1639164897.0,False
rdc453,"I believe numpy has some Fortran source for the fast mathematics, so not really edge case.",ho1oxcg,t1_ho0wn6q,1639176251.0,False
rdc453,"Well, in all fairness though, unless you're specifically modifying the Fortran code, you can use more user-readable  variable names.",ho1u7av,t1_ho1oxcg,1639178483.0,False
rdc453,I was just answering about the fact that Fortran is still used today in a iper used library. Of course using numpy doesn't require using Fortran (thank god) but just because it's under the hood it does not mean it's a super edge case.,ho3r91g,t1_ho1u7av,1639216341.0,False
rdc453,"fwiw, this is no longer the case, but there is still a lot of legacy FORTRAN out there.",ho22vej,t1_ho0noyi,1639182259.0,False
rdc453,"Yeah, even back when I was first learning FORTRAN (long long ago), our instructor emphasized 'implicit none' to turn off that auto-variable nonsense.",ho2o849,t1_ho22vej,1639191913.0,False
rdc453,"Fortran is not faster than C.  For numerical stuff, both are basically equivalent.  The only real difference that I have been able to find is for recursive functions, where Fortran is much slower.

A lot of fast code has been written in Fortran, and it will remain important I'm scientific computing, but it isn't a unicorn.",ho3mgp9,t1_ho0noyi,1639212498.0,False
rdc453,">some of the fastest code (faster than C)

smiling in assembly

Look I might want to explode every time I look at any x86 assembly, but damn is it fast.",ho3u375,t1_ho0noyi,1639218539.0,False
rdc453,What is it about those quirks that make it so fast? Why don't we use those quirks in the design of new languages to make them faster?,ho4ic9p,t1_ho0noyi,1639233657.0,False
rdc453,"Disclaimer: I'm not a compiler expert. This is just what I understand from reading over the years.

FORTRAN is a very simple language. It's in fact the first compiled ""high level"" computer language. The name FORTRAN comes from ""Formula Translation"" and originally it was just a simple way to write math. The problem with, for example C, is some of C's rules prevent super optimization of code. The one I understand that causes the most problems is aliasing. [https://en.wikipedia.org/wiki/Aliasing\_(computing)](https://en.wikipedia.org/wiki/Aliasing_(computing))

If the same memory location can be represented by multiple variables, then the compiler has to assume worst case and cannot optimize certain paths. If the language doesn't allow such flexibility, then the compiler can crack its knuckles and go to town on optimization.

(Lots of hand waving here, sorry. It's been a long, long time since compilers class!)",ho4zoou,t1_ho4ic9p,1639241445.0,False
rdc453,"That's actually a really good explanation and a good starting point for my investigation. I'll look in to that more, thanks!",ho5jafw,t1_ho4zoou,1639249634.0,False
rdc453,Reference: [https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/data-types-constants-and-variables/variables-1/data-types-of-scalar-variables/implicit-typing-rules.html](https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/data-types-constants-and-variables/variables-1/data-types-of-scalar-variables/implicit-typing-rules.html),ho0m80p,t1_ho0m2bd,1639160730.0,False
rdc453,Holy shit! I was SO sure you were just making this shit up.,ho13t6x,t1_ho0m80p,1639167752.0,False
rdc453,Oh noooo... Somehow it feels like every day I learn a new fun fact about old-school Fortran that makes me gag haha,ho1285z,t1_ho0m2bd,1639167125.0,False
rdc453,IMPLICIT NONE!!! One of the first things I learned about when I was learning Fortran.,ho1gt8a,t1_ho0m2bd,1639172894.0,False
rdc453,Indeed. We had 'implicit none' hammered into us when I as learning FORTRAN. Woe be to those who had to maintain enormous libraries of older FORTRAN.,ho2ofcz,t1_ho1gt8a,1639192007.0,False
rdc453,"Hence the old joke in nerd circles: ""God is real unless declared integer"".",ho3eule,t1_ho0m2bd,1639206698.0,False
rdc453,"Fotran was created by mathematicians. This notation using i,j as indices was there before Fortran was invented.",ho3dubw,t1_ho0m2bd,1639205978.0,False
rdc453,"We keep using (i,j) because it is evocative of all the other times we used (i,j).  We reuse them precisely so that it's like all the other times we used them, so that you can see it in a glance and have a sense of what it means.  We use x when we mean an unknown real number or a variable of a real number, we use z when it's an unknown complex number.  It's really helpful to quickly understand something, when we reuse these names.  I would find a text nauseating if it didn't use this orienting technique.",ho0eg29,t3_rdc453,1639157685.0,False
rdc453,Right. It's the principle of least surprise.,ho3eyh4,t1_ho0eg29,1639206779.0,False
rdc453,"I think OPs argument is against certain typefaces and fonts in which the two letters (i,j) can appear identical on initial glance.  Some people are more prone to this than others due to a variety of reasons.",ho0rrlw,t3_rdc453,1639162934.0,False
rdc453,"It was hard to write, it should be hard to read! /s",ho1odxc,t3_rdc453,1639176023.0,False
rdc453,"Complain to whoever came up with the alphabet, all those pairs are letters next to each other and that's why they're used like this.

'i' is index, 'n' is number (or natural), 'v' is vector or vertex.",ho0ejj8,t3_rdc453,1639157722.0,False
rdc453,"I just like to use letters that are close to each other, because it’s easier for me than coming up with another name. Sometimes I’ll even use a,b,c for indices. But if I’m not working with complex numbers, I often use i because it’s easy to remember i = index. And after that comes j,k,l.",ho07w0x,t3_rdc453,1639155093.0,False
rdc453,"Convention, mostly. I can see good, descriptive variable names being useful, but convention will almost always win out in math/compsci.   


None of us want to incur the wrath of our professors, and I know professors that would be instantly annoyed at using something less conventional.",ho07081,t3_rdc453,1639154746.0,False
rdc453,"The i is mostly a convention for [i]terators, I guess the J was the logical follow-up people thought of when they started this trend. I for one only use i in very concise and simples loops/iterations, otherwise short and descriptive names make more readable code / math problems.",ho0aclg,t3_rdc453,1639156069.0,False
rdc453,Lmao how can you get i and j mixed up. I can understand if you're reading handwriting maybe,ho06xng,t3_rdc453,1639154717.0,False
rdc453,"See page 6/30 (or 262) top of the right column of [this](https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf) paper I was just reading (a printed version of). 

Not sure how to attach a screenshot of it without too much effort.",ho0hrtn,t1_ho06xng,1639158982.0,True
rdc453,"Wow fair enough, terrible choice of font there",ho0owtl,t1_ho0hrtn,1639161798.0,False
rdc453,"Speaking as someone from a 'doin stuff' side of research more than hard core computer science, we have been trying and failing to have compsci and maths researchers branch out to use actual words that tell you what they are for as variable names for decades now with no success. They are more maths than program, twisted and evil.
 
The font issue is the journal they're publishing in, which was set for printed physical journals and has never been updated from their original latex style file.
 
This all comes down to the fact the reviewers in any discipline (not a compsci issue, this is an academia issue) are an incestuous club that all know each other and all know the work, so they expect to see an i where there is an i and a j where there is a j and it all feels natural to them. They would apply more scrutiny to a paper that broke from their club's tradition and wrote out inner_index and outer_index instead, and if you suggested updating the font they would crucify you.",ho0cbrq,t3_rdc453,1639156849.0,False
rdc453,"Self-documenting variables make sense in code, but using words for variables in formal proofs is not a good idea. Just put pressure on publishers to use better fonts and we can keep the conventions about what variable letters stand for in various contexts. That part is actually reasonably important for quickly grasping what's going on.",ho0mlv1,t1_ho0cbrq,1639160877.0,False
rdc453,what makes self documenting variables a bad idea in math?,ho0ubao,t1_ho0mlv1,1639163962.0,False
rdc453,"Mostly that it will make proofs hard to read. Formal statements are compact with lots of single character width symbols and potentially many variables. So if variables need to be entire words or multiple characters, these statements will no longer be compact. 

Further, formal statements are often quite general, so whatever words you pick for variable names are going to be quite general too, where it makes no difference if it's named `x` or `realNumber`.

Take the Pythagorean theorem for a simple example: 

`a^2 + b^2 = c^2`

Compare that to:

`sideOneLength^2 + sideTwoLength^2 = hypotenuseLength^2` 

But now imagine for much more complicated statements or entire proofs written out like this. 

In proofs, compactness is a virtue that helps with readability and understandability. There's a reason separate mathematical language was invented and we don't write formulas out like 

`some number minus another number times, the first number plus the second number, equals the first number squared minus the second number squared`

in a natural language like English. We just write 

`(a-b)(a+b) = a^2 - b^2`.",ho11ix5,t1_ho0ubao,1639166845.0,False
rdc453,You feel this way as you've never studied math. It can happen.,ho1xtw2,t3_rdc453,1639180055.0,False
rdc453,"I think it’s because you have to take the index of things a lot, but it’s shorter to type i instead of index. And then it just went to j after then and so on and so forth",ho0fmm9,t3_rdc453,1639158151.0,False
rdc453,"It’s mostly for convention. Picking alphabetically close variable names for variables that are related to each other is just common sense. It feels and reads weird, and gets confusing really fast, when you have a group of variables that are related but they all use arbitrary names. Even more so when the proof is several pages long and then you have to remember at each step what each individual variable does/is. 
 
So, if you pick i for something because it reminds you of index, iterator, image, etc. the logical follow up is to pick j after and not ξ, Ω, z, or “mojito”.",ho0lqsm,t3_rdc453,1639160543.0,False
rdc453,"Not sure about all of the common ones but I believe that ""i"" is typically shorthand for ""iteration""? At least that's the impression I got and why I use it. Also ""n"" is just shorthand for ""number"", so that's what I use as the input number for a function that then gets calculated in various ways.

Some don't make sense to me, like the connection weights in artificial neuron functions are typically a ""g"", and I'm not really sure why.",ho0vdkz,t3_rdc453,1639164391.0,False
rdc453,as long it's not i and jota i'm fine. lol,ho0wb6k,t3_rdc453,1639164764.0,False
rdc453,From now on I’m just going to use 39 i’s in a row as a variable to represent row index and 40 to represent column index,ho15myi,t3_rdc453,1639168460.0,False
rdc453,"i = iterator

j = next letter in the alphabet, if you've already used i as an iterator, same as k etc.

It really does make some amount of sense.",ho1v6je,t3_rdc453,1639178905.0,False
rdc453,*foo* rules!,ho1vgxi,t3_rdc453,1639179030.0,False
rdc453,felt this way starting algebra as I always thought multiplication was the 'x' symbol I asked why the hell would they make 'x' different,ho217qq,t3_rdc453,1639181532.0,False
rdc453,"I find it simple.

i, j, k for loops

m, n for matrix dimensions

p, q for probabilities

u, v for random variables

The point is, they are almost exclusively used as throwaway variables (index, dimensions, temporary, etc.). It’s very easy to understand what arr(i,j,k) means. arr(row,col,depth) isn’t as easy on the eyes. Also, there are languages where row and col may be reserved for functions.

In some languages, i refers to complex numbers so some people use ii instead to distinguish the i complex number from the ii variable.",ho2dct7,t3_rdc453,1639186951.0,False
rdc453,"Some fields of math likes to use character set to denote type, so you will have vector v in one vector space being projected into vector ν (Greek letter nu) in another vector space. Physics likes to do the same thing too with v and nu being the velocity of some object in different frames of reference. It's because all those academics don't need to maintain a code base over two decades of enhancements and bug-fixing.",ho2ej54,t3_rdc453,1639187474.0,False
rdc453,I will use capital i and l together. Thanks!,ho2hjzq,t3_rdc453,1639188856.0,False
rdc453,"LOL. OP complaining about i, j, k has clearly never encountered ξ in the wild.",ho2if1t,t3_rdc453,1639189244.0,False
rdc453,I’ve been revisiting my maths lately and have found that letters as variables can be easy to visually distinguish if you write the variables in cursive. I and J would not look the same in such a scenario.,ho2uzw6,t3_rdc453,1639195160.0,False
rdc453,I only use cool greek letters like ζ ϕ . It make me look smart.,ho2vq44,t3_rdc453,1639195523.0,False
rdc453,"I feel this also depends on the context. Like, you see those kinds of variables in proofs and in papers because the target audience is people who are interested/working in the field and so they know what they represent and they can make sense out of very quickly. It just gets straight to the point you know.

But then if you are say teaching to kids, then using variables like i, j is not helpful. They should be more meaningful and much more clearer. Like, forced documentation in a way. This allows less confusion and makes it easier for kids to understand for instance.

But I am with you. I tend to have long variable names because it's just so much more clearer to me when I am coding. And for anyone reading the code as well. Yes, it can clutter but there are times when it's helpful to have long names for certain code snippets. Not for everything but for some.",ho37t3j,t3_rdc453,1639202087.0,False
rdc453,Sometimes i use funny variables to fuck with my bosses,ho3d62u,t3_rdc453,1639205505.0,False
rdc453,"i for index, j comes after. They aren't that hard to distinguish.",ho3e76o,t3_rdc453,1639206225.0,False
rdc453,"Man, many people don't seem to actually read your post before commenting 🤦",ho3hpmg,t3_rdc453,1639208853.0,False
rdc453,Im pretty sure i is short for index and j is just the next letter. n is short for number and m is just the next letter and p is short for prime and q is just the next letter,ho3ndik,t3_rdc453,1639213230.0,False
rdc453,Because it’s more accessible to use pairs of similar letters to define similar things,ho3pdf3,t3_rdc453,1639214850.0,False
rdc453,"well matematician are a weird bunch, but every human being is not exempt. let them have their convention. as a programmer i learned that different lingo and conventions are used in each work environment so it's really not only a problem of mathematics.

i'm all for longer and auto documenting variables, but it's always a failed crusade from the start.",ho4cs00,t3_rdc453,1639230830.0,False
rdc453,"I study quite a bit of mathematics. I see (u,v), (m,n), (I,j), (alpha,beta) - and more - all over the place.",ho4i70i,t3_rdc453,1639233586.0,False
rdc453,"I prefer to choose these
I,k,n
t,p
Also a,b,c in some cases. Because they are distinguishable.
And just avoid mirrored or cloned letters all together.
No q,m,j (especially fuck j).
Also im annoyed by people who use single letters T,U,V
For generic parameter names. Hope they burn in hell.",ho4j6hs,t3_rdc453,1639234076.0,False
rdc453,"Now I want to use greek letters for my variable names. ([Swift](https://docs.swift.org/swift-book/ReferenceManual/LexicalStructure.html) permits this.)

Can you imagine something like:

    for ι in Α {
        ...
    }

Better yet, Swift allows custom operators to be defined, which means I now want `∈` to be defined as a binary operator to perform a 'set contains' operation.",ho4kjc6,t3_rdc453,1639234744.0,False
rdc453,u v was the worst for me when turning Assignments that are hand written,ho4yrrx,t3_rdc453,1639241068.0,False
rdc453,"Always have in the back of my head that ""i' stands for index and ""n"" for numbers... #OkayCaptainObvious :|",hpktxwk,t3_rdc453,1640193800.0,False
rcla2n,"NFTs in theory are one thing, but in practice they are nothing more than digital beanie babies or trading cards, except you don't actually own the beanie baby or trading card and just have a receipt for it. It's a disaster and [the top 10% of traders for NFTs have traded 97% of all assets](https://www.nature.com/articles/s41598-021-00053-8#:~:text=the%20top%2010%25%20of%20traders%20alone%20perform%2085%25%20of%20all%20transactions%20and%20trade%20at%20least%20once%2097%25%20of%20all%20assets) and the ecosystem is rife with fraud and wash trading (trading with yourself to inflate the perceived value of an NFT).",hnwetsd,t3_rcla2n,1639083631.0,False
rcla2n,"You *do* own the NFT, it's just that the NFT *is not the fucking art.* It's too expensive to hold an image on the chain, so all of those NFT images are stored on bog standard servers that *will* go down. Probably immediately after NFT's stop being a fad, for the big players, or when the creator dies, in the case of something like etherrock, in which case it'll be when he dies (or gets bored) and someone buys the domain and replaces the images with something like goatse.",hny4gum,t1_hnwetsd,1639110526.0,False
rcla2n,"Too expensive on transport protocols that require gas fees that are exuberant. We can store entire blu-ray movies in a decentralised manner with little to no cost. Non-fungible tokens aren’t limited to the transport layer they reside upon, they are method to verify ownership - just as pgp was used to verify authors. 

Cryptocurrency on the other hand is generally the protocol and the ledger it resides upon and none have are even close to getting that right just yet. Mining, staking, etc are no the answer.",hnyqum8,t1_hny4gum,1639124365.0,False
rcla2n,"The actual technology behind them tho is elegant. I believe OP came here looking for scientific reasoning, not scorn opinions. A decentralised global database where multiple applications can access and have verifiable proof that the data is what it says it is.",hnyoxfb,t1_hnwetsd,1639122866.0,False
rcla2n,"NFTs seems like mostly nonsense in their current applications (or more precisely, the applications I see most prominently featured in high profile discussions). It seems like an elaborate trick to make non-technical people believe that there is some form of scarcity for digital objects, which is ridiculous because they are inherently fungible. 

The idea of owning an NFT for an artwork is essentially meaningless. It's not tied to any kind of copyright law, and there is nothing stopping someone else from using  artwork for their own commercial purposes. It's barely a step above those websites that sell asteroids or stars to naive people. If asteroid mining becomes a thing, SpaceX isn't going to need your permission to mine all the precious metals on ""your"" asteroid because you have a certificate from a hack website. NFTs are just the same scam dressed in fancier tech jargon.",hnxbmbw,t3_rcla2n,1639097568.0,False
rcla2n,"> those websites that sell asteroids or stars

This is an excellent analogy, I'm using that, thanks.",huqat9q,t1_hnxbmbw,1643471187.0,False
rcla2n,"I foresee issues with the availability and scalability of a decentralized data store. Can we trim old data off the ledger to maintain its size growth? Will these systems be truly decentralized, or will there be some authoritative servers around to ensure low latency of queries?",hnwm220,t3_rcla2n,1639086571.0,False
rcla2n,"My feelings on it seem to flip every week or so, but I always come back to the opinion of it just re-inventing the wheel. I think there are great use cases for crypto like the transfer of money on a global scale, instantly. Other than that, I think it’s just an over-engineered concept that is in a saturated market (FinTech). 

Some of the qualities that Crypto has just don’t make sense to me on how they help the average person do average financial tasks. 

An open ledger of every transaction? Ok, cool. I’ll probably look at it once then never again. 

Decentralized assets? Neat and some people’s cup of tea, but now I have all the responsibility of managing them. 

I don’t think Crypto will ever go away. I think history tells us not to bet against innovation. In the current state, I do hope the Crypto bubble implodes like the Dotcom bubble. Too many scams, too many get rich quick mindsets, and too many shitty opinions (like mine lol)",hnw7mgc,t3_rcla2n,1639080734.0,False
rcla2n,Most FinTech companies are just wrappers around the existing system that hide things like delayed settlements and give you nice APIs. The fact that so many 100+B payment companies exist shows how much money is being made by being a middleman. Things like micropayments will never be possible with the existing system. With crypto this is possible. For example with brave you get payed in BAT for the ads you see.,hnx2ycy,t1_hnw7mgc,1639093684.0,False
rcla2n,"Can you elaborate more on micropayments not being possible with the current system? I sometimes use Brave and loved the idea of their BAT system. 

Other than that, what other micropayments are out there? How is it not possible with the current system? 

I have used mobile apps before where you can rack up points while using the app’s particular function (granted I can’t remember the name). Credit card rewards are basically the same thing as Brave’s Bat system. I can transfer those rewards into cash. I don’t see how crypto makes any new leaps in that department",hny39gr,t1_hnx2ycy,1639109948.0,False
rcla2n,"Modern banking is built on ACH, which is where the stuff he's talking about comes from. It's got a 3 day clearing time, when micropayments need to be faster. Of course, building a replacement for that on a slow, expensive, polluting chain is stupid and can't actually deliver. Fortunately, the fed is creating an ACH competitor called [FedNow](https://www.frbservices.org/financial-services/fednow/about.html) which should hopefully drive these middlemen out of existence, or at least lower their take.",hny543b,t1_hny39gr,1639110836.0,False
rcla2n,Here in EU we already have instant bank transfers within countries and soon also instant bank transfers across the EU. So instant crypto transfers hasn't really covered many use cases here.,hnyassb,t1_hny543b,1639113794.0,False
rcla2n,"As others have mentioned there are now system that allow instant transfers but I don’t think something like BAT would be possible with these systems. They are also only available regionally and in their local currencies. 

In addition to micropayments tokens make new incentive structures possible. For example early facebook or twitter users didn’t get any upside for participating but where some of the most important users. With tokens you could pay early users. This leads to super charged growth since their are more incentives to use a new product. The equivalent in the fiat world would be to give early users stock but this is regulatory nightmare and can definitely not be done in a micropayment way.",hnyhocf,t1_hny39gr,1639117880.0,False
rcla2n,"I understand what crypto and a NFT is. I thought crypto was cool back in 2010 when I first found out about it.

Nowadays crypto (bitcoin) has so many problems imo. Electricity usage, money laundering, used for illegal activities, etc.

NFT just seems like a pyramid scheme / bubble. But it could go on for a long time. Im happy with owning stocks and not messing with crypto / NFTs personally. 😊",hnvfxh7,t3_rcla2n,1639069810.0,False
rcla2n,"[1% of crypto is used for illegal purposes](https://www.forbes.com/sites/haileylennon/2021/01/19/the-false-narrative-of-bitcoins-role-in-illicit-activity/).  Regular cash is close to 5%. I don't get why people wouldn't be interested in this technology.  

The idea of a tamper proof [block chain](https://www.youtube.com/watch?v=bBC-nXj3Ng4) is also very interesting to me from a technical aspect. I've seen governments and organizations even testing out the block chain to store documents as a test to see if it could increase transparency. 

Ethereum is a [virtual machine](https://www.youtube.com/watch?v=gjwr-7PgpN8) that's decentralization.  It might not be practical or have a ton of use cases but the concept is wild isn't it?



[DAO](https://www.youtube.com/watch?v=KHm0uUPqmVE) and [smart contracts](https://www.youtube.com/watch?v=pA6CGuXEKtQ) are again yet another interesting development.

Am I wrong in thinking this stuff is interesting.  Are they dead ends?",hnwrbi9,t1_hnvfxh7,1639088719.0,False
rcla2n,"Imo you've done a great job of encapsulating how I feel about cypto. Which is, a bunch a vague ""cool ideas"" and ""interesting concepts"" that are ultimately _currently_ useless, for most people. I'm sure the very small (in global terms) number people who have become extraordinarily wealthy overnight vehemently disagree with me, but that does not a practical use case make.",hnxbd3x,t1_hnwrbi9,1639097452.0,False
rcla2n,Haha exactly. I suppose  I don't know enough to see it as useless right now. I'm old enough to remember the same comments about Netscape and such.,hny76ep,t1_hnxbd3x,1639111847.0,False
rcla2n,"Doesn't matter if it's interesting, it's using the energy of a small country while making less transactions than your average bank, this is a huge issue and anyone who pretends it not is deluded.",hnx7bbe,t1_hnwrbi9,1639095621.0,False
rcla2n,"Nothing is free in life.  The energy is used to secure the global network and valid every transaction. Think about how much bitcoin is worth. That energy is doing something and it's still less than the phantom energy that's drawn by people leaving their electronics plugged in all the time.  At the same time, proof of stake consensus will use a fraction of what proof of work uses.  Saying all the, adding more transactions doesn't burn more energy.  The energy is consumed by miners competing.",hny8qzz,t1_hnx7bbe,1639112679.0,False
rcla2n,The phantom energy is contributed by millions if not billions of people. Crypto miners/users/traders make up a tiny fraction of this,hnyu5ve,t1_hny8qzz,1639127116.0,False
rcla2n,"Yet that energy in crypto is securing and validating billions worth of wealth. It is doing work.  Like the phantom energy problem, it's disturbed and therefore large.  But unlike the phantom problem, people are actively working to solve and innovate ways to reduce that problem.  But at the end of the day it's still doing work.",hnzagve,t1_hnyu5ve,1639139720.0,False
rcla2n,"[citation needed] on that energy comparison to phantom energy. I think that’s complete rubbish, commonly spread by Bitcoin advocates to distract from its fatal energy problem.",hnylilk,t1_hny8qzz,1639120395.0,False
rcla2n,">Bitcoin consumes a sizable amount of electricity. As of June 2021, estimates suggest something around 110 terawatt hours (TWh) per year, which, for scale, is close to the electricity consumption of the Netherlands (111 TWh) but a bit less than the global ‘phantom’ electricity consumption from electronics that are left plugged in while in standby mode (124 TWh). 


https://www.coincenter.org/education/crypto-regulation-faq/understand-bitcoins-energy-use/

Crypto Currency has an energy problem. I'm not going to argue its fine. But people are trying to solve it. The energy used also does work. It secures and validates all transactions worth billions of dollars. One of my first classes in school the prof detailing how writing buggy code that gets distributed globally ends up wasting energy. Energy consumption isn't just a bitcoin problem. But the energy it consumes doesn't mean what it does is useless or not worth learning about. The consensus mechanism are super interesting to me. I'm reading proofs and white papers and digging into how cryptography works because the whole concept of this network is fascinating.",hnzgntb,t1_hnylilk,1639143241.0,False
rcla2n,"Crypto is used way less than cash for illegal activities. The electric usage will be hugely mitigated when the blockchain will switch to a new transaction verification system call proof of stake. Ethereum (the 2nd biggest) will switch soon next year, countless others already did. 
I think it's really superficial to just mention the issues that are already getting overcame, without mentioning its possible use cases and the blockchain potential.",hnvwuh2,t1_hnvfxh7,1639076421.0,False
rcla2n,"I hope crypto does well, im just not willing to bet money on it going up from here.

I live in Sweden where almost no one uses cash anymore. Everything is card or swish (free instant digital money transfer). Sure it would be nice if something like Swish existed globally, which was the hopes of bitcoin several years ago. There are still problems that need to be solved though like money laundering, transfer fees, exchanges getting hacked and you losing all your coins 😊",hnvy1lb,t1_hnvwuh2,1639076910.0,False
rcla2n,"I also think a thing that got in the way of crypto is that it became an investment rather than money, specially in the mainstream. It feels like most people who get into crypto are in it to make money, rather than the actual reason crypto was first invented (and the reason you probably thought it was cool in 2010 but dislike it now).

Also, the fanbase. Like so many other things the crypto/nft fanbase is cringy to the point of being annoying and I wouldn’t want to deal with them, and that includes financial transactions and such.",hnw36gc,t1_hnvy1lb,1639078972.0,False
rcla2n,"Yes, that's definitely true, but how else is it supposed to grow as a project and start being actually used? 
Why should someone buy crypto to use it? 
It's just like stocks - most of the times people buy it as an investment to earn money, not really caring about what's behind. The same thing goes for crypto, and that's to be expected - how else is it supposed to become widely used, if not through investing at first? 
It is slowly being adopted widely and having a real use case, but of course it'll take time. 

Regarding the fanbase, sure, can't disagree with you, however that's a big generalization. There is a good chunk of really smart people who are trying to actively improving things in this sector with new projects and interesting discussion about blockchain technology, don't get biased by what you see on the surface, as in most fields the worst part of the fanbase is what makes the most noise.",hnw7n47,t1_hnw36gc,1639080741.0,False
rcla2n,"IMHO, the first part of your comment doesn’t really make sense. Cryptocurrencies are meant to be a currency. This is entirely different from stocks, which are, by definition, an investment. There are valid reasons to use crypto as a currency, too, namely the very reasons it was created for: decentralization, anonymity, universality, etc. In fact, it was used for those ends before it was an investment. However, these are things that the average person doesn’t care about, and “making an investment out of them to begin with” doesn’t change that. Stocks have been around forever now, and they’re still not used as currency (you don’t buy things in the market with stock, or pay for services, etc.) because, again, they’re fundamentally an investment, and not a currency, as opposed to cryptocurrencies. 

I’d go so far as saying that crypto as an investment is inherently a bubble based on hype and misinformation. Stocks are associated with the worth of a company, so, in theory, it makes some sense that the price changes with time, as the company’s value also varies according to how well it performs. Crypto, on the other hand, is entirely based on: will people adopt this currency? However, as long as people face it as an investment, they won’t be using it as currency (because it lacks the stability that a currency needs, after all, if you can’t be sure of its worth tomorrow, you probably won’t want to stake your livelihood on it, which is why people won’t be inclined to take it directly in exchange for goods and services on the mainstream, which is even more reason for it’s instability since it exists only tethered to some other currency, i.e.: it doesn’t have “independent” value, it’s worth only however many dollars people are willing to pay for it at the moment)

So, I’d argue that using crypto as an investment only pushes it farther and farther from it’s intended use, and the only use that I, personally, see value in, which is as a currency.",ho2yyp6,t1_hnw7n47,1639197170.0,False
rcla2n,"It's been in a constant up trend for the past ~5 years, why should it stop now, when more and more people and getting involved? 
Take a look at the big picture and check the price of bitcoin or other coins just 1 year ago and you'll see the huge increase. 
Sure, some problems still need to be solved, but most of them are already solved. Exchanges got really secure and I haven't heard about an attack to a major exchange in the last couple of years. Defi platforms are surely more vulnerable as they are really new, so they are expected to be less secure.",hnw89kt,t1_hnvy1lb,1639080988.0,False
rcla2n,"Because it’s all speculative investing (with no real value), not actually used as a currency",hnw9yb1,t1_hnw89kt,1639081660.0,False
rcla2n,"Just because it's not used as a currency it doesn't mean it has no real value. Also, the current banking system has been used for more than 200 years, do you think it can change in just a few years?",hnwl2i6,t1_hnw9yb1,1639086170.0,False
rcla2n,"Blockchain tech: real value. 

A currency increasing in value by 100% in a day: speculative investing (and a bubble that will crash). 

If it one day doesn’t do this, maybe I’ll reconsider that take.",hnwlkme,t1_hnwl2i6,1639086374.0,False
rcla2n,"It's been ~10 years, and there has been no major crash yet, or no crash that hasn't been recovered, I think that can be a solid proof. 

Obviously, it's still a new sector, so huge changes are expected. But as the total market cap grows, it will become more and more stable. Right now, the total market cap is less than just Apple's, so it has a potential of growing",hnwpj0c,t1_hnwlkme,1639087976.0,False
rcla2n,"Yeah but with this argument, you still aren’t getting it. Bitcoin is meant to be a currency. Bitcoin is not an investment. Nobody talks about currency’s market cap or growth potential. A currency will not be used as a currency if there is a huge potential to grow, because it’s not advantageous to actually “spend” that currency.",hnx9xfo,t1_hnwpj0c,1639096790.0,False
rcla2n,"Bitcoin is not meant to be used as an actual currency. It's meant to be a store of value, like gold. There are other, way more valid candidates to be used as actual currency, with really fast transaction speeds and low (or zero) fees such as NANO. Also, stable coins are also a thing!",hnycyqx,t1_hnx9xfo,1639115044.0,False
rcla2n,Madhof’s Ponzi scheme lasted 30 years.,hnylsm7,t1_hnwpj0c,1639120576.0,False
rcla2n,Apple makes something. Bitcoin does not. A currency should not increase in value like a stock.,hnx3qpw,t1_hnwpj0c,1639094030.0,False
rcla2n,"> It's been in a constant up trend for the past ~5 years, why should it stop now, when more and more people and getting involved?

Said by everyone who was so sure **their** investment bubble of choice would **never** burst like all those *other* bubbles did.",hnxfcm8,t1_hnw89kt,1639099265.0,False
rcla2n,"I genuinely do not understand the downvotes.
Addressing Bitcoin problems when talking about the blockchain technology is such a poor argument.",hnx8f5q,t1_hnvwuh2,1639096114.0,False
rcla2n,"NFTs are just outside of their time; not early or late to the party, just irrelevant for our world, I feel. An NFT could, in theory, represent something like a certificate of authenticity for digital goods or even paired with real goods, but that isn’t how I see this trend going. I don’t think the adoption will allow for something dope like Nike starting an authentication service to issue NFTs of authenticity to help mitigate fakes in the show resale market. I don’t see something like a physical plot of land or house coming with decentralized documentation of ownership in the way deeds exist now. So many other things would need to happen in legislature that never will. 

I’d love to be wrong but I think the concept is meant for a society more capable of adaptation than the one we’ve found ourselves in.",hny3nfb,t3_rcla2n,1639110133.0,False
rcla2n,"As far as my understanding goes, NFTs are just mappings of some asset, be it digital or physical, to a digital signature.

It is the interpretation of this as some form of ownership that people focus on. If the majority does believe that having a record that maps something to you represents this, it effectively becomes the truth.

Same thing for crypto currencies, where value is placed on an association with your wallet and the sum of all your transactions.

Seems arbitrary, but the same thing goes for money - if nobody would believe it has value, it would not have it.

My opinion is still sceptical. Crypto currencies praise themselves as decentralized - which may be true as long as you stay in an ecosystem where all goods and services may be paid for with such a currency - but when you want to exchange them for ""real"" money you need to go to a centralized exchange, where you can be identified and privacy flies out of the window as well.

So yes, as long as there is a more relevant ecosystem outside of crypto, it will not be achieve what it promises to be - while polluting the environment and driving GPU prices up.

As for NFTs, your suggestion about money laundering did not occur to me before that but I would have my doubts (see the above statement about privacy).  


Both currently look mainly like speculative assets, less than stable or serious alternatives or additions to what we have currently - while NFTs may just be a tiny bit weirder than the actual art market \^\^",hnwhq1o,t3_rcla2n,1639084806.0,False
rcla2n,"> It is the interpretation of this as some form of ownership that people focus on. If the majority does believe that having a record that maps something to you represents this, it effectively becomes the truth.

Wrong. Ownership requires control, but NFT's provide no control over the image, only the receipt on the blockchain.

> Same thing for crypto currencies, where value is placed on an association with your wallet and the sum of all your transactions.

Wrong, not the same. You can control what happens to the crypto in your wallet.

> Seems arbitrary, but the same thing goes for money - if nobody would believe it has value, it would not have it.

Mostly wrong, cash's value is maintained by the fact it's the only way you can pay taxes and therefore participate in the US's (or whichever countries) economy. So it's only as valuable as the ability to participate in the economy, which can be torpedoed if that suddenly becomes less valuable through various mechanisms, some of them with a psychological component. 

Crypto's largest selling point is the trustless ability to conduct transactions. Not useful in the modern, high trust world but extremely useful in trustless exchanges such as for crime, drugs, or scams, which it was immediately used for and then for basically nothing else.",hny708b,t1_hnwhq1o,1639111760.0,False
rcla2n,">Wrong. Ownership requires control, but NFT's provide no control over the image, only the receipt on the blockchain.

True that. You can not effectively control your asset through that association - people would *really* have to believe in NFTs to restrict ther usage to only you!

&#x200B;

>Wrong, not the same. You can control what happens to the crypto in your wallet.

No question about that - I was trying to point out that the perception of value is similar.  
But you could also argue that you have ""control"" over which NFTs you ""own"", since you can decide to purchase them. Their value (like crypto currencies) is then just a result of what others believe it to be.

Sure, ""leaving"" conventional money would be a whole lot more complex than something like Blockchain, but that tail of taxes, economy is merely the result of people seeing it as something valuable for a very long time.

I fear that my comment seems a little too pro-Blockchain, so let me just note, that this is absolutely not the case! Mainly I wanted to point out that people have alway put value in objectively useless assets and that these hype-things are just another incarnation of it, solving mostly issues that were tailor-made for them.",hnymng5,t1_hny708b,1639121173.0,False
rcla2n,">	cash's value is maintained by the fact it's the only way you can pay taxes and therefore participate in the US's (or whichever countries) economy.

I don’t think cash’s value is so necessarily tied to taxes. If the US government decided tomorrow that it would continue to back and support the dollar through the Fed, but not charge taxes, I don’t think people would consider the dollar to have lost value as currency.

Rather, cash’s value is based on a shared belief in that value as it relates to goods and services. It’s that shared belief that means that we can agree that a dollar is worth a glass of lemonade, or fifteen dollars is worth an hour’s labour at a convenience store. The goods or services that a dollar is worth can change over time. There is no equivalent for crypto - its value is determined by how many dollars (or other fiat) it’s worth, not how much it can be traded for in goods or services. So long as crypto measures value against fiat, it’s not a currency, it’s a good (in this case, a speculative investment).",ho6lwvz,t1_hny708b,1639266846.0,False
rcla2n,"Cashes *definite* value is a shared fiction, but the reason it *has* that value is because all goods and services are *forced* to be related to it or else the government will punish you. Without that universal value assignment, cash becomes just strips of paper.

> If the US government decided tomorrow that it would continue to back and support the dollar through the Fed, but not charge taxes, I don’t think people would consider the dollar to have lost value as currency.

First of all, that support isn't cheap. Financial crimes, counterfeiting, trade management, political ties, and national self defense *all* feed into the value of the dollar, to say nothing of the vast amount of infrastructure, laws, education, enforcement, etc that keeps the 300 million person society ticking. Those 300 million people form a market whose lingua franca is the dollar, and *everything* they do is based on that dollars value. Without taxes, they can do whatever they want in any medium of exchange they want. ""Taxes give the dollar it's value"" isn't just a simple statement, it captures the fact that without taxes *there is no dollar.*",ho6whd1,t1_ho6lwvz,1639271881.0,False
rcla2n,"I think it's a sad joke. The only way it is sustainable (and I use that word very loosely) is if you perpetually manufacture artificial scarcity.
It remains viable only by sustaining growth, ergo growth by artificial scarcity. Growth for the sake of growth is - well - the very definition of cancer. NTF and crypto are cancerous, and operate on the same model as Capitalism, quite frankly. We do not live in an infinite-growth paradigm and any model based on that will end disastrously.",hnwq89e,t3_rcla2n,1639088267.0,False
rcla2n,"Still waiting for crypto to offer one, just one of all these hypothetical use cases that would be of any value to me, because I have been reading about how it is set to revolutionize X, Y, and Z for years now. Does anyone use DLT for anything at all in their daily lives, other than speculation? It doesn't seem like asking much given the insane amount of brain power and capital that has poured into crypto over the past decade. Decentralized money is still cool, admittedly.",hnwoxpz,t3_rcla2n,1639087733.0,False
rcla2n,"There are many interesting use cases for crypto/Blockchain tech, the issue really comes with the scalability, or lack of it, which renders most of those use cases totally invalid.",hnx547y,t1_hnwoxpz,1639094629.0,False
rcla2n,blockchain has many use cases and I personally know if it being used in chemists to track supply - does the agency I know of that is using this utilise or have a need for a token? no,hnyra02,t1_hnwoxpz,1639124705.0,False
rcla2n,"Monero is pretty cool if your use case is money laundering, otherwise it's all useless",hnyazh1,t1_hnwoxpz,1639113896.0,False
rcla2n,"Not as useless as ur mother
***
^I ^am ^a ^bot. ^Downvote ^to ^remove. ^[PM](https://www.reddit.com/message/compose/?to=YoMommaJokeBot) ^me ^if ^there's ^anything ^for ^me ^to ^know!",hnyb2aj,t1_hnyazh1,1639113941.0,False
rcla2n,"Nfts will be used by ticketmaster for tickets. This alone if it works will be huge as all baseball/football games tickets could move that way as well as theatre cinema tickets, it creates the possibility for secondary markets, you can then have follow up “souvenirs” in the secondary market and more consumer analysis. Then if you combine it with recoverable wallets it could be a game changer for all sorts of things where people register ownership to literally anything.

Cheap, reliable, fraud proof, digitised. The future

Nfts that are “art” I don’t get but I can understand the historical significance of this tech right now having value.",hnwhghf,t3_rcla2n,1639084700.0,False
rcla2n,"Thing is, having a secondary market for tickets doesn’t require NFTs. It just needs the venues to allow resale or reassignment via their own platform.

The reason they try not to is because secondary markets already exist with vastly inflated prices.",hnwk1n0,t1_hnwhghf,1639085747.0,False
rcla2n,"Having a digital market place and being able to verify on chain makes the process a lot more trustworthy and fraud proof vs using physical tickets. If it wasn’t any better, companies like ticket master wouldn’t waste their time.. I trust they can see more potential than I can.",hnwp63b,t1_hnwk1n0,1639087829.0,False
rcla2n,">  it creates the possibility for secondary markets

Not a sports fan, but this sounds horrible to me. Won't that just open it up for scalpers to exploit another market?",hnytnjd,t1_hnwhghf,1639126683.0,False
rcla2n,Could enforce a price policy that makes profiteering impossible with on chain transactions,hnyu6c2,t1_hnytnjd,1639127128.0,False
rcla2n,"Hum have you ever heard something about magic beans ?

https://lib.rs/cryptography/cryptocurrencies

Personnaly I think NFTs are just an another overhyped thing only based on speculation. Nothing new.",hnx0lwq,t3_rcla2n,1639092662.0,False
rcla2n,"It's another fucking stupid application of a niche technology - and edgelords everywhere are being suckered into it at lightspeed.  


Riddle me this: Who enforces your so-called ""Digital property rights""?   
What is the advantage of putting it on a blockchain? 

The answers are: Nobody, and nothing. It was another idea created simply so someone could, essentially, sell thin air. Only a fucking moron would buy into it.

...But I am at least honest with myself: I know that folk will put value into anything. So with that in mind ***inb4*** *""x number of people would disagree with you!!!11one""*",hnxyjqx,t3_rcla2n,1639107763.0,False
rcla2n,Mostly a way to launder money.,hnyye3o,t3_rcla2n,1639130702.0,False
rcla2n,you asked a very good question op,hnzg18s,t3_rcla2n,1639142915.0,False
rcla2n,"There are dozens of cool applications for NFTs. Digital art is what made them popular but what's next is cooler

* diplomas
* certficate of authenticity
* land title
* concert tickets",hnyew4b,t3_rcla2n,1639116189.0,False
rcla2n,Home/Car title is the only really good use for NFTs IMO,hocpqa4,t1_hnyew4b,1639379144.0,False
rcla2n,[deleted],hocrg82,t1_hocpqa4,1639380448.0,False
rcla2n,It would make fraudlent selling a lot harder because everyone can see who owns the NFT title on the blockchain and no one would buy without the seller showing the NFT in their wallet.,hoctapd,t1_hocrg82,1639381879.0,False
rcla2n,[deleted],hoctq43,t1_hoctapd,1639382222.0,False
rcla2n,"It isn't ""needed"" per say, it would just increase consumer confidence that they are buying from the owner of the property.",hocunrt,t1_hoctq43,1639382978.0,False
rcla2n,"I think it’ll make waves in the gaming world. With the death of brick and motor, the inability to recycle titles like you normally would is going away. These could be exchangeable in an NFT marketplace and would make an attractive business model to consumers, and the publisher could get paid for every resale transaction. I can see Amazon doing the same thing with Ebooks maybe.",hny4o66,t3_rcla2n,1639110625.0,False
rcla2n,This is the only use case of NFT’s I’ve heard of so far that makes sense to me.,hnymj05,t1_hny4o66,1639121085.0,False
rcla2n,"Resale of digital goods is the only use case I've thought about that might work, but it has a couple of problems that probably means it won't ever happen. Using games as an example, currently keys on every platform are single use. Meaning even if you tied ownership of keys to people on the blockchain, you couldn't resell it. Platforms like Steam would need to support it and immediately its no longer decentralized. Every transaction would need to contact the platform the key is linked to, to transfer ownership. At that point the blockchain is irrelevant, you could do the same thing without it because you require the platform to support it.

No matter how I think about it, for resale of digital goods the platform the goods are used on would need to support it.",hnytgr6,t1_hny4o66,1639126525.0,False
rcla2n,What about a platform for fractionalized nfts (erc721 + erc20) for companies that want to reissue their securities on the blockchain?,hoh5yrt,t1_hnytgr6,1639459022.0,False
rcla2n,crypto is overhyped,hnyzb23,t3_rcla2n,1639131470.0,False
rcla2n,The money laundering one. Also scams.,hnziyv9,t3_rcla2n,1639144401.0,False
rcla2n,Limited technical application from what I understand.  Also it's a picture of a monkey with your name on it it's not worth $50k,hnydezt,t3_rcla2n,1639115310.0,False
rcla2n,"A lot of good points in this thread, but I'll add my two cents: Because NFTs must be verified by a trusted party, it's unnecessary and wasteful to host them on a platform whose appeal is being decentralized & trustless.",hnz5t3f,t3_rcla2n,1639136608.0,False
rcla2n,"The basic idea is quite good

At the moment they’re being used for digital beanie babies, and it’s hard to see true value there

But looking at real world potential: I’ve just spent £1000 on solicitors fees to check the title deeds and other information about the house I’m buying, and most of that cost is retrieving the information they need. A tokenized system could be ideal for something like that. Whether that takes the same exact form as an NFT, I don’t know, but the idea was floated long before NFTs existed

Similarly car ownership, maintenance and inspection records etc would seem like great candidates for a token system - allowing authorized dealers and garages to log information on a decentralized blockchain, which could be much more reliable and cheaper to run than a centralised system.

I wouldn’t say they’re definitely suitable for such tasks, but certainly there’s real world potential there

Perhaps more importantly, digital ownership of songs, movies, books etc would be a perfect candidate. Streaming is great but not ideal because you can’t buy something to keep. But buying digital assets relies on the store/service you use staying in business

NFTs could solve this by allowing you to own a song more generically, so record labels or publishers can reassign the rights to sell those items to another distributor/store/service, on the proviso that they are sold as NFTs and the new provider will supply a download to the owner of the token. That way, you can buy a movie knowing that it won’t just vanish if Google decides to pull the Play Store one day, or buy a game knowing it doesn’t matter too much if Steam/Valve go bust, or a book without relying entirely on Amazon staying in business.",hnzcolc,t3_rcla2n,1639141048.0,False
rcla2n,ITT: people who don’t understand that an NFT is a digital certificate of authenticity wrapped in a rich metadata audit trail authenticated by cryptography and hosted on the closest thing to a permaweb humanity has achieved.,hnzdjzr,t3_rcla2n,1639141555.0,False
rcla2n,"Obvious scam is obvious. You are buying nothing with the expectation that the nothing you bought will be worth something in the future because other people also believe so. It is just a big bubble and when you look at the price graphs, the vast majority of them go up like a rocket and then come crashing down faster than light when the bubble bursts. Making NFT:s is fine I guess, but the market is oversaturated for obvious reasons so good luck making any money. At that point it is basically just a waste of time.",hnzygi6,t3_rcla2n,1639151310.0,False
rcla2n,"Duplicates of digital media can be exact copies. But money is paper that we all agree has value, it’s not much different. The idea that it can be akin to art is a bit off to me. No one will copy exactly the any of the most famous pieces of art, copy’s will always be copies. But a copy of a digital image is an exact copy. Same ones and zeros.",hnyf7bf,t3_rcla2n,1639116374.0,False
rcla2n,"I think there are some interesting potential uses cases in supply chain management and probably others as well, I don’t understand the current trend of digital artwork though.",hnybsdl,t3_rcla2n,1639114361.0,False
rcla2n,Just wait for digital assets outside of nfts my friend 🙏🏼,ho3dc7h,t3_rcla2n,1639205622.0,False
rcla2n,"An NFT is like writing the name of the owner on a piece of art using invisible ink.  It does not give you exclusive use of the art in any way other than allowing you to sell to someone else the right to erase your invisible ink name and write their own.

What is the value of that?  That is completely dependent on what people value.",hok35cs,t3_rcla2n,1639514762.0,False
rcla2n,"In a nutshell, NFTs are bad for two reasons:

1. They are bad for the environment, as they rely on cryptocurrencies that cause huge amounts of carbon emissions. They will continue to rely on these systems for security reasons (despite claims to the contrary about moving to other systems).
2. They are only valuable as tools for money laundering, tax evasion, and greater fool investment fraud.

There is actually zero value to NFTs. Their sole purpose is to create artificial scarcity of an artwork to supposedly increase its value (it doesn't do this, but the pretense that it does can be used for illegal purposes by those who recognize that fact).",ht2jl6h,t3_rcla2n,1642445021.0,False
rcj9zu,"Everything you want to know can be found in the book Digital Computer Electronics by Albert Paul Malvino. The things turning on and off are parts of an electric circuit. You can see this playing out at http://visual6502.org/JSSim/index.html. You can construct the architecture (SAP, ""Simple As Possible"") covered in Malvino's book yourself by following Ben Eater's video series on youtube.",hnv6tj6,t3_rcj9zu,1639066319.0,False
rcj9zu,"Also ""The hidden language of computers"" a real masterpiece of a book and easy to understand",hnw9dic,t1_hnv6tj6,1639081429.0,False
rcj9zu,It executes one instruction regarding a piece of data at a time.,hnwmfr5,t3_rcj9zu,1639086724.0,False
rcj9zu,Very quickly,hny0979,t3_rcj9zu,1639108546.0,False
rcip1t,"I would give them alot of projects , I had numerical analysis course in uni and I learned tons by doing projects in Matlab and python , like Taylor's series and mclaurin series with their applications on finding the approximated values of infinite fractions , I still remember those algorithms because I actually used these bad looking infinite series in something useful( in my case was just getting roots and approximating numbers but you get the point(",hnuzs1u,t3_rcip1t,1639063448.0,False
rcip1t,Numerical Analysis was the shit,hnxazie,t1_hnuzs1u,1639097280.0,False
rcip1t,"To some extent this question strikes me as ""how should history be taught to CS students""?

Answer: Exactly the same way it's taught to everyone else.  You shouldn't silo knowledge, and end up having a highly ""inbred"" understanding of the world.  You could end up thinking your corner of academia is the center of the universe.  One should be able to take knowledge from a different domain and integrate it into your project without having the professor both cut your food and chew it for you too.

Now I do think math classes could be helped by showing more applications in CS.  We're still living through the era when physics was seen as the great mathematization of a science, and so a lot of our examples are in that field.  Today CS is at least as successful a scientific mathematization, with just as many examples and applications, so in that way we could modernize by using more examples in CS.",hnvhhra,t3_rcip1t,1639070418.0,False
rcip1t,"There does need to be a difference in how you teach different students with different motivations - it isn't about siloing knowledge, it's about providing the best opportunities for learning for each student. In a perfect world, this would be tailored to the individual person, but due to various unavoidable realities the best we can hope for is that it is tailored for the different cohorts of students you get in each class.
 
Yes, the best students won't care and can shift between perspectives on their own, they'll succeed in a generally-taught class - but guess what? Those students will succeed if you give them a textbook and a stick to draw in the dirt with, focusing educational design as if those are your target is a complete waste of your time.
 
The knowledge stays the same, but how you present it, lead in to it, connect it to what a student already knows and cares about, that is what determines whether you get the right engagement from the students who need to be taught, as opposed to those who can learn on their own.",hnxjwlj,t1_hnvhhra,1639101307.0,False
rcip1t,"Sure, if you're just trying to be a code monkey or do data entry, then you don't need to know algorithms.  Up to a point that's true, but if you're getting a CS degree, you need to be able to ""speak math"" at the very least, since that's just part of what a CS degree means.  If you're in a CS program, then you  need to be able to understand math in exactly the same way that the math students do.",hnxqda1,t1_hnxjwlj,1639104133.0,False
rcip1t,"You've completely missed the point. You don't teach them different things, you teach them the same things in different ways. The same knowledge, the same skills, but the way you explain them and build up to them is going to be different.",hnxrmoy,t1_hnxqda1,1639104681.0,False
rcip1t,"Yeah, I get the point.  I don't agree.",hnxs3cx,t1_hnxrmoy,1639104884.0,False
rcip1t,"Literal reams of educational research says you are incorrect, and your comment makes it clear you don't get the point at all. By the end of their degree, someone taught in a way that builds from their existing domain knowledge rather than being given a stock standard approach is going to know more, and know it better.",hnxsdgv,t1_hnxs3cx,1639105010.0,False
rcip1t,Calculus should be taught through application not pure mathematics. It’s more important people learn when to use/apply calculus then just to learn the calculus. Because most people learn and forget or even worse learn just how to solve without understanding of what a derivative or other calc item is doing. I imagine most people won’t be able to find a tangent line using derivative .,hnv98w9,t3_rcip1t,1639067245.0,False
rcip1t,Calculus is not pure at all. The real pure calculus is real analysis. Calculus honestly is mostly just plug and chug at most universities.,hofabn1,t1_hnv98w9,1639428116.0,False
rcip1t,Yeah but it shouldn’t be plug and chug. U should be learning application of calculus items,hoflvez,t1_hofabn1,1639432887.0,False
rcip1t,I agree with you. Im just saying thats how it's taught at most colleges. It's also not professors fault because the colleges want them to stick to the curriculum where professors have little say in it,hogdf8s,t1_hoflvez,1639445434.0,False
rcip1t,"Implementing integrators and approximating derivatives with sampling, applied to a few physics problems (springs?). Partial derivatives with image manipulation.

Then move to symbolic manipulation to make sure the common language is understood.",hnutvlk,t3_rcip1t,1639060889.0,False
rcip1t,What you described is exactly how my Calc. Teacher taught it. The class was a mix of physics and CS students so anything we learned was tied to real world problems. It really made The class fun and easier to learn.,hnvsput,t1_hnutvlk,1639074800.0,False
rcip1t,"Learning monstrous functions has to be part of it. Part of what a CS education should prepare you for is engaging with CS-related research (papers and literature) which is full of these functions.

But I would also consistently emphasize the underlying concepts (e.g. rates of change, etc). And I would use applied examples both from Physics and CS to give both a physical and practical understanding.",hnv13r9,t3_rcip1t,1639064010.0,False
rcip1t,Depends on who you are going to get as a result: “artist” or “artisan”,hnuzmcl,t3_rcip1t,1639063382.0,False
rcip1t,I had a class in college (got engineering degree) called Numerical Methods. Junior or Senior level class that was basically how to do calculus with discrete methods on computers.,hnv3occ,t3_rcip1t,1639065057.0,False
rcip1t,"I loved that class.  I took it at Rutgers with a professor that worked in the petrol industry and insisted we work in FORTRAN.  That one part I didn't love.

I gave it a different name though.  It's been years, but it was something like ""computational guesstimation"".",hox6rqo,t1_hnv3occ,1639751557.0,False
rcip1t,"If its actually calculus, then its good. But if its real analysis... RIP.",hnwjkxw,t3_rcip1t,1639085560.0,False
rcip1t,Just let them read this book: \`Calculus the Easy Way\`.,hnvjt8n,t3_rcip1t,1639071308.0,False
rcip1t,"There are a lot of great applications for calculus in AI and Machine Learning in general that might help drive calc home. I found in university that calc 1 and calc 2 tried to cover too many topics too fast, and lacked context/application, making the work feel very abstract. In a perfect world I think calc 1 could even be two classes, as given the spread of topics there is not enough time to have students actually connect with the material. The semester I graduated calc 1, around 50% of the class failed.",hnwkbpm,t3_rcip1t,1639085862.0,False
rcip1t,"The reason you typically have calculus classes in your freshmen year is that they also teach the basics of proofs, set theory and so on in the first few weeks. Now, of course, that is not directly connected to calculus, you can just as well do this in a linear analysis class, or in something focusing on theory of computation (I think that's what they do at stanford).

Now, if you move that part somewhere else, you can teach calculus differently or move it to a different place of the CS curriculum.

The question is whether you should, and if yes, to where and whether you need it at all. I don't really know. Calculus is kind of important for many areas of CS, for example to define big-O notation you kind of need it. Also, a CS curriculum without calculus is hard to imagine, especially for the people in charge of making these since they likely also attended such a curriculum.",hnwrqlu,t3_rcip1t,1639088893.0,False
rcip1t,Unless they hire extra professors for the cs department but other than that I believe there would be different applications for different majors. I think the overall concept is the critical junction while teaching this skill.,hnygqno,t3_rcip1t,1639117298.0,False
rcip1t,"The same way my uni did it, you take calc 1 like everyone else, and then go into comp sci related math courses. You need to understand the basic concepts before you can reasonably be expected to apply them in the real world.

There's a reason why in calc and above most proffesors don't care how ""fancy"" your calculator is, punching in numbers only gets you so far, you need to have some level of understanding why things work they way they do.",hnyvjd3,t3_rcip1t,1639128278.0,False
rcip1t,"Honestly, I don't think it should be, unless the student is going into graphics design, or 3d modeling, or simulating either of those.

Calculus (and beyond) is less a guide to logic, and more a guide to memorizing formulas.

All that said, to answer the question, no, I don't think it should be taught differently based on whether you are in Comp Sci or not.  Definitely _add_ connections to comp Sci, but don't create a lesson ""for computer scientists"".",hnz10wj,t3_rcip1t,1639132912.0,False
rcip1t,Lmao it really shouldn’t,hnzlodf,t3_rcip1t,1639145699.0,False
rcip1t,Calculus has helped a lot but learn optimization. Not like linear optimization but mixed constraint shit,ho3ddya,t3_rcip1t,1639205656.0,False
rcip1t,"I certainly agree that it could be taught in a better way, but I think most schools will not create a separate math class just for CS. There are definitely a lot of practice problems and applications of calculus that could come from CS.

I've had to integrate using polar coordinates, integrate logs, partial derivatives, and all the possible things I've integrated in typical integral classes so I found them quite useful already.

As for the ugly functions, I agree that they aren't as relevant, but I used it as a way to practice and learn how not to make mistakes.

Edit: If calculus was taught with CS applications, even some basic caculus methods may require a probability background or additional CS background. I suppose teaching calculus at such a late stage in your career might be less than ideal unless you knew for sure that you were going into CS.

Edit 2: If I were teaching a calculus class, I might preface a problem in calculus with an application in another field if I see applications that are easily explanable within a few minutes. Otherwise, it might take too much time and take away from actually learning new methods.",hobba9x,t3_rcip1t,1639353472.0,False
rcip1t,"Don't know that the following applies to your question but found it to be an interesting source for learning.

https://brilliant.org/courses/",hoefom8,t3_rcip1t,1639415826.0,False
rcip1t,"I think this depends on whether the class is being taught at a university that aims towards providing a liberal education, or if the class is being taught at an institution that functions more like a trade school.

A trade school trains students for practical skills in a given profession. It makes sense that coursework is tailored to highlight the application of each subject to that profession. Students don’t need to learn underlying theories or ancillary topics if they can’t be applied in their future profession.

A university should be different. Classes should be taught in a way that encourages students to explore the depths of the subject itself. This is part of the process of providing a liberal education.


I admit this stance is a little extreme. I’m sure there are ways of adapting a calculus course to highlight CS-related content without losing anything. I just get very suspicious when university courses start to focus too much on practical application, and stop teaching theory.",hohraat,t3_rcip1t,1639474458.0,False
rcip1t,I got 12/40 in cal1 while getting 38/40 in c++ lab soooooo,hnuxx68,t3_rcip1t,1639062664.0,False
rcip1t,"Real talk, computer scientists should be taught calculus through the lens of computing gradients for neural network parameters. You can start with really simple differentiable estimators then work your way up to fancy deep NN's and gradients over really complex transforms like image perspective transforms.

There's a nice range of difficulty, and it's very easy to see why calculus is worthwhile when you see it so directly applied in backpropagation.",hnw9xfy,t3_rcip1t,1639081651.0,False
rc9jzq,"> Can AI theoretically solve the halting problem or **at least most of it?**

The issue is that the halting problem isn't defined probabilistic-ally. You can't solve the halting problem with ""I'm 90% confident the given program will halt, because it looks similar to halting programs that I've been trained on"", because by definition that is _not_ determining whether the input program will halt, it's just making an educated guess.

If you want to move from ""this will probably/won't probably halt"" to deterministically solving the problem, then we're back to square one.",hntdhsk,t3_rc9jzq,1639025291.0,False
rc9jzq,"You can't solve any problem with a computer with 100% confidence. A quantum fluctuation or a cosmic ray could flip a bit somewhere and completely change the result or behavior. So really it's just about your level of confidence. Also if you trained a neural network sufficiently, maybe it could give 100% confidence assuming no quantum fluctuations. If you trained a sophisticated enough neural network to genius level then who knows what could happen. The neural network might be so sophisticated that it creates it's own execution engine to run certain parts of the code for example.",hntiv4t,t1_hntdhsk,1639028354.0,True
rc9jzq,"> You can't solve any problem with a computer with 100% confidence

That's a pedantic argument. You _can_ develop algorithms that solve a problem 100% of the time if executed faithfully - the fact that computer hardware is fallible does not mean that the theory is incorrect. It's not about levels of confidence.

> The neural network might be so sophisticated that it creates it's own execution engine to run certain parts of the code

How would this work? If you run a piece of the program to check whether it halts then you can only observe a termination, but not a lack of termination. That is, if it halts, great, you know that it halts, but if it _doesn't_ halt then all you know is that it hasn't halted _yet._",hntju3a,t1_hntiv4t,1639028954.0,False
rc9jzq,"Well in reality and in all of science theories come down to levels of confidence. Even in imaginary land there is no such thing as absolute truth if you want to get into philosophy because you would never know with absolute certainty that an algorithm can solve a problem 100% of the time if executed faithfully. Absolute confidence is a myth. The point is not that the theory is incorrect but rather at a certain level of precision and accuracy become indistinguishable from absolute certainty.

And a neural network can emulate almost any complex function. So it's not that it would just run the code blindly but rather execute certain portions of the code if it seems relevant and use advanced heuristics to make determinations. With enough training, neurons, and architecture sophistication a neural network might be incomprehensibly intelligent on determining whether code will halt or not. I can't really tell you what kind of strategies a neural network with an IQ of 10000 would come up with obviously but there may be a point where it can tell you with maximal confidence for most halting problems, or perhaps all.",hntm9sl,t1_hntju3a,1639030523.0,True
rc9jzq,"If we assume that inductive and deductive logic can reveal truth, then there is such a thing as absolute confidence through mathematical proofs. Algorithms can be proven in this theoretical domain, even if their implementation in hardware is imperfect. If you do not agree that objective truth exists at even this level, then we don't have enough shared axioms for this conversation to go anywhere.

I can't engage with your second paragraph, because it hinges on whether we consider heuristics to be a valid solution to the halting problem. Since I believe a solution is only valid if it is deterministic, and you appear to hold that determinism does not exist, we're at an impasse.",hntn7wy,t1_hntm9sl,1639031159.0,False
rc9jzq,"A neural network can be seeded to run deterministically if you run it on a perfect computer that never makes mistakes. It can be deterministic. With the same seed and the same inputs it would arrive at the same result each time. Heuristic is not necessarily a subset of non deterministic. There is no good demonstration or proof that a heuristic is unsuitable for coming up with solutions. If you did have a proof of that I would be very interested to see that.

But saying that there is absolute confidence in mathematical proofs might be valid if the entity proving the math was perfect. Imperfect human beings cant prove anything to absolute certainty because humans aren't 100% reliable. As soon as you accept the inherent unreliability of human beings, and say each mathematician has a 1 in 1000 chance of making a mistake in validating or interpreting a proof, then there is still a non zero chance that all proofs have a mistake or error. The problem in your assumption is that human beings are incapable of using logic perfectly to arrive at a deduction with 100% certainty. It is standard in philosophy to go with maximal certainty e.g. the maximum certainty a human brain is capable of.",hntv6ub,t1_hntn7wy,1639037112.0,True
rc9jzq,"In theory of computation, for an algorithm to be said to *solve* a problem, the algorithm must provably produce the correct answer for every possible input, not just ones that you can come up with. That may be quite different from everyday use of the word, but it's crucial if you want to talk in terms of theory of computation.

Could an artificial neural network solve the halting problem in the sense the word is used in computability theory? No, unless you're willing to postulate that the different proofs showing the undecidability of the halting problem -- and the more general Rice's theorem -- are wrong.

Could you train a neural network that appears to, in practical terms, be able to answer the question for example programs that you come up with? Maybe. But there are already other methods for answering limited cases of the halting problem, and you don't necessarily need an AI for that.

Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.",hnu5jur,t3_rc9jzq,1639046035.0,False
rc9jzq,"What does provably produce the correct answer mean? If we build a super artificial intelligence that solves certain problems but the proof is so complex that no human could ever understand it, then does that mean it hasn't been proved? Or consider a proof by evaluating all possible inputs and outputs. It may be impossible for a human to ever go through all of the billions of answers to verify it, and yet the computer did verify it. And that doesn't account for the fact that humans are totally unreliable and have a non zero chance of being wrong or making a mistake particularly as things get more complicated. 

""Could an artificial neural network solve the halting problem in the sense the word is used in computability theory? No, unless you're willing to postulate that the different proofs showing the undecidability of the halting problem -- and the more general Rice's theorem -- are wrong.""

This is the crux here. I think the question is theoretically could a maximally intelligent God solve the halting problem? And i don't mean  a maximally intelligent God could create a proof solving the traditional halting problem, but rather give any code to that maximally intelligent being and it could determine whether the code halts or not.  It seems like a very different type of question whether intelligence can solve a problem compared to a direct computational approach. The way I think about it is humans create solutions and proofs in math, using intelligence, that would never be achieved with standard computation. They seem like different problem domains.

""Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.""

That applies to all the high end math proofs humans make today. I could never determine whether the advanced math proofs made today are correct.amd yet I still believe they're correct.",hnv8zh0,t1_hnu5jur,1639067145.0,True
rc9jzq,"> What does provably produce the correct answer mean?

In case you don't have a CS background, it means formally proving that an algorithm, if followed exactly, will logically and inevitably reach the correct answer. It's similar to a proof in mathematics.

If you do have a CS background -- as you probably do considering the understanding you show -- you knew that already, so I'm not sure what the point of the question is.

> If we build a super artificial intelligence that solves certain problems but the proof is so complex that no human could ever understand it, then does that mean it hasn't been proved?

Okay, so I'm assuming we're talking about an ANN producing a proof about something rather than just producing yes-no answers to the question of whether a program will halt.

Can you directly prove that the proof (supposedly produced by the AI itself) is valid? Or that the proof has been generated using a method that's formally proven to only produce valid proofs? If the answer to either of those is yes, then I would generally consider it proven even if we can't directly understand the proof itself, if the methods used for either producing or checking the proof were formally proven to be correct. That is, of course, with the usual caveat that extraordinary claims require extraordinary evidence, and that we should carefully check the validity of whatever logic was used for supposedly vouching for the validity of the proof.

However, it's worth noting that artificial neural networks don't generally work in such a way that you could formally prove anything about the validity of their results. They just empirically seem to work for a bunch of problems. So, if we're dealing with an ANN, we can't prove that the method (the ANN itself) with which the supposed proof was found was guaranteed to only produce valid proofs, and so the proof that it produces would need to be validated in some other way. That could be either by humans understanding the proof and meticulously checking it for errors, or using automated theorem proving. (Automated theorem proving is then again an undecidable problem in the general case, although it's decidable for some subset of possible inputs, so YMMV.)

However, the point is kind of moot if we're talking about perfectly solving the halting problem. It has been proven to be undecidable in the general case, at least in the sense that for a hypothetical algorithm that purports to solve it, it's always possible to construct an input for which it fails.

You might be able to have a neural network that produces what empirically seem to be correct answers to various instances of the halting problem, with or without any formal proof of anything. You might be able to have a neural network produce a formal and provable theorem about *which* kinds of special cases of the halting problem are decidable. But you wouldn't be able to have a neural network that provably (in the formal sense) correctly answers the halting problem for any and all instances of it.


> This is the crux here. I think the question is theoretically could a maximally intelligent God solve the halting problem? And i don't mean a maximally intelligent God could create a proof solving the traditional halting problem, but rather give any code to that maximally intelligent being and it could determine whether the code halts or not.

That's kind of getting into metaphysics. I don't think there will be a valid answer to such a question, any more than to any other question regarding what results from omnipotence.

> It seems like a very different type of question whether intelligence can solve a problem compared to a direct computational approach. The way I think about it is humans create solutions and proofs in math, using intelligence, that would never be achieved with standard computation. They seem like different problem domains.

That's a valid question to think about. Humans, and actual intelligence (whatever that means), do usually arrive at solutions in a different way than direct computation.

I don't think that necessarily helps a hypothetical AI formally solve the halting problem, although I guess I kind of see where you're coming from. Yes-no answers to the halting problem could hypothetically be accompanied by a proof or reasoning of why a program always halts or why it doesn't, and an intelligence that doesn't (superficially speaking) work by means of classical mechanical computation might be able to creatively come up with answers and proofs that a classical algorithm wouldn't.

The problem is that, if whatever produces those answers and proofs *doesn't* do it systematically, e.g. by enumerating all possible proofs, you can't really prove that it will always be able to find one for *all* instances, even if the answers could be shown to be correct about the ones for which it does. The same is of course true of humans.

> ""Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.""
>
> That applies to all the high end math proofs humans make today. I could never determine whether the advanced math proofs made today are correct.amd yet I still believe they're correct.

Of course, but I don't think that's the same thing. *Someone* has been able to present those proofs, and they've been independently reviewed by others with the required expertise. While people might be able to understand parts of why a nontrivial artificial neural network behaves the way it does, large ANNs are largely black boxes to *everybody*.",hnvvavp,t1_hnv8zh0,1639075814.0,False
rc9jzq,"At some point wouldn't the only way to determine reality on the inner halting or not, be to actually run the code to prove the halt. 

It's sorta like the quantum physics cat example. The truth is unknown till the truth is known",hntb6yf,t3_rc9jzq,1639024059.0,False
rc9jzq,"I see what you're saying but perhaps the AI can give an estimate as to how accurate it thinks its results are. And as you add more examples it should be able to get more accurate over time. If you were to scale things up massively could it give you near perfect answers? Quite possibly its hard to say. You know if it can tell you with 99.99999% (six sigma) certainty that it will halt, I'd say that's good enough. At that point we consider scientific theories to be essentially true.

Or could there also a tipping point where it becomes such a genius neural network that it can determine with 100% accuracy whether a program will halt?",hntbh0g,t1_hntb6yf,1639024208.0,True
rc9jzq,"Mathematical proof is not ""good enough"" if it does not cover 100% of the cases.",hnzibxz,t1_hntbh0g,1639144085.0,False
rc9jzq,"Especially if it's code written by humans. It would potentially pick up on an inexperienced coder or common mistakes leading up to a halt. 

Could save a lot of computational power if it think 99% it will halt and just save the full test for codes it thinks might actually go through.",hntbsji,t1_hntbh0g,1639024375.0,False
rc9jzq,"Sounds almost like Microsoft Clippy for programming.

> Looks like you're trying to set up a CRUD server! Can I help?",hntc71k,t1_hntbsji,1639024588.0,False
rc9jzq,"The best way to tackle termination problems is by using symbolic methods, not neural networks.

There are various techniques that can solve the problem for a given class of algorithm, up to some degree of complexity. So, by not defining what ""most of it"" means, we could say that (symbolic) AI is able to solve a reasonably non-empty input fraction.

In general, whenever you have a rigorous symbolical representation of your input, the best way to approach it is to keep the nice symbolic representation and reason about it in a symbolical way. You would just complicate your life and impoverish your starting knowledge by adopting a neural network approach. 
There exists hybrid approach, but they are still in the early phase.

Anyway, the halting problem is in general undecidable and theoretically there is nothing that can change that.",hntyj0m,t3_rc9jzq,1639039894.0,False
rc955j,"x = n + n/2 + n/4 + n/8 …

Multiply both sides by 2.

2x = 2n + n + n/2 + n/4 + n/8 …

Subtract each side of the the first equation from the corresponding side on the second.

x = 2n

Which is O(n)",hntd7gv,t3_rc955j,1639025135.0,False
rc955j,Thank you so much! I've been thinking about this problem for about 2 hours and I think it just clicked. I appreciate it!,hnteheb,t1_hntd7gv,1639025837.0,True
rc955j,You wanna share the code?,hnt8ibf,t3_rc955j,1639022665.0,False
rc955j,"Apologies, I thought I had it attached when I posted it",hnt9nw7,t1_hnt8ibf,1639023260.0,True
rc955j,"Please, don't post images of code when you can just post code (4 spaces before each line for correct formatting)",hnx4g1f,t1_hnt9nw7,1639094340.0,False
rc955j,"Let M be the original value of N before the code starts. Then the number of executions is M + M/2 + M/4 + ... This is a geometric series (look that up if you're not familiar). Any time the length of the loop changes like this, you should look at whether that shrinks the complexity. That's how sorting algorithms can be O(nlogn) with two nested for loops.",hntf8eo,t3_rc955j,1639026247.0,False
rc955j,So this guy Zeno has this paradox right...,hnwx8zt,t3_rc955j,1639091204.0,False
rc955j,What algorithm is it,hnt8jc2,t3_rc955j,1639022679.0,False
rc955j,"Sorry, I thought I had attached it when I initially made the post",hnt9pq3,t1_hnt8jc2,1639023285.0,True
rc955j,"You sure complexity is O(n)
Because there is no n in that code",hntaube,t1_hnt9pq3,1639023874.0,False
rc955j,"""n"" is an arbitrary common label used, not referring to the specific variable name in the code. You could call it O(m), O(p) etc.",hnuend3,t1_hntaube,1639052860.0,False
rc955j,Got you,hnve5mm,t1_hnuend3,1639069124.0,False
rc955j,"Well, if we're using the variable in the code, then it is O(N)",hntbatv,t1_hntaube,1639024116.0,True
rc3wsu,"Mechanical computing device. Physically pretty big and over engineered for durability. Most of its function is to be a database lookup to relevant stone tablets with the actual information.
Combine with smaller mechanical devices that use directed questions to allow ignorant people to create a useful search for the big device.
If you really want to go for the long haul, the final lookup is actually directions through the massive tablet storage area instead of a tablet retrieval mechanism.
Ideally you won't have any computing, just knowledge, which is where most sci-fi authors go with this problem. But making it a device is a fun conceit 🤔",hnsal50,t3_rc3wsu,1639007258.0,False
rc3wsu,"Mm, yeah I thought about that, and certainly the ""books with pages made of hammered gold"" type thing appeals to me, especially through deep time like thousands or millions of years. For this particular story the aim is to be useable for several centuries, and I think it might be useful for these people to have engineering tools, computer aided design and simulations, perhaps even access to radio for the purpose of communicating with old satellites (as some high orbit satellites will still be in place for millions of years, it's not unreasonable to also make a satellite that might be able to do things for a very long time).",hnsbio2,t1_hnsal50,1639007669.0,True
rc3wsu,"Does your story allow for caretakers of this repository, or should it be completely abandoned and forgotten for a time?  And how long does it need to last?",hnsdwx5,t1_hnsbio2,1639008756.0,False
rc3wsu,"No caretakers, no maintenance, completely abandoned and forgotten. It would be located in a geologically advantageous environment - perhaps a desert at high altitude.

Anything from a century to a thousand years would be fine, but the longer the better, story-wise. A million years would be excessive. I want to be able to describe the making of it in some detail.",hnsep68,t1_hnsdwx5,1639009110.0,True
rc3wsu,"To be fair, for just a century or three you could simply print Wikipedia on archival paper with a laser printer and store it in a cardboard box, if you had a dry cave in the high desert and a way to keep animals out.

If you're committed to something electronic, I would look at solid state storage, something older (and thus likely more robust), like mask ROMs.  Even consumer grade mask ROMs last decades with very little care (think Atari 2600 cartridges, some are from the 1970s, most of them still work if they're clean).  You could probably take inspiration from Viking and Voyager space probe computers, the Voyagers are still functioning after decades.  A nation-state actor could surely take lessons learned and make a very robust system in a similar manner.  Voyager used dual redundant systems and CMOS volatile memory with a direct, permanent connection to the RTG.  You'll also probably want some kind of nuclear-derived power.

My main thought is things need to be designed to be fault tolerant and redundant, the system should have the ability to bypass or work around failed parts, and the storage area should be as clean, even temperatured, and dust/moisture-free as possible, say, in a cement-lined cave.  A big part of why electronics fail is due to temp cycles.  Moisture in the air combining with contaminated on the hardware can cause corrosion.  Avoid the use of electrolytic capacitors.

But really, if you can get away with something non-technical like books, go for it, it'll be more believable.  There are books in the world from the 15th century and (much) older that just laid around an attic most of that time.",hnshq24,t1_hnsep68,1639010461.0,False
rc3wsu,"There will be various abandoned/dead outposts throughout the solar system, as well as a dying generation ship - ultimately I'd like the characters to have some kind of communication with them, even if it's just to witness their slow death. It would also be really interesting if they could connect to some remaining satellites in orbit to get an overview of Earth.

Libraries and the like would certainly exist as well as this thing.


There are a few plot threads I'd like to explore concerning a computer database versus libraries as well - if the computer fails, all of that knowledge is lost at once (with hope for repairing the machine, possibly).

Of course there's also the capability for physics simulations, computer aided design, accurate and precise timekeeping etc. There would some interesting possibilities for showing video and sound as well.


There's also good reason for this thing to be located in a desert - (weather bad) not a great place to live but even just visiting it may make for an interesting story. The user might even become a User or a High User if civilization decides to treat it as a sort of religious artifact or something - and especially interesting if it breaks and The Great Council of Quertyuiop calls upon a crusade to retrieve the Holy Heatsink or something.",hnyqod5,t1_hnshq24,1639124229.0,True
rc3wsu,"Those all sound like interesting story beats to me, I would read that novel.",hnz443g,t1_hnyqod5,1639135368.0,False
rc3wsu,"Also want to add that the story includes automated outposts throughout the solar system, and potentially a (failing) generation ship, made by the same or similarly capable group of people. I want the technology to be consistent so one will inform the other.",hnsfe4v,t1_hnsdwx5,1639009416.0,True
rc3wsu,"Also, an afterthought here - yes, the facility could have caretakers, but they'd have to arrive after the fact, know nothing of how it works or what needs to be done, beyond what can be learned from using it. Adding certain materials to a reservoir, or turning a handle, or hooking a shaft up to a windmill or a primitive electrical generator, fine. Pre-existing knowledge of re-imaging the operating system or soldering surface mount components to a board, not so much. Replacing parts in storage, as instructed by careful study and use of the device, fine.

Any maintenance would have to be done by people who have no idea how it works",hnsg09b,t1_hnsdwx5,1639009689.0,True
rc3wsu,"The physical storage medium is something I considered as well, even for an electronic computer - given unlimited size something like a vinyl record but made of quite hard materials could last for a *very* long time.",hnsbrk0,t1_hnsal50,1639007780.0,True
rc3wsu,Gigantic over engineered card catalog?,hnujej0,t1_hnsal50,1639055668.0,False
rc3wsu,"Yes!  The idea of a technology that could survive a thousand years and be used by ignorant people is preposterous, but that is the requirement of this thought exercise 🔥so  you just have to run with it.",hnuysl6,t1_hnujej0,1639063037.0,False
rc3wsu,"A “foundation,” as it were.",hnt6t06,t3_rc3wsu,1639021804.0,False
rc3wsu,Technically you don’t have to create a thing to store all knowledge.   You only need to create a thing that motivates and instructs the user as to how to turn on the computer.,hnsd7e5,t3_rc3wsu,1639008434.0,False
rc3wsu,"“If you wish to make an apple pie from scratch, you must first invent the universe.”

I say this tongue-in-cheek, but you're right of course and that is a major consideration here. The user would have to choose and be able to operate the device in the first place, and perhaps even have some useful help in learning how to use it.",hnsdsx6,t1_hnsd7e5,1639008706.0,True
rc3wsu,If things really did get back to Neanderthal levels you would need to start with some simple interactive games to build familiarity and trust with the device.   Things like matching shapes or colors like toddlers toy.   Then you could build up to simple spoken language.   It could be like a fast paced survival of the fittest scenario where status in the society was based on the ability to interact with the machine.  Eventually your users could start seeing story boards that explain what happened and build up to learning what we know in detail.   You couldn’t just throw that at someone who might even never have seen a number or a wheel.  They would be frightened of it,hnso0ub,t1_hnsdsx6,1639013289.0,False
rc3wsu,"Easy, you just need to store the data on a Nokia 3310 somehow! 

Or something like NASAs golden record. 

Imo it would not work though:  When, for example, the ancient egypt empires fell, most of the knowlege was lost despite it being available in writing at the temple walls (and a lot of it is lost till today). After a collapse most people would not be able to read anymore and people using computer would be seen as crazy.",hnttzp3,t3_rc3wsu,1639036160.0,False
rc3wsu,"Diamonds and lasers! But in all seriousness, I don't think it's possible to make indestructible computers, but it should be possible to make easily maintainable ones. Where a replacement part can be a lens or some metal bracket.",hnt839p,t3_rc3wsu,1639022450.0,False
rc3wsu,Plastic Fortran punch cards.,hnujzvv,t3_rc3wsu,1639055996.0,False
rc3wsu,"In the 3 body problem they thought about all kinds of ways of storing information for 100,000 years into the future, and finally figured out that the best way was invented eons ago: etching it into stone. Stone will last tens of thousands of years under the right conditions. So your best bet would be to etch information into stone, but if you're really set on the whole 'computer' idea it would have to be something mechanical that interacts with the stone somehow, like retrieving tablets or something. Kind of like the machines you see in the dwemer ruins in skyrim.",hnunzgc,t3_rc3wsu,1639058077.0,False
rc3wsu,"make a system that can repair itself and make its own parts. that way you get some longevity.

then give it the task to conserve educational videos in a RAID 1 configuration and show them on a crt when a hooman is present",hnsgwkc,t3_rc3wsu,1639010093.0,False
rc3wsu,"My first thought for building an archive like this would be to find a star and construct a Dyson sphere, which would ensure it's survival by sending a Von Neumann probe out to replicate the knowledge base when the star wanes (the dying million years or so).

How the information is expressed is an interesting abstract problem though, since those encountering ""The Archive"" might not have the same understanding of technology as it's constructor.",hnt0p0j,t3_rc3wsu,1639018909.0,False
rc3wsu,"Running for _centuries or longer?_ I'd ditch the computer in favor of a card catalog system. Doesn't need any electricity, won't be effected by an EMP or nuclear fallout or whatever led to the societal collapse. It will last as long as the room and printed material endure. Paper becomes fragile with age, but by laminating the cards and coating books in some kind of a sealant they could be made to last much longer. Hopefully the library could contain enough books (maybe in microprint with magnifying glasses to store as high a density of information as possible?) to aid in redeveloping society and technology.",hnt84xp,t3_rc3wsu,1639022475.0,False
rc3wsu,"Like the seed vault, the trick for preservation is cold.  This way items last for a very very long time.

Instructions on how to read with physical books is necessary, because there is no guarantee of infinite power.. maybe geothermal, but it's beyond my knowledge of the topic.  So you have to teach someone how to get power, teach them how to make a circuit and so on.  Have items for them to use, play with, and learn like breadboards and what not.  Once they learn how to make power they can plug in any preserved computer.",hnttya7,t3_rc3wsu,1639036129.0,False
rc3wsu,Poneglyph,hnvg2sp,t3_rc3wsu,1639069868.0,False
rc3wsu,Gameboy Cartridges.,hnzemm3,t3_rc3wsu,1639142151.0,False
rc1j2y,"data type - is the data a string, a number, a date etc.  different languages will have their own types (int, float, string, datetime)

data structure - ""a data organization, management, and storage format that enables efficient access and modification"" examples of these would be an array, a binary search tree, a hash table.  they are all different ways of storing data that provide different benefits (speed, size, inserting, sorting, etc)

data schema - i believe this is for describing the organization of data in a database. if you are new and don't know much about databases i don't think examples will be of much use

data model - i'm familiar with this as a part of the MVC (model, view, controller) design pattern. the data model is how the data for your application is represented.  a key part of mvc is that this is kept separate from the view (what displays the data to the user) and the controller (the ""brain"" of sorts, it manipulates the views and data so they are always separate and unaware of the other)

I guess they are all related in the sense that they all have the word data in them.  Data schema would have data types in it, as would a data model.  a data structure as a concept is independent of the rest in a sense, but you would also see an array in some languages declared with a certain data type",hnrvjru,t3_rc1j2y,1639000946.0,False
rc1j2y,"My CS Professor weirdly said a schema is the way a table is structured but in my 7+ years of data it has always referenced the organization of tables within a database. Like, a database is made up of schemas, the schemas are made up of the tables. Sorry if this is more confusing!",hnsjdy9,t3_rc1j2y,1639011211.0,False
rc1j2y,"That's exactly the opposite of how we learned to use those terms in databases courses: the database is made up of tables, each organized/defined by their schema.

I don't know which is right, and maybe one use of terms is used in theoretical/academia and another use is common in industry.  I'm just reporting what we were taught.",hnzwwqx,t1_hnsjdy9,1639150666.0,False
rc1j2y,I think we’re saying the same thing and my Professor said something different.,ho0mbh5,t1_hnzwwqx,1639160767.0,False
rbz19p,"What you describe is called an ""access control policy"", see e.g. [https://en.wikipedia.org/wiki/Access\_control](https://en.wikipedia.org/wiki/Access_control).

Many such systems are based on a notion of roles. You can search for the keyword RBAC, for role-based access control.

This will give you plenty of things to read ;-)",hntlpub,t3_rbz19p,1639030157.0,False
rbz19p,"Research IRM solutions.  They protect documents by only distributing encrypted copies, and users gain access with client software that is given the decryption key via a server you control, backed by whichever AAA structure you're already using.

The advantage is you can enroll/unenroll people and groups, federate with vendor AAAs, set limits for online/offline use, and revoke access to files even if the data is on a third party system.

Another advantage is there are existing solutions you can buy, which is probably cheaper, faster, and more robust than rolling your own.

By client software, I mean things people are already familiar with, like patched MS office, etc.",hnzxk16,t3_rbz19p,1639150934.0,False
rbwnot,"Practice is the best way. Write code and run it so you can see what it does. Play with the classes and libraries you're using and just see what they do. If you ever think ""I wonder what happens if I..."" then write a program and try it. Unless you're doing silly things like manipulating files in system directories it's pretty difficult to actually damage your computer in any meaningful way accidentally. The best way is to just get your hands all over it and write code.",hnqrhlp,t3_rbwnot,1638985659.0,False
rbwnot,">If you ever think ""I wonder what happens if I..."" then write a program and try it.

This is really a good point, I enjoyed a lot playing with pointers in C this way.",hnuv39v,t1_hnqrhlp,1639061423.0,False
rbwnot,Practice https://adventofcode.com/ :),hnr669c,t3_rbwnot,1638991262.0,False
rbwnot,and compare your solutions at r/adventofcode!,hnscngs,t1_hnr669c,1639008182.0,False
rbwnot,"Here's a sneak peek of /r/adventofcode using the [top posts](https://np.reddit.com/r/adventofcode/top/?sort=top&t=year) of the year!

\#1: [Thank you Eric!](https://np.reddit.com/r/adventofcode/comments/kjtou6/thank_you_eric/)  
\#2: [Too often](https://i.redd.it/2dwtt64moq461.jpg) | [62 comments](https://np.reddit.com/r/adventofcode/comments/kbnh5i/too_often/)  
\#3: [\[2020 Day 18 (Part 1)\] Outsourcing the solution. They never care about the order of operations anyway](https://i.redd.it/ugjy19khxy561.png) | [21 comments](https://np.reddit.com/r/adventofcode/comments/kfnt2s/2020_day_18_part_1_outsourcing_the_solution_they/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[Source](https://github.com/ghnr/sneakpeekbot)",hnscolz,t1_hnscngs,1639008197.0,False
rbwnot,"There are some great books by oreilly on the python programming language and many different applications such as web development, finance, data science, and more.

I would say for now, it’s good to be aware of those applications while you learn the language and build intuition and muscle memory.

Your class projects will be really good for applying fundamental  concepts. These projects will be important in your portfolio as they demonstrate basic competency.

What will really connect you to programming will be different projects of practical ideas and personal interest I would suggest using Udemy’s 100 days of code (I have heard great things about it and I will be starting it soon), or other beginner projects you find on the internet.

These projects are beginner to intermediate and are especially helpful if you are not sure what your programming interests are. As you do some of these projects, you will be noticing what you like and dislike. From there, you will be able to guide yourself. 

Programming can seem daunting early on in this vast space especially if you have classmates who have been doing it for a while. It is important to start small and not bite off more than you can chew. You might get discouraged and have a negative experience that lingers on. 

Tldr: get comfortable with the basics, start small, divide and conquer, find your interests, enjoy yourself",hnqt4g4,t3_rbwnot,1638986278.0,False
rbwnot,I think practice is the biggest thing. Write a bunch of code. I also think reading and understanding other peoples code can be very useful. You could probably look at some open source projects to find code to read.,hnqt9vc,t3_rbwnot,1638986335.0,False
rbwnot,"ignore the people linking to shit like advent of code and hacker rank, solving those problems and actually doing projects are two different skills. You need to focus on doing projects , stop thinking about if its unique or anything. Just do it, and you will get better.",hnrcswq,t3_rbwnot,1638993773.0,False
rbwnot,"As everyone else has been saying, code more. But in my experience, find a semi-big project that you will really enjoy doing, and go at it. I was in the same boat, and I found a really fun project: making a password guesser that could get into anyone's school account at my school. Yeah, that project sounds really bad, but I told all my high school teachers about it and I was very transparent about it so I wouldn't get in any trouble. Anyway, I had an absolute blast trying to code the program since it made me feel so cool, like a hacker. You could try a basic or advanced project; just make sure it forces you to code A LOT.",hnrid6y,t3_rbwnot,1638995895.0,False
rbwnot,How did the code work?,hnwbn4y,t1_hnrid6y,1639082344.0,True
rbwnot,"Well, at my high school, it was easy to make this program. For their school logins, everyone had a username that could be easily determined. It included their name and graduation year (Ex: John doe graduates in 2022: jdoe22). For the password, it was always the word ""pass"" and then a 4 digit number (Ex: pass9233). My program first asked the user to type in the student's full name and graduation year. After that, it generated the username automatically and stored it in a variable (very simple code). Then, it opens up a tab in Chrome, goes to our school portal website, and starts entering the username and every possible combination of the password until it works. pass0001, pass0002, pass0003 and so on. In reality, I had it open 4 different Chrome tabs at once to do it faster, but it worked on every single student. Does your school have a password system like that? Just curious",hnwdlhm,t1_hnwbn4y,1639083127.0,False
rbwnot,Nah we get to choose our own passwords,hnwdt6j,t1_hnwdlhm,1639083213.0,True
rbwnot,Experience and read  3 party code,hnqwom7,t3_rbwnot,1638987636.0,False
rbwnot,"I suggest going through a book like SICP https://mitpress.mit.edu/sites/default/files/sicp/index.html

Solving a bunch of programming problems using a lisp language really helped me learn how to methodically solve problems programmatically.",hnrgica,t3_rbwnot,1638995194.0,False
rbwnot,"If you want to hire a virtual tutor, feel free to message me!",hnsxw52,t3_rbwnot,1639017643.0,False
rbwnot,"Lots of practice problems are available online. Try looking into coding competition sites. They have questions sorted by difficulty, and you can build your way up.

Here are some challenges on [HackerRank](https://www.hackerrank.com/domains/python) specifically for python.",hnqu85z,t3_rbwnot,1638986700.0,False
rbwnot,"Write more code. Look up python beginner projects and code along with them, watch python videos on freecodecamp's YouTube page, complete a leetcode question every day or 2. Just do something. You can't learn the wrong things. Just keep learning and the more you learn, the easier it will get to decide/know what to learn next.",hnqumei,t3_rbwnot,1638986852.0,False
rbwnot,"So I am a deadly practical person, and I like to always see the effects of what I am doing, so my way of getting better at coding was (and still is)  - thinking about what would I like to learn or build. Maybe a mobile application? In this case, I read how to build one, what languages you have. I think about the features which this app should have. Then I start doing tutorials which are showing specific elements which I want this app to have, for example how to do AR, or how to make the app use camera and read text from a picture.",hns6k7b,t3_rbwnot,1639005482.0,False
rbwnot,"Make tons of projects, check this link [https://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/](https://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/) try to make all of the projects. Doesn't really matter about programming languages, use whatever you're similar",hnsmoxv,t3_rbwnot,1639012702.0,False
rbwnot,Read good code.   Not enough time is given to reading successful code bases IMHO.,hnt9oif,t3_rbwnot,1639023268.0,False
rbwnot,"I got better by trying to automate various server tasks. Everything from renaming files to web scraping to video encoding etc.  Find those little mundane things you do and try and code it, then go back and improve the code once you've gotten better. You'll get there v",hntc1t8,t3_rbwnot,1639024512.0,False
rbwnot,"You improve coding primarily by coding. Read a lot of software written by others and write a lot of software of your own: there is no substitute for reading and writing software. Thankfully due to free software and open source software movements there are basically endless amounts of software you can read free of charge.

At the same time, you'll want to study as much discrete mathematics as you are able, which will help remove walls from your path before you encounter them in your programs.

You will need to study algorithms and data structures until they are completely internalized into your being, they are indispensable. I recommend Introduction to Algorithms or basically anything on the matter by Charles E. Leiserson. Their lectures are also available free of charge on youtube.",hntfb7g,t3_rbwnot,1639026291.0,False
rbwnot,"Write code, read books, watch videos about programming, check others code",hntpofx,t3_rbwnot,1639032890.0,False
rbwnot,Practice! There’s no short cut. Try contributing to open source projects.,hnqupug,t3_rbwnot,1638986888.0,False
rbwnot,"I know I’ll get some hate for this but I learned the most when I started using lower level languages like C and even Assembly (I would not recommend doing assembly tho). Having to implement everything from scratch with only the essential libraries will let you appreciate and understand runtime a little better. Even tho I use Python from time to time when I need to create a tool fast, I feel that always coding in Python will drive you to the lazy programmer path that can’t do anything without a library. But there’s nothing wrong with that if the requirements are met.",hnst8v1,t3_rbwnot,1639015591.0,False
rbwnot,"If you are looking to improve your problem solving abilities, check out Exercism or Kattis",hnqvd01,t3_rbwnot,1638987134.0,False
rbwnot,"You just have to put in the time.  Think of some kind of program you'd like to have that doesn't exist, and give it a try.  If it seems like ""a little too much work"", it's perfect, try it.  You'll learn a lot just researching the documentation and libraries needed, and fixing bugs.",hnqy0lz,t3_rbwnot,1638988143.0,False
rbwnot,Touch code everyday.,hnr0tin,t3_rbwnot,1638989214.0,False
rbwnot,"Have a goal like

draw up a wireframe of single webpage and try to implement it. Use internet search to implement it. Remember your primitives, loops, bool, ect. Follow software design principles.",hnr1934,t3_rbwnot,1638989378.0,False
rbwnot,"
Hello! You have made the mistake of writing ""ect"" instead of ""etc.""

""Ect"" is a common misspelling of ""etc,"" an abbreviated form of the Latin phrase ""et cetera."" Other abbreviated forms are **etc.**, **&c.**, **&c**, and **et cet.** The Latin translates as ""et"" to ""and"" + ""cetera"" to ""the rest;"" a literal translation to ""and the rest"" is the easiest way to remember how to use the phrase. 

[Check out the wikipedia entry if you want to learn more.](https://en.wikipedia.org/wiki/Et_cetera)

^(I am a bot, and this action was performed automatically. Comments with a score less than zero will be automatically removed. If I commented on your post and you don't like it, reply with ""!delete"" and I will remove the post, regardless of score. Message me for bug reports.)",hnr1asw,t1_hnr1934,1638989395.0,False
rbwnot,"Just do a simple project and look forward to errors that you'll get. every time you get an error either write it down or remember it (will happen a lot!!) at first it will be extremely confusing all these new errors and words you never heard of! but then over time you notice some errors that look familiar. and before you know it you can fix those errors and even predict and prevent yourself from writing buggy code. 

I also recommend specializing in one thing! maybe at first you are more general in your approach and you do anything and everything but then over time you find what you like and do just that! specialize in one thing! web dev, ML, AI, whatever. this way as you spend time developing your skills in that area you are not spreading yourself too thin.",hnr8l94,t3_rbwnot,1638992180.0,False
rbwnot,If you're a beginner and don't know the fundamentals very well then do lots of tutorials.  I personally recommend Scrimba as they have a very unique built in editor in their videos.  After you understand most of the basic move on to making projects.,hnrjj2n,t3_rbwnot,1638996328.0,False
rbwnot,"As the others have pointed out, practice is important. But something that also helped me were guidelines like ""The code should tell you what it does and the comments should tell you why it's doing that."" 

Also looking at actually good code will make you notice your own weaknesses. (For example I used to watch the YT Channel ""Coding Adventures"" at breakfast for a while and that helped me.)

Lastly, I think to better understand advanced concepts, learning some more esoteric languages like Haskell and Prolog, is actually quite entertaining and useful.",hnrwfv8,t3_rbwnot,1639001298.0,False
rbwnot,The same way you get better at anything. Practice.,hns3hd3,t3_rbwnot,1639004169.0,False
rbwnot,"Participate in an open-source project.

Learn how to use a debugger - debuggers are not just used for finding bugs; you can use them to step through code that you want to learn, try changing variables while debugging, learn how to set breakpoints.  This alone will make you a better developer than 50% of the competition (obviously a estimate).",hns5jyx,t3_rbwnot,1639005051.0,False
rbwnot,"Learn more than one language for one! Figure out how they're different, what their strengths are, when it makes sense to use which.

Google things like the docs or best practices and find simple ways to apply it. Take a hello world java program, make it a hello to an array of all planets. Maybe make a planet object with different fields. Call it via for loop. Then try writing it more expression based using streams and filters.

Google interview questions and try to solve them. Find coding challenges based on doing things in certain time efficiency where the brute force way is simple. Look for things that aren't just accomplishments in what they do, but that give you a better understanding of how something works or adds a new tool to your belt that you didn't know you could do before.",hnt1h20,t3_rbwnot,1639019267.0,False
rbwnot,Look at the source code for open source projects that have been around for a long time.,hnu0rlm,t3_rbwnot,1639041833.0,False
rbwnot,"1. Learn different programming languages and their paradigms (OOP, functional, logical etc)
2. Study complicated to understand and keep in mind math concepts — good example is combinatorics — just fo brain training)
3. Practice, but not blindly — i.e. having particular objectives. But the objectives should be feasible in a reasonable time. Good example is to participate in online programming competitions where you have to solve non-trivial enough problems of different difficulty levels and where your solutions strongly verified automatically (so you will always be confident if you succeed or not with the task)",hnuek8l,t3_rbwnot,1639052805.0,False
rbwnot,"Boot camp, projects , video tutorials",hnuw601,t3_rbwnot,1639061894.0,False
rbwnot,create projects with your knowledge do not do tutorials forever,hnvafs7,t3_rbwnot,1639067698.0,False
rbwnot,Spend as much time doing the best kind of learning you can. The best kind of learning is play.,hnrqbr8,t3_rbwnot,1638998909.0,False
rbwnot,[removed],hns792i,t3_rbwnot,1639005774.0,False
rbwnot,"Thanks for posting to /r/computerscience! Unfortunately, your submission has been removed for the following reason(s):

* **Rule 2:** Please keep posts and comments civil.



If you feel like your post was removed in error, please [message the moderators](https://reddit.com/message/compose?to=/r/computerscience).",hntd25r,t1_hns792i,1639025057.0,False
rbng33,"Best I can figure, it comes from [core dump](https://en.m.wikipedia.org/wiki/Core_dump). I still don't understand how ""dumping"" got that particular meaning when it comes to computers, and now that I've noticed that I don't know, it bothers me slightly.",ho83f8t,t3_rbng33,1639296186.0,False
rbng33,"**[Core dump](https://en.m.wikipedia.org/wiki/Core_dump)** 
 
 >In computing, a core dump, memory dump, crash dump, system dump, or ABEND dump consists of the recorded state of the working memory of a computer program at a specific time, generally when the program has crashed or otherwise terminated abnormally. In practice, other key pieces of program state are usually dumped at the same time, including the processor registers, which may include the program counter and stack pointer, memory management information, and other processor and operating system flags and information. A snapshot  dump (or snap dump) is a memory dump requested by the computer operator or by the running program, after which the program is able to continue.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",ho83ghv,t1_ho83f8t,1639296212.0,False
rbng33,"Well, I'm assuming Screen Dump comes from the concept of ""core dumping"" which is when a file of a computer's documented memory of when a program or computer crashed.

The term ""core dump"" likely originated in the 1960s when early computers used magnetic core memory. When a running program crashed, all of the data in the entire core was printed out on paper to help with debugging.

Core dumps are generated when the process receives certain signals, such as SIGSEGV, which the kernel sends when it accesses memory outside its addressed space.",ht2k3h9,t3_rbng33,1642445213.0,False
rbng33,"Thank you, that sounds likely!",ht5w6ru,t3_rbng33,1642503917.0,True
rb9f0g,"This is probably more a physics question than computer science question but ones used for my graduate courses have been:

“Optical Waveguide Analysis,” K Kawano and T. Kitoh, Wiley, 2001

“Computational Electrodynamics: Allen Taflove and Susan C. Hagness, Artech House, 2005, (Third Edition)

“Plasmonics”, S. A. Maier

“Surface Plasmon Nanophotonics”, M.L. Brongersma and P.G. Kik, Eds. 

“Principles of Nano-Optics”, Novotny and Hecht 

“Surface plasmons on smooth and rough surfaces and on gratings,” H. Raether 

“Near-field optics and surface plasmon polaritons,” Edited by: Satoshi Kawata

They aren't all computational books and really require rigorous computational math methods course",hno92pg,t3_rb9f0g,1638932174.0,False
rb9f0g,"A quick google search showed many books on this subject.   
No idea how good any of them are.",hno8snk,t3_rb9f0g,1638932028.0,False
rb71fe,RL?,hnmtvn8,t3_rb71fe,1638908576.0,False
rb71fe,"""real life"" I guess, but... honestly I have no idea where the assumption that DP in school is different from the real deal comes from.",hnn3d5n,t1_hnmtvn8,1638912508.0,False
rb71fe,"You rarely need to create algorithms yourself ""in real life."" You mostly find the ones you need on the internet.

In the rare cases that you do need to, I don't see why it should be different from you learned in school. Reality is often more complicated, but you learn this stuff for a reason.",hno7mys,t1_hnn3d5n,1638931431.0,False
rb71fe,My guess is reinforcement learning. That kinda makes sense.,hnosr51,t1_hnmtvn8,1638943797.0,False
rb71fe,"Yea, that makes more sense than ""real life""",hnosxcy,t1_hnosr51,1638943928.0,False
rawuw6,"had a spare camera in my cubicle 

used it to detect faces and send a desktop notification 

used it to switch to work from watching random videos on YouTube when someone was around",hnkzxwu,t3_rawuw6,1638877595.0,False
rawuw6,You got a git for that? Unironically sounds useful :D,hnl1iod,t1_hnkzxwu,1638878691.0,False
rawuw6,"I'll share it with you it's not on GitHub

it's pretty basic just found a interesting use case for it",hnl1ref,t1_hnl1iod,1638878854.0,False
rawuw6,Can you also share it with me?,hnn40yf,t1_hnl1ref,1638912779.0,False
rawuw6,"you know, for *science*",hno8sdv,t1_hnn40yf,1638932024.0,False
rawuw6,"will share it in the comments
I'll have to rewrite it but it's pretty easy",hno3yl1,t1_hnn40yf,1638929577.0,False
rawuw6,+1 share pls,hnop0bv,t1_hnl1ref,1638941204.0,False
rawuw6,Share please!!,hnorgay,t1_hnl1ref,1638942854.0,False
rawuw6,Saw something similar on YouTube where some device detects movement (not capturing faces) near the area and the computer screen switches to desktop automatically. It just switches automatically and I thought that was pretty cool.,hnl2k9l,t1_hnkzxwu,1638879390.0,False
rawuw6,Niceee,hnl1pks,t1_hnkzxwu,1638878819.0,True
rawuw6,"I work with ECG signals. My company has TBs of  labeled data from 30+ years in the ECG monitoring business. We are using it to build an automated arrhythmia detection system.

In practice, ECG data is just a very large array of 16-bit integers. Labels can be for a single heart beat or for a segment of any length.

There’s some bureaucracy to it because everything is FDA regulated, but I love my job.",hnl67ae,t3_rawuw6,1638881617.0,False
rawuw6,That's fantastic! I love the idea of working on research that might save someone's life. Nicely done!,hnl7gcq,t1_hnl67ae,1638882325.0,False
rawuw6,What job title/field would this be considered as? I want to get into something similar.,hnmocsf,t1_hnl67ae,1638906168.0,False
rawuw6,"My title is Data Scientist but since it’s a small company I wear many hats. Data engineering, Software Dev, MLOps, etc. I like that aspect of the job as well (some poole don’t).",hnn0s2o,t1_hnmocsf,1638911437.0,False
rawuw6,I am planning to study AI or computational science. Which is the best way would you say I can work in research programs in scientific/medical fields? Should I apply for internships and slowly build experience in that area?,hnpsfqt,t1_hnn0s2o,1638970991.0,True
rawuw6,"While there are some exceptions, the vast majority of research jobs in both academia and industry require a PhD. Sometimes ""research technician"" jobs will be done by people with a [B.Sc](https://B.Sc) or [M.Sc](https://M.Sc); however, they are not usually doing research but assisting with building the technology used by the researchers.

Getting a research technician type job usually some expertise in the software being used, e.g., Python, R, Matlab, etc. So learn to program in software languages used in scientific circles. Also, it is good to have a solid knowledge of analytics and statistics since it comes up frequently.  


Again, there are always exceptions but this describes the majority of such positions.",hnr1c9v,t1_hnpsfqt,1638989410.0,False
rawuw6,"I'm currently doing my bachelors in computer science, which is the best course would you say will help me achieve my goal?",hnr2p7o,t1_hnr1c9v,1638989934.0,True
rawuw6,"1. Any courses on AI, machine learning or computational intelligence.
2. Any courses on analytics, statistics.
3. If your university offers an introduction to research as a graduate course, then see if you can get an exemption to take it as an undergraduate. Such courses are usually not very difficult and this would be very helpful.
4. Since computer vision is still big in medical research then courses on computer vision would be good.
5. Any courses that deal with time-series data.

&#x200B;

If you DM me a link to your university course catalogue, then I could probably make more specific suggestions.",hnr3tu0,t1_hnr2p7o,1638990362.0,False
rawuw6,"Thanks for the list. 
My college is pretty avg and doesn't offer any of those course so I'll have to go somewhere else for PG.",hnr6c17,t1_hnr3tu0,1638991323.0,True
rawuw6,"Ultimately, it is all about having the experience to convince somebody you can do the job. The ""easiest"" (without saying it is easy) is to do a course, but if you can get the experience in another way, then it all counts. I certainly hope you manage to do it. Nothing better than somebody achieving their dreams. :)",hnr6lvq,t1_hnr6c17,1638991428.0,False
rawuw6,"I'm currently doing my bachelors in computer science, which is the best course would you say will help me achieve my goal?",hnr2nxn,t1_hnr1c9v,1638989928.0,True
rawuw6,Trained an NLP model to generate the onion articles,hnmrp5c,t3_rawuw6,1638907675.0,False
rawuw6,What were the results like?,hnoh0kq,t1_hnmrp5c,1638936368.0,False
rawuw6,"Still a work in progress, here’s a shitty proof of concept with an overfitted model (http://67.205.142.210:3000/). FYI it’s a bit slow since I didn’t feel like paying for a GPU machine lol.",hnpvx4c,t1_hnoh0kq,1638972728.0,False
rawuw6,Generated a Frank Ocean inspired album using deep learning,hnmxb83,t3_rawuw6,1638910029.0,False
rawuw6,lemme hear,hnmxrkt,t1_hnmxb83,1638910224.0,False
rawuw6,im currently training an AI model to predict race winners,hnmwmii,t3_rawuw6,1638909736.0,False
rawuw6,[current progress](https://imgur.com/a/QEMeVcF),hnpgoge,t1_hnmwmii,1638963852.0,False
rawuw6,"Getting an RSA algorithm working by hand was hell, but kinda fun",hnmje60,t3_rawuw6,1638904001.0,False
rawuw6,"I’m still at the beginning of my career but I worked on music in my undergrad. My latest project visualizes emotions in non-Western music.

Open source on [GitHub](https://github.com/nhstaple/feelskunaman) 🎸. Anyone can use it you just need SpotifyAPI keys.",hnnloab,t3_rawuw6,1638920643.0,False
rawuw6,"Most recent project was object detection and instance segmentation for orthopaedic x-rays. We were looking to detect certain abnormalities. A big part of the project became generating synthetic data for training since medical imaging data is so expensive (and that’s just for the images, not even labelled). At the moment I’ve just started my PhD and am working on explainable AI for healthcare, particularly around generative models since I have some experience with those.",hnnnd2z,t3_rawuw6,1638921429.0,False
rawuw6,"As part of a research project, I implemented a module to a legal document management system. When a case file was uploaded, it was ranking and suggesting possible legal precedents that could be used in court, saving intern time.

Nothing fancy, it was just using a bag of words model, with tf-idf, but it was getting the job done as the case files were very similar.",hnobzos,t3_rawuw6,1638933671.0,False
rawuw6,"Still in school, so this is an academic project, but I'm working on a project to teach an AI to play Hnefetafl. It's really fascinating seeing and working with the math involved.",hnohhwz,t3_rawuw6,1638936638.0,False
rawuw6,this is an interesting post,hnox7op,t3_rawuw6,1638947242.0,False
rawuw6,Didn't get much comments tho :(,hnpg9kn,t1_hnox7op,1638963544.0,True
rawuw6,[deleted],hnl4ysj,t3_rawuw6,1638880895.0,False
rawuw6,That's awesome. I hope you do win an award for all the work.,hnl6r6k,t1_hnl4ysj,1638881933.0,True
rawuw6,They just lost their job,hnnavor,t1_hnl6r6k,1638915695.0,False
rawuw6,"LOL!! Love it! :)  (not true thankfully, but I love it)",hnpryxl,t1_hnnavor,1638970752.0,False
rawuw6,what did he say?,hnnd3dw,t1_hnl6r6k,1638916692.0,False
rawuw6,Said that they worked on some really important projects that could end up changing the field of AI. Not sure why the comment got deleted,hnorkig,t1_hnnd3dw,1638942938.0,True
rawuw6,It was getting downvoted into oblivion so I assumed people did not want to hear about my research. :),hnprwgh,t1_hnorkig,1638970717.0,False
rawuw6,Would you be able to share some of the publications from your work?,hnl6leh,t1_hnl4ysj,1638881841.0,False
rawuw6,[deleted],hnl7c5d,t1_hnl6leh,1638882261.0,False
ram49s,"Provider provides, sure.  Marginally I could see the utility of breaking up interfaces in this way, for composability, but broadly, the implementation sure seems messy.  OOP is a helluva drug.",hnj4gx2,t3_ram49s,1638837329.0,False
ram49s,"Is there a behaviour behind those providers?   
The pattern here is to inject different strategies (usually interfaces, btw, not classes) where some could apply. If you are just returning a fixed value, it has no meaning.

Title provider makes sense if you have or might have several ways to generate a title out of... Something. But it should provide a function for that, so that different implementations could provide different behaviours.

Height provider, for a human, doesn't make a lot of sense as its just a fixed value.",hnkj7tv,t3_ram49s,1638863701.0,False
ram49s,reinventing swift protocols,hnkm4fs,t3_ram49s,1638866076.0,False
ram49s,"I'm soon to be a junior engineer and to me it looks like he knows nothing about OOP. It's impossible that someone had to sit through a faculty-level of OOP course and do assignments by using inheritance **that** way. That is literally impossible. Also, being able to pass OOP course **without** using inheritance is **also** impossible. Ultra necessary basic concept, and honestly him **not even knowing that it exists** is very strange. So my honest advice for him is to get some quality book and exercise to get his way of thinking right. 

I don't think it should be your job to teach him this and it's something that he needs to use a bit of his time to sit it through and cram some examples to get into his way of thinking. If he's using it this way than, to me, it beats the entire purpose of OOP.",hnj6qox,t3_ram49s,1638838359.0,False
rajca8,Take a look at your second and third year courses. You won't have a ton of free time for long.,hnjiu2t,t3_rajca8,1638843640.0,False
rajca8,Exactly,hnko8nc,t1_hnjiu2t,1638867893.0,False
rajca8,[deleted],hnkd3ip,t3_rajca8,1638859082.0,False
rajca8,this,hnkmhjl,t1_hnkd3ip,1638866386.0,False
rajca8,I’m a fan of cyber security and hacking. It’s a great addition because I doubt your program will cover it and it’s an amazing skill to have,hninnir,t3_rajca8,1638830013.0,False
rajca8,"Its a whole field, and i think op should at least study network architecture AND operating systems to barely start.",hnlpwev,t1_hninnir,1638891103.0,False
rajca8,"I have to respectfully disagree. I have learned those subjects as needed in my work on cyber security. For example you don’t need any of that to learn about a basic buffer attacks. Once you understand them you can learn a little about paging on the OS to understand w^x protections, but then get right back into hacking. 

I feel like it’s similar to just starting some coding projects and learning about data structures and algorithm complexity when you need to.",hnmt4tr,t1_hnlpwev,1638908278.0,False
rajca8,The missing semester of your CS education: https://missing.csail.mit.edu/,hnktq2o,t3_rajca8,1638872671.0,False
rajca8,"You beat me to it. This is the best advice around here. OP master everything on this course. 

Although, if you are on a program worth its salt, unless you are a  genius, your lots of free time has its days counted.",hnl4zn7,t1_hnktq2o,1638880909.0,False
rajca8,learn git. you will thank me later,hnkmh1k,t3_rajca8,1638866374.0,False
rajca8,Do universities really not teach git? I don’t imagine there is any software dev job where it’s not a necessity,hnkwp11,t1_hnkmh1k,1638875150.0,False
rajca8,My university didn't teach me git. Hell they didn't even teach me DS and Algos properly lol,hnl8v8q,t1_hnkwp11,1638883111.0,False
rajca8,"That’s just mind boggling tbh..

Especially as you could teach git alongside other topics.. at very least they should be teaching the basics of it",hnl9h72,t1_hnl8v8q,1638883431.0,False
rajca8,"Currently in university for CS, nowhere near learning git for class",hnm7zl8,t1_hnkwp11,1638899135.0,False
rajca8,never seen it even once,hnmej2h,t1_hnkwp11,1638901943.0,False
rajca8,Spend more time on your foundational courses to really *understand*. I don’t understand how you can have a lot of spare time as a first year CS student.,hnky1oo,t3_rajca8,1638876197.0,False
rajca8,"With previous experience (and depending on the specific courses they require, as what you’re thinking of may be second semester and/or second year) the first year of CS can be blown through somewhat easily

For example, where I go, the foundational courses are moreso second year. First year they basically just taught us Java and C in preparation for the next courses",hnm8602,t1_hnky1oo,1638899212.0,False
rajca8,"Before my first year I had taken more school maths classes than most of my peers, learned to program apps in Java, and did some basic computer architecture stuff in Minecraft (building a redstone computer). 

Definitely gave me a headstart and more free time in my first year - but the advantage certainly didn't last long! I imagine OP is just in a similar position, or they're just some freakish genius.",hnnwj7w,t1_hnky1oo,1638925855.0,False
rajca8,Get some weird board (not like a pi or arduino) and see what you can do with it,hnjvmfj,t3_rajca8,1638849243.0,False
rajca8,"Kung fu. Great way to develop discipline, stay fit and not get your ass kicked. You’ll spend most of your career barely moving as a programmer/software engineer so getting in the habit of being active is gonna pay dividends in the long run.",hnlcpbd,t3_rajca8,1638885103.0,False
rajca8,"What kinds of things do you want to do when you graduate? 

For web dev learn more about socket programming and/or webassembly. That will put you ahead of people who only know how to use existing frameworks and tools.

For AI/ML study math and stats such that you can compete with stats majors or get into a grad program. 

For desktop apps learn the Windows API.

For mobile apps study any mobile app toolkit. 

For embedded learn either VHDL or SystemVerilog using an FPGA. That will make you more competitive with EE and CE majors. 

For OS development do as much bare metal programming as you can in C and assembly and learn all the common hardware interfaces. 

For game development learn some Blender and other 3D tools. 

Basically take your preferred career domain and study information that is useful for it but won't be covered or isn't part of CS.",hnlcy1u,t3_rajca8,1638885223.0,False
rajca8,Do leetcode problems. It’ll help you get interviews and pass technical interviews,hnnhkbk,t3_rajca8,1638918723.0,False
rajca8,"ML, basic calc and linear algebra are enough to get started",hnjgn1a,t3_rajca8,1638842687.0,False
rajca8,"He said he’s a first year computer science student that wants to study things not in the program. Calc and linear algebra are basic things he’s likely already taken, and if he chooses to he’ll do some ML soon.",hnkcvie,t1_hnjgn1a,1638858929.0,False
rajca8,is linear algebra course in compsci hard?,hnkgbw3,t1_hnjgn1a,1638861440.0,False
rajca8,Soft skills. That is what is going to make the difference in an interview,hnkwgvb,t3_rajca8,1638874968.0,False
rajca8,"Statistics, get to logistic regression.",hnl2afs,t3_rajca8,1638879209.0,False
rajca8,"Just do personal projects - best way to learn and you can put them on your resume. Hey, if one is really good you might just get your own company going.",hnl4r9f,t3_rajca8,1638880770.0,False
rajca8,"I am also a first year cs student and its amazing how some people have this foreign concept called “free time” , i am studying for my final in 2 hours as i write this…",hnlb5qm,t3_rajca8,1638884316.0,False
rajca8,I'm finna get a C below for this math 141 final on god,hnosw0e,t1_hnlb5qm,1638943899.0,False
rajca8,"Join the debate club, or the boardgame society, or perhaps the rock society, etc.

Maybe volunteer for one of the charitable student organisations?",hnlfknk,t3_rajca8,1638886516.0,False
rajca8,Data Structures & Algo,hnlpqos,t3_rajca8,1638891035.0,False
rajca8,Learn how to write unit tests. They'll save you time in the long run.,hnm6c66,t3_rajca8,1638898411.0,False
rajca8,"this is the next obstacle in my journey.

any recommendations for resources that will teach this from the ground up?",hnsk2ig,t1_hnm6c66,1639011524.0,False
rajca8,"Whatever book or website you choose to learn this from should be fine. It's not hard to learn. It's hard to make it a practice, because once your code works, what's the point of writing tests to see if it works? So sometimes people will write the unit tests *first* \-- you can look up ""test driven development"" for this approach.

I didn't have any resource to learn from -- I just copied the style where I worked. Most of the code already had unit tests, and if you changed anything you had to add a unit test that covered your change.

Python has a built-in unit test framework now, but it didn't have that when I started; so my own unit tests are sometimes a little janky, lol.",hnvgh4c,t1_hnsk2ig,1639070020.0,False
rajca8,Thank you Mr. Numbers!,hnvj786,t1_hnvgh4c,1639071069.0,False
rajca8,"It's hard to say when:

1. You don't list what subjects you're currently doing and
2. You don't say what field of IT you want to go into...",hnysxj8,t3_rajca8,1639126074.0,False
rajca8,Check out pathfinder.fyi and choose hard skills most in demand from there,hovf4hr,t3_rajca8,1639712729.0,False
rajca8,"Look at web dev frameworks like Django or Laravel Backend. React or Angular frontend, if you'd consider web dev as a career option.",hnkr9yx,t3_rajca8,1638870547.0,False
rajca8,"Depends on what you want to do. Where I work a cert for Azure Fundamentals would have been a good step in the door. It is related to CS, but not CS itself.",hnjryc9,t3_rajca8,1638847600.0,False
rajca8,LearnOpenGL.com,hnl9kv3,t3_rajca8,1638883485.0,False
rahxnv,Since you finished formal automata and still interested in theory part of cs you can continue with Computability Theory then Complexity Theory.,hnii59h,t3_rahxnv,1638827791.0,False
rahxnv,Where/how did you study Automata Theory? Was it in college or some online course/self-teaching? I'm very fascinated in the subject but am always worried about missing key information while self-teaching.,hnjlhka,t3_rahxnv,1638844776.0,False
rahxnv,"College. Just finished my semester today on it. We used Peter Linz’s text book. 

We followed the book very closely. The teacher was all about us learning and didn’t care about our grades, so he challenged us a lot on the assignments and exams. Some of the problems on the assignments would take ~2-3 hours to complete (like devising Turing Machines, PDAs, DFAs)",hnjlt1p,t1_hnjlhka,1638844915.0,True
rahxnv,besides the mentioned computability and complexity theory you can also double down. There are flavours of automata that handle infinitely long words (e.g. Büchi Automata) which can be used to do software verification.,hoscpbo,t1_hnjlt1p,1639667089.0,False
rahxnv,"So you now seem to have ""half"" a understanding of what would be taught in my class on theoretical computer science. ""Basic"" things you might be missing (based on a cursory skim of that book's chapters)  


* The Myhill-Neorde theorem
   * See https://bosker.wordpress.com/2013/08/18/i-hate-the-pumping-lemma/
* A bit more complexity theory 
   * Thinking about NP as ""verifiable problems""
* More undecidability/computability theory
   * (Many-One) Reductions
   * SNM and Recursion Theorem
   * Index Sets
   * Hierarchy of uncomputable problems / Arithmetical Hierarchy
      * That one in general is interesting since it's one of the more graspable connection between logic and computability theory, if that interests you  


Unfortunately, I can't really recommend any good book. But the wikipedia articles on each of them are somewhat useful, though of course they don't offer practice exercises. Perhaps the open logic project might have more, especially on ""undecidability"" theory.",hnjxyez,t3_rahxnv,1638850320.0,False
rahxnv,A natural next step is compilers.,hnjt6et,t3_rahxnv,1638848141.0,False
raflir,"Typically a program/malware is impossible to run without supported environment or libraries, hence you have now two options:

1) write a program in C since it's almost supported by all OS then let it silently install the required environment/libraries. After that it can run the malware.

2) most of the systems support some high level languages by default and are installed beforehand, like, .NET for windows which run C#, vb.net or python for linux.

As for the libraries it's pretty simple, you can make two files inside each other, the first one checks for libraries and install the missing ones then it unpack the second and run it.

Edit: hmm it's cs subreddit, I think r/malware would be better",hni08xe,t3_raflir,1638820878.0,False
raflir,"I see, thank you",hni1nyl,t1_hni08xe,1638821437.0,True
raflir,"I have heard of payloads that included the interpreter with them. For example you’d have a power shell script that has the b64 encoded binary of the python interpreter. The powershell would decode the binary then execute it with the python as input. This assumes you know the computer architecture for the binary and that it runs powershell, but those are easy enough guesses.",hnmvmtu,t3_raflir,1638909310.0,False
raflir,"Think of them as ""self-contained"" programs (in order to run), that may connect to a online host (a server) in order to carry out a certain function.  Of course depending on the malware, will dedicate what it does, if it needs to connect to an external online host, etc...",hni1dj4,t3_raflir,1638821322.0,False
rabwtu,"Yes of course it is worth it! most practices are the same for more than 40 years.  
Unless there are new technologies on the matter, it won't make much difference. And even if there are, it doesn't matter so much, it would still be worthwhile. You will only have to do some ""upgrades"" on what you know.",hnhnoyh,t3_rabwtu,1638815955.0,False
rabwtu,Yes.,hnhnctt,t3_rabwtu,1638815823.0,False
rabwtu,Wow this is a good website! Information are good! It is not old at all!,hnk6euz,t3_rabwtu,1638854827.0,False
rabwtu,"The same theory I learned in 2000 is totally valid today. While programming languages and frameworks come and go, the theory and engineering principles are the same. The worst part is people want to reinvent the wheel all the time instead of reading and understanding and using what people already figured out 30-50 years ago. So, go through those textbooks your might learn a thing or a dozen.",hnkcrfi,t3_rabwtu,1638858850.0,False
raadci,Tree structures are very common when you analyse text. For example to solve a mathematical equation or interpret a code file. You could look at [this](https://en.m.wikipedia.org/wiki/Parse_tree) but I would also recommend a text on computer languages like SICP.,hnhxy5x,t3_raadci,1638819982.0,False
raadci,"Desktop version of /u/forsasateri's link: <https://en.wikipedia.org/wiki/Parse_tree>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hnhxzmq,t1_hnhxy5x,1638819998.0,False
raadci,thanks,hninvze,t1_hnhxy5x,1638830109.0,True
ra4e6d,"Graphics is basically all linear algebra, if you want some motivation you could look at for example ""the raytracing challenge"" which starts with a lot of matrix stuff.

In general: basically any modern subject in maths has some connections to linear algebra, because linear algebra makes a lot of difficult problems ""easy"" (e.g. solving partial differential equations using FEM) and so we try to really find those connections - and given how ubiquitous maths is in programming you can probably find some connections to whatever you want to do (in audio for example: dft is a linear transformation which already leads to matrices and a dft'd signal is a vector).",hng7get,t3_ra4e6d,1638792406.0,False
ra4e6d,Your graphics card is basically machine that does all the linear algebra related to rendering graphics on monitor. Graphics card is like a linear algebra calculator.,hngdzii,t3_ra4e6d,1638796473.0,False
ra4e6d,"Essentially all machine learning is also done on matrices. Actually, a lot of data science is, like principal component analysis (related to taking eigenvalues and vectors of a covariance matrix). In fact, all data is really just matrices with rows being observations and columns being properties associated with those observations.",hngf9o5,t3_ra4e6d,1638797173.0,False
ra4e6d,"I was in the same boat. Took the class and didn’t get the point. Since then, I’ve taken a computer graphics class, a machine learning class, and a deep learning class and have really regretted not taking it more seriously. But I do think they should have made a point of explaining applications at the time…",hnitfwt,t1_hngf9o5,1638832445.0,False
ra4e6d,It's extensively used in machine learning. Good to build a solid foundation early.,hngffdo,t3_ra4e6d,1638797260.0,False
ra4e6d,"Another application a little bit more theoretical (but really interesting) is quantum computing, almost everything in QC is linear algebra (and probability).

You should also read about the Google page rank algorithm, it has a lot of linear algebra.

Good luck with the subject! Give it a chance because is really beautiful",hngr220,t3_ra4e6d,1638802908.0,False
ra4e6d,"Not directly CS related, but relevant:  if you end up working in decision analytics or project management, a solid foundation in LA helps *a lot*. Basically all optimization problems are most easily solved as matrix manipulations. Need to figure out the perfect combo of manhours, materials, and profit for different products/systems? You can do that in a single, fairly simple, simplex tableau.",hnh75ft,t3_ra4e6d,1638809498.0,False
ra4e6d,"https://youtu.be/rowWM-MijXU
Watch this. Linear algebra is like epitome of mathematical applications today. Almost everything you can think of has linear algebra in it some way or the other. It is extremely abstract but hold onto it.",hng170d,t3_ra4e6d,1638787561.0,False
ra4e6d,"I failed linear algebra at university and had to re-take the exam; not because I’m bad at math (I got top grades in all our other math courses) but like you I struggled to connect it to the rest of the curriculum. I knew it had a connection to 3D graphics but we weren’t doing any of that, nor was I planning a career in game engine development.

Fifteen years later I’m building platforms for machine learning and I have done a couple of Coursera courses in ML as well, and let me tell you, linear algebra rocks. There are plenty of software engineering careers where you won’t miss it, but if you think ML/AI sounds like something you would like to explore, you should dive into linear algebra as soon as possible.

The Youtube channel 3Blue1Brown has a great video series about it: https://youtu.be/kjBOesZCoqc",hnghc9b,t3_ra4e6d,1638798275.0,False
ra4e6d,"I'll also throw in scientific computing. Scientists used to wonder if the universe would expand forever, equalize, or eventually shrink back down into another big bang. To answer that question they basically did one giant matrix multiply over measurements of the cosmic background radiation. Everything from chemistry to cosmology uses linear algebra everywhere. Many problems have ""perfect"" solutions (like the traveling salesman problem or laying out wires on a computer chip). Those problems can be turned into a system of equations, stuck in a matrix, and solved using a bunch of the stuff you're learning now.

I know people always talk about how important math is to CS. The reality is that I rarely deal with most math in my work. The exception is linear algebra and statistics, those are everywhere. Some professors once tried to identify the most important patterns in computing so they could build specialized chips. Linear algebra was one of only seven core computations that made the list. I could go on. The point is, not only is math generally useful in CS, you've actually chosen the single most relevant field of math to ask about!",hnh6cy7,t3_ra4e6d,1638809189.0,False
ra4e6d,Do you have any more information on those seven core computations? Sounds interesting and I'd be curious to learn more.,hnw7av3,t1_hnh6cy7,1639080604.0,False
ra4e6d,"As mentioned by others, linear algebra is vital to graphics programming.

A pragmatic example would be a list of 3D vertices that make up a cube. How would you transform the cube? Transformation matrices.

Other examples are intersections, raytracing, and quaternions.",hngop5l,t3_ra4e6d,1638801855.0,False
ra4e6d,"There are two aspects that motivate Linear Algebra in CS (at a high-level):

1. So, so, so, so many real-life computational problems can be formulated as a linear algebra problem; often this is a) some model of a real-world process or b) a collection of data, both of which can be structured in terms of LA objects (i.e., vectors, matrices, & tensors).
2. Modern hardware can do matrix operations \*incredibly efficiently\*. This is largely a product of decades of computer architecture work and optimized implementations of algorithms. A lot of this boils down to data reuse/locality and vectorization.

TL;DR: When you use 1) to represent a problem as LA structure, you can leverage 2) to solve that problem very efficiently (at least, as efficient as your algorithm will allow).",hnhu3te,t3_ra4e6d,1638818471.0,False
ra4e6d,"Machine learning, 3D graphics; game design.

I actually wanted to get more in depth into it myself but have been way too busy my junior year. Hopefully next semester in my senior year I can learn some from discrete math. Haven't taken a math class in 2 years.

I'm not awfully familiar with audio programming but I don't think it would require a lot of interest and knowledge in linear algebra, maybe some in calculus.",hnhyhil,t3_ra4e6d,1638820194.0,False
ra4e6d,"Game development is a major (and fun) one. Representation of graphics and transformations of them (rotation, translation, etc) is usually done through vectors/points and 3D/4D matrices. Jason Gregory (lead dev for Naughty Dog) has a whole chapter about it in his [book](https://www.gameenginebook.com/) - I highly suggest you read at least the “3D Math for Games” chapter for some real world use cases for linear algebra.

(source: I’m a game developer at a studio, and I can confirm linear algebra actually matters in my daily job)

Machine Learning and Deep Learning are also big use cases but other comments have mentioned it already.",hnizunv,t3_ra4e6d,1638835235.0,False
ra4e6d,You mentioned audio programming. I’d look into digital signal processing (dsp) as that’s what a good chunk of audio programming really is under the hood. Things like the discrete Fourier transform and related operations involve things you’d learn in your linear algebra course.,hnjymwp,t3_ra4e6d,1638850641.0,False
ra4e6d,It's extensively used in machine learning. Good to build a solid foundation early.,hngfge3,t3_ra4e6d,1638797275.0,False
ra4e6d,"Hi there I'm happy to say one of my friends has created a really cool website to help with linear algebra and webgl.

[http://www.invectorize.com/home](http://www.invectorize.com/home)",hnggdu5,t3_ra4e6d,1638797773.0,False
ra4e6d,"Maths’s hugely used in IoT, game development and thereabout, you’ll never know where you’d be in the future, so you’d better learn it now as you have such opportunity",hnhs4ou,t3_ra4e6d,1638817688.0,False
ra4e6d,"A matrix, A, can be a transformation of a vector, v.  So for instance There is a matrix that can rotate v by 90 degrees.  This is useful for many things.  In computers, it is useful for graphics.  It's useful in quantum computing as well.  There is something called a hadamard matrix which puts a quantum vector (v) into superposition of multiple states (you can look that up) which in certain systems in quantum programming is a rotation of the state vector on what's called a Bloch sphere  (coordinate space for a 2 level qubit) .  

https://en.wikipedia.org/wiki/Rotation_matrix

https://en.wikipedia.org/wiki/Hadamard_transform  ​

That being said, a vector can represent a lot of things.  It just depends what space it is in and its axes. 

The same goes with computer graphics.  On your screen are pixels that are each composed of RGB filters.  To display a large range of colors the human eye can see, each sub-pixel will have some value for how bright it needs to be in combination with the other sub-pixels for a specific pixel color.  This can be in the the form of a vector and objects created with the pixels can be moved around by linear algebra and matrix transformations.",hnk735u,t3_ra4e6d,1638855222.0,False
ra4e6d,"Gilbert Strang has an excellent book on the topic of linear algebra ([Introduction to Linear Algebra ](https://www.amazon.co.uk/Introduction-Linear-Algebra-Gilbert-Strang/dp/1733146652/ref=asc_df_1733146652/?tag=googshopuk-21&linkCode=df0&hvadid=500859832694&hvpos=&hvnetw=g&hvrand=2832046874964718038&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=1006523&hvtargid=pla-1235394973160&psc=1&th=1&psc=1)).

As many have pointed out here, matrices and vectors have a lot of applications in graphics. 

However, it goes far beyond that. In fact, linear algebra turns up almost everywhere, eg physics problems, data science, optimisation, etc. It is a topic that is well worth having a decent knowledge of, whether you are a mathematician or a computer scientist. 

I study as a mathematician (doing my PhD), and I am continually surprised at how often numerical linear algebra springs up. 

[Here is a free PDF of Gilbert Strang's book](http://libgen.li/edition.php?id=138573030). This is the 5th edition, though a 6th has recently been released. I'm not sure what the changes are. 

What is especially wonderful about the Gilbert Strang book is that it has a selection of sister lectures from MIT available free online on YouTube. So, if you read a topic in the book and struggle on the exercises, you can then turn to the lecture series for additional information. I think its a great way to learn. There's a chance it goes a little deeper than you might need, and might be more generalised, but at least you can get an appreciation for the breadth and importance of this topic!

Edit: you are also likely to run across NLA for things like image reconstruction and file compression, which are computer science topics :) let me know if you want more info. You talk briefly about enjoying audio - well, the discrete fourier transform is all linear algebra, and so it has applications in removing white noise etc (as mentioned by another commenter)",hnkx7xo,t3_ra4e6d,1638875562.0,False
r9qvnk,"I don't think that you should worry about your lack of programming experience. Your role is to lead the team, not do all of the work yourself. Talk to the more experienced programmers and get their opinions about how best to structure and split up the code. Lean on them to help with the more technical aspects and focus on the leadership stuff, like making sure everyone has something to do, making sure people are comfortable with their tasks, setting up tools to plan the project like Trello, and that kind of stuff.

For regularity of meetings, every week is best so that people stay engaged and keep doing stuff with the project. You can always add more to it if it gets done early, but projects always take longer than you'll originally expect them to. As an example of stuff to add, if it's a website, you can always do more to the ui, like making it both computer and mobile friendly.

To help with the skill gaps, a good strategy can be to pair up experienced and inexperienced people to work on tasks together. Pair programming is slower, since you have two people doing the same thing, bit in the long run it will help everyone get to the same skill level.",hnepknk,t3_r9qvnk,1638757310.0,False
r9qvnk,"Just to add, pair programming is great not only to help the less skilled, but it helps the more experienced programmer learn how to communicate concepts and ideas clearly and helps them better their own understandings of how things work when they have to explain it and potentially get questions they never asked themselves.",hnet754,t1_hnepknk,1638758939.0,False
r9qvnk,"Maybe you should step down as a team lead, if you really feel you aren’t experienced enough. I can guarantee you that eventually your inexperience will surface through one way or another, and someone more skilled may challenge your position as team lead.",hndrhpp,t3_r9qvnk,1638742868.0,False
r9qvnk,"I completely disagree with this sentiment. This is a valuable learning experience that OP should go through. It's a safe project to learn with, since it's just a highschool club project that, as he puts it, should be simple. Stepping down should only be considered if OP feels like the amount of work it takes to be a team lead is negatively affecting them outside the club.",hnenef5,t1_hndrhpp,1638756336.0,False
r9qvnk,"I can tell you as a high school student, appointing someone inexperienced into a leadership position will only spur a lot of negative sentiment, especially from the experienced folks you want to retain.",hnewt85,t1_hnenef5,1638760584.0,False
r9qvnk,"Or negatively effects the team experience. Just because it helps OP learn doesn't mean it helps the other 9 people trying to gain experience as well. Not saying OP should or shouldn't step down, just that it really depends on the group as a whole what's truly best.",hnesx7x,t1_hnenef5,1638758814.0,False
r9qvnk,"By the sound of it, the project you’ll be working on is somewhat already decided. In that case, try setting up an introductory meeting to get to know everyone and discuss skill sets, etc. Then, start planning the project with roadmaps, visualizing the code and its dependencies within the scope of what you’re building. This visualization can be a guide, as it provides tasks of what needs to be built. Some things will be more complicated, while others will be much more simple. As long as you keep track of how the different tasks interconnect and maintain consistency, you can split these tasks amongst the group by skill and interest.

There’s a variety of tools out there that can help with this kind of thing, such as GitHub with its repository projects functionality. Try to outline the project, divide the tasks, and approach it systematically (for example, start off with pseudocode before getting more complicated). Don’t forget regular check-ins to make prevent issues with merging code later on.",hndsbn8,t3_r9qvnk,1638743198.0,False
r9qvnk,"Focus on figuring out the leadership stuff. The thing to remember is you’ve got 9-10 coders and only one leader. So you should start with assuming you’ll be doing none of the coding. Once you’ve got a handle on the management you might find that there’s time to schedule some coding to yourself. Being leader doesn’t mean you have to have all the ideas yourself; lean on those members who have more experience, make it clear you’re listening to them.",hnfsf12,t3_r9qvnk,1638779976.0,False
r9bx1d,Switch != Bridge,hncih7u,t3_r9bx1d,1638726478.0,False
r9bx1d,"Sometimes, people don't explain stuff well enough for me to understand, making this programmed type of speech the chearest possible means for getting information across.

 Or, put simply,


 dataLoss(speechstyle = ""program-esque"") < dataLoss(speechstyle != ""program-esque"")",hqwygwn,t1_hncih7u,1641113128.0,False
r9bx1d,"What this article also shows is how useful personifying things can be. If you think of programs as actors or agents and personify them/have conversations with them, it makes it easier to conceive of them.",hncx22l,t3_r9bx1d,1638731787.0,False
r9bx1d,Much appreciated.,hnb21y8,t3_r9bx1d,1638696551.0,False
r9bx1d,This is a gem. Thanks!,hnb8hnx,t3_r9bx1d,1638701873.0,False
r9bx1d,Are there more kind of resources about networking for developers or beginners?,hne8jkj,t3_r9bx1d,1638749819.0,False
r9bx1d,"Awesome resource, thanks!",hneyrov,t3_r9bx1d,1638761458.0,False
r95hk3,SVG’s are descriptions of images. So the computer can generate an arbitrarily good pixel display of the description. See Wikipedia on Rasterization,hnaah2v,t3_r95hk3,1638678247.0,False
r95hk3,"Agreed [rasterization](https://en.m.wikipedia.org/wiki/Rasterisation) is the general name of the process that computers use to turn mathematical description of shapes (as stored in SVG) into actual pixels for display. Note that this occurs ONLY when you view the result and takes into account how zoomed in you are when you are viewing it. So if you zoom in more, the rasterization process is re-run. This is what allows you to zoom as much as you want and never run out of “resolution”.

Also to answer the specific request for the how this goes for a circle, this [Wikipedia article](https://en.m.wikipedia.org/wiki/Midpoint_circle_algorithm) shows one of the classic algorithms for that case.",hnclroe,t1_hnaah2v,1638727695.0,False
r95hk3,"It’s math. Take a circle with its center at (xc, yc) and with radius r. Now take a point that you’re interested in, (x, y) and check if it’s part of the circle, how do you know? Well is the point further than the radius of the circle from the center of the circle or not? In other words sqrt((x - xc)^2 + (y - yc)^2 ) <= r. If that statement is true the point is in the circle, if it’s false it’s outside of the circle. When you zoom in you check each and every pixel, using its corresponding coordinates within the image, color them appropriately.

There’s equations for rectangles, rotated rectangles, triangles, ovals, various curves, spheres, cubes, etc etc etc.",hnambx7,t3_r95hk3,1638684901.0,False
r95hk3,"Its a lot less complicated than you're assuming.

**S**calable **V**ector **G**raphics format stores image data not as pixels, but as shapes and curves (and colors and widths, etc). An SVG viewer **draws** the shapes to display the image.

Here's a simple example: below is literally the SVG code for a circle with radius 50, centered at position (100, 100), on a canvas that is 200x200 units in size.

&#x200B;

    <svg width=""200"" height=""200"" xmlns=""http://www.w3.org/2000/svg"">
      <g>  
        <title>Layer 1</title>  
        <ellipse ry=""50"" rx=""50"" id=""svg_2"" cy=""100"" cx=""100"" stroke=""#000"" fill=""#fff""/>  
      </g>  
    </svg>

If your more curious, heres the full SVG specification, which defines the SVG code, talks about the document object model (DOM, also a part of web page rendering), and the rendering model: https://www.w3.org/TR/SVG/",hnaajq7,t3_r95hk3,1638678285.0,False
r95hk3,Key word: scalable,hnazrmc,t1_hnaajq7,1638694641.0,False
r95hk3,"Bézier curves, [https://en.wikipedia.org/wiki/Bézier\_curve](https://en.wikipedia.org/wiki/Bézier_curve), can represent curves using straight lines. A straight line is defined by its endpoints, which are just numbers. The section of the Wikipedia article on constructing the curves is what I was most curious about the first time I looked it up.",hnaf5np,t3_r95hk3,1638680689.0,False
r95hk3,math.  vector images are based on math.  computers are not bad at doing math and drawing it on the screen.,hnavvl5,t3_r95hk3,1638691555.0,False
r95hk3,It's about vector graphics: https://en.m.wikipedia.org/wiki/Vector_graphics,hnbb4do,t3_r95hk3,1638704027.0,False
r95hk3,"**[Vector graphics](https://en.m.wikipedia.org/wiki/Vector_graphics)** 
 
 >Vector graphics, as a form of computer graphics, is the set of mechanisms for creating visual images directly from  geometric shapes defined on a Cartesian plane, such as points, lines, curves, and polygons. These mechanisms may include vector display and printing hardware, vector data models and file formats, and software based on these data models (especially graphic design software, Computer-aided design, and Geographic information systems). Vector graphics are an alternative to raster graphics, each having advantages and disadvantages in general and in specific situations.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hnbb57a,t1_hnbb4do,1638704047.0,False
r95hk3,"Simple answer: No.

A computer cannot even draw a mathematical point (which has no dimensions whatsoever)—in fact one cannot exist in any real sense. A mathematical circle has no height and no width.

No perfect mathematical shapes exist in nature, although some shapes come very close. Even when a physical law applied to a certain situation predicts a perfect mathematical shape, there are always extra factors that are not considered by the physical law that mess things up: friction, air resistance, relativistic corrections, quantum uncertainty, atomic granularity.

An example of something very close to a perfect circle in nature is the orbit of Venus about the Sun. This near perfection is attained because:

1. The initial velocity of Venus was such that its orbit is nearly circular and not as elliptical
2. There is very little air resistance or friction in space
3. The Sun is so distant from Venus and so round that it acts almost exactly as a point source of gravity
4. Venus is so big that quantum effects are very small
5. Sun's gravity is weak enough that Newton's law of gravitation is reasonably accurate

But, all of these statements are not perfect, so there are still many small sources of deviation from a perfect circle, even for Venus' orbit.

Consider trying to draw a perfect circle on paper with graphite. Even if you were able to use an AFM tip, laser sensors and a feedback loop to perfectly space every single carbon atom to form the circle, you still have the fact that the circle is made out of atoms. Zoom in enough on the circle and it's not smooth anymore because of the profile of the atoms.

(This moved fast from CS to Science)",ht2lguo,t3_r95hk3,1642445732.0,False
r8z2ei,"Commercialized means that you can't produce a product with their API and make money off of it. I assume this also means that you probably can't publish it on the app store, but don't quote me on this.",hn8unba,t3_r8z2ei,1638655018.0,False
r8z2ei,"But I can use it as a personal project, and put it on a resume for sure right?

As for the publishing on the app Store things still aren't clear to me. As long as I'm not monetizing my app, I'm curious if it still falls under ""commercialization"".",hn8w8w8,t1_hn8unba,1638655704.0,True
r8z2ei,"You can use it on your portfolio, yes. As long as you aren't making money off it, you can do whatever you want with it!

Edit: Commercializing something simply means making it available to buy.",hn8wh6m,t1_hn8w8w8,1638655799.0,False
r8z2ei,"I see, that clears it up. Thanks!",hndvph5,t1_hn8wh6m,1638744552.0,True
r8z2ei,If you release it for free and don’t include any in app purchases you can definitely publish. Allowing people to download the built binary for there device is not commercialization. Even better if you open source it. Saying you can check out my app on the AppStore would be amazing for your resume.,hn9vzzh,t3_r8z2ei,1638671373.0,False
r8z2ei,"Thanks for the clarification!!

>Saying you can check out my app on the AppStore would be amazing for your resume.

YES it really would lol. I've been applying to jobs a lot but my resume just never gets shortlisted for some reason. Plus so many recruiters have this requirement of ""must have at least one app published on the store"".",hndvzgm,t1_hn9vzzh,1638744663.0,True
r8z2ei,"I’m not a lawyer but I believe there’s no widely agreed consensus on what “commercial use” means or where the boundaries of it are. For this reason the “NC” family of Creative Commons licenses are best avoided and generally considered to be incompatible with open source. One of the tests in that scenario is “if you include it on a DVD of software, could you sell the DVD to cover manufacturing costs?” I guess that’s analogous  to your scenario where you have costs to publish an iOS app (active developer subscription) if you attempted to recoup that. To sum up, “it depends”. Might be worth avoiding the potential problems.",hnb2l6s,t3_r8z2ei,1638696995.0,False
r8z2ei,Hmmm. Well at least I can use it on my portfolio.,hndvrze,t1_hnb2l6s,1638744580.0,True
r8upnl,"They did it just like modern computers do it. The processor executes a program written using numeric machine code instructions, which are basically a set of digital logic gate configurations that get triggered by specific values. Instruction 01 could be a ""load from memory address into the accumulator register"" instruction, for example, and so when the processor reads an instruction and receives a sequence of bits that corresponds to that instruction, it enters a state where it will next read a numeric address, and then finally it will read the contents of that memory cell and store the value in the accumulator.

Then let's say the next instruction is an ""add accumulator with value"" instruction, and that causes the processor to enter a state where it next reads a raw value, and finally performs a binary addition operation (also implemented using logic gates) with the read value and the value in the accumulator, storing the result in the accumulator.

And so on. This is of course a rather simplified description even by early standards, but it's fundamentally how a computer works. Modern languages always get translated into machine language in the end.",hn8e4oi,t3_r8upnl,1638647867.0,False
r8upnl,"A programing language is just a way to interact with the computer's hardware. You can take a look at digital circuits to see how hardware is built to preform specific tasks. If you look at industrial automation and process control you can find many examples of how real world information is captured by computer systems. 
Have fun!",hn81vev,t3_r8upnl,1638642760.0,False
r8upnl," To understand the classical computer, it's useful to sort of build up to it in complexity. We started with electrical circuits where humans could manually open and close circuits via switches. A switch allows a circuit to store a single bit of information which says either ""the circuit is closed"" or the circuit is open. But what if, instead of a human having to flip a switch, we could create a switch that would open or close a different circuit based on whether or not its own circuit is open or closed. Such as switch is known as a transistor, and through the triumph of the transistor we can go beyond simple circuits and arrive at machines which can perform arbitrary computation by opening and closing circuits in complex patterns.

So however a punch card reader worked physically, it ends up in opening and closing a circuit in a pattern that can then be stored in the open and closed states of the computers internal circuits. Then the computer can use this stored set of open and closed circuits as a starting point and by opening and closing circuits in it's particular pattern( as specified by its instruction set), the computer can execute the stored program. (This might be a slight oversimplification, because real computers might rely on more complex physics for some sorts of memory, but all the logic in a computer could be implemented with circuits in theory)
    
Hopefully by this explanation, you can see that these processes are purely physical, there is no need for natural language such as that which might be used by a human.",hn915z3,t3_r8upnl,1638657806.0,False
r8upnl,It's all about those 0s and 1s.,hn7vy6z,t3_r8upnl,1638640406.0,False
r8upnl,i beleive the computer has each charecter defined hardware wise as a string of bits.,hn87pk3,t3_r8upnl,1638645162.0,False
r8upnl,"I guess in your example I would consider what was on the cards to be the ""programming language"" or ""input.""

Just a guess. Someone else probably has a better answer.",hn87t8h,t3_r8upnl,1638645204.0,False
r8upnl,"Each computer has with a specific machine language associated with it. IE if you feed the cpu's input lines with a specific train of 0s and 1s (low and high voltage levels, in the language of electronics) it will do a cpecific thing (this is an oversimplification obviously), early computers were programmed this exact way.",hn8iftf,t3_r8upnl,1638649727.0,False
r8upnl,[Crash course computer science](https://youtube.com/playlist?list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye),hn9o9zx,t3_r8upnl,1638667837.0,False
r8rl8f,"Computer graphics is its own subfield of computer science. Specific course names may vary wildly from school to school. You'd really need to check the course catalogue for any possible course of interest.

Some examples may include:  
Introduction to Computer Graphics  
Computer Graphics  
Applied Computer Graphics  
Application of Computer Graphics

etc.",hn79xnc,t3_r8rl8f,1638630844.0,False
r8rl8f,Thanks!,hn7h5nj,t1_hn79xnc,1638634189.0,True
r8rl8f,My pleasure. Good luck with your studies!,hn7hpvb,t1_hn7h5nj,1638634440.0,False
r8rl8f,"Here's an overview of what topics would be covered in such a course: http://www.cs.cornell.edu/courses/cs4620/2017sp/cs4621/

OpenGL resource:
https://learnopengl.com/

WebGL resource: https://webglfundamentals.org/

Cool youtubers to check out:

https://www.youtube.com/c/SebastianLague/videos

https://www.youtube.com/c/IndigoCode/videos",hnad28v,t1_hn7h5nj,1638679567.0,False
r8rl8f,"Interactive media, creative computing, etc.",hnao6sr,t1_hn79xnc,1638686110.0,False
r8rl8f,"if you are interested in lighting and how things look, then look for courses focused on: rendering, light transport or material appearance modelling.

if you want to get into computer graphics then it's good to try everything under the umbrella of graphics. the course I have taken and plan on taking are: Computer graphics, rendering, image analysis, digital signal processing, computer vision, geometry processing.
All of these areas overlap in different ways and things learned in one area can be used in another area. there are probably more that I haven't mentioned.",hn7fsfx,t3_r8rl8f,1638633572.0,False
r8rl8f,There’s linear algebra involved as well depending how deep you get into computer graphics.,hn838uk,t3_r8rl8f,1638643326.0,False
r8rl8f,"Yeah, if you're at all interested in university level computer graphics, linear algebra is more or less mandatory.",hn8a3g1,t1_hn838uk,1638646160.0,False
r8rl8f,Does the category of computer graphics also include the digital signal processing for the video or is that of another CS category?,hn89vew,t3_r8rl8f,1638646066.0,False
r8rl8f,"DSP is generally an EE field, unless you have a specific example in mind?",hn8lthy,t1_hn89vew,1638651174.0,False
r8rl8f,"At my undergrad school, we had a CS class in DSP and it did fall under the Computer Graphics umbrella (as Image Processing).",hn9o4fh,t1_hn8lthy,1638667767.0,False
r8rl8f,"It depends on what you want to do. Images can be thought of as signals of red, green and blue channels and videos are just sequences of images. Performing analysis in on these signals (known as the frequency domain) allows us to get some useful information easier than just looking at the image itself. This is used in computer graphics for things like object tracking and also used in AI as part of the pipeline for things like object recognition.",hn95mpf,t1_hn89vew,1638659703.0,False
r8rl8f,We have it as a separate subject in our course so it's probably a field of its own.,hn7ugpj,t3_r8rl8f,1638639798.0,False
r8rl8f,"If you're interested in 3D game engines, I will tell you to also become really familiar with (in addition to computer graphics, linear algebra, etc.) computer systems (especially microarchitectures, multithreading, etc.) Newer 3D APIs (Direct3D 12, Vulkan) require a lot more knowledge in these areas than older APIs.

Also, if you can, skip OpenGL. Outside of mobile games and 3D modeling, it's a boomer relic (and unfortunately that's all that schools teach you). If you can, learn Direct3D 12.",hn9ovua,t3_r8rl8f,1638668112.0,False
r8rl8f,"> (and unfortunately that's all that schools teach you)

&nbsp;

Used WebGL in our Computer Graphics class lol",hnacioc,t1_hn9ovua,1638679286.0,False
r8rl8f,Yeah it was just called Computer Graphics for the subject I did and we used OpenGL to create 3D scenes. But there's a whole library of books for this one subject.,hna8bnf,t3_r8rl8f,1638677181.0,False
r8rl8f,Learn cpp. Graphics are specifically about vectors and points on the screen. Reflections and shadows are the pixel colors used. If you can program in cpp you be great it. Start learning everything about openGL.,hna8wsj,t3_r8rl8f,1638677479.0,False
r8rl8f,"Might be an Unpopular opinion, but Dont studdy Computer Science If making games and graphics is your only Motivation. You can take sub courses that specialise in that, but don't go into the world because you like games. 

I have personally experienced this over and over, first year first semester we are packed with students who want to be game developers, by the end of the first semester the attrition rate is like 70%, and it only ends in tears.

But then after you understand how computers work then it gets cool.",hnauyjd,t3_r8rl8f,1638690863.0,False
r8rl8f,"It is its own category. (Usually)
It's probably the most related to math.
My uni has a class for opengl under the cisc program. It's called ""computer graphics"" (so catchy)
I'd start out double checking you understand geo and calc fairly well, then see if MIT or someone has free courses online. 
Reflection and path tracing and stuff requires a lot of math and a decent bit of experience with matrix math and a good understanding of data structure type stuff from what it seems.",hnbuspp,t3_r8rl8f,1638716359.0,False
r8rl8f,Yes,hnf247u,t3_r8rl8f,1638762987.0,False
r8rl8f,https://pathfinder.fyi/results/Computer%20Graphics,hovfaud,t3_r8rl8f,1639712810.0,False
r8mrol,"I have resorted to using pointers within certain functions to help increase the readability of the code and also to help reduce obtuse indexing. But often times it simply comes down to the data structure that's being accessed and the most reasonable way to do that. A good example would be a linked list. By their very nature (especially in C/C++) they use pointers. Traversing a linked list naturally requires the use of pointer. Yes, if you organize the data well, you **could** access the list via an index, but it's probably faster and easier to use pointers.",hn6q0t5,t3_r8mrol,1638619054.0,False
r8mrol,I have used a pointer wen parsing [an obj file](https://en.wikipedia.org/wiki/Wavefront_.obj_file) to choose wich of my std::vetors to put data,hn7vqen,t3_r8mrol,1638640317.0,False
r8mrol,How did you use pointers to do that? could you explain in a little more detail?,hn9whzr,t1_hn7vqen,1638671607.0,True
r8mrol,"The file has different kinds of vectors (position, normal, texture) that all have to be paresd the same but indexed separatly.

Basicly I store a pointer to where it is supposed to go parse it and put it where the pointer points",hnqf10z,t1_hn9whzr,1638980866.0,False
r8mrol,"Yes they are. Consider a char array (mutable string) that you want to iterate over, inspect chars from and possibly modify all in one scope. You could use a pointer to do that.",hn6s3k9,t3_r8mrol,1638620599.0,False
r8mrol,">The main purpose of pointers is to stop the wastage of memory by copying of the values stored in a variable to another variable

No, not really. The main purpose of pointer is to point at something.

When pointer is used only for saving time/space, you can rewrite the code to be pointer-less and it will still work.  
Try it with dynamic data structures, e.g. Linked List.

Or when on lower-level, you need to write something to specific place in memory (e.g. writing `1` at address `53280` on C64 will change the screen frame color to white). And what type is for storing memory addresses? Pointer.",hn6pvs8,t3_r8mrol,1638618944.0,False
r8mrol,"I can right now think of 2 reasons

\- Polymorphism.  
\- Big bunch of memory you don't want to store on heap.",hn7skit,t3_r8mrol,1638639006.0,False
r8mrol,[deleted],hn6kt6r,t3_r8mrol,1638614706.0,False
r8mrol,"Actually, I asked whether pointers have any use in the same scope and not between function calls.",hn6ombx,t1_hn6kt6r,1638617928.0,True
r8f02f,"It's **not** a ROM chip. It's some sort of flash memory, NVRAM, or EEPROM chip that can be updated with newer code.",hn5ahyi,t3_r8f02f,1638584605.0,False
r8f02f,Oh thank you. Could you explain what NVRAM and EEPROM are?,hn5bisu,t1_hn5ahyi,1638585114.0,True
r8f02f,"https://en.m.wikipedia.org/wiki/Non-volatile_random-access_memory

https://en.m.wikipedia.org/wiki/EEPROM",hn5gxll,t1_hn5bisu,1638587783.0,False
r8f02f,Thank you I now understand.,hn5ozn0,t1_hn5gxll,1638591868.0,True
r8f02f,"In my experience, EEPROM's are commonly used to contain system specific data like serial number, encryption keys, MAC address, etc... while NVRAM is used for more general stuff like system software images.",hn6svxr,t1_hn5ozn0,1638621171.0,False
r8f02f,Do you know why?,hn7f0os,t1_hn6svxr,1638633221.0,True
r8f02f,"ROM is typically used for permanent data such as serial numbers, calibration tables for the specific device, or programs that are not changeable such as a hardware specific boot sequence.  
NVRAM is typically used for performance or simplicity of design where the content needs to be changed rapidly.  Dynamic RAM (ie VRAM) often must be refreshed for the content to be retained.  Flash memory can be rapidly read and slowly written. Many flash memory devices also have limits on the number of times they can be written. Generally EEPROM has a much higher limit on the number of writes so they are often used to store data that needs to be preserved over a system restart ( power cycle ) such as a position of a gate or valve overnight.  So the types of memory for a computer system is selected based on targeted application function:  
     Dynamic RAM — Typically large memory so refresh cost is minimized; low cost; good to excellent performance; 
                                  ok to lose content I f reset/power loss.
     NVRAM — higher cost, simple design, Excellent performance.  Typically content preserved over reset/power
                        cycle.  Typically smaller memory amount.
     EEPROM — Fast read, slower write, higher write cycles vs FLASH; Content preserved over reset/power.  Typically 
                         Small memory amount.
     FLASH  — Fast read, very slow write, limited write cycles; Content preserved over reset/power cycle. Larger 
                       memory amount.
     ROM — Fast read, no write; Content preserved over reset/power cycle. Typically small memory amount.",hn88c1a,t1_hn7f0os,1638645424.0,False
r8f02f,Reading this response felt like watching Neo when he got Karate downloaded to his brain,hnc6xm8,t1_hn5ozn0,1638721687.0,False
r8f02f,Is that a good thing?,hncja7l,t1_hnc6xm8,1638726793.0,True
r8f02f,[removed],hn5tyxd,t3_r8f02f,1638594560.0,False
r8f02f,what kind of chip does it have? there are bios/eeprom chip programmers you can buy like ezp2019 and ch341a.,hn60l3p,t1_hn5tyxd,1638598531.0,False
r8f02f,I don't know how.,hn5ua1s,t1_hn5tyxd,1638594733.0,True
r8f02f,I have a WiFi smart camera which got bricked during firmware update. It's an Imou Ranger - 2 camera,hn5uxvu,t1_hn5ua1s,1638595111.0,False
r8f02f,What is a smart camera? I'm unfamiliar with this device.,hn5xr9m,t1_hn5uxvu,1638596736.0,True
r8f02f,r/techsupport,hn6qa0r,t1_hn5tyxd,1638619253.0,False
r81i62,Minecraft redstone tutorials. I'm not even kidding.,hn3e8lp,t3_r81i62,1638554238.0,False
r81i62,OMG! Hell Yeah. Thank you for this,hn4rk75,t1_hn3e8lp,1638575460.0,True
r81i62,"Tbh, it was quite fun to implement a T-flipflop after learning the theory behind them.",hn3nk8t,t1_hn3e8lp,1638557951.0,False
r81i62,"Free game where you solve increasingly difficult logic gate puzzles. Maybe not the best use of your time considering you only have a few days, but I think it would give you some nice intuition.

https://apps.apple.com/us/app/make-it-true-solve-circuits/id1536287319",hn2xhsl,t3_r81i62,1638547673.0,False
r81i62,"oh thank you bro, this so much fun",hn4umxu,t1_hn2xhsl,1638576911.0,True
r81i62,it’s tricky to solve with the minimum number of clicks,hn69tq2,t1_hn4umxu,1638605306.0,False
r81i62,That is the very first thing I learned. Are you sure you want to write that exam this semester?,hn4r728,t3_r81i62,1638575289.0,False
r81i62,"Just for context I'm not in college, it's my high school exams and Boolean Algebra & Logic Gates consists more than half of the paper. There are other stuff like arrays, strings etc etc that I already know",hn4rgx9,t1_hn4r728,1638575419.0,True
r81i62,Ah ok. Sorry then,hn4w0i7,t1_hn4rgx9,1638577563.0,False
r81i62,neso academy on youtube!,hn6a5d8,t3_r81i62,1638605569.0,False
r80r4b,"Aside from a mathematics textbook focussing on lambda calculus, I think your best bet would actually be getting some hands on experience with functional programming languages. It'll probably make understanding lambda calculus a lot easier in the long run",hn32n3i,t3_r80r4b,1638549722.0,False
r80r4b,"I second this. I took a course at uni that involved lambda calculus. They taught lambda calculus first and it was super difficult to understand. Then, they taught us a functional programming language and it all made sense. Suddenly after learning functional programming I didn't even need to study lambda calculus, since using functional programming languages is basically using lambda calculus rules.",hn5gym0,t1_hn32n3i,1638587797.0,False
r80r4b,"Barendregts ""The Lambda Calculus, its Syntax and Semantics"" is an absolute classic. (But very math-oriented)",hn3n9mk,t3_r80r4b,1638557831.0,False
r80r4b,"u/Exourion made a good suggestion in my opinion. 

You can start learning Haskell, which is a pure functional programming language that uses Lambda Calculus. 

I am using the book ""Haskell Programming from First Principles"" by  Christoph Allen and Julie Moronuki and the very first chapter is about Lambda Calculus. 

Hope that helps.",hn4uiq6,t3_r80r4b,1638576856.0,False
r80r4b,[Alligator Eggs](http://worrydream.com/AlligatorEggs/) is fun entry point to ideas,hn5f0kj,t3_r80r4b,1638586839.0,False
r80r4b,SICP,hn3fsmh,t3_r80r4b,1638554849.0,False
r80r4b,"??? Wikipedia does a better introduction to the lambda calculus. 

Unless SICP has a new chapter 6 I’m not familiar with, I’d vote no on this one. Though “The Little Schemer” does a nice job of sneaking in the Y combinator at the end, and makes you think you discovered it yourself.",hn4nvqi,t1_hn3fsmh,1638573760.0,False
r80r4b,"I think it’s a good intro to func prog more than anything, but yeah you’re right not the best for lambda calc",hn5k8wa,t1_hn4nvqi,1638589442.0,False
r80r4b,https://www.amazon.com/Introduction-Functional-Programming-Calculus-Mathematics/dp/0486478831/ref=sr\_1\_1?crid=281AXHPBSTE5L&keywords=lambda+calculus&qid=1638553357&sprefix=lambda+calcu%2Caps%2C184&sr=8-1,hn3c2do,t3_r80r4b,1638553392.0,False
r80r4b,"This looks quite cool.  I wouldn't want to learn functional programming instead of the lambda calculus, but I also wouldn't want to learn the lambda calculus without seeing it in a functional language.  Nice pick.",hn7ogzd,t1_hn3c2do,1638637303.0,False
r80r4b,"I'd take a look at chapter 2 of

https://www.microsoft.com/en-us/research/uploads/prod/1987/01/slpj-book-1987-full.pdf",hn6kgv8,t3_r80r4b,1638614415.0,False
r7x6t1,"They are both very broad fields, and so there will certainly be some overlap.

At the highest level, AI is about creating algorithms that can solve problems through reasoning rather than explicit instructions as used in traditional software. AI has many subfields that can include making algorithms that think in human-like ways, to machine learning where solutions are discovered.

Computational science is about solving complex (scientific) problems using computational methods, and so recently has used AI quite a bit; however, it is not strictly focused on AI (parallel computing, hardware, etc. are also used). Generally, it focuses on maximizing the quality of models and simulations.

They both contribute to scientific research equally so neither would be closer or further from your goal. You would need to decide on a subfield to really answer that question.",hn2803f,t3_r7x6t1,1638535794.0,False
r7x6t1,"My PhD concentration is in computational science. I’ll speak to that and let others focus on AI.

* Computational science has its origins modeling physics and engineering problems. Think PDEs.
* The field has naturally evolved to include randomized algorithms, numerical multilinear algebra, streaming problems, among others. 
* The application side has shifted from high-energy physics modeling to data science and ML (like so many areas). 
* Your most essential tools come from a numerical analysis class and maybe optimization. 
* Everyday questions are “do I get the same approximation error with 10 eigenvectors as I do 1000?” or “Should I pay the price for a Newton method or just do fixed point iteration and wait?”",hn30j8r,t3_r7x6t1,1638548891.0,False
r7nt4h,"I read ""The Introduction To Algorithms"" and found they explained it pretty well. While formal, they apply it to many Algorithms to give a good intuition. You'll quickly find that algorithms with smaller asymptotic growth (like O(log(n)) as opposed to O(n)) ""do less work"" so to speak; and from there,I had to read a million examples which it provides as well.

About what you said in your rant; you certainly could compare run times of Algorithms directly.  But doing so introduces many factors not really part of the algorithm itself. Like for example the speed of the computer running the algorithm.
With asymptotic growth, we're really interested in the algorithm on an abstract level, away from all implementation.

This is also part of why we say things like ""O(3n+log(n) + 2n^2) = O(n^2)"". All the constant disappear because they don't really matter for our purpose; if the algorithm runs in O(2n^2), one can insert a processor of double the speed to combat it. But one cannot ""fight"" against asymptotic growth; no matter the speed of your computer, O(n^2) becomes arbitrarily large and at a speed where any smallere term like O(7n) becomes completely negligible if we increase n far enough. And we like our n to be big; the amount of data is ever increasing!

Went on a bit of rant here too :) But I recommend Introduction.to Algorithms; one can easily find a PDF online",hn1ael8,t3_r7nt4h,1638510475.0,False
r7nt4h,Thanks you so much! I'm always blown away by how helpful this subreddit is :),hn2vw5r,t1_hn1ael8,1638547026.0,True
r7nt4h,"You probably got answers to some of your questions in the video already. Feel free to ask further if not.

> Like if you already have a function that perfectly describes your algorithms time complexity, what's the point in simplifying it in a weird way

One of the problems is that you *don't* necessarily know the exact function, at least not with precision that would make those exact details meaningful. What would you base the exact complexity function on? Which programming language is it written in? The same algorithm that works based on the exact same idea will have its exact details, and the exact number of steps, look different in different programming languages. Or, more importantly in terms of performance, if it's compiled into machine code, which processor is that? If it's interpreted, which interpreter? The machine code is going to be different for an ARM processor than for an x86-64 one, and the number of machine instructions is going to be different. Some CPU instruction sets might provide individual instructions that do more work in a single instruction but could take longer to execute. Moreover, different instructions take a different number of cycles on different CPU models even if the instruction set is the same.

Asymptotic complexity doesn't change from a language to another, or from a CPU instruction set or a physical processor to another. It's a mathematical property of the algorithm itself. A quadratic function is always going to be quadratic regardless of the constant multipliers; a linear one is going to be linear; an exponential one is going to be exponential. That way you can talk about the algorithm itself more generally rather than its implementation in a particular language, or on a particular CPU.

Including every constant and every term when talking about an algorithm more generally would often mean giving more detail than you perhaps have grounds for. It would be a bit like reporting seven digits in your calculated result about a physical phenomenon when the original measurements are only really accurate enough to warrant two. It kind of looks more accurate but most of it is just noise.

You could argue for keeping the lower-degree terms even if the constants were discarded, of course. It might sometimes make sense to think of an algorithm's time complexity as being in the order of N^2 + N if that happens to be true rather than just N^2. And that wouldn't (probably, at least not always) even depend on the implementation, and could be part of the behaviour of the algorithm itself.

The highest-degree term starts to dominate as N grows indefinitely, which is what asymptotic analysis deals with, so the lower-degree terms are skipped. But you could reasonably argue that the lower-degree terms are part of the algorithm's behaviour, too.

> that breaks algebra rules?

This is a bit of an aside, but I don't think it really actually breaks algebra rules. The common way the notation is used perhaps does.

What for example O( n^2 ) actually denotes is the set of functions that are bounded above by n^2 * c for some constant c. The pedantically correct way would be to say that an algorithm's worst case complexity is *in* O( n^2 ), for example, rather than that it equals O( n^2 ). That's one of the things that people very often write in a way that's rigorously speaking not quite right. Once you figure it out, or if you just don't think about it that far, the sleigh of hand generally doesn't hurt understandability.",hn22x1t,t3_r7nt4h,1638532419.0,False
r7nt4h,">The pedantically correct way would be to say that an algorithm's worst case complexity is in O( n2 ), for example, rather than that it equals O( n2 ).

This helped me so much! Thank you! This subreddit is so helpful that sometimes I feel like I should be paying to ask these questions. Once again, thank you!",hn2wxil,t1_hn22x1t,1638547447.0,True
r7nt4h,"No worries. I can see how it could look confusing and dissatisfying if you look at things with a more mathematical eye and something doesn't seem to fit. The theory, or at least the parts that are part of the canon, do actually fit, though. Things just often get bent a bit and rigour gets lost in more everyday use.",hn38qbd,t1_hn2wxil,1638552101.0,False
r7gaav,Great channel! Love their sorting algorithms competitions.,hn0898h,t3_r7gaav,1638492421.0,False
r7gaav,I was able to follow right up till MIP\*… great video!,hn1jdoi,t3_r7gaav,1638516551.0,False
r7fkp9,"There is no way to do this. You can never, ever verify that what is happening on an uncontrolled device is what you think it is. The only thing you can verify is that the input you are being given 'makes sense' based on your expectation of what is allowed - this is the problem things like Valve Anti-Cheat are designed to tackle.",hmzq7o8,t3_r7fkp9,1638484556.0,False
r7fkp9,My first question would be what's stopping someone from realizing this and sending over a fake hash they gained from a legitimate source?,hmz3146,t3_r7fkp9,1638475677.0,False
r7fkp9,That's what I'm asking,hmz332r,t1_hmz3146,1638475699.0,True
r7fkp9,"I suppose what I'm getting at is that you should never trust the client.

Verify all input server side.",hmz3d58,t1_hmz332r,1638475808.0,False
r7fkp9,Yeah but how,hmz4itk,t1_hmz3d58,1638476257.0,True
r7fkp9,"Depends on what you're doing. 

Realistically, a client can send you good data, bad data, invalid data, malicious data, malformed data. Data can get corrupted between the client and the server, or can be intercepted and modified.

The servers job then is to take any data and verify that the data is logical and within valid parameters. If I'm entering my name, I can put Joe or 2847 or Joe792!?5Schmo, or even nothing. At some point you have to define what a valid name is.

Think about programming Chess. The client can send the move Black Queen to E5. Sounds reasonable? Well, maybe the queen isn't even on the board anymore, or would be an illegal move, or whatever.",hmz5oy5,t1_hmz4itk,1638476704.0,False
r7fkp9,"You're best bet would be to implement OAuth and use a server to run proprietary functions. A user must then send an untampered token, which is given from and verified using a secret on, the server to authorize them on different resources.",hn1gg83,t3_r7fkp9,1638514399.0,False
r7fkp9,"To add to what everyone else said, are you trying to authenticate a user? Use a login system. Are you trying to validate that a piece of software has been paid for? Actually, use a login system. 

I've actually written a licensing library for windows applications - I also wrote the licensing server. No matter how complicated I made it, there's always going to be some person that will go that extra step of decompiling that library or application and bypassing it. You know what can't be bypassed? A system where the user has to login to a server. You still have to follow best practices and security protocols on your server, though. This is why you hear about companies being hacked and sensitive information being stolen",hn1120i,t3_r7fkp9,1638505363.0,False
r7fkp9,"Have a look at remote attestation, it's probably the closest thing to what you want to do.",hn1c6lg,t3_r7fkp9,1638511570.0,False
r71nbu,"The vt-x extension means VMs can have their CPU instructions executed by the host CPU directly (if everything is configured right). But it doesn’t address anything else, just the CPU. Both VMware and virtual box can take advantage of vt-x.",hmyooiz,t3_r71nbu,1638470104.0,False
r71nbu,Good to know! Thanks.,hmys32a,t1_hmyooiz,1638471415.0,True
r70ia7,"Your explanation in words is incorrect. It would be correct if the relation was T(N) = 5N + N-1, but the recurrence element T(N-1) needs to be factored in. Another way to think of it that might make more intuitive sense: 

The T(N-1) component of the recurrence relation means that for all values of n, from the first index to the Nth, this function will perform 5n work. If you expand it out, the total amount of work will be 5N + 5(N-1) + 5(N-2) + 5(N-3) ... until you get down to n=1 or the first element. 

Each of these individual terms (ex. 5N) is in O(N), and there are a total of N different terms being added together, which is where the multiplication comes in. Hope this helped!",hmwp31c,t3_r70ia7,1638431521.0,False
r70ia7,"Here's another way to work it out.

First, an important summation to know is:

1 + 2 + 3 + ... + N = N(N+1)/2

Start substituting the formula recursively, and notice the pattern.

T(N) = 5N + T(N-1)

T(N-1) = 5(N-1) + T(N-2)

T(N-2) = 5(N-2) + T(N-3)

...

T(1) = 5(1) + T(0)

T(0) = 0

Combine them all:

T(N) = 5N + 5(N-1) + 5(N-2) + ... + 5(1)

= 5(N + (N-1) + (N-2) + ... + 1)

= 5( N(N+1)/2 )

= (5/2) (N\^2 + N)

= (5/2) N\^2 + (5/2) N

= O(N\^2)",hmxmhim,t3_r70ia7,1638454811.0,False
r70ia7,"From StackOverflow:

  
T(n) = T(n-1) + n  
T(n-1) = T(n-2) + n-1  
T(n-2) = T(n-3) + n-2  
and so on you can substitute the value of T(n-1) and T(n-2) in T(n) to get a general idea of the pattern.  
T(n) = T(n-2) + n-1 + n  
T(n) = T(n-3) + n-2 + n-1 + n  
.  
.  
.  
T(n) = T(n-k) + kn - k(k-1)/2    ...(1)  
For base case:  
n - k = 1 so we can get T(1)  
=> k = n - 1  
substitute in (1)  
  T(n) = T(1) + (n-1)n - (n-1)(n-2)/2  
Which you can see is of Order n2 => O(n2).

Source: https://stackoverflow.com/questions/13674719/easy-solve-tn-tn-1n-by-iteration-method",hmwhtax,t3_r70ia7,1638426278.0,False
r6vpdh,"OP, please seek help. Plenty of people in this thread have sent you resources and people to call to try and improve your situation mentally, and the automated message I sent you contains even more resources. I understand that the world fucking sucks, especially for us queer folk, but ending your life solves nothing in the end.

The problem may go away, but so does all the good stuff. Allow what good exists in your life to carry you forward into tomorrow. I don't know you personally, but almost no one has 0 reason to live. Even at your darkest times, please try and find something, anything, to encourage you to keep living.

Do feel free to reach out in DMs, I'm usually able to respond quickly.",hmzcci9,t3_r6vpdh,1638479194.0,False
r6vpdh,The solution you looking for is a lawyer and a letter in an envelope,hmvmvzp,t3_r6vpdh,1638410769.0,False
r6vpdh,But that costs money :(,hmvnf34,t1_hmvmvzp,1638410998.0,True
r6vpdh,Like everything in life. Storing some data also costs money. There is no free lunch.,hmvnqcj,t1_hmvnf34,1638411133.0,False
r6vpdh,:(,hmvntam,t1_hmvnqcj,1638411169.0,True
r6vpdh,"My dad passed last year due to covid, and the best thing that helped us is that he had his password written up onto a paper and stored it in his desk. And he used the same password for everything, and if not, we could access his emails and reset the passwords. Facebook as it turned out has a feature to take over a profile and create a memorial for it, there are multiple ways like sending the certificate of death to them.",hmvofn2,t1_hmvntam,1638411438.0,False
r6vpdh,[removed],hmw0kbz,t1_hmvmvzp,1638416791.0,False
r6vpdh,"a) sounds like ur life isn't going so hot sorry to hear that, lmk if you need to talk or anything   
b) one way to do it is to do it kinda an old fashioned, this is how people did in the middle ages/classical period, give people trust an encryption key or algorithm, but not the password itself. Next find an obituary API one with data from your area, and spin up a web server that pings the API once per day, if your name shows up, send (via email or snail mail) the encrypted password to people you trust.",hmw9kr7,t3_r6vpdh,1638421386.0,False
r6vpdh,"Kk thx, wish anyone could help but there's nothing I can do but suffer 🙃",hmwavy8,t1_hmw9kr7,1638422110.0,True
r6vpdh,"Hey OP, I was looking through your comments and I know you're feeling suicidal, but I would urge you to try to get help before you do anything you will regret. According to your post history, you're transgender, so [The Trevor Project](http://thetrevorproject.org) might be helpful to you. However, there are other options - if you are in the UK, you can access [Childline](http://childline.org.uk), or [Samaritans](http://samaritans.org). If you are outside the UK, [here is a list](https://www.thecalmzone.net/2019/10/international-mental-health-charities/) of international mental health charities. Over here on Reddit, we have r/suicidewatch, and if you want to talk to someone else, feel free to send me a message. 

Good luck OP, and remember, there's always someone there who is ready to support you.",hmwkhqo,t3_r6vpdh,1638428090.0,False
r6vpdh,"Thx but there's nothing I can do that I'm not doing, I just have to sit here and suffer 🙃",hmwkt43,t1_hmwkhqo,1638428317.0,True
r6vpdh,"I know you think that, but I don't believe that. I felt the same way - but actually using these resources was really helpful. Have you spoken to an anonymous councillor before?",hmwkyn2,t1_hmwkt43,1638428427.0,False
r6vpdh,"Yeah but all they do is ask how I'm trying to kill myself until they just say bye, didn't really get a solution",hmwl1kc,t1_hmwkyn2,1638428484.0,True
r6vpdh,"Yup, that sounds familiar. If you haven't, I'd urge you to try again - I find that it's a sort of lucky dip. Alternatively, is there anyone you can speak to at school/home? Or even a friend?",hmwl5g6,t1_hmwl1kc,1638428562.0,False
r6vpdh,Schools would put me in grippy sock jail and I don't wanna annoy my family (bc they might do the same thing) and I have no friends 🙃,hmwn4r2,t1_hmwl5g6,1638430031.0,True
r6vpdh,"I'm sorry to hear that. Well, if you want to talk, feel free to just PM me.",hmwnd41,t1_hmwn4r2,1638430210.0,False
r6vpdh,"Please stay with us friend. You have friends here. We care. You mention school so perhaps you are still pretty young too, in which case definitely hang on to life. Things get soooo much better once you get out of school. Truly it does, I promise. If you need to talk please dm me.",hmx5ct3,t1_hmwn4r2,1638444988.0,False
r6vpdh,"Interesting project called ""horcrux"" that splits a file into encrypted fragments and only decrypts if you have all the fragment. 

https://github.com/jesseduffield/horcrux

But yes echoing others I do hope you are ok. I'm older now and a lot of stuff can seem incredibly tough when you're going through it so hope you can work through things.",hmx56oc,t3_r6vpdh,1638444863.0,False
r6vpdh,"That's really cool, thx!",hmytj6a,t1_hmx56oc,1638471972.0,True
r6vpdh,Is everything OK for you OP?,hmwdsae,t3_r6vpdh,1638423763.0,False
r6vpdh,No 😎,hmwdtoz,t1_hmwdsae,1638423785.0,True
r6vpdh,Please don't do what it sounds like you're planning. Is there somebody you can talk to? Some way I can help you?,hmwe316,t1_hmwdtoz,1638423937.0,False
r6vpdh,No and no 😎 all I can do is suffer,hmwe9ly,t1_hmwe316,1638424046.0,True
r6vpdh,"Ah,  yes, downvoting this comment will make OP feel better",hmy1ubu,t1_hmwe9ly,1638461313.0,False
r6vpdh,[removed],hmvsmqh,t3_r6vpdh,1638413248.0,False
r6vpdh,"Damn ur smart, thx",hmvy4vq,t1_hmvsmqh,1638415661.0,True
r6vpdh,Don't actually do that. Other people with your name will die before you and send your password to everyone.,hmwfm5g,t1_hmvy4vq,1638424865.0,False
r6vpdh,True :/,hmwfzl6,t1_hmwfm5g,1638425100.0,True
r6vpdh,"Change your name to something unique before dying, obviously.",hmymrgo,t1_hmwfm5g,1638469372.0,False
r6vpdh,[removed],hmwemu4,t3_r6vpdh,1638424262.0,False
r6vpdh,[removed],hmwfjei,t1_hmwemu4,1638424819.0,False
r6vpdh,[removed],hmwgig4,t1_hmwfjei,1638425429.0,False
r6vpdh,[removed],hmwhhzs,t1_hmwgig4,1638426069.0,False
r6vpdh,"This is morbid af but I will try to answer. OP hope you get better. Been there myself and thought about this a lot.

Encrypted 7Zip file with a password on it - AES or better. Depending where you are you can get a will kit very cheap (some post offices have them) you can get them endorsed by a JP (I think) and then it is binding. On your death or when you are deemed unable to look after yourself a guardian will take over your trust and the will can be released to them.

Safe travels bud and if you are somewhere toxic get the fuck out of there at all costs.",hmx8o1q,t3_r6vpdh,1638447318.0,False
r6vpdh,Your code is that important it needs to be given to ur family but u dont want to spend a single dollar. Ok,hmw1tgq,t3_r6vpdh,1638417385.0,False
r6vpdh,"Yeah I gotta spend that money on other stuff, there's a reason I'm close to death :/",hmw2c46,t1_hmw1tgq,1638417635.0,True
r6vpdh,hey i hope you are okay and if preventable i hope you find a solution. message me if you need to talk to someone.,hmwf289,t1_hmw2c46,1638424522.0,False
r6vpdh,"Thx, there's a solution but there's also a lot of stigma and bigotry, only thing to see is if I break before my life is worth living 😎",hmwff95,t1_hmwf289,1638424747.0,True
r6vpdh,well i hope you solve the solution soon. message me if you ever need someone to talk to,hmwfjo8,t1_hmwff95,1638424823.0,False
r6vpdh,"If this is the case, just give access to someone you trust now.",hmw42xn,t1_hmw2c46,1638418499.0,False
r6vpdh,I don't trust anyone lol,hmw4mvm,t1_hmw42xn,1638418781.0,True
r6vpdh,You can trust me bro,hmw8i2d,t1_hmw4mvm,1638420796.0,False
r6vpdh,If you trust google they have a function to send mails after a time period not being logged in.,hmwu74u,t3_r6vpdh,1638435739.0,False
r6vpdh,"Well first you need to establish what the parameters are, how long will you need to go without resetting it? 

But a very basic way to do this would be to put the instructions for accessing your data in an email that you schedule (can do this with gmail) to be sent at a certain time in the future. 

Then you need to come back before that date, and reset it (i.e. delete the original schedule, and make a new one at a later date). If you don't come back before that date (i.e. you died), then the email will be sent.",hmww0b6,t3_r6vpdh,1638437289.0,False
r6vpdh,Really need quorum keys for this.,hmz0okt,t3_r6vpdh,1638474733.0,False
r6vpdh,Wtf is that?,hmz2dfd,t1_hmz0okt,1638475417.0,True
r6vpdh,You see why it's needed.,hmz545d,t1_hmz2dfd,1638476483.0,False
r6qahx,"Having a central processing entity you were talking about (I.e. the proverbial snakes head) for the invading cyborgs is not a stretch when you are already talking about manure level cyborgs.

Also consider strides in quantum computing which means larger numbers being processed many times faster than today’s binary computers.

Or consider organic computers, operating chemically. (Proteins are essentially single purpose chemical computers that occur naturally)

Source: Have degree in film and one in CS",hmv930p,t3_r6qahx,1638404761.0,False
r6qahx,"You need to give more details of the process, because as of now, I can even think about believable bulls**t which would explain a slightly more powerful smartphone to be capable of it.",hmv08wi,t3_r6qahx,1638400825.0,False
r6f0od,"B

Always practice while you learn",hmspnuv,t3_r6f0od,1638367935.0,False
r6f0od,B for sure,hmsvd34,t3_r6f0od,1638370540.0,False
r6f0od,"Agree with everyone who said B. In case you haven't seen it and want some resources to learn OS development, this repo is pretty damn cool.

https://github.com/danistefanovic/build-your-own-x#build-your-own-operating-system",hmtej9c,t3_r6f0od,1638378386.0,False
r6f0od,B is the right choice,hmsyc99,t3_r6f0od,1638371831.0,False
r6f0od,Option B. You might not understand a few things in OS without knowing C.,hmt6vp2,t3_r6f0od,1638375355.0,False
r6f0od,"You should read about it (you will have to read a ton actually) and practice at the same time. Osdev Wiki should be a good starting point. Also the Tanenbaum Book on operating systems has some exercises you can do while reading iirc.

Writing your own OS is a pretty big project. I did a university course where we wrote our own OS and had a really solid foundation given to us and it was still pretty complicated.",hmu655u,t3_r6f0od,1638389063.0,False
r6f0od,In my OS class we use Operative Systems in three easy peaces and I really recommend it. Such a joyfull experience.,hmveo11,t3_r6f0od,1638407209.0,False
r6f0od,"B. It really helps you understand how these things work, also you gain practical experience in low-level programming, which you may use in future.",hmu302l,t3_r6f0od,1638387856.0,False
r6f0od,"If all your after is round robin and multiprocessing, i wouldnt read the whole book. 

Id be flexible and keep your hands dirty, also have a look into Minix 3.  Minix was designed to be an operating system for learning. I think at one point the author was reluctant to expand as it got too big to learn, but its now expanded Minix3 to be a fully fledged OS while maintaining the goal of being an OS to learn on.

[http://www.minix3.org/](http://www.minix3.org/)

&#x200B;

[https://wiki.minix3.org/doku.php?id=www:documentation:start](https://wiki.minix3.org/doku.php?id=www:documentation:start)",hmuzvr0,t3_r6f0od,1638400657.0,False
r6f0od,"I did option C. I had a summer internship where I worked on bringing up a system with an RTOS, muddled my way through it (plus help from others in the lab), then in the fall I took an Operating Systems class at school and all the pieces fell nicely into place.",hmv1wjl,t3_r6f0od,1638401602.0,False
r6f0od,"I am also trying to write an OS. And option B is the best way to proceed. You understand the concept, implement it, if it doesn't work look where u messed up, and continue

https://wiki.osdev.org/",hmwgr99,t3_r6f0od,1638425588.0,False
r6f0od,"Everyone is saying B, but in reality if you are going through a good book which contains plenty of exercises then A is certainly a good option.

What you don't want to do is just read a book without doing any sort of practical work in the meantime",hnut7yc,t3_r6f0od,1639060594.0,False
r6eo9a,"My research program is largely related to those subjects. No particular language is necessary. Python is generally useful in research because of the large number of libraries available. I personally write code in Python or Java if I'm working with a student (because often it is all they know) or C++ when working by myself (largely because over the years I've developed a large AI/ML library in C++). R and Matlab (or similar products) is also quite useful for doing analysis or data preprocessing. For example, there are good Matlab addons for working with EEG and fMRI. These products (and Python) are also useful for computer vision.

You could also start taking a look at scholarly papers on the subjects of interest, and examine how they did their work. If there is a particular specific work that interests you, then using similar technologies is likely to be useful.",hmsr80w,t3_r6eo9a,1638368673.0,False
r6eo9a,"Thanks for the suggestions, I'll start looking at some papers then !",hmsy4gm,t1_hmsr80w,1638371738.0,True
r6eo9a,"If you decide on a more specific area, then I might be able to recommend something. :)  Just reply here, and I'll drop some links if I can.",hmsznut,t1_hmsy4gm,1638372392.0,False
r6eo9a,"I'm mostly interested in human language and cognition so I guess NLP (not sure if cognition is being studied from a computational perspective).

Thanks !",hmt0mci,t1_hmsznut,1638372796.0,True
r6eo9a,"Yes, although research tends to be more niche, e.g. cognitive models of schizophrenia. Here's a good starting point for some foundation. Good luck and have fun! :)  


https://www.sciencedirect.com/science/article/abs/pii/S136466130600132X",hmt155c,t1_hmt0mci,1638373012.0,False
r6eo9a,If your using ur for research r studio will make things a lot easier for you,hmub04a,t3_r6eo9a,1638390903.0,False
r6eo9a,"You might be interested in:

https://probmods.org",hmx8uxt,t3_r6eo9a,1638447446.0,False
r6eo9a,Seems interesting ! Thanks !,hmxaz4v,t1_hmx8uxt,1638448783.0,True
r6ei7a,"Cisco use a program called Packet Tracer. Its pretty good. Not sure if its used by professionals or not for planning, but its good for training.",hmu3w7r,t3_r6ei7a,1638388205.0,False
r6ei7a,"I've heard about that one before, but I am curious about more proffesional stuff.",hmufh8q,t1_hmu3w7r,1638392548.0,True
r6ei7a,Check out GNS3 and Eve-NG.,hmuzz5r,t1_hmufh8q,1638400702.0,False
r6ei7a,">Eve-NG

From a little research, it seems for huge scale networks ( like cities or campuses ) OPNET is still superior to those. 

I'm definately going to try GNS3 for small projects though!",hmv8ob5,t1_hmuzz5r,1638404583.0,True
r6bl5i,"I don't know the answer to your question, but the distances (or weights/costs/whatever) in general TSP aren't necessarily Euclidean. The edge weights can, in principle, be arbitrary, and they don't necessarily even conform to the triangle inequality, so the weight of a direct edge from A to C can even be greater than the sum of the weights from A to B and B to C.

The visualization in your image looks Euclidean, but is that what the weights actually are, or is that just what the visualization makes them look like?",hmsbnbs,t3_r6bl5i,1638359937.0,False
r6bl5i,I used the concorde executable which generate random euclidean graph,hmsygx4,t1_hmsbnbs,1638371887.0,True
r6bl5i,"The green line segments are not connected; they are not a path. If you want to compare the lengths of the two paths, just compute the length of each.",hmsq3ip,t3_r6bl5i,1638368145.0,False
r6bl5i,"Yes sorry I thought it would be clear that I kept the linking edge, I updated the picture",hmsykda,t1_hmsq3ip,1638371927.0,True
r6bl5i,"Can you elaborate on what the green path ia supposed to be? Because right now it is not really a path.

Concorde is a well established program and well known in TCS and OR communities. It is likely that you are misunderstanding something. 

Also, how large is the instance you are trying to solve?",hmsvpkf,t3_r6bl5i,1638370693.0,False
r6bl5i,"The instance was 300 random cities, I changed the picture to better show what I mean by the green path",hmsy8mp,t1_hmsvpkf,1638371787.0,True
r6bl5i,"Well,  the picture is much more clear now. 

Have you checked if the values are indeed better for your solution? Both solutions could very well have the same optimal value.

If you name your nodes from left to right: A,B,C and D.

Concorde's solution is: BC + AB + AD

Yours is: AC + AB + BD

What are these values?",hmt2gmr,t1_hmsy8mp,1638373563.0,False
r6bl5i,"Unfortunately, concorde don't let you see the adjency matrix as far as I know",hn36u8t,t1_hmt2gmr,1638551362.0,True
r6bl5i,"Can you see the coordinates of each node?

I find it strange that you cannot see the instance or save it somewhere. I have not used concorde though, I only know what it does and can be used for.",hn6lklt,t1_hn36u8t,1638615372.0,False
r639yl,"This is a complicated question. Computer science students study these topics (usually as part of Operating Systems and Computer Architecture courses) but the deep work at industry-level is generally done by computer engineers or software engineers with significant training or experience in computer engineering.

I’ve heard good things about Nand2Tetris for CPU stuff. Any graduate-level computer architecture textbook will have a ton of information like the kind you’re looking for. We used [this book ](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1?countrycode=US&format=electronic) in my graduate program. I’ve heard the PDF is floating around online. This book explains hard drives, CPU design, RAM, etc. 

For the GPU, you should look into NVIDIA’s CUDS documentation. 

At a slightly higher level, [the OSDev wiki](https://wiki.osdev.org/Main_Page) is a great reference as well.",hmqqmgk,t3_r639yl,1638323970.0,False
r639yl,"I just want to say, as a computer science major, absolutely don't do computer science for this. 

At least where I go to school, CS is more for software stuff, and only went into details on this for a single class (Operating Systems), and even then, it wasn't super deep.

Computer Engineering (again, at least at my school), goes down into the technicalities of how everything actually works, and how to build it. 

Computer science -> Software Engineer

Computer engineer -> Designing and learning about hardware components and circuits",hmr04q6,t1_hmqqmgk,1638328219.0,False
r639yl,Good advice for sure. It was the same way at my school. :),hmt40rr,t1_hmr04q6,1638374196.0,False
r639yl,"Thanks, very helpful.",hmsz3wf,t1_hmqqmgk,1638372155.0,True
r639yl,[deleted],hmrdk4s,t3_r639yl,1638334808.0,False
r639yl,Thank you!,hmsz4nr,t1_hmrdk4s,1638372163.0,True
r639yl,"When you do this just keep in mind that this video series covers an architecture used for teaching called SAP - ""Simple As Possible"" created by Malvino for teaching, and our actual hardware doesn't operate very much at all like these model systems.

Generally you would be replacing each part of that assemblage with an entire ecosystem of related hardware. It gets very deep, very fast, and the worst part is a lot of it is considered ""secret sauce"" by manufacturers, so you won't get a complete description of what's taking place in something like a developer's reference manual.

That said, a lot of the time those developer's manuals are often your best bet for learning the things you should know about hardware such as ram or your cpu, and are distributed by manufacturers.

Often you can also find information provided by brave souls who have done the dive for you -- https://people.freebsd.org/~lstewart/articles/cpumemory.pdf",hmz0z0x,t1_hmsz4nr,1638474848.0,False
r639yl,"You're looking for this thing: https://www.bookdepository.com/The-Elements-of-Computing-Systems/9780262539807

Get your hands dirty and *really* find out how this stuff works. It's a really good hobby project.",hmrw273,t3_r639yl,1638347098.0,False
r639yl,"> Does this fall under computer science? 

Yes and no.

It did, traditionally. And a lot of Computer Science courses will teach this stuff. But these days it's considered Computer Engineering, but there's a huge cross over between the two, as CE can be seen as a subfield of CS, or a cross over of CS and Electrical Engineering.

Check out /r/ComputerEngineering for more.

Anyway, here's my stock answer for this question:

If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

This will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc. Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board"")",hms4xu6,t3_r639yl,1638354738.0,False
r639yl,Thank you!,hmsz7v3,t1_hms4xu6,1638372202.0,True
r639yl,Check out Computer Systems a Programmers Perspective by Bryant or Computer Organization and Design by Patterson and Hennessey.,hmrfts3,t3_r639yl,1638336037.0,False
r639yl,https://www.nand2tetris.org/,hmsrlp0,t3_r639yl,1638368851.0,False
r639yl,there’s an IT professional course by google. check it out,hmrz1f5,t3_r639yl,1638349605.0,False
r639yl,"If you’d like, I can send you a link to a pdf (through libgen) of the textbook we’re currently using in my computer architecture class for a more beginner-esque read",hms05gt,t3_r639yl,1638350574.0,False
r5yufr,"I just want to emphasize, this post is not a tech support question. I already have the fix. I just really want to understand why the fix works.",hmpvj7h,t3_r5yufr,1638310288.0,True
r5yufr,"There's a **lot** of incorrect information in this thread. Pun intended.
 
Multi-threading was a thing long, long before multi-core CPUs were a thing. Setting a process priority to high and giving it a single-core affinity does **NOT** make it single threaded. This is misunderstanding the concept of process, thread, and core entirely.
 
What you are doing by changing those settings is ensuring that it is the only thing running on that 1 core, on the proviso that nothing *else* is being given above normal or higher priority, and that you never, ever saturate all of your other cores with other processes (which will override that priority). So in essence turning it into a single core machine as far as that software is concerned.
 
Dual- and higher- core processors were very, very new to the consumer space in 2006, with the vast majority of consumer PCs being single core, and the vast majority of consumer software being written in such a way that single core performance was what was prioritised. This doesn't mean that those programs were single-threaded - far from it, the majority were multi-threaded, so the GUI remains responsive while processing work happens in the background. For example, if you're using Internet Explorer, the page can still be loading while you type things into a search bar, you didn't have to wait for that page loading work to be complete before it could respond to your keyboard input.
 
The difference between multiple threads all running on the one core at the 'same time' and multiple cores, is that multiple threads (or processes) running on the same core don't actually run at the same time at all. Through various algorithms they are sliced, diced, and given effectively a time share of that chip's work, one after the other after the other, to fake things running at the same time. Only 1 instruction is every being worked on at the same time, but it could be some from Thread 1, some from Thread 2, some from Thread 3, then back to 1, and so on (the same for processes, it might do something for the game, then the notepad window in the background, then the OS, then the game again, picking up each thread each process has).
 
What multiple cores do is make it so that time sharing is now done across multiple things that can actually execute instructions simultaneously. So if you are dual-core there are 0-2 instructions being processed at any given instant, from up to 2 threads in up to 2 processes. And scaling up to quad-core with 4, etc etc. This means that so long as you never have 1 big-ass thread that wants all the processing power, a dual-core CPU can be twice as fast as a single core, quad-core is 4x faster so long as you have 4 threads that want all that power, etc.
 
The reason the game is crashing when it is running on multiple cores is because the code that it uses to manage its threads (so they don't all try to change the same data, or so that thread 1 that is waiting on some calculations from thread 2 doesn't start too early, etc) was written when there was a physical guarantee that 2 instructions on 2 threads would not happen at the same exact time. When it is on multiple cores, this guarantee disappears, and that thread management fails. There are a bunch of technical reasons this may occur, usually from attempting to do things more quickly (2006 consumer PC hardware was not fantastic by modern standards) and skipping various safety checks.
 
But it is still multi-threaded on 1 core.",hmrjr2m,t3_r5yufr,1638338323.0,False
r5yufr,"Your comment is giving the impression that single core multi threaded applications have higher guarantees than multi core multi threaded applications. It is actually not so. Any operation which is non-atomic will have same guarantees in single core as well as multi core. Similarly, any operation which is atomic will have same guarantees whether it is in single core or multi core. (Atomicity can be established either by explicit lock or atomic machine instruction). Anyway I am not convinced that multi threading is an issue. 

https://www.reddit.com/r/computerscience/comments/r5yufr/why_might_changing_process_priority_and_forcing_a/hmrj6af/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3",hmrwq31,t1_hmrjr2m,1638347660.0,False
r5yufr,"I'm not giving an impression of anything, I am outright stating that there are literal physical guarantees that apply to single-core CPUs that do not apply to multi-core CPUs. You cannot have 2 instructions simultaneously operating on a single core CPU by definition, you *can* have 
 n instructions operating simultaneously on an n-core CPU. This is a fundamental truth to reality.
 
At some point the code in the game is relying on that one instruction at a time guarantee (which is not a guarantee that gets written into an RFC, it just 'is') - the developers probably never even knew they were relying on it, because it would never have come up in testing on any single-core machine.
 
This is not the sort of thing that will come up if you are doing everything by the book - it comes up when you use hacks to make things run faster. For example (no idea if this is related to why it's happening, but it would show this behaviour) - using thread priority to ensure you don't get into a race condition to avoid the need to declare and check locks or sleep/wake. Works when those threads are on a single core and the prioritised thread is doing its thing, doesn't work when the deprioritised thread has another core it can run off onto.",hmrxrp7,t1_hmrwq31,1638348528.0,False
r5yufr,"Thank you for the lengthy explanation.

There is just one thing I want clarification on. Does this mean that any program designed for only single-core processors will try to use multiple cores to execute its threads if there are multiple cores available? I ask this because you said ""...written when there was a physical guarantee that 2 instructions on 2 threads would not happen at the same exact time. When it is on multiple cores, this guarantee disappears, and that thread management fails.""

If so, I have another question. Why do other old PC games I play work just fine on my modern PC? For example, the previous game in the series of *Shiny Days* (*School Days*) does not have similar bugs on my system.",hn63o63,t1_hmrjr2m,1638600628.0,True
r5yufr,"Most of the time it isn't an issue - going from multi-thread single-core to multi-thread multi-core won't break anything if your code is following the correct rules. Like I said, it comes up due to hacking things to break the rules in ways that don't actually break under your testing environments, but leave your code 'fragile' for changes you *didn't* anticipate. (Or I suppose it could also be bugs you put in by mistake that are masked by single-core operation). Most software written back then aren't relying on that guarantee.",hn64hom,t1_hn63o63,1638601218.0,False
r5yufr,"Well it's clearly there is something wrong with your code which handles some threads or asynchronous tasks, something could be wrong here.

And since you put it above normal, this will just suck more cpu resources which shouldn't be opt in even in complex game in normal situations. So as someone already mentioned this doesn't consider a fix. And since it freeze at specific moment try to debug it(could take some time) and find how much resources are being used.


So what programming language and frameworks do you use ? And does it also fail at that point when it's not in full screen? What if it's in big screen but not full (95%) does it work?",hmq7pot,t3_r5yufr,1638315474.0,False
r5yufr,"I'm sorry if I made this unclear but, this isn't a game I coded. Another company made this game and has not made the source code public so I can't tell what language it was written in. Shiny Days is a remake of the game Summer Days (Wikipedia article linked below)

[https://en.wikipedia.org/wiki/Summer\_Days](https://en.wikipedia.org/wiki/Summer_Days)

I was just asking this question because, as a player and as someone majoring in Computer Science, I'm just very curious as to why this workaround works, I couldn't think of anything. I tend to think a lot about why my software acts the way it does when it shouldn't act that way and how the developers could have programmed it wrong.

[https://jast.freshdesk.com/support/solutions/articles/12000055415-school-days-shiny-days-randomly-freezes-and-crashes-when-trying-to-play](https://jast.freshdesk.com/support/solutions/articles/12000055415-school-days-shiny-days-randomly-freezes-and-crashes-when-trying-to-play)

Above is a link to the company's post on this workaround. All they said is that it helps with stability on some CPUs but, I wasn't sure why.

&#x200B;

Also, no it does not crash at all in windowed mode at this point in the game. I cannot adjust the window size either. It is locked at either a specific size in the windowed mode that I can't change or full screen by the developers, so I can't put it at 95%.

By the way, just so you know Visual Novels are usually a series of cartoons/anime where you make very occasional choices that affect the story of the game. Almost like a choose your own adventure game. The scene where it freezes is when it seems to transition from the first scene of the game to the second scene of the game",hmqd0e0,t1_hmq7pot,1638317844.0,True
r5yufr,"**[Summer Days](https://en.wikipedia.org/wiki/Summer_Days)** 
 
 >Summer Days is an erotic visual novel developed by 0verflow, released on June 23, 2006, for Microsoft Windows and later ported as a DVD game and for the PlayStation Portable (PSP). It is the second installation of School Days line of series, succeeding the visual novel of the same name and preceding  Cross Days. Unlike the previous titles, that exist in the same continuity however, Summer Days is a spin-off of the original story retold from the perspective of Setsuna Kiyoura, a high school student out for summer vacation who finds herself attracted to Makoto Itou, a classmate and fellow patron of a restaurant she eventually comes to work at.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hmqd1uw,t1_hmqd0e0,1638317863.0,False
r5yufr,Nice,hmrdvj2,t1_hmqd1uw,1638334975.0,False
r5yufr," the game is released in 2006 lmao so it's very likely that's not 100% compatible for new systems, so no wondering it's buggy. The libraries in directx could be new and the game depends on older versions.

So now there are lots of possibilities.",hmqetgv,t1_hmqd0e0,1638318659.0,False
r5yufr,"That is a definite possibility. Graphics libraries tend to be finicky in older games. I wonder though why this game has such difficulties but other games around this time and games older than it doesn't have this same bug. It is unlike anything I have ever seen and I play a lot of old games, including the game earlier in the series (*School Days*).",hn63xmx,t1_hmqetgv,1638600812.0,True
r5yufr,"Based on the “fix” (which I would consider more of a workaround than a fix), it sounds like your issue is that your program cannot properly handle being run multi-threaded. I don’t know what language you’re using, but the task of ensuring your program runs properly in a multi-threaded environment is generally a nontrivial task. In terms of google-able terms, things like “deadlock” or “race condition” would be good things to read up on.",hmq6kfy,t3_r5yufr,1638314964.0,False
r5yufr,"Thank you, for those terms, I'm definitely going to do some research into those it seems interesting.

By the way, this isn't a game I programmed nor is it open source. It just has this huge bug and this workaround temporarily fixes it. As a generally curious person who is also studying Computer Science, I was just interested in why this workaround works so well.",hmqde7r,t1_hmq6kfy,1638318017.0,True
r5yufr,"Ah, got it, I assumed this was something you were working on yourself.

To give you a little bit more information, running a program single-threaded (what your workaround does) causes each line of the program to happen sequentially, as if you were running through the code by hand. Running multi-threaded allows you to run different chunks of code in parallel, giving you way more power, but it’s a perfect example of “With great power comes great responsibility.” If you don’t make sure that the multiple threads “play nice” with each other, you could run into issues.

A *race condition* is some bug that only expresses itself when the threads get scheduled in a certain way. The first example you’re normally given is

> Imagine a program that creates 2 threads, that we’ll call Ping and Pong. The Ping thread will access a global variable, lets say Score, and increment it by 1, 1000 times, and the Pong thread will access Score and decrement it by 1, also 1000 times.

The logical conclusion is that, at the end of the program, Score would be equal to 0 because we incremented it 1000 times and decremented it 1000 times. However, what’ll most likely happen is that it’ll be left with some non-zero value because the threads didn’t “play nice” with it. This is because you need to first access Score in memory, update it, then replace it in memory.

If the Ping and Pong threads grab the value of the Score at the same time, increment and decrement respectively, and then reassign to Score, whose value actually made it? Does the new value of Score reflect the increment that Ping did, or the decrement that Ping did? The answer is: it’s entirely dependent on how your OS schedules the threads. When you learn about this topic in your studies, you’ll learn about things like “semaphores” or, more generally, “locks” which are mechanism to help make sure your threads “play nice.”

Deadlocking is a concept that comes up once locking is being used. In that Ping Pong example, we can make the threads play safe by using a lock on Score, so Ping and Ping are only allowed to modify the value of Score if they currently hold the lock we associate with Score, which prevents the scenario from earlier where they overwrite each other’s work. Once your program gets more complicated and multiple locks are at play, you may end up with a situation where one thread says “I won’t free up/release/give back my lock on **X** until I get I get a lock on **Y**” while another thread says “I won’t free up/release/give back my lock on **Y** until I get I get a lock on **X**”. That battle over locks causes your program to come to a screeching halt, since neither thread will give up their lock until they get the other, which is why it’s called a deadlock.

Hope your studies go well!",hmqh70s,t1_hmqde7r,1638319730.0,False
r5yufr,"Maybe it is not properly programmed so some variables are accessed by 2 threads at the same time and caused unexpected behavior. Or like u/chrisxfire says, there is a deadlock somewhere ( one thread is waiting for another thread to leave a code section, but there's no other thread in that code section )  


Edit: The fix is kind of a quick solution that they just figured that in that scenario the problem doesn't happen. The only real fix is that they go debug their code, which could be a huge unmanagable spaghetti. In the GameDev industry there's too much spaghetti ( please don't hate me for saying this )",hmreawc,t3_r5yufr,1638335207.0,False
r5yufr,"My initial thought is that at that point of the game a process kicks off that needs to wait for another process to have completed, so when process b gets to a certain point it doesn’t have the expected app state and the app freezes. When you prioritize only one cpu then the game runs “synchronously”, and process b won’t start until the cpu is released after finishing process a",hmqqx0u,t3_r5yufr,1638324099.0,False
r5yufr,"All of these explanations are explaining away why setting cpu affinity might be solving the issue. But I am still curious why does windowed mode not result into game freeze (windowed mode game would still be multi threaded without setting cpu affinity). For that matter, why would increasing process priority be needed if you have already setup cpu affinity (game would be single threaded even if process priority is high/low)",hmrj6af,t3_r5yufr,1638337974.0,False
r5yufr,"Exclusive full screen is going to hit different code to windowed - which gives a very good indication of exactly where in the software the bug is without the inherent nature of window vs full screen needing to have much to do with it at all. Windowed being a way to avoid the code with the bug, single-core operation being a way to avoid the bug causing a crash. As for the priority it is to stop other processes from acting on that CPU core and delaying the game's threads (see my top level comment for why it's not actually single threaded though).",hmrwydi,t1_hmrj6af,1638347850.0,False
r5yufr,Yup I agree with your statement about prioritization not affecting threaded behavior. I assumed full screen and window mode of any application would be handled by operating system (some window server to be specific) and not dependent on application. Damn.,hmrx8rs,t1_hmrwydi,1638348092.0,False
r5yufr,"It's mostly handled by the graphics library being used, but will involve some OS code and some game code as well. It's not as true these days with more modern graphic environments, but back in the day exclusive full screen gave a much 'closer to the metal' access to the GPU, whereas windowed had to go via the OS. Now that isn't so true borderless windowed is becoming the standard instead (and DirectX 12 has removed exclusive full screen completely).",hmrye73,t1_hmrx8rs,1638349051.0,False
r5yufr,"Most likely, there's a bug in the code trying to use multi-threaded operations resulting in a deadlock which presents itself as a frozen or hung application as you're seeing. Setting the application to use a single core prevents it from obtaining multiple threads, thereby working around the bug.

I can't answer why changing the process's affinity to Above Normal is required. I could only speculate.",hmqlphz,t3_r5yufr,1638321751.0,False
r5w1xe,"There's two things going on here. Your course work for virtualization and doing virtualization from a practical point of view. For the first part, do what they want you to do and pass the exam. Even if it's completely useless shit.

The second part, install Virtualbox or another hypervisor, get on youtube and watch videos on the subject and just repeat on your own PC or Laptop. It'll become much clearer. At some point it'll just click and you'll wonder why you thought it would be difficult to learn.

Look into using containers such as Docker and LXC too, it's sort of lightweight virtualization.",hmrq182,t3_r5w1xe,1638342498.0,False
r5w1xe,"Thanks for the insight. 

I have been working with Docker for a while and that is what drew me to this course. The practical part is fairly easy for me (maybe because I have never dug deep into the lower levels and fundamental knowledge). I should have been clearer in the original post, all I want is advice for the coursework part :D Like ISA virtualization, binary translation, dynamic opt, HLLVM, ...",hmuygd4,t1_hmrq182,1638400002.0,True
r5rwl8,"In theory, the only thing your scenario requires for the client to get a response is that an application be listening to the appropriate network interface and port in the server.

In practice, the application doing the listening is usually a [Reverse Proxy](https://en.wikipedia.org/wiki/Reverse_proxy) which can then route requests to the appropriate resources (applications, static files, other sockets) or respond themselves. You can have site1.com route to one application, site2.com route to another, site2.com/thing route to a third and anything else gets a 404 error. Plus it can handle a lot of other useful stuff like certification keys.

Depending on the application stack, other middleware may exist between the client and the server application. 

In Python for instance, web frameworks usually have their apps served behind a WSGI, a Web Service Gateway Interface (or an ASGI, it's asynchronous successor). One of my services, for instance, was made with Flask and hosted with gunicorn and nginx.",hmq8lww,t3_r5rwl8,1638315872.0,False
r5rwl8,"It really depends on the server and API in question. Using PHP as an example, the NGINX web server communicates to php-fpm (a separate process that handles PHP interpretation) via [tcp or Unix socket](https://www.nginx.com/resources/wiki/start/topics/examples/phpfcgi/), whereas Apache can link to a dynamic library and then do the interpretation itself.",hmphe0l,t3_r5rwl8,1638304594.0,False
r5rwl8,"In order to remove abstractions you first need to understand the abstractions - they're there to help you understand the entire system. Back before the internet when network communications were more bare boned and direct, sure, you could just go learn the entire thing together, but these days there are so many complex moving parts that you'll never understand it properly that way.
 
First go for the TCP/IP layer model (the OSI reference model will probably help with this). Understanding what packets are and how the protocols work and how each component of the network infrastructure is expected to deal with things needs to be the basis of your understanding.
 
That will let you then seek out implementations of each layer, which will do all the things you describe in different ways. Someone has already mentioned how NginX and Apache differ on parts of the Application layer side, but there will be differences in how the network interface layer works (is it wifi, ethernet, etc), then how those interface packets are reconstructed into network layer (IP) packets, then those are reconstructed into transport layer (TCP, UDP) packets, which are then reconstructed into application layer data, and in the other direction too.",hmqj889,t3_r5rwl8,1638320642.0,False
r5rwl8,"> Data Arrives to the network interface for the machine, the driver for network interface card tells the OS that it has data

Yes

> (through an interrupt?)

Maybe, maybe not, depends on how the people who build the network adapter designed it

> The OS then looks at the IP:Port# and if there is a connected socket listening with that IP and Port

Kinda. There is a lot (!!) more to network routing than this, but let's just say the OS reconstructs the TCP packet and gives that to the socket

> Is the server and the API the same process?

Maybe, maybe not. For enterprise applications you usually have a lot of microprocesses all doing different things in the backend. You have a database (or, even larger, a lot of databases all distributed over different datacenters all around the globe while guaranteeing some form of consistency), handler threads, web worker threads which do HTTP parsing, perhaps a bunch of seperate processes for different endpoints etc. Your API server might be a data center with many different machines, all being coordinated and working together.

For very small application you can have a single monolith.

> and the dotnet framework (or any API framework) is baking in the socket code and acting as a server as well?

Again, depends on the framework. Simpler frameworks (like flask) have a build-in web server, more complicated ones may outsource this to nginx or some dedicated web server.

> is it something I'm completely missing?

Not really, everything you mentioned is being done in practice.",hmqfnzp,t3_r5rwl8,1638319044.0,False
r54to6,I'm reading a good book called Computer Networking: A Top Down Approach. I haven't gotten very far yet but so far it is easy to follow. I'm not sure if that is what you are looking for though. I have no experience in this area.,hml0u5s,t3_r54to6,1638222677.0,False
r54to6,"I second this. This was the textbook I used for my computer networks class. Honestly did not read too much of it, the lecture slides were sufficient (the authors of the textbook created the slides). Still learned lots tho. There’s also tons of reviews questions, practice problems, and assignments. There’s even a website with interactive problems. I feel like it’s got everything you could ask for.",hmlbj3y,t1_hml0u5s,1638227168.0,False
r54to6,"I also used this textbook for two of my classes, but instead of the slides I ended up basically reading the entire thing on my own because I had a timing conflict with my lectures.

Network topics have a tendency to turn into a bunch of alphabet soup for me with all the protocols and their overlapping acronyms, but there’s a ton of visuals in there (and also included in the corresponding slides, I believe) that really helped with recalling everything later on. Also, the way the information is laid out makes it so you form a progressively bigger picture throughout the semester. Was probably the most digestible textbook I had for those reasons.",hmn36o6,t1_hmlbj3y,1638260477.0,False
r54to6,"When you get to a concept or term you don't recognize, if the text you're reading doesn't explain it, just stop and check another resource.

For example:

[https://techterms.com/definition/protocol](https://techterms.com/definition/protocol)

[https://en.wikipedia.org/wiki/Communication\_protocol](https://en.wikipedia.org/wiki/Communication_protocol)

then pick up the original text where you left off.

I really doubt you'll find a complete-enough tome that explains \*everything\* in sufficient detail, the body of information is just too big.",hmkrfg4,t3_r54to6,1638218929.0,False
r54to6,"Computer Networks - Book by Andrew S. Tanenbaum.
This one helped me in similar situation.",hmnpref,t3_r54to6,1638277921.0,False
r54to6,"Came here looking for the comment suggesting the Tanenbaum, cheers!",hmnr1nj,t1_hmnpref,1638278629.0,False
r54to6,"Start with the CCNA/CCENT, JNCIA or equivalent in today's books. Then as /u/berrmal64 said, look up the unknowns or acronym soup in wikipedia, take a note if you want to come back and dig to learn more, and then move forward.

What I'm telling you as someone who's been in the networking field professionally for 20+ years is that you cannot decompose this field into 'first principles' as it is too vast. i would suggest to you, instead, to be task oriented. Learn what you need about the field to complete the tasks at hand and then learn more in the next research phase of the next task.

Get a starter cert for a networking vendor, but be aware that all vendor certs are biased by design towards their implementation.

I mean, you could read a selection of a few hundred of the first 2000 IETF RFCs, and you might be successful in learning something, but a good starter book and a few online references is a much better place for a beginner than any deep dive.

To start as an entry level network tech, effectively, you need knowledge an inch deep and a mile wide, not the reciprocal. 

I approach all my tasks this way, and spend a bit of time researching deeper on topics I am not completely familiar with today, as a support engineer for a vendor that makes networking equipment even after 20+ years of working in the business.",hmo26mj,t3_r54to6,1638284098.0,False
r54to6,thank you sir appreciate it!,hmoadq7,t1_hmo26mj,1638287573.0,True
r54to6,"I’ve been teaching myself using professormesser.com. His Network+ study materials are a good starting point for a total beginner regardless if you plan on going for the cert or not. Of course it won’t teach you everything there is to know since it’s geared towards the CompTIA exam objectives, but you can move on towards more in-depth material once you have those basics.",hmkt5vc,t3_r54to6,1638219609.0,False
r54to6,If you want much basic and theoretical concepts go and watch ravindrababu ravula computer networks in yt,hmm7pfu,t3_r54to6,1638241710.0,False
r4y6sf,Amazing result! Should this not be on the reddit main page?,hmmf75m,t3_r4y6sf,1638245185.0,False
r4y6sf,Hell yeah! Randomness can catch these hands.,hmma1gf,t3_r4y6sf,1638242769.0,False
r4y6sf,"Yeah I grokked maybe 70% of that. 😅
Seems super cool, am going to give it another read later and try to digest it better.


Thanks for sharing 👍",hmmwtkm,t3_r4y6sf,1638255538.0,False
r4xo16,"Being more specific with parameters would make it a different method. One thing I think you are confusing is the difference between accepting and requiring.

Your code has the `Animal.speak` method which requires a `LoudSpeaker` parameter and returns a string. This means any subtype of `Animal` must have a method that *accepts* a `LoudSpeaker` and returns a string (or string subtype?).

Being more specific (covariant) with the return type doesn't violate the method contract. If you returned a subtype of string (is that possible with string?) your are still by definition returning a string.

Being more specific with the parameters does violate the method contract. If the `Cat.speak` method *required* a `ScreamingSpeaker` (i.e., the method definition specifies it) then it can no longer *accept* a `LoudSpeaker`. Keep in mind that the  `speak` method can still *accept* a `ScreamingSpeaker` when you call it. If you wanted `Cat` to only take `ScreamingSpeaker` you could detect the type of the passed argument and throw an exception (or return null or whatever) if it is not a `ScreamingSpeaker` but since the method signature did not change that is **not** being covariant with the parameters.

Being less specific (contravariant) with parameters is allowed because it still fulfills the method contract. If `Cat` can take (requires) a more general `Speaker` in it's `speak` method, then it still *accepts* a `LoudSpeaker`.

I'm not sure what you mean on line 37 with ""According to LSP I shouldn't pass in LoudSpeaker here?"" because that is wrong. The `Animal.speak` method *requires* a `LoudSpeaker` which means it can also *accept* a `ScreamingSpeaker`.

Edit: Used OP's example code and fixed mixing up terms. This is why I need coffee in the morning.",hmjmfsd,t3_r4xo16,1638202418.0,False
r4xo16,"> If the Cat.speak method required a ScreamingSpeaker (i.e., the method definition specifies it) then it can no longer accept a LoudSpeaker. Keep in mind that the speak method can still accept a ScreamingSpeaker when you call it.

I understand I can pass in anything at runtime. What I'm saying is that Liskovs Substitution Principle dictates (as I understand it) that if the parent class's method (Animal.speak) requires a Loudspeaker, then the child class's method (Cat.speak) can only require a Loudspeaker or it's parent Speaker as a parameter. Regardless of what can be passed in at runtime.

So the methods themselves, because they are in a subtype relationship, must be *contravariant*.

However, any other function, in order to adhere to LSP must be *covariant* in their parameters.

There still seems to me to be a disconnect here. I'm sure I'm still misunderstanding something, I don't think I've found a formal flaw or anything like that but on it's surface it appears that LSP has an implicit contradiction.

Edit: Think about it like this.

Forget about the classes. I'm looking at all the functions in that code whether they are methods or not, and I'm wondering why some functions have to be covariant in their inputs and adhere to LSP and why some (subtype methods) have to be contravariant in order to adhere to LSP.

I'm trying to tease out what implicit assumption I'm making to make it seem like that.

Words are hard, sorry. lol.",hmk55ut,t1_hmjmfsd,1638210001.0,True
r4xo16,"Don't doubt yourself too much. It's a really good sign that you are questioning principles without immediately thinking you've found something wrong with it.

&#x200B;

>I understand I can pass in anything at runtime. What I'm saying is that Liskovs Substitution Principle dictates (as I understand it) that if the parent class's method (Animal.speak) requires a Loudspeaker, then the child class's method (Cat.speak) can only require a Loudspeaker or it's parent Speaker as a parameter. Regardless of what can be passed in at runtime.  
>  
>So the methods themselves, because they are in a subtype relationship, must be contravariant.

This is nearly correct. The *parameters* of the method *may* be contravariant. The `Cat.speak` method can require a `LoudSpeaker` or any parent-type in the method definition. This allows the method to still accept a `LoudSpeaker` but it may also accept a more general (parent) type.

&#x200B;

>However, any other function, in order to adhere to LSP must be covariant in their parameters.

Not quite sure what you mean here. Are you referring to the parameters that are passed in when the function is called? Taking the function with the definition `speak(LoudSpeaker $speaker)` and calling it with `speak(new ScreamingSpeaker())` is not covariance it's just polymorphism. A `ScreamingSpeaker` *is* a `LoudSpeaker` so it can be taken as a parameter. If `Cat` overrides it with `speak(Speaker $speaker)` (being contravariant), then you can still call it with `speak(new ScreamingSpeaker())` but you could also do `speak(new Speaker())` or `speak(new QuietSpeaker())` where `QuietSpeaker` exteneds `Speaker`.

To help clear a little confusion, the types that a function takes in according to it's definition are generally referred to as **parameters**. The actual values that are passed in are generally referred to as **arguments**.",hmk8tvp,t1_hmk55ut,1638211457.0,False
r4xo16,"> This is nearly correct. The parameters of the method *may* be contravariant.

Wikipedia and everywhere else states that in order to adhere to LSP, it **must** be contravariant. Meaning that you must allow for either what the parent class expects as a parameter, or something more generic than that.

[Contravariance of method parameter types in the subtype.](https://en.wikipedia.org/wiki/Liskov_substitution_principle)",hml4xnj,t1_hmk8tvp,1638224372.0,True
r4xo16,"That is what I meant by ""may"". It either is the same as the parent class, or it is contravariant and it takes a more generic type.  
  
I suppose some may say that it is contravariant even if it uses the same type but that seems odd to me.",hmlfa88,t1_hml4xnj,1638228826.0,False
r4xo16,"But one thing you *can't* do is pass in a more specific type in the subclass's methods. I mean you *could* but you'd be in violation of LSP. And therein lies the issue for me. In one sense, LSP is saying that for subclassing, you have one rule in the method's parameters (contravariance), but in other instances it calls for covariance in function signatures. I'm not sure how to square that.",hmlhk3i,t1_hmlfa88,1638229844.0,True
r4xo16,"Passing in a more specific type *is* LSP, not a violation of it.

    signatures:
    Animal.speak(LoudSpeaker) // Base signature
    Cat.speak(Speaker) // Contravariance, no LSP violation
    
    function calls:
    animal.speak(new LoudSpeaker()) // Normal call
    animal.speak(new ScreamingSpeaker()) // ScreamingSpeaker is a subtype of LoudSpeaker, no LSP violation
    cat.speak(new Speaker()) // Allowed because of the contravariance in the signature
    cat.speak(new LoudSpeaker()) // LoudSpeaker is a subtype of Speaker, no LSP violation
    cat.speak(new ScreamingSpeaker()) // ScreamingSpeaker is also a subtype of Speaker, no LSP violation",hmlnuhy,t1_hmlhk3i,1638232730.0,False
r4xo16,"> Passing in a more specific type is LSP, not a violation of it.

I agree. That's how I've always understood it.

However, when it comes to method parameters of a subtype, LSP is pretty explicit that you *should NOT* pass in a more specific subtype than what is required. You can however pass a more generic parent type.

In Wikipedia and everywhere else I've read it says explicitly that LSP requires subtype method parameters to be **contravariant** in their input.

Do you not agree that to be **contravariant** in your input means that you cannot pass a more specific subtype in? I think that's where we're both getting stuck.",hmlvgif,t1_hmlnuhy,1638236200.0,True
r4xo16,"It does **not** mean that you cannot pass a more specific variant. It means that you cannot **require** (in the method definition) a more specific type.

You have to think of the *use* of a method and the *definition* of a method separately.

>However, when it comes to method parameters of a subtype, LSP is pretty explicit that you should NOT pass in a more specific subtype than what is required. You can however pass a more generic parent type.

LSP is explicit that the method must support *at least* the specific type, but it may support a more general type.

It looks like you are misusing the term ""passing in"". Passing is **only** referring to the *use* of a function. Accepting/requiring/taking refers to the *definition* of a function. Contravariance is referring to what a function can **take** as an input, not what you actually **pass** to the function.",hmlwtt3,t1_hmlvgif,1638236852.0,False
r4xo16,"> Contravariance is referring to what a function can take as an input, not what you actually pass to the function.

Right. As I've said before I understand that you can pass in anything you want.

I think we're getting hung up on some of these terms.

How about this. I can just reformulate the question. Why does LSP *require* a subtype's method parameters to be *contravariant*?

And then, why does LSP require that any other function that accepts a type be *covariant*? And why are those two seemingly different?

Also, you are the most patient person in the universe and I offer my many thanks. :)",hmlxnkh,t1_hmlwtt3,1638237219.0,True
r4v7w6,"Disclaimer: not a C# programmer but I do know a thing or two about memory.

I'm not 100% sure what you mean by memory stack, as you've mentioned the same theoretical structure used in two different places: the stack data structure. I'm going to assume you wanna know about the ""stack memory"" used by programs during function calls.

First off: there are different types of memory at your disposal when you write programs:

1. Text : this is where the source code of your program resides. If someone (or something) tries to change it, you'll get an error as it's read-only
2. Data and BSS : these two are used by variables and objects that live during the whole lifetime of  your program. So global and static variables are usually stored there (at least in C and C++)
3. Heap : this is dynamic memory which you can request from your OS for your program. Of course, if there's not enough free memory, you'll get some indication of failure (nullptr in C++ and maybe an exception in higher-level languages)
4. Stack : I assume this is what you wanna know about. Whenever you call a function, your compiler has to allocate some space to be used by the parameters and local variables of that function. Also, you want to store a pointer to place from which you called the function, so that you can return to that place after the function finishes executing. Now, this ""stack"" is fundamentally no different than the user-defined stack data structures created by programmers in languages like C and C++ (some higher-level languages provide stack data structures as part of their standard libraries). The practical difference is just that the memory stack happens to be a useful data structure when dealing with function calls and is used by the compiler when it emits assembly code (this is what C/C++ compilers do, not so sure about C#). You can see PUSH and POP instructions if you disassemble an executable. It's what computer scientists call a LIFO structure (last element that goes in is the first one to come out), which makes perfect sense in the context of function calls since you want to free up space (i.e. pop) used by the variables of the last function you called so that the next function has a fresh memory space to work with, while you also want to make sure to keep the memory used by the functions higher up the call chain untouched.

Why is this important? Couple of important considerations:

1. Stack tends to be faster. I say tends to because I've worked in some environments where the difference in performance between stack and heap is negligible. That being said, in many environments stack seems to be much faster than heap. So prefer it whenever you can.
2. It tends to be more memory-constrained than heap. You can't just shove gigabytes of data into it and expect things to go smoothly. Sometimes it can happen even when there is no obvious indication that you're shoving a lot of data into it. For example, remember when I said the location from which you called the function is saved into the stack? Well, in a more popular scenario when a programmer forgets to put an exit condition in a recursive function (google it if you don't know, it's basically a function ending up calling itself infinite amount of times) then you will get a ""stack overflow"" since all those saved pointers end up filling the stack. You can overflow stack much quicker in embedded systems with limited resources, than on a desktop gaming PC.

That's what I can think of now. I hope more experienced programmers will elaborate more on this issue and maybe even find a fault in my own answer",hmj2d2w,t3_r4v7w6,1638192944.0,False
r4v7w6,"One more thing about heap allocations: They can cause memory fragmentation if you use them a lot in your code. Probably not that big of a problem if you don't care about performance, but still, take care not to litter your code with it too many times.",hmj329n,t1_hmj2d2w,1638193333.0,False
r4v7w6,"The only fault I would bring up is that program text is not source code, it is the machine code of your program, produced by translating source code to machine code. The ""code"" bit is an overloaded term, but in this case refers to executable instructions.",hmjr3w2,t1_hmj2d2w,1638204357.0,False
r4v7w6,Oops! Should've clarified that!,hmjrd3b,t1_hmjr3w2,1638204461.0,False
r4v7w6,Well explained sir. The stack OP is familiar with in C# is a data structure. The memory stack is a an application of that data structure in the way compilers deal with particular type of memory.,hmkkjbo,t1_hmjrd3b,1638216147.0,False
r4v7w6,"Sorry for the late reply was trying to wrap my head around Big O notation last night 🤦‍♂️ but this is great thank you, this makes it a lot clearer to me and I understand it a lot more now",hmnmpug,t1_hmj2d2w,1638276142.0,True
r4v7w6,"Stack is an overloaded term. It's both a general data structure and also a specific stack is used as you mention as a part of what is referred to as an ABI. The ABI related usage is the one that is used for function calls. Generally its referred to as the ""call stack"" of a program. Your operating system is also managing and using a stack of this type. For more information on the data structure check the C# documentation related to the Stack class in System.Collections. For more information on the ABI related usage of the term, search for information related to what is called the ""calling conventions"" used by the ABI for your computer's architecture.",hmjqbt9,t3_r4v7w6,1638204039.0,False
r4v7w6,"Basically the stack is where local function variables are stored in most programming languages.   
To be clear

    val x = 5 // not a function variable
    def myFunction(y /* not a function variable */) = {
      val z = 10 // function variable
      return x + y + z
    }

So if you had some code like 

    def myFunction2(n) = {
      val pi = 3.14
      // breakpoint 1
      return n + pi
    }
    def myFunction1(n) = {
      val x = 5
      val y = myFunction2(n)
      val z = x + y
      // breakpoint 2
      return z
    }
    myFunction1(1)

then at `breakpoint1` your stack would look like

    3.14
    5

then at `breakpoint2` it would look like

    9.14
    4.14
    5

notice that at `breakpoint2` our `3.14` is gone. because once `myFunction2` returned, then the stack pointer (pointer to the top of the stack) was moved back to the location it was at before the function was called. thus `3.14` was overwritten with `y` and `z`.

hopefully this makes sense to you, if not I'm sure there are lots of great intro CS / unmanaged language classes online that can explain i more depth.

ps. note we never actually ""pop"" the memory stack, we just move the memory pointer back to where it was before a function was called. when we want to reference a value on the stack it's done by accessing the memory address of the value directly.",hmko2sv,t3_r4v7w6,1638217585.0,False
r4v7w6,"Others' answers give good context. This may be a bit more direct:

The ""call stack"" is basically like a C# `Stack` of ""stack frames"". The OS sets aside memory for the call stack before a program starts. **A stack frame contains all the information needed to return from a function call and give back control to the caller.** This information is usually just a ""return address"", which is a pointer to the instruction that should be executed after returning.

A stack frame can also store local variables and function arguments. This is different from a C# `Stack`, which only stores one type of value. The compiler will keep track of the size of each function's stack frame, so the return address can still be found by popping that many values off the stack.

Source code is compiled into a list of simple instructions that the CPU can execute. These instructions operate on tiny chunks of memory inside the CPU called registers. Some instructions are loads or stores, which move data between registers and RAM. Some instructions are arithmetic, which just manipulate the contents of registers. Some instructions also do both.

**There are usually dedicated registers for the ""stack pointer"", which is the address of the top of the call stack, and the ""instruction pointer"", which is the address of the next instruction to execute.** The instructions that operate on these dedicated registers are the answer to your question. 

Most CPUs have instructions dedicated to manipulating the stack pointer. A `PUSH %reg` instruction will increment the stack pointer and store the contents of the general-purpose register `%reg` at that address. A `POP %reg` instruction does the reverse. The compiler can use `PUSH` and `POP` instructions in pairs, or manipulate the stack pointer directly, to store as many local variables as each function needs without losing track of the return address.

Most instructions implicitly increment the instruction pointer. A `JUMP` instruction instead explicitly overwrites the instruction pointer, so control transfers to a new location. **A `CALL` instruction pushes the instruction pointer onto the stack before jumping, like a combination of `JUMP label` and `PUSH %ip`. A `RET` instruction does the opposite of `CALL`, which is essentially `POP %ip`. This is how function calls and returns use the stack.**",hmm26uf,t3_r4v7w6,1638239256.0,False
r408nu,Oh damn them feels. I’m a CS major and all that tawk about how you’ll get rich working for Big Tech or Silicon Valley really made the field seem off putting. I wish that they taught CS (and other STEM fields) like how they teach philosophy.,hmi3lln,t3_r408nu,1638166818.0,False
r408nu,Yes yes,hmi5qz6,t1_hmi3lln,1638168307.0,False
r3rold,"Some of the simplest solutions include hosting the resume on a GitHub repository, or hosting it on a website linked to from the GitHub profile. If you're set on a cryptographic solution, though, some possibilities include:

* You include your public key on your GitHub profile, and a signed message in your resume, or vice-versa

* You could use some kind of [identity proofs, like what KeyBase does](https://book.keybase.io/guides/proof-integration-guide). This can prove that the same human controls a website, GitHub profile, Twitter account, etc.

Seems over-complicated to me, though, I'd stick with ""my CV is on my website, which is linked to from GitHub, and my CV includes links to both my website and GitHub profile.""",hmcfxo6,t3_r3rold,1638062627.0,False
r3rold,That seems to be a concise explanation. Thank you.,hmcjczr,t1_hmcfxo6,1638064246.0,True
r3rold,The employer could simply ask the interviewee to login and show them that they own the github account? But some type of cryptographic signature seems like a good idea.,hmdhrj4,t3_r3rold,1638083159.0,False
r3rold,"In the case where you want only one way verification and *don't* want to overtly tie your github username to your actual name, the company can just ask the person to add something specific to their bio, comment on an issue on a company repo, etc.",hmdc5ac,t3_r3rold,1638079430.0,False
r3rold,"I have a plain text file with URIs to my profiles on social media etc (GitHub, Twitter, Facebook…) signed with my PGP key and hosted on my website.",hmdyvz1,t3_r3rold,1638096439.0,False
r3rold,"This is pretty much the same problem certificate providers have to solve. They have to check is someone is the owner of a website.

Usually the prove is done by creating a specific file with a content given by the certificate provider on the website. An other way is to set a HTTP header with a value given by the certificate provider. However, the idea is always the same: Provider gives you a secret and the owner has to place it on the website.

To not have to place a secret on the github page for everyone that want to verify the owner. The owner could place a public key. The verifier can than send a challange (random string) to the owner, the owner can than sign the challange with the private key and send it back. If the verifier can verify the signature with the public key from the repository it ownership is proven.",hmew0db,t3_r3rold,1638115902.0,False
r3exb5,[deleted],hma1zlm,t3_r3exb5,1638024879.0,False
r3exb5,16 * 65536 = 1048576 bits? which is not 128 kilobits?,hma30v2,t1_hma1zlm,1638025398.0,True
r3exb5,"There are 2^16 (65536) locations, each storing 16 bits (2 bytes, also called a *word*).  65536x2 (bytes per location) is 128kb.

It's _addressable_ with a 16-bit number, meaning one 16 bit number can represent any of the locations.",hma5q1s,t3_r3exb5,1638026694.0,False
r3exb5,"Is this correct? I might be having a stroke, because for some reason I don't get this at all, I might've understood something completely wrong because:
65536x2 = 131072 bytes? which is => approx 1048kb (kilobits).

So I've 65,536 locations
If I store 16 bit worth of something into every single location available it should be exactly 1048576 bits. 

This is my thought process and for some reason I don't get this at all. Be it 16 bits or 2 bytes, essentially it should be the same? As in 1 byte is 8 bits and 2 bytes is 16 bits, hence your explanation seems exactly the same as mine but using bytes?",hma88bc,t1_hma5q1s,1638027863.0,True
r3exb5,"Yeah I don't think any of that is wrong (except kb is kilobytes, not kilobits).  There are 2^16 slots, each slot can store a 16-bit number, so that's 1048576 bits.  Kb here is kilobytes, which is the standard for storage (always in bytes, not bits).",hma8z15,t1_hma88bc,1638028205.0,False
r3exb5,"Is the material wrong? Because it says 
> This means it can store a total of only 128kb

As in kilobits, not in kilobytes. If the material would have kB in it, I would understand it, but no matter how much I wrestle with the bits and bytes here I cannot see how the memory is only 128 kiloBITS in size.",hma9r1w,t1_hma8z15,1638028559.0,True
r3exb5,"Nobody measures storage in bits.  Storage is in gigabytes, megabytes, kilobytes, or bytes.  KB is kilobytes.  The material is correct.  It's kilobytes though, not kilobits.  You can't rely on seeing KB instead of kb.  You may often see gb, mb, or kb...they're all in bytes.",hma9wrg,t1_hma9r1w,1638028634.0,False
r3exb5,"Technically, OP is correct and the material seems to be a bit sloppy about the unit symbols. Traditionally (and also according to an [IEEE standard](https://en.wikipedia.org/wiki/IEEE_1541-2002)), an uppercase B is used for bytes and a lowercase b is for bits. It's technically wrong to use ""kb"" for kilobytes.

Of course nobody actually bothers to get it correct nowadays, so you're right that you need to kind of figure it out from the context rather than relying on notation actually being correct. However, considering that the page is about rather low-level stuff that involves both bits and bytes, one might reasonably expect the author to actually clearly distinguish between the two. OP's confusion is understandable since they apparently learned the unit symbols right at some point, although it's not much of a stretch to figure out the author of the documentation just didn't bother to get the units right.",hmegkd5,t1_hma9wrg,1638108473.0,False
r3exb5,"Technically yes, but lots of people don't bother to get the unit symbols for bits and bytes right.",hmegtql,t1_hma9r1w,1638108611.0,False
r3exb5,"Yeah, I wasn't truly aware of that to be honest and I'm ok with it. I genuinely was just very confused because I literally googled the meaning of ""kb"" and realized it meant kilobits and rest is history. But yeah, I thought that material being rather low-level would've super precise details and didn't consider the possibility of the context at all.   


You live and learn. Now I know a ""bit"" more! :)",hmgj1p9,t1_hmegtql,1638139827.0,True
r3exb5,"It’s a common misconception that 128 kb = 128,000 bytes. It’s actually 2^17 = 131,072. This is the same concept that 1 kb does not equal 1,000 bytes, but rather 1024.",hmdg805,t3_r3exb5,1638082118.0,False
r3cbfk,maybe also ask also on physics/math related subs/forums,hmacs8x,t3_r3cbfk,1638029945.0,False
r3cbfk,You would probably have better luck in an engineering or physics sub.,hmbz32e,t3_r3cbfk,1638054905.0,False
r3cbfk,"[https://www.youtube.com/watch?v=qsYE1wMEMPA](https://www.youtube.com/watch?v=qsYE1wMEMPA)

[https://www.youtube.com/watch?v=uG2mPez44eY](https://www.youtube.com/watch?v=uG2mPez44eY)",hm9uvqv,t3_r3cbfk,1638021044.0,False
r3cbfk,"Thanks for advice, sadly i've ready watched them :D",hma4nxr,t1_hm9uvqv,1638026187.0,True
r3cbfk,Look at some of the MOOC sites!,hmdg911,t3_r3cbfk,1638082137.0,False
r39ah9,"Fetching images from RAM and context switching is expensive.  To do that many times in a single frame while rendering slows everything down.

Atlasing allows you to pack textures into a single file, normally minimizing the amount of empty space around sprites (thusly size required to store images in memory), and store metadata which is useful in the logic and rendering part of your code.  

The GPU can now use one or two textures instead of switching context between multiple textures residing in memory.  It is hard to understate how big of an improvement texture atlases can be to your rendering times (and it is worth doing because it is simple to do).  A good experiment would be to build two like projects rendering a hundred 2D sprites, stationary, but one utilizes an atlas and the other has every sprite in a separate file.  You will see the importance then.

One of the techniques I like, since I don't like having all of my images in one file, is to build atlases on the fly in memory during scene changes in the game.  I'll have all relevant information for the graphics at that point.  Some images that may not repeat a lot I usually won't put into the atlas because it doesn't make a great difference, but for the most part I always have character textures, equipment textures, and scenery textures in there.  

There is a plethora of really in depth information about this and context switching out there if you want more detailed information.",hmambok,t3_r39ah9,1638034105.0,False
r39ah9,"You are ABSOLUTELY right!

One big texture or 20 small textures, should be the same, problem is old OpenGL is fucked.

Basically the trick is that texture unit filling is expensive, for some FUCKED reason they didn't think games would need many textures, and many of us are still paying for this shortsightedness.

The issue is larger actually, GPUs are great once data is resident but their ability to stream data in and out is very unsupportive.

Overall writing games is so hard that packing textures is never that bad but i totally agree that we should talk openly about how this is not something we (as game devs) should really be dealing with.",hmdkt68,t3_r39ah9,1638085253.0,False
r39ah9,But isn't opengl an ever updating standard? Shouldn't some new version fix this?,hmdohu5,t1_hmdkt68,1638088085.0,True
r39ah9,New versions do fix it but they are less compatible,hmgjl2u,t1_hmdohu5,1638140052.0,False
r2ye5m,You will get completely different answers based on WHAT is being built. A static we site requires a different stack from a web service than a video game than a video editing program than a social media app. Tech are tools and a good computer scientist simply uses the tools for the jobs they were built for.,hm7sdr2,t3_r2ye5m,1637972552.0,False
r2ye5m,"If I was being pedantic (and of course I am because I’m a Computer Scientist) I would point out that this is not something you can have an opinion on. 

It’s a piece of factual information that you may or may not be able to find an accurate answer to. As someone else said it’s almost certainly HTML/JS by the numbers.",hm8740j,t3_r2ye5m,1637980014.0,False
r2ye5m,Go to job boards and sort openings by language. See which language has most openings.,hm8w63k,t3_r2ye5m,1637994192.0,False
r2ye5m,"In sheer volume? HTML, CSS, JavaScript hands down.",hm85z8p,t3_r2ye5m,1637979442.0,False
r2ye5m,"That's not a stack, that's just the front end...",hm8jni4,t1_hm85z8p,1637986622.0,False
r2ye5m,"A Stack is just a complete set of systems that can run independently in its entirety without the need for any extra jazz. Millions of websites are written without the need of a backed that do more than just display text. JavaScript is Turing Complete, so you can put all of the work client-side without issue. Yes, you're right, that's just the front end, but that doesn't exclude it from being a Stack.",hm8rlvz,t1_hm8jni4,1637991256.0,False
r2ye5m,"There is no website in the world that is written without the need for a backend. Some way and some how you have to get that website to the end user, even if that way is walking up to them and handing them a USB stick. That is what a 'stack' is, it includes the web server that is serving those pages even if once they are served they don't need any more info to or from the server.",hm8tbvk,t1_hm8rlvz,1637992321.0,False
r2ye5m,"Saying you still have to host the site doesn't really go to the OP's original question of ""What's the most popular stack"" because the backend is totally irrelevant for a front end stack. There are millions of websites that don't care how you host them, S3 buckets, GitHub Pages, whatever, so saying that a Stack is ""HTML, CSS, JavaScript, and some kind of hosting solution"" is pointless, just like it's pointless to say that you technically need a browser to interact with websites that have a UI.",hm8x7y6,t1_hm8tbvk,1637994896.0,False
r2ye5m,"Except that choice is literally what a stack is. Yes, you can swap out a SQLServer database for MariaDB for Postgres and the site won't give much of a shit, but when someone says 'what stack are you using' your choice of RDBMS is a part of that. Words mean things whether you want them to or not.",hm903ft,t1_hm8x7y6,1637996915.0,False
r2ye5m,"So you would have felt better if I said ""HTML, CSS, JavaScript, Static Hosting""? Fine then, that's my answer if it'll make you feel better. You don't need a database to have an application.

How would you answer OP's question? By rejecting its phrasing and saying it's a bad question?",hm93her,t1_hm903ft,1637999522.0,False
r2ye5m,"It's not a bad question at all, you just answered in a way that is misleading to a newbie who wouldn't know any better. Static hosting is in no way the most common way an application is served, so your answer is wrong in any event. Java EE is the most common in traditional commercial settings, LAMP/MERN elsewhere.",hm94h84,t1_hm93her,1638000311.0,False
r2ye5m,"For backend, Java and sql",hm8rjqi,t3_r2ye5m,1637991219.0,False
r2ye5m,I thought PHP was still more popular,hm9i502,t1_hm8rjqi,1638011983.0,False
r2ye5m,"Really, I didn't know java is still widely used.",hm9d1ll,t1_hm8rjqi,1638007557.0,True
r2ye5m,"In enterprise backend systems, Java is by far the dominant language still. Other languages like Golang are getting more popular for sure, but there's still a lot of Java code out there and will continue to be so for the foreseeable future",hm9hici,t1_hm9d1ll,1638011433.0,False
r2ye5m,"Yeah I've worked at a few large companies. Amazon for example is almost entirely Java. Tried and true, to deliver fast, dont have time to experiment with other tech. Also more resources to reference internally.",hmacayf,t1_hm9d1ll,1638029727.0,False
r2ye5m,.NET ecosystem,hm98t3h,t3_r2ye5m,1638003911.0,False
r2ye5m,Must be a Microsoft guy lol,hmavex1,t1_hm98t3h,1638037905.0,False
r2ye5m,"HTML/CSS/Javascript.

Backend? PHP - legacy languages always dominate volume metrics.

If you love legacy code bases, or want to be in the same position as COBOL developers are now, those are the languages to learn.",hm8fbmu,t3_r2ye5m,1637984266.0,False
r2ye5m,I have seen more Java/Spring backends judging by global contractor and job opportunities.,hm8jzw6,t1_hm8fbmu,1637986816.0,False
r2ye5m,PHP is the [overwhelming](https://w3techs.com/technologies/details/pl-php) most popular backend language.,hmau74v,t1_hm8jzw6,1638037389.0,False
r2ye5m,"Would Personal Home Page ever really be like COBOL? It’s not too different from Java or C in syntax, COBOL is sort of a different beast",hm8g91q,t1_hm8fbmu,1637984762.0,False
r2ye5m,"A decent Python programmer can side-skill in PHP in about a week to be productive, a year to be the equivalent of a senior dev. Totally different to COBOL both in terms of language paradigms and what is done with the language. The reason COBOL devs are in such high demand is because COBOL runs core banking software that is simultaneously very difficult to test and absolutely cannot, ever, ever fuck up.",hm8jxxe,t1_hm8g91q,1637986785.0,False
r2ye5m,I once did pair programming that touched my companies billing code in PHP and it felt really tough and scary. I can’t imagine doing the same but in COBOL for a massive bank.,hm9ev9x,t1_hm8jxxe,1638009155.0,False
r2ye5m,"I assume that you are asking this question to decide upon what stack to choose to learn. It would have been a lot easier if you had stated a specific niche it would have been easier to answer. Learn a general programming language like Python which you can use anywhere(web, ml, application programming, etc). If it is web, then I recommend frameworks like node.js for an easier learning curve, choose go and/or rust for a steep learning curve but better perf.",hm8tmqy,t3_r2ye5m,1637992515.0,False
r2ye5m,"as far as legacy it's xAmp (os of choice, apache for server, mysql for db, and php for code base language) based stacks but I personally don't forsee many new projects being built with this but it is what most of the internet is currently built with. 

Newer stacks that are common would be mean, mern, mevn and conversely fern, fevn, fean stack (f or m for mongodb or firebase dB, express js for server, [angular, react, or vue js for front-end framwork], and node js for server-side code-base.

Ruby on rails had its moment in the sun IMHO and is still quite valid as a framework but you could also argue is not nearly as popular as a few years ago.

In the end it has less to do with what is popular over all since that can vary regionally, and more to do with choosing the right tool for the right job. Generally speaking they all do the same thing, which is provide the devs/engineers a blueprint to fast track the build of an application. It is your job as the engineer however, to figure out which will provide the most useful functions for what you need.

also take any opinion of mine or otherwise, with a grain of salt",hma30ju,t3_r2ye5m,1638025394.0,False
r2ye5m,We’ll with cloud migration u can have several stacks to pick and choose from. Now ur apis can be made with anything and consumed together through something like aws lambda or fargate. Ur front end can be mixed as well. Really depends on what most your companies code was built with prior though,hmb1tth,t3_r2ye5m,1638040619.0,False
r2ye5m,I have never undrstood the serverless architecture. I will have to do research on lambda or fargate…,hmb4lri,t1_hmb1tth,1638041753.0,True
r2ye5m,"It’s def worth learning the basics. U can essentially set up any api u want with any architecture to be consumed as a service. So at my job we got node, spring boot, python backend apis and either angular or node front end. I believe some react as well, but not as much",hmbbv9k,t1_hmb4lri,1638044736.0,False
r2ye5m,TCP/IP.,hm9ozwd,t3_r2ye5m,1638017354.0,False
r2sjef,"I suspect part of it would be the fact that while you’re editing video, the editor has it in a format optimized for editing rather than the usual playback-optimized format. When you export it, it converts it into one such (compressed) playback-optimized format, so the output video runs a lot smoother than a video being edited would.",hm71zhm,t3_r2sjef,1637959954.0,False
r2sjef,"Hardware support for playback. H264 format and other codecs are well supported by hardware decoder in most devices. 

A video editor will open the files and read and convert to its internal format, likely a RAW video format. This takes time and a lot of CPU power.",hm751pp,t3_r2sjef,1637961378.0,False
r2sjef,This. To do complex calculations on each frame the video would be stored in a raw format. If you wanted to preview the changes being made you have to calculate each frame in real time from a raw format.,hm7pp6p,t1_hm751pp,1637971213.0,False
r2sjef,"The version of the video inside of the editor is separated into parts that make is easier to edit.   The regular version is optimized for size and playback.  When the the video is exported, the whole thing has to be converted from a list of editable files to one long file that is encoded in some regular playback format.   When the editor is playing back a version, it is converting those parts  while you are watching it.",hm868i8,t3_r2sjef,1637979573.0,False
r2sjef,"Video compression codecs work on a delta basis: the next frame’s data is often stored as the differences from the previous frame. For normal playback, you have the previous frame data, get the delta and calculate the next frame, repeat. Plus this job can be offloaded when hardware decoding support is available. For anything else: like non-linear video editing, where you need a preview of the whole video in the timeline and jump around in a non-linear fashion, the whole thing basically needs to be decoded up front.",hm9sa0e,t3_r2sjef,1638019477.0,False
r2sjef,This is better suited for /r/AskReddit,hm6myk6,t3_r2sjef,1637952897.0,False
r2sjef,"Thanks, I will post it there as well.",hm6nm8s,t1_hm6myk6,1637953205.0,True
r2rbee,The art of computer programming there’s 6 volumes I think. Often touted as one of the greatest computer science series ever written.,hmhou1r,t3_r2rbee,1638158539.0,False
r2e13s,"Each problem has two bounds. NP is a upper bound: meaning a problem is at most as difficult as NP. NP-hard is a lower bound: meaning a problem is at least as difficult as NP. NP-complete means a problem is both NP and NP-hard. If you want to show a problem is NP-complete, you need to go both directions.",hm4htyu,t3_r2e13s,1637905632.0,False
r2e13s,"> If X is in NP-Complete (so all NP problems reduce to X), and X reduces to Y, then Y is also NP-complete right?

That reduction just means that X is not harder than Y. It doesn't provide any guarantee like Y being no harder than X. For example, Integer linear programming (which is NP-complete) is easily reducible to mixed-quantifier Presburger arithmetic (which is not in NP).",hm49l2x,t3_r2e13s,1637900607.0,False
r2e13s,"Thank you, the specific example is very helpful. I'm still a bit confused though. Why would we need a guarantee that Y is no harder than X to conclude that Y is NP-Complete? 

My thinking was that all NP problems reduce to X in polynomial time, and X reduces to Y in polynomial time, so all problems in NP reduce to Y in polynomial time, thus Y is NP-Complete. Is there an error there?",hm4amel,t1_hm49l2x,1637901217.0,True
r2e13s,That means Y is NP-hard. An NP-complete problem is one that's both NP-hard and in NP.,hm4bk79,t1_hm4amel,1637901772.0,False
r2e13s,"You need further assumptions. First, when stating that X reduces to Y, it needs to be a *polynomial* time reduction (i.e. problem X can be transformed into a subset of problem Y in polynomial time with respect to the length of the input of problem X). Second, you need to assume (or prove) that Y is an NP problem. 

Recall that an NP-complete problem X is simply:
1. an NP problem
2. a problem where all NP problems can be reduced to X in polynomial time (i.e. X is NP-hard).

Intuitively, assuming Y is in NP and there exists a polynomial reduction from the NP-complete problem X to Y, Y becomes an NP-complete problem as follows:
1. For any NP problem A, reduce this to X in polynomial time. This is possible because X is NP-complete.
2. From X, reduce this problem to Y in polynomial time. This is possible since this is assumed.
3. Thus, any NP problem A can be reduced to Y (which shows that Y is NP-hard) in polynomial time, since the two-step reduction process above takes polynomial time.
4. Finally, we assumed that Y is an NP problem. This shows that Y is in NP and Y is NP-hard, which then shows Y is NP-complete.",hm4wcll,t3_r2e13s,1637916145.0,False
r2e13s,"Not necessarily.

One thing is that from your question, you only mentioned ""X reduces to Y"", but does not specify what kind of reduction it is. This I can already imply, there is no information on how difficult Y is.

To give a fun example (I like to argue by using examples), consider X = sorting an array of n integers. This is in polynomial (in P), and this is trivial to verify. Then, if I were suddenly feeling cute and decide to come up with this Y:

1. Set up an email connection
2. For each integer i, do:
   1. Compose a message; specify that the message should only arrive on the ith day after the sending date; set the receiver to self
   2. Add the message to the Message Array
3. Send everything out in the Message Array

>Listen to the incoming email connection and print out the results. On the 1st day you will receive 1 (if there exists any ""1"" in the original array), on the 2nd day you will receive 2, ... ad infinitum

Then the complexity suddenly becomes much higher, because literally another computer is required to do this job (sending the email back to you). Also, sorting integers (X -(reduce)> Y) suddenly becomes unbounded.

It is important to specify the kind of reduction you are talking about. But let's say you are talking about polynomial reduction. It still doesnt say anything about whether Y is NP-complete or not. If I find some Y which is in P (or more realistically, where Y is in EXPONENTIAL) then I would have disproved your idea.

In fact as others may have pointed out, if someone happened to find some Y such that Y is in P, then they would have solved P = NP and would become famous. It is just that no one have found such algorithm of Y and so I cannot give any examples to this. ""People can only think of NP"" does NOT imply ""there is only NP in this world"".

\-------

Actually in situations like this, it is often reversed: I want to know what kind of problem X is, and so I try my best to reduce X to some ""simplest"" problem Y, and I literally cannot get more simpler than Y. But oh look, Y is e.g. in NP! So unfortunately X will have to be in NP too.

This mindset can be useful if you happen to also want to explore undecidability. In the broader scheme of things, P, NP, EXPONENTIAL, ... all are in ""DECIDABLE"", and for Turing, one of his contribution was that he found a whole bunch of problems that were ""UNDECIDABLE"", i.e., you literally will never get any result if you tell any Turing machine (e.g. a modern computer) to ""go calculate it"". EG, ""does this program terminate?""",hm585sg,t3_r2e13s,1637926344.0,False
r2e13s,"I have been a software developer for 5 years. I have literally no idea what this means. I am not ashamed to admit it, but I am curious what this has to do with computer science(besides the obvious mathematical connection) is this an algorithm thing, or is it reference to some type of hardware configuration?",hm75zl0,t3_r2e13s,1637961813.0,False
r2cwfc,I have never heard the term data width before.,hm467w8,t3_r2cwfc,1637898725.0,False
r2cwfc,"I’ve heard of “wide data” vs “tall data” in a data science context(ie looking a tabular data, the relative ratio of rows to columns). Doesn’t really have anything directly to do with bandwidth as far as I know.",hm5biqf,t3_r2cwfc,1637929048.0,False
r2a6yz,"A BIOS software image is flashed with specialized software, and hardware that applies power to the chip and uses a chip-specific protocol to write the image.  The hardware can either be a specialized harness or board, or after installed on a full board (in-circuit programming).  That's how programming software directly onto most microchips works.  Chips themselves need ways of getting software loaded directly to them, so manufacturers provide that.

The BIOS is the software responsible for several tasks in a computer, one of which is loading the bootloader software, which locates and loads the full OS.",hm443re,t3_r2a6yz,1637897580.0,False
r2a6yz,"That was a very informative. What is a BIOS software and when you say specialized software do you mean software specifically made to write a program onto the chip in question? Finally, what chip is it written into? The CPU?",hm44isi,t1_hm443re,1637897807.0,True
r2a6yz,"BIOS software (or it's UEFI counterpart) is the software that lives directly on a computer (stands for ""Basic Input/Output System""), not a hard drive like your OS (windows or whatever).  Its usually written by the people who make your motherboard (or logic board or whatever, depending on what kind of device were talking about).  It has many jobs, but a main one is to realize when you turn the computer on and start to load more complex software (like windows or whatever).  Big oversimplification, but that's the idea.

That software is written into ROM (read-only memory) area installed on that board somewhere...not the CPU.  The CPU itself doesn't have a BIOS, it has some other really small software that helps it do it's job (called microcode or firmware usually).  A computer has several different chips/boards/components, each to do a different job, and most of them have some level of firmware that's loaded in some way similar to my first comment (usually developed by the manufacturer).  Even a hard drive has some tiny amount of software on it.

The software that does the loading is special for each exact kind of chip/board/thing, it's usually called programming software but can have any number of names.

The bottom line is that computer components get their first software on them because they were designed to be able to accept programming from the manufacturer, and another component or computer does that programming.",hm45m5d,t1_hm44isi,1637898404.0,False
r2a6yz,"After a CPU powers up, it starts executing instructions starting from a fixed address. Exactly what address, and the power up sequence leading up to, can vary based on the specific hardware. A ROM chip (there a number of kinds) that has the initial 'operating system' on it is wired up so that the memory the CPU starts executing from, corresponds to the address of that chip. 

For the last 25-35ish years, that initial 'operating system' is a small program that generally called a boot loader. It is very small, and able to just enough to to read the operating systems from disk into the memory, and then jump to the start of that memory. 

That's the generic idea of how the hardware works. How'd the text of code to get be compiled code on that ROM chip? Someone typed it into a computer, used a computer to compile it, and a computer to write it to a ROM. 

Of course, if you go back far enough, you'd have a time where you had a computer you wanted to run something, but not a computer to write/compile your program separately on. Initially the programs were translated to their binary representation by hand, and the bits were loaded into a computer manually. Literally set a series of 8 switches to the bit values of the byte you want, then press a button to load it to memory and more on the next byte. 

There has been a long evolution of tech between manual switches and using a fully functional, separate computer. It should be pretty obvious that once you have a computer that will do part of the tedious process for you, you'd use it. You'll find this progressive boot strapping of technology all over engineering. You build tools, so you can build better tools.",hm41xce,t3_r2a6yz,1637896418.0,False
r2a6yz,"When we load the operating system onto our memory for operations, we call that process 'bootstrapping' or 'booting', hence, we are 'booting' up the computer. Booting is done via a 'boot loader' program.  This program is what we are interested in here, the bootloader changes depending on what kind of device we are working with. In modern devices, the boot loading process is sophisticated, so we employ multi-stage bootloaders to be able to boot up a system using chain loading.   
As the name 'chain loading' suggests, your boot loader loads up something and then proceeds to load another boot loader which does the next process. 

Your bootloader firmware is installed together with the BIOS, in non-volatile memory (basically your ROM). This is what you'd call a stage 1 bootloader, it simply starts up, detects your boot device, and then moves to the next stage. (The BIOS does a lot more at the initial stage, but we aren't concerned with it for now).

The Master Boot Record is the first disk block and this is where the real booting process starts. This is the very beginning of your disk partition. The MBR stores the first stage boot loader and the disk partition table.

After the BIOS phase is completed, MBR starts to scan the partition table and loads up the Volume Boot Record, VBR is basically what partitions (and what kind) this particular disk has.   


Just like MBR, VBR's initial segment has what partition type and size the partitions in this particular volume are, followed with the Initial Program Loader (IPL).   
This IPL is our second stage bootloader, which is basically coded to actually load our operating system. (Depending on what kind of operating system you are using, IPL will load up another program that will actually bring the contents of the operating system to the main memory and make it ready for execution).    
Like for example, IPL will load up the NT loader for windows which actually loads up the content of the windows operating system. Or IPL will load up GRUB for Linux.  These would then follow different stages to load up the operating system. 

All in all, it's a complex chaining process that starts from static code and then points you to another location where more sequence of actions are stored. 

(Also most machines now use UEFI over BIOS).  
(Another fun fact, this process can be abused to load up malware or even prevent the completion of the bootloader process. A lot can be done with this since the initial code is actually a static sequence, and if able to tinker with it, the threat actor has a lot of power) 

ps: sorry if this doesn't make sense xD",hm4tkkn,t3_r2a6yz,1637913935.0,False
r2a6yz,This guy has it sus succinct and accurate,hm5jvh5,t1_hm4tkkn,1637934549.0,False
r2a6yz,The motherboard already includes software: UEFI.,hm3j11z,t3_r2a6yz,1637886448.0,False
r2a6yz,How does the software get onto the motherboard and on which chip is it located?,hm3j6i2,t1_hm3j11z,1637886529.0,True
r2a6yz,Guy in my dreams 2-4 days ago said there were machines that write onto motherboards and SBCs similar to how we used to burn data to read only DVDs.  I was thinking about this lately which is why I dreamt it,hm3qs0a,t1_hm3j6i2,1637890505.0,False
r2a6yz,"Bios/UEFI is firmware, not software.",hm58fm6,t1_hm3j11z,1637926580.0,False
r2a6yz,Do you think that is a helpful distinction in this context?,hmxsvg8,t1_hm58fm6,1638457622.0,False
r2a6yz,This is a hard concept to grasp without first understanding BIOS and firmware. I would look up some YouTube videos about them. At the airport right now but when I have some time I’ll come update with some links,hm4vprj,t3_r2a6yz,1637915628.0,False
r2a6yz,They burn a small program into Read-only Memory (ROM or EEPROM) that does some initialization and look to read in the OS when the computer is first turned on.,hm3t7cf,t3_r2a6yz,1637891790.0,False
r2a6yz,">that

Did you mean ""then"", or did you mean ""that does""?


What is the ""initialization""? Does it just mean set up?


Finally, thank you very much.",hm3upzs,t1_hm3t7cf,1637892590.0,True
r2a6yz,"Yeah, it should be ""that does"", I fixed it now.

As far as this question is concerned, ""initialization"" just means telling the CPU where to find the instructions that load & boot the OS. It used to be done by having the CPU go to the Master Boot Record which would contain the essential info for getting the OS booted (along with some other stuff). This is now considered legacy BIOS and most systems use UEFI (of which I know less about).",hm4g56f,t1_hm3upzs,1637904561.0,False
r2a6yz,"Bootloader (BIOS in PC context, firmware in others) is basically a mini-OS that can read/write disc, it allows for installation of bigger OS.

How do you get a bootloader onto your computer? Depends on the architecture:

* in x86, it comes in it's own chip on your motherboard
* in phones/embedded, it could be the first partition of the main storage disk. To install first time, you take out the disk and write to it with another device. Afterwards, the bootloader can support self update through something like TFTP or reading specific file on a removable disk. (Hence the reason, you can brick the device by bad update)",hm5gyx9,t3_r2a6yz,1637932800.0,False
r2a6yz,"The computer has rom (or read only memory) that gets loaded with instructions for how to boot

The data gets loaded on electronically, ie the chip has pins that they use to load data onto it in the factory so it already has the data on it when it gets assembled and sent to you",hm70jej,t3_r2a6yz,1637959284.0,False
r2a6yz,">The data

Did you mean the ROM data?",hm73q9e,t1_hm70jej,1637960765.0,True
r2a6yz,We have this post every day.,hm45vmz,t3_r2a6yz,1637898544.0,False
r2a6yz,"The best way to understand it, is to DIY with linuxfromscratch. The experience of putting your own OS together is satisfying.",hm58tdh,t3_r2a6yz,1637926903.0,False
r27hds,I would imagine most of the power would be spent storing the entirety of Wikipedia rather than displaying/reading the files,hm3ayev,t3_r27hds,1637882297.0,False
r27hds,Doesn't take any energy to store things at rest in a non-volatile medium (aka disk). Shuffling it to and from the disk is what uses energy.,hm3biuv,t1_hm3ayev,1637882584.0,False
r27hds,"Good point, that makes sense",hm3bpbi,t1_hm3biuv,1637882674.0,False
r27hds,"physically, why does it take more power to store more text?
edit: assuming the text has already been downloaded",hm3d7vt,t1_hm3ayev,1637883438.0,True
r27hds,"For something like this (long term archive?) I would look at technologies known to last a very long time, and work backwards to find the needed power.

I'm not very well versed in hardware, but my first thought is something like a mask ROM, where the data isn't editable, but also is burned into the structure of the chip.  I've got games cartridges with this tech that are 30 and 40 years old with no signs of deterioration at all, I don't think 100 years is a stretch.  I don't think it's very memory sense, and probably not low power either (5v TTL? Idk) but there must be a more modern iteration that is more dense and efficient but similarly reliable, and I don't know if off the shelf, consumer grade 3.3v flash memory on an Arduino will survive that long without data errors. 

For display, you could use something like a 1-line character LCD, like used in simple solar calculators, or even an e-ink display.  I know those are fairly low power, and last decades with care.",hm5f8cp,t3_r27hds,1637931668.0,False
r27hds,"For display, eink is likely your best bet. It only needs power to change. It does not need any power once the image is ""set"".

DNB batteries are snake oil for any general usage application. They output extremely low power over an extremely long time. They are not meant for random bursting use like a consumer device. Unless you are planning on making extremely low power, extremely long life, extremely feature limited, devices that do exactly nothing 99.9999% of the time then it's best to forget they even exist.

Pie-in-the-sky theoretical ""wonder technologies"" almost never pan out, and when they do it's almost never anywhere close to the imagined uses of how the mass media reported on it.",hmblflb,t3_r27hds,1638048818.0,False
r27hds,thanks for the explanation. would it be more practical to use a solar panel?,hmbm1xy,t1_hmblflb,1638049087.0,True
r27cn1,Honestly just creating a program that operates with these equations would be really cool. I would love to see a project like this,hm39zas,t3_r27cn1,1637881793.0,False
r27cn1,What would such a program actually do?,hm64kb4,t1_hm39zas,1637944719.0,False
r27cn1,Everything that can be expressed in binary equations. So everything a normal computer can do.,hm6megs,t1_hm64kb4,1637952639.0,False
r27cn1,... just slower and more complicated. Got it!,hm6n2y9,t1_hm6megs,1637952955.0,False
r27cn1,"So it appears to make sense! 

I absolutely love this. It reminds me of the sort of stuff I'd doodle on slow nights at work. That said, I'm not gonna choose to check your adder.

In terms of a turing machine, a turing complete system can handle recursive functions, such as ackerman's function. I'm struggling to see how it could be used to perform recursion when there's no actual mechanism to perform a branch (like an if). I'd therefore propose that it's probably not.

I can't think of any way of using this encoding of these gates that wouldn't stop.",hm3anqy,t3_r27cn1,1637882145.0,False
r27cn1,"Yes, this can only encode acyclic graphs, i.e. trees.

Yet even then you can not build a Turing machine out of finitely many bits",hm3z6i1,t1_hm3anqy,1637894959.0,False
r27cn1,"That's a cool concept, thank you. Your NOR is wrong though, it should be

1-(x+y-xy) = 1-x-y+xy

With your formula, 0 NOR 1 would be 2.

You've got the same error with XNOR.",hm4wvud,t3_r27cn1,1637916579.0,False
r27cn1,"    x=0,y=1:
    OP:        1-x+y-xy => 1-0+1-0 => 1+1 => 2
    Corrected: 1-x-y+xy => 1-0-1+0 => 1-1 => 0

Edit: Fixed above block, now showcasing the difference between OP's and u/Lornedon's; some signs are flipped.",hm5cpfx,t1_hm4wvud,1637929933.0,False
r27cn1,"The formula OP had in the post for NOR was 1-x+y-xy.

They two formulas mentioned in the correcting comment look the same because they are the same. That's why there was an equal sign between them",hm5e0w6,t1_hm5cpfx,1637930861.0,False
r27cn1,"Gotcha! I see where I went wrong; the one in the OP is almost the same, just some signs are flipped. Cheers for pointing it out",hm5jex4,t1_hm5e0w6,1637934278.0,False
r27cn1,Crap I will be correcting this along with a couple of other errors in my sums. Thanks for letting me know,hm6yb8s,t1_hm4wvud,1637958234.0,True
r27cn1,So 1+1+1 == 43?  What am I missing?,hm4hjki,t3_r27cn1,1637905448.0,False
r27cn1,"1+1+1+8\*1\*1+8\*1\*1+8\*1\*1+16\*1\*1\*1A note I could have probably added in was that 7xy doesnt mean 710 or 711 it means multiply 7 by x by y

I am just reckoning this is where you went wrong

Or I just totally messed up my 4 bit adder and honestly it is probably my error",hm4s28e,t1_hm4hjki,1637912800.0,True
r27cn1,"    1+1+1+8*1*1+8*1*1+8*1*1+16*1*1*1 == 1 + 1 + 1 + 8 + 8 + 8 + 16

    == 43

So like I said, 1+ 1 + 1 == 43.  What am I missing?",hm4ubfy,t1_hm4s28e,1637914509.0,False
r27cn1,"I think the carry bit z should be input as 2 and the result should be 100 (i.e. binary 4). However, that doesn't quite work, and nor do most other test cases:

    x + y + z + 8xy + 8xz + 8yz + 16xyz
    
    x=1,y=1,z=2?
    1 + 1 + 2 + 8 + 16 + 16 + 32
    76 => not binary :(
    
    x=1,y=1,z=0
    1 + 1 + 0 + 8 + 0 + 0 + 0
    10 => 2!
    
    x=1,y=0,z=0
    1 + 0 + 0 + 8 + 0 + 0 + 0
    9 => not binary :(
    x=0,y=1,z=0
    0 + 1 + 0 + 8 + 0 + 0 + 0
    9 => not binary :(
    
    Attempt to treat the carry as 1 or 2, x and y are 0:
    0 + 0 + 1 + 0 + 0 + 0 + 0
    1 => 0 + 0 + 2 != 1
    0 + 0 + 2 + 0 + 0 + 0 + 0
    2 => This works

Must be something a bit off somewhere! Perhaps the next iteration of these equations will sort it out these issues (or we're both reading it wrong!).

EDIT: Realised I messed up my cases where the z was set but x/y were 0; have fixed them now and found the correct value for z is 2. Still doesn't quite work when adding 1 + 1 + 2, sadly",hm5cc72,t1_hm4ubfy,1637929663.0,False
r27cn1,"I think the intention is to make functions which take decimal inputs that look like binary, then emulate logic systems even when they have multiple inputs and outputs.

If we write `1001` in binary, we're talking about `9` in decimal. But here, we want `1001` in *decimal* as a result of a binary system with 4 outputs, using decimal math.

The whole thing doesn't seem particularly useful. Admittedly, I'm having trouble sifting through the rah-rah here, the OPs word salad, and the math errors, but that's my takeaway.

If we think of [a full-adder](https://en.wikipedia.org/wiki/File:Full-adder_logic_diagram.svg), we have two inputs X and Y, plus a carry input Z. (I'm using the OPs input variables, the image I linked uses A, B, and C instead.) It outputs two bits: sum and carry, which you can conveniently think of as a two digit binary number with carry as the MSB and sum as the LSB.

The goal appears to be to write a function that does this all in *decimal*, such that `f(1,1,1)` for example, results in `11` *decimal*. The function would have this truth table: 

X|Y|Z (Carry)|Result
:--|:--|:--|:--
0|0|0|0
0|0|1|1
0|1|0|1
0|1|1|10
1|0|0|1
1|0|1|10
1|1|0|10
1|1|1|11

It's important to remember these functions have very limited domains: `x` and `y` and `z` can be 0 or 1, and absolutely nothing else ... even though the function definition looks like a plain old polynomials over the reals.

The function given for a full adder:

> Heres an adder that takes a carry and 2 bits and outputs the binary in:
> x+y+z+8xy+8xz+8yz+16xyz

is pretty obviously incorrect, as we're finding.

I think we're a lot closer with this (tested):

    x + y - 2xy + z - 2z(x + y - 2xy) + 10(xy + z(x + y - 2xy) - xyz(x + y - 2xy))

which simplifies a bit (untested):

    y + z + 8yz + 10yzx^2(-1 + 2y) +  x(1 + y(8 - 16z) + 8z - 10zy^2)

[This Google Sheets spreadsheet](https://docs.google.com/spreadsheets/d/1PCjgBgT8Dq2L6hh5oB_iGN440q2mbXe4CaQzqQ9D51M/edit?usp=sharing) tests that expression, and it looks a lot better than the OPs original to me.",hm68jtv,t1_hm5cc72,1637946456.0,False
r27cn1,">The whole thing doesn't seem particularly useful. Admittedly, I'm having trouble sifting through the rah-rah here, the OPs word salad, and the math errors, but that's my takeaway.

I love how you turned my garbage writing in to a food outlet, I should start a business about word salad

You are correct that the aim is to take Denary numbers that look like Binary and add them in Denary to give an output that looks like Binary

As far as math errors goes I believe that I only messed up which way round the + or - signs go

`x+y+z+8xy+8xz+8yz-16xyz` 

Does work and is far more compact then the solution you had, mostly because 0\^2 or 1\^2 are totally meaning less and since xy and z all are ones and zeros you will find that you can just remove them and then cancel down.

I will be correcting my mistakes in an edit",hm6zii0,t1_hm68jtv,1637958808.0,True
r27cn1,Great! Happy that I could help you find something that works.,hm78jzm,t1_hm6zii0,1637962996.0,False
r27cn1,I totally messed up and forgot that you must deduct the sixteen not add it,hm6ygwa,t1_hm4ubfy,1637958309.0,True
r27cn1,"Lol. I remember doing this in first year. If you allow inputs to vary smoothly between one and zero, you end up with basic probability, but you lack an easy way to model dependant variables.",hm6dge9,t3_r27cn1,1637948622.0,False
r27cn1,"But that modeling is the art of it. Really, these constructs can end up being involved in simple machine learning models ... as long as you can correctly fit your data into the [0, 1] continuous range in a meaningful way.",hm7a5x4,t1_hm6dge9,1637963738.0,False
r27cn1,">could we write a Turing machine like this?

Yes! In fact, you can simulate any logic using solely NAND gates as they are functionally complete. An arbitrary amount of NAND gates could be used to implement any possible algorithm based on binary encoding.",hm4gi9p,t3_r27cn1,1637904793.0,False
r27cn1,"I think turing completeness != functional completeness

You can implement every possible boolean function using NAND gates, but implementing a turing machine is a different thing",hm5dkp8,t1_hm4gi9p,1637930546.0,False
r27cn1,How is a latch represented in this schema?,hm4hpfc,t1_hm4gi9p,1637905552.0,False
r27cn1,"I did at one point fiddle around with trying to write a latch

I ended up using the answer system written into the calculator 

so Ans + 1 = Ans gives you a value that increments over time incrementing by one",hm4sy4e,t1_hm4hpfc,1637913458.0,True
r20mys,"You can reach out to the researchers, you can post on stack exchange, you can go learn the math that the paper is using on Khan academy or something.

Those are my best suggestions.",hm1za86,t3_r20mys,1637860883.0,False
r20mys,"Thank you for the reply. I have a few questions.

Do you think it matters how much I know on the subject before I reach out to the researchers? I want to move on this quickly, but I’m also worried they may not wish to engage if they believe I’m novice in the subject. Not sure if that’s common.

Do you recommend exploring anything on stack exchange in particular? I’ve used stackoverflow plenty, but am not familiar with its cousin.",hm5jdh2,t1_hm1za86,1637934254.0,True
r20mys,Yes. Eastern Promises is brilliant. Second this. For a long time ago but it didn't run as expected.,hmdruai,t1_hm5jdh2,1638090767.0,False
r1xcw9,"At this time, it is not only not possible, we don't even know if it is possible. Or as I like to say, not only do we not have a path to ASI, we don't even know if such a path exists. AI, as it currently exists, is simply a computational tool (or aide) for certain types of problems.",hm1aokc,t3_r1xcw9,1637849707.0,False
r1xcw9,Ok thank you so much. Do you know if aurora21 is anything like ASI or what even is it?,hm1b9st,t1_hm1aokc,1637850004.0,True
r1xcw9,"It is just a brand (in a sense) of a supercomputer. The project they did is to map the connections in a brain, which is very complex and requires a lot of computing power. It would be like mapping the connections made by every road, sidewalk, path, railway, etc. But it is just a map. The goal is to be able to understand how different structural connections relate to different conditions. E.g., can we diagnose Alzeimer's earlier by scanning the connections in the brain, hence treat it earlier; thereby, improving outcomes.",hm1bily,t1_hm1b9st,1637850130.0,False
r1xcw9,"Ok cool, so it dosent take into account the functions of connections? Could that really be done in 3 years, i heard 30 before?",hm1c6u1,t1_hm1bily,1637850466.0,True
r1xcw9,"Maybe, but unlikely.

ASI has been 10 years away for about 60 years. :)  As I said above, there is no known path to ASI right now. Could somebody discover it tomorrow? Yes. Is that likely? No.

Also, the dangers of an ASI are greatly overexaggerated. First, an ASI would have to be hostile. It is not certain that would be the case. So, a hostile ASI could cause a lot of disruption, but it has no way to cross the physical divide, so there are extreme limits to what it could do.",hm1cz57,t1_hm1c6u1,1637850873.0,False
r1xcw9,"> Also, the dangers of an ASI are greatly overexaggerated. 

The dangers of an ASI would be very real, although I agree there is no clear path toward one.

> First, an ASI would have to be hostile. It is not certain that would be the case.

It doesn't need to be hostile per se, any slight misalignment would have drastic consequences. [Accurately aligning advanced AI systems is a difficult unsolved problem.](https://youtu.be/IeWljQw3UgQ)

> So, a hostile ASI could cause a lot of disruption, but it has no way to cross the physical divide, so there are extreme limits to what it could do.

[It's impossible to effectively sandbox an ASI.](https://youtu.be/i8r_yShOixM?t=305) It would be a better manipulator than any human who ever lived. By communicating with its human operator, it impacts the real world and crosses the physical divide. Even our current dumb AI systems are scarily good at manipulating humans (e.g. social networks maximizing engagement).",hm1jry8,t1_hm1cz57,1637854137.0,False
r1xcw9,"1. While there is of course no upper bound on the potential danger that can be caused by an ASI, if we look at it through a realistic lens, then the dangers are overexaggerated.
2. Hostile, within the context of AI, means unaligned or incompatible with human desires or needs.
3. RE: AI as a master manipulator. There's no indication that this is necessarily true. Our current AI systems are not really that good at manipulating us. Humans are good at creating systems that use AI as a computational tool to do such manipulation. These are vastly different things.",hm1p9f5,t1_hm1jry8,1637856581.0,False
r1xcw9,"Disagree. The fundamental root of computation is Binary decision making. I’m not sure what the fundamental structure of an official first and functional ASI will be (quantum , etc). However, an ASI likely does not conform to our human concept of ‘hostile’ . Take the anthill scenario for example 

The TRUE danger is if we allow it to control our governments in the name of computational precision. Control our nuclear weapons, etc. Above this, we do not have the ability to even fathom the repercussions as it is outside of our precedented human range of understanding and mental capacity",hm39vgc,t1_hm1cz57,1637881740.0,False
r1xcw9,"You are quite welcome to disagree.

I don't really understand your first paragraph. It does not make much sense to me. I think perhaps you are misunderstanding the term hostile as it applies to AI ethics. As I posted elsewhere, hostile in that context means unaligned or incompatible with human needs or desires. It does not imply maliciousness or anything to do with ants.

As for the second paragraph, this same can be true for any safety-critical piece of software. Flawed software can have serious repercussions whether AI-based or not. As for the last sentence, this is simply fundamentally flawed (in my view anyway) and based on science fiction or pure speculation (usually from non-experts, such as Elon Musk). ASI does not mean that it can do everything, and it certainly does not mean it can do the impossible. It is in fact quite possible to examine ASI in a scholarly way by making reasonable extrapolations based on what we know about AI. Of course, even such work is speculative because we do not really know much about ASI (see my other posts), but at least it is justified by existing literature.

If you're really interested in this, then I'd suggest looking at some works on AI ethics. There are some good works on ASI as well. Nick Bostrom as written some works on the dangers of ASI (which I personally feel are flawed), and then there are a number of good rebuttals to his arguments. So this is a good place to start.

[https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=ai+ethics&btnG=](https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=ai+ethics&btnG=)

tps://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=superintelligence+bostrom&btnG=&oq=Superintelligence+Bostr",hm47otl,t1_hm39vgc,1637899530.0,False
r1xcw9,"Thanks for sending, I’ll give them a look. And yes I misunderstood your application of hostile. Sure, most software has risk and security related concerns and implications, AI based or not

My argument is that the potential caliber of ASI, when applied to certain dimensions or areas, could potentially have indefinite negative repercussions, and we lack the capacity to even forecast such repercussions. Sure, non experts will fear monger with what is built on speculation. So: the dangers of *current* applications of ASI  are more or less benign. 

Please allow me to illustrate my thought process. I believe that there is power and greater understanding with dimensions. Example: our human existence is likely tied to 3 dimensions. We most likely do not live in a 3 dimensional universe, in fact, maybe at least 4  (a reality with more than an x, y, and z axis). I’m guessing that we are likely unable to comprehend the laws of higher dimensions as we are built with / possess the syntax of 3 dimensions (likely. Unless our body is 3 dimensional but our mind is not. I don’t know). Im saying all of this to demonstrate our lack of understanding as humans. We don’t want to create something which exceeds our ability to a highly significant extent, where variables can’t even be assigned as we lack the ability to detect such variables. Moreover, assign critical roles to said technology. So long as most variables are understood and data is established, the technology is ok to deploy. I hope this makes sense",hmcuson,t1_hm47otl,1638069839.0,False
r1xcw9,And then you have the issue of discovering variables as you deploy the technology into the environment,hmcv352,t1_hmcuson,1638069984.0,False
r1xcw9,"No. *The* premier whole-animal conectome simulation is [OpenWorm,](https://openworm.org/) which is currently working on simulating a less than 1000 cell animal. 

> OpenWorm aims to build the first comprehensive computational model of the Caenorhabditis elegans (C. elegans), a microscopic roundworm. With less than a thousand cells, it solves basic problems such as feeding, mate-finding and predator avoidance. Despite being extremely well studied in biology, this organism still eludes a deep, principled understanding of its biology. 

Despite the organisms simplicity, we have failed to simulate it. There are about 86 billion cells in the human brain, and they are far more complex than the worm's cells. 

You should only start worrying about ASI when you see, for example, a brainless mouse hooked up to a supercomputer and doing mouse things.",hm235in,t1_hm1c6u1,1637862477.0,False
r1xcw9,"Thanks for the reply.

Does the human brain itself not prove the possibility of general intelligence? If brains are nothing more than huge information processors, then given enough time(a thousand years is nothing on the cosmic scale) we will eventually be able to mimic it's full architecture digitally, no?",hm1fkc9,t1_hm1aokc,1637852126.0,False
r1xcw9,"No, it’s not clear at all that biological brains (human or otherwise) can be modeled digitally.  We do know that computers can implement any algorithm, but we don’t know that everything a brain does is algorithmic. And we do also know that certain problems cannot be solved algorithmically (e.g., the halting problem).  Furthermore, for the few things the limited AI of today can do, we know that computers accomplish many of those tasks differently than humans.",hm1ggg2,t1_hm1fkc9,1637852569.0,False
r1xcw9,"What ComputerSystemsProf is accurate. We know that general intelligence is possible; however, as they pointed out computers seem to think differently than us. So this raises some interesting questions as potential paths to general intelligence:

1. How can we make a computer generally intelligent without duplicating human intelligence?
2. How can we make a computer duplicate human general intelligence?

We do not really know if either of those can be done. We can ponder the question, but there is no really clear direction to the goal. Of course, some people have some ideas (general AI is one of my side projects, so I happen to believe it is possible) but to date, none of them have really panned out or again, kind of provided a very clear ""Oooooooh"" moment, that's how we can do it.",hm1hejp,t1_hm1fkc9,1637853025.0,False
r1xcw9,"Well the good news is that we are very likely decades away from real artificial intelligence.

What is often called AI by buzzword salesmen is more properly called machine learning, though even that is a but presumptuous.

The way most of it works is by randomly trying out curves on a graph again and again until one or more of the models guesses the right answer. 

That model is then used and sold as AI even though its really just guessing some fancy math equation that would just take a while to do by hand.

The model of the brain probably cant be used to simulate a mind yet, we don't fully understand how neurons work fully let alone the nervous system as a whole.

Also while MRIs can monitor brain activity (local only, cant be done remotely) they are a long way from being able to read thoughts and especially put them there.

Lastly we don't even know the a true general AI is even possible and the definitions even gut a but blurry.

 I prefer Virtual intelligence to describe a system that appears intelligent but isn't true a thinking mind (""agent"" in philosophy); its only AI once it is properly a novel mind, e.g. displays sentience, sapience, self-awareness, consciousness, and intelligence.

Tldr: most of what is called AI really is just not; they are programs that excel at pattern recognition and functional optimization, but they cant actually think and adapt like a real intelligence. Mapping the brain is only one piece to the puzzle and decades more work is needed; As a result planting thoughts is almost certainly not possible at this time.",hm1ytep,t3_r1xcw9,1637860683.0,False
r1xcw9,"You have people with PhDs in the comments telling you everything you need to know. But most importantly, whoever got you on this path of worrying about AI, stop listening to them. Take care of yourself.",hm5lxmk,t3_r1xcw9,1637935705.0,False
r1xcw9,"The fuck? You have Schizophrenia. The literal last bullshit you should be worrying yourself with is artificial intelligence. Look, our AI amounts to some calculus and linear algebra -- it's a neat accounting trick for estimating functions. There is zero intelligence involved, and AI is a misnomer title. 

We are so far from general AI that we literally don't even have a vocabulary set to properly even DISCUSS the topic.

Even if we did, creating a ""brain map"" is worthless with regards to creating general AI systems. The fact that a  ""supercomputer"" is doing this ""mapping"" is not relevant to AI: biologists want to know about human biology, they aren't trying to build AI systems with that data.

I can't stress this enough: You know you have Schizophrenia, and you have to already be aware that these types of delusions regarding AI are unhealthy and inappropriate. Don't feed these types of thoughts to the point where you have to reach out to people to confirm your suspicions are incorrect.",hm23m0n,t3_r1xcw9,1637862664.0,False
r1xcw9,Preach brother nice comment,hm2o7v3,t1_hm23m0n,1637871460.0,False
r1xcw9,How is reaching out to informed parties not exactly what we should recommend to individuals with doubts and suspicions -- even doubly so for those with mental disabilities?,hm3t59y,t1_hm23m0n,1637891760.0,False
r1xcw9,Or just dive in with some pkd,hm383o1,t1_hm23m0n,1637880855.0,False
r1xcw9,"I suggest learning about computers and programming a bit yourself.

You'll see very very quickly that computers are dumb as bricks and that we're extremely far away from anything remotely resembling agency.",hms3uhh,t3_r1xcw9,1638353808.0,False
r1lsow,"Most of the time, we talk about worst-case complexity. We also only really care about large data sets. In those scenarios, the highest order function dominates lower ones, ie if a function is say... O(nlog(n) + n\^2) then we can say that it's really O(n\^2).

Assessing complexity is mostly about the relationship between the algorithm and the growth rate of data. An algorithms class shouldn't teach you memorization of the major algorithms in different areas, or the runtimes of those. Instead, it's about learning how to develop and assess algorithms, and the different strategies that led to the development of the major algorithms. It should also teach you how to reduce problems down into simpler ones, or transform them into a similar problem with a known or easier solution.

The overall complexity of your algorithm will depend entirely on the most complicated step, which is your for-loop that will run the sqrt(n) times.",hlzl113,t3_r1lsow,1637809626.0,False
r1lsow,"Makes sense, we take the worst case of all discrete cases and use that. Thanks.",hlzuxe1,t1_hlzl113,1637814540.0,True
r1lsow,"Additionally, it is worth noting that asymptotically what you are doing with the 6n + 1 case is improving the best case of the algorithm without reducing the worst case (which is likely not possible in the general case).

This means that without the optimization you would be able to characterize the algorithm as having a complexity of Theta(sqrt(n)), but with the optimization your best case becomes Theta(1), meaning the complexity of the overall algorithm becomes Omega(1) and O(sqrt(n)). This kind of thing is pretty common in algorithms, where you can solve a specific subset of problems really quickly, but cannot solve EVERY possible problem quickly.

Asymptotics can be confusing because they are really about mathematical functions, not just algorithms, but the general logic to follow is that you cannot say you are reducing the runtime of an algorithm unless you do it for all possible inputs.",hm0azxc,t1_hlzuxe1,1637823899.0,False
r19gul,"Here you go:

https://www.cs.usfca.edu/\~galles/visualization/",hlz7i6q,t3_r19gul,1637803534.0,False
r189ka,I would love this as well!,hlze5en,t3_r189ka,1637806499.0,False
r189ka,"Well, writing such a book is dangerous 😂",hm03r38,t3_r189ka,1637819364.0,False
r0ss7l,"OH, I think I know why. Is rosetta converting apps before running them into code it can run, whereas running linux in a VM is a constant task? Even then, there are still questions.",hluexve,t3_r0ss7l,1637717255.0,True
r0ss7l,"Rosetta is doing several different things that simply ""run Windows or Linux in a VM on Apple silicon"" can't easily do.

- First, as you say, there is ahead-of-time recompilation of x86 code to ARM code to allow apps to run without being interrupted by a just-in-time recompiler.
- The M1 CPU has custom features for emulating the x86 memory model, which aren't part of ARM normally. Rosetta uses these in its recompilation.
- Dynamically linked system libraries need no translation, so many components are still running natively, not being recompiled.

By contrast, a VM probably can't compile ahead-of-time, and may not even be able to JIT much without choppiness and memory overhead, it has to translate the entire running OS rather than just a single userspace binary, and it may not be able to access certain internals of MacOS or of the M1.",hlv5pk6,t1_hluexve,1637730489.0,False
r0ss7l,"It's called [Rosetta 2 because it's a *translation system*](https://en.wikipedia.org/wiki/Rosetta_Stone) between different processor instructions. Translating a book ahead of time is a lot less of a hit on overall performance than translating each word as you go along. That's why the first launch of a Rosetta-run app is slower, but subsequent runs are much faster.

https://en.wikipedia.org/wiki/Rosetta_(software)",hlx2bii,t1_hluexve,1637771947.0,False
r0ss7l,"AOT vs JIT essentially, and Apple have the time and resources needed to make it as performance as possible.

Basically Mac translates the app as much as it can, and then emulates the bits that aren't already translated.",hlv4ieg,t3_r0ss7l,1637729815.0,False
r0ss7l,"Because an app isn’t a whole OS. An operating system is a kernel, drivers, modules, tons of apps and schedulers and timers all working together.

QEMU or whatever emulator you use has to actually emulate a working x86 processor, reserve contiguous memory etc, not just a subset required to run a single app, but every single instruction in a continuous loop, translate it and run it typically using a smaller instruction set. The x86 instruction set is very complex.

It’s why emulating a Power or MIPS processor on x86 to boot Linux also runs like dog shit even though the design is decades old.

There are people working on getting Linux working natively on Mac, that will save a lot of that translation step.",hlvdn8x,t3_r0ss7l,1637735252.0,False
r0ss7l,"I'd say those are two entirely different kinds of problems.

Running code compiled for one instruction set architecture on a different ISA usually comes at a steep performance cost since perhaps the most straightforward solution -- emulating the other CPU with software -- is slow. I don't know how Rosetta works in detail, but since it's either JIT or AOT (ahead-of-time) compiling the x86-64 machine code to its native ARM code, and the compiled ARM code is then run on the CPU, it can achieve much better performance than outright emulation.

Other than that, an application running through Rosetta is still code that has been built to run on an Apple OS, running on an Apple OS. Apple knows all the system calls and any other macOS APIs that an application can make use of; it's their design after all. Since they (probably?) still have the implementations of those same APIs in their OS, now just running on ARM, Rosetta can just mechanically make whatever modifications are required to the machine code so that the system calls now work on the ARM version, and re-link the application executables with the ARM version of any library ABIs provided by the operating system, and whatever else they need to do. (I don't know if re-linking is what they're doing, and I'm speculating, but the point is that once they've got the translation logic done right and all the corner cases working, it's a mechanical translation. Most of the APIs themselves are probably still identical in content. The implementation of the binary interfaces just differs, and the difference is systematic.)

It's still a nice piece of work, but Apple essentially knows and controls all the pieces of the puzzle. They also had time to prepare.

Supporting a modern operating system on a particular piece of hardware is a different thing. An OS needs to be able to work with the specific hardware on which it's running. This requires not only compiling the code for the correct CPU instruction set, but also being able to properly communicate with any other device-specific hardware such as the GPU (integrated in case of the M1) and the motherboard chipset. A modern operating system also needs to deal with power saving modes for all of that, and so on.

Writing the code to do this, not to mention figuring out how the hardware works exactly if no detailed public documentation is available, is a lot of work that has to be done for each individual component that a machine might have. Also, I don't know what if any documentation regarding these on the M1 is publicly available, but Apple isn't exactly the kind of a company to go out of their way to release technical information they don't need to.

The same is of course fundamentally true about running Linux or Windows on any other random hardware regardless of the CPU architecture. However, the new Apple hardware brings in a lot of new components to the mix, and there probably isn't a lot of public documentation available for non-Apple developers. Also, if it were some random laptop from some random brand not working well with Linux, you just wouldn't have heard about it. You hear about the M1 because it's Apple.

--
--

TL;DR: Rosetta needs to solve running (application) code from one CPU instruction set architecture on a different CPU with a different instruction set; running Linux or Windows on the M1 comes with all the usual problems of supporting random undocumented hardware.",hlx33x1,t3_r0ss7l,1637772259.0,False
r0ss7l,"What if I told you that there's an app that let's  you run an application whatever OS you use? if you're having troubles with your emulator, I'd suggest you try [this](https://www.shells.com/l/en-US/)  since it's the closest app that I can think of that can be really beneficial for you.",hlw3rp3,t3_r0ss7l,1637755331.0,False
r0ss7l,"It will get better. 

Rosetta is translating a single application and there will be a ton of optimisation because its translating binaries for Apples own OS.

Proper Linux and hopefully Windows ports will come to ARM making this all less of an issue. Virtualising whole x86 machines will probably improve too but as the ARM versions improve it will probably make sense to emulate those so the CPU architecture is the same.

I always preferred bootcamp/dual boot for real world use, direct hardware access for graphics etc. I’d be happy with that for consumer use. 

I think for now if you have taken the plunge with M1 then you need to be happy with MacOS but options will open up. Apple are always innovators, only someone like them who can do the whole thing from the chip upwards can make it immediately beneficial, which is thanks to their closed system design. They are effectively pushing the industry forward like they did in the 70s and 80s. ‘Build it and they will come.’",hlza1bx,t3_r0ss7l,1637804659.0,False
r0ss7l,Noteseeee x84 ctm jajajaajajajajajajajaa,hlujnzf,t3_r0ss7l,1637719469.0,False
r0ss7l,"X86_64, colloquially known as x84",hlujsn0,t1_hlujnzf,1637719529.0,True
r0ss7l,"Colloquially x64, not x84.",hlv3vbc,t1_hlujsn0,1637729457.0,False
r0ss7l,"105 IQ, huh? 🤔",hlw14ws,t1_hlujsn0,1637753453.0,False
r0ss7l,Saleeee wn jajajajaa,hlujw68,t3_r0ss7l,1637719574.0,False
r0eadx,Cool idea. Do you consider English translation?,hlsnhnl,t3_r0eadx,1637690834.0,False
r0eadx,"Yes, in a few days.",hltjf2w,t1_hlsnhnl,1637703352.0,True
r0eadx,The website is now in English as well : https://www.codepuzzle.io/en,hrrr9xe,t1_hlsnhnl,1641645402.0,True
r07asz,[Wikipedia](https://en.wikipedia.org/wiki/Parallel_ATA) to the rescue!,hlu7f0q,t3_r07asz,1637713728.0,False
r02lfl,Reverse-engineer the problems existing websites solve and come up with your own solution on pencil and paper?,hlskagy,t3_r02lfl,1637689611.0,False
r02lfl,"The whole point of studying algorithm design and analysis is knowing if they work and how they behave asymptotically without having to test them first.

Do as Dijkstra, and program on paper.",hltok5n,t3_r02lfl,1637705401.0,False
qzwduc,Build different sorting algorithms. There's a reason they show up so commonly as early assignments in a CS degree. They are very good practice for working with lists and logical thinking.,hlovby3,t3_qzwduc,1637617396.0,False
qzwduc,Are you talking about bubble sort kinda stuff? I have already built almost all of them and I wanted some nice questions that show me how they'd be used in a real world scenario.,hlovtyt,t1_hlovby3,1637617604.0,True
qzwduc,You could build a basic accounting software.,hloxc0t,t1_hlovtyt,1637618224.0,False
qzwduc,"Try LeetCode webpage. It's mostly for interview questions, but there are also beginner level stuff and general DS&A problems. For example, implementing your own linked list and then reversing given one, sorting or merging two sorted into one big sorted.",hlpd6fw,t3_qzwduc,1637625210.0,False
qzwduc,This is exactly what I am looking for. Should I take one of the paths they offer?,hlrkfa1,t1_hlpd6fw,1637674358.0,True
qzwduc,You can try competitive coding on platforms like codeforces and codechef,hlr2eb1,t3_qzwduc,1637661380.0,False
qzwduc,"Try pepcoding 

Start from the basic problems of DSA 1 

even if you think that you can solve those ""easy"" problems, still put in the effort and solve everything. It'll take you 6-7 months to complete all those problems.",hlrhbq5,t3_qzwduc,1637672625.0,False
qzwduc,"Try doing a [WAVL Tree](https://en.m.wikipedia.org/wiki/WAVL_tree).

They are somewhat of a halfway point between AVL and red-black trees in terms of performance.

AVL trees have height at most 1.44 * log(n).

Red-black trees have height at most 2 * log(n).

WAVL trees have height at most 1.44 * log(n) if only insertions are performed but can get closer to 2 * log(n) if deletions are performed. It will build the exact same tree as an AVL tree if only insertions are performed.

AVL trees need at most log(n) tree rotations per operation.

Red-black trees need at most a constant number of tree rotations per operation.

WAVL trees need at most log(n) tree rotations per operation, however amortized over time it is a constant number.

Here is the original [paper](http://sidsen.azurewebsites.net/papers/rb-trees-talg.pdf) which details the operations.

As far as the difficulty of implementation I would say they are again somewhere between AVL and red-black trees lol.",hlrd1jq,t3_qzwduc,1637669983.0,False
qzwduc,Solve algo class exercises from other universities. They are usually in depth and require a lot of thinking,hlrtw88,t3_qzwduc,1637678954.0,False
qzwduc,I am in a uni myself and I do solve the assignments myself. The reason I asked this was I wanted to find individual questions giving me an idea of what can be done with a Ds concept like reversing a list. Leetscode and similar platforms seems good enough.,hlrzv5s,t1_hlrtw88,1637681537.0,True
qzwb8e,I think one month would be very challenging. Do you have an existing data set upon which to build the search?,hlotm0b,t3_qzwb8e,1637616697.0,False
qzwb8e,"So to clarify, this doesn't have to be a search engine that can handle anything I throw at it from halo to food science papers. The goal is just to demonstrate my ability to build such a feature. I think my uni can provide me with datasets but I really want to build my own crawler and parser. Of course the demo will be carefully crafted to show what the search engine can do with the right data set and I'll be completely transparent about it.",hlou0wl,t1_hlotm0b,1637616865.0,True
qzwb8e,"If you have the existing data, and it is not too absurdly big it might be possible. There's kind a couple of interesting problems for a project that would be good. For example, building an effective data structure to facilitate the search. The actual search algorithm could be interesting, especially if you incorporate some machine learning to do some recommendations of related material. It is doable, no mistake about that, but it is a pretty big project for a month. I wonder consider using an existing web crawler and focus on data storage and search. Overall, just try to keep the scope manageable. :)",hlounxk,t1_hlou0wl,1637617122.0,False
qzwb8e,"You're probably right, given how I need to take classes in this month as well, so I can't devote all my time. Plus, I can always build a web crawler later on and incorporate it in this search engine. 
Unfortunately, all machine learning stuff is off the table as I don't know anything about it and I am pretty thorough with my learning, so it'd take me at least 2 months to get a decent grip on it. I do have a course on ml, later on in my degree so I'll probably incorporate some rudimentary ml techniques in this project after that course.",hlovlli,t1_hlounxk,1637617508.0,True
qzwb8e,">Unfortunately, all machine learning stuff is off the table as I don't know anything about it and I am pretty thorough with my learning, so it'd take me at least 2 months to get a decent grip on it. I do have a course on ml, later on in my degree so I'll probably incorporate some rudimentary ml techniques in this project after that course.

When you do decide to have a look at those things, [watch some 3Blue1Brown for the high concept background information](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), and also [have a gander at one of the most commonly used libraries in the field, for the practical information](https://pytorch.org/tutorials/). I've watched the 3Blue1Brown videos and found them as useful as good university lectures even when you are just watching them to slightly increase your understanding and most of the math goes over your head*, and pytorch is a gold standard which is on my list just for the experience even though I don't use python much. If you are comfortable with such personal goals as building a search engine then this is fairly reasonable and probably worth your time.

*It certainly did for me, but I got enough of it to grasp the idea and to know that I could learn it if I took the time, which I certainly will in the very near future.

Edit: I hope it's appropriate to hand out resources like that. I'm sure someone will find them useful.",hlp7vum,t1_hlovlli,1637622803.0,False
qzwb8e,"Thanks man, I'll definitely take a look at them once I get the time or my ml course starts",hlrlbak,t1_hlp7vum,1637674825.0,True
qzwb8e,"However, if you still thinks so then let me know what you think would be a better approach?",hlouli1,t1_hlotm0b,1637617094.0,True
qzwb8e,"I would focus primarily on the data structure aspects. I think they're interesting enough for a (2nd year I'm guessing) project. Non-trivial for certain, and doable within the time frame.",hlow57j,t1_hlouli1,1637617731.0,False
qzwb8e,"Some terms you can search for: Information Retrieval system, Inverted index, PageRank, Web crawlers",hlotqb6,t3_qzwb8e,1637616746.0,False
qzwb8e,"Currently I am planning to build one using the inverted index strategy. I do want to include page rank as well. My thought process was to crawl the web, build a big enough dataset by parsing the pages I crawled, apply pagerank on that dataset and store it in order of their score. When a query happens, I use an inverted index strategy to query the webpages and display them according to the pagerank score I stored earlier.",hlougfz,t1_hlotqb6,1637617037.0,True
qzwb8e,"Hi we had a course on big data last year. To break down one approach is the following.

1. Get Hadoop set up on a system. If you have a cluster available by your school/university, definitely request access, as it will massively increase what you can do for this project.

2. Once set up, build a map reduce job. This is the most important part. When you work with big amounts of data, you need some way to quickly traverse this data, and filter only those relevant results to be displayed. An example dataset could be  found at https://commoncrawl.org. You can take an entire segment of the entire set if you can get a large cluster. NOTE THIS IS MULTIPLE HUNDREDS OF TERABYTES. Otherwise use the indexer to find a smaller sample dataset. 

3. Now how do you map reduce. The idea is simple: You have several cycles where each element is mapped, filtered and shuffled throughout the entire Hadoop cluster. These operations often can be done in parallel and are really trivial by themselves. Important is to ensure that you bring the data to the point of computation, not the other way around. Network traffic will be a large bottle neck. Instructions on how to do this are available online.

4. You could do many, many things now to optimise this process. Map reduce is by no means the end of big data. But it’s a good start, especially for a tiny project.

If this is too much for a small project consider doing a part of it, or just setting up a tiny Hadoop server with a toy example of the search engine!

Good luck",hlph3vq,t3_qzwb8e,1637627059.0,False
qzwb8e,I'd set up elasticsearch and build an interface to it,hlp56pt,t3_qzwb8e,1637621612.0,False
qzwb8e,"Correct me if I am wrong, but that is already a search engine right?",hlpu50c,t1_hlp56pt,1637633181.0,True
qzwb8e,Correct. Upon reading your post that's not exactly what you're looking for,hlpvey5,t1_hlpu50c,1637633773.0,False
qzwb8e,"Actually, no. The goal isn't to have a search engine, but to build one so I can learn more about it and the decisions that go behind creating something like this; specially when it comes to the storage of dataset to allow quick retrieval. While elastic search would build a search engine, it wouldn't achieve those goals",hlqydzr,t1_hlpvey5,1637657814.0,True
qzwb8e,"A quick idea:

Updating:  
Your engine should be informed about any new title/tag ""t"" entering the ""network"". t gets added to a hashmap.

Retrieving:  
A few waves of search for ""s"" in the hashmap, starting from most important to least.  
Search one:  
look for exact matches of s  
Loop as many times as you think it's necessary:  
generate similar strings to s, s'. Search for s'

Notes:

* every next loop is on s', s'', s''' etc
* if your ""network"" was created first, you gotta search through the entire thing unfortunately and update your map

Edit: You can have an in-between wave between s and s', which uses a dictionary to find similar words, or correct the ones you already searched for.",hlptjm3,t3_qzwb8e,1637632918.0,False
qzwb8e,Are you talking about a vector search technique here?,hlrlpfv,t1_hlptjm3,1637675027.0,True
qzwb8e,"I made it up, what do you mean by vector search technique? Can I have a link?",hlro95g,t1_hlrlpfv,1637676332.0,False
qzwb8e,"Vector search is [like this](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/), basically you build vectors to represent your 'objects' then compare them to find the most 'similar' vectors (eg those closest geometrically, or with the smallest angle between them).

I do think the hashing approach you made up is similar (although not the same) as [locality sensitive hashing (LSH)](https://www.pinecone.io/learn/locality-sensitive-hashing/), but in that case you're using a hash function specially designed to hash similar items into the same bucket, so when you have a search query you hash it with LSH and identify which other items were hashed to the same bucket.",hlrq5xq,t1_hlro95g,1637677259.0,False
qzwb8e,"First of all thank you for the valuable information!These are topics I am very unfamilliar with, but what I presented above can be implemented in any way you like, why not.What I can think of out of this is that if you manage to somehow create a Vector-space with meanings of words, It would make the search engine even greater as it would not be limited to spelling. But it's good to mix it with searches of similar words grammarly, because someone can mispell something that probably doesn't exist in a dictionary e.t.c.

LSH sounds awesome, but won't there be a lot of collisions in your hashmap?",hlrswzg,t1_hlrq5xq,1637678519.0,False
qzwb8e,"Yes exactly, there was a [cool example recently](https://youtu.be/7RF03_WQJpQ?t=150) which implemented both BM25 and NLP models for searching through wikipedia on 'what is the capital of the US?', and the BM25 approach comes up with a load of random things that contain 'US', 'capital', etc - whereas the NLP models pull in the right responses.

On LSH yes you can get too many collisions, you have to increase the number of possible buckets until you reach a point where there's not too many in each bucket - you increase the number of buckets by increasing the number of bits (which I think of as being similar to increasing the resolution in photos)",hlsxihu,t1_hlrswzg,1637694737.0,False
qzwb8e,https://youtu.be/mu6ExLCtFsQ,hlrul14,t1_hlro95g,1637679257.0,True
qzwb8e,"Besides the pseudo algorithm I'd like to add:  
No it won't be much more difficult than a sophisticated dictionary structure regardless of implementation. Unless if you have to deal with the real internet which will land a lot of complications along the way and require a lot of computational power, memory and money.",hlpw8v7,t1_hlptjm3,1637634165.0,False
qzwb8e,"A search engine is a variable beast - there are a whoooooole lot of steps to take, each step of which is technically a search engine, and notably **does not have to operate on websites to be one**. One that worked on your lecture slides for the class would be just as valid.
 
First, you have the most basic 'return by exact match on field'. You have a database, you go through that database and pluck out the matching elements by title, or keyword, or author.
 
Next, you can apply rules to those searches - This AND That AND The Other Thing OR Something Else. Many search engines are sitting here, for example https://www.scopus.com/
 
The step after that is 'content based' searching, which is going to look at the body of the material and look through there, including all your rules and whatnot (this is not difficult for function, but is a nightmare for optimisation).
 
Next after that you have internal, implicit rule generation. That is things like implicit 'stemming' of words, synonyms or close to them, and other basic natural language processing 'pre-processing' steps. There is still no machine learning at this stage, just related steps you would normally do before throwing it into your algorithms. (note: this is probably the step I would recommend for a good 2nd, average 3rd year programming assignment in a data science curriculum, perhaps excluding body content).
 
After that you add machine learning for natural language processing. You start looking at the connections between words, related topic, density of the topic in the elements being found, working out subject/object, etc etc etc. This has no indefinite end and is where Google spends a significant amount of their time and money.
 
But that's just core functional capabilities. You have non-core and non-functional capabilities as well, of which the most important are probably weighting and optimisation. Weighting is being able to take your matches and assign them a weight. Easy at the lower ends of core functionality (how many rules did they match, and how often do they match them?), when you get into the pre-processing you need to look at how much pre-processing was necessary - so if you searched for 'speedily', then the ranks for matches would probably go speedily->speed->fast, for example. And when you're into NLP land it gets much more complex.
 
Optimisation is working out how to sort out your indices, sharding etc etc to support not just searching, but very rapid searching, which is a whole different ballgame and involves a whole lot of different techniques.",hlpto9w,t3_qzwb8e,1637632975.0,False
qzwb8e,">Next after that you have internal, implicit rule generation. That is things like implicit 'stemming' of words, synonyms or close to them, and other basic natural language processing 'pre-processing' steps. There is still no machine learning at this stage, just related steps you would normally do before throwing it into your algorithms. (note: this is probably the step I would recommend for a good 2nd, average 3rd year programming assignment in a data science curriculum, perhaps excluding body content).

So basically you mean I should search for words using synonyms, but only do so in the title?",hlrp8di,t1_hlpto9w,1637676810.0,True
qzwb8e,"And keywords, if you can get them. If you're doing web pages these are in a meta tag.",hltuurp,t1_hlrp8di,1637708060.0,False
qzwb8e,"If I am building a search engine that doesn't work on the internet, how do I assign a score? I mean websites could be ranked using some page Rank or some other variation of it, but if we are taking something that works on lecture slides or something else, how do I give them a score?",hlrpn2w,t1_hlpto9w,1637677011.0,True
qzwb8e,Work out some criteria and base it off that. Number of times the search term appears is the easiest.,hltv030,t1_hlrpn2w,1637708124.0,False
qzwb8e,"Yep that seems good. Maybe I can even add a review field to it, to simulate the resource bank of a college where all the teachers have submitted their slides and we will be showing the ones that are rated higher by the students at the top.",hlu3su3,t1_hltv030,1637712047.0,True
qzwb8e,"There are plenty of options, if you're going to go for a fast 1-month project you might want to go with elasticsearch/BM25 etc, other commenters have covered this so I won't repeat the same.

If this is a passion project and you decide to do more on it (or maybe this is within your scope anyway), Google relies more and more on ML/AI methods in their search to allow you to search with meaning/concepts - Elasticsearch and BM25 will not be able to do any of that for you, they rely more on word matching (which does still work well, but means that you need to choose the correct words). If you're interested in the ML/AI version, you need two 'pillars' - vector similarity search and NLP models (typically transformers like BERT).

It's super fascinating imo and worth looking into, for the NLP models intro I [wrote this](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/), and for the vector similarity search (or 'semantic' search when used with NLP) [I wrote a 'course'](https://jamescalam.medium.com/free-course-on-vector-similarity-search-and-faiss-9b3e91a91384?sk=b5cb406932f62c1fb69eb0944efba92c).

Whichever route you take for your first month, I hope you at some point have the opportunity to explore the AI/ML approach because it is fascinating.",hlqnw8p,t3_qzwb8e,1637649417.0,False
qzwb8e,"That is the correct one. You don’t need to learn about databases or hadoop to learn about how google does it. 

BM25 and Transformers (aka BERT* variants) are how Google achieves search results.",hlqvqdp,t1_hlqnw8p,1637655533.0,False
qzwb8e,"I wouldn't bother with PageRank or other extra features for a month long project. I would focus on the very core of the idea of a search engine.   
Mainly I would care about implementing BM25 based scoring, inverted index and some method to deal with spelling errors. Finite State Transducers are a great fit here and the one by burntsushi is great for this use case. BM25 scoring is simple enough to implement I think, and I believe burntsushi's library does provide some levenshtein based spell corrector. 

Writing this up should be possible in a month or so in my opinion, I say this because I built the above system in a month or so.",hlph71q,t3_qzwb8e,1637627101.0,False
qzwb8e,"I have literally just done a search engine data structures assignment.

We didn't build it from scratch, rather we had to complete it in parts and do exactly what the assignment specs had told us to do. For example, using a certain algorithm etc.. 

We used the pageRank method (you can google this) and sorting algorithms to produce search results from key words. We didn't web-crawl or anything like that, rather they provided us with files with url links, and then we had to search those files and access those url links and read the pages.",hlq410f,t3_qzwb8e,1637637858.0,False
qzwb8e,"I would just take a look at their first paper on it: http://infolab.stanford.edu/~backrub/google.html

I think you could build a version of that system.  This goes over the various pieces they use and how they use them.",hlsss2h,t3_qzwb8e,1637692899.0,False
qzwb8e,"Hey, so I took a look at this and didn't find much information on the actual implementation of the data structures. Any ideas on where I can find that? Not expecting the actual code here. They mentioned they have designed the data structures to reduce disk writes but there was no explanation as to how they did it",hmlw8hw,t1_hlsss2h,1638236553.0,True
qzwb8e,"I would narrow the scope of what your search engine can find like a search engine for plants, or a search engine for  cars, then use Google to build your dataset.",hm86go2,t3_qzwb8e,1637979687.0,False
qzo8bt,"I learned scalability and software design patterns over a fairly long period of time by osmosis, so I don't really know of any good resources to help you in that regard, but I do want to say something about the OOP hierarchy problem that you mentioned, which was one of the first non-trivial design problems I encountered when learning OOP.

The trick to that particular problem in OOP, as I see it, is in recognizing inheritance as a secondary feature of the paradigm (the primary feature being just polymorphic message passing). My first language was modular and procedural, so I actually had a bit of difficulty learning OOP as a concept (with Java 6) probably due to the fact that I was so used to thinking procedurally. Whatever the reason for my troubles, learning Smalltalk, with which I quickly replaced Java as my main OOP language, was immensely helpful in elucidating for me the core of OOP and thus in helping me to gain an intuition for what complex OOP software should look like from a high level.

All this to say that focusing upon inheritance relationships (aka ""is a"" relationships) tends to result in hierarchies that are difficult to follow or even to formulate in the first place, which is the issue you mention. Instead of using inheritance, try using interfaces, (or whatever ad-hoc class level polymorphism your language provides). This is a key element in an approach to OOP that is often referred to as ""composition over inheritance.""

For a while, try writing OOP programs using no inheritance at all (other than mandatory inheritance, like from Object in Java, ofc). I don't believe that inheritance is always evil, but refusing to use it is a good way to train yourself to not have to, and thus to become more flexible and better able to accurately model the domain of your software.

Alternatively, I guess you could do what I did and just write really shitty software for a while before eventually figuring it out, but your approach, asking for help and resources, seems to me the definitively smarter way to go about it.",hloj44f,t3_qzo8bt,1637612465.0,False
qzo8bt,"[Design Patterns](https://en.wikipedia.org/wiki/Design_Patterns) is the canonical reference for the patterns themselves, it's mainly a catalogue, but there are use cases there too from what I recall.  Still, for me, this text and any of a number of videos or tutorials on the subject do little to instill in me the ability to recognize when to use what pattern beyond the more simple ones.  I guess it just takes a lot of practice, and prior to that learning the patterns.",hlovoo1,t3_qzo8bt,1637617542.0,False
qzkegn,IOT hacking by No Starch Press,hlmy2nv,t3_qzkegn,1637588549.0,False
qzkegn,"Ok thanks! 
Does it contain application and importance of different devices?",hln2sc5,t1_hlmy2nv,1637590961.0,True
qzkegn,"Not sure, sorry",hlnjro6,t1_hln2sc5,1637598351.0,False
qzkegn,Ok thanks... I'll check it out anyway!,hlnvhae,t1_hlnjro6,1637603041.0,True
qzkegn,Building the Internet of Things by Maciej Kranz,hlnc3nl,t3_qzkegn,1637595184.0,False
qytazd,search subnet questions and you will find them,hli6urh,t3_qytazd,1637499670.0,False
qz9jwv,"Not an expert, but I do know that kernel programming is very difficult due to the fact that encountering a bug in the kernel means it’ll likely crash to the metal. Plus, recompiling kernels takes a very long time.",hlobf17,t3_qz9jwv,1637609361.0,False
qz023c,"You can find a lot of literature on Google Scholar on any of those subjects.

&#x200B;

For example, [https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=genetic+algorithms&btnG=](https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=genetic+algorithms&btnG=)

&#x200B;

I'd suggest limiting the search to the 1970s, 1980s and 1990s to see the foundational papers. E.g., Holland on Genetic Algorithm.",hlj81jx,t3_qz023c,1637516772.0,False
qz023c,Thanks a lot 👍,hlj8xs4,t1_hlj81jx,1637517133.0,True
qz023c,"No worries. I was in a meeting so my last reply was short.

I'd recommend Holland, Back, Grefstennette as all good authors on GA and evolutionary algorithms. Mitchell is good as well for AI/ML in generall.

A good paper on Bayesian inference. Mitchell covers it as well

[https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1968.tb00722.x](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1968.tb00722.x)

A could of good paper on hyperparameters.

[https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)[https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)

&#x200B;

If you decide to get into optimization, then make sure to read the ""No Free Lunch Theorem."" Very important and generally poorly understood.",hljb5pg,t1_hlj8xs4,1637518035.0,False
qz023c,"Holland's paper is really good so far (although there is a surprising amount of small typos). I'm just starting it, but it's obvious that this is related to machine learning (talk of hill climbing algorithms in the first page). 

Thank you for posting this and your other comment as well, and thank you to the OP for bringing this up. I'm going to make a week out of this.

Update: [This paper is really good, actually.](http://www2.econ.iastate.edu/tesfatsi/holland.GAIntro.htm) I've been able to reproduce some of the easier parts of it in Kotlin and it is very neat once you see it in action. In a few weeks or months I'll post a link with the full reproduction of the Prisoner's Dilemma example, because that's a very good one which teaches you the fundamentals of building a little framework in which to solve problems in this way. 

It's so good I've decided to order one of John Holland's books (*Signals and Boundaries*). Very neat stuff. Check that paper out if you're in to this stuff. I'm gonna be having a field day with it for a few weeks at least. Definitely a subject I want to study in depth.

Update 2: I've decided to make a little library around this idea for learning. Early stages yet, but I've implemented many of the ideas from the paper. Feel free to take a gander, as I have open sourced it. Just bear in mind that it is early stages and a prototype. It may prove helpful to anyone interested in grasping how Genetic Algorithms work (at least according to that paper): [Genetic Playground](https://github.com/sgibber2018/GeneticPlayground).",hljq1fv,t1_hlj81jx,1637523826.0,False
qz023c,"Cant help much with genetic algorithms or Bayesian optimization, but for reinforcement learning I strongly suggest  [Sergey Levine's video lectures](https://www.youtube.com/watch?v=JHrlF10v2Og&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc). Going from the basic algorithms and methods used into more difficult problems without shying away from the math involved.",hljqrzd,t3_qz023c,1637524123.0,False
qz023c,"Yep, haven't watched them, but also heard these are great from Sergey.

So is the ""RL: An Introduction"" Sutton & Barto book ([here](https://incompleteideas.net/book/RLbook2020.pdf)) or David Silver's (guy behind DeepMind's AlphaGo) [lecture series](https://www.youtube.com/watch?v=2pWv7GOvuf0) \- I used these two and I ended up publishing research in RL!",hlnigoe,t1_hljqrzd,1637597821.0,False
qz023c,Koza wrote a very detailed book or two on genetic programming https://www.goodreads.com/book/show/644125.Genetic_Programming,hlme97c,t3_qz023c,1637574105.0,False
qyyi89,If you’re creating new ways to do it and studying the theoretical side it’s computer science. If you’re dealing with implementing and adapting an already understood method to real world constraints then it’s engineering.,hliyz6i,t3_qyyi89,1637513043.0,False
qyyi89,"That's kinda a misrepresentation of the boundary between science and engineering in tech.

If you're researching the known limits of the field, you're doing science. If you're implementing scientific discoveries in industry considering costs and scalability, you're engineering.

Though it is always still under the umbrella field called Computer Science.",hljo9ef,t3_qyyi89,1637523118.0,False
qyyi89,One could also argue that it's just a subfield of math,hlix3j9,t3_qyyi89,1637512272.0,False
qyv2sd,"Is this resource useful? 
https://ifs.host.cs.st-andrews.ac.uk/Books/SE9/WebChapters/PDF/Ch_27_Formal_spec.pdf

You may want to try this question over at 
https://www.reddit.com/r/learnprogramming/",hlih7cx,t3_qyv2sd,1637505411.0,False
qyv2sd,"Hey, thanks a ton. This helped a lot !",hlirgr4,t1_hlih7cx,1637509952.0,True
qyv2sd,Check out Agile Systems Engineering by Bruce Douglas. It’s fantastic.,hlkoklg,t3_qyv2sd,1637538345.0,False
qys9kx,"I think you've drawn it oddly, which is causing the confusion.

Usually the ""fetch"", ""decode"" ""execute CMD"" is written on the X-axis or time-axis and the sequence of CMDs in the program along the Y-axis.

&#x200B;

See [https://www.sciencedirect.com/topics/computer-science/stage-pipeline](https://www.sciencedirect.com/topics/computer-science/stage-pipeline) for an example (I cannot figure out how to include a picture).",hlia05u,t3_qys9kx,1637501577.0,False
qys9kx,"An easier approach would be to make a line of all the instructions you are going to execute and calculate the time it takes for the last instruction in the line to pass the last stage in the pipeline. (See the image)

[https://i.ibb.co/qMBJ2MV/image.png](https://i.ibb.co/qMBJ2MV/image.png)

Circles are instructions. You have ""n"" of these

Squares are the pipeline with ""x"" stages

So the very last instruction has to travel a distance of (n-1+x) steps

All these take ""y"" nanoseconds each

So the total time should be (n-1+x)\*y nanoseconds",hlhv1ds,t3_qys9kx,1637491122.0,False
qys9kx,"Wait but that equation would mean it would take way longer to complete the program and therefore isn't staged?

I just dont understand what youre saying, could you describe it differently?",hlhx3rx,t1_hlhv1ds,1637492745.0,True
qys6ai,"Something like this is actually used, but not to encode more than two states.

It is used in high end fibre optic connections used for internet communication. In a single fibre you not only establish a single connection but establish multiple connections that are multiplexed by different wavelenghts.

I remeber a youtube video from someone that visited a american fibre cable provider where this (remarkable unspectacular) beige box with a throughput of multiple Tb/s, through a single fibre, was shown, where multiple 10Gb/s links were connected and modulated onto different wavelenghts.

However, this only works for transmission. For computation you would need to design a transistor capable of process the different wavelengths parallel. But I doubt that this would improve calculation conpared to just use multiple regular transistors.",hlidlcr,t3_qys6ai,1637503555.0,False
qys6ai,.... What?,hli5r9t,t3_qys6ai,1637498983.0,False
qys6ai,"So instead of us looking at 0-1 values. 

If we had away of using colours to group values together. 
0 being a white light. When passed through an optic gives a value of a zero. 

When a red light is passed through gives on gives you a value of 00

When a blue light is turned on a value of 1 is passed through the optic.

Green a value of 11.

The colour spectrum is so vast, slight variations of colours could send down different formations of values.

What I was suggesting, by different colours could represent a different number. What I meant was a different colour could represent a different segment or sequence of binary.",hli7kad,t1_hli5r9t,1637500108.0,True
qys6ai,"The idea of a 0 and 1 at a physical level is an arbitrary line drawn between high and low voltage. Simply speaking, a voltage measurement above this line is a 1 and below the line is a 0. 

To instead encode 3 different values you can have three different voltage levels. You can have however many voltage levels you want. You do not need colors. The reason binary is used is because it's reliable and simple to work with (it's extremely easy to work with boolean algebra, etc...)",hljd6wy,t1_hli7kad,1637518853.0,False
qys6ai,"OR...we could, I don't know, maybe just use some more bits to denote larger numbers?   
I mean, perhaps we could assign 8 bits together in some way, like, say each one is a power of 2? That way the least significant bit could be 2\^0 or just 0 or 1, then the next one could be 2\^1 or just 0 or 2, and so on. Collecting 8 bits together like that could represent integer numbers from 0 to 255! And if we keep going, using more bits, we could really get some very large numbers! Just thinking out loud.",hlia73g,t1_hli7kad,1637501689.0,False
qys6ai,[See Wavelength-division multiplexing](https://en.wikipedia.org/wiki/Wavelength-division_multiplexing) and [optical computing](https://en.wikipedia.org/wiki/Optical_computing).,hliexsy,t3_qys6ai,1637504270.0,False
qyo107,this is managed by the operating system. the OS retrieves the memory capacity from the underlying hardware via syscalls. it is responsible for allocating memory to running applications.,hlh9vz6,t3_qyo107,1637475022.0,False
qyo107,"Ah that makes a lot of sense, Cool! And I am assuming the operating system adds to the used memory variable as programs write to memory. I am also assuming it is dependent on priorities, like with Windows you can set the priority of a program and it will allocate more resources to that program if available",hlha3oz,t1_hlh9vz6,1637475168.0,True
qyo107,"operating systems can use several memory management models including single contiguous, partitioned, and paged allocation. this wikipedia article is good literature: https://en.wikipedia.org/wiki/Memory_management_%28operating_systems%29?wprov=sfla1

the priority in windows operating systems you are referring to is process priority, also known as ""niceness"" on Unix/Linux-based operating systems. priority/niceness modifies scheduling, which is an allocation of various resources including CPU time.",hlhc7vr,t1_hlha3oz,1637476673.0,False
qyo107,Awesome! Thanks for clarifying that and thanks for the help,hlhdhzh,t1_hlhc7vr,1637477617.0,True
qyo107,"Something along the lines of:

There are 4,447 places in memory info can be stored. Program x wants to use locations 1 - 47, thus 4,400 are free?

Except, that changes many thousands of times per second. Programs x, y, and z are all taking memory and letting go as needed.

Am i in the right ball park?",hljg0bs,t1_hlhc7vr,1637519931.0,False
qyo107,you're in the right ballpark. your example is one of contiguous allocation.,hljkgxl,t1_hljg0bs,1637521651.0,False
qyo107,"Hmm, does the OS retrieve the memory capacity from hardware via syscalls? My understanding is that syscalls are used to call priveleged functions in the OS (such as reading a file, or allocating memory) from functions in the user space, and that the OS keeps track of all the memory that it has allocated in additional data structures, which is how it keeps track of the memory usage. Correct me if I am wrong.",hljepaa,t1_hlh9vz6,1637519429.0,False
qyo107,"you're correct. my mistake, I just reread my post. the os queries the BIOS or UEFI for the systems memory capacity. it makes memory available to applications in user space and reserves memory for itself in kernel space. user space applications have no visibility to memory reserved in kernel space, though the os makes the capacity (amount) of that memory available to some applications via syscalls.",hljk1z8,t1_hljepaa,1637521495.0,False
qyo107,"Most memory and storage hardware doesn't really have a sense of full or not. There is something at every address. It's up to software to decide if that information is meaningful or not. The operating system breaks that memory up and gives it out (assigns it) to processes or to itself. The new ""owner"" of the memory can write useful stuff into it instead of whatever garbage happened to be there initially. The hardware can help with controlling which processes can and cannot write where.

If you'd like to learn more, you can Google for ""operating system memory allocator"" it's a pretty well defined topic so I bet there's good material out there for more details.",hlir6vz,t3_qyo107,1637509840.0,False
qyo107,How does a hotel know how many of its rooms are full?  It keeps track of them as they are occupied and records that.,hljavsd,t3_qyo107,1637517924.0,False
qyo107,"This is exactly the kind of question that is answered in an OS class. If your university has one, be sure to take it! If not, may I suggest Operating Systems: 3 Easy Pieces, which is a relatively easier book to read (as far as CS textbooks go), but covers all the main topics (and a few extra ones too)",hljefd3,t3_qyo107,1637519324.0,False
qyo107,great question !!,hljnld8,t3_qyo107,1637522854.0,False
qykxoh,"The memory says eg ""hello, I am 16 GB"" when the bios starts. It is then known. There's no calculation at all.",hlgyz2v,t3_qykxoh,1637468261.0,False
qykxoh,"Ah okay kinda makes sense, the reason I thought it calculates the amount is because if you look at older machines you can see the amount of RAM count up until like 65536K for example

Edit: A good example I see is in my old Pentium 4 Machine with 700MB RAM. It counts up rapidly until it sees i think 768MB RAM and on screen says 768MB RAM OK or something similar to that",hlh0aep,t1_hlgyz2v,1637469029.0,True
qykxoh,"It's testing the memory for errors before booting. Modern computers can still do this, but the firmware in many cases is configured to skip the tests by default because they generally aren't thorough, modern users are generally impatient and expect startup to be quick, and there's specialized software for testing memory (e.g. MemTest86).",hlh0nt2,t1_hlh0aep,1637469247.0,False
qykxoh,"Yeah while playing a match of Dead By Daylight I realized that, Thanks for the help! Both of you",hlh1xfi,t1_hlh0nt2,1637469998.0,True
qykxoh,A slightly off topic comment but how did you learn about the computer architecture ? Is there a book / site that is helpful to get started with concepts related to building a computer ?,hlig6iv,t3_qykxoh,1637504907.0,False
qykxoh,"I learned from a book called ""But How do it know"" and a series that ben eater made on youtube: [https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU)

He has a lot  of really good videos on the topic that I think you may like",hll7x5j,t1_hlig6iv,1637547328.0,True
qykxoh,Apologies for the late reply - But thanks !,hm1qycq,t1_hll7x5j,1637857330.0,False
qyiizq,"Look at the relationships any gate operators relate to a gate symbol you can draw for a circuit diagram.

A cleanish way to draw the diagram would be to have inputs (normally a,b,c,…) along the top with lines coming down and then lines that divert off of that horizontally into the circuit gates. 

So if you have A+B in your truth table then you know you need input for A and input for B and an AND gate.

There are tons of images if you checkout google. Programming Circuit Diagrams",hlg7onb,t3_qyiizq,1637454386.0,False
qyiizq,"Well, if the function is multi-argument (more than two), I would go for the Karnaugh map in order to minimize the function.",hlg7uv2,t3_qyiizq,1637454466.0,False
qyiizq,I think circuit simulators should usually have an option to input the desired truth table and then automatically create the circuit. At least that is what I used to use in my university. The tool we used is 'logisim'. Search for it and you should be able to find a download link. I also think the software is free or open source. Hope that helps!,hlhmcro,t3_qyiizq,1637484177.0,False
qyiizq,I actually downloaded Logism however I was still curious as to what processes would be involved in converting manually for a deeper comprehension of the full adder. I'm not a computer science student or anything but I feel that it would be very cool knowledge to have. Thanks for replying.,hlkg746,t1_hlhmcro,1637534618.0,True
qyiizq,"Well I remember we did make a full adder in uni. I think we somehow first made a half 1 bit adder. Then a full adder. And I also remember the nice thing about logisim is that you could make your own components. So we took the full adder and made it a component with two inputs and two outputs(result and carry?). Not sure if it's called carry, but it is the overflow bit. So having this new component you could make any n-bit adder. For example a 4 bit adder.",hlkmg6h,t1_hlkg746,1637537394.0,False
qyiizq,"Introduction to the Logical Design of Switching Systems by h.c. torng 

It is an old textbook, but I think it is what you are asking for.  Because it predates all the computer-based methods of doing this, it explains everything from scratch.",hlkmnhw,t1_hlkg746,1637537485.0,False
qyiizq,"Damn 1967, that's an old textbook alright. Sounds like it would be useful though, I suspect there's nowhere online that I can view this however considering its age.",hlkr6in,t1_hlkmnhw,1637539527.0,True
qyiizq,"Indeed it is.  Looks like you could get a copy from Abe Books for just a few dollars though.  The edition I am familiar with is the 1966 one.  I suspect the later ones drop the chapter on relay logic design, since by then they were only used in elevators I suspect.  But if you want to build something like a 4-bit relay adder, it's essential.",hlkvr20,t1_hlkr6in,1637541618.0,False
qyiizq,Might be best to first use a KMap and get the simplified equation... then convert that in your head to a circuit diagram.,hlrlm2s,t3_qyiizq,1637674979.0,False
qyiizq,This is how you would do it by hand,hlrln1e,t1_hlrlm2s,1637674992.0,False
qyeqvw,"You can always just encrypt it with <insert encryption algorithm here> and it won't be immediately obvious how to decrypt it, or where the key is in the code. Less effort is something like Base 64 encoding.",hlflhpi,t3_qyeqvw,1637444195.0,False
qyeqvw,That <insert encryption algorithm here>  part is what I need help with. Basically deciding which to use since most implementations depend on some larger libraries.,hlh7fdn,t1_hlflhpi,1637473365.0,True
qyeqvw,"You could store them as integers (e.g. 0-25 for A-Z) instead of ASCII. That's basically still a Ceasar shift, but one step better, because it will prevent them from showing up as ASCII strings — someone using the [`strings`](https://linux.die.net/man/1/strings) command won't find them.",hlfofs3,t3_qyeqvw,1637445513.0,False
qyeqvw,"That's a good idea, going off of that idea... let's say I have 2 letters in my password, is there a way to store just one number? For example is there a way to store two unique numbers in one number that can then be taken apart to reliably obtain the two unique numbers back out?",hlh7o3d,t1_hlfofs3,1637473522.0,True
qyeqvw,"Yeah, your numbers are probably 4 or 8 bytes wide. You said you don't have bitwise operators, but if you have division and modulus, you can separate each byte by dividing by 256^(n) then modulo by 256.",hlh806u,t1_hlh7o3d,1637473743.0,False
qyeqvw,"Interesting, could you explain that more to me? For example If I took the number 2000, and did 2000 / 256 I would receive 7.8125. Are you saying there is a way to return to 2000 from 7.8125 using the modulo operation? Forgive me if I am not seeing this right away.",hliq0wb,t1_hlh806u,1637509356.0,True
qyeqvw,"I meant integer division, where you floor the result. The number 2000 represents two integer bytes: 7, and 208.

(2000 // 256) % 256 = 7

2000 % 256 = 208

... where ""//"" means floored division, and ""%"" means modulo.

It might be easier to understand in the reverse direction. If we want to pack two integer bytes, 7 and 208, we multiply 7 by 256, then add 208.

7 * 256 + 208 = 2000

If you're familiar with bitwise operations, multiplying by 256 is equivalent to left shifting by 8, which makes room for a new byte in the least-significant position. Likewise, dividing by 256 is equivalent to right shifting by 8. Modulo 256 selects only the least-significant byte (equivalent to bitwise AND 255).

EDIT: Oh, and if your language treats all numbers as floating point, it likely uses ""double precision floats"" meaning you can use 53 bits before losing precision, so you can pack 6 byte-sized numbers (integers between 0-255 inclusive) in one ""number"".",hlisjtw,t1_hliq0wb,1637510403.0,False
qyaklu,[CoRecursive: Coding Stories](https://corecursive.com),hlepxnw,t3_qyaklu,1637430425.0,False
qyaklu,"That one is soooo good.  Very in depth, long enough to get pretty technical, not so long that it gets boring.  I've especially liked their ""Apple 2001"" and SQLite shows.",hlevysh,t1_hlepxnw,1637432924.0,False
qyaklu,"[Code Newbie](https://www.codenewbie.org/podcast), [Coding Blocks](https://www.codingblocks.net/), [Command Line Heroes](https://www.redhat.com/en/command-line-heroes), [Soft Skills Engineering](https://softskills.audio/)",hleu2c9,t3_qyaklu,1637432130.0,False
qyaklu,"I like Advent of Computing, but it is more of a deep dive into the history of computing.",hlghna7,t3_qyaklu,1637459187.0,False
qyaklu,"Not compsci but cybsec related darknet diaries are really good, very light in terms of tech but good to wind down with. Malicious life, privacy security and OSINT are very good if you like something I little more technical.",hleptmy,t3_qyaklu,1637430377.0,False
qyaklu,[Algorithms + Data Structures = Programs](https://adspthepodcast.com/),hlguxyy,t3_qyaklu,1637466025.0,False
qyaklu,Lex Fridman has some really good guests. Grumpy Old Geeks is kinda fun.,hlf9zf4,t3_qyaklu,1637439014.0,False
qyaklu,Wes Bos and Scott Tolinski's Syntax podcast is great for web development stuff. I'm not really into web dev but I still find it entertaining!,hlfblsq,t3_qyaklu,1637439742.0,False
qyaklu,Any body know any british coding podcats?,hlgzbs0,t3_qyaklu,1637468467.0,False
qyaklu,"https://oxide.computer/podcasts

https://www.softwareatscale.dev/?utm_campaign=pub&utm_medium=web&utm_source=copy",hlftond,t3_qyaklu,1637447917.0,False
qyaklu,Real Python and Django Chat are on Spotify.,hlhbtux,t3_qyaklu,1637476394.0,False
qyaklu,"Its not directly coding, but Darknet Diaries is really great :)",hly6zyq,t3_qyaklu,1637787902.0,False
qyaklu,What’s it about?,hm1k173,t1_hly6zyq,1637854255.0,True
qy9ke0,"They're electrical, radio or optical pulses so if other pulses are in the same medium they can interfere.",hlejxjn,t3_qy9ke0,1637427891.0,False
qy9ke0,"There's [a whole field of philosophy that deals with the relationship of parts to the whole](https://plato.stanford.edu/entries/mereology/), so I'd imagine they'd have something to say about whether a collection of particles is a physical body.",hlepsbs,t1_hlejxjn,1637430362.0,False
qy9ke0,what is a physical body other than a collection of particles?,hlf6yqm,t1_hlepsbs,1637437662.0,False
qy9ke0,The fact that all physical bodies are collections of particles doesn't imply that all collections of particles are physical bodies. That's just logic. I'm super logical.,hlf870y,t1_hlf6yqm,1637438203.0,False
qy9ke0,"Hi, super logical. I’m dad.",hlg5yxk,t1_hlf870y,1637453573.0,False
qy9ke0,Take it. Take your upvote.,hlgh73v,t1_hlg5yxk,1637458964.0,False
qy9ke0,They certainly aren't metaphysical.,hlentlg,t3_qy9ke0,1637429529.0,False
qy9ke0,LOL GOT 'EM,hlepuao,t1_hlentlg,1637430384.0,False
qy9ke0,"They are physical occurrences, of electrical and em wave signals.

But they are not physical bodies. More analogous to sound waves, which are also physical but not objects.

Interference is like if I say something and you can't understand because some other noise is too loud to hear enough. Collision is like if I say something and you don't catch it because someone starts talking to you specifically at the same time, or I interrupted myself half way to say something else.

It's just electrical and electromagnetic signals not sound waves.",hlhbr4l,t3_qy9ke0,1637476339.0,False
qy9ke0,My network prof told us today an IP packet (single) is long kilometers,hlezbst,t3_qy9ke0,1637434335.0,False
qy9ke0,It's said that each IP packet is a separate miracle.,hlff3sc,t1_hlezbst,1637441314.0,False
qy9ke0,https://www.phys.uconn.edu/~gibson/Notes/Section5_2/Sec5_2.htm,hlepqjm,t3_qy9ke0,1637430341.0,False
qy9ke0,"Lol, TCP stack will retransmit anything lost, dont worry about lost frames or packets :)",hlipx74,t3_qy9ke0,1637509313.0,False
qy9ke0,Information is just an abstract representation of the motion of physical bodies,hliu377,t3_qy9ke0,1637511027.0,False
qy34w1,"Viruses don't work by magic. They are programs like any others and just do nefarious things. They don't magically start executing, something has to explicitly start them. Usually this happens without you wanting to, e.g. when opening an infected file, or a malicious website etc.

However a virus executable can just be stored on your hard drive without any issues. Security researchers do it all the time. The key is to never execute it.

So if you have an encrypted virus somewhere, you're safe since you can not execute it, because the file is encrypted. But the encryption really does not matter.

Of course, once you execute the virus, the fact that they were encrypted at some point in time becomes irrelevant.",hldmyvd,t3_qy34w1,1637411242.0,False
qy34w1,I thought there were viruses that could do things themselves and that Trojans were the ones that had to be executed by the user? Of course I may be wrong but this is why I thought virtual machines were a thing. I've heard of viruses that can duplicate and spread all over a system. Do they all need to be activated by the user?,hlluvii,t1_hldmyvd,1637559169.0,False
qy34w1,"No. Computers are not sentient beings. Everything happens because it is explicitly caused by some other thing.  


Viruses typically exploit security issues in programs to trick those programs into executing (e.g. sometimes E-Mail programs automatically attempt to download attachments, trip up and then execute these attachments because someone fucked up while programming them).  


Viruses also don't ""duplicate all over the system"". That's not how any of this works. A virus is a computer program. Its primary goal is to get executed, so that it can do its malicious activitiy. A virus especially wants to get executed after a restart, or when the user deletes the original file. Hence, the virus copies itself to a few new folders, and registers these copies of itself as programs to be run when booting the system.

Trojans typically exploit the user by masquerading as a useful program. They often even perform a normal function besides their malicios activity so that the user does not become suspicious.

&#x200B;

Besides, these denominations are all fuzzy in practice. The names also are. Just because it is called ""Virus"" does not mean that the analogy holds for everything. You would not expect to be able to vaccinate computers, would you?",hloecrs,t1_hlluvii,1637610550.0,False
qy34w1,"Encryption is not a container. It is a function you apply to the data. Example: If you apply + to the numbers 2 and 3 it is not like you are putting the 2 and the 3 into a box with a + on top of it, instead you are generating a new number (5). Like with encryption you can not reverse the function only knowing that the new data is 5, you need addition information. This additional information is the encryption key. If the 2 was your virus it is no longer there.

If a folder is encrypted you actually have to concatinate all the files in it to generate a single file and encrypt this, or encrypt every file seperate.

To answer your question. An encrypted virus is just meaningless data. All data in an encrypted container is encrypted.",hldug6g,t3_qy34w1,1637416009.0,False
qy34w1,As I suspected. Thanks for the answer :),hlluwlr,t1_hldug6g,1637559188.0,False
qy34w1,"Even unencrypted, a virus file just sitting on your hard drive can't do anything until something opens it. It's just zeroes and ones waiting to be executed, not a living creature which runs by itself.",hldn27o,t3_qy34w1,1637411307.0,False
qy34w1,"If a virus is encrypted it cannot be executed. It needs to be decrypted first. Similar to how you cannot start software that has been compressed (compression is a type of encryption), you must decompress first. Of course there could be additional malicious software that decrypts the virus. Viruses are just software that do things you don't want.",hle9r5s,t3_qy34w1,1637423526.0,False
qy34w1,If there’s a virus in your computer it all goes out the window,hldfkui,t3_qy34w1,1637405388.0,False
qy34w1,I never said there was. I just want to know if encryption works both ways.,hldfo5u,t1_hldfkui,1637405466.0,False
qy34w1,Not when you have a virus.,hldfu9q,t1_hldfo5u,1637405608.0,False
qy34w1,"Ok, if I'm the one who created a virus, but then I put it into an encrypted container and sent it to someone and they cannot decrypt the container, could the virus be a threat to them?",hldfzbl,t1_hldfu9q,1637405727.0,False
qy34w1,"If you mean an encrypted zip i can't see how it would ""escape""",hldioqa,t1_hldfzbl,1637407952.0,False
qy34w1,"Or a Veracrypt container. Anything with proper encryption (password protection doesn't always mean encryption eg windows lock screen isn't encrypted, bitlocker is)",hldivsu,t1_hldioqa,1637408107.0,False
qy34w1,"u/JoJoModding's comment pretty much says it all.

A virus (or any kind of malware) is made of program code.

What separates viruses or malware from any other programs is that their program code 1) attempts to do something malicious, and 2) malware usually tries to include itself in your computer's startup programs or other startup routines so that their code is automatically executed whenever you start your computer.

Malware might also have some other kinds of tricks to make itself harder to detect, for example.

Fundamentally, though, malware is just program code. Program code only does something when it's being run, one way or another; it doesn't do anything by sitting on the disk. In fact, a better way of looking at it might be not to consider the program code as doing something (malicious or otherwise) by itself; rather, when you run a program, the operating system and the computer's CPU take the program code they find in the program file and start doing what the program code says.

If the code on the storage device is encrypted, it's not a question of whether it can ""escape"" from somewhere. The program code that has been encrypted does not make sense as program code, and it doesn't look like program code to the operating system or to the computer: it cannot be executed in the first place, so it cannot be doing anything, inside or outside of that folder. It's just inert bytes.

In that sense it doesn't necessarily make sense to say that encryption ""works both ways"", as it's not as if the virus is alive but being contained within the encrypted container somehow and restricted from accessing the rest of the system by the encryption.

You're correct that if malware code has been encrypted, and it's not being decrypted (or the password simply isn't available), the malware can't possibly be doing something. However, that's not really ""the other way around"": it's exactly the same way that encryption works for any other files. MS Office cannot open an encrypted Word document that hasn't been decrypted; the CPU cannot execute program code that has been encrypted without it being decrypted first.",hle7og3,t1_hldivsu,1637422621.0,False
qy34w1,"Yeah, a zip file is encrypted with AES thou :)  
If the computer can't access the virus code how can it run it XD",hle1wl5,t1_hldivsu,1637419922.0,False
qy34w1,Yes very.,hldgcr0,t1_hldfzbl,1637406044.0,False
qy34w1,May I ask how though? Encryption as far as I'm aware turns all of the data into a hot mess until it's decrypted. How can anything function in such a state?,hldghrw,t1_hldgcr0,1637406161.0,False
qy34w1,Why is OP replying only to trolls and not any of the serious answers?,hlg2xel,t3_qy34w1,1637452161.0,False
qy34w1,"Just from that comment alone I'm suspecting you yourself are a troll. The ""trolls"" commented first hence why I replied to them first. Then I got busy as other answers started rolling in, now I'm back again and responding to others.",hllv50f,t1_hlg2xel,1637559332.0,False
qy32gm,"I think you have the basic functionality of the routing table down. What I think you haven’t understood yet is that in networks there is a “routing protocol” which is responsible for figuring out what needs to be in the routing table for each router in order for it all to work. There are lots of different routing protocols from simple to complex (e.g. RIP, OSPF, BGP, etc). But they all are basically applications that run on each router, send packets of information to directly attached neighboring routers, and build up some kind of internal state about what the network “looks like” from which they decide what the routing table should be. If they do this correctly then the network works: packets get forwarded over the correct links to reach the correct destination. However it’s highly non-trivial to do correctly when things in the network start changing. Building such a routing protocol is a classic example of distributed systems.",hldq62c,t3_qy32gm,1637413414.0,False
qxxaab,"No, because although I have a B.S. in Computer Science, I am a software engineer.",hlcl93b,t3_qxxaab,1637382293.0,False
qxxaab,"Yeah, this is me. Im a software engineer, even though my B.S in CS",hlcpbbt,t1_hlcl93b,1637384671.0,False
qxxaab,"computer alchemist, got it.",hleimju,t1_hlcl93b,1637427358.0,False
qxxaab,Same here. I guess if I was in the research world then computer scientist would be appropriate.,hlem6c0,t1_hlcl93b,1637428844.0,False
qxxaab,Same,hlf6s0s,t1_hlcl93b,1637437579.0,False
qxxaab,"Since I have a master's degree in CS, and not a PhD, I just refer to myself as a computer master.",hlcyp5j,t3_qxxaab,1637391016.0,False
qxxaab,"I have BS in Computer Science, but did not finish my Master’s degree. So I refer to myself as a computer slave, or on days when I have abandoned human and returned to monke, code monkey. 

https://www.cafepress.com.au/mf/4785794/warning-monkey-coding_sticker?productId=12172404",hle64t5,t1_hlcyp5j,1637421928.0,False
qxxaab,very cool,hldsla9,t1_hlcyp5j,1637414920.0,False
qxxaab,If you get a PhD you would be a computer doctor,hlekpy9,t1_hlcyp5j,1637428221.0,False
qxxaab,"Yes, depending on the context. Sometimes I just call myself a scientist or a medical researcher. It depends on the audience. The problem with telling laypeople that I am a computer scientist is they think this means I'm a programmer (and boy do they have a great app idea they want me to build) or a technician (and boy do they want me to fix their PC/laptop). As you may notice in this subreddit, we get a lot of such confusion with numerous tech support and programming posts. There is some overlap (especially with programming) of course, but a lot of differences as well. Anyway, TL;DR, depends on the context.",hlckoca,t3_qxxaab,1637381963.0,False
qxxaab,What do you actually do if not programming?,hlfhjp7,t1_hlckoca,1637442404.0,False
qxxaab,"CS subjects generally lays down the fundamentals in programming but that doesn't mean everything in CS is based on programming. It's more like on the creation of a more efficient algorithm, or set of instructions. We can also apply this in robotics, mechatronics, avionics, and other fields.

Basically, CS deals with logic and how, why, and when it can be applied to a computing device. This device is not limited to traditional computers. You can use matchsticks for all I care.",hlfn84e,t1_hlfhjp7,1637444978.0,False
qxxaab,"Generally my life goes like this:

1. Get inspired by something.
2. Read the scientific literature about it.
3. Have an idea for a contribution to the literature.
4. Develop it further into a research proposal (data, metrics, analytics, methodology)
5. Gather any necessary data.
6. Preliminary data analysis.
7. Design and develop an algorithm to solve the problem (here lies programming although quite different then industrial software development, which I did for 14 years).
8. Evaluate the algorithm.
9. Fail.
10. Enhance the algorithm or build something entirely new (lots of thinking and some programming)
11. Evaluate the algorithm.
12. Probably still fail.
13. Repeat 10-11 until a breakthrough (literally had a massive research breakthrough yesterday after months of work that will completely change algorithmic inference forever, very exciting!!)
14. Write paper(s).
15. Publish papers(s)
16. Have paper(s) rejected.
17. Revise paper(s).
18. Repeat 16-17 until the paper is accepted.
19. Goto 1. :)

And on the side, mentor students, apply for funding and answer Reddit posts. :)

Note, I'm an applied machine learning researcher. A theoretical computer scientist would not do much if any programming at all.",hlfo2gl,t1_hlfhjp7,1637445352.0,False
qxxaab,This is an easy TLDR I’ve heard before: programming is to a computer scientist as a telescope is to an astronomer. Programming is a tool through which they conduct their research,hlgafmy,t1_hlfhjp7,1637455682.0,False
qxxaab,That's a very apt description.,hli5zb1,t1_hlgafmy,1637499120.0,False
qxxaab,"No, I usually think of scientist as someone who does research as their job. I’m a software engineer.",hlcna7i,t3_qxxaab,1637383457.0,False
qxxaab,Data scientist has entered the chat,hlcy69i,t1_hlcna7i,1637390616.0,False
qxxaab,I refer to myself as a code monkey.,hldbbvi,t3_qxxaab,1637401677.0,False
qxxaab,I refer to me as my own bad self.,hlefjvn,t1_hldbbvi,1637426063.0,False
qxxaab,😂😂😂,hlewbu5,t1_hldbbvi,1637433079.0,False
qxxaab,"I have yet to conduct my own science outside of perscribed assignments, so no. But I will happily award myself the title once I conduct my own research to answer my own question no matter how trivial the question is.",hlcn4ta,t3_qxxaab,1637383372.0,False
qxxaab,Depends on how badly I'm trying to win the argument,hldsmlg,t3_qxxaab,1637414942.0,False
qxxaab,Or impress a girl,hlfow7b,t1_hldsmlg,1637445720.0,False
qxxaab,I refer to myself as just some dude who sits in front of a computer lol,hlcouxk,t3_qxxaab,1637384396.0,False
qxxaab,I don't know what the hell I am anymore.,hle0azc,t3_qxxaab,1637419149.0,False
qxxaab,I feel you mate...,hle6gcp,t1_hle0azc,1637422078.0,False
qxxaab,"No, because I'm not a computer scientist. I refer to myself as a programmer, because I program computers and other devices.",hlddk2s,t3_qxxaab,1637403645.0,False
qxxaab,"Computer scientist by study, software engineer in practice. So yes, but also no",hlds9od,t3_qxxaab,1637414725.0,False
qxxaab,"No, I call myself Slick Fatsack",hldnz53,t3_qxxaab,1637411959.0,False
qxxaab,"The usual nomenclature is this.

The term computer scientist typically refers to people who have the educational background, which is usually Phd, to carry out of original research or contribute to original research in the field of computer science.

People who therefore do not fit this definition do not usually go by the description of computer scientist.",hlcxnxo,t3_qxxaab,1637390241.0,False
qxxaab,"That might be the degree, but you are only a computer scientist if you study computation.",hldcl5s,t3_qxxaab,1637402782.0,False
qxxaab,"In my language i dont recall anyone actually saying it. Programmer, tech guy, it, engineer, developer, but never ""scientist""",hle6lzq,t3_qxxaab,1637422145.0,False
qxxaab,"Nah, Depressed usually does the job.",hle9drs,t3_qxxaab,1637423369.0,False
qxxaab,"No, just like how traders are to math statisticians I am to computer scientists",hldfv2h,t3_qxxaab,1637405628.0,False
qxxaab,Do math statisticians generate and analyze statistics about maths?,hldu6mf,t1_hldfv2h,1637415858.0,False
qxxaab,"I've been called a Computer Science Major, Software Engineer and a Developer in my career. I don't care what they call me at work. As long as it's not derogatory.",hldnbng,t3_qxxaab,1637411502.0,False
qxxaab,"Can they call you ""the most beautiful man/woman/else in the office""?",hlg3l3x,t1_hldnbng,1637452468.0,False
qxxaab,I'm a computerer,hlfe407,t3_qxxaab,1637440864.0,False
qxxaab,I call myself a computer scientist on the grounds that I have the prerequisite training to do research in the field of computer science and I can teach the fundamentals of my field as a teacher (even though I'm not actually a CS teacher). Daily reminder that computer science is a discipline of mathematics.,hldbknf,t3_qxxaab,1637401883.0,False
qxxaab,I don't agree. I think it uses math but it is its own thing.,hldcboq,t1_hldbknf,1637402548.0,False
qxxaab,"The basis of all computer science is all about the mathematical concept known as the computation. In anything related to computer science, I expect a trained computer scientist to formally prove the mathematical properties about any given computation. The consequence of being able to formally prove the nature of any computation means that computer science is fundamentally a discipline within mathematics.",hldl7xc,t1_hldcboq,1637409938.0,False
qxxaab,"I’m not disputing what you are saying about the basis of CS being math, but does it matter? You see, the basis of Medicine is  Biology, the basis of Biology is Chemistry, and the basis of Chemistry is Physics. Yet, by current standards, those are considered separate fields of science, with a lot of theory and methods shared among themselves. Do you expect a biologist to prove/explain the nature of her/his work by the formalities of chemical reactions?

The definition of what’s a field or another is highly debatable. A good analogy is to see the human knowledge as a continuous variable, and fields of knowledge being our attempt to discretize it.

Applied Computer Science which IMO is by far the most important part of the field, spreads across all other fields, e.g Computational Biology, Computational Chemistry, and even Computational Sociology. These fields couldn’t care less about proofs btw. So, how do we classify it? Are those subfields, mixed fields? 

By current standards of what constitutes a field, I dispute the idea of CS not being one in its own, but it doesn’t really matter. The goal of every field of scientific endeavor - applied or theoretical - is to further the knowledge of humankind, debating which field your work falls on, is utterly pointless.

Edit_0: typo",hle913z,t1_hldl7xc,1637423217.0,False
qxxaab,"I'm going to refer to every medician, biologist, chemist, physicist, and mathematician as ""normal scientist"". This is because every field of endeavour is to further the knowledge of mankind, am I right? I'm being facetious here, I don't actually think this. 

The labels for each discipline of study exists because of the location where the locus of study is centred around. It's not surprising that there are overlapping themes, lessons, and tools that are shared between the distinct disciplines. The main idea is that the discipline of study is large enough that it deserves its own label to distinguish where is the locus of attention; the main idea is that the discipline has enough people thinking about the specific lessons for whatever motivation they decide.

Applied computer science exists upon the foundation set by the study discipline that we call computer science. No matter where you choose to apply the lessons of computer science, the foundational lessons of computer science remain a discipline of thought that exists without regard for application into real world practice. I consider applied CS to chemistry as computational chemistry. Formal proofs are still important within computational chemistry based on lessons learned within CS. 

For example, one lesson of general CS is that we can calculate the runtime cost and the runspace cost of any given computation. We can analyse whether the computation in question is a solution within P-time. We can analyse whether the problem itself is possible to be solved as a P-problem. If we find that the problem is a ""hard problem"", this means we won't be able to completely solve the problem on normal computers. One way to deal with this in the real world is to find a partial solution the problem. Or perhaps we find that the problem is so difficult that it's not actually possible to find a computation that gives a partial solution; we give up on trying to solve the specific problem and try to reformulate the problem into something that is feasible. Your average computer programmer who is untrained in these matters cannot apply this kind of analysis to the problem they are trying to solve; they could probably apply a naive ""hard"" solution because they don't know any better. This is just one lesson that is the consequence of understanding the cost of problems and computations, there are many other useful lessons that are a normal part of the computer science discipline.

I like to distinguish the discipline of computer science and the discipline of computer programming as being not the same discipline; you don't need computer science training to work as a computer programmer. If you consider yourself a trained computer scientist, then I expect to you be able to produce your formal proof that explains the different properties of your computation.",hlg1zwp,t1_hle913z,1637451737.0,False
qxxaab,Yeah that makes a lot of sense actually. My mind is changed for sure. I study the damn thing I didn't even consider that haha,hle6338,t1_hldl7xc,1637421906.0,False
qxxaab,"Well, maybe?  Even though i’m now a senior executive i still call myself a software engineer, but when i’m teaching computer science to high school students (through my non-profit), i will sometimes say i’m a computer scientist, especially when knee deep in talking abou measurement of algorithms or complexity classes of problem spaces.

I actually hate the field name ‘computer science’, but dont have one I prefer to replace it with.",hle43ni,t3_qxxaab,1637420986.0,False
qxxaab,"Yes absolutely why not, I successfully wrote hello world and can create html in notepad in windows.",hlekgjn,t3_qxxaab,1637428110.0,False
qxxaab,I do! :),hlddznd,t3_qxxaab,1637404035.0,False
qxxaab,"Refer to myself as a software developer personally but that also happens to be my current job title, even when it wasn’t though- I still use software dev",hle8b3y,t3_qxxaab,1637422904.0,False
qxxaab,no just a humble data janitor,hleaz0o,t3_qxxaab,1637424060.0,False
qxxaab,"No, i have a degree in computer but I'm a systems administrator",hleb0u6,t3_qxxaab,1637424082.0,False
qxxaab,I refer to myself as a senior data engineer,hlegahv,t3_qxxaab,1637426381.0,False
qxxaab,"No, I refer myself as a developer. Lately with the faaaaaaang stuff the lens on what developers should know has shifted to more to the comp sci frame ignoring so much more of the job.

So yeah, unless your doing research type activities your not a computer scientist (IMO).

It seems that ‘engineer’ is a title that some are using to differentiate between developers and those in between scientists.

In the end it’s just a title.",hlemac3,t3_qxxaab,1637428890.0,False
qxxaab,I have a BS in computer science and I’m an electrical engineer.,hlf3y8b,t3_qxxaab,1637436334.0,False
qxxaab,"No, i tell people Im a computer engineer so they don’t think all I do is plug in cables and open Microsoft word.",hlfa3e3,t3_qxxaab,1637439063.0,False
qxxaab,"It’s applied math using computers.

Third order of Hogwarts.",hlfonth,t3_qxxaab,1637445614.0,False
qxxaab,I prefer the term IT Lich.,hlfty37,t3_qxxaab,1637448040.0,False
qxxaab,"I do science with and about computers, so yes, computer scientist is a good term.",hlg3p6e,t3_qxxaab,1637452520.0,False
qxxaab,I have part of a degree in CS and a whole degree in liberal arts. I am a “byte wizard”.,hlgjhy6,t3_qxxaab,1637460097.0,False
qxxaab,Yes.,hle5jv2,t3_qxxaab,1637421658.0,False
qxxaab,Yes,hleok3v,t3_qxxaab,1637429840.0,False
qxxaab,Yes and a games programmer.,hlduumi,t3_qxxaab,1637416235.0,False
qxv7ln,"Because to mirror a write means it's not complete until the last disk completes. So you are only ever going to get the performance of your slowest disk.

 Reading, on the other hand,  can be split up between disks. So if you wanted to read 2000 sectors of data, disk one might read sectors 0 - 999 while disk two is reading from sectors 1000-1999. Thus twice the speed.",hlcjs43,t3_qxv7ln,1637381464.0,False
qxv7ln,"Writing already happens ""in the background"" essentially, because writes are cached at the OS level and/or the disk controller level. What we call the ""write speed"" is how long it takes for the write operation to completely finish.",hlcge8t,t3_qxv7ln,1637379616.0,False
qxv7ln,Unless I am mistaken RAID 0 does what you are describing,hlehuwr,t3_qxv7ln,1637427047.0,False
qxv7ln,"Yes it does, but then you lose the redundancy (RAID 0 is striped). What I am asking about is this system I laid out where you achieve that performance, but still have the redundancy. Its such a simple solution that it either has been considered already and there are unknown reasons why it wouldnt work, or its so simple that it hasnt been done.",hlemrne,t1_hlehuwr,1637429091.0,True
qxv7ln,"This is a really interesting question. Would like to see what others have to say. I think with most modern RAID cards the cache handles this then writes to both and eliminates most performance issues with 2 drive RAID1. I don’t have real world experience with this since I never use RAID1, but… https://arstechnica.com/civis/viewtopic.php?t=1216469",hlcc56i,t3_qxv7ln,1637377375.0,False
qxv7ln,"A cache could handle this. But if the systems fails and the cache got corupted, you loose the redundancy, which was the reason to use a RAID in the first place.

Modern RAID systems like build into ZFS have various ways of handling reading and writing.",hldwb7i,t3_qxv7ln,1637417052.0,False
qxv7ln,"Because if you call sync() you expect the data to be present REGARDLESS of what happens next (power outage, disk failure etc). What happens if your disk fails milliseconds after the call to write to the disk? How long are you willing to wait if the disk IO is 100%? 

You’re basically proposing your data to be written in RAID0 until the disk is free to repair itself. You can simply do that by enabling a writeback cache, that way your writes go to RAM until such time that the disk is available to finish the write.

There are various better ways of improving performance, sacrificing your data safety is generally the wrong proposal.",hlf8m8f,t3_qxv7ln,1637438394.0,False
qxrdbd,"It's called ""self-modifying code"".

It's uncommon because it's really hard to write and there is no benefit to writing such code. Why bother?",hlbfxnt,t3_qxrdbd,1637362686.0,False
qxrdbd,"Very true. And good luck maintaining someone else's self-modifying code!

OP, here's a [blog](https://blog.osteele.com/2006/04/javascript-memoization/) discussing self-modifying code that is elegant and clever—but isn't exactly transparent, so I'd worry about it in production. For example, can you guess what this is about?

`OSGradients.initialize = {  
  OSGradients.initialize = function() {};  
  ... // initialization  
}`",hlbiq7f,t1_hlbfxnt,1637363849.0,False
qxrdbd,You've reminded me of this interesting video from Computerphile (Dr Julian Onions) https://youtu.be/SWU_DgjSwRU,hlehc01,t1_hlbiq7f,1637426829.0,False
qxrdbd,"Isn't this ""trick"" quite well known for implementing memorization? I don't think it's that bad.

Of course, I still won't use it in production code - in Python, the decorator `functools.cached` implements memoization, and I'm sure there's something similar in JavaScript also.",hldf63u,t1_hlbiq7f,1637405038.0,False
qxrdbd,"you mean \`lru\_cache\`? Also, the above code isn't for memoization. At least, there isn't anything about it nearly similar to what I've seen and done with memoization.",hldgrqh,t1_hldf63u,1637406390.0,False
qxrdbd,"`cache` is a new decorator (in Python 3.9), which is basically equivalent to `lru_cache(maxsize=None)`. See the `functools` docs: https://docs.python.org/3/library/functools.html

In Python, we can do something similar to the JS code like this:


    class A:
        @property
        def x(self):
            ans = expensive_computation(self)
            del self.x
            return self.x = ans


Of course, this is not a one-liner, but I'm sure a one-liner is also possible 🙂

This code does memoization / lazy loading of some expensive computation. Of course, we don't need to do this manually - the better version is:

    class A:
       @functools.cached_property
       def x(self):
           return expensive_computation(self)

**Edit:** for some reason, the Python cide is not being formatted correctly.",hleq3ms,t1_hldgrqh,1637430492.0,False
qxrdbd,"There's lots of reasons to write self modifying code, such as it being fun, interesting and emergent. I'm sure there's plenty of academic reasons to do so too.",hlchmh1,t1_hlbfxnt,1637380279.0,False
qxrdbd,"Well, yes, there is value in doing things ""because they are hard"". Unfortunately, that value is not recognized by companies expecting production-ready code, and self-modifying code is hard to reason about or formally verify in general.  


I originally hoped to add some qualification to my answer by linking an ""self-modifying code competition"" or something like that, but that does not seem to exist. The closest thing is the IOCCC, and that is not really about self-modifying code since your code must be cross-platform (I believe).",hlci6ou,t1_hlchmh1,1637380590.0,False
qxrdbd,"I find it a lot easier to write self modifying code actually. One's mileage may vary. Anyway, I said ""fun, interesting, and emergent"" which has nothing to do with difficulty. There's few things more interesting than watching some self modifying code do something elegant, emergent, and unexpected. I write silly procedural games, not production ready code (for now), so there's neither harm nor foul in it. Academically it must surely have value for people really good at it, taboos notwithstanding. It's the emergence that is fascinating.

In my opinion, making code that is fully referentially transparent and clean is way harder. Just my amateur opinion.

Edit to clarify: nothing against clean code. It's important to be able to do. Just saying I find it harder.",hlciw37,t1_hlci6ou,1637380977.0,False
qxrdbd,"More power to you, but be careful if you ever have to be responsible for drafting an RCA",hlcrujw,t1_hlciw37,1637386249.0,False
qxrdbd,"I'm not bragging. I am self taught so I got used to what pleased me and would need to train up new habits to be a professional. I value all kinds of coding. 

May I ask what an RCA is? I am not a professional. Just an eager amateur.",hlcse3c,t1_hlcrujw,1637386589.0,False
qxrdbd,"Root Cause Analysis. Investigating a problem like a service outage, security vulnerability, equipment malfunction, etc.",hld0bs8,t1_hlcse3c,1637392283.0,False
qxrdbd,Many thanks!,hlezimv,t1_hld0bs8,1637434415.0,False
qxrdbd,"Yeah, Generally speaking, most development regards building production systems which need to be explainable when things go wrong. We’re already in the midst of a crisis in Machine Learning explainability; 

Self modifying code would be a development nightmare if it causes any problems for a project using that code.

If I were in charge of a build system, I’d scan for self modifying code in its modules and auto fail the build, and tell the devs to use different modules",hle0ue4,t1_hlcse3c,1637419411.0,False
qxrdbd,"My understanding of machine learning is lean at best, but is it even possible to do something like a neural net without some degree of emergence which is impossible to explain fully? That seems to be one of the reasons it works at all. Not all code can be exactly the sum of its parts. That field is very attractive to me but I'm not quite good enough to get into machine learning as deeply as I would like, yet. Can you explain this crisis in explainability to me, or point me to an article?",hlf0596,t1_hle0ue4,1637434689.0,False
qxrdbd,"I like to outline this crisis as such

Let’s say you build a bridge and after a year or so, it fails, killing and injuring a large number of people. The city needs three things: the families to be compensated, the bridge to be repaired, and justice to be served.

So you get sued, and the city does an investigation to determine what went wrong and why so they can fix it.

If the bridge is built by standard practices, and complying with regulations, they might find that their was a design flaw where something like temperature variations in the particular metals used caused joints to loosen and over time, caused structural failure. They’d be able to determine this because they have records of both the design specs and construction records.

But this analysis might not possible if* the bridge were designed by machine learning because the specs could be too novel to perform a sure analysis.

Result: unclear how to repair the bridge, and now you’re in jail for using designs that weren’t repairable

Case and point: explainability == accountability. If it can’t be explained, then its unaccountable and that is untenable when people’s lives are involved",hlgef69,t1_hlf0596,1637457614.0,False
qxrdbd,"That explains exactly why too much unaccountable emergence could be bad in production. Is there a way to make neural nets without it, though? Is it even possible to log every last step in the process? I would like to know more about the math behind machine learning and maybe make my own little neural net some day but I'm not there yet. What research I've done suggests it would be impossible or highly burdensome to make them fully explainable. For systems used in the real world which could have consequences on peoples' lives or legal standing, however, that burden seems justified if there is a way to achieve that emergence while logging everything. But then maybe you wind up needing another neural net just to parse the data!

Edit to clarify position: Just because it's hard doesn't mean it's impossible, and the benefits outweigh the risks. I would rather live in a world where we take a chance on these things.",hlggzyq,t1_hlgef69,1637458867.0,False
qxrdbd,"I believe in affording for experimentation, and leaving it isolated from work. Too many companies end up doing both trying to cut on costs and all it does is add technical debt",hlh9x6g,t1_hlggzyq,1637475045.0,False
qxrdbd,Can I trouble you for some examples? I don't work in the industry. I'm just an eager amateur who may or may not find a job doing it some day. I would enjoy some analysis from someone with more perspective.,hlha2k9,t1_hlh9x6g,1637475147.0,False
qxrdbd,"In addition to being hard to write, it would be hard to deploy and monitor. Not only do you have program state to worry about, but also source code state and whatever kind of reloader state that exists.",hle3der,t1_hlbfxnt,1637420644.0,False
qxrdbd,"I agree with hard to write, but disagree with there being no benefit.  In truth, we simply don't know if there are benefits.  Nobody has written a (non-esoteric) language that forces its use, or even encourages it.

For all we know, it could be the easiest way to create sentient AI, time machines, and replicators.

All that said, it's true there is no _known_ benefit.

As for why to bother?  Perhaps to be the first to make it easier and mainstream.",hldc8uf,t1_hlbfxnt,1637402480.0,False
qxrdbd,They are rare because you need to be a genius to write them without causing horrible bugs.,hlbyr1x,t3_qxrdbd,1637370929.0,False
qxrdbd,"Really hard to design and debug. The last time I saw it used was in microprocessor assembly for optimization purposes in a very constrained environment.
I wonder if a modern OS could even unprotect program memory to allow this in the first place, huge security hole if you can overwrite code...",hlbi71d,t3_qxrdbd,1637363622.0,False
qxrdbd,"Yeah, I've written a fair amount of self modifying code, but I work in a highly constrained environment where every single bit counts. If I know that a pile of instructions will never be called again (like initialisation code) well, that's just free real estate.

Edit: even then the best example I can think of was to add a bit of debug code so I could follow the code path. It's not very often I've shipped it in production",hldehja,t1_hlbi71d,1637404464.0,False
qxrdbd,[deleted],hlbreoq,t1_hlbi71d,1637367610.0,False
qxrdbd,"This is not true, you can't write in .text memory mappings in any modern OS. Those are mapped read-execute, without the write permission. You'd need to go out of your way to do it (and some security mechanisms sometimes prevent you from having a memory mapping write-execute, known as W\^X).",hlcanv5,t1_hlbreoq,1637376620.0,False
qxrdbd,"This is why we have a security team at work, they watch out for us 💯 and audit published code.  So much to keep track of.",hlcfx3i,t1_hlcanv5,1637379362.0,False
qxrdbd,Definitely. And RWX sections in memory are a huuuge giveaway that something fishy is going on!,hlclxmm,t1_hlcanv5,1637382681.0,False
qxrdbd,"That makes sense.  Too much time in low level environments for me, appreciate the confirmation 👍",hlbuj3o,t1_hlbreoq,1637369016.0,False
qxrdbd,Who told you this?,hlczipk,t1_hlbreoq,1637391648.0,False
qxrdbd,"I disagree with most of the answers given so far. I think most commenters are thinking of programs written in, say, C++ or assembly, and in the context of those languages it is true that self-modifying code is rare. 

But in many languages the self-modification of code is just a natural part of writing programs. Think about how in Python decorators work by dynamically defining an inner function. Or consider the low-level details of how Python works, how basically everything is a `dict`.

There is a related concept called [homoiconicity](https://en.wikipedia.org/wiki/Homoiconicity). A language is homoiconic if you can manipulate code as if it were data. Lisp and Prolog are the usually cited examples, but for me it was Mathematica that really made the concept click in my brain. In Mathematica, functions are just expressions that have a particular evaluation discipline.  So you might construct a mathematical function, say, and then apply that function to a list of values.  Or you might start with a function and then deconstruct the pieces of the function. For example, your function might be a polynomial, and your program might make decisions based on the degree or coefficients of the polynomial. Seasoned Mathematica programmers don't restrict the dynamic manipulation of code to just mathematical functions. 

So the answer is, people write self-modifying code all the time, but it's such a natural part of the languages in which it is done that it may not *feel* like you are doing anything fancy.",hlcprj9,t3_qxrdbd,1637384951.0,False
qxrdbd,"mathematica and wolfram lang deserve more love (and to become open source so that it finally starts gaining contributors and traction)

and yeah, people cite lisp bc it was the first one, and the one where it's actually not weird to do this. functional languages do this all the time. it's fun.",hlcwjz8,t1_hlcprj9,1637389430.0,False
qxrdbd,This is an extremely good answer.,hlfenrt,t1_hlcprj9,1637441113.0,False
qxrdbd,"In a very real sense many, perhaps even most programs will modify themselves all the time - you just need to expand your idea of 'themselves' beyond the base source code. Quite often the source code is a seed for some other thing, and that other thing is what is actually being executed, and will modify itself on the fly a whole lot. Polymorphism in OOP is probably the most approachable example of this.",hlc5340,t3_qxrdbd,1637373881.0,False
qxrdbd,"It is called meta-programming and I once tried to write a ""meta-program"" or a program that modifies itself. It was an year ago, and it was JavaScript... so I could never complete writing the program because it was very hard to write it and there are ""relatively"" less resources about meta-programming (particularly js meta-programming) on the internet. 

It has a use case, just like my case, it was a blockchain(kind of) written in node.js(ikr bad choice) and I wanted the stdin value to be the name of a variable which stored an object `Wallet`. My program's workflow was like this: it would accept a value via stdin [using `readline` module of node.js] and then it would use the value inputted by the user as the variable name to store the user wallet in. Please do not hate me for this approach, ik this is a shitty approach but I just used it because I coded it in less time and did not want to set up a database kinda thing.

Edit: if anybody is interested to know how to do the task I wanted to do, I later found the solution, it can be accomplished using promises in JavaScript.",hlcg88p,t3_qxrdbd,1637379528.0,False
qxrdbd,Self mutating codes are gold mine for malware researchers. They are easy to hide from antiviruses.,hlcj5el,t3_qxrdbd,1637381120.0,False
qxrdbd,"If you think of an operating system + all the currently running programs as a single program, then that program is self modifying. It modifies itself each time a program is opened or closed.",hlbrvc7,t3_qxrdbd,1637367816.0,False
qxrdbd,"To extend that, VMs like Java Virtual Machine (JVM) that have Just in time compilation (JIT) enabled, are technically self modifying.",hlctf73,t1_hlbrvc7,1637387262.0,False
qxrdbd,"It's called meta programing , if language is designed around it it can be viable ,its pretty good in Lisp family of languages specially in Racket",hlc3d6n,t3_qxrdbd,1637373061.0,False
qxrdbd,Getting downvoted by COBOL programmers.,hlcfif1,t1_hlc3d6n,1637379148.0,False
qxrdbd,"I don't understand it ,was something wrong with my answer ?",hlcieii,t1_hlcfif1,1637380706.0,False
qxrdbd,"I don’t think so. I think it’s a wonderful answer. We’ve literally had this since the late 50s, and people shied away from it. So weird. A tool is a tool",hlcivap,t1_hlcieii,1637380964.0,False
qxrdbd,"Now those downvoted seems to be gone ,weird !",hlcry1z,t1_hlcivap,1637386307.0,False
qxrdbd,I may have meta-programmed them.,hlcwft6,t1_hlcry1z,1637389347.0,False
qxrdbd,"As others have said, if debugging and maintaining someone else's code normally, imagine how it would be with the added difficulty of the code changing every time you opened it on an editor/IDE, any possible optimizations would seldom be worthwhile.

That said, there are some cases where the use of self-modifying code techniques are particularly common, malware development immediately comes to mind.",hlc5bo3,t3_qxrdbd,1637373996.0,False
qxrdbd,"There’s some use cases discussed in papers in Lisp.

Programs that edit themselves sounds dope. I’ll try it out.

I don't think saying it's useless is demonstrably correct. It's a tool, the fact is that people haven't been able to think on use cases. I know it should be really useful for generative systems. 

We must galaxy brain meme this and eventually someone will figure something out.",hlcfdsy,t3_qxrdbd,1637379081.0,False
qxrdbd,Do you want Skynet? Because this is how you get Skynet!,hlcnsg0,t3_qxrdbd,1637383756.0,False
qxrdbd,"Skynet runs on lisp then, we've had meta-programming in Lisp since when it was implemented in 1959. 

Crazy stuff.",hlcwetp,t1_hlcnsg0,1637389328.0,False
qxrdbd,"So what you're saying, is we're absolutely safe from human destroying robots?  Because this is the internet, so I'm going to believe you, and use this conversation as source material down the road.",hld7vff,t1_hlcwetp,1637398591.0,False
qxrdbd,At least we didn’t get killed by Java Skynet.,hldbcgm,t1_hld7vff,1637401691.0,False
qxrdbd,Because it couldn't grow past 3B devices,hldlnrx,t1_hldbcgm,1637410275.0,False
qxrdbd,"In many computer architectures (usually of the RISC variety), the instruction cache does not have to be coherent with the data cache. Thus, a self-modifying user-mode program would have to manually manage instruction cache invalidation (by calling the OS). This is a sufficiently difficult undertaking, so in terms of practicality, self-modifying programs may as well be impossible on these architectures.

It's been pointed out in this discussion that when an OS loads a program and executes it, it is itself a self-modifying program. This is correct. It can be generalized; from a strict technical perspective, all stored-program computers are by their nature, self-modifying, unless the program is in ROM and never changes. The OS can be self-modifying because it's much more restricted. It knows where it's loading the program into, and can invalidate the instruction cache accordingly.",hlcp91v,t3_qxrdbd,1637384633.0,False
qxrdbd,"One archaic use case that hasn't been mentioned is for bootstrap loaders back when ROM was not common and, when available, very small.  The first computer I programmed machine language for would boot by putting a -7 in the X register, 2 in the PC, an input from channel to location 2 in the instruction register, and start the paper tape reader in binary 4 character mode.  On the tape you had:

    2: input to location 10 indexed
    3: increment X and branch to location 2 if X negative
    4: load X with 9
    5: input location 0 indexed
    6: skip if buffer not ready
    7: increment X and branch to 5 (X always negative)
    8: branch to start address
    9: (load address + sign bit)
    10 ... end-of-tape (whatever you want)

Anybody know which machine it was? \[Hint: it wasn't named after a radical student group\]",hlcuxao,t3_qxrdbd,1637388274.0,False
qxrdbd,Wouldn't that just be ai?,hlcwm1o,t3_qxrdbd,1637389473.0,False
qxrdbd,They aren't? JIT compilation is pretty common our days.,hlcypii,t3_qxrdbd,1637391023.0,False
qxrdbd,"Self-modifying code can be used for obfuscation - making a code much harder to analyze and reverse engineer. Can be for malicious purposes or just plain anti-reversing (you don't always want your code to be easily understood, as it implements some algorithm you do not want people to know how it works).",hld4wo9,t3_qxrdbd,1637396028.0,False
qxrdbd,Would polymorphism count as style modifying to you?,hldhlnx,t3_qxrdbd,1637407070.0,False
qxrdbd,"Yes, they make reference to it in a book called ""The Art of Computer Virus Research and Defense"" (Symantec Press) by Peter Szor. Part 1, Chapter 7, section 7.6, page 269, ""matamorphic viruses"".

> Virus writers try, of course, to implement various new code evolution techniques to make the researcher’s job more difficult. The W32/Apparition virus was the first-known 32-bit virus that did not use polymorphic decryptors to evolve itself in new generations. Rather, the virus carries its source and drops it whenever it can find a compiler installed on the machine. The virus inserts and removes junk code to its source and recompiles itself. In this way, a new generation of the virus will look completely different from previous ones.

If you're interested in this area take a look at a Java library called Java poet.

https://github.com/square/javapoet",hldit2w,t3_qxrdbd,1637408048.0,False
qxrdbd,"Well if you’re willing to accept that a computer program = source code + data, then modifying data is akin to changing program behavior. Under that interpretation, this happens all the time",hle3jfa,t3_qxrdbd,1637420720.0,False
qxrdbd,"I don’t think it’s a stupid question. Here is an example of a self-modifying software which is capable of running compiled C code in Linux, Windows, macOS, and BSD. It sounds like sorcery, but it’s actually an incredibly clever piece of code. 

https://redbean.dev/

http://justine.lol/cosmopolitan/",hlep0i7,t3_qxrdbd,1637430039.0,False
qxrdbd,It's a pain in compiled languages (which rule serious development),hll1iy5,t3_qxrdbd,1637544344.0,False
qxrdbd,"Oh yeah, that’s called Siri",hlbyvc9,t3_qxrdbd,1637370982.0,False
qxrdbd,Its name is Madness ...  MADNESS I SAYYY,hlcmfsf,t3_qxrdbd,1637382969.0,False
qxl1o1,"DHTs are a building block to the larger whole. So, simply put, it depends on the system where the DHT is being used.

There is nothing about DHTs in and of themselves that declares how data is stored. It simply defines how keys map to values and how many different nodes can connect/disconnect without causing considerable disruption to the system.

In a real application, one might PUT some data into the DHT and the implementation thereof will decide where to store it and with what actual backend. When one wants to GET the data, the implementation will get it from any available node that has it.

Take BitTorrent as an example. The content is described by a hash, and clients look up the hash. The DHT responds not with the data itself but with IP/Port pairs that the client can connect to in a subsequent request to download the data.

Don't know if this helps or not. Basically, how and where the data is stored is not defined. That's up to the implementation. DHT simply defines the infrastructure of the table and how it can maintain high availability with many nodes.",hla63sv,t3_qxl1o1,1637344916.0,False
qxl1o1,"Got it! Thank you so much, this cleared up a lot of things for me.",hlaija2,t1_hla63sv,1637349514.0,True
qxl1o1,Glad it could help!,hlb4xeq,t1_hlaija2,1637358129.0,False
qxixkk,"Understanding functional programming is important in common languages as well. 

When you pass something by reference to a function, does it modify that object? Is it clear that that object is modified? That's a common source of bugs in C# or Node. 

Additionally, modern frontend frameworks (Redux, rxjs, etc) can be very functional programming. You have a state. That state can't be modified, so everything computing the new state is functional (ish). 

It's not something that's used as a whole all the time, but the concept of ""make it so that nothing is changed behind the scenes without making it very very clear"" is very useful in a lot of situations.",hl9pvgr,t3_qxixkk,1637338568.0,False
qxixkk,"But you can write immutable classes in any OO language. It is a very fundamental concept called Value Object in DDD, which is like 20 years old.",hla4wwm,t1_hl9pvgr,1637344451.0,False
qxixkk,"Its not about immutability really. People tend to get caught up in that, but the biggest selling point of fp is composability. Functions as truly first-class values is a very powerful feature that allows simple and  robust code reusue. Mutability doesn't compose well, so it's discouraged (or disallowed) in fp. Combine first class functions with rich static typing and expression based control flow, and you've got yourself a language that is phenomenal for writing any deterministic or highly formal software, e.g. compilers, financial systems, theorem proving, verification, AI, etc.",hlacgke,t1_hla4wwm,1637347330.0,False
qxixkk,"> But you can write immutable classes in any OO language.

And that gives you some of the benefits of programming in a functional style. In the same way, you can write functions that act as object methods in C, and that gives you some of the benefits of OO programming.",hlb3yo1,t1_hla4wwm,1637357738.0,False
qxixkk,"FP is over 60 years old, and the math behind it over 80. I don't know the history of  the value object idea, but in general, FP has been informing other paradigms for a long time. So the ability to easily implement FP ideas in not-primarily-functional languages is often thanks to FP.",hlaqfdk,t1_hla4wwm,1637352461.0,False
qxixkk,I agree with u/raedr7n and want to add that sometimes you can't make the whole thing 100% immutable (eg what if a deep copy takes HUGE amounts of time?). It doesn't mean that functional programming isn't useful. Making things composable (aka reusable) and limiting side effects is always a good programming practice.,hlarb7v,t1_hla4wwm,1637352798.0,False
qxixkk,I guess that makes sense. I mean even in an OOP language you’d want to minimize side effects. Writing clean and concise code seems to follow some FP principles. Although I still don’t understand how FP languages would work in a complicated engineering systems that needs to do a lot of different things,hlaovup,t1_hl9pvgr,1637351877.0,True
qxixkk,"Depending on what the specifics of that complicated engineering system entail, there are FP patterns like monads and sum types that can capture and handle a lot of complexity.",hlbhsf1,t1_hlaovup,1637363453.0,False
qxixkk,"I guess where I get stuck is state changes. For example, let’s think about a cars computer. It probably needs to know what gear you are in, which is a state. It probably wants to know if you are breaking or accelerating, which  a state can keep track of. Same goes for using your turn signal, headlights, etc. 

How do you get around not using the state for these objects? I get that for math functions, FP is very good at solving them and maybe even can eliminate some complexity. Maybe people using python are using FP without even realizing it. But how do you ignore state for every case?",hlbn4ak,t1_hlbhsf1,1637365730.0,True
qxixkk,"You'd probably have some type whose values are all states the car can take on, and then you can have functions that alter this state -- that is, they take in the old state and produce a new one. Such functions can be cascaded (composed) to create more complex functions that alter the state. Ultimately, you can simulate stateful systems with fundamentally stateless components (pure functions). 

If you mean to suggest something more like changing the state of something external to the program, something that will have some effect in the real world, then this is something that is addressed in purely functional programming. Altering external state is a behavior of impure functions (functions like print) which makes it less straightforward to model them as mathematical functions. Purely functional languages like Haskell, Clean, etc. have their own solutions to this problem.",hlc484n,t1_hlbn4ak,1637373470.0,False
qxixkk,"I work on a production Haskell codebase.
Basically, you do have state. Haskell allows you to allocate memory and pass/modify it by reference.

These actions have to occur in an expression with the type IO. But the top level entrypoint, `main`, has the type IO, and so there's no fundamental barrier to using state at the top level.

You use pure functions as the default, since the compiler can so aggressively optimize them with out-of-order execution and fusion. But for things that have to be stateful, you give them the type IO, which effectively just turns those optimizations off and makes the machine execute your function line by line like an imperative program.

There are more elaborate APIs people often wrap IO with, but that's the basic idea.

Edit: the other big reason to use pure functions, beyond compiler optimizations, is that you can eliminate while classes of runtime errors from ever occurring. Pure functions should never throw an exception or close a socket when it shouldn't have, etc.",hlf8gu8,t1_hlbn4ak,1637438325.0,False
qxixkk,"From a high level perspective, functional languages are nice because they are very deterministic. Immutable data structures and very strict typing allows for a clean and predictable program.",hlb1zcp,t3_qxixkk,1637356943.0,False
qxixkk,"Functional languages aren't a good fit for everything, but they tend to excel whenever there's a complex domain model and/or a pipeline-like solution to the problem at hand. I wouldn't write a game in Haskell, just like I wouldn't write a compiler in C++. Vice versa, however, would work great.",hlab158,t3_qxixkk,1637346810.0,False
qxixkk,This,hldomnf,t1_hlab158,1637412401.0,False
qxixkk,"It is easier to analyze mathematically (in terms of runtime, correctness, and so on) and some people see it as ""pure"" computer science.  

I remember one of the core Haskell developers as saying, Java has started from a powerful but unsafe place.  Haskell has started from a safe but powerless place.  As time has gone on, Haskell has become increasingly powerful while maintaining safety, and Java has become increasingly safe while maintaining its power.  Both are converging to best-of-both worlds from both directions.  From that perspective if you were more concerned with safety than power, you might prefer a functional and strongly typed language.  

I've heard about recent developments in OCaml about using it (I think through Merlin) to utterly get rid of the layer between it an C.  If this is very successful it offers the promise of a simpler process to deliver an executable, but this has not yet been fully fleshed out.  There are promises about when and if this will happen but ... you know.  ""Promises""

Ultimately, I can't fully understand certain people's love of functional programming other than it being a religion.  I'm much more omnivorous.  I like OO, I like functional, I'll do whatever the fuck I want.",hlbmqj9,t3_qxixkk,1637365567.0,False
qxixkk,Hahaha that’s fair. Honestly I feel like I usually use OOP at a higher level but then within the class I try to adhere to some FP principles when writing classes,hlbnf69,t1_hlbmqj9,1637365859.0,True
qxixkk,"Here’s a nice little discussion on the topic:

https://stackoverflow.com/questions/36504/why-functional-languages",hl9n7yq,t3_qxixkk,1637337501.0,False
qxixkk,">The languages that FP is generally used in are annoying to write software in, as well.

Chances are that you think so because you've mainly used imperative / OO languages. I'd argue that writing Java or C# is *way* more ""annoying"" than Haskell or F#.

>but I don’t understand how it could work for more complex systems

Just try it",hla9k98,t3_qxixkk,1637346263.0,False
qxixkk,Every single large system I've ever heard of being written in a functional programming language since event-driven programming became the mainstream has regretted it. The paradigms don't mesh well.,hlbeekv,t1_hla9k98,1637362055.0,False
qxixkk,"Can you name some of those systems? And funnily enough even driven programming works \*very\* well with a functional style (see for example Scott Wlaschin's ""Domain Modeling made functional"").  


Certainly there are a few failed projects but the same is true for (for example) OOP and there's also very succesful projects / companies that use FP succesfully for large scale projects (e.g. the FPGA design system and hardware synthesizer of QBay (and compilers in general), Facebooks spam detection, (sadly) a bunch of crypto shit, Hasura's whole platform, ... or just look at Erlang's (German - the English one doesn't list all the companies and projects) wikipedia article to see how insanely good FP can work)",hlbh5dq,t1_hlbeekv,1637363186.0,False
qxixkk,Reddit and Yahoo are the classic examples.,hlbl15w,t1_hlbh5dq,1637364829.0,False
qxixkk,"Yeah trying to follow the state of a mutable object as it's passed around a graph of classes and method calls is annoying. Trying to retrofit error handling in a codebase fill with `throw`, trying to reason about which functions throw exceptions and when is maddening.

Good thing is imperative language designers are more and more understanding the value of FP and adding features. It took Java 19 years to get closures, but hey, progress is progress",hlbfu9x,t1_hla9k98,1637362647.0,False
qxixkk,Because the grass is always greener on the other side,hl9pyvt,t3_qxixkk,1637338605.0,False
qxixkk,"For the right situation fp can be really great. I work in data science, and we use languages like haskell or R to do transforms quite a bit. When you care a lot about being to reproduce results  and be aware of all changes happening to data fp is a nice paradigm to be in.

I've heard they can be used for scalable software engineering but I don't know much about that.

&#x200B;

I imagine for a lot of people, solving problems in a functional way is very satisfying and that will bias their opinion a bit even if it not always the most practical aproach.",hl9zg5o,t3_qxixkk,1637342326.0,False
qxixkk,"You use Haskell?  [intriguing](https://c.tenor.com/CSvXOimoG5kAAAAC/spock-eyebrow.gif)  Does Haskell have dataframes?

(For anyone who is curious, scientific programming has two roots, research and AI which does in fact has a heavy FPP and [literate programming paradigm](https://en.wikipedia.org/wiki/Literate_programming) history, though more LISP than Haskell, and super computer programming in FORTRAN.)",hlcfsgg,t1_hl9zg5o,1637379295.0,False
qxixkk,"Haskell doesn't have dataframes natively, but there are libraries implementing it, such as:

[https://hackage.haskell.org/package/Frames](https://hackage.haskell.org/package/Frames)

(I have done no work to evaluate if this library is any good.)

But I suspect dataframes are one of those things that will only ever be productively useful in a dynamically typed language like R or Python.",hlh1ukj,t1_hlcfsgg,1637469951.0,False
qxixkk,SICP does a great job explaining this.,hlaz080,t3_qxixkk,1637355750.0,False
qxixkk,Go back to /g/,hlb2rhb,t1_hlaz080,1637357256.0,False
qxixkk,"Functional programming is awesome in every regard, including ease of writing and learning.

The problem is you're (likely) coming at it from an Object Oriented viewpoint, and seeing the more complex aspects of Functional programing. It's like a procedural programmer seeing object oriented for the first time. 

To put it simply, it's a known fact that any task you can do in programming, you can do in Procedural, Object Oriented, and Functional.

The biggest way I can highlight the advantage of Functional is this: If you want to break down working code into its most indivisible units that still work, what you're going to be left with varies on which it is. Procedural will likely leave you the whole program. Object-Oriented will frequently leave you monolithic Objects with large supporting libraries. Functional will leave you with... one line. 

And that makes all the world of difference. If you're hopping into some new code, Procedural requires you understand the entire program.   
Object Oriented requires you understand the object, the connected libraries, the state, the variables, the ways output can vary from the object, etc.   
Functional requires you to understand one line. 

Further, Functional is insanely more reusable than Object Oriented since it's so atomic. Anything you've done before, you can treat like a buffet table, grabbing what you need. 

After you've grasped it, coding in functional is faster, cleaner, more effecient, easier to pick up, snappier, and all around more fun to write.",hlax6ob,t3_qxixkk,1637355044.0,False
qxixkk,"> Functional requires you to understand one line.

Which one line is that?",hlbn851,t1_hlax6ob,1637365776.0,False
qxixkk,The one you want to change.,hleebcf,t1_hlbn851,1637425536.0,False
qxixkk,"Well, what complex systems have you tried writing in a functional language?

But you have an important point, and as much as I like functional programming, I have to concede it's frequently pretty horrible to have to debug functional code. It's also a lot less straight forward to read and comprehend than some other styles, which matters a lot for maintainability. In most real world scenarios, I think functional programming should be used to complement a more direct, imperative/OO programming style.",hlb3cog,t3_qxixkk,1637357491.0,False
qxixkk,"I tend to find most people exist within a sub-domain in programming, of which there might be some overlap and they decide that whatever works best for them, must be the one true way.",hlazx2a,t3_qxixkk,1637356111.0,False
qxixkk,"I learn JavaScript as my first programming language and as I was introduced to OOP in JavaScript by MDN and FP in JavaScript by [Academind](https://www.youtube.com/watch?v=aoE-92Ac4zE). I also learnt Java which has one of the best OOP styles.

Personally, I like OOP more be it the standard OOP or prototypal OOP. In JavaScript, I had to write a lot of functions to accomplish a task and considering that JavaScript recommends using camelCase and my problem of failing to think of nice function names, it was difficult. 

When I tried to accomplish the same task in OOP js, it was a lot easier for me primary because code can be organized into objects and nested objects which makes it very easy to name and remember the method names like `bank.depositCash` rather than its FP counterpart.

Now, I mainly use Golang and Rust as my daily drivers so I like FP and you can really use FP in complex systems. I like FP in Golang and Rust primarily because of their module systems, like in Golang, packages contain modules and modules contain functions which makes it really easy to import and use them without any hassle.",hlciedy,t3_qxixkk,1637380705.0,False
qxixkk,"They're caught up in [The Churn.](http://blog.cleancoder.com/uncle-bob/2016/07/27/TheChurn.html)

----

Functional programming is older than Object-Oriented Programming, yet people are rediscovering it like it's something brand new. Like ""protocol-oriented programming"" or the other ""programming"" styles floating around that are supposed to be better than sliced bread.

Everything is a tool. Some tools are more useful for some tasks than other tools. You wouldn't drive a screw with a hammer, nor would you pound in a nail with a screwdriver.

And sometimes you use one tool wrapped in another because it solves the problem better. For example, I will routinely (in Java) wrap functional programming in singleton objects and anonymous classes declared from protocols; basically I'm using the class and interfaces as a sort of namespace mechanism to isolate internal state of a collection of functions.

But they're just tools, and a good craftsman learns how to use all the tools in the toolkit, and when it's appropriate to use them.",hlatamz,t3_qxixkk,1637353553.0,False
qxixkk,"Okay that’s similar to what I do, I think. Like OOP at the high level, but utilizing some FP principles inside of the classes",hlay0wj,t1_hlatamz,1637355370.0,True
qxixkk,"Well, Java kinda thinks everything is an object. And while Swift has the notion of functions and function closures, it seems happiest if you organize things around objects as well. 

I also do a lot of mobile iOS and Android development, and OOP is the primary hammer used in making user interfaces. So there's that.",hlb1u5e,t1_hlay0wj,1637356885.0,False
qxixkk,"functional programming help a lot to write explicit and elegant code without side effects using immutability and pure function.

It's a way more easy to ready and understand. Another great feature of functional programming is pattern matching.

Null or nil value are commonly badly used to verify if something has failed or is not allocated. Null or nil value are not explicit, this doesn't give you any explicit information to help you.

That's why such language like Rust use enumeration like Result<S, E> ou Option<T>",hlax5yu,t3_qxixkk,1637355036.0,False
qxixkk,"No state, which is the source of many problems, keeping track of state and communicating mutations of state across the system.",hlaznvz,t3_qxixkk,1637356009.0,False
qxixkk,Hmm interesting. But don’t many problems in software involve changing a state?,hlb14ct,t1_hlaznvz,1637356596.0,True
qxixkk,"Functional programming forces the programmer to be explicit about where state lives in their program and how that state changes.

This can make things quite a bit easier to deal with than the worst case of OO programming, where state is spread all through a graph of objects, any of which can change arbitrarily at pretty much any time.",hlb3h1t,t1_hlb14ct,1637357541.0,False
qxixkk,Lambda calculus is also Turing-complete,hla7i86,t3_qxixkk,1637345473.0,False
qxixkk,Yes..? Why is that worth mentioning?,hlacsz3,t1_hla7i86,1637347453.0,False
qxixkk,"I suppose it highlights that the expressive power of FP languages (which build upon the lambda calculus) is equivalent to imperative ones, as both are Turing complete?",hlc4t1v,t1_hlacsz3,1637373747.0,False
qxixkk,"I don't ""know"" the answer, but I speculate it might be similar to why mathematicians enjoy solving problems. It's kind of like a fine art. When you have it mastered and you can accomplish difficult tasks in a fascinating way, it's really rewarding.",hlaku88,t3_qxixkk,1637350359.0,False
qxixkk,Yeah that makes sense to me. Maybe it’s like mathematicians/physicists look at a problem differently than an engineer would because they apply different principles when solving problems,hlamh5p,t1_hlaku88,1637350977.0,True
qxixkk,"That’s a flawed analogy, because it implies that functional programmers aren’t doing engineering.",hlb6l4y,t1_hlamh5p,1637358796.0,False
qxixkk,"Yeah I didn’t mean to imply that. I guess I was just trying to say that engineering principles seem to be focus on modularity and breaking big systems down into smaller subsystems that operate without know what the other systems are doing (kind of like how your car’s engine doesn’t care if your headlight is broken. They operate somewhat independently and the whole system is modular). OOP fits that idea pretty well for the most part, as long as there isn’t too much encapsulation/inheritance",hlbm9hk,t1_hlb6l4y,1637365363.0,True
qxixkk,"It is true that FP is impractical from the perspective of complex systems, however, what is fascinating about FP is never the engineering applications, but the clarity of understanding that it brings. FP is lesson 101 in programming language theory. We learn to replace design patterns with higher-order functions, to eliminate illegal states with ADT, to improve program composition with abstraction, to prove the correctness of programs with invariant. These things are important enough on their own, furthermore, they also open the door to further study of PL theory.",hlbguj3,t3_qxixkk,1637363062.0,False
qxixkk,"Yeah that has been my understanding of it as well. It is good for a lot of theoretical stuff and proving things out, but can be difficult to implement in the real world",hlbmis6,t1_hlbguj3,1637365474.0,True
qxixkk,"Functional programming can be super useful for certain types of software. For example, we used the functional language OCaml in my compiler construction course in college. I couldn’t imagine how much harder that would have been in an imperative language. The recursive nature of a functional language was quite useful in that context.",hlarf39,t3_qxixkk,1637352839.0,False
qxixkk,"All I can say is I enjoy it a lot, I get a greater sense of satisfaction solving problems in FP, it’s made my work in imperative code bases better and the most complex things I’ve ever built have been in Haskell.

Edit to add: FP isn’t a new shiny thing for me though, I first learned it about 20 years ago.",hlb6pox,t3_qxixkk,1637358847.0,False
qxixkk,"FPP is a toolkit, a set of tools, not a singular tool.  It's easy to overlook this, because FPP programing languages can be limiting at first and seemingly unusually restrictive.

Because FPP is a set of tools, many modern mainstream languages have taken pieces from FPP.  If you're using a popular mainstream programming language today, you're using bits and pieces of FPP already without knowing it.

>Why do some people like it so much and act like it’s the greatest?

Some people like some of the tools FPP provides.  It's ideal to ask them what parts they like to get a better idea.  Odds are the parts they like you've already used and might be able to relate with them, if they don't use convoluted terminology to explain themselves so they are easy to understand.",hlcf46h,t3_qxixkk,1637378939.0,False
qxixkk,What does FPP stand for?,hllsj3e,t1_hlcf46h,1637557725.0,False
qxixkk,Functional programming paradigm.,hlm13jk,t1_hllsj3e,1637563360.0,False
qxixkk,Why not just call it FP?,hlnefhj,t1_hlm13jk,1637596165.0,False
qxixkk,"1. The conceptual simplicity of pure functions allows compilers to perform some elaborate optimizations they otherwise couldn't. E.g. [Elm is blazing-fast and builds small bundles](https://www.freecodecamp.org/news/a-realworld-comparison-of-front-end-frameworks-with-benchmarks-2019-update-4be0d3c78075/), and Haskell performs favourably in benchmarks relative to other high-level languages, sometimes approaching the speed of e.g. C. (YMMV, sufficiently complex Haskell applications tend to have hard-to-debug space leaks owing to ""laziness"", though the community has been moving towards more strictness in I/O, and laziness is not a feature of all functional languages. The community has also been building more understanding of how to fix these issues in recent years.)
2. Fearless refactoring. When people talk about ""functional programming"" these days, they often implicitly include a static ML-family type system. These expressive type systems allow you to encode a lot of information, preventing nonsensical states from being represented in the language. E.g., it's generally impossible to represent an out-of-bounds enum member in Haskell, unlike in C. When adding a new variant of a data type, the compiler will enforce that you handle it in all of your switch statements. Also, impure functions are carefully restricted into the IO type and every type ""inheriting"" from it (i.e. every instance of the MonadIO class). That means that when you're refactoring non-IO code, you can be sure that you haven't introduced an error merely by changing the sequence it's written in.
3. Effective abstraction encouraging uniform interfaces. Historically, software engineering has taken a very unscientific approach to design patterns. But many of them are patterns that are studied by computer scientists and mathematicians. There are arbitrary API differences in many languages between strings, arrays and vectors, but Haskell (specifically via its ML-style type system) allows a high degree of polymorphism for functions acting on these types. Now that I know the common interfaces (the most common are functor, applicative, monad, monoid and traversable), I find myself being able to pick up new libraries very quickly without having to learn their specific, ad-hoc interfaces.",hlg8eas,t3_qxixkk,1637454716.0,False
qxixkk,"Also, /u/WiggWamm, when a program is expressed in pure functions, it's trivial to parallelize it. Most of the speed increases in CPUs in recent years have happened via increasing core count instead of increasing clock speed or register width. Therefore, speedups in program execution time are mostly going to come from improved parallelism and concurrency. Parallelism and concurrency with conventional programming languages is difficult to get right, and takes lots of time. But the fact Haskell has parallelism API that is so simple is an enormous advantage.",hlh1kvj,t1_hlg8eas,1637469790.0,False
qxixkk,"I like this article to explain some things https://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html

But in a nutshell it makes code easier to reason about, easier to compose and refactor, and less prone to bugs.",hlb6cq5,t3_qxixkk,1637358700.0,False
qxixkk,"Well it's just plain fun ,not everything has to be 100% practical . If you like pythonic code fp in Haskell at least is like that on steroids . I like it specially for doing problems on code wars",hlc2c6q,t3_qxixkk,1637372584.0,False
qxh10d,"Listen, learning C has nothing to do with compiler design, first study compiler design, then try to relate the concept. And you can find on internet the compilation process of a C program.....that is pre processing, compilation , assembly , linking . U can study these steps in details",hl9bk0m,t3_qxh10d,1637332567.0,False
qxh10d,"Yes , but I want to study it to get more confident that what actually is happening behind the scenes. I tried to study it in this way , like when I am not able to relate any concept I got for searching over internet and its a lot more time taking. Thank you for responding!!!",hl9d8p9,t1_hl9bk0m,1637333321.0,True
qxh10d,"Yes, you are doing good work, whenever u in doubt feel free to search, there are multiple courses on YouTube, one I can recommend is neso academy, there others too, but I liked their way of teaching",hl9msnv,t1_hl9d8p9,1637337334.0,False
qxh10d,Neso academy is fantastic.,hla3y60,t1_hl9msnv,1637344076.0,False
qxh10d,[deleted],hl9qclz,t1_hl9d8p9,1637338753.0,False
qxh10d,"I referred several books like Let us C , C in depth and also I read from my professor and internet. I also followed KN King for some topics but Ritchie is the book that I read almost complete.",hl9tfr4,t1_hl9qclz,1637339983.0,True
qxh10d,[deleted],hl9trih,t1_hl9tfr4,1637340112.0,False
qxh10d,"Yeah I started revising C. I can understand most of the things but where I am lacking is , I am not able to related Compiler theory I read on my course to the programming. Like I am getting this or that error what might be the cause in term of compiler and many other things.",hl9uo2g,t1_hl9trih,1637340467.0,True
qxh10d,[deleted],hl9uyhp,t1_hl9uo2g,1637340579.0,False
qxh10d,"Haven't you read compiler design till now? My intention is when I read CD it was more theoretical and examples used form real programming were very much less. So I want to see that theory happening in real C programming. But I think no one studies in this way, I am the only one ( may be I am thinking wrong ) :), and no one gonna write a book or course just for me. Thanks for your consideration.",hl9vr94,t1_hl9uyhp,1637340889.0,True
qxh10d,"I would not try and understand how the higher level abstractions translate to the lower C level. I would instead learn from first principles on how a computer works and build the abstractions up from there. You will learn how a CPU works. How the data bus and registers are used. How memory is laid out and accessed. The call stack and how that works, etc.. This will go a long way in understanding how C sits on top of this and how it's data structures like arrays and structs map to this and understanding how pointers work the way they do and why. Check out these resources:


1. Read [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. Watch [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
5. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
6. Grab a copy of [C programming: A Modern Approach](http://knking.com/books/c2/index.html) and use it as your main course on C.
7. Follow this [Tutorial On Pointers And Arrays In C](https://github.com/jflaherty/ptrtut13)

The first four really help by approaching C from a lower level of abstraction (actually the absolute lowest level and gradually adding layers of abstraction until you are at the C level which, by then is incredibly high!) You can do all four or pick one or two and dive deep. The 5th is a great introduction to computer science with a decent amount of C programming. The sixth is just the best tutorial on C. By far. The seventh is a deep dive into pointers and one of best tutorial on pointers and arrays out there (caveat, it's a little loose with the l-value/r-value definition for simplicity sake I believe.)",hl9ga7q,t3_qxh10d,1637334643.0,False
qxh10d,Ty for this list,hlaalla,t1_hl9ga7q,1637346649.0,False
qxh10d,I'm taking th CS50 class now. It's amazing. Well thought out lectures. Long but fun.,hlcwjcy,t1_hlaalla,1637389418.0,False
qxh10d,"Thanks, well that's a big list as I am a final year undergrad and also many topics, I may already have learned. I will study as much I can.",hl9u2o5,t1_hl9ga7q,1637340231.0,True
qx827n,Not an embedded dev but I'd guess it's probably some type of PROM or EEPROM or some such for the program memory and just SRAM for RAM,hl7z8i8,t3_qx827n,1637300282.0,False
qx827n,Yup precisely. I think EPROM would be a better option for ROM though but yeah EEPROM and PROM works too,hl807k5,t1_hl7z8i8,1637300839.0,False
qx827n,"For a small calculator, the program memory is probably just masked into the chip, and then it uses SRAM cache. For a calculator with slightly more advanced features, it might have EEPROM or even some flash if it's a nice graphing calculator, AFAIK UV-EPROM has almost been entirely phased-out.",hl9o7gz,t1_hl807k5,1637337900.0,False
qx827n,"Go look at a Casio teardown. Although it's possible that these days it's all done on a single IC, in which case I imagine it's just some kind of RAM. It's not like it persists between power ons.",hl8d2us,t3_qx827n,1637309848.0,False
qx22or,"The midian of two sorted arrays in time complexity log(n) where n is the sum if lengths is not quite this simple im afraid. Rather then the coding part, thinking about the algorithm needed is harder.",hl8474x,t3_qx22or,1637303433.0,False
qx22or,"What you have come across is the fact that in maths, ""empathy is hard"". This, of course, does not mean that mathematicians are cold people, but that it is very hard to judge whether some problem is easy or hard, especially for someone who does not have the same skills as oneself.

This is because you often require an insight into why something is right, which is hard to come by, and even if you so, the road you went down to find it is likely unique to you.

Thus, the person who created this problem likely thought that this was easier than it actually is, for a variety of reasons.",hlsi6c9,t3_qx22or,1637688805.0,False
qx22or,Number 4 is hard if do it log(m+n),hl9dafw,t3_qx22or,1637333342.0,False
qwtqeb,"I'm confused. How does the algorithm decide which bits to flip without an RNG? If you know how to flip bits randomly, you've already solved the problem.",hl50r8j,t3_qwtqeb,1637254016.0,False
qwtqeb,Excellent point. I guess I was thinking in terms of the physical world where you can just flip 8 coins and have a random byte value.,hl54fb3,t1_hl50r8j,1637255453.0,True
qwtqeb,"If I understand what you mean, it is to 'physically' randomly change bits. I think that would be against the idea of computers in the first place. If computers randomly changed physical bits we wouldn't be able to predict and program them. 

Anyways I could also be wrong there :D If so I would be very interested in an example/explanation.",hl5ja1e,t1_hl54fb3,1637261250.0,False
qwtqeb,"As far as computer science is concerned nothing is random but you can for example take a temperature sensor with 10 bit precision and try to get a 16 bit reading from it, the last 6 bits will essentially be thermal/electrical noise. Then you can treat that as an I/O device and acquire random bits like that.",hl5kgo5,t1_hl5ja1e,1637261712.0,False
qwtqeb,I think that you want to read this: https://en.m.wikipedia.org/wiki/Hardware_random_number_generator,hl5qbj9,t3_qwtqeb,1637264044.0,False
qwtqeb,New computer have hardware modules who measure temperature an think like that to generate true random number if not you need a seed you could make alot of functions  for different people and burn it like zcoin but the seed still exists,hl60rlx,t3_qwtqeb,1637268153.0,False
qwtqeb,Because pseudo random number is a very complicated thing. Math rocks!,hl7gc8p,t3_qwtqeb,1637290674.0,False
qwtqeb,"First, theres no reason to shy away from math - its marely a tool.

Second of all, the solutions is impractical - even if you could ""flip a coin"" to determine if a bit would be flipped it would cause problems when using upper/ lower bounds.

For example lets say you have two bits and want a random number between zero and three. You roll both bits and both gets flipped to one, because this is outside of your bound you need to turn that number into the range [0,3] with equal probability. The only way to do that is roll *both* bits again. 

So despite having this ""bit flipper"" black box the solution would still be subpar to simply sampling from semi random thing like tempeture.",hl857ri,t3_qwtqeb,1637304114.0,False
qwqv67,"A concise way I like to look at it is a database does two things:

1. Stores data in some kind of file system (a data store).
2. Has a way to access that data systematically.",hl4rnyj,t3_qwqv67,1637250401.0,False
qwqv67,"I usually recommend Martin Kleppmann's book ""Designing Data-Intensive Applications"". It starts by considering a very simple database (a single file on disk) then builds from there. I'm sure it will help you build intuition.",hl4uicn,t3_qwqv67,1637251539.0,False
qwqv67,this book is really good! you won't regret reading it,hl4v6mh,t1_hl4uicn,1637251807.0,False
qwqv67,"Yup, DDIA ftw",hl7gib0,t1_hl4uicn,1637290751.0,False
qwqv67,"I think you're overcomplicating it a bit. Its really just a program that stores data. Nothing special happens to a computer or server when you create a database instance on it. As far as interacting with a database there are usually multiple interfaces. Some will have interactive shells (the two I'm familiar with are postgres and hive) where you can write sql or some other basic commands to manage the data. There are also db browsers which are basically a frontend client for interacting with the database; you can execute sql, see the results of queries, explore the database, ect. This is how most developers would interact with the database. There is also usually some sort of API for a website backend or data pipeline to interact with. 

> Secondly, if you could also briefly talk about the computer science of database management systems?

There's a myriad of different concepts that go into a database, do you have a more specific question on how they work?

> Please, also let me know if I'm thinking about this incorrectly. I am a self taught person with a lot of black boxes.

The biggest tip that I have for this is to try not to overcomplicate like I mentioned before. Just take whatever the concept is in its most simple form and try to build off of that understanding rather than worrying about all of the complexity up front. It just makes understanding a little more approachable.

Hope that helps! 

Edit: Comma Splice, addition",hl4nkkc,t3_qwqv67,1637248732.0,False
qwqv67,This is a great and much appreciated response. I understand. Thank you!,hl4r7cb,t1_hl4nkkc,1637250217.0,True
qwqv67,"The term database is pretty abstract.

It can be implemented in multiple different ways, but essential is a software that stores organized data.

Can store data in memory, on disk, tape, anywhere pretty much.
Can be in form of a library (SQLite) or a standalone software (postgres).

Hope this helps!",hlc441e,t3_qwqv67,1637373415.0,False
qwqv67,"Von Neumann is one a of the fathers of modern computations and his bigger cs contribution was make a paradigm when all data the same, one part of the data have ""meaning"" and are programs, you could store the data in whatever way you want for example put all in the same place whit index and then say to the machine search here this, most dB engines(all) take what and where you send for example in SQL and optimize and help whit replication and errors but a database is literally a file which have some time of structure like a excel (or the data science alternative csv)",hl631ze,t3_qwqv67,1637269058.0,False
qwqv67,Are these similar or the same amount 0f prestiges,hm2s8ne,t1_hl631ze,1637873254.0,False
qwnv1m,"OK, programming sounds like a gothic romance now.",hl446od,t3_qwnv1m,1637239377.0,False
qwnv1m,"She looked lustfully at her daemon, wondering what he was thinking. Her hand on his, her breath breathing his breath, her chest heaving in anticipation. She had one of the semaphores, she just needed the other. She slipped her fingers along his iostream and reached for the mutex. He gasped in ecstasy and dropped the semaphore. Finally! But, alas, as she reached for the now-available mutex, she realized that in her haste she had dropped the one already in her other hand.",hl4x3dj,t1_hl446od,1637252570.0,False
qwnv1m,One day this will become as part of theatrical performance.,hl4yyjh,t1_hl4x3dj,1637253313.0,False
qwnv1m,OMG.,hl4xgi2,t1_hl4x3dj,1637252715.0,False
qwnv1m,"Same thing, different names. Sounds even more dramatic in French: étreinte fatale.",hl4cfb0,t3_qwnv1m,1637243742.0,False
qwnv1m,"Deadlock (which is sometimes called the deadly embrace) is another crippling condition.  
 It occurs when two or more programs are each waiting for the others to   
complete - or even just to produce a data value - before proceeding. ...  
 There's a historic reason why deadlock exists.",hl45x3s,t3_qwnv1m,1637240364.0,False
qwnv1m,"They're the same thing, deadlock is the common term in CS.


Let's assume processes P1, P2 and resources R1, R2


P1 locks R1
P2 locks R2
P1 tries to lock R2, fails, waits until R2 is released
P2 tries to lock R1, fails, waits until R1 is released


Now both processes are waiting for the other process to release the resource they need, but neither can release the locked resource before locking the other one. The end result is that both processes are stuck waiting indefinitely.",hl4a6tr,t3_qwnv1m,1637242623.0,False
qwnv1m,"One common way to avoid deadlock is to have processes acquire resources in the same order.

In the example above: if P1 and P2 always attempt to acquire R1 first and then R2 second, then you can guarantee that the system won’t deadlock.",hl4stxw,t1_hl4a6tr,1637250870.0,False
qwegv6,"Well to start with, it can check whether argument and return types match at compile time.


    int takesAnInt(int x);
    
    int main() {
        char c = takesAnInt(3); // Error, return type isn't a char!
        takesAnInt(""foo""); // Error, argument should be an int!
        return 0;
    }

Those checks aren't possible ahead of time in a dynamically-typed language, because variables don't have definitive types until they're evaluated.",hl2oc7x,t3_qwegv6,1637204612.0,False
qwegv6,Depending on your compiler settings that first error would actually just be a warning.,hl3kuw7,t1_hl2oc7x,1637224560.0,False
qwegv6,Python solved that with type hints.,hl49bym,t1_hl2oc7x,1637242181.0,False
qwegv6,IDEs can basically compile as you go (sort of) and show you many type errors in real time. So if you try assigning a string to an int variable you'll immediately get a red squiggly line telling you how dumb you are. Same thing with argument and return types as said in the other answer. This can be especially helpful when using interfaces and/or inheritance as the the variable you give a function might not be *exactly* the same type as the function requests (could be a subtype). Most IDEs are also pretty good at inferring what function overload your are trying to use too.,hl3ks3o,t3_qwegv6,1637224497.0,False
qw8yg4,Cool. Was thinking about logic gate with complex number integration. You can take the work from here...,hl21oev,t3_qw8yg4,1637194273.0,False
qw8yg4,Really cool. I just ordered 3. ☺️,hl3xyyr,t3_qw8yg4,1637235360.0,False
qw8yg4,Agreed.  I just ordered 2!,hl4twxd,t1_hl3xyyr,1637251301.0,False
qw8yg4,:) Hope you love them. The new Kits are super sleek. Be careful soldering the micro USB so that theres no shorts. Any questions ask me. Instructions here: https://tsjelectronics.com/instructions,hl5vptj,t1_hl4twxd,1637266155.0,True
qw8yg4,Thanks! I hope you like them! If there's any problems just message me on here for through the contact page at website.,hl5veh7,t1_hl3xyyr,1637266031.0,True
qw8yg4,"""My mom said it's special""

I love this.",hl5n2u1,t3_qw8yg4,1637262758.0,False
qw8yg4,\>.<,hl5v75b,t1_hl5n2u1,1637265952.0,True
qw8yg4,Just ordered a blue one for my desk at work! Awesome little project. Congrats on the article!,hl9xqq7,t3_qw8yg4,1637341660.0,False
qw8yg4,"Thanks, I was really geeked that someone wrote a little something on my project, and thanks for supporting me making more stuff. Hope you love it.",hla57r7,t1_hl9xqq7,1637344568.0,True
qw8tom,"It’s definitely possible. Like imagine if the python developers decided to build into the interpreter a stage where it runs a static type checking tool like mypy on your code and refuses to execute it if it fails. 

Java might even fit into that description because the compiler requires static typing but only compiles code to byte code which if interpreted by the Java Virtual Machine when you run the program.",hl1pth2,t3_qw8tom,1637189020.0,False
qw8tom,"Yes of course, there is typescript for example.
 
You can have a compiled language not strongly typed or an interpreted language that is typed. The two concepts are pretty orthogonal.

Usually interpreted languages are not typed cause their usual main goal is development speed and adding types means you need to type more (no phun intended lol).",hlc4kkq,t3_qw8tom,1637373634.0,False
qw5zd4,"Just that the optimal makespan has a time greater than or equal to the longest job, which of course it must. Trivially, if there is only one job, then T\* = t\_1. But no matter what T\* can never be shorter than the longest-running single job.",hl0r4mu,t3_qw5zd4,1637174926.0,False
qw5zd4,Okay so why is it max\_i t\_i not just max t\_i?,hl0ru6r,t1_hl0r4mu,1637175200.0,True
qw5g3c,"Doesn't sound like this is exactly what you are looking for, but the best way to accomplish that for me has been to find past exams on the topic with answer keys. Either from my own university or another. If helpful, here are the past finals and midterms for the first OOP class at my old school, [https://exams.ubccsss.org/cs210/](https://exams.ubccsss.org/cs210/).",hl106st,t3_qw5g3c,1637178530.0,False
qw5g3c,Thank you kind stranger 🙏,hl16cth,t1_hl106st,1637180985.0,True
qw5g3c,"Doesn't your school keep copies of old exams on file? 

They are often in library if not online already.",hl173zj,t3_qw5g3c,1637181285.0,False
qw5g3c,Only some teachers do that but not often. If there's a website that has exams of every lesson it would be great. Googling past exams is a but chaotic,hl194jk,t1_hl173zj,1637182095.0,True
qw5g3c,I thought at most universities it was an exception when they don't have old exam copies  for undergrad courses. My university regulation required professor to make special request to exclude them from library database.,hl1n4nb,t1_hl194jk,1637187883.0,False
qw5g3c,"Geeks for geeks has hundreds if not thousands of practice interview questions where you can filter by subject, company, difficultly and programing in java, python, c++, inside their own web environment.",hl2uro8,t3_qw5g3c,1637207702.0,False
qw5g3c,Upvoted cause I want to know the same,hl0sx23,t3_qw5g3c,1637175627.0,False
qw5g3c,Check out [https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/) . They have quizzes as well as other resources related to all CS subjects.,hl44hf8,t3_qw5g3c,1637239552.0,False
qw5g3c,Yeah it’s called the end of chapter textbook questions,hl1hdjn,t3_qw5g3c,1637185484.0,False
qw5g3c,"Quizlet, chegg",hl2u04h,t3_qw5g3c,1637207321.0,False
qvrimr,"A ""directed acyclic graph"" means:

1. A ""graph"", sometimes called a ""network"". You have nodes with edges to other nodes. This could represent a ton of things, from a search tree to Twitter following relationships.

2. ""Directed"", means the edges between nodes have a direction, like ""A -> B"", and not ""A <-> B"".

3. ""Acyclic"" means there aren't any cycles. If you have ""A -> B -> C"", then C can't have edges to A or B, and B can't have edges to A.

You can represent DAGs many ways in memory. If this were a homework assignment in C, we might make a struct for a node, which contains an array of pointers to other nodes. Like a tree, with a variable number of branches at each step. Alternatively, you could represent a DAG as an adjacency list, an adjacency matrix, or just about any other way you'd store a network.",hkyg6nu,t3_qvrimr,1637128850.0,False
qvrimr,"Thanks, man!  
Appreciate  it.",hl0jh8q,t1_hkyg6nu,1637171951.0,True
qvrimr,"Directed Acyclic graph is a directed graph which doesn't have cycles. This is useful if you want to solve something like constrained scheduling, eg. chalk out a sequence of activities where some activities depend on others to complete. So if you hypothetically have a case where job A requires job B to complete and job B requires job A to complete, you have a cyclic Directed graph, and you cannot have a topological order (so you cannot logically draw out a sequence for doing those jobs). On the other hand, if job B required job C and C required A, you have a DAG and you can solve the job ordering problem (A->C->B).

In general, a directed graph can have cycles. A DAG is a special kind of directed graph.

In order to understand memory representation of a DAG, you have to understand memory representation of an ordinary, ""undirected"" graph. An undirected graph is a set of vertices and the edges joining them. Say you have V vertices and E edges between them, so you represent the graph as follows:

* You represent the vertices as an array of length V, having indices from 0 to V-1.
* Each array entry 'arr\[v\] 'points to a (linked) list of vertices 'w' which are connected to the vertex 'v', where v is any integer between 0 and V - 1. The list of vertices is called the Adjacency list, or 'adj'.
* So for example, if vertices v and w are connected, v will have w in its adjacency list and w will have v in its adjacency list.
* When you add an edge to the graph, say add\_edge(a, b), b will be appended to a's adjacency list and a will be appended to b's adjacency list (since in an Undirected Graph, edge a-b is same as b-a --- and *this is where a directed graph differs from an undirected graph*)

Now, in a directed graph, edge a-b means edge a->b, and edge a-b and b-a are not the same. So when you add an edge a-b, you only add 'b' to a's adjacency list, and not 'a' to b's.

Now it is possible to have 3 edges a-b, b-c and c-a in a directed graph, which will cause a directed cycle. That's all.",hkz121g,t3_qvrimr,1637145202.0,False
qvrimr,Wow! Thanks for the detailed response. It made my day.,hl0jkit,t1_hkz121g,1637171986.0,True
qvrimr,"A DAG could be represented in memory a bunch of different ways. It really depends on how you were going to use it.  As far as I know there is no reference implementation for a DAG in memory , but I could be wrong.",hkyfomz,t3_qvrimr,1637128529.0,False
qvrimr,"Thanks, appreciate your answer. 

Can you mention a few ways in which it could be implemented in memory.",hkyh46s,t1_hkyfomz,1637129449.0,True
qvrimr,Would it be the same as a tree?,hkzamtp,t3_qvrimr,1637152148.0,False
qvrimr,"Tree implies a single root, so while all trees are DAGs, a DAG is a more general class of graph",hl0gay2,t1_hkzamtp,1637170721.0,False
qvi861,"Multi-tasking: multiple different tasks sharing a cpu core. Each task gets a share of cpu time, then the next task. On one given core, only one task can run per cpu cycle. By switching between tasks, we can multitask. Note that with multiple cores, we can have true multitasking.

Parallel processing: one task is subdivided among many cores. Think GPU processing a physics simulation, or DNN inferencing. Some problems are parallelizable, which means they are suited for parallel processing. Hardware accelerators like GPU are used for these types of tasks.",hkwp9ys,t3_qvi861,1637099691.0,False
qvi861,Thank you! I think this would it much more simple and appropriate answer for an A-level question (If such was to come up in mock or exam).,hkws5mn,t1_hkwp9ys,1637100858.0,True
qvi861,"I'm cooking dinner. And while I'm chopping vegetables, my neighbor knocks on the door to give me a miss-delivered package, so I stop what I'm doing, put down the knife, answer the door, take the package, then return to chopping my vegetables.

In that scenario, I was doing two different tasks (chopping veggies and receiving a package) where at least one task was not completed in its entirety before also doing/finishing another, but only one of those tasks is being worked on at any single moment. This is ""concurrency"" (also sometimes called multi-tasking).

Now, I'm still cooking dinner, but while I'm chopping vegetables I'm also boiling noodles. The noodles are still cooking even though I'm not attending to them. Two tasks are being worked on (chopping, boiling) at exactly the same moment. This is ""parallelism"".",hky3qlg,t3_qvi861,1637121838.0,False
qvi861,"Multitasking is more of an operating systems concept that means being able to run multiple individual processes (or programs) at the same time, or so that their executions overlap. The execution of multiple tasks may or may not actually take place at the same moment, and multitasking and what appears to the user as simultaneous execution can be achieved e.g. through rapidly switching between tasks instead.

Concurrency and parallelism are more general concepts, and they can take place in several different ways and in different kinds of contexts. Concurrency and parallelism are relevant to the OS concept of multitasking, but they're really concepts of a different level.

Out of those two, concurrency means being able to execute more than one thread of control so that their executions overlap. The threads can be from the same or from different processes. The execution doesn't necessarily take place at the same actual moment, i.e. it may be that any particular moment in time only one thread of control is being executed.

You could think of multitasking as a form of concurrency on the OS level, although when talking about concurrency, you'd usually be interested in multiple threads of control that have some kinds of dependencies or communication between each other.

Parallelism means actually being able to perform multiple operations at the same moment. Depending on the context (and which level of parallelism we're talking about), the operations can be from the same task, from different threads, or from entirely different tasks. But what separates parallelism from concurrency is that multiple things are actually being done at the same moment.",hkwoqgw,t3_qvi861,1637099473.0,False
qvi861,"Ah, that does make sense when thinking it like that. And from reading your reply as well as u/LowLvlLiving, I believe that I now have a better understanding of it now:

* Multi-tasking is more to do with OS types and its ability to perform multiple task nearly simultaneously.
* Assuming conditions are correct for both concurrency and parallelism. Parallelism will be slightly faster in performing task as it does not require constant switching between two or more jobs.
* Parallelism will also be more effective in processing larger or more tasks due to previous bullet point.

(I hope this correctly summarise the points from both of your replies? Also thank you both for answering my question.)

\#Edit: Second bullet point (new) - parallelism is slightly faster in performing task as it does not require the task to be finished before starting the next task, since a coordinator or job scheduler have already assigned it to another CPU/GPU core.",hkwr20v,t1_hkwoqgw,1637100410.0,True
qvi861,"That's generally the gist of it.

> Multi-tasking is more to do with OS types and its ability to perform multiple task nearly simultaneously.

Well, yeah. Or so that the OS or its user can switch between multiple applications or programs without having to terminate one of them first. You'll generally want the OS to be able to switch between them quickly and relatively often so that switching between applications is fluent and so that your music doesn't break up and so that a web server can get around to responding to requests quickly. You want it to be fast enough to *look* like the computer can run multiple things at once. Technically, however, it's multitasking whether it's really quick or not.

Multitasking is something you generally take for granted nowadays, as there haven't been any non-multitasking operating systems in mainstream PC use in the last ~25 years. But, well, *somebody* had to implement multitasking, and sometimes it can be useful to know how and why it works, so we still learn it.

> Parallelism will be slightly faster in performing task as it does not require constant switching between two or more jobs.

Depending on the task (or tasks) and the resources you have, it can be faster by any factor. For example, if you have *n* processor cores and your task can be divided into *n* equal parts that can be run (mostly) independently of each other -- or if you have *n* separate tasks you want to run -- you can ideally get a speedup of *n* times over doing those parts or tasks one after the other. In the best case anyway.

You might save something by not having to constantly switch (as the switching itself has a cost), but the real reason parallelism can give speedups is that it allows you to make use of the multiple cores or other parallel resources you have available.

Parallelism is kind of like dividing work between multiple workers and having them actually work and advance tasks at the same time instead of one of them working and another having to wait for the first one to finish before starting their own part.

Some tasks are easy to parallelize, but in case of other tasks it might be harder, so the speedup you can achieve might be smaller or even none.",hkwx7p3,t1_hkwr20v,1637102933.0,False
qvi861,"in short and to not get confused, just target the core, one core doing multiple tasks is multitasking while many cores doing one task is parallel processing",hkymgs3,t3_qvi861,1637133182.0,False
qvi861,"From what I understand, concurrency (multi-tasking) is about responsiveness and parallelization is about doing more work at the cost of more resources.  


If we have 2 tasks: A and B, each takes 5 seconds to complete. If we run the tasks concurrently, the operating system will do, say, half a second of work on A and then switch to do half a second of work on B - switching back and forth until both are completed. This will actually take a little *longer* than just running them end to end. So why use it?  


Well, what if task A is processing a file and task B is checking for user input. If we run A then B, while A is running the computer will become totally unresponsive as it's completely focused on completed job A when computers have other task to consider.  


It we parallelize A and B we need to have a process free to take on the new load of work and to make sure that running these jobs at the same time isn't going to have any side effects. Parallelization at a large scale can cause nightmares with race conditions if you're not careful, but it great for small batch work - if you've got the computing power to spare.",hkwk7z2,t3_qvi861,1637097679.0,False
qvhov8,"I don't use bit manipulations or many bitwise operations, but my favorite bit of hijinkery is fast inverse square root",hkwkc3k,t3_qvhov8,1637097724.0,False
qvhov8,The book hackers delight was just packed full of them. Lots of branch free operations. Don’t have a favourite but I do like the trick where you multiple by 1 or 0 to avoid an if statement.,hkwr6cb,t3_qvhov8,1637100459.0,False
qvhov8,"Finding all permutations of a set by looping from 0 to (1<<N)-1 then picking members based on bits.

Binary indexed tree and its variants. 

Finding the highest power of two in a number with (n & ~(n - 1)). 

XOR for finding a unique numbers in a sequence of duplicates. 

Tries with bit operations as edges.

These examples are mostly for fun. There are some bit-based data structures have been useful for me professionally though: hyperloglog, Bloom filters, roaring bitmap.",hkynu0s,t3_qvhov8,1637134230.0,False
qvhov8,What are some things that you find interesting?,hkwpnys,t3_qvhov8,1637099847.0,False
qvhov8,"I can’t say I’m using whole lot. Just simple arithmetic, such as int division/multiplication by 2. Albeit I read some where that it’s less performant in those simple cases compared to regular division. 

But I still use them because they are cool and remind me that I gotta learn and use them more. 😁",hkype2p,t1_hkwpnys,1637135440.0,True
qvhov8,"calling popcount 5 times for a branchless and fast way to OR all the bits

1 or 0 to  all true by sll by 31 and then sra by 31

also theres a ton of fractals that can be done as bit hacking",hkz2i7m,t3_qvhov8,1637146389.0,False
qvhov8,[Hacker's Delight](https://en.wikipedia.org/wiki/Hacker's_Delight) is a book full of various bit flipping tricks as well as other useful low level tricks.,hlca3iu,t3_qvhov8,1637376327.0,False
qvgwfj,Keeping it accurate as the code base changes.,hkwf1qy,t3_qvgwfj,1637095654.0,False
qvgwfj,"This. I always find docs are amazing at the beginning when the engineer(s) who do the initial implementation do a great job of summing everything up. Then over time more engineers get introduced to the codebase and make changes which affect documented features without being aware of the documentation, creating a drift.",hkyfdj3,t1_hkwf1qy,1637128335.0,False
qvgwfj,Waiting until the very end of coding and debug to start.,hkwiwq5,t3_qvgwfj,1637097163.0,False
qvgwfj,Getting (non-great) engineers to understand how important it is.,hkxacz7,t3_qvgwfj,1637108633.0,False
qvgwfj,"The hardest thing in writing documentation is somewhat similar to how we try to code the same way in a team using code conventions.

Defining your team's documentation conventions and having everyone on board is freakin' **hard**.

Here's what I've been preaching to my team and those who work with us.

1. Start with high level infrastructure stuff using something like https://c4model.com/ The 2 first levels are usually enough to figure out where I should do a fix or where I should attempt to work.

2. Before coding something, start with writing functional specifications first. IMHO, a good reference for writing no-bullshit-functional-specs is https://www.joelonsoftware.com/2000/10/02/painless-functional-specifications-part-1-why-bother/ 

Its form could be simple sentences and bullet points just like within the article. A flow diagram or a sequence diagram is sometimes more efficient in communicating the overall intent. My preferred tool to draw those diagrams is https://mermaid-js.github.io/mermaid-live-editor Since it's all text, you can even commit your diagrams within your code repository.

3. I usually don't care much about inline comments unless it's really trippy and complicated.

4. I do care about good naming conventions for your classes, methods, functions and variables.",hkxtnqm,t3_qvgwfj,1637117131.0,False
qvgwfj,"Depends on the project and the end user. In my previous project at work, I developed the entire app and had to also do a large of acquiring and documenting the business process from the customer. So not only did I had to comment and document my code, but I also had to document the business side.

The business side is generally where I hate having to document, usually because the business folks often don't have a consistent process.",hkwrwhe,t3_qvgwfj,1637100755.0,False
qvgwfj,"For me it's most difficult to find find the right time to write documentation comments. In the first place I think, that the code will change too much while debugging. After I got it working I struggle with the amount of methods which I have to comment. ^^""",hkwjxkf,t3_qvgwfj,1637097567.0,False
qvgwfj,It can be difficult to strike the correct balance between too little and too much comments.,hkwo7z1,t3_qvgwfj,1637099271.0,False
qvgwfj,"If you're doing things the correct way, your code is going to be negotiated, and should flow from tests - to the point the test code itself is a significant source of documentation in itself. This helps to solve the most significant problem with documentation, i.e. that it goes stale, because when you can't commit code that doesn't pass your unit tests, then your unit tests inherently must be up to date. And because you negotiate your code with those people who rely on it (larger system architecture or clients) you help to solve the *other* most significant problem with documentation, i.e. that it's only able to be understood by the person who wrote the code at the time they wrote it.",hkx0j8w,t3_qvgwfj,1637104352.0,False
qvgwfj,"I find that writing documentation is a chore as much as designing algorithms and writing software as a chore: it's all equally tedious. For the case of documenting code, I structure my labels and functions together with commentary for the purpose of a ""clear narrative"". My aim is that I can read my code after six months of not seeing the code. Clear code narrative supports bug hunting and feature tweaking.

As for writing user documentation, this is equally important as writing correct code. Every software system is unique even when there are overlapping themes that are common to software titles. I want my users to be able to comprehend the intent of the features I've exposed to them. Powerful software systems are sophisticated by nature, users deserve some explanation about ""why"" and ""how"".

For my perspective, documentation is not optional. I need to communicate to the code writers and to the users about the meaning of my software. This perspective helps me slog through the chore that is writing computer software and communicating the meaning through documentation.",hkxeenm,t3_qvgwfj,1637110420.0,False
qvgwfj,The part where I realize no one will ever look at it or do anything with it.,hkx2rxf,t3_qvgwfj,1637105323.0,False
qvgwfj,Where I work we just don’t have enough and the systems are very complex. End up having to dig through code to figure it all out,hkxanqb,t3_qvgwfj,1637108764.0,False
qvgwfj,Turning on the computer,hkz65vv,t3_qvgwfj,1637149195.0,False
qvgwfj,"If your document involves APIs, use Postman. Postman automatically generates documentation for you. So, if there are any changes to your APIs, just call it once with the changes on Postman and it will change the documentation for you. In this way, your work will be made much easier and you'll be spending a lot less time on documentation.",hl0juuz,t3_qvgwfj,1637172096.0,False
qvgwfj,"The most difficult part is that my boss doesn't value documentation until he's made to, so we aren't encouraged to do our best in preparing for projects with checklists and assessments, comment the code,  or document the project holistically once done. I try my best to get into the habit of documenting my own work, but we're lacking that important push from management to really cement that thinking",hkwxi9u,t3_qvgwfj,1637103056.0,False
qvgwfj,"Yeah but you dont need documentation until you need it so go back and write it all down after something goes wrong, of course.",hkxnjn1,t1_hkwxi9u,1637114442.0,False
qvgwfj,Keeping it in one place. Inevitably management will change tools of reorganize everything so then nothing is searchable.,hkydexy,t3_qvgwfj,1637127144.0,False
qvgwfj,"As I am learning this, it is the SAD part. Not sure if this is even actually used in building a software. Can anyone working in the industry answer this?",hkyenth,t3_qvgwfj,1637127893.0,False
qvgwfj,"I always found it difficult to put my thoughts into words. Like I know why I'm doing certain stuff because most of it is intuition but when I'm asked to explain why I did something it's takes me some time to explain it in words. I don't know, I think it's just a problem I have.",hkyh1kj,t3_qvgwfj,1637129402.0,False
qvgwfj,"Getting started, especially if large parts of the codebase have outdated or no documentation.",hkyw8ua,t3_qvgwfj,1637141110.0,False
qvgwfj,writing it after the fact. like when im done coding but only god knows how it works,hkyxc6i,t3_qvgwfj,1637142048.0,False
qvgwfj,Knowing when to do it and knowing when not to. Knowing who the audience will be. Making it be good but also generated.,hkzwkby,t3_qvgwfj,1637162902.0,False
qvgwfj,personally for me writing a documentation is the hardest part of writing a documentarion,hl08akz,t3_qvgwfj,1637167610.0,False
qvgo6i,"Take a look at software defined radios. One possible approach is to find a (relatively) inexpensive SDR with TX / RX capabilities. You can then use open-source software to ""control"" the SDR. A very popular interface for this application is GNURadio. 

Since this is software-defined, then you essentially control what happens at the GUI level, using the SDR as an RF front-end. All of the mod/demod will happen at the Pi itself, as opposed to being processed at the SDR. Speaking from experience, you may find that the Pi is rather underpowered to handle certain SDRs or signal types.",hkwma10,t3_qvgo6i,1637098495.0,False
qvgo6i,"I don’t know specifically what your set up would be doing but I work with CAN a lot and have experience with TCP/IP and those use a “packet” which is a fancy term for a specific cluster of bits being sent from device to device. The packet is predefined in a standard somewhere and the devices know how to read it or can be told how to read it. There’s some identifying information regarding the source and the destinations and the contents, the data itself, and some CRC data as well. All of that gets framed inside the predefined packet so the devices can recognize where each packet starts and stops. The bits themselves are sent by literally sending specified voltage pulses. Which is beyond my scope I deal more with filling and reading the data after the machine handles the packet.",hla2cou,t3_qvgo6i,1637343462.0,False
qvcvqa,"I also want to know, I didn't find any books but there are some blogs and YouTube videos on about them. You can check Intel tino core uefi project maybe that will give you some reference!",hkydoki,t3_qvcvqa,1637127303.0,False
qvcvqa,You should ask this in r/embedded because usually embedded engineers are responsible for implementing first stage bootloader( bootloaders are very tied with hardware because bootloaders are responsible for hardware initialization ),hkye96x,t1_hkydoki,1637127646.0,False
qvcvqa,"That's a good one, I did so. Thanks!",hl1y4wn,t1_hkye96x,1637192658.0,True
qvcvqa,"I find a book on UEFI it is a good one ""Beyond BIOS Developing with the Unified Extensible Firmware Interface""",hl2k9pm,t1_hl1y4wn,1637202728.0,False
qv9yun,"Take some very simple programs, like just one loop or if statement if you don’t understand those and walk through it by hand. Like have a list of what the variables values are and update them every instruction. Also ask yourself why each line of code exists. 

Then after you are done with that run the program in a debugger to compare. If you did it right awesome, try a different harder program, if not figure out why you made the mistake.",hkv1jkc,t3_qv9yun,1637076127.0,False
qv9yun,"Here are some things that really helped me:
1. Relating the algorithm or concept to something in the physical world (like imaging sorting a deck of cards when doing sorting algos)
2. Physically drawing diagrams and tediously going through every step using examples.
3. Going through each line of code and trying to understand its purpose, dissecting every component of the syntax as steps you would take to complete the problem irl.
4. Practice writing small programs related to what you’re learning, you can make this part fun by involving other stuff you’re interested in (when I was learning object oriented programming in high school I made a drumset program since I’m into that)

Good luck :)",hkw2b8y,t3_qv9yun,1637090628.0,False
qv9yun,Thanks so much!!! I will try implementing these steps into my learning process,hkz7e68,t1_hkw2b8y,1637150049.0,True
qv9yun,"I found it helpful to put some simple programs into this: [https://pythontutor.com/](https://pythontutor.com/) 

And then step through them. Despite python in the name, it also does a bunch of other languages.",hkw9jsd,t3_qv9yun,1637093496.0,False
qv9yun,"One thing that helps me is running things on a REPL, basically it shows the output of stuff as you write it in real time.

It’s been around as a tech for decades, but a lot of people don’t know it exists. It’s the standard way to do things in Lisp.",hky3hkz,t3_qv9yun,1637121716.0,False
qv9yun,Give yourself much more time than you think you'll need.,hkyc36k,t3_qv9yun,1637126359.0,False
qv9yun,"I like to draw out tricky stuff using a flowchart and kinda note out where things are happening and what variables are being handled. I personally think it helps to know what the machine is doing when you tell it something, so maybe think about that as you look at the code. Also, is it your code you’re having trouble with or are you looking at others’ code on stack overflow or an example online or text book?",hl9zimg,t3_qv9yun,1637342353.0,False
qv9yun,It’s mainly other people’s code - especially my professor’s code. A couple people have recommended drawing things out before but I sometimes don’t even know where to start 😅,hlddcm3,t1_hl9zimg,1637403455.0,True
qv9yun,Start with main or if it’s just a certain few lines look at the function/method they are in and just draw that part out. Draw.io is a good online tool but I keep a small dry erase board and markers nearby for exactly this and math. Anything to help visualize the ins and outs as they get processed. Good luck with your studies.,hles2kn,t1_hlddcm3,1637431304.0,False
qv9yun,"Just start doing things in the language... Say a sentence for what you want to happen then write the code the does it. Python is awesome for this and an excellent first language for this reason.

    python = list()
    doing = True
    things_to_do = 5
    
    print(""We want to do "" + str(things_to_do) + "" things!"")
    
    while doing == True:
        python.append(""things"")
        if python.count(""things"") == things_to_do:
            print(""Looks like we've done enough things for now."")
            print(""Here are the things in python: "" + str(python))
            exit()
        else:
            print(""Let's do more things."")

Doesn't matter if what you do has a purpose or not, just become fluent with the syntax first.",hli2n2s,t3_qv9yun,1637496873.0,False
qv9yun,Thanks so much!,hli7f8h,t1_hli2n2s,1637500022.0,True
qv9g3m,"It will be very difficult to find research papers that are 500 to 600 words. Short conference papers are usually 4-6 pages, and it goes up from there. You might find some published abstracts but they are not so common in CS (they appear more frequently in psychology, medicine, etc.). The only thing I can recommend is to add ""abstract"" to your search terms but I don't think that will really work.

&#x200B;

Also, you may want to check with your teacher because perhaps they meant articles and not research papers. In which case, there should be plenty online.",hkuxqz2,t3_qv9g3m,1637074499.0,False
qv9g3m,"Yes, I found some abstract in my research but they are too short and are less than one page... I think I can look over neural network cause it's fashion this time and can just say to my teacher ""Ho, neural network is a kind of data structure"" btw. Idk if it's true or false but I can maybe have a bit more luck in that way ...",hkxhfdx,t1_hkuxqz2,1637111763.0,True
quwl9r,"Because the application is downloaded using multi-threaded download at the beginning, the speed will suddenly be very high. At the end of the download of resources, multi-threading is not worth it, and the number of threads will be reduced. At the end, it will be a single-threaded download. very slow。",hkuxal8,t3_quwl9r,1637074300.0,False
quwl9r,Loading/Downloading percentages are sometimes completely arbitrary and do not reflect the actual state of the loading. It just makes the user feel better to have something on the screen to look at. There even studies around this.,hla2qgv,t3_quwl9r,1637343613.0,False
qunqn2,So old they put wood paneling on it,hkretam,t3_qunqn2,1637006866.0,False
qunqn2,That's how you know it was released in the 70's.,hkrq3n6,t1_hkretam,1637011281.0,False
qunqn2,Woodgrain grippin,hkt2wlc,t1_hkretam,1637032313.0,False
qunqn2,No seriously. What is that?,hkvctdk,t1_hkretam,1637080692.0,False
qunqn2,"It is a cover. There is a silicon wafer in the middle, under it. Fine wires connect from the legs to pads on that wafer.

original chip:

[https://www.semanticscholar.org/paper/The-Intel-4004-Microprocessor%3A-What-Constituted-Aspray/7ba1ae43acd70844b8c00e47c436354339c12bfe/figure/6](https://www.semanticscholar.org/paper/The-Intel-4004-Microprocessor%3A-What-Constituted-Aspray/7ba1ae43acd70844b8c00e47c436354339c12bfe/figure/6)

modern-day recreation similar to old design:

[https://fuentitech.com/diy-silicon-people-build-integrated-circuits-similar-to-intels-4004-cpu/192855/](https://fuentitech.com/diy-silicon-people-build-integrated-circuits-similar-to-intels-4004-cpu/192855/)",hkvvnua,t1_hkvctdk,1637088010.0,False
qunqn2,you can't trick me this is a hexbug,hkrt3np,t3_qunqn2,1637012453.0,False
qunqn2,Came here to share some nostalgia with my fellow 2000’s kids.,hkv254d,t1_hkrt3np,1637076374.0,False
qunqn2,That is a hexbug,hkv2uko,t1_hkrt3np,1637076668.0,False
qunqn2,The top part looks like an ice cream sandwich,hkreod3,t3_qunqn2,1637006811.0,False
qunqn2,"Three other CPU chip designs were produced at about the same time: the Four-Phase Systems AL1, done in 1969; the MP944, completed in 1970 and used in the F-14 Tomcat fighter jet; and the Texas Instruments TMS-0100 chip, announced on 17 September 1971. The MP944 was a collection of six chips forming a single processor unit. The TMS0100 chip was presented as a ""calculator on a chip"" with the original designation TMS1802NC. This chip contains a very primitive CPU and can only be used to implement various simple four-function calculators. It is the precursor of the TMS1000, introduced in 1974, which is considered the first microcontroller—i.e., a computer on a chip containing not only the CPU, but also ROM, RAM, and I/O functions. The MCS-4 family of four chips developed by Intel, of which the 4004 is the CPU or microprocessor, was far more versatile and powerful than the single-chip TMS1000, allowing the creation of a variety of small computers for various applications.

//////////////////////

For more information on little-known history and other 1970s events, consult the 50YearsAgoLive Project, a Twitter program that reports events from exactly 50 years ago as if they’re happening in real time. It is meant to stoke an interest in history by making it accessible to the everyday reader:

[https://twitter.com/50YearsAgoLive](https://twitter.com/50YearsAgoLive)",hkr5evh,t3_qunqn2,1637003155.0,True
qunqn2,"The 4004 is rather lovely to study and play with, you can read the datasheet and write code for it in raw hex in a matter of hours.  What most people don't understand about the 4004 is it is less a general-purpose CPU and more a ""multichip microcontroller"".  
  
The 8048 microcontrollers and beyond are spiritual successors to the 4004 I understand.  
  
The 8080, then 8086 that all our PCs hark from have their roots in the 8008 which was the first general-purpose single-chip CPU I think.",hkto1mp,t3_qunqn2,1637043788.0,False
qunqn2,What happened to Intel 0 through 4003?,hkux1l3,t3_qunqn2,1637074192.0,False
qunqn2,This is very cool,hkv8jdk,t3_qunqn2,1637078990.0,False
quk6my,"Computer science: the scientific discipline of information processing, includes a lot of theory about for instance computability theory, complexity classes, abstract machine models like finite state machines and turing machines, ... also lots of math

Programming: the process of writing code, i.e. text that is in such a language that a computer is able to understand it

Software engineering/developing (don't know of any difference): the discipline of creating software, this involves programming, but also a LOT more, like organization, planning, designing architecture that fulfills quality standards and allows expanding and changing the software later, testing and QA",hkrfgi3,t3_quk6my,1637007124.0,False
quk6my,"From what I know is computer science is a degree which is useful to software engineering and can help you, but a computer science degree isn't required to get a software engineering job

Software engineering/software developer is a job that targets in building, analyzing, developing new things by Programming.

Programming is a process by making a program with code.

Sorry for bad English and I'm just new to these stuff. You guys can correct me, it would be an honor.",hkqurbq,t3_quk6my,1636998951.0,False
quk6my,"10 years ago Computer Science was/is a degree that focuses on theories and data structures. Computer engineering focused more on hardware/electronics. These may have changed since I got out of school.

Programmer/software engineer/software developer differences are going to vary from place to place and often are interchangeable. When different generally a engineer focuses more on designing solutions and developers focus more on producing.",hkqwucd,t3_quk6my,1636999771.0,False
quec2x,"I'd say the DQN paper is good to know for games, https://arxiv.org/abs/1312.5602",hkrxhy7,t3_quec2x,1637014199.0,False
qu8lzi,[Good video here.](https://youtu.be/0oDAlMwTrLo),hkpjfgv,t3_qu8lzi,1636976426.0,False
qu8lzi,CS50 week 3 :),hkq59h8,t3_qu8lzi,1636988513.0,False
qu4c71,"I think the fundamental misunderstanding here is between the difference in read / write operations. A mechanical arm responding to an electrical impulse within the brain specific to the area that lights up when someone tries to move their arm is much easier to read and mimic than it would be write some information to the brain. 

We are beginning to understand how to read impulses from the brain for simple operations. But we have no idea how to write back to it.",hko9khj,t3_qu4c71,1636945061.0,False
qu4c71,">But we have no idea how to write back to it.

We sorta do, conceptually. Education is all about this. As is martial arts, and sports. In fact, anyone who has ever learned anything and gone through the process of creating a repeatable ""function"" (muscle memory* is the phrase I've most often heard) out of something that used to require much more attention during the learning phase is then aware that some kind of self-write operation is a part of daily life and learning new things, beyond just being a passive sensor. If we can map what happens in the brain during other things, then it stands to reason that we could map what happens in the brain when people are intensively learning and training themselves. The idea of insta-grok technology does not seem far fetched to me, but I'm not an expert. I would add the important caveat that the brain is complex and consciousness even more so. It would probably take far longer to perfect and understand how to use such technology than it would to get it going in the first place.

I am not sure I would replace the learning process itself with a Matrix style ""insta-download-to-your-brain"" system, but it would be cool to have more control over the process which we already use when teaching ourselves things. Oftentimes training is like playing the part of both the animal and the trainer at the same time, and even if you're a smart trainer you might still also be a stubborn animal. Some people's muscle memory (and neuroplasticity, for that matter) is less stubborn than others, and easier to tame for this purpose or that. I personally enjoy the process of learning and practice, but I would enjoy having more control over it. Perhaps current Sci Fi depictions only scratch the surface of what cybernetics and biotech could allow, in theory. 

*Muscle Memory is not simply memorization. It is creating repeatable functions which you then use for higher level tasks, like tools in a tool belt.",hkordik,t1_hko9khj,1636954530.0,False
qu4c71,"One I enjoyed your response.

But if I can, the question isn’t that we don’t understand the general process of repetition leading to acquired skill or knowledge. The question is what the actual underlying physiological process occurring during repetition is. And whether or not we could repeat that. 

I see that as a far stretch from mapping some part of the brain which lights up when a person attempts to lift their arm.

Certainly not impossible, but well beyond our current knowledge of how the brain works. Unless you listen to Elon musk of course. Then we’ll have all of this done in no time",hkoryks,t1_hkordik,1636954895.0,False
qu4c71,"I don't think repetition alone teaches a person much. To use the sports and martial arts analogy again: you want to build mental, logical, conditional reflexes for different situations in addition to building the muscle memory of the actions themselves. In general this means applying what you've learned in a variety of situations and getting ""practice"", which is distinct from repetition, although repetition is a byproduct of practice. A surprising amount of, say, Math is about building this kind of thing so that when you see a certain kind of problem the right solution hits you like a reflex. Intuition comes from practice.

I'm not a doctor or an expert programmer (or expert anything), so I'm not sure what the current ""resolution"" limits are for making sense of brain scans. I'm skeptical because it is hard to measure the conscious meaning of even a face expression from a friend, nevermind a gazillion neurons being examined in n-dimensions of data analysis. That said, it seems like a surmountable task with enough effort, given that we have already mapped out so much.

Edited for typo.",hkostqd,t1_hkoryks,1636955442.0,False
qu4c71,Did you just coin the word insta-grok? I love it.,hkqhzjg,t1_hkordik,1636993878.0,False
qu4c71,"I didn't coin the word *grok* (which is from a Heinlein book I've never read and means something like ""to grasp a concept""), but I will happily take credit for *insta-grok* if nobody else has said it before. I'd be shocked if I were the first though, as the word has been around for about as long as microwaving food. 

Glad you enjoyed!",hkr2uwu,t1_hkqhzjg,1637002137.0,False
qu4c71,"It's from Stranger in a Strange Land which was a counter culture hit in the 60s & so the word grok entered the lexicon. But insta-grok was new to me.

Edit: I just searched & it's already in use as a website name.",hkr8f9f,t1_hkr2uwu,1637004350.0,False
qu4c71,Glad to hear I'm not the first! It would be a good brand name for all kinds of things. Thanks for reminding me which book it was from. I read Starship Troopers as a kid but that's the extent of my Heinlein.,hkr99zy,t1_hkr8f9f,1637004690.0,False
qu426c,"LaTeX is like vim. If you really know how to use it, it's just so much more powerful and easy to use tool, you don't want to touch anything else.",hknuz9m,t3_qu426c,1636938330.0,False
qu426c,"I see, thanks.",hknw33r,t1_hknuz9m,1636938850.0,True
qu426c,Simple: Word and GDocs equations look like absolute dog shit in comparison.,hknzcxp,t3_qu426c,1636940363.0,False
qu426c,"Also, and perhaps more importantly: LaTeX lets you write once and use anywhere, which is important if you want to submit your work into a journal that used a different format than you did, and then also submit it to a second different journal that uses a different format than the first one. You don't actually have to do very much.",hknw6nl,t3_qu426c,1636938897.0,False
qu426c,"The primary reason is that in LaTeX you can put in mathematical equations and they print out like mathematical equations. Try doing that (properly) with Word. Second reason, it will print out the way you define it, Word can’t even guarantee your text will be on the same page even if it is in the same versions across platforms.",hko38xx,t3_qu426c,1636942144.0,False
qu426c,"When you have a large and complex technical document, latex manages figures, tables, citation and equations without  moving your hand from the keyboard, which is very efficient when you know the syntax.",hkootrh,t3_qu426c,1636952977.0,False
qu426c,"As other have mentioned LaTex still does MANY things well that Word/Gdocs does not. And that goes well beyond just equations. I have used it extensively for papers which rarely included equations, but here are the some of the most important factors I found:

* Automatic generation of bibliography with references in paper order. ALL papers in every field have citations/references and numbers range from tens to hundreds. Being able to collect that in a bib file and reference a cite with a simple function is worth its weight in GOLD.

* Separation of content/structure from formatting. When writing in LaTex you write the content with minimal structural markup (like sections/subsections). Almost ALL the formatting like fonts, page layouts, etc are separate and usually GIVEN to you by the journal/conference/publisher so you don’t spend much if any time fiddling with it. For those familiar with it LaTex splits content from style kind of like HTML and CSS.

* Simple ASCII file format that is human readable makes it possible to recover from mistakes/corruption as well as collaborate using revision control systems designed for code. When using a binary format or machine generated XML, this is impossible. If a tool screws something up you are up shit creek and may need to rebuild from scratch. When working with large documents like a thesis or book, this would be a disaster.",hkr0hgy,t3_qu426c,1637001208.0,False
qu426c,"Aside from the features you mentioned, it seems still nothing does justification as well as tex/latex, even after all these years.",hkp04m2,t3_qu426c,1636960470.0,False
qu426c,"I am not a LaTeX user but I usually use Markdown (also a Markup language) to quickly write some text with formatting.
The advantage of using markup language for formatting is that your hands do not need to leave the keyboard to do formatting. If you are familiar with most syntaxes, you can do formating much more efficiently than using GUI to do so. Markdown, for example, let's say I would like to make a quote block or code block. I can simply type '>' or '```' to create one quickly and start inputting what I want. In Microsoft Word, I'll need to perform additional steps to create similar things.
From what I know LaTeX is widely used to make mathematical formulas (correct me if I'm wrong). As I don't know how to use LaTeX, whenever I want to create a mathematical formula, I have to manually add symbols by choosing the list of symbols in the menu. It is definitely inefficient to do so.",hko2mke,t3_qu426c,1636941862.0,False
qu426c,Many Markdown environments are LaTeX extended. Wrap with double dollar signs and try it out. $$6x_2 \geq x_1^2$$,hko3pml,t1_hko2mke,1636942356.0,False
qu426c,"I think most people answered the actual reasons well, but I’d just like to add it can be fun. I don’t know about y’all, but it feels really satisfying to manually specify the proper formatting and then have it appear. Then again, it can also be incredibly frustrating too.",hkpa3ws,t3_qu426c,1636968632.0,False
qu426c,"Ew, no. I don't want to hand-draw everything in a GUI, are you kidding me?

Just because it's hard for you doesn't mean it's hard for everybody.",hknvrfe,t3_qu426c,1636938698.0,False
qu426c,"I never said it was hard, i simply asked why it was used....",hknw17w,t1_hknvrfe,1636938824.0,True
qu426c,Control,hkqipbj,t3_qu426c,1636994169.0,False
qu426c,legacy stuff. journals only accept latex. also the lack of competition. latex is dogshit but its all we can use,hkz30sz,t3_qu426c,1637146809.0,False
qttk9c,"This is the trend for every single development in the industry.

Probably the most recent large one that maybe goes unnoticed is the migration to the cloud. 10 years ago, I would never imagine not having a server room. Now, the idea of physical machines hosting your application is probably something very foreign to people, and of course it was resisted immensely at first.",hkoeqbq,t3_qttk9c,1636947536.0,False
qttk9c,See also [The Story of Mel](http://www.jargon.net/jargonfile/t/TheStoryofMel.html).,hkmtd5a,t3_qttk9c,1636922451.0,False
qttk9c,"Throughout time, the acceptance and adoption of new technology has always followed this pattern: 

Denial, Anger, Bargaining, Depression, Acceptance. 

Oddly enough, this is also happens to be the five stages of grief. Go figure.",hko9aac,t3_qttk9c,1636944926.0,False
qttk9c,What stage would you say Bitcoin/Crypto is in currently?,hkp769t,t1_hko9aac,1636966076.0,False
qttk9c,"IMHO, bargaining. Some adopters, but not mainstream yet. When it reaches about 45% adoption, we'll start to hear all sorts of sad stories about it. That's when you'll know it's in the depression phase.",hkpnn9x,t1_hkp769t,1636979351.0,False
qttk9c,Not relevant but it reminds me of a historian who claimed ancient writing was purposely more complicated than it needed to be because the scribes would benefit.,hkmbk4g,t3_qttk9c,1636915811.0,False
qttk9c,You don't need to go that far. Try studying Law. Completely unnecessary verbose just to gatekeep others from understanding it so they remain necessary.,hkmq6u9,t1_hkmbk4g,1636921229.0,False
qttk9c,Yeah but my example highlights that we've been avoiding getting replaced for 6000 years.,hkn0ink,t1_hkmq6u9,1636925248.0,False
qttk9c,Wasn't it Socrates that claimed that writing was bad because people wouldn't learn to memorize? (Which we learned through Plato's writings),hkn7kdw,t1_hkn0ink,1636928030.0,False
qttk9c,its ironic that we (mankind) only 'remember' this now because it was written down.,hkpuw9f,t1_hkn7kdw,1636983508.0,False
qttk9c,"That’s funny. On the other hand the first Lisp compiler and interpreter was implemented by a grad student who was tired of having to write Assembly code.

John McCarthy told his grad student: ""You can't make a computer language out of a notation language."" 

Grad Student: Lmao.",hkofh83,t3_qttk9c,1636947899.0,False
qttk9c,"it's the same stuff programmers (usually the narrow minded or ignorant ones) have been saying for most new language or framework...

This is similar to the mindset that older programmers who come from C/C++/Java backgrounds call younger programmers coming from a web dev, JS, python background ""script kiddies""",hkpa4ra,t3_qttk9c,1636968653.0,False
qttk9c,"scripts are actually awful, its so unnorganized and all over the place. i was literally horrified at some of the spaghetti python code a peer had done (they were only 4 years younger than me, their code was very affective, but i dont think they thought about someone else being able to read and understand it).

JS as a script has its place because web.",hkpvg37,t1_hkpa4ra,1636983795.0,False
qttk9c,Thank you for posting this! Very very interesting,hkqcf8n,t3_qttk9c,1636991599.0,False
qtrrml,You are mistaking experience with genius.,hklhewu,t3_qtrrml,1636903542.0,False
qtrrml,"Thats right, most coding done in courses at universities is basic stuff. Writing 20 lines of correct code is not that complicated when you are doing it for several years.",hklkuks,t1_hklhewu,1636905068.0,False
qtrrml,"Maybe but I'm pretty ""experienced"" at this point and taken point on a few very large data intensive systems and I think if you pointed me at a random problems without a few days leetcode warmups I'd probably struggle if it wasn't in my domain. If it isn't genius its at minimum someone very skilled.",hkln41o,t1_hklkuks,1636906053.0,False
qtrrml,"Yea, but a teacher would be experience at the type of problems students ask. That is his domain. The problems uni students come up are not really that “random”
I am happy that op has a good teacher.",hkn8oft,t1_hkln41o,1636928494.0,False
qtrrml,And a teacher would be more experienced than a student who has taken some courses rather than taught.,hkok1cb,t1_hkn8oft,1636950248.0,False
qtrrml,"It is skill, for sure.",hklnwry,t1_hkln41o,1636906396.0,False
qtrrml,Well it probably seems impressive to me since it's so different from the other profs. Also I checked and it's more like 35 lines for each example which probably still isn't that impressive for some. But to me it is something new to see someone just program multiple examples with about 35 lines each lesson without using some editor that shows you when you fuck up. Just haven't seen someone do that before,hknti47,t1_hklkuks,1636937628.0,True
qtrrml,"So he uses nano or vi? And then he complies using the CLI G++? You guys learning C++?

I had a pretty good professor who was comfortable enough to type out code demos in lecture. It kept the class fun and engaging. I think he used an IDE, though.

I wouldn't mind using a terminal text editor, but I think I gotta have color syntax highlighting, minimum feature....",hkohdtu,t1_hknti47,1636948858.0,False
qtrrml,"Until I have achieved flow, I want rainbow ide's for training wheels. And even then, i might still want rainbows",hkp0tv0,t1_hkohdtu,1636960992.0,False
qtrrml,Pretty sure he uses gcc for compiling. We are learning a few advanced Java methods and the basics of Haskell and Prolog with him this year.,hksktqb,t1_hkohdtu,1637024295.0,True
qtrrml,"Also him coding on the spot is a good example. The students should be paying attention to how he’s coding on the spot and his reasoning, ask if he’s not explaining out loud",hkmmvgj,t1_hklhewu,1636919993.0,False
qtrrml,There exist experienced geniuses!,hklmb84,t1_hklhewu,1636905710.0,False
qtrrml,Agreed. He's just been programming a long time.,hkmazh2,t1_hklhewu,1636915598.0,False
qtrrml,"Exactly. I do this all the time in front of the class, and I can tell you: I am NO GENIUS! 
OK I don't exactly use just bare bones text editors like vim or Notepad (I prefer VS Code), but on occasions I would use vim and compile on the command line (for example to show different compilation levels, compiling with debug info etc) and quickly show the effects to the students. 
We also do a lot of variations of our programming excercises (C/C++) on the spot, and if the students have problems, I will just code them on the fly in front of the class. 
You can do this too. You just need more time and determination.",hkohgja,t1_hklhewu,1636948896.0,False
qtrrml,Well this is the first prof that does it that way and he teaches us 4 different languages that are all pretty different from each other. Our profs from the last semesters always prepared their code and often still had to later edit it because they made mistakes. Also my first time seeing a prof just using text editor and the terminal for everything. I think being able to take in all that experience also needs intelligence.,hkns8p1,t1_hklhewu,1636937031.0,True
qtrrml,Ok but what editor does he use?,hklqzj6,t3_qtrrml,1636907704.0,False
qtrrml,Would be funny it was gedit,hklwrdg,t1_hklqzj6,1636910095.0,False
qtrrml,Whats wrong with gedit 😡,hkmj762,t1_hklwrdg,1636918611.0,False
qtrrml,"Nothing, I use it all the time",hkmzxpr,t1_hkmj762,1636925035.0,False
qtrrml,Nano is superior.  s/,hkpmdz2,t1_hkmj762,1636978524.0,False
qtrrml,Everything. It isn't Hedit.,hkoppqh,t1_hkmj762,1636953513.0,False
qtrrml,inb4 it is regular windows notepad to screw with everybody.,hkm3n5i,t1_hklwrdg,1636912890.0,False
qtrrml,Google docs,hkm3yip,t1_hklqzj6,1636913016.0,False
qtrrml,Excel,hkm5wea,t1_hklqzj6,1636913714.0,False
qtrrml,The true Chad editor. Vim,hkn1a2x,t1_hklqzj6,1636925542.0,False
qtrrml,Obviously,hkoi731,t1_hkn1a2x,1636949276.0,False
qtrrml,atom gang rise up,hkmc4k7,t1_hklqzj6,1636916015.0,False
qtrrml,"The correct editor: neovim.

(Ducks...)",hkmb5j2,t1_hklqzj6,1636915660.0,False
qtrrml,"Nothing wrong with any vim. Neovim looks beautiful when prepped up. 

I'm using ((((DOOM!)))) >:D",hkov9nx,t1_hkmb5j2,1636957032.0,False
qtrrml,Doom nvim is very nice. I was using doom emacs but Doom nvim meets my needs better.,hkpyj01,t1_hkov9nx,1636985358.0,False
qtrrml,that's a thing?? :O gonna check it out,hkq0fd5,t1_hkpyj01,1636986279.0,False
qtrrml,Just the text editor,hknrcr9,t1_hklqzj6,1636936617.0,True
qtrrml,There are a couple of them. Getting to know what the ubiquitous ones are and how to use them could help you in your career later.,hkoo1ap,t1_hknrcr9,1636952507.0,False
qtrrml,"If it's on Linux, gedit is labelled just ""Text Editor"" at least on the Gnome desktop (kind of like Gnome calls the Nautilus file manager ""Files"", etc.), so it might be that.",hkph1by,t1_hkoo1ap,1636974554.0,False
qtrrml,Yeah exactly. I probably should've explained what I mean better 👍,hkslyd8,t1_hkph1by,1637024798.0,True
qtrrml,"What you mentioned about ""knowing the answer upon being asked"" is actually a skill you can pick up on.   

It's something I've done for basic problems myself and realized they if I keep practicing the art of ""problem solving"" I'll get to do it for harder questions.   

Say `hello world!` as you embark on this world of programming, and good luck to you, buddy!",hklm3ip,t3_qtrrml,1636905620.0,False
qtrrml,This is funny.,hklmx1s,t3_qtrrml,1636905970.0,False
qtrrml,"What I found funny was the use of 'Chad' as an adjective. I have never seen it used in a postitve way like in this post. I have only seen it used by incels as a way to describe the guys they simultaneously envy/loathe.

I'm not trying to be rude; I'm completely serious. I have never seen someone call someone a 'Chad' in such a positive way.

For context: [Chad, on the incel wiki](https://incels.wiki/w/Chad).

Do normal people really use 'Chad' as an adjective these days? It just seemed so odd in this context.",hkn6d46,t1_hklmx1s,1636927538.0,False
qtrrml,"Teenager here: yeah, we use ""chad"" as equivalent to ""worthy of respect and admiration""",hko0rpn,t1_hkn6d46,1636941015.0,False
qtrrml,"normal might be a stretch, but its very common now to use 'Chad' as an adjetive, the internet is weird.",hkp455d,t1_hkn6d46,1636963579.0,False
qtrrml,"Definitely, and , anecdotally at least, more often than not I see it used in a positive way. Though, whether or not the people I am around (mostly other students) can be classified as “normal” is heavily debated.",hknael8,t1_hkn6d46,1636929221.0,False
qtrrml,I just thought it was funny that simply being able to code on the spot was such a huge deal to OP.,hknggmx,t1_hkn6d46,1636931816.0,False
qtrrml,"Well that is probably because the profs I had in the last years always prepared the code and students were still able to find mistakes or criticize it. They also often replied to questions with ""I'll research that once I get home and tell you next lecture"" or would think for a quite long time and experiment a little before getting it right. I mean we are 400 students and a few already are experts at the field and simply need a degree so some of the questions are very specific and rather complicated. I am just amazed how he seems to immediately know the answer and starts typing.",hknr19e,t1_hknggmx,1636936469.0,True
qtrrml,"Congratulations, you are finally getting what you're paying for, a competent professor.",hknz2jz,t1_hknr19e,1636940229.0,False
qtrrml,Yeah dude this passed into general slang with some semantic shifts a long time ago lmao,hknlckj,t1_hkn6d46,1636933930.0,False
qtrrml,It has become a popular meme,hknoyz1,t1_hkn6d46,1636935547.0,False
qtrrml,"say chad to a youth and theyll think of that buffed up image of that mascular guy, it was a meme thing. chad = powerful and worthy.",hkpicuv,t1_hkn6d46,1636975602.0,False
qtrrml,I had a prof who did this in university. It sure helped me learn more when I saw him actually typing everything and explaining as he went. I wanted him for every class lol,hklw75l,t3_qtrrml,1636909869.0,False
qtrrml,"If you’re from ucla you know what prof codes on Microsoft word, das.",hkloomj,t3_qtrrml,1636906727.0,False
qtrrml,"Using a shell in a terminal instead of clicking around in some GUI is a lot more efficient once you're used to it. The Unix model is incredibly ingenious, with its small tools that do one thing, and that can be stringed together using an efficient language to perform pretty much any task. I don't see the point in using an IDE since I took the time to learn the POSIX shell, and haven't used one, privately or professionally, in seven years.

Highly recommend. It makes using a computer and programming more or less synonymous, again.",hkmogbu,t3_qtrrml,1636920579.0,False
qtrrml,"While I do use terminal very heavily, there are certain aspects to what an IDE can provide that the terminal simply cannot - primarily to do with visual indicators. None of them are deal breakers, but particularly when you're dealing with very complex systems (think code that has been allowed to grow too 'Enterprisey' and large web application systems) or developing graphics-heavy applications, they can be a significant benefit.
 
I effectively ban them in my teaching until students are in at least 3rd year though, otherwise they turn into a crutch.",hkn2wl9,t1_hkmogbu,1636926161.0,False
qtrrml,"Personally I don't find them that beneficial for those cases either, but it's a personal preference: I find it easier to spot systemic flaws in an architecture if I have to study it manually than if I can just ""jump around"" in an IDE and not fully suffer the consequences of how the project is structured.

I can see there's a point to them for very verbose languages like Java, just to save some typing, but I'm not sure they provide a gigabyte of value for that.",hkopo33,t1_hkn2wl9,1636953485.0,False
qtrrml,"I would love to be in at least one of your classes with this professor, he sounds amazing.",hkltju6,t3_qtrrml,1636908776.0,False
qtrrml,">	He programs everything using the text editor

As opposed to what?

Anyway, good story, made me smile.",hkn7szn,t3_qtrrml,1636928129.0,False
qtrrml,Real men draw the code in MS paint and rename the file to an .exe.,hkpu6m8,t1_hkn7szn,1636983133.0,False
qtrrml,"""IDE""",hkp71r1,t1_hkn7szn,1636965968.0,False
qtrrml,"I agree with the comment ""You mistake experience for genius"" because it brings up an important point - You can easily get to the point you see your professor at if you dedicate yourself to learning the tools, methods, and theory behind the practice. Hopefully that motivates you throughout your studies, I wish you luck.",hkn6ukm,t3_qtrrml,1636927736.0,False
qtrrml,"Those People are great! 
I also find those people pretty impressive that can control the whole PC just with their keyboard. And not even that, they even do it like twice as fast as i ever could with my mouse.
On top of that, they know hotkeys, no person ever in the history of mankind had heard of.",hknicb5,t3_qtrrml,1636932631.0,False
qtrrml,What university do you go to?,hko7038,t3_qtrrml,1636943854.0,False
qtrrml,honestly op would be way better off with his name,hkpc6cm,t1_hko7038,1636970440.0,False
qtrrml,I thought calling someone a Chad was an insult? This seems like a complement,hkp0qbw,t3_qtrrml,1636960917.0,False
qtrrml,"I do the terminal thing too, it's because standard out and standard error are being sent to the terminal so if it's not logging to a file you can see what's going wrong or why it errors out. Not every application does this, also I almost forgot the #1 reason is because you can ctrl + c out of buggy programs like firefox with 400 tabs open. The kind of ""lag"" so bad that you can't even open the task manager to kill the program",hkpeo8f,t3_qtrrml,1636972585.0,False
qtrrml,"Learned how to compile and run Java programs through the command line today. Quite cool, cus I felt like Mr. Robot for a sec.",hkmj5bv,t3_qtrrml,1636918593.0,False
qtrrml,"There is a recorded lecture series called “The missing semester of your CS education”

https://missing.csail.mit.edu/

What you describe about live coding, using the terminal, using a text editor, etc. is part of that lecture, and the best professional engineers drive their computers from the command line like this all the time.  It’s why we prefer posix-compliant shells over the dos command line.",hkneqo7,t3_qtrrml,1636931061.0,False
qtrrml,"Is this understandable for 1st Year CS student like me, I only have a tiny amount of background about programming, so this would really help if I could understand this lecture.",hko5o75,t1_hkneqo7,1636943241.0,False
qtrrml,This because he spent years coding alone in his office and preparing for the lectures.,hkmfx28,t3_qtrrml,1636917393.0,False
qtrrml,Colored words and red squiggles FTW!!!,hkmuckj,t3_qtrrml,1636922836.0,False
qtrrml,"This isn't abnormal. When teaching intro programming courses, I'd do virtually the same thing. I'd be using a IDE because that is what we teach the students to use, but for programs that size, the eclipse (at the time) was too slow to do anything past color/highlighting before I finish jotting a segment in. 

This is just a matter of experience. Those examples really are trivial to him, and hopefully will be to you eventually.",hknip6o,t3_qtrrml,1636932787.0,False
qtrrml,My professor at JuCo was like this. Seriously a legend. I thoroughly enjoyed watching him work. I wish all professors lectured this way. It’s soooooo beneficial to see the process rather than slides and 10 minute recorded videos,hknvhl0,t3_qtrrml,1636938568.0,False
qtrrml,I need this type of profs in my Master degree,hko0umq,t3_qtrrml,1636941051.0,False
qtrrml,"I'm  glad that pumped you. Stuff like that really has gotten me into working in terminals, learning vim / Doom Emacs, stop being scared of low level langs, etc. 

This talk absolutely blew my mind when he started coding live: https://www.youtube.com/watch?v=OyfBQmvr2Hc Due is writing at a million miles per hour. 

There's also the Sussman / Ableson MIT course where they code live in the terminal in Lisp AND ON THE BLACKBOARD! This was in the 80s mind you. Pretty neat stuff.",hkovlwy,t3_qtrrml,1636957258.0,False
qtrrml,Imagine not living in a tty in linux and only ever using CLI.,hkovyhl,t3_qtrrml,1636957492.0,False
qtrrml,My programming prof just shows us videos from youtube of an inaudible indian guy talking about c programming in turbo c. Most of her presentations of codes has syntax issues which makes the whole class boring.,hkowc5c,t3_qtrrml,1636957756.0,False
qtrrml,Girls with autism vs boys with autism,hkpbyfi,t3_qtrrml,1636970249.0,False
qtrrml,"Ah, yes. I did that as well, while I was teaching. As I was in the told, that skill is very rare, although I never understood why, and what's so difficult about writing a program that has less than 200 lines or code on the spot and that works as it should from the first time",hkphzez,t3_qtrrml,1636975305.0,False
qtrrml,"There are many talents, but memorization is just one of them. There is also:
- Creativity
- Discipline
- etc.",hkpk4zh,t3_qtrrml,1636976951.0,False
qtrrml,This is the kind of professor I want to be in the future. I don't want to be a boring old man with powerpoint.,hkpmb64,t3_qtrrml,1636978469.0,False
qtrrml,probably because he teaches the same class every year??????,hkpo6kr,t3_qtrrml,1636979684.0,False
qtrrml,He doesn't. The program has changed a lot during the years. But he definitely is very experienced. Haven't had a prof yet that is this good at his job.,hkslh1u,t1_hkpo6kr,1637024585.0,True
qtrrml,"I don't know why you guys are dragging him down. I mean yes, writing 20 lines of code for an introductory class is not hard, same goes for opening everything over cli; I can do it and I am in my sophomore year, but this guy managed to inspire one of his students to take interest in what he teaches and get the student to do the assignments himself, without forcing them. That's what the teachers job is. I am in CS because of a teacher like this and I can't thank him enough, ever. You can learn syntax and basic concepts over internet; probably better than they will teach you at the university, but getting inspired by your teacher is a priceless thing.",hl5pz36,t3_qtrrml,1637263907.0,False
qtrrml,"I think it is my fault for not explaining it properly. Reading it again really sounds lame and there are a few things I should have mentioned. It is not an introductory class but an advanced class. Writing 30 lines of code might still not seem that much but the damn speed with which he is able to do it even when students ask for veeeery specific examples. He can also easily switch between the languages with no problems. I would also be able to open programs over the terminal but I've never seen someone not using a mouse at all and controlling every aspect over it.  


It might still not sound that impressive to experienced people but you should really see that guy. I also did some more research and he seems to be the chairman of the programming language faculty as well and he regularly holds international speeches. He definitely is different from the other profs we had until now. So many others already had to google shit during class multiple times.",hlgp8c5,t1_hl5pz36,1637463009.0,True
qtrrml,"I usually write dozens lines of code in different files/directories and it often works from the first running

I mean I even see where I will get problems in my code, and where I won't

It's not a magic, it's just a skill and knowledge....",hkmh96q,t3_qtrrml,1636917879.0,False
qtrrml,[deleted],hklj5sl,t3_qtrrml,1636904320.0,False
qtrrml,"i agree with your sentiment, but the way you said it was so rude and condescending...",hklk42n,t1_hklj5sl,1636904742.0,False
qtrrml,What was it?,hklrusx,t1_hklk42n,1636908070.0,False
qtrrml,"just a bunch of unnecessary stuff like ""20 lines of code?? wow!!""",hklz6am,t1_hklrusx,1636911071.0,False
qtrrml,"He learned to use the terminal when a Mouse did not exist.  It is what he's learned and only what he uses.  It is his happy space.  He's probably a lunatic and uses VI, arggg. EMACS is better, but why?  Nano does just fine.

Set up your phone, recording his entire lecture. We never had that (when we walked 8 miles uphill in a blizzard across campus, both ways).

Oh, yea he probably still has stacks of cards in his office.  Visit, be impressed.  Build some xxix servers of your choice either as VM's or on Raspberry Pi's.  Either way, you learn to use the terminal.  Do things his way.  You'll learn.  Enjoy.

You want a real CS job.  Here's one that will make you UNREPLACEABLE!  [https://www.popularmechanics.com/space/a17991/voyager-1-voyager-2-retiring-engineer/](https://www.popularmechanics.com/space/a17991/voyager-1-voyager-2-retiring-engineer/)

https://voyager.jpl.nasa.gov/mission/did-you-know/",hkp4erf,t3_qtrrml,1636963792.0,False
qtrrml,8 miles is 12.87 km,hkp4fd6,t1_hkp4erf,1636963805.0,False
qtrrml,"Ahoy joelhuebner! Nay bad but me wasn't convinced. Give this a sail:

He learned t' use thar terminal when a Mouse did nay exist.  It be what he's learned n' only what he uses.  It be his grog-filled space.  He's probably a lunatic n' uses VI, arggg. EMACS be better, but why?  Nano does just fine.   

Set up yer phone, recording his entire lecture. Our jolly crew nary had that (when our jolly crew walked 8 miles uphill in a blizzard across campus, both ways).  

Oh, yea he probably still has stacks o' cards in his office.  Visit, be impressed.  Build some xxix servers o' yer choice either as VM's or on Raspberry Pi's.  Either way, ye learn t' use thar terminal.  D' things his way.  You'll learn.  Enjoy.",hkp4f85,t1_hkp4erf,1636963802.0,False
qtrrml,8 miles is 12.87 km,hkp4fvc,t1_hkp4f85,1636963817.0,False
qtrrml,"Oh, holey hell!",hkp4weo,t1_hkp4f85,1636964187.0,False
qtrrml,8 miles is  41133.29 RTX 3090 graphics cards lined up.,hkp4fra,t1_hkp4f85,1636963814.0,False
qtrrml,8 miles is 12.87 km,hkp4gij,t1_hkp4fra,1636963831.0,False
qtrrml,8 miles is 6849.71 Obamas. You're welcome.,hkp4ff5,t1_hkp4erf,1636963807.0,False
qtrrml,8 miles is 12.87 km,hkp4g3u,t1_hkp4ff5,1636963822.0,False
qtl1ir,"big O is always an approximation.  It is not a measure of how long something takes but rather a measure of how fast the time it takes grows as n grows.

The reason for the approximation is to make the math easier.

Check out below for a more complete explanation of how 

https://www.baeldung.com/cs/fibonacci-computational-complexity",hkw87zo,t3_qtl1ir,1637092971.0,False
qtkimi,"I’ll give some unconventional advice here. Check out Arduino, it’s a dev board based on the atmega328p microcontroller. A microcontroller is essentially a tiny computer, this particular one has 32kb flash memory, 2kb ram, and runs at 20mhz. But they’re simple, especially compared to a modern PC and operating system, much easier to grasp everything that’s going on. Typically you run them without an operating system, your code just goes right on top of the hardware and you have full access to all of the registers, you can set and read the voltages on the pins, you handle interrupts yourself, etc. You learn a lot this way.

Arduino is a good place to start, it mainly just provides a nice interface for flashing code into the microcontroller, but once you get a little more familiar with it you can use the microcontroller on its own on a breadboard, then use a programmer like the atmel-ice and AVRstudio to flash code into it, AVRstudio also has an assembler you can use as well.

After you get familiar with that you can move upto a multicore family of microcontrollers like stm32 and get exposure to concurrency this way.

The reason I recommend this route is I think these topics are best learned in their simplest possible forms and the PC ecosystem is not really the place to find that. I also think computing is best understood bottom-up instead of top-down.

There’s Arduino kits such as [this](https://www.amazon.com/ELEGOO-Project-Tutorial-Controller-Projects/dp/B01D8KOZF4/ref=mp_s_a_1_1_sspa?crid=2RW64OFAM05ZF&keywords=arduino+kit&qid=1636939028&smid=A2WWHQ25ENKVJ1&sprefix=arduino+kit%2Caps%2C127&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEzMkY3OFBNOTNHQUpSJmVuY3J5cHRlZElkPUEwODcwNzExMVRWOEdTTzhPSlRZRiZlbmNyeXB0ZWRBZElkPUExMDAxMzc2M1RTT1dKR0NNR05TQiZ3aWRnZXROYW1lPXNwX3Bob25lX3NlYXJjaF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl) to start with.

You can get the [Arduino IDE](https://www.arduino.cc/en/software) here. It’s C++.

There’s no hello world because it doesn’t have a screen but the equivalent would just be a program to blink an LED.

    void setup()
    {
        pinMode(LED_BUILTIN, OUTPUT);
    }

    void loop()
    {
        digitalWrite(LED_BUILTIN, HIGH);
        delay(1000);
        digitalWrite(LED_BUILTIN, LOW);
        delay(1000);
    }

setup() runs at startup

loop() runs immediately after and is looped forever until power down, there’s no operating system to return to so your program can never end.

pinMode() just sets a pin as output or input, you’re either outputting a voltage on any particular pin or reading it.

LED_BUILTIN is just a constant for the pin connected to an LED on the arduino. Equal to 13 in this case.

digitalWrite() sets the state of a pin configured as an output.

HIGH and LOW are just constants for digital high and digital low, equal to 1 and 0 respectively.

delay() just loops the cpu for a given amount of milliseconds.

You can flash your code to it by hooking it up to a USB port, selecting that port in the Arduino IDE and then hitting upload.

r/electronics is good if you need help with the circuitry aspects.

r/embedded is good if you need help with programming it or the hardware concepts.

[atmega328p datasheet](https://www.sparkfun.com/datasheets/Components/SMD/ATMega328.pdf)

[AVR instruction set](http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf)",hknwkyu,t3_qtkimi,1636939083.0,False
qtkimi,"That is very cool, and really in a way, a small dream coming true - ever since my cyber days I wanted to utilize microcontrollers - and I didn't even think for a second to get one for these purposes.  


I will def consider getting now, as I get myself a book on the subject",hkohkop,t1_hknwkyu,1636948956.0,True
qtkimi,"""Operating Systems Concepts"" by Silberschatz is probably what you want to start with. 

And of course, simply googling for material on each topic will yield mountains of reading material for you.",hkljpo9,t3_qtkimi,1636904562.0,False
qtkimi,Reading that right now for OS classes. Its a hard class and a struggle sometimes but I feel a lot more confident about programming after reading this. The author explains concepts very well,hklqkf9,t1_hkljpo9,1636907525.0,False
qtkimi,"Looks great! I will start diving right into it. Its just that googling gets me all over the place..   
But I was looking for something more linear that could help me establish solid grounds. Thanks for the recommendations!",hkoh95k,t1_hklqkf9,1636948791.0,True
qtg70j,See [One's Compliment](https://en.wikipedia.org/wiki/Ones%27_complement).,hkjd4xx,t3_qtg70j,1636858056.0,False
qtg70j,"**[Ones' complement](https://en.wikipedia.org/wiki/Ones'_complement)** 
 
 >The ones' complement of a binary number is the value obtained by inverting all the bits in the binary representation of the number (swapping 0s and 1s). This mathematical operation is primarily of interest in computer science, where it has varying effects depending on how a specific computer represents numbers. A ones' complement system or ones' complement arithmetic is a system in which negative numbers are represented by the inverse of the binary representations of their corresponding positive numbers. In such a system, a number is negated (converted from positive to negative or vice versa) by computing its ones' complement.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hkjd6c2,t1_hkjd4xx,1636858074.0,False
qtg70j,"Not sure what the OP's ""and it's a multiple of 4"" is about, but most machines now use 2's complement.  Otherwise, either form uses the high-order bit as sign, so in hex that would be spelled as \[8-F\] in the high-order nibble.",hkjnf6u,t1_hkjd4xx,1636862988.0,False
qtg70j,"Yeah, I was just referring to that page to show how the MSB can represent the sign bit for any bit-sized integer value, which is essentially what the OP was asking IMHO.",hkkybgl,t1_hkjnf6u,1636893806.0,False
qtg70j,That's what I assumed.  I just thought he might  go down a 1's complement rabbit hole without a little more context.,hklxq9a,t1_hkkybgl,1636910485.0,False
qtg70j,"Someone mentioned it in the thread, but using ones complement (inverse of a hex string) will give you the negative number",hkk0ho4,t3_qtg70j,1636869748.0,False
qtg70j,‘8’ is a hex number that begins with 8; is a multiple of 4; is positive.,hkkd3k6,t3_qtg70j,1636878327.0,False
qtg70j,"Well, technically it’s negative in 4bit representations, that’s why we precede it with 0s",hkqnzh0,t1_hkkd3k6,1636996273.0,False
qtg70j,"In the pure sense; hexadecimal is a number system, so a negative hexadecimal number would be -0x…. 

However what your referring to is probably a byte type, in which 0x80 is 128 which is the start of the negative range.

Unless it’s 1 complement which I’m not familiar with.",hkktnc9,t3_qtg70j,1636890691.0,False
qtg70j,"Recall that the sign of a binary number is determined by its most significant bit (number furthest to the left). Every hexadecimal digit is represented as 4 binary bits. For example, Hex digits 0 through 7 in binary are 0000 through 0111. On the other hand, starting at 8 we have 1000 through F is 1111, where the most significant bit is always 1 and which would indicate negative. Therefore, any hex number whose first digit is at least 8 is negative.",hkpkxiu,t3_qtg70j,1636977517.0,False
qtet0i,"To be this seems to get a few things mixed up at once.

O(nk) can be as said on SO an outer loop with n iterations and an inner loop with k. In the case of k = n you get O(n^2 ).

O(n+k) would be on for loop with n iterations followed by a for loop with k iteration, again if k = n then you'd get O(2n) ~ O(n).

I then don't fully understand what you mean with n = 10 and k = 0.

So in the case of O(n^k ) which you could think of k nested for loops with n iterations, if k = 0 it would mean there are no nested loops, in fact no loops at all -> O(1)

In the cubic sense of O(n^2 ) there's no dependency to k. Now if you'd consider O(nk) though you'd have one loop with n iterations and a nested loop with 0 iterations. Now if the second loops doesn't do anything -> O(1).
Might be easier to think of it as the loop with k iteration being the outer loop.

In both these cases if k = 0 the code will not scale with larger inputs and therefore finishes in constant time or O(1). With that said k = 0 to me seems like more a theoretical exercise than anything else.",hkj6yj2,t3_qtet0i,1636855247.0,False
qtet0i,"thanks a lot!

&#x200B;

>theoretical

yea it was just an assumption to understand the difference but like you said there is no nested loops if k is 0 so it's clear to me now!😊",hkjmokx,t1_hkj6yj2,1636862633.0,True
qtapfy,"https://ieeexplore.ieee.org/Xplore/home.jsp

Some of it is free. A lot of it is very technical",hkjc4kd,t3_qtapfy,1636857594.0,False
qt4qqy,This photo of spaghetti is making me hungry...,hkh6t44,t3_qt4qqy,1636824081.0,False
qt4qqy,before her suicide...,hkh2gmz,t3_qt4qqy,1636822241.0,False
qt4qqy,Can you share something more about this? I googled but there aren't much information about her life. She doesn't even have a wiki page :/,hkjsokj,t1_hkh2gmz,1636865521.0,False
qt4qqy,I think it was a joke? I could not imagine troubleshooting a panel like this. You would have better a chance sending it to an Italian bistro for repair with all that fucking spaghetti,hklhz4p,t1_hkjsokj,1636903796.0,False
qt4qqy,was a joke...,hkm86pk,t1_hklhz4p,1636914567.0,False
qt4qqy,The amount of “nope” associated with the general lack of labels is off the charts,hkh9f0f,t3_qt4qqy,1636825174.0,False
qt4qqy,Imagine trying to run those wires through panels!,hkkygvg,t1_hkh9f0f,1636893905.0,False
qt4qqy,The horror,hklswtr,t1_hkkygvg,1636908512.0,False
qt4qqy,That is like a scene right out of Star Trek. That's awesome.,hkh708z,t3_qt4qqy,1636824164.0,False
qt4qqy,Beautiful colors. I love how gentle film photography is,hkhcc6l,t3_qt4qqy,1636826413.0,False
qt4qqy,"The colors are fake. This photo was black and white, then colorized (poorly). Notice the gray on her arms, and behind her ear.",hkhhqr2,t1_hkhcc6l,1636828729.0,False
qt4qqy,I too would call it quits after tracing all those wires.,hkhi6e5,t1_hkhhqr2,1636828919.0,False
qt4qqy,Pretty sure ML does that,hkqnc19,t1_hkhi6e5,1636996010.0,False
qt4qqy,"Ugh that just gives me a headache. Much love to her, for doing something I would dread doing.",hkij7yj,t3_qt4qqy,1636844752.0,False
qt4qqy,So much better [now](https://www.reddit.com/r/pcmasterrace/comments/5r1cr4/cable_management_from_the_depths_of_hell/)...er...,hkjegjx,t3_qt4qqy,1636858656.0,False
qt4qqy,Cable management would like a word with you,hkl8qwm,t3_qt4qqy,1636899441.0,False
qt4qqy,Imagine getting segfault on this.,hkhsdh2,t3_qt4qqy,1636833313.0,False
qt4qqy,meanwhile the students in my digital logic class are struggling to make a 4 bit adder,hkhg9gn,t3_qt4qqy,1636828092.0,False
qt4qqy,Are you joking? Is she from Star Trek?,hkif9zq,t3_qt4qqy,1636843058.0,False
qt4qqy,my uncle did stuff like this,hkk37em,t3_qt4qqy,1636871399.0,False
qt4qqy,"I wonder, if connecting lines on the modern circuit board represents the wires on this photos",hkk66oz,t3_qt4qqy,1636873408.0,False
qt4qqy,This photo makes you really appreciate PCBs.,hkkh7ul,t3_qt4qqy,1636881391.0,False
qt4qqy,"I guess this is basically FPGA equivalent?

(also, all that work to colourize, and they left her arm like that? strange choices..)",hkl3ceg,t3_qt4qqy,1636896668.0,False
qt4qqy,r/forbiddensnacks,hkl49yr,t3_qt4qqy,1636897140.0,False
qt4qqy,What a cool kitchen,hkkb5we,t3_qt4qqy,1636876930.0,False
qt4qqy,[deleted],hkjxd0l,t3_qt4qqy,1636867961.0,False
qt4qqy,The wires are the source code,hkkok7m,t1_hkjxd0l,1636887028.0,False
qt3e8y,"Computers can understand any base that they are designed to. However, it turns out that representing numbers in a base 2 system is currently electrically optimal.

Computers don’t look at a number and try to understand what value it is. A computer is a combination of digital circuits that apply a set of predetermined transforms on a set of signals, in our case these are binary signals. 

I think the best thing to help your understanding is to look at how some of the simple digital circuits operate. Google around and look at an adder circuit, digital comparator, and multiplier. This should help you understand that computers don’t “know” things, they just operate or compute.

This is a good place to start: https://en.m.wikipedia.org/wiki/Adder_(electronics)",hkgtz8e,t3_qt3e8y,1636818522.0,False
qt3e8y,Username checks out :O,hkgv8m9,t1_hkgtz8e,1636819091.0,False
qt3e8y,This is wonderfully explained answer.,hkh4f2q,t1_hkgtz8e,1636823078.0,False
qt3e8y,Thank you\~,hkhd3s7,t1_hkgtz8e,1636826734.0,False
qt3e8y,I also want to know and look forward a valuable answer. Thanks!,hkgsfsu,t3_qt3e8y,1636817807.0,False
qt3e8y,"Imagine for example, we have two addition problems,. let's say 125+67 and 12+43. We do not do anything special in either case. There is a definitive algorithm on how to add 2 numbers and we just apply it to both problems. We do not have to recognise the numbers to carry out the addition. This is also what a computer does. It just carries out whatever algorithm we tell it to on the values we give it. This is just a basic example, but I hope this gets the idea across.",hkh0bem,t3_qt3e8y,1636821303.0,False
qt3e8y,Guy there’s 1000 page text books written about this.,hkjfoye,t3_qt3e8y,1636859252.0,False
qt3e8y,"How does the computer know which one is 1, 2, 4 etc? Well that's depending on the prcoessing unit's architecture. It depends on how the words are stored in memory. Usually it's MSB (most significant bit) on the left and LSB (least significant bit) on the right. But it could technically be the otherway around.

So in memory at a random adress 0xCAFE with MSB first it'll look something like this (given this represents an unsigned integer, for other cases check out two's complement for signed or IEEE 754 for floats)


 | 2^3 | 2^2 | 2^1 | 2^0 | at 0xCAFE


Now does the CPU care what this binary represents? The sinplified answer is no. It will take this binary number and feed it to whatever place we tell it to. 
E.g. when we tell it to add 1 it will feed it into the adder and do that. The interpretation of what this binary number is, happens on higher levels, also know as the code we run on the hardware.

Example for that, you know how you can open a binary file in an editor? It just looks like random gibberish and that's because we take 0s and 1s that don't represent text (ASCII or any other encoding) and try to display it as such.",hkieop2,t3_qt3e8y,1636842812.0,False
qt3e8y,ffs I can't get that formatting right for the number on mobile sorry for that.,hkif19x,t1_hkieop2,1636842957.0,False
qt3e8y,"Computers do not know which binary values represent which numbers. Computers do one thing: they take an input signal (a string of electrical pulses) and return an output signal. So how does a computer know what to do with the input signal? A computer engineer physically designed the computer to perform *the most fundamental computer actions* the technical name is called the **Instruction Set Architecture opcodes**. A computer programmer is a person who is trained to understand the meaning of the ISA opcodes, programmers will write a computer program that conforms towards the computer opcodes. The computer program they write is also a long string of input signals that the computer will perform.

So you can input any string of binary values into the computer and the computer will perform an action according to the definition of the computer opcodes. This doesn't mean that the computer understands what it is doing, it's just performing computations because it was designed to work that way. So when you enter a number that's supposed to represent a number value, the computer doesn't understand this at all. It is supposed to be the programmer's responsibility to give meaning to the binary string value: it is the programmer who is interpreting and assigning meaning to the binary string value.",hkjirfh,t3_qt3e8y,1636860742.0,False
qt3e8y,i suggest going through the nandgame. it really cleared a lot of this stuff up for me.,hkwe6m8,t3_qt3e8y,1637095318.0,False
qsn40p,"> That said, he then explained us that for that reason (not every state with an input I will end up in a steady state) we need an external synching (a clock) and a memory to make a state steady. Why is that? And why don't we need that for asynchronous circuits?

It's kind of by definition. The use of the clock's edge to trigger input/output latching is what makes a circuit synchronous. Whereas an asynchronous circuit is a ""natural"" electronic one that is always in flux.


So a synchronous circuit is the normal stuff you get in digitial logic - a clock, lots of flip-flops, and then random logics gates between the flips, usually defined by a state machine.

An asynchronous one can be made up of things like [Muller C-Gates](https://en.wikipedia.org/wiki/C-element).


I don't know of a good resource to direct you to for this. Do you know much digital logic? Asynchronous circuits is quite an ""advanced"" topic there, but understanding basic digital circuits and you understand the synchronous case.",hkegq7h,t3_qsn40p,1636764192.0,False
qsn40p,"First note that synchronous and asynchronous only apply to sequential logic, not combinational logic.

In a synchronous circuit, the inputs to a stage are sampled only when the clock signal goes active.  So the clock needs to be slow enough that all inputs are valid when the clock goes active, otherwise the output may be wrong.

In an asynchronous circuit, the inputs to the stage are sampled either all the time, or when it is known that all inputs are valid.  The former solution can have glitches or race conditions if the circuit is not designed properly.

In short, it's all about how to knowing when your input data is valid.  Either wait long enough, or have something tell you when it is valid, or design your circuit so that transitory invalid inputs don't make a difference.",hkegrlh,t3_qsn40p,1636764211.0,False
qsis3v,"I'm not sure about the companies you mention renting out time, but supercomputers ('High performance computing', or HPC) are not as rare or difficult to create as you might imagine. Many Universities and companies with heavy R&D (e.g. pharmaceutical companies or engineering firms) own their own supercomputers. They will usually approach a specialist company that helps them design, procure, install and maintain it. Or help them build one in the cloud.

  
In terms of what is run, a workload manager like Slurm is usually used. This allows for different groups of users/ different queues for compute jobs. So this could be say your astronomy department, your biology department and your engineering department. They can submit jobs to a queue, which will run when resources (CPUs, GPUs and memory) become available. Queues can be given different priorities and resource allocations.

  
It would be up to the person/ people in charge of the supercomputer to determine who can submit jobs and what queues can be used.",hkdoxnl,t3_qsis3v,1636751427.0,False
qsis3v,[deleted],hkdojb3,t3_qsis3v,1636751257.0,False
qsis3v,No shit? Interesting. Well thanks for replying. I definitely thought that the cost of buying one outright would not justify the research efforts.  Now I'm gonna fuck off the next two hours and see what the pricing models look like for funsies!,hkdoz63,t1_hkdojb3,1636751445.0,True
qsis3v,I mean you can get a 500 ish core system for maybe 250,hkdpaey,t1_hkdoz63,1636751583.0,False
qsis3v,"If you wanted to really get hands on, it is possible to create a small one of your own with a few Raspberry Pis: https://www.raspberrypi.com/news/supercomputing-with-raspberry-pi-hackspace-41/",hkg1oqi,t1_hkdoz63,1636801258.0,False
qsg140,"_DNS - Domain Name System_, ergo: no domain = no point in storing random address on DNS server",hkctk03,t3_qsg140,1636738218.0,False
qsg140,"You register a name to an ip. So your ip isn’t stored within that specific carrier until it’s associated by registering it. The internet is IP address, and can be navigated as such, a dns resolution just equates a string to an ip.

Your pc won’t be a open network device, you connect to the internet through a router and modem, these devices have their own IP and that is queryable in the network table, which is a ledger of known IPs. Your pc has a local IP within that devices network and not the public network.

Your pc -> router -> modem-> buncha different devices -> target server -> buncha different devices -> request origin modem -> request origin router -> request origin local IP.

The other thing to note here is that the device you request data from might have the same local IP as you, but within a different parent network.",hkdnn15,t3_qsg140,1636750870.0,False
qsg140,"Generally speaking your isp who assigns you your IP address will have a DNS entry associated with it. This is not always the case it just usually is. Theres not too much extra information there, but often you can get what city the IP is in and who your ISP is. Either directly or by tracing the names of routers on the way to the IP.

The one for my cell phone right now is:
69-232-153-116.lightspeed.tukrga.sbcglobal.net

(Incidentally I'm pretty sure this IP is shared with a bunch of other folk, and changes since it's mobile, so I'm not terribly concerned with security here, don't share your IP as a general rule)

The actual ""DNS ledger"" is distributed over a number of different servers. To figure out what specific computer has this entry more or less works by reading from right to left. There a few root nameservers that are all synced that can point you to nameservers that control top level domains like .net. Those can point you to nameservers that control domains like sbcglobal.net. It usually ends there but there might be a separate nameserver that controls a specific subdomain like tukrga.

Once you get the specific nameserver, in this case ns2.attdns.com, you can ask it for what ip 
69-232-153-116.lightspeed.tukrga.sbcglobal.net points to.",hkfm7ra,t3_qsg140,1636787714.0,False
qs8gkb,"You can try to study slowly, take more notes, and then do a comprehensive project for each chapter, use it continuously, and you will become proficient.I think the most important thing in learning is to use it, not to memorize it.",hkbwwjh,t3_qs8gkb,1636724231.0,False
qs8gkb,"First, I pick books based on whether I see them on multiple “must read lists.” It’s not perfect but Hacker News is generally where I pull book recommendations. 

Second, before buying, check the Table of Contents. In the store or online you can usually see the first few pages. See if the chapter names are totally new or known topics. 

Then, if the books a good fit and has good reviews, I get it. Most books have a “how to read this book” in the intro. So the answer to your question is really “it depends.” Some are only meant to be read front to back. Others have pick-and-choose style. Some have a read the first half in order but then you can pick after that. 

If I’m studying, like for interviews, I take an index card and write down chapters & sections I want to read. 

I usually take notes in the Apple Notes app if it’s mostly text (not a lot of code or special characters) or the GoodNotes app on my iPad with a pencil. You can just use a pencil and notepad for less than $5 too. 

Repeating or refreshing material on an exponential back off is CRUCIAL to actually learning. Look up “how to learn” resources. I know that sounds condescending but learning how to learn is the most important meta-skill you can have.",hkcv8sw,t3_qs8gkb,1636738896.0,False
qs8gkb,If it’s technical that I want to remember I wrote it down even if I won’t ever read those notes again just for the memory boost. But if it’s a story or something I don’t plan to use frequently I just read to understand and don’t write notes. U can always go write notes should I need it,hkc8g01,t3_qs8gkb,1636729592.0,False
qs8gkb,"Personally, I partially read them on an on-demand basis, depending on the problem I’m trying to solve. Never really read them from start to finish.",hkc2ao2,t3_qs8gkb,1636726843.0,False
qs8gkb,I just read it like I would any other book I guess. Start with the first page and finish with the last one; don't go on until you know what you've just read. That's it really.,hkdwxvs,t3_qs8gkb,1636754940.0,False
qs8gkb,"Hi, for me it really depends on the kind of book. For a programming/AI style book I will read it quite swiftly and then start some small projects to play with the software or algorithms. Then, instead of googling when something isn’t working the way it should, read the according section thoroughly. 
Now, for books that focus on the theoretical side, e.g. Algorithms by Sedgewick, I will focus throughout the book and read the import sections more than once next to taking notes and summarising the essentials.",hkdznl0,t3_qs8gkb,1636756166.0,False
qs8gkb,"Lots of good answers here.

I like to read books that have type in programs that actually run so I do two passes through the book. One pass I type everything in.  One pass I read everything but the code.  I get context from the reading and first hand line by line exposure to the code by typing.  The two passes can happen concurrently for example I might type in chapter 4 in the morning and read it as im going to sleep that night.

This is the best way for me and ive been through 4 books this way and Its fun and I love it.

Good luck",hkekjvh,t3_qs8gkb,1636766078.0,False
qs8gkb,"I read my books like a novel (just reading straight through) and I take the approach of repeating the book multiple times. I am making use of the proverb that **repetition is the key to learning**.

1. First exposure is all about getting exposure to ideas within the book. I don't worry about remembering ideas or the way the ideas connect, I just want exposure to the ideas so I know that they exist.
2. Second reading is about considering how the ideas relate to one another. I don't worry about memorizing anything, I want to focus on a narrative that connects a weave of ideas from beginning to end.
3. Third reading is when I start to write notes if I feel like writing notes. Since I have exposure to the ideas and how they connect, I now have a simple understanding of the book, I can write effective notes about the book. If all I wanted was a simple understanding, three iterations is normally good for me.
4. Any more readings above three is to reinforce the learning that I've already done. Repeating the study material multiple times means you can pick up subtle ideas that are easy to miss when you're faced with a whole book's worth of detail.  The more repetitions you do, the stronger your imprinting for the ideas you already know, the easier it is to detect subtle details that are easy to miss. 

This strategy allowed me to read books that where initially incomprehensible to me. The value of repetition means I can (eventually) absorb ideas that have no meaning to me. I don't always need to understand ""why"" or ""how"", but I can ""know"" precisely what the idea says through the power of repetition.",hknagf1,t3_qs8gkb,1636929242.0,False
qs8gkb,very cool method!,hkpfg6f,t1_hknagf1,1636973243.0,True
qs8gkb,Bring that manual in the bathroom! I read when I can! 🚽,hkegxpj,t3_qs8gkb,1636764295.0,False
qs8dso,"Computer science is more theoretical, e.g. analyzing and comparing algorithms. Programming is more practical, with specific details about a particular programming language. Sort of like the difference between a Linguistics class and a French class.

The course descriptions should provide more specific details on the two classes.",hkbghmo,t3_qs8dso,1636713465.0,False
qs8dso,Ohh ok thanks!!,hkbgjrh,t1_hkbghmo,1636713518.0,True
qs8dso,"I liked this comparison the most. Simply saying, you can learn one language and yet stuck in it threw the end of your life. Or learn the grammar, from scratch, and learn how to gather the appropriate resources, just to learn any other language. What you choose?",hke10cs,t1_hkbgjrh,1636756782.0,False
qs8dso,"Most unis typically get it wrong, and have computer science as 'programming for good students' and programming/software as 'programming for weak students but we still want their money'. So the real definition may not apply here.
 
Computer science is the mathematics discipline that studies computation, which is the storage and manipulation of data. So data structures, algorithms, formal languages/grammars, logic. Programming, or software development/engineering is the *use* of that computation to solve problems in the real world.",hkbgh2d,t3_qs8dso,1636713451.0,False
qs8dso,"Ohh I hope they don’t cheat me lol, thank u!!",hkbglgo,t1_hkbgh2d,1636713560.0,True
qs8dso,"See I thought that's what was going to happen to me as a CS major, but boy was I wrong. Now in in Automata Theory and Advance Algorithms breaking down individual components and I want to quit so bad. But I'm so close to graduating so I can't. I hate theory, always have.",hkeewkx,t1_hkbgh2d,1636763305.0,False
qs8dso,"Honestly though it’s going to make you a good engineer. If you can write a proof you can write good, tight code. Mathematical fluency will never be a disadvantage",hkelthn,t1_hkeewkx,1636766704.0,False
qs8dso,This !,hkez94c,t1_hkelthn,1636773386.0,False
qs8dso,"Computer science is the what and why, programming is the technically how.",hkbq3so,t3_qs8dso,1636720463.0,False
qs8dso,Computer Science is to programming as Physics is to Mechanical engineering,hkcy0qb,t3_qs8dso,1636740021.0,False
qs8dso,That's my go-to explanation,hkd00kj,t1_hkcy0qb,1636740838.0,False
qs8dso,"Others have distinguished the two well but I will say the following.

Studying CS will mold you into a better problem solver. You will be able to analyze problems with analytical reasoning; thus, helping you to become a better programmer. CS is all about learning why and how it works; hence, knowing what happens ""under the hood"" is very beneficial for any programmer. You will have the skill to solve problems efficiently if the need arises and you will have the skill to optimize code as well. With CS knowledge, you will write better maintainable code. 

You can definitely study programming and learn CS along the way as well. So, it's up to you but know that CS is very applicable and very useful to programming.

Best of wishes.",hkbjg0j,t3_qs8dso,1636715850.0,False
qs8dso,Thank u so much!!,hkbjo7f,t1_hkbjg0j,1636716026.0,True
qs8dso,I was expecting a joke. This post disappoints.,hkc5hc8,t3_qs8dso,1636728299.0,False
qs8dso,You ever heard about the guy who went from VIM to Emacs?,hkggvwu,t1_hkc5hc8,1636811983.0,False
qs8dso,"In addition to the existing answers:

Roughly the difference between driving and studying the materials science and physics involved in driving.",hkbhsse,t3_qs8dso,1636714561.0,False
qs8dso,So I should take computer science first before programming?,hkbhys8,t1_hkbhsse,1636714697.0,True
qs8dso,I'd pick computer science. If you pick programming you'll probably be programming a lot of stuff without knowing why whereas if you do computer science you'll learn why it works and have a better understanding of why you are programming stuff in a certain way when you pick up programming. That's been my experience anyway,hkbiy8c,t1_hkbhys8,1636715468.0,False
qs8dso,Ohh ok I’ve solidified my choice lolol thank u!!!,hkbj83z,t1_hkbiy8c,1636715676.0,True
qs8dso,"It's not always the case. There was an episode of The Big Bang Theory where the guys broke down by the side of the road. Leonard asks if anyone in the group knew about automobile engines. They were all eager to describe how an internal combustion engine works, then Leonard asks who knows how to fix one; they all fell silent. :-)",hkcki2s,t1_hkbiy8c,1636734576.0,False
qs8dso,"Well, that depends on which trajectory you want.

There is a lot of overlap: you can't learn one without getting a fair idea of the other. If you are going to get into programming, you could in principle start with either one.

However, if you start with computer science and pick up programming, the thing that is going to trip you is all the stuff that is neither CS nor programming. Collaboration, systems analysis, etc.

If you start studying programming, you will possibly get the work-related stuff (there are different kinds of programming courses) and a gist of CS.

If you simply mean that you will study programming, then CS or CS, then programming before you start working, I imagine it's roughly the same.",hkbimg9,t1_hkbhys8,1636715216.0,False
qs8dso,"Oh I see, thank u so much!",hkbiury,t1_hkbimg9,1636715395.0,True
qs8dso,"If you can, try mixing both. During school I'd suggest to do more of CS, but still you need some practice to really get what all the sciency stuff means",hkbweuo,t1_hkbhys8,1636723981.0,False
qs8dso,"its harder to learn computer science than programming, imo you can learn programming yourself online and if you take computer science you should get enough experience to cover for the other course and obviously you will learn more theory. i dont think taking both is worth it but it depends on what exactly they include",hke71ex,t1_hkbhys8,1636759565.0,False
qs8dso,"Computer Science is the study of algorithms.  You will learn programming to help you study algorithms.  Essentially, programming languages are a tool and computer science is a means to understand how to use that tool well.",hke2o99,t3_qs8dso,1636757538.0,False
qs8dso,"Do you go to UT Dallas and are you talking about CS 1200 and CS 1325? Bc then everyone in this comment thread is wrong. One is about what the curriculum teaches and the other is actually learning programming concepts. 

I always tell people to post which courses they’re talking about bc without reading the descriptions no one will know what you’re talking about. People made some pretty wild assumptions.",hkc4743,t3_qs8dso,1636727722.0,False
qs8dso,[deleted],hkbub8a,t3_qs8dso,1636722875.0,False
qs8dso,My schools cs program includes a math minor.,hkd50ek,t1_hkbub8a,1636742920.0,False
qs8dso,"In practical terms, it's like construction vs architecture.",hkbzz3z,t3_qs8dso,1636725742.0,False
qs8dso,Programming is computer science but computer science isn’t necessarily programming,hkc88pf,t3_qs8dso,1636729504.0,False
qs8dso,I don't know about that.  I've some programming that's more similar to Italian pasta.,hkd8xrp,t1_hkc88pf,1636744585.0,False
qs8dso,What's the difference between learning medicine and applying a Band-Aid?,hkct32d,t3_qs8dso,1636738028.0,False
qs8dso,"I forget where I heard this analogy but basically like how astronomers study the stars and use a telescope as a tool to do so, computer science and programming can be thought of in the same way. Programming is just a tool we use to study computer science.",hkcyipy,t3_qs8dso,1636740226.0,False
qs8dso,I would probably go with CS to understand the *why*; most of my colleges that have taken programming in school have discovered you mostly learn programming on the job.,hkd2dp8,t3_qs8dso,1636741819.0,False
qs8dso,"It is sort of like studying music vs playing the guitar (or any instrument).  


The guitar is a technical skill and you may be able to use it well even without formal music education, but knowing music theory is a new domain. Sure, they interact a lot. Also, there is no much sense in learning musical theory if you don't play any instruments.  
Finally, playing the guitar will probably help you pay the bills, but most of the greatest guitarists know a lot about music theory.",hke2qvq,t3_qs8dso,1636757570.0,False
qs8dso,"My university has a blanket computer science degree, and several different emphases for specializing. Data science, bioinformatics, software engineering. In addition to some core computer science practices, software engineering at my university covers material related to the development life cycle as well as some practical programming skills like some ides, in depth practice with git, testing and verification, etc. 

I'm just pointing out there can be overlapping material in the labels, and that there's also software engineering.",hke3pfg,t3_qs8dso,1636758010.0,False
qs8dso,The same difference as carpentry and architecture. Programming is practical and computer science is more theory.,hkfbgeh,t3_qs8dso,1636780194.0,False
qrlsrw,Programming will never be obsolete the languages just become higher level.,hk7amy7,t3_qrlsrw,1636639014.0,False
qrlsrw,"I'm inclined to agree. Even when they are programming themselves, there will probably always be a market for someone to dig in there and take it further. One of the first things programmers did was automate part of their jobs by inventing compilers. The field seems to be founded on raising the level of interaction through this kind of virtuous cycle.",hk7pfl3,t1_hk7amy7,1636645698.0,False
qrlsrw,Self teaching for over a year and already prophesying  the end of all things,hk7b90q,t3_qrlsrw,1636639314.0,False
qrlsrw,"Yeah... I don't think AI is going to be capable of coding in a right way as there is no right way to code, have you seen GH Copilot?",hk7arqg,t3_qrlsrw,1636639079.0,False
qrlsrw,Copilot is part of the reason I ended up on this thread.,hkclvdt,t1_hk7arqg,1636735127.0,False
qrlsrw,[deleted],hk7ipzw,t3_qrlsrw,1636642764.0,False
qrlsrw,"Basically coders will be made obsolete, engineers will become more important.",hk7s09h,t1_hk7ipzw,1636646799.0,False
qrlsrw,"You are right actually, that's spot on what I was thinking about. !",hk85h0f,t1_hk7s09h,1636652158.0,False
qrlsrw,"People have been saying that technology will put everyone out of work for something like 200 years.  It was only a couple of years ago that everyone was saying how necessary a universal basic income was as machines were taking over work and when there's nothing to do, it's not realistic to expect everyone to support themselves by working.

Look around.  There is no shortage of jobs, there is a shortage of labour.  I don't see any reason this pattern will not continue.",hk7czgm,t3_qrlsrw,1636640147.0,False
qrlsrw,Something like 200 years?  Socrates laments in the Phaedrus that the new invention called “paper” would destroy memory. Obsolescence is an argument as old as time.  Keeps not happening.,hkab0eh,t1_hk7czgm,1636685881.0,False
qrlsrw,Have you ever dealt with a customer? An AI wouldn’t be able to deal with that bullshit.,hk7rota,t3_qrlsrw,1636646665.0,False
qrlsrw,"Not what we have now, we would need human level AI.",hk8038t,t1_hk7rota,1636650021.0,False
qrlsrw,Which is definitely not a couple years out,hk85m4y,t1_hk8038t,1636652216.0,False
qrlsrw,Decades minimum (if it ever happens).,hk8agsf,t1_hk85m4y,1636654095.0,False
qrlsrw,"Nah. Computers cant figure out scope or what brings value.

AI is also highly overrated. The “AI revolution” is just an explosion of data mining and data storage.
Computers 20 years ago could have done the same shit but data is more availa le.

What data would you mine to make an original program? None.",hk7yc37,t3_qrlsrw,1636649334.0,False
qrlsrw,"Lol forget programming my dude, the circus could use someone like you",hk7epnm,t3_qrlsrw,1636640950.0,False
qrlsrw,Be careful on what you read and who you learn from. Most information online is from people that don't know anything and just write articles about AI taking over programmer's work.,hk7oq7i,t3_qrlsrw,1636645398.0,False
qrlsrw,"I enjoy coding.

And as a result of the course that I did, I realised coding isn't something I'd personally wanna do as a job anyway (well at least in the way it works in most companies), and I'm okay with that.",hk7cpom,t3_qrlsrw,1636640015.0,False
qrlsrw,"I mean, if AI is really at a point it can automate programming itself, it's likely it'd have automated away other, manual jobs already by then",hk7qd0s,t3_qrlsrw,1636646091.0,False
qrlsrw,AI to do development work? That's decades away at least (if it ever happens at all).,hk7zzr8,t3_qrlsrw,1636649983.0,False
qrlsrw,"AI has been threatening to remove devs for 40 years.

I'm not scared.",hk81nvw,t3_qrlsrw,1636650653.0,False
qrlsrw,"It's not going to replace programming jobs.

Some kind of an AI thing may be able to produce some code for common cases or tasks. That's something we've seen in recent demonstrations of ""code-writing AIs"". It's not going to be able to produce brand new kinds of code for brand new tasks that don't resemble something they've seen in the material they've been trained with. Even more importantly, any kind of an AI like that is not going to be 100%, and it's not going to be able to tell when its output is wrong or to correct itself. You'll need a programmer to at the very least tell if the code is reasonable or does what it's supposed to. And reading and debugging code can be at least as hard as writing it in the first place.

Good luck just pulling some code out of an AI, plugging it into production and thinking you're done with it, without having someone with expertise involved. That's just not going to happen.

Being able to always get correct code from an AI, or having the AI also understand whether the code actually makes sense, would require artificial intelligence on a much higher level than we're able to even imagine building at the moment. If that ever happens, the entire society will have to be rethought, and programming is almost certainly not going to be one of the first victims.

So, I largely deal by trying to ignore the clickbait titles and senseless articles that proclaim AI is going to replace programmers.",hk8cbxi,t3_qrlsrw,1636654841.0,False
qrlsrw,Buddy McDonald’s is still paying real people to dump fries out of a basket into smaller cardboard baskets. I think we’re quite a long way from AI replacing programmers.,hk8f70s,t3_qrlsrw,1636655983.0,False
qrlsrw,"...said no AI researcher, ever. Don't listen to journalists when it comes to computer science. AI research is still in its infancy after 50 years, and struggling to even define what intelligence is. Real world AIs can be rather frightening though, but mainly because they increasingly get entrusted to make real decisions affecting people's lives, despite being so utterly dumb and manipulable.",hk8gnxc,t3_qrlsrw,1636656570.0,False
qrlsrw,"If all goes wrong, I will still code for the fun. I love coding, earning money is just a consequence.",hk7oem4,t3_qrlsrw,1636645259.0,False
qrlsrw,Even ai will need more people to do operation,hk7s6o4,t3_qrlsrw,1636646874.0,False
qrlsrw,[deleted],hk81zrd,t3_qrlsrw,1636650783.0,False
qrlsrw,What class did you take?,hkcm82o,t1_hk81zrd,1636735269.0,False
qrlsrw,Don’t believe the sensationalist hype.  They’ve been saying the same thing for years.,hk86dha,t3_qrlsrw,1636652509.0,False
qrlsrw,"Yeah… have you actually dealt with any real AI? The most widely used “advanced” AI still autocorrects “fuck” into “duck”. Sure there are some neat AI’s out there but they are extremely limited in what they do, like recognizing real world objects, or analyzing pictures for certain content. The ability to write functioning code just isn’t in the cards for AI yet, and even then, there will still be humans writing the code for the AI… so not really something I’m gonna worry about.",hla58r0,t3_qrlsrw,1637344579.0,False
qr8cpv,Don’t do it. Especially not on Twitter,hk5b4a6,t3_qr8cpv,1636594467.0,False
qr8cpv,Lol. Who are the biggest SWE influencers to avoid?,hk5bpaa,t1_hk5b4a6,1636594727.0,True
qr8cpv,"If you’re looking for real content I would suggest signing up for a weekly newsletter on something you’re interested in.

SWE Twitter is essentially the same 5 recycled jokes and low effort content",hk5py03,t1_hk5bpaa,1636601141.0,False
qr8cpv,Twitter SWE is basically JavaScript porn.,hk52iks,t3_qr8cpv,1636590604.0,False
qr8cpv,any specific accounts you follow?,hk53x6s,t1_hk52iks,1636591233.0,True
qr8cpv,"I follow synsation on Instagram 
Really inspiring. She was a baker then became a web developer",hk52x4u,t3_qr8cpv,1636590785.0,False
qr8cpv,thanks,hk53y7a,t1_hk52x4u,1636591246.0,True
qr8cpv,IAmTimCorey on YouTube is about it. And he pretty much only deals in C# and ASP.NET content,hlas7uv,t3_qr8cpv,1637353141.0,False
qqxc02,"    def step_func(analog, threshold=0.5):
        if analog > threshold:
            return 1
        return 0

The above code is in Python. But it's a very simple algorithm, and can be implemented in any language. You'd have to map that function over your entire wave in a preprocessing step.",hk6h40p,t3_qqxc02,1636617879.0,False
qqxc02,"Not sure what you got going on but usually one sets up the sensors, then for each channel an adc, at some sample rate, with some scaling, fft and filters in sw.",hk3kzdb,t3_qqxc02,1636568997.0,False
qqxc02,Noise is usually dominant at higher frequencies. So you can try removing higher frequency waves. Usually Fourier analysis is used for this task. I guess your removing higher than mean method might work better with frequencies separated though I'm not entirely sure.,hk39jp9,t3_qqxc02,1636564590.0,False
qqxc02,Then a simpler low pass filter method might also work.,hk4ljih,t1_hk39jp9,1636583274.0,False
qqxc02,"Are you computing the mean as the data comes in or are you preprocessing it? 

If you’re analyzing data as it comes in your mean will fluctuate and might not give you accurate results.

If you’re preprocessing, this could be very slow as you need to make an initial pass through per run.


What you might consider is seeing what range your sensor outputs and determining what the minimum amplitude is for it to be a 1. Doing this would require you to run the sensor and do some analysis on the data you get.",hk3jicj,t3_qqxc02,1636568433.0,False
qqxc02,"If you want to turn an LED on only when a certain observation is made in the EEG data stream, you might want to look at reinforcement learning methods. There is a Matlab tutorial with an engineer teaching a DNN to detect him making a high five motion based on accelerometer readings. You might be able to find that one online.",hk4lx67,t3_qqxc02,1636583429.0,False
qqxc02,"I suggest first to look at your data, see if you can find the noise at specific frequencies (like the top comment says its usually higher frequencies but not always), if so then a simple filter would do.

If the noise and data sits on the same frequncy your problem change based on your snr and how presice you want to be.

The most important is to know your data well!",hk6icwn,t3_qqxc02,1636618895.0,False
qqxc02,"You are going to want to use a low pass filter before you threshold your ""on/off"" values.

Here is an easy way to do it with scipy. [https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html)",hk9a53h,t3_qqxc02,1636668700.0,False
qqxc02,You could also look into M of N sliding window detection to determine with a good probability that the light should be on. (If the value is greater than the threshold for M out of N ticks) A tick would be some time unit defined by you.,hk9amu8,t1_hk9a53h,1636668906.0,False
qqxc02,"There are a lot of programable senders out there that will do this for you, including raspberry pi devices. You just set the parameters you want and let it do it’s thing. Tweak it as you go until you get what you want. Some of them even support CAN messaging.",hla4fbb,t3_qqxc02,1637344261.0,False
qqvz15,"I had never heard of it; however, looking it up it seems like a legitimate conference. It does not trip any of the red flags for a conference. H indices are ok. Committee sizes are pretty normal. The acceptance rate is a bit high at 30+%. Normally, you should aim for conferences that are closer to 20-25%. But it is not absurd like 70%.",hk2ojp8,t3_qqvz15,1636556235.0,False
qqvz15,Can I please message you personally?,hk2sbvs,t1_hk2ojp8,1636557793.0,True
qqvz15,Sure,hk2twvo,t1_hk2sbvs,1636558438.0,False
qqpf3o,"Document formats aren't a field of research, they're a field of application. There is significant research being done into the component aspects of document storage, display, transmission, translation, etc etc etc, but they won't be published as such because they are useful in many more areas than just documents, and as such it is very unlikely that there is a journal specialising in it (certainly there is no high impact journal, and nothing from ACM or IEEE).
 
Transactions on Graphics would publish things that are relevant to documents in terms of image storage within documents. There are relevant journals for the compsci algorithms to do with text and other data compression (which are not document-specific), there is a growing field of computer history which includes looking at data archival and longevity (including reading old formats of documents), and a sub-field of business anthropology looking at the development of document standards.
 
The application work to bring that research together into some new document standard wouldn't be published in an academic journal, it would be released as a white paper from the company that developed it.",hk1xzzr,t3_qqpf3o,1636541418.0,False
qqpf3o,Thanks for this. I would like to know more about fields that have to do with text compression algorithms.,hk5j52i,t1_hk1xzzr,1636598028.0,True
qqpf3o,"I don't think this kind of thing is being worked on at all. Storage is so bonkers cheap that even if all pdfs were cut it half by a new compression algorithm, almost no one would even notice.
  
Things that are possibly being worked on is getting stuff to load faster, but generally the bottleneck in this situation is going to be the CPU/Memory/Storage interfaces so a new algorithm isn't going to help too much.",hk1or8a,t3_qqpf3o,1636533392.0,False
qqpf3o,"One doesn't usually research file formats, those are very much an application. However, people do regularly research into things that might eventually be used in a new file format if companies ever care enough to implement it.

For example, take this (relatively) young new compression algorithm: [ZStandard](https://en.wikipedia.org/wiki/Zstandard?wprov=sfla1)

Maybe this will one day be used in a document file format, maybe it won't. That's not really something for researchers to decide but for companies building those software and products",hk2byii,t3_qqpf3o,1636550413.0,False
qqpf3o,"Something to keep in mind is that file/document formats are almost never about quality/merits/capabilities of the formats. The format people use is determined by what software they, and the people they are sending documents to, have.  

A good example is JPEG. There have been a few formats that give equal or better quality images, use less space, and use about the same resources to decompress/view. Why haven't those formats pushed JPEGs out of existence? Well everyone's web browsers support JPEG and don't support fancy new format X. While it might seem practical to just say, everyone should just update their browsers' to a never version, and then we can switch. Experience has shown that this just isn't the case. And that's not getting into all the backend and image generation software on the server side, or the firmware on physical cameras, and anything like that. 

Everyday file formats aren't a matter of someone building a better mouse trap. Adobe has a lot of power in the document/image sphere because their software is entrenched there. If you can't get Adobe to make their whole software line support a PDF-successor format, that format is dead in the water. 

Put another way, file formats are a matter of economics not computer science. 

PDF files don't use a single specific compression algorithm by the way. They use a pile of them, specialized for the part of the document they are being used for. Compressing vector graphics, fonts, blocks of text, color images, and greyscale images with different algorithms that are tailored to the specific task. 

Note that while introduced in 1993, PDF was a closed, proprietary format until an open standard for it was established in 2008. The number of versions of PDF is around 10-20 I think, it has been updated/changed many times over the years. The most recent update was in December of 2020. While user visible features have certainly been introduced,fill in forms for example were added in version 1.2 in 1996, most updates have been adding encryption and compression algorithms to the multitude that PDF supports. Keep in mind that 1996 form feature is still vastly underutilized.  Being able to view a PDF, and even more so being able to 'print' anything into a PDF, without installing specific software that the user downloaded and installed (as opposed to came with their operating system) are pretty new things. 

In many ways, you could argue that PDF is more of a container format than anything else. Something that's the case for a lot of file formats nowadays. All audio/video file formats and even zip files fall into this category. The standards define a standard way of holding different hunks of data and metadata, with different formats being used by the individual hunks. A new compression algorithm can be added to the PDF standard without throwing away the format or backwards compatibility.

I am curious if there is anything about PDF you find lacking. Of course, file using less space is nice, but that applies to all files. I'm sure that most people, even in CS, don't know the majority of the features that PDF even support. I'd definitely include myself in that category.",hk2m4d7,t3_qqpf3o,1636555195.0,False
qqpf3o,"This is a great and informative post, even though it doesn’t directly address the question.

Please enjoy my Reddit-welfare award.",hk3z1q6,t1_hk2m4d7,1636574448.0,False
qqpf3o,"It wasn't that I found PDF lacking in features. Wavelets are a thing in applied math, and Daubechies wavelets are used in JPEG. This intrigued me, because it's a great example of using something from research to implement a very practical thing related to pictures. Since I deal with PDF documents on a regular basis, and really like the quality of infinite zoomability of crisp documents that use vector graphics, I was wondering if there is a similar application of research level applied math, or algorithms, not for images as in JPEG but for documents. I realise my question was naive, but all the replies (including yours) have been very helpful in clearing up misconceptions. Your comment about PDF requiring different algorithms for different features was particularly helpful; I was under the impression that for a PDF compiled from LaTeX only vector graphics is involved, I was not aware that fonts are handled differently.",hk5lv1m,t1_hk2m4d7,1636599247.0,True
qqpf3o,"Something to keep in that is that decompression/zooming stuff has to be fast enough to use in vaguely real time because that's what people expect with documents.  Fonts and vector graphics can be scale indefinitely, but raster images you really can't do anything perfect with them. 

There a lot of interesting projects using AI/ML to fill in the missing information on pictures. Some could be used to generate high-resolution images so you can zoom more.",hk826so,t1_hk5lv1m,1636650861.0,False
qqpf3o,"> Is there any chance new discoveries are made (e.g. more efficient algorithms to compress documents)

https://en.wikipedia.org/wiki/Zstandard

> Zstandard (or zstd) is a lossless data compression algorithm developed by Yann Collet at Facebook. Zstd is the reference implementation in C.

You mean like this?",hk1xa1m,t3_qqpf3o,1636540828.0,False
qqpf3o,"I have used this before, it is great when you have a lot of similar files. I used it for a very large web crawler storage, where there was lots of repeated data. Although rebuilding a new dictionary was annoying (and heaven forbid you aren't backing it up safely), I could simply make a dictionary file and the average response was like 4kb instead of 60. I actually used it on pickled python response objects, but same idea. AFAIK Facebook uses it to store people's chat histories, which often use similar phrases and speech patterns. It's also high performance. It's great for that kind of stuff

I will say, though (edit: and this is reiterating other comments) formats like PDF are rare. It's not about whether there's a new better standard, it's about adoption. PDF and DOCX were shoved into widespread adoption and that's why they are standard, because they were pushed by Adobe and Microsoft. A better format that nobody uses is worse than a bad format that everyone uses. It takes a long time (or a killer feature) for there to be any significant migration.",hk3samw,t1_hk1xa1m,1636571825.0,False
qqpf3o,"There are/have been working groups and standards bodies working on document formats. On Computerphile, Prof. Brailsford has presented many videos on his experience working in the field.",hk2elq9,t3_qqpf3o,1636551731.0,False
qqpf3o,"I agree with what has been said about this being not a tremendous issue given modern storage capabilities.

If you are interested in compression generally, I would recommend investigating signal processing and digital signal processing, probability theory, and information/coding theory (especially coding theory); typically, the actual format of these files is just some compression mechanism (typically one or more lossy encodings, followed by an entropy encoding) dressed up in fancy clothes.",hk1qii7,t3_qqpf3o,1636534915.0,False
qqpf3o,"I feel like the “cutting edge” research is mostly in other areas: compression, verification, performance, security, etc.",hk26r4s,t3_qqpf3o,1636547532.0,False
qqpf3o,"This would be dealing with Encoding/Decoding/Transcoding.

You would wanna focus on studies of Algorithms, Cryptography, Computer Graphics, and Programing Language Theory.

Reasons why? The goal. Making a super detailed, small memory required document format that is also computationally less intense. So the fields above would give you the insight to facilitate that.

Good luck!",hk3oo4j,t3_qqpf3o,1636570419.0,False
qqpf3o,"http://www.cs.nott.ac.uk/~psadb1/

http://www.eprg.org/research/",hk2nuqr,t3_qqpf3o,1636555939.0,False
qqpf3o,Did you by chance recently ask if open source software is discouraging innovation citing the numerous frustrations associated with PDFs?,hk2o51e,t3_qqpf3o,1636556061.0,False
qqpf3o,"As far as file compression, document formats, pdf's, etc... go, the technology used today has existed for quite a while and I don't think there's a huge amount of research going into it.  However, we are entering the age of Cloud Computing and there is definitely a lot of new technology being developed around streaming compression, distributed documents, document security/e-signatures, and that sort of stuff.",hk38nat,t3_qqpf3o,1636564236.0,False
qpu9g8,"""Code"" by Charles Petzold is what you're looking for.",hjw4wbz,t3_qpu9g8,1636428588.0,False
qpu9g8,"thanks, I'll look into it",hjw6uxv,t1_hjw4wbz,1636429500.0,True
qpu9g8,"Great book, reading it before starting my studies really helped me grasp some early concepts.",hjwwb08,t1_hjw4wbz,1636445520.0,False
qpu9g8,"Seconded, fantastic book and explains exactly what you want to know plus a whole lot more.",hk05xl1,t1_hjw4wbz,1636502860.0,False
qpu9g8,"That's maybe a really weird way to learn it, but you can try building a Minecraft CPU. I tried it, and from that point on I really realized how CPUs work, what you need the individual parts for, etc. For example, I never really understood what the control unit is used for. (Well I thought I understood it, but I didn't) That was, until I had built my ALU inside Minecraft, and realized ""cool, I have a ALU that can add & multiply things, but how do I set the correct enable bits for the corresponding instruction? Multiplying takes a lot longer than adding, how do I wait longer before executing the next instruction? How do I even get to the next instruction?"". That's what the control unit is for.

All the questions you didn't even think of will answer themselves when you try to build your own CPU.",hjwxsxz,t3_qpu9g8,1636446817.0,False
qpu9g8,"Lol, that's how I've learned how logic circuits work - either by building them from real components or building them in Minecraft. It was kind of eye opening.",hjz7mzn,t1_hjwxsxz,1636488674.0,False
qpu9g8,Wait what’s a minecraft cpu,hjxisli,t1_hjwxsxz,1636463012.0,False
qpu9g8,"I haven't played Minecraft in quite a few years but I think the game is pretty advanced in terms of redstone and all the things you can build with it? 

[Here's something I found on it.](https://minecraft.fandom.com/wiki/Tutorials/Redstone_computers) (I only briefly glanced at it so I'm not sure if it's what we're looking for here)",hjxmmj3,t1_hjxisli,1636465069.0,False
qpu9g8,ok I've never played minecraft before but now I really want to. That is insane,hjxqb9v,t1_hjxmmj3,1636466904.0,False
qpu9g8,"Here's my [stock answer](https://www.reddit.com/r/C_Programming/comments/qpa6pp/i_am_a_curious_boy_having_lots_of_interest_in/hjt2xkg/) for this:

If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest)
3. If you have the time and energy, do https://www.nand2tetris.org/

This will let you understand *what* a computer is and how a CPU works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how a compiler works etc. Arguably it will also give you the tools to design all of that, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's computer on plastic prototyping board"")",hjx874b,t3_qpu9g8,1636455982.0,False
qpu9g8,"I'm a Senior CS student and even though I took Computer Organization, I'm constantly thinking, ""I get how this works, but HOW does it work a level deeper?"" 

For this reason, thank you for one of the most useful posts I've ever come across.",hjxn339,t1_hjx874b,1636465307.0,False
qpu9g8,"Did you university not have a course on digital logic?


(Although, thinking about my own course years ago, it's possible only the basic gate stuff was mandatory, and the RTL-level stuff where you mush gates together into a computer might have been optional / for computer engineering students)",hjxoxao,t1_hjxn339,1636466226.0,False
qpu9g8,"Unfortunately not, at least not that I'm aware of. 

We have a course on ""the fundamental topics of modern computing systems"" which focuses heavily on assembly, but not the deep-level of why it works, just how to get it to work.

We also have another course on assembly that acts as an introduction to CPU architecture (pointers, logic, etc) but that doesn't go in depth on a hardware level. 

Either way, there's *something* missing, it could definitely be that I'm not an engineering student, but some of your links satisfy that itch I've always had.",hjxvs4c,t1_hjxoxao,1636469418.0,False
qpu9g8,"You should send your University an nasty letter calling them poopy heads.

Digital logic is something that should be touched on in every degree, I think. If only because the basic gate stuff is fun!

Instead you probably had to endure some crap about databases that you could probably have guessed.",hjy25b4,t1_hjxvs4c,1636472145.0,False
qpu9g8,[deleted],hjyb3qp,t1_hjxn339,1636475784.0,False
qpu9g8,"> My boss just says, don't question how it works. As long as it does.

I think this is bad advice. In my experience the people who question how it works will write better code, and they'll be much more useful in a situation of when stuff goes wrong, because when stuff goes wrong abstractions break down, and if you have no knowledge of those lower foundations you've had your legs swept from underneath you.",hk1yzql,t1_hjyb3qp,1636542205.0,False
qpu9g8,"That is exactly my problem. Let me explain more - I haven't had a chance to review all the great links everyone posted, so maybe they already explain what's missing in my head.

=======

CPUs are made of billions of transistors, assembled in logic gates, assembled in more complicated circuits like the Adder, ALU, etc.

I'm sitting in front of that computer and I write a very basic program, compile it and for the sake of argument, I store it in RAM.

'storing' something in RAM means that at the lowest levels, a transistor/capacitor pair gets activated and one capacitor gets filled with current storing 1 bit (I'm guessing now ?). 

so reading something from memory must mean that an electric current arrives in the ram circuitry that says: give me the status of all capacitors in row 10

Then how would the status (full/empty) of capacitors be communicated back to the CPU and what does the CPU do with that info ? Does the CPU in turn also have transistor/capacitor pairs that get activated in order to 'work' with this information ? 

========

This is the kind of explanation I'm looking for because otherwise I've seen plenty of documents that discuss address space, and lookup functions, adding functions but they never explain how \*those\* work.",hjy1hu4,t1_hjxn339,1636471873.0,True
qpu9g8,"> Then how would the status (full/empty) of capacitors be communicated back to the CPU and what does the CPU do with that info ? Does the CPU in turn also have transistor/capacitor pairs that get activated in order to 'work' with this information ? 

The address and data buses are a vital part of a CPU <-> RAM interface. Check out the resources for more :) They'll tell you how such a bus is implemented and how RAM module multiplexors are usually implemented.",hk1yuju,t1_hjy1hu4,1636542091.0,False
qpu9g8,Commenting to save for later,hk0dz03,t1_hjx874b,1636506449.0,False
qpu9g8,Look into https://en.wikipedia.org/wiki/Von_Neumann_architecture,hjwaqms,t3_qpu9g8,1636431342.0,False
qpu9g8,Do you know about logic gates? Start there.,hjw4cbo,t3_qpu9g8,1636428330.0,False
qpu9g8,"I do, for example I know about adder circuits - [https://en.wikipedia.org/wiki/Adder\_(electronics)](https://en.wikipedia.org/wiki/Adder_(electronics))

and coming from the other end I also know about microcode.

So starting with assembly code, say someone writes:

add eax,1

then that gets compiled to binary

when you execute that binary, I would like to know exactly what happens from start to finish. Who interprets the binary code, how does the adder circuitry get involved, who ""puts it in motion"" . It's black magic to me that a string of 1s and 0s can have physical consequences inside a CPU.

I realize this is pretty low level stuff but I never understood it and I'd like to know more about it",hjw7kgh,t1_hjw4cbo,1636429833.0,True
qpu9g8,"Others have linked tons of helpful documentation, but for me what really clicked was watching Ben Eater build his own CPU. Seeing the bus, registers, and gates physically laid out helped it all make sense, and he explains every detail.

https://youtu.be/dXdoim96v5A",hjwfq9e,t1_hjw7kgh,1636433890.0,False
qpu9g8,That’s awesome! This dude is genius . I don’t remember when was the last time to enjoy someone explaining stuff in an interesting way and to take my attention 100%. Thank you for sharing !,hjxqe4y,t1_hjwfq9e,1636466944.0,False
qpu9g8,Yea this was what I was going to suggest.  Even having it on stand by is good if it doesn't make sense now. His videos got more and more cooler the more I learned.,hjxvj68,t1_hjwfq9e,1636469309.0,False
qpu9g8,Dude I’m so glad someone mentioned him. Discovered him a couple weeks back. He’s amazing,hjxivfo,t1_hjwfq9e,1636463056.0,False
qpu9g8,This is a super complicated topic and someone typing it all out is a big ask - this is documented extensively online though!,hjwamue,t1_hjw7kgh,1636431291.0,False
qpu9g8,"Check out [https://www.nand2tetris.org/](https://www.nand2tetris.org/) the go from logic gates to cpu to writing assembly and finally create tetris.

Havn't done the whole course yet myself but if you want to explore it further it is worth a look",hjx04cn,t3_qpu9g8,1636448901.0,False
qpu9g8,"I read this book 2 years ago and I think it answers *exactly* what OP is asking. I didn't follow any course, just the actual book.

Seeing how an adder works revealed the mystery of how circuitry can actually do math.

Then seeing how the whole circuit starts off in chaos where any parts (not good with terminology) could be on/off but once electricity has flowed through the whole circuitry it reaches an equilibrium.

And how certain parts (registers) are protected from that chaos and are updated only after that equilibrium state is reached.

>Who translates 'mov' into what it actually does and who knows how to interpret that command ? 

An assembler does this. It does what you think, it sees mov and there is a set binary sequence which corresponds to that. It just sticks that down as the next bytes of the binary file.

The CPU never sees the letters m/o/v, it is fed the first 8 bytes of the binary file. Those bytes are just put into a CPU register. The bits in those bytes set up the initial conditions of the circuitry which will come to some equilibrium. Then it automatically puts the next 8 bytes of that file into the same register (unless there was a jmp command or an interrupt etc.).

For example, the 10th bit might be connected up to a AND gate, so if that bit is a 0, the output of the AND gate will be off. That's just an example of what I mean by initial conditions affecting the circuitry.

I started out giving a very general answer and went into some details so it's a bit inconsistent but I hope I've advocated for the book well enough.",hjzi8sg,t1_hjx04cn,1636492876.0,False
qpu9g8,Youtuber ben eater [made his own 8 bit computer](https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU) from logic gates and then shows how he programs it. It really helped me understand it all,hjxchd8,t3_qpu9g8,1636459159.0,False
qpu9g8,"Somewhat of a simplified answer, but I think this conveys the jist of what you’re asking for.

So first understand what a [Boolean function](https://en.m.wikipedia.org/wiki/Boolean_function) is. Every function of a processor is built as one of those, the mov, ldr, str, etc. will be a function of ones and zeros being input, and translates to a certain series of ones and zeros output. You can build any possible Boolean function you desire with the correct series of transistors like this.

Next you need to understand what a [Multiplexer/Demultiplexer](https://en.m.wikipedia.org/wiki/Multiplexer) is. Essentially, it’s like a type of Boolean function where you give it an address of ones and zeros and it will pick from either a corresponding input or output bus. This is how you “select” things in a computer.

Assembly code correlates almost one-to-one with the machine instructions that gets put into a processor. Each machine instruction is a series of ones and zeros (most commonly 32 or 64 bits, this depends on the specifications of the processor). A certain number of those bits represents the “op code,” which would be your mov, ldr, add, etc. and that specific series of bits would go into one multiplexer to “select” the function that is being performed.

Each of those functions requires other information (ie add needs two values to add together and a register to store them, mov needs a source and a destination register, etc.) which would be passed as arguments. Other bits in the instruction represent those arguments, which would be either your registers or immediate values in your assembly code. Each of those strings of inputs go into a multiplexer that selects the appropriate value.

The largest section of the machine instruction is usually at the end which is the offset. This is needed when accessing data from large memory stores because your memory addresses point to large sections of data, and when you need a specific piece of the data you would specify an offset.

Your assembly then gets translated into these instructions.

Look into Von Neuman architecture and this might put some more of this in context. I hope my explanation at least answers more questions than it creates.",hjxjzeh,t3_qpu9g8,1636463673.0,False
qpu9g8,"Desktop version of /u/JDHuff185's links:

 * <https://en.wikipedia.org/wiki/Boolean_function>

 * <https://en.wikipedia.org/wiki/Multiplexer>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hjxk0ls,t1_hjxjzeh,1636463692.0,False
qpu9g8,"It's a vastly simpler example then a modern CPU, but I found it really helped my understanding - https://youtu.be/dXdoim96v5A

Ben Eater's 8-bit CPU defines an 11-bit ""control word"" which is hand-crafted by the chip designer and stored in ROM for each instruction (and each step of each instruction; they have 5). The control word has a number of bits that each correspond to a transistor that enables/disables a piece of functionality in the CPU .

For example, one step of a Load Memory Into Register instruction have an enabled control word bit that tells the Memory module to ""output"" its current value to the bus, and the Register module has its ""input"" bit enabled to read from the bus.

The CPU works its way through a series of control words to move data around in the CPU, or toggle functionality (for example one of the control word bits switches the arithmetic unit between Addition and Subtraction modes).",hjws2z6,t3_qpu9g8,1636442070.0,False
qpu9g8,"I don't know why this bothers me so much but it isn't ""ben eater's 8 bit cpu"", that video series is covering SAP-1, designed by paul malvino for teaching students digital electronics.",hjwta3d,t1_hjws2z6,1636443025.0,False
qpu9g8,"Yeah, though it's not precisely SAP-1; there are changes, improvements and optimisations he's made, as well as his own clock module, designing the layout etc. There's a lot he's added that I think qualifies it as his CPU, in the same way that Apple's M1 is based on ARM's design",hjwtgdz,t1_hjwta3d,1636443168.0,False
qpu9g8,"Those things were in SAP2 through SAP3... No. It's all in the same reference book, written by Malvino. And people are expected to make changes while building that's what it's for, it's an architecture used to teach.",hjwtiwu,t1_hjwtgdz,1636443223.0,False
qpu9g8,"CPUs aka microprocessors are just made for processing data. They are interfaced with different interfacing IC which does rest of the actual work, like moving data, sending inputs from peripheral devices and sending output to the said peripheral devices. 
For example, you have DMA or Direct Memory Access which allows hardware or peripheral to access the main memory without the need for microprocessor to do anything. 
Microprocessor mainly performs logical and control operations. Like picking data, processing it (doing logical or arithmetical operations like >,<,+,-,/ etc.)
The way they work largely depends on what kind of architecture is being used. Different microprocessors work differently. 

A great way to learn this is to learn how the older processors worked. An 8-bit one would be perfect in my opinion. Something like 8051, 8085 or 8086 are still taught in many places to teach the basics of microprocessors.",hjxjdc0,t3_qpu9g8,1636463332.0,False
qpu9g8,"1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",hjxu3yg,t3_qpu9g8,1636468676.0,False
qpu9g8,"https://youtube.com/playlist?list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye
Try crash course computer science.",hjzwuty,t3_qpu9g8,1636498862.0,False
qpu9g8,"I don't understand why people are so eager to recommend books when you're looking for a relatively simple answer. Start the other way around, watch easily consumable medium and then dive into books for detailed information. 

**I recommend** [**this video by Computerphile**](https://www.youtube.com/watch?v=IAkj32VPcUE) **which covers exactly what you've asked** (and a bit more) although on a somewhat high abstraction level. For further reading, what you're looking for is Fetch-Decode-Execute cycle.",hjwx0rk,t3_qpu9g8,1636446127.0,False
qpu9g8,"Probably because books are better than videos, especially in terms of information density. And OP is after a topic that is particularly complicated and requires a lot of information.

I would say watching that 11 minute video isn't going to satisfy OP's questions, whereas reading Code will, and it'll also give them a huge amount of other relevant information they never knew to ask for.",hjx8cgy,t1_hjwx0rk,1636456103.0,False
qpu9g8,"There is a crash course on YouTube on computer science. It explains from ground up all you need to know. No technical knowledge required, you can watch it in your leisure time. https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo",hjx0wpu,t3_qpu9g8,1636449616.0,False
qpu9g8,"There is a module on the CPU called instruction decoder. There is a very simple 4 bits CPU simulation that helps you understand it. ""simulator.io | Sample - 4-Bit CPU"" https://simulator.io/board/AWZpw7Fy3I/2",hjxkkus,t3_qpu9g8,1636463995.0,False
qpu9g8,"If you want some in-depth knowledge on how a CPU works and how you build it up from logic gates, I can recommend ""Digital Design and Computer Architecture"" by Harris & Harris.",hjxtlvk,t3_qpu9g8,1636468451.0,False
qpu9g8,"This is quite a large topic area and other users have given good resources to start with. for some footnotes essentially your general purpose cpu is made of a few components such as an arithmetic logic unit (ALU), registers (the fastest form of memory), and a control unit (which controls how these pieces communicate. each flop is controlled by your computers clock so each cycle of electricity will cycle through the cpus network of transistors to perform a task.

the ALU as it's name suggests can perform simple arithmetic operations such as addition and subtraction. From these two opperations we can build more complex operations such as multiplication, division, modulo. etc (see ripple adder for a basic circuit bit number addition).

 small segments of data can be saved in the registers which hold data that should be frequently accessed with less frequently accessed data going to ram then hard-drives.


going down a level these components are made of circuits such as a JK flip-flop which can store a single bit of data, D or data flip-flops, timers, and comparators. 

these in turn are made of the low level logic gates we all learned about at somepoint such as your OR's, AND''s, XOR's, etc. 

finally you get down to the transistor a simple off and on switch where several are used in the correct configuration to create the gates.


beyond this you get into theoretical EE territory which is not my wheelhouse. hope this helps and if I made any mistakes let me know as I am a bit rusty on the topic myself.",hjyclic,t3_qpu9g8,1636476376.0,False
qpu9g8,"A specific answer to your ""Who translates 'mov'"" is that all CPUs have a specific set of instructions. This is called an instruction set. Different CPUs may have different instruction sets. For example, Intel and ARM CPUs do not have the same instruction set. Therefore a compiled Assembly program may run on an Intel CPU but not on an ARM CPU. Or it may run, but yield different outputs. The instruction sets are basically a giant table of Hexadecimal numbers which correspond to a specific instruction.

But if you want to create an instruction set, you have to program the gate logic for the instructions. Gate logic is basically a series of cycles where each cycle has a set of operations for which gates and busses to open/close. Some instructions take more cycles than others to be completed depending on the CPU architecture and instruction set which is why we have concepts like ""Branchless programming"". Branchless programming is basically writing if/else logic differently to avoid switching branches in the compiled machine code since switching branches can be a heavy instruction and takes more cycles.

A good but small difference between ARM and Intel instruction sets is that an 8-bit integer is signed differently. You know the difference between unsigned and signed integers in programming. Say you have a C program where you declare a variable c:

    char c;

On an Intel CPU or most x64 CPUs, this basically compiles or is equivalent to:

    signed char c;

Since an integer is evaluated as a signed integer by default. But on ARM CPUs this compiles to:

    unsigned char c;

So you have to take that into account when you program for ARM CPUs. For example, if you compile this code below to an ARM instruction set. Based on what I mentioned earlier, what is the problem?

    char abs(char c) {
        return c < 0 ? c * -1 : c;
    }",hjzjih2,t3_qpu9g8,1636493383.0,False
qpu9g8,"[http://www.buthowdoitknow.com/](http://www.buthowdoitknow.com/)  
This book explains CPU in very easy to follow way

[https://www.youtube.com/playlist?list=PLuiLMR-Dpj-3s72aqvmKC5Ik\_d6GB6KOf](https://www.youtube.com/playlist?list=PLuiLMR-Dpj-3s72aqvmKC5Ik_d6GB6KOf)  
This playlist explains concepts and logics you need for building computer in minecraft but they apply to real life too

[https://www.youtube.com/c/BenEater](https://www.youtube.com/c/BenEater)This channel got a lot of videos for low level concepts and hardware",hjx287p,t3_qpu9g8,1636450791.0,False
qpu9g8,"[https://ict.iitk.ac.in/wp-content/uploads/CS422-Computer-Architecture-ComputerOrganizationAndDesign5thEdition2014.pdf](https://ict.iitk.ac.in/wp-content/uploads/CS422-Computer-Architecture-ComputerOrganizationAndDesign5thEdition2014.pdf)

read this textbook",hjykt1i,t3_qpu9g8,1636479615.0,False
qpu9g8,especially chapter 4,hjyl08v,t1_hjykt1i,1636479692.0,False
qpu9g8,"Mods, we get this question like EVERY day. Can we please add something to the sidebar/wiki?",hjypm88,t3_qpu9g8,1636481509.0,False
qpu9g8,Sometimes they channel voltage and sometimes they don't. Duh.,hjyzjvo,t3_qpu9g8,1636485453.0,False
qpu9g8,There are lots of useful links. You pretty much need to start with smaller to larger blocks. transistors -> logical gates -> ALU -> registers -> clock. A modern day CPU is an insane piece of engineering and close to magic that it even works.,hjznc10,t3_qpu9g8,1636494912.0,False
qpu9g8,Here you go https://youtu.be/sK-49uz3lGg,hjzrh16,t3_qpu9g8,1636496614.0,False
qpu9g8,"Maybe a bit more high level than you want, but after you done with other links posted here check out
https://www.nand2tetris.org/

It deals with this exact question -- how you go from transistors to operating system.",hk01osf,t3_qpu9g8,1636500969.0,False
qpu9g8,"Comment for personal archive* this is an amazing thread, also pulling in minecraft, beautifully done!",hk01y8y,t3_qpu9g8,1636501086.0,False
qpr8lq,r/netsec r/cybersecurity,hjw0wvt,t3_qpr8lq,1636426763.0,False
qpl4wt,All of them!,hjuovnq,t3_qpl4wt,1636405832.0,False
qpl4wt,This. Imagine doing statistics without a computer.,hjvgcnd,t1_hjuovnq,1636417444.0,False
qpl4wt,you can have computers without computational science though,hjwj59i,t1_hjvgcnd,1636435896.0,False
qpl4wt,"Well, maybe. But who are the people making those algorithms go brrr (sorry)? What people are responsible for making it feasible to program them, for making it secure etc",hjx71tv,t1_hjwj59i,1636455027.0,False
qpl4wt,"The human genome project was only possible with computer science. Sequencing using accurate machines was going too slow and not really working right. Then someone had the idea to use cheap and high throughout machines even though they made tons of mistakes. The trick was to read each genome lots of times on these fast machines and use computers with statistical algorithms to piece it back together and repair errors. This is now the standard way to sequence genomes in general (at least when doing the whole genome rather than looking for specific markers). Genomics is very computational in nature and has had a direct impact on medicine and agriculture (and tons of other stuff).

The reality is that I could probably do this for any scientific or engineering field. Airplane companies only do wind tunnel stuff at the very end because of computers. Drugs are analyzed and candidates chosen using computers. Materials are investigated with simulations and machine learning and such. Astronomy uses massive computations to answer questions and make sense of the massive amounts of data we collect. There are projects to map neurons in brains using computer vision techniques. All of these involve algorithms and math, but they also rely on advances in computer hardware, networks, languages, and systems. Ultimately we do what we do so that other people can use computers for what matters (hopefully).",hjvmmrr,t3_qpl4wt,1636420341.0,False
qpl4wt,"Pretty much every field needs robust solvers for systems of linear equations, least squares problems, eigenvalue problems, and singular value problems. That's computational science (not computer science) in a nutshell.",hjvql2k,t3_qpl4wt,1636422140.0,False
qpl4wt,The entire field of bioinformatics is a good example,hjuqis6,t3_qpl4wt,1636406484.0,False
qpl4wt,"Was just about to suggest this. Only thing is that you need a PhD and some research experience, and additional knowledge of biology.",hjvn2o8,t1_hjuqis6,1636420546.0,False
qpl4wt,"I did some computational research as a chemist when I was in my undergrad.  Not much was accomplished by myself.  However, the school I graduated from literally has a super computer to do quantum calculations for chemistry and biology, including a whole department that does research just for computation.

EDIT: I imagine physics and many others use it a lot too for other stuff, but during my time there I only saw biology and chemistry stuff.",hjvohqv,t3_qpl4wt,1636421191.0,False
qpl4wt,"I have an entire course named computational physics, which is all about using computer programs to find solutions and different scenarios to physical equations.",hjw3zn2,t3_qpl4wt,1636428171.0,False
qpl4wt,I'm pretty sure every laboratory uses software to analyze and store data,hjvs1eo,t3_qpl4wt,1636422792.0,False
qpl4wt,The proof for the 4-color theorem was the first one to be done with help from a computer iirc if you're looking for specific examples,hjupyc8,t3_qpl4wt,1636406262.0,False
qpl4wt,It was not the first one to be done with help from a computer - it was the first proof where the calculations involved in the proof essentially *had* to be done on a computer. Computers were assisting in research and proofs long before that.,hjvhu12,t1_hjupyc8,1636418122.0,False
qpjwrs,"> I don't see why we don't just use a [NULL] character at the end of every string if it's so much more efficient than the alternative.

It's not ""so much more efficient."" In many cases it is _far less efficient_, for example, when you need to determine the length of the string. If the string is stored along with its length, determining the length is instantaneous. But if it's stored with a null terminator, you need to count every character.

Storing the length also makes it possible to reference substrings from anywhere within a larger string. With null-terminated strings, you can only reference a substring which goes until the end. For example if the string is ""HELLO WORLD"", in C (a null-terminated string language) you could refer to the substring ""WORLD"", by adding 6 to the start address, but you could not refer to the substring ""HELLO"" without modifying the data to replace the space with a null character. In Rust (another systems language, with length-based strings), you can split the string into ""HELLO"" and ""WORLD"", without modifying the ""HELLO WORLD"" string data.

Null-terminated strings are only more efficient by saving a few bytes per string, which rarely matters anymore, because memory capacities have increased from kilobytes to gigabytes.",hjuba6o,t3_qpjwrs,1636400370.0,False
qpjwrs,"Why not go with both then? If the real struggle these days is in efficiency over storage, why not store the length and a null character, and the compiler could just use whatever is most effective for a specific operation? Or is that overthinking it a little too much?",hjvndnd,t1_hjuba6o,1636420684.0,False
qpjwrs,"That does happen, for interoperability's sake. It doesn't save any storage — it uses slightly more, because you store the length _and_ an extra null character. The C++ specification requires a `string` to store its length, but most implementations also add a null terminator, making it easier to send the string to C functions.",hjvyc4f,t1_hjvndnd,1636425596.0,False
qpjwrs,"Ah okay, that last part was interesting, I guess I worded it wrong because I already understood that first part, but it's interesting that it exists like that",hjw9ikp,t1_hjvyc4f,1636430756.0,False
qpjwrs,"It's more memory efficient, but computationally less efficient.",hju8sc6,t3_qpjwrs,1636399374.0,False
qp5vod,"Yeah, that's pretty dense. But if you go one sentence at a time and really think through each of the definitions, you can make sense of it. Reading math literature (which this essentially is) is not like reading a novel. You frequently have to go to prior sentences to remind yourself of what something was defined as being.

What Knuth is actually saying though isn't too complex (though he certainly makes it look like it).

In the first two sentences, he's remarking on the fact that the algorithm defined previously (I'm looking in my copy of Volume One of *The Art of Computer Programming*, though mine is the second edition from 1973) as quadruple (*Q*, *I*, Omega, *f*) is not sufficiently restrictive, as it isn't necessarily *effective* (previously defined as being computable by a pen and paper in finite time).

He then goes on to provide a restriction of the quadruple such that the algorithm *must be* effective. He then remarks about how the restriction he provides could be done in many different ways, specifically noting Turing's notion of effectiveness (the most famous and earliest such example), and Markov's notion given in *The Theory of Algorithms*.

How what he wrote corresponds to what I just described can be tricky to figure out, but if you want to read *The Art of Programming* I suggest you try to get used to it.",hjrpbuu,t3_qp5vod,1636347120.0,False
qp5vod,Thanks so much for the reply. I guess I’ll have to dig into this more and try to diagram it out for myself a bit. I appreciate you taking the time!,hjrqs13,t1_hjrpbuu,1636348033.0,True
qp5vod,Yeah sure thing! And definitely have a piece of paper next to you when reading this kind of stuff. You can only keep so much in your head at once.,hjrrgzp,t1_hjrqs13,1636348484.0,False
qp5vod,"I've found that it is much easier to keep a lot more in my head when it is something I'm writing, speaking, or coding than it is when reading something that someone else wrote. One of the real quirks of cognition right there, as the difference is massive. One of the things that makes something like that harder to hold in the mind (for the reader, as I have done this plenty as a writer and felt no shame about it until a few weeks later when trying to read what I'd written) is the use of single letter variables and Greek letters rather than words. There are likely a great deal of programs or examples of pseudocode that are easier to read than that, despite being potentially more complex or lengthier. 

That said, the last time I took an algorithms class I did get pretty good at turning math-speak into pseudocode and found the process very rewarding once I got the hang of it. Once you've done it a few times, Wikipedia is your oyster and the sky is the limit for implementing random stuff based on vague or highly complex descriptions in the language of math. 

For someone looking to get better at it, who enjoys math, logic, and coding but lacks a higher math education, would you say that ""The Art of Computer Programming"" is a good and useful read in the 21st century for someone looking to get good enough to contribute to the field in a serious way?",hjru72d,t1_hjrrgzp,1636350293.0,False
qp5vod,"Totally agreed. I often read something I had written a few weeks ago and become totally confused. But if I write down something someone else wrote I immediately understand it better. And I'm at the point where some Greek letters (alpha, beta, gamma, delta, epsilon, theta, omega) are easy for me to read, but most of them introduce cognitive load that makes understanding more difficult. Which is weird given that they're all just arbitrary symbols.

But I'm not nearly at the point where I can comment on contributing to the field. And I read *The Art of Computer Programming* more out of historical interest than a desire to learn computer science. I can say though that it's definitely not good as an introduction to computer science. Knuth was a younger man when he first wrote them, and younger men almost always try to look smart. If you're not already knowledgeable in the subject much of what he wrote will go over your head (and by design). Doubly so if you lack a math background. What you want is an author who's trying to seem less intelligent than they actually are (Brian Kernighan is a great example, though he hasn't really written about computer science, just computers). For that though I don't have many good recommendations, though one book I always recommend to people trying to learn more about computers is *Code*, by Charles Petzold.",hjrxtdk,t1_hjru72d,1636352887.0,False
qp5vod,"Many thanks! I saw an interview with Kernighan once and was shocked at his easy and humble demeanor. Really chill dude. I admit to wanting to give Knuth a read just to see if I can, though. It's always fun to try and climb those kinds of mountains.",hjs2sfu,t1_hjrxtdk,1636356748.0,False
qp5vod,"I know, Kernighan is great, and especially for his age! He talks with the energy of a twenty year old, but with the humility and wisdom of someone, well, his age. I hope I can be as sharp as him when I get up there in years.

And I totally relate. It's kind of a sense of pride when you read ""the canon"", the works of The Greats. I'd say go for it if you want to, just don't feel the need to do so right away, and come well prepared.",hjs3ca6,t1_hjs2sfu,1636357204.0,False
qp5vod,"Yeah, I sometimes just scribble something when reading, visualizing the definitions. That can help.

Thanks also for the explanation.",hjsf1oa,t1_hjrpbuu,1636367723.0,False
qp5vod,"I am a little rusty on the details but I believe he is describing a version of Churchs lambda calculus, one of the earliest ways to define an abstract notion of what a human can compute: https://en.m.wikipedia.org/wiki/Lambda_calculus

Turing came up with an equivalent version that most (including Church) think is more natural and elegant: https://en.m.wikipedia.org/wiki/Turing_machine",hjt3n8y,t3_qp5vod,1636382575.0,False
qp5vod,"**[Lambda calculus](https://en.m.wikipedia.org/wiki/Lambda_calculus)** 
 
 >Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It is a universal model of computation that can be used to simulate any Turing machine. It was introduced by the mathematician Alonzo Church in the 1930s as part of his research into the foundations of mathematics. Lambda calculus consists of constructing lambda terms and performing reduction operations on them.
 
**[Turing machine](https://en.m.wikipedia.org/wiki/Turing_machine)** 
 
 >A Turing machine is a mathematical model of computation that defines an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the model's simplicity, given any computer algorithm, a Turing machine capable of simulating that algorithm's logic can be constructed. The machine operates on an infinite memory tape divided into discrete ""cells"". The machine positions its ""head"" over a cell and ""reads"" or ""scans"" the symbol there.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjt3ojb,t1_hjt3n8y,1636382592.0,False
qp5vod,"Thank you so much for taking the time to link those pages for me! I appreciate the response.  You’re absolutely correct as well, the next portion mentions these very briefly.",hjuh2zs,t1_hjt3ojb,1636402715.0,True
qp5vod,"Shouldn’t it be obvious?

The f theta omega lambda = theta N, with respect to x1 … xj such that the compute sequence is within Q",hjtvxiw,t3_qp5vod,1636394164.0,False
qp5vod,Not helpful but thanks for taking some time anyway,hjuh4da,t1_hjtvxiw,1636402730.0,True
qp5vod,"FYI this was just a joke comment at how complicated it looks. 

I was mostly just trying to say random variables and pretend I knew what it was saying.",hjv58oq,t1_hjuh4da,1636412506.0,False
qp5vod,Oh sorry haha! I’m used to seeing the /s on Reddit!,hjvzklv,t1_hjv58oq,1636426150.0,True
qp5vod,"Just in case you are still thinking about this. This website does a wonderful job of explaining what's going on with Knuth's definition of an algorithm and compares it to a Markov Algorithm.
https://www.rudikershaw.com/articles/computationalmethod3",hk8fohv,t3_qp5vod,1636656177.0,False
qoyr6v,"i've worked with embedded systems and board bring-up (baremetal or w/ u-boot) is what i do ... reading RAM specs, and programming the memory controller.

1 RAM has many interface, mix and match is a PITA. physical interface. access protocols. then ranks, and banks. (*i shudder as i remember a project, F that sheyt*)

2 you need a memory controller specific to the RAM modules. most controllers i've used are built-in to an ASIC

3 you need to think about battery backup and good power smoothing

4 RAM is very fast, USB is slow. you are at the mercy of USB throughput speeds.

> the fact that this software exist is proof that a ram disk is possible

remember (*or maybe not*) DOS had a RAMDISK.",hjqbxw5,t3_qoyr6v,1636323618.0,False
qoyr6v,"I am indeed too young for DOS 😂 but I do know what you are talking about, thank you for the helpful input. I'll likely ask you and others more questions as I dig deeper into this project. 

Hmm...  I'm looking at cat8 data transfer rates and it looks like we have a max trans. Speed of 40gbs and under Old ddr3 ram I'm seeing Max trans. Of 12.8gbs on the (possibly more common) 1600's. I imagine thats per module, so I'd max out the cat8 or USB gen 3.2 bandwidth at around 3 modules. Assuming I could get an integrated controller to distribute the data properly amongst the modules. Which for now, is good enough for proof of concept... Though eventually I'd like to make that 4 or 6 module capacity for people who have upgraded more than 1 laptop, I currently have 4 sticks of ram from 2 different laptop upgrades I performed. Different sizes, and potentially different generations (one may be ddr4, laptop was almost 10 years newer) I'm liking this idea for a cool tinkerer project. 😁",hjqe0ua,t1_hjqbxw5,1636324509.0,True
qoyr6v,"cool tinkerer project as it is, but you really need to properly interface (physically and protocol-wise) those RAM properly first (HW and very low level firmware). i don't see any hobby boards that provide slots/HW interfaces for these modules.",hjr4ajn,t1_hjqe0ua,1636336395.0,False
qoyr6v,"I was thinking about the hardware already, I figured I could scavenge or buy some ddr3 dim slots from somewhere, as for the low level programming needed for the firmware, I'd have to find a way to write a custom code, likely in c+ to organize and identify the hardware, and initialize a storage standard like nvme or something to layer on top of the ramdisk software. It's gonna be a pain, but yea. 
It's doable, a 3d printer would be good to make the dimm module housing.
 Mind sharing a source on good learning material to write low_level firmware? I'm familiar with a little bit of python and JavaScript.
 But nothing worth noting, I'm not currently capable of making standalone apps as of yet.",hjr5gab,t1_hjr4ajn,1636336942.0,True
qoyr6v,I am open to learning tho.,hjr5hd7,t1_hjr4ajn,1636336956.0,True
qoyr6v,"Check out Gigabyte's i-RAM. It is an ""external"" RAM disk, but it uses the PCI bus and SATA.

https://www.youtube.com/watch?v=bYbCYgYZVT8",hjrmk9v,t3_qoyr6v,1636345501.0,False
qoqmin,Wow,hjqj23r,t3_qoqmin,1636326702.0,False
qoqmin,"finally, no TLE for me on LC questions",hjrmv2v,t3_qoqmin,1636345670.0,False
qoqmin,"The supercomputer, called Jiuzhang 2, can calculate in a single   
millisecond a task that the fastest conventional computer in the world   
would take a mind-numbing 30 trillion years to do..",hjokkkb,t3_qoqmin,1636298520.0,True
qoqmin,According to China,hjr2wxe,t1_hjokkkb,1636335741.0,False
qoqb6q,"There is a lot of research. I'm not sure exactly what you are looking for, but if you refine this search you are likely to find it.

https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=non+computable+functions&btnG=&oq=non-comput",hjp39dh,t3_qoqb6q,1636305950.0,False
qoqb6q,"Check out Barak’s Intro to Theoretical Computer Science book, available online.

He presents a very explicit concept of uncomputable, and the many functions that fall into that category (including the Halting problem). Highly recommended.",hjr1o7t,t3_qoqb6q,1636335157.0,False
qoqb6q,"Look for papers by Alan Turing, Alonzo Church, Kurt Gödel, Jacques Herbrand, and Andrey Markov Jr.

Generally speaking it comes down to declaring that what computation is are processes that complete in a finite number of steps to reach a result, and showing that some processes cannot be shown to have a finite set of steps that will do so. The ones that cannot be shown to do so are precisely those that are not computational processes. Those algorithms are not computable.For Turing it was showing that some processes can't be encoded as instructions on a machine that can be shown to stop processing when the machine processes the instructions, and there was no set of instructions that could tell you whether the instructions for other programs would result in a machine that stops processing or not. For Church and others not being computable meant there was no way to reduce some logical formulas using recursive processes in such a way that would eventually be considered maximally normalized, and there is no such recursive process that can show whether another formula has a maximally normalized form or not.",hki1xl0,t3_qoqb6q,1636837382.0,False
qon555,"Mr Robot, ep1 reference to GNOME vs KDE.

And generally, the series shows lot of real commands from Linux and explain cyber security stuff",hjny21c,t3_qon555,1636287191.0,False
qon555,"Cool! I haven't seen this series yet, but I'll definitely put it on my list",hjnypju,t1_hjny21c,1636287628.0,True
qon555,Silicon Valley is great,hjosyn2,t3_qon555,1636301878.0,False
qon555,Yup! like gilfoyle the most!,hjrg7gp,t1_hjosyn2,1636342144.0,False
qon555,"Numbers is a TV crime/investigation TV show that centers around a math professor's insights into problems. Math, physics, CS, etc. are all part of every episode.",hjojwch,t3_qon555,1636298243.0,False
qon555,"Typically crime shows tend to have a character that is their tech/hacking specialist. The first show that came to mind for me was Criminal Minds, there is a character who’s only job is hacking and the show often shows her referencing linux, pinging IP addresses, etc",hjqhzjf,t3_qon555,1636326222.0,False
qon555,Futurama is loaded with CS jokes and references.,hjor9pb,t3_qon555,1636301201.0,False
qon555,"The Devs miniseries has references to some actual quantum computing topics, including a random mention of Shor's algorithm IIRC (although that one was just in some chitchat).",hjr1q7g,t3_qon555,1636335183.0,False
qon555,Halt and Catch Fire. Excellent drama at the backdrop of 4 different eras of tech booms,hjrs7mk,t3_qon555,1636348968.0,False
qon555,Definitely silicon valley and mr robot!,hjrk2ys,t3_qon555,1636344155.0,False
qon555,The Imitation Game,hjrp2v4,t3_qon555,1636346967.0,False
qoj64n,"That's the beauty of the TCP/IP network stack; each layer is independent of the others. Link layer, network layer, and transport layer don't really care what the others are doing. Want to use only wireless tech for the physical layer? Go ahead. Want to use carrier pigeons for your physical layer? Sure. Hell when SpaceX' Starlink is fully operational this would be a totally feasible scenario.",hjnqcc0,t3_qoj64n,1636281066.0,False
qoj64n,"It would be possible if we had completely wireless network infrastructure across the whole planet. There's no reason why this wouldn't be theoretically possible (although in practice, the whole network will probably crawl to a halt with current wireless tech, as it comes nowhere near the throughput of fiber optics), it's just that we haven't built this way (for good reasons, probably).",hjnnpsz,t3_qoj64n,1636278689.0,False
qoj64n,"Exactly, there is only so much RF spectrum we can use which must be shared between wireless devices that are near each other. Whereas each cable can do whatever over the whole spectrum they support with little/no interference with other cables and devices.",hjqns50,t1_hjnnpsz,1636328755.0,False
qoj64n,"Technically yes, practically no.",hjp1pqa,t3_qoj64n,1636305348.0,False
qoj64n,Have you been watching what Elon has been doing lately?,hjo9osm,t3_qoj64n,1636293705.0,False
qoj0r4,[deleted],hjndhun,t3_qoj0r4,1636268981.0,False
qoj0r4,"I see, so if someone wanted to have a password on their PC but very deep within the encryption have it locked to altitude, that way all you need do is be at the specific height. 

If when entering the password you are not at the altitude required to have the PC unlocked could it then be set to erase all data and then it is set to overheat and then explode inside insuring that all Data is as unrecoverable as possible?

So the altitude is the door and the password is the key. 

Without the door itself the key is useless. 

What do you think?",hjnevuy,t1_hjndhun,1636270291.0,True
qoj0r4,[deleted],hjnge0q,t1_hjnevuy,1636271739.0,False
qoj0r4,"This is mainly for personal curiosity, although any and all knowledge gained could come in useful. 

So things can be tricked and tempered with, what about old school satellites that are used for navigation instead of the internet, if somehow someone managed to connect the PC so it was only accessible when connected to a sat nav and it’s can’t connect to the internet because it’s hardware was removed and the software was also corrupted specifically the software for internet access. If then it could only be unlocked with connection to a sat nav connection and then only when you reached a certain altitude with 10m SQ at around that certain altitude, then you enter the password and then it unlocks safely, if however the altitude is way off or the password is incorrect on the first attempt, it would then erase all data followed by the CPU and the data drive heating up until it is completely destroyed. 

Would this be more secure instead of having it rely on GPS via the internet?",hjnicmp,t1_hjnge0q,1636273681.0,True
qoj0r4,[deleted],hjniv05,t1_hjnicmp,1636274185.0,False
qoj0r4,and there's the other thing that this is basically just a longer password. Adding a couple characters to the password is going to be better than jumping through hoops like this for anything except an ARG.,hjnmepw,t1_hjniv05,1636277507.0,False
qoj0r4,"So this is a form of multi-factor authentication. Instead of ""something you know"" (password) + ""something you have"" (fingerprint, phone, etc) it's ""somewhere you are"" (location). Even if the location could be spoofed, an adversary would have no way of knowing the position of the device ahead of time, and a robust lock-out mechanism would guarantee brute forcing it becomes unfeasible.

...or does it? In your threat scenario, your laptop is tied to a physical location. If it's stolen, an adversary would probably notice the GPS sensor and find a way to spoof it so it reports the same location where it was stolen (having them stolen it themselves). So at best, it's not a great advantage.

You could do variations of the above where the GPS coordinates (down to a reasonable number of decimal points, to have precision but be usable) could be used to initialise some variation of [HOTP](https://en.wikipedia.org/wiki/HMAC-based_one-time_password) like TOTP (think Google Authenticator) so that instead of time, the second factor is coordinate-based. Which would make for an interesting Cicada-like game, where the laptop can be unlocked only at different geographical locations... or be given to different people, each of them able to unlock it only in one location only they know. 

I'm fairly certain there are some corner cases whereby the procedure would risk failing open (for example, how to ""prime"" the laptop to unlock in certain locations without someone knowing all the locations ahead of time), but it might make for a good story :)",hjnn8a4,t3_qoj0r4,1636278249.0,False
qoj0r4,"**[HMAC-based one-time password](https://en.wikipedia.org/wiki/HMAC-based_one-time_password)** 
 
 >HMAC-based one-time password (HOTP) is a one-time password (OTP) algorithm based on hash-based message authentication codes (HMAC). It is a cornerstone of the Initiative for Open Authentication (OATH). HOTP was published as an informational IETF RFC 4226 in December 2005, documenting the algorithm along with a Java implementation. Since then, the algorithm has been adopted by many companies worldwide (see below).
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjnn93j,t1_hjnn8a4,1636278269.0,False
qoj0r4,"Firstly, yes. Secondly stop watching too much agents of shield. Thirdly, i fully agree with the answer of u/No_Engineering8506 but would say its safer to base it on position, time of creation and motion, i.e. the device would only be accessible on the earth but only directly beneath the iss (or something)",hjnyr0i,t3_qoj0r4,1636287654.0,False
qoj0r4,"Assuming a “trusted geolocation device” which does both encryption and decryption based on (location, password) there is still the possibility of an attack by manipulating the satellites. This may result into the access of the encrypted data or worse the inaccessibility of all data encrypted with TGDs.",hjnk1w9,t3_qoj0r4,1636275314.0,False
qoj0r4,"technically yes, but very difficult to make integrally. it might be easier to do it with local network (if this still meets your criteria).",hjnmld6,t3_qoj0r4,1636277671.0,False
qoj0r4,"You might be able to use nearby wifi SSIDs for geolocation to avoid spoofing 🤔 that comes with its own host of problems, like if the wifi around you changes suddenly you can't unlock your device.",hjp088t,t3_qoj0r4,1636304760.0,False
qoj0r4,"Yes, it would simply be another layer attributed with multi-factor authentication methods. How would you determine location? Longitude/latitude can have whatever degree of precision (by degrees, minutes, and seconds) and altitude is harder to measure and identify on a latitude/latitude basis. Your level of precision would influence the length of the brute force process, and unless you had ridiculous sensitivity on location - which would be troublesome - it would become easier to bypass this authentication method.",hk0pvmx,t3_qoj0r4,1636511802.0,False
qoifaz,[deleted],hjok49s,t3_qoifaz,1636298334.0,False
qoifaz,I mean we initialize the entire array _(each box)_ in the code to 0. Why do we only take zero? Why not another number?,hjoke3x,t1_hjok49s,1636298448.0,True
qoifaz,"If your asking why the first row and first column are zero, I think we can rationalize as if we have zero items, then no matter what weight we have we cannot generate any value, thus the value is zero. Additionally if we have zero available weight, then no matter how many items we have we cannot generate and value. Thus the first row and first column must be all zeros. From a mathematical perspective, zero should be the base case as when considering element number 1, we just take the weight of zero elements as zero if we cannot add the first element to the knapsack ( it’s weight is too high).",hju598p,t3_qoifaz,1636397950.0,False
qoh47t,"There is a lot of literature on genetic algorithms and schema theorem. I would suggest reading Holland's original work on it.

To give you a somewhat concise answer, the basic principle is that high-quality solutions have elements (building blocks as Holland called them) that are likely to appear in other high-quality solutions. Hence, with genetic algorithm (GA), a population of high-quality solutions is maintained. Two members are selected and their building blocks (genes) are intermixed. By mainly using the building blocks from these solutions it increases the likelihood of finding a better solution than those in the population. However, since it is possible that a key building block is missing from the population and hence cannot be selected by simply intermixing (crossover), a mutation operation is added to give a chance of finding such an element.

Since GA is non-deterministic it cannot guarantee to find the globally optimal solution. However, it is guaranteed to find better solutions over time because only better solutions are preserved in the survival/culling step. The computational cost for finding a better solution generally increases over time.

Note, that GA is not optimal for all search problems and is highly parameter driven (see various papers by Grefstennete and ""No Free Lunch Theorem""). Parameter optimization is required to make it work well for even those problem spaces for which it can search effectively.

GA is ineffective when the underlying hypothesis is not true, i.e., that high-quality solutions are not composed of building blocks of other solutions. Element independence is good for GA, and interdependence can be bad, as can deceptive search spaces. The best way to really know how to search a particular space effectively is to conduct a fitness landscape analysis.",hjnafhu,t3_qoh47t,1636266276.0,False
qoh47t,Great explanation!,hjp0ktk,t1_hjnafhu,1636304901.0,False
qo5xff,Your terminology is a bit confusing. A set only has unique elements to start with. Give an example of what you want?,hjkwted,t3_qo5xff,1636223675.0,False
qo5xff,"Yes, but a want to maximize unique elements among sets. Choose x number of sets that maximize unique elements",hjkxkct,t1_hjkwted,1636223994.0,True
qo5xff,Look into set cover problem and see if you can write your problem like it. If you can then use the best available method for finding set covers?,hjkxvl5,t1_hjkxkct,1636224132.0,False
qo5xff,"Thank you. I'll take a look into it. I was familiarize with the existence of optimization problems, but have never seem the term ""discrete optimization"".

[Looks indeed](https://www.youtube.com/watch?v=cjSeHSjPmsk) very similar to what I'm searching for here.

Thanks.",hjl3o10,t1_hjkxvl5,1636226679.0,True
qo5xff,"What are the input parameters to the algorithm? I think that's what I'm not understanding. 

It almost sounds like you have a minimum subset cover problem on your hands, but that depends on what exactly are the input parameters to the function and what do you want it to output exactly.",hjky7ll,t1_hjkxkct,1636224278.0,False
qo5xff,"Simpler version: I have a list of sets and want to find the best n sets that (together) maximize unique elements.  


less simpler version: I have a list of sets and a list of elements and I want to find the best n sets that (together) maximize unique elements that are present in the element list",hjkytbg,t1_hjky7ll,1636224541.0,True
qo5xff,"That still doesn't really answer the question. 

Are you given n? Or do you find n? And what do you mean by ""best n sets?""",hjkz860,t1_hjkytbg,1636224722.0,False
qo5xff,"Parameters:

I) collection of sets

II) n - size of group to consider

III) (Optional) - list of elements

Output: which n sets from collection should be selected in order to maximize unique elements (if parameter III is given, consider only elements from that list) considering all sets from collection.

Is that description more accurate?",hjkzlvs,t1_hjkz860,1636224887.0,True
qo5xff,"Yes I understand now. 

This seems like a problem that will generally take exponential time since something like a greedy algorithm wouldn't work here (you can't just pick the largest sets). 

Your best bet is likely to use a backtracking approach, but with some clever pruning to cut down on runtime (stop searching using a specific set if it isn't adding enough elements to the total cover or something like that).",hjl1h8c,t1_hjkzlvs,1636225705.0,False
qo5xff,"Thank you. That pruning idea seems very usefull to be used together with some sort of discrete optimization problem mentioned above.

I'll search more about what you've mentioned. 

Appreciate it.",hjl47ru,t1_hjl1h8c,1636226917.0,True
qo5xff,"This is set cover, which is NP-Complete for finding the optimal solution: [https://en.wikipedia.org/wiki/Set\_cover\_problem](https://en.wikipedia.org/wiki/Set_cover_problem)

You probably want to just pick a group of sets at random, or to pick K sets from largest to smallest and take their union (this is the greedy approximation algorithm described here: https://en.wikipedia.org/wiki/Set\_cover\_problem#Greedy\_algorithm).",hjl8pc0,t1_hjkytbg,1636228887.0,False
qo5xff,"Thank you. I'll study it. But as far as I can tell, adding the next subset with the most uncovered points each time is not an optimal way to do it in that case. Is that what greedy aprox do? (Don't bother to answer if you don't want to, once I have not completely understood the wiki page yet). tks!",hjlbsik,t1_hjl8pc0,1636230255.0,True
qo5xff,"Actually, it's closer to this related problem, which is still NP-Hard: [https://en.wikipedia.org/wiki/Maximum\_coverage\_problem](https://en.wikipedia.org/wiki/Maximum_coverage_problem)  


It has the same sort of greedy approximation algorithm, which chooses next the set with the most uncovered items.",hjlf9o5,t1_hjl8pc0,1636231783.0,False
qo5xff,"FWIW, you \*can\* solve this exactly for N sets by checking all 2\^N combinations of sets, and seeing which has the largest number of covered items for the fewest chosen amount of sets, but it's an exhaustive exponential search that's only feasible for small N.

You can get an approximate result (which may be good enough) by randomly choosing K < N sets, checking how many items are covered and tracking how large K is, and then iterating repeatedly to keep the solution with the most covered items for the smallest K. This is sort of similar to monte carlo integration.",hjlfqfy,t1_hjl8pc0,1636231989.0,False
qo5xff,"Example: I am proggraming something to find (among a list of texts, songs for example) the best n songs to learn in  order to maximize vocab (based or not in a desired vocab list)",hjky1ub,t1_hjkwted,1636224208.0,True
qo5xff,">  A set only has unique elements to start with

btw I think that sets can have duplicate elements to start with, but it is useless to have duplicate elements. In other words, it is not a rule that a set cannot have duplicate elements but there is no use of having duplicate elements. You can read more about it [here](https://stackoverflow.com/questions/10011475/can-a-set-have-duplicate-elements)",hjn1n3p,t1_hjkwted,1636260138.0,False
qo5xff,"A set does not have ""duplicate elements"" means you don't write a single element name twice. E.g., `{1,1,1,1} = {1}` and both sets have cardinality 1. A set is defined by membership rules. An element belongs to a set if it passes those membership rules. If an element passes membership rules for numerous reasons you still only write it once in the set. Z unioned with Z is still Z. Now there does exist a [multiset](https://en.wikipedia.org/wiki/Multiset) which allows for duplicates as you are thinking of them.

Stack Overflow is good place to get help with code but a terrible place to get your maths definitions from. Usually, you can visit [math.stackexchange.com](https://math.stackexchange.com/).",hjn2ovr,t1_hjn1n3p,1636260756.0,False
qo5xff,"**[Multiset](https://en.wikipedia.org/wiki/Multiset)** 
 
 >In mathematics, a multiset (or bag, or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The number of instances given for each element is called the multiplicity of that element in the multiset. As a consequence, an infinite number of multisets exist which contain only elements a and b, but vary in the multiplicities of their elements:  The set {a, b} contains only elements a and b, each having multiplicity 1 when {a, b} is seen as a multiset. In the multiset {a, a, b}, the element a has multiplicity 2, and b has multiplicity 1.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjn2q2j,t1_hjn2ovr,1636260776.0,False
qo5xff,"Searching set coverage is NP-hard, and thus has no polynomial time solution. As such a good greedy algorithm is your best bet to find the *exact* optimal solution. If instead you are interested in finding an *approximation* of the optimal solution, then there are many options in terms of [sampling](https://www.mit.edu/~mahabadi/slides/sublinear-sc.pdf) or [linear programming with relaxed constraints](http://theory.stanford.edu/~trevisan/cs261/lecture08.pdf).",hjlj3sg,t3_qo5xff,1636233520.0,False
qo5xff,"So, you have X sets (sets cannot have the same element in them twice, those are usually called ""bags"") and you get to choose n of them, which n do you choose to maximize the number of elements in the total union of all n sets?

This is definitely related to the set cover problem:  
[https://en.wikipedia.org/wiki/Set\_cover\_problem](https://en.wikipedia.org/wiki/Set_cover_problem)

I'd recommend looking at using a dynamic programming algorithm for this, there may be a simple recursive formulation that, when used with memoization yields a simple, elegant, performant algorithm",hjm0cv5,t3_qo5xff,1636241349.0,False
qo5xff,"**[Set cover problem](https://en.wikipedia.org/wiki/Set_cover_problem)** 
 
 >The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. It is one of Karp's 21 NP-complete problems shown to be NP-complete in 1972. It is a problem ""whose study has led to the development of fundamental techniques for the entire field"" of approximation algorithms. Given a set of elements                         {         1         ,         2         ,         .
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjm0ej6,t1_hjm0cv5,1636241370.0,False
qo5xff,"This seems very similar to this Problem:
[Set over](https://en.m.wikipedia.org/wiki/Set_cover_problem). 

This a NP-Complete problem so there is a good chance you wont find a polynomial algorithm for it. 

I thought about this problem a bit now and i dont think you can get something much better than brute force. There might be a dynamic solution though that runs in linear time with respect to the Capacity (the range of possible values in your case). 

If you need further help just ask me^^",hjmhyg1,t3_qo5xff,1636249623.0,False
qo5xff,"Thank you! I'm trying to read about the cover set problem, many people pointed me there and seems to be similar indeed.

I don't know much about math but what you mean by dynamic solution is something related to optimization problems? Maybe it could be solveable through optimization algorithms considering cost function as number of chosen sets or something?",hjmiiut,t1_hjmhyg1,1636249893.0,True
qo5xff,"They are most likely referring to Dynamic Programming https://en.m.wikipedia.org/wiki/Dynamic_programming
Programming in this sense is referring to filling in a table with solutions as you go along.",hjn1kl6,t1_hjmiiut,1636260097.0,False
qo5xff,"Desktop version of /u/codeIsGood's link: <https://en.wikipedia.org/wiki/Dynamic_programming>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hjn1m3k,t1_hjn1kl6,1636260122.0,False
qo5xff,"You want to select the set with the most unique numbers?

Find the set with the most numbers, let's call it ""set X"", than count how many unique numbers it has, let's call it ""N count"". Than, go through each set. If the set size is smaller or equal to ""N"", than don't count it. If it's bigger, than count it.

I don't see how you could find the set with the most unique numbers without counting the unique numbers on each set. A good optimization is to count the number for each set that is added or changed, so when you need to find the set, you already know it",hjmjjlk,t3_qo5xff,1636250389.0,False
qo5xff,This won't work. It's possible a union of smaller sets yields more unique elements than the largest set.,hjn1as1,t1_hjmjjlk,1636259945.0,False
qo5xff,"If a set has 10 unique numbers, than no set with less than 11 numbers can contain more than 10 unique numbers. Simple

Might as well start with the largest set of random numbers, because it's more likely to be the one",hjn6jsx,t1_hjn1as1,1636263310.0,False
qo5xff,"The algorithm returns a list of sets to choose not a single set to choose.

Example where this doesn't work.

Set 1: {A,B}

Set 2: {C}

Set 3: {D}

 Set 4: {E}

The algorithm would output sets 2, 3, and 4 since the union of sets 2, 3, and 4 has more unique elements than only set 1, yet set 1 has a higher cardinality than any other individual set.",hjoj9i8,t1_hjn6jsx,1636297971.0,False
qo5xff,Parkour has been on-point since Origins. It was an accident. They ended up giving up on Yamaha and just sent them to a custom body shop to get it over my leathers,hjymczv,t1_hjn6jsx,1636480222.0,False
qo5xff,"Believe this is an NP hard probelm, although I remember that the greedy approach (always choosing the set with the most uncovered items (items which are not already seen) is discussed as an approximation.",hju4d4m,t3_qo5xff,1636397594.0,False
qnl3xe,This is a good question.........,hjilk4q,t3_qnl3xe,1636176052.0,False
qnl3xe,"If you’re curious about answers I also posted this question to r/statistics and got a fair deal of responses. Paraphrasing, the consensus seems to be that statisticians are less concerned with neural networks and favor interpretable, empirical verified, analytically sound models. CS people by contrast tend to treat it like more of an experimental science.",hjilvx2,t1_hjilk4q,1636176279.0,True
qnl3xe,"Here's a sneak peek of /r/statistics using the [top posts](https://np.reddit.com/r/statistics/top/?sort=top&t=year) of the year!

\#1: [\[D\] Accused minecraft speedrunner who was caught using statistic responded back with more statistic.](https://np.reddit.com/r/statistics/comments/kiqosv/d_accused_minecraft_speedrunner_who_was_caught/)  
\#2: [\[D\] Very disturbed by the ignorance and complete rejection of valid statistical principles and anti-intellectualism overall.](https://np.reddit.com/r/statistics/comments/k88ifr/d_very_disturbed_by_the_ignorance_and_complete/)  
\#3: [\[E\] The 2nd Edition of An Introduction to Statistical Learning released. Still free. Lots of new topics.](https://np.reddit.com/r/statistics/comments/p0yg74/e_the_2nd_edition_of_an_introduction_to/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/)",hjilwm7,t1_hjilvx2,1636176293.0,False
qnl3xe,"I think this is sort of like asking what is the difference between a hammer for a carpenter and a hammer for a blacksmith.  Like, they're both tools, and they both hit things, but it's the what the tool is building that's the different.  The carpenter builds a house, the blacksmith builds more tools.  The Statistician (carpenter) is using ML build some data or interpret it.  But CS is using ML to build more or better tools.

I think as you mentioned that /r/statistics is more concerned about the interpretable models they get out using ML.  And that makes sense.  I think CS is more concerned with the algorithms to make those models.  

There's a ton of overlap, ML is like applied statistics using computer science algorithms.  But, CS tends to focus more on how to efficiently build the tools (so to say).  Which algorithms best fit the application and how to arrange the data so that it can be taught to the model. When working with ML there's more focus on the kinds of regressions, variance and bias from some given training set vs the test sets.  The overall goal in CS ML is to find ways to get the best resolution of the data while limiting the BigO time it takes to do it.

For instance.  A statistician doesn't really need to care too much about how long it takes for an ML to process a model.  If they're just analyzing numbers of some huge data set about global wind trends, or aggregate user data and viewership trends.  There's no pressing need that the ML be able to instantly pop out a prediction in miliseconds given some new input, just that whatever answer it arrives at is correct.  CS will say, okay, it's correct, but now my autodriving car just crashed because it wasn't able to quickly make that prediction.  So CS will want to know, ""how can I get as accurate of a model as I can, that can process a new input as quick as I can?"".

And obviously, all this with a grain of salt.  This is just generalties betweens fields but obviously individual use cases for each person in their specific industry may differ.  Like I said, there is a ton of overlap.",hjiqprm,t3_qnl3xe,1636179842.0,False
qnl3xe,"Saw this thread when it was posted in the stats reddit. Here's my tongue in cheek summary.

If you do ml in a stats dept you are smarter, better, more principled, more rigorous, and more interested in interpretability.

If you do ml in a cs dept you will make more money.",hjkw79u,t3_qnl3xe,1636223405.0,False
qnl3xe,One prob focuses more on how mathematically and less dealing with code while the other prob emphasizes more application of techniques,hjk17n1,t3_qnl3xe,1636210119.0,False
qnj83v,"No, no relation. it's 'making' a memo, for example if you want to keep a list of solutions",hjgkguf,t3_qnj83v,1636143025.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh36m,t1_hjgkguf,1636297036.0,True
qnj83v,"I have never seen the two used interchangeably.

If I had to guess, I'd say that a lot of people use 'memorize' in place of 'memoize' because the former is a far more common word than the latter and they're making an innocent typo out of habit.",hjh2cqz,t3_qnj83v,1636149858.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh3r9,t1_hjh2cqz,1636297043.0,True
qnj83v,"Memoization is a specific technique where you store the result of an expensive computation so you don't have to redo that calculation everytime you need it.  It's from an old programming method called 'dynamic programming'.  One example that I see a lot is in Rails, where you might do an expensive database query. Instead of doing that query everytime you need it, you can make it an instance variable:   
\`@some\_var ||= DatabaseRecord.where(query\_is\_expensive)\`    
The ||= operator only does the query if that variable isn't already assigned.  

Another example might be if you are doing a loop with an expensive computation every time. You can just save that computation and reference that variable instead of doing the same thing each time.",hjgmmi8,t3_qnj83v,1636143830.0,False
qnj83v,"Calling dynamic programming an ""old programming method"" isn't quite right. Fundamentally it is just a technique for designing algorithms with recursion - it's not ""old"" in the sense that it is outdated. It would be like calling calculus an ""old method for computing derivatives"".

I think the term ""programming"" can throw one off - when the term was introduced it was a more mathematical term, compared to now it is a more engineering term. The term ""linear programming"" is a helpful comparison.",hjhk3je,t1_hjgmmi8,1636157164.0,False
qnj83v,Fair. I meant it more in the sense that it’s been around for a few decades but good point.,hjhlkxq,t1_hjhk3je,1636157823.0,False
qnj83v,Makes sense - tbh I figured you meant that. I just wanted to make it clear incase your explanation would mislead someone who hadn't heard of dynamic programming but had done some computer programming.,hjhoxv0,t1_hjhlkxq,1636159323.0,False
qnj83v,"I confirm, dynamic programming are used every days in bioinfirmatics.

It's just another way of thinking.",hjhrtm5,t1_hjhk3je,1636160641.0,False
qnj83v,What is the difference from caching?,hjgwcg4,t1_hjgmmi8,1636147519.0,False
qnj83v,"My understanding is that memoization typically refers to saving the result of some computation.

Caching usually refers to saving data locally that lives in some far away location, not necessarily requiring computation. For instance, web browsers cache certain data from servers for later use, CPUs cache data that lives in RAM, etc. 

Though I'm not an expert by any means, I'm sure there could be exceptions.",hjhf8so,t1_hjgwcg4,1636155109.0,False
qnj83v,Your answer really makes sense.,hjhfnd6,t1_hjhf8so,1636155279.0,False
qnj83v,"I recommend you to see a memoized Fibonacci function.

Memoization is awesome because it's a simple thing which avoid to use a lot of useless computations. The best is, many programming languages provide memoization in their standard lib.

Memoization works only for pure functions. Your function has to always provide the same output for a specific input.",hjhsi9m,t1_hjhfnd6,1636160956.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh7xr,t1_hjhsi9m,1636297093.0,True
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh7bd,t1_hjhf8so,1636297086.0,True
qnj83v,"u/JazzGateIsReal's answer is perfectly acceptable, but I would add that really there is a bit of ambiguity.

Caching is a more general term - memoization is a specific form of caching that particularly helps when computing recursive functions.

For instance, in Python you can implement memoization by using the built-in `functools.lru_cache` as a decorator on a function (LRU means Least Recently Used - which is how it removes items from the cache).

Caching can be done in more general ways, e.g. to minimise calls to a server in a networking context so that the server is free to attend to more pressing requests.",hjhpyuu,t1_hjgwcg4,1636159793.0,False
qnj83v,It’s a form of caching. Caching of the results of functional calls.,hji2joh,t1_hjgwcg4,1636165647.0,False
qnj83v,My Rails example could probably be more accurately called caching.  Memoization more strictly defined is used in recursive functions (as I understand it).,hjgz3bz,t1_hjgwcg4,1636148576.0,False
qnj83v,"I had this thing in OCAML classes. I always thought it come with ""lazy programming"" since my teacher didn't name it and the main subject was lazy programming.",hjiy3dc,t1_hjgmmi8,1636185633.0,False
qnj83v,"*OCaml

It's not an acronym anymore.",hjjn6te,t1_hjiy3dc,1636203565.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh4z8,t1_hjgmmi8,1636297058.0,True
qnj83v,Note: my Rails example may not be a strict example of memoization but it's a similar concept: https://en.wikipedia.org/wiki/Memoization,hjgnse9,t1_hjgmmi8,1636144269.0,False
qnj83v,"Can do the same thing in Python with Pandas. Run the expensive query once, store it in a dataframe, assign it to a variable and have access to it repeatedly while only running the query once",hjgv7ib,t1_hjgmmi8,1636147089.0,False
qnj83v,'Never memoize what you can infer' - Albert Einstein,hjif2sb,t3_qnj83v,1636172113.0,False
qnj83v,Nope,hjjaiwz,t3_qnj83v,1636195534.0,False
qnduwn,"The M1 doesn't have any eDRAM, it mounts LPDDR4 memory on the same package the processor, resulting in a system-in-package (SiP).

This isn't a novel idea in mobile devices and embedded computers; for example, the original Raspberry Pi from 2012 did it, though it was to reduce the size of the system, rather than to improve memory performance. In the server space, Intel's Sapphire Rapids Xeons support HBM on package. The are coming out later this year or earlier next year, the last time I checked.

Outside of CPUs, GPUs have been doing this for a while; as have the NEC SX-Aurora Vector Engines (since 2017) and FPGAs. Other processors needing high-bandwidth, like neural network processors might be doing the same thing, but I don't know.",hjfh7lm,t3_qnduwn,1636128073.0,False
qnduwn,"ohh, that makes more sense, I was misled by diagrams of the layout in Apple's marketing I guess. I know mobile processors have been using the SiP design, its still interesting to see it in more powerful designs despite not being what I thought it was.",hjfjky6,t1_hjfh7lm,1636128976.0,True
qnduwn,"I don’t know, seems like a great way to limit upgradability … although I’m not sure there’s too much of that to begin with on that platform.  They had something similar a decade ago or so with stacked RAM. Basically soldered on top of the CPU.",hjffwqf,t3_qnduwn,1636127563.0,False
qnduwn,"To be fair, these systems aren’t meant to be upgraded. Like most other laptops.",hjg547e,t1_hjffwqf,1636137259.0,False
qnduwn,Or repaired,hjjb4yo,t1_hjg547e,1636195984.0,False
qnduwn,I'm not exactly sure about ram but M1 will and is having a huge effect on the hardware industry. check out the new intel cpu's and you'll notice how alder lake also boosts a number of effeciency and performance cores just like the M1 does. In no time w'll see more ARM in the desktop space.,hjfp5jp,t3_qnduwn,1636131102.0,False
qnduwn,"More integrators will do it, Intel has something in the pipeline I believe for Core processors.

Won’t be practical for the high end though, but you can already see in Xeon and IBM POWER, they’ll be growing L1-3 cache from MB to GB, merging L2 and L3 with the POWER architecture looking at shared and concurrent cache across chiplets and even across chips.

It will be some interesting years to come. For best results you’ll have on-die RAM and slower external RAM and even slower PMEM backed up by SSD as permanent storage with PCIe NVMe as the slow tier. That can all be integrated as a single access tier with controllers managing the migration of data from RAM to NVMe. That’s where Apple is going by baking all that into the chip and with tight integration in the OS, the SSD is basically the “slow” RAM.",hjigowe,t3_qnduwn,1636173025.0,False
qnduwn,IBM integrates eDRAM inside IBM Power Server processors since Power8 in 2014 as a cache.Apple is soldering ram with the SoC just like the GPU or APU used in PS5 or XB SX|S so it's not the same as eDRAM. If you want to upgrade M1 from 8 GB to 16 GB or M1 Pro from 16 to 32 or M1 Max from 32 to 64 it will take you a lot of time to get the process right assuming that you can use a soldering machine.,hjj3k3i,t3_qnduwn,1636190128.0,False
qnduwn,"It is close to impossible to upgrade any post 2015 mac. But with the introduction of the M1 chips. Damn they are quite powerful, a laptop that is truly worth the title of pro.",hjgbphp,t3_qnduwn,1636139744.0,False
qnduwn,"If integrating dram provides incomparably greater performance over others (with similar form factor), yes.
Ram modularity is not that important for most of comsumer product, compared to everyday performance and battery life.
Not sure about M1 Pro or M1 Max, but I'm pretty sure that M1 competitors will follow in some time.",hjib0gm,t3_qnduwn,1636169928.0,False
qna9yb,"Because writing simple business logic code isn't the point of computer engineering. Computer engineering focuses on computer architecture, computer organization, systems programming (things like firmware, operating systems, device drivers, etc.), and electronics engineering.",hjeprb4,t3_qna9yb,1636116412.0,False
qna9yb,"I agree, but I think 90% cs major are pursuing swe now",hjeq5qz,t1_hjeprb4,1636116620.0,True
qna9yb,computer engineering is not software engineering,hjew80m,t1_hjeq5qz,1636119535.0,False
qna9yb,I am saying most of cs majors will be a swe(or want to be),hjexpwo,t1_hjew80m,1636120207.0,True
qna9yb,It doesn't change the fact that SWE is not Computer Engineering and vice versa (despite sharing *engineering* in name),hjfkgm6,t1_hjexpwo,1636129316.0,False
qna9yb,Is software engineering not just applied computer science.,hjex0xc,t1_hjeq5qz,1636119899.0,False
qna9yb,"most of the time, no. Computer science studies the math and algorithms behind a certain problem. CS studies the beautiful world of computer logic whereas swe (\*sighs\*) is not cs because most of the sw engineers just write basic crud code",hjeye18,t1_hjex0xc,1636120508.0,False
qna9yb,"You need to be good at CS to be good at software engineering, tho.",hjf68k4,t1_hjeye18,1636123763.0,False
qna9yb,"Yep, if you aren't good at CS, you won't be as able to understand when applying stuff from CS would be beneficial vs not doing it, as well as, understanding what you should even apply in the first place.",hjf9079,t1_hjf68k4,1636124862.0,False
qna9yb,"Agreed. You don’t necessarily need to understand software engineering to be good at CS, but you need to be good at CS to be a good software engineer",hjfqcdw,t1_hjf9079,1636131564.0,False
qna9yb,"Sounds like it is dependent on where you went to school and how you are approaching the problems you are working on. My school was very much focused on ""the beautiful world of computer logic"" and then how to use it to build software and solve problems with it. As for problems I'm working on, just yesterday I was having a conversation with the architect on my team about how we could multithread a part of our ""basic crud"" application to reduce overall runtime complexity on an endpoint while also avoiding putting more stress on our company mainframes than we absolutely need to.",hjf0mtd,t1_hjeye18,1636121486.0,False
qna9yb,"I think we are pursuing something academically ""clear"" separation but it's never so easy. Like at my former university: it is called computer science engineering. We were taught heavy math, physics, electronics, networks, programming, computer science, engineering modeling, etc. It was really a mixture, and later you could specialize.",hjezavw,t1_hjex0xc,1636120914.0,False
qna9yb,"I'm sorry, I don't quite follow. If you agree with me that computer engineering is distinct from computer science, then why does it matter at all whether 90% of CS majors pursue SWE or not? They're still different disciplines.",hjew3p2,t1_hjeq5qz,1636119479.0,False
qna9yb,That’s cause computer science degrees generally prepare you for that line of work. Computer engineering is more focused on things like embedded software and other things closer to the hardware level,hjf64ff,t1_hjeq5qz,1636123718.0,False
qna9yb,Computer engineering is not software engineering,hjeu1c1,t3_qna9yb,1636118522.0,False
qna9yb,"To make things more funny in our system when you do the computer engineering bsc, on the 5th semester there is a specialization for software engineering or infocommunications.",hjev84s,t1_hjeu1c1,1636119081.0,False
qna9yb,Usually first two years of all 3 majors are same. Studying math and CS fundamentals.,hjft2g0,t1_hjev84s,1636132623.0,False
qna9yb,Computer science isn't software engineering.  There are software engineering specific programs but if you just changed the title of a program from computer science to software engineering without changing the nature of the program it'd be misleading.,hjetdrn,t3_qna9yb,1636118213.0,False
qna9yb,"> Why do we keep calling CS not CE?

Why do we call Computer Science Computer Science? Because it's Computer Science... There's a massive overlap between CS and CE, but I don't see why we'd rebrand it? If we called CS CE, what would we call CE?

Just because people who get a CS degree go on to be software engineers that doesn't mean they didn't study CS. Just like a lot of Maths majors go on to become financial analysts, doesn't mean we should rename ""Maths"" to be ""finance"".

Personally I did a degree in Computer Engineering but I still become a Software Engineer (although it was CE-adjacent).",hjf0kz0,t3_qna9yb,1636121464.0,False
qna9yb,"Makes sense!
I think most of cs major will become an engineer, that is my point. Also the cs major courses *are* engineering oriented.",hjf1zis,t1_hjf0kz0,1636122057.0,True
qna9yb,"I think you're confused about what Computer Engineering is.

If you'd have said: ""software engineering vs computer science"" or ""Why do we keep calling CS not SE?"" then it would have made more sense.

I would draw the spectrum as something like:

          Maths          ..CS..
            Physics  EE  CE  SE",hjf489x,t1_hjf1zis,1636122966.0,False
qna9yb,"Engineering is the use of scientific principles to design and build ""things"". Computer Engineering is an engineering discipline where you learn how to build computer/information systems and solutions on a theoretical (logical) and implementation level as well. As every engineering discipline we are working with models, and since models are an abstractions of the real world (or the system to be built) from a certain view, this is why you need to study differential equations, graph theory, physics, electronics, distributed systems, etc. so that you can model things correctly. 

All those things are taught at the university, but you can learn it also on your own. 

In Europe/Hungary we use the name Computer Engineering, or Computer Science Engineering.

https://www.bme.hu/computer-science-engineering-bsc?language=en

Most people actually won't become engineers. To become an engineer you need to go through an university and get your bsc or msc. They will become programmers.",hjerwrp,t3_qna9yb,1636117498.0,False
qna9yb,"Lol I agree most of it, except that I think a programmer is a engineer, same",hjeswl8,t1_hjerwrp,1636117981.0,True
qna9yb,A programmer is a computer engineer the same way as a bricklayer is an architectural engineer.,hjetxfj,t1_hjeswl8,1636118470.0,False
qna9yb,"No I don’t agree, there is a reason why they spend six figures to hire a programmer",hjev580,t1_hjetxfj,1636119044.0,True
qna9yb,It has nothing to do with salary 😂,hjevb4q,t1_hjev580,1636119121.0,False
qna9yb,"I haven’t heard companies hire programmers..it is same as software engineers, and with current devops culture, even not much difference between a swe and architect",hjexkca,t1_hjevb4q,1636120139.0,True
qna9yb,"swe is just a formal name for programmers. companies don't hire computer scientists because they don't build anything. computer engineers design and build computers. software engineers design and build software. they are symbiotic, not synonyms",hjeyaxr,t1_hjexkca,1636120470.0,False
qna9yb,"They give it fancy name, but if you are don't have an engineering degree, you are not an engineer. The same way as you are not a doctor if you don't have a doctor degree. It is rather simple. You might do some level of ""engineering"" work, or just code some Rest interface with some business logic.",hjey3oc,t1_hjexkca,1636120378.0,False
qna9yb,There's a difference between them. Computer engineering is electrical engineering applied to computers with a bit of computer science.,hjeyo1e,t3_qna9yb,1636120634.0,False
qna9yb,Computer engineering is typically electronics engineering. Computer science can either be actual CS or software engineering,hjeywmm,t3_qna9yb,1636120741.0,False
qna9yb,Oh that makes sense my bad,hjezpxw,t1_hjeywmm,1636121091.0,True
qna9yb,"Some, I think valuable, distinctions:

Computer Science is the study of algorithms, computing, formal languages, recursion, runtime analysis, data structures, operating systems, databases, graphics, sound synthesis, machine learning, embedded systems, and anything related really. Some of which is more math and paper oriented, some of which is more coding and practice oriented. 

Software engineering (SWE) is an application of those skills listed above, some more than others maybe, to build systems that accomplish some goal for a product or project. Being more well versed in CS is the same thing as being better at SWE. CS is the foundation of good, scalable, fast software.

Computer engineering is a non standard term, and to me is a bit ambiguous as to if you’re talking about systems engineering which I think requires some Electrical Engineering coursework as well, or just SWE which is what your question seems to hint. 

To answer your question:
“Why do we keep calling CS not [SWE]”

Just because some people go on to get a PhD in CS and advance the field with new algorithms and tools, doesn’t mean that those who don’t are not studying CS. CS is a whole host of skills, many of which would be the exact same ones you should learn if you wanted to just skip school and go straight to industry to do SWE. Universities, being places that want to attract people to do PhD programs, keep the title CS. Industry, being it’s own entity that can decide any number of labels for its employees has largely chosen SWE as their moniker. Most people don’t really care that they’re different words, but that’s why they’re different. In practicality, they’re the same field with practitioners and theorists, just like medicine has with doctors.",hjfg1wr,t3_qna9yb,1636127620.0,False
qna9yb,"There  are many engineering majors in the universities too, my point is that computer should belong engineering school",hjfk92w,t1_hjfg1wr,1636129233.0,True
qna9yb,That’s not what you asked in your question. My school includes CS in the Science and Engineering department. Even got a school of engineering hoodie. Sounds like you should take this up with your school,hjfl9ib,t1_hjfk92w,1636129633.0,False
qna9yb,"Most CS programs I’m familiar with are apart of the engineering dept, it’s quite rare to have a separate CS department as far as I know. 

What you do after your degree does not dictate what your degree is called. SWE is the profession that is in most demand that a CS degree qualifies you for. There are also so many paths that range from network architect to systems analyst that boiling the degree down to computer Engineering (which is a field already) does not accurately describe what you learn in a CS degree, especially at the bachelor level.",hjj1vtq,t1_hjfk92w,1636188740.0,False
qna9yb,"Computer science engineer here.

I did physics and differential equations.

Only difference between my major and an EE is the extra software classes and computer oriented hardware classes, versus power conversion and power transmission stuff.


I am NOT a software engineer.

We weren't taught how to write software. We were taught to use computers as a system to solve problems.",hjf5h1t,t3_qna9yb,1636123461.0,False
qna9yb,"Now, that's rather interesting. Let me try to remember what kind of classes we had 20 years ago.... (computer science engineer, fault-tolerant systems major)

Analysis (diff equation, fourier, etc.), Linear algebra (equations, matrices, etc.), Physics (2 semester), Electronics (mostly transistors, 1 semester), Digital systems (2 semesters), Graph theory (1sem), Algorithm theory (1sem), Signals and systems(1sem), Controllers (like PID 1 sem), Probabilty theory (1sem), 3D graphics (1sem), Coding technology (error coding, compression, des, rsa, etc.) , Computer architectures (1 sem), Programming in various languages (3 sem.),  Web programming (1 sem), Communication systems (1 sem), Databases (1 sem), Software development methodologies (1 sem) etc. 

And for the major we had stuff like: model based system engineering, fault tolerant systems, system integration, system validation, and various labs.  


How does it compare with what you learned?",hjfdojc,t1_hjf5h1t,1636126697.0,False
qna9yb,"Sounds pretty similar, but we do engineering statistics and statics as well.",hjfijxh,t1_hjfdojc,1636128582.0,False
qna9yb,"If not swe, What position will be then?",hjfg0yy,t1_hjf5h1t,1636127609.0,True
qna9yb,"An engineer.

Software engineering is a method of producing software by applying engineering-like techniques.

It's not really engineering.

Edit : Why are you booing me? I'm right.",hjfisn6,t1_hjfg0yy,1636128675.0,False
qna9yb,"Software engineer is an engineer, isn’t?",hjfjy3r,t1_hjfisn6,1636129117.0,True
qna9yb,"""Coincidence"" of names",hjfmfw3,t1_hjfjy3r,1636130074.0,False
qna9yb,All engineering basically producing products and considered applied science.,hjfth32,t1_hjfisn6,1636132781.0,False
qna9yb,There's a difference between programming and applied science.,hjfxjsn,t1_hjfth32,1636134349.0,False
qna9yb,Programming is a general way of implementing algorithms to compute mathematical problems. Hence programming is applied science.,hjg8ww1,t1_hjfxjsn,1636138702.0,False
qna9yb,"That's like saying putting together a kit is applied science.

Try writing your own array sorting algorithm and using it at work. Your senior will ask why you didn't just use the default library.",hjg9vzk,t1_hjg8ww1,1636139073.0,False
qna9yb,"At my work we write our algorithms. Your point is invalid. It’s like when a manufacturer company is building a robot should they build their sensors and cameras from scratch too or just buy one from the market? It’s a basic engineering concept, don’t reinvent the wheel.",hjgab3p,t1_hjg9vzk,1636139227.0,False
qna9yb,"I'm not saying that. I'm saying that a lot of programming is done using already been cooked code.

It's a different thing entirely to talk about sensors and physical devices because you still need to design the interface, regardless of which IC you use.",hjgakvb,t1_hjgab3p,1636139327.0,False
qna9yb,This shows that you really don’t know what you are talking about. If all codes are “cooked” out there I wonder why I’m getting $250k a year. Can’t my company just copy the codes and paste them together and save them millions? Have a nice day Mr. Genius.,hjgbam7,t1_hjgakvb,1636139591.0,False
qna9yb,"Calm down there Javascript master. No one is questioning your ability to invert a binary tree or anything.

All I'm saying is that you could be replaced by a trained monkey. No need to be so salty.",hjgcz2c,t1_hjgbam7,1636140216.0,False
qna9yb,"Same for sw, it is also a design phase",hjgoqfg,t1_hjgakvb,1636144624.0,True
qna9yb,"Computer Engineering != Software Engineering

if(person.inCS_major()){

   personCanBeSWE  = true;

   personCanBeDataArtitect = true;

   personCanBeGameDev = true;

   personCanBeComputerEngineer = false;

}",hjj7gg2,t3_qna9yb,1636193219.0,False
qna9yb,"Your question makes little sense. What is precisely the context between computer science versus computer engineering?

If you are referring to a degree program, than the answer is obvious. A degree is named based on the course content and syllabus, not on the eventual career outcome of people receiving education under that program.

Majority of people who receive any degree at undergraduate level and masters level, and even significant number of people who go all the way to Phd level ( or any equivalent qualification) will not have a career as a researcher in that field or equivalent position that requires the epitome of specialization in that field. Therefore, select few physics major become physicist; select few creative writing majors become full time authors of creative literature; select few business majors go on to become a CEO ; and you can expand this argument to many other fields.

So, a computer science is called that because the university offering it believe that the courses and syllabus fall under the field. And, it is common for colleges to differentiate between a computer science and computer engineering course. At my school these are two different majors despite considerable overlap, and at most school the same applies.

If you are referring to how the term CS is used in casual conversation — e.g CS pays well— there can be some semantic ambiguity which by the pragmatics of the language it is very clear anyway. For the very example I have cited, it is clear that speaker is making a comment on specific subset of the field, rather than a statement on the field in its entirety.

Some lay people may not understand the differences and the intersection of different fields but that usually has little to no bearing on the use of terminologies, jargons, and other common lexicon, and it is very easy to convey these differences if and when the situation requires for it.",hjfd8ph,t3_qna9yb,1636126525.0,False
qna9yb,Computer engineering deals with hardware. Computer science deals with theory. Software engineering deals with software and applications. All of them overlap in different degrees.,hjfspl4,t3_qna9yb,1636132482.0,False
qna9yb,"I know of (non-computing) engineers who are pissed at the term software (etc) engineering because it’s borrowed the terminology without taking the rest of what makes an “engineer” along with it i.e. the notion of chartered status, a regulatory body for professional standards, etc",hjfznbk,t3_qna9yb,1636135172.0,False
qna9yb,"The names are conflated because CS spawned out of the math department at some schools and out of engineering departments in others. It spans a wide variety of traditional categories, and the names have never really settled.

In my mind SWEs are programmers, the only “real engineers” are hardware/architecture people, and “computer scientists” span a wide range of computing theory to very applied stuff.

Also, my college degree officially says “computer science and computer engineering” even though it was just one major, and my Master’s says “electrical engineering and computer science” even though it was on very abstract CS stuff. Just a matter of what the departments were called, lol.",hjhlozy,t3_qna9yb,1636157872.0,False
qna9yb,Op are you 12?,hji369w,t3_qna9yb,1636165947.0,False
qna9yb,maybe you would like to rephrase the post...,hjeyj8p,t3_qna9yb,1636120573.0,False
qna9yb,"Yeah sorry I meant it should be an engineering degree not a science degree , well maybe it does not matter",hjezyh6,t1_hjeyj8p,1636121193.0,True
qna9yb,Computer engineering is more about the hardware interaction than software,hjf6zn3,t3_qna9yb,1636124058.0,False
qna9yb,I don’t think you know what computer engineering is.,hjg43os,t3_qna9yb,1636136873.0,False
qna9yb,Computer science is a branch of mathematics. Computer engineering is a branch of electrical engineering. A computer scientist is a specialized mathematican. While a computer engineer is a specialized electrical engineer.,hjht8fj,t3_qna9yb,1636161298.0,False
qn6kqf,"I can't tell you every database but Postgres has some pretty good Documentation that goes through how their protocols work. [HERE](https://www.postgresql.org/docs/current/protocol.html). You'll probably want to look at the Message flow section to get to the networking bit.

TL:DR for the other Databases they'll all be some flavour of a standard TCP connection flow aside a couple that may do some flavour of UDP.",hjeclfa,t3_qn6kqf,1636108021.0,False
qn6kqf,That was a great read. Thanks for recommending it.,hjz7fk2,t1_hjeclfa,1636488592.0,True
qn6kqf,"Do you mean like the OSI model layers? Here’s a pic:

https://electricalacademia.com/wp-content/uploads/2018/12/image-result-for-osi-model-layers-and-its-function.gif

Layer 1 is physical layer (actual electric signals through cables) all the way up to presentation and application layer (6 and 7) where your data format is understood and your app presents data on the screen. For connection management look to layers 5 the session layer, that were you’ll find all the goodies about host and client sessions. But it’s just part of a bigger picture, so if you want to deep dive into how computers connect and talk to each other I think looking up the osi model will help, and you can drill down where you are interested.

https://www.freecodecamp.org/news/osi-model-networking-layers-explained-in-plain-english/

I hope I understood your question and could be of some help!",hjkghzv,t3_qn6kqf,1636216727.0,False
qn6kqf,Opening a connection literally creates a thread that both the server listens/sends on and the client listens/sends on. Data goes up and down the OSI model (depending on the protocols used) when data is transmitted. The rest is the basics with networking.,hk0qrmp,t3_qn6kqf,1636512200.0,False
qn56a6,"Yes, although metadata (file system info) will be different. But copying a file results in a precisely identical copy. This is one reason why digital is superior to analog recordings (from a degradation standpoint); digital copies are perfect, but analog copies lose fidelity.",hje11vn,t3_qn56a6,1636097770.0,False
qn56a6,"And interestingly enough, sometimes analog's degradation qualities can be desirable (see the proliferation of tape and vinyl simulation audio plugins, and big name music engineers who still record to tape).",hjehyzd,t1_hje11vn,1636111886.0,False
qn56a6,"Yes, there’s a healthy lo-fi movement.",hjf5zg6,t1_hjehyzd,1636123664.0,False
qn56a6,"I'm not a sound engineer, but I think any high-end audio tape recording still in existence today would be using DAT (Digital Audio Tape).",hjeorrv,t1_hjehyzd,1636115892.0,False
qn56a6,"No, I'm referring to [regular analog tape](https://www.youtube.com/watch?v=D5VHW1J5o0Q).",hjetvpv,t1_hjeorrv,1636118447.0,False
qn56a6,That was interesting to watch. I had no idea people still used analog tape these days.,hjex3dp,t1_hjetvpv,1636119928.0,False
qn56a6,"And it's interesting seeing the development of digital software to simulate those ""imperfections"" and subtle characteristics you get from recording to tape, using analog compressors/EQs/preamps, analog synths with variations in the aging components, tube guitar amps, etc.",hjf87kz,t1_hjex3dp,1636124544.0,False
qn56a6,"It's less common for sure, but digital data can degrade as well, albeit under a different set of circumstances. Having redundancies and quality checks for data nowadays mitigates this to the point where risk is negligible, but [bit flips by cosmic rays](http://www.businessinsider.com/cosmic-rays-harm-computers-smartphones-2019-7) can also introduce errors.",hjfs5je,t1_hje11vn,1636132267.0,False
qn56a6,"Yes, and there are plenty of other issues that can degrade digital signals but I only meant to discuss typical/normal operation.",hjhdwmv,t1_hjfs5je,1636154536.0,False
qn56a6,"This entirely depends on the operating system, but I believe that some OSes won't actually copy the file (just pretend to copy it) until something wants to modify it.",hjdz7ja,t3_qn56a6,1636096136.0,False
qn56a6,"Officially known as ""Copy on Write"".",hje7eiq,t1_hjdz7ja,1636103596.0,False
qn56a6,Oh wow that's actually pretty interesting,hjfjtf9,t1_hjdz7ja,1636129068.0,True
qmvvyw,"I think Rust and Go are allowed into the party of languages you wouldn't mind in your kernel, but those are fairly recent developments and they have a lot of catching up to do. C++ is still shunned in OS development, partly due to old Torvalds sentiment that a lot of people picked up on, which... sure. There are few in this world that could challenge the pedigree of that outlook and I am not one of them. Objective-C is higher level than C and that's supposedly what MacOS runs a lot of kernel level stuff on but that language is just digital pain.",hjdfgx4,t3_qmvvyw,1636082722.0,False
qmvvyw,Go’s mandatory garbage collector would be problematic in a kernel.,hjeteds,t1_hjdfgx4,1636118222.0,False
qmvvyw,"> can we build an OS from Python

Sure, I don’t see why not. *Should* you build a language in Python? No, probably not. People have built OSes in high-level languages though: https://node-os.com/ https://github.com/froggey/Mezzano

And probably plenty more. 

What language *should* you use today? I’d say Rust. The Linux kernel is obviously written by incredibly smart people, but programmers make mistakes. Google has a project to identify bugs in the Linux Kernel and found that over 70% of them are memory issues or data races. Mozilla, Microsoft, and many other companies have said the same things about their C/C++ code bases. However, these issues are, by design, impossible in Rust. Rust’s unique ownership model makes data races impossible and allows it to properly free your memory without the need for programmers to do it manually like in C/C++ *and* without the use of a garbage collector like all high level languages. And Rust is just as fast as C/C++ and sometimes faster, while still being a higher level language.

If you want to learn a low-level language or make an OS today, consider Rust over the traditional choice of C/C++.",hjdvtsg,t3_qmvvyw,1636093279.0,False
qmvvyw,"I know that Rust prevents memory leaks because of ownership, but does it really help prevent race conditions?",hje9mbe,t1_hjdvtsg,1636105554.0,False
qmvvyw,"Yes, it has rules for safe concurrency enforced through the borrow checker and ownership too.",hjeajz8,t1_hje9mbe,1636106354.0,False
qmvvyw,"Not all race conditions, only specifically data races as far as I know. But for data races specifically, yes, it completely eliminates them. I don’t actually know Rust so I can’t really explain how it works though.",hjfpjor,t1_hje9mbe,1636131255.0,False
qmvvyw,"The answer is C, thank you.",hjdgcyd,t3_qmvvyw,1636083187.0,False
qmvvyw,"Answers about C are wrong. Kick every of that responders.

There are at least some os kernels on golang, you can just Google them. [Example](https://www.google.com/url?sa=t&source=web&rct=j&url=https://github.com/gopher-os/gopher-os&ved=2ahUKEwiC0M-snoH0AhXSwosKHS-EBocQFnoECEMQAQ&usg=AOvVaw2Oce0Pru-lcy5pyqaAEwwA)

It's possible to create os at least by using any language, that is compiled into machine code",hjep5xd,t3_qmvvyw,1636116100.0,False
qmvvyw,I think its C,hjefink,t3_qmvvyw,1636110221.0,False
qmvvyw,It's all machine code in the end so you can do it however you like so long as you got the compute resources.,hjffjxo,t3_qmvvyw,1636127423.0,False
qmvvyw,"Microsoft Excel, but first you must invent the universe.",hjfsfqy,t3_qmvvyw,1636132377.0,False
qmvvyw,"> I guess I'm asking programming languages that best communicate with modern hardware architectures

What did he mean by this?",hjmpn6h,t3_qmvvyw,1636253417.0,False
qmuxrz,python is not compiled.,hjc5p0d,t3_qmuxrz,1636062423.0,False
qmuxrz,"Okay then a different example, like Java or C++",hjc5xum,t1_hjc5p0d,1636062521.0,True
qmuxrz,"Java isn't compiled to machine code either 🙂

To answer your direct question the `if (x == 2)` might to produce the same machine code. Basically a `cmp` and a jump instruction (`jne` maybe). 

`print(x)` would be pretty different between languages (not just the keyword / function name itself) due to string interpolation, formatting, buffering, flushing, etc. There's a lot that goes into printing stuff out.

For your most specific question (regarding C++ and C), **if** the same compiler is used and **if** the basically the same source code is used, then it _might_ be the same machine code. Though C++ `std::cout << x` likely produces different machine code than C `printf`.

(this is all a vast generalization and I am **not** an expert, not even close. Please anyone else correct me)",hjc8b30,t1_hjc5xum,1636063487.0,False
qmuxrz,"Yeah I thought as much that if the syntax is different (even though the logic be the same) it would have different machine code. (but I'm also not certain)

So I would truly like to know if the analogy is true of choosing the programming language is like choosing a different brand of screwdriver for making a car. Is it truly not any less efficient to program operating systems with C++ or C# or a more modern language?

Like I know programming languages were designed to program is specific areas, like SQL for database and Java for websites, and Python for applications. So my question as I originally stated is will C always be the language we use to communicate with hardware?",hjcd28x,t1_hjc8b30,1636065465.0,True
qmuxrz,"C/C++ were designed as systems languages, as something you can put right on top of the hardware. They have fixed bit length variables, direct memory address access, pointer arithmetic, direct hardware register access, interrupt handling. They essentially do a good job at reflecting what the hardware is actually like. Most others were designed as applications languages, to produce programs that only need to talk to an operating system but don’t deal with the hardware directly. That’s not a strict rule you can use either for the other purpose but they’re not really designed with that purpose in mind. If you take a look at embedded engineering where it’s still common to write programs right on top of the hardware with no operating system in between it’s almost exclusively C/C++ for this reason.",hjclx8w,t1_hjcd28x,1636069220.0,False
qmuxrz,"> So my question as I originally stated is will C always be the language we use to communicate with hardware?

_Always_ is a long time! At some point, quantum computing will be mainstream. Far into the future, who knows? We may all be living in a simulation with a universal VM where our every thought is merely a representation of some form of bytecode in the Zuckerverse.

In the short to medium term, yeah, C will still be king with regards to communicating with hardware. Rust will gain some traction in container runtimes, device drivers and small parts of OS kernels, but C will still be the lingua franca",hjd0zwh,t1_hjcd28x,1636075832.0,False
qmuxrz,"> Like is the instruction of if (x==2) then print(x) the same machine language for C and python?

Answering the question directly, no.

Python compiles down to its own byte code that is unique to python.

Java compiles down to byte code for the JVM (Java Virtual Machine). The JVM hosts other languages as well such as Clojure.

C compiles down to _whatever instructions set you want_. That might be x86, that might be x86_64, that might be 6502 or ESP8266.

Compilation just translates the code from one language to another, usually simpler, language. Usually down to some machine-readable code, either a virtual machine (NOT like VMWare) or physical machine code. Physical machine code is interpreted by physical hardware rather than software (we must get this low level at some point) and depends on the exact machine you're running the code on. So C doesn't even have to compile to the same code/instruction set as C.

Finally, the `print(x)` actually must ask the OS to do the printing for you. So in the case of the example code, if you  compile the code for an intel mac, and a windows PC, or a Linux PC, you'll use the same instruction set, but it will result in different code as the code will talk to the OS in different ways.",hjcgi7w,t3_qmuxrz,1636066914.0,False
qmuxrz,"Short answer, yes, but a lot of languages are not compiled to machine code but are instead interpreted, either directly or via an intermediate language.

A given processor (or family of processors) will have a language that is unique to it. All compilers for that processor will generate the same language.",hjdiwde,t3_qmuxrz,1636084544.0,False
qmuxrz,"No. For example, Go, Rust, and C all have different calling conventions in the assembly they compile down to.

Compilers can also be non-deterministic, and with options for different optimizations and mitigations, the same lines of C could result in wildly different assembly.",hjf5dd7,t3_qmuxrz,1636123419.0,False
qmuxrz,"You asked about different languages, but let's do it for the same language!

[GCC](https://godbolt.org/z/GeqGPvdr1) and [Clang](https://godbolt.org/z/9TT4Tr1zd) - same code, same language, for the same platform, Clang is even somewhat compatible with the GCC, and yet there already differences.  
Turn optimizations on - [GCC](https://godbolt.org/z/f38vYvKWW) and [Clang](https://godbolt.org/z/19Td9j6je) output at first glance different Assembly.

If such is the case for the same code using two compilers, then between languages the differences are even bigger.",hjfjz04,t3_qmuxrz,1636129126.0,False
qmtz3q,"the copying is the installation. One does not happen before the other, one *encompasses* the other. (Some OSs instead download everything from the internet during installation, but downloading is just another form of copying)

For question 2: No. Yes.
The hard drive is not ""your whole computer"". It's the part that stores data. But yes, if you take it out and into another PC, your *system* will most likely boot there (assuming the new PC does not have special hardware requiring drivers you did not need to install for the old one) and be the same system.",hjdecfn,t3_qmtz3q,1636082141.0,False
qmil8h,"The physical memory is organized such that one value can occupy several adjacent addresses. Say your stack contains 8 bit integers (uint8), when looking for the next integer, the program will skip 8 bits and start reading again.",hj9ry2h,t3_qmil8h,1636027008.0,False
qmil8h,"This means that programs will usually have constants which will be used in operations. For instance, there may be a constant named c in your program with an assigned value of 1 (int c = 1). This constant c may be used in an operation to increment a variable i with an initial value of 0 assigned to it (int i = 0). This operation would be i = i+c. 

The variable i may be used to access items of an array. Suppose you have an array A. Accessing an item at index i of array A would be done by having the following in your program: A[i]. Incrementing i by c first would allow you to access the next item in A if you were to call A[i] again in your program. 

Hope this explanation made sense.",hj9s8aw,t3_qmil8h,1636027182.0,False
qm9wc6,Github Repo containing visualisation: https://github.com/angary/simulated-annealing-tsp,hj8aqko,t3_qm9wc6,1635990330.0,True
qm9wc6,This is so sick. Just last week I was learning about Simulated Annealing in my Performance Modeling Class. Applications are endless!,hj8w3eo,t3_qm9wc6,1636001584.0,False
qm9wc6,Sick! 😎,hj8bsat,t3_qm9wc6,1635990802.0,False
qm9wc6,So cool dude here I am writing  and **proving the correctness** of the same old bellmen ford for my homework :(,hj8bq0w,t3_qm9wc6,1635990773.0,False
qm9wc6,that was satisfying,hj8ib6a,t3_qm9wc6,1635993879.0,False
qm9wc6,Oh man I’ve forgotten this from uni 😥,hj9i488,t3_qm9wc6,1636020024.0,False
qm9wc6,Wow this is so coooooool!!!!,hj9xx5g,t3_qm9wc6,1636030321.0,False
qm9wc6,"Cool visualization! I'm actually working on a probabilistic analysis of simulated annealing, so it's always nice to come across it in the wild.

What cooling schedule are you using, if I may ask?",hja0n4b,t3_qm9wc6,1636031687.0,False
qm9wc6,I'm using a geometric cooling schedule - however in the gif I speed up the playback rate during towards the lower temperatures (just so you can see more of the action faster at the lower temps),hja7m60,t1_hja0n4b,1636034862.0,True
qm9wc6,Geometric schedules cool quite quickly no? Are the results significantly better than just straight 2-opt?,hja86cn,t1_hja7m60,1636035106.0,False
qm9wc6,"Cooling quickly can be a benefit as you don't need it at a temperature too long (however, you can adjust the cooling rate).  


So far, the SA is better than pure 2-opt, however not significantly (though I may not be using low enough cooling rate - lower cooling rate == better results, though takes significantly)",hja8kzs,t1_hja86cn,1636035285.0,True
qm9wc6,"Yeah that's what I figured, with a geometric rate you ""freeze"" so quickly that it's not *that* much better than iterative improvement. Getting rigorous bounds on that is one thing I'm interested in.

You can probably get better results pretty easily by implementing a basic version of Lin-Kernighan. The full LKH algorithm isn't even really necessary to outperform simulated annealing.",hjad1pr,t1_hja8kzs,1636037127.0,False
qm9wc6,This is awesome! I just wrote an Operations Research exam today and was tested on simulated annealing,hjaovio,t3_qm9wc6,1636041753.0,False
qm0yhw,They are probably encrypted and not hashed.,hj6qkji,t3_qm0yhw,1635967071.0,False
qm0yhw,"Websites store passwords hashed, but I don't think they generally hash email-addresses. If they did, they wouldn't be able send password-reset emails, marketing emails, and so forth.

EDIT: BTW, this probably isn't the right sub, maybe r/webdev ?",hj6lmrm,t3_qm0yhw,1635965181.0,False
qm0yhw,"Marketing emails no, password reset yes. Why? Because the user has to provide the email to do a password reset do you know what the unhashed value is.",hj7swci,t1_hj6lmrm,1635982466.0,False
qm0yhw,"I remember reading [somewhere](https://termly.io/faq/are-email-addresses-personal-data/) that emails are personal data, and so I though they'd also be hashed. I'll post in webdev to double check. Thanks for the response!",hj6nc8g,t1_hj6lmrm,1635965830.0,True
qm0yhw,"Considering there are ~~no legal ramifications~~ (view reply) for companies to do so, I doubt many actually encrypt everything that would be considered PII, but I guess we don't actually know.

I can't imagine anyone hashes PII, as then it obfuscates the actual data which you need, which is something you can deal with in regards to passwords and such, but not PII.

You would encrypt all PII if you wanted to follow the best practices, though I doubt most companies do. So most companies probably just store emails normally unless they use some third party company to handle / host their data which may do that for them.

This would all fall under [non-sensitive PII](https://www.virtru.com/blog/6-steps-to-securing-pii-for-privacy-and-compliance/) which is why no one really cares. If it was sensitive PII it would be a different story",hj6pprq,t1_hj6nc8g,1635966743.0,False
qm0yhw,"It's fairly easy to enable disk encryption, the process for migrating an existing database to an encrypted drive wouldn't be too hard.",hj6rzxf,t1_hj6pprq,1635967625.0,False
qm0yhw,"webdev here, we don't hash email, hash is one way, we need to retrieve the email in someway for many purposes. Forget password is the easiest example.

password is hashed + salted, other than that, we probably just save it as is. probably some shops encrypt their data too for privacy but it shouldn't be hashed.",hj6sstw,t1_hj6nc8g,1635967935.0,False
qm0yhw,"*disclaimer: I don't actually own a website and I have never coded a website which had to store any user data. I may have made some mistakes in the following post, due to my inexperience. Please point them out if you notice any*

I think passwords should be hashed, such as with BCRYPT, user data should be encrypted, such as with blowfish, and emails should be stored plaintext (to recover passwords).",hj6o60s,t1_hj6nc8g,1635966147.0,False
qm0yhw,"You're talking cryptography more than anything webdev specific. There are one way functions ([hashing](https://en.wikipedia.org/wiki/Hash_function)) where you can't retrieve the original piece of data, and two way functions ([encryption](https://en.wikipedia.org/wiki/Encryption)) where if you have the appropriate key you can get it back. Quite often personal data like emails will be encrypted, to prevent it from being as easily stolen, since you need the key too. This is not seen as that useful for personal info in practice though, since if your database is compromised usually your web server software is too - and the web server has to have the key to do its job, so the attacker can obviously decrypt the info they want. It's more useful for data transmission than storage.",hj79896,t1_hj6nc8g,1635974258.0,False
qm0yhw,[deleted],hj7hv0v,t1_hj79896,1635977711.0,False
qm0yhw,"If your web server is compromised, then your data throughput is compromised. If your data throughput is compromised then you can *always* get access to the decrypted data by using whatever mechanism the web server uses to get the decrypted data, which is must have access to by definition since it *uses* the decrypted data. There is literally no way around this, owned web server = owned data, period. That's why you protect the web server like it's gold, and never store encrypted passwords.",hj7is34,t1_hj7hv0v,1635978092.0,False
qm0yhw,"Yes, emails are personal data. So is your address, phone number, and credit card number.

But if Amazon wants to post you your order, call you about a problem with your account, or charge a payment to your card, they need that information in cleartext.",hj6oida,t1_hj6nc8g,1635966281.0,False
qm0yhw,Not hashed but encrypted,hj8b820,t3_qm0yhw,1635990549.0,False
qm0yhw,"Because they are not? I can certainly see why sometimes one might want to encrypt them, but I can't imagine why you'd want to hash them aside from very niche cases.",hja9m4g,t3_qm0yhw,1636035725.0,False
qm0yhw,"Well, comparing the hashes of two values indicates a certain probability that these value are equal. But it’s not an absolute proof. Keep in mind that f(x) = 1 is a perfectly fine hash function, albeit not a very useful one.",hj6ujbl,t3_qm0yhw,1635968608.0,False
qm0yhw,"Usually you do not hash emails, but if you do, you should have a master key for hashing, just decode them with the encrypted hash master key, is the same for enceypted data with othe methods. If you’ve lost the master key, good luck asking the users for their emails and matching them manually on your db.",hj89gcq,t3_qm0yhw,1635989757.0,False
qm0yhw,"Hash all possible email addresses and compare the hash values. Eventually, you will get all the addresses if you give it sufficient amount of time...

Some potential optimization would be to wrap it as a proof of work for some crypto currency and let others help you.",hj8j5uh,t3_qm0yhw,1635994297.0,False
qm0bpt,"why analog? i do a fair amount of AI and ML, but would analog have much benefit? it seems unnecessarily niche",hj6zzq8,t3_qm0bpt,1635970714.0,False
qm0bpt,"Purely theoretically, analog computing offers much better efficiency, as a signal can represent any value within its bandwidth, instead of just 0 and 1. You can also perform mathematical operations directly in the electrical circuit using operational amplifiers, instead of relying on clocks and algorithms. 

New Mind has a good [Youtube video](https://www.youtube.com/watch?v=owe9cPEdm7k) on the topic.",hj77z60,t1_hj6zzq8,1635973761.0,True
qm0bpt,Sounds like a bad idea,hj6zd7k,t3_qm0bpt,1635970476.0,False
qm0bpt,Elaborate,hj6zlgj,t1_hj6zd7k,1635970563.0,True
qlzztd,"The bit used in rdt 2.1 is only in response of the ability to handle corrupted ACK/NAK’s (not the actual data sent). If an ACK/NAK is corrupt, the sender needs to resend the package to the receiver to get another ACK/NAK that isn’t corrupt. If the receiver has already received the previous package, when it is resent by the sender it’ll be a duplicate package, so we need a way for the receiver to know whether the package sent most recently is the previously received package or a new one. The single bit (sequence number) you refer is used by the receiver to check whether the received package is a duplicate or a new package. 

This bit is updated in the receiver end after sending an ACK, so for example say it’s set currently at 0, it receives a package with bit 0 from sender, sends an ACK back, now bit is update to 1. Let’s say that ACK was corrupted so the sender resends package with bit 0. Since the current sequence number in the receiver end is 1 and 0 != 1 the receiver knows this isn’t the expected package so it must be the previous package and it resends the ACK for bit 0 (when you resend an ACK from a previous package you don’t update the sequence number)

The reason we can use just 0 or 1 for a sequence number is because this is a stop and wait protocol and rdt 2.1 assumes no packages will be lost (rdt 3 handles that after)",hj6pd09,t3_qlzztd,1635966606.0,False
qlzztd,"> this isn’t the expected package so it must be 

Thanks for the explanation, but my question is still not answered. I kind of understand the F.S.M. shown...but i can't grasp whether it's bulleproof or not...( and I won't test all possible scenarios, I'm not a masochist. I ran a few and it seemed okay, but something bothers me with it. )  


Is that bit sufficient for error control? Is it bulletproof? ( considering there are no lost packages )",hj6srb8,t1_hj6pd09,1635967918.0,True
qlzztd,"I’m not sure what you mean by error control, but if you mean corrupted packages, then that is addressed in rdt2.0 by the use of checksums and ACK/NAKs. Rdt 2.1 builds on this by adding the extra bit (this is only meant for handling of corrupted ACK/NAKs as everything done prior to this in rdt 2.0 already handles other errors - aside from package loss)

Simply put though, if we assume there isn’t package loss this is enough for reliable data transfer. I don’t have a proof for this but you may convince yourself by walking through all possible scenarios and seeing how it will work in those scenarios.",hj6tvvl,t1_hj6srb8,1635968356.0,False
qlzztd,"Yes by error control i mean I mean corrupted packages.  
We built on 2.0 in order to prevent situations like receiving a corrupted NAK and sending a package twice, therefore destroying the entire file to be transfered.   
No package loss is not enough to consider data transfer realiable. You also need to look out for packet corruption to consider a data transfer reliable. For example rdt2.0 is not reliable for the reason i mentioned above.",hj740pr,t1_hj6tvvl,1635972225.0,True
qlzztd,"I think you misinterpreted what I meant. When I said ""if we assume there isn’t package loss this is enough for reliable data transfer."" I meant ""if we assume there isn’t package loss *RDT 2.1* is enough for reliable data transfer"". By *this* I meant *RDT 2.1*. Sorry I should have been more clear.

I do want to clarify though, that the bit alone isn't what handles corrupted packages, it's the combination of the bit, the checksums, and the ACK/NAKs (all of which are included in RDT 2.1). RDT 2.0's only fault (assuming no package loss) is that it doesn't handle corrupted ACK/NAKs, RDT 2.1 fixes this.",hj76udu,t1_hj740pr,1635973311.0,False
qlvaqy,"Stuff with visuals can lead you down many roads. 
I wondered the same thing myself as a student.


Start with https://p5js.org/ . Go to the showcase or examples and you will see that in 3 lines you are able to make a shape move around the screen with mouse and keyboard.

In a couple more lines you can animate an image in tons of cool ways.

There's a channel called coding train on YouTube which will show you a beautiful world of creative coding with this language, from image to sound to interpreting nature via code and creative approaches to common CS problems. 

Now, if you want to get into games, you can of course do tens of unity tutorials, but i find that GameMaker Studio's drag and drop or coding tutorials (they're both just as easy) will within a span of 2 hours allow you to create a game and understand what is going on.

All games are, commonly, are loops that take input. You add conditions to that loop like character interactions, movement, dialog/event triggers. The game ""engine"" then processes it in standardized ways and usually displays some graphics in a way it decides. It really is just a simple program with a lot going on. If you start wondering about how the engine handles physics, or how it displays to the screen, you are asking questions about the engine itself and not how to make games. I'd say start with making games first and then you can journey onto the engine if you want

I'd start at these two places to get the instant satisfaction of making something along with understanding it!",hj6jq12,t3_qlvaqy,1635964449.0,False
qlvaqy,"Look into becoming familiar with an engine. I'm no expert and I'm sure someone here can give better advice but many engines support integrating your own c++ etc. Unity, and unreal are both great and powerful tools in the hands of anyone including a beginner.

I know it probably feels like you would forget many of the languages you've learned if you take on more and more information in CS but 80 percent or so of the information stays with you in your long term memory. You just have to reflect on them once and a while. Even if only passing  thoughts. It's reactivation the neurons responsible for storing that memory which will keep it fresh as when you first learned it.",hj5yt5l,t3_qlvaqy,1635956557.0,False
qlvaqy,"That depends on what exactly you want to make. If you want to make GUI desktop programs you'll have to learn a GUI library. There are a few that are cross platform that you could use like Qt, WxWidgets, and GTKmm but for the most natural UI you would want to use your OS's native GUI system. None of these options are easy. At the very least you'll need to know how to build and link against an external library and how to write code to its interface. 

Most GUI toolkits use event driven code(registering callbacks for events, etc.), state machines, and extensive multi-threading. It's a lot to learn at once. If all you've done so far is simple `cin` and `cout` command line stuff, all of that will have a steep learning curve in comparison but that is how real world GUI applications like MS Word, etc. are made. This is why a lot of companies cut corners and use electron and similar web frontend based tools despite their steep performance and memory usage penalties. 

If you want to make games you'll have to learn to use a game engine like Unreal, Unity, or Godot. The alternative would be hand writing everything yourself like the renderer (2D or 3D), audio engine, physics engine, input support, etc. which would be far too much pointless work. That said learning a game engine will still take time and effort but it's a much more reasonable goal than anything else I've mentioned.",hj613gj,t3_qlvaqy,1635957411.0,False
qlvaqy,"Forget games. Developers are plentiful. 

Get into VR! It's the future, and a couple of years of personal projects will make you a prime hire when you graduate.

If your school has a CS department, they'll likely have a VR lab. If not, launch one.",hj6b4rr,t3_qlvaqy,1635961187.0,False
qlvaqy,"I would recommend looking into some libraries like SFML or SDL. They allow for some pretty basic rendering tools that are fun to play around with.

If building things up from the basics isn't what you're interested in then game engines like unity offer tons of high level utilities for easily developing games.",hj6mz0e,t3_qlvaqy,1635965691.0,False
qlvaqy,Just download Android Studio; there are plenty of examples and tutorials built in! Plus there's a GUI for designing UI elements that makes things much more intuitive (imo).,hj75vs8,t3_qlvaqy,1635972937.0,False
qlvaqy,"Most people use a GUI framework, for cpp something like qt.

For games depending on the type of game most people go with a game engine, unless you want to tinker with a low level gl(graphics library) yourself.

I've heard of people using gl libs for softwares too, for instance I think the GUI for Blender was written in opengl. I suppose since it's a 3d modelling software the people working on it are familiar with it anyway but I don't think many people would.",hjaaxyg,t3_qlvaqy,1636036276.0,False
qlj8jr,There's a ruby library called Futurama; maybe that's it? I'm afraid that's all I've got.,hj3m6tj,t3_qlj8jr,1635907731.0,False
ql9rw5,Well written text which introduces math as necessary. I'd say just dive in and only catch up on something if necessary.,hj1xhw2,t3_ql9rw5,1635883156.0,False
ql9rw5,"This book is so extensive and covers algorithms in a practical and theoritical approaches, you will only need the math already included and well explained inside the book",hj2csne,t3_ql9rw5,1635888992.0,False
ql9rw5,"I really don't think there's a prerequisite needed for this book other than discrete math.

For that I would recommend Concrete Mathematics by Donald Knuth.

As for the book, I think Algorithms Design Manual by Skiena is far more readable than CLRS which is IMO more of an encyclopaedia than learning resource.",hj3xx5e,t3_ql9rw5,1635913339.0,False
ql9rw5,"Love Skiena's book too. Got the 2nd edition recently which was good enough for me and cheaper than the 3rd. Good hardback binding, typical Springer quality. As fir the content, it has implementations in C and I liked also the war stories sections that show practical applications for each algo.

I'd like to buy CLRS in the future as indeed it's considered the algos & ds Bible, and I think it's very good as a reference, if you want to dive in the details. I wouldn't study it from cover to cover.",hj41bqq,t1_hj3xx5e,1635915289.0,False
ql9rw5,I remember this book....,hj30517,t3_ql9rw5,1635898489.0,False
ql9rw5,"I think you need to practice to appreciate the amazing algorithm! Like why we need a heap? after a good program which dramatically reduce O(n) , you will agree who thought about it must be a genius! same for dsu or quick sort.",hj3hobd,t3_ql9rw5,1635905779.0,False
ql9rw5,Learn by doing. Don't understand an algorithm yet? Code it. Don't use a library that implements it; sit down and code it.,hj2hccb,t3_ql9rw5,1635890754.0,False
ql9rw5,"This CLRS book is indeed amazing but don't be discouraged if you prefer something else.  I use it as a reference.

As for a textbook I used ""Algorithms in C"", 3rd ed. by Sedgwick.  The author is one of Knuth's regular collaborators.  The 4th edition is significantly simplified, and I think it's ideal for self-starters or beginners in algorithms and data structures.

Skiena is amazing, but it's *about* algorithms, not really a standard algorithm textbook.  Likewise with Concrete Mathematics.  I have Knuth Vol. 1 and I flip through it occasionally, but again, it's more for inspiration.  I would rather read Concrete Mathematics for mathematical fundamentals and use CLRS or Sedgwick for the algorithms.  Skiena is also an essential book for me.

If I had to choose the smallest number of books, I would choose Skiena and Sedgwick 3rd ed. (the latter comes as two books) or Sedgwick 4th ed. (one book) if you're really new to this.

Everything you'll ever need to know is in the following:

* Sedgwick 3rd ed. (two books)
* Sedgwick 4th ed.
* CLRS
* Skiena latest ed.

There are many other great textbooks but you can't go wrong with any of the above.",hj50u83,t3_ql9rw5,1635942124.0,False
ql9rw5,Are you reading it cover to cover? I generally skim each chapter for uses and running times that way if I come across anything that might require that algorithm I know to open CLRS for a deeper dive. But I've never on depth read this book cover to cover.,hj2iare,t3_ql9rw5,1635891129.0,False
ql9rw5,"It's an amazing book, I live by it. Haven't cracked it in years, but back when I was studying CS, it helped me so much.

At some of my friends' workplaces, they call it the ""White Bible"", based on its color (Numerical Recipes is likewise the ""Red Bible"").

It's dense so don't try to get it all at once. I would *personally* recommend implementing every algorithm and data structure in there you really care to understand yourself. If you just need a taste, skim and don't feel bad about it.

The most important thing you can do with any algorithm in CS is to know that it exists. Anything beyond that you can work towards when the need arises.",hj2tnmf,t3_ql9rw5,1635895788.0,False
ql9rw5,"Best book ever, that's my feedback",hj1mrfn,t3_ql9rw5,1635879006.0,False
ql9rw5,"Maybe suppliment with MIT opencourseware 's algos. 
https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/

It's quite good and the content on eg. Max flow is very detailed and helpful",hj2oveo,t3_ql9rw5,1635893813.0,False
ql9rw5,"My advice is don't try to ""get through it faster"". Try to engage with it every day, even if you are only reading a couple pages per day. When learning anything, you want to think critically about the material and actually mull it over. After you've read a given passage, ask yourself what the main point of it was, try to do a kind of instant replay in your mind of the information you just consumed. Do the exercises after each chapter. It's a marathon, not a sprint.",hj5rx7m,t3_ql9rw5,1635953965.0,False
ql9rw5,"I'm pretty new to CS, and I've always heard that books are quite useless when it comes to learning computer science or programming in general?",hj6ga9d,t3_ql9rw5,1635963138.0,False
ql9rw5,"That's not true, for me at least... I think is very important to learn the theoretical part in order to master the practice, is always import to have references, they will give you a solid base for problem solving, and to make you understand why certain approachs for certain problems work.",hj6lv7i,t1_hj6ga9d,1635965271.0,True
ql9rw5,"No I definitely agree. Because otherwise there's always a huge disconnect in ones understanding 

However, since I've never heard anyone else but myself say that, I don't know if it's true or not.",hj94hig,t1_hj6lv7i,1636007890.0,False
qkkwwt,"If i is between n and n/2, the inner loop only runs once. If i is between n/2 and n/4, the inner loop runs twice etc. This means that the total amount of loops is (n/2) \* 1 + (n/4) \* 2 + (n/8) \* 3 + ... + (n/2\^log(n)) \* log(n) times. If I got it correct the total amount of loops will be about n \* 2, meaning that the complexity is indeed O(n).",hix5iz3,t3_qkkwwt,1635795285.0,False
qkkwwt,Wow this makes complete sense. Thank you.,hix6j8e,t1_hix5iz3,1635795692.0,True
qkkwwt,The real answer is…. This code is O(infinity) since your code will loop forever since 2 * 0 =0,hiy1ozw,t3_qkkwwt,1635808725.0,False
qkkwwt,"The best way of thinking about this is to work backwards.  With a range n, with have n/2 elements that are are >= n/2 so will iterate once, we have n/4 elements that are >= n/4 <= n/2. so will iterate twice.  We have n/8 elements that are >= n/8 <= n/4 and so will iterate 3 times etc.  So we get the sequence n/2 \* 2\* n/4 + 3\*n/8 + 4\*n/16 etc.  = n\* sum (i/2\^i). This turns out to be precisely 2, since its 1/2 + 1/4 + 1/8 ....  = 1 ,  + 1/4 + 1/8 + 1/16 ... = 1/2 + ...  = 1 + 1/2 + 1/4 + 1/8 ...  = 2",hiyqxmm,t3_qkkwwt,1635820126.0,False
qkkwwt,"The other answers are correct that it is O(n) I wanted to explicitly outline why your log(i) approach was wrong because it seems you, me, and several others were confused by this at first.

Tl;dr:
We aren't doing work equal to Σlog(i) but rather Σlog(n/i). This causes O(n) instead of O(n*log(n)). I provide a proof below showing it is O(n) without Stirling's formula.

Proof:
Σlog(i) for each i is wrong because we aren't doing log(i) units of work for each i. That would be how many times we can double 1 without exceeding i; However, in the function the units of work are based on how many times can we double i without exceeding n. These equations model this value (where x is the number of times we can double i to equal n).

`i*2^x=n ⟹ 2^x=n/i ⟹ x=log(n/i)`

To model the entire function we sum each iteration:

`Σlog(n/i)`

`= log(n/1) + log(n/2) + ... + log(n/n)`

`= n*log(n) - (log(1) + log(2) + ... + log(n))`

`= n*log(n)-log(n!)`

The proof that n\*log(n)-log(n!)∈O(n) can be done by induction (after spending too much time on it and hating yourself).

`Let g(n) = n*log(n)-log(n!)`

`Let h(n) = 2*n`

Claim to be proven:

`P(n): g(n) < h(n) is true for all integers n >= 2`

Base Case:

`P(2): 2*log(2)-log(2!) < 2*2 ⟹ 1 < 4 ⟹ P(2)=True`

Induction Step:

Let's define a function that represents the increase of g(n) to g(n+1). That is, we will define function gΔ(n) be the difference between g(n+1) and g(n).

`Let g(n) = n*log(n)-(log(n!))`

`Let g(n+1) = (n+1)log(n+1)-log((n+1)*n!)`

I did this bit by hand to be sure, but [wolfram alpha can confirm](https://www.wolframalpha.com/input/?i=%28n%2B1%29log_2%28n%2B1%29-log_2%28%28n%2B1%29*n%21%29-%28n*log_2%28n%29-%28log_2%28n%21%29%29%29) the following (assuming n is positive, which it must be as we only increase from the base case of 2):

`gΔ(n) = g(n+1) - g(n) = n*log(1+1/n)`

Doing the same for h(n) is trivial (it always increases by two):

`hΔ(n) = h(n+1) - h(n) = 2(n + 1) - 2n = 2`

The limit of gΔ(n) [turns out to be 1/ln(2)](https://www.wolframalpha.com/input/?i=limit+of+n*log_2%281%2B1%2Fn%29+as+n+approaches+infinity) as n approaches infinity, which implies gΔ(n) never exceeds 1/ln(2). Note 1/ln(2) < 2. Now we know if P(n) is true, P(n+1) must also be true because h(n) to h(n+1) is an increase by 2, while g(n) to g(n+1) cannot increase by more than 1/ln(2). It follows:
`P(n)=True ⟹ g(n) < h(n) ⟹ g(n) + n*log(1+1/n) < h(n) + 2 ⟹ g(n+1) < h(n+1) ⟹ P(n+1)`

P(n) ⟹ P(n+1) for all integers n>=2, which proves:

`g(n) = n*log(n)-(log(n!)) ∈ O(n)`

There are other ways to prove this (e.g. Stirling's formula) but I felt this is the most intuitive, albeit long winded, approach.",hj8h3aq,t3_qkkwwt,1635993288.0,False
qkkwwt,Wish Reddit will support LaTeX in some point...,hk95rz0,t1_hj8h3aq,1636666871.0,False
qkkwwt,"  
def  
f(n):  
    c = 0 -------------------- 1  
    for i in range(n):---- n +1  
        j = i ------------------ n  
        while j < n: -------- (O(logn) +O(log(n-1) + O(log(n-2)+…+1) + 1  
              j = 2\*j ---------- (O(logn) + O(log(n-1) + O(log(n-2)+…+1)  
              c = c + 1 ------- (O(logn) + O(log(n-1) + O(log(n-2)+…+1)   
   
T(n) = 3 + 2n  + ((O(logn)\*(O(logn)+1))/2) <= 3 + 2n + (O(logn))\^2   
Since there is c0 > 0 and n0 >= 0 such that:  
(T(n) <= 3 + 2n + (logn)\^2 <= n\*c0) for every n >= n0, 

T(n) belongs in O(n)  


In simpler words: T(n) is 3 + 2n + something kinda (logn)\^2,   
but we only care about the behavior of the time function and since (logn)\^2 < n  after some n we just ignore it and express T(n) as O(n)",hj6r3ba,t3_qkkwwt,1635967272.0,False
qkkwwt,"My explanation has a few flaws, but I won't edit it as reddit's edit is acting weird and messes up the whole thing...and I am too lazy to try and fix it :P   


I hope you get the general idea. If you need any further explanation ask me.",hj6s5xo,t1_hj6r3ba,1635967690.0,False
qkkwwt,"Because the main factor in its performance is how many of ""n"" you are iterating over.",hix3c8x,t3_qkkwwt,1635794399.0,False
qkkwwt,"I understand that n grows faster than log(n), but the complexity of the inner loop (logn?) is multiplied but the outer loop (n). As I understand it, if the loops were not embedded then you would have O(n) + log(n) which would be the case were the main factor (n) is what you use.",hix4kkl,t1_hix3c8x,1635794897.0,True
qkkwwt,"Heh, was this a trick question? I got home and took a closer look, and because your range starts at 0 you're going to get an infinite loop here. You can multiply 0 by 2 all day and it will still be less than n. Tell your professor that trick questions are lame.",hixpup5,t1_hix4kkl,1635803570.0,False
qkkwwt,"Haha your right. I’ll bring it up to him, I bet he intended to use range(1,n) on the quiz question.",hixszq6,t1_hixpup5,1635804911.0,True
qkkwwt,Thank you for posting it. I deserved those downvotes for leaping before looking. To make up for it I'll post a neat Kotlin example profiling that loop using 1-based ranges in a day or two. Might be insightful. Best of luck.,hixtutz,t1_hixszq6,1635805289.0,False
qkkwwt,"Alright, so I decided to run an experiment in order to defend my intuition that this would be O(n). Please correct me if I'm wrong, but the numbers seem to back up my case. Computational Complexity is not an exact science and a certain amount of ""common sense"" is involved in deciding which trends deserve to be considered in the final result. I was doubting my intuition here after receiving those well-deserved downvotes for being so hasty, but my tests seem to suggest I was right. I'm glad I took the time to take a closer look. 

The following example code is written in Kotlin and uses some experimental features which were suggested by my IDE, but this stuff translates to Python and any other language you are using:

    import kotlin.time.Duration
    import kotlin.time.ExperimentalTime
    import kotlin.time.measureTime

    @OptIn(ExperimentalTime::class)
    fun main() {
        // For testing this kind of thing, a custom Result class is handy. We're going to use a list of them:
        val results = mutableListOf<Result>()

        // We're going to do 9 iterations, which should cover up to around a billion:
        val numTests = 9

        // We're going to start with 10 and increase by a factor of 10 every test:
        var nLimit = 10
        repeat (numTests) {
            results.add(
                f(1..nLimit)
            )
            nLimit *= 10
        }

        // We'll cap it off by checking each result:
        results.forEach {
            println(it)
        }

        /*
            Conclusion: Although the amount of time used is vanishingly small and will vary from test to test, the
                trend is clear when you view the number of iterations in the inner loop. As n increases by a factor of
                ten, the number of inner iterations also increases by about a factor of about 10.
                This is experimentally true, which makes it about 0(n) in my amateur opinion.

            Result(n=10, time=4.927005ms, innerIterations=16)
            Result(n=100, time=56.142us, innerIterations=194)
            Result(n=1000, time=488.659us, innerIterations=1990)
            Result(n=10000, time=3.690477ms, innerIterations=19990)
            Result(n=100000, time=5.846741ms, innerIterations=199988)
            Result(n=1000000, time=10.475195ms, innerIterations=1999986)
            Result(n=10000000, time=24.845879ms, innerIterations=19999984)
            Result(n=100000000, time=237.521900ms, innerIterations=199999979)
            Result(n=1000000000, time=2.323592679s, innerIterations=1999999977)
         */
    }

    @OptIn(ExperimentalTime::class)
    data class Result(
        val n: Int,
        val time: Duration,
        val innerIterations: Int,
    )

    @OptIn(ExperimentalTime::class)
    fun f(n: IntRange): Result {
        if (n.first == 0)
            error(""0-based ranges will result in an infinite loop here."")

        var innerIterations = 0

        val time = measureTime {
            for (i in n) {
                var j = i
                while (j < n.last) {
                    j *= 2
                    innerIterations++
                }
            }
        }

        return Result(n.last, time, innerIterations)
    }",hj14vpq,t1_hix4kkl,1635871980.0,False
qkkwwt,"Hey man, thanks for doing the nitty gritty analysis, to see that it is in fact O(n)! Idk why you got downvoted but what I meant by my comment was that “the n is is the dominant factor” didn’t make intuitive sense to me, not that you were wrong about it being O(n). But I understand it now, the inner loop starts at O(logn) for first few iterations but quickly becomes essentially constant time, so the outer loop does in fact win out.",hj1i97b,t1_hj14vpq,1635877275.0,True
qkkwwt,"I would not have known for sure until I tested, so my intuition was suspect from the start and I probably deserved the down votes for jumping the gun. I certainly don't mind them in this case.

Thanks again for posting it! Was a fun excuse to play with Kotlin on my new setup.

Edit: More: I also probably should have explained that the increase in complexity was constant in proportion to n, no matter how big n is. The language I used in the down voted post isn't quite clear enough to pass muster in, say, an interview or class assignment. This is why down votes exist, though. Sometimes they help you clear up your style.",hj1nr0n,t1_hj1i97b,1635879384.0,False
qkkwwt,"The answer will be n(logn) I think. 
The for loop will run n times. 

For the while loop:

let's say n=64
now j=i means j will be 1 initially. 
For each iteration of while j is multiplied by 2 which means:
1 2 4 8 16...64  == 2^0 2^1 2^2....2^k.

2^k= n

k= log(n) 
 For each iteration of for loop while loop is executing. 
So total complexity is n*log(n)",hixftxk,t3_qkkwwt,1635799417.0,False
qkkwwt,The higher order operation is for I in range n which runs in O(n). Since j runs less than n we can omit it from our thinking. That’s the easiest way to see it. At a minimum u must iterate through all n once,hj0two2,t3_qkkwwt,1635867535.0,False
qkkwwt,"I’m not sure if I understand that reasoning exactly since a loop that runs n times with log(n) in the inner loop is O(nlog(n)), not n, even though the inner loop runs a factor slower.",hj0uezp,t1_hj0two2,1635867746.0,True
qkkwwt,U can think of it as running in log n but then u have to consider it goes log(n)+ log(n/2)+log(n/4)… in which case u end up with total of n anyways. It’s simpler to just consider the highest order being o(n) because we still gotta iterate through n times. So focusing on highest fact that we need minimum of n iterations will leave u with O(n). Technically it’s be correct to add up all the logs but it’s the same result as just considering the main for loop. Any for loop from i to n will leave u with O(n),hj0xdci,t1_hj0uezp,1635868958.0,False
qkkwwt,Not in the classical definition of O(n). O(n) is an upper bound. Just because something loops over n does not make it O(n). It makes it Ω(n).,hj7pd33,t1_hj0xdci,1635980918.0,False
qkkwwt,Yeah but most people don’t seem concerned with bounds other than top bound as it’s the limiting factor,hja401a,t1_hj7pd33,1636033257.0,False
qkkwwt,"My point is just looking at the first for loop isn't enough to deduce the upper bound.

You say ""Any for loop from i to n will leave u with O(n)"". 

Counterexample:

`for i in range(0,n):
    mergesort(n_elements)
`

This loop is not O(n) but it loops over n in the highest loop. It's O(n^2 log(n)).",hjc13za,t1_hja401a,1636060579.0,False
qkc8u6,"This was a really good watch, Thanks !",hivs8cv,t3_qkc8u6,1635774029.0,False
qkc8u6,Thank you for your support !,hiyehqu,t1_hivs8cv,1635814428.0,True
qkc8u6,"Great job!

Would you be interested in walking through the part where you went from CPU and data sheet to figuring out how to hook up memory and clock and so on?",hiw9wuq,t3_qkc8u6,1635782134.0,False
qkc8u6,"Thank you!

It's good to know that you would be interested in such video. I didn't really know how far I should in the explanation of the making of, mainly because there are already plenty of interesting  and well made videos about the subject on Youtube. I will consider it seriously, thanks for your feedback!",hj558ui,t1_hiw9wuq,1635944429.0,True
qkc8u6,👏👏,hivyg8b,t3_qkc8u6,1635777009.0,False
qkc8u6,Thanks !,hiyejf7,t1_hivyg8b,1635814448.0,True
qkc8u6,Nice!,hivq6is,t3_qkc8u6,1635772967.0,False
qkc8u6,Thank you!,hivx4qb,t1_hivq6is,1635776410.0,True
qkc8u6,"The first assembly language I learned was Z80 on a TRS-80.  
Compared to today's CPUs, it was a joy to program.",hiybwud,t3_qkc8u6,1635813265.0,False
qkc8u6,"Exactly, this was actually what motivated me starting this project. Today, even if you get an MCU board such ESP32 or Arduino, you get to develop in C or C++. Even if one chooses assembly, he'd have to learn Xtensa or AVR assembly which are far more complex than Z80 assembly.

So the goal of this project is to be able to learn Z80 assembly and do nice things with it, including graphics!😄",hiyfft4,t1_hiybwud,1635814856.0,True
qk9vxt,"Good article. The last three points especially 
* Hard Realtime
* HW-SW-codesign
* Safety-criticality",hiw0vcj,t3_qk9vxt,1635778101.0,False
qk5g35,"fast.ai

kaggle",hiwawdj,t3_qk5g35,1635782566.0,False
qjz6f7,"I can't think of anything that fits those criteria exactly, but here are some good channels that come to mind (in order of increasing specialization):

Tom Scott has a [really good series called ""The Basics""](https://www.youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha) which teaches general concepts to keep in mind.

I hope you've heard of [Kurzgesagt](https://www.youtube.com/user/Kurzgesagt), which has amazing science videos. (I'd check them out, even though they're not strictly math or computer science)

[Junferno](https://www.youtube.com/channel/UCRb6Mw3fJ6OFzp-cB9X29aA) covers random computer science related subjects with a healthy dose of deadpan humor. You might enjoy [this video](https://youtu.be/MBQvN03i4-4).

[Fireship](https://www.youtube.com/c/Fireship) has a great series called 100 seconds of code, where he goes through basic concepts quickly. Take a look through [this playlist](https://www.youtube.com/playlist?list=PL0vfts4VzfNiI1BsIK5u7LpPaIDKMJIDN) to see if any of the topics are interesting to you.

[AlphaPhoenix](https://www.youtube.com/channel/UCCWeRTgd79JL0ilH0ZywSJA) is a great channel about science, and computer science. [I liked this video](https://youtu.be/TOpBcfbAgPg).

[HoneyPot](https://www.youtube.com/channel/UCsUalyRg43M8D60mtHe6YcA) produces pretty good documentaries for developers.

[Two Minute Papers](https://www.youtube.com/c/KárolyZsolnai) covers real world math papers in a fun and entertaining way. A lot of his videos relate to AI.

[Sebastian Lague](https://www.youtube.com/c/SebastianLague) has some amazing videos about game development, which often involve more math than you think. I particularly recommend his [coding adventure series](https://www.youtube.com/playlist?list=PLFt_AvWsXl0ehjAfLFsp1PGaatzAwo0uK).

[freecodecamp.org](https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ) has a pretty good youtube channel that does a lot of deep dives into particular subjects.

[Ben Eater](https://www.youtube.com/c/BenEater) has really good videos about low-level computing. If you are interested in learning about how computers *really* work, this is a good channel for you.

[LiveOverflow](https://www.youtube.com/c/LiveOverflow) has some amazing educational videos about cybersecurity, for more intermediate audiences. He's not directly related to Math, but his content is so good I can't not mention it for people interested in cybersecurity.

I hope that helps!

*Edit*: [mCoding](https://www.youtube.com/channel/UCaiL2GDNpLYH6Wokkk1VNcg) is also great if you're interested in Python or C++.

[jdh](https://www.youtube.com/c/jdhvideo/) does a lot with low-level programming, and has interesting videos such as [making his own operating system](https://youtu.be/FaILnmUYS_U) and [writing minecraft in C](https://youtu.be/4O0_-1NaWnY).",hitjm93,t3_qjz6f7,1635720708.0,False
qjz6f7,this is a top tier list 👌,hivomm8,t1_hitjm93,1635772129.0,False
qjz6f7,thanks! I hope it justifies all the time I spent watching youtube :),hixe8x0,t1_hivomm8,1635798780.0,False
qjz6f7,You missed Harvard CS50,hivz5dj,t1_hitjm93,1635777326.0,False
qjz6f7,Do you know any that go other logic gates and truth tables? I recent failed a test because I couldn’t make a truth table that fit a certain criteria. Also how the CPU and memory work? Like how does the computer save stuff in memory or how it moves thing to different memory cells?,hiv17hg,t1_hitjm93,1635753754.0,False
qjz6f7,[deleted],hivc3nr,t1_hiv17hg,1635763833.0,False
qjz6f7,Thanks dude I appreciate it!,hix7p3q,t1_hivc3nr,1635796163.0,False
qjz6f7,"I think the closest thing to what you're looking for is probably Computerphile. They're similar to Veratisium, but for computer science.",hitskus,t3_qjz6f7,1635725274.0,False
qjz6f7,Even more similar to Numperphile than Veritasium.,hiv4kzw,t1_hitskus,1635757078.0,False
qjz6f7,"People have already mentioned the popular ones you’re looking for but also [CS50](https://youtube.com/c/cs50) is a good starting point … after the CS-crash course series already recommended.

>![random related playlist](https://youtube.com/playlist?list=PLkh6icMoy1Q46-gs9D3Bo8zLKDya8UBFr)!<",hiusnun,t3_qjz6f7,1635745917.0,False
qjz6f7,"u/BimphyRedixler already has the best answer. Especially seconding Two Minute Papers, Ben Eater, and Sebastian Lague. Saving a couple that I hadn't heard of before. A couple more to add:

* [What's a Creel?](https://www.youtube.com/c/WhatsACreel) has some fun computing experiments. Some of the x86-64 assembly stuff might be of interest.
* [Coding Secrets](https://www.youtube.com/c/CodingSecrets) is mostly about the fascinating lost art of squeezing entire games into the tiny ROM's available to 90's game consoles. Occasionally some neat low-level tricks covered.",hiubtrl,t3_qjz6f7,1635735250.0,False
qjz6f7,"Thanks! I watch way too much YouTube :)

I haven't heard of What's a Creel before. Thanks for sharing!",hiukzvx,t1_hiubtrl,1635740382.0,False
qjz6f7,The vlog brothers “Crash Course” has a computer science course that is more fun history based then technical explanation,hitvvp8,t3_qjz6f7,1635726986.0,False
qjz6f7,"My top 3 picks:

\- Hussein Nasser for backend engineering topic  
\- freeCodeCamp.org for software engineering (they cover many things, from competitive programming to front end dev) topics  
\- LifeOverflow for cybersecurity topic",hiuab4h,t3_qjz6f7,1635734454.0,False
qjz6f7,"Imo the best one is [reducible](https://www.youtube.com/c/Reducible),  is literally the 3b1b of computer science, his videos are awesome and the topics are so interesting.

I also love [computerPhile](https://www.youtube.com/user/Computerphile), probably is the most famous of this type of channels.

And finally there is a guy who used to make really great animations of compSci videos, the problem is that it seems that he is no making videos anymore, but well, you should check it out [Spaning tree](https://www.youtube.com/c/SpanningTree/videos).

Hope to be useful, have fun!",hitz21t,t3_qjz6f7,1635728603.0,False
qjz6f7,"The coding train is a pretty good channel that does mostly p5 js but he's great at explaining what he's doing.

I watched him all throughout college and I think it helped me a bit",hiu9iq0,t3_qjz6f7,1635734041.0,False
qjz6f7,Creel,hiuzrjg,t3_qjz6f7,1635752348.0,False
qjz6f7,"You could try [Sentdex](https://youtube.com/c/sentdex), he covers mostly Python, but has some great full walkthrough for beginners. 

I used his Tkinter guide to help me finish my pythons course at college (his videos taught me more than that $3k class)",hiwwrvk,t3_qjz6f7,1635791697.0,False
qjz6f7,I enjoy watching Eric Demaine’s lectures from MIT opencourseware on YouTube! Check it out,hix1f5c,t3_qjz6f7,1635793608.0,False
qjz6f7,Ex google tech lead and millionaire,hiu7g92,t3_qjz6f7,1635732949.0,False
qjz6f7,"Seems like you aren't updated well enough, it is:

""Ex-google Ex-facebook multi-millionaire techlead""",hivbkb8,t1_hiu7g92,1635763406.0,False
qjz6f7,"The only thing i can really recommend you from my experience, don’t watch youtube to learn something, educational platforms with courses (paid or even free) or books will be way more useful.

If you really want to use youtube for what ever reason, there are probably talks by known people in the community you are wanting to learn something in, try to stick to them.

There are great books on coding paradigms, good practises and efficient ways to program, i d recommend you those


This advise may seem harsh, but as someone who mainly watched youtube to learn programming and  stuff  related to programming, i d say that even if it was fun, i wouldn’t do it again.
After switching from youtube to books and text courses / professional courses, i was amazed by the difference of quality",hitd4op,t3_qjz6f7,1635717557.0,False
qjz6f7,"Great comment! I started off programming by learning it from YouTube and now I never watch YouTube except the people talking about their programming experiences, etc.",hivbnvo,t1_hitd4op,1635763485.0,False
qjz6f7,same,hivca8n,t1_hivbnvo,1635763976.0,False
qjz6f7,[deleted],hithqv0,t1_hitd4op,1635719792.0,False
qjz6f7,depends on what you want to learn,hithurm,t1_hithqv0,1635719845.0,False
qjz6f7,"I recommend Pluralsight for online courses, it includes video, tons of code examples and slides too. I also like their way of doing the video transcripts, you click on the text and it jumps to that section of video.   They also include short tests at the end, then give you the links to that section of video and that you did not learn on the first pass.  
They have high quality lecturers, tons of fundamental concept classes and also cutting edge software.
Udemy is ok, but not as pro as Pluralsight.",hiwrxc1,t1_hithqv0,1635789703.0,False
qjz6f7,skip courses and just buy books to read and do problems from. You will retain so much more.,hiup7gd,t1_hithqv0,1635743283.0,False
qjz6f7,Not at all some courses are really great,hiw33cy,t1_hiup7gd,1635779115.0,False
qjz6f7,"Harvard’s CS50 is a great intro class but there is a reason college is not just lectures. 

Cal Berkeley’s CS61A, and CS61B courses can be found online which give you access to lectures, books, notes, homework, labs, and past exams. Spending the time to write notes from the lectures and doing the homework’s were what taught me the most. Anything is is just superficial. As soon as you experience a problem you won’t know how to apply critical thinking to it. This is why tutorials fail to truly teach you.",hiwmkuj,t1_hiw33cy,1635787510.0,False
qjz6f7,"I don’t know what you experienced, but i can tell you that there are great in depth courses which even supply you with practise exercises and direkt help systems on dir example udemy.com for like 15$",hiwv4zo,t1_hiwmkuj,1635791025.0,False
qjz6f7,"Let me ask a question so I don’t assume here. Do you have an engineering college degree?

Is so you would tend to agree with me that tutorials, udemy, and coursera are lacking in comparison to actual college classes. The aforementioned could be helpful for refreshers on specific tools like git or algos like binary trees but unless you spent time fundamentally understanding trees the udemy class on trees won’t leave you with lasting understanding. 

Furthermore computer science is much more than just programming. Learning about networks, distributed systems, computer architecture, and OS are a few of the fundamentals that will help you understand what happens when you use a language. Most udemy type classes don’t teach this but instead they teach you how to create a list in Python. What’s the point if you don’t understand that python is created on top of C which uses linked lists to on the fly create a Python list that you can add elements to. This is one reason why Python lists are slower than C lists.",hiwx2n2,t1_hiwv4zo,1635791821.0,False
qjz6f7,"No i did not visit an engineering college.

You can not compare a course with around 10-15h or content worth 15$ to an collage class which has hundreds of hours (if you include studying for exams and homework) and which you pay thousand of dollars for.
Most courses are not meant to be a “learn all about this theme that exists in detail” they are more like another resource you can use to learn about the topic you are studying and in my opinion they are way more efficient if you want to study something, by the facts that these are people who scripted the video content they deliver and they try not to waste time (which is very different from schools and colleges (this is what i experienced)).
I also enjoy the fact that you can replay, change the playback speed and choose when you are studying, this gives me the opportunity to manage my schedule better and gets me generally more motivated.

If we are talking about the quality of these courses, i can tell you that the instructors are skilled and oftentimes known people which put really much (i m talking about oftentimes 6-9 months) time into it and script it incredibly well to make the most efficient content they can, they mostly pay a lot of attention to what they say and you can see exactly what they are going to teach you, in which order and in which way. I have some friends who visited a cs university and reported me the way in which many teachers teach really old and unrecommended ways of c++, because they learned like this in their university but aren’t up to date anymore (of course this isn’t the case everywhere and there are also great teachers out there, but i ve heard about it a lot), this is something you will not experience in courses because you can easily see detailed recommendations, exactly what he teaches you and in which way as i ve already said.

Some courses are really great and teach you valuable things, it would be a shame to miss them about, i can recommend searching for good rated courses about topics you want to learn about and use them as a resources (even if it oftentimes do not go into great detail, they are an awesome way to get started)",hixdxmb,t1_hiwx2n2,1635798653.0,False
qjz6f7,"Could you recommend any book to learn programming ? It doesn't matter the language, i want the concepts",hitxszv,t1_hitd4op,1635727953.0,False
qjz6f7,"What are you interested in; Algorithms, Software Architecture? It's not like programming is a monolithic subject.

And what kind of search requests failed you? To be honest, it's quite easy to get started with programming in all kinds of ways now. The [Humble Book Bundle](https://www.humblebundle.com/books) alone hast 3 different bundles this month that get you started with low level related coding stuff.

^(Edit: fixed Markdown)",hiv8d9g,t1_hitxszv,1635760701.0,False
qjz6f7,"Take it from me, there are no ""Good"" channels. 

You can pick any one of them, start coding yourself, and improve. Oh and you're not gonna understand the 'deep understandings' of the language without grinding it for a long time on it yourself. 

I would strongly advise you to just start coding. Waiting to find that perfect channel is procrastination.",hiuivmx,t3_qjz6f7,1635739081.0,False
qjz6f7,"It doesn't seem to me this person is waiting for the perfect channel. They seem to just be looking for good recommendations while they do actual work

I agree on the importance of getting your hands dirty though.",hiul5bf,t1_hiuivmx,1635740476.0,False
qjz6f7,">Take it from me, there are no ""Good"" channels.

Thats why I created my own channel 😎",hivbiev,t1_hiuivmx,1635763365.0,False
qjz6f7,"I really enjoy back to back swe

You can find his channel [here](https://youtube.com/c/BackToBackSWE)",hiut3ei,t3_qjz6f7,1635746283.0,False
qjz6f7,https://youtube.com/user/lefticus1 C++ Weekly,hivvvbu,t3_qjz6f7,1635775815.0,False
qjz6f7,"You can start with the computer fundamentals 

[https://www.youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha](https://www.youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha)

Regarding computer basics start with any computer language you should start with c/c++

[https://youtube.com/playlist?list=PL2\_aWCzGMAwLSqGsERZGXGkA5AfMhcknE](https://youtube.com/playlist?list=PL2_aWCzGMAwLSqGsERZGXGkA5AfMhcknE)",hiw2nt9,t3_qjz6f7,1635778917.0,False
qjz6f7,Continuous Delivery from Dave Farley is very interesting.,hiw96fl,t3_qjz6f7,1635781818.0,False
qjz6f7,"For theory stuff I am yet to find a good channel, if you know any let me know, for programming I recommend Fireship (focused on web mainly) and Brad Traversy (Has a plethora of stuff and also hosts guests on his channel for languages that he has no or little experience in, also, he's a really good guy all around)",hiw9lmq,t3_qjz6f7,1635782000.0,False
qjz6f7,"""Thenewboston"" has some great videos, carried me through CS classes that were in c++ and explains those coding concepts well. This is more implementive knowledge though. In CS you have to learn the theory and the implementation, but I think thenewboston is a fun and easy way to start!",hiy4r6i,t3_qjz6f7,1635810081.0,False
qjz6f7,Find Corey Schaeffer if Python is your thing and Python should be your thing.,hitd73u,t3_qjz6f7,1635717590.0,False
qjz6f7,why should python be your thing?,hitely0,t1_hitd73u,1635718265.0,False
qjz6f7,Good question!,hitfwpg,t1_hitely0,1635718894.0,False
qjz6f7,"If you are not fully committed to learning how to code, then choose Python but there are many shortcuts and libraries that does all the work for you without you having to think about how you should solve the problem by yourself.

Obviously, if you are experienced and you know how to code, then using Python is great because you can use all those tools that the language gives you but if you are starting out new, I don't recommend starting from Python because it's not a language that forces you to think.

If you are starting from Python and you are a beginner, then learn about computer memory as well. Mainly about how main memory works and how different kinds of data are stored, the stack and the heap.

Say if you start learning C and you are a total beginner, then the learning curve is very high, there is a lot to learn; not just about the language but you will also learn about how memory works. However, you will get better in problem solving when using a language like C since there aren't a lot of tools/libraries/shortcuts that C gives you to solve problems. So, it forces you to build those tools in a way, which definitely improves your critical thinking.

So, here is my advice. If you are not sure whether to learn coding or not, try Python first. Learn a bit of Python and see if it interests you. If you are really serious about coding and you want to be a good at problem solving, then learn either C/C++ or Java. Java is a great starting language as well.",hitncam,t1_hitely0,1635722582.0,False
qjz6f7,"I agree, but this doesn’t clear up that the comment creator said    that python should be your thing",hitocd4,t1_hitncam,1635723078.0,False
qjz6f7,"If one is new and wants to get their feet wet then Python is a great starter. You can make incredible progress learning the language and structure of programs in very little time. Easy to write and read, low barrier to entry etc...",hitqchc,t1_hitocd4,1635724104.0,False
qjz6f7,"I agree and that is what I said as well; Python is the way to go if you have no clue what programming/coding is and you simply want to see if it is for you.

And yes, you can definitely make a lot of progress by understanding about the basics of programming such as loops/conditional statements/functions/classes/recursion and so on.

But when comparing to languages such as C/C++/Java, Python is not a language that forces you to understand why something works and how something works. Knowing how and why things works can significantly help you to write better code and also to think like a computer.

For example, in Python I can swap two variables like so:

    x, y = 1, 2

The above is very intuitive. x = 1 and y = 2. This is very easy for a beginner to understand but does he/she understand what is ""actually"" happening? This only teaches the beginner how to code but doesn't teach how to problem solve at all.

Now, let's try the same in C:

    int x; int y; int hold_x_for_me;
    hold_x_for_me = x; 
    x = y; 
    y = hold_x_for_me;

The above gives, x = 1 and y = 2. I know, you can do the same with Python but it's very easy to fall into the trap of using the shortcut because it's much easier to understand and because it doesn't force the beginner to learn why it works.

The above is a bit more complex for a beginner but now the beginner can at least try to understand why the hold\_x\_for\_me is needed. It forces he/she to think why this extra variable is needed.

It allows beginners to start thinking about computer memory, how data is stored, what variables really are (an abstraction to memory address) and these are very important concepts to learn when learning how to code as well. Because by learning such concepts, beginners tend to develop/improve their critical thinking and also improve to think more logically, which is what you need when coding.

As you can see, C has types as well and thus when declaring variables, the correct type must be used. This again forces beginners to learn about how different kinds of data are stored, which is again another important topic. But when using Python, it's very easy to just brush it off.

BUT, you can still learn Python and also learn all the other important concepts I mentioned above. When I learned Python at Uni, we did also learn about how things and why things worked (what happens under the hood) and so it was very useful to learn with Python.

So, if you are a self learner/in the path of self teaching, then make sure you learn what happens under the hood as well (computer memory, namespaces, scoping, data types, how things are stored) if you are going to learn Python.

Even if you do choose to learn C/C++/Java, you may learn what happens under the hood as well depending on how great the resource (book/video) is and so definitely do your research as well.

I will name few important concepts any beginner should be learning when learning how to code:

\- Namespaces  
\- Scoping  
\- Mutability  
\- Data types  
\- Computer memory (main memory/stack/heap in particular)",hitzm0f,t1_hitqchc,1635728889.0,False
qjz6f7,Well for one it is easy to learn. My personal thought is that it's the easiest.,hitpyn1,t1_hitely0,1635723908.0,False
qjz6f7,"Python handicaps a person if they want to learn more powerful languages later.  The dynamic types and and weird class/method structure is so far removed from what the norm is.  Python is good to learn first I'd that's the only thing you're ever going to learn.  But for all other cases, youd be better served learning some kind of OOP first.",hitwro3,t1_hitpyn1,1635727429.0,False
qjz6f7,Well we can agree to disagree :) This is my recommendation and I can tell you with utmost confidence that learning Python does not hamper future learning of other languages. You simply build on it. That's how learning works.,hitygbj,t1_hitwro3,1635728290.0,False
qjz6f7,"While I agree that learning Python first and breaking out into more maintainable languages, I disagree with pushing towards a particular paradigm (like OOP or FP). My first language was Java and the boilerplate was super confusing for me at the time.",hiv8nxy,t1_hitwro3,1635760972.0,False
qjwzwd,"https://www.cs.dartmouth.edu/~ac/Teach/CS105-Winter05/Notes/kavathekar-scribe.pdf

Does this help (especially around 6.3)?

Also at the beginning of each iteration of the loop P is the previous augmenting path found in the previous iteration (or in the case of the first iteration P is defined before the loop, the first found augmenting path). This is essentially signifies if there is an augmenting path , and then how to augment the matching using P (flipping matched and unmatched edges) before repeating once more. Once there are no augmenting paths M is maximal.",hju9urb,t3_qjwzwd,1636399801.0,False
qjrkb9,"There are two types of compression. 

Lossless compression , is basically what you said, so for example if I had 5 a's in a row I'd do A5 instead of AAAAA. 

Then there's lossly compression it's a combination of the above and also throwing away ""unnecessary"" data. You technically loose information and quality but usually it's good enough.",hisd7pk,t3_qjrkb9,1635702210.0,False
qjrkb9,"Lossy compression would not be the same data- for example jpegs would lose quality. Lossless - I think so, but I'm no way an expert on this",hirt8sg,t3_qjrkb9,1635693567.0,False
qjrkb9,"Yes, compression is about removing redundant information from a dataset. I don't think whether it's lossy or not matters for purposes of definition, it just boils down to by what level of accuracy you're preserving the information in the set.",hiv4rzi,t3_qjrkb9,1635757268.0,False
qjntyw,Press X to doubt,hisgyty,t3_qjntyw,1635703792.0,False
qjntyw,Math is wrong. It is 51 thousand times faster.,hj3ttaw,t3_qjntyw,1635911209.0,False
qjntyw,"Says you... how did you come to that conclusion? Also even if that would be true, its still a huge step forward!",hj77djg,t1_hj3ttaw,1635973522.0,True
qjntyw,"Probably by reading the abstract of the [actual journal article](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.127.180501), which is linked to at the bottom of the science-news article:

> We estimate that the sampling task finished by Zuchongzhi in about 1.2 h will take the most powerful supercomputer at least 8 yr.

(1 year is about 8766 hours, or 8760 if you ignore leap years)

Also, the science-news article claims ""its calculation complexity is more than 1 million times higher than Google’s Sycamore processor"", but the journal article actually claims:

> The computational cost of the classical simulation of this task is estimated to be 2–3 orders of magnitude higher than the previous work on 53-qubit Sycamore processor",hj88rz6,t1_hj77djg,1635989455.0,False
qjntyw,"Feel free to correct me if I am wrong but -  
  
Fugaku can hit 442,010 TerraFLOPS/second and this new Chinese quantum computer dusts that. It means buisness. Since in 8 years Fugaku could do 1,858,563.648 YottaFLOPS (A YottaFlop is 1 followed by 24 zeros of very precise mathematical operations) in 8 years. This article is comparing a quantum computer to a super computer so naturally it is going to do well. 8 years vs 72 minutes is 5,110,000% faster though. So that would be a grand total of only - 94,972,602,412.8 YottaFLOPS, or ninety four billion septillion nine hundred and seventy two million sextillion six hundred and two thousand quintillion and four hundred and twelve point eight two FLOPS in 72 minutes.",hjxdzpj,t1_hj77djg,1636460167.0,False
qjk3eb,Run length encoding,hiqnif2,t3_qjk3eb,1635665412.0,False
qjk3eb,"^ what this person said

https://en.wikipedia.org/wiki/Run-length_encoding",hiqtmf1,t1_hiqnif2,1635670603.0,False
qjk3eb,"That's the technique applied here, correct. It's not technically the name of the data structure though.
There is no specific name for this structure but the technique is what's interesting.",hirf3h5,t1_hiqnif2,1635686632.0,False
qjk3eb,"There is, this data structure would just be a list",his2ofk,t1_hirf3h5,1635697681.0,False
qjk3eb,"Yeah, but that's not really helpful for OP, is it? He already stated that this is a list or array...

He obviously is interested in understanding if the way the initial data set is transformed into a run-length encoded data set has a particular name. Yes, a list or array are data structures, but that's a given.",hisddxr,t1_his2ofk,1635702311.0,False
qjk3eb,"I mean you said there’s no specific name for this type of structure, and I just wanted to point out that there is, and it’s just a simple list. The format of the data inside the list can take many different forms but that doesn’t change the data structure.",hisdu5m,t1_hisddxr,1635702486.0,False
qjk3eb,"Yeah sorry for the misunderstanding. I meant to say that it's in the end ""just"" a ""standard list/array"" (nothing special about this part), and that the special thing here is the run-length encoding.
That's it.",hiso9o3,t1_hisdu5m,1635706820.0,False
qjk3eb,"It’s cool, no problem. We agree 👍",hisodw2,t1_hiso9o3,1635706870.0,False
qjk3eb,Perfect. 😉,hisohx3,t1_hisodw2,1635706914.0,False
qjk3eb,"If you want to be pedantic about it, it’s run-length encoded data. You can store a linked list in an array, however it’s still referred to as a linked list. “Data structure”, in its name, literally means you’re talking about the structure of the data.",hisn7cp,t1_hirf3h5,1635706379.0,False
qjk3eb,"Cheers bro.

(Not sure why you would ever want to ""store a linked list in an array"" btw.)",hiso233,t1_hisn7cp,1635706733.0,False
qjk3eb,"Because when it comes down to it, all of memory (except in some exotic architectures) is an array. We structure data inside that array so that we can perform efficient operations on that data, hence “data structure”. I could just as easily say what you call “a list” is just linear data in array starting at a specific address. That’s not helpful, and isn’t what the op was asking for.  

You could extend this to any primitive. For example, let’s say our primitive is a DAG. If I organize the data as a binary tree. If I tell you it’s a binary tree are you going to come back and say “technically the name of the data structure is a DAG.”?

Edit: By the way, in answer to your parenthetical question, if my above explanation as to the architecture of memory as an array (individual data primitives addressed with contiguous integers) didn’t answer it, I’ll say it more explicitly. What you call a linked list is stored in memory, which is an array, so you already store it in an array, that’s why you’d want to do that “bro”.",hispng6,t1_hiso233,1635707390.0,False
qjk3eb,"Oh my. A few misconceptions right here.
If someone asks you what the difference between a linked list and an array is in a software dev interview, I hope you are not going to tell them that a linked list is stored in an array. That's definitely not what they will want to hear.
An array is usually stored contiguous in memory, not the integers. It's a random access data structure.
A linked list is usually not contiguous in memory since it traditionally needs to allow for constant time insertion and removal.

I would suggest you not to go around and throw random technical terms together and instead be precise in what you talk about. The former is generally not appreciated in the industry.",hiuhjpp,t1_hispng6,1635738301.0,False
qjk3eb,"\> Oh my. A few misconceptions right here.

Great, starting with a condescending attitude. You may find that's not ""appreciated in the industry"" either.

\> If someone asks you what the difference between a linked list and an array is in a software dev interview, I hope you are not going to tell them that a linked list is stored in an array. That's definitely not what they will want to hear.

Did you even read what I wrote? Do you understand computer architecture? How do you access memory when you code? It's addressed as a big one dimensional array. A memory address is just an index in to that array and tells you where data is stored. A linked list isn't some magical beast that lives outside of memory, it's implemented on top of memory. That big array. I was trying to make a point that every data structure is an abstraction of other data structures, though you seem to have missed that point, so perhaps I didn't do a very good job communicating it.

As someone who interviews software developers on a regular basis, I can tell you that I'd be impressed if someone actually talked about how data structures like linked lists were actually stored in memory. It would clue me in that they actually understand the underlying mechanism they are writing abstractions over. This is sadly lacking in many of today's software developers.

\> An array is usually stored contiguous in memory, not the integers.

I'm honestly unable to understand what point you're trying to make here. Yes, an array is not stored in ""the integers"" because that sentence doesn't even make any sense. If you're referring to my statement ""individual data primitives addressed with contiguous integers"", did you perhaps misunderstand? I can speculate on why you said what you said, but honestly I don't know, so I won't.

\> A linked list is usually not contiguous in memory since it traditionally needs to allow for constant time insertion and removal.

I'm not sure why you felt to need to point this out. I never said a linked list was stored in contiguous memory. I did talk about a ""list"", referring to one of your comments to GeronimoHero, but a ""list"" is not the same data structure as a ""linked list"".

\> I would suggest you not to go around and throw random technical terms together and instead be precise in what you talk about. The former is generally not appreciated in the industry.

First, I don't throw around random terms, and I don't reply when I don't know what I'm talking about. Please let me know what ""random terms"" I've mentioned in my replies, I'm genuinely interested.

Second, I'm not particularly worried about not being appreciated in the industry as I've very likely been developing software longer than you've been alive (perhaps you're older than 34, but I'm guessing younger based on this conversation) and don't seem to have any difficulty being appreciated. Except perhaps in this asinine thread.",hiuuten,t1_hiuhjpp,1635747774.0,False
qjk3eb,"Btw, I have an M.Sc. in computer science and have actively developed proprietary real-time physics simulation software in C++ as tech lead for more than 15 years.
So, I'd consider my input if I were you. But that's your choice.

Re ""integers"": it's interesting that you don't understand what I meant here given that I quoted you. Please reread what you wrote about ""contiguous integers"" and then my answer. This might help you understand what I meant to say.",hiw93ki,t1_hiuuten,1635781784.0,False
qjk3eb,"So I said ""individual data primitives addressed with contiguous integers"" (that's a copy and paste.) How does that not describe memory, or an array?

As you have an [M.Sc](https://M.Sc). in computer science, please define what a data structure is for me and how RLE is not a data structure.

Honestly, I'm not going to consider your input about what the industry appreciates.",hixoky5,t1_hiw93ki,1635803032.0,False
qjk3eb,Condescending attitude: speak for yourself. If you go back to your first comment you will see where it went sideways. You set the tone by calling me pedantic.,hiw8q49,t1_hiuuten,1635781620.0,False
qjk3eb,"Your first comment is literally ""It's not technically the name of the data structure though."" How is that not pedantic? I wasn't attempting to patronize you, I was pointing out an error in your technicality.",hixno48,t1_hiw8q49,1635802654.0,False
qjk3eb,"Data structure wise, it would just be a list where each entry would be an `(int, int)` pair.  
 
The name of the technique that this employs, which is what I'm guessing is what you're really interested in, is called run-length encoding.",hiqlx5u,t3_qjk3eb,1635664087.0,False
qjk3eb,"For a compression algorithm that uses something similar, take a look at the burrows-wheeler transform!",hir1kcy,t3_qjk3eb,1635677285.0,False
qjk3eb,"Thats where I am coming from. I went through the functional Pearl by Richard Bird, and was thinking about how I can further use the pre-processing BWT offers.",hir65hr,t1_hir1kcy,1635680795.0,True
qjk3eb,"Okay, so now this going deep on subject-specific algorithmics, but a nifty little piece of code is [cufflinks](https://www.nature.com/articles/nbt.1621).

I don't have a university-library account anymore (so can't access the paper proper), but if I remember correctly, they use the BWT to build libraries which they use to map millions of length-50-words unto a smaller library of ""words"" of the sizes ranging from low hundreds to thousands. 

Why that's something you need is very biochemistry, but I remember thinking the implementation is quite smart. This problem, transcriptome assembly, usually deals with datasets sizing into the terrabytes.",hir6snq,t1_hir65hr,1635681253.0,False
qjk3eb,Its called run length encoding,hiryr4r,t3_qjk3eb,1635695983.0,False
qjk3eb,"it is basically a list of tuple of the form \[(int, int)\]. But rather than having a list of tuple, much efficient data structure would be dictionary unless you don't want the data to be mutable. 

Python already provides a function for it from the collection module. You can use counter class from collections module, which takes a iterable as a argument and creates a dictionary, with unique elements as the key and the frequency of the elements as value.

for reference: [https://docs.python.org/3/library/collections.html#collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)",hiqupwp,t3_qjk3eb,1635671579.0,False
qjk3eb,"It’s called a bag, and a set is the unique values",hiscrre,t3_qjk3eb,1635702025.0,False
qjk3eb,List of tuples.,hiqseha,t3_qjk3eb,1635669555.0,False
qjk3eb,Map masquerading as an Array..jk,hiqlpgx,t3_qjk3eb,1635663913.0,False
qjk3eb,Mapping is not the purpose here. So that's incorrect.,hirfenq,t1_hiqlpgx,1635686813.0,False
qjk3eb,"Because the data structure still seems and array you perhaps want the name for similar algorithm not structure. 

I not remember the name used but if search compression algorithms, you will find the name easy. Probably the most popular technique.",hiqo06y,t3_qjk3eb,1635665828.0,False
qjk3eb,Run-length encoding.,hirfbm2,t1_hiqo06y,1635686763.0,False
qjk3eb,inverse look-and-say. jk i have no idea,hiqlkou,t3_qjk3eb,1635663805.0,False
qjk3eb,It could be easily achieved using Map (Hashmap).,hir6n50,t3_qjk3eb,1635681150.0,False
qjk3eb,Tupperware,hiurr8e,t3_qjk3eb,1635745194.0,False
qjk3eb,[deleted],hiqqhv9,t3_qjk3eb,1635667944.0,False
qjk3eb,A list of triples or a dictionary (?),hirfdh9,t3_qjk3eb,1635686794.0,False
qjk3eb,Hashtable or Hashmap?,his8327,t3_qjk3eb,1635700014.0,False
qjk2pi,"Start with the known inputs and desired outputs, resolve the steps needed to process the input into output.",hiqnrt9,t3_qjk2pi,1635665635.0,False
qjk2pi,Takes time and experience as everting else do not quit easily,hiqpd05,t3_qjk2pi,1635666977.0,False
qjk2pi,"Start with easier problems. All forms or problem solving are essentially about breaking down the problem into more manageable subproblems or steps, so it would probably help to get more practice with easier problems so that you know how to break apart the more difficult ones.",hiqyd4n,t3_qjk2pi,1635674753.0,False
qj9666,"I know you're just trying to ask a simple question, but the formatting of your post is like a fork in the eyes, you might get a better answer if you fix it. There are any number of references online about how to format Reddit posts.

Also, as a non-answer: insertionSort is more elegant in a functional language and easier to see that the invariant holds. But I suggest you draw a diagram of what's going on here, you can see how the invariant holds from doing that.",hiovq6k,t3_qj9666,1635628944.0,False
qj9666,"The hint on the inner loop is that it doesn't re-order anything existing in the array, so as long as it puts the new item in the right place, you've maintained the invariant.",hiow6v6,t1_hiovq6k,1635629153.0,False
qj9666,"Existing in which array? If you mean the array A\[1..j-1\] I think that it would reorder things in this array potentially, it could reorder elements in this array by sandwiching the item to insert between them.  So believe it re-orders things in the array?",hipu6vw,t1_hiow6v6,1635645867.0,True
qj9666,"No, there is no re-ordering in this array. Look at how the copying works: `A[i+1] = A[i]`. If you apply that to a contiguous sequence in the array in descending order, you will have that same contiguous sequence shifted ""up"" by one in the array. If you doubt this, run through this algorithm yourself using a short array on a piece of paper, with indices on each entry in the array.

There is no re-rodering here, only a copy of a contiguous sequence from one location to another in the same order.",hj1o965,t1_hipu6vw,1635879573.0,False
qj9666,"They point out in the text that they didn't want to get bogged down in describing the loop invariant of the while loop at the bottom of page 19. The actions of the while loop are the little arrows underneath the blocks in Figure 2.2. I think the invariant for the while loop is that the element is either out of order until it is in order? Basically the condition of the while loop. It's doing the work of insert that you would be doing in the following Haskell code, but I believe in the opposite direction. 

    insert :: Ord a => a -> [a] -> [a]
    insert x [] = x:[]
    insert x (y:ys) = if x < y
                      then x:y:ys
                      else y : insert x ys
    insertionSort :: Ord a => [a] -> [a]
    insertionSort [x] = [x]
    insertionSort (x:xs) = insert x (insertionSort xs)

Ultimately, if you go by what the book describes, you have five cards face down, you pick up the first card. Is that one card in sorted order?",hippk3w,t3_qj9666,1635643503.0,False
qj9666,"When you say ""the element is either out of order until it is in order"" what do you mean?",hipubia,t1_hippk3w,1635645933.0,True
qj9666,A[i] will be greater than key as long as it's not inserted in the correct order.,hiqyhqn,t1_hipubia,1635674864.0,False
qj9666,"Probably not the best way to go about proving the sortedness, but here's an inner loop invariant:

(1) A\[1..i\] is the same as ""A\[1..i\] before the loop started"",

(2) A\[i+2..j\] is the same as ""A\[i+1..j-1\] before the loop started"", and

(3) A\[i+1\]=A\[i+2\]>key.",hiqegti,t3_qj9666,1635658036.0,False
qj9666,Sorry maybe I'm missing something clear but how is A\[i+1\] = A\[i+2\] a loop invariant? Once i gets decremented in the loop wouldn't that change?,hiqf388,t1_hiqegti,1635658502.0,True
qj9666,"There's an A\[i+1\]=A\[i\]. Then i is decremented. So at the end of the loop that's A\[i+2\]=A\[i+1\].

But it's actually true that that's not a loop invariant. That's only true after the first run of the loop, not before it starts, so if you add that clause it's fine. (Not sure about this, because I'm not sure of the exact defn of loop invariant)

The A\[i+2\]>key is important to show that once you exit the loop, if you set A\[i+1\]=key, the whole list is the sorted version of the original A\[1..j\].

The A\[i+1\]=A\[i+2\] is useful for proving by induction that the loop invariant holds even after the next run of the loop.",hiqgjir,t1_hiqf388,1635659625.0,False
qj9666,"Just to be clear, the invariant isn't saying something like A\[35\]=A\[36\], because when i becomes 34 it could change A\[35\]. What the invariant says is ""When you're done with any number of runs of the loop and you look at the value of i at that time, then you'll also see that A\[i+1\]=A\[i+2\] at that time.""",hiqw84a,t1_hiqf388,1635672909.0,False
qiu1wc,"I agree that knowing CS will make it easier to learn any language. But when a senior dev is ""flexing"" about knowing this and that language, they might actually be flexing about knowing its ecosystem inside out and about being immediately productive on a large-scale project. When I read ""Java developer with 5 years of experience"", I do not read ""has implemented algorithms in Java for 5 years when they could have done it in pseudocode"". I read ""this guy has battlefield experience and can quickly get my Java project up and running"". 

You can learn CS for 5 years and it will 100% make it easier to understand all the quirks of Java. But being actually productive in a real-world Java project is something else entirely. Do you know Lombok? Maven? Maybe Gradle? Are you familiar with the JVM? Can you spot a performance problem using its monitoring tools and fix it in the corresponding code? Do you know the Reactive programming librairies? Spring? How familiar are you with JUnit? Etc. And if for some weird reason you had to switch this real-world project say from Java to Python, CS wouldn't help you very much.

English is a tool, but learning grammar won't make you good at English. And it will certainly not make you knowledgeable about the whole ecosystem of jokes, proverbs, expressions, synonyms, etc.

> And I won't even like to call you a programmer if you just know the programming language.

A programmer builds useful programs with a programming language. This kind of pointless statements just make you sound arrogant, and arrogance is probably the worst personality trait in a developer.",him6ind,t3_qiu1wc,1635577095.0,False
qiu1wc,">This kind of pointless statements just make you sound arrogant, and arrogance is probably the worst personality trait in a developer.

Yeah... I also think that I was arrogant in the post and I am sorry for that. Thinking to delete the post...",himg5r3,t1_him6ind,1635584935.0,True
qiu1wc,No worries. We all make blanket statements sometimes...,himgwe5,t1_himg5r3,1635585559.0,False
qiu1wc,"I think there are two sides to this. A programmer builds useful programs with a programming language. A good programmer builds useful programs that have a low count of lurking bugs, handle bad inputs gracefully, are easy to read and maintain, are easy to install/deploy, meet customer requirements, give clear error messages, have a clear version history... Those are essential skills for a programmer that are largely unrelated to the language you're working in.

When I'm hiring, I'm much more interested in someone convincing me they have those skills than that they've worked in language X for 5 years. If someone clearly has those non-language skills, I'm prepared to hire them with no experience in the relevant language at all, on the assumption that they'll pick it up quickly enough that it won't be an issue. Just asking someone to demonstrate having worked on a project in language X for Y time I view as very risky - it's surprisingly easy to bullshit your way into a junior role in a big organisation and then look busy while never actually achieving anything. You can get away with this for a long time.

A couple of years ago, there was a guy contracting at one of the companies I work with. He was hired because he appeared to have solid Go experience on his CV. When he arrived, it pretty quickly became obvious that he could sort of talk the talk and had seen people working in Go, but had never actually written anything useful. As soon as you started asking detailed questions, his bullshit turned to defensive bluster. As soon as you asked him to write some code, nothing happened. If you really pressed him to produce something, he would deliver something cobbled together from StackOverflow examples that didn't really address the requirement and was nearly up to the code standard you'd expect for a five minute demonstration that something was possible. Usually it involved some wildly inappropriate technology choice, because that's what the examples he found used and he didn't know how to adapt them (trying to replace sqlite with Elastic on an embedded system already struggling for memory is an example that sticks in the mind - he didn't understand why we wouldn't listen to his amazing ideas). Error handling, when it happened at all, consisted of `panic()`.

He was bagging £400 per day and as far as he was concerned, he'd made it. He had no awareness that a problem existed and no interest in improving - as far as he was concerned, everyone else was the problem. His contract still got extended three times and it was only after two years that we managed to convince HR that his contributions were entirely negative and to stop extending.

Six months later, his recruitment agent told me he'd spent the profits from his two years on a three month holiday in Croatia, then moved to London and used his experience with us to bag a gig paying nearly double what he'd earned with us. Said recruiter was surprised I didn't think this an excellent outcome.",him96ir,t1_him6ind,1635579217.0,False
qiu1wc,"“Learn how to program and you won’t ever need to learn a language”

Each language has its own paradigm, but these paradigms can all be learned. I had a class in university called ‘Structures of Programming languages’ where we learned just this. Eventually you learn that a language is sort of like a spice, its just a matter of picking the certain one you need for your dish or program.",hilypve,t3_qiu1wc,1635571384.0,False
qiu1wc,"I can only assume you've not had to write substantial amounts of JavaScript, lol.

I do think that once you limit the field to somewhat competence languages you are generally right. If you are good at programming, you'll be served just fine most general purpose languages out there.",hing7og,t3_qiu1wc,1635606927.0,False
qiu1wc,"Programming languages are like storage of computer, not as important as cpu but a computer can't work without it",hilxsel,t3_qiu1wc,1635570750.0,False
qiu1wc,"If you have read my entire post, I highlight that programming languages are not as important as the concepts. I never mentioned that you never need programming languages",himg0jy,t1_hilxsel,1635584814.0,True
qiu1wc,True..,hin5ua8,t3_qiu1wc,1635602078.0,False
qiu1wc,"You can learn a language in a night, maybe. Its library framework though? Hoo boy. Also you're not gonna write a website in Cobol. Pick the language that's appropriate for the job you are doing. That said, I generally agree.",hiqt9ck,t3_qiu1wc,1635670286.0,False
qiu1wc,"ya its sort of like if you know one you know them all. ""i know x programming languages"" sounds like something you only see on the internet though. i cant imagine someone unironically saying that in real life",him5qji,t3_qiu1wc,1635576483.0,False
qiq0jd,"The classic ""[Engineering V](https://www.researchgate.net/figure/The-Systems-Engineering-V-Diagram_fig2_320585817)""?

DO-178C?

ARP4754?",hilcc9b,t3_qiq0jd,1635559018.0,False
qidftd,"I don't think so, a function is either recursive or it is not.

There are functions that are interesting in their recursivity, or complex in their recursivity, but it is not that they are ""more recursive"" than the others, they are simply recursive functions.",hiijcb6,t3_qidftd,1635516686.0,False
qidftd,"It's not a gradient of less to more recursive, it's a truth- or falsehood; a function is either recursive (invokes itself) or is not",hiipll3,t3_qidftd,1635519263.0,False
qidftd,Well some recursive functions call themselves once while others call themselves more than once.,hiiyeev,t3_qidftd,1635522780.0,False
qidftd,"[Yes.](https://en.m.wikipedia.org/wiki/Ackermann_function)

Or, to put it less tongue-in-cheek: there is a concept of ""primitive recursive,"" and recursive functions that are *not* primitive can be considered, in a sense, *more recursive* than usual. They cannot be computed with loops whose upper bounds are known in advance.",hijxlc4,t3_qidftd,1635536541.0,False
qidftd,"There are non-primitive recursive functions (like Ackerman) which cannot be formulated using for loops, so I guess these can qualify as “more recursive”?",hjubwq8,t3_qidftd,1636400623.0,False
qibyr4,There are companies out there that are developing their own operating systems but it tends to be for very specific applications - aerospace and automotive industries for example. Outside of that I can't think of many good reasons why you would want to.,hii9cbr,t3_qibyr4,1635512171.0,False
qibyr4,"TempleOS helps you talk to God, for example",hiionee,t1_hii9cbr,1635518881.0,False
qibyr4,"Very, very special purpose",hiipf2a,t1_hiionee,1635519189.0,False
qibyr4,"A divine purpose, if you will",hikgtla,t1_hiipf2a,1635544287.0,False
qibyr4,"It did predict covid, after all.",hijvhxm,t1_hiionee,1635535729.0,False
qibyr4,"And it will predict the next one, the time of your death and the heat death of the entire universe. Terry wasn't messing around.",hiqwz2h,t1_hijvhxm,1635673562.0,False
qibyr4,"Ha!  You got me!  I immediately googled TempleOS.  Well played, sir.",hij27rw,t1_hiionee,1635524278.0,False
qibyr4,That’s definitely a rabbit hole that’s east to fall into haha 🕳,hij3lby,t1_hij27rw,1635524819.0,False
qibyr4,Real time centered,hiiebn4,t1_hii9cbr,1635514487.0,False
qibyr4,"There are plenty of custom proprietary operating systems being developed, but they are mostly used for specific industrial applications. For systems that you interact with on a daily basis, the reality is that the two dominant stacks that emerged in the mid 1990s (Unix derived systems and Windows NT) are good and adaptive enough to do everything.

What is also important, and in the case of Windows it's the ultimate reason for its success, is backwards compatibility. Being able to build on decades of supported software is not something you just throw away.

Looking at the Linux kernel, as well as Windows NT (well, you can't really look because it's closed source), replacing something like that is just not realistic at this point. It's impossible to estimate how many man years were spent developing those operating systems. Building something that can compete with these systems is an insane task.

The only reason to build something completely new would be to identify something the others cannot do and that everybody needs.",hiigdsw,t3_qibyr4,1635515405.0,False
qibyr4,"I think it has more to do with economics than anything else. With the exception of Linux, all of the companies that supported those OS are huge corporations that make money off of all kinds of software.

You correctly note that lots of innovation in the OS space since the 80s and 90s were adopted by the others. Indeed, there is a lot of cross pollination - you’ll notice this with the vendors competing for your workflows. How often have you seen one vendor pop up with some feature, then another copy it the next release cycle?

It’s because at some point in the past, the financiers and engineers are mega large corporations realized that the future was in a “platform”: a way that arbitrary improvements and extensions could be hooked onto an underlying system. I began to understand this through Microsoft Services in my work.

There’s a platform, which runs plug-ins, or services, which are small pieces of functionality that complete a specific task. A collection of these and their behavior + our interface with them is a “program”

So why does that matter? Simple. If I have a great platform that people almost *have* to build on for whatever reason, I essentially own a piece of every plugin that runs on it. I become the gatekeeper.

If I have the requisite skills on my teams I can build my own features, or copy what’s out there to keep up with the joneses.

If I have enough cash I can just buy the plugins that are most successful. Look at Apple and JAMF for a recent example.

The consequences of the apple epic ruling also come to light.

So I build a platform once and can get tons of value out of it. From an investment perspective it’s superior to one-off bespoke development.

Edit: clarity and a few more examples",hiim27n,t3_qibyr4,1635517817.0,False
qibyr4,Linux is no exception.  A very significant amount of the development that goes into Linux and the software around it is done by large corporations with for-profit motives.,hiipxua,t1_hiim27n,1635519401.0,False
qibyr4,"This is true, I wanted to take a general “economics of software development” at enterprise level, and ignored open-source for clarity.

Open-source behaves differently from a closed-source software venture, and I don’t understand it as well as closed-source development.",hiivjyo,t1_hiipxua,1635521667.0,False
qibyr4,"A very good insight into why open source became such a major thing is to drive down prices for your complements. 

A good read on the topic is https://www.gwern.net/Complement

This one article opened my eyes to a lot of questions concerning why some corp. did open source and what is their possible end goal.",hijthy5,t1_hiivjyo,1635534931.0,False
qibyr4,"*pulls a soapbox out of a bag of holding*

Selfish motives aside, a strong and vibrant culture of open source software development and free computing increases tech and math literacy while growing the traditional pool of people from which innovation occurs. I think it is clear that neither fully proprietary nor fully free points of view will prevail, because nature likes a good idea and widespread literacy, invention, and technological agency without limits is a very good idea. I personally get a kick out of the corporate fondness for open source software as a perfect example of selfishness breeding something universally pretty good. The very people doing the extending and embracing are often responsible for propagating ideas farther and wider than the hardliners ever would have on their own, by their own ideology, even as such hardliners and ""never corp"" types keep the other side from getting too carried away. Like all ideological concerns, it is a funny little spectrum with important details that get obscured by the level of rhetoric involved (which creates the gang warfare mentality so common in all ideological concerns). 

In short: I am pretty happy that big companies find it in their interest to support free software, while I am equally happy that there are people so extreme about it that the concept is fairly safe from total co-option. This is a thing that is good for generational tech literacy, and even national security when you extrapolate from that.

*puts soapbox back in bag of holding and runs off*",hik0xul,t1_hijthy5,1635537823.0,False
qibyr4,Agreed. Thx for sharing🙌,hikjuuh,t1_hik0xul,1635545578.0,False
qibyr4,Soapbox of speech craft,hilcjjd,t1_hik0xul,1635559118.0,False
qibyr4,Thank you for the link! I appreciate the opportunity to learn 😁,hijvens,t1_hijthy5,1635535692.0,False
qibyr4,"It is the same with graphic drivers and browsers, and to some extend also with networking protocolls, usb, hdmi/DP, filesystems, image formats, file compression and so on.

There are new inventions but they are not improved enought that it is worth it to switch. For example, did you know that there is an image format called JPEG2000, which improves many aspects of JPEG, but JPEG is just good enough.

In some cases like task sceduling, deadlock resolution or file compression it is already proven to be optimal solved. So there is no way to improve it.",hiiv004,t3_qibyr4,1635521447.0,False
qibyr4,"Casey Muratori talks about exactly this in his talk ""The Thirty Million Line Problem"" [https://www.youtube.com/watch?v=kZRE7HIO3vk](https://www.youtube.com/watch?v=kZRE7HIO3vk).

Basically we expect so much of an OS these days that it's almost imposible to make a brand new, competing OS.",hiigts3,t3_qibyr4,1635515596.0,False
qibyr4,"Same issue with the Web as a whole, if I remember correctly Chrome had more lines of code than Windows.",him01qk,t1_hiigts3,1635572297.0,False
qibyr4,Because you don’t need to reinvent the wheel?,hiia4vt,t3_qibyr4,1635512548.0,False
qibyr4,Work slows down at good-enough.,hikz0ce,t1_hiia4vt,1635552522.0,False
qibyr4,"Noooo this statement is such nonsense. If today's solutions are terrible, why wouldn't you want to make something better?",hiicwi7,t1_hiia4vt,1635513834.0,False
qibyr4,"That's not what ""don't reinvent the wheel"" means.  It means if there is already a *good* solution, it's a waste of the time, effort, and investment needed to make another solution, that is either no better, or only marginally so.

Of course you're right that if all current solutions are *bad* we should try to make something better.

Imo for the purposes of desktop/workstation OSes though, Linux and Windows are both pretty good these days, you'd have to come up with something reaaaaaly different and amazing to call current solutions ""terrible"" by comparison.",hiiexab,t1_hiicwi7,1635514759.0,False
qibyr4,"Yeah I guess that's a valid interpretation of the phrase.

The kernel of current OSes are good afaik, it's just the huge pile of bad user code they have on top what makes them bad.",hiif9z6,t1_hiiexab,1635514914.0,False
qibyr4,"The “bad code” is what, exactly?

There are plenty of cases where the solutions are often good enough to not warrant rewriting.  If someone decides a part of that system is completely unacceptable, there’s nothing getting in the way of them starting a new project.",hiilz6a,t1_hiif9z6,1635517782.0,False
qibyr4,"Not a windows user, eh?

Eta: you can't search the files on the same fucking hard drive it's installed on and return the right results?  That's 100% unacceptable lol, and yet windows is ubiquitous",hijlvnn,t1_hiilz6a,1635531895.0,False
qibyr4,"The OS isn't made to search files, you can use a program for that. The fact that Windows let's you easily develop and then run a program to find files is why it's a good (enough) OS",hijr4nm,t1_hijlvnn,1635533984.0,False
qibyr4,The operating system that is used by every person who doesn't know anything about programming or computers doesn't need to have file search capabilities?  What??,hik3pxb,t1_hijr4nm,1635538911.0,False
qibyr4,"I mean ideally it would have a better one, I'm not arguing that point - I'm just saying that isn't the primary point of the OS, so doesn't count as a reason to write a new OS since you can use a tool easily to fix that. That's like saying buy a new house because you don't like the colour of the walls instead of painting them.",hikemmk,t1_hik3pxb,1635543365.0,False
qibyr4,Is file search a top concern when picking an OS? If there was a windows alternate but within great file search how much would you pay to switch? Even as a new user would you pay more than windows? Even if somehow the new OS could run all windows software I don't think you would put a high $ amount on it,hijv85i,t1_hijlvnn,1635535620.0,False
qibyr4,"I mean yeah I know why people stick with it.  Its actually incredibly awful user experience, though. Remember idk, the whole thing where they had an illegal monopoly?  Lol come on.",hik3hqm,t1_hijv85i,1635538823.0,False
qibyr4,"I'm not saying it's perfect but the annoying parts aren't even close to getting me to give up the convenience never mind work environments where the cost of switching would be significant. No one is developing a windows alternative because people aren't mad enough to pay the cost of switching. You can have some feelings about how MS used to behave, that's fair. Doesn't change the fact that building a windows competitor doesn't make sense financially",hik5d1u,t1_hik3hqm,1635539564.0,False
qibyr4,"I agree but to me it's like, if the whole system is running on Microsoft, that's a public utility and should be high quality, extremely reliable, and open source",hik5uqp,t1_hik5d1u,1635539762.0,False
qibyr4,Thats true but without financial motivation it would be really difficult. Most of the things an open source OS would need to compete with windows aren't sexy or fun and you would struggle finding qualified people to work on it for free,hikbs3f,t1_hik5uqp,1635542179.0,False
qibyr4,"That’s a straw man.  The monopoly has nothing to do with the adequacy of the software or platform.

Windows does have file indexing built in, and it does work.  I don’t think the service is on by default.  There are other options like Everything, which also works.",hik7ly6,t1_hik3hqm,1635540468.0,False
qibyr4,Oh please do explain how it's a straw man! 😇,hikbiqj,t1_hik7ly6,1635542069.0,False
qibyr4,It was the second sentence,hile3o8,t1_hikbiqj,1635559894.0,False
qibyr4,"Well, Google is developing Fuchsia completely from scratch (supposedly to overcome inherent architectural limits of the Linux kernel), but as someone noted, it's a whole lotta work   
https://fuchsia.dev/",hikj07z,t3_qibyr4,1635545210.0,False
qibyr4,"iOS, Android (granted android is functionally a linux distro), and Microsoft has flung more pasta at the proverbial mobile OS wall than almost anyone. The majority of consumer computing is done on mobile devices today, so if there's money to be made in the production, maintenance, and control of an operating system it's there. There's also Chrome OS which dominates in education. Is your question maybe more in the realm of why there aren't new kernels being developed? It would be fair to say most operating systems today are built on top of some fork of either a windows or linux kernel (this includes apple operating systems), but the OS space itself seems healthy.",hiik24b,t3_qibyr4,1635516989.0,False
qibyr4,big and hard + not worth it,hiix1d0,t3_qibyr4,1635522250.0,False
qibyr4,"~Urbit seems to be an interesting project that builds a full stack OS from the ground up based on data ownership. They have some weird details, as you’ll begin to read, but it’s attracting some interest in the cypherpunk community.",hij2xay,t3_qibyr4,1635524554.0,False
qibyr4,I think computers are changing as a whole. Smart phones came out in the mid 2000s and now most of people's daily device is either a tablet or phone.,hiig9w2,t3_qibyr4,1635515357.0,False
qibyr4,"And yet they all run on an OS 'originally' written for mainframes, which migrated to desktops, network servers, and workstations before ending up in your pocket.",hiii27j,t1_hiig9w2,1635516135.0,False
qibyr4,There’s just no need for new OS’s for public use? There is already an OS for everything. Specific OS are still being made for specific industries.,hijltdf,t3_qibyr4,1635531871.0,False
qibyr4,"It turns out that linux is already very good and because it is an open source project, it is constantly getting better…so yeah.",hik1963,t3_qibyr4,1635537945.0,False
qibyr4,"If you have a long layover at an airport like I did a few weeks ago, you’d might be interested in this [Timothy Roscoe presentation](https://youtu.be/36myc8wQhLo). He talks about how OS’s are far more complex than academics realize/admit to. With SoCs becoming more common and transistors becoming denser and cheaper, the OS extends far beyond the kernel. This only tangentially answers your questions, but is an interesting topic nonetheless.",hikggin,t3_qibyr4,1635544135.0,False
qibyr4,"Because applications are the most economically motivated projects. An operating system doesn’t actually do anything - it enables other applications to solve a problem. An application that can be monetized has much more market pressure to exist. 

And, existing OS’s are very stable and serve their purpose pretty well so, there’s really no reason to completely reinvent the wheel with them.",hil83fb,t3_qibyr4,1635556939.0,False
qibyr4,"It's a hell of a lot easier to build upon a pre-existing coding language than make a new one from scratch. Hence why Linux and Unix distros are so popular. and why Javascript and shit like that are also equally popular. Though if you want an OS that was coded recently I mean, TempleOS?",hiiifv6,t3_qibyr4,1635516302.0,False
qibyr4,Did you search the subreddit for previous questions about this exact topic?,hiic9dk,t3_qibyr4,1635513545.0,False
qibyr4,Microkernel OSes proved to be slower and not much safer than monolithic OSes. The major selling point of old OS is the tools and applications that they provide. A new OS would have to have a major advantage to overcome the lack of software.,hij42t2,t3_qibyr4,1635525006.0,False
qibyr4,"There is no need for a new OS from scratch.

Is you think there is then why not make it?",hik808p,t3_qibyr4,1635540629.0,False
qibyr4,"Linux. 

For most purposes that an operating system would have been made from scratch twenty years ago or even forty years ago, Linux either fits the bill very well, or is good enough. 

Though keep in mind that in the few places you still some regular OS development, though as hardware gets more and more powerful, it become less worth the trouble. The two ends of the spectrum if you will, HPC and embedded. 20 years ago there were generally using OSs there either developed for that kind of system or even running a custom OS. Using Linux in either place makes development of software and systems insanely easier, and the only reason that Linux wasn't used before was it was too slow or had too much overhead. Nowadays, to the degree that overhead is still there, just throwing some more hardware at the problem fixes. 

In terms of advances, so much of it happens behind the scenes. The Linux kernel of today does think that weren't invented yet in the 90s. As long as the interface between userspace and the kernel stay the same, the kernel can be radically changed, and it has. One thing that effectively future proofed a lot things was the assumption that a lot of processes/threads were logically, if not actually, running at the same time. Doing so provide a huge amount of flexibility, even back when only servers and high-end systems had more than one cpu/core. This abstraction made going from systems with 1-2 cores to systems with 10-20 cores possible. It takes a lot of stuff to make that performant, but again it's all under the hood, hidden from the user, and from userspace developers.

Same thing in terms compilers, language implementation, and runtimes. Programming languages have changed a lot in the last 20-30 years, the best being  the only real software was written in C or C++, period. There were exceptions of course, but outside of very specific domains, there wasn't really a choice. There were a number of scripting languages  that people used, many  Perl (and PHP if you want to think about it at all), which traded a lot of performance and/or safety for ease of use. The web wasn't doing anything like it does today, so scripting languages were generally good enough. 

That's some to keep in mind in general. Doing anything from scratch takes a lot of resources, generally at least a couple orders of magnitude more, than using an existing solution. Anyone that cares about costs, which is basically everyone, is going work with something that is good enough if they can.",hiky16q,t3_qibyr4,1635552053.0,False
qibyr4,"For most hardware, the reasons are that current OSs are

* good enough for most existing hardware
* general enough to be adapted to new hardware
* too large to efficiently rewrite
   * Linux is estimated to cost several billion dollars, if one wanted to re-write it
   * Alternatively, you would need >50.000 person-years
* around long enough so that backwards compatibility becomes an issue
* in general of good quality, better than what most can produce on their ow
   * having been widely used for a long time helps removing bugs",hil2crv,t3_qibyr4,1635554125.0,False
qibyr4,Because the big 3 are very good,hilwdja,t3_qibyr4,1635569827.0,False
qibyr4,…and how is this analogous to crypto/web3,hilz7um,t3_qibyr4,1635571725.0,False
qibyr4,"Negative complexity entropy

We should have systems that boot up in milliseconds, but alas...",hiqrl0u,t3_qibyr4,1635668869.0,False
qi8v4v,"The data for all the game assets like levels, characters, 3d models, sounds and all that stuff is kept on the DVD.

What the save data does is stores your *progress* in the game. Things like which level and checkpoint you're on, numerical information like stats, your Player name. things like that which take relatively little data. This could all fit easily on the card.",hihqznk,t3_qi8v4v,1635500073.0,False
qi8v4v,[deleted],hihzvyu,t1_hihqznk,1635506907.0,False
qi8v4v,"Current game consoles store a lot more stuff on disk, like game updates and such which take a lot more space.",hii0nfw,t1_hihzvyu,1635507391.0,False
qi8v4v,I think the newer playstations put the entire game on the console,hik5188,t1_hii0nfw,1635539432.0,False
qi8v4v,"well nowdays it cant read the disc fast enough to keep up, since graphics take up sooo much more space nowdays. so it copies everything from the disk to your hardrive so that when you play the game you dont have to wait for it to read the disc (remember those awful loading screens? if it was the same ud be waiting for 20mins nowdays)",hii6sii,t1_hihzvyu,1635510905.0,False
qi8v4v,I would guess that the internal storage would be used to cache some data of the game from the CD (for purposes of fetching game data faster).,hii0ume,t1_hihzvyu,1635507515.0,False
qi8v4v,Because they sell digital games now and allow screen caps and clip storage.You’re thinking way too hard about this.,hii0qlt,t1_hihzvyu,1635507445.0,False
qi8v4v,lmao yeah my bad :),hii4vug,t1_hii0qlt,1635509883.0,True
qi8v4v,"why u delete? even if u were wrong, ppl cant see the context of our replies. just put an edit in.",hii8sjd,t1_hii4vug,1635511908.0,False
qi8v4v,"Game assets and what-have-you are stored on the medium, like the bits on your hard drive or the cd you put into your console.

Your progress in the game can be kept in much smaller space because it isn't saving all the models, assets, so on; it is saving your current location, your current inventory, and your current plot points.

For example, (I don't know GoW specifically so here's a generic example), You can keep track of every individual area of the game in terms of an integer Id, you can keep track of your position within that level in three floats, you can store your inventory as an array of integer Ids, and you can save your plot progression with some minimized objects.

Example file:

```
{
  currentArea: 30321,
  currentPosition: {
    x: 44.0115,
    y: 188.9492,
    z: 65.7193
  },
  currentInventory: [
    10438,
    192873,
    1294674,
    49081,
    19237,
    487
  ],
  currentHitPoints: 74,
  plotProgression: [
    { withCharacter: ""Poseidon"", progress: 3 },
    { withCharacter: ""Athena"", progress: 8 },
    { withCharacter: ""Apollo"", progress: 1 }
  ]
}
```",hij2po3,t3_qi8v4v,1635524471.0,False
qi8v4v,"Because save files store minimum amount of data of current game state that is needed to be able to run the game from that same state next time. It doesn't store graphic textures, entities, map, locations... Those are all recreated (instantiated) from disc when save file is loaded from memory card. 

For example, saved file could store: id of last mission passed, id of current map, coordinates of last checkpoint, player stats (weapons, health, magic, armor)... With all that data, proper game files are loaded from disc and game resumes based on saved state.",hihrc0u,t3_qi8v4v,1635500372.0,False
qi8v4v,"On top of my head, just save location id, opened chests, gorgon eyes, feathers, level ups, orbs + current level state",hii9t2p,t3_qi8v4v,1635512392.0,False
qi8v4v,"Ps2 required a disk to be inserted. Most of the data was on that disk. The only thing saved in the 8mb memory card was the actual save data. Just the location or level you were on, level of your character, orbs you had, etc.",hiiai0d,t3_qi8v4v,1635512720.0,False
qi8v4v,"Old games had less data, because lower quality, less background things going on, plus most stuff was stored on the CD. Saves aren't huge. I frequently back up my PS4 saves of certain games onto a flash drive and most of the time it's usually only 4 or 5 megs\*, after 60+ hours of gameplay.",hihyduh,t3_qi8v4v,1635505909.0,False
qi8v4v,4 or 5 GB of data just to save game progress? Which games?,hihzjv4,t1_hihyduh,1635506685.0,False
qi8v4v,"Update, I was wrong. 4 or 5 Megabytes. Ghost of Tsushima is the most recent.",hihzxpc,t1_hihzjv4,1635506938.0,False
qi8v4v,Look up .kkrieger. not an answer to your question but you might come back with a new set of questions :),hiqrxel,t3_qi8v4v,1635669161.0,False
qhohv1,Thoughtless design or bad programmers can also make programs consume more energy.,hielrf8,t3_qhohv1,1635442014.0,False
qhohv1,Theoretical attacks do not scare me :),hiehmw7,t3_qhohv1,1635440407.0,False
qhohv1,Maybe they can make an AI that can figure out how to be more efficient?,hih1524,t3_qhohv1,1635480251.0,False
qhohv1,The whole internet is just a huge waste of energy. There are some justified uses but a lot of it is just worthless junk. We only see a tip of an iceberg.,hihg0g4,t3_qhohv1,1635490498.0,False
qhhz7s,"There is.

The simplest way to recall it is to think of:

\- negation as unary minus;

\- conjunction as multiplication;

\- disjunction as addition.

So that ""not p and q or r"" is just as ""-1 x 2 + 3"".

So because you would parenthesise the arithmetic expression as ((-1) x 2) + 3), you would interpret ""not p and q or r"" as ""(((not p) and q) or r)"".

Signs such as implication, double implication are even lower down the line than disjunction.",hidhtnl,t3_qhhz7s,1635425319.0,False
qhhz7s,In Mathematics grouping symbols are typically always used for everything except negation which is unary.,hiczw68,t3_qhhz7s,1635413166.0,False
qhhz7s,"I don’t think there’s a universal consensus, but if you google e.g. ‘Java operator precedence’ you’ll see a table detailing the priority of both mathematical and Boolean operators for that language",hicw3zf,t3_qhhz7s,1635409864.0,False
qhhz7s,"“A and B or C” = AB + C

And take precedence over or

Just look up the boolean order of precedence. Your answer is the first thing that pops up.",hidlfer,t3_qhhz7s,1635427104.0,False
qhhz7s,"Wouldn't it be faster to look at the or first? If that is true, then no need to do anything else.",hie42ca,t1_hidlfer,1635435090.0,False
qhhz7s,Yes.....? Pretty sure....,hicvyar,t3_qhhz7s,1635409731.0,False
qhcmwq,"It depends on the implementation. Ultimately, it probably doesn't matter, but I'm partial to having the head store value, because cons cells.",hic2l10,t3_qhcmwq,1635389951.0,False
qhcmwq,"Consider this 3 element list...

Head of List Pointer -> Item 0 \[This is just a pointer\]

Item0  (pointer -> Item1, data) \[aka Head of list\]  
Item1 (pointer -> Item2, data)   
Item2 (pointer -> Null, data) \[end of list\]

The initial pointer has no data, it is just a pointer, not actually part of the list.",hic6yad,t3_qhcmwq,1635392152.0,False
qhcmwq,[deleted],hicryk1,t1_hic6yad,1635406277.0,False
qhcmwq,You're getting downvoted because u/CypherAus states that Item0 is the head.,hidznr1,t1_hicryk1,1635433293.0,False
qhcmwq,"the head is the first item n=0, contains the item and the pointer to next item. the LinkedList object stores the pointer to the head.

so if you delete the first item the head pointer in the LinkedList object will point to the second item, note that the LinkedList object is the same and only the head pointer value within the object has changed.",hid0lx5,t3_qhcmwq,1635413778.0,False
qhcmwq,"Think about the language you're using.

Does the head **node** store a value?

The answer is yes, because all nodes store values. That's what they're for.


If you were you change the question to ""does the head **pointer** store a value?"" then things get more complicated to answer.",hid5rhl,t3_qhcmwq,1635417898.0,False
qhcmwq,"You can do either I guess, but it is generally just like any node you decide to mark as head. (i.e. a pointer to the node at the beginning of the list)",hibzf86,t3_qhcmwq,1635388487.0,False
qhcmwq,"Agree with /u/CypherAus - The implementations I've seen typically have the head being just a pointer.

This is good from a ""separation of concerns"" perspective - Your node class only ever holds data, your linked list class only ever manipulates nodes.",hicilnb,t3_qhcmwq,1635399070.0,False
qgzhy7,"With a modern optimizing compiler either way to express the equation should result--more or less--in the same generated code. 

(The exception is if certain operations have defined 'code execution' blocks which cannot be optimized out. For example, if `y'` actually represents a function call, and the compiler cannot know the function call doesn't have side effects, writing `y' + y'` may result in two calls to the function `y'` instead of one.)

Generally when I write code, I favor clarity over compactness. So I would write:

    ey = y'-y;
    ex = x'-x;
    e = y' + ey + x' + ex;

over

    e = y' + (y' - y) + x' + (x' - x);

Exception: there are certain well-known equations that have meaning to us when expressed as a single expression, such as the Pythagorean theorem. So I'd favor writing:

    r = sqrt(x*x + y*y);

over

    a = x*x;
    b = y*y;
    r = sqrt(a + b);

----

All of this--since it makes little difference to the compiler--is a matter of style.

And I tend to write assuming another human being later on will have to maintain what I write--so I tend to write for legibility.",hi9n5ln,t3_qgzhy7,1635353704.0,False
qgzhy7,"You can try exploring how your compiler processes your code, i.e. on godbolt.com",hia0g7q,t3_qgzhy7,1635358894.0,False
qgoxsp,"The simplest and often most effective predictive text engine is simply a statistical use of n-grams. N-grams are sequences of n words. These can be 2,3,4,5, or more words. You’ll likely want n-grams of different lengths to handle the beginnings of sentences where there are few if any words already typed. In order to build a predictive text engine, you would need to parse some corpus of text. Things like ebooks, websites, Text messages, etc. Parse these texts and determine the frequency of unique words following each unique n-gram. Once you’re done parsing, prune away the low frequency words following each n-gram and leave only the (3?) most common ones.  From there, you simply use a lookup table to provide text suggestions by determining which n-gram has already been typed and suggesting the three most common words to follow it. 

This is a type of Markov decision process.

You can update the engine as needed by incorporating more recent text and weighting in favor of that while decaying the weight of old text.",hi9p366,t3_qgoxsp,1635354470.0,False
qgoxsp,"To make a good predctive text generator you would want to use ML, which mean you would need data.

To code from scratch is an impossible task, so use libraries that offer the functionality you need (some deep learning package) and  choosing the correct algorithm is also not trivial.

I suggest to simply follow a guide and understand what they done, to understand why they done or how its done internally i suggest to watch a deel learning course (standford in youtube is great).

Example guide: https://medium.com/analytics-vidhya/build-a-simple-predictive-keyboard-using-python-and-keras-b78d3c88cffb",hi7pk8d,t3_qgoxsp,1635312680.0,False
qgoxsp,"The common approach used to be Hidden Markov Models, or Markov Chains",hi7w59a,t3_qgoxsp,1635317622.0,False
qgoxsp,[deleted],hi87h44,t3_qgoxsp,1635327391.0,False
qgoxsp,"**[Trie](https://en.wikipedia.org/wiki/Trie#:~:text=In computer science, a trie,key, but by individual characters)** 
 
 >In computer science, a trie, also called digital tree or prefix tree, is a type of search tree, a tree data structure used for locating specific keys from within a set. These keys are most often strings, with links between nodes defined not by the entire key, but by individual characters. In order to access a key (to recover its value, change it, or remove it), the trie is traversed depth-first, following the links between nodes, which represent each character in the key. Unlike a binary search tree, nodes in the trie do not store their associated key.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hi87i1j,t1_hi87h44,1635327413.0,False
qgoxsp,*laughs in Github copilot*,hi9u98w,t3_qgoxsp,1635356516.0,False
qgc7oj,"The question is a bit vague, which is probably why you're not finding anything definitive. You'd need to define your terms more clearly:

&#x200B;

1. What constitutes a ""breakthrough search algorithm"", i.e., by what metrics is something a breakthrough search algorithm?
2. What constitutes ""classical computing""?",hi59ubj,t3_qgc7oj,1635273024.0,False
qgc7oj,"Breakthrough was a bit unnecessary, I’m just looking for an early or the first search algorithm. By classical computing I’m talking about basically anything that isn’t quantum computing",hi5bvhc,t1_hi59ubj,1635273826.0,True
qgc7oj,"Search algorithms predate computers as we know them, which is why it would still depend on the specifics. For example, depth first search was first created by a French mathematician and is probably one of the first search algorithms as we know them. It is still used extensively either directly or as the basis for other search algorithms; however, it isn't very efficient in many cases. So does this count? It is possible to argue both yes and no.

&#x200B;

[https://en.wikipedia.org/wiki/Tr%C3%A9maux\_tree](https://en.wikipedia.org/wiki/Tr%C3%A9maux_tree)",hi5cjcd,t1_hi5bvhc,1635274090.0,False
qgc7oj,I’m gunna put linear search ahead of a DFS.  People have been looking down lists as long as there have been lists to look down.,hi5jez3,t1_hi5cjcd,1635276759.0,False
qgc7oj,"Binary search also probably came first. If you are looking for a word in the dictionary you will automatically do a shoddy binary search for the word, so I assume that was formalized in math a long time ago",hi5pniy,t1_hi5jez3,1635279141.0,False
qgc7oj,"Well, it depends. This is why the question is difficult to answer precisely without establishing the terms. Much of the analysis of these algorithms as we know them today (in a computational sense) came about in the 1940s-1960s. For example, John Mauchly did the first real analysis of binary search as a computational algorithm in 1946. Of course, it is based on notions that might be much older but it is difficult to pinpoint them to a specific point in history as an algorithmic process (again as we know such things today). The same is true for linear search. Tramaux's work was really important in laying the groundwork for the development of algorithms as we think of them. The question, as posed, is somewhat unanswerable. An argument could be made for a lot of different ways to search. I mean evolutionary search predates almost any other search mechanism on Earth by millions of years, but one would be hard-pressed to call it an algorithm as we think of it, even if we are inspired by it today.",hi7hxtm,t1_hi5pniy,1635307960.0,False
qgbxhk,"My interpretation is that you have 4 years of experience with web and you're having trouble with a basic assignment. 

I would take a step back and try to determine why it's difficult, and be critical, like, it is algorithms making it difficult or is the problem beneath you because of your experience?

If experienced with the topic, approach it as an auditor of the problem and report your opinion and your improvements for the problem to whomever created it (TA or instructor). You get the problem done, instructor gets feedback (they love that), and future students benefit. Win win",hi629fg,t3_qgbxhk,1635284098.0,False
qgbxhk,"No the assignments and even the exam is rather easy, but it's  the learning of for example Javascript weird comparisons, when I already know and used Javascript a lot.

The Tips that you gave were else very usefull.",hi88pbu,t1_hi629fg,1635328422.0,True
qgbxhk,"There's always more to learn. That's the nature of computer science and of programming. If you're not being challenged then power through and use the extra time to learn about stuff that interests you. Push yourself towards expertise if you feel so strongly. Or just take the good grade for now honestly. As an academic field and as an industry, self-driven learning is a critical skill to pick up.",hi5a35q,t3_qgbxhk,1635273123.0,False
qgbxhk,"Yep, well that is the challenge to overcome.",hi88wi1,t1_hi5a35q,1635328589.0,True
qgbxhk,Gamification sometimes helps. E.g. leetcode or alike.,hi552wg,t3_qgbxhk,1635271141.0,False
qgbxhk,That sounds like a nice approach.,hi88q77,t1_hi552wg,1635328442.0,True
qgbxhk,"I'm in a similar boat. Been doing entry level dev work for the last seven years basically. Now I'm going back to school to actually get the stupid slip of paper, and it's so hard to put my mind to stuff because I already know the bulk of it. Some classes have well structured activities instead of lectures and those are fine, but it's the ones with video lectures that are just mind numbing to me. 

Honestly, my solution is to just pound energy drinks like there's no tomorrow and then muster all the willpower I have to force myself to focus. It helps to remind myself that if I finish my stuff asap I can spend time on personal projects instead. Ultimately, my best motivator is probably deadlines though.",hi5j0u4,t3_qgbxhk,1635276610.0,False
qgbxhk,"Yes exactly, mind numbing describes it so well. Not so sure about the energy drinks method but deadlines are definitely of good use.",hi8923x,t1_hi5j0u4,1635328725.0,True
qgbxhk,">Not so sure about the energy drinks method

Yeah, I think the energy drinks only work for me because I have untreated ADHD haha",hi8t8c0,t1_hi8923x,1635341209.0,False
qg1nhc,"Adaptive processing is already a thing, though its primary concern at the moment is scaling of, and access to, resources such as caches.",hi4sqbn,t3_qg1nhc,1635266301.0,False
qg1nhc,self-modifying fpga :thinkface:,hi5dtfo,t3_qg1nhc,1635274586.0,False
qfwfbn,"I feel like you're answering your own question. More power means scaling everything else up (not just for heat dissipation - a high powered CPU doesn't really mean anything when the system that uses it becomes its own bottleneck). And scaling everything else up defeats the purpose of a raspberry pi.

In reality though, a raspberry pi is overkill for a lot of projects that people want to use a raspberry pi for. It's a great learning tool but if the goal is efficiency when it comes to power and memory, there are much better options out there (they're just harder to get working).",hi33nq5,t3_qfwfbn,1635230917.0,False
qfwfbn,"I'm curious, what kind of options? Can you provide any links? Thanks in advance!",hi37vfv,t1_hi33nq5,1635234524.0,False
qfwfbn,Arduino and similar microcontrollers are enough for a lot of projects,hi3891d,t1_hi37vfv,1635234857.0,False
qfwfbn,"As the other reply mentioned - arduinos are good kits. But even these can be overkill. As always, it all depends on your use case. If you had a simple IOT project that used very little power and required very little memory, something like this would make a lot more sense than a Pi or Arduino: https://en.wikipedia.org/wiki/ESP32

But the learning curve for these is higher, so starting with an Arduino or Pi makes a lot more sense in the beginning.

r/microcontrollers and r/embedded are good places to look for more information.",hi9u9ey,t1_hi37vfv,1635356518.0,False
qfwfbn,Really interesting stuff. Thanks!,hiaqaiu,t1_hi9u9ey,1635368834.0,False
qfwfbn,"It doesn’t need it?

Same reason a Toyota Corolla doesn’t have a 6L V8… Different use cases",hi2y5cn,t3_qfwfbn,1635226612.0,False
qfwfbn,cpu demands are main reason raspberry pi doesn't incorporate a better cpu,hi2zq4t,t3_qfwfbn,1635227789.0,False
qfwbyw,"The main mathematics you need for AI:

\*Linear algebra (essential to understand most ML / AI approaches)

\*Basic differential calculus (with some multivariable calculus)

\*Coordinate transformation and nonlinear transformations (key ideas in ML / AI)

\*Linear and higher order regression (making predictions based on existing datasets)

\*Logistic regression (classifying a data as one thing or another)

\*Numerical analysis (because converting mathematical formulas into reliable and running code is often harder than you think)

\*Basic statistics (ML / AI use many statistics concepts).

If you're asking for a specific path, I really don't know. I recommend you look at the undergraduate curriculum of the [AI program at Carnegie Mellon,](https://www.cs.cmu.edu/bs-in-artificial-intelligence/) which is the best in the world in Artificial Intelligence and  I believe the only one in the United States that offers AI degrees at this level.",hi2hjst,t3_qfwbyw,1635216582.0,False
qfwbyw,"As a grad student studying AI, this. All of this. You may want to get familiar with signal processing (particularly  Fourier transforms) as well if you want to do some advanced things with computer vision, audio, or other time series data.",hi2hw4x,t1_hi2hjst,1635216754.0,False
qfwbyw,"First of all, agree 100% on the math.

But is Carnegie Mellon (Thrun) really (currently) the best in the world in AI? Better than University of Toronto (Hinton), University of Montreal (Bengio, Goodfellow), Stanford (Fei-Fei Li, Andrej Karpathy, Andrew Ng), Berkeley (Russell, Norvig, Jordan) and MIT CSAIL (Minsky, Kurzweil)?",hi391de,t1_hi2hjst,1635235565.0,False
qfwbyw,[removed],hi2wlb2,t1_hi2hjst,1635225486.0,False
qfwbyw,You need to what?,hi320p9,t1_hi2wlb2,1635229589.0,False
qfwbyw,"> Isn't htis part of LInear algebra?

The linear ones are. The nonlinear ones aren't. Like the name says.",hi4kb4z,t1_hi2wlb2,1635262955.0,False
qfwbyw,"This is great info, thank you sir",hi2j7dg,t1_hi2hjst,1635217416.0,True
qfwbyw,Excellent list and I agree with all of it. I'll also just add some probability theory to it. A basic understanding of MLE and probability distributions is very useful although I think we can assume that these topics will be covered in the statistics course you've mentioned.,hi49471,t1_hi2hjst,1635258376.0,False
qfwbyw,"This is a good list. The biggest ones are linear algebra, probability, and all of the calculus. For cs in general though the more math you study the better. The ideal of math to know is ""all of it.""",hi5sjxu,t1_hi2hjst,1635280256.0,False
qfwbyw,"You need to know linear algebra very well so getting advanced concepts and tons of practice from basics to advanced is more than necessary.

You need to know calculus very well.

You will also need to know probability very well. Statistics very well. 

You also need to know about computing, graphs, and algorithms. 

It’s a good time, one of thise work hard play hard areas. =) 

Check out mitocw if you have not done so. MIT literally provides free course lectures and materials",hi2j6ur,t3_qfwbyw,1635217409.0,False
qfwbyw,"graph theory (Dijkstra's algorithm, traveling postman etc).

differentiation and integration.

matrix theory.

these are parts of Discrete Mathematics, so a text book on this topic would be ideal.",hi3fx9k,t3_qfwbyw,1635241465.0,False
qfwbyw,https://mml-book.github.io/book/mml-book.pdf,hi3ajob,t3_qfwbyw,1635236927.0,False
qfwbyw,"For the linear algebra, I can recommend the 3Blue1Brown video series: https://www.3blue1brown.com/topics/linear-algebra

Also, Andrew Ngs free course on Coursera walks through the math as it goes along, not as intuitive as 3b1b but all in the context of machine learning algorithms.",hi37wky,t3_qfwbyw,1635234551.0,False
qfwbyw,"This is an atypical learning path, but I think it's good. It's also textbook heavy, cus tbh I have no idea how people learn things from watching youtube.

First, read ""Linear Algebra through Geometry"". I've worked with people in ""AI"" (computer vision) who are brilliant coders and on paper linear algebra whizzes, but don't have a geometrical intuition in their body. This might be an over generalisation but at this point AI is just making things closer to each other in an N dimensional space. 

Then, read some intro calc book. I've never found one satisfying so look elsewhere for this one. You really don't need to go that deep, but you should intuitively understand derivatives and integration fairly well. 

Now, read Algorithms for Optimisation. 

(Given that you say you have time, I'm recommending the following two, but if you're reading this and you just want a job ASAP honestly you could probably skip them for the moment. But DON'T SKIP THEM FOREVER).

Now read Rudin. Welcome to math.

Now read a stats textbook. Casella and Berger if you wanna get dirty. 

Vector calculus by Hubbard and Hubbard is dope but not strictly needed. 

Now IMO you're ready to get into the ML. A good option is intro to statistical learning. If you've followed the path so far, it'll be pretty straightforward. Breaking with my thread here, I'd say do the coursework and watch all the lectures on cs231n Stanfords ML class. The material is available online and it's pretty dope. 

After that, you'll know where to go. Also, if you get through all that, the field will be vastly different by then, so ask again!",hi46wtq,t3_qfwbyw,1635257430.0,False
qfwbyw,"ML is mostly regressions and its called AI for some silly reason.

Pathfinding algorithms used to be considered AI but aren't anymore.

If you want to do video games, video game AI is just if then else type stuff (discrete state machine).

The thing to remember about AI is that its not that impressive once you get to the other side. You start to see how ""artificial"" it is. I have less faith in driverless cars being adopted widespread in my lifetime.

Biological intelligence is far superior in reacting to things not in a data set. AI is still garbage in, garbage out.",hi3mynn,t3_qfwbyw,1635246859.0,False
qfwbyw,anything with numbers and some of the stuff with edges and vertices,hi30eic,t3_qfwbyw,1635228317.0,False
qfwbyw,Start with linear algebra and stats as that will provide the most benefit there’s also books like math for ml free online which are catered to getting down the required math skills,hi59ghk,t3_qfwbyw,1635272873.0,False
qfwbyw,This inspired me to start learning again too!,hi59q0j,t3_qfwbyw,1635272977.0,False
qfwbyw,I’m taking a course titled AI this year in university. I can send you the course outline which lists all the topics we cover. You can then research them from there if you like.,hi6999e,t3_qfwbyw,1635287087.0,False
qfwbyw,"Start with calculus and linear algebra.

On your way learn some statistics as well.

For most applications this should be enough to get you going.

Learn the rest as you go.",hiae43c,t3_qfwbyw,1635364185.0,False
qfwbyw,Probabilities and some statistics.,hiraytf,t3_qfwbyw,1635684150.0,False
qftnw9,"Imagine a situation: you are having a problem you don't really understand. Instead jumping into coding (why on earth are people still doing that, I have absolutely no idea) first you need to gain insight and understanding. To understand, you need to remove the unnecessary parts, and start to make an abstraction of the problem. In computer science, we have plenty of such abstractions, including: concept modeling, process modeling, state machines, graph theory, physical models (F=m\*a is also a model), petri nets,  etc. What we want to do is in order to solve the problem, we transform it into one (or multiple) of well known abstraction, because for those abstractions we have a huge amount of tools.  


Imagine a case when you need to develop a software which calculates traveling times from cities to cities. This problem can be abstracted to a graph theory problem: finding shortest paths. Imagine a case when you need to write a calculator with different modes. Dang, this is actually a state machine. Imagine there are lots of various nouns in a long description where you are absolutely lost (like we were when we had to create a document for an audit). Then you you concept modeling when you model the various concepts and the connections between them. Or you need to understand a process and people ask you what if there are too many customers standing in the line, how will our system react to it? Then you have a way to describe it (like activity diagram, or bpmn, etc.), or transform it to some kind of markov chain and give insights. 

The best part is that if you have an abstraction, a model, you can reason for or against it, you can even simulate how the system will work without the need of building it.   


This is why we do modeling.   


https://www.sebokwiki.org/wiki/What\_is\_a\_Model%3F",hi2tu31,t3_qftnw9,1635223587.0,False
qftnw9,draw a picture of how it works to do a reality check,hi3000y,t3_qftnw9,1635228003.0,False
qfnew0,"Damn, a few upvotes but no answers :/ must be a problem a lot of us are running in to.",hi2tb9e,t3_qfnew0,1635223249.0,True
qfnew0,"Yeah, sadly this is often a killer before even hitting the keyboard for me. Data is expensive and you get what you pay for.

While the veracity can't be guaranteed, I've spotted a few decent crowd-sourced data repositories on GitHub. Not much use if you need live data of course, but it's something!",hi8jf9z,t1_hi2tb9e,1635336012.0,False
qfnew0,"Thank you! The only other thing I can think of is building a web scraper for what ever it is I would need but that doesn't seem reliable as a single change to the webpage could break everything.. Maybe the best route is to create a virtual service to mock the data and then when your ready, pony up and pay for the real thing.",hi939e8,t1_hi8jf9z,1635345664.0,True
qfnew0,use google images as training data to guarantee bias in your model. 11/10 best free training set,hi30byg,t3_qfnew0,1635228263.0,False
qfhy4b,"Algorithm is precisely defined series of steps that need to be performed to get the appropriate output for an given input. Emphasis on ""precisely defined"" and ""appropriate output"" - all cases must be covered and for every input algorithm must produce valid output. So, first characteristic of good algorithm is its [correctness](https://en.wikipedia.org/wiki/Correctness_(computer_science)).

Now we can talk about algorithm efficiency. There are different types of it, but most common are [time complexity](https://en.wikipedia.org/wiki/Time_complexity) and [space (memory) complexity](https://en.wikipedia.org/wiki/Space_complexity). These Wikipedia articles cover those topics deeply so if you are interested do read them. 

Time complexity is expressed as a function of the size of the input. These functions can be classified using [asymptotic notation](https://www.codecademy.com/learn/cspath-asymptotic-notation/modules/cspath-asymptotic-notation/cheatsheet).",hhzt5nu,t3_qfhy4b,1635175224.0,False
qfhy4b,I really feel like this is best answer and I wish it had more upvotes.,hi0gl0a,t1_hhzt5nu,1635184855.0,False
qfhy4b,"Wish granted, I guess. It's the favorite answer by a landslide now.",hi14zxr,t1_hi0gl0a,1635194626.0,False
qfhy4b,"To dive deeper, which would you say is more important, worst-case complexities or average-case complexities? Is it use-case dependent?",hi19c4l,t1_hhzt5nu,1635196411.0,False
qfhy4b,"Pretty much. Bubble sort, for example, is extremely good when arrays are almost sorted and extremely useless otherwise. Average case is usually more important since it's *the average* but worst-case might be particularly horrific or unfortunately common in your use case.",hi2scju,t1_hi19c4l,1635222636.0,False
qfhy4b,Simple and clear,hi16apy,t1_hhzt5nu,1635195149.0,False
qfhy4b,After those most important characteristics I like the ones that appear to have a sense of beauty or elegance. Take for instance solving exact cover problems with Algorithm X using the dancing links technique. Just beautiful.,hi1mwgq,t1_hhzt5nu,1635202307.0,False
qfhy4b, /thread. Well said!,hi23kuq,t1_hhzt5nu,1635209990.0,False
qfhy4b,"Excellent response.

I'd add elegant simplicity, i.e. it's plain to see what is happening and no superfluous parts to it.",hi2f7jn,t1_hhzt5nu,1635215433.0,False
qfhy4b,"""Good"" is context sensitive. If you have space constraints, then an otherwise desirable algorithm that has an unacceptable space requirement isn't good for your needs, is it? If time is top priority, then you want to look at its time complexity. I have a book on parsing around here somewhere that goes into detail about a number of algorithms. I recall that one algorithm has a worst-case time complexity that is quadratic. The thing is, it's provable that in the parsing algorithm, you can never hit the worst case scenario. Or if you know something about your data, you might be able to pick an algorithm that has a specific complexity just for that.  Quick sort has a good average time complexity, but something more specific might be better.

And algorithmic complexity is only there to tell you the ""shape"" of the algorithm. Two algorithms that are both O( n^2 ) might have vastly different running times. Bubble sort is n^2, yet if your data set is small enough to fit in a single cache line, it literally doesn't matter what other algorithm you use, because you'll always be IO bound on the memory bus. A better algorithm, you'll still be waiting for a cache read or write. A constant time complexity is usually the goal, because the time requirements are understood perfectly, but if a bubble sort takes 2 hours to run for a given data set, that's still better than a given constant time algorithm that takes 3 years. So constant complexities aren't a panacea.

If you're looking at algorithmic complexity in terms of a practical application, it's a starting point, not the ending point.",hi0cc0j,t3_qfhy4b,1635183114.0,False
qfhy4b,"There's no universal definition of a good algorithm. It depends on the problem you are trying to solve and what you are trying to optimize (time complexity, space complexity, readability, etc.).",hi08l21,t3_qfhy4b,1635181579.0,False
qfhy4b,"I'm coming at this from an application development perspective.

If it meets the business need, its a good algorithm.

I would say that from a project management perspective, getting bogged down in any one algorithm is a bad idea. You generally should just see everything as ""get it all working first"" and then optimize.

Completing a working application is one of the most difficult things to do.

I'm just a lowly hobbyist with a game I made on the side, but I am about to submit it to the App store after 3 years of work. I think the main reason I got it to market where others don't is I have no problem coming out with garbage that works in the hope that I can come back to it and improve later.

Algorithms aren't really a major focus of application development, but games have tons. But in those, efficiency doesn't matter all that much. Your game isn't going to fail because a suboptimal pathfinding algorithm that could have been improved 2x. 10x maybe I guess.

The areas I can think off that algorithms matter are like people that make bot traders on Wall street. If I were one of them, I'd be sh\*tting my pants about each line because of the $ amounts involved.

If your doing JPL/NASA/Life critical you got to design things with that in mind. JPL has their own standards and its pretty strict - no mallocs and each loop has to have a definite end (no i<variable, it has to be like i<1000). They don't want someone to die in space because of a loop that doesn't terminate.",hi0dka9,t3_qfhy4b,1635183619.0,False
qfhy4b,"An algorithm is only good for certain tasks, there’s isn’t a sorting algorithm that’s best at sorting anything. Quick sort is good because most of its use cases end with a pretty efficient runtime BUT there’s still use cases where it does poorly; this is where you would use a different sorting algorithm that would normally perform very poorly compared to quick sort but in this particular instance it performs exceptionally well.",hhzr36s,t3_qfhy4b,1635174327.0,False
qfhy4b,"correct me if im wrong; but, divide and conquer has the best time order for large unordered sets.",hi1bsvg,t1_hhzr36s,1635197460.0,False
qfhy4b,"Again, it's context dependent. Any comparison sort can't be faster than O(nlog(n)), but there are faster sorts for other use cases, such as radix or counting sorts. Or perhaps you're very much memory constrained, and have to do everything in place. Some of those algorithms get much more difficult to implement. Context is everything with algorithms.",hi1pyv5,t1_hi1bsvg,1635203701.0,False
qfhy4b,"This is very true, efficiency isn’t everything with an algorithm: memory consumption plays a huge part in whether or not to use it as well.",hi231ze,t1_hi1pyv5,1635209745.0,False
qfhy4b,"Yeah you’re right, it is about average case performance. An algorithms performance is dependent on the context in which you’re implementing said algorithm. Learn about runtimes and Big O notation.",hhzonvw,t3_qfhy4b,1635173276.0,False
qfhy4b,Worst case could also be more important especially for algorithms with large inputs. It depends on the situation whether an algorithm is good or not.,hi07tq6,t1_hhzonvw,1635181264.0,False
qfhy4b,Yeah I'm a novice and was doing some hackerrank problems.  I was doing a problem where you have to check if a number and its reverse are evenly divisible by k.  I assumed that I might save time by just accounting for any reverse number of i upon iteration i if reverse i was in the array as well.  Maybe I did it poorly but just simply checking them individually timed faster on my pc.  I was thinking maybe it was dependent on the array size though.,hi0cmx9,t1_hi07tq6,1635183238.0,False
qfhy4b,Searching for an item in an (unsorted) array is linear time which if you put it in a loop turns into quadratic. A set is better since it offers near constant time lookup.,hi0fgi1,t1_hi0cmx9,1635184394.0,False
qfhy4b,Would say focus is on worst case in the CS field. Average case concerns is more for real-world stuff. :-),hi0bkr4,t1_hhzonvw,1635182807.0,False
qfhy4b,Good point!!,hi0dh9y,t1_hi0bkr4,1635183584.0,False
qfhy4b,"Space complexity is not a big deal when it comes to *algorithms*. How much memory your algorithm uses is a concern, but not as concerning as the time your algorithm needs to execute. So I think that time complexity comes first.",hi0398q,t3_qfhy4b,1635179381.0,False
qfhy4b,"Well you have to look at what it does, and then how it does that in comparison to other ways of doing it. Is it more or less efficient? If so in what ways? Is it possible it is sacrificing something, e.g. memory, in order save more on computation?

Sometimes you have lots of resources. Sometimes you're building something for a small device with battery, storage, compute etc limitations.

Part of being an engineer is to be able to make those kinds of design choices.",hi0fjkp,t3_qfhy4b,1635184428.0,False
qfhy4b,"It actually very complicated to determine if an algorithm is good. This is why people study computer science.

It is like asking how to determine if a plane is good. Or if this particle accelerator is good. Or if this surgery is good.",hi0jgsk,t3_qfhy4b,1635186036.0,False
qfhy4b,"you can look at space and time complexity which is the most obvious first step. but additionally algorithms can be suited to expected inputs. for instance tim sort isn't a particularly fast algorithm when it comes to its average constant factor on random data. however, tim sort is very very fast when subsets of the list have been pre-sorted. in python this helps speed things up for data science applications where you might combine pools of already sorted data. 

algorithms are good because they suit an application correctly",hi0mkbv,t3_qfhy4b,1635187292.0,False
qfhy4b,"logarithmic time order

O(n) = log(n)",hi1be7y,t3_qfhy4b,1635197288.0,False
qfhy4b,What is 'good'?,hi1xg4f,t3_qfhy4b,1635207178.0,False
qfhy4b,"Before I get into my comment, I want to preface that I'm a beginner too, so whatever I say here, definitely fact-check me and make sure I'm right!

Good depends on your usage case. Time and space complexity are asymptotic and have to do with the performance of algorithms especially when dealing with inputs of very large size or when you're doing an operation many, many times. For example, quicksort and mergesort are similar if we just look at the time complexities, but depending on the data we're dealing with, quicksort or mergesort could be faster than the other. On the data structures side, we can also see that a hash table implemented using linear/quadratic probing (to account for collisions) versus one using chaining can be better, despite them having similar performance characteristics -- and this is because, on the machine level, a hash table using probing can take advantage of caching to be faster. So there are many things to be considered when working with algorithms, like the kind of inputs you'll be feeding them (consider more than just their size) and the machine that you're working with.

There's also more complicated stuff to consider when you're dealing with amortized algorithms and datastructures, which (overall) can be efficient, but in certain cases can experience slowdowns that might be a negative (i.e. real-time systems with hard deadlines, I believe).",hi28bqx,t3_qfhy4b,1635212179.0,False
qfhy4b,If it has a significantly better time complexity than existing soloutions,hi2u14o,t3_qfhy4b,1635223715.0,False
qfhy4b,"You want to research Big-O, AKA algorithm effeciency.

Link: https://towardsdatascience.com/introduction-to-big-o-notation-820d2e25d3fd

That is what we had to learn in school. You can kinda rough estimate things by eyeball once you practice a bit.",hi2yaim,t3_qfhy4b,1635226719.0,False
qfhy4b,"Definitive: An algorithm should have well defined input and output values. If you add two numbers, you should get a number, not a character. 

Exhaustive: a good algorithm needs to work in all cases (or have as few edge cases as possible). For example if you had a sorting algorithm that could only sort 10 elements than that algorithm is not as useful as one that can sort arbitrary arrays.

Stable: an algorithm is stable, if given a small change in input, you have a small change in output. An unstable algorithm is the YouTube algorithm. It shouldn't be the case that if I play a song for my 4 year old cousin that then I should get children videos 2 weeks after.

Fast: being able to work on modern computers well. This usually boils down to having low complexity but it's not enough as low complexity could still mean slow implementation on actual computers.",hi4gue1,t3_qfhy4b,1635261554.0,False
qfhaf7,"This might be better suited for a computer engineering subreddit, but I'll answer the best I can. My understanding is some simulators treat two different inputs going into the same wire like an OR, and others just give an error. [Here's an example in Logisim](https://i.imgur.com/4zITit5.png) where two input pins of opposite value result in an error. Not sure what happens in real life, but this probably results in some sort of unpredictable behavior. I don't think I've ever seen a circuit diagram with two wires connected like that without an OR. Couldn't find anything else on this topic though.",hhzy6a3,t3_qfhaf7,1635177308.0,False
qfhaf7,"That's great, thank you. I think I will repost it to a computer engineering subreddit, but your answer makes sense",hi022g8,t1_hhzy6a3,1635178893.0,True
qfhaf7,"Shifters are generally constructed from multiplexers. A textbook example of a 2-to-1 multiplexer selects the value of one out of two inputs like this:

    q = (a & ~sel) | (b & sel);

The OR gate propagates a one from the selected input to q.",hi0f2cn,t3_qfhaf7,1635184232.0,False
qfhaf7,"You can't drive a net with multiple sources, hence you need to OR the signal so that if one is a zero and the other is a 1 you get only one output value.",hi0m8j8,t3_qfhaf7,1635187159.0,False
qfgg3g,https://stackoverflow.com/questions/3980416/time-complexity-of-euclids-algorithm,hiaehgq,t3_qfgg3g,1635364326.0,False
qfah2t,[deleted],hhz3zpo,t3_qfah2t,1635162581.0,False
qfah2t,Yes. Crash course is fantastic,hhzmgt3,t1_hhz3zpo,1635172323.0,False
qfah2t,Crash course is so good for getting a basic understanding on how computers work at a very low level.,hhzti92,t1_hhzmgt3,1635175373.0,False
qfah2t,"Looking for the same thing. In all of my conversations with programmers, I find that the most difficult thing for me is establishing a common vernacular. Working on the product testing end of an IoT company, I ran into engineers who knew only how to speak their own “language”",hhyi9me,t3_qfah2t,1635144336.0,False
qfah2t,"If they only know how to speak thier own language, chances are they themselves don't know what they're talking about exactly.",hhypzda,t1_hhyi9me,1635151031.0,False
qfah2t,Ben eater,hhyvy0x,t3_qfah2t,1635156437.0,False
qfah2t,"Wanted to recommend his Playlist, but then I realized that the question is about terminology, not technology...",hhzeb0o,t1_hhyvy0x,1635168487.0,False
qfah2t,Damn I misread. Thanks.,hi9r3kd,t1_hhzeb0o,1635355277.0,False
qfah2t,People here are answering from a computer science point of view. But are you just talking about a normal user who wants to use things like Microsoft Word and web sites?,hhzuu5i,t3_qfah2t,1635175940.0,False
qfah2t,8 bit guy,hhyq30v,t3_qfah2t,1635151125.0,False
qfah2t,Mention few course and youtube channels to get in touch with it,hhyg6gu,t3_qfah2t,1635142625.0,True
qfah2t,https://youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha,hhzz7s2,t3_qfah2t,1635177734.0,False
qeyub5,"Ecology may have some examples for you. Like the way certain trees form relationships with fungi to re-distribute nitrogen, potassium to their saplings. I wouldn't consider them the same thing, but I think there are probably a few different layers that could be further compared with the OSI model.

&#x200B;

[https://www.youtube.com/watch?v=-8SORM4dYG8](https://www.youtube.com/watch?v=-8SORM4dYG8)",hhwc7yt,t3_qeyub5,1635103640.0,False
qeyub5,That's really fascinating. Thanks a lot!,hhwp0uv,t1_hhwc7yt,1635108841.0,True
qeyub5,There’s also the anternet https://www.nbcnews.com/tech/tech-news/ant-inspired-internet-anternet-may-be-coming-soon-flna966571,hhxoqpv,t1_hhwp0uv,1635125883.0,False
qeyub5,https://en.wikipedia.org/wiki/Emergence,hhwolu5,t3_qeyub5,1635108673.0,False
qeyub5,"**[Emergence](https://en.wikipedia.org/wiki/Emergence)** 
 
 >In philosophy, systems theory, science, and art, emergence occurs when an entity is observed to have properties its parts do not have on their own, properties or behaviors which emerge only when the parts interact in a wider whole. Emergence plays a central role in theories of integrative levels and of complex systems. For instance, the phenomenon of life as studied in biology is an emergent property of chemistry, and many psychological phenomena are known to emerge from underlying neurobiological processes. In philosophy, theories that emphasize emergent properties have been called emergentism.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hhwonhn,t1_hhwolu5,1635108692.0,False
qeyub5,Good bot!,hhwpmlp,t1_hhwonhn,1635109094.0,True
qeyub5,Wow thanks for blowing my mind!,hhwpvxt,t1_hhwolu5,1635109205.0,True
qeyub5,You should also look into the Game of Life.,hhwyihw,t1_hhwpvxt,1635112986.0,False
qeyub5,"Technically, *any* form of communication could fit into the OSI model (albeit sometimes with a little creativity) since OSI is a *conceptual framework*. People tend to think about it with regards to modern computer networking, but that's just a specific *implementation* of the model. Think back when we only had the telegraph for long distance communication. I'm sure you can easily see the 7 layers of the OSI model in that implementation if you think about it.",hhxihgb,t3_qeyub5,1635122721.0,False
qeyub5,"When I make a mistake, that's a fault at layer 8",hhxgcf3,t3_qeyub5,1635121656.0,False
qewt85,"The basic idea for all counting problems like this, is to find the recurrence relationship.  Your going to have a tree that looks like this Root (tree a) (tree b) (tree c)...   you want to start building your tree, until you have all subtrees that ""look like"" the whole tree, only smaller. Then you'll get a reccurence relationshiop of the form T(n) = O(1) + T(n-a) + T(n-b)  + T(n-c)...  

Solving the recurrence relationship gives you T(n).

Here's a simple example.  Consider the language with symbols a and b sth there are no two bs next to each other.  (ie. aaabaaa is allowed, but not abbaaa).

With a string of length n, we can right the tree as a(tree of length n-1) + ba(tree of length n-2) so, the difference equation is T(n) = T(n-1) + T(n-2), which gives you the fibonacci sequence.",hhw6f0t,t3_qewt85,1635101292.0,False
qewt85,There are more complicated scenario like a+a+..+a with grammar expr->expr+expr which follows Catalan number's recurrence which is not even linear,hhxfsy5,t1_hhw6f0t,1635121381.0,False
qewt85,Yes... i was giving a simple example...,hhxkoh8,t1_hhxfsy5,1635123830.0,False
qebyt6,"Simply put (so likely a little inaccurate, even if it gets the point across), whenever you call a function inside another, the computer writes down what it was doing before calling the function and puts it on a pile. Then, when the called function completes, the computer can take that paper off the pile, read it, and pick up where it left off.

Now imagine your pile of papers is 10000 pages tall and tips over. That is a stack overflow.

It’s not bad to write recursive functions, but if the implementation is incorrect, you can easily cause an overflow. There are many best-in-class algorithms that are recursive; mainly you just have to make sure to never redo work and to always make progress (and always have a base case).",hhryd35,t3_qebyt6,1635018760.0,False
qebyt6,This is much better than the top answer. The stack is an abstraction that programming languages use to enable function calls.,hhsmtia,t1_hhryd35,1635029939.0,False
qebyt6,"Thanks for the clear explanation! So I guess this stack also includes stuff running in the background, like the OS, Firefox, File Explorer etc. So if you close down everything except your IDE, you should have more space on the stack, right?",hhrzlip,t1_hhryd35,1635019305.0,True
qebyt6,"No, each program has its own stack.",hhs1zju,t1_hhrzlip,1635020348.0,False
qebyt6,"Yep, how Jake Peralta put it, it's ""stacks on stacks on stacks""",hhsp8yd,t1_hhs1zju,1635031082.0,False
qebyt6,I'm a noob and is this anything related to virtual memory?,hhuoktn,t1_hhs1zju,1635076504.0,False
qebyt6,"No, virtual memory is using disk space as extra memory.",hhuv4x2,t1_hhuoktn,1635080553.0,False
qebyt6,"Virtually memory is not that; it’s a virtual address space that real memory (and other stuff depending on the computing architecture like ROMs) is mapped into, on a per process basis, and it’s related to stuff like memory protection. Virtual memory is often enabled by a hardware MMU.

What you  are describing is “swap”.",hhwiyg7,t1_hhuv4x2,1635106349.0,False
qebyt6,Thanks for the explanation.,hhuvaud,t1_hhuv4x2,1635080647.0,False
qebyt6,"Each thread(?) has its own stack. Each function call pushes the current context onto the stack, executes the function, and then pops the context off the stack to continue where it left off. If the functions keep calling more functions, usually when recursive, you can run out of stack.",hhs2tem,t3_qebyt6,1635020722.0,False
qebyt6,The Stack is an area in memory that is a stack data structure. https://www.sciencedirect.com/topics/engineering/stack-memory,hhry34s,t3_qebyt6,1635018632.0,False
qebyt6,"Thanks, I'll try to give this a read.",hhrzo4p,t1_hhry34s,1635019336.0,True
qebyt6,[OSTEP](https://pages.cs.wisc.edu/~remzi/OSTEP/) for if you really want a deep dive,hhsqobj,t1_hhrzo4p,1635031761.0,False
qebyt6,"Every time a function is “called” the system sets up what is called a “frame” which contains all the information the function needs to run (the arguments, local variables, etc.) and it’s this frame that is added to the stack. When the function finishes, it returns a value to the previous function/frame and gets removed from the stack. If the code calls a function and there’s not enough room for another frame on the stack (which is stored in memory), this is what’s called a stack overflow.

Many languages have an optimization called “tail-call optimization”. What that does is, if the last thing a function does is call another function, then it removes the frame from the stack before calling that function because in that case it does not need to be there. With that in place, if you write your recursive function in such a way that calling itself is the last thing it does, then stack overflow is no longer going to happen. So, “it’s bad” to write recursive functions only if you do t know what you’re doing.",hhsk7fa,t3_qebyt6,1635028723.0,False
qebyt6,"Is it still true that several popular languages, including python and Java, do not include tail call optimization?",hhsr86a,t1_hhsk7fa,1635032017.0,False
qebyt6,Two languages that I do not use. I’m not really sure.,hhsrlj4,t1_hhsr86a,1635032191.0,False
qebyt6,"Python and Java are specifications; neither cover this in their specifications.

The most popular implementations (CPython and OpenJDK respectively) do not currently provide TCR. This does not stop other implementations from providing it, however.",hhw3f3c,t1_hhsr86a,1635100109.0,False
qebwa6,My goal is to upload videos to a feed with the users picture on the top left,hhrwsho,t3_qebwa6,1635018041.0,True
qebwa6,"I would say just do whatever works, then go back and try to optimize. I have no knowledge of the inner workings of any social media platforms but my guess would be that’s what they did ultimately, they got a working product then as it grew they went back and optimized with people that knew how to do that.",hhs0tys,t3_qebwa6,1635019843.0,False
qebwa6,Got you thank you for the response. I was thinking I'd probably end up doing it by trial and error since no one in the history of the universe has ever gotten their code bug free in the first try haha.,hhs43qf,t1_hhs0tys,1635021290.0,True
qebwa6,"Think about it this way, mark zuckerberg and jack dorsey were just college kids with a little bit of programming skills, they weren’t geniuses or anything so my guess is they just got the minimum viable product up and running then they went back later when they needed to scale and actually hired engineers to do that for them.",hhs9433,t1_hhs43qf,1635023594.0,False
qebwa6,I’m sure you could implement a social media platform based on just those ideas! Observer pattern is powerful. It would be a cool project to have under your belt.,hhscqzy,t3_qebwa6,1635025278.0,False
qebwa6,Awesome! Thank you for the response and insight if nothing else it will be a fun learning experience.,hhsgwo2,t1_hhscqzy,1635027191.0,True
qebtrg,"Some fun and/or useful ones off the top of my head:

Bubble sort! The traditional ""bad algorithm"".

Bresenham's line drawing algorithm.

Dijkstra's shunting-yard algorithm.

Mark-and-sweep.

The Mandelbrot fractal.

Binary search.

Recursive descent parsing.

Distance field raymarching.",hhs3pzj,t3_qebtrg,1635021117.0,False
qebtrg,"Also take a look at the **Window Sliding Technique** and **Kadane’s Algorithm**.  
These two have helped me solve some subarray problems MUCH faster than my brute force solutions to those type of problems. Also came up in an interview once.",hhsmw5g,t1_hhs3pzj,1635029974.0,False
qebtrg,"I've met Jay Kadane! He's a badass!

I asked him about the history of the maximum sub-array problem. I knew about Bentley presenting the problem and this is roughly his response:


JK: ""You ever de-bone a trout?""

Me: ""Nope""

JK: ""Well you can either pick out the pin bones individually or you can remove the backbone and pin bones in one shot. It was like they were individually picking out the pin bones.""


Very cool!",hhv48v3,t1_hhsmw5g,1635085324.0,False
qebtrg,"I’ll be honest, that’s such a wild way to go about describing the history of that algorithm, I did NOT see that coming! What an absolute legend!",hhvgplh,t1_hhv48v3,1635090959.0,False
qebtrg,Actually learned about line drawing algorithms in my graphics class this semester. Really interesting stuff.,hhtlv6a,t1_hhs3pzj,1635047067.0,False
qebtrg,I kinda love bubble sort being in this list.,hhstei2,t1_hhs3pzj,1635033054.0,False
qebtrg,"Along with binary search I think it's one of the best for introducing some solid computing fundamentals, even to people outside the field in general.",hhu14a5,t1_hhstei2,1635057111.0,False
qebtrg,I think it's just the natural sorting algorithm. Like as a child I would sort numbered blocks by picking up 2 and putting them in the right place and move to the next number,hi0keny,t1_hhu14a5,1635186420.0,False
qebtrg,shunting-yard makes parsing calculator or logical functions sooo much easier,hhu1li6,t1_hhs3pzj,1635057479.0,False
qebtrg,What can we use the MandleBrot for in the real world,hhu6me3,t1_hhs3pzj,1635061568.0,False
qebtrg,"Most valuable is the INSIGHT, how simple algorithms can produce most complex results.",hhu76as,t1_hhu6me3,1635062037.0,False
qebtrg,I’ve just always thought it’s good for making a cool design I didn’t know there’s a use case for it,hhu794l,t1_hhu76as,1635062105.0,False
qebtrg,Making cool images and animations.,hhuc95x,t1_hhu6me3,1635066318.0,False
qebtrg,"I'd figure search and sort algorithms are a good start, but make sure to know about Big O notation along side it",hhrwlpv,t3_qebtrg,1635017955.0,False
qebtrg,"Yeah, probably not much news to anyone here, but without knowledge of the Big O time complexity, if I had to pick between a 5 line Bubble Sort or multi-line Timsort... I'd probably go for the bubbles.",hi8jzho,t1_hhrwlpv,1635336342.0,False
qebtrg,"Binary search, radix and merge sort, breadth and depth first tree traversal, A*, Dijkstras algorithm, KMP and Boyer-Moore string search, and maybe counting sort for an example of a great hack that challenges what you thought you knew about sorting complexity.

Also for good measure it’s fun to take a deeper look at RSA, search indexing, neural nets (and gradient descent), and randomized algorithms.

Man I love algorithms and data structures. So much fun and still so much more to learn and explore.",hhs3mkr,t3_qebtrg,1635021076.0,False
qebtrg,[deleted],hhsst4e,t3_qebtrg,1635032771.0,False
qebtrg,Same found it super useful,hhszu5b,t1_hhsst4e,1635036060.0,False
qebtrg,"My use case was where we wanted users to be able to search a list and return a close match accounting for spelling errors, without using a like or wildcard match.

The length of the distance was determined by the length of the search string to prevent enumerating the entire list.",hht5lan,t1_hhszu5b,1635038835.0,False
qebtrg,Literally did the same thing with a company that had two separate databases that needed to be joined on customer name. It was painful.,hi95z52,t1_hht5lan,1635346770.0,False
qebtrg,Merge sort,hhs1mdm,t3_qebtrg,1635020190.0,False
qebtrg,"Branch and Bound techniques are widely applicable but much unused!

Techniques like A* are actually instances of a much more general set of algorithms loosely amenable to a search.

I find that some people solve almost everything with branch and bound while others (most people) rarely use them at-all.

Tasks like optimal voxel meshing or optimal triangle strip ordering are powerfully useful examples where branch and bound is king.",hhs0tqq,t3_qebtrg,1635019840.0,False
qebtrg,dfs,hhsq3ec,t3_qebtrg,1635031483.0,False
qebtrg,Least squares regression.,hhsvvrh,t3_qebtrg,1635034238.0,False
qebtrg,"Bogosort: considering various arrays, the optimistic case is the fastest of all other algorithms. So if you feel lucky, try it out!",hhud7xu,t3_qebtrg,1635067160.0,False
qebtrg,"I don't know that there are algorithms *everyone* should know, more that there are algorithms everyone doing certain things should know - and even there the sorts of things you need to 'know' in order to 'know' an algorithm are very different.
 
Sedgewick is the classic foundation. Knuth if you are doing real computer science rather than software development. But then there are a bunch of different ways to go from there - parallel computing algorithms, distributed computing algorithms, database specialised algorithms, data processing algorithms, various algorithms that prioritise certain things that are *not* cpu processing time (memory, reliability, security, network utilisation, power consumption).
 
Both Sedgewick and Knuth are a bit of a slog though. You could go with [Algorithm Design](https://theswissbay.ch/pdf/Gentoomen%20Library/Algorithms/Algorithm%20Design%20-%20John%20Kleinberg%20-%20%C3%89va%20Tardos.pdf) or [Algorithm Design and Application](https://canvas.projekti.info/ebooks/Algorithm%20Design%20and%20Applications%5BA4%5D.pdf), for a different kind of slog.
 
The day they release a Head First Data Structures & Algorithms will be the day I agree to teach a data structures and algorithms class... until then those classes will always be a friggin nightmare.",hhu41o8,t3_qebtrg,1635059439.0,False
qebtrg,I always found Knuth absolutely unreadable.,hhu7c3z,t1_hhu41o8,1635062176.0,False
qebtrg,"Have you checked out ""Grokking Algorithms""? It's not a head first and pretty limited in its content but it follows a similar idea.",hhyr5ek,t1_hhu41o8,1635152110.0,False
qebtrg,You can iterate down a list and get the top X elements in O(N) complexity. I've been asked that on 3 different job interviews so far.,hhunl9v,t3_qebtrg,1635075819.0,False
qebtrg,you have piqued my curiosity. At first I thought it would be X linear selects but that would be O(XN). How do you do this?,hhuwy4b,t1_hhunl9v,1635081593.0,False
qebtrg,I think quick select for the Xth largest and then scan through for elements bigger than that works.,hhv2qcs,t1_hhuwy4b,1635084589.0,False
qebtrg,Right.. how did I not think of that hahaha,hhv6ecq,t1_hhv2qcs,1635086345.0,False
qebtrg,Bogo sort,hhvhsy5,t3_qebtrg,1635091431.0,False
qebtrg,Stalin sort,hhsceuo,t3_qebtrg,1635025119.0,False
qebtrg,Remove the population until you're left with an empty (sorted) list?,hht1mmp,t1_hhsceuo,1635036910.0,False
qebtrg,remove everything out of order. the remaining items are sorted,hht5f9l,t1_hht1mmp,1635038753.0,False
qebtrg,"Well. If you just need a sample, that is O(n).",hhu9jet,t1_hht5f9l,1635064062.0,False
qebtrg,This is big O of what the leader wants it to be and nothing else!,hhud0s1,t1_hhu9jet,1635066984.0,False
qebtrg,Bubble tea. (Sic),hhtfdlb,t3_qebtrg,1635043628.0,False
qebtrg,Linear sum assignment,hhtm06h,t3_qebtrg,1635047137.0,False
qebtrg,Definitely berlekamp massey /s,hhtne5d,t3_qebtrg,1635047893.0,False
qebtrg,"bogo sort (aka monkey sort)

sleep sort",hhulfae,t3_qebtrg,1635074175.0,False
qebtrg,"I'm going to add BFS/DFS (personally prefer BFS cuz it's easier to implement without recursion), because I have had to use it in several places, both on online coding challenges as well as my own school project.",hhuwsa6,t3_qebtrg,1635081500.0,False
qebtrg,Metropolis hastings if ya nasty,hhverth,t3_qebtrg,1635090131.0,False
qebtrg,Fast Fourier Transform,hhw673h,t3_qebtrg,1635101205.0,False
qebtrg,Stack heap.,hirb4fs,t3_qebtrg,1635684248.0,False
qe36q2,Do you have the link?,hhqjnla,t3_qe36q2,1634993224.0,False
qe1vmc,"I'll give you my intuitive definition:

AI is simply Artificial Intellingece, so anything that we could call intelligent behaviour that's not an act of living being, i.e. from machines.

Machine Leaening and Deep Learning are methods that can be used to implement AI.

Data Science is something that aims to make sense of data. It can achieve this using statistical methods or AI.",hhqdpuw,t3_qe1vmc,1634989191.0,False
qe1vmc,"And machine learning vs deep learning: machine learning is any technique that tries to use data to make a prediction about things it hasn't seen yet. There are lots of machine learning techniques, deep learning is just one of them. It uses deep neural networks (a class of algorithm) to do machine learning. Other algorithms include decision trees and support vector machines, among others (if you feel like googling them).",hhqvkve,t1_hhqdpuw,1635000438.0,False
qe1vmc,This too thanks,hhrx1tn,t1_hhqvkve,1635018157.0,True
qe1vmc,Noted,hhrx0qu,t1_hhqdpuw,1635018143.0,True
qe1vmc,"I think it's essentially:

Data science(base of most of ai research)

ML(implementation of data science to train an entity to predict data)

Deep learning(implantation of ML to learn (mimic) the same way that humans do)

AI(implementation of deep learning and ML and other methods to mimic intelligence itself)

It's kind of blurry at the AI level but AI is generally the combination of everything, while data science is generally the base of it all.",hhqvl9c,t3_qe1vmc,1635000444.0,False
qe1vmc,Appreciate the help thanks,hhrwwk2,t1_hhqvl9c,1635018091.0,True
qe1vmc,"Deep learning- learns it's self
Ml- needs to be taught
Ai- coding
Ds- extracting info from data, prediction of future.",hhqxhdb,t3_qe1vmc,1635001524.0,False
qe1vmc,Ok thank you,hhrwrv7,t1_hhqxhdb,1635018033.0,True
qe1vmc,"In other words. Deep learning is subset of ml, ml is subset of AI. and data science is playing with data.",hhssl2h,t1_hhrwrv7,1635032665.0,False
qe1vmc,"AI = systems imitating intelligent behavior.

ML = decision/diagnostic/prediction algorithms optimizing themselves by being ""trained"" by data.  This is a subset of AI.

DL = Methods that involve ""learning"" imitating how humans themselves learn (therefore, a subset of ML).

Data Science = Commercialized + rebranded statistics: includes statistics and things that enable statistics (adjacent tech, business processes, etc.).  Often, but does not necessarily, include using AI (often ML, and sometimes DL).",hhs0na5,t3_qe1vmc,1635019761.0,False
qe1vmc,"**AI** isn't really a strictly defined field, and the word can mean lots of things. Generally, though, it means software or systems that behave in a way that looks like it would require some kind of intelligence. This could mean a software system or a robot that's able to adapt to its environment rather than just following strict and narrow predefined patterns. It could mean something that mimics human behaviour; for example bots in multiplayer games are meant to do this, along with hopefully playing the game well enough. Software that performs a task which would appear to require intelligence or expertise if the task were performed by a human could be considered AI. This could be for example finding signs of disease or injury in medical images.

You could write at least an entire book chapter on what AI means. At its broadest, though, AI can mean any kind of an artificial system that behaves in a way that you could, by some stretch of imagination, call ""intelligent"".

Intelligent (or at least intelligent-looking) behaviour can be achieved using different kinds of techniques. Although AI is often used nearly synonymously with machine learning nowadays, AI is not really any single technique or even field of techniques. It literally just means something human-made (practically software) that acts as though it possessed some kind of intelligence, regardless of how that's technically achieved.

**Machine learning** studies algorithms that infer some kind of knowledge or behaviour based on examples it has been given. A classic example would be an image classifier: you feed the algorithm images that contain pedestrians (for example), along with the information that the images contain them. You also feed it negative examples, or images that don't contain pedestrians. The algorithm *learns* some kind of a model about which kinds of images contain pedestrians and which don't. After training the classifier with the samples, you can then feed it new images and it will hopefully be able to automatically tell whether a new image it's given has a pedestrian in it or not.

That's just an example, and there are other kinds of machine learning as well. The general gist is that ML is about algorithms that can automatically ""learn"" something from examples the algorithm has been given.

There are lots of different algorithms and techniques that can be used for machine learning. Some of those algorithms are more suitable for some tasks while others work better in other tasks. All of ML is essentially computational statistics, though: you feed some data into a machine learning algorithm, and the algorithm infers some kinds of patterns from the data. Some ML algorithms are obviously just statistics and probability math being done on a computer, while others seem more like some kind of magic that just happens to work for some reason.

One class of techniques that can be used in machine learning are artificial neural networks (ANN). The inspiration for them originally came from trying to mimic (a really simplified) idea of how human or animal neurons and brains were assumed to work. (In reality, ANNs don't actually resemble real brains that much.)

Without going into details, **deep learning** is just machine learning done with artificial neural networks that are larger (or ""deeper"") than the simpler networks that had been previously used. ""Deep"" ANNs are quite flexible in terms of what they can be trained for, and they can perform well in many tasks if they're trained with enough example data. However, they also require a lot of computational resources. Other, more traditional machine learning algorithms can be trained with much less processing power.

The connection between ML and AI is that ML is one of the most common techniques used for building ""intelligent"" systems. Automatically learning from examples is obviously useful for building software that can behave ""intelligently"". ML as a field arose from research into artificial intelligence, although many other kinds of techniques were also still used for trying to build intelligent systems at the time.

**Data science** is really just an umbrella term for all kinds of statistics, data analysis, and machine learning type things that are done computationally. It includes (computational) statistics, data mining (finding interesting patterns in data), machine learning, and doing all of this efficiently on a computer so that large amounts of data can be processed.",hhs67d4,t3_qe1vmc,1635022254.0,False
qdmby0,"I also remember using that!

The website is http://nandgame.com/ if I'm thinking of the same one as you.",hhnnrxi,t3_qdmby0,1634930211.0,False
qdmby0,"Yes I am sure it is that!! I tried so many websites, thank you for letting me know :)",hhps707,t1_hhnnrxi,1634970454.0,True
qdkg3r,"Studying quantum computers requires a good knowledge of both physics (quantum physics) and computer science.

Here are some good papers to get started.

https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=quantum+computing+primer&btnG=",hhmz2uo,t3_qdkg3r,1634920162.0,False
qdkg3r,"Study this.

https://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176

Good luck.",hhpwo1l,t3_qdkg3r,1634974285.0,False
qdkg3r,Thx,hhpx4bu,t1_hhpwo1l,1634974670.0,True
qdgm1o,"Algorithms (4th Edition) https://www.amazon.com/dp/032157351X/ref=cm_sw_r_cp_api_glt_fabc_V43KK74XWVV7F9EXNGNW

Read this book and program all examples and all questions in each chapter. 

Once you do a couple and see how the algorithm translates into code via the examples in the book then it’ll get easier",hhm888y,t3_qdgm1o,1634909134.0,False
qdgm1o,Thank you so much!,hhmpvol,t1_hhm888y,1634916442.0,True
qdgm1o,Library genesis link for it http://libgen.li/edition.php?id=137306805,hholu9j,t1_hhmpvol,1634945676.0,False
qdgm1o,thanks so much :)),hhorrth,t1_hholu9j,1634948720.0,True
qdgm1o,"This book also has a corresponding site that has lecture slides and videos and lots of good resources! I'm using it in my data structures and algorithms class and it's rly good. 

https://algs4.cs.princeton.edu/home/",hhnlue1,t1_hhm888y,1634929410.0,False
qdgm1o,Thank you!!,hhorra4,t1_hhnlue1,1634948713.0,True
qdgm1o,"When you're stuck, it's usually one of the two categories:

1. Kinda feel it should go this direction, but you're convinced/afraid something won't work. For this you just need to read others' code see how they deal with that.

2. Don't even know where to start. This indicates you don't grasp the theory as good as you think you do. Should go back to reading theory.",hhm980m,t3_qdgm1o,1634909573.0,False
qdgm1o,"Hmm yeah, but I feel like when I look at others' code, I'm just copying and I don't actually learn anything?",hhmq0f7,t1_hhm980m,1634916494.0,True
qdgm1o,"You need to squash that notion or it will hold you back from learning at the pace that you’re capable of. Looking at other people’s code and yes, even copying it verbatim, is exactly what you need at this point in your development.

For a while it may feel like you’re not learning but believe me, you are, as long as you take the time to read through the code as you’re copying it — make sure you understand what every statement and variable does before you allow yourself to move on.

By doing this, you’ll expose yourself to implementations of algorithms that you aren’t yet able to implement from scratch, but by repeatedly using them you’ll train your mind to think in those abstractions - variations of trees, maps, tries, graphs, etc. - and soon you’ll find yourself combining them, tweaking them, and eventually reimplementing them from scratch.

This is how you learn, not by studying the bike but by riding it. After all, training wheels are not there to stop you learning, they are there to stop you falling.",hhn4x4x,t1_hhmq0f7,1634922529.0,False
qdgm1o,wow... thank you,hhot318,t1_hhn4x4x,1634949386.0,True
qdgm1o,"If you're not copying the code for the exact homework problem you submit, but rather you read/copy code to learn first, *then* do homework problem *from scratch*, you'll learn more than you expect",hhos35t,t1_hhmq0f7,1634948882.0,False
qdgm1o,"ooh that does make sense, thank you!",hhowgr3,t1_hhos35t,1634951141.0,True
qdgm1o,"There are only so many ways to implement a solution to common basic algos. There will be differences, but it's not going to huge.",hhot7zk,t1_hhmq0f7,1634949456.0,False
qdgm1o,"Bro, I'm a software engineer and currently doing a PhD, I can tell you that looking at other people's code is a huge part of the job. By doing this you learn new ways to implement things and inspiration for your future project. In short learn from people ! 

Plus forums such as stack overflow (huge basic) have a great community willing to help anyone for anu problem and most of the team with a teaching spirit. 

By forging one becomes a black smith well by coding one becomes a coder ! Don't be afraid to make misteaks (and listen to your compiler/interpreter goddamnit !)",hho36zn,t1_hhmq0f7,1634936705.0,False
qdgm1o,"haha, thanks for the insight :)",hhosfga,t1_hho36zn,1634949057.0,True
qdgm1o,"Show me :) 

More specifically, post some code online (a github repo would be ideal, nobody will care how crappy your code might be, don't worry about that). I've got a good feeling that just writing the code and posting it online, trying to make sure you didn't miss anything.. will make you ""get"" where the mistake is. If not, there's always [stackoverflow.com](https://stackoverflow.com). 

When it comes to translating algorithms into code, sometimes being familiar with a language helps. For example, you could try to implement a linked list in ANSI C, but perhaps having familiarity with Python or Java would make it easier. 

What language are you using? What algorithm are you implementing?

Keep in mind that every Turing-complete language can program every algorithm that will ever be written, but a computer program is meant to be understandable to humans. The executable code is meant for a computer.",hhne186,t3_qdgm1o,1634926209.0,False
qdgm1o,thanks!,hhotwl6,t1_hhne186,1634949798.0,True
qdgm1o,"What language are you trying to implement these algorithms in?  If it's a language that doesn't have pointers/references you're going to have a hard time.

It's common to implement algorithms in C/C++/Java or similar languages.  You can do it in Python, but it's as not recommended due to the language not explicitly having some of the concepts necessary to implement algorithms.",hhotpoh,t3_qdgm1o,1634949702.0,False
qdgm1o,"yeah, I'm trying to implement them in c++",hhow8i3,t1_hhotpoh,1634951020.0,True
qdgm1o,"It can be quite the hurdle to get to the point where you're good enough in C++ to be able to write the interface part of a class, let alone everything before it.  So already you're starting with a high barrier of entry before beginning to write a data structure.  C++ is definitely hard mode, but well worth it if you stick it out.

Are you familiar with the basics like https://en.cppreference.com/w/cpp/language/rule_of_three ?",hhrnh5p,t1_hhow8i3,1635013947.0,False
qdgm1o,"Yeah, thanks :)",hhtq0kv,t1_hhrnh5p,1635049439.0,True
qdgm1o,"Is this is Robert Sedgewick course?

Let me warn you… I volunteer with a math teacher to help teach high school computer science.  A few years ago I worked with a group of students on that exact course, and I broke several of them… that course is not geared towards high school… it would be rough on a freshman or sophomore in college.  There are much better resources out there for a high schooler to learn algorithms.

If you find it tough, don’t get discouraged… just find better material.

I’d suggest anything aimed at career software engineers who transitioned into the field from another career path… like books from PragProg or Manning.

https://pragprog.com/titles/jwdsal2/a-common-sense-guide-to-data-structures-and-algorithms-second-edition/",hho1y1j,t3_qdgm1o,1634936150.0,False
qdgm1o,"Nah, I'd rather struggle through it if I think I could learn more, even if it takes me more time and effort. Thanks for the advice though!",hhoslzn,t1_hho1y1j,1634949150.0,True
qdgm1o,"If you want more resources, check out [visualgo.net](https://visualgo.net) and [https://www.cs.usfca.edu/\~galles/visualization/Algorithms.html](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html). Don't be overwhelmed by the sheer amount of algos/ds - some of them are more widely used than others. If you want to visualize trees on your screen instead of drawing them, you use Graphviz: [https://dreampuf.github.io/GraphvizOnline](https://dreampuf.github.io/GraphvizOnline).",hhpiwuh,t3_qdgm1o,1634963608.0,False
qdgm1o,Thank you so much!!!,hhpjmvj,t1_hhpiwuh,1634964065.0,True
qdgm1o,Search for the algorithm in Wikipedia. You'll find pseudocode that you just translate into whatever language you are using,hhnpwmp,t3_qdgm1o,1634931090.0,False
qdgm1o,"yeah, the translating part is the problem",hhov8g0,t1_hhnpwmp,1634950495.0,True
qdgm1o,"you clearly lack experience in programming. with practice and experience you will learn to translate your thoughts into code. you are just not good at translating yet.

learning more about algorithms is probably secondary for this problem but it may also shape your thought process in a good way that is more similar to code for example more procedural, structural and recursive instead of intuition and association based.

look for other people's code AND explanations to leetcode type problems.

Edit:

how much would you struggle writing code to find the index of the second largest element in an array? can you reverse a linked list in constant size? the simple code maneuvers to juggle and keep track of data in variables is probably what you need to get good at first. solving big problems is a bunch of these maneuvers after you construct the solution in your head but that solution needs to be code-friendly i.e easy to transfer to code, for that see before edit",hhnuwhi,t3_qdgm1o,1634933135.0,False
qdgm1o,thanks! I'll work on that more,hhow4po,t1_hhnuwhi,1634950966.0,True
qdgm1o,[removed],hhnwq5a,t3_qdgm1o,1634933905.0,False
qdgm1o,thank you for the tip!,hhou8jn,t1_hhnwq5a,1634949969.0,True
qdgm1o,"That's normal. Make sure you understand what Abstract Data Types are. And learn how a ADT can be implemented in many ways. Try to implement a simple operation in data structure. For instance try implementing push/pop methods in Stack. If you can't come up with solutions, it's okay too. Look at the examples, learn, reflect and brainstorm how it can be implemented in various ways.",hhnwrpo,t3_qdgm1o,1634933923.0,False
qdgm1o,thank you!,hhostq0,t1_hhnwrpo,1634949260.0,True
qdgm1o,"Don't worry! It sounds like you are still in that phase of your learning to code where you get the core concepts but things haven't sunk in yet. Specifically, you are not yet fluent in the language you are writing in. This feels like you know what you want to say but you don't know how to say it. 

I get this again every time I learn a new computer language. For me the only solution is to power through it. If I had a conceptual problem I could go for a walk or sleep on it and it would help. But when stuck with writers block, I just have to power through. This means just sitting at my computer writing code and googling things until I get it. This can take hours, but there really is no other way. Remember that the code is a language to tell the computer what you want it to do, and errors are the computer telling you it doesn't understand what you are saying. Think about if you were learning french. You can't really get good at it until you speak it to Someone and actually realize how to communicate in that language. Especially if you are communicating complex topics like philosophy. It requires just as good a grasp of the philosophy as of the language. 

Some tips:

- Break things down into smaller steps. 

- Write psuedo code. Start with plain English your mama would understand and gradually modify and expand that to be closer to the language and what the computer needs to do. 

- Try every idea you get, even if you think it won't work. Maybe you will get a better understanding of why it doesn't work and it will open new paths. 

- be curious. Learn how everything in the language works exactly. Read a couple blogs on every concept! And 

- don't copy any code. It's ok to find the code you need somewhere, but until you are fluent, rewrite EVERY code snippet you use. The motor memory helps in amazing ways. 

Good luck!",hho3cjr,t3_qdgm1o,1634936774.0,False
qdgm1o,Thanks for the tips :) they really help,hhos3a2,t1_hho3cjr,1634948884.0,True
qdgm1o,"If you “know” a word, but can’t define it: you don’t know the word.

If you “know” your position is right, but can’t give a clear proof of it then you never actually knew that, you just suspected it.

If you “understand” an algorithm, but then can’t implement it then you don’t understand the algorithm (assuming you know enough basic coding to implement things).

None of this is meant to discourage.  
This is to encourage you to understand how our brains work.  
We often “know” something — and what has happened is that we understand *part* of it or are able to answer *some* questions  about it.  And our brains, which evolved to be lazy to be efficient, say: “done!”, no outstanding problems so we got kt.

That is a *cognitive illusion*.  
The reason we must test ourselves is because feeling we understand something is not proof of understanding.

TLDR:  you understand some part of it, but have more to learn.  No problem.  That’s how learning works.  Don’t be hard on yourself for not really understanding, but *do* be suspicious of the part of you that say you understand things — and use that suspicion to discover deeper understanding than most people allows themself.",hhoa0sw,t3_qdgm1o,1634939833.0,False
qdgm1o,"alright, thanks for the advice!",hhou1n5,t1_hhoa0sw,1634949870.0,True
qdgm1o,Join a discord community. I was only able to grow is due to people that helped me while I was getting stuck a lot of times,hhpxp1q,t3_qdgm1o,1634975182.0,False
qdgm1o,Do you have any recommendations?,hhq9qda,t1_hhpxp1q,1634985995.0,True
qdgm1o,https://discord.gg/H2MVDhSs,hhu30vb,t1_hhq9qda,1635058613.0,False
qdgm1o,"It sounds like you just need some more experience with programming. Keep at it, and your brain will start to recognize the patterns!",hhq28va,t3_qdgm1o,1634979287.0,False
qdgm1o,Thank you!!,hhq9ryd,t1_hhq28va,1634986034.0,True
qdgm1o,"My friend recently did a data structure course on coursera and If it was the same one then I wouldn't be much surprised, the course was really difficult even for my friend, who had been coding for a year now. I would suggest reading [this book](https://www.amazon.com/Grokking-Algorithms-illustrated-programmers-curious/dp/1617292230), It helped him understand the basics a lot better than the course did.",hhn1kzf,t3_qdgm1o,1634921163.0,False
qdgm1o,"Haha, thanks! Unfortunately I'm a broke highschool student, but I'll look into the book :)",hhn3jb6,t1_hhn1kzf,1634921957.0,True
qdgm1o,Look up library genesis… :) (use u-block origin too),hhnh1c3,t1_hhn3jb6,1634927423.0,False
qd6bj4,"Take a look among [these linux distros](https://www.techradar.com/best/best-linux-distros-for-education), geared towards education.",hhkysia,t3_qd6bj4,1634876683.0,False
qd6bj4,"Installing things like books, calculators, games, video/editing software, and anything else that would be helpful to have on a PC that you don't necessarily need internet to maintain or use.",hhlfe73,t3_qd6bj4,1634889115.0,False
qd6bj4,Do you have any suggestions where I can download such things?,hhlmiro,t1_hhlfe73,1634895479.0,True
qd6bj4,you can also torrent software. It's okay it's for education it's still ethical,hhmfw7e,t1_hhlmiro,1634912397.0,False
qd6bj4,"Wouldn't they most likely format them and install what they want on them? They wouldn't want to risk you having left your porn folder on them :)

Have you asked them?",hhkf1cj,t3_qd6bj4,1634866571.0,False
qd6bj4,Yes people in developing areas with limited experience and access to electronics will simply format and install whatever they want from their local fiber optic internet connection. Of course!,hhlf7gx,t1_hhkf1cj,1634888958.0,False
qd6bj4,Lol dude read the part that says no fucking internet,hogkst3,t1_hhkf1cj,1639448773.0,False
qd6bj4,"Pack some books and resources for the teachers so they can pass the knowledge to students. Also remember  that even most adults in poor areas don’t know how to use the computer, so even basic Microsoft Office will help them.",hhn1s36,t3_qd6bj4,1634921244.0,False
qd6bj4,You should leave porn on them.,hhku4s4,t3_qd6bj4,1634873976.0,False
qd407r,"The only thought I would have is the reset function of the registers that would set the loop back to 00. Or if there was a if statement that turned off the register. For example you could have an if statement of “if S > n then remove clock from register or remove the ability to load a new number”

There is no if logic in the circuit and the loop has to have if logic to stop so it has to be wrong. 

(Note: I may be making a mistake, it’s been a while since I worked with this stuff)",hhknsqz,t3_qd407r,1634870749.0,False
qd407r,"I understand registers on a software level. I've interacted with them a lot when I had an internship doing reverse engineering. But, I've wanted to up my knowledge so I'm resisting the digital logic implementations of them. I digress, but it was to illustrate I have somewhat of an idea of what I'm doing.

My thoughts when is when the control signal from the register is sent (e.g., reset), that indicates the end of the loop.

As for the initialization, the load signal indicates to replace the state of S. In other words, initialize S.

So, to recap:

The register does the following ( slight over simplification):

1. CLK = State Change
2. LD = Initializes
3. Reset = Control",hhkt47z,t1_hhknsqz,1634873430.0,True
qczvme,"A while loop will continue running as long as the condition is true. 1 is, by definition, true (depending on the language)",hhj60rq,t3_qczvme,1634846439.0,False
qczvme,Just to clarify: 0 is considered false. True is anything that's not 0. You could just as well type while(-1) or while(69420).,hhjus8m,t1_hhj60rq,1634857041.0,False
qczvme,Nice,hhk0j3s,t1_hhjus8m,1634859769.0,False
qczvme,Found the programmer!,hhk3y4x,t1_hhjus8m,1634861401.0,False
qczvme,"…Except in Lua, Ruby, and basically in Lisp. In those languages also *0* wouldn’t be considered false.

Edit: I put a typo. Forgot the negation / “n’t”.",hhkjuve,t1_hhjus8m,1634868848.0,False
qczvme,He said 0 is considered false. So what is the difference then?,hhl95h6,t1_hhkjuve,1634883950.0,False
qczvme,elaborate,hhlnqx7,t1_hhl95h6,1634896486.0,False
qczvme,"The difference would be that 0 would actually be considered *true* in Lua, Ruby, and Lisp.",hhlq1mh,t1_hhl95h6,1634898389.0,False
qczvme,Is “0” (a string) considered true or false?,hhm40ng,t1_hhjus8m,1634907162.0,False
qczvme,"Some languages might automatically convert it to an int, but in most cases, it will be true. That's because strings are generally just an array of Unicode codes. The Unicode code for the char '0' is actually 30. In general, if you want the actual value 0 in a string, you have to put a backslash behind it, like so: ""\0"". 

But even then, it still won't evaluate to false, because strings actually have an additional hidden 0 at the end of them, used to know where the string ends. So ""0"" is actually an array that is equivalent to [30, 0]. 

Even an empty array [] will be considered true unless they get automatically converted by the language. That is because an array is actually a reference to a memory location, or in other words, a positive integer representing the start of your array.

For a string, or any other array, to evaluate to false, the reference would actually have to contain the value 0. This is typically called the nullpointer, or in other words, null.",hhmkgql,t1_hhm40ng,1634914243.0,False
qczvme,"Interesting, thank you!",hhmt1i3,t1_hhmkgql,1634917716.0,False
qczvme,"either true or a type error, depending on the language

edit: actually I think it's false in PHP",hhm4nh4,t1_hhm40ng,1634907473.0,False
qczvme,"Yes 
1 is equivalent to true and 
0 is equivalent to false.",hhlofoa,t1_hhj60rq,1634897068.0,False
qczvme,Because 1 is usually true in programming languages,hhj4qwd,t3_qczvme,1634845932.0,False
qczvme,"The while() statement expects an expression that resolves to a boolean value. In C, any nonzero integer resolves to true and zero resolves to false. Hence while(1) is a loop with an always true condition.",hhj6o4w,t3_qczvme,1634846698.0,False
qczvme,while(2)  is even more fun.,hhjaneu,t3_qczvme,1634848271.0,False
qczvme,Not as fun as while(-1),hhjk8aw,t1_hhjaneu,1634852279.0,False
qczvme,while(infinity),hhk28r8,t1_hhjk8aw,1634860577.0,False
qczvme,NameError: name 'infinity' is not defined in this namespace,hhm4jnx,t1_hhk28r8,1634907421.0,False
qczvme,while(Math.infinity),hho2utm,t1_hhm4jnx,1634936558.0,False
qczvme,While(infinity + 1),hhlhxsd,t1_hhk28r8,1634891347.0,False
qczvme,while( ( (1 + 1) == 2 ) == 1),hhk2pa8,t1_hhjaneu,1634860798.0,False
qczvme,efficient,hhlcmfc,t1_hhk2pa8,1634886816.0,False
qczvme,"Surprised no one mentioned it, but go look up truthy logic in different programming languages. It is the concept applied here",hhjlkxa,t3_qczvme,1634852862.0,False
qczvme,"To elaborate on the other answers: when they say ""1 is true"" what they mean is that the value 1 is typically used to *represent* something being true. And 0 is typically used to *represent* something being false. It's an arbitrary choice. A clearer way to write an infinite loop is ""while(true)"".",hhj6ibi,t3_qczvme,1634846633.0,False
qczvme,"I'm honestly curious, and I hope you can correct me if you know the correct answer. C (not C++) doesn't have 'true', right? What I mean to say is it isn't defined as a keyword... I'm beginning to doubt myself because it sounds idiotic... but I swear I learned something like this once long ago...",hhk9h30,t1_hhj6ibi,1634863987.0,False
qczvme,"Yes, correct. But C does have <stdbool.h> which #define's bool, true and false as well.",hhkb66a,t1_hhk9h30,1634864778.0,False
qczvme,Not really. They are just convenient types you're supposed to convene to the reader that something is boolean. C only has integer types. Even `_Bool` is just a 1-bit integer by the standard. The semantics of conditions are defined by comparsion with 0. The semantics of relational comparsion operators are defined to literally give 0 or 1.,hhloic4,t1_hhkb66a,1634897128.0,False
qczvme,So... which of my statements are incorrect?,hhm1tsa,t1_hhloic4,1634906032.0,False
qczvme,"C has ""true"" and ""false"" defined via preprocessor to be 1 and 0.",hhkesyt,t1_hhk9h30,1634866465.0,False
qczvme,"Most languages have this concept called **truthiness** which you neglected to mention.  &for loops are more common therefor 
```
for(int i=0;~i;[=]{
    if(breakCondition)
        i=~0;
});
```
is easier to understand. 
/s but I’ve seen this argument before at work.",hhju2i9,t1_hhj6ibi,1634856706.0,False
qczvme,I need to bleach my eye,hhk55re,t1_hhju2i9,1634861979.0,False
qczvme,"1, 2, 3 any none zero number Is true",hhjhq7u,t3_qczvme,1634851198.0,False
qczvme,"What many are saying here isn't entirely accurate; it's not that 1 is true, it's that anything that's not 0 is true.",hhjhy8w,t3_qczvme,1634851292.0,False
qczvme,"It depends on programming language. You expect to have some boolean (true/false) value as condition in parentheses. Some languages would convert value in brackets to boolean if it's not boolean already. In others you will get error message.


For example, see below link on what rules PowerShell uses to make such conversions:

https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_booleans?view=powershell-7.1",hhj7yb6,t3_qczvme,1634847207.0,False
qczvme,"In a transistor, “1” means it’s on, which means true to the computer. Likewise, “0” is off, and subsequently means false.",hhjlx75,t3_qczvme,1634853010.0,False
qczvme,"While what everyone else says about 1 being true is correct, there's another piece...

It's *also* that the while loop doesn't have an *alternative* exist clause. Such as

\`\`\`if (time.current > time(""Noon"")){ break; }\`\`\`",hhjkfxd,t3_qczvme,1634852373.0,False
qczvme,"While(true){}

  
  This is how it translates",hhjqr1u,t3_qczvme,1634855173.0,False
qczvme,"It comes down to truthy and falsy values. Truthy values are pretty much anything but your language's equivalent of `null`, `undefined`, 0, `false`, etc. Everything else is truthy including empty objects `{}` if we're dealing with JavaScript",hhjsn6s,t3_qczvme,1634856046.0,False
qczvme,you forgot my favorite value: -0,hhl5kzh,t1_hhjsn6s,1634881211.0,False
qczvme,Programming languages without a built in boolean type sometimes handle comparison operations by having the expression return zero for false and non-zero for true. I'd have to check to be sure but I think most CPUs use something like this to return the result of comparison instructions and conditional jump instructions that are used to build loops in compiled languages.,hhjup0l,t3_qczvme,1634856999.0,False
qczvme,"Technically, it depends on what's inside the loop. A `while(1)` loop is a perfectly valid construct, as long as there's some code inside the loop that will issue a `break;` \- usually tied to some conditional.

But yes, `while(1);` is a complete and absolute endless loop, since the `1` is considered *not false*, and will never change.",hhjvls0,t3_qczvme,1634857432.0,False
qczvme,Now I have to try this,hhjx8b5,t3_qczvme,1634858204.0,False
qczvme,"in c++ bool, char, int, ... - are all numbers. 1 <=> char(1) <=> true",hhk2hzs,t3_qczvme,1634860700.0,False
qczvme,"It’s the same as while(true). Everything’s a bit eventually: 1 is true, 0 is false.  

Doing this instead of throwing an error can make looking at conditions “cleaner”.

Another example would be null being evaluated as false to make null checks easier.",hhkie7y,t3_qczvme,1634868154.0,False
qczvme,Because 1 is always not zero.,hhkoqcn,t3_qczvme,1634871209.0,False
qczvme,Why will an open window endlessly let the outside air environment in?,hhl3i4s,t3_qczvme,1634879742.0,False
qczvme,"its called ""truthiness"" 

here's an article about truthiness in javascript

https://developer.mozilla.org/en-US/docs/Glossary/Truthy",hhl55nn,t3_qczvme,1634880905.0,False
qczvme,Is it actually endless? Can you prove that? Look up the halting problem for a real  head scratcher.,hhl7e7v,t3_qczvme,1634882570.0,False
qczvme,Always true 🤣,hhlbymg,t3_qczvme,1634886253.0,False
qczvme,In a while loop if the condition is true the loop never stop. In many languages integer 1 is considered as true and 0 is considered as false. If you know python you may know that wrapping an integer 1 with bool ( bool (1) -> true ) gives true,hhlnpbj,t3_qczvme,1634896449.0,False
qczvme,Didn’t a mod make a post about questions like this recently? I thought we didn’t allow these types of questions,hhn6wkd,t3_qczvme,1634923338.0,False
qczvme,Also: https://www.google.dk/search?q=why+is+while(1)+an+endless+loop,hhn73w5,t1_hhn6wkd,1634923424.0,False
qczvme,"1 and 0 are binary equivalents to true and false. In bitwise logic 1 is considered on, or true. So this is the same as while(true). In many programming languages you can instantiate a value as Boolean type, and still give/expect 1 or 0 as valid arguments",hhjdtt5,t3_qczvme,1634849544.0,False
qcxk48,The IBM 650 did just that.,hhiula0,t3_qcxk48,1634841795.0,False
qcxk48,"They can, but there is no reason to do so.",hhj0wvn,t3_qcxk48,1634844377.0,False
qcxk48,"This is absolutely right, I'll add some additional details and other good reasons not to.

The easiest answer is as /u/Zepb said: Spinning hard disks are so incredibly slow (humans are bad at orders of magnitude, but we're talking 100000x slower than DRAM). You can easily run a full blow neural net in the time it takes to read one byte from a disk. Talking to a server on the other side of the country is only about 20x slower than reading one byte from a disk. It would be pointless to try and read that from disk every cycle.

That being said, not all storage is that slow. Some non-volatile memory (NVM) technologies are getting fast enough that it's not totally insane to think about treating them like memories (e.g. only 10x slower than DRAM). In this case, some people are looking at addressing storage (info stays put if you turn it off) like main memory. When you have caches and stuff, it's actually plausible. The problem then becomes how you make sense of the information you stored. If my program crashes half way through, how do I make sense of the information it's already written? If two processes are reading/writing to the storage, how do we coordinate (particularly if we want everything to continue making sense later)? These are very tricky issues and this technique is a very active area of research. My personal take is that then non-volatility (storage) part of NVM is frankly not that interesting. The fact that it's really big and can be turned off to save power, however, is very interesting.

Ultimately, there are always a million tradeoffs we make in software/hardware. How we write programs, how we think about storage, how these big complex systems behave and interact, this all controls what we decide to do. Markets and technology change, and there's lots of room for creative memory solutions.",hhj6h8b,t1_hhj0wvn,1634846621.0,False
qcxk48,"I like to add why RAM is important, besides being fast, since this is what OP want to know. RAM stands for Random Access Memory which means that every address (you can think of it as a single storage cell) can be accessed in the same amount of time. This helps to make a programm run equally fast each time it is executed, regardless of the distribution of the data in memory. A HDD is usually not considered to be a RAM since the access time depends on the position of the read-head and the position of the data on the disk. So there is a huge variation in access time.

To unblame HDDs, they are very good at storing huge amount of data and if the data is read in consecutive blocks it is pretty fast. But the advantages of HDD are only at handling large volumes of data which is not needed in programm execution.

HDDs are not exactly RAM but also not exactly not-RAM. A better example is tape memory, these are LAM Linear Access Memory. There the data is stored on a big magnetic tape like the old video-tapes. You have to scroll through the tape until you can read the data from a single position. Depending on where the data is stored it takes different amount of time to access it. To some extend a HDD has similar limitations.",hhl2pfw,t1_hhj6h8b,1634879198.0,False
qcxk48,"So the way a program runs, at a very low level, is that it's all just 1s and 0s. For each ""step"" of the program, a cycle runs, where depending on instructions that it's given, it'll make changes to the information it's dealing with. To do this, it runs through the entirety of the information stored in the memory. 

The CPU is processing and writing data to the ram is a fairly continuous cycle, not byte addressable is referring to how it's both running through the entire set of data it's working with, and that it's using a variable amount of that data- both things that are hard if you just have a massive volume to try and deal with at once. 



It's hard to find let alone work off of a recipe in your bookshelf, it's easy if you are just working with the cookbook.",hhirkvn,t3_qcxk48,1634840583.0,False
qcxk48,"CPUs expect to get their data via memory, so they have a memory controller that electrically interfaces to the memory.

CPUs don't have a hard disk controller, so there needs to be something in between the CPU and the hard disk so the CPU can get the program instructions from the disk.   In current systems, that 'something' is the operating system, which interfaces to the hard drive control hardware.

And you could make a hard drive byte addressable, but the overhead in making a request is high so it isn't very useful.",hhj63gv,t3_qcxk48,1634846468.0,False
qcxk48,"There's this thing called swap file that Windows uses in case it runs out of RAM. I changed its configuration without knowing what I was doing and my computer was booting for over 30 minutes.

I don't know how it's working exactly, but I'm guessing that because I've setup file to be quite large system was accesing hard drive much more frequently looking for data that would normally be stored in RAM.

It had maybe 64 or 128 MB RAM and probably around 10GB HDD. As @ArgoNunya writes here, maybe with modern SSDs it would be much faster than what I've experienced. But I'm sure average PC user will never need to find this out, because we have gone quite a long way since year 2000 in terms of RAM capacity.",hhjbvmg,t3_qcxk48,1634848759.0,False
qcxh83,Your BIOS is in ROM - Read-Only Memory.,hhiqcx2,t3_qcxh83,1634840091.0,False
qcxh83,Except you can update it. So it’s not all that read-only.,hhj1a73,t1_hhiqcx2,1634844528.0,False
qcxh83,I guess back in the day was true ROM when you had to replace the chip to update the bios. Now it's more of a flash memory locked unless you need to update.,hhj6dpz,t1_hhj1a73,1634846583.0,False
qcxh83,I think we still just call it ROM for short when in reality it something like EPROM (Erasable programmable read only memory).,hhjfs1i,t1_hhj1a73,1634850360.0,False
qcxh83,"Also, to be clear, mobile phones have 32 or 64 (or however much) of storage, never of their memory (ram)",hhipchr,t3_qcxh83,1634839677.0,False
qcxh83,"There is a hard drive in your computer, the operating system is on it.


Memory (ram) clears when your computer it turned off, storage (HDD/SSD) doesn't. 



Edit: every computer component has some rom but you usually can't access it.",hhinz3q,t3_qcxh83,1634839112.0,False
qcxh83,ROM is faster than SSD?,hhiovej,t1_hhinz3q,1634839482.0,True
qcxh83,ROM isn't a component to be comparing. It's not a term you need to deal with in beginning CS or if you're building your computer.,hhip11a,t1_hhiovej,1634839546.0,False
qcxh83,"I have not thought about this in a solid 6 years, so forgive me.

But wouldn't ROM be at least semi important, as it contains the instructions for bootstrapping during startup? Like where to find the OS and such.",hhiplkx,t1_hhip11a,1634839780.0,False
qcxh83,"Right, but instead of ROM, we usually have Read-Write Memory, so that we can change things about bios, install a new driver, update the operating system, et c. Only things that would never for any reason ever have to change (like the framework of the BIOS itself) would be written to ROM, so as such, not a lot of space is dedicated to it. 



All of the storage (be it flash, HDD or SSD format) is Read-Write memory",hhiqccw,t1_hhiplkx,1634840084.0,False
qcxh83,"Older computers used rom only though, correct?",hhiuyvm,t1_hhiqccw,1634841946.0,False
qcxh83,"Not since punchcard days, anything that saved information once the computer was off had a write function.",hhivcor,t1_hhiuyvm,1634842101.0,False
qcxh83,"I have a confusion , are RAM and ROM are physical stuffs or these are just concepts?",hhipn95,t1_hhip11a,1634839799.0,True
qcxh83,"Ah, got it, so we're dealing with two things here- RAM, random access memory is a term being used twice here, it's both a function AND a component. For lack of creativity, the ram in your computer is provided by stick of ram. 


Read Only Memory is also a function, but it doesn't have a piece of hardware dedicated to it, all the parts have tiny chips that have ROM functions, but there isn't a ROM component.",hhiq207,t1_hhipn95,1634839967.0,False
qcxh83,"RAM and ROM are general concepts.

Random access memory is memory can stored to or read from anywhere in the memory space (in contrast to, say, a tape where you've got to read the data one after the other).

Read only memory is memory that can only be read from and not stored to. For example, a CD.

However, in terms of the real world and building PCs, ""RAM"" is often shorthand for the component known as ""DRAM"", which is a computer component (look at [this wikipedia page](https://en.wikipedia.org/wiki/Dynamic_random-access_memory) for more) that holds the working memory for a computer. The CPU will read to and write to it randomly (random here meaning freely, at will, at any point of memory).

The concept of Random Access Memory (shortened to RAM) and the computer component commonly known as ""RAM"" are not identical.

There is no component in a PC named ROM (even though the concept of read only memory does exist in a PC, such as in firmware)",hhiquoo,t1_hhipn95,1634840291.0,False
qcxh83,"ROM, as far as consumer computers go, is known as BIOS. This is the system that tells your computer how to startup. You can think of it kinda like the starter in your car; in that is mostly useful in the boot process (though, it does provide a lot of hardware-level functions during normal operation like turning on the fans when the CPU gets too hot). It is almost never an interesting question to ask ""how much ROM do I have"" because it's tiny (a few Kbs) and the information stored there is only useful if you are installing a new operating system or trying to manually modify your hardware operation (which, you shouldn't unless you know what you are doing). 

&#x200B;

RAM and HDD/SDDs (gonna call it ""disk"" from here on) are interesting because they are where your actual data and operating system get stored. In practice, RAM is ""Working memory"" and disk is ""long term memory"". When you start up the OS, it is on the disk, but gets copied into RAM so it can be run. Same with any program you start up; it gets copied from disk to RAM, then it starts executing in the CPU. All your devices also write essentially directly to RAM as well. Your keyboard, mouse, USB drive, network card, etc all dump data into RAM (well, it is a little more complicated than that, but it's true enough).  The reason you have to load it from Disk to RAM is partly because disk is so far, Partly because disks are cheaper and therfore store more information, and partly that the CPU would be waiting too long. But the most important reason is that RAM is erased when you turn off your computer/phone, but Disks are not.  

&#x200B;

So finally, to answer your question, disks are slow, large, persistent storage. They don't count as RAM because they are so slow that you have to copy from them to RAM to even do anything with it. The 32/64 GBs of memory in your phone might be RAM, but it is probably disk. Phones, even high end ones have around 0.5-8GB. The latest iPhone has 6GB, but hey, there might be an android phone that has 64GB of RAM: it would be strange but not impossible. So that 32/64GB in your phone is probably disk. And if you upgrade it, like with an SD Card, that would be an increase in disk, not RAM.",hhjayxe,t3_qcxh83,1634848401.0,False
qcxh83,"Initially the OS stays in the hard disk of your computer. When computer is turned ON your BIOS which is hardcoded in the motherboard loads your OS into RAM. 

Every computer, laptop, or mobile has both RAM and secondary storage called Harddisk present in it. Secondary storage keeps all the programs in it. Programs which are important / needed at a time those are transfer to RAM and from there processor process the program",hhqppbz,t3_qcxh83,1634996553.0,False
qcjqwn,"Assume a box and you put a label on that according to what you want to store in it, that label is called data type and how you arrange those boxes is data structures.

For example stack, queue, linklist, trees are data structures,
Where as int, float, double, bool are data types.

Array is a data structures where each element (boxes) are placed at continuous location(one after another).",hhghfts,t3_qcjqwn,1634792989.0,False
qcjqwn,"That's a solid start, but you didn't mention that data types can be unions of types (see Discriminated Unions in any number of programming languages, or non-discriminated unions in something like C/C++).

Types can themselves also be a data structure. And while arrays are certainly a data structure, in many languages `int []` is a type, as well as a structure of contiguous memory and effectively a struct and/or vtable for object attributes. Similarly a lot of functional programming languages might have a `'a list` or `'a Tree` to represent those data structures. I suppose you could draw a distinction between the name and the in-memory structure, but that can lead to confusion.

And then of course there are structural types where the type is literally defined by the structure of the data...",hhjmfbs,t1_hhghfts,1634853230.0,False
qcjqwn,All data stored in a classical computer can be thought of as data structures of simple sets of bits.,hhkin1u,t1_hhjmfbs,1634868267.0,False
qcjqwn,"Sure, but that's not a particularly useful way to think about it. If you're at that low of a level, you're not going to want to abstract away the hardware to that extent. Even if you're only talking about in-memory storage (why?) all addresses are created equal--they could point to anything from hardware registers to no-execute pages, memory-mapped files, etc. And don't get started on what addresses work out to when dealing with solid state storage.  
The point of structuring data is to make reasoning about it and making algorithms to handle it easier and more efficiently. Many of the best results in data structures have nothing to do with bits--they'd work just fine using trinary, or some sort of discretized analog memory.",hovpt31,t1_hhkin1u,1639717971.0,False
qcjqwn,Superb explained 👏,hhigoio,t1_hhghfts,1634836178.0,False
qcjqwn,"I think you meant to say contiguous, but continuous kind of works too.",hhipnii,t1_hhghfts,1634839802.0,False
qcjqwn,Nobody enjoys the grammar police,hhiz7cw,t1_hhipnii,1634843678.0,False
qcjqwn,"It’s not grammar, it’s just the right word selection.",hhizohz,t1_hhiz7cw,1634843875.0,False
qcjqwn,"People trying to get a bit too meta in this thread.

Data types describe what a variable can contain. For example, an ""integer"" datatype can hold integers. A ""String"" data type can hold ""strings"". If you create a class called VendingMachine then a VendingMachine variable can hold instances of VendingMachines.

A data structure is an abstract scheme for organizing data. For example, a binary tree is a way to organize data into nodes, grouped into a tree structure. But this is abstract. The computer doesn't automatically know how to use a data structure. An ""array"" - colloquially speaking is a (sometimes not 100% formally defined) data structure. You're saying the data is lined up in some kind of linear, arbitrarily accessible list (the word list here is used informally).

Now the combination of those two are where things get interesting. You ""implement"" mechanisms in a programming language to use data structures. Most commonly, you can implement a class. For example in python I can write a `class SawBinaryTree` to define my own class (and data type) called ""SawBinaryTree"". I can write all the methods to store data in a binary tree format. But you could also write a ""AcreyesBinaryTree"" class to implement a binary tree data structure, maybe more efficiently than I did. They both represent data to the user in the same way, but they are distinct types in your language. So those are 2 ""data types"" that implement the 1 ""binary tree"" data structure.",hhhmp8x,t3_qcjqwn,1634823579.0,False
qcjqwn,"Data - a block of 1s and 0s.

Data type - how to we interpret that data? Is it numbers?  Floating point or integer? Signed or unsigned? ASCII code? Unicode?  How many 1s and 0s are in each discrete chunk?

Data structure - how do we arrange those interpreted, sized chunks, so that we can find the ones we want again easily?  Put them all next to each other in a row?  Is that row sorted in some way?  In the order we got them? In the order we want to send them?  In a tree so we can search them quickly?",hhha3aj,t3_qcjqwn,1634816580.0,False
qcjqwn,no idea how you got a downvote for this,hhhiex9,t1_hhha3aj,1634821426.0,False
qcjqwn,"Lol, idk, Reddit is a capricious mistress.",hhhj3vx,t1_hhhiex9,1634821788.0,False
qcjqwn,"“Data type” is used twice, the second where “data structure” should be.",hhhjzg4,t1_hhhiex9,1634822237.0,False
qcjqwn,stack overflow level of stick-up-butt and toxicity,hhhp9ha,t1_hhhjzg4,1634824783.0,False
qcjqwn,"Oh, thanks for the heads up, I'll fix the typo",hhhtl7y,t1_hhhjzg4,1634826721.0,False
qcjqwn,We stack overflow now,hhhmp4i,t1_hhhjzg4,1634823577.0,False
qcjqwn,lol absolutely. I didn’t downvote; it’s just my best guess.,hhhndy6,t1_hhhmp4i,1634823905.0,False
qcjqwn,"A data type is particular to a language or architecture. It is how a particular type of data (eg integer, float, string) is stored in that language or platform.

A data structure is a conceptual way of organizing and accessing data, eg a tree or a list. Some languages have certain data structures implemented as data types (eg lists and hash tables).",hhi3wx1,t3_qcjqwn,1634831096.0,False
qcjqwn,"A data structure is a nonempty set, a collection of operations of finite arity, and a set of axioms the operations satisfy. Go ahead and open a book on set theory if you want to choke on it. I tell you the theoretical definition so we can all give it a nod and forget about it, unless you're doing work in mathematics and theory of computation.

A data structure is data and the functions that operate on that data. Part of this definition includes how the data is arranged. A single bit is a data structure, as is a structure with multiple fields that have some sort of relationship - an array of elements and the length of the array, for example. Functions that apply to that array make assumptions about the nature of its elements and its length.

A type gives the structure a name. It's a form of data itself that can be reasoned, typically described in source code and applied by the compiler. In C++, for example, templates ain't nothing but compile-time functions that take types as parameters, they're executed by the compiler, and the output are more types and functions. Lisp don't give a shit, and there's hardly a distinction between code, data, and execution.",hhj0h72,t3_qcjqwn,1634844200.0,False
qcjqwn,[deleted],hhgiax6,t3_qcjqwn,1634793587.0,False
qcjqwn,"Good information with one (purely academic) exception that an array is not an ADT. It's actually closer to a data type, just non scalar, than an ADT. Some textbooks regard it as a composite data type rather than a data structure.",hhhpwm8,t1_hhgiax6,1634825084.0,False
qcjqwn,"https://brilliant.org/wiki/arrays-adt/
Please tell me if my understanding is wrong.
Edit: I am not confused, but, just my theoretical understanding is different.",hhhqact,t1_hhhpwm8,1634825257.0,False
qcjqwn,An array ADT is not an array and an array is not an ADT. An array is not abstract. It is homogenous data types stored in a contiguous section of memory.,hhhy3ls,t1_hhhqact,1634828654.0,False
qcjqwn,I appreciate your comment 👍,hhhyf2g,t1_hhhy3ls,1634828788.0,False
qcjqwn,"Data type is label that determines how the 1s and 0s in the data are interpreted by the program. Data structures are how data is organized; stack, queue, heap, tree, etc.

So an array would be a data structure.",hhhlngd,t3_qcjqwn,1634823068.0,False
qcjqwn,An array is a data structure. Data types are very abstract.,hhi7txa,t3_qcjqwn,1634832707.0,False
qcjqwn,"Data types - ( for language - how the info is stored , we use %d , %c for different ways to show op of a data stored) , ( for us - what we can stored , like . , or letter/chars)

Data structures - ( how to arrange a different units of data type in memory as our convenience ) linked list , queue etc

Arrays( non primitive data type and primitive data structures )

non primitive data type - stores group of values 

primitive data structures - that are predefined in a language",hhin4g6,t3_qcjqwn,1634838767.0,False
qc7295,Where my Dinosaur Book peeps at?,hhes8br,t3_qc7295,1634763520.0,False
qc7295,Dinosaur book !,hhescad,t1_hhes8br,1634763560.0,False
qc7295,Came to look at the comments just to see if people are talking about the Dino book,hhghvpy,t1_hhes8br,1634793293.0,False
qc7295,"Dinobook is what we're using. The text is easy to digest given certain requisite knowledge. Except for a few parts here and there, I've been able to follow along very easily",hhhq8la,t1_hhes8br,1634825234.0,False
qc7295,"Tanenbaum is still regarded as the canonical text.  Details of operating systems have changed but the core principles haven't changed enough for it to be outdated.  

(People always talk about how fast tech changes, and that's true.  But what that really means is that every time you learn a ""framework"" it's obsolete by the time you've learned it.  Core computing concepts and principles don't change that fast.)",hheno13,t3_qc7295,1634761730.0,False
qc7295,[OSTEP](https://pages.cs.wisc.edu/~remzi/OSTEP/) is quite popular and is fairly engaging.,hhedin5,t3_qc7295,1634757750.0,False
qc7295,I go to UW madison and remzi rocks,hhfjvnb,t1_hhedin5,1634775796.0,False
qc7295,10/10 would recommend,hhfnwe3,t1_hhedin5,1634777610.0,False
qc7295,We use this at Cornell,hhfvr9l,t1_hhedin5,1634781251.0,False
qc7295,Ostep is what we use at my university,hhezv21,t1_hhedin5,1634766705.0,False
qc7295,"Not in USA but in Australia, we refer to the ""Modern Operating Systems"" by Tanenbaum.

The book is very informative, has a lot of information and does go into depth but one thing the book lacks which I think is very important to have in any OS book are diagrams/images.

There is just not a lot of diagrams to illustrate a concept/idea and I wish it did, it would have definitely enhanced understanding.",hhf32ze,t3_qc7295,1634768127.0,False
qc7295,This is what we used (U.S.) and it is a fantastic book.  I still have mine on the shelf behind my desk at home with highlighter marks and post-it notes throughout.,hhfwh3o,t1_hhf32ze,1634781588.0,False
qc7295,"Yeah definitely is a good read, if it had more diagrams/images, it would have been so much more interesting to read. I haven't finished reading but it's great so far.",hhfwznd,t1_hhfwh3o,1634781827.0,False
qc7295,https://pages.cs.wisc.edu/\~remzi/OSTEP/,hhgrjp6,t3_qc7295,1634800933.0,False
qc7295,When I took OS we used ‘Operating Systems Principles & Practice’ Volume 1 and 2,hhefj5w,t3_qc7295,1634758531.0,False
qc7295,"https://www.zybooks.com/catalog/operating-systems/#toggle-id-5-closed

It’s a highly-interactive introduction to OS",hhg2sf3,t3_qc7295,1634784571.0,False
qc7295,Designing Data Intensive Applications book.,hhhsx7a,t3_qc7295,1634826427.0,False
qc6rth,"Binary trees, b+trees, heaps, etc are very low level technologies. They are baked into nosql fast access data services like Redis, DynamoDB, etc.",hhg5i6z,t3_qc6rth,1634785955.0,False
qbv5z3,"The best place to start is with something you can do. I know that sounds silly but hear me out. 

Think of a problem like “get the user to input two numbers and output whether they have a common prime factor. 

Start with

OUTPUT “Enter two numbers”
Num1 = Userinput
Num2 = Userinput 

Commonprime = False

If commonprime 
OUTPUT “Common prime factor found”
Else
OUTPUT “”No common prime factor found”

That doesn’t solve any of the hard part of the problem but I bet you can do that in your preferred programming language. 

You’ll be amazed how much better you’ll feel with some code on the page. Next think about the next easiest part of the problem. Can you work out all the factors of a number. 

If you can’t do any single part of the problem you probably need to start with some easier problems and work your way up - that will allow you to develop a range of strategies.",hhc58z1,t3_qbv5z3,1634716038.0,False
qbv5z3,Thank you that really help. I saw that after looking at the code and understanding it I had an idea on how to start the next problem get some lines in it before looking at the answer now.,hhc6muc,t1_hhc58z1,1634717300.0,True
qbv5z3,same here Any competitive programming sites you know of?,hhc45xi,t3_qbv5z3,1634715094.0,False
qbv5z3,Codewars is amazing they have simple problems for what you need,hhcsawt,t1_hhc45xi,1634733802.0,False
qbv5z3,"Try to do the questions for which you already saw the answers. Most times I would see the answers and think I understand, but find myself struggling when I try to do it. If you find yourself struggling too much, refer back to the answer. This time it’ll stick.",hhc53h5,t3_qbv5z3,1634715900.0,False
qbv5z3,So I should try I rewrite the code in another way? Or just look at the code (that I understand now) then rewrite it?,hhc6g1i,t1_hhc53h5,1634717124.0,True
qbv5z3,Understand how it works and implement without looking at the code unless you are stuck.,hhc73zk,t1_hhc6g1i,1634717748.0,False
qbv5z3,"I checked your profile and saw your age. That's the biggest factor, experience and exposure will grow you. 

If you look back at the code you wrote today in 2 years time, you'll shake your head and laugh. Actually, I'll laugh at the code I wrote 6 months ago. 

It's a constant learning experience. But if you want to be able to tackle problems easier, try to solve some problems rather than trying to build a solution instead.

Go through the development life cycle.

Customer says they have a problem.

You enquire.

You build requirements.

You show them requirements.

They agree on some and disagree on others.

You re-iterate. 

They agree.

You design a solution using a mix of A technology and B technology. 

You build the solution.

You give the customer the solution.


I saw from your previous posts that you're looking for a side hustle and development can really help there. I got my break into Software Development from a lifetime in Environmental Science by making and selling VR and AR apps. 

Give yourself a challenge, you learn more from your failures than your successes.",hhcplne,t3_qbv5z3,1634732297.0,False
qbv5z3,"I've had the same problem when I started competitive programming.

The problem you are facing is something very common when you are not gifted in problem solving. But good news it can be learned. Basically you have the same issue that you may have faced in math class : I understand the math but fail when it comes to solving the exercise. 

What I did is that on websites like codeforce or hackerrank I worked only on one type of problem for instance problems using linkedlist. And even though at first i needed to check the solution to understand how to solve the problem. At some point I just knew what I had to do since you do mostly the same thing. Then i moved on different types or problems following the same method. Use previous solved problems to implement their algorithms in other problems !

I suggest to take the time to carefully read the problem, understand what outcome is required and what information are needed to solve the problem. Don't hesitate to draw the problem, too many people tend to work in their heads instead of writing stuff down to visualize the problem. In short don't go too fast ! 

Another suggestion that I would give you  is to make sure that you are not starting off with too difficult problems and that you know your programming language well ( i know it sounds dumb but i figured when i started competitive programming that my knowledge in c++ was quite bad and that even though i knew what certain functions did when i saw them in the solution my brain would just not think about using them ). 

A last recommendation that I could give you is to watch youtube videos about programming and problem solving. William lin or Erichto for instance are two youtube chanels that will help you understand some concepts ( it served me pretty well especially when it came to shorten my code and make it more simple ). Don't hesitate to ask people for help and look for documentation. Try as much as you can to solve a problem without looking for the solution. As it was said in another comment writing some code and trying to understand how to make progress towards the answer of your problem will make you improve even if you need to rely on the solution ay the end ( i say that because i used to give up pretty fast and look for the answer as soon as i faced difficulties, it's fine if you don't manage to solve a problem in 1 hour ). 

Becoming better at problem solving, algorithm etc is a long way that will require you rigorous practice. The more you code and solve problems the better you will get. Kind of like the progress it took you from learning how to code until you made your first snake game if you will. You need to persevere and keep working.The process is definitely very enjoyable and you will be proud the 1st time you will manage to solve a problem on your own (and also your coding interview skills will be very good and sharp).

Hope that helps and good luck on your grind !",hhcr2s5,t3_qbv5z3,1634733142.0,False
qbv5z3,"Find simpler questions? Project Euler has a wide range of difficulty, from easy to (IMHO) absurdly difficult. Try to ask yourself what a first step would be - or, what the last step might be. Sometimes thinking in reverse and satisfying dependencies can be an easier way to solve a problem.",hhd73zs,t3_qbv5z3,1634740770.0,False
qbv5z3,Just practice and before writing any code just think about the solution first remembering the logic behind classic algorithms. It helps a lot.,hhdl4b7,t3_qbv5z3,1634746482.0,False
qbv5z3,"We may compete on codeforces, leetcode or hackerrank etc. I was in the same situation with you almost 2 months ago, with no ability to solve but the ability to understand completetly. My first codeforces contest score was 350 :D. Now I am around 1000, which still sucks, but I guess I am getting better. However, only problem-solving is not enough, reading the intro. to. algos. or similar books would definitely help you.",hhdpgrz,t3_qbv5z3,1634748185.0,False
qbv5z3,Thank you so much. And I don’t know what that score is for but all I know is that it went up and that a good thing.,hhghgx5,t1_hhdpgrz,1634793011.0,True
qbv5z3,"Psuedo code it. 

It breaks the problem into pieces, code it, and build “into” whatever language you’re using.",hhfk44e,t3_qbv5z3,1634775905.0,False
qbv5z3,Thanks for asking this question I've been having these same exact problems.,hhfy5px,t3_qbv5z3,1634782366.0,False
qbl118,"https://www.coursera.org/learn/algorithms-part1

https://www.coursera.org/learn/algorithms-part2",hhaah8e,t3_qbl118,1634679562.0,False
qbl118,Awesome thank you!,hhaazfy,t1_hhaah8e,1634679777.0,True
qbl118,"You want the book Introduction to Algorithms, and basically any content at all by Charles E. Leiserson. Their lectures are on youtube, and so forth.",hhad84r,t3_qbl118,1634680751.0,False
qbl118,MIT open courseware,hhaiz0x,t3_qbl118,1634683324.0,False
qbl118,Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms),hhb0iza,t3_qbl118,1634691554.0,False
qbl118,"[https://www.youtube.com/watch?v=HtSuA80QTyo&list=PLUl4u3cNGP61Oq3tWYp6V\_F-5jb5L2iHb](https://www.youtube.com/watch?v=HtSuA80QTyo&list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb)

It's a good introduction also you should practice and solve problems if you want to really get better. I recommend [https://leetcode.com/](https://leetcode.com/) go for its problems and if it's hard for you just read the solution.

don't be afraid of solving algorithm problems just go for easy ones and after solving some you will notice your progress.",hhccw5m,t3_qbl118,1634723087.0,False
qbl118,Introduction to algorithms by cormen is pretty good and extensive,hhdxvv7,t3_qbl118,1634751497.0,False
qbl118,"Important to know the theory, but useless until you apply it. Learn something and go to leetcode for practice.",hhbw9pz,t3_qbl118,1634708734.0,False
qbl118,"The canonical text is CLRS. That's the acronym for the authors' names, the book title is *Algorithms*.",hhbgfn9,t3_qbl118,1634699185.0,False
qbl118,I used youtube,hhcpsuv,t3_qbl118,1634732413.0,False
qbkvnl,"ASCII is old, Unicode is new, so Unicode is what you do.

Simple rhyme, simple message, true though. Unless you’re working on older systems like me with xbase++ which is a still-developed sidekick of Clipper. For any import export operation I have to closely catch any needs to translate ascii<->Unicode. When possible stick with Unicode, so my rhyme in first place remains true.",hha5uh2,t3_qbkvnl,1634677603.0,False
qbkvnl,"The first 127 symbols in the Unicode are the 127 ASCII symbols. So Unicode is backwards compatible; an extension.

There is only one benefit to ASCII, from the top of my head: It has constant size characters. With UTF-8, the size of the string (in bytes) is not necessarily the length of the string (in characters).",hha73ey,t3_qbkvnl,1634678126.0,False
qbkvnl,"ASCII and Unicode are two alternative ways of encoding text as bits. Other comments have addressed the technical details: ASCII is older, and includes 127 characters, each one byte long. Unicode is newer, and after the first 127 characters (which are the same as in ASCII for backwards compatibility) may require additional bits for a total of about 144K unicode characters. From an end-user perspective, however, the big difference is that ASCII is English-alphabet-only! Even if you're writing software for an English user-base, many English-speaking people have accent marks in their names. Writing software that doesn't support non-English characters is likely to limit the experience of your users, even if ASCII is smaller and a little easier to reason about.",hhbkuho,t3_qbkvnl,1634701555.0,False
qb5blt,"Your credit card is run via a completely different company afaik. They just sell through your bank. I've had issues using it as ID in branch for the same reason. They don't access it.

I guess the technical infrastructure for debit cards would have to be massive as there's so many of them.",hh7eo9w,t3_qb5blt,1634626075.0,False
qb5blt,"That actually clears things up, thanks.",hhfway4,t1_hh7eo9w,1634781508.0,True
qb4bof,"Mods just got maaaaad........
I feel you, bro.",hh7dqzf,t3_qb4bof,1634625305.0,False
qb4bof,"Not mad really more just annoyed. Hopefully people will actually read the big capital letters saying ""stop posting this bullshit here"" but I doubt it",hh7eamg,t1_hh7dqzf,1634625755.0,True
qb4bof,What about a flair that says “tech support” when they use that flair an auto mod replies with all the other tech support subs then deletes their post? Kind of a sneaky sneak way of helping out.,hh8sicz,t1_hh7eamg,1634657913.0,False
qb4bof,"Oohhh, I like that",hh958nd,t1_hh8sicz,1634663132.0,True
qb4bof,"I hate to be the one to tell you, but... Chances are really, really, low.

But, doesn't hurt to try, eh?",hh7en3q,t1_hh7eamg,1634626048.0,False
qb4bof,"Making a stickied thread actually cut down on the tech support posts *significantly*. I would wager somewhere within the range of 75-90% fewer IT posts were made once we linked to tech support subs. Hopefully this will cut out the few ""what mac book should I buy?"" and ""my wam no work"" posts that we still get, but I doubt it, especially when it comes to the second group. Some people just cannot get it into their skulls that Computer Science is not IT. 

Side note - I wish I could be bothered to dig through modmail cause a few months back I had one guy try to argue with me that CompSci and IT were the same thing for like 3 days. I responded maybe twice but the guy kept sending messages every day until I got bored and muted them. Worst part was his problem was something super simple. My guess was he probably needed to reseat his RAM or something really basic like that.",hh7gtdn,t1_hh7en3q,1634627959.0,True
qb4bof,Do you know how I can add this post to my bookmarks? I'm on a 2019 Macbook Pro and have Chrome browser thanks,hh79buz,t3_qb4bof,1634621810.0,False
qb4bof,"*What the fuck did you just fucking say to me you little user? I'll have you know I graduated top of my glass from Reddit Mod Academy and have been involved in numerous thread deletions and have over 3000 confirmed bannings. I am trained in Toolbox macros and I'm the top Automod scripter in the entire reddit mod cabal. You are nothing to me but another user. I will fucking ban you into next week with a macro the likes of which have never been seen on reddit, mark my fucking words. You think you can get away with posting shit like this on the internet? Think again, fucker. As we speak I am filing a report to the reddit admins to obtain your IP address, maggot. The ban wave that wipes out the pathetic little thing you call your reddit account is coming. You're fucking banned, kid. I can mod anywhere, anytime, and I can ban you in over seven hundred ways, and that's just with automoderator. Not only am I extensively trained in shadow banning, but I have access to the entire toolset of the reddit admin team and I will use it to its full extent to wipe your miserable ass off the frontpage of this website, you little shit. If only you could have known what unholy retribution your ""clever"" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. You're fucking dead, kiddo.*",hh7hhf9,t1_hh79buz,1634628562.0,True
qb4bof,This is by far the best thing I’ve ever read on this app,hh7lkyi,t1_hh7hhf9,1634632348.0,False
qb4bof,"My stupid ass hand typed that out of boredom, so it's good to know my ""hardwork"" is appreciated",hh7m8xq,t1_hh7lkyi,1634632958.0,True
qb4bof,"Stupid ass-hand

[xkcd: Hyphen](https://xkcd.com/37/)

---

^^Beep ^^boop, ^^I'm ^^a ^^bot. ^^- ^^[FAQ](https://pastebin.com/raw/vyWra3ns)",hh7m9di,t1_hh7m8xq,1634632970.0,False
qb4bof,Well now that I know you had to type it and didn’t have a macro for everything you say is making me less scared of your threats,hh8w43y,t1_hh7m8xq,1634659422.0,False
qb4bof,Post it on r/copypasta please I want daddy cummy 🥺🥺,hh7ndcq,t1_hh7m8xq,1634633991.0,False
qb4bof,I'd rather it not end up getting spammed around Reddit,hh99joa,t1_hh7ndcq,1634664864.0,True
qb4bof,"If your r/computerscience \- moderating ass is that bored, consider re-reading CLRS, or Numerical Recipes.

:-P",hqhidwt,t1_hh7m8xq,1640826457.0,False
qb4bof,You... you monster. :-D,hh7p2zq,t1_hh7hhf9,1634635561.0,False
qb4bof,\*golf clap\* Bravo. :),hhduigl,t1_hh7hhf9,1634750168.0,False
qb4bof,r/woosh,hh7imbr,t1_hh7hhf9,1634629608.0,False
qb4bof,where the upvote button need help thank,hh7aui0,t1_hh79buz,1634622974.0,False
qb4bof,Im going to be starting my first cs class....how many rtx 3090s do I need in my laptop?,hh8ohm9,t3_qb4bof,1634656227.0,False
qb4bof,As many as you can fit on the pc,hh97g0i,t1_hh8ohm9,1634664025.0,False
qb4bof,"You need to carry atleast two server racks with you full of equipment. Two is the minimum but it is recommended to have atleast four which have as many rtx 3090s as they can fit. Also they should atleast have 10PB of storage combined.

- Your Professor",hp4jd9g,t1_hh8ohm9,1639883899.0,False
qb4bof,"I need help downloading chrome, someone said I need to delete system32 for it to work but I’m having trouble with that so is there something else I need to do?",hh89w9k,t3_qb4bof,1634649664.0,False
qb4bof,"Yeah, you're supposed to reboot after deleting the folder",hh8i1a2,t1_hh89w9k,1634653479.0,False
qb4bof,First you have to set the owner of the files in the system32 folder and subdirectories. Then delete. It’s easier if Windows isn’t running.,hhaciac,t1_hh89w9k,1634680433.0,False
qb4bof,"You are expelled from the class for using Windows.

- Your Professor",hp4jgp5,t1_hh89w9k,1639883947.0,False
qb4bof,What does != Mean? Is that a new key on the new Mac?,hh7bn05,t3_qb4bof,1634623599.0,False
qb4bof,I don’t know but you should make a post on the sub asking about that,hh97k4c,t1_hh7bn05,1634664071.0,False
qb4bof,It's pretty much the same as <>,hhalz0v,t1_hh7bn05,1634684719.0,False
qb4bof,It's about time this was said.,hh7fu1g,t3_qb4bof,1634627082.0,False
qb4bof,"Correct. This is a sub where people learn that going to college to learn to code is a waste of time.

Unless you are interested in learning to code in java like it's  2001",hh7hgtj,t3_qb4bof,1634628549.0,False
qb4bof,Gotta have that piece of $15k+ paper to impress the companies! And then have 3 years experience to get the junior dev job so you can get the 3 years experience to get a junior dev job so you can get the 3 years experi....,hh7hnu0,t1_hh7hgtj,1634628725.0,True
qb4bof,"Ah yes, _recursion_",hha4t1r,t1_hh7hnu0,1634677178.0,False
qb4bof,I doubt the people that will need to see this post will actually see it.,hh83k16,t3_qb4bof,1634646277.0,False
qb4bof,r/pcmasterrace is a pretty laid back place to get help and ask questions.,hh8s9y5,t3_qb4bof,1634657817.0,False
qb4bof,"Pretty sure people see ""computer"" and stop reading. Maybe there should be r/computersupport or something",hh9amct,t3_qb4bof,1634665286.0,False
qb4bof,Basically. I'm tempted to make Computer Support and have the only post be a link to tech support,hh9awoh,t1_hh9amct,1634665398.0,True
qb4bof,/r/computers gets a lot of tech support questions.,hhai9oi,t1_hh9amct,1634683008.0,False
qb4bof,"> ~~despite what all my relatives think~~

I feel you.",hp4itqp,t3_qb4bof,1639883614.0,False
qb4bof,"Would also be nice to clarify that it's not a homework help sub, either. Subs like this (and /r/cprogramming comes to mind) always get ruined by dozens of people posting low-effort, straight-out-of-the-textbook type questions that add nothing in terms of content.",hh9fhpy,t3_qb4bof,1634667207.0,False
qb4bof,Gotta love it. I'm subscribed to /r/artificial and it attracts a small dose of crazies  that just need to share their weird theories with the world.,hhac86q,t3_qb4bof,1634680311.0,False
qb4bof,Finally someone said it.,hhb0gku,t3_qb4bof,1634691523.0,False
qb4bof,Can you fix my printer,hhb6hu5,t3_qb4bof,1634694306.0,False
qb4bof,Is the discord still active?,hkampdv,t3_qb4bof,1636691522.0,False
qb4bof,thank you for the links!!,hukx4s9,t3_qb4bof,1643377324.0,False
qb4bof,[removed],hh8jpns,t3_qb4bof,1634654198.0,False
qb4bof,[removed],hh8v5qg,t1_hh8jpns,1634659019.0,False
qb4bof,[removed],hh8x5tl,t1_hh8v5qg,1634659858.0,False
qb42z7,"It’s more general than that. Its a field where you have terms or expressions and  rules that rewrite them. E.g. `map a . map b = map (a . b)` is a rewrite rule describing map fusion for some functional language. Some compilers (eg ghc) have support for rewrite rules as part of their optimisation passes. The study of the field is more formal/general, covering things like does a given system of rules and expressions always terminate, etc",hh7gmvr,t3_qb42z7,1634627797.0,False
qauqn2,"They are indeed equivalent. But in some cases the argument is much easier when you use strong induction, and the reason is you just assume more, so you have more to play with. However in many cases simple induction is enough and you don't need the extra assumptions.",hh5kd56,t3_qauqn2,1634590341.0,False
qauqn2,"Both of these are consequences of the least element principle.  
By the way, I strongly suggest using quantifiers in your formulations in general.",hh5kzvs,t1_hh5kd56,1634590691.0,False
qauqn2,"To add on, both Induction and Strong Induction can be derived from the Well-Ordering Principle, but the opposite is true as well: you can derive Well-Ordering from either Induction or Strong Induction :D",hh69xkh,t1_hh5kzvs,1634602592.0,False
qauqn2,"Yes, they are equivalent. Strong Induction is stronger, as the inductive hypotheses is more general, just as you describe.

From a practical point of view, often having that extra generality is very useful. A particular CS example is when dealing with recurrence relations. Say you want to prove that  F(n) = F(n-2) + F(n-1)  is the (insert the equation for the nth Fibonacci number). Strong induction gives us both F(n-2) is the (n-2)th Fibonacci number and F(n-1) is the (n-1)th Fibonacci number. Weak Induction would only give use the F(n-1). 

Another sign that you want use strong induction, is if you want to make claims about all numbers smaller than n. A common proof technique is to select the smaller number n for a which a property holds. A good example of this the proof of prime factorization. Basically, any time where proving the induction step is based on more than just one number, you want to use Strong Induction. 

The question you might want to consider is if Strong Induction gives a more general inductive hypothesis, why both with Weak Induction.
I don't have a good answer for it, but it's an interesting question to ponder.",hh63of0,t3_qauqn2,1634599567.0,False
qauqn2,Sometimes with certain recursive formulas it’s much easier to use the strong form when you have a few different layers of recursion and are trying to figure out a discrete formula.,hh5y0ei,t3_qauqn2,1634596846.0,False
qauqn2,It’s also useful when you are working with multiple base cases,hh73xme,t1_hh5y0ei,1634618002.0,False
qauqn2,"A simple example where strong induction really shines is factoring. 

Suppose I want to show every integer is the product of primes. Well I can start with an integer n, and if n is prime itself we’re done, so it must be composite. Then n=ab and we’d like to use our inductive hypothesis on a and b, but all we have for sure is that a,b<n if we’re using weak induction we are very sad, but if we’re using strong induction then we’re done.",hh6b58a,t3_qauqn2,1634603179.0,False
qauqn2,"What everyone else said is right.

As a possible minor benefit of strong induction, especially for a CS context: It more resembles structural induction.  Take for instance ""Consider the language formed by the formation rules: epsilon | (e) | e\*e.  Every expression has an even number of parentheses.""  You wouldn't want to waste energy indexing every word in the language with a natural number, only to find that the inductive hypothesis for a sentence indexed by n won't then help you prove the inductive claim.

Much better to approach this with structural induction:  Prove the claim for the base-case of a ""zero-complexity"" word.  In this case that just means the word ""epsilon"" with no parentheses.

Then assume S is a word formed by rule 2 or 3, and suppose that we already know that *any sentence of less complexity* (this is the part that's a lot like strong induction) has an even number of parentheses.  Either S = (e) or S = e1\*e2.  In the former case e has an even number of parentheses and so S has 2+(that many) and so S has an even number of parentheses.  In the latter case e1 and e2 each have an even number of parentheses and S has (that many)+(the other many) which is an even plus an even.  So again S has an even number of parentheses.  Therefore in all cases S has an even number of parentheses.

So anyway, point being, strong induction is very similar to the structural induction proof above, because we make a blanket assumption for the inductive case: that the claim holds for everything ""less than"" the current case.",hh6dcvp,t3_qauqn2,1634604251.0,False
qauqn2,"They are equivalent. As you point out, proofs using strong induction will be at most a few words shorter than proofs using induction.

But there are places where something like strong induction is needed. And that's when you're not doing induction over the natural numbers. Sometimes you might want to prove that something is true for all trees. There it's more natural for the induction hypothesis to look like ""If something is true for all subtrees of the tree T, then it is true of T as well"". A tree doesn't have a unique ""largest subtree smaller than it"", as opposed to a number n that has n-1. So the strong version is more suitable.",hh72qsi,t3_qauqn2,1634617239.0,False
qakzly,"I haven't had the chance to ponder this too much, but is there a reason to think that this might be an NP problem? I've very skeptical of an O(N\^2) solution for this that works with all possible inputs (and I'm generally skeptical of any greedy algorithm).

Is it possible to construct a pathological example for a given N that would cause a combinatorial explosion in flows to be considered? Is there a way to prove that that's not possible due to, say, the transitive nature of the exchanges?",hh3od24,t3_qakzly,1634560833.0,False
qakzly,"Personal experience, GeekForGeeks should not be trusted blindly. There's a lot of good and bad articles all mixed in there.",hh3kgd8,t3_qakzly,1634558481.0,False
qakzly,But I cant seem to find any solution for this question anywhere on the internet other than the method described in the article .,hh3l65o,t1_hh3kgd8,1634558935.0,True
qakzly,">to find any solution for this question anywhere on the internet other than the method described in the a

This algorithm doesn't exactly minimizes number of transactions as pointed out by the first comment on your post.",hh3vtqc,t1_hh3l65o,1634564741.0,False
qakzly,"No need to be shy. It's simply wrong.

And pretty sure it can reduce to subset sum question, by making everyone's balance as number in the set, and then # of transactions <= n-2 is equivalent to finding a subset of zero sum.",hh3rr6r,t3_qakzly,1634562678.0,False
qai7co,Ben Eater on youtube made a computer from ICs. He explains how everything works along the way. I highly recommend you watch,hh3ajqu,t3_qai7co,1634551070.0,False
qai7co,I opened this thread expecting Ben Eater to be the top comment and I'm pretty dismayed that he's not.  Emphatically seconded.,hh47sze,t1_hh3ajqu,1634570134.0,False
qai7co,dude is awesome,hh4z6q6,t1_hh3ajqu,1634581475.0,False
qai7co,The first few videos of [Crash Course Computer Science](https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) cover this. Highly recommend the entire series.,hh33sp8,t3_qai7co,1634544876.0,False
qai7co,Thanks!,hh3411c,t1_hh33sp8,1634545095.0,True
qai7co,"This crashcourse is awesome. Easy to understand and fun to watch, but it‘s really just a crashcourse, it does not go too deep",hh3517r,t1_hh3411c,1634546037.0,False
qai7co,I'd say main problem with my teachers at University is that they only go from result-to-result without really explaining and I had quite a hard time understanding those concepts. The main problem with all this is how simple on/off can do such things?,hh3571x,t1_hh3517r,1634546188.0,True
qai7co,"You start with [transistors](https://simple.wikipedia.org/wiki/Transistor). You build [logic gates](https://simple.wikipedia.org/wiki/Logic_gate), like an [AND Gate](https://simple.wikipedia.org/wiki/AND_gate), from transistors. You build specialized circuits, like a [Binary Adder](https://simple.wikipedia.org/wiki/Binary_adder), from logic gates. You build slightly more generalized circuits, like an [Algorithmic Logic Unit](https://simple.wikipedia.org/wiki/Arithmetic_logic_unit) or ALU, from specialized circuits. Now you can add numbers from on/off signals. Addition is the basis of all computation a computer performs.",hh37jxz,t1_hh3571x,1634548377.0,False
qai7co,Happy cake day!,hh4qckd,t1_hh37jxz,1634577844.0,False
qai7co,"Code by Charles Petzold is a must read book for you if you want to understand this. It goes from on/off to CPU to simple OS.

It’s an easy and fun read if you really want to understand computer architecture. It will get you started on the right foot.",hh5oc6m,t1_hh3571x,1634592475.0,False
qai7co,"There's a book with a free course called ""from Nand to tetris"", it basically teaches what you are asking for by making you create a virtual version of all pc components starting from the chips and go all the way up to creating your own language.",hh33x0y,t3_qai7co,1634544989.0,False
qai7co,Appreciate it!,hh3423l,t1_hh33x0y,1634545122.0,True
qai7co,FYI the subject you’re interested in is named “computer architecture.”,hh346i5,t3_qai7co,1634545236.0,False
qai7co,Know any good sources on the subject?,hh34iu3,t1_hh346i5,1634545556.0,True
qai7co,"I learned from textbooks and hands-on experimenting, but google the term and see what you get.",hh3645f,t1_hh34iu3,1634547038.0,False
qai7co,"If you want real life hardware examples and explanations, check out the channel ""Ben Eater"" on yt!",hh35id5,t3_qai7co,1634546483.0,False
qai7co,"http://www.infocobuild.com/education/audio-video-courses/computer-science/cs61c-spring2015-berkeley.html
This is the best course u will find. Professor is awesome. They start from C language then assembly the computer architecture and logic gates.

There is one other course from MIT if u want, just search OCW computation structures.",hh3hpbl,t3_qai7co,1634556662.0,False
qai7co,"Grab a copy of the book Digital Computer Electronics by Paul Malvino, it covers a computer architecture used for teaching what you want to learn, called the ""Simple As Possible"" architecture, SAP.",hh3tft8,t3_qai7co,1634563556.0,False
qai7co,"I don't know which architecture will be teach to you but you could take one digital design book and check out this new game [Turing Complete](https://store.steampowered.com/app/1444480/Turing_Complete/). The game don't teach you, because of that i suggest check this game with a book. this might help a lot for better visualization and trial and error for learning.",hh3wvbl,t3_qai7co,1634565241.0,False
qai7co,"[Sebastian lague released this video](https://youtu.be/QZwneRb-zqA) on how computers work starting with a basic binary system and on/off switches that could be represented in hardware as physical electric current toggles. From there, you just keep combining small modules to achieve the ability to perform arithmetic and store memory.",hh43kul,t3_qai7co,1634568312.0,False
qai7co,"You will probably want to learn about *digital electronics* then, including things like logic gates (AND, OR, NOT, XOR, etc). Then you can check out stuff like binary adder circuits to get a sense for how it does mathematical computation.",hh5ww1k,t3_qai7co,1634596324.0,False
qai7co,"I can't recommend the book ""But How Do It Know"" by J Clark Scott. Starts with simple gates and works up to a fully working computer.",hh658bv,t3_qai7co,1634600311.0,False
qai7co,There's a pretty good book by William Stallings called Computer Organization and Architecture,hh6muc9,t3_qai7co,1634608763.0,False
qai7co,Read Elements of Computibg Systems and do the work.,hh96mr2,t3_qai7co,1634663699.0,False
qah5lk,"Computers can't deal with irrationals - they use integers or an approximation of the real numbers called ""floating point numbers"". Even for floats alone there are usually multiple algorithms: one for integer and one for non-integer exponents. Rationals are usually not supported directly by the hardware and are instead ""simulated"" on the software level, so you might find any algorithm here.",hh2zsf5,t3_qah5lk,1634541285.0,False
qah5lk,"I know computers don't support irrationals but what I am asking is, for example, 3\^4 is an easy task a computer just multiplies 3,  4 times with itself. However, 3\^e cannot be done in the same way, so maybe computers use series to calculate.  

>there are usually multiple algorithms: one for integer and one for non-integer exponents

Thanks, this answer is what I was seeking. Besides, if you possess /know any article about the topic please share, I couldn't find much.",hh30avk,t1_hh2zsf5,1634541722.0,True
qah5lk,"It depends on the program. Some intended for scientific calculations will recognise e and use an efficient algorithm that gets an accurate result. Some will just truncate e to a certain precision as soon as they need to calculate it.

A nice example which worked until very recently but I can't seem to replicate now was typing into Google search (0.1*0.1)/0.1. The answer was 0.09..... because google simply truncated 0.1 (which in binary has an infinitely recurring repeat after the point) instead of calculating it another way.",hh32av6,t1_hh30avk,1634543489.0,False
qah5lk,"Your calculator example might be related to this [https://dl.acm.org/doi/pdf/10.1145/3385412.3386037](https://dl.acm.org/doi/pdf/10.1145/3385412.3386037) It's been a while since I read it but iirc it targeted specifically calculators (I think the windows calculator uses it?) with the goal of removing the cases that really fuck with ""normal people""'s knowledge of math. There's also a talk about it on youtube if you're interested.",hh32yud,t1_hh32av6,1634544104.0,False
qah5lk,I was talking about a specific case with Google's built-in calculator that was still a problem as late as 2019. Thanks for your help though!,hh37tq9,t1_hh32yud,1634548633.0,False
qah5lk,"I don't know of any articles in particular but

* to check hardware support you can check out the instruction set of the platform (e.g. for x86 you'll find an instruction to compute `x log_2(y)` which can be used to write a clear (but not necessarily good) exponentiation algorithm)
* to see how compilers handle it check out their generated assembly (either directly or via [https://compiler-explorer.com/](https://compiler-explorer.com/) )
* to see how people actually implement it (probably black magic bit-hackery) check the source code of math libraries. e.g. for C: [http://www.netlib.org/fdlibm/](http://www.netlib.org/fdlibm/) (in particular the e\_pow.c file)",hh32gqv,t1_hh30avk,1634543637.0,False
qah5lk,"thanks, I'll definitely look.",hh3ncf4,t1_hh32gqv,1634560247.0,True
qah5lk,e is estimated as a rational number. as far as the computer is concerned its exponentiating a rational by a rational.,hh3at0q,t1_hh30avk,1634551300.0,False
qah5lk,It depends... generally for floating point numbers yes.  Big int libraries optimize integral powers,hh42a75,t3_qah5lk,1634567744.0,False
qabacx,"Calculus is heavily used in some fields of computer science, particularly in computer graphics and machine learning (a little bit of the analysis of operating systems and networks as well).  Ideas from calculus (in particular ideas about limits and sequences and series) are also really important when you start to analyze algorithms, later on in your coursework.  

You'll use some other topics in ""heavy mathematics"" more often, like graph theory!",hh1uxy9,t3_qabacx,1634516884.0,False
qabacx,"I’m actually just learning computer science as a hobby since I’ve already completed courses in I.T related work and programming as well. However computer science is way different from what I expected so far so I’m just making sure that this is something that could fit me right.
I’m not bad at math but it doesn’t change the fact that it’s still scary :D",hh1vc66,t1_hh1uxy9,1634517083.0,True
qabacx,"Don't be scared!!    You don't need to be some super math genius to do computer science, and there's a lot of really cool computer science that isn't very mathematical at all... 

These are just the areas of cs that use more calculus.",hh1vtdj,t1_hh1vc66,1634517324.0,False
qabacx,"Oh well we’ll see soon enough for sure :) so far I’ve only learnt about the binary and hexadecimal system which aren’t really anything special so hearing about all that stuff made me think a little
Thanks a lot for the help tho!",hh1wh4x,t1_hh1vtdj,1634517651.0,True
qabacx,"Remember, that there are other majors that are much harder than CS.

Would you rather try and fail or fail by not trying?",hh2ryvj,t1_hh1wh4x,1634535094.0,False
qabacx,"Mathematic is much more of a training thing than most people realize. Most ideas are quite simple, too, it's just a bit of a language barrier. There is hard stuff, as well, ofc, but that's usually nothing you come across in the undergrad.

Not sure where I read it, but there is almost no person who can just read through a math textbook. Sometimes it takes hours to ponder over a single paragraph.

So, don't be scared. It just takes a bit more effort :-)",hh33hj5,t1_hh1vc66,1634544582.0,False
qabacx,"Well hello there, I'm about to have a course on Computer Graphics and could you be kind enough to say where Calculus in particular may show up? I thought Linear Algebra would be enough for basics and is it for something advanced?",hh30z7g,t1_hh1uxy9,1634542316.0,False
qabacx,"Linear Algebra is a course that builds off of calculus, so if there’s Lin Al, there’s also some amount of calculus. Plane intersections and the like are all taught in multivariate calculus, but are topics within Linear Algebra as well.",hh3gipa,t1_hh30z7g,1634555839.0,False
qabacx,Thanks for the response.,hh3mzcg,t1_hh3gipa,1634560037.0,False
qabacx,"I finished high school as someone who was “characteristically bad at math” and I initially dreaded the math in CS. I’ve found though as time goes on that CS has allowed me to approach math through a more enjoyable lens. It’s not nearly as bad as I was worried it would be. You don’t have to be a *math person* to learn computer science. 

Don’t get me wrong though, there’s a decent amount of math. As a computer science student I’m required to complete courses in calculus (thru calc 3), computational statistics, probability and linear algebra. In most of my classes so far we’ve used math to some degree for proofs, run time calculations, etc. Topics like Machine Learning and AI can be very math and computationally heavy.",hh25l8k,t3_qabacx,1634522211.0,False
qabacx,I've been getting into audio programming and digital signal processing and it seems like a calculus-heavy field.,hh1yem2,t3_qabacx,1634518610.0,False
qabacx,"You're moving more into EE territory though so that's not exactly typical CS. It definitely is math heavy though with lots of convolution and transforms. Thankfully the latter can be used to turn most problems into algebra or simple arithmetic, but it's still a heavy field as you said.",hh3k01p,t1_hh1yem2,1634558186.0,False
qabacx,I'm pretty interested in learning some more about this. Do you have any recommended sources you've enjoyed?,hh2ar9j,t1_hh1yem2,1634524870.0,False
qabacx,"https://www.amazon.com/Digital-Signal-Processing-Computer-Perspective/dp/0471295469?ref_=d6k_applink_bb_marketplace
I've found this book.",hh4d7wv,t1_hh2ar9j,1634572432.0,False
qabacx,"Check out these YouTube channels: The Audio Programmer, JUCE, and the Audio Developer Conference",hh4eyeo,t1_hh2ar9j,1634573157.0,False
qabacx,"In terms of calculus specifically, the thing you’ll need the most learning computer science is an intuitive understanding of what a derivative is and what an integral is. Not in the abstract mathematical sense with symbols (although that is useful and necessary if you want to go deeper than surface level into the areas like machine learning where calculus is relied upon more deeply), but in terms of understanding the fundamental relationship between a function and its derivative, what zeroes, minima, and maxima mean in terms of a functions derivative or integral, etc.

For that sort of stuff, college courses can be hit or miss (especially at the intro level like Calculus 1 where the professor is far more likely to be following the traditional approach of presenting formal definitions and working through problems rather than approaching intuitively and then formalizing). If you take one that is a miss in that regard, look to online videos on YouTube channels like 3blue1brown where they take a more intuitive approach. It can be very helpful, especially if you’re able to contextualize those intuitive explanations with the formal definitions you will probably learn from a class.

——

As I get into my thoughts on Math required for Computer Science in general, it’s probably useful to point out where I’m coming from on it. For my undergraduate, I double majored in Computer Science and Math. I’ve been working as a Software Engineer for 3 years since then and I’m one year into my MS in Computer Science (part time).

I consider Computer Science to be a subfield of Mathematics, but the line is a bit blurred between the fields of Computer Science and Software Engineering. Many people will go into a CS degree expecting to learn how to code and practical skills for developing software and be surprised by the mathematical side.

That being said, many undergraduate CS degrees, including my own degree, are catered more towards this audience and therefore teach many practical skills and focus on the theoretical concepts that are more useful in software engineering (e.g. design patterns, agile methodology, requirements analysis, software testing).

Therefore broad math knowledge ends up being really useful, but deep knowledge is only necessary for the specific field(s) of math used in whatever area(s) you dive more deeply into (i.e. beyond an introductory course in the subfield).

Some examples:

Computer graphics makes heavy use of linear algebra. I’ve played around with some low-level 3D graphics programming, and I found myself doing matrix multiplication quite frequently as I tried to make sense of the transformations I had to do to map object vertex coordinates to world space and to screen space.

If you do anything with Simulation or Game Development, in addition to the graphics stuff, you’ll need to understand basic physics, which is applied Calculus.

If you want to optimize an algorithm, you will need to do some algebra to find the asymptotic runtime of the algorithm. This can get pretty complex, especially when recursion is involved, and limits are commonly used.

Some subfields of machine learning (including deep learning) rely heavily on derivatives, but not just the kind you’ll see in Calculus I, but partial derivatives too. This is the one place in CS where I’ve used anything I learned in Calculus III.

Anything low-level or on the hardware side is going to run into Boolean Algebra. This is where the idea of CS as a subfield of Math comes in, because you’re unlikely to learn this in a Math class, but many CS programs have a course that covers it while teaching about digital circuits. Still, the field is much closer to what I learned in the math course Abstract Algebra (it covered groups, rings, and stuff like that).

Cryptography touches Boolean Algebra, Group Theory, and I think some Geometry as well. It can and will probably pull in many fields of math, if it can leverage them to create difficult one-way “trapdoor” problems. If linear algebra isn’t used there, it probably will be in the not-too-distant future.

Graph Theory comes into play in many areas of Computer Science, especially when networks are involved, but also in Parsers/Compilers.

I’m sure there’s really big obvious one’s I’ve left out, but there’s a few examples of post-secondary math used in Computer Science.",hh2z3mb,t3_qabacx,1634540688.0,False
qabacx,"> I consider Computer Science to be a subfield of Mathematics, but the line is a bit blurred between the fields of Computer Science and Software Engineering.

As someone with an exceptionally similar background (bachelors in CS with a minor in math, and a masters in CS), I’ve always seen Computer Science as a field similar to architecture.

It’s heavily based in math, and you wouldn’t be able to work out most of the underlying features without math, but the subject itself encompasses things outside the scope of math (building actual buildings; writing code) and wouldn’t just be considered a subfield. At my company for example, I don’t think many of the employees who actually write code would say that what they do has to do with math, more than systems planning and whatnot.

If you exist in theoretical computer science, you’re correct in that it’s exceptionally similar and has a ton of overlap with pure math (complexity theory being one of those topics you mentioned). As soon as you get into practice/industry though, it becomes fairly detached from math.

Since the OP seems to be self-teaching CS, it seems a lot more likely they’d be able to pursue what they wanted without needing to delve into a ton of math (unless their goal is to do one of those math heavy things lol) like you or I had to do as part of our coursework.

> Therefore broad math knowledge ends up being really useful, **but deep knowledge is only necessary for the specific field(s) of math used in whatever area(s) you dive more deeply into** (i.e. beyond an introductory course in the subfield).

And exactly this

> If you want to optimize an algorithm, you will need to do some algebra to find the asymptotic runtime of the algorithm. This can get pretty complex, especially when recursion is involved, and limits are commonly used.

I mentioned this is another comment, but algorithm optimizations can go arbitrarily far into math. For my Masters I was working on implementing bounded distance calculations for things like k-Means and KNN, and since that touches problems used in ML, the math was way heavier.",hh3nfkm,t1_hh2z3mb,1634560298.0,False
qabacx,"The reason I say it’s a subfield of math is because I don’t count Software Engineering as part of Computer Science, but rather as a separate field that makes practical use of Computer Science. However, I do recognize that many Computer Science programs are currently a mixture of both fields and the most likely job outcome of a degree in CS is to become a Software Engineer, so when I’m not being pedantic, I do merge the two a bit (which I did a bit towards the end of my comment).

So for the architect example, if there was no such thing as an Architecture degree, but you could become an architect by getting a Physics degree, then it wouldn’t change the definition of Physics, even if a good number of Physics programs started offering electives purely about Architecture.

But yeah, the only reason I led with the pedantry was to call out the distinction so if that was not already clear to the OP, they might start to look into what exactly it is they are trying to learn, whether that’s theoretical CS for the sake of knowledge, or just enough to be practical in programming.

For the latter, my recommendation is the book _Cracking the Coding Interview_, which in addition to the expected practice problems and advice on interviews, has some textbook-style information on a broad range of topics. I often recommend it to self taught developers since it’s basically all the important parts of a CS degree.",hh75o31,t1_hh3nfkm,1634619171.0,False
qabacx,"> The reason I say it’s a subfield of math is because I don’t count Software Engineering as part of Computer Science, but rather as a separate field that makes practical use of Computer Science.

Which I wanted to clarify is a “you” thing. I’ve never seen someone argue that CS is strictly a subset of math when it contains things well beyond the scope of a math degree.

> So for the architect example, if there was no such thing as an Architecture degree, but you could become an architect by getting a Physics degree, then it wouldn’t change the definition of Physics, even if a good number of Physics programs started offering electives purely about Architecture.

But architecture is significantly more than just physics, in the way CS is significantly more than math. Architecture isn’t just making a building that doesn’t fall over, it’s about art as well. If you were to kneecap architecture and lock it wholly within physics, our cityscapes would just look like someone copy and pasted the same building over and over. The portion of the field you’re dropping off in your explanation is sort of fundamental to the field, and not something you can ignore. That’s why *I* went pedantic and pointed that out.

> whether that’s theoretical CS for the sake of knowledge, or just enough to be practical in programming.

Based on some of the other comments, the OP seems exceptionally fresh to this realm, with binary and hexadecimal bases being one of the more recent concepts they learned, so I think it’s okay to be a little less verbose with things. With how wide our field is, describing it as such to folks who are only interested in a narrow sliver of information can be a daunting turn off. Math is hard enough for a lot of folks, so if you present it as just a subset of math, and not a field which requires significantly more nuance, you run the risk of “scaring” folks away.

Just my 2 cents from trying to convince my engineering friends to pick up coding to help them in their courses/work, and then hearing what sort of arguments they use for not wanting to do that.",hh7xaq3,t1_hh75o31,1634642306.0,False
qabacx,"I don’t necessarily want to drag this on since I don’t even feel all that strongly about what I said, but I do want to be clear in what I did mean.

First, it seems you misunderstood my analogy. I was comparing Software Engineering and Computer Science to Architecture and Physics. I don’t think anybody would call Computer Science art, but people certainly consider art to be a part of programming. Any other engineering field would probably be a better comparison but I didn’t want to drag the whole licensure thing into it.

And I do want to point out that considering Computer Science to be a subfield of math isn’t just a “me” thing. A lot of people argue on either side of the matter. It comes down more to whether you consider Computer Science to be “the set of things taught in a CS degree” or what I described, where that is a larger set of topics that includes computer science.

It doesn’t help that everyone has a different idea not just of what computer science is, but also of what math is. Both have fuzzy borders. I googled a bit just now to make sure it wasn’t just some strange opinion I just happened to have heard from a few outliers, but it’s definitely a thing that people question. An answer I saw was that “theoretical computer science” is math but some non-theoretical aspects of the field are not. I think at the end of the day, since there’s no formal definition of what is or isn’t math or CS, nobody really cares to come to a consensus on it and they just agree to disagree.",hhnz0ky,t1_hh7xaq3,1634934877.0,False
qabacx,First and second derivatives are really important in numerical algorithms. Many real world problems are posed in such a way that you want to minimize a convex function. First and second order information is used to find a candidate answer as well as to say the candidate fits some criteria to accept it as the solution and terminate computation.,hh286mb,t3_qabacx,1634523541.0,False
qabacx,"Like the other comments mentioned, it really depends on *what* exactly in computer science you’re doing. It’s a huge field, so if you could specify what exactly you’re hoping to do with your knowledge, it’d be easier to tell you how much math you’d need to do that.

For example, you don’t need any serious math to make a webpage or write basic systems applications, but you would need a lot of math knowledge if you were interested in theory like algorithm design and machine learning.

I’m someone who minored in math in college (doing Numerical Analysis courses, Theory of Numbers, and the like) and I got my Masters where I focused entirely on theory (like Automata, Complexity Theory, Graph Theory, Compiler Optimizations), and I now work as a software developer who has to do like no math at all. It *really* depends on what exactly you do within computer science, as to how much math you really need lol",hh24n31,t3_qabacx,1634521725.0,False
qabacx,"Oh I mentioned it before I’m just practicing rn as a hobby so I’m just curious on what’s coming as I’ve already pretty much mastered pc/server building and programming in languages such as python/Java/C+
I just saw a lot of complicated algorithms earlier today and they’ve been bugging me a little",hh2d4ca,t1_hh24n31,1634526111.0,True
qabacx,"> I’ve already pretty much mastered ... programming in languages such as python/Java/C++

Trust me, if you’re just learning about hexadecimal and binary, then you’ve still got plenty to learn from those 3 languages ;)

If you’re just talking about like the basics though, then that’s a great set of languages to get involved with, since it’s a great *breadth* of languages.

> I just saw a lot of complicated algorithms earlier today and they’ve been bugging me a little

Could you maybe expand and point to some of the ones you mentioned (I’d also be happy to talk through those if you wanted)? Algorithm design can be math heavy, but is really dependent on what you’re doing.

For example, I worked on some stuff for my Masters related to bounding distance calculations using an approximation that took fewer CPU instructions, where the math required *was* higher dimensional calculus, but only in the proof, not the creation.

We were using this framework to help speed up various common machine learning algorithms and in order to better tweak our work to fit those you needed to bust out some fairly complicated stuff. Now, while figuring out what we needed to do required a lot of math, actually doing it didn’t require like any at all, because it’s more about understanding what problem you’re working on, and how the code will be able to mimic that, than any of the underlying math.",hh2iedw,t1_hh2d4ca,1634528959.0,False
qabacx,"I received my BSc in Computer Science and worked in industry for a while before moving to product management. I did novel undergraduate research in Computer Vision, and have written software deployed in fortune 50 environments. I’ve seen a bunch, but not everything. It’s hard to emphasize the size of this field.

Computer Science definitely requires math, but not necessarily the kinda of math you are used to calling math.

There’s a whole world of math we use in computer science like formal logic, finite state systems, automata, graph theory, algorithmic optimization, the list goes on. A better understanding of calculus is helpful, but not necessary for every discipline.

It’s a lot like IT, where you have a tool or approach to a problem, and as long as it “works” it’s kind of up to you the degree you wanna understand it. “It works” isn’t good enough for computer scientists, often we focus on eking out any little optimization we can because we think at scale. Approximation of changes due to scale require some math, and it enables us to make generalized comparisons across different problem sets.

A lot of it doesn’t feel the same as math, except when you’re taking an exam. I still have nightmares about “One knave only tells truths, and the other lies! If you can’t figure it out you repeat the semester! HeeHee!”",hh2b1ed,t3_qabacx,1634525016.0,False
qabacx,"A very valuable tool is series functions, like series addition or multiplication. These are actually pretty easy to do in code as most languages have some form of ""for-next"" loop. Recurring functions can produce results that single-pass equations can't always. But it's not the solution to everything, sometimes I write a complex function only to find some simple algebraic equation that does the same thing. Just another tool in the toolbox!",hh2b8dp,t3_qabacx,1634525117.0,False
qabacx,"Computer science is a very big field, and in a lot of subfields, maths is definitely needed. However, the specifics of maths needed might differ based on the specific computer science field you are studying. Calculus might be needed in fields such as optimization, control systems, computer graphics, etc. However, I would say that the biggest and most important branch of mathematics that is used in almost any subfield of computer science is discrete math: combinatorics, graph/tree, number theory (arithmetics), and other discrete structures.",hh2e9rt,t3_qabacx,1634526723.0,False
qabacx,"I dont know very much about graphics programming but i've heard it heavily uses calculus and linear algebra. Certain topics you learn in calculus will come up everywhere in computer science in general. Newtons method is used a lot in optimized math related code. Microcontrollers that need to do trig functions fast will store a table of values and a table of derivative values to approximate the values very very fast. Series, sequences, and recurrence is also used in complexity analysis and stuff.",hh2svxb,t3_qabacx,1634535771.0,False
qabacx,"It really depends, a lot of the math is really just to deeply understand what you are doing. For example, Machine Learning is one area which relies super hard on Math, but there are so many tools available which allow you to do all kinds of Machine Learning without using or even understanding any math at all.

The biggest part that math comes in handy is for interpreting and problem solving if the out of the box solutions/algorithms don't work. In the machine learning example, multivariable calculus provides so much insight into what is going on, that it can help you identify and resolve problems even if no one has ever taught you how.",hh3cka2,t3_qabacx,1634552808.0,False
qabacx,"You need discrete math / mathematical logic much more than calculus. I have read 100 papers in the last couple years, all of which use formal logic, none of which use calculus.",hh3hkua,t3_qabacx,1634556576.0,False
qabacx,"Web dev with sysadmin things on the side. Trudged through anything regarding theoretical maths and physics, didn't need them since. I assume it depends heavily on your field, of course I wouldn't need them for example :D

It is often the case that courses despite not being directly important do give you more spherical understanding of what's what, and I feel every single bit helps build your knowledge on various connected topics exponentially. However, most of it stays in the trash can for now in my case, basically anything beyond general concepts and considerations.",hh40ihd,t3_qabacx,1634566950.0,False
qabacx,"Fourier transform (not FFT) uses calculus if I recall.

Used to transfer audio data from amplitude-time to frequency-time and phase-time.

Used extensively in audio processing, and speech recognition software.

You'll find a lot of cases like this where something somewhat specific is implemented using the basis of calculus for other more widely used stuff.",hh5g8jp,t3_qabacx,1634588450.0,False
qabacx,i feel the same fear. same position .,hh5w2fh,t3_qabacx,1634595938.0,False
qa3kdv,"If by ""computational method"" you mean ""algorithm"" you might like the Algorithm Design Manual, by Skiena. It's a good introductory choice.",hh1qwox,t3_qa3kdv,1634514895.0,False
qa3kdv,The Comprehemsive algorithms book is Cernan et al. Intriduction to algorithms,hh42eq6,t3_qa3kdv,1634567798.0,False
q9x87e,Thanks.,hh1m6xk,t3_q9x87e,1634512635.0,False
q9waxg,"Well, I guess that's an answer of sorts.",hhcnu11,t3_q9waxg,1634731224.0,True
q9u6uz,"try writing the whole number out. you would be left shifting instead of right shifting then. also remember the implied 1. 

&#x200B;

1.1101011011\*10\^10001 first number ignoring the bias of 15 (dont ignore if its homework)  
1.0001100101\*10\^01001 second number the same way

expand them  
111010110110000000.0  
1000110010.1

add them  
  111010110110000000.0  
\+000000001000110010.1  
\-------------------------------------  
  111010111110110010.1

round it  
  111010111110000000.0

figure out exponent  
1.110101111100000000 its 10001

subtract one and remove digits past 10:  
1101011111

combine exponent with fraction and sign bit:

0100011101011111  


I'm pretty sure your mistake was either forgetting the 1 before the fraction or shifting the number wrong",hgyvupj,t3_q9u6uz,1634465580.0,False
q9mje2,"Start with Wikipedia, then google each modulation",hgx9eb6,t3_q9mje2,1634429251.0,False
q9mje2,"no i will not, that is why i posted this post.",hgxctam,t1_hgx9eb6,1634430845.0,True
q9exqs,"Doesn't exactly fit into pure computer science, but the Darknet Diaries podcast is fantastic and he discusses computer security without all of the buzzword sensationalism.",hgvmdiz,t3_q9exqs,1634403193.0,False
q9exqs,Thank you just added it!,hgxb3yk,t1_hgvmdiz,1634430056.0,True
q9exqs,Scott Hanselman's podcast is decent. As is the Stack Overflow podcast.,hgvjpgg,t3_q9exqs,1634402016.0,False
q9exqs,Thank you. Adding both!,hgvlqco,t1_hgvjpgg,1634402914.0,True
q9exqs,"The Lex Friedman podcast is fantastic. He certainly has ventured into other topics, but has a strong passion for AI and a massive amount of brilliant guests!",hgxa0l5,t3_q9exqs,1634429545.0,False
q9exqs,Agreed. I have him: https://app.cicero.ly/people/427089628,hh06y8u,t1_hgxa0l5,1634490734.0,True
q9exqs,"""People on social media talk about people and things, I just want to do what the computers want""",hgvurcd,t3_q9exqs,1634406754.0,False
q9exqs,Bjarne Stroustrup,hgw7x9i,t3_q9exqs,1634412398.0,False
q9exqs,Add [CoRecursive](https://corecursive.com/)!,hgw8mi8,t3_q9exqs,1634412693.0,False
q9exqs,Nice one! Done,hgwelkq,t1_hgw8mi8,1634415294.0,True
q9exqs,I think Ben Eater has a great YouTube channel for the fundamentals,hgxbfb2,t3_q9exqs,1634430200.0,False
q9exqs,What exactly amazing  has done Lex in computer science besides that amazing comp sci/physics/science/deep learning podcast?,hh04068,t3_q9exqs,1634489487.0,False
q9exqs,it's Fridman. he literally says it in the beginning of his podcast.,hhgct3o,t3_q9exqs,1634790009.0,False
q9exqs,[Bryan Cantrill](https://youtu.be/cuvp-e4ztC0),hiegiiw,t3_q9exqs,1635439974.0,False
q9e5oo,"Worse yet, it's not really about computers!",hgvb5nj,t3_q9e5oo,1634398237.0,False
q9e5oo,"“Computer science is no more about computers than astronomy is about telescopes.""",hgyhm6f,t1_hgvb5nj,1634454017.0,False
q9e5oo,It's like that holy roman empire.,hgyrh3b,t1_hgvb5nj,1634462052.0,False
q9e5oo,"Does it matter? Call yourselves what ever you like. Computer engineer, computer scientist, computerologist, programmer or developer. I like to switch it up depending on what I'm working on. If I'm working on the web I'm a developer. If I am working on research, I am a scientist. If I am programing in c or c++, I am an engineer.",hgw4urz,t3_q9e5oo,1634411094.0,False
q9e5oo,IMHO you're an Engineer if you have a degree in Engineering.,hgyirvd,t1_hgw4urz,1634454906.0,False
q9e5oo,"""Let's see: you're a undergraduate computer science student with emphasis on...""

*\*squints\** 

""Software ~~Engineering~~ Creationing""",hgys1cl,t1_hgyirvd,1634462511.0,False
q9e5oo,"Lol when I was in school, computer science was considered part of engineering",hgz3e30,t1_hgys1cl,1634471396.0,False
q9e5oo,It is where I studied engineering.,hgz7glx,t1_hgz3e30,1634474083.0,False
q9e5oo,"Well, a title verifies that you have that knowledge. That doesn't mean you can't have that knowledge and be an engineer without the title.",hgyznll,t1_hgyirvd,1634468611.0,False
q9e5oo,That logic breaks down if you apply it to lawyering or doctoring.,hgz7c1r,t1_hgyznll,1634474008.0,False
q9e5oo,"One thing is what a job or the state requires yo to have (a title, sometimes), and other thing is what jobs you're capable to do.",hgzj254,t1_hgz7c1r,1634480267.0,False
q9e5oo,"The laws are there for a reason. Just because it isn't enforced in the US yet, doesn't mean it won't be. Requirements for calling yourself a Software Engineering will come sooner than you think. At first for particular industries like finance and infrastructure, then more generally. The only reason it hasn't come yet is because it is such a young field (60 years vs thousands of years for doctoring and lawyering).",hgzmxnl,t1_hgzj254,1634482048.0,False
q9e5oo,"Yeah, old minded people will try to force it, like everything",hgzyn64,t1_hgzmxnl,1634487183.0,False
q9e5oo,"If they ask at the airport desk: computer engineer, if someone asks randomly: programmer.",hgxnfm4,t1_hgw4urz,1634435987.0,False
q9e5oo,Computer engineers are specialized electrical engineers. Try software engineer.,hgyx6a1,t1_hgxnfm4,1634466655.0,False
q9e5oo,"Actually the translation from where I from is systems engineer, which corresponds to the degree of systems engineering that is very similar to computer science/sofrware engineering usually with a bit more physics and hardware stuff.",hgzelt7,t1_hgyx6a1,1634478061.0,False
q9e5oo,"Those mean different things to me:

**Programmer**: Anyone who can type code. It says nothing about your skills in developing software.

**Developer**: You know how to build software, more than just scripting.

**Computer Scientist**: You develop algorithms, be that with pen and paper or whatever. 99% of them are academics. Often their actual software developing isn't that high, and would need someone else to commercialize their algorithms.

**Computer Engineer**: You have a formal degree in engineering, specializing in computer engineering. Generally you work on hardware. That said, this is different from country to country, notably in the US anyone can be an Engineer with any background.

Then also:

**Software Engineer**: You have a formal degree in engineering, specializing in software engineering. Generally you work as a developer building large/complicated systems. That said, this is different from country to country. In the US being a software engineer and a developer is probably synonyms.

In general: If you wrote computer/software engineer on your resume here in Norway and didn't have a formal education in it, I'd automatically disqualify you as a fraud.",hgz6x43,t1_hgw4urz,1634473758.0,False
q9e5oo,I swear to God. If I hear someone call themselves a Computerologist..... Well I'll just lose my mind!,hgz394n,t1_hgw4urz,1634471299.0,False
q9e5oo,"Up in here, up in here!",hgzi4f2,t1_hgz394n,1634479812.0,False
q9e5oo,Ya'll gonna make me go all out,hgzmtsl,t1_hgzi4f2,1634481999.0,False
q9e5oo,Similar to how mathematics is not a science but its often called that.,hgw659r,t3_q9e5oo,1634411645.0,False
q9e5oo,"We should continue referring to it as computer science if for no other reason than ""computology"" is a fucking stupid moniker.",hgvg58c,t3_q9e5oo,1634400431.0,False
q9e5oo,Is calcomancy okay? May I be a Computemancer?,hgw1w7j,t1_hgvg58c,1634409836.0,False
q9e5oo,"The politically correct term, is Technomancer.",hgxikdc,t1_hgw1w7j,1634433604.0,False
q9e5oo,I feel like Ray Kurzweil is the only true technomancer tho. I could never 😮,hgxzoz0,t1_hgxikdc,1634442286.0,False
q9e5oo,"A prof of mine in college preferred ""procedural epistemology""",hgxm7oz,t1_hgvg58c,1634435375.0,False
q9e5oo,"Computer science is a misnomer. I like to describe my field as ""the mathematical study of the mathematical idea of the computation"". I distinguish computer science as a distinct discipline to computer programming.",hgya0j8,t1_hgvg58c,1634448452.0,False
q9e5oo,I think just Computation or Computing would've been a better name.,hgyl9c8,t1_hgvg58c,1634456932.0,False
q9e5oo,We shall call us techpriests from now on!,hgyqffk,t1_hgvg58c,1634461170.0,False
q9e5oo,"Science in other languages is often used for ""discipline"" rather than the English ""empirical study of a natural phenomenon"". I think it works for this case as the discipline use.",hgvvaif,t3_q9e5oo,1634406992.0,False
q9e5oo,What languages? I speak 3 romance languages and all of them hold a very similar meaning to english. Scientia in latin.,hgw3x72,t1_hgvvaif,1634410693.0,False
q9e5oo,"German has the word ""[Wissenschaft](https://en.wikipedia.org/wiki/Wissenschaft)"", which is a broader concept than science and includes many kinds of academic inquiry. At least Swedish, Finnish and (according to the Wikipedia article) Polish have words with a similar broad meaning.

Neither is there a direct translation for these words in English, as far as I know, nor is there necessarily a direct translation of the word ""science"" with a similar scope in those other languages. I don't know all of those languages well enough to be sure of the latter in all of them, though.",hgwcyec,t1_hgw3x72,1634414566.0,False
q9e5oo,Very interesting thanks!,hgwohp4,t1_hgwcyec,1634419615.0,False
q9e5oo,Yep I heard it first from German,hgwq8ck,t1_hgwcyec,1634420407.0,False
q9e5oo,"To add to that nice piece of information:

In Dutch the equivalent of that is ""wetenschap"" and is also used to mean academic inquiry. It can be loosely translated as ""knowledge"" as well, being a concatenation of ""weten"" (to know) and ""schap"", which is a suffix coming from ""scheppen"" which means ""to create"", and the latter suffix now also means something like ""a state of being"". Dutch, English, German, Swedish, Icelandic, Danish and Norwegian all arose from proto-german, so there might be an English or Old-English equivalent.",hgyoigz,t1_hgwcyec,1634459588.0,False
q9e5oo,"I also came across the Dutch equivalent and thought it was probably similar in meaning but wasn't sure since I don't know Dutch. I guess all the equivalent Germanic words in those other languages might be similar.

The Swedish word for computer science is ""datavetenskap"", so it kind of does what the original commenter said. As a bonus, it doesn't mention computers. :)

As for old English, it seems like the word ""science"" has been in the English language since the 14th or 15th century [1] and changed its scope since. I wasn't able to find an old English equivalent for the Germanic word but I didn't look very hard.

I'm a bit confused from reading various dictionary entries for the word ""science"", though. [The Wiktionary page for ""science""](https://en.wiktionary.org/wiki/science#Usage_notes) indicates the word would also be inclusive of social and formal sciences, and computer science is certainly one of the latter. I wonder if even English is really consistent in terms of what the word means.

--

[1] https://www.etymonline.com/word/science",hgz9xnn,t1_hgyoigz,1634475552.0,False
q9e5oo,Cool. Didn't know that. I guess meaning can be a rather complex topic. E.g. Wittgenstein and his [language game](https://en.m.wikipedia.org/wiki/Language_game_\(philosophy\)) thing.,hh3a39q,t1_hgz9xnn,1634550666.0,False
q9e5oo,"Desktop version of /u/Abiogenejesus's link: <https://en.wikipedia.org/wiki/Language_game_(philosophy)>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hh3a445,t1_hh3a39q,1634550687.0,False
q9e5oo,"**[Language game (philosophy)](https://en.m.wikipedia.org/wiki/Language_game_\(philosophy\))** 
 
 >A language-game (German: Sprachspiel) is a philosophical concept developed by Ludwig Wittgenstein, referring to simple examples of language use and the actions into which the language is woven. Wittgenstein argued that a word or even a sentence has meaning only as a result of the ""rule"" of the ""game"" being played. Depending on the context, for example, the utterance ""Water""! could be an order, the answer to a question, or some other form of communication.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hh3a470,t1_hh3a39q,1634550689.0,False
q9e5oo,"Breaking it down, first there's the word for knowledge, and then a suffix that translates to -ship or -hood.

So literally I think it means knowledge-ship or knowledge-hood.  
But translating more loosely I think I'd go with ""Wise-work""",hgzdlno,t1_hgwcyec,1634477547.0,False
q9e5oo,"Scio means I know in Latin. Not sure what it would mean in modern Romance languages. 

In most Indo-European languages the terms for science derived from the (often archaic) terms for knowledge.

In Slavic languages there is no distinction between ""arts"" in the B.A. sense and sciences in B.Sc. sense. A single word is used for all academic disciplines.",hgyck16,t1_hgw3x72,1634450228.0,False
q9e5oo,In german it is Informatik. Like Mathematik but for information technologie. I like this name. Sometimes I heard it called informatics (english pronounciation).,hgwbdoq,t3_q9e5oo,1634413873.0,False
q9e5oo,"In theory I like it as well but ""Informatiker"" is such a bad stereotype (btw I started as EDV guy ;))",hgwiwvp,t1_hgwbdoq,1634417154.0,False
q9e5oo,"I know, I know ... but its getting better. The stereotype is changing. At least I hope so ... I really do my best to show that an it guy is able to behave like a normal person :D",hgwjpyt,t1_hgwiwvp,1634417496.0,False
q9e5oo,"In Europe we generally picked the word informatics to describe it. Which I find it nicer, from the French information automation. I have a degree on  informatics engineer, that covered soft and hardware.",hgyppgb,t1_hgwbdoq,1634460561.0,False
q9e5oo,Simply put: computer science is a formal science like mathematics - since computer science is a branch of mathematics.,hgxtfo5,t3_q9e5oo,1634439006.0,False
q9e5oo,"To an extent, I agree.

Computer science is not _a_ science.  But it is _science_.

To call it computology, is to call it ""the science of computing"", as that's what -ology means.

So it is correctly named Computer Science, as it is the science of computing.",hgwhks0,t3_q9e5oo,1634416578.0,False
q9e5oo,"I think it is a science. Science of computing. How can we compute something with logical steps, what can be computed, how to design and implement computation models, how to design and implement machines for doing computation. Once you have profound science of computing, you can use this to develop products and applications, and there comes the engineering. Now they both go on parallel thats why its taught as Computer Science and Engineering.",hgymcqu,t3_q9e5oo,1634457821.0,False
q9e5oo,"Computer Science is a branch of mathematics. A sub-branch of discrete math, to be precise.
Since math is not science, neither is Computer Science.",hgvofta,t3_q9e5oo,1634404075.0,False
q9e5oo,"The problem with your argument is this:

Discrete math deals with the formal/symbolic specification of theories, algorithms and so on. Now the formal definitions on their own have no impact in the physical world, however, when computed on a machine, there is some residual effect that can be observed and translated into domain knowlege.

Now, given the def of science being, 'a systematically organized body of knowledge on a particular subject', Computer Science is def a science, just not a natural science.",hgvxbhb,t1_hgvofta,1634407890.0,False
q9e5oo,"Next they’ll say higher education has nothing to do with how the professional or real world operates, and it’s all one giant scheme to sell debt.

Wait…",hgxby3c,t3_q9e5oo,1634430439.0,False
q9e5oo,Ngl you had us in the first half 🤪,hgxvffv,t1_hgxby3c,1634440030.0,False
q9e5oo,Higher education shouldn't be a vocation training center. It should be an institution of theoretical learning.,hgya4a3,t1_hgxby3c,1634448520.0,False
q9e5oo,"I've always thought this, and I have a Master's degree in computer science",hgwp54e,t3_q9e5oo,1634419910.0,False
q9e5oo,But political science is science?😬😬,hgxva9g,t3_q9e5oo,1634439961.0,False
q9e5oo,And it isn't about computers 🤣🤣🤣,hgyhf80,t3_q9e5oo,1634453871.0,False
q9e5oo,"It is the reason why there is a phrase called Natural Science. As long as a decipline answers why and how a certain thing works, it's qualified to be called a Science. That's it. I dont think this should be an argument.",hgyt9cf,t3_q9e5oo,1634463473.0,False
q9e5oo,"I consider computer science a science because you actually use a ton of equations from physics to get to the results we have.  Of course, lot of advanced studies use some math (abstract algebra, calculus, etc).  This leads to having a lot of theories that define our base assumptions.",hgzhx8u,t3_q9e5oo,1634479717.0,False
q9c7cp,"What do you mean with **This helps automate the process of writing code for many programs.**?

You need a compiler (or at least an interpreter) to run a program. The CPU does not know how to execute text.

You do not need to write a complete compiler from scratch for every language. You can compile to an intermediate representation and use a gcc-backend or LLVM or something similar to get executable instructions for your CPU (or VM).",hgv0m24,t3_q9c7cp,1634393088.0,False
q9c7cp,I think they mean IDE not compiler,hgvc1mr,t1_hgv0m24,1634398643.0,False
q9c7cp,"In academics they build their own compiler to give you the practical experience in compiler design. But in practice you will not build a compiler until you are developing a new language. Then where does my knowledge of compiler design comes hands on? Answer is, optimizing the existing compilers, building interpreters for small languages, research in compiler design etc.",hgxrwxg,t3_q9c7cp,1634438237.0,False
q9c7cp,"Compilers are designed not for programs but for programming languages.
There are some reasons for developing a new compiler:
1. For existing language:
1.1. To produce more effective code than other compilers 
1.2. For new hardware/software platform 
2. For new languages — as there are no languages completely free of any issues and research in this area doesn’t stop 

Also there is a couple of interesting theoretical concerts about compiler design: metacompilation & universal compiler",hgv18dw,t3_q9c7cp,1634393419.0,False
q95xo2,My suggestion is to start with Automate the Boring Stuff with Python chapters 11 - 14 will contain useful information to get you started . You first want to start with the first few chapters on Python programming basics. But 11 - 14 will get you working with the sorts of documents that will most likely contain the data that you seek to manipulate.,hgu25qi,t3_q95xo2,1634367362.0,False
q95xo2,is this on freecodecamp or something else?,hgu9asd,t1_hgu25qi,1634373531.0,True
q95xo2,"The book is free online: https://automatetheboringstuff.com/

And the udemy class is almost always free (or exceptionally cheap Too)",hguzwd0,t1_hgu9asd,1634392710.0,False
q95xo2,"It would help if you could tell us a little more, such as:

1. The kind of data you’re receiving; Excel files sent by email, csv exports from some school system, or?

2. The kind of data processing you need to do once you have all the data; are you just ranking students by worst grades (simple sorting)? Or are you looking at students with otherwise fine grades but one or two grades are failing (variance / outlier detection)? Or students who used to get good grades but who seem to be slipping (regression)? Or something totally different?

Basically what you want to do is probably not too difficult and you should be able to learn enough to build a good working POC by next summer. But the input and concrete analyses determine how you should proceed so a little more detail would help.",hguotvj,t3_q95xo2,1634386357.0,False
q95xo2,"I will be able to find all of that out from the principal on monday, and will let you know then. Thanks!",hgxzgu4,t1_hguotvj,1634442165.0,True
q95xo2,"Transact SQL aka T-SQL.

It's what Microsoft SQL servers aka Databases use, which is what the majority of corporate businesses use for data management. 

Structured Query Language is a strongly typed, static, succinct language used for interacting with Relational Databases.

There is no shortage of work for people who can use SQL well - https://learnsql.com/blog/sql-programming-language/ 

It's also a very easy to understand language. You write it similar to how you'd write a request in English.

Example- SELECT LastName FROM Customers WHERE FirstName = 'James' 

This will return you a list of all the last names for records in the database table ""Customers"" where the first name is James. 

You can play around with SQL here, it's very widely used and if you learn SQL, you'll definitely have jobs lined up! 

https://www.w3schools.com/sql/ 

Once you have a good enough understanding of SQL, add that to a Microsoft Product ""PowerBI"" and you'll be able to solve your problem with no sweat.",hguwg13,t3_q95xo2,1634390898.0,False
q95xo2,"Have a look at Python for Everybody by University of Michigan on the Coursera platform. The courses are fairly interactive and the final capstone project on Course 5 may be of interest to you: Retrieving, Processing and Visualizing Data with Python.

[Python for Everybody](https://www.coursera.org/specializations/python)",hgwpmiz,t3_q95xo2,1634420129.0,False
q8w84n,An emulator simulates the hardware in the machine on top of the software. The absolute requirement is that every piece of hardware beyond the base compiler language is simulated.,hgsktni,t3_q8w84n,1634336274.0,False
q8w84n,"In short, the difference is somewhat a blackbox and whitebox approach. Simulation is blackbox. As long as the inputs and outputs match, the internal design can function however we like. Emulation is whitebox. We are recreating the exact logic used internally by the source device.

As an example, let's look at drawing a 5-pixel horizontal line. In a simulator, we can draw that line however we like, even using specialist hardware available on the machine running the simulation for improved performance.  
In an emulator, we need to determine the method of *how* that horizontal line should be drawn. Do we start from the left or the right? What interval is there between drawing each pixel? Is it even drawn pixel-by-pixel, or all at once? What state should the registers hold following the operation? All these have to be considered, and as another commenter mentioned, to achieve this we need to specifically define this logic in software!


For the second part, any Little Man Computer implementations are *simulators*. The reason for this isn't much of a techie one - It's simply because the LMC instruction set was designed as a teaching guide rather than stemming from actual hardware. As the name implies, it's based on the concept of a busy little man in a mail room. Until we get fully human AI and we find out who this mystery mail man is, it might be a while before we get wholly accurate emulators!",hgt67ks,t3_q8w84n,1634347172.0,False
q8w84n,This seems like a homework question...,hgsouuj,t3_q8w84n,1634338238.0,False
q8w84n,Lol what,hgsqcm6,t1_hgsouuj,1634338987.0,True
q8w84n,A crap one.,hgub7ia,t1_hgsouuj,1634375268.0,False
q8w84n,This.,hgsadmd,t3_q8w84n,1634331409.0,False
q8w84n,"If a data stuct. will represent a man's dick, will it be an array or a linked list?",hgt5ta6,t3_q8w84n,1634346955.0,False
q8w84n,Linked list might be better. Don’t want to reallocate and copy when you get a hard-on,hgu7a1n,t1_hgt5ta6,1634371732.0,False
q8rs2z,"Start from first principles 

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)

Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms)

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)

And finally, [play the long game when learning to code.](
https://stackoverflow.blog/2020/10/05/play-the-long-game-when-learning-to-code/)",hgsjjkr,t3_q8rs2z,1634335654.0,False
q8rs2z,Not OP but thanks for the list!,hgu0avg,t1_hgsjjkr,1634365853.0,False
q8rs2z,"I'd say from the beginning, or at least an earlier era, but it depends on your general disposition. Computers were a lot easier to comprehend 40 years ago, whereas a modern computer is almost impossibly complex, requiring you to operate with more of an abstract model of how a computer works. Learning machine-level programming on a simpler device will help reveal the nature of computers, so to speak.

But then again, that may totally not be your thing. If you're interested in an academic approach then ""Structure and Interpretation of Computer Programs"" by Abelson & Sussman is an excellent book that somehow manages to cover more or less everything, while also getting you started with the very CS-credible programming language of Scheme. The book is available for free online, in lots of different formats, if you don't want to invest in a physical copy.

If you want to achieve things quickly then look into learning Python programming. It's a language that's easy to pick up and which is very expressive (allows you to achieve a lot with relatively little code), and also very popular amongst scientists of various persuations. In traditional CS domains it's mainly made inroads in AI and machine learning.

Really, pretry much anywhere can be a good place to begin, but it helps immensely if it's somewhere you find at least moderately interesting. Good luck!",hgrc7uv,t3_q8rs2z,1634317099.0,False
q8rs2z,"I think I'd put a hard cap on that 40 years. Anything earlier and you regress away from some of the improvements in computer design that made life easier. For example, concepts like RISC are a huge improvement over what came before.",hgrwqbf,t1_hgrc7uv,1634325532.0,False
q8rs2z,"Well, in the beginning everything was RISC. 40 years ago was the era of the eight bit micros that later served as inspiration for the RISC architecture. The CISC ""monsters"" didn't really break through until the late eighties (though CISC also isn't bad by definition; the Motorola 68000 series is a wonderful CISC architecture for example).",hgs1nc1,t1_hgrwqbf,1634327612.0,False
q8rs2z,I don't know. I'm currently reading TAOCP which uses 60s inspired design for it's fake MIX architecture. It is stupidly complex to me when looking at the newer MMIX RISC inspired design using modern sensibilities. That original design is so unnecessarily complex to me in comparison that I've completely paused my reading until I can get the addendums with the easier to understand and more relevant modern design.,hgs3hlm,t1_hgs1nc1,1634328401.0,False
q8rs2z,"Yeah, there were definitely some wild architectures in the fifties and sixties! Thankfully there's very little risk someone would accidentally stumble over one of them today, without knowing what they are looking for. :-)",hgs46ou,t1_hgs3hlm,1634328705.0,False
q8rs2z,"Haha yeah, that's why I think it's just best to hard cap it to the 70s or 80s as the earliest. Just minimizes the chance of running into weird oddities like decimal/binary hybrid computers with variable length words or whatever other weird stuff that wasn't standardized yet. Basically don't go back in time to before C or UNIX.",hgs5cmy,t1_hgs46ou,1634329210.0,False
q8rs2z,Yeah... Or just something as subtly insane as ones' instead of two's complement. No earlier than Unix is a good dividing line.,hgs6jks,t1_hgs5cmy,1634329728.0,False
q8rs2z,"I agree. Python is probably the best place to start and quickly learn CS principles. 

I tried starting with C# before I knew anything and it felt impossible to breakthrough.

Start by doing what's possible then suddenly you're doing the impossible.",hgt4p8v,t1_hgrc7uv,1634346346.0,False
q8rs2z,"It probably depends on what sort of student you are. If you are self directed and motivated you might pick up a book on programming and work through it. Getting a bit of the history of computing is good. Learning something about the hardware is good. You may find a more specific interest in any one of these areas and go deeper.

I do better in more structured, classroom settings. If I were in your position I would either enroll in a college/university or a community college and take a series of classes. If you are not in the US I am not sure what the equivalent of community colleges might be. These often offer 2 year degrees in many subjects.

Computer Science is a remarkably large field with a rich history. Few people are experts in the whole of CS. Most people tend to gravitate towards a fairly narrow sub-field and specialize in that. 

Good luck on your journey",hgrbfrm,t3_q8rs2z,1634316779.0,False
q8rs2z,Back in the beginning a found a quick overview of the history of computers helped me place things in my head. From there I started reading and watching about increasing amount of stuff. There are a lot of recorded talks from conferences and stuff where someone will go on at length about a very specific topic. I always found those fun and informative. I always had an interest in games so I watched through John Carmack talking about how he created various graphical things. Definitely fascinating though very complex. I'm sure there's a lot left to find me fill in the rest though. Youtube is a good resource.,hgr6qas,t3_q8rs2z,1634314853.0,False
q8rs2z,Thank you so much!,hgr9qra,t1_hgr6qas,1634316097.0,True
q8rs2z,"It's been mentioned elsewhere in this thread, but Harvard's CS50x is a great place to start.",hgtm7ww,t3_q8rs2z,1634356097.0,False
q8rs2z,I'm afraid that won't be enough to get a job or build a career at any levels. Comp sci domain is like dog years. Things tend to move pretty fast here compared to other fields. And the interviews are super competitive.,hgu52hm,t3_q8rs2z,1634369820.0,False
q8rs2z,"I think my strategy would be instead of just starting with history or basic concepts, begin with the things that interest you the most. A lot of us are old and may not remember that our passion for CS comes from discovering things we like and deep diving on those. Eventually we round out our knowledge with formal training or effort like the excellent comments before mine suggest, but for the absolute beginner the motivation must be true and not just getting into it because tech is popular or makes a lot of money.",hgutwec,t3_q8rs2z,1634389456.0,False
q8rs2z,"Start by learning binary math.    
When you start to learn programming, avoid the scripting languages.  Start with something like Java.  Learn how to compile and run from the command line.  Later, learn a scripting language, not to be confused with an interpreted language.  
After that, study computer archenteric, automata, and some assembly language.    
Then you will be ready to begin.",hgtg3zp,t3_q8rs2z,1634352550.0,False
q8rs2z,What even is this comment,hgv1kqq,t1_hgtg3zp,1634393596.0,False
q8rs2z,Likely some cs sophomore trying to feel impressive.,hgv62dn,t1_hgv1kqq,1634395816.0,False
q8rs2z,"I like how you start with binary math because duh. And then after I’ve learned computer (I assume he means) architecture and assembly and I can write programs in Java and assembly, and I also happen to have some basic theory of computation stuff, then I’m ready to begin. Like, dawg, we began a long time ago. Just start coding lol it’s not that deep.",hgv7xah,t1_hgv62dn,1634396702.0,False
q8rs2z,I think he's just drawing on his 10 years of experience teaching at a research university.,hh2wc8c,t1_hgv62dn,1634538421.0,False
q8rs2z,">""Then you will be ready to begin""

No easier way to make everyone lose all possible respect for you.",hgz8nih,t1_hgtg3zp,1634474809.0,False
q8rs2z,"I say start with Head First Java.

I did Head First Java up to programming threads and it taught me stuff I'm still using while I learn about everything else all the time.",hgz8uf9,t3_q8rs2z,1634474926.0,False
q8rs2z,"Yeah, a University",hhi6h96,t3_q8rs2z,1634832152.0,False
q8rs2z,"[John Zelle's Intro Python book](https://www.goodreads.com/book/show/80440.Python_Programming) is a good starting point for beginners! It might gloss over some low-level things, but if you're looking to try your hand at coding right away, it's a pretty good choice and easily digestible I think.",hgrsg14,t3_q8rs2z,1634323732.0,False
q8rs2z,Teachyourselfcs.com,hgulet1,t3_q8rs2z,1634383938.0,False
q8rs2z," Chances are, you’re going to become very interested in a particular part of CS. For me it was programming at first, now I’m into OS and hardware security. Keep your mind open and don’t be afraid of any topic.",hgv6l76,t3_q8rs2z,1634396065.0,False
q8kgky,"No other book goes to the same depth and breadth as Knuth.

But the Wizard Book - Structure and Interpretation of Computer Programming by Abelson, Sussman and Sussman - should certainly be on your list.",hgq29ib,t3_q8kgky,1634294773.0,False
q8kgky,"SICP 

K&R 

Dragon book 

CLRS Introduction to Algorithms

Not sure about this one but Sisper's Introduction to the Theory of Computation",hgte7c7,t3_q8kgky,1634351500.0,False
q8kgky,Artificial Intelligence: a Modern Approach.,hgu4eop,t3_q8kgky,1634369260.0,False
q8kgky,The dragon book?,hgpxbi3,t3_q8kgky,1634290638.0,False
q8b9g3,Byzantine Generals problem. This is pretty much the basis of bitcoin/cryptocurrencies. An open ledger,hgol6xb,t3_q8b9g3,1634260355.0,False
q8b9g3,"A ledger that contains all completed transactions. This would include timestamps, amounts, as well as the payer and payee.",hgogrtc,t3_q8b9g3,1634258325.0,False
q8b9g3,"Is there any feasible way for this ledger to exist such that it does not need to be held by an external (i.e., not A, B, or C) party? How could A themself prove they had not given an IOY to B?",hgoh2v2,t1_hgogrtc,1634258466.0,True
q8b9g3,"Depends on the definition of third party. If by third party you mean some entity aside from A, B, or C, then I suppose each entity could have their own ledger. Within A’s ledger there should be a record of all transactions in and out of the vault.

If by third party you mean some entity other than C (the querying entity in the given example), I’m not sure they would be aware of transactions taking place unless there was some kind of publish-subscribe situation you could apply. Without querying A directly for the information about outstanding debts, that is.

Otherwise, if that third party can be some payment processor that holds the ledger for all transactions, that might be another option.",hgoia8z,t1_hgoh2v2,1634259019.0,False
q8b9g3,"I guess the question comes down to: how can C hold A accountable for what A tells C. If A says they did not give B an IOY, how could C know that is the truth?",hgokle7,t1_hgoia8z,1634260079.0,True
q8b9g3,"If you can’t trust information given by A, cannot contact B, and there is no publicly held ledger, I’m not sure there is a solution.",hgols8l,t1_hgokle7,1634260624.0,False
q8b9g3,"I think that is right. A public ledger solves the problem if A&C record the IOU on the ledger and C only finishes the handoff after the transaction is accepted in the ledger. Because at that point, C can check if there were any IOUs written by A between the recording of the $100 and the A&C transaction. That guarantees no entity B exists in the official record.

Perfect use case for distributed ledgers.",hgonf19,t1_hgols8l,1634261348.0,False
q8b9g3,Isn’t this just Bitcoin?,hgom63h,t3_q8b9g3,1634260797.0,False
q8b9g3,Was basically trying to ask if there is a way to solve this problem without using a public ledger --> which is what bitcoin uses.,hgp1oo4,t1_hgom63h,1634267920.0,True
q8b9g3,Ahhh I gotcha.,hgq8cwx,t1_hgp1oo4,1634298981.0,False
q8b9g3,That is the first time I've seen IOU spelt like that.,hgplaem,t3_q8b9g3,1634280044.0,False
q87ymv,"They already exist. Google has their TPU. Amazon has a chip called inferentia. A bunch of start ups have dedicated deep learning chips as well. It's a fast growing market, especially for cloud vendors.",hgnr9u3,t3_q87ymv,1634246574.0,False
q87ymv,"Do you think this split will hit the consumer market as well, eventually?",hgpdo05,t1_hgnr9u3,1634274536.0,True
q87ymv,"What do you mean, this has already happened, that's what people are telling you.

https://www.xilinx.com/products/boards-and-kits/alveo.html

You can buy and use any of these boards. The bits about data centers is targeted marketing for a core audience, not limitations.",hgqidlj,t1_hgpdo05,1634304398.0,False
q87ymv,"TPU.

Hardware made for specific algorithms are called: ASIC. aka Application Specific Integrated Circuits.",hgnyvq0,t3_q87ymv,1634250029.0,False
q87ymv,"I think you have your history of graphics wrong. Before GPUs we had video cards and chipsets that implemented eg VGA, it wasn’t exclusively all done on the CPU. Once upon a time FPUs were also discrete.",hgpux9m,t3_q87ymv,1634288485.0,False
q87ymv,ASICs and FPGAs are already being used for real time computer vision systems.,hgv2al9,t3_q87ymv,1634393970.0,False
q87ymv,"Apart from TPUs, which are primarily used for deep learning, you also have experimental neuromorphic chips which offer more bioplausible learning rules.",hgpij4s,t3_q87ymv,1634277911.0,False
q87ayn,It depends on what you are implementing. But as a rule of thumb I’d say 50/50.,hgoyvn8,t3_q87ayn,1634266567.0,False
q87ayn,"Do you mean the ratio of the two activities in relation to each other or relative to the entirety of a software engineering job? Because like most jobs, the majority of the time is spent on things like email and other mundanity",hgpimaf,t3_q87ayn,1634277976.0,False
q87ayn,Just the coding part of the business logic / interface / .. versus the coding part of the tests.,hgpjkk5,t1_hgpimaf,1634278709.0,True
q87ayn,"Compsci is not software development. You probably want something like /r/webdev/, but tbh software developers don't talk shop on Reddit, it's mostly students and self-taught hobbyists. You want hackernews or lobsters.",hgomw2y,t3_q87ayn,1634261113.0,False
q87ayn,"You both need to read the pinned thread in the subreddit.

1. New to the area should ask questions there
2. The first phrase says “new to programming or computer science?”. Then yes, this subreddit is also for programming. Also in the subreddit description says it is for “all things computer science”. And that includes software development.",hgoyl2x,t1_hgomw2y,1634266429.0,False
q87ayn,"I've read it, and I don't particularly care - the subreddit can say it's about anything it wants, what matters is who is here and what answers you're likely to get.",hgoznf3,t1_hgoyl2x,1634266934.0,False
q87ayn,"You were saying he was asking in the wrong place and that this subreddit was not related to his question, which is WRONG. His question was fine for this subreddit.
Each sub has its own rules, you don’t get to decide on what is proper or not based on your own taste. LOL",hgp4t1g,t1_hgoznf3,1634269517.0,False
q87ayn,"I mean you can write rules on a piece of paper and stick them on an empty shed, then shout your question into the shed, it's not going to get you the answers you're looking for.",hgp76p4,t1_hgp4t1g,1634270796.0,False
q87ayn,"One thing is give an advice for other places where more and better answers. Other is the to say it is not the proper place. 

I can see we’ll not agree. So I think it is no good to keep talking further here. 👋",hgp8i5p,t1_hgp76p4,1634271526.0,False
q87ayn,"I said compsci, not /r/computerscience. You just read more into it than was there...",hgp9bdn,t1_hgp8i5p,1634271987.0,False
q87ayn,It has not much to do with web development but understand the concern. I am glad to have any kind of scientific research as a form of research paper as well tbh.,hgp4qq7,t1_hgomw2y,1634269484.0,True
q87ayn,"/r/webdev is just the most active softdev community of any kind on Reddit where you're likely to find actual developers talking about developing. There's developers of all flavours on Reddit, but they tend to talk shop less on here and more in other places that aren't so completely overrun by students and people who don't know what they're doing.",hgp7cp7,t1_hgp4qq7,1634270888.0,False
q87ayn,"Here's a sneak peek of /r/webdev using the [top posts](https://np.reddit.com/r/webdev/top/?sort=top&t=year) of the year!

\#1: [The website I have been tasked with updating today...](https://i.redd.it/pumkw1ulx2l71.png) | [933 comments](https://np.reddit.com/r/webdev/comments/pggevn/the_website_i_have_been_tasked_with_updating_today/)  
\#2: [Wanted to share a coding task I was asked to do in an interview. This is how to handle this type of situation.](https://i.redd.it/evg91bkr09t61.png) | [527 comments](https://np.reddit.com/r/webdev/comments/mr5s0o/wanted_to_share_a_coding_task_i_was_asked_to_do/)  
\#3: [18 Cards of how to design web forms](https://np.reddit.com/gallery/nm6wcl) | [387 comments](https://np.reddit.com/r/webdev/comments/nm6wcl/18_cards_of_how_to_design_web_forms/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/)",hgp7dwr,t1_hgp7cp7,1634270907.0,False
q87ayn,Testing time is a lot like the time complexity of your function - you wanna get it as low as possible.,hgqn4tx,t3_q87ayn,1634306617.0,False
q805c4,"See also the paper on Arxiv ""Insidious Nonetheless: How Small Effects and Hierarchical Norms Create and Maintain Gender Disparities in Organizations"" https://arxiv.org/abs/2110.04196",hgm0e9s,t3_q805c4,1634219576.0,True
q7mriy,*Relative Community Sizes per Year,hgkzxle,t3_q7mriy,1634193235.0,False
q7dw91,"Super intro

https://towardsdatascience.com/a-very-brief-introduction-to-fuzzy-logic-and-fuzzy-systems-d68d14b3a3b8",hgi5kgf,t3_q7dw91,1634144440.0,False
q7dw91,"If you already have experience with other types of control (i.e. PID), here is a comparison for building temperature controls without too much math. 

&#x200B;

See section 5.2 - Static Fuzzy Logic Controller (FLC):  
""Since fuzzy control theory is somewhat new to those involved in building systems; it is appropriate to review some of the basic concepts. The reader interested in a more   
 comprehensive review of the subject will find (Lee, 1990a, b; Gouda, et al. 1997)  
helpful.""

&#x200B;

https://www.sciencedirect.com/science/article/pii/S1474667017369008/",hgiowkd,t3_q7dw91,1634152110.0,False
q7dw91,Which literature do you use in class?,hgj3w1t,t3_q7dw91,1634158204.0,False
q7dw91,"Lotfi Zadeh is the classic, and pretty easy to grok.",hgk02a5,t3_q7dw91,1634172743.0,False
q71mpj,"You'll have to be more specific, how do you define Moore's law?  The literal definition about transistors? Note that performance is not linear in the number of transistors.

What are some concrete examples of these ""counteractions""?  I think software has only gotten better, but maybe your everyday experience disagrees because of your specific use cases.",hgfwjq8,t3_q71mpj,1634095930.0,False
q71mpj,"Take Windows for example. Sure it has gained many features but it seems like a mess under the hood. At least, that's what I've heard. Maybe that's not accurate?",hgg8hi7,t1_hgfwjq8,1634102883.0,True
q71mpj,"I'd recommend taking an introductory class to operating systems.

Windows has it's fair share of issues, but I think on average it has only gotten better. Memory allocation, process scheduling, hardware utilization, these are all things that windows is plenty good at and only continues to improve upon with each iteration. Windows 11 is likely to take advantage of big-little CPU architectures like intels alder lake for example.

People love to complain about windows just like they love having something to complain about, and its a thing most people have in common. Though it is a fair statement that Mac operating systems are generally more stable, but thats because Apple has a full monopole on the machines running it. Unix based operating systems in general are great because Unix has been actively developed and contributed to for decades in the open source community.",hgga7n1,t1_hgg8hi7,1634104102.0,False
q71mpj,"I don't think software became worse, but definitely it hasn't been improved as much as hardware has been. Typically, multi thread hardware is now a standard, but applications using it at is best are rare, mostly because this is hard to do (vicious, hard to detect bugs).

Also, software is multi layered : a program relies on a programming langage and libraries, which themselves rely on the OS and the assembler. Each of these have to be upgraded to improve performance. One well-known issue is backward compatibility, for instance in Intel processors (then support modern instructions set as well as 30 years old ones, and everything in between). This mean that some efficiency is traded for the ability to run old programs (which slowly disappear anyway).

Finally there are readability and security. Even if compilers help to transform a readable program into an efficient one, they are not perfect. And some safety measures induce a loss in performances (one low level but somewhat recent example are the Meltdown and Spectre vulnerabilities)

I could also add that instead of doing old things faster, we try to do new, more complex things at a reasonable speed thanks to the improvements of hardware (typically : AI)",hgga4dw,t3_q71mpj,1634104035.0,False
q71mpj,"Absolutely! Almost all consumer grade computation is hindered by the serial execution model used by most hardware. Even our strongest computers running our best parallelized algorithms are running on a hardware based simulation of parallelization that is emulating the design choices of a computer architecture from 1970. We know how to construct stream processing based computation hardware but the computing environment is difficult to use. It is unlikely that architectures used for general use computation changes any time soon.

For further reading on the subject consider the publication `David Chisnall, ""C Is Not a Low-level Language. Your computer is not a fast PDP-11."", ACM Queue, Volume 16, issue 2.`

Also, Windows is not poo. In depth, it's fairly remarkable: I doubt you can do better. A lot of great minds went into Windows.",hggtstx,t3_q71mpj,1634121006.0,False
q71mpj,">Also, Windows is not poo. In depth, it's fairly remarkable: I doubt you can do better. A lot of great minds went into Windows.

OP probably couldn't do better, but in terms of performance and UX the MS could do way better (better than [this](https://imgur.com/a/h5pU2Wx))",hgifsj5,t1_hggtstx,1634148425.0,False
q71mpj,"Oh I definitely could not do better. I do however see UX nonsense like opening a page or an app and things shift around multiple times in the first second or two. It's like playing whack-a-mole with the button I want to click.

Most apps I'm fighting to get it to do what I want. Only a handful don't make me want to avoid the app.

Again, I'm not trying to blame devs for any of this. My hunch is that this is primarily a systems and incentives issue, not a talent issue or something. Like great apps aren't inherently difficult to build if you give devs what they need. So I'm trying to get to better understand what's going on. Or is my understanding way off in some way?",hgnmh7u,t1_hgifsj5,1634243756.0,True
q71mpj,everything is web based which limits everything. not to mention people dumping their apdfgnpaiosdfpisdjfpsdpoasdfjpisd tech stack to slow everything down.,hgnatxf,t3_q71mpj,1634238960.0,False
q71mpj,"This seems to be at least part of what I was getting at. Mind you, I'm not trying to blame any devs. I think most devs would make far better, innovative code if they had the opportunity to.

Would you elaborate on this problem and/or solutions?",hgnkeq5,t1_hgnatxf,1634242902.0,True
q71mpj,"The problem is that companies don't see these things as critical. For a company, if they can guarantee 100 people can work on features simultaneously and the result is ""fast enough"", then there's not really a problem in their eyes. Some companies really do care about software speed but they are not selling software to normal people. 

The only way to get high speed software for ordinary things is esoteric old open-source software (programs like vim and emacs open lightning fast compared to vscode). Unfortunately, open software is, in a word, pathetic. The speed of this software comes at the cost of reliability, support, and feature range. Linux has less overhead and so software runs faster on it, but if you can't use basic programs like photoshop then the performance gains mean nothing.

As for solutions go, I think there are a couple options. Things like electron pop up because they are easy to use for web devs. There's no reason that an alternative with reasonable speed wouldn't catch on, it's just difficult to make. I think these solutions will come eventually but for right now we are in a weird transition period.",hgpqoe1,t1_hgnkeq5,1634284616.0,False
q71moh,Coding is like anything else in life.  You grow through failure.  There is nothing wrong with asking for help.  Too many people assume that is a weakness.   The brightest software engineers I know ask way more questions than they provide answers.,hgfqiy9,t3_q71moh,1634092850.0,False
q71moh,"Sure, I can learn how to build a house, it’s just that I’m not the best of handymen.",hgj16lo,t3_q71moh,1634157078.0,False
q71moh,"Coding doesn’t come natural to anyone it’s actually the opposite of everything we do in our daily lives. Example, as humans we don’t think about the little details we automate it and move on. When was the last time you thought about washing your hand after toilet, or drinking when you’re thirsty. You just do it because we automated it. But, in coding it’s the opposite of automation you have to make every single decision consciously. And it’s hard, because it’s weird and that is Ok, it’s hard and weird to everyone and the best thing people do is accept for what it is and try to do their best. I would highly encourage watching the unedited video screens of professional developers from twitch to YouTube and see how many times they stumble over the smallest things. In your mind you have an image of a software engineering who exactly know what he/she is doing and fluently converts that thought into a code, but that is not reality. Again WATCH twitch and YouTube video of live coding and see how they handle mishaps.",hgfrr90,t3_q71moh,1634093459.0,False
q71moh,"I think what you're saying isn't universally applicable.

I think coding actually does come naturally to me because it's so much like something I do a lot in my life. I love playing board games. In the end, board game rules are basically a way to program humans, and I went through a lot of these.

My programming courses are therefore basically free (at least at the moment). When it comes to math though I struggle like the worst of them, so it all balances out.",hgz9n5s,t1_hgfrr90,1634475378.0,False
q71moh,"No, despite what people desire to believe everyone is not able become a software engineer. However, there are a great deal of people who could, who do not, regardless of any innate ability or inclination. If you love to read programs like you say, it sounds like you have the inclination. Regarding seeking assistance: That's a smart thing to do.

At the end of the day only you can determine if software development is ""for you"".

I recommend you spend time writing software that you personally will use, in whatever language you prefer to do it in. Keep reading software written by others. There is no substitute for reading and writing computer software.

If there are specific road blocks you run in to, seek assistance with them.",hghaka7,t3_q71moh,1634131410.0,False
q71moh,everyone is capable of it yes. but when you learn anything the largest factor in how well you learn is how much you like it. not everyone likes to code and so learning is exponentially harder.,hggfxki,t3_q71moh,1634108610.0,False
q71moh,I used to think so.,hggffzx,t3_q71moh,1634108201.0,False
q71moh,"No body codes “well” even the biggest companies with the most brilliant engineers make dumb mistakes, and everyone constantly find better ways to do something even after the software is released, that is why we have all the ongoing updates for any software. 

I have worked on code, and after writing 100 lines for 3 days  I realize that I could have replaced it all with 3 lines. 

The question is, can you write something workable, and do you have the presence of mind to go back and fix mistakes.


Contrary to popular belief, you don’t have to be the “best” in anything, May be you aren’t a brilliant or talented developer, but with enough trying I think anyone can become a passable one.",hghrh39,t3_q71moh,1634138713.0,False
q71moh,Anyone can do it. However not everyone can build something.,hgj7k5z,t3_q71moh,1634159739.0,False
q71moh,"Struggle is fine. Everyone struggles in something when studying. Get some time to learn on your own and you will see that confidence comes with it. 

Aside from that, Comp. Sci. is a heavy study area so sleep and eat well or that already heavy and hard area will become even harder with a stressed head.",hgjefoq,t3_q71moh,1634162699.0,False
q71moh,A good resource for learning to code is definitely w3schools. It taught me the basics very fast and it has an easy to understand layout. I would highly recommend it,hgmmu1k,t3_q71moh,1634229033.0,False
q71moh,"No one is born a programmer. Sure, logic might come easier to some, but every good programmer learns from being a bad programmer.",hgv751f,t3_q71moh,1634396328.0,False
q71moh,"I don't think so.

I know people who couldn't become bad programmers even if they tried, much less good ones.

But if you're actually interested in becoming one, your chances already went up by a lot.

You say you kept bugging the person next to you to explain stuff to you. I think that's the best, single most important attribute any programmer could have and I wish I had more of it.",hgz91sf,t3_q71moh,1634475047.0,False
q6so82,"I would have to study this algorithm in more detail, but from what I can see, if you enter case 2, you also modify z (z=z.p). Therefore, the ""case 3"" lines behave differently (now operating on a different z node) than in the case when you do not enter case 2 before.

Could that be a clue?",hgfd1b7,t3_q6so82,1634086573.0,False
q6lq9i,What do you mean? We use them in our engineering work. We model external stresses using Calculus. Some languages have these functions built in but one can always write their own.,hgd4u49,t3_q6lq9i,1634052601.0,False
q6lq9i,"Linear algebra was essential when I studied artificial neural networks/machine learning and evolutionary systems. Would be impossible to understand research papers or write my own ideas down otherwise. 

Not used it since and have been working as a security specialist and developer for several years now. Have to deal with discrete mathematics in software engineering/development but that's about it.",hge39bt,t3_q6lq9i,1634066712.0,False
q6lq9i,Thanks for thorough reply! I’ve just started my degree in CS and Discrete Structures has got me good ;_;,hge5c1e,t1_hge39bt,1634067544.0,True
q6lq9i,"It depends on what you mean by ""on-the-job"" software is a big field and there are many different types of programs. If your making a website you probably won't need much math. If your developing a physics engine you will need calculus and linear algebra. So the amount of math you need depends on what you want to do. In my opinion the more interesting positions require a deeper understanding of math.",hge4d5h,t3_q6lq9i,1634067153.0,False
q6lq9i,I meant to ask for SDE roles specifically; but I do realise SDE requirements can vary a lot too,hge71kf,t1_hge4d5h,1634068230.0,True
q6lq9i,Computer graphics is heavily reliant on linear algebra,hgevwwy,t3_q6lq9i,1634078632.0,False
q6gpe2,"He wasn't the sole creator of C and Unix, but his contributions to computer science are immense. A lot of the computing we do today relies on that work. RIP Dennis Riche, I will never free() you from memory.",hgbwntv,t3_q6gpe2,1634025044.0,False
q6gpe2,"I’m not crying, it’s just a memory leak!",hgcnfv9,t1_hgbwntv,1634044850.0,False
q6gpe2,[deleted],hgbyugc,t1_hgbwntv,1634027095.0,False
q6gpe2,What kind of language are you writing in bro,hgc1f0x,t1_hgbyugc,1634029501.0,False
q6gpe2,"A pseudocode based in JS.

A **really shitty one** at that.",hgc2pok,t1_hgc1f0x,1634030705.0,False
q6gpe2,You could have at least written in C.,hgexkdg,t1_hgbyugc,1634079374.0,False
q6gpe2,Yuck 🤢,hgdybry,t1_hgbyugc,1634064695.0,False
q6gpe2,what other things did he do?,hhs3bg0,t1_hgbwntv,1635020944.0,False
q6gpe2,C,hgc8rm9,t3_q6gpe2,1634035901.0,False
q6gpe2,C,hgcdq3e,t1_hgc8rm9,1634039282.0,False
q6gpe2,C,hgcjul6,t1_hgcdq3e,1634042969.0,False
q6gpe2,C,hgfez68,t1_hgcjul6,1634087457.0,False
q6gpe2,Si,hgcoyj4,t1_hgcjul6,1634045599.0,False
q6gpe2,UNIX,hgcqb88,t1_hgcoyj4,1634046253.0,False
q6gpe2,C,hgmmi09,t1_hgcdq3e,1634228899.0,False
q6gpe2,You in information heaven,hgfliu6,t1_hgc8rm9,1634090470.0,False
q6gpe2,"malloc some memory for him. Don't free it though, let it leak",hgcenqq,t3_q6gpe2,1634039884.0,False
q6gpe2,Maybe I'll just realloc some other memory,hggdif9,t1_hgcenqq,1634106650.0,False
q6gpe2,this brings to mind this video from Bell Labs https://youtu.be/tc4ROCJYbm0,hgcxjnz,t3_q6gpe2,1634049541.0,False
q6gpe2,"This man in the intro has no business being so suave in this video.

His low voice, the turtleneck, foot up and hunched over. YouTubers should take note.",hgd0kpj,t1_hgcxjnz,1634050845.0,False
q6gpe2,"they are all chill, listen to Kevin Thompson; it was before the era of anxiety inducing  management practices",hgd5jro,t1_hgd0kpj,1634052887.0,False
q6gpe2,RIP,hgc0q9n,t3_q6gpe2,1634028861.0,False
q6gpe2,Legend never dies. He lives in our hearts,hgcvd7l,t3_q6gpe2,1634048590.0,False
q6gpe2,"""10 years ago today,....""",hgdt92x,t3_q6gpe2,1634062638.0,False
q6gpe2,F,hgbwvcr,t3_q6gpe2,1634025239.0,False
q6gpe2,"No, it was C.",hgc7dhh,t1_hgbwvcr,1634034832.0,False
q6gpe2,take my upvote and leave,hgcct06,t1_hgc7dhh,1634038679.0,False
q6gpe2,He is paying respecc,hgcqjka,t1_hgc7dhh,1634046365.0,False
q6gpe2,"Dennis, you're a legend in computer field and we'll never forget you",hgdffa7,t3_q6gpe2,1634056926.0,False
q6gpe2,C,hgekbkv,t3_q6gpe2,1634073657.0,False
q6gpe2,C,hgff1qd,t1_hgekbkv,1634087489.0,False
q6gpe2,He may be gone but his legacy will never die,hge3cpz,t3_q6gpe2,1634066748.0,False
q6gpe2,That man literally paved the way for everyone else who would go on to pave the way. Legend.,hgeiasc,t3_q6gpe2,1634072818.0,False
q6gpe2,"Just came to say, fuck Jobs. Thats it",hgflkyw,t3_q6gpe2,1634090497.0,False
q6gpe2,rest in peace,hgd0pco,t3_q6gpe2,1634050901.0,False
q6gpe2,Simplicity and elegance both fit to he language and the person.,hgdfj5p,t3_q6gpe2,1634056968.0,False
q6gpe2,Thank you good man.,hgewen1,t3_q6gpe2,1634078847.0,False
q6gpe2,C inspired many programming languages. Thank you.,hgg0w6u,t3_q6gpe2,1634098296.0,False
q6gpe2,You should see the goodbye world script,hgd9iwo,t3_q6gpe2,1634054503.0,False
q6gpe2,Sir thank you for Java,hgcf6c6,t3_q6gpe2,1634040213.0,False
q6gpe2,Thank you for saying thank you,hgdyg90,t1_hgcf6c6,1634064746.0,False
q6gpe2,C,hggag5c,t3_q6gpe2,1634104276.0,False
q6gpe2,Who is dat,hgcqg7l,t3_q6gpe2,1634046321.0,False
q6gpe2,The man whose feet you should prostrate yourself before if you hope to be anything in the field of computer science.,hgff53a,t1_hgcqg7l,1634087532.0,False
q6gpe2,You can just say “Ten years ago”,hgcs1lq,t3_q6gpe2,1634047070.0,False
q6gpe2,"""Ten years ago today""",hgey3qc,t3_q6gpe2,1634079622.0,False
q6g6ho,"since E is constant: yes.

If not: Do you assume we already ""ran"" the algorithm on the previous graph and now only need to update the already existing state?",hgde0xb,t3_q6g6ho,1634056355.0,False
q6g6ho,"Yes. Before, increasing the capacity of the edges, we had a maximum flow function.",hgdf3ta,t1_hgde0xb,1634056796.0,True
q6ejfh,"When dealing with caches you can optimize the data structure as much as you want to get O(1) and ensure data is in cache in 99% of the cases at one point the bottleneck will be memory speed.

To deal with that you need to use latency hiding techniques, make sure to read the papers and watch the talks on nano-coroutines and ""killer nanoseconds"" to do sub-1ns fetches:

- CppCon 2018
  - NanoCoroutines: sub-nanosecond cost to hide cache latencies in databases
    - https://www.youtube.com/watch?v=j9tlJAqMV7U
    - https://github.com/GorNishanov/await/tree/master/2018_CppCon
 
- CoroBase: https://arxiv.org/pdf/2010.15981.pdf
- Interleaving with Coroutines: A Practical Approach for Robust Index Joins\
  Very Large DataBase conference
  - http://www.vldb.org/pvldb/vol11/p230-psaropoulos.pdf
  - https://infoscience.epfl.ch/record/231318
- Exploiting Coroutines to Attack the “Killer Nanoseconds”\
  http://www.vldb.org/pvldb/vol11/p1702-jonathan.pdf
  > A key requirement for efficient “interleaving” is that a context-switch must take less time than a memory stall.
  > Otherwise, switching contexts adds more overhead than originally imposed by thememory stalls.
  > This requirement renders many existing multi-threading techniques useless,
  > including light-weight, user-mode threads, known as fibers or stackful coroutine
- Bridging the Latency Gap between NVM and DRAM for Latency-bound Operations\
  https://www.semanticscholar.org/paper/Bridging-the-Latency-Gap-between-NVM-and-DRAM-for-Psaropoulos-Oukid/1b3e3dd80c1ae2c02c6a2745e941d8cccb75f6c1",hgbq5mt,t3_q6ejfh,1634019566.0,False
q6ejfh,"Thanks, that's plenty of material.",hgbsgto,t1_hgbq5mt,1634021412.0,True
q6ejfh,Yes.  There are many optimal caching strategies.  In general the problem is np hard however,hgbqvtn,t3_q6ejfh,1634020144.0,False
q6ejfh,Any pointers to papers ?,hgbsedj,t1_hgbqvtn,1634021359.0,True
q6ejfh,"Belady's algorithm (evict farthest first) is optimal for fixed sized pages, and is a greedy alogrithm. The proof will be in any standard work on algorithms.  Kleinberg and Tardos discuss it in chapter 4 of their textbook for example",hgd0rbs,t1_hgbsedj,1634050924.0,False
q61r8g,"It's very common. Get what knowledge you can out of it for your future prospects and try not to let it affect your mental health.

I'm in my 2nd year of a computer engineering senior project class and... Let's just say my first year was not fun.  Once 2nd semester began, I verbalized my feelings about the last semester and basically said that of it happens this semester I'll be going to the professor or higher if need be. Attitudes changed and although the team doesn't communicate, everyone is at least paying their part now.

Also, in my opinion, having a student be a""project lead"" makes no sense and if anything is lazy of the professor. If the prof was our team lead, everyone would do their work and communication would flow naturally.",hga0ynq,t3_q61r8g,1633988244.0,False
q61r8g,"Hate to say it, but this is training you for the real world.",hgakita,t3_q61r8g,1633997748.0,False
q61r8g,"Go and talk to the professor about this problem. As an undergrad student (soon to be grad) this is on the poor side of groups, but not shocking. If you legitimately tried to do work as a group (and it sounds like you have) and they’ve ignored you, you have to talk with the TA or prof. The earlier the better because it gives them more time to fix the problem.",hg9i7j6,t3_q61r8g,1633979847.0,False
q61r8g,"I have a hard time believing this is just on the poor side and not shocking. I've never been part of a team project that is this hard to work with. My expectations were definitely high coming from a few years of working on operations at a tech company, but I haven't been this stressed in a while. Thanks for the tip, I'll see if the prof or TA has any advice",hg9p2zu,t1_hg9i7j6,1633982840.0,True
q61r8g,"> I have a hard time believing this is just on the poor side and not shocking.

This genuinely happens a lot.

From your work experience you know that communication within a team is essential for getting the job done. In a school environment this feedback loop doesn't exist, and some students believe that pulling all the work towards themselves and shutting out the rest of the group is the best way to a good grade (since obviously their design is perfect and nobody else could understand their genius).

You've already had this lesson, they have not, and they might not get it until they enter the work force.",hga2sd5,t1_hg9p2zu,1633989096.0,False
q61r8g,"I'm not joking when I say that your experience sounds like every single group project I've ever been a part of.


As for advice, I second the suggestion to go to the professor. If you have records of you reaching out and being ignored, then your professor should take that into consideration.",hgar1ra,t1_hg9p2zu,1634000843.0,False
q61r8g,"Great representation of what will happen in the real world if you don't have people with good leadership skills in the team. Take it as a valuable lesson. Usually you would bring this issue up in a team retrospective (hopefully in a scrum-based or otherwise agile work environment) and you would have a good and invested team lead or scrum master that would moderate the discussion and help the team to define action items to improve on the points made.

In a University project group: no clue.",hgfdtvk,t3_q61r8g,1634086936.0,False
q61r8g,"Welcome to university software development group projects, I did 80% of the work for my 6 person group project and we all had to share the credit equally, that's life.

Talk to your prof but it might also be that you just need to knuckle down with the other person who is willing to work and get it done.",hg9m9am,t3_q61r8g,1633981607.0,False
q61r8g,"Here I was thinking that people go to uni for software development at least enjoy it. Honestly I'd prefer if I had to do most of the work because then I'd actually learn a lot, but the lack of communication is killing me. I think my only options are to talk to the prof, and basically just continue this one sided communication and claim as much work as possible. Thanks for replying, it's helpful to know that this is probably the norm",hg9q0k2,t1_hg9m9am,1633983255.0,True
q61r8g,"Tbh I don't understand group coding assignments. Unless everyone is there when the first line of code is typed, it's pretty much guaranteed that one person will end up doing at least 80% of the code because they're the ones who understand how everything works. Also, everyone has their own programming style, so even if you know what the code is doing, it will never make 100% sense since the program is based on other person's abstract thinking",hg9slr1,t3_q61r8g,1633984404.0,False
q61r8g,"Sounds like not everyone on your team is being a team player unfortunately.  Have a chat with the professor about it and see what they suggest.  When it comes to grading your teammates, is it really all about code?  Or are there other metrics such as communication, planning, etc?",hg9vubw,t3_q61r8g,1633985854.0,False
q61r8g,"I hate working in a team, I always feel like everyone is slacking off and builds pressure on me until I'm obliged to just do everything 

I thought it was going to change later as people would get ""more mature"" on the degree but nope it's still the same on my fina gradl investigation

That's not always the case but it's pretty common for me and I must say I really really really hate working on any group project for this reason",hgbxqj6,t3_q61r8g,1634026043.0,False
q61kx5,"Imperative programming languages give the computer a series of instructions, and the computer does those instructions, and then you get an answer.  In declarative languages, you provide a definition that specifies the solution, and the compiler then uses an algorithm to convert that into code that the computer can run.",hg9djcr,t3_q61kx5,1633977859.0,False
q61kx5,"Good [intro to the topic](https://ui.dev/imperative-vs-declarative-programming/) to start with.

One thing to keep in mind is that the languages themselves are not limited to be one or the other. Some languages may favor one style, but almost always these styles (""paradigms"") exist at the same time and complement each other.

They exist at the same time, for example you might have a function `calculateTax` where inside the function you are *imperatively* coding each step that calculates the tax. Elsewhere in the codebase, when you *call* that function, it's declarative because you're just saying ""calculate the tax"" without describing the steps exactly.

**Somebody** has to write the imperative code so that you can use declarative code. Another example is regular expressions, which is very declarative in its usage, but somebody implemented the engine that powers the regex in imperative code.

Finally, just a restating that it's not one or the other, often it's mixed together or an application will have low-level imperative routines called by higher-level declarative functions.",hg9fxus,t3_q61kx5,1633978871.0,False
q61kx5,"Type deduction is basically a small declarative subsystem, and depending on how sneaky you are you could do some real work with it. Honorary mention to [typing the technical interview](https://aphyr.com/posts/342-typing-the-technical-interview) in which Aphyr hacks the Haskell type system into solving nqueens.",hg9u79e,t1_hg9fxus,1633985119.0,False
q61kx5,"I am so dumb to understand this 

edit: this website's code",hga9dkb,t1_hg9u79e,1633992274.0,False
q61kx5,"It's designed to barely be understood, even by people who are perfectly proficient Haskell programmers. It's more a clever demo than anything anyone actually needs to be able to do. It happens to be the most detailed demo I know of computation clearly happening during type deduction.",hgaa1tq,t1_hga9dkb,1633992605.0,False
q61kx5,"Imperative says *how* to do something and declarative says *what* to do.

An example of an imperative language would be C where you declare variables, assign values to them, use if statements and loops to alter the flow of control to do whatever it is you're trying to do.  

An example of a declarative language would be SQL where you don't really know how it's going to be getting records and filtering based on predicates and so on, you just say what it should do, get all records from this table where they satisfy some condition.",hg9oubs,t3_q61kx5,1633982737.0,False
q61kx5,Your examples made the concept click for me. Thanks!,hgbhr24,t1_hg9oubs,1634013692.0,False
q61kx5,"*Declarative programs tell the computer what's supposed to happen*, but not how to do it.

*Imperative programs tell the computer the steps to make something happen*, and (sorta) the order to do them in.",hg9zcr3,t3_q61kx5,1633987485.0,False
q61kx5,"In imperative programming you have detail and specify all the steps required to compute a result from a specific input.

In declarative programming, you specify the inference rules / data transitions, alongside with some base facts / initial conditions, known about that domain/problem. Then when you query the system it will process those inference rules and facts to compute all the logical rule steps required to satisfy said query.",hg9vd3x,t3_q61kx5,1633985637.0,False
q61kx5,"Think of it like driving. 
Imperative: you're a navigator giving your computer step by step driving directions.
Declarative: you give your computer a very detailed map for it to drive itself.",hga27y0,t3_q61kx5,1633988833.0,False
q61kx5,"Biggest practical difference is ""mutation"". In declarative paradigm, you avoid mutation (e.g. config, pipeline). In imperative, you either mutate an object (OOP) or mutate the world (procedural).",hgb5uc0,t3_q61kx5,1634007573.0,False
q61kx5,So in an imperative programming language you mutate and object to get your code to do something without having to declare a bunch of stuff?,hgbuuzv,t1_hgb5uc0,1634023432.0,True
q61kx5,"Yes, that's the difference of ""imperative code"" vs ""declarative code"". 

Most major modern languages can do both at variant degree, so IMO strictly classifying them is contrived, but many have their roots in one or the other.",hgd9g5a,t1_hgbuuzv,1634054470.0,False
q61kx5,"I never really got this distinction, or why it is useful. Even in “imperative contexts” (assigning variables, writing control flow in C), we can still say this is declarative because we didn’t tell the computer *how* to multiply two numbers, for example, so can’t we also call this declarative? It always seemed like an arbitrary, sort of useless distinction to me.",hgbq058,t3_q61kx5,1634019448.0,False
q5y2n6,"Well, neuroscience is against your opinion.",hg8m6w7,t3_q5y2n6,1633966539.0,False
q5y2n6,"Cap. IQ is genetic and you need at least average IQ to be a comp sci major.

Also you need to have analytical/logical thinking skills. I  have a more creative brain and couldnt understand all the logic no matter how much I tried.

Im not saying it would have been impossible for me. I'm saying it would have taken hours upon hours of learning, private tutors, retaking classes, struggling to find a job, and then struggling in my job.

Not all majors are for everyone.",hgafaru,t1_hg8m6w7,1633995186.0,True
q5y2n6,"That creative brain thing is pseudoscience.
Otherwise you're right, getting a major is indeed hard.",hgan0sn,t1_hgafaru,1633998943.0,False
q5y2n6,"You dont think that some people are creative thinkers and some are logical/analytical thinkers?

I know it's not left brain right brain black and white shit. But if you put say a comedian in front of a computer he probably wouldnt do as well as an accountant.",hgaxdtj,t1_hgan0sn,1634003742.0,True
q5y2n6,yes because the accountant learned analytical thinking. an actual comparison is whether someone with no training at all would be more likely to be good at both coding and accounting instead of coding and comedy.,hgkz2jo,t1_hgaxdtj,1634192547.0,False
q5y2n6,Coding and computer science are not the same thing.,hg8pdwh,t3_q5y2n6,1633967865.0,False
q5y2n6,I think you shouldn't compare your self to others that much. Who cares if my class mates are smarter than me. And what is that idea that you have to born to do something? that is a really cheap elitism.,hg8zppt,t3_q5y2n6,1633972134.0,False
q5y2n6,"I agree, I struggled hard in my beginners python class in college but steady work in it and pushing myself harder then others bc I needed it paid off when I went from the least knowledgable one to the one people came to for questions. Now I’m repeating that cycle in c language but I don’t think you should ever doubt your dreams.",hg9dz2h,t1_hg8zppt,1633978043.0,False
q5y2n6,"I agree with u mate 
""If u born poor is not your fault but if u die poor is your fault""
the family or genetics doesn't matter
what matters is your effort
if you really want something fight for it",hgd8s70,t1_hg8zppt,1634054199.0,False
q5y2n6,This honestly sounds like someone that just gave up. Effort and discipline goes a long way.,hhoxlr2,t1_hg8zppt,1634951740.0,False
q5y2n6,Why did you post this,hg9g4kv,t3_q5y2n6,1633978952.0,False
q5y2n6,Because I was thinking about my past and how much time/money I wasted,hg9uy02,t1_hg9g4kv,1633985446.0,True
q5y2n6,"This is the most fixed-mindset post I think I've ever read. Sure, being realistic is important, but giving up on something you're interested in because other people are smarter than you is misguided and terrible advice.",hg9z4ve,t1_hg9uy02,1633987386.0,False
q5y2n6,"I was kind of interested in coding. I didnt quit because of a few prodigies, I quit because 90% of students were better than me and I couldnt even complete HW assignments.",hgaefsl,t1_hg9z4ve,1633994758.0,True
q5y2n6,"The reason I say this is terrible advice is because, especially if you are that much of an outlier, a newbie who might be having a hard time could read this and think they're not cut out for it, which would be awful. Sorry if you had a bad experience, but there's no need to project that onto others.",hgaet19,t1_hgaefsl,1633994941.0,False
q5y2n6,Well how long do you encourage someone until you tell them to move on? Too much wishful thinking is actually harmful. Im just saying not everyone who likes computers is cut out for Comp sci and definitely not everyone can do it.,hgaxr4l,t1_hgaet19,1634003907.0,True
q5y2n6,"But you don't see me posting on med student subs about how not everybody is cut out for medicine... If you don't enjoy it, that's okay, doesn't mean you have to issue a take that could potentially be harmful to others",hgay26p,t1_hgaxr4l,1634004048.0,False
q5y2n6,Try again in a few years and you will probably be better at solving abstract problems. Good luck with yourself,hg9wkj1,t3_q5y2n6,1633986189.0,False
q5y2n6,bro you have a loser mindset thats the reason you failed,hhbvct8,t3_q5y2n6,1634708061.0,False
q5y2n6,I think you gave up and are convinced your failure is an innate feature of yourself to make you feel better. Honestly a BS in CS should be a pretty easy degree.,hg92xrh,t3_q5y2n6,1633973448.0,False
q5y2n6,"I couldn't even pass my beginners Python class. I did well until after loops and then everything was moving too fast and I had to pay indians to do my coding assignments.

Everyone else codes the shit in an hour while it would take me days.

So yeah I'm either overall stupid (which I dont think is the case) or im stupid with this subject.

I know a lot of people who could never code.",hg95bds,t1_hg92xrh,1633974398.0,True
q5y2n6,"But you can be ignorant about a lot of things, that doesn't mean that you can never learn, obviously it will be hard but that happens with everything at first",hg95ndd,t1_hg95bds,1633974531.0,False
q5y2n6,"Yeah paying for assignments won't help you learn better to just fail and retake then actually parse details. Find your learning style and apply it here like any other subject. Read the text book material, write notes, go to every lecture, test example code, and talk with professors or TAs between classes.

Paying to take classes, then paying for the assignments, not sure you want to pay for the job.

Also big secret... Professors almost always know when someone else did your code and that you maybe changed variable names and assignment order. At my school this also gets you a big 0 which would bite.

Just saying this whole post and your comments read of self-defeat as others have mentioned. Until you sit and chew through these roadblocks you will be sitting in this rut.

Finally, analysis applies to every major of study be it actors analyzing scripts and characters or a english major analyzing rhetoric and the sort. You're right OP everyone does things uniquely but we can find our processes and improve them.",hgbkh2y,t1_hg95bds,1634015424.0,False
q5y2n6,"Do wtv you want to do, who cares.",hg8ndo9,t3_q5y2n6,1633967034.0,False
q5ryuw,"I'm not entirely sure what you're asking, but I suspect the 'dog' in your example is referring to a [Salt value](https://en.wikipedia.org/wiki/Salt_(cryptography)) in Cryptography, which is used to defend against [Rainbow Tables](https://en.wikipedia.org/wiki/Rainbow_table)",hg7lbz3,t3_q5ryuw,1633944962.0,False
q5ryuw,thank you so much!!,hgefprl,t1_hg7lbz3,1634071762.0,True
q5ryuw,"MD5 takes in a variable length series of bytes and produces a fixed length hash. The output is constant for a constant input. 

If you are thinking about salting a password, you could accept a password, pull a random animal out of the dictionary, concatenation those into a string “passworddog”, get the MD5 of that (call it ABC123) and then store both the hash result and the salt (animal) you used. 

User=OkWar;HashPass=ABC123,dog

To verify a user, you take their offered password, append their salt (“dog”), run it through MD5, and see if it matches ABC123.

That’s how Unix systems used to work. Hell they stored the HashPass in a publicly readable text file, almost daring us to crack them.",hg7nwpo,t3_q5ryuw,1633947220.0,False
q5ryuw,thank you!! yeah ahahah I'm learning about Unix now,hgefveg,t1_hg7nwpo,1634071824.0,True
q5ryuw,"When it comes to password hashing, we don't want to use MD5, for a variety of reasons; primarily, MD5 is not designed for passwords.


An algorithm that is designed for passwords, however, is [bcrypt](https://en.m.wikipedia.org/wiki/Bcrypt). And the neat thing about bcrypt is that it is designed with a salt in mind, which is what you're asking about. 


But first, let's talk about a few properties of hash functions, as they relate to this discussion:
1. Hash functions result in an output of a fixed size/format
2. Hash functions are deterministic, which means that every input will always result in a certain, unchanging output.


A [salt](https://en.m.wikipedia.org/wiki/Salt_(cryptography)) is random data that is input along with a password into a hash function. The need for this becomes clear when you think about it: if hash functions are deterministic, then two people with the same password will have the same hash! And this is what salts do - by adding in random data to the password before hashing it, even the same passwords will have different outputs.


So whenever you store a user's password, you generate a random salt and hash it with `bcrypt( salt, password )`. The output of bcrypt will include the value of the salt, unhashed, so you can then use that same salt when checking the password that the user enters when attempting to log in - and if both the salt and the password match, because hash functions are deterministic, you will get a matching output.


To summarize:
- MD5 should never be used for passwords
- Each password is given a random salt before being hashed, which means different users with the same passwords will have different hashes
- That salt is also stored unhashed in the database for future authentication attempts


Side note: You can also have something called a [pepper](https://en.m.wikipedia.org/wiki/Pepper_(cryptography)), which is closer to the example you gave - it's a secret but consistent value that is mixed in with both the password and the salt, though typically a sufficiently random salt and properly configured bcrypt is secure enough.",hga8bqz,t3_q5ryuw,1633991761.0,False
q5ryuw,"thank you so so much. I have another question. what if the attacker gets hold of the file that stores all the passwords? if the passwords are hashed, but only md5 is used, the attacker can easily find the password. but even there, if we are using salt, will the attacker not be able to read what the salt is since it is stored in the database as well? Do you mean that the pro here is that at least each user has a different salt so the same password with different salt will not give the same result? What Is the best way to store passwords in a database so that even if the attacker gets hold of the database they will not be able to crack the passwords?",hgefni4,t1_hga8bqz,1634071737.0,True
q5qcer,"Yeah, the complexity analysis in that article is pretty silly. You can't arbitrarily resize a sudoku board, sudoku is pretty much only defined for 9x9 grids. If N was greater than 9, every puzzle would be impossible, because they hardcode only 9 available digits. With their code, you could change N to be 6 or 3, but that's not really sudoku anymore.",hg7luog,t3_q5qcer,1633945434.0,False
q5qcer,Is the general Sudoku (n * n) time complexity unknown?,hg7oubx,t1_hg7luog,1633948024.0,True
q5qcer,"Yes, it is unknown.

According to [this Wikipedia article](https://en.wikipedia.org/wiki/Mathematics_of_Sudoku), the problem is NP-complete:

> The general problem of solving Sudoku puzzles on n^(2)×n^(2) grids of n×n blocks is known to be NP-complete.

We don't know the optimal time complexity of NP-complete problems. If we did, we would have proven or disproven P = NP.",hg7rlxn,t1_hg7oubx,1633950298.0,False
q5qcer,"**[Mathematics of Sudoku](https://en.wikipedia.org/wiki/Mathematics_of_Sudoku)** 
 
 >Sudoku puzzles can be studied mathematically to answer questions such as ""How many filled Sudoku grids are there""? , ""What is the minimal number of clues in a valid puzzle""? and ""In what ways can Sudoku grids be symmetric""? through the use of combinatorics and group theory.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hg7rmxw,t1_hg7rlxn,1633950318.0,False
q5qcer,Doesn't the article say that it still has a super polynomial time?,hg8501q,t1_hg7rlxn,1633958711.0,True
q5qcer,"Where does it say that? The best known algorithm might be super polynomial, but we certainly haven't proven it is the optimal solution. If you proved the optimal solution to any NP-complete problem, you could solve P vs NP and win $1,000,000.",hg86cx3,t1_hg8501q,1633959403.0,False
q5qcer,">If you proved the optimal solution to any NP-complete you could solve P vs NP and win $1,000,000.

Hey, we don't know the optimal algorithm that solved it with optimal complexity, right? But if my algorithm is recursive backtracking, so what would be the time complexity for n * n going by that?",hg86qq6,t1_hg86cx3,1633959594.0,True
q5qcer,"depends on what you regard N as, but you choose the length of the side of the puzzle then it's known to be NP complete. 

here's a link to paper on this:

[https://www.sciencedirect.com/science/article/pii/S097286001630038X](https://www.sciencedirect.com/science/article/pii/S097286001630038X)

I dont know very much math so I can't explain this paper to you though.",hg7n2z4,t3_q5qcer,1633946516.0,False
q5qcer,"I am talking about a 9 x 9 board. However, is the general Sudoku complexity unknown?",hg7oslt,t1_hg7n2z4,1633947984.0,True
q5qcer,how do you describe the input size? the amount of blank tiles?,hg7ruxl,t1_hg7oslt,1633950484.0,False
q5qcer,The board size. 81 squares (including blank and filled),hg7s3q0,t1_hg7ruxl,1633950678.0,True
q5qcer,"if your asking how long it takes to solve a fixed size input then you can in theory do it in constant time by storing all 6,670,903,752,021,072,936,960 possible boards (though this is obviously infeasible). 

it sort of doesn't make sense to ask the time complexity of a fixed size problem

&#x200B;

if you consider the amount of blank squares to be the input size and you guess all possibilities until you find the right one then it's going to be O(2\^n) (although n cant scale to infinity so this still doesn't make much sense)",hg7tzen,t1_hg7s3q0,1633952071.0,False
q5qcer,"It's equivalent to graph colouring with 9 colors, so NP-complete I guess",hg7vugc,t3_q5qcer,1633953345.0,False
q5qcer,Isn't the time complexity O(1) in case of 9 x 9?,hg7w4t3,t1_hg7vugc,1633953541.0,True
q5qcer,"General Sodoku is NP-Complete. It can easily be transformed to k-coloring.

But for a fixed grid like 9 you have no ""n"", since it is fixed. So you can list every possible solution, store it and look it up. This is in O(1) (asuming you can somehow build a function to map solutions to array positions that executes in constant time, else it would have the complexity of this function). But the space complexity is higher (something like n^3, I guess).",hg8iuy6,t3_q5qcer,1633965113.0,False
q5q3qo,"I would break that down - go and understand object oriented design first, once you have that then look to an OO language where you can practice it. Sure you can do C++ its a perfectly fine language for OO (i use it myself), you may find Java or c# or something similar slightly easier.",hgip3sa,t3_q5q3qo,1634152192.0,False
q5ou0f,"Most languages have a function to generate a pseudorandom floating point number, uniformly distributed between 0 (inclusive) and 1 (exclusive). In JavaScript, that function is [`Math.random()`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random). So if you want something to have a 50% chance of happening, generate a random number and check if it is < 0.5.",hg74ana,t3_q5ou0f,1633930620.0,False
q5ou0f,"Assuming n is a natural number between 0 and 100, you can form an array with n many true values and 100-n many false values. Just write a for loop and append however many you want. After that you select a random element from this array with the random module(I know python only, so whatever satisfies it in javascript is good)",hgsmkw9,t3_q5ou0f,1634337128.0,False
q5mjcb,"A container can interfere with performance of another container. Think shared memory, cache etc",hg6p1rb,t3_q5mjcb,1633921687.0,False
q5mjcb,Rephrasing slightly “a process can interfere with performance of another process”.,hg7aij6,t1_hg6p1rb,1633935312.0,False
q5mjcb,"Here are a few:

Virtual machines have some performance overhead. Sending traffic between different data centers adds latency. There's lock-in with whatever hosting solution or microservice provider you work with. Breaking a problem into too many small discrete pieces can make it harder to understanding system behavior as a whole, and can needlessly introduce dependencies and risk cascading failure.

Depending on the problem you're solving, maybe some of these cons won't apply, or maybe the benefits outweigh them, but it's certainly a design trade-off.",hg6olua,t3_q5mjcb,1633921450.0,False
q5mjcb,Containerisation and kubernetes in its most common use case (containers) don’t use virtualisation.,hg7afyl,t1_hg6olua,1633935256.0,False
q5mjcb,"Cons … it is complicated as fsck… paying k8s admins today is insane … I have a k8s 8 RPI cluster running at home, that’s easy, at work we have 5 16-32 nodes clusters across multiple sites that all need to share data … when that shit breaks its down for hours and hours …",hg6op3g,t3_q5mjcb,1633921497.0,False
q5mjcb,"Trade-off when comparing k8s to what? Comparing it to other container orchestrators like docker swarm? Or comparing it to a container runtime like docker? Or comparing it to a software architecture like microservices? Comparing it to a software architecture like a monolith?

Kubernetes supports containers and virtual machines (KubeVirt). It seems the future will be serverless anyway.",hg9lbb9,t3_q5mjcb,1633981192.0,False
q5lf6c,"This fills me with joy, also reminded me of my copy of the book that's just been lying there",hg6nazu,t3_q5lf6c,1633920758.0,False
q5lf6c,Cool. Did you also read the book? :-) Or just bought or borrowed :)),hg6slwj,t1_hg6nazu,1633923614.0,True
q5lf6c,Bought but haven’t finished,hg71sgs,t1_hg6slwj,1633928893.0,False
q5lf6c,I read it when the electricity goes out. At 38% for now :),hg7567v,t1_hg71sgs,1633931248.0,True
q5lf6c,">the delight in being able to see how much you can do.

This is funny considering the first guy ever to get this disease also developed the theory surrounding what can and can't be done with computers.",hg8c2m4,t3_q5lf6c,1633962129.0,False
q5lf6c,"I wonder who he means by ""the poor fellow who invented the thing""",hg8mlra,t3_q5lf6c,1633966711.0,False
q5lf6c,He means Mr. Frankel. He invented a system to do complex calculations for their scientific needs.,hg8ytrc,t1_hg8mlra,1633971771.0,True
q5lf6c,Haha I read that book a little while ago as well. That's such a great scene.,hgacluh,t3_q5lf6c,1633993860.0,False
q5lf6c,holy molly. even back then they felt this way,hg7nbb7,t3_q5lf6c,1633946715.0,False
q5lf6c,"I am basically a monkey with a keyboard, I can't even imagine what a genius like Feynman could come up with with todays technology.",hg7svi2,t1_hg7nbb7,1633951268.0,False
q5lf6c,he sure knew how to romanticize his life,hg8kp7r,t3_q5lf6c,1633965917.0,False
q5993u,"A fairly straightforward approach would be to sort the house prices, then iterate from the beginning of the sorted list, subtracting the house prices from your budget until it reaches 0 and keeping a count of how many houses you have bought. This would be an O(nlogn) solution.",hg3xa3e,t3_q5993u,1633877245.0,False
q5993u,"Sorry, this didn't really answer your question. In terms of learning how to solve this problem, I think taking an algorithms course and practicing is the best way. If I understand the problem correctly it isn't terribly complex. A good starting point for any problem is the brute force solution, in this case you would try buying every combination of houses where the prices add up to your budget and choose the combination with the most houses in it.",hg3y83n,t1_hg3xa3e,1633877679.0,False
q5993u,"That's a good starting point. Thinking in Time complexity is the second step. The first is to find a solution and check this solution afterwards. Maybe N is 2 so you can do a simple comparison. Maybe N is in the millions and you need another solution. Think in actions that have to be made to solve a problem and ways to reduce the amount of these actions. A good time complexity is always based onto manipulation of the data and its structure. You rarely want to compare anything with anything.

In general it's like any complex problem. Reduce complexity first. Complexity is relative and based onto the limits of our minds and lack of information. This is done by finding the different parts of the problem and it's connections / relations. For example, solve the problem and optimise the solution are two parts. Next you check if you need to divide these parts again. For example to solve the problem you need to understand the variables and its meaning until you have a good understanding of all the parts and parts of the parts of the parts... This way you can focus onto the parts instead of all the complexity at once. This is divide and conquer. Visualisation of structure and behaviour are always useful tools on this way. So start drawing. :)",hg5txd2,t1_hg3y83n,1633906457.0,False
q5993u,It’s been a while since I saw algorithms but this seems to be the knapsack problem. It’s a pretty good problem from a learning perspective since it has many easy to understand solutions which you can study to understand pros and cons of different approaches.,hg45m1r,t3_q5993u,1633881014.0,False
q5993u,"It's not the knapsack problem, since that involves different cost and ""size"" parameters for each object. So you seek the optimal subset of input items such that their cost is maximal but their size does not exceed a certain value.

&#x200B;

The given problem is solvable in O(n log n). Knapsack is NP-complete.",hg63m69,t1_hg45m1r,1633911004.0,False
q5993u,"> It's not the knapsack problem, since that involves different cost and ""size"" parameters for each object. So you seek the optimal subset of input items such that their cost is maximal but their size does not exceed a certain value.

> The given problem is solvable in O(n log n). Knapsack is NP-complete.

Correct me if I’m wrong, but this is the Subset Sum Problem converted into an optimization problem. The problem is NP-Hard, so by definition doesn’t have a *true* polynomial-time solution.

We have [an approximation algorithm](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.1162&rep=rep1&type=pdf), but it’s still not a true polynomial-time solution, just a “best approximation” *which is probably the correct answer*. I just wanted to clarify this, since if OP has never come across this kind of problem, they’ve probably also never come across approximation algorithms or complexity classes.",hg6cwfq,t1_hg63m69,1633915583.0,False
q5993u,"Since OP's problem was solved in O(n log n) [here](https://www.reddit.com/r/computerscience/comments/q5993u/comment/hg3xa3e/?utm_source=share&utm_medium=web2x&context=3), it is highly unlikely to be NP-complete.


It also is not Subset-Sum since there you 
* are allowed to  choose a single entry multiple times
* have to make your numbers sum to the exact value B, whereas here you are asked to find the highest number of houses where the weight is smaller than B.",hg6dhah,t1_hg6cwfq,1633915863.0,False
q5993u,"> Since OP's problem was solved in O(n log n) [here](https://www.reddit.com/r/computerscience/comments/q5993u/how_do_i_learn_to_solve_this_type_of_problem/hg3xa3e/), it is highly unlikely to be NP-complete.

That person’s proposed solution (even when accounting for a solution with a leftover budget, which that doesn’t) doesn’t work for the same reason a greedy algorithm doesn’t work for the Knapsack problem. If the budget is $400k, with 3 houses priced $210k, $200k, and $195k, that commenter’s solution would greedily select the $210k house instead on the $200k and $195k homes.

> It also is not Subset-Sum since there you are allowed to

> •	⁠choose a single entry multiple times

Where are you getting that? A “subset” means it has to be a set of the original inputs, which means you can’t select one of the members multiple times.

> •	⁠end up with a sum smaller than the budget, whereas Subset-Sum asks whether your subset sums to precisely a given number.

This is why I said “converted into an optimization problem”. Just in case you aren’t aware, an optimization problem is one where instead of achieving an exact goal, your algorithm attempts to optimize the solution. The definition of SSP as an optimization problem is

> “Find a subset *s*, of input set *I*, with budget *B*, that optimizes the expression sum(*s*) <= *B*

If you didn’t, check out the paper I linked, since it discusses this problem and explains how they achieve an approximation algorithm that runs in O(n log n), but is still an approximation algorithm",hg6ey2j,t1_hg6dhah,1633916569.0,False
q5993u,"If I understand the linked solution correctly, given your constraints (B = 400, A = [210, 200, 195]), we would first sort A to get [195, 200, 210] and then see that the prefix [195, 200] of that list is the largest prefix with sum <= 400, and hence return length([195,200]) = 2 as the (correct!) solution.


The problem can (most likely) not be reduced to the Subset-Sum problem, as you can see when you consider the following instance:

(B = 10, A = [3,10]). The answer for this problem is 1. The answer for the largest subset-sum of A less than 10 is 9, since we choose the multiset {3,3,3}. However, we can not buy the houses [3,3,3] since we can only buy each house once.",hg6fq92,t1_hg6ey2j,1633916938.0,False
q5993u,"> If I understand the linked solution correctly, given your constraints (B = 400, A = [210, 200, 195]), we would first sort A to get [195, 200, 210] and then see that the prefix [195, 200] of that list is the largest prefix with sum <= 400, and hence return length([195,200]) = 2 as the (correct!) solution.

I interpreted the “sorted” to mean decreasing, since that’s normally how houses are listed, but if it was sorted increasingly, you’re correct that it gets the correct solution.

For a decreasing sort, the inputs [210, 220, 390] and budget 400 would yield a non-optimum solution with the greedy algorithm.

The purpose of the example was just to show that sorting and selecting down the line *is not* guaranteed to be an optimum solution. It *can* be, depending on what the input is, but it also fails on an infinite number of test cases.

> The problem can (most likely) not be reduced to the Subset-Sum problem, as you can see when you consider the following instance:

> (B = 10, A = [3,10]). The answer for this problem is 1. The answer for the largest subset-sum of A less than 10 is 9, since we choose the multiset {3,3,3}. However, we can not buy the houses [3,3,3] since we can only buy each house once.

Again, where are you getting your definition of SSP? I’ve literally never seen anyone interpret “subset” to be what you’re saying it is. In every single instance I’ve come across this problem the output subset is a strict subset of the input set. What you’re doing is something entirely different.

Btw I Mastered in Computer Science and focused on Automata Theory, Class Complexity, and Compiler Construction, so I’ll be pretty baffled if my interpretation of SSP is incorrect here. Every single reference to this problem in my courses referenced it as I have, with the output being a strict subset (which is what the paper I linked does) so I’m hoping you can link me to what source you’re seeing that defines the problem as you are",hg6k0u2,t1_hg6fq92,1633919095.0,False
q5993u,">it also fails on an infinite number of test cases.

Do you have an example?

Your definition of subset-sum is unlike the one I heard (but nontheless NP-complete), yet the algorithm here does not require you to solve an NP-hard problem. It just requires you to output the size of the largest set with sum <= B, without explicitly constructing such a set. Since the hard cases are those where B <<= 2\^(length A), actually enumerating the subsets of that size is still hard. (If that makes sense).",hg6lxdr,t1_hg6k0u2,1633920049.0,False
q5993u,"... I just realized I read the OP wrong. I read “What’s the maximum total you can buy” as in “buy the highest total land value you can”, and not the actual question. In that case, yeah, sorting and choosing in ascending order works.

But, beyond that, I think you have the Subset Sum Problem confused with something else (which I now acknowledge that OP’s problem is *not*), and it’s at least partially my fault for bringing it up when it actually doesn’t have anything to do with the problem. SSP says that the output is a subset of the input, so that example you gave a comment back with [3, 3, 3] is not an example is an subset that optimizes the sum. [Here’s another explanation of the Subset Sum Problem to clarify what that is](https://stackoverflow.com/questions/4355955/subset-sum-algorithm).

Sorry again about that confusion, I totally misread the OP",hg6omgg,t1_hg6lxdr,1633921458.0,False
q5993u,"This is not knapsack as we are not seeking to maximize the sum of the value of the purchased houses, only the number of houses purchased. Since we want to purchase as many houses as possible, it makes sense to buy the cheapest ones we can until we run out of money. Sort the list of houses from least to greatest (something like Collections.sort(houses) in Java) and iterate through the list subtracting the value from B until B cannot be used to purchase the next house. The number of houses seen is your answer. A solution like this is called a “greedy” algorithm, for at every step we make the simplistic choice pointing in the direction we want (buying the cheapest available house)

Your university’s data structures and algorithms course will give you a good intro into topics like greedy algorithms, and the other topics needed to solve problems like this (sorting and searching, graphs, etc)

The knapsack problem some other comments in this thread have mentioned is a more complicated problem where we want to maximize the value of all the houses we purchase, while being constrained with a budget of B. That’s more complicated and a greedy algorithm will not work in all cases for that. you can use a technique called dynamic programming to solve the knapsack problem but that’s probably better to work on after you get familiar with the more approachable topics.",hg61wwc,t3_q5993u,1633910167.0,False
q5993u,"This is the knapsack problem in disguise, once you have your first lecture on algorithms you will immediately recognize such a thing, though sometimes it's of course more hidden. But don't stress, just take a couple of algorithms lectures ;-)",hg57p5y,t3_q5993u,1633896877.0,False
q5993u,"Disclaimaer im developer but not cs


"" Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won't usually need your flowchart; it'll be obvious."" -- Fred Brooks, The Mythical Man Month

If I saw the data structured in tables (sql like) i know what to do 

Step 1 
 Make a little table whit the data 

Step 2 
Make a brute force solution

Step 3 
Use ""dynamic programming "", try to make a better solution like if you sort the data if one solution  being 1 to 10 (1 small 10 big) if 3 and 7  is over the budget then  4 and 7 is also over the budget",hg5pc7n,t3_q5993u,1633904414.0,False
q53ako,"Head. The first element in a list is the head of the list, and the rest is the tail.",hg2wrbr,t3_q53ako,1633852584.0,False
q53ako,"Initial, head, top (of stack) or last (depends on need), first, index.",hg393kl,t3_q53ako,1633862801.0,False
q53ako,"This is a very comprehensive list, thanks pal. Could root be included in that?",hg3a5jh,t1_hg393kl,1633863683.0,True
q53ako,root can be included if the data structure you’re using resembles a tree.,hg4dysl,t1_hg3a5jh,1633884683.0,False
q53ako,In a blockchain was my line of thinking. Like blocks in a blockchain with the root block and then the rest,hg4hn3x,t1_hg4dysl,1633886248.0,True
q53ako,"In my opinion this resembles a linked list, for those we designate the first element as head. I would use head, but obviously your project so if root makes more sense to you then use root. If your tossing it off to other devs then I suggest using head.",hg4s484,t1_hg4hn3x,1633890531.0,False
q53ako,"Makes sense, thanks mate!",hg5eqse,t1_hg4s484,1633899773.0,True
q53ako,"For blockchain, I believe first block is traditionally called the ""Genesis Block""",hg5k6y2,t1_hg5eqse,1633902102.0,False
q53ako,"Sure! I treat root to do with trees, but it is more than acceptable.",hg5pb11,t1_hg3a5jh,1633904399.0,False
q53ako,You have a great username. Cypher is just a cool looking word.,hg5pknn,t1_hg5pb11,1633904519.0,True
q53ako,I've used it for years.,hgc10hw,t1_hg5pknn,1634029125.0,False
q53ako,Car.,hg2yy2j,t3_q53ako,1633854407.0,False
q53ako,Cons? I like it.,hg3axsa,t1_hg2yy2j,1633864323.0,False
q53ako,Con: You can tow a car on the back of a truck. Is the truck the head or is the car in that situation?,hg4e2pc,t1_hg3axsa,1633884731.0,False
q53ako,initial,hg31du1,t3_q53ako,1633856459.0,False
q53ako,first. take it or leave it,hg33in7,t3_q53ako,1633858202.0,False
q53ako,Initializer,hg2y57d,t3_q53ako,1633853729.0,False
q53ako,first,hg43bu4,t3_q53ako,1633880015.0,False
q53ako,"Head, initial, or first. I like to use head since it refers to the front of a type of list, while first could be confusing in terms of first element added, priority, etc",hg4r30i,t3_q53ako,1633890114.0,False
q53ako,Zeroth (counting usu begins at 0),hg3eskj,t3_q53ako,1633867181.0,False
q53ako,"Indexing begins at zero, but nobody really says ""zeroth"". What comes after zeroth, oneth? It can't be ""first,"" because zeroth is the first.",hg3njgh,t1_hg3eskj,1633872501.0,False
q53ako,"For many languages, indexing starts with 1. Most of the early languages used 1 as the first index or element. Fortran, COBOL, Algol, APL, all use 1 as the first index. Even some modern languages like Julia, Mathematica, Matlab, etc.. These languages were mathematical and scientific languages so it makes sense that they would choose that starter.

Now, C (and all languages derived from C) is different. It chose 0 as the first index/element. The reason is because it is not an index. It is an offset. The reason they chose an offset is because they chose to make arrays a contiguous block of memory that is allocated with the memory address returned as a pointer to the beginning of that block of memory. They used array notation to make it easier to do pointer arithmetic to find the nth element  in this block of memory based on the size of the type of array (int, char, long, etc). The number in the brackets is the offset from the beginning of the memory block allocated for that variable.  This was unique to C at the time and since most modern languages are either based on C or are using C as the compiler or parser, they use 0 as the first ""index.""",hg62lp7,t1_hg3njgh,1633910506.0,False
q50xlp,"1/ Hashing functions are one-way in that it's easy to calculate the output from an input, but difficult to calculate an input from an output. [Wikipedia lists several methods.](https://en.wikipedia.org/wiki/One-way_function)

2/ The idea of a rainbow attack is that the attacker has pre-computed a ton of common password hashes and then can compare to your leaked database. By salting the passwords, the attacker now must compute the hash for each password + salt combination rather than just doing it once.

To put it differently, if the attacker has N common passwords and you have M users, without salting the attack might take O(N + M) hash operations. With salting it would take O(N\*M) hash operations.",hg2fbyc,t3_q50xlp,1633839866.0,False
q50xlp,[deleted],hg2fxjt,t1_hg2fbyc,1633840226.0,False
q50xlp,"The salt is different for each user and stored alongside the hash.

Say an attacker has access to your database and a list of 1M common passwords. Without salting, she just needs to run the hash function once for each common password and check for matches in your database. So, 1M hash operations no matter how many users.

Now, suppose your database stores a unique salt and hash(password + salt) for each user. The attacker's precomputed list of hashes is useless and they'll need to recalculate for each user, so 1M \* \[number of users\] hash operations.

Meanwhile, it's easy for you since you have the salt and the user's input. Just run hash(user\_input + salt) and see if it matches the value in the DB. One hash operation.",hg2haev,t1_hg2fxjt,1633841027.0,False
q50xlp,"O(M * N) would be the complexity assuming that we have a list of salts right ?
If we don't have a list of salts, cracking seems almost impossible.",hg2sbqg,t1_hg2haev,1633849057.0,False
q50xlp,"Salts are typically stored alongside the hashed passwords in db, so if you manage to get the latter you most likely also have access to the former.

Now let's assume for some reasons we have only the hashed passwords without the salts. In that case it is as if every user password is at least as strong as the hash thus extremely difficult to crack as long as salts are strong enough. Even when a user uses a stupid password ""abc"" its concatenation with the strong salt will be strong and difficult to crack.",hg2upda,t1_hg2sbqg,1633850932.0,False
q50xlp,That's what I was thinking thanks,hg2wodo,t1_hg2upda,1633852516.0,False
q50xlp,I store salts on a completely different database server for my systems. It increases the time for a user to authenticate by only one hundred or so milliseconds.,hg490n4,t1_hg2upda,1633882512.0,False
q50xlp,"You can also add the idea of ""pepper"". Not commonly used but also an option. Essentially add a random 8 bit value that's not stored and you have to check every option. This will be done on top of salt. 

Another thing that you can do on top of salt, also not commonly used, is have a secret key that is used on the server side as well. Usually your source is kept separate from your DB so assuming this, you can add the secret key to the password for added security.",hg4wn49,t1_hg490n4,1633892403.0,False
q50xlp,"I thought a pepper was an extra number set in the server config, not something you bruteforced every time? At that point, why not use a proper slow hash like bcrypt?",hg69aka,t1_hg4wn49,1633913837.0,False
q50xlp,"hash(user\_input + salt) is too easy, I use hash(hash(user\_input ) + salt) on my systems.",hg48nby,t1_hg2haev,1633882344.0,False
q50xlp,"I'm confused about the runtime analysis. If you have N passwords that you want to try on M users, shouldn't that be O(N\*M)? And then with P salts you want to try it'd be O(N\*M\*P)? I don't know much about this so I'm not seeing where O(N+M) comes from.",hg50w5e,t1_hg2fbyc,1633894166.0,False
q50xlp,"If there are no salts, then each password can be checked against all users at once. With a hash table, this check can be very fast. So I think it would be more like O(N*log(M)). Not sure why it'd be O(N+M), but I might be missing something.",hg69l64,t1_hg50w5e,1633913978.0,False
q50xlp,Ohh good point. I guess adding the M users hashes to a hash table would be O(M) and then lookups in a hashtable I think have an amortized time of O(1) so I think that may be where O(N + M) comes from.,hg6lj42,t1_hg69l64,1633919852.0,False
q50xlp,"Hashing is one way.

Example: if you mix two paint colours, you get another colour, and every time you mix the same two inputs you get the same output, but it's impossible to go back to the exact two original colours.

In this weird example, salting is just putting your own colour in the mix to make it more complex. If you keep your salt colour secret, you can validate the inputs are correct without disclosing anythin by comparing the ""hash"".",hg2xb06,t3_q50xlp,1633853035.0,False
q50xlp,">  if you mix two paint colours, you get another colour, and every time you mix the same two inputs you get the same output, but it's impossible to go back to the exact two original colours.

I really like this example.",hg3y1n3,t1_hg2xb06,1633877596.0,False
q50xlp,Salts don't even need to be kept secret. They mostly just ensure that 2 people using the same password don't have the same hash.,hg50a2s,t1_hg2xb06,1633893912.0,False
q50xlp,This. Salt doesn’t need the same level of protection as real secret. Otherwise you will need salt of salt to make salt safe😜,hg5hjbr,t1_hg50a2s,1633900968.0,False
q50xlp,"An easy explanation is:

FACT:

Remember that hash is a fixed length.

There are infinite possible combination of a plaintext.

So:

Imagine the hash ""SHLK""

It might output ""mypassword"" or ""gibberishPasswordThatHasTooManyLengthToBeInputted""

Another way to see it it's

10 + 30 = 40

In this case 10 + 30 is my password and 40 is the hash result

but we can get 40 from -1000000 + 1000040 right?, but it might not be a valid password due to length.

In the case of hash, it has enough combination and with the correct algorithm that makes it really hard to two input that have the same output. This might happen though see [Hash Collision](https://en.wikipedia.org/wiki/Hash_collision) for further reading.

Salting is basically just to make the result more unique, each user usually have different salt written in plaintext. Although it's not best practice let's make the username the salt.

Username: 5  
Password: 10 + 30  


Username: 10  
Password : 10 + 30

&#x200B;

username 5 and 10 have the same password, now if we put hash it 

Username: 5  
Password: 10 + 30  
Hash: 45

&#x200B;

Username: 10  
Password: 10 + 30  
Hash: 50

&#x200B;

With this we don't know if username 5 or 10 has the same password. This is important for common password (dates, weak password)",hg31w1a,t3_q50xlp,1633856878.0,False
q50xlp,"**[Hash collision](https://en.wikipedia.org/wiki/Hash_collision)** 
 
 >In computer science, a collision or clash is a situation that occurs when two distinct pieces of data have the same hash value, checksum, fingerprint, or cryptographic digest. Due to the possible applications of hash functions in data management and computer security (in particular, cryptographic hash functions), collision avoidance has become a fundamental topic in computer science. Collisions are unavoidable whenever members of a very large set (such as all possible person names, or all possible computer files) are mapped to a relatively short bit string. This is merely an instance of the pigeonhole principle.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hg31wv5,t1_hg31w1a,1633856897.0,False
q50xlp,[deleted],hg2jy9h,t3_q50xlp,1633842783.0,False
q50xlp,Hashing and salting were things long before 30 years ago.,hg3b0k2,t1_hg2jy9h,1633864385.0,False
q50xlp,"Did you work with cipher's algorithm :P 

I remember in school I stored my password in base64 and my friend keylogged it and came back saying he knows my password! 

I was like try it, he did and couldn't figure out. why it was not working. 

Good old Base 64 encoder and decoder :)",hg2sanq,t1_hg2jy9h,1633849033.0,False
q50xlp,"Is it good practice to use salt that's derived from username? This way you don't have to store salt value, just know the algorithm used.",hg2s9im,t3_q50xlp,1633849007.0,False
q50xlp,"It is typically much easier for an attacker to get access to usernames, which are often public or present in email communications etc, than to get access to salts stored in a secure production database which never leave it.

So deriving salts from usernames makes it pretty easy to know the salts. You can't however do much if you have the salts but not the hashed passwords. And as both are stored in the same db, if you get access to one you also get access to the other. I cannot find a scenario where your strategy would weaken security.

Good question then! Hopefully someone else will chime in.",hg2vnc6,t1_hg2s9im,1633851680.0,False
q50xlp,"Using salts related to usernames would allow for hashes of common passwords to be precomputed prior to a data leak. Sure, it's still a lot of work doing it for every user, but being able to do it before stealing the actual password hashes gives much less time for users to change their passwords.

Storing a random salt with the hash means hackers have to compute after the breach.",hg46z0n,t1_hg2vnc6,1633881616.0,False
q50xlp,"Brilliant, I would never have thought of that. Thanks for sharing!",hg512su,t1_hg46z0n,1633894243.0,False
q50xlp,"You could, but will run into issues if you ever want to allow the user to change their username, or if you need to change it because it's flagged as inappropriate.",hg3lsvh,t1_hg2s9im,1633871541.0,False
q50xlp,"Good practice is to use a hash function intended for password storage, which in itself incorporates a salt. One such function is argon2id. 
Remember that many frameworks today offer secure ways of dealing with password storage. This is usually the way to go if you're working on web development and such.",hg2xxm5,t1_hg2s9im,1633853554.0,False
q50xlp,"I'm answering this from Discrete Math, and not from Cryptography. I'm probably misrepresenting some stuff.Hashing values is an irreversible process (ehhh usually). I'll use the RSA cryptosystem as an example

2 primes of arbitrary lengths are generated. These 2 numbers are n and p

A third number, m, is generated by finding a number that is coprime to (n-1)(p-1). r is the inverse of that coprime modulo (n-1)(p-1)

The reason that this is not reversible is that prime factorization is really hard if you don't know the primes to begin with. No fast method exists to find n and p, so this cryptosystem is irreversible even over an unsecure network.  


Edit to say: I'm not actually sure that the RSA cryptosystem is classed as a hashing algorithm since it *can* be decrypted if you have all the information generated.",hg3o9yp,t3_q50xlp,1633872882.0,False
q50xlp,"It works on the basis that the cost and effort of backwards calculating the password that is hashed outweighs the cost of actually having access to the password.

Unlike keeping encrypted credentials, it is far harder to use the result of reversing one hash to be able to reverse other hashed passwords.

At least, it is if you don't use a awful algorithm to do it (looking at you, MD5).",hg56ko4,t3_q50xlp,1633896434.0,False
q50xlp,"Warning warning warning!

For password you need to use a key derivation function. The recomme ded one nowadays is Argon2 then scrypt and otherwise pbkdf2.

Hashes need to be fast since they are often bottlenecks in crypto proticols to verify message integrity (that they were not tampered with).

On the contrary, a password hash function needs to be extra slow so we can't try many times per second to defend against bruteforcing.

Do NOT use say SHA256 to hash passwords!",hg5lvsw,t3_q50xlp,1633902848.0,False
q4fa3b,"There are a bunch of projects trying to do this sort of thing: https://github.com/kgryte/awesome-peer-to-peer

The standout project is probably [IPFS](https://ipfs.io/), which provides a distributed global filesystem. It wouldn't be too hard to combine that with web workers to create locally dynamic websites that didn't depend on centralized hosting after the first load. 

That raises one of the big question for any scheme like this: How do you avoid a centralized first load? What do you use to replace DNS? Have fun with that.",hfzfndo,t3_q4fa3b,1633789095.0,False
q4fa3b,"Look into fedeverse sites, and the global file system. These seem to be along the lines of what you’re talking about. I do believe this is the next step in the evolution of the internet.",hfzf5v9,t3_q4fa3b,1633788851.0,False
q4fa3b,"Yeah, Fediverse is definitely the place to look. It's not exactly what OP wants, but it should be a good starting point. For example, there's already a blocking mechanism for the network such as what happened on Mastodon with Gab. I'm sure the blocking could be extended in a way to block based on source signature rather than a curated list of excluded nodes.",hfznkhv,t1_hfzf5v9,1633792873.0,False
q4fa3b,"You are looking something similar like eth is doing, but instead of smart contract, you will have ""smart"" website. Same or similar concept can be applied here, but not that much attractive as smart contracts on eth. Also eth smart contract can be used to deliver dynamic website, but use case would be too much expensive in cases where you must write into contract ""memory"" for later use. Not to mention how much such contract would be big in size and how much money you will need to invest for contract to enter the blockchain.",hfzdpma,t3_q4fa3b,1633788131.0,False
q4fa3b,"What would be the use case for a decentralised website like that?

Probably best would be to actually start by creating a blockchain. You can then use special serving nodes that serve the current version on blockchain.",hfyh4pe,t3_q4fa3b,1633765525.0,False
q4fa3b,"There's not a particular use in mind for this project other than I am interested in it and wanna learn more.

I could see it being used for cryptocurrency exchanges. Many people like staking cryptocurrency on these exchanges and having one central organization controlling your funds ruins the point of defi cryptocurrencys.

I'll look into the blockchains a little more in depth. Do you by any chance know any good resources for this I could also take a look at?",hfyhuhg,t1_hfyh4pe,1633766105.0,True
q4fa3b,"There is an experimental browser called beaker which does p2p decentralized sites.

https://beakerbrowser.com/

I’m not sure if there is much development happing there but if you check out some of their talks on YouTube it’s interesting to learn how they pulled it off.

Also, more recently the brave browser has integrated with IPFS.  https://brave.com/ipfs-support/",hfyuyt6,t1_hfyhuhg,1633776466.0,False
q4fa3b,Thanks this is perfect,hfzeslz,t1_hfyuyt6,1633788669.0,True
q4fa3b,Came here to say IPFS too,hfzilan,t1_hfzeslz,1633790524.0,False
q4fa3b,"IPFS gives you static site hosting, including JS. If you really need some sort of backend that you can't ship to the user (do you? Why?) then you could use your JS to find a backend in the swarm and open a websocket.",hfzqqsb,t3_q4fa3b,1633794342.0,False
q4fa3b,"Take a look at ZeroNet https://zeronet.io/
They have dynamic content as well https://docs.google.com/presentation/d/1_2qK1IuOKJ51pgBvllZ9Yu7Au2l551t3XBgyTSvilew/pub?start=false&loop=false&delayms=3000&slide=id.g9a745d911_1_44",hg1kli1,t3_q4fa3b,1633824129.0,False
q4fa3b,"Sounds like you're trying to throw buzzwords at what's basically just **CDN**: 

https://en.wikipedia.org/wiki/Content_delivery_network

https://aws.amazon.com/cloudfront/",hfzqfmr,t3_q4fa3b,1633794200.0,False
q4dei9,"That's pretty unusual phrasing, I wouldn't worry too much about it, other than for passing a test from this particular professor.

By ""online"" they don't mean the internet, they mean its input is received over time, and it produces output before waiting for the entire input stream.
https://en.wikipedia.org/wiki/Online_algorithm

But the rest doesn't make much sense, it still has input and output, and does terminate upon shutdown. It's infinite in that it doesn't need to terminate to be useful, but that's a feature of all online algorithms.",hfxwcvj,t3_q4dei9,1633751681.0,False
q4dei9,I think they were just trying to define online - i.e. rather than a finite runtime with one input and one output it runs forever and has a stream of IO. It's just awkward phrasing.,hfy04rk,t1_hfxwcvj,1633753738.0,False
q4dei9,"It's certainly not colloquial. I wouldn't say it's ""wrong"", but only because there aren't exacting definitions.

You may be missing the lesson, though. If you're in an OS class, this could be the first time you're being asked to create something that doesn't terminate rather than a program to produce a specific output for grading. That will be new, and you'll like have to show functionality by running your professor's tests on top of your project rather than submitting the output of your code.

Also, it's not like OSs don't use algorithms. Parts of the OS use algorithms for scheduling processes, scheduling disk I/O, binpacking memory, and cache replacement strategies.",hfyybup,t3_q4dei9,1633778983.0,False
q4dei9,"The term 'algorithm' does not really have a single rigorous definition. It's more of a colloquial term, generally describing instructions specified within some degree of rigour to achieve some goal. It can be as specific as a Turing machine that computes a specific function, or as vague as a rough set of instructions that a human would probably be able to follow. Computer scientists tend to the more rigorous definitions, but will still use 'algorithm' to refer to a less than formal grouping (e.g. They will call all the instances of merge sort 'the merge sort algorithm', rather than a single canonical Turing machine that implements it. They also call probabilistic, online, and other such programs algorithms even though they tend to differ from the definition they claim.). You should use whatever definition your class uses.

The colloquial definitions of algorithm tend to be relatively narrow in their goal. An OS does a lot of different things, and intentionally has logical separation between its components, so while you could define an OS in this way (the instructions are the source code, the goal is to transform user inputs into display outputs) it's not usually done because it's not particularly illuminating - we would prefer to use some more sophisticated framework that describes an ecosystem of interacting algorithms.",hfymiol,t3_q4dei9,1633769635.0,False
q4dei9,"This is a lot like saying that water doesn't freeze fully because it's atoms are still vibrating. 

Sure, they're technically correct, and they're telling you a thing that you may not have known. But, it ignores the reality that we all deal with day to day, and puts way too much emphasis on a weird point.",hfz7c7y,t3_q4dei9,1633784701.0,False
q4dei9,"An algorithm is by definition finite. In addition it is well-defined in a mathematical sense. An operating system is typically implemented using a number of algorithms, but it is not an algorithm in itself.",hfzokln,t3_q4dei9,1633793340.0,False
q4dei9,Why not? The behaviour of an OS is well defined it is just reliant on a lot of different states including hardware states. Sheer complexity shouldn't disqualify it being an algorithm.,hg043cx,t1_hfzokln,1633800277.0,False
q4dei9,"If you have such a broad definition of an algorithm that any program qualifies as being one, then what is the point of making the distinction between a program and an algorithm?

Talking about algorithms is interesting because it brings with it a set of mathematical tools that can be used to analyze their properties. You can prove that the worst case time complexity of a bubble sort algorithm is O(n²) for example, whereas for quicksort it is O(n logn). That is not something that is possible with a program that is not finite and well-defined, in the mathematical sense.",hg08wo3,t1_hg043cx,1633802372.0,False
q4dei9,"You can still do quite a bit of useful analysis [by thinking in terms of stream processing](https://en.m.wikipedia.org/wiki/Coinduction#Codata) rather than computing on finite data structures. In the case of streams (codata), the algorithm potentially runs forever and generates an infinite amount of data, but each step of data processing is finite and can be analyzed with normal tools for finite-runtime algorithms. For example, operating system scheduling can be thought of as taking a stream of program states and producing a stream of processes to run. 

There are also [infinite word length generalizations](https://en.m.wikipedia.org/wiki/%CE%A9-automaton) of e.g. finite state machines. A lot of the tools from standard automata theory that let you analyze finite runtime algorithms can then be generalized to the infinite word length case. As an example of an infinite word length language, give a letter to each one of the possible programs a scheduler could run at a given time. Each infinitely long word in the scheduling language is then a series of possible scheduling decisions and you can analyze the scheduling algorithm by understanding properties of this language.",hg2f8jk,t1_hg08wo3,1633839810.0,False
q4dei9,"Desktop version of /u/OddInstitute's links:

 * <https://en.wikipedia.org/wiki/Coinduction#Codata>

 * <https://en.wikipedia.org/wiki/Ω-automaton>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hg2f9zc,t1_hg2f8jk,1633839833.0,False
q4dei9,"Scheduling is indeed an algorithm, and finite.",hg2fx0r,t1_hg2f8jk,1633840217.0,False
q4dei9,An operating system is not an algorithm.   Your lecturer is not competent.,hfy6pvl,t3_q4dei9,1633757939.0,False
q4dei9,No. Not even remotely,hfxwew3,t3_q4dei9,1633751711.0,False
q4dei9,Are moderation rules considered algorithms? If a sub reddit bans people from another automatically is that considered an algorithm? Or is it just a rule,hfz07hx,t3_q4dei9,1633780301.0,False
q4dei9,"Think about a sort algorithm that takes a list of integers as input. When sort is invoked, all of the data must be present, and it sorts the list to completion. The algorithm is carried fully carried out in one invocation.

Now think about an operating system. An operating system waits for user interaction, then responds and performs some computation, then waits again for the next input. This is what “online” means in this case - really “interactive” is a better word.

Then, every computer program is just an algorithm that can actually be executed and ran. An algorithm is just a set of steps that performs some kind of task. In the case of an OS, that task is hardware abstraction and management.",hfz3r3o,t3_q4dei9,1633782596.0,False
q4dei9,"It is more likely a ""procedure"" which is Knuth is mentioned in The Art Of Computer Programming Vol 1. Algorithm is more robust finite set rules.",hfzwloo,t3_q4dei9,1633796973.0,False
q4dei9,I think the problem arises even before comparing an operating system with an algorithm. For me it becomes already cumbersome by the fact that apparently properties of an algorithm are defined but the validity of these properties are relativized later on. It becomes thereby quite difficult to understand this content. It would be better to define properties that always have validity and then define subsets that extend them. From this perspective the knowledge here could also be used more appropriately for the way of thinking of computer scientists or educate to think this way.,hg17mcn,t3_q4dei9,1633817839.0,False
q4dei9,"Write in point form and plain English everything that the operating stream does and how it does it. If you do it well, and easily, you may be into something.",hg25398,t3_q4dei9,1633834437.0,False
q4bz42,"Think about how you read off the following code to someone over the phone:

ABBB CCCC DDDD DDDD

""one a, three Bs, four Cs, eight Ds"" or encoded as 1A3B4C8D. Notice how it's shorter than the original

Done naively you can actually end up with an encoding that's longer than the original in some cases. It's not a great encoding but just shows an example that you could explain and implement easily.

Improving on it is left as an exercise to OP ;)

Read for more: https://en.wikipedia.org/wiki/Run-length_encoding

    public static void main(String[] args) {
        System.out.println(encode(""ABBBCCCCDDDDDDDD"")); // => ""1A3B4C8D""
        System.out.println(encode(""AABBBCDEFGHIJKLM"")); // => ""2A3B1C1D1E1F1G1H1I1J1K1L1M""
    }

    public static String encode(String s) {
        char[] chars = s.toCharArray();
        StringBuilder sb = new StringBuilder();
        char prev = chars[0];
        int count = 1;
        for (int i = 1; i < chars.length; i++) {
            char curr = chars[i];
            if (prev == curr) {
                count++;
            } else {
                sb.append(count);
                sb.append(prev);
                count = 1;
                prev = curr;
            }

            if (i == chars.length - 1) {
                // wrap-up the end
                sb.append(count);
                sb.append(prev);
            }
        }
        return sb.toString();
    }",hfxth9y,t3_q4bz42,1633750153.0,False
q4bz42,Nice implementation:) you can get rid of that for loop and use a tail recursive approach to exploit the JVM optimisation 😊,hg060vm,t3_q4bz42,1633801114.0,False
q478w4,So said electronic device will still need a processor and WiFi chip to connect to said network.  What is the advantage of a centralized computing resource when an ARM SoC already does all of the computing that will be required locally for like $0.25?,hfwynds,t3_q478w4,1633734906.0,False
q478w4,"I imagine it would probably come under as being not as profitable for tech companies, although it would provide environmental benefits, upgradeability benefits and what not. It doesn’t increase profit. Regarding tech companies that are pro green and willing to take the hit in profit, I don’t think it would be adoptable as every device would have to be able to communicate and use this central computer which would lead to everyone wanting to use different standards.",hfwlif3,t3_q478w4,1633729036.0,False
q45e9c,"I feel like you'd get a better quality answer if you read this in a textbook (Russell, Norvig pg. 167).",hfww8bt,t3_q45e9c,1633733774.0,False
q45e9c,What?,hfzqqy4,t3_q45e9c,1633794344.0,False
q45e9c,Is this homework,hfztshs,t3_q45e9c,1633795725.0,False
q3tlim,"You need to have ""an eye"" for it; i.e. you don't need to be good at painting, but you need to be at least decent at graphical design. Understanding what to change, add or take away to improve an image is at least as important as the technical aspects.",hfw3yp3,t3_q3tlim,1633721673.0,False
q3tlim,"You could check the language ""processing"" :).",hfzfk5t,t3_q3tlim,1633789051.0,False
q3kkxr,"It is, checkers, just like tic-tac-toe, are what's called [solved games](https://en.wikipedia.org/wiki/Solved_game)",hfshdfu,t3_q3kkxr,1633649330.0,False
q3kkxr,"To be more precise, checkers is weakly solved.  There are certain board positions in checkers where there is no winning move.  There is a computer program that knows how to ensure that it gets to one of these board positions, so the computer will at best get to a draw.  There is no computer program that always wins, or a proof that you can always find a draw from both positions.",hfstsfa,t3_q3kkxr,1633655442.0,False
q3kkxr,"Again, if they’re using the correct algorithm, yes it’s impossible.",hfsiur1,t3_q3kkxr,1633650062.0,False
q3jwi3,"I don't know what you're seeing on desmos, but 15^n grows much faster than n^15.",hfs9m0k,t3_q3jwi3,1633645589.0,False
q3jwi3,"A log scale is helpful for graphing fast-growing functions.

If you look at log(n\^15) compared to log(15\^n) you get a clear view of what's going on. Here's a Desmos link: [https://www.desmos.com/calculator/qtibvk7r8t](https://www.desmos.com/calculator/qtibvk7r8t)",hfsfzw1,t3_q3jwi3,1633648655.0,False
q3jwi3,"n^15 < 15^n.

Proof:

n^15 < 15^n

15 ln n < n ln 15

In our analysis, we don't care about constants

ln n < n

Logarithmic algorithms grow at a slower rate than linear algorithms.",hftgdr6,t3_q3jwi3,1633667237.0,False
q3jwi3,"I don't think what you're seeing in your graph shows things far enough. The exponential growth will overtake the polynomial one eventually but even the point where they trivially equal (n = 15) gives such a ludicrous value for both that it's probably impossible to see.

Edit: a back-of-the-envelope calculation says that if the n=15 point on your x axis were visually 1 centimeter to the right from the origin, the 15^15 point on the y axis would need to be approximately 0.03 light years up. So, no, probably not visible.",hfs9f2n,t3_q3jwi3,1633645499.0,False
q3jwi3,The light-years thing is a neat heuristic!,hfthd78,t1_hfs9f2n,1633667854.0,False
q3jwi3,"not really, n\^15 grows faster at the beggining but eventually 15\^n surpasses it, to see that in the graph you would need to go very high up, 10\^15 > 15\^10 but 20\^15 < 15\^20",hfs9qnc,t3_q3jwi3,1633645650.0,False
q3jwi3,"As mentioned in other replies on this post, 15^(n) grows faster than n^(15).  A pair of witnesses for this would be any n > 15 and any m >= 1.  Such that n^(15) <  m15^(n).",hg049kd,t3_q3jwi3,1633800354.0,False
q3dv02,"Same way most clocks keep time. Crystals that vibrate at a specific frequency, or in extreme use cases, atomic clocks.        
https://en.m.wikipedia.org/wiki/Quartz_clock",hfrd2y3,t3_q3dv02,1633631675.0,False
q3dv02,"Desktop version of /u/inre_dan's link: <https://en.wikipedia.org/wiki/Quartz_clock>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hfrd4s3,t1_hfrd2y3,1633631696.0,False
q3dv02,Good bot!,hfrmxqs,t1_hfrd4s3,1633635760.0,False
q3dv02,Username checks out!,hfu72dt,t1_hfrmxqs,1633688974.0,False
q3dv02,Thanks.,hfrkeqq,t1_hfrd2y3,1633634718.0,True
q3dv02,"Ultimately the time comes from a crystal oscillator which is the electrical version of a tuning fork, you hit a tuning fork and it oscillates at some resonant frequency determined by its dimensions. A quartz crystal does the same when excited by a rapid change in voltage, except it gives off an analog electrical signal instead of sound waves. A feedback loop keeps it oscillating after it starts up.

The output is an analog sine wave so it’s fed to a comparator which is a circuit that outputs digital high or digital low based on the analog inputs value, this makes it into a digital square wave the processor can use.

Some oscillator circuits have an additional input for an analog voltage which can fine tune the resonant frequency up or down within some range, called voltage controlled oscillators.

It’s difficult to make crystals which oscillate at ghz frequencies because their dimensions become increasingly small. There’s transistor based oscillators which can oscillate at these frequencies however they have poor characteristics, high temperature dependence, high jitter (variability between clock pulses), high variability in frequency. The solution is a phase locked loop, this circuit is a transistor based oscillator which will lock the output frequency frequency to some rational multiple (P/Q) of the input frequency. This way you can have the nice characteristics of a crystal at the speeds of transistors. P and Q may even be dynamically adjustable as well.

If you need a very accurate frequency there’s temperature controlled crystal oscillators, which put the crystal and control circuitry inside a small box with a heater resistor and temperature sensor which keeps the insides at a constant temperature, this keeps the frequency very stable. These can also be mounted on shock absorbers to keep mechanical vibrations from influencing them. People get nutty with this and do rf/magnetic shielding and a whole host of other things to keep it stable.

Beyond that you need atomic clocks, in these you blast a certain element with microwave radiation of a roughly specific frequency, causing the electrons to move up an energy level, when they fall back down to their old energy level they will re emit microwave radiation whose frequency is almost entirely proportional to the difference in energy between those energy levels, which is very precisely known. From there you base your time scale on the frequency of this re emitted radiation, usually by locking a crystal oscillator to a fraction of this frequency via a frequency divider (binary counter with reset) and from there it’s much of the same.

I’ll also add that processors require multiple frequencies, for example while the CPU cores may be running at 4ghz, this is a difficult frequency to carry on a PCB without distortion to say a USB hub so you might also need 12mhz to talk to this USB hub, something else for communication with RAM, something else for the PCIe ports, etc, etc. You might have multiple phase locked loops to generate your various frequencies based on a single crystal. Different sections of the digital logic on the processor will be running on different frequencies, said to be in different clock domains, getting signals across clock domain boundaries requires special attention too.",hfrr60e,t3_q3dv02,1633637527.0,False
q3dv02,"Thank you for your time and precious informations,  much appreciated ! I'll have to save this comment.",hfrxgty,t1_hfrr60e,1633640173.0,True
q3dv02,"Great answer. You should make YouTube vids or something. I got lost at the PQ part, what is that exactly?

Edit: did a quick Google, and think it's power & charge 😉",hfu7o1j,t1_hfrr60e,1633689464.0,False
q3dv02,"P/Q is usually used in mathematics to describe the ratio of two integers, meaning their quotient is rational (a terminating decimal is another way of thinking about it I guess). Given they use the word rational right before it, I assumed this is what they meant.",hfujplz,t1_hfu7o1j,1633697340.0,False
q3dv02,"Slight nitpick, a rational number need not have a terminating decimal, look at 1/3 for an example.",hfumu59,t1_hfujplz,1633698965.0,False
q3dv02,"Good point. I should have said ""terminating or repeating.""",hfuodot,t1_hfumu59,1633699736.0,False
q3dv02,"It's weird how it does this but there's a kind of forward-backwards relationship with Piezoelectrics. 

You apply pressure to quartz and it generates a voltage relational to the pressure applied. 

And in reverse, You apply voltage to quartz and it generates pressure relational to the voltage applied.

This kind of forward-backwards relationship exists in thermodynamics, electromagnetics and many other scientific disciplines.",hfud3zr,t1_hfrr60e,1633693439.0,False
q3dv02,Are you an electrical engineer?,hfv1mec,t1_hfrr60e,1633705706.0,False
q3dv02,"Computers have [real-time clocks](https://en.wikipedia.org/wiki/Real-time_clock) that allow for keeping time. They might also have other time-keeping or timer devices, but they have at least something like that.

I don't know what exactly happens on the CPU level -- I guess that might even depend on the operating system -- but a couple of sources [1][2] seem to suggest that at least one thing that happens is that the operating system puts the waiting thread in a not-ready-to-run state, so it won't be scheduled for running on the CPU. The OS will then only place the thread back in the run queue after at least the desired amount of time has elapsed.

If that's all there is to it, there's probably nothing particularly special happening on the CPU level. That of course wouldn't guarantee that your task will resume running exactly five seconds later, so I don't know if some kinds of hardware timers might be used in some cases.

I seriously doubt any modern system implements the timing based on CPU cycles, as suggested in another comment, ~~mainly because that would seem to imply busy-waiting, and~~ because the number of cycles run by the CPU in a given amount of wall-clock time is not constant or predictable when CPU clock frequencies are automatically being scaled based on load. Something like that might have happened in some old systems, or perhaps in some very limited embedded systems, but I see no reason to believe that any PC operating system would do that.

[1] https://stackoverflow.com/questions/1719071/how-is-sleep-implemented-at-the-os-level

[2] https://en.wikipedia.org/wiki/Sleep_(system_call)#Low_level_functionality


Edit: Now that I think of it, using CPU cycles as a measure of time probably wouldn't require busy-waiting. The problem with the variable number of cycles per second still exists.",hfrbs3s,t3_q3dv02,1633631135.0,False
q3dv02,"Much appreciated, so this whole time management is about hardware clocks which are totally independent from the cpu, it's more practical this way. Thank you for your time.",hfrjdpr,t1_hfrbs3s,1633634294.0,True
q3dv02,"Not exactly. The OS has told a hardware timer to interrupt the CPU at some rate. Historically, 60 times per second was common, so those ticks/interrupts would be 16.667 milliseconds apart. When the interrupt occurs, the CPU switches away from whatever program is running, and basically calls the OS’ scheduler function. When your program said “wake me in 5 seconds” the OS converted 5 seconds into 5x60 ticks per second = 300, stored that in a variable dedicated to that program (or thread, etc.), and put the program/thread into a waiting state so that it would not be allowed to run. Each time the OS scheduler runs due to the timer interrupt, it subtracts 1 from that countdown variable. When the countdown hits 0, the program/thread is put back into a ready state so it’s execution can resume.

Every OS is a bit different, and we have “tickless kernels” now that don’t work anything like this at all, but hopefully you find this helpful.",hftc0c7,t1_hfrjdpr,1633664583.0,False
q3dv02,E clock,hfsbftn,t1_hfrbs3s,1633646456.0,False
q3dv02,"https://cs.stackexchange.com/questions/54933/how-do-computers-keep-track-of-time

To learn more, see Real-time clock and CMOS battery.

Also, on many computers, when you connect your computer to an Internet connection, the OS will go find a time server on the network and query the time server for the current time. The OS can use this to very accurately set your computer's local clock. This uses the Network Time Protocol, also called NTP.",hfrwah4,t3_q3dv02,1633639673.0,False
q3dv02,"Everyone else here is somewhat right. Your computer keeps time with a time chip often in combination with a crystal oscillator, a GPS, atomic or other universally stable time source. BUT that is NOT the CPU keeping time, CPU have separate oscillators for THEIR internal timing (clock signal at eg. 8MHz or 4Ghz) whereas a timer clock will always be at a very high but stable frequency (16-20kHz for crystals) and there is a fast, but fixed, timing chip that increments and keeps track of the current time even when your computer is off (it uses incredibly little energy compared to your CPU)

For your example however, at the very base of a modern CPU stack there is one (or multiple) PIC (programmable interrupt controller) such as the 8259 - you can find these in almost every advanced microprocessor, even many of the 8 bit ones such as the Arduino. You basically tell that PIC (this used to be an external chip) to interrupt your CPU within n seconds or when the clock mentioned above has reached a particular time. Your CPU isn’t (or shouldn’t be) polling for the current time and then compare that to a registry, as you know frequency isn’t stable enough and your CPU may be busy doing other things which will cause it to overshoot that time.

So if you say:
[code]
sleep 5; 
echo “hello future”
[/code]

What happens on the bottom is that you reprogram one of the external timers (although in modern times, there are many and they are probably on-die) to interrupt your CPU from whatever it will be doing 5s from now and then your CPU will jump to the echo instruction. THAT timer is held in sync with whatever time source, your CPU technically doesn’t know what time it is at any time.

As I said, the other way to do this (if you don’t have a PIC) would be to make the CPU jump to a routine after every few instructions or you could for example tag onto another periodic interrupt such as the screen refresh interrupt or an audio interrupt, which for a screen happens every 1/60th of a second in the US or 1/50th of a second in the EU with CRT screens, read the time from an external clock at that time, and if you are at or above the target time, execute your program. Obviously this wastes a LOT of cycles and is horribly imprecise but it is useful if you don’t have interrupts.",hftg02v,t3_q3dv02,1633666997.0,False
q3dv02,A hardware component records time for the computer.,hfr9spw,t3_q3dv02,1633630315.0,False
q3dv02,"Usually you have a hardware oscillator that drives a clock chip on the motherboard. That chip has a backup battery to keep time when the system is off. When the system is on, it generates an interrupt to the processor at fixed intervals. 

When the OS boots up, it can query the clock chip and/or the internet for the current time. It keeps that time value in memory for future uses (as a count of ticks since a chosen start time) and increments the number every interrupt from the clock chip.

If you want a time, the OS gives you the current time. If you want to wait 5 seconds, code can wait for the time counter to reach Time Count + (5 * ticks/second) and then do what you want.",hfrxyyy,t3_q3dv02,1633640388.0,False
q3dv02,It doesn’t understand anything its just a fancy calculator,hfs6svl,t3_q3dv02,1633644282.0,False
q3dv02,"it doesn't, but the computer has a component that keeps track of the vibrations of a crystal and it knows that x vibrations =  1 second, from that it knows that 60x = 1 minute, 3600x = 1 hour, and so on.",hfsa160,t3_q3dv02,1633645790.0,False
q3dv02,"As others have mentioned, computers as we think of them, e.g. your laptop, use crystals, but there is another way to produce a regular clock pulse that is used in some older, more simple applications, and that is by way of a combination of capacitors and resistors (and op amps).  Capacitors are like little batteries which take time to charge and discharge based on how much resistance they have going into and out of them and you can get an arrangement of them to charge up until they get to a certain point then start discharging until a certain point, and then start over again, creating a repeating clock pulse, a series of a high and low voltages, i.e. a square wave.",hftcfpl,t3_q3dv02,1633664828.0,False
q3dv02,"There are a couple different conversations happening in the comments. One is with regards to how the CPU and other circuitry is operating based on crystal oscillators, the other is conversation about realtime clock hardware and that is what addresses your questions related to ""5 seconds"". The bits of your computer dealing with clock time aren't deducing that time by using the oscillators driving the circuits, generally. There is specific hardware in your motherboard that is a realtime clock, and bios/operating system support for interacting with it. That's the bit that relates to ""5 seconds have passed"". 

In the CPU, people are simplifying things quite a lot. Generally the signal from an oscillator is not used directly, but rather will be split into multiple clock signals that are out of phase with each other that various circuitry uses. It becomes very complicated very quickly, but the gist is what people describe, with regards to duty cycles. It's just that the one signal is used to create numerous other signals that are actually being used to drive work in circuits.",hfurub3,t3_q3dv02,1633701387.0,False
q3dv02,I guess it counts the number of cycles the CPU completes in a given amount of time,hfr4p5h,t3_q3dv02,1633628232.0,False
q3dv02,Except that the CPU clock frequency can and often does change to either save energy or give more performance.,hfrdyop,t1_hfr4p5h,1633632039.0,False
q3dv02,"I thought about that but i don't think that's the optimal method to do so. Like said in other comments, an integrated oscillator producing electric pulses makes much more sense.",hfrhg9z,t1_hfr4p5h,1633633478.0,True
q3dv02,"There’s an internal clock, just like the ticking of an actual clock this internal ticker oscillates so many times per second. So all you have to do is keep track of the ticks and do the math in reverse and you get the elapsed time.",hfrctl5,t3_q3dv02,1633631565.0,False
q3dv02,"Meaning that a determined number of oscillations will represent 1 second, and each oscillation is kind of the born of a new second, which makes this method really logical and optimal. Thank you.",hfrithy,t1_hfrctl5,1633634056.0,True
q3dv02,"Yes, it comes from a crystal and an RC circuit that causes it to oscillate at a set frequency. This feeds into the entire CPU, with each tick it moves things forward inside the CPU. It’s probably too much to get into with a comment but that’s the extreme gist of it.",hfrwx74,t1_hfrithy,1633639940.0,False
q3dv02,What a stupid question. Modern computers are OVERCLOCKED dude. That means they know what time it is. smdh.,hfriwri,t3_q3dv02,1633634094.0,False
q3dv02,"They have an oscillating signal (high current then low)  that drives the whole CPU and computer. Every time it's emitted the CPU transitions from one state to the next - many sub-systems are connected to this master signal and do their own transitions. so technically the computer knows about steps that are created by a special circuit, it is only able to tell time because that circuit emits these signals at a regular frequency (by for example relying on vibrational resonance), but if that clock was random the computer could still function in a general sense

Maintaining a truly accurate mapping between the pulses of the CPU clock and real world time is not trivial, and once you have multiple computers it's impossible and there has to be a synchronization correction e.g. NTP",hfrgmw9,t3_q3dv02,1633633146.0,False
q3dv02,We tell it to,hfsbayy,t3_q3dv02,1633646393.0,False
q3dv02,Every computer has an internal clock that runs at all times and counts the number of milliseconds since 1/1/1970,hftc2hm,t3_q3dv02,1633664616.0,False
q3dv02,"Since there are plenty of good answers to the question you asked in the description I'd like to muse about ""understand the concept of time"" part from your title. Computers mostly don't have anything resembling ""understanding"" in the way humans do. But the relatively young field of artificial intelligence is where I'd expect to start seeing this skill. Since AI can now create art, generate human faces, voice, and even fairly intelligible conversation, I wouldn't be too surprised if they have already achieved some level of consciousness, if not something resembling self consciousness. With this will likely come concepts and understanding similar to the behaviour of human neural networks.

For a human to understand something they generally create a mental model of that thing. This could simply be visuals and sound one hallucinates in their imagination, or in the case of more abstract concepts like time, we lean heavily on narratives. We develop a kind of abstract mental model by telling a story about how the thing we understand behaves. Time falls very much in this approach since most people don't understand time as a dimension of the universe but through our experience of a sequence of events. When we visualize that sequence as having regular repeating intervals we can apply our understanding of measurement to this abstract concept of time. We basically tell and hear stories to understand.

This is where I think AI might diverge quite a bit from human understanding. Although AI can generate images like our minds do, and even hallucinates very similarly to how we do, I don't think the way we train neural networks has a strong emphasis on sequence and narrative. Our brains and AI both are pattern matching machines, but AI tends to be given very fixed static stuff to train on, like a database of billions of images in no particular order and without any story to describe their relationship except the patterns of pixels it picks up on. Whereas the training we get for our minds is always in the context of our ongoing experience of the world. Our pattern recognition is always factoring in many more dimensions than just a 2d image for instance.

So all that said. In my humble opinion, I have no fucking clue how a computer would understand time if it could, but I highly doubt it would be comparable (on a neural network level) to how humans understand it.",hfsakqp,t3_q3dv02,1633646048.0,False
q3dv02,"Oh come on. Stop this nonsense. No AI we have produced is conscious, our machine learning models are basic calculus and basic linear algebra. They operate in no way like any brain ever produced by nature so far, conscious or otherwise. You don't understand AI or human biology: stop being a crack pot.",hftn1pw,t1_hfsakqp,1633671776.0,False
q3dv02,"Yikes, I understand the world is tense now, but damn I'm just saying I'd be surprised but not ""too surprised"" like not shocked, if we some day found out current gen AI had something resembling consciousness. Calm down man this is just Reddit, take your emotionally abusive dad energy elsewhere. Seriously you should probably look into therapy.",hftr0ja,t1_hftn1pw,1633674836.0,False
q3dv02,You don't get to spout nonsense about things you barely understand and then insult people who call you out on being a bullshitter. Stop being a crack pot. You can wax philosophical about time being a dimension but since you've already shown yourself to be full of shit I feel safe assuming you have absolutely no understanding of tensor calculus and general relativity either. Don't drop your nonsense bags here and expect people not to call you out for it.,hftsgph,t1_hftr0ja,1633675981.0,False
q3dv02,"Now you’re deflecting and projecting. I feel sorry for anyone who has to suffer by having you in their lives. You seem unable to accept having differing opinions on something even when someone clearly presents their ideas as opinions and musings, nothing close to truth claims. Honestly you somehow think insulting my intelligence and calling me a crackpot is a normal human reaction to me simply speculating on consciousness, something completely unknown that neither of us understand? Why don’t you actually offer something in response to my speculation? If you’re so convinced it’s wrong, why not engage in an interesting and dynamic conversation about points that lead you to that conclusion? I assume you didn’t because you’re just a bully who thinks you’re smarter and better than others as a defence because you lack any control on your irrational anger at things you don’t understand. Get therapy.",hfuqcpd,t1_hftsgph,1633700691.0,False
q3dv02,"Computers use quartz crystal to keep track of time.

Humans use crystals to warp time.",hfty0ek,t3_q3dv02,1633680842.0,False
q2qkic,"[Ben Eater's channel](https://www.youtube.com/channel/UCS0N5baNlQWJCUrhCEo8WlA) has some good videos - specifically [division](https://www.youtube.com/watch?v=v3-a-zqKfgA), [addition](https://www.youtube.com/watch?v=wvJc9CZcvBc) and [negatives](https://www.youtube.com/watch?v=4qH4unVtJkE)

Hope that helps!",hfmz9x6,t3_q2qkic,1633547080.0,False
q2qkic,Does he also work with number representation both binary and non binary ???,hfn4eyi,t1_hfmz9x6,1633549215.0,True
q2qkic,slightly - the division video is exploring decimal -> binary representation but the channel's mainly about the 8 and 16 bit computers he's built and exploring the knowledge required to explain each section,hfnab7r,t1_hfn4eyi,1633551678.0,False
q2qkic,The book *Code* by Charles Petzold,hfocqol,t3_q2qkic,1633569462.0,False
q2pjf4,Perhaps you'd have more luck at r/alevel or r/6thForm,hg2g5j6,t3_q2pjf4,1633840362.0,False
q2pjf4,Thank you so much. I'll look into it.,hg57u9r,t1_hg2g5j6,1633896936.0,True
q2pjf4,"It would depend on the exact course, computer science exam questions could be a lot of subjects so the question is kind of vague",hfmy8i4,t3_q2pjf4,1633546653.0,False
q2pjf4,uhhhh. I mean like. This is the book.(Computer science for cambridge internationl AS and A level Coursebook). i wanted answers for this books exam style questions. If this helps.,hfmytj0,t1_hfmy8i4,1633546892.0,True
q2oi78,"I used to agree with you. For a 30 year career as a software engineer i agreed with you.  About 10 years ago i got into recreational math, and my programming ability suddenly seemed to have purpose beyond CRUD and office Workflow apps.

Forget the experience of the college courses and go pick up a book by Smullyan or Matt Parker and have some fun.",hfn9f73,t3_q2oi78,1633551310.0,False
q2oi78,">Smullyan

I don't think the Silmarillion has much math in it",hfqnqt6,t1_hfn9f73,1633621353.0,False
q2oi78,Any recommendations?,hfpu45y,t1_hfn9f73,1633606968.0,False
q2oi78,"Other than the stuff already mentioned above look up project euler for math stuff. You can learn a lot about numerical algorithms if you search the non-trivial ways to solve those questions (after attempting your own solution of course lol). 

My personal favorite thing to do is generative art which can be math heavy depending on what you want to do. Coding train channel on YouTube has plenty of good videos or check out r/generative for inspiration. 

Another suggestion is computational stats if you’re interested in that side of maths. As someone with a programming background I only understood basic stats concepts like sampling distributions after I wrote a for loop to literally generate the samples and see the outputs.",hfq29qp,t1_hfpu45y,1633611615.0,False
q2oi78,"Smullyan has a bunch of books on recreational math and logic. My wife got upset with my ""organizational"" method (piles of books everywhere) so I don't have my recently read ones at hand but here's [his bibliography](https://en.wikipedia.org/wiki/Raymond_Smullyan#Selected_publications). *What Is the Name of this Book?*, *The Lady or the Tiger?*, and *To Mock a Mockingbird* are ones I can specifically recommend.",hfqriid,t1_hfpu45y,1633622886.0,False
q2oi78,"http://abstract.ups.edu/

A college course book but with some fun excerises.",hizbck8,t1_hfpu45y,1635832266.0,False
q2oi78,I hated math and then i learned how to program (working professionally) before getting a cs degree and then when i studied stuff like calculus afterwards it was 1000x easier because i just related the concepts to programming,hfnbjap,t3_q2oi78,1633552194.0,False
q2oi78,"That's how I feel, obviously I'm still early in my programming journey but I found that calc has helped my understanding",hfni9p6,t1_hfnbjap,1633555097.0,False
q2oi78,"Same, when I got into series I just thought of them as for loops",hfnphfx,t1_hfnbjap,1633558342.0,False
q2oi78,Hope I feel the same too :),hfo4csq,t1_hfnbjap,1633565535.0,True
q2oi78,IMHO everyone loves math itself but no one likes bad math teachers.,hfo1y1x,t3_q2oi78,1633564368.0,False
q2oi78,Exactly!!! Had one in high school and it ruined everything :(,hfo4rgi,t1_hfo1y1x,1633565730.0,True
q2oi78,"I hated calc, then i had this great teacher who really took the time too come to my level and understand what i knew, he was able to say just a few well targeted sentences and suddenly all my confusion evaporated.

Calc is now a core part of my regular work, good teachers are everything",hfopcaa,t1_hfo4rgi,1633575855.0,False
q2oi78,"If you hate maths and love programming, you've never had a good maths teacher or a good attitude.",hfo09mm,t3_q2oi78,1633563534.0,False
q2oi78,Real Analysis with a bad lecturer can doom degrees,hfonn08,t1_hfo09mm,1633574856.0,False
q2oi78,Most degrees are pre-doomed with this “trick” based learning that happens in most US high schools. I still remember that it wasn’t until calculus in college that I realised that math was meant to mean something.,hforkms,t1_hfonn08,1633577131.0,False
q2oi78,Id say for me it was attitude. In hs I completely ignored math because for some reason I thought that law was my only path. I really wish I put more effort into those courses. It would make my path to getting into a CS program much shorter 😭. I’ll get it done though. I’m only 19.,hfu24f1,t1_hfo09mm,1633684620.0,False
q2oi78,You might be right on that,hfo4j4b,t1_hfo09mm,1633565619.0,True
q2oi78,"I don't hate math, but I never enjoyed math class, and I know for a fact that it was because my math teachers all sucked ass. I love programming.",hfolgvj,t1_hfo09mm,1633573658.0,False
q2oi78,"Well I believe that maths and programming  go hand in hand. Sure, you might not be using complicated integrals to create a bubble sort program, but programming involves lots of logical and abstract thinking, which can also be found in mathematics. Also, most of the times when you're creating a program you'll need to implement functions that do basic arithmetic, which is, of course, math. So in my opinion if you like programming, you like solving problems that require this kind of reasoning, and therefore you're being mathematical in a way",hfmi65s,t3_q2oi78,1633539821.0,False
q2oi78,"I love discrete math and calculus. Also, it’s hard to deep dive into data structures and algorithms analysis, without a good understanding of math.",hfnfxsv,t3_q2oi78,1633554078.0,False
q2oi78,"I'm here, who loves math but not programming. >.>  I see programming as the boring bit that lets me do the cool math.",hfmzx81,t3_q2oi78,1633547345.0,False
q2oi78,Same here. I've always found concepts more interesting than pure programming.,hfo9noh,t1_hfmzx81,1633568023.0,False
q2oi78,"Kind of in the same boat. I find CS interesting, but I find math much more interesting.",hfnvoj7,t1_hfmzx81,1633561255.0,False
q2oi78,I am the complete opposite haha,hfo12q6,t1_hfnvoj7,1633563937.0,False
q2oi78,"char boat[100] = ""same"";",hfo49o2,t1_hfo12q6,1633565493.0,True
q2oi78,Same,hfo1l37,t1_hfo12q6,1633564191.0,False
q2oi78,"I don't see how CS isn't math

https://en.wikipedia.org/wiki/Computer_science

 
Software Engineering on the other hand: that's certainly not a branch of theoretical mathematics. Gathering requirements isn't very mathy but reducing space and time complexity and asking questions about computability definitely is.",hfo9riv,t1_hfnvoj7,1633568073.0,False
q2oi78,I agree. An additional argument is that algorithms are proven theoretically with maths in computer science. This is a dead giveaway of the close relationship between CS and maths.,hfp6opq,t1_hfo9riv,1633586768.0,False
q2oi78,"**[Computer science](https://en.wikipedia.org/wiki/Computer_science)** 
 
 >Computer science is the study of algorithmic processes, computational machines and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software. Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hfo9t6v,t1_hfo9riv,1633568095.0,False
q2oi78,"I used to agree with this distinction, but have found that most software engineering researchers would content that they belong in CS depts/consider themselves  to be computer scientists.  I feel the meaning of Computer Science has shifted over time as entire disciplines of applied/swe research have appeared.",hfq54rl,t1_hfo9riv,1633613069.0,False
q2oi78,"I think that phenomenon is less evidence of CS shifting and more evidence of SE coming into its own. After all, the maturation of Mechanical Engineering as a discipline hasn't forced physics researchers to shift. It's its own discipline just like SE.

I would tend to agree that SE and CS belong in the same department the same way physics and mechanical engineering being in the same department.

I don't think CS as a field of study is becoming something that we call SE as the people who want to do pure CS will want to keep doing that while the people who want to write good software without worrying about Turing Machines will also want to keep doing what they're doing.",hfqma0n,t1_hfq54rl,1633620737.0,False
q2oi78,">I would tend to agree that SE and CS belong in the same department the same way physics and mechanical engineering being in the same department.

Except this is my point --- Mechanical Engineering and Physics are rarely in the same department; at many universities, they're in separate schools/colleges entirely!  This is similarly true for Chemistry/Chemical Engineering, Biology/Biomedical Engineering, etc.  

The closest we see to ""theoretical and applied living under the same roof"" is in Applied Math and Pure Math.  

I don't see schools treating CS as purely the theoretical side and Software Engineering as its own separate thing off in the corner --- otherwise, we'd start to see a plethora of ""Software Engineering"" degrees instead of ""CS Degrees.""  Instead, ""Computer Science"" is treated as a parent topic that has ""Software Engineering"" as a child.",hfr41ro,t1_hfqma0n,1633627967.0,False
q2oi78,I like both. Math interests me more but I like how the two areas can combine or maybe theoretical comp sci and mathematics. Only in my first year of my degree but depending on what I like more will decide whether I go down the road of academia/research or not. Don’t think I’ll be able to take any theoretical stuff till I transfer for my junior year though (unless discrete mathematics counts).,hfogp01,t1_hfmzx81,1633571261.0,False
q2oi78,[deleted],hfmu966,t3_q2oi78,1633544954.0,False
q2oi78,*cries in machine learning algorithms*,hfop1lw,t1_hfmu966,1633575671.0,False
q2oi78,"Programming is not computer science - many unis fuck this relationship up.
 
Computer science *is* maths. No ifs buts or maybes about it, it **is** maths, inherently and indivisibly.
 
Software development and programming is not computer science any more than electrical engineering is physics. Just like electrical engineering *uses* physics, software development and programming *uses* computer science (and thus maths).
 
You need to do a bit of it to have a basic understanding when you're learning, but you don't need to live there for your whole career.",hfneqqh,t3_q2oi78,1633553568.0,False
q2oi78,"Yes. Programming and CS aren’t the same thing. 
You can program without CS and I have seen a lot of CS graduates that can’t program worth beans.",hfoaxl0,t1_hfneqqh,1633568610.0,False
q2oi78,"> Computer science is maths.

Yes, but you can still like math but not computer science. I like math a lot. I got my degree in it. But some branches of math, like differential equations, I don't particularly enjoy. So I can say I don't like differential equations, but I do like math, because differential equations is just a small subset of math. And there are far too many branches of math for anyone to enjoy all of it.",hfpbfco,t1_hfneqqh,1633590752.0,False
q2oi78,"That's like saying 'I like cricket, but I don't like sports'. It's a complete contradiction. You might say 'I don't like *some* sports', but not sports in general.
 
There are so many things involved in computer science that it touches maybe 50% of maths overall, so it really is a case of 'some'.",hfq9kfy,t1_hfpbfco,1633615193.0,False
q2oi78,"> That's like saying 'I like cricket, but I don't like sports'.

Nope. Read what I said again. I did not say 'I like differential equations, but I don't like math'. I said 'I like math but I don't like differential equations'. If not enjoying a tiny part of math means you don't enjoy math as a whole, then no one enjoys math, because there's just too many different branches of math for anyone to enjoy all of it. Likewise, if someone does enjoy a tiny part of math, but not all of it, I would still say they enjoy (a specific subset of) math.",hfqo5oz,t1_hfq9kfy,1633621526.0,False
q2oi78,"No, the comment you responded to is saying, ""i like sports but i don't like cricket""

Understanding logic, logical statements, and how sets/subsets work is imperative understanding mathematics.",hfqaibv,t1_hfq9kfy,1633615630.0,False
q2oi78,preach!,hfo02iy,t1_hfneqqh,1633563433.0,False
q2oi78,Here,hfn3p55,t3_q2oi78,1633548917.0,False
q2oi78,I love math but I hate proofs.,hfoct6c,t3_q2oi78,1633569494.0,False
q2oi78,Proofs are like 99% of math once you get past lower division classes.,hfpb3xc,t1_hfoct6c,1633590470.0,False
q2oi78,"I went all the way up to Calc III, Discrete Math II, and Matrix and Linear Algebra.      
The only math classes of those that really incorporated proofs was Discrete math.",hfpbe8q,t1_hfpb3xc,1633590724.0,False
q2oi78,Yeah that’s all lower division level math,hfquhnw,t1_hfpbe8q,1633624091.0,False
q2oi78,What are the upper level maths?,hfqx262,t1_hfquhnw,1633625120.0,False
q2oi78,"Usually starts with real analysis and abstract algebra. Complex analysis, topology, number theory, etc. There’s subfields and usually simpler subjects get reintroduced as well, so youll see stuff like differential equations but from an analytic perspective, algebraic topology, functional analysis… there’s a lot of math",hfqynkp,t1_hfqx262,1633625762.0,False
q2oi78,I’m on team hate proof.  But my linear algebra course was heavy on proofs. At least my professor was,hfq6nkt,t1_hfpbe8q,1633613813.0,False
q2oi78,"Really? I don't recall ever doing proofs in my linear algebra class, thank God lol.     
We went over proofs in my Calc 1,2, and 3 class but they were never on the tests or homework.     
But my Discreet 1&2 classes were soooo damn heavy on the proofs.",hfqa3f9,t1_hfq6nkt,1633615438.0,False
q2oi78,"I actually hate thinking and existing, but I still do it.",hfoiwjk,t3_q2oi78,1633572326.0,False
q2oi78,"At one point I thought I didn't like math, because of some mind-numbingly boring classes in high school. I always did just enough to get a decent grade but never felt I actually liked it. Then when I got older I realized that what I didn't like was just the way it was taught and the highly theoretical nature of most of what we saw. What I do enjoy is the process of abstracting some solution to a problem so that it becomes a formula you can use to solve all other problems of the same kind, and I feel like that's basically what programming is mostly about. It's math, but structured in a more intuitive and practical way. Now it's one of the subjects that interests me most, kinda weird how that worked out",hfo6voz,t3_q2oi78,1633566721.0,False
q2oi78,"Damn, that's almost relatable. Though I am still not in love with maths and there's still time for it ( considering that I am in my 2nd year of University), I had always liked the linear method of solving problems, when you  have a defined step/formula to reach the solution. And like you said, I guess our experience of maths always depends on the teacher and their methodology too.",hfo99ps,t1_hfo6voz,1633567841.0,True
q2oi78,"Almost changed majors cuz of calc 2. Literally getting destroyed rn lol Then I stumbled on a cool game I programmed years ago that reminds me of why I love CS. Everyone says stick with it but online learning is def not the way, well for me at least.",hfp2kmh,t3_q2oi78,1633583700.0,False
q2oi78,"Cool!!!
Yea, its the reason why jumped into CS - the art of making things that doesn't even exist (physically), yet so powerful... I always had imagined how all these softwares work and I consider them nothing short of magic (and if I brewing magic, I'm a magician myself right? lol). But, now Im in the middle of University years and maths is hitting me real hard...",hfp4huu,t1_hfp2kmh,1633585081.0,True
q2oi78,Dude yea! CS majors are people that eventually turn into wizards lol It’s pretty remarkable what a silicon rock with electricity running through it can do. Obviously there’s more to it but ya know what I mean. Lots of wizards here say maths makes the transformation from human to wizard easier so I’ll have to take their word for it. Soon you and I Reddit friend will be wizards,hfpa29q,t1_hfp4huu,1633589563.0,False
q2oi78,"Yeah. I do like math, but I like programming much more. Although, the only kind of math I like is CS theory.",hfn6tki,t3_q2oi78,1633550222.0,False
q2oi78,[deleted],hfp1q38,t3_q2oi78,1633583115.0,False
q2oi78,"Well, I have been searching for a niche in CS for a long time, and I had liked app dev and game dev the most (not able to decide what to pick lol). I agree that you gotta know some 3D physics for game dev, but for app dev I didn't think that you actually need math at all...like there's just the use of proper syntax,basic logic and design.
Correct me if I am wrong 😁",hfp2o7r,t1_hfp1q38,1633583768.0,True
q2oi78,Logic is math,hfpry4m,t1_hfp2o7r,1633605519.0,False
q2oi78,"I hate calculus for the most part, however i do enjoy algebra and geometry. My father tought me well and i got a knack for it

Anyhow, i do get interested in things when i find a use for it. Recently i digged into RiemannSum because an objects movement was described by a curve, and i found it pretty cool to study",hfnrplw,t3_q2oi78,1633559345.0,False
q2oi78,"I love both.. Given a choice, I’d do math for a living!",hfoa544,t3_q2oi78,1633568246.0,False
q2oi78,You don’t need a lot of math in a lot of programming. Basic math will go a long way in regular business programming.,hfoak92,t3_q2oi78,1633568442.0,False
q2oi78,Nice to hear that lol,hfobveq,t1_hfoak92,1633569054.0,True
q2oi78,"It depends on the business. If you are an aerospace company then some of those programmers need a lot of math. 
Same if you write high volume automated trading engines. 

Most of us don’t do that.",hfod1r7,t1_hfobveq,1633569601.0,False
q2oi78,"Here, I like geometry, that’s the extent I like about math lol 😂",hfolzj5,t3_q2oi78,1633573953.0,False
q2oi78,Lmao facts,hfoxisb,t3_q2oi78,1633580554.0,False
q2oi78,I love maths and programming but not stats fcuk stats.,hfph19u,t3_q2oi78,1633596030.0,False
q2oi78,I want to be a programmer but am not that good at math,hfpoec8,t3_q2oi78,1633602836.0,False
q2oi78,"If there's a will, there's a way😌",hfptsqx,t1_hfpoec8,1633606763.0,True
q2oi78,I hate dumbasses who pose as maths teachers and i have a feeling everyone on this sub/the whole wide world agrees,hfpq1xs,t3_q2oi78,1633604135.0,False
q2oi78,"Lol, can't agree to this more😂 I love how this thread turned to programming/maths to targeting maths teachers",hfpratb,t1_hfpq1xs,1633605052.0,True
q2oi78,"Well, I always liked math (and consequently double majored). However, something I noticed among my math-hating CS classmates was that what they meant by math was largely the calculation side of math. They may have been competent, or even good at it, but they hated algebra (as they learned it in school), calculus, linear algebra, and similar things.

There's another side to math that's more focused on structures and logic. That is, abstract algebra, discrete math (combinatorics, graph theory), set theory, etc. Most of them never really saw these subjects (outside of discrete) and so they never had a chance to see some of the areas of math that more strongly related to computer science, or they didn't see it in enough detail to have a chance to figure out if they liked it or not. I will say, our discrete math teacher was also *awful*, a CS professor pretty much reading from the book. Those who took a couple math electives with me ended up also double majoring once they'd seen what math actually could be (that is, it was taught by mathematicians who would do more than just read from a book).",hfqsod3,t3_q2oi78,1633623360.0,False
q2oi78,"Just started my BS.c in Computer Science (it is my 2nd degree). I had 6 years to interact with maths (since high school) and I have to say politely to kill me. 

Have a great academic year everyone",hfqum29,t3_q2oi78,1633624140.0,False
q2oi78,"Programming without math appears to be easy initially, but immediately I would think there were severe limitations without a strong logical background. 

ex: using AI automated programming without any idea how it functions.",hfrfo2t,t3_q2oi78,1633632741.0,False
q2oi78,"i love both but there are some things i just dont get behind that ruin my grades: mathematical proofs 
i can do all the calculations no problem but proofs i just cant do no matter how hard i try to understand it",hg3v909,t3_q2oi78,1633876309.0,False
q2oi78,"""I hate math."" 

""What? But you love programming!"" 

""Exactly. I program a calculator and have it do the work for me.""",hfnagw9,t3_q2oi78,1633551743.0,False
q2oi78,Haha lol,hfo4koe,t1_hfnagw9,1633565640.0,True
q2oi78,I enjoyed math until I hit calc and now I just hate it. I understand the concepts and all but fuck is math dry as shit.,hfnhydr,t3_q2oi78,1633554960.0,False
q2oi78,For me saying that you hate math is weird because it is the language of the universe. It’s like saying you hate life.,hfndl8g,t3_q2oi78,1633553079.0,False
q2oi78,I thought hate was the universal language…,hfne75i,t1_hfndl8g,1633553339.0,False
q2oi78,Never heard that,hfng67g,t1_hfne75i,1633554179.0,False
q2oi78,How many maths are we talking about?,hfneo44,t3_q2oi78,1633553538.0,False
q2oi78,YSK that only the US and Canada shorten 'mathematics' to 'math' without the s.,hfns1hr,t1_hfneo44,1633559498.0,False
q2oi78,"Yea I’ve traveled, it’s a joke",hfnsyza,t1_hfns1hr,1633559930.0,False
q2oi78,Math is the best part for me. Doing projects without math becomes boring pretty quickly IMO.,hfnkaxf,t3_q2oi78,1633555995.0,False
q2oi78,I study for cs I hate math and idk so much :)) do you think can I finish this school guys,hfpeb9g,t3_q2oi78,1633593389.0,False
q2oi78,We all can 💪,hfplee6,t1_hfpeb9g,1633600192.0,True
q2oi78,"If I can, so can you!",hfphv74,t1_hfpeb9g,1633596844.0,False
q2oi78,Aye!,hfna2oh,t3_q2oi78,1633551580.0,False
q2oi78,I like maths but I find it a lil tough,hfpi9jl,t3_q2oi78,1633597234.0,False
q2oi78,"It depends on the programming your doing, but the closer to the metal you get the more math you are relying on.",hfqa3wy,t3_q2oi78,1633615444.0,False
q2oi78,I love both. I write games.,hfqft2s,t3_q2oi78,1633617986.0,False
q2oi78,Hola game dev👋,hfqh2fg,t1_hfqft2s,1633618530.0,True
q2oi78,"I love math and I like programming. But I hate what I've started calling ""commercial programming"". That is, CRUD, MVC and making API's.
It's literally just translating business rules to code and it's extremely boring and unrewarding.",hfqhjbp,t3_q2oi78,1633618732.0,False
q2oi78,"Actually the opposite.
Not flexing, but I'm more good in mathematics than programming though I'm in my graduation in computer science engineering.
But the programming I know is only because I'm good at Maths",hfqief6,t3_q2oi78,1633619102.0,False
q2oi78,"Maths can really be fun, but learning the basics can be difficult and ... not so fun. 

But I am sure there are some branches you could actually like if you studied them -- maths is much much more than what you have to learn in standard calculus and linear algebra courses.

Stay on it and work your way to the fun parts, and don't give up because of your frustration.",hfqo0zg,t3_q2oi78,1633621471.0,False
q2oi78,Fuck yes!,hfqoewt,t3_q2oi78,1633621632.0,False
q2oi78,This is an invalid question. Programming is discrete math.,hfo679e,t3_q2oi78,1633566397.0,False
q2h7gz,"This is already happening in some respects. I know a fair number of companies that don't have any word processor or spreadsheet software installed on their machines, it's all Google Docs in the cloud.

But this also has its drawbacks. You need a steady internet connection, which not all countries/locations have. There are possible security concerns with having your data in a nameless server somewhere rather than on your internal network (I suspect the defence industry, for example, would be very reluctant to move to cloud-based tech). Some software also requires huge amounts of data (think 3D CAD stuff, or game dev with huge amounts of media assets) which might make using the cloud harder.

Finally, presuming it would just be a few cloud-based companies offering such services (and the internet has a habit of 'winner takes all' when it comes to new markets), there's also a danger of half the companies in the world being unable to do any work if the service goes down. Think about this week where Facebook and Instagram disappeared (which wasn't a huge loss), but it also took WhatsApp offline, which is the de-facto means of mobile communication for lots of companies and individuals outside of the US. Similarly, an outage at one of the big hosting companies earlier in the year took vast swathes of the internet offline, just because so many companies used it.",hfl5icu,t3_q2h7gz,1633515370.0,False
q2h7gz,"No because there are always going to be apps you want to use offline.

/edit and apps can do things that browsers might not allow, like notifications while the browser isn’t open.",hfl54id,t3_q2h7gz,1633515031.0,False
q2h7gz,[deleted],hflimok,t1_hfl54id,1633524165.0,False
q2h7gz,"They might expand their capabilities for simple things, but I would never anticipate a web browser to expose certain operating system functions to web application. For example I would never want my browser to let some JavaScript execute a process on my machine. But a dedicated desktop app can do that.",hflrhj5,t1_hflimok,1633528611.0,False
q2h7gz,"Mostly speculation, as it really depends on the changes in hardware plus infrastructure. 

Personally I believe we will not see that in the living generations. There has been a bit of a cap to the rate of increase in technology. 

Web is already massive though, so the transition has started some into that direction. Just think through all the pages you visit on a daily bases, even companies with mobile apps tend to have a web app that is responsive for mobile to some degree.",hfl507l,t3_q2h7gz,1633514925.0,False
q2h7gz,"You could (and people did) ask the exact same question 20 years ago, but non-web apps are still here today.",hfmaghc,t3_q2h7gz,1633536656.0,False
q2h7gz,"More abstractly, all computing requires are 1) compute and 2) data. Having those on the cloud just means compute and data are further away from the user's device. There are tradeoffs, governed by current technological limitations which are hard capped by physical limitations. 

So will web apps that offload most of the compute and data to a remote server replace the local compute and server? For a lot of use cases, the tradeoff make sense. But as mentioned above in other responses, certain use cases and requirements would make more sense to use the local compute and data. The limitations that are due to current technology may eventually be solved, but limitations that are hard capped will likely never be solved (unless we solve some fundamental ways to do compute or networking e.g. quantum compute and entanglement)",hfmdakf,t3_q2h7gz,1633537827.0,False
q2h7gz,"No. The iPhone didn’t take off until it offered native, installed apps. 

There are still lots of parts of the US where cell signal is spotty, not to mention cell signal inside of big box stores, for example. So locally installed applications that can run offline will be important for a long time.",hfmqg9l,t3_q2h7gz,1633543237.0,False
q2h7gz,might be the same as the age old would TV eliminate Cinema question,hfn1srd,t3_q2h7gz,1633548124.0,False
q2h7gz,"Many applications require high throughout to hardware such as audio and video recording, mixing and editing, data acquisition and machine control, patient monitoring, etc. These may fetch application executables from remote servers but they will not be sufficiently fast to be web or cloud hosted, at least with current network bandwidths.",hfwoq7h,t3_q2h7gz,1633730434.0,False
q2h7gz,"[https://www.kaiostech.com/](https://www.kaiostech.com/)

web apps exist for the convenience of code base/ hiring web devs. web apps are not a good idea in terms of functionality.",hg7sgbf,t3_q2h7gz,1633950950.0,False
q2h7gz,"Most software isn't graphical, so no. If you mean only graphical software, God I hope not, and also still probably no.",hfm3wkd,t3_q2h7gz,1633533976.0,False
q2h7gz,I think you can get a free upgrade from MS-DOS to Windows 11. You should probably get on that.,hg7s51x,t1_hfm3wkd,1633950709.0,False
q1txed,Link to the referenced paper: https://link.springer.com/chapter/10.1007/978-3-030-22479-0_6,hfgvwkh,t3_q1txed,1633434935.0,False
q1txed,Constant spying by mobile devices seems like it would be pretty easy to detect via things like temperature and variance in battery life.,hfj0kjs,t3_q1txed,1633468733.0,False
q1txed,"“Are apps listening to people's conversations to improve ad targeting?” I was contacted by leading Italian newspaper @repubblica to comment on this controversy. Find my response in this thread.
\#privacy \#dataprotection \#spying \#smartphones \#listening \#ads \#surveillance \#apps 1/n 

***

posted by [@JL_Kroger](https://twitter.com/JL_Kroger)

Photos in tweet |  [Photo 1](http://pbs.twimg.com/media/FA7I_7gWEAIBKAh.jpg) | [Photo 2](http://pbs.twimg.com/media/FA7I_73XMAMYzxC.jpg) 

^[(Github)](https://github.com/username) ^| ^[(What's new)](https://github.com/username)",hfgqk9s,t3_q1txed,1633431158.0,False
q1txed,"I don't see how this is a _surprise_

There are apps on your cellphone that say in their TOS that they use specific ad companies that do exactly this.

All someone really needs to do is hack into a cellphones always-on microphone, or get any of the specific sensor data that can potentially allow for this.

In order for Google, Microsoft, and Apple to even be able to give you the ""always at the ready"" assistant, that means they have to be listening to the mic at all times.  It's entirely possible that these companies are also listening in, considering they made the software that runs on the phone.

I'm not saying they actually are doing this, just that it's not outside the realm of possibility.",hfj2zn1,t3_q1txed,1633469805.0,False
q1txed,whats your proof?,hfid8j5,t3_q1txed,1633458807.0,False
q1txed,"So the tweet itself references a research paper. Here is a quote I’ve taken straight from the abstract:

>	Based on previous research and our own analysis, we challenge the widespread assumption that the spying fears have already been disproved. While confirming a lack of empirical evidence, we cannot rule out the possibility of sophisticated large-scale eavesdropping attacks being successful and remaining undetected. Taking into account existing access control mechanisms, detection methods, and other technical aspects, we point out remaining vulnerabilities and research gaps.

The point of this paper isn’t to provide definitive proof of spying, but to illustrate the point that the fears are not entirely unfounded. It is entirely possible, with current technologies and methods, to secretly eavesdrop on phone users and escape detection while doing so. They do openly admit, however, the lack of any empirical proof of this occurring. 

So this is a case of reminding us that “The absence of evidence isn’t the evidence of absence”. I would imagine a broader point trying to be made in terms of security research is that eavesdropping by smartphones isn’t something to be ruled out, either.",hfj158f,t1_hfid8j5,1633468982.0,False
q1swpn,"Space complexity is non-trivial in the big data industrie. There algorithms of time and space complexity n are not good enough.

They use techniques/algorithms like:

- Hyper log log counter
- Bloom filters

It's the world of creating great estimation algorithms, reducing lookup times, and smart use of cache.

Edit: So to answer what algorithms have a space complexity that is non-trivial. All algorithms with a space (or time) complexity of O(n) are non trivial.",hfgnbcj,t3_q1swpn,1633428431.0,False
q1swpn,"Ah yes, thank you for pointing this out!",hfgo2x7,t1_hfgnbcj,1633429112.0,True
q1swpn,"Not 1:1 related, but there's a whole branch of ""sub linear algorithms"", to be used when the data is just too big to handle.

As for algorithms needing GC, i think some wait/lock-free algorithms are impossible to implement without GC.",hfgorwm,t3_q1swpn,1633429697.0,False
q1swpn,"Indeed, I was not aware of the, for example, [USTCON](https://en.wikipedia.org/wiki/SL_(complexity)) problem which admits a log-space algorithm!",hfh63x0,t1_hfgorwm,1633440488.0,True
q1swpn,"I bring two cases that are a bit different than your examples:

1. Graph algorithms using matrix manipulations (matrix multiplication). Such algorithms may need to transform the graph from an input format to a matrix and back.
2. We can look at space complexity from the other side. Consider structures that are close to optimum. There are different limits we can reach: [https://en.wikipedia.org/wiki/Succinct\_data\_structure](https://en.wikipedia.org/wiki/Succinct_data_structure).

I did my master's thesis on the topic of the succinct representation of unlabeled ordered trees. Since the optimum is `2n - o(n)` bits, we are allowed to use `o(n)` extra bits (where little-oh (`o`) denotes ""asymptotically less""). One natural representation is using the equivalency of trees and balanced parentheses. The hard part is to equip this simple structure with additional ""indices"" that use `o(n)` extra memory to perform operations (navigation, degree or depth or height of a node, lowest common ancestor, ...) in constant time.",hfgpinr,t3_q1swpn,1633430315.0,False
q1swpn,"**[Succinct data structure](https://en.wikipedia.org/wiki/Succinct_data_structure)** 
 
 >In computer science, a succinct data structure is a data structure which uses an amount of space that is ""close"" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations. The concept was originally introduced by Jacobson to encode bit vectors, (unlabeled) trees, and planar graphs. Unlike general lossless data compression algorithms, succinct data structures retain the ability to use them in-place, without decompressing them first. A related notion is that of a compressed data structure, in which the size of the data structure depends upon the particular data being represented.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hfgpjbq,t1_hfgpinr,1633430331.0,False
q1swpn,"Succinct data structures seem very cool, thanks for sharing :D",hfh7qyv,t1_hfgpinr,1633441282.0,True
q1g1gh,"If your real target is a network router and is close by, possibly...and by close by, I mean in the same AS.  If your real target is not a network router/device, it won't happen, pretty much ever...  If your real target is a network device on another AS, you would need to either source-route the packet (which you probably don't want to do given your question), get incredibly lucky, or monitor/map all of the AS peering points between you and the real target.  Not easy.  Likely not even effectively possible in practice.",hff3ikv,t3_q1g1gh,1633391379.0,False
q1g1gh,"If the destination ip of the packet doesn't match an IP assigned to the device that receives it. The device will not process the rest of the packet. If the device is a router it will forward the packet. If it is not a router it will drop the packet, unless it's NIC is in promiscuous mode.",hfffz48,t3_q1g1gh,1633397679.0,False
q1g1gh,[deleted],hfgbo2w,t3_q1g1gh,1633417247.0,False
q1g1gh,"Yes, I did. End up on this eventually [https://en.wikipedia.org/wiki/A\*\_search\_algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm)

But it is just a scratch of the surface.I am seeking for algorithm spec used to assign an ip address to devices (public ones).

Maybe with it I can find a way to create oracle algorithm to find suitable address for routing path.",hfia7a5,t1_hfgbo2w,1633457632.0,True
q16owj,"There’s so much wrong with that last paragraph.  I really recommend that you start with some basic memory concepts/pipelines and follow through what is actually moving around at each point.   You seem to be confused about “word addressing” and a few other things. 

You don’t get to divide the blocks yourself\* >!(you can but probably aren’t)!< but you can use memory aligned data types (or various offset types that are advantageous)…

…it kinda sounds like you’d be interested in knowing how pointers work…they aren’t just random numbers.  Same idea at the hardware level.

…there was a good visualization of how memory is addressed in one of those crash course computer science pbs videos I’m pretty sure.

Also use real numbers - these made up numbers are hurting your understanding.",hfd62ec,t3_q16owj,1633364071.0,False
q16owj,"Yes , I am totally confused. I want to know does each memory area is given a predefined address or its on implementation to how to refer it.",hfd7yzz,t1_hfd62ec,1633364806.0,True
q16owj,"Yes, they have predefined addresses at the OS & hardware level but these addresses get abstracted away by higher level programming languages which reference things like virtual memory addresses built on top of the physical addresses.",hfd8lup,t1_hfd7yzz,1633365053.0,False
q16owj,isnt virtual memory mapping part of the OS instead of programming languages?,hfe99ca,t1_hfd8lup,1633378647.0,False
q16owj,"Yes you are right, I shouldn’t have misspoke.  

I didn’t want to explain virtual address spaces.  Each process gets its own memory space except when it doesn’t.  

Worth noting that this also virtually expands your memory’s capacity into your hardrive’s space as well allowing you to easily deal with large sets of data, albeit with a significant performance penalty.",hfeehlk,t1_hfe99ca,1633380554.0,False
q16owj,"The visualization is the same for accessing an entire block of memory or word within a page within that block of memory.

Overly basic but: https://youtu.be/fpnE6UAfbtU?t=340",hfd9cu1,t1_hfd7yzz,1633365341.0,False
q16owj,[deleted],hfd9uqs,t1_hfd7yzz,1633365533.0,False
q16owj,BTW I am following this video https://youtu.be/VePK5TNgQU8,hfdaauz,t1_hfd7yzz,1633365705.0,True
q16owj,"That's good / I referred him to this more basic video section: [https://youtu.be/NKTfNv2T0FE?t=6481](https://youtu.be/NKTfNv2T0FE?t=6481)

which has another version here: [https://youtu.be/Fa6Jq0Iue3U](https://youtu.be/Fa6Jq0Iue3U)

\---

both of you would probably benefit from going through one of the free hands-on labs where you simulate building a CPU & memory from transistors making up AND gates which make up other components like switches, ALUs, multiplexers, & decoders.

or mapping your current understanding to some real microarchitecture like then one in your system looking at some core diagram like: [https://en.wikichip.org/w/images/thumb/c/c7/haswell\_block\_diagram.svg/1380px-haswell\_block\_diagram.svg.png](https://en.wikichip.org/w/images/thumb/c/c7/haswell_block_diagram.svg/1380px-haswell_block_diagram.svg.png) and then creating benchmarking tool in an attempt to get the advertised FLoPs/IoPS/Memory Thoughput performance to fill in gaps in your understanding.",hfdc84r,t1_hfdaauz,1633366441.0,False
q16owj,"&#x200B;

Thanks man for the resources many doubts are cleared. But one more as a physical address can be divided into TAG bits - LINE NUMBER bits - BLOCK OFFSET. So i was solving some problem( [youtube](https://youtu.be/PjzDNTppraQ?list=PLEbnTDJUr_IdI9QZ7bkrhMX2ZpNW0dZUo&t=232) ) where we need to find the size of the cache. So the instructor solved it by 2\^(LINE NUMBER bits + BLOCK OFFSET ) ( neglecting valid/invalid bit etc.) . So where does tag bits are stored.",hfga1jt,t1_hfdc84r,1633415883.0,True
q16owj,Wow I didn’t learn that.,hfctjcc,t3_q16owj,1633358746.0,False
q15vs5,"For the technical details on how a particular protocol functions, just search for the RFC. E.g. 'FTP RFC' will take you here: [https://www.w3.org/Protocols/rfc959/](https://www.w3.org/Protocols/rfc959/)

To see the protocols running in action, I'd suggest downloading [WireShark](https://www.wireshark.org/) or similar, select your network adaptor, and then filter on the port you're interested in. For example, set the filter to `tcp.port==21` and open an FTP session to some known site you've used before. In WS you'll soon see the login packets flying between your machine and the server, and will be able to expand each packet and see each each layer on the packet (MAC addresses, IPs, then the actual FTP text data, e.g. '`USER` [`foo@bar.com`](mailto:foo@bar.com)`\nPASS letMeIn\n`', etc.)",hfcqjcv,t3_q15vs5,1633357401.0,False
q10w5o,The fact that everybody has been totally cool with this inconsistency between language specs for decades should be the hint everybody needs as to how often you (should or do) need to use mod with negative numbers.,hfcc98m,t3_q10w5o,1633349886.0,False
q10w5o,Salient observation 😂,hfd0l17,t1_hfcc98m,1633361845.0,False
q10w5o,"Modulo is weird with negative numbers, and inconsistent between different programming languages. Most languages use the ""truncated division"" definition, which is not very intuitive to me. The vast majority of the time, I want floored division, or Euclidean division.

This Wikipedia article shows the different definitions of modulo: https://en.wikipedia.org/wiki/Modulo_operation#Variants_of_the_definition",hfbt7z3,t3_q10w5o,1633334224.0,False
q10w5o,"
It is nothing to do with math theory, it is about digital machine architecture: https://stackoverflow.com/questions/11720656/modulo-operation-with-negative-numbers",hfbt7uq,t3_q10w5o,1633334220.0,False
q10w5o,"tbh u should use mods with unsigned ints only, add the sign afterwards if required.",hfc25sc,t3_q10w5o,1633342665.0,False
q10w5o,"Yes, your question is indeed part of math theory, but, the main idea is how the modern CPU architecture is design considering the language that it uses.

&#x200B;

Modern Turing complete computers work mainly with Natural numbers (0,-1,+1,-2,+2...) and not with Real numbers . The reason for this is that computers are working mainly as **finite algorithms** so that they need a string of numbers which is finite for their calculations. In the case of Real numbers there exist infinite number after the decimal point.

&#x200B;

So why one language treats the modulo operator as a whole(10 / -4 is the same as -10 / 4), while the other is not?

If you compare the modulo operator in Python to that of C you will see that the output is different, and its mainly depends on the utility of the language and how it was built serving its purpose. While C is high level language, there is a case where 10 / -4 will leave a remainder of 2 which is very handful for complex calculations, but in Python the same remainder will always be -2 for that\`s how Python was built by theory - treat 10 as a whole Natural number and serve the denominator so that it will not be equal to zero and leave the reminder as Natural number. This explanation is in depth of every language fundamentals so you can check the Syntax of each language for more explanation how the same operators manipulate differently the data in different languages.",hfbw2l9,t3_q10w5o,1633336896.0,False
q10w5o,"You’re confused bc it is confusing. 

Just remember that the result of % takes the sign of the dividend. Period. 

% isn’t mod. It’s actually remainder even though it’s called mod. 

https://en.wikipedia.org/wiki/Modulo_operation?wprov=sfti1

https://stackoverflow.com/questions/13683563/whats-the-difference-between-mod-and-remainder

Edit: had to correct myself",hfeirql,t3_q10w5o,1633382131.0,False
q10w5o,"It's actually implementation dependent. See your wiki link:
""Standard Pascal and ALGOL 68, for example, give a positive remainder (or 0) even for negative divisors, and some programming languages, such as C90, leave it to the implementation when either of n or a is negative (see the table under § In programming languages for details).""",hfgdp1t,t1_hfeirql,1633419056.0,False
q0ta23,"The amount with which it 'kills out' data from 'left to right'. I went for the weirdly simple explanation but I think this quantifies it the best. Don't forget that activation functions go hand-in-hand with the manipulated data i.e. normalised data, where scaling from -1 to 1 and using a ReLU is quite illogical in the sense of the data flow. Choosing the right activation function is also extremely problem-dependent, but keep in mind what and how I tried to explain it myself.",hfdvvrr,t3_q0ta23,1633373762.0,False
q0m3n0,"Shell talks with terminal emulator and it still doesn't talk directly with kernel (no more than e.g. web browser does).

GUI can, but doesn't need to have any CLI connectivity",hf90v6a,t3_q0m3n0,1633283256.0,False
q0j4vh,"As far as I'm concerned, if he doesn't publicly post the reasons why it was rejected, then I'm going to assume this is a case of a big shot in the field thinking they're untouchable (or trying to take advantage of the fact that others think he's untouchable). I got the impression reading this that he either thinks or wants us to think that hey, it's Yann LeCun, he's awesome, why on earth would any conference reject his work? When in fact the conference could have articulated perfectly reasonable concerns (even if his work is great!) that he is not being up front about.

Unless they specifically rejected it because it was posted and cited on ArXiv, this this is not news. People get rejected all the time, and no one is too good for rejection.",hfa83vd,t3_q0j4vh,1633301046.0,False
q0j4vh,So where are the reviews? Maybe they make good points justifying rejection.,hf92j73,t3_q0j4vh,1633283921.0,False
q0j4vh,Isn’t NEURIPS supposed to have a doubly blind review process?,hfayd4n,t1_hf92j73,1633313815.0,False
q0j4vh,Why would that preclude Yann himself publicizing the reasons for rejection?,hfcgx9b,t1_hfayd4n,1633352624.0,False
q0j4vh,"It doesn't. But if so, it is weird to see him make a big deal out of a paper rejection. Or maybe he didn't mean to make a big deal but people made it a big deal because it's lecun.",hfjwuqr,t1_hfcgx9b,1633484271.0,False
q0j4vh,Is this seriously news material?,hf8mcmw,t3_q0j4vh,1633277205.0,False
q0j4vh,"Having read the blog post, what matters here is that the paper has already been cited 12 times, yet struggles to get officially published. The traditional conference and publication cycles are too slow. 

Aside from the obscure headline of the post, I do think this is an important issue in general.",hf8nvvv,t1_hf8mcmw,1633277862.0,False
q0j4vh,"Bad papers are often cited more than good ones (because people will pick apart the arguments).  Not saying this one is bad, but simply saying it was already cited 12 times doesn't really mean much...",hfaf2e5,t1_hf8nvvv,1633304294.0,False
q0j4vh,">  the paper has already been cited 12 times, yet struggles to get officially published.

I'm not sure this is a great metric. LeCun could probably put out garbage and get a dozen citations. But a no-name person getting a dozen citations and not getting published is a different story.",hfavz18,t1_hf8nvvv,1633312613.0,False
q0j4vh,"I agree, I think the idea was more like the actual research is way faster than the publication turnaround time. If papers are routinely getting cited before they are published, the publication and vetting system needs improvement.",hfczj7x,t1_hfavz18,1633361389.0,False
q0j4vh,"Definitely. I mean to LeCun's argument, I'm a no-name researcher, have 10 citations, and got rejected. It sucks and I know it was mostly bad reviewers, but hey man, you got an h-index that's 100x mine. I think the problem is that people like LeCun can say ""proudly rejected"" with no issues since their career is well established. The system hurts those that are the most vulnerable: the grad students. It's harder for us to take losses and wastes more of our time. LeCun isn't doing research now, hes managing. Though it does waste his grad students' time.",hfdfvmu,t1_hfczj7x,1633367830.0,False
q0j4vh,"I mean, I agree that there are issues with the conference publishing model, but appealing to authority isn't the way to solve it",hf8o35l,t1_hf8nvvv,1633277949.0,False
q0j4vh,"I actually thought this was in r/MachineLearning - where the rate of research progress is exceptionally high at the moment. 

The blog post author used a famous name in the headline for clicks, but the real substance of it is that it has already been cited a dozen times before publication, and then got rejected by the randomness of the review process, so it's going to take even longer now. 

If a paper gets cited 12 times and still hasn't gotten published, that's a problem, regardless of who wrote it.",hf8ohpc,t1_hf8o35l,1633278117.0,False
q0j4vh,"> If a paper gets cited 12 times and still hasn't gotten published, that's a problem, regardless of who wrote it.

Citations are not reviews.  The problem is to cite non-reviewed work.  And yes, sure, the review / publish cycle may be too slow, but those citations can't replace it.",hf9pyzh,t1_hf8ohpc,1633293312.0,False
q0j4vh,"I don't know why it was rejected, but I disagree about the citation claim. 

Do you believe authors both read and understand the contents that they cite? In a field that tries hard to be cutting edge and publishes papers like crazy, the reasonable assumption is no.",hf8zdv3,t1_hf8ohpc,1633282663.0,False
q0j4vh,Citations are meaningless and can be played like a game.,hfb473g,t1_hf8ohpc,1633316755.0,False
q0j4vh,"This leads me to wonder, why does he indicate the review process to be too slow? Is it that he’s trying to pull a fast one, get some credibility so he can attract investors in some product he spun off before some fraud is detected? The review process should be slow, that is by nature of science. Obviously this field develops quickly, all fields develop faster than papers can be reviewed and published, but that doesn’t make his paper invalid in the long term. Pre-publish is a common thing, if he gets cited there, that should boost his confidence that he can get accepted next year, but sour grapes for not being included in a yearly conference?",hf9ozde,t1_hf8nvvv,1633292907.0,False
q0j1h2,"Maybe. Depends on the way the clocks work internally. If they turn on and off in 5m intervals it is possible to start them so that they never run at the same time.

In general your system would be periodic with a period of lcm(p1,p2) where p1 and p2 are the periods of clock 1 and 2. I would not call this 'random'.",hf8fj6y,t3_q0j1h2,1633274148.0,False
q0j1h2,"If the periods were large-ish primes, that might still have the desired effect. Even better with more than two.",hfbnqbp,t1_hf8fj6y,1633329419.0,False
q0j1h2,"The thought you’re having (I think) is that if you read a number of clocks every once in a while, you will get entropy and this randomization. 

But there are critiques I can think of…

Where are you getting the two clocks?  If you’re sticking to one machine, you’re just getting the entropy from any mistakes the machine is making (and those mistakes would be good for you and bad for everyone else, so don’t count on it)

If you get a clock over the net, then the network latency is what generates the entropy, and you’re essentially using the clock to measure that entropy. But then you’ve added a resource dependency.

It’s a boring answer, but: kernel devs already think about how to find entropy on the machine and store it for later use in true RNGs (think /dev/random), so if you really need true randoms, try to find the system call that gets it, or the library call that does that for you.",hf8v848,t3_q0j1h2,1633281001.0,False
q0if47,"Yes, in real computers, most instructions are longer than 8 bits. An instruction can be many bytes long, including a full memory address. The video you linked is a toy example, which fits each instruction into a single byte for simplicity. Since it only has 4 bits available for an address, it can only access 16 bytes of memory.",hf8c9qq,t3_q0if47,1633272667.0,False
q0if47,If I wanted more bits for an instruction would I need more modules?,hf8cqfw,t1_hf8c9qq,1633272877.0,True
q0if47,Not necessarily. You would need a larger instruction register to hold multiple bytes.,hf8d1tb,t1_hf8cqfw,1633273020.0,False
q0if47,"Yes but engineers are smart, we have found ways to represent the same number of instructions with fewer bits.

E.g. In MIPS architecture, the last 2 bits of the instruction are ignored since every instruction occupies 4 bytes (32 bits) and the first 4 bits are also not needed since this information is stored elsewhere in the circuit.

Therefore, we can represent a full 32 bit instruction with just 26 bits in MIPS. 

And there are other techniques that enable the access of instructions that are out of reach. So we don't need to be able to directly encode all the instructions.",hf8ern3,t3_q0if47,1633273803.0,False
q0if47,"Short answer: No.

Longer answer: Modern computers (re: CPUs) contain a very sophisticated [MMU](https://en.wikipedia.org/wiki/Memory_management_unit). The bit width of the memory module itself doesn't matter when fetching data\*. Sure, a fetch may pull in 256 bits (or even 256 bytes for that matter), but it all gets cached for the CPU to pick'n'choose as desired. 

(\* Certainly the wider the data bus to the memory is, the faster/more data can be accessed in one shot, but that simply increases *performance*. It doesn't impact functionality.)",hf8ch9k,t3_q0if47,1633272760.0,False
q0if47,[deleted],hf86zm0,t3_q0if47,1633270219.0,False
q0if47,[deleted],hf8766u,t1_hf86zm0,1633270307.0,False
q05rtf,"Memory is addressed by byte, not by word.",hf5pyef,t3_q05rtf,1633217347.0,False
q05rtf,"In all modern architectures this is true.  There were some experimental architectures back in the earliest days of CS that had different memory widths, but that has completely died out.

What hasn’t died though is memory alignments and multibyte loads.  Many RISK architectures cannot load a 2 or 4 byte load on an odd memory alignment and will instead trap to the kernel to emulate the larger load as a sequence of smaller loads.",hf5us1k,t1_hf5pyef,1633219512.0,False
q05rtf,"Memory is a collection of unique bytes, so to access each of them you need to be able to address it somehow. In 32bit system, for addressing we are using 32bit addresses (in C/C++ if you have a pointer it will be 32bit value). Because of this, you are able to address only 4GB of memory.   


In 64bit system addresses are 64bit value so you can address 2\^64",hf5vidl,t3_q05rtf,1633219846.0,False
q05rtf,Because whatever you choose as a word length is the minimum size you can access at once. For example if I only want to check 8 bits for a char and the word length was 32 then I would waste 24 bits. So a 20 char string would take much more ram space with 32 bit words. It makes more sense when ram is expensive to optimize for smaller words and less ram because home computers tend to have a lot more 8-16 bit variables then 32 bit ones.,hf5yvyd,t3_q05rtf,1633221378.0,False
q05rtf,"Word length is generally 32 bits in a bunch of different ISAs and microarchitectures. Regardless, most architectures are byte addressable and a word of 32 bits is then stored over 4 consecutive addresses.",hf6oe2f,t3_q05rtf,1633234015.0,False
q05rtf,"Oh right, so even if it were 32 bit Word length, it would have 4 addresses assigned! Wow, even more complicated. I guess it could be horrible inefficient to have such a large word length with only one address, in terms of wasted space?

So I assume the architectures transfers four 1 byte blocks simultaneously (a single word)?

Many thanks for your answer.",hf9exgy,t1_hf6oe2f,1633288936.0,True
q05rtf,"No that would be horribly inefficient. Depending on the cache architecture, you could do anything along the lines of cacheing 32 bytes at once from memory to the cache and only read the word the CPU requested. In my CPU architecture class, that is what we do, on a cache miss, we allocate a new block of 8 words starting from the requested word address then we can index the 4 bytes requested and return the data to the cpu. Not sure how much real modern cpus allocate on misses at once but you get the gist of it",hf9gmqj,t1_hf9exgy,1633289601.0,False
q05rtf,"Current computer architectures have settled on using 8-bit bytes as the fundamental addressed component of memory, even if, from a hardware perspective, memory is accessed by 16-bit or 32-bit blocks of memory.

What happens is that the bottom bits (A0 or A0-A1 of the address) is never exposed externally by the CPU chip, but instead, A2-A(n) are exposed to the RAM sockets, and data is accessed externally 16-bits or 32-bits at a time.

Then internally to the CPU, the appropriate 8 bits out of the 16-bit or 32-bit word are then accessed.

----

This is, by the way, why some processors are faster accessing 16-bit or 32-bit chunks of memory if they are 'word aligned'. If you have a 32-bit processor with 32 external data lines, and you're accessing a 32-bit word whose last bits in the address are 00, then the processor simply loads the word in one read operation. 

But if the last bits of the address are not word-aligned: they're (say) 01 or 10, then the processor has to load two adjacent 32-bit chunks of memory in two separate memory read operations, then shift things around internally.

(Some processors use a caching scheme which makes accessing words in memory the same speed regardless of alignment as long as the word doesn't cross page boundaries--but that's beyond the scope of all this.)

----

Note, by the way, 8 bit bytes as the fundamental block of memory was a relatively recent convention--there are older computers which use different sizes of memory. Some earlier, older computer chips used 4-bit objects as the basic unit of memory access. And some older mainframes used other sizes--one (the Symbolics L-machine LISP machine, for example) used 36-bit objects as the basic addressable unit of memory. (36-bits worked well for LISP, because it could be used to represent a basic LISP cell: 36-bits is big enough for 2 16-bit addresses, and 4 control bits useful for garbage collection.)",hfdjsfw,t3_q05rtf,1633369288.0,False
q05rtf,"Thank you W3Woody,Super interesting.",hfeh34s,t1_hfdjsfw,1633381497.0,True
q00zo3,"Yeah no I wouldn't start with C. Don't want to scare the newcomers lol. I'd say learn the big things first in an easy language like Python then if you're still interested, then you go to c/c++. As for text books, there are just too many. Just like the other comments suggested, I'd go with some online lectures. I prefer the Lynda lectures but I'm assuming Udemy is good too but both cost money so maybe look for some free ones in YouTube. 

Watch different lectures by different people and when you pick one, use the text book they recommend.",hf60jbw,t3_q00zo3,1633222109.0,False
q00zo3,"maybe an unpopular opinion, but i think if you really want to learn this things detailed you d be better to go with C/C++ There are dozens of books and courses about it (e.g. udemy.com)

If you really want to do it in python, i m sure you can find courses on udemy as well",hf5b6s3,t3_q00zo3,1633210810.0,False
q00zo3,Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms),hf69ct5,t3_q00zo3,1633226250.0,False
q00zo3,"There are a lot of content online on this topic. Look it up on YouTube, you can find a lot of good lectures there.",hf78swp,t3_q00zo3,1633247842.0,False
q00w48,"Dynamic routing algorithms can certainly be computed at a central point, but then one has to ask the question: how would you get all the link information to that central point, and then how would you distribute the result to a whole network of nodes. Shortly you realize that in order to do that you have almost built a distributed routing algorithm already (just only to a central node rather than any node). Usually at that point it’s easier / simpler / with less network overhead to just use a distributed algorithm. The exception usually only ends up being when you have a very complex routing metric / optimization that does not map well to distributed implementation.",hf4sy56,t3_q00w48,1633202875.0,False
q00w48,"I'd quite like a CNC router, but I'm not sure it needs to do much routing dynamically, let alone distributing them.",hf4nou1,t3_q00w48,1633200575.0,False
pzvpje,"Have you considered the Knuth-Morris-Pratt (KMP) algorithm?

There are a few different ways to apply it but if I understand your examples correctly I think KMP is the way to go",hf3rkws,t3_pzvpje,1633186681.0,False
pzvpje,"Although it may seem close, it's not what I'm looking for. I'm only taking matches at the beginning of the fixed strings, nothing other than the prefix.",hf4pj4j,t1_hf3rkws,1633201385.0,True
pztodf,Anywhere between _'none at all'_ to _'all the time'_,hf399em,t3_pztodf,1633177452.0,False
pztodf,"While true, I would say as a generalization across all programming jobs it will be somewhere between Slim and None….maybe even less.",hf5tnj7,t1_hf399em,1633219003.0,False
pztodf,"And if you don't know calculus, you can guarantee that none is an upper bound.",hf6s36h,t1_hf5tnj7,1633236101.0,False
pztodf,This,hf76bv2,t1_hf399em,1633245903.0,False
pztodf,"Depends on what you're programming. If its AI/ML, it'll be a shit ton of calc. Don't be like me , I hardcore regret not studying harder in my high school calc classes :(",hf36lpo,t3_pztodf,1633175837.0,False
pztodf,"Hmm, can you pick up those calculus on the fly while learning AI/ML? I'm trying to teach myself calculus but feel really unmotivated to do so because I haven't seen their application for thos fields",hf54c21,t1_hf36lpo,1633207804.0,False
pztodf,It’s really about learning multivariate and matrix. You’ll see the use in those once you begin to understand it,hf56bt1,t1_hf54c21,1633208662.0,False
pztodf,"Just learn it when you require it. If you havent come across the need to understand calc then sure, dont bother about it. I think once you get to the itty gritty details of ML, you will start seeing the applications of calc. Concepts like backprop, gradient descent, fourier anaylsis in signal processing all involves calc (these r just some, there r much much more applications)",hf5twd9,t1_hf54c21,1633219113.0,False
pztodf,"Honestly, this doesn't help you at all, but I didn't understand calc UNTIL I applied it in my field. (Not CS but still)",hf5926r,t1_hf54c21,1633209866.0,False
pztodf,It will definitely be super slow going to learn the concepts as a part of their application in ML because you will learn a bit of ML and have to spend a bunch of time learning the fundamental math. There are also a ton of applications in physical simulations for video games if you'd like something fun and applicable for 2d calculus while you are learning.,hf6sdpr,t1_hf54c21,1633236274.0,False
pztodf,Didnt you have to take it in uni?,hf4ky6h,t1_hf36lpo,1633199377.0,False
pztodf,"Interesting, I am doing research in the ML and I don’t deal with calc at all. It’s mostly finding a lot of models to see if it fits the data and tweaking if need be. Maybe you are trying to create your own model from scratch. But even than linear Algebra dominates",hf5ev24,t1_hf36lpo,1633212423.0,False
pztodf,"Yea i guess i was referring more to the ""build your own model from scratch"" aspect. When i tried to build some of the NN architectures from scratch, i realized the amount of calc there was.....all the gradient descent and backprop stuff",hf5sonb,t1_hf5ev24,1633218568.0,False
pztodf,"It is usable in the fields of Artificial Intelligence, Machine Learning, Game Development, Computer Graphics, And in my favorite area: Procedural Content Generation for games.",hf3l60g,t3_pztodf,1633183638.0,False
pztodf,What is the last one about?,hf4wpo7,t1_hf3l60g,1633204516.0,False
pztodf,"Using mathematics for generating mountains, ocean waves, grass, trees and other interesting effects, I.e. drawing some game elements using mathematics rather than drawing them by hand (for example using Photoshop or similar software).",hf4x00n,t1_hf4wpo7,1633204639.0,False
pztodf,Wow sounds cool and hard.ty!,hf4x3c3,t1_hf4x00n,1633204679.0,False
pztodf,"r/creativecoding and r/generative are cool places.

[Perlin Noise](https://en.m.wikipedia.org/wiki/Perlin_noise) is a way to start generating cool stuff.",hf6x4iu,t1_hf4x3c3,1633239152.0,False
pztodf,"Numerical analysis too, I had to code a math project for 2 months. What a pain in the ass.",hf6d4p3,t1_hf3l60g,1633228135.0,False
pztodf,"If you're planning on going into front-end development you're really going to need to have calc and differential equations. You'll probably also want very strong multivariable calculus as well, for DOM manipulation.",hf50iia,t3_pztodf,1633206151.0,False
pztodf,Is this some kinda joke?,hf5hvqz,t1_hf50iia,1633213749.0,False
pztodf,r/whoosh,hf6rf4r,t1_hf5hvqz,1633235713.0,False
pztodf,If you are doing something like data science the answer is also either all the time or none at all,hf5vftp,t3_pztodf,1633219814.0,False
pztodf,"I believe each aspect of programming brings its own set of math related work. In video gaming you will most likely have to deal with vectors, matrix, linear algebra. With api/microservice you will need to deal with rates for throttling, hash and cryto. In embedded non-10 base counting and discrete math. In any work related to data engineering you will need statistics and maybe set theory.  Someone may argue that some of those concept are more CS related than math but in the end good understanding of the math behind it will make you better.

For example last week after implementing new metrics on our app I had to make sure the PM understood the difference between an average and a moving average so they can use the metric correctly.",hf66v6v,t3_pztodf,1633225051.0,False
pztodf,">more CS related than math

Then you can tell them that CS is a branch of math XD",hf6x749,t1_hf66v6v,1633239199.0,False
pztodf,A LOT of if you go into Machine learning or AI,hf5gvbw,t3_pztodf,1633213305.0,False
pztodf,"Assuming you’ll just be a typical web developer with no obvious math or physics bias, you will use nothing you learn in calculus. However, the process of learning calculus you will carry for your entire career in my opinion (the type of thinking that you develop)",hf69wxa,t3_pztodf,1633226527.0,False
pztodf,Calculus is what makes algebra useful.  I would argue that anyone solving real-world problems numerically needs basic calculus.,hf6i7o3,t3_pztodf,1633230676.0,False
pztodf,Maybe > 0 if working on computer graphics or game development.,hf6wpad,t3_pztodf,1633238881.0,False
pztodf,"Could you be any more vague?

What will you be programming?",hf3enjg,t3_pztodf,1633180392.0,False
pztodf,Could you be any more sarcastic?,hf4rkgy,t1_hf3enjg,1633202280.0,False
pztodf,Oh yes I could. That was sarcasm level zero. I have an unlimited supply available.,hf4ryq6,t1_hf4rkgy,1633202449.0,False
pztodf,Well?,hgjsbcl,t1_hf4ryq6,1634169079.0,False
pztodf,Well what? Be more specific and I'll try and give you an answer. Answer the question I already asked. What will you be programming?,hgl9tv6,t1_hgjsbcl,1634202000.0,False
pztodf,I’m not the OP. I just wanted more sarcasm.,hgnj02m,t1_hgl9tv6,1634242320.0,False
pztodf,Really.,hgnxam9,t1_hgnj02m,1634249335.0,False
pztodf,0,hf545uc,t3_pztodf,1633207729.0,False
pztodf,"The vast majority of programming jobs?

Zero.

The more interesting and highly paid programming jobs? 

""Some"" to ""a hell of a lot.""",hfdlnc2,t3_pztodf,1633369970.0,False
pztodf,[deleted],hf368r6,t3_pztodf,1633175598.0,False
pztodf,Then why do I have to take so many calc courses at college,hf36bri,t1_hf368r6,1633175655.0,True
pztodf,"I give you a hint: Don’t look at math as math, but as learning how to think in an abstract way. It’s probably the most efficient way to train the thinking and of that you will use a lot in programming.",hf36xxz,t1_hf36bri,1633176056.0,False
pztodf,"For a real answer and not the urban legends being thrown around by some in here that study of mathematics will develop your coding skill, look into the empirical research for the concept of transfer of learning, which deals in what you are wondering about. 

In short, there is no use for calculus in programming unless you're solving calculus problems. Modern curriculums are descended from the ancient view that general problem-solving skill is obtained through the practice of certain subjects (e.g latin, mathematics), and that is why you are taking calculus today. Our universities simply don't know yet how to deal with the empirical studies showing that general problem solving ability is not developed further by study of any particular subject. This is why so many coding boot camps and coding-only universities have popped up lately. 

See [Sweller, J., Clark, R., & Kirschner, P. A. (2010). Teaching general problem-solving skills is not a substitute for, or a viable addition to, teaching mathematics.](https://www.researchgate.net/publication/254913324_Teaching_general_problem-solving_skills_is_not_a_substitute_for_or_a_viable_addition_to_teaching_mathematics):

>In over half a century, no systematic body of evidence demonstrating the effectiveness of any general problem-solving strategies has emerged. 

and on the strategies detailed in Polya's ""How to Solve it"":

>There is no body of research based on randomized, controlled experiments indicating that such teaching leads to better problem solving.

The larger body of empirical research on transfer of learning concludes the same for programming, chess, and a myriad of other skills on their transferability to other domains (i.e. near zero).",hf6cq70,t1_hf36bri,1633227931.0,False
pztodf,Thanks,hf6cv1o,t1_hf6cq70,1633228000.0,True
pztodf,"Calculus is the math of continuously-changing systems. This includes almost everything in the physical world, but also includes things like probability distributions and the growth rate of functions. Being able to think about probability distributions is useful for stats and ml, but also answering questions like “when do we expect the packets to show up?”  in networking and “how many users do we expect to get from this ad campaign”. Function growth rate is useful for asymptotic analysis of algorithms and also growth rate of data storage.

A university CS degree is meant to give you a broad intellectual base so you are prepared to solve anything that might come your way in the future. Not knowing calculus will keep you from engaging with a bunch of fields and problem-solving techniques.",hf4gdfv,t1_hf36bri,1633197397.0,False
pztodf,Just consider the possibility that those that have gone through this before might have a better understanding what might be valuable for that specific course of education.,hf4x8l6,t1_hf36bri,1633204742.0,False
pztodf,"It’s important to understand the theory and foundations of calculus, but you will almost never need to manually take an integral or derivative when programming. Of course it depends what you are programming",hf3crpd,t1_hf36bri,1633179402.0,False
pztodf,Python developer right here,hf3e4n0,t1_hf368r6,1633180118.0,False
pztda7,Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo),hf35cru,t3_pztda7,1633175007.0,False
pztda7,"I like tom scott and computerphile. heres a link that covers both [https://www.youtube.com/watch?v=-enHfpHMBo4&list=PL96C35uN7xGLux5q2c4P\_IqbKF11-pfsR](https://www.youtube.com/watch?v=-enHfpHMBo4&list=PL96C35uN7xGLux5q2c4P_IqbKF11-pfsR)

ben awad is also pretty cool

https://www.youtube.com/watch?v=ijL0h6-1qbQ",hf36q0v,t3_pztda7,1633175916.0,False
pztda7,read Algorithms by CLRS.. you’ll lose what little interest you have!,hf5ltce,t3_pztda7,1633215495.0,False
pztda7,"Base.cs podcast is very enjoyable. Funny hosts, and they really have a way of breaking things down simply. I'm finding it helpful as a companion to cs50 and the modern  c programming book.",hf5q39m,t3_pztda7,1633217406.0,False
pztda7,"The Innovators by Walter Isaacson.  It's a history of how we go to where we are now with computers.  The Audio book version is very easy to listen to and follow.

https://www.amazon.com/Innovators-Hackers-Geniuses-Created-Revolution/dp/1476708703",hfe2vg1,t3_pztda7,1633376332.0,False
pztda7,Annotated Turing.,hfzg5mn,t3_pztda7,1633789346.0,False
pztda7,[deleted],hf33svx,t3_pztda7,1633173912.0,False
pztda7,>I'm not asking for a how to code or anything too technical,hf35mit,t1_hf33svx,1633175188.0,False
pzrkv4,"Maybe this animation (from me) is helpful: [https://youtu.be/LTT9GftcY\_Y](https://youtu.be/LTT9GftcY_Y)

It shows a unit vector (blue) multiplied with the covariance matrix. As you can see, the resulting vector (orange) is large when the blue unit vector points in a direction with high variance.

A unit vector multiplied with the covariance matrix gives a vector whose length is proportional to the variance in that direction (top left corner). This vector points in the same direction as the unit vector (i.e. is an eigenvector of it) when the variance is highest.",hf2zg9z,t3_pzrkv4,1633170641.0,False
pzrkv4,"The eigenvector, really the singular value vector, is the loadings or weights which make up a linear combination of your components. Those loadings have a magnitude of one, if you sum them up and take the square root, the vector with loadings is just a unit vector. The eigenvalues, really the singular valueless, is the magnitude of the principal component. The principal component loadings or weights are all scaled to one. Then the magnitude of the singular values, tell you the magnitude of those vectors, tells you which principal components are most important. You can plot the singular values on a graph with 1 having the first singular component and seeing how many components are important. Can you compress your data using just 1 principal component or do you need 5 or 100. It depends on how much data you have and how much information is captured by your system or the data.",hf3ns2o,t3_pzrkv4,1633184899.0,False
pzrkv4,"Perhaps a bit of geometry will help with the intuition.

Picture your data as a cloud of points in some space. If your data is well-behaved (near normal distribution) that cloud will look like a somewhat flattened rugby ball (ellipsoid). The eigenvector corresponding to the greatest eigenvalue is the direction where your points are farthest apart. That means that if you project the points on an axis colinear with the vector (your first principal axis) you get the best possible one-dimensional separation between them.

The next step is then to project the points on the hyperplane perpendicular to the first vector – effectively neglecting the distribution along the first axis. And you start anew with the dimensionality of your data having been reduced by one: the new ""best"" vector will correspond to the second greatest eigenvalue.

And so on.


Perhaps physical intuition will be more appealing to the engineer: the first eigenvector is the direction where your cloud of points has its greatest inertia, et caetera for the following ones.

Be aware that PCA can give misleading results if your point distribution is badly behaved (not looking like the rugby ball), so always examine your data with other tools also.",hf63ckz,t3_pzrkv4,1633223396.0,False
pzqak4,"Graphs are commonly represented using what's called an adjacency list. Basically you would have a data structure that contains a node's value and an array that contains references to it's neighbors. So in your example you would have a node object with the value 1 and an array that contains refs to a1, a2, etc. Similarly, node a1 would just have an array with a reference to node 1. This way you can represent all the nodes with the same structure and easily perform traversal algorithms on all of the nodes. You can also use what's called an adjacency matrix which is a 2D array where each row and col refers to a specific edge between nodes. Here's a good breakdown: [Graph representations](https://www.geeksforgeeks.org/graph-and-its-representations/)",hf2nssd,t3_pzqak4,1633161103.0,False
pzqak4,"Wait so if I store an array for every node, wouldn't it make it hard if, let's say, I need to insert a new node in the graph?",hf3s7k6,t1_hf2nssd,1633186975.0,True
pzqak4,"no, I think it will be easier with adjaceny list please check [this link](https://www.geeksforgeeks.org/comparison-between-adjacency-list-and-adjacency-matrix-representation-of-graph/) for time complexity of adding a new edge or vertex.

Also I have a doubt in this representation what happens in case of a cycle, for example when child node of a1 is connected with root node 1. In which node does the address of new node go.",hf3tgvf,t1_hf3s7k6,1633187561.0,False
pzqak4,"I'll go through the link in a while :) , but in my representation, if any node has 1 or more child nodes, it'll be considered a root node. So, a1 will now be a root node, so the address of the new node can be stored in a1, and yes loops cannot make sense in my implementation.",hf3u6wd,t1_hf3tgvf,1633187895.0,True
pzqak4,"You could use a linked list to store the adjacent nodes (hint hint) instead of a contiguous array, if you care about minimal insertion / removal complexity and don't mind fragmenting the memory that store your child node pointers.

Note: if you usually have only a small number of adjacent nodes (low degree in your graph), e.g., ~10, then using a contiguous array to store the adjacent node pointers will be faster in practice for most operations (even insertion and removal) since your data structure would be more cache friendly.
If you want to know for sure what's best, write a few variations of your data structure (e.g., linked list adjacent nodes or array adjacent nodes) and run benchmarks (insertion, removal, traversal) for varying numbers of nodes and different structures (e.g., different vertex degrees).",hf4tqo5,t1_hf3s7k6,1633203221.0,False
pzqak4,"You're correct for adjacency matrix.

""This representation requires space for n^2 elements, the time complexity of the addVertex() method is O(n), and the time complexity of the removeVertex() method is O(n^2 ) for a graph of n vertices.""

https://www.google.com/amp/s/www.geeksforgeeks.org/add-and-remove-vertex-in-adjacency-matrix-representation-of-graph/amp/",hf4ad5q,t1_hf3s7k6,1633194836.0,False
pzqak4,"He doesn't need to use a full adjacency matrix representation for his entire graph. So, careful with the conclusion on complexity.

He just needs to store the neighbors for every vertex. So his insertion time complexity will not be dependent on the full number of vertices in the graph (n) but will depend on the number of edges that are added.",hf4uw45,t1_hf4ad5q,1633203724.0,False
pzqak4,Wait adjacency list just blew my mind it's so good wow😳,hf3vluv,t3_pzqak4,1633188520.0,True
pzqak4,"Wait till you learn how you could store a (full/complete) binary tree with only arrays, and no pointers/lists/etc.",hf609s8,t1_hf3vluv,1633221990.0,False
pzqak4,"Another good option is an Ebert graph, which comes with some different time/space trade offs.
https://developers.google.com/optimization/reference/graph/ebert_graph",hf4e8e2,t3_pzqak4,1633196488.0,False
pzqak4,"Here is the original publication from the 80s which I think is helpful to understand this better.

https://www.researchgate.net/publication/220425539_A_Versatile_Data_Structure_for_Edge-Oriented_Graph_Algorithms",hf4vrq3,t1_hf4e8e2,1633204109.0,False
pzqak4,Whoaaa I'll surely give this a read. Thanks :),hf6qv8s,t1_hf4e8e2,1633235395.0,True
pzqak4,"how do you decide which node is root node? node 1 can be considered branch node of node 2

besides that, are you storing one pointer to branch nodes LL in root node? if yes this can make search expensive?

According to what I have understood, you are making a linked list with all the children nodes of a root node, and storing it's head pointer in root node.

But if we slightly change it, and instead of storing one pointer to linked list all the childrens of root node, we store all the children pointers, it is basically adjacency list representation.",hf3qir1,t3_pzqak4,1633186186.0,False
pzqak4,"Ahhh yes I've not thought about searching in my algorithm yet, although I think the brute force approach would be going to every ""root node"" (I know the word root is not generally used in this way), and checking all the nodes in its ""Another LL"" for a match, and if not found, proceeding to the next node and repeating the same process.",hf3tast,t1_hf3qir1,1633187482.0,True
pzqak4,How do we classify root nodes? What are root nodes exactly,hf3tnsw,t1_hf3tast,1633187651.0,False
pzqak4,"Longest chain of nodes with one or more children. 
I haven't given much thought to this implementation. It was just an idea that came in my head so I posted it here, so I might not have answers to some of your questions, and some answers won't make sense so pardon me :')",hf3v4ok,t1_hf3tnsw,1633188310.0,True
pzqak4,"Unless your data structure is a tree, there is no unique concept of a root here. You can always make something up of course depending on the problem you are trying to solve with your data structure.",hf4v4fp,t1_hf3tast,1633203828.0,False
pzqak4,"To further simplify your idea, I suppose you could just represent a graph as a massive linked list with duplicate nodes where the entire graph is a linked list snaking back and forth between all the nodes. Then to determine if two nodes are connected you have to crawl through the entire list of nodes. This is horribly inefficient though. I don't see much value in the branch node concept because it isn't a property of graphs for them to have independent branches that don't interconnect with other branches. That's a tree. Sure it's technically possible that a general graph has the structure of a tree, because a tree is a more specific graph; However, there often won't be a clean branch to represent as connected to a root node.",hf48sxm,t3_pzqak4,1633194182.0,False
pzqak4,"Right, that could be one way to represent it. So what I was talking about was a tree, not a graph, I suppose.",hf6qzbz,t1_hf48sxm,1633235461.0,True
pzqak4,Random thought: Did you draw this in Concepts?,hf636ce,t3_pzqak4,1633223316.0,False
pzqak4,I drew this in some drawing software that came installed on my chromebook :D,hf6qoyc,t1_hf636ce,1633235294.0,True
pzqak4,huh neat! Thanks for sharing,hf9df3u,t1_hf6qoyc,1633288332.0,False
pzfvrt,"Yeah. It’s called brute force. 

That’s it. That’s all there is. 

Literally every programming language has a way to generate randomness* (it’s actually pseudorandom but for this pseudorandom is random enough).",hf15h2m,t3_pzfvrt,1633129181.0,False
pzfvrt,"For random operations you use a pseudorandom number generator. Every language has one.

For many problems you may wish to ""brute force"", you don't actually need randomness at all. You can just iterate over every possible solution and check ""is it correct?"". Eventually you will find the solution, but depending on the problem it may take practically forever.",hf1ckya,t3_pzfvrt,1633132653.0,False
pze8dj,"When it comes to space complexity, you're measuring how much memory you need to commission to run the function. This includes arrays, temporary arrays that you need to copy, etc.

I would approach it the same way you approach performance complexity to keep it consistent, that is, using the Big-O rules.

Steps:

1. Omit any multiplicative constants

>`4n` becomes `n`.

2. If a factor has a higher power, ignore smaller powers

>`n^2 + n` becomes `n^2`.

3. Any exponential dominates any polynomials

>`2^n + n^2` becomes `2^n`

4. Any polynomial dominates any logarithm

>`n^2 + log(n)` becomes `n^2`

Essentially, when calculating complexity you want to *generalize* the costs. If you're working on large data sets, the idea is that when the set becomes arbitrarily large, the constant factors such as input variables and the like will become insignificant.",hf4xyqp,t3_pze8dj,1633205061.0,False
pze835,Literally the code. There are open-source database engines.,hf1pmj7,t3_pze835,1633139148.0,False
pze835,"Thought that might be the case, I'm guessing the majority of it is C/C++/C# algorithms/data-structures?",hf30gzz,t1_hf1pmj7,1633171436.0,True
pzcvjq,"Project Euler

[https://projecteuler.net/](https://projecteuler.net/)

Its emphasis is mostly on number theory and combinatorics.",hf01j81,t3_pzcvjq,1633110850.0,False
pzcvjq,"Not sure which sites you're referring to; but I know leetcode does tell you the speed and memory usage of your code, and which percentile of all submissions you fall within in those categories.",hf0k6ab,t3_pzcvjq,1633119080.0,False
pzcvjq,"Not a direct answer to your question but it's simple enough to time a single piece of code/function to figure out its run time. Then just set a task, like a maths equation, image manipulation or something more complex, and keep on editing it until you can't gain faster speeds.",hf0ht5g,t3_pzcvjq,1633117997.0,False
pzcvjq,Check out competitive coding problems. These are great for practicing problem solving and algorithm design. Google Informatics. If you’re after “interview” style questions Kaggle has a bunch and a great community.,hf32667,t3_pzcvjq,1633172709.0,False
pzbae2,"This is an excellent book: [Computer Networking: **A Top-Down** Approach](https://www.pearson.com/uk/educators/higher-education-educators/program/Kurose-Computer-Networking-A-Top-Down-Approach-Global-Edition-7th-Edition/PGM1089141.html) 

It was recommended reading for 2 semesters in my CS college course.  
As the title suggests the book takes an different approach then 99% of other Networking books.  
Rather than going Bottom-Up it goes **Top-Down starting with the Application Layer**. ;)  
It's a great book and I think the ""reverse"" approach is easier to understand when you are just starting out.",hf0i8fo,t3_pzbae2,1633118191.0,False
pzbae2,You got 2 semesters to go through it? We had to read the whole thing in a 10 week term lol,hf1jtb3,t1_hf0i8fo,1633136229.0,False
pzbae2,damn... lol,hf1ndzq,t1_hf1jtb3,1633138004.0,False
pzbae2,This is the approach I am going to take. Many thanks.,hf25gsg,t1_hf0i8fo,1633147818.0,True
pzbae2,"You are welcome dude.  
Enjoy... Networking is really fun when you get your head around it.",hf29z5w,t1_hf25gsg,1633150798.0,False
pzbae2,Looks great but the link says it’s out of print 😕,hf0k70k,t1_hf0i8fo,1633119088.0,False
pzbae2,"I'm sure there are newer editions bro...  
I just picked the top link in my search...

Look for: **Computer Networking: A Top-Down Approach, James Kurose & Keith Ross**  
Here's 2016 - 7th edition: [link](https://www.amazon.co.uk/Computer-Networking-Top-Down-Approach-Global/dp/1292153598/ref=sr_1_1?dchild=1&keywords=Computer+Networking%3A+A+Top-Down+Approach&qid=1633122066&sr=8-1)  
Here's 2021 - 8th edition: [link](https://www.amazon.co.uk/Computer-Networking-Global-James-Kurose-dp-1292405465/dp/1292405465/ref=dp_ob_title_bk)",hf0qzh6,t1_hf0k70k,1633122213.0,False
pzbae2,Cheers 👍🏽,hf2ruxx,t1_hf0qzh6,1633164427.0,False
pzbae2,"You don't really need to understand those details as a beginner. If you're not too deep into it, I would suggest reading *Computer Networking: A Top-Down Approach*. It's number one on the best textbook I've read and it's used in most if not all top CS schools so you know you're in good hands.

But if you're purely looking for prerequistics look up Calculus, Fourier Analysis, Discrete Math, & Algorithms.",hf1tqje,t3_pzbae2,1633141274.0,False
pzbae2,As a software engineer with some background in Electronics I'd like to ask if these low level stuff is actually useful for most network engineers and can be treated as a prerequisite. It's not like I have to know how transistors work in order to be a good programmer.,hf2dax2,t1_hf1tqje,1633153124.0,False
pzbae2,If you’re dealing with Fourier series I believe getting an understanding of them from multiple angles will help. First look at them through the lens of Calculus and then again through the lens of Linear Algebra. It’s quite amazing to see all the applications Fourier series have for us in the modern day.,hezz445,t3_pzbae2,1633109800.0,False
pzbae2,[Neso Academy](https://youtube.com/c/nesoacademy) has an awesome playlist on [Computer Networks](https://youtube.com/playlist?list=PLBlnK6fEyqRgMCUAG0XRw78UA8qnv6jEx),hf0ju2f,t3_pzbae2,1633118925.0,False
pzbae2,My advice would be to just continue with the book as it gets easier later on. Maybe then come back if you really want to understand the low level stuff.,hf03orb,t3_pzbae2,1633111787.0,False
pzbae2,"You don't need to understand Fourier analysis to learn computer networking. You start with the OSI model and work your way up through each layer. Computer networking is not only about the guts of the electrical or optical signal transmission  but the majority of the intercommunication protocols that have been developed and adopted as international standards, such as TCP/IP and IMAP and so on. 

If you're asking for pre-requisites, you can safely bet there are none other than a basic understanding of electrical wires, and some boolean logic, which will come in handy later.",hf2rawp,t3_pzbae2,1633163956.0,False
pzbae2,"Like others said, the book by Kurose & Ross is probably the best resource for learning computer networking. It's used internationally by universities, has a great structure for beginners, and has a ton of projects to help you learn. Tenenbaum's book is also great, but more understandable if you're already somewhat familiar with networking.",hf3wbft,t3_pzbae2,1633188828.0,False
pzb24x,The book Artificial Intelligence: A Modern Approach by Peter Norvig and Stuart Russell.,hezu7jw,t3_pzb24x,1633107673.0,False
pzb24x,"100% agree. Back when I was first learning AI, this was my go-to book. Highly recommended.",hf03jy9,t1_hezu7jw,1633111730.0,False
pzb24x,Can I read it and benefit from it with virtually no cs experience yet ?,hf121kl,t1_hf03jy9,1633127552.0,False
pzb24x,"I would say probably not. At a minimum, it would be a third-year CS textbook, and more likely at the 4th year or [M.Sc](https://M.Sc) level.",hf1thbb,t1_hf121kl,1633141142.0,False
pzb24x,Ok thanks,hf4dc6e,t1_hf1thbb,1633196108.0,False
pzb24x,Been thinking about picking this one up. Have you read it?,hezwt82,t1_hezu7jw,1633108798.0,False
pzb24x,"It's fantastic but you should know that it is a dense read; and I mean DENSE. So, be warned.",hf0c999,t1_hezwt82,1633115523.0,False
pzb24x,This is the textbook for the GaTech M.S. in C.S. AI class. Highly recommended.,hf0kaut,t1_hezu7jw,1633119137.0,False
pzb24x,"Deeplizard YouTube channel for deep learning, keras tensor flow",hf1emu9,t3_pzb24x,1633133666.0,False
pzb24x,Have you tried... searching for it?,hezv9i5,t3_pzb24x,1633108126.0,False
pzb24x,"Thomas Back

[https://scholar.google.ca/citations?user=x7LEID0AAAAJ&hl=en&oi=sra](https://scholar.google.ca/citations?user=x7LEID0AAAAJ&hl=en&oi=sra)

Tom Mitchell

[https://scholar.google.ca/citations?user=MnfzuPYAAAAJ&hl=en&oi=ao](https://scholar.google.ca/citations?user=MnfzuPYAAAAJ&hl=en&oi=ao)  


Both very good sources that I cite frequently.",hf05yb6,t3_pzb24x,1633112772.0,False
pzb24x,"Coursera: deep learning specialization. All of the specializations by deeplearning.ai are worth it. it doesn't get you all the way there but it's an excellent start.

(Am senior MLE at big company and this is how I got started)

Edit: for more traditional AI stuff Norvigs book is good as mentioned. I assumed you meant ML as that's where most of the interest is these days.",hf19vh1,t3_pzb24x,1633131319.0,False
pzb24x,The 100pg ML book,hf2q2ab,t3_pzb24x,1633162943.0,False
pzb24x,"I can't personally vouch for any of this material, but an AI-related subreddit seems to have some links to online courses and other material: https://www.reddit.com/r/artificial/wiki/index",hf3v3jg,t3_pzb24x,1633188296.0,False
pzb24x,the internet,hezut0p,t3_pzb24x,1633107931.0,False
pz9k60,If you have taken a C programming course then Computer Systems: A programmer's perspective is really good.,heznqvw,t3_pz9k60,1633104929.0,False
pz9k60,I'm in the basics of the basics,hezoo6t,t1_heznqvw,1633105326.0,True
pz9k60,"The P. K. Sinha book looks okay.

There's also this book: Introduction to Computation and Programming Using Python by John Guttag.
(Prof. Guttag is a professor at MIT.)

And also Harvard University has a course called CS50. Just google Harvard CS50. 
 
All above recommendations are based on the fact that you know little or no programming.",hezpllj,t1_hezoo6t,1633105726.0,False
pz9k60,"I prerer books, I already work in IT and when I go back home I'd like to read more than watch a video",hf7xyj5,t1_hezpllj,1633265514.0,True
pz9k60,Nice book btw,hf7xvmz,t1_heznqvw,1633265470.0,True
pyybaf,"More info would be great.

 Personally I don't think it's possible to do better than: https://www.youtube.com/watch?v=2Op3QLzMgSY",hey7klm,t3_pyybaf,1633072895.0,False
pyybaf,"Change of bases , special cases of change if base , binary representation, binary format unsigned integer format , INPUT OUTPUT operations with the unsigned integer format , hexadecimal dump ( memory dump) , READ and WRITE operations which is essentially INPUY OUTPUT operations with the unsigned integer format, representation of range on the unsigned integer format , maximum and minimum.

Arithmetic on the unsigned integer format , carry abs overflow . Sign / magnitude format , READ AND WRITE operations on the sign magnitude format",hezazyy,t1_hey7klm,1633099420.0,True
pyybaf,I live in Puerto Rico and the way this class is taught is extremely different compared to how professors in the US teach it,hezb56i,t1_hey7klm,1633099484.0,True
pyybaf,no doubt,hezc24z,t1_hezb56i,1633099884.0,False
pywerf,"Depends. When I had courses that required a lot of coding I sort of lost track but I’d say like 3+ hours a day. During summer and when I don’t have courses heavy in coding, I’d do some leetcode puzzles here and there and maybe work on small projects that often went unfinished so like about an hour a day. I’d recommend doing leetcode or something similar to both learn the language and stay in touch with algorithms/logic. Not to mention it’ll be a big help in technical interviews if you plan on such a career. If you’re starting from scratch on a language, maybe do a short codecademy course (most are free).",hex3n1j,t3_pywerf,1633049060.0,False
pywerf,I’ve been spending 8 hours a week but I guess I need to step it up,hexglaz,t1_hex3n1j,1633055307.0,True
pywerf,"If you are not behind, there is no real reason to. Effort should be spent for a purpose. If you want a new skill, or better grades, or you enjoy it, then code more, but if you're hitting your personal benchmarks there's no reason to meet arbitrary metrics.",hexxc84,t1_hexglaz,1633064830.0,False
pywerf,"You just started programming. I wouldn't just code to code. Like u/UntangledQubit said, I would code with a purpose. Furthermore, if you really want to get ahead. Do some reading. In fact, I would read for the same amount of time that you code for. Anything by Robert Martin is good. I would definitely try to read a lot of articles about SOLID principles. Especially Dependency Inversion and Single Responsibility. The Open Closed principle is also very important as well. You can get years ahead of your peers if you understand these concepts well, and use them in your practice. Alot of times schools (at least the one I went to) don't do a great job teaching these things. They focus too much on stuff like inheritance which isn't even used that much in modern applications. People opt for composition more often these days. Good luck.

Edit: Also read about design patterns. A good rule of thumb with design patterns is that you should never start with one (unless your sure of it). Usually patterns present themselves as you code. Refactoring is a good time to implement design patterns where they should go. Many people think over-engineering is the gravest sin, so try to be smart about when you choose to use them.",hf0bkn7,t1_hexglaz,1633115222.0,False
pywerf,"Writing code or thinking about the code I need to write? For the former, I've recently re-learned that I need to stop writing code and give myself time to recover, so 8 hours a day.

Thinking about code? Doesn't stop. This week I had trouble not thinking about an algorithm I was working on even when I was in my therapist's office, and I realized what the latest problem implied while I was riding my bicycle today. I will probably wake up during the night and start thinking about it again.",hexf4h6,t3_pywerf,1633054594.0,False
pywerf,I’ve thought about it at night and it’s woken me up,hexgdar,t1_hexf4h6,1633055198.0,True
pywerf,"probs about 4-6 hours a day. im working on an artificial intelligence project atm so higher than usual. when i was in Uni it would be about 3-4 hrs (except near end of semester for coursework obvsly).

the project im doing now is a java webserver which serves html and css (maven managed spring boot application). feel free to fire any questions my way!

remember its quality not quantity!!!!

(if your working on safety critical software (where money or health risk is involved) then it would be frowned upon to keep hacking away, you only write 10 lines a day, most of the time is planning and paperwork. if you write tons of code it costs the company a lot to verify its safety and to monitor it afterwards. if you can do the same thing with alot less code or using a verified library then it is a lot easier to verify from a safety point of view)",hey6e3y,t3_pywerf,1633071868.0,False
pywerf,24,hex2xu4,t3_pywerf,1633048706.0,False
pywerf,That few?  I usually do 36 to 38...,hex8oc2,t1_hex2xu4,1633051486.0,False
pywerf,Damn guess I’m gonna do 52,hexce7a,t1_hex8oc2,1633053278.0,True
pywerf,Now you're just showing off...,hexfbp6,t1_hexce7a,1633054690.0,False
pywerf,24 what? Apples? Oranges?,hf01xru,t1_hex2xu4,1633111027.0,False
pywerf,"Currently 0, just a shi ton of math this semester

Over the Summer 10-12 for work and personal projects",hezhf40,t3_pywerf,1633102216.0,False
pywerf,42,hexbw7f,t3_pywerf,1633053038.0,False
pywerf,"For people in here saying 8-10+ hours a day cuz it's your job... Uhm, do you have meetings? What about documentation? What about planning? Surely it's impossible to actually spend that much time every single day just programming. I'd say on highly effective coding days I put in a solid 5, but that doesn't happen every day because I'm also a manager.",hez91ui,t3_pywerf,1633098531.0,False
pywerf,Are you learning CSS through your uni or are you using some site ?,hexzrev,t3_pywerf,1633066552.0,False
pywerf,Uni,heyxzn0,t1_hexzrev,1633093245.0,True
pywerf,5ish. dont measure your code time up. dont be insecure if someone elses code time makes your code time look small. size doesnt matter.,hey1tnz,t3_pywerf,1633068118.0,False
pywerf,"I'm working as a dev and it's maybe about 1 hour a day. The rest of the time is meetings, organizing tasks, writing docs, configuring stuff, and testing.",heyaa1i,t3_pywerf,1633075352.0,False
pywerf,"This thread is making me stressed af. I took a cyber security degree, dropped programming because I suck and now have to do a dissertation on software development. SQL, HTML, CSS, Python, maybe PHP. I barely do 2 hours a day let alone fucking 6.",hezo9qp,t3_pywerf,1633105155.0,False
pywerf,"At the beggining of the course I programmed a lot more (about 4hrs most days, more than that if you average out all-day close assignments sessions), now in the later stages it's mostly a lot of math, and I get tired fast, so I go days without opening the editor.",hf09pnj,t3_pywerf,1633114411.0,False
pywerf,"I work as a software engineer, there are days that i code for 1 hour, most of the time around 3-4 hours and on rare occasions 8+ hours.",hf16x0j,t3_pywerf,1633129885.0,False
pywerf,"8 days a week rollin’ hard on my Kinesis Advantage Pro, bruh",hex87tw,t3_pywerf,1633051264.0,False
pywerf,8-10 5 days a week.,heyrqwm,t3_pywerf,1633089713.0,False
pywerf,But computer science is not learning HTML and CSS. Go grab some book of algorithms. You will make codes in your dreams.,hez4qr0,t3_pywerf,1633096572.0,False
pywerf,I try to do a 1:30:00 when I’ve made the commitment. Been lackin lately though.,hexs09k,t3_pywerf,1633061356.0,False
pywerf,Like 3.50,hey8cvl,t3_pywerf,1633073600.0,False
pywerf,"I have exams after a few days so I'm taking a break from coding

Usually I used to learn or practice for around half to two hours",hez2cpx,t3_pywerf,1633095432.0,False
pywerf,"It is my full-time job. So 8.5 hours per day, approx.",hez6vhl,t3_pywerf,1633097556.0,False
pywerf,"It's my job, so I'm obviously doing it at least 40 hours a week at a minimum.  I spend an additional 10-20 hours programming for pleasure, doing the fun things my job doesn't require.  Exploring data structures, learning ways to make programs more efficient, and learning system design.",hez7kb0,t3_pywerf,1633097871.0,False
pywerf,I spend 15-30 mins,hezav62,t3_pywerf,1633099360.0,False
pywerf,I’m a tech lead at a mature startup and today I have spent 0 seconds coding.,hezdftk,t3_pywerf,1633100480.0,False
pywerf,3-4 hours,hezetzw,t3_pywerf,1633101090.0,False
pywerf,8-10/day,hezsd8n,t3_pywerf,1633106893.0,False
pywerf,Sadly I seem to spend WAY more time in meetings and fucking with CI,hezvm9x,t3_pywerf,1633108281.0,False
pywerf,Couple hours at most,hezxu0s,t3_pywerf,1633109238.0,False
pywerf,When I have a job I spent like 10 hours from Monday to Friday. A tip for you is before coding you should do physical exercise it helps a lot.,hf02y2o,t3_pywerf,1633111465.0,False
pywerf,2 hours at least cause I'm in school too,hf0fai7,t3_pywerf,1633116856.0,False
pywerf,Check messages plz!,hexgq8j,t3_pywerf,1633055374.0,False
pyrzy8,"`a+b` means `a` *or* `b`, so the order does not matter. This regular expression matches the strings `a` and `b`, but not `ab` and not epsilon (the empty word).

`ab` means `a` *and then* `b`, so the order does matter. This regular expression matches only the string `ab`.

`a*` means a sequence of *zero or more* `a`s. E.g., `(a+b)*` matches epsilon, `a`, `b`, `aa`, `ab`, etc. In fact, this regular expression matches any string over `{a, b}`, because it literally means ""zero or more occurrences of the letters `a` and `b`"".

Another example: `a((a+b)*)` matches the words which start with `a`.

Another example: `((ab)*c)*` matches `abc`, `ababc`, `ccc` and so on, but *not* `abab`, `cab`, or `abcbac`.",hewiuik,t3_pyrzy8,1633038996.0,False
pyrzy8,"Wouldn’t you’re last example, in human language, read “any number of ‘ab’ followed by any number of ‘c’? Therefore abab is indeed in the language?

EDIT: I see it now, thank you!",hfi71gy,t1_hewiuik,1633456297.0,True
pyr63w,"When I was 14 I understood everything, now in my 40's I don't think I really understand anything.

Really it took me two years in a competitive environment to be good enough.",hew9w1h,t3_pyr63w,1633035085.0,False
pyr63w,"I came here to say almost the exact same thing. When I was 17 and thinking about college, I didn't want to study Computer Science because I already knew everything and how much more could there be to learn. Ended up graduating with a CS degree, then a MS in CS, and I spent the last 16 years working as a software engineer. Still trying to figure the damn thing out.",hewbqtr,t1_hew9w1h,1633035862.0,False
pyr63w,9 i had only my pc and Internet as friend beside school,hewq4bh,t3_pyr63w,1633042379.0,False
pyr63w,"I had a good grasp of programming at age 10.  Learned BASIC, and based on what I knew I could do simple programs.  Games (other than guess the number) were cool but foreign and I did not know how to pull off that cool stuff they were doing.

(Remember the qbasic demo game where you were an ape/gorilla and you threw bombs at the other player?  Yeah, I loved it, but even looking at the code, that young, couldn't understand it.)

By around age 12 when I learned Euphoria, that's when I truly had a good grasp of programming.  Euphoria taught me a lot, and it was them I knew I wanted to learn all the languages of programming.

Fast forward to college, I learned C++, Java, python, R, promela, Processing, PHP, SQL, BrainF, and a lot of other languages.  That's when I put my grasp of programming in general to the test.  Attempted to learn a lot of different languages as fast as I could.  Boasted I could learn any programming language to fluency in 72 hours or less.  (And was right.  Still can)

My first job is where I learned programming is good and all, but learning the tools you need to work effectively is more important.  I cannot learn every tool to fluency in 3 days, yet.  Some, like bootstrap, sure.  But others, like Spring Boot, or NodeJS, or Git, take longer.

But I digress.  Ultimately, 12 years old is when I actually had a good grasp of what I was doing.  And that was in the year 2000.  21 years ago.

The answer to this question will vary, but I believe you can get a good grasp of programming in a relatively short time.  Less than 5 years.  Took me around 2 to actually get it, but I still thought I had that grasp earlier, just didn't understand why I couldn't understand complicated programs.

I kind of want to see if I could understand that gorilla game now, now that i have a much better idea what I'm doing, programming-wise.",hewz85o,t3_pyr63w,1633046863.0,False
pyr63w,The better question is how many years did you take to have good grasp on programming.,hex7p4u,t3_pyr63w,1633051016.0,False
pyr63w,22. I had been trying to learn it for a couple of months and then suddenly one day while fiddling around with Nodejs it suddenly came to me. I had a function return a string based on a given boolean. And from that day I began understanding the concepts if programming. Lol. Remember it as it was yesterday,hex55k4,t3_pyr63w,1633049789.0,False
pyr63w,Sounds a whole lot like the lightbulb that went off in my head last night on a school assignment lol.,hex8qw2,t1_hex55k4,1633051521.0,True
pyr63w,"Theres conputer programming and software engineering.

One can be done by a newbie, the other requires a significant amount of non programming skills. People management, budgeting, business, marketing to some extent, and design skills are all needed to take a product to market, even if that market is an internal usage.

I am 38 and learned C at 14, did some other not CS stuff for a while, and made this https://imgur.com/eWIgLQl

An iPad game, that will be on the market soon. 

The difference between making an algorithm and making an application is huge. The difference between making an application and sustaining a business is also huge.

I would say it was my non programming experience that helped more than programming.",hewfald,t3_pyr63w,1633037403.0,False
pyr63w,16,hewhg0n,t3_pyr63w,1633038362.0,False
pyr63w,"About 16 to do something reasonable, forever for perfect lol.",hexs4qi,t3_pyr63w,1633061430.0,False
pyr63w,Programming feels like the easiest part of my CS classes right now. All the programming assignments are basically free but the rest is rough.,hf3kf4q,t3_pyr63w,1633183273.0,False
pylxxq,"For the model in your picture, the orange appears better for the model, but not for the red on its own, the green appears better for the blue jeans. You are also being deceived because orange is a 'warmer' color while the girl is pretty, wearing red and standing open which is a psychological indicator of passion and eroticism so you perceive it to be better on a brighter background, because it indicates the setting of the person, rather than the color on a ""duller"" green background. You also have gradations and a better finish on it, so I wouldn't exactly call it a very neutral approach to which is better.

You're looking into the psychophysics of color and the effect on cognition, not just simple programming colors anymore. The study of the color red has been recently scientifically documented, one of my colleagues, Daniela Niesta, did one of the early studies on this more than a decade ago.

As far as color PERCEPTION and cognition goes, you should really delve into the literature: [https://pubmed.ncbi.nlm.nih.gov/?term=%22Color+Perception%22%5BMAJR%5D](https://pubmed.ncbi.nlm.nih.gov/?term=%22Color+Perception%22%5BMAJR%5D)

Not sure if it's feasible to create a program today to do what you ask given the poor understanding we have of cognition as a whole thing. You could perhaps train an AI to suggest what your target group in general (on average) perceive, but then you just have a filter targeted at a specific group of people.

If you truly care about the colors, then leave subjectives out of it. Simply put a square in some colors and see what has the best contrast (if you want people to notice it, higher contrast is indeed better), you'll indeed end up with complement colors. Once you start into the marketing/artistic term of what looks good, there is a lot more that goes into it than just a pretty color.",hev4oag,t3_pylxxq,1633017719.0,False
pylxxq,"Hey! This is exactly the answer I needed! Thanks for taking the time to write this up. A relevant pubmed helps a lot.

Time for me to enter the rabbit hole that is color psychology.",hewhuzx,t1_hev4oag,1633038551.0,True
pylxxq,"Even if we can agree on the subject aspects of color perception, i.e. there are some that objective with regards to what we respond to and how our brains react and so on, in order for there to be a ""mathematical"" way to determine a color there would need to be some order in the way in which the range of colors is encoded.  I can see that there is *some* order, but I'm not sure that the the domain of possible wavelengths of light map onto that range of color numbers in a way that mirrored the physics of it.  Not that it would need to the physical relationship necessarily for it to be ""mathematical"", but that would be my first guess.",hew276g,t3_pylxxq,1633031665.0,False
pyj6av,">Is it theoricaly possible to have an .rar file that contain itself in?

Not sure about the .RAR file format in particular, but this [Stackoverflow thread](https://stackoverflow.com/questions/3169246/zip-file-that-contains-nothing-but-itself/3169256) links to an article and zip file which when unzipped produces the same zip file. I think the term you are looking for is a 'zip [quine](https://en.wikipedia.org/wiki/Quine_(computing))'.",heufvaj,t3_pyj6av,1633006830.0,False
pyj6av,"maybe ""compression quine"" is better",heuquch,t1_heufvaj,1633011929.0,False
pyj6av,"'zip quine' seems to have over twice the number of results in Google compared to 'compression quine' (although of course, quantity may not equal quality...)",heurowq,t1_heuquch,1633012294.0,False
pyj6av,op is asking about rar quines. js its a more general conception lol,heuru4e,t1_heurowq,1633012356.0,False
pyj6av,"Sorry, 'js'?",heus97g,t1_heuru4e,1633012538.0,False
pyj6av,just sayin,heusal7,t1_heus97g,1633012555.0,False
pyj6av,Ah ok. Gotcha.,heuse11,t1_heusal7,1633012596.0,False
pyj6av,Ah thought it was JavaScript,hexage8,t1_heuse11,1633052346.0,False
pyj6av,"Answering the first question:

Let h be a hash-function, e.g. md5. Then there are inputs x1 != x2 such that h(x1) = h(x2). (To make a hash-function safe the values x1 and x2 should be very hard to compute.) Now take a .txt file containing a random hash, say h(x1). Create another .txt file, a ""nonce"". Compress the hash .txt file and the nonce into a .rar file x2. Then check if h(x1) = h(x2). By altering the nonce you are able to get different values for x2 and therefore also for h(x2). So you may be able to brute force it.

Some time ago I stumbled across the same kind of problem in a different context. It is in some way a ""chicken and egg"" problem. Say you want to issue a digital certificate, e.g. a .pdf file, with a digital signature. What you want to do is: Hash the .pdf file, sign the hash and append the signature to the file. But now the hash of the .pdf file changed! So your institution cannot verify the certificate with the produced signature anymore. The solution: The signature is appended in a part of the document that does not get hashed.",heuii62,t3_pyj6av,1633008140.0,False
pyj6av,"In theory, yes. Both of your ideas are possible.

The .rar containing itself will be a so-called rar-bomb. This is generally prevalent as zip-bombs.

For more extreme examples, take a look at [PoC||GTFO](https://www.alchemistowl.org/pocorgtfo/). A small excerpt:

> **Technical Note:** This file, `pocorgtfo19.pdf`, is valid as a PDF document, a ZIP archive, and a HTML page. It is also available as a Windows PE executable, a PNG image and an MP4 video, all of which have the same MD5 as this PDF.",heujthc,t3_pyj6av,1633008769.0,False
pyj6av,"> The .rar containing itself will be a so-called rar-bomb. This is generally prevalent as zip-bombs.

Zip bombs aren't compressed files that contain themselves, they're compressed files that require a very large/infinite amount of resources to decompress.",heunldv,t1_heujthc,1633010498.0,False
pyj6av,"However, they can be recursive. I wasn't aware of the ""quines"" which were benign self-extracting archives, but the hostile ones, which are recursively extracting themselves (not random-ish data like most bombs) as infinite extractions.",heuonhy,t1_heunldv,1633010970.0,False
pyj6av,"The “text file is an md5 hash of a rar file that contains it” is the type of problem that often has an attractor. 

Create a file that is the length of an MD5 hash but contains all 0’s (or some other random seed). Make a RAR of it. Compute the RAR’s MD5 hash. Put that number in the text file. Repeat.

Sometimes this leads you on a long journey of seemingly random MD5 signatures. Sometimes it quickly settles on a value whose RAR hashes to itself. Or bounced back and forth between two values, where A RAR-hashes to B and B RAR-hashes to A.",hey9zmy,t3_pyj6av,1633075088.0,False
pyj6av,"So the only way is to find and md5 hash that when he is ""hashed"" it return itself?",heymksa,t1_hey9zmy,1633086227.0,True
pyj6av,"Well .rar is based on a compression algorithm.

And .rar is a container, so u want to contain a container  that can contain things.

Deffo possible with scripts and emulation I think. Exactly how possible..... but I think this can be done.",heufqb9,t3_pyj6av,1633006759.0,False
pyiccs,"The gist of the issue is [IEEE754 floating point standard](https://en.wikipedia.org/wiki/IEEE_754).

An example is present [here](https://en.wikipedia.org/wiki/IEEE_754#/media/File:Float_example.svg), but read the article on the subject. Being familiar with it is important.",heucho2,t3_pyiccs,1633005019.0,False
pyiccs,Thanks for the help!,heukxec,t1_heucho2,1633009292.0,True
pyiccs,It's basically like scientific notation if I remember correctly,hewu67a,t1_heukxec,1633044351.0,False
pyiccs,Craigndave explain [this](https://www.google.com/url?sa=t&source=web&rct=j&url=https://m.youtube.com/watch%3Fv%3DdcIDAnfp8Dc&ved=2ahUKEwiJ_sXn3abzAhUMzDgGHTJKB0oQwqsBegQIBhAD&usg=AOvVaw1ChWamoK0SfjSVwLcsbpNG) topic and many others very well,heueo49,t3_pyiccs,1633006207.0,False
pyiccs,Thanks! Exactly what I was looking for,heul3jv,t1_heueo49,1633009371.0,True
py9o7e,Andrej Kaparthy has a good example doing something similar to this in his article “The Unreasonable Effectiveness of RNNs” (2015).  Pretty sure the github repo is linked.  There’s better ways to do text generation now but it’s a fun example of character by character prediction.,hesz495,t3_py9o7e,1632969571.0,False
py9o7e,Sounds good! I'll give it a look!,het5mtk,t1_hesz495,1632972978.0,True
py9o7e,"I could be wrong, but this just sounds like text prediction. I'm not even sure what it is called, but when you type something on your phone or whatever, words or phrases are predicted based on what you have already typed. There are probably some sophisticated solutions out there.",hesygpk,t3_py9o7e,1632969244.0,False
py9o7e,"Also if you have a percentage or likelihood, wouldn't it be better to just go down the list instead of using ""random"" selection? Hope any of that helps, sorry if it doesn't.",hesyrl7,t1_hesygpk,1632969393.0,False
py9o7e,"Hey u/thoroughlyswooped, thanks for the response! What you mentioned is actually a pretty good application of this same concept, but honestly I was thinking much simpler: Just taking an input phrase from a user and seeing the quickest that we can manage to rewrite the entire phrase (once again, not sure how useful this will be)

It might be better in some cases just to go down the list, but my thought process was that to get to common letters like ""r"" or ""s"", you first have to go through unusual letters like ""q"". And then almost always, ""q"" is followed by ""u"", etc. But yeah, you might be right, I can just honestly rearrange the array of the characters that my program selects from to somewhat match the most used letters in the alphabet. Honestly, the main reason I even came up with this idea was to implement some sort of logical coding and possibly a machine learning algorithm to speed up an already pretty-optimized script.",het5lbc,t1_hesyrl7,1632972955.0,True
py9o7e,"There's a few ways you could do something like this. You could use a look up table with [data on letter frequency that already exists](https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html) or creating that data yourself by looping over a dictionary and counting how often letters appear. Ordering your letters like this would help you look at letters like ""e"", ""t"" or ""s"" before ""q"". 

The next couple ways involve some machine learning. Now the simplest way to do something like this is with a genetic algorithm. You create an array of say 1000 random strings of the same length and compute how close they are to your target and use the best to make a new set of strings and repeat. I wrote something like this a while ago. It's slow and memory intensive, but it's one solution. 

Then you can start looking at RNNs. These are special neural networks that work on sequential data. You train it on a bunch of words and it will pick up patterns like ""u"" always follows ""q"" or that ""s"" is your most likely first letter and you check each character in terms of what the RNN spits out.",heu0h90,t1_het5lbc,1632996777.0,False
py9o7e,"Sound like you're looking for Markov Chains?

https://brilliant.org/wiki/markov-chains/",hetlccx,t3_py9o7e,1632983209.0,False
py0ewf,"It's algorithms

https://en.wikipedia.org/wiki/Algorithm?wprov=sfla1",heso9zq,t3_py0ewf,1632964429.0,False
pxx9fk,"It's not about validity but that the body has not been tampered with, the signature uses cryptography over a digest of the body to detect any alterations to the body. So you can have a token that's secure and self contained as in doesn't need the server to store anything about the token only to have the private key to check the signature of any tokens it emit.

https://youtu.be/DMtFhACPnTY
https://youtu.be/b4b8ktEV4Bg",heqiz11,t3_pxx9fk,1632931249.0,False
pxx9fk,"Carrying that to your example we check that a1a1 have not been tampered with. For example if the server signs a token with body a1a1 and the signature for it is b2b2, and the user changes the body to a1a2 then the signature b2b2 will not be valid anymore and the server will know to reject the token.",heqjro1,t1_heqiz11,1632931571.0,False
pxx9fk,So how does the signature know if the body has been tampered with?,heqka22,t1_heqjro1,1632931778.0,True
pxx9fk,"The backend calculates the signature based on the headers and body again. If it matches the signature on the request, it hasn’t been tampered with.

The signing is done with a private key that is very secret. You can’t conduct the signing unless you have access to that

So when you issue the token, you sign it and put the signature on the token. Then when you read it back, you sign it again and make sure it hasn’t changed.",hercgto,t1_heqka22,1632943239.0,False
pxx9fk,">The backend calculates the signature based on the headers and body again. If it matches the signature on the request, it hasn’t been tampered with.

Not always. When JWT signatures are computed using asymmetric keys, the backend doesn't need to (and often can't) recalculate the signature based on the headers and body. Rather, it just uses the public key to decrypt the signature back to a digest, and then it recomputes the digest from the header and body and compares the two.

This is actually where JWTs are most useful. When the entity that needs to verify the integrity of the JWT is not the same entity that produced and signed the JWT to begin with, it often doesn't have access to a database against which to compare the contents of the JWT header and body. But it can still verify the token's integrity, since it has the public key.

In contrast, if the verifier is also the signer, JWTs are basically never necessary, given that you could just use a random bearer token instead and store the associated header and body in a database; they just save you a trip to the DB at the cost of losing the ability to revoke the tokens, which isn't worth it in most applications.",heswkbk,t1_hercgto,1632968316.0,False
pxx9fk,"This, it's pretty much how you do it",heseosw,t1_hercgto,1632959918.0,False
pxl0k4,"Consider two expressions:

2x + 3000

x^2

When the value of x is small, the first expression is bigger than the second, because even when x = 0, the first expression is 3000. But when x is big (about 54 or more), the second expression is bigger. And from that point on, it will quickly get much bigger than expression 1.

In CS terminology we say that the first expression is *linear* and the second expression is *quadratic*. The quadratic expression grows much faster - it is *asymptotically bigger*.

However, each of these expressions is a polynomial of x. Consider now this expression:

2^x

This is not a polynomial. It is an *exponential* function, and it grows much faster than *any* polynomial. The practical consequence of this is that it grows too fast for any technological improvement in computer power to keep up with it. Thus, the dividing line between polynomial algorithms and exponential ones is the practical barrier between useful algorithms and ineffective ones. For such situations, we need to use approximate algorithms and heuristics instead.",heo9e6m,t3_pxl0k4,1632881449.0,False
pxl0k4,Follow up dumb question. When you say it’s “asymptotically bigger”. What is it asymptotic too? I understood that word to mean “close to but never approaching” but not a measure of magnitude?,hepufea,t1_heo9e6m,1632920575.0,False
pxl0k4,"No questions are dumb, when one is genuinely trying to understand something.

In the above example we considered what happens when x becomes very large, i.e. approaching infinity. This is why it's called asymptotically bigger - second expression is bigger when x is ""close"" to infinity, (very) loosely speaking.",hepzgyn,t1_hepufea,1632923023.0,False
pxl0k4,"An asymptotically bigger expression always overtakes a smaller one *at some point* when x is rising. Consider:

10000x + 9999999999

x^2

The first expression is huge. It will be bigger than x^2 for a very long time. Nevertheless, as we raise x, there will eventually be a point when x^2 overtakes the first expression.

So, to compare two expressions asymptotically is to ask which one is bigger when x is arbitrarily large (aka ""approaching infinity"").",heq3b46,t1_hepufea,1632924750.0,False
pxl0k4,"Basically it takes into consideration the derivative and the starting conditions, and wraps it into a complex name for little actual gain.",hes491m,t1_heo9e6m,1632954957.0,False
pxl0k4,"It's asymptote (not asymptomatic), which is a line which gets continually closer to a curve but never touches it.   
So if you were to plot a curve on where one axis is the number of elements being processed and the other axis is the amount of time it took the algorithm to run, then the asymptotic size of the algorithm is the asymptote of the curve you drew.


Or like you're five: it's how long an algorithm takes to run if you put lots of elements in it",heqypa6,t3_pxl0k4,1632937554.0,False
pxl0k4,"Imagine you are in a shop with 2 isles and 1 staff member. You want to buy some chocolate, but you are in a hurry and want to get out as soon as possible. You can either check each isle or you can ask the staff member. The 

In this situation most people would probably just look for the chocolate as there aren't many isles to check, and you'd probably just lose time walking over to the staff member, getting their attention and then going back to the correct isle.

However, now imagine you are in a shop with 100 isles. Unless you have crippling social anxiety, it is probably way faster to ask the staff member where the chocolate is, rather than checking each individual isle.

Now consider the hellish case where there are 100,000 isles. Even if you have crippling social anxiety and take ages to muster up the courage to ask where the chocolate is, you're probably still better off than checking each individual isle.

You can consider the ""isle checking algorithm"" to take linear time with the number of isles; the more isles there are, the longer it will take you to find the chocolate. On the other hand, the ""staff asking algorithm"" takes a constant amount of time to find the chocolate; you don't need to spend more time in the store the larger it is (discounting that you may need to walk a larger distance to reach the chocolate. Either way it's still faster). 

The searching algorithm is asymptotically larger than the asking algorithm. This is because each time you increase the ""problem size"", it always requires at least as much of your resource (in this case time). 

Not sure if this is an acceptable explanation for five year olds, but I think I've gotten quite close.

I have no idea how to convey what a polyomially larger algorithm is without getting into some basic maths. To put it concisely, function A is polynomially larger than function B if the largest power in function A is higher by at least 1 than the largest power in B. Consider the above problem:

n - number of isles (the problem size)

t - time to search a single isle

c - time to ask the staff member (and muster up the courage to...)

Algorithm A (isle searching): takes tn time to find the chocolate in the worst case. We can write this as a polynomial: A(n) = t \* n\^1

Algorithm B (staff asking): takes c + t time to find the chocolate in the worst case. Writing this in polynomial form: B(n) = (c+t)n\^0

The largest power of n in A is 1. The largest power of n in B is 0. The largest power of n in A is larger than the largest power of n in, hence A is polynomially larger than B (in n).",herm4h1,t3_pxl0k4,1632947136.0,False
pxl0k4,"If we are comparing two algorithms asymptotically, we are basically saying ""Computers are complicated and there are a lot of different types. We can't know ahead of time what computer this will get used for, or how it will affect how long it takes."" and comparing the two while ignoring all the complex little things that affect how long it takes on those different computers. If we compare them this way, and one always takes more time for any amount of stuff we give the two to process, we say it is larger.",heodlvb,t3_pxl0k4,1632883468.0,False
pwwkmh,"When you play a video on YouTube, your computer is just grabbing a bunch of images from the server and playing them back to you in the order they come in. It actually isn't that hard for a computer to display 60 images a second to you, no matter what resolution they're in, as long as they're already \*rendered\*. Before a video is uploaded to YouTube's servers, it's rendered by the content creator. When a video is rendered, the GPU does all the intricate math that's required for every single one of the 60 images a second - every frame - to look right when you view it. Blur effects, shading, particles, anything under the sun - your GPU will keep track of the state of each of those effects and decide how the next frame should look with respect to the previous frame.

For example, suppose a content creator wants to use an effect such as high-detail snow with motion blur. When the content creator places the effect in the video editor, they aren't just placing an overlay on the video and calling it good - the GPU needs to decide what the final image should look like for every frame. The GPU would have to take into account the locations of each snowflake, the colors of the image behind it, how the snowflake was moving in the previous frame, and a bunch of other stuff, and it has to do this for every one of the 60 images a second. It has to make decisions on how the snow falls, and in some cases take into account a physics engine to do this. Once the creator's computer has done all the math required, the video is ""rendered."" Instead of being a video file with an instruction telling the computer ""Do a snow effect at this part"", it is now a series of pre-determined images in sequence at many frames per second, and it will look the same every time.

Video games don't consist of pre-rendered videos, and this is where our GPU really shines. Many modern video games just consist of a bunch of 3D models rendered as 2D images (since a monitor can only display 2D images). In other words, to keep a decent frame rate, we cannot effectively pre-render frames, yet we need to render the video we need as soon as the frames are required, before your eye notices the time taken to render the frame (video lag). This is why computer animation for video games isn't quite as pretty as the animation used for cinema - there are limitations to the detail that a computer can render on the fly before your eye notices something is wrong.

Does that make sense?",hejxkmg,t3_pwwkmh,1632799544.0,False
pwwkmh,"I'm not sure this is quite accurate. Video decoding is usually offloaded to the GPU, it depends on the browser settings, the incoming video codec and the GPU/drivers to determine weather it uses cpu or GPU. Videos are a lot more complex than just frames in most cases. In this circumstance, it is likely that the GPU is doing the hard yards for decoding, but because it is such a simple process compared to what the GPU is designed for, that would be why the usage appears low- it's not a taxing operation for the hardware decoder. The CPU also plays a role in the actual streaming of the data to the GPU and that might be why the usage is high. Hardware acceleration could be disabled on the browser as well.",hekf8zh,t1_hejxkmg,1632811003.0,False
pwwkmh,"Great response! All recent GPUs include an ASIC for h.264 and h.265 codecs. Overhead is now displayed separately in Windows task manager for video encoding, decoding and 3D rendering",hel48tj,t1_hekf8zh,1632831255.0,False
pwwkmh,I’m afraid that’s just not true. Your browser doesn’t download a video a full frame at a time from YT. It streams the compressed and muxed content and does client-side demuxing and decompression (and then scaling to the requested display size and other post processing). Some of that work can be offloaded to GPUs.,hekhmdc,t1_hejxkmg,1632812918.0,False
pwwkmh,"Decoding is not nearly as computationally expensive as rendering though. Compression algorithms and scaling are basic linear transformations of simple low dimensional matrices. Rendering on the other hand involves a ton of high dimensional matrix multiplications. GPUs have more space to keep more of the calculation in local memory and are better optimized for this type of linear algebra. they have some different low level operations that prevent the system and In turn your applications from using the gpu whenever they want. 

Essentially what I’m trying to say is, unless that low level GPU support is built into the application, you won’t get the benefits of the GPU.",helb52e,t1_hekhmdc,1632834957.0,False
pwwkmh,"this is accurate but also a bit of a red herring. 

Those things are more expensive, but decoding high resolution high frame rate video with a modern codec is still pretty computationally expensive and very commonly offloaded to a hardware accelerator

You're right that the software has to support it, and the hardware does too, and this support is even limited to specific codecs so it's possible that some videos using different codecs won't be accelerated at all. But to OPs. point.. it's reasonable to expect a modern setup playing commonly available videos to use hardware accel and it's worth looking in to why it is not in this case",henlnuz,t1_helb52e,1632870308.0,False
pwwkmh,"Why would you expect accelerated processing when your cpu can already play basic videos at high frame rates and resolutions? 

Your basic hardware can handle this type of load fairly easily. Chrome books, tablets and cell phones easily do this work without fancy dedicated GPUs. It doesn’t make sense to rewrite algorithms specifically for GPU acceleration when your CPU is capable already.

Video editing or streaming makes sense to me to want hardware acceleration.",heothls,t1_henlnuz,1632892565.0,False
pwwkmh,">Why would you expect accelerated processing when your cpu can already play basic videos at high frame rates and resolutions?

First because most people can't. Because it can be done more efficiently, which results in less demand for cooling or if battery powered longer battery life. And in either case can free up your cpu cores to continue doing things that can't be hardware accelerated.


>Your basic hardware can handle this type of load fairly easily. Chrome books, tablets and cell phones easily do this work without fancy dedicated GPUs.

You're conflating hardware accelerating with fancy GPUs.  The only relation is that fancy GPUs do have chips specifically made for hardware acceleration, and on a gaming desktop those are likely the best ones you have access to.

Those devices you mention almost all include hardware accelerated decoding.


Just as an example, Apple uses the M1 in their latest generation of devices both desktop and mobile. It does HW accelerated decoding of all major codecs. https://www.cpu-monkey.com/en/cpu-apple_m1-1804

Latest snapdragon used in most android phones and tablets? Also HW accelerated decoding. https://www.cpu-monkey.com/en/cpu-qualcomm_snapdragon_888",hf90kqh,t1_heothls,1633283137.0,False
pwwkmh,"I’m not disagreeing with your point that hardware acceleration can be done on a general purpose CPU. 

Wasn’t OP asking specifically about why their GPU wasn’t being used for video playback? My point is that it’s not necessary.",hf9ngaj,t1_hf90kqh,1633292289.0,False
pwwkmh,Yes! Thanks for the super detailed response!,hek0o46,t1_hejxkmg,1632801234.0,True
pwwkmh,Your gpu has a dedicated video encoder/decoder separate from the cores it uses to render games or whatever else,hem9bjl,t3_pwwkmh,1632849516.0,False
pwwkmh,"Playing a video is like flipping through a picture book.

Playing a video game is like drawing each page as you flip through it.

Short answer is it's a joke for them to play a video. High-end graphics cards are one of the most powerful compute devices we have today. It doesn't break a sweat.",hekxof5,t3_pwwkmh,1632826986.0,False
pwwkmh,Even modern GPUs can hiccup while attempting h.265 real-time software decoding. This is why an ASIC is included on the die for video encoding/decoding,hel4fh7,t1_hekxof5,1632831366.0,False
pwwkmh,"Correct, I guess i should adjust the term ""render"" - i meant ""play"" a video",heldj5t,t1_hel4fh7,1632836092.0,False
pwwkmh,no math to do. video data to output is easy,heto1n1,t3_pwwkmh,1632985402.0,False
pwwkmh,"When playing a video at high resolution the initial concern is network. If the network can download high resolution video files then there isn't any limitations in sending or receiving data. So this could be your first road block, but isn't what you're asking. Let's say you have an extremely fast internet connection, so you can download large images. 

These images get put into a cache or buffer to be processed. When downloaded and moved to the temprorary cache this is binary data. 0's and 1's. This data has to get converted into information that will tell the computer what every single pixel on the screen is going to do. The higher the resolution the more information being sent. 

Lets break the task down to a single image. The image that will cover the screen is a 2d matrix. The data will determine at every place within the matrix what the color should be, converting the binary information into hex data that can represent the entire color wheel. After determining where everything should be placed, the info is sent to be converted into screen pixels and displayed on the screen. This process is repeated for every frame of video.

Conventionally, the gpu is much better at handling this process as the gpu can handle many concurrent processes with threads. Think of the gpu as doing the most of the calculations here. The GPU is a lower level cache. So everything the CPU determines will pass to the GPU. 

That is why having a weak CPU can create a bottleneck, and not be able to pass much information between input/output operations and the GPU that is doing the computations. 

There could also be the problem of having too powerful of a GPU compared to the CPU and the bottleneck will occur at the CPU, not allowing you to get full performance out of the GPU. 

Ideally you want to maximize the performance of both the CPU and the GPU. I am not sure exactly what your computer has, but the layout can be a significant factor in this. If you are running multiple cores in your CPU it can be slower to pass data between cores than to compute in one core for example, or send to a lower cache, and your higher level caches will handle that information. 

Video cards are mainly used for video games, where an entire 3d room with objects is being rendered as compared to a 2d image. The cpu will most likely be responsible for video calculations as compared to a video card also called a graphics card, in terms of watching videos.",hetwzgw,t3_pwwkmh,1632993644.0,False
pwwjqp,"You’ll have to look at computer history, but in the beginning someone literally looked at light bulbs or cardboard stock that had ones and zeros for both inputs and output.

So basically you punched in a mathematical question and through mechanical gears and later electronics it would spit out a mathematical answer.

Once you get some logic gates going and a way of saving results in between, and then later a way of reading instructions from storage, then you make things more complex resulting in the first assembler languages, then programming languages, then compilers for those languages then once you have a compiler, you can rewrite your compiler in your new language and run this through your old compiler to get a new compiler.",hek1t44,t3_pwwjqp,1632801891.0,False
pwwjqp,The first “computers” didn’t use electricity though. Babbage’s difference engine and the Jacquard loom could both be cited as examples of “computers.”,hemqyty,t1_hek1t44,1632856793.0,False
pwwjqp,"So could the abacus then. Babbage's difference engine was a calculator for approximating polynomials, it didn't really have things we consider computers to have (such as a stack, memory and programmable logic) and the Jacquard loom was a programmable loom, but basically operated as an ""operational amplifier"".

They each had their contributions, but I wouldn't consider them a programmable computer.",hemtr8l,t1_hemqyty,1632857927.0,False
pwwjqp,"Maybe not the difference engine, but switch the cards in the loom and you get different patterns…

After a point it breaks down into questions of language, and who needs those?!",hgjttdx,t1_hemtr8l,1634169792.0,False
pwwjqp,Computer organization is a class you need,hekqshk,t3_pwwjqp,1632821392.0,False
pwwjqp,Yes,hejrnb3,t3_pwwjqp,1632796594.0,False
pwwjqp,Check our crash course computer science! Great series of videos that will explain a lot!,hektpdd,t3_pwwjqp,1632823882.0,False
pwwjqp,link pls,hey2gq4,t1_hektpdd,1633068617.0,True
pwwjqp,https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo,hey2os4,t1_hey2gq4,1633068796.0,False
pwwjqp,"Here you go. I think this would be a good starting point for you.
https://youtu.be/QZwneRb-zqA",hekfd9c,t3_pwwjqp,1632811095.0,False
pwwjqp,"3 years of university courses, tests and exercises, just to understand the computer and all the math behind it. but its ok i think ull be able to understand it by asking a question on reddit 😅",hemf1md,t3_pwwjqp,1632851871.0,False
pwwjqp,You’re asking two different questions. Is your question about how computers were first created or is it how was software first created?  The concept of the computer predates the concept of software by about 100 years or so.,hemrfug,t3_pwwjqp,1632856985.0,False
pwwjqp,"i am not that technical when it comes to computer


but this is how im gonna put it, for example a brain was installed with a thought....


brain is the physical part (hardware), im just curious how the thoughts are put inside of that brain.... how a simple machine back then was able to have codes and stuffs",hey2oi4,t1_hemrfug,1633068790.0,True
pwwjqp,"Think about it like this: if you have a light switch, you have a program entirely comprised of hardware that can hold two “thoughts,” an on thought and an off thought. You go out and buy another switch. Now you can have four thoughts, right?  on/on, on/off, off/on, and off/off. Add another switch and now you can have 8 thoughts, then 16, then 32, then 64…etc. if you’re smart about what these thoughts represent, you can get to all kinds of things. 8 switches is enough to represent all the characters on the keyboard, for instance. String enough of these 8 switch representations together and maybe you get a cookie recipe, or an email, or all the works of Shakespeare.  Does that give you a general sense of it?",hgjtj5s,t1_hey2oi4,1634169656.0,False
pwwjqp,"omg, best explanation",hgw1n57,t1_hgjtj5s,1634409728.0,True
pws5tm,"We probably can.

In fact, we are trying to; with the advent of Quantum Computing we are starting to rethink how things are programmed, and how they can be made better.

I think that the mentioned architectures are the _simplest_ you could come up with, and likely the easiest to understand, but that doesn't always equate to best, most reliable, or fastest.

I do think they can be made faster, more reliable, and/or smaller, but with every improvement, it will introduce more and more complexity.  This naturally means a 'better' solution may very well be harder to understand.

I would love to be wrong, and 20 years down the line figure out an even simpler way that's unarguably better than what Turing found, but right now, I'm not certain it's possible.",hej0r33,t3_pws5tm,1632783985.0,False
pws5tm,"I am not an expert, but it is possible that machine learning may benefit from a different architecture that is more similar to a real nervous system.",hekq6ki,t1_hej0r33,1632820841.0,False
pws5tm,"Sure enough, there are candidate architectures. Neuromorphic computing is one. Which, I believe, is not even digital. The are are simply too many factors to determine whether alternatives can be viable.",hel3v7k,t1_hekq6ki,1632831029.0,False
pws5tm,"We already have rethought the implementations of computers. 

The Von Neumann Architecture is one of the earliest and simplest architectures. There are plenty of limitations with it, such as the fact that memory and data throughput gets bottlenecked by the bus. It is also noteworthily non-parallel, with everything running through one central processor.

Today, the architectures have matured to include parallel computing. [Here's](https://www.mcs.anl.gov/~itf/dbpp/text/node8.html) a quick overview of various parallel models.",hekymba,t3_pws5tm,1632827658.0,False
pws5tm,"Definitely go for it. 

Just a few things to consider, tho. First, you don't want to fall into the common cs pitfall of proposing something you think is better, but is actually a matter of preference (see how programming languages get proposed and adopted). Second, Turing and von Neumann are based on basic ideas like state and single operation processing, respectively.  These work because of how logic gates work in hardware which is honestly beautiful.  That's kinda why quantum re imagines these ideas, because things like state and processing are flexible in the quantum world (because it's essentially magic).

Lastly, go for it. Turing machines are super fun imo.  If you haven't already come across it, Michael Sipser's Introduction to the Theory of Computer Science has a boring name but it might give you the jumping off point you need.",hejagxe,t3_pws5tm,1632788668.0,False
pws5tm,"Thanks, sounds really interesting",hejlu7h,t1_hejagxe,1632793860.0,True
pws5tm,"+1 for Sipser, if you like the theoretical cs stuff you’ll love this",hekdujn,t1_hejagxe,1632809956.0,False
pws5tm,"While I can't deny its succesful application, I do have some principal concerns about the stored-program computers. Having code lie side by side with data and treat code like data has always felt a bit sketchy to me.

I don't completely like the concept of ""installing"" software on a computer, not even operating systems and device drivers. It may sound radical, but the Amiga way of doing things might not be a terrible, obsolete idea.",hekop9o,t3_pws5tm,1632819459.0,False
pws5tm,"Can you elaborate on ""the Amiga way of doing things""? 

You might already be familiar with Harvard architecture which separates code from data, but it makes the machine rather inflexible after being programmed.",helbzjz,t1_hekop9o,1632835361.0,False
pws5tm,"What I mean by ""the Amiga way"" is running your programs by booting from an external disk drive. I'm not commited to this idea, but I think it's intriguing and worth considering.

If you are interested in this direction of thinking, you might want to take a look at talk called The Thirty Million Line Problem by Casey Muratori, a very experienced software developer from the video game industry.
https://www.youtube.com/watch?v=kZRE7HIO3vk

I'm superficially familiar with the Harvard architecture. You're right, it clearly shows the huge advantages of the von Neumann architecture and possibly why it was so successful.",heluma7,t1_helbzjz,1632843497.0,False
pws5tm,"It’s definitely possible, if not happening already. The two main problems I see are 1) creating new systems may require overhauling infrastructure which is expensive and time consuming for something that already works. 2) lots of CS people love to say “I wouldn’t do it that way, or “its better if we do it this way” when in reality the difference is marginal at best. 

If it’s not broken don’t fix it!",helcgcl,t3_pws5tm,1632835584.0,False
pws5tm,"You may be interested in looking up hypercomputation. We have theoretical models of computation that are vastly superior to modern computers; however, they have limitations typically rooted in physics. If somebody could find a way to build a hyper computer, it would be a revolution in computer technology. It is probably impossible but you might be interested in reading about it nonetheless.",helaom7,t3_pws5tm,1632834730.0,False
pws5tm,[The wikipedia article](https://en.wikipedia.org/wiki/Hypercomputation) was a great read. Thanks!,heoket5,t1_helaom7,1632886996.0,False
pws5tm,"I'm not a super pro, but there are my thoughts.

First, the guy who ""invented"" nulls deeply regretted that decision, I think he called it his billion dollar mistake or something dramatic like that. One thing I want to know more about, I've heard that functional programming (Haskell, Scala, Lisp, Clojure) may reduce this null problem? (and I hope the internet can correct me on this if I'm wrong... go internet go).

Second, I remember sitting in a CS class back in 2010-ish, and we were talking about x32 vs x64 vs ARM. My teacher (seriously the smartest guy I've ever met) said that because of the existing body of software in the wild, radically changing architecture is a MASSIVE undertaking (see Apple moving to Intel in the late 90s/early 00s). Someone asked if we could scrap it all (ISA and legacy hardware interoperability, etc) and start over with a more modern architecture, what would happen. That teacher said we'd see AT LEAST a 2x-3x perf improvement for free (or commensurate reduction in power).

I think that finding a way to make it little more intuitive for programmers to multi-thread would be good too, since single-thread perf improvements have slowed a bit, but cores are relatively easy to add.",helugg3,t3_pws5tm,1632843430.0,False
pws5tm,"Possibly the problem is that changing the bases would change everything, manufacturing and its methods, development, etc. and it is surely more practical and profitable to improve with the existing bases than to change the entire industry.

Except for some change that the improvement is seen as an immense change, and even so it will be long and complex to introduce it and unseat the current bases.

Quantum Computing is a change in the bases, but even when this technology is consolidated for production, it not will easy  introduce it and  displace or cancel the current one.",hekosn0,t3_pws5tm,1632819550.0,False
pws5tm,"Digital logic design, discrete mathematics and theory of computation are def classes you'd need for that.",hektg8f,t3_pws5tm,1632823666.0,False
pws5tm,"Never ask theorists about practicalities, lol.",helld8b,t3_pws5tm,1632839601.0,False
pws5tm,"Biological computing is an interesting  reconceptualization of a computer. Instead of using electrical components, using proteins and DNA for example. The idea of storing data in DNA seems fascinating.",hemfpg7,t3_pws5tm,1632852149.0,False
pws5tm,i started in the early 70 bet people try to come up with something every day.,henayxw,t3_pws5tm,1632865438.0,False
pws5tm,What do you mean ?,henbrsi,t1_henayxw,1632865793.0,True
pws5tm,in the 70 a computer the size of a football would still have less power than a cell phone.,henehs3,t1_henbrsi,1632867005.0,False
pws5tm,[deleted],hej0dt3,t3_pws5tm,1632783807.0,False
pws5tm,Could you give me a title that I could search to explore this topic further please,hej12ut,t1_hej0dt3,1632784143.0,True
pws5tm,The field is called [Computer Architecture](https://en.wikipedia.org/wiki/Computer_architecture) and is a subset of Computer Engineering.,hekxj2i,t1_hej12ut,1632826877.0,False
pwfr13,"It's not a book, but the crash course in computer science videos are a great intro:

https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo",hegpci8,t3_pwfr13,1632745511.0,False
pwfr13,"On top of this, I strongly recommend watching Ben Eater's video series on building a breadboard computer. It gives a really good introduction to how computer hardware and logic actually works.",hehufga,t1_hegpci8,1632764455.0,False
pwfr13,"And if you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!",hel0xm0,t1_hehufga,1632829208.0,False
pwfr13,This.  Watch the related video before each course or book so you don’t get hung up on terminology or vaguely referenced overarching concepts that are so basic that no professor I’ve ever had bothered to explain them.,hehq8ff,t1_hegpci8,1632762608.0,False
pwfr13,"English is not my first language. Sorry, but I cannot understand your comment.",herod86,t1_hehq8ff,1632948047.0,False
pwfr13,"I guess it depends on how deep you wanna go into the topics. Modern Computers are incredibly complicated and you can work your whole life just wrapping your head around parts of them.

For a very basic understanding you can look at ""Code: The Hidden Language of Computer Hardware and Software"". Note that this is more of a pop-sciency book rather than some technical text.

A good next step (that has the great advantage of actually getting your hands dirty) is the nand2tetris / elements of computer systems you mentioned - although it simply can't cover *everything* in perfect detail by virtue of being a 300 page book. You'll definitely learn a ton by working through it though. I also wanna mention Ben Eater https://eater.net/8bit and crafting interpreters https://craftinginterpreters.com/ in this domain as some further practical things that can teach you a lot.

If you're more interested in the theory you can look at for example Tanenbaum's ""Structured computer architecture"" and ""Modern operating systems"". He also has a book on networks, although I can't say anything as to whether that's a good one. From there on you can of course still specialise in certain topics, but these books should give you a good starting point for further studies.",hegs5ud,t3_pwfr13,1632747091.0,False
pwfr13,"A lot of people are suggesting comp architecture books and I agree with them, but I think you should start with digital logic before that. Digital logic is probably as low level as it can get.",heh4x4h,t3_pwfr13,1632753229.0,False
pwfr13,I second this. Harris and Harris will be a good book to learn digital logic.,hejyzec,t1_heh4x4h,1632800304.0,False
pwfr13,"You're interested in two different levels: OS & hardware.

- For hardware side, you can look to Hennesy & Patterson's  ""Computer Architecture - A Quantitative Approach"".
- For the OS level, you can refer to Linus (and my) favorite OS book: Tenebaum's ""Modern Operating Systems"".

These are proper, low level textbooks which give the foundational and fundamental information.",hegpz30,t3_pwfr13,1632745878.0,False
pwfr13,"I would recommend “Computer Systems: A Programmer’s Perspective [Bryant, O’Hallaron, 3rd Ed.]” before reading Computer Architecture. From cover to cover, you’ll learn cpu architecture fundamentals like caching, instruction level parallelism, pipelining, etc in the first section. In the second half, you’ll learn about multi-threading, virtual memory, processes, the OS, etc. It is an excellent book. My University uses this book for both their Machine Organization and Computer Systems courses. The Computer Architecture book you mentioned assumes you already have this knowledge, and places some of these topics in their appendixes.

Nonetheless, the pdf version of either book can be obtained at z-lib.org for free",hehkhpv,t1_hegpz30,1632759825.0,False
pwfr13,"> The Computer Architecture book you mentioned assumes you already have this knowledge, and places some of these topics in their appendixes.

You're right, the books I've recommended may not be very beginner friendly however, since these are the books I've read and know, I've recommended them. Looks like this comment thread has a nice booklist from starter to advanced already.

Thanks for your additions.",hehyycj,t1_hehkhpv,1632766522.0,False
pwfr13,"""Computer Organization and Design"" is the undergrad-level Hennesy & Patterson book. I've read both this one and ""Computer Systems: A Programmer's Perspective"". They're both good, I personally prefer the former.",hei519u,t1_hehyycj,1632769269.0,False
pwfr13,Thank you so much for all your recommendations.,heif7lp,t1_hehyycj,1632773899.0,False
pwfr13,!remindme 2 weeks,hej7u5m,t1_hehkhpv,1632787423.0,False
pwfr13,"I will be messaging you in 14 days on [**2021-10-12 00:03:43 UTC**](http://www.wolframalpha.com/input/?i=2021-10-12%2000:03:43%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/pwfr13/how_do_i_learn_about_computer_architectures/hej7u5m/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fpwfr13%2Fhow_do_i_learn_about_computer_architectures%2Fhej7u5m%2F%5D%0A%0ARemindMe%21%202021-10-12%2000%3A03%3A43%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20pwfr13)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hej7ys9,t1_hej7u5m,1632787485.0,False
pwfr13,"If you are at all interested in or curious about the implementation side of Operating Systems, I would switch out ""Modern Operating Systems"" for ""Operating Systems: Design and Implementation"". It's also by Andrew Tanenbaum and explains Operating Systems by actually walking you through the code of one (Minix)! An albeit quite old and outdated OS by now, but that's not so important I'd say.",heh7cfl,t1_hegpz30,1632754274.0,False
pwfr13,"although compilers and OS and computer architecture are highly related you need to learn them separated and all of them are so deep and need so much effort to master.

 It depends on how much you want to learn or what you want to do.

for computer architecture I recommend ""Code: The Hidden Language of Computer Hardware and Software"" or ""computer system architecture"" by Morris Mano also you can try online courses and youtube videos.

also I recommend learning assembly language as well.",hegqkrc,t3_pwfr13,1632746218.0,False
pwfr13,"The com-sci crash course vids are great, also theres a youtube course that shows you how to design your own CPU on Logisim software. I can try and find it if youre interested.",hegrdhz,t3_pwfr13,1632746664.0,False
pwfr13,Which videos are you referring to?,hehnugb,t1_hegrdhz,1632761407.0,False
pwfr13,[https://www.youtube.com/watch?v=EgXftqD6fcQ](https://www.youtube.com/watch?v=EgXftqD6fcQ) Videos here from this channel.,hei5t7e,t1_hehnugb,1632769625.0,False
pwfr13,"nice, thanks",hei6x6l,t1_hei5t7e,1632770140.0,False
pwfr13,anytime man,hei9czj,t1_hei6x6l,1632771255.0,False
pwfr13,"A great project to do, which you will learn a lot, would be learn what happens after you turn the power switch on. Learn from bootstrap loader, BIOS, POST, Kernel, all the way till you take control.  Then you can go deeper by studying threads, parallelism, concurrency , multi-thread, core/virtual cores, and study the Memory.  We wrote a lot of papers on the computer components and memory, race condition, thread lock, just depends on how far you want to dig in to it.  There is a lot more to learn then what I have wrote off the top of my head. 

Teachyourselfcs.com is a great site 

https://github.com/ForrestKnight/open-source-cs

https://github.com/Kottans/computer-science",hegu7sk,t3_pwfr13,1632748185.0,False
pwfr13,"1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)",hehbkfn,t3_pwfr13,1632756066.0,False
pwfr13,"I like how we both post almost the exact same list practically every time this topic comes up.

But I always put nand2tetris last, because realistically I expect it to be last.",hektgqu,t1_hehbkfn,1632823678.0,False
pwfr13,"Depending on the context I tack on a paragraph about the language of the problem space and core principles of CS and add the following links as well

Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms)

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)

And finally, [play the long game when learning to code.](
https://stackoverflow.blog/2020/10/05/play-the-long-game-when-learning-to-code/) 

Edit: Used to have nand2tetris last but I then started putting Ben Eater's kit on there last as I think physically building a 8-bit computer using transistors and CMOSs wired  together on breadboards, powered by a 5v battery should be the final physical product of your long incredible journey studying computers from first principles.",hekynf4,t1_hektgqu,1632827680.0,False
pwfr13,It wasn't for me but thank you anyway,heibhrk,t1_hehbkfn,1632772218.0,False
pwfr13,[deleted],hei8vyv,t3_pwfr13,1632771039.0,False
pwfr13,This is worth a bookmark. Thank you,heiv1z3,t1_hei8vyv,1632781264.0,False
pwfr13,"I’m a fan of No Starch Press texts. Two that might be what you’re looking for are kinda similar and will get you a little better versed. They aren’t going to cover everything you need to know, so you may want to choose to focus after:

[How Computers Really Work](https://nostarch.com/how-computers-really-work)

I can’t think of the other one right now.",hegt3yw,t3_pwfr13,1632747604.0,False
pwfr13,"https://nostarch.com/foundationsofcomp

I believe you are thinking of this one. I have it and its awesome.",hegw8ag,t1_hegt3yw,1632749212.0,False
pwfr13,"That’s the one. Yeah, I read them both and while I don’t intend to do any binary notation or algorithms by hand, I at least understand it more.",hegwmkx,t1_hegw8ag,1632749411.0,False
pwfr13,"watch this channel, but get ready or hardships.

Carnegie Mellon Computer Architecture",hegy3qr,t3_pwfr13,1632750127.0,False
pwfr13,"by the way there is no text book that is sufficient for even a subset or the subject ""Computer Architecture"", even the instructor is going to refer to numerous papers from the 80's and 70's and 90's.",hegybiz,t1_hegy3qr,1632750230.0,False
pwfr13,"> Will reading a computer architecture book help me understand the OS, kernel, compilers, CPU, etc. 

If you want to learn about computer architecture and computer engineering:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU)
3. If you have the time and energy, do https://www.nand2tetris.org/

This will let you understand *what* a computer is, how a CPU works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how a compiler works etc. To get the full picture of that, you can watch the rest of Crash Course.

There are lots of other resources to help cover OS, Kernels, [Compilers](https://craftinginterpreters.com/) etc, but I think before doing those you need a good idea of what computation is and how we currently tackle that problem. (If you do nand2tetris then this will basically answer every question you have)",hehbvv2,t3_pwfr13,1632756199.0,False
pwfr13,Nand2tetris is pure gold,hekqfzz,t1_hehbvv2,1632821078.0,False
pwfr13,"I wanted to also add my plugs for 

1) Charles Petzold'- Code: The Hidden Language of Computer Hardware and Software

Walks you through the concepts of digital logic circuits to develop rudimentary processing and decision making. What is especially good is he used 18th century telegraph technology (simple switches and electromagnets) to illustrate the various logic gates and how they can be used to create much more complex circuits. At one point, he then switches to the modern world to explain the Intel family (At least up to something like 2002) of microprocessors).

Fantastic book that walks you through exactly what is going on.

2) nand2tetris.org- not enough can be said about this. Following the book, or course, you are guided (but you have to figure out the specifics) to developing an entire 16 bit computer from a single nand gate. This is a fantastic course where you spend the 1st half on the hardware, then develop an assembler, a compiler and finally an OS. So it is TRULY the full stack. 

I view it as similar to working on old cars. While modern car engines and drive trains are incredibly complex, building and playing with early versions gives you a feel for exactly what is going on beneath the hood. This would be comparable to building the engine from scratch- as in casting the metal and creating all the parts and putting it together so that it meets specifications.

I learned an incredible amount from this course and loved it. It was challenging, but truly, writing and running software to run on a computer you built from scratch- literally!- is unlike anything you can imagine. This is the full stack of computer development. After this, the mystery disappears- and yet you can appreciate the ingenuity and genius of what we use every day in a way you never could before.

Truthfully, it is worth the time and effort to do.",hehlrko,t3_pwfr13,1632760420.0,False
pwfr13,"I may be very late to this discussion, but there is a very interesting book called the three easy pieces. It is an operating systems book but it does cover almost everything. 

If you finish that book with a good understanding, you will definitely be in a very good shape understand linux based systems.

I love this book and have a learnt a lot from it.",hehwzxl,t3_pwfr13,1632765620.0,False
pwfr13,"https://www.google.com/url?sa=t&source=web&rct=j&url=https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2017/&ved=2ahUKEwi3wvi_xqDzAhU0muYKHc3qCTwQFnoECAgQAQ&usg=AOvVaw2ScodoGW6f43VvjwNUwHnp&cshid=1632793701009

This course is all you need",hejln02,t3_pwfr13,1632793765.0,False
pwfr13,"Nand2tetris has 2 parts so you could use that as a nice starting point. 
Hardware: 
Once you're done with that pick up the book on Computer Architectures by Carl Hamacher. He does a really good job of explaining the different architectures used in computers (ex. The vin Neumann and the Harvard Architecture) and gives a proper reasoning for both. You can also try your hand at learning more about a basic processor like an Intel 8085/8086 to understand how registers, ALUs and PCs work. If you really want to get your hands dirty, then Ben Eater a youtuber has put out a video on building an 8-bit computer from scratch! It's an incredible resource and he even sells prepackaged kits for the project. 

Software:
For this, I recommend you start learning C. It's one among the most important fundamental programming languages that can teach you a lot about how memory assignment, process scheduling etc. are implemented. I recommend the wonderful book ""C programming language"" written by the great Dennis Ritchie himself. The next step would be to understand how low level programs such as operating systems work. For this there are many textbooks you can refer. I personally recommend the one ""Operating Systems"" by Silberchartz. There are also many amazing resources on GitHub that are parts of college courses which allow you to implement your own tiny os! 
Compilers are another huge part of computer science and I'm currently learning about those too! I've been learning from the Stanford course on edX so you could do the same! 

Good luck!",heht7yr,t3_pwfr13,1632763911.0,False
pwfr13,"Yes, being familiar with computer architecture is necessary for understanding how everything “really” fits together.",hei4x2b,t3_pwfr13,1632769213.0,False
pwfr13,"I found building an emulator (any emulator will do, but I recommend starting with chip-8) helped me learn a lot about systems and architectures in general. Hope it helps!",heidxjy,t3_pwfr13,1632773317.0,False
pwfr13,https://www.amazon.com/Computer-Systems-Programmers-Perspective-3rd/dp/013409266X,hej90gj,t3_pwfr13,1632787987.0,False
pwfr13,"Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)

https://www.coursera.org/learn/build-a-computer?

Build a Modern Computer from First Principles:  Nand to Tetris Part II (Project-Centered Course)

https://www.coursera.org/learn/nand2tetris2",hejr6m3,t3_pwfr13,1632796375.0,False
pwfr13,!remindme 2 month,hek0k8y,t3_pwfr13,1632801174.0,False
pwfr13,"[How Multimedia Computers Work ](https://www.youtube.com/watch?v=zzV_i03EvCo) : A CD-ROM from the 90s that was pretty good at explaining how PCs work. This is a 1 minute clip of highlights but the actual thing is a masterpiece if you can find it. My recommendation would be this clip, followed by Structured Computer Organization by Tanenbaum, followed by Computer Architecture by Hennessey.",hek1c3j,t3_pwfr13,1632801615.0,False
pwahyf,"In the past **_2_** people were interested in **A** and **_2_** people in **B**.  
Now **_3_** people are interested in **A** and **_10_** in **B**.
 
Is **A** dead? No, it gained growth.  
True, **B** gained more, but most of those interested in **B** were never interested in **A** in the first place.

Also, in this case, **A** is crucial for **B**.",hefvbx5,t3_pwahyf,1632720641.0,False
pwahyf,"Sounds good, thanks a lot!",heg8q80,t1_hefvbx5,1632732297.0,True
pwahyf,I love theoretical CS more,heghsxz,t3_pwahyf,1632740641.0,False
pwahyf,"Theoretical CS is what makes new languages and sanity-checking them possible. Theoretical CS is also makes processors tick (cache optimization, new prefetching methods, what not).

It's generally harder, less fun and is positioned in lower, underwater parts of the CS iceberg. So it's relatively invisible, and less loved, but it's everywhere.",hegga9t,t3_pwahyf,1632739455.0,False
pwahyf,Thank you for the clear explanation! I'm learning compiler design and theoretical CS piqued my interest. Most of my fellow CS classmates want to make it big in the AI scene so I got confused.,hegja0l,t1_hegga9t,1632741705.0,True
pwahyf,">  Most of my fellow CS classmates want to make it big in the AI scene so I got confused.

Because it's the hot topic right now. AI is going through a resurrection with the help of new algorithms and accelerators. It's always exciting and hip when something's moving forward. It's natural to want to be part of the hype. However, one would be better if he/she thinks about the long game and plays it.

It's not bad to get specialized in any subject, let it be theoretical or practical (AI has a theory heavy side too). However, keeping an open mind and letting your knowledge cross pollinate from other subjects is good practice.

While AI and related tech is nice, useful and exciting, remember that ""*AI is just informed search*"". I don't say this to downplay what it is, but to put it to perspective.

Also, AI is not a silver bullet for everything. See this [article](https://www.getrevue.co/profile/shift-happens/issues/moire-no-more-688319). An excerpt to pique your interest further:

> “You don’t need ML,” Bryan said. “What you need is inverse FFT.”",hegl8iv,t1_hegja0l,1632742993.0,False
pwahyf,"Theoretical CS has been a relatively small field for as long as I've seen CS. All university-level CS students learn some of it, but a minority ends up working on it in any capacity.

It wasn't exactly the type of field to attract masses of students who go after the hype in the first place.",hegd0ot,t3_pwahyf,1632736669.0,False
pwahyf,Of course not. There will always be hype of one form or another. There will also always be the math behind the hype.,heg97ca,t3_pwahyf,1632732837.0,False
pwahyf,"Without TCS we wouldn't have ML/AI nor would it advance this fast, is it dead? As dead as any theoretical science.",hegmzeu,t3_pwahyf,1632744080.0,False
pwahyf,"The op talks like AI is a new thing.

Also CS is a lot more than just AI",hehkpvr,t1_hegmzeu,1632759928.0,False
pwahyf,"Of course I get that, but I've noticed trends where professors and students are more focused towards building ""intelligent systems"". I've not seen much of an interest in TCS from my peers, hence the question.",hehsl4i,t1_hehkpvr,1632763629.0,True
pwahyf,"Is ai going to write a compiler or OS?

Is it going to prove a nuclear power station won't blow up due to a bug?

Also take an intelligent car, it still runs on an embedded system with less memory and CPU your used to. So memory and processor usage is still an issue.

You don't just run AI and be done with it.",hehtr7g,t1_hehsl4i,1632764148.0,False
pwahyf,"Dude I get it, I've just seen a general shift in interest which is why I asked.",hehu5zv,t1_hehtr7g,1632764334.0,True
pwahyf,"No probs. Just explaining.

It's always good to ask questions. It's how we all learn.",hehv9g2,t1_hehu5zv,1632764834.0,False
pwahyf,AI has seen many cycles of waning and waxing interest.,hegqk69,t3_pwahyf,1632746209.0,False
pwahyf,"Most of the people that are into and use ML aren’t computer scientists at all. You don’t need to know hardly any CS in order to utilize an ML model, just some stats and probability.",heh81f3,t3_pwahyf,1632754569.0,False
pwahyf,"Yeah AI has always been ""the goal"" but in order to advance that fish you gotta advance computers and in order to do that, you need comp sci theory.  

Although we've gotten to a point where you can just do ML and kinda advance it on its own so more people can go into it in a more isolated way as well as the rumors of lots of money which always influences popularity.  But still, in order to make actual contributions you need a solid math and theoretical cs background.  

And for my final but: I find going into ML like going into software engineering, there are plenty of jobs you can get in either without knowing any fancy concepts and just being able to do basic stuff.  You don't need to know cs theory in order to code.",hegsiz9,t3_pwahyf,1632747290.0,False
pwahyf,Lol I'm reading this during my Theoretical CS class,heguwb2,t3_pwahyf,1632748536.0,False
pwahyf,"I can see where you are coming from, but theoretical computer science can never die as long as there are computers.

Then again it does concern me how many computer science “graduates” know little to no computer science concepts. When last has an interviewer asked a candidate to explain Turing Completeness or P vs NP? That’s how bad things have become.",hek9sv2,t3_pwahyf,1632807039.0,False
pwahyf,Why in the world would an interviewer ask about those things? The point of an interview is to find candidates that are fit for the job. Unless the job is within cybersecurity I don't see why anyone would care about your knowledge of P vs NP.,hem28r5,t1_hek9sv2,1632846623.0,False
pwahyf,"As a systems programmer, demonstrating knowledge of algorithm efficiency and the capabilities of programming languages shows that you are likely a programmer who cares about how to best solve problems over just having a solution that might not be optimal.

Secondly, developers who have no knowledge of such issues tend to have limited capabilities in problem domains they they would otherwise be able to solve.

If these questions are completely irrelevant then why are they taught as part of computer science? Concepts are all about computer science.",hembkoq,t1_hem28r5,1632850437.0,False
pwahyf,"> As a systems programmer, demonstrating knowledge of algorithm efficiency and the capabilities of programming languages shows that you are likely a programmer who cares about how to best solve problems over just having a solution that might not be optimal.

> Secondly, developers who have no knowledge of such issues tend to have limited capabilities in problem domains they they would otherwise be able to solve.

These are just assumptions. By that logic, each applicant should take a theoretical physics exam too, since there might be a correlation between abstract thinking and programming. The most concrete way to know a person's aptitude in a given field is to ask them questions related to that field. 

> If these questions are completely irrelevant then why are they taught as part of computer science? Concepts are all about computer science.

Because Computer Science is not just about programming.",henhdpg,t1_hembkoq,1632868321.0,False
pwahyf,"I literally don’t get what you’re trying to say there. If you are going to be a web developer then fine, you’re using tools that abstract you away from complexity. If you are working with games or applications were correctness is paramount, then you really need to understand your algorithms and related fields.

There is literally something to be said for grads from the best schools who will be able to answer such questions like it’s second nature vs somebody from Leetcode or bootcamp looking for the quickest route to earning a healthy salary, right?

I graduated computer science in 1999 and in these 20+ years I have seen even at degree level that the subject matter has been drastically reduced to lower the bar to admission and graduation, and I must admit I am not impressed. A lot of the content that I took on at bachelors level has been pushed to masters programs. What’s that?!",henjm1l,t1_henhdpg,1632869357.0,False
pwahyf,"It's not dead, but it is niche. There is still lots of interesting research going on in algorithms, data structures, and even formal languages and automata. You can consider quantum computing to be a region of TCS, which is certainly active, although way overhyped. There's even theories of AI, which is its own thing! To get funding, though, it is best to suggest possible applications (such as AI) of the research area in your grant proposals.",hegvska,t3_pwahyf,1632748990.0,False
pwahyf,Many math majors are getting more and more interested in theoretical computer science. It’s far from dead.,hf7hv9o,t3_pwahyf,1633254823.0,False
pwahyf,That certainly a theory,heg7qk3,t3_pwahyf,1632731079.0,False
pwahyf,Are you crazy?,hehkh4f,t3_pwahyf,1632759818.0,False
pwahyf,AI is overhyped. also AI is based on traditional CS. go have some fun pulling your hair out over shell sort,hg7slpu,t3_pwahyf,1633951063.0,False
pvx5dq,"The com sci ap exam is in Java.  You can find past years exams at the college board website so you can get a feel for what the exam is.

I TA both intro and AP computer science in Loudoun County, VA, and I’ve been a software engineer for 30+ years.  The curriculum we use in Loudoun for the first year is based on Fairfax County’s which is available here:

https://compsci.sites.tjhsst.edu/CSweb/index.html

It might be possible to self-direct through that curriculum.  That wont totally prepare you for the AP exam… we do that in the second year class.",hee7ft3,t3_pvx5dq,1632690190.0,False
pvx5dq,"For my AP CS course, we learned Java. OOP, inheritance, polymorphism, and recursion were present, along with core Java. Lots of code tracing questions, especially with loops/recursion. If you are learning Java before even the AP course, you will likely do fine. 

Everyone from my class got 5s, we had very rigorous practice the month before. Code tracing, multiple choice questions about Java, etc.",hee6drx,t3_pvx5dq,1632689734.0,False
pvx5dq,Teachyourselfcs.com,hedblhu,t3_pvx5dq,1632677413.0,False
pvx5dq,"Do you have any existing programming experience?

You could look through [previous years questions](https://apcentral.collegeboard.org/courses/ap-computer-science-a/exam/past-exam-questions) on the test. Direct your learning around understanding what they're asking, and what the answer is.

If you have some programming experience you could try [java koans](https://github.com/matyb/java-koans). 

If you have none you could take an [intro programming class online](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/index.htm) (assuming you can't get into the class at school).",hee37bf,t3_pvx5dq,1632688404.0,False
pvx5dq,"Nand2tetris.org teaches basics of digital logic, guides you on building basic operating system, and maybe you'll make a Tetris program or something else.",hee5hcd,t3_pvx5dq,1632689355.0,False
pvx5dq,"I took AP computer science around 2016 I believe, so it may have changed. I recommend you first learn about how computers work, as well as understanding binary/hex etc. It was part of the course when I took it. After you have a good basis understanding you can either get a head start by learning java (I like w3 schools) or something else like python or c/c++ (harder approach).",hee33to,t3_pvx5dq,1632688363.0,False
pvx5dq,it easy. just learn java and lumlmlmlu (whatever they are) diagrams. try to see how many chairs you can steal during the test.,hef9vf4,t3_pvx5dq,1632708078.0,False
pvx5dq,I can't tell you what to learn if I don't know what the test consists of. Sounds like you would benefit from learning Java though.,hed543c,t3_pvx5dq,1632674762.0,False
pvx5dq,Thank you for the reply! I dont know what might be on the test but the pre requisite was the java course.,hed66yj,t1_hed543c,1632675221.0,True
pvx5dq,"My country doesn't really have AP CS courses in high schools so I can't tell you anything useful :/

Hope you get in though!",hed6r5f,t1_hed66yj,1632675459.0,False
pvx5dq,Thank you!,hed7amy,t1_hed6r5f,1632675690.0,True
pvx5dq,This was just asked. https://www.reddit.com/r/computerscience/comments/puwwd6/cs_beginnernoob/he7zryx?utm_medium=android_app&utm_source=share&context=3,heddlgk,t3_pvx5dq,1632678184.0,False
pvx5dq,Thank you!,hedsq4g,t1_heddlgk,1632684178.0,True
pvx5dq,"If you want to learn Java, Mooc.fi has the best training platform I have seen so far and it is free.  They have fullstack, devops, data analysis with Python and other free courses there.",heg8hkc,t3_pvx5dq,1632731992.0,False
pvx5dq,cs50 (google it),hegaq6p,t3_pvx5dq,1632734390.0,False
pvx5dq,What does ap mean?,hegncqz,t3_pvx5dq,1632744306.0,False
pvx5dq,"**This word/phrase(ap) has a few different meanings.**

More details here: <https://en.wikipedia.org/wiki/AP> 



*This comment was left automatically (by a bot). If I don't get this right, don't get mad at me, I'm still learning!*

[^(opt out)](https://www.reddit.com/r/wikipedia_answer_bot/comments/ozztfy/post_for_opting_out/) ^(|) [^(report/suggest)](https://www.reddit.com/r/wikipedia_answer_bot) ^(|) [^(GitHub)](https://github.com/TheBugYouCantFix/wiki-reddit-bot)",hegndkj,t1_hegncqz,1632744320.0,False
pvx5dq,"Advance placement, basically in the US you can take these courses in highschool then you take a test at the end of the year and based on your score you can get college credit",heums17,t1_hegncqz,1633010128.0,False
pvx5dq,I’m 10th grade and I was wondering the same thing so thank you for posting this. I’m still a beginner when it comes to computer science and engineering.,hegxbj1,t3_pvx5dq,1632749749.0,False
pvo5ag,"IPv4 uses 32 bits (in 4 octets but that information is not required to answer your question) to represent each address. Each bit has 2 states, one and zero. The base comes from the 2 states and the exponent comes from the number of bits.

Think of it this way. How would you calculate the number of possible combinations for any 32 digit long number? You would multiply the number of possible states at each position. 

For example, this ends up as 2\*2\*2\*2 = 2^4 = 16 for 4 digit binary numbers. For 10-base numbers it would be 10\*10\*10\*10 = 10^4 = 10000.

Edit: Formatting. Writing on my phone",hebf54h,t3_pvo5ag,1632637419.0,False
pvo5ag,TBF S/He may asked this question because IPV4 addresses normally represented to user in decimal dot notation like 192.0.2.1 and is not binary. [This](https://en.wikipedia.org/wiki/IP_address#/media/File:IPv4_address_structure_and_writing_systems-en.svg) is how dot notation converts to binary  so you can see why it is limited to 2\^32 possible addresses.,heboj7c,t1_hebf54h,1632644635.0,False
pvo5ag,"Yes, that is a very good point which I neglected in my answer. Thanks for adding it :)",hebq7bg,t1_heboj7c,1632646012.0,False
pvo5ag,alternatively: 256^4 = 2^8^4 = 2^(4⋅8) = 2^32,hebysze,t1_heboj7c,1632653284.0,False
pvo5ag,Me thinks 2^84 might just be a smidge larger than 256^4?,hecw05q,t1_hebysze,1632670951.0,False
pvo5ag,[deleted],hednivm,t1_hecw05q,1632682075.0,False
pvo5ag,I know what he meant. Was making a joke. :D,hedssyg,t1_hednivm,1632684210.0,False
pvo5ag,"**[IP address](https://en.wikipedia.org/wiki/IP_address#/media/File:IPv4_address_structure_and_writing_systems-en.svg)** 
 
 >An Internet Protocol address (IP address) is a numerical label such as 192. 0. 2. 1 that is connected to a computer network that uses the Internet Protocol for communication.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",heboka8,t1_heboj7c,1632644660.0,False
pvo5ag,Thank you for the explanation! I understood it very well now!,hec39df,t1_hebf54h,1632656530.0,True
pvo5ag,Because a bit can have two states: it is either 0 or 1. Therefore you have 2 possible values for each of your 32 Bits in the Ipv4 address.,hebfjpi,t3_pvo5ag,1632637711.0,False
pvo5ag,"Others have mentioned the answer. I will add: this area of math is called combinatorics, and I highly recommend learning more about it. Its about counting things like this, combinations, permutations, etc. I find that it comes up everywhere in life, but especially programming.",hec2p9g,t3_pvo5ag,1632656155.0,False
pvo5ag,"It’s because of binary. When talking about possible addresses, think of it as octets which is converted to binary. When you convert a subnet or the last octet to binary, it starts with all 0s and ends with all 1s. Every time you change 0 to 1 starting from the right, that’s one address.",hebf028,t3_pvo5ag,1632637316.0,False
pvo5ag,"It uses base 2 cause the way that computers store things on the circuit, i dont remember the exactly name of the eletronic component, but it has two states, on and off, the max value that the components reserved for run ipv4 can have is 2^32",heft5z8,t3_pvo5ag,1632719104.0,False
pvo5ag,"Basically we use 2 as the base for the same reason scientists use X*10^y: it's the easiest one for our profession. Since binary is base 2 it's a lot easier for a programmer to understand and use the value of 2^32 than 4,294,967,296, or 4*10^9",hedd82o,t3_pvo5ag,1632678038.0,False
pvo5ag,"It's a set of permutations out of 2 possible states (0,1) when you have 32 digits to make a number with. In other words, if you have only 0 and 1, in how many ways can you arrange them to fill 32 spaces?

The same reasoning applies, for example, to 10^(2) = 100. All it's asking is, if you have ten numbers to choose from (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), how many permutations can you make with them to make a number with 2 digits? Well, turns out you can make 100. That is 00, 01, 02,...,75,...,99.

Also note that when I say permutation I'm referring to the case where the order of the digits matter and repetitions of the digits are allowed. So you can use 2 and 3 to make 23 and 32 and they both mean a different thing because the way they are ordered matter; and you can also repeat them to make numbers like 11, 22, 33, 44, etc.",hed5co8,t3_pvo5ag,1632674864.0,False
pvo5ag,"Look at the subnet mask.  Destinations inside of local network match the subnet mask, bit by bit.  Destinations outside of the local network will not match subnet mask and will be outside of the local network.  If this done in base 2 then it can be implemented in circuitry w/o conversion so determining LAN vs WAN is a cycle.  
Years ago they started representing subnet mask with a slack and number of bits. eg a gateway address could be [192.168.0.1/24](https://192.168.0.1/24)   and  [192.168.1.42](https://192.168.1.42) is local and [192.168.1.42](https://192.168.1.42) is on another network",hefsoil,t3_pvo5ag,1632718767.0,False
pvmav0,"Princeton, Stanford and a number of other universities have Data Structures and Algorithm courses and specializations with capstone projects on the Coursera platform that are worth taking a look at, and cover hashmaps, trees and more advanced topics.

They have free trials available but also you can apply for financial aid if you want to do them for free.",hebgzm3,t3_pvmav0,1632638749.0,False
pvmav0,Princeton has a set of free courses on this on Coursera,hebi95a,t1_hebgzm3,1632639702.0,False
pvmav0,"Hashmaps and binary trees fall more under data structures and algorithims for Computer Science,

I'd recomend buying a algorithim book on amazon tht deals with programming Primarily C++ !",heb2m6e,t3_pvmav0,1632629712.0,False
pvmav0,"Ngl I thought they taught DSA in CS courses. Hashmaps, Binary Trees are like one of the most basic of concepts.",hedyzk3,t1_heb2m6e,1632686690.0,False
pvmav0,"All the language evangelizing is horrible, meaningless advice. There is literally no language that is ""best"" for learning data structures and algorithms besides the one which makes the most sense to you. Language choice is literally irrelevant next to book/course selection, which itself depends heavily on your own background in the usual pre-reqs for a standard DS&A curriculum (e.g how solid your discrete math is).",hebjjih,t3_pvmav0,1632640666.0,False
pvmav0,"While I agree with this to an extent, using a language like python to learn data structures can be a bit messy. A list is also an array, dynamic array, stack, and queue. I believe having more of a stricter language can make it easier to become familiar with the concepts",hebkibg,t1_hebjjih,1632641410.0,False
pvmav0,"> While I agree with this to an extent, using a language like python to learn data structures can be a bit messy. A list is also an array, dynamic array, stack, and queue.

Only the built-in ones. If you're learning data structures, you should be building your own.",hecjt5l,t1_hebkibg,1632665522.0,False
pvmav0,If you’re creating them yourself then yeah. All my assessed assignments in DSA were more on developing algorithms for certain problems using specific data structures though,hecl1fe,t1_hecjt5l,1632666090.0,False
pvmav0,"Thats what I thought, Getting the grasp is possible in any language but really understanding it imo you should have dealt with the references to the objects that are being structured. I recommend Java/C/C++ or anything simmilar to that.",hebtbag,t1_hebkibg,1632648698.0,False
pvmav0,"That's just splitting hairs to me. I can point to idiosyncrasies in C and C++ and say something similar. Every language has its quirks, and there's no way for a stranger to know which language is pedagogically superior for learning DS&As for any particular individual, which is why the opinions expressed in some comments here asserting that there is an objective best language for learning DS&As is categorically wrong. A dedicated student using Python will be no worse off in the study of data structures and algorithms than a student using C. Both will be frustrated by the quirks of their respective languages along the way, and the extent to which one ends up being more frustrated than the other will be substantially up to forces beyond their control and our foresight.",hebus3u,t1_hebkibg,1632649969.0,False
pvmav0,"Yeah for sure, I agree with you. I also do believe C/C++ could be a hard one while learning data structures. In the UK almost every uni chooses to teach data structures in java. This is mainly due to C++ being too low level which shifts the focus to have to learn more about how the computer works and dealing with memory management rather than just data structures. Afaik no uni here has ever taught data structures in Python, most likely due to the reason I mentioned above.",hec0eeg,t1_hebus3u,1632654485.0,False
pvmav0,University of Edinburgh uses solely Python in their Introduction to Algorithms and Data structures.,hecejtf,t1_hec0eeg,1632663082.0,False
pvmav0,Damn that’s interesting! First time I heard that,heckmmd,t1_hecejtf,1632665903.0,False
pvmav0,"I think they took a more conceptual approach, where they didn't focus on implementation too much. This allowed them to cover more topics than a typical DSA course, but at the cost of having only two relatively small coding courseworks. I don't think that's particularly great in the current climate with high emphasis on leetcode style questions, but it is what it is.",heclg96,t1_heckmmd,1632666280.0,False
pvmav0,I disagree; if you learn how to code data structures in c you would have a much better conceptual understanding than someone who used a language like python.,hedcqt2,t1_hebjjih,1632677850.0,False
pvmav0,Data structures (and algorithms),hech6zu,t3_pvmav0,1632664310.0,False
pvmav0,"Data Structures, the best language for data structures is C++ or Java.",heb2g12,t3_pvmav0,1632629617.0,False
pvmav0,How do we mean best here? Best to learn in or the best collections in the language?,hebfupm,t1_heb2g12,1632637933.0,False
pvmav0,"I agree that statically typed languages are definitely the way to go for Data Structures and Algorithms. You don't want to interpret away the details when you're the one trying to implement them.

With that said, I feel a bit offended that you included Java but not C#. I find C# superior to Java, especially when generics come into play. For instance, just look at how to create generic arrays in the different languages.

In C#, you just do a simple 

    T[] array = new T[size];

In Java, on the other hand, you have to pull off this monstrosity 

    T[] array = new (T[]) object[size];

Seeing a bunch of these ugly lines in your Resizeable Array code really makes you scratch your head.",hec203c,t1_heb2g12,1632655674.0,False
pvmav0,"> strongly typed

Given the context, I think you mean ""statically"" typed instead of strongly typed? Like declaring the types at runtime. I believe ""strong"" typing just has to do with type coercion (adding a number to a string with no explicit cast).",hec7bsu,t1_hec203c,1632659114.0,False
pvmav0,"Whoops, you're right!",hecawx9,t1_hec7bsu,1632661194.0,False
pvmav0,"No, it's C or OCaml.",heb5h5z,t1_heb2g12,1632631301.0,False
pvmav0,"Although it's besides the point, learning OCaml really helped me appreciate the recursive nature of lists. But realistically you can replicate that on most languages.",hec7kz6,t1_heb5h5z,1632659260.0,False
pvmav0,Join r/usaco,hedlbes,t3_pvmav0,1632681175.0,False
pvmav0,Theoretical CS,hecxh8k,t3_pvmav0,1632671570.0,False
puwwd6," Regardless of programming language or operating system or hardware. Study from first principles and the programming language will come naturally based on the best fit for your problem. Whether that's embedded, cryptography, kernel, ML, DS, AI, Web, etc.. learning the language is the least of your challenges. Check out these resources. This is hard stuff. Its supposed to be. Otherwise anybody can do it by simply learning the syntax of an ""easy"" language. 

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)
7. Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms)

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)

And finally, [play the long game when learning to code.](
https://stackoverflow.blog/2020/10/05/play-the-long-game-when-learning-to-code/)",he7zryx,t3_puwwd6,1632578032.0,False
puwwd6,"What an incredible comment, this should be stickied. Please listen to this person OP. Code (the hidden language of computers) is a book that will help you rethink what a computer actually is from your current perspective. 

I particularly want to note how helpful teachyourselfcs.com is - the books they recommend, if you have the willpower/discipline to power through and understand them (that’s a *long* journey) will give you more knowledge than the average cs grad.",hea4exd,t1_he7zryx,1632612843.0,False
puwwd6,"A lot of people recommend Harvard CS50 on Youtube.

If you intend to go down the educational path of CS, I recommend brushing up on your algebra if you're weak in math. Academic Computer Science is mostly math, and there is a lot of it.

If you want to get started with coding, start with Java because you'll be using it in college. People recommend Python as a first language because it's simple. Play with Python if you don't feel confident, but you'll have to learn Java for school anyway. Someone else can recommend coding books/tutorials because I don't have any off the top of my head. Don't fall into tutorial hell though. Finish ONE tutorial, then try to build something small on your own with just the docs for reference.

Edit:  
I spoke a little ""matter-of-fact""-ly about Java. As the replies state, C/C++ is commonly used in many CS programs, especially at the upperclassmen level. Many of the schools in my region start underclassmen with Java and then switch them to C++, so I forgot to consider that other regions do things differently.   
I still think Java is a good starter language  (though I hate Java and prefer C#), but if anyone feels confident and likes the challenge, there are benefits to starting with C++.",he65jaa,t3_puwwd6,1632535975.0,False
puwwd6,[deleted],he6hjmq,t1_he65jaa,1632541897.0,False
puwwd6,Yeah we had to use C and C++ and for architecture and OS and OOP courses. The rest of the courses was up to you but students mostly used Python. Never had to use Java or C# in a course.,he6yx5t,t1_he6hjmq,1632553069.0,False
puwwd6,"I would say that for me python or c/c++ actually sound like the most logical first language depending on the approach. In my school c++ was being taught, but what I dislike about that approach now is that we were essentially taught algorithms. There were no topics on computer architecture or things you would typically need to know to understand how c/c++ work and when they can be used. The only thing relating to that was intro to pointers but no pointer arithmetics, nothing of why they needed etc, array still treated like a magical data structure. Essentially whole programming was very much high level, however the language chosen was low level. I do not like that approach since it is something like learning to write algorithms with extra work that you do not really understand why you are doing in the first place. Thus I would argue that python would be the best language if computer architecture and similar things are not covered or even mentioned. If, however, the focus is to learn low level programming, then c/c++ are excellent of course. The main thing I am trying to say is that I believe you should decide what you want to learn first (writing algorithms, solving issues) or (computer ins and outs, how programming works generally, pointer arithmetics etc) and choose your language accordingly.",he76e0f,t1_he6hjmq,1632558802.0,False
puwwd6,"CS is a broad field for sure. Is there a particular subfield you're interested in? Programming? IT? Security? There's so many great resources out there I would say: start with a small thing you'd like to build or accomplish (ie: make a website, set up a home network, etc.) and start searching as you run into trouble. Have fun learning!",hea41y8,t3_puwwd6,1632612665.0,False
puwwd6,Right now I’m trying to learn the basics then see what’s out there. Thanks for the advice! Are you majoring in CS?,hea4dn6,t1_hea41y8,1632612825.0,True
puwwd6,"I'm a full time software engineer with a few years experience; self taught programming + CS degree. Happy to answer any career questions I can help with!

I haven't looked at it myself, but I have heard great things about MIT's open courseware CS stuff. That would probably be a good place to start and you could get ideas for little projects from there. 

I also think Python programming language is great (I use it for work and a lot of personal projects). It's a powerful language, but also easy to use. Following some tutorials would be another good way to get your feet wet.",hea6doq,t1_hea4dn6,1632613799.0,False
puwwd6,That’s pretty cool. Engineering and CS. Was the CS degree worth it?,heaasp6,t1_hea6doq,1632615949.0,True
puwwd6,"Definitely! Self teaching matters a lot IMO on staying fresh and learning how to learn/search for info. But the CS degree gives a lot of theoretical underpinnings like data structures/algorithms, discrete math, etc. that are very helpful for understanding CS on a deeper level.",heamhym,t1_heaasp6,1632621585.0,False
puwwd6,"You could read ""Computer Science Distilled"" if you want a high level, pop-sci view.",heahvjm,t3_puwwd6,1632619344.0,False
puwwd6,https://github.com/ossu/computer-science,he7lwhd,t3_puwwd6,1632570667.0,False
puwwd6,This: https://github.com/ossu/computer-science,he92hb0,t3_puwwd6,1632595431.0,False
puwwd6,Start by applying to a university,hecb0p2,t3_puwwd6,1632661252.0,False
puwwd6,I’m in community right now. I’m doing uni after,heioxrt,t1_hecb0p2,1632778305.0,True
puwwd6,That's a good start!,heitnke,t1_heioxrt,1632780599.0,False
puwwd6,Thanks! Hby? Are you in college as well?,heiv7rp,t1_heitnke,1632781340.0,True
puwwd6,"Yeah, I'm in my third year.",hekffuu,t1_heiv7rp,1632811152.0,False
puwwd6,same questions keep getting asked again and again and again!,he777r7,t3_puwwd6,1632559485.0,False
puwwd6,damn that’s tough,he7ht15,t1_he777r7,1632567959.0,True
pup5rl,"Sorry to break it to you, but this is like a whole 2 courses on a undergraduate level to get the gist down. Just go by what r/wsppan said, the reccomendations are awesome.

Edit: it's closer to a whole curriculum in a computer science bachelors worth its salt (that means it actually cares about hardware and makes it a big thing parallel to maths/CS).",he4gpqz,t3_pup5rl,1632509200.0,False
pup5rl,"This ^ 

Theory and Compilers is a master degree on its own.",he4rjgm,t1_he4gpqz,1632513680.0,False
pup5rl,"I had to write an interpreter in undergrad. Literally had to build everything from string tokenizing, grammar interpretation,  variable management... it was a lot of work but quite fun. The leap from interpreter to compiler is not that big.",he6apds,t1_he4rjgm,1632538417.0,False
pup5rl,"> Theory and Compilers is a master degree on its own.

Huh?  At what school?  I did both (and Theory was in spades) as an undergraduate.",he56356,t1_he4rjgm,1632519804.0,False
pup5rl,"The undergraduate courses are like an introduction to the topic. It is much larger than what can be covered in the handful of courses offered in a bachelor's.

One of my co-workers did his masters thesis specifically on memory management strategies in VM based languages, and is now off doing a PhD on something to do with compilers.

That branch of computer is incredibly deep, varied and interesting and still has lots of room to grow.",he59lgw,t1_he56356,1632521329.0,False
pup5rl,"Layer of abstraction after Layer of abstraction, I swear to properly learn computers you need to start at the beginning of time haha! But all the material has really been helping, its a lot of information but its necessary. This is one of the things my mind has been struggling to come to terms with.",he76to8,t1_he4gpqz,1632559161.0,True
pup5rl,"I suggest just reading about it on the side while you get your education on CS. It's not that maths is harder, it's just that the introductory books on the topics you want to learn are more like read and understand, whereas maths/CS is really hands on and head cracking.",he780a4,t1_he76to8,1632560125.0,False
pup5rl,computer engineering is seperate from CS. I have a Degree in CE and it focuses on hardware. cs tends to focus on software.,he5dqvm,t1_he4gpqz,1632523157.0,False
pup5rl,"It follows the [instruction Set Architecture](https://en.m.wikipedia.org/wiki/Instruction_set_architecture)  that the particular hardware uses that tells you exactly how to encode the bits for the hardware to do what you want it to by allowing electricity to flow via opening and closing gates inside its integrated circuits. Check out these resources: 

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)",he4dvr3,t3_pup5rl,1632507993.0,False
pup5rl,"Thank you so much for this! I actually started reading Code (resource number 1) a week ago and its really simplified a lot of concepts. I will definitely take a look at the other resources.

Ben Eater is a beast.",he6zkhn,t1_he4dvr3,1632553547.0,True
pup5rl,"[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )


[What I Have Learned (a master list of what to do and what not to do - contributions welcome)](https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share )

Follow this advice for Ben Eater""s breadboard kits. No idea how his manages to work with out resistors on his LEDs among other things.",he7glq3,t1_he6zkhn,1632567081.0,False
pup5rl,"In a sense, it doesn't convert anything to binary. Computers only know binary representations, nothing else. A binary number is just that: a number, and we have chosen certain numbers to be associated with different letters of our alphabet. So when you see certain letters on the screen, they exactly correspond to certain binary values already stored in the computer's memory.

Some old graphics chips have hardwired logic for drawing certain glyphs on the screen when they see the number associated with that glyph (letter) in the display memory.",he4dm78,t3_pup5rl,1632507879.0,False
pup5rl,"Understood. On the note of binary representations, I suppose that one set of binary might mean one thing to one part of the computer whereas the same set of binary might mean something else in another part (e.g. an ASCII code vs fetch instruction). So whatever instruction we give there would need to be a distinction as to where exactly it needs to go huh?",he705w6,t1_he4dm78,1632553986.0,True
pup5rl,"When you go to assemble your assembly code, you have to define what architecture you’re building the code for, and that architecture distinction tells it how to make the final 1s and 0s. If you want to read about it, I found [this page](http://www.c-jump.com/CIS77/CPU/x86/lecture.html) that looks pretty good. I’m trying to find the pdf of my course notes for my Assembly course in college, since we had a table that literally showed all of the encodings for x86 ASM with examples of generating the bits by hand, and converting the bits back into instructions, but I think it’s on a dead hard drive :/

As a made up example, let’s use the `INC` instruction. You could have 

    INC
    INC register
    INC [memory]

Each of those `INC`s encodes with a different opcode, usually sequential, like 0xC3, 0xC4, 0xC5.

The CPU know that 0xC3 is a single byte instruction, since it corresponds to incrementing the accumulator.

0xC4 takes more space to tell the CPU what’s going on, since we need to use a byte to tell it which of the registers we’re incrementing, so

    INC rcx

may encode to `C4 02` to show it was register C (rax=00, rbx=01, rcx=02, ...).

For 0xC5, you need to store a **ton** more info, so often that would be 2-3 bytes to denote what pointers and offsets are used.

If I can find that old set of notes, I’ll link it here. Hope the quick example helped shed some light on this for you!

Quick edit: [Here’s a useful online tool](https://defuse.ca/online-x86-assembler.htm#disassembly) for seeing exactly how code assembles/disassembles. It shows the instructions directly next to their assembled bits, so you can see directly how things change when you give the instructions different parameters.",he4flzm,t3_pup5rl,1632508729.0,False
pup5rl,"Alright I see, architecture definitely plays a big part. I understood what you were explaining with the example, its helped to shed some light. I've just checked out some of the links you sent, I'm definitely going to set aside time to read and fiddle around with it especially the code assembler/disassembler to get a better feel of it. I was playing a bit with 68K but no harm in experimenting with the x86.

If you do manage to find those notes of yours I would greatly appreciate having a look at them. Thank you very much for the assistance.",he734w1,t1_he4flzm,1632556213.0,True
pup5rl,"Whatever every one else has said. Also, look up parsing in regards to compiling. Parsing is a large part of the early steps of the computer trying to ""understand"" what we're typing in a language(say C or C++).",he55d3l,t3_pup5rl,1632519492.0,False
pup5rl,Let me do some research on that. Thanks!,he73ew9,t1_he55d3l,1632556423.0,True
pup5rl,"Lol, comp sci major here, swinging by looking for a gist on 2-3 undergrad courses.",he67m1q,t3_pup5rl,1632536939.0,False
pup5rl,The ultimate crash course haha!,he73nyf,t1_he67m1q,1632556619.0,True
pup5rl,"I think your difficulty is in the understanding of the conversion from the letters you type in your code (high level language) to eventually machine code (binary) or bytecode (which are also binary but can only be understood by a virtual machine running on your actual computer - like the JVM). I had the same problem in the beginning. Try to read up on the LC3 architecture and you will start to appreciate what goes on behind the scenes of converting human readable code to machine/byte code). Check it out here

https://justinmeiners.github.io/lc3-vm/",he6lnks,t3_pup5rl,1632544202.0,False
pup5rl,Yes. This is part of the difficulty I have been facing. Just took a brief look at the link and I'm definitely going to set aside time for this (got a lot of reading to do this weekend 😅). Thank you for sharing.,he7497o,t1_he6lnks,1632557077.0,True
pup5rl,"An assembler is the simpler of the two in that it is mostly just a translation layer that converts human readable assembly into the binary that the CPU uses.

Example: NOP -> 0x00

A compiler has a more difficult job to do as it takes intent and attempts to carry it out in the assembly available to it. You can think of each assembly instructions as a Lego brick, and your program as an idea like ""castle with a gate"" and the compiler as a master builder that makes it happen. 

Essentially what it boils down to is that people have figured out how to the higher level language ideas like for loops in assembly and built this knowledge into the compiler. The really cool magic happens with all the optimizations that compilers can do for us.

Edit: typo",he6nwrb,t3_pup5rl,1632545577.0,False
pup5rl,"Understood. I'm guessing the compiler has also got its own layers of receiving, breaking down and interpreting the intent that we have into a workable, machine understandable set of instructions.",he75cnx,t1_he6nwrb,1632557959.0,True
pup5rl,"http://www.craftinginterpreters.com/contents.html
That site has a neat breakdown of the various bits of an interpreter/compiler. They don't end up at assembly, but at byte code for a VM that you write. Going all the way to assembly would be more, similar work.",he872fc,t1_he75cnx,1632581446.0,False
pup5rl,A compiler converts a higher level language into assembly language. An assembly language is specifically based on the instruction set architecture of a particular hardware. An assembler converts the mnemonics of the assembly language into machine code (bits). Assembly and machine code have a 1:1 correspondence with assembly just using mnemonics to make it easier to read and write.,he6za0t,t3_pup5rl,1632553337.0,False
pup5rl,"Compiler converts source code (C) into assembly code through things like constraint solvers and type systems, and register allocation trackers. Assembler takes the assembly code and turns it into machine code, pretty much 1 to 1. The machine code is a sequence of well-defined instructions which can be read and executed by the CPU.",he8nzmg,t3_pup5rl,1632589087.0,False
pup5rl,"tldr: the name (string of characters) of the command is checked against all executables in the PATH system variable, the arguments are passed to the corresponding executable and it is assumed the executable knows how to handle them.",he8pkty,t3_pup5rl,1632589773.0,False
pup5rl,"try doing it by hand just for funsies 

https://inst.eecs.berkeley.edu/\~cs61c/resources/MIPS\_Green\_Sheet.pdf",hef9lc7,t3_pup5rl,1632707937.0,False
pup5rl,"I'm not really sure what specific part you're worried about. Computers are usually best understood by seperating them into layers of abstraction. You have C code, which can become assembly, which can become machine code etc. Once you know what these abstractions are you are still going to spend a lot of time learning how to best convert between them (and when that is even possible).  


In your case, I assume you have some assembly commands, and you want to know the binary representation. That particular case is easy since both are designed to basically be easily convertible into each other. In essence, you just get a huge table which tells you what binary command each specific assembly command corresponds to, along with some general schema for how you convert the argument registers. This is what defines an instruction set architecture. These documents can be easily approachable, for example for RISC-V it's just this PDF ([https://github.com/riscv/riscv-isa-manual/releases/download/Ratified-IMAFDQC/riscv-spec-20191213.pdf](https://github.com/riscv/riscv-isa-manual/releases/download/Ratified-IMAFDQC/riscv-spec-20191213.pdf)), chapter 24.",he4t1mr,t3_pup5rl,1632514305.0,False
pup5rl,"So the instruction set is making sense for the most part. The layer you spoke of ""You have C code, which can become assembly, which can become machine code.."", that level of abstraction is where the problems start to rise for me. So far with material that's been shared (yours included) it's helped put a lot into perspective and understanding for me. I just need to go through everything.

Thank you for your explanation.",he76mfu,t1_he4t1mr,1632558990.0,True
pup5rl,which layer specifically? I mentioned at least 4 in my answer :),he7gtp4,t1_he76mfu,1632567248.0,False
pu4kv8,That was far less of an interesting argument than I expected. Pretty much amounted to O(n) but poor query optimization means O() is a lie,he0lva0,t3_pu4kv8,1632434135.0,False
pu4kv8,i think the title is a somewhat satirical and attention-grabbing title. He acknowledges in the article that the title is a joke. I think it's a pretty good article outlining how the constant factor can have actual real-world effects.,he26tpw,t3_pu4kv8,1632463516.0,False
pu0sj1,designing data intensive applications is good book for database dev and implementation,he49k77,t3_pu0sj1,1632506175.0,False
pu0sj1,"https://teachyourselfcs.com/
has some good recommendations.",he4j81y,t3_pu0sj1,1632510243.0,False
ptptpz,"LUTs are a classic trade off between time and memory. It isn't really considered a data structure in and of itself because it's really just an array which is its own structure.

But you can only use them to a certain degree without exhausting your memory, and if you have an algorithm that requires precise values whose calculations take in real time parameters, you probably won't be able to fit your LUT in memory. Buuuut many algorithms use LUTs practically, especially ones that have very few return values.

Mu-law Companding of digital voice is one classic example ""μ-law algorithm - Wikipedia"" https://en.m.wikipedia.org/wiki/%CE%9C-law_algorithm
Where most algorithms just map all the discrete values. 

I believe many calculators use LUTs for trig functions as well.",hdxwem1,t3_ptptpz,1632385089.0,False
ptptpz,"Desktop version of /u/codeIsGood's link: <https://en.wikipedia.org/wiki/Μ-law_algorithm>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hdxwfr8,t1_hdxwem1,1632385119.0,False
ptptpz,Good bot,hdxwhew,t1_hdxwfr8,1632385160.0,False
ptptpz,"Keep in mind that modern processors are so much faster than memory, that it's rarely worth it to fetch a look up table instead of just using the CPU to compute three value.",hdygcxs,t3_ptptpz,1632400511.0,False
ptptpz,LUTs are still faster as long as the LUT fits in cache. moving a value from cache to register takes 3 to 15 cycles of delay while actually doing the math would take much longer,he02efi,t1_hdygcxs,1632425498.0,True
ptptpz,"Really depends on the math, modern processors can do a lot of math in 15 clock cycles, especially with SIMD units.",hf4ivq3,t1_he02efi,1633198489.0,False
ptptpz,luts cant really do simd and thats also not the point. an lut is good for doing a lot of stuff to a single piece of data which is sort of the opposite of simd.,hf4x77c,t1_hf4ivq3,1633204726.0,True
ptptpz,"Right, but sometimes you can recompute e.g. a local polynomial approximation using SIMD units faster than looking up the result. Even though it's computing a result for a single piece of data, you can get parallelism from things like the polynomial coefficients. My only point is that the memory/compute trade-off is sometimes less clear-cut than you'd expect because modern processors can do so much compute per memory read even from caches. Really depends on a ton of specific details though including how willing you are to optimize what parts of your program.",hf6rxg0,t1_hf4x77c,1633236008.0,False
ptptpz,"Lookup tables are used all the time, it's just that they're typically called Hash Tables. In Python, they're called Dictionaries. But DatAlg courses typically start by introducing the most simple data structures, such as linked lists, and work their way up.",hecbrp4,t3_ptptpz,1632661652.0,False
ptptpz,"i think youre confused what i mean by look up table. a look up table is a small array that can fit in the cache of a computer and does a very specific and difficult to compute task. For instance, trigonometry functions are often computed with a look up table that stores the value and derivative value of many points on the trig graph. then the function becomes something like f(x)= lut\[(x\*8192)\] + lut\[((x\*8192)%1)\*8192 + 32768\] which is a lot faster than computing the actual trig function every time. you can, in theory, use a lut in python by using a dictionary but its not going to be fast",hf4w90z,t1_hecbrp4,1633204314.0,True
ptptpz,"What you're describing is an implementation of a lookup table at the hardware level. Data structures are theoretical models that go beyond implementation-specific domains. Even a phonebook can be seen as a lookup table. 

In theory, there are a bunch of tricks you could use to save processing speed in the CPU. The reason you don't see them too often is that the Instruction Set Architecture becomes harder to pipeline when it needs to support more machine instructions. Nowadays, High Performance Computing is all about Reduced Instruction Set Computers, which allow the processor to execute machine instructions much easier.",hf51qcn,t1_hf4w90z,1633206675.0,False
ptptpz,"the point of a look up table is that you emulate hardware in software. an lut works on any hardware architecture that has memory or cache so its not more domain specific than any other data structure. an lut can be described in one language and implemented in basically any other language (with varying results), but fundamentally the big-o of that lut does not change from implementation to implementation. 

also why are you talking about risc vs cisc? every architecture has some sort of memory or cache.",hf56v21,t1_hf51qcn,1633208892.0,True
ptptpz,"You're gonna have to be more specific when you say lookup table because they exist at literally every design level. Are you referring to using lookup tables in language interpreters, hardware lookup tables, or storing objects data in a lookup table on software level?",hf5l597,t1_hf56v21,1633215191.0,False
ptptpz,youre clearly confused by the idea of computing something with a look up table. heres a resource on bitbashing that might help. theres plenty on look up tables in it. https://graphics.stanford.edu/\~seander/bithacks.html,hf6cp5f,t1_hf5l597,1633227917.0,True
ptptpz,"Your link doesn't work, but what I'm trying to say is there are many things that can be called lookup tables. Your example of using them in low-level programming to optimize calculations is just one of many. The fact that there are so many different use cases for Lookup tables shows that they are in fact not underrated, as they're used pretty much whenever applicable.",hf6dpw4,t1_hf6cp5f,1633228430.0,False
ptc09c,"C is high-level. Always has been, always will be. When people mistakenly call C a low-level language, they are usually talking about memory management, high performance, and C's domain of kernels, drivers, and other operating system stuff. Those things do not make a language low-level by definition. C is architecture independent and abstracts from the hardware in many many ways, end of story.  Also, that article was rubbish.",hdx9pv5,t3_ptc09c,1632368450.0,False
ptc09c,"They are not right.

C is a high level language, but spectre and meltdown have absolutely nothing to do with C.

C is converted during compilation to a lower level language: it's a high level language, full stop.",hdwhhgi,t3_ptc09c,1632354513.0,False
ptc09c,"The claim in the title of the article is correct. The body isn't really about this topic though.

C is high-level language simply by definition:

  * [low-level language](https://en.wikipedia.org/wiki/Low-level_programming_language) - a programming language that provides little or no abstraction from a computer's instruction set architecture or functions in the language map that are structurally similar to processor's instructions. Generally, this refers to either machine code or assembly language.
  * [high-level language](https://en.wikipedia.org/wiki/High-level_programming_language) - a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even *hide* entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language. The amount of abstraction provided defines how ""high-level"" a programming language is.

C falls under definition of high-level language, because there is whole lot of things you don't really need to care about when writing a C program.

Although notice the last sentence in definition of high-level: ""_The amount of abstraction provided defines how ""high-level"" a programming language is_""  
In this case, the C programming language definitely fall on the **lower** end, because it provides just enough abstraction. I call such languages **middle-level**.

I know some people argue that the terms are relative and ""_Pyhton so high, that C much low - otherwise division meaningless_"", but it's incorrect. For one, Python is more similar to C than C is to Assembly. For seconds, there are relative word which can be used: ""lower"" and ""higher"". And also terms like **middle-level** or [**VHLL**](https://en.wikipedia.org/wiki/Very_high-level_programming_language) exist.

But you also need to differentiate between low-level **language** and low-level **programming**. C is high-level language capable (or even suited) to low-level programming. Even C++ (despite having even higher level of abstraction) is capable of low-level programming. Heck, you could even try to do it in Python.  
Basically, when determining the level of language, we look how high we can reasonably go with abstractions. When level of programming how reasonably low we can go.

In conclusion, when talking about the C's level of abstraction the correct terms are: **high-level**, **middle-level**, **lower-level**",hdwibud,t3_ptc09c,1632354915.0,False
ptc09c,"**[Low-level programming language](https://en.wikipedia.org/wiki/Low-level_programming_language)** 
 
 >A low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture—commands or functions in the language map that are structurally similar to processor's instructions. Generally, this refers to either machine code or assembly language. Because of the low (hence the word) abstraction between the language and machine language, low-level languages are sometimes described as being ""close to the hardware"". Programs written in low-level languages tend to be relatively non-portable, due to being optimized for a certain type of system architecture.
 
**[High-level programming language](https://en.wikipedia.org/wiki/High-level_programming_language)** 
 
 >In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e. g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language.
 
**[Very high-level programming language](https://en.wikipedia.org/wiki/Very_high-level_programming_language)** 
 
 >A very high-level programming language (VHLL) is a programming language with a very high level of abstraction, used primarily as a professional programmer productivity tool. VHLLs are usually domain-specific languages, limited to a very specific application, purpose, or type of task, and they are often scripting languages (especially extension languages), controlling a specific environment. For this reason, very high-level programming languages are often referred to as goal-oriented programming languages.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hdwidnl,t1_hdwibud,1632354939.0,False
ptc09c,"[High Level Programming Languages ](https://en.m.wikipedia.org/wiki/High-level_programming_language)

[Low Level Programming Languages ](https://en.m.wikipedia.org/wiki/Low-level_programming_language)

In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. 

A low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture—commands or functions in the language map that are structurally similar to processor's instructions. Generally, this refers to either machine code or assembly language. 

The terms high-level and low-level are inherently relative. Some decades ago, the C language, and similar languages, were most often considered ""high-level"", as it supported concepts such as expression evaluation, parameterised recursive functions, and data types and structures, while assembly language was considered ""low-level"".

>The author of this article claims that not only is C not really a low level language because it's not really very close to the hardware, but that people trying to force it to be one is the reason for the spectre and meltdown security vulnerabilities from a few years ago.

Level is relative. See above quotes from Wikipedia. The claim of spectre and meltdown due to people forcing C to be low level is utter rubbish.",hdvh2qn,t3_ptc09c,1632334703.0,False
ptc09c,"In the first half of the comment you provide definitions which state that low-level means basically machine code and Assembly - ergo the terms are not relative.  
And yet in the second half you claim that the terms are relative...",hdwfete,t1_hdvh2qn,1632353517.0,False
ptc09c,That is from the Wikipedia articles. Most believe today that C is low level as compared to the high level of abstraction most modern languages provide but the answer has to be relative since C has been around for a long time and back in the day it was considered high level as it was considered portable and the abstraction was above any particular hardware platform. So you will get different answers from different people relative to their perspective and history with programming languages.,hdwk5an,t1_hdwfete,1632355780.0,False
ptc09c,"Who is this ""most""? People who barely touched C and never wrote a line of Assembly in their life? Why the answer **has to** be relative?

Do you know how many people believe in popular myths despite them being far away from truth?",hdwnks0,t1_hdwk5an,1632357420.0,False
ptc09c,"Agree, most people would say that C is low level, but most people in Florida voted for Trump, doesn't make it right...",hdwoktw,t1_hdwnks0,1632357894.0,False
ptc09c,"C is a high level language, it was specifically invented to abstract from the underlying hardware, which is the definition of 'low level', to abstract from hardware.

For example, x86 assembly is low level, because it's specifically targeting x86 hardware.

C is high level because your C code will run on x86, ARM, PPC or whatever without changes.

For some trivia, there are some processors that can run java bytecode, which means you could argue that Java is a low-level language... [https://en.wikipedia.org/wiki/Java\_processor](https://en.wikipedia.org/wiki/Java_processor)

Some would argue that C allows direct memory access, and therefore is low level, but you could make the same argument for Swift, and nobody is saying Swift is low level.

Agree with u/dontyougetsoupedyet, C is high level, no question.",hdwoanz,t3_ptc09c,1632357758.0,False
ptc09c,"'High level' in a programming language context just means 'architecture independent code', and in that respect, C qualifies.",hdv8h5t,t3_ptc09c,1632331233.0,False
ptc09c,name checks out,hdw8w8p,t1_hdv8h5t,1632350510.0,False
ptc09c,"I think it is a low level language, after all if it's a high level language then the term becomes meaningless since it's now presumably in the same category as JavaScript or Python, etc.

I only briefly skimmed the article, but I'm not entirely sure how Spectre and Meltdown are a C-problem specifically. Surely since it's a CPU-level issue, it's a potential vulnerability in anything that compiles to that architecture..?

BTW, there are [some](https://www.reddit.com/r/programming/comments/8g8ix6/c_is_not_a_lowlevel_language_acm_queue/) [pretty](https://www.reddit.com/r/C_Programming/comments/kiwfqp/c_is_not_a_lowlevel_language/) [unfavourable](https://www.reddit.com/r/programming/comments/96yz21/c_is_not_a_lowlevel_language/) responses to this paper previously on Reddit if you're interested.",hdv78p2,t3_ptc09c,1632330728.0,False
ptc09c,">if it's a high level language then the term becomes meaningless since it's now presumably in the same category as JavaScript or Python, etc.

Fibonacci  in Python, C and Assembly. Is C really so different from Python? I've taken code from [here](https://github.com/luc99a/Assembly-Codes/blob/master/fibonacci.asm) (except C, which I **copy-pasted** from Python and just fixed syntax...)

## Python

    #!/usr/bin/env python
    # -*- coding: UTF-8 -*-

    high = 1
    low  = 1

    for i in range(0, 20):
        temp = low
        low  = high
        high = temp

        print(str(low))
        high = high + low

## C

    #include <stdio.h>

    int main()
    {
        int high = 1;
        int low  = 1;

        for (int i = 0; i < 20; ++i) {
            int temp = low;
            low      = high;
            high     = temp;

            printf(""%d\n"", low);
            high = high + low;
        }
        return 0;
    }

## Assembly

    section .bss
    toprint:	resb	1

    section .data
    lf:	db 0Ah

    section .text
    global _start
    _start:
    	mov eax, 1
    	mov ebx, 1
    	mov ecx, 20
    	loop:
    	add ebx, eax
    	push eax
    	push ebx
    	push ecx
    	call printnum
    	pop ecx
    	pop eax
    	pop ebx
    	dec ecx
    	test ecx, ecx
    	jnz loop
    	mov eax, 1
    	mov ebx, 0
    	int 80h


    printnum:
    	xor edi, edi
    	divide:
    	xor edx, edx
    	mov ecx, 10
    	div ecx
    	push edx
    	inc edi
    	test eax, eax
    	jnz divide
    	print:
    	pop eax
    	add eax, '0'
    	mov [toprint], eax
    	mov ebx, 1
    	mov ecx, toprint
    	mov edx, 1
    	mov eax, 4
    	int 80h
    	dec edi
    	test edi, edi
    	jnz print
    	mov eax, 4
    	mov ebx, 1
    	mov ecx, lf
    	mov edx, 1
    	int 80h
    	ret",hdwlsbc,t1_hdv78p2,1632356561.0,False
ptc09c,"I can see from the comments in this thread that I'm perhaps in the minority in classifying C as a low-level language, but here's a few comments on the examples you gave above.

I'm not sure it's a fair comparison between C and Python since the program is so simple that it doesn't need to make use of the aspects of C that some of us would consider 'lower-level'. There's no mallocing and freeing of memory (and the dangers of heap corruptions and memory leaks that possibly follow if done incorrectly, unlike the garbage collected Python), no pointers or function pointers, no bounds checks when accessing arrays lest our program march silently into undefined behaviour, no checking of potential integer under or overflow on maths operations since we're using finite ints rather than Python's arbitrarily large big-nums, and so on. 

My feeling has always been that if we decide to put C at the high-level end of the scale, then the vast majority of programming is simply 'clumped together' at one end. The scale becomes Assembly at the low-level, then a vast gap of nothingness, then C as a high-level language, and then what? Python and other garbage-collected languages are high*er-*level languages? Haskell and Prolog are high*er-er-*level languages, and perhaps those awful block-based drag-and-drop languages are high*er-er-est* languages?",hdy0ir4,t1_hdwlsbc,1632388991.0,False
ptc09c,"Half of the things you listed as examples of ""low-level"" are true for many if not most programming languages. And the other half is already an abstraction from underlying CPU instructions.

But I do agree there may be a need to differentiate between abstractions of C and Python, but lumping C with Assembly makes even less sense that with Python. Like **middle-level** of which adoption I advocate for.

Because you proposition is clumping together more and more languages of varying abstraction each time something even higher level is created.  
If we assume machine code to have level 0 of abstraction, Assembly lvl. 1, C level 3 and Python being of level 5, you suggest to start saying that C is now of level 1 when something of level 10 comes up. It's like calling a billionaire a millionaire because trillionaire showed up.",he0066q,t1_hdy0ir4,1632424549.0,False
ptc09c,"To be fair, it's not quite fair to use a verbose assembly language to compare with. In Mips, there would be a comparable number of lines to the C example:

    main:	.text
        	li 	$t0, 1
        	li	$t1, 1
	        li	$a0, 1
	        jal	print
	
    loop:	jal	print
        	move	$t0, $t1
        	move 	$t1, $a0
	        add 	$a0, $t1, $t0
	        blt	$a0, 20, loop	
	
    exit:	li 	$v0, 10
        	syscall
	
    print:	sw	$a0, 0($sp)
        	li 	$v0, 1
	        syscall
	        li 	$a0, 10
	        li	$v0, 11
	        syscall
	        lw	$a0, 0($sp)
	        jr	$ra

Half the code is the print function, which one could import, just like in C.",heu7k12,t1_hdwlsbc,1633002028.0,False
ptc09c,I think my point still stands with both those Assembly examples.,heuczvb,t1_heu7k12,1633005299.0,False
ptc09c,"Sure, but that's because your Python code is mimicking the C syntax.
If you wrote it using List comprehensions, it would look to C what C looks to Mips. 
   
    fib = [1, 1] ; [fib := fib + [sum(fib[-2:])] for _ in range(18)]
    print('\n'.join(map(str, fib)))",heuuzqf,t1_heuczvb,1633013690.0,False
ptc09c,"    int fib[20] = { 1, 1 }; for (int i = 2; i < 20; ++i) { fib[i] = fib[i-1] + fib[i-2]; }
    for (int i = 0; i < 20; ++i) { printf(""%d\n"", fib[i]); }

Beside inverted `for` syntax the only major difference is in lack of concatenation",heuy4cp,t1_heuuzqf,1633014993.0,False
ptc09c,"Cheeky, I like it. But let's be real, there's no way you're ever gonna see C code like that, while the Python example is fairly common in modern Python. 

But ultimately, it's just looping problem. I think the real differences between C and higher level languages start to show up when polymorphism and memory management come in to play. Most programmers don't even know what a pointer is because it's abstracted away nowadays.",hev0jqs,t1_heuy4cp,1633016006.0,False
ptc09c,"I do agree we are discussing beside the point. So allow me to reaffirm what my initial point was to begin with:

> C programmer will have much easier time understanding Python code than Assembly.  
> Python programmer will understand C basically the moment they understand what a pointer is.  
> Both of them need to learn Assembly from the beginning if they want to understand it.
>
> Thus the conclusion: C is in fact closer to Python than to Assembly",hev41lq,t1_hev0jqs,1633017458.0,False
ptc09c,That's fair! My original point was that your Assembler example made it look more complicated than it is,heviwns,t1_hev41lq,1633023568.0,False
ptc09c,"The thing is, a lot of beginners \*want\* to believe it's a low level language simply because it makes them feel good, which is why any correction to their point of view is met with such disdain.

Indeed Spectre or Meltdown is nothing to with C.

C is high level, it's totally abstracted from hardware, it was specifically invented for that purpose.

If it's direct memory access, then Swift also has that, but I don't think anyone would argue Swift is low level!",hdwp8xv,t1_hdv78p2,1632358210.0,False
ptc09c,It's not just beginners calling it low level.,hdx1x3l,t1_hdwp8xv,1632364352.0,True
ptc09c,"Agree, but that's mostly what I'm seeing.",hdx2f0o,t1_hdx1x3l,1632364599.0,False
ptc09c,"""This is essential because it allows C programmers to continue in the belief that their language is close to the underlying hardware.""

""The proposed fixes for Spectre and Meltdown impose significant performance penalties, largely offsetting the advances in microarchitecture in the past decade. Perhaps it is time to stop trying to make C code fast and instead think about what programming models would look like on a processor designed to be fast.""

The article argues both that C is disjointed from the underlying hardware, and then later argues that the hardware is optimized for C code. This seems like a pretty braindead and low effort article. 

&#x200B;

The article asserts that spectre is only a problem because compilers optimize for ILP and that GPU parallelism is better. Then it later goes through how a C compiler optimizes for SIMD operations and why thats bad. Another room temp iq contradiction. 

&#x200B;

C is in fact, by the articles criteria, a low-level language (when it comes to certain architectures). C syntax is based on the idea that code blocks should exist and that jumping instructions are used to navigate into the code blocks that should be executed. With the exception of CMOV and some AVX512 instructions, every instruction that is predicated is a branching instruction. C syntax makes very little sense for instruction sets for GPUs, old ARM architecture, Itanium and some others. The closest thing we have to syntax to represent the underlying architecture for predication-based languages is the ternary operator that exists in many languages. 

&#x200B;

What makes a higher level language is that abstractions don't represent actual correspond to machine code. So for a python loop there is no identifiable branching instruction in any code anywhere that corresponds directly to a python if statement. There's probably like 5 that are used to decide what code is interpreted next. In C if you write an if else (that's not going to get turned into cmov) then you can probably go find the exact branch instructions that correspond to it.",hdxlxve,t3_ptc09c,1632376298.0,False
ptc09c,[deleted],hdvpf03,t3_ptc09c,1632338069.0,False
ptc09c,? Doesn't sound even a little bit right...,hdwonfz,t1_hdvpf03,1632357929.0,False
pt657b,[deleted],hdu5aa1,t3_pt657b,1632314219.0,False
pt657b,Touche,hdu5g63,t1_hdu5aa1,1632314308.0,True
pt657b,"what do you mean? if i can make a dynamic program in the language, it should count.",hdx0fz7,t1_hdu5aa1,1632363608.0,False
pt657b,"That is an extremely low bar for any language, and completely meaningless for some... writing a DP in C++ does _not_ mean you know C++.",hdydg5u,t1_hdx0fz7,1632398817.0,False
pt657b,r/whoosh,hdyp51i,t1_hdydg5u,1632404872.0,False
pt657b,Doh! I've spent too much time around people who actually believe this...,hdypa33,t1_hdyp51i,1632404936.0,False
pt657b,A decent chunk of astronomy's existence is due to telescopes and vice versa.,hdu9d64,t3_pt657b,1632316347.0,False
pt657b,"Yeah but the telescopes are basically about engineering. Trying to see farther better. That is promoted by astronomy, but the science of astronomy itself is about examining what you can see with the telescope.

Similarly, I don't build computers. It's in my interest to help advance the engineering of computers, but my focus is not to do that myself (unless I so choose). Computer science is about using the computer to build better automation software more efficiently. Or, depending on the subfield such as A.I. building a smarter program.",hdv96yl,t1_hdu9d64,1632331524.0,False
pt657b,"Weirdly computer science it's probably one of the most dogfooding-like fields

As you build the computer you use the computer to build a better computer (which part of it really depends as you said)",hdvkztn,t1_hdv96yl,1632336287.0,False
pt657b,[deleted],hduwuti,t1_hdu9d64,1632326514.0,False
pt657b,"That might be true for a lot of theoretical fields. However, software engineering, human computer interaction and many other disciplines would not exist without actual computers.",hdv0b6d,t1_hduwuti,1632327919.0,False
pt657b,"With all due respect to Dijkstra (and there's bucketloads of it), this is an ill-fitting analogy that is starting to age. His statement made sense for his time. He wanted people to know that there's fundamental science and math to computer science and that he wasn't just a technician hammering away at a system until it starts working but someone who deals in theories, hypotheses and proofs. 

A lot of early computer science was stuff that was very independent of computers. You can run Dijkstra's algorithm on a mechanical computer made of sluice gates and water wheels, etc. if you're motivated enough to make the contraption. Similar to how you can study astronomy without telescopes if you have super sharp eyesight. 

But today things are too intertwined between computer science and engineering for Dijkstra's analogy to fully hold. It still holds for some topics such as the theory of computation but that's the extent of it. Take cybersecurity, for example. It doesn't exist without actual digital computers and not just any digital computers but ones running modern operating systems and modern network stacks. A lot of the field has turned into the ""science of computer engineering"" as opposed to being strictly in the computer science or computer engineering realm. Take this [paper](https://www.researchgate.net/publication/354473627_Design_of_a_security_and_trust_framework_for_5G_multi-domain_scenarios) for example: ""Design of a security and trust framework for 5G multi-domain scenarios"". It's neither purely astronomy nor purely telescope engineering. 

It's a bit of a double standard for people to say ""stop making car analogies to operating systems"" but still keep worshipping this old quote from Dijkstra. All this quote has become is a badge of elitism for people with CS degrees to use in looking down upon PHP/python programmers who make CRUD applications for a living and are often making better money due to market forces than those who know more about theoretical CS.",hdw3wa7,t3_pt657b,1632348199.0,False
pt657b,I agree,hdx3wnk,t1_hdw3wa7,1632365365.0,False
pt657b,"I agree with everything, but I think changing ""science of computer engineering"" to ""science of software engineering"" would fit better as, at least in my experience, it is more about applying the concepts to developing software to solve some problem, and less about the hardware. Hardware still plays a roll, but seems to be less about designing the hardware for the software, but instead the other way around. It is just symantics, and my limited view though. I'm sure it can change depending on the role you typically fill.",hdx94xu,t1_hdw3wa7,1632368130.0,False
pt657b,"> I agree with everything, but I think changing ""science of computer engineering"" to ""science of software engineering"" would fit better as, at least in my experience, it is more about applying the concepts to developing software to solve some problem, and less about the hardware.

That's fair. Things are also a lot more software oriented now than before. Von Neuman introduced programmable computers and we're undergoing almost a second Von Neumannification with things like Cloud computing, Software Defined Networking, FPGA programming and even Arduino style programmable hardware.

The hardware guys aren't sitting silent and they are working on stuff such as shrinking the die size of chips, inventing sensors, accelerating matrix operations, and working around energy consumption and dissipation limits, but their focus is on increased performance for the most part, and not actual use cases for the technology.",hdxaka1,t1_hdx94xu,1632368924.0,False
pt657b,"well, I studied technical computer science, which was much more hardware focused.",hdy5c9f,t1_hdx94xu,1632393201.0,False
pt657b,"Genuinely curious, how is that different from Computer Engineering? I've always heard Computer Science as being software focused and Computer Engineering as being hardware focused. Granted, where I graduated from there was a lot of overlap. Comp Sci & Eng were both considered engineering degrees with Comp Sci essentially being software engineering, and Comp Eng being hardware engineering. We all also took all the same Freshman and Sophomore level classes, and we only diverged for the Junior and Senior levels. Even then though, we could take each other's classes as electives.",hdyl0ty,t1_hdy5c9f,1632402942.0,False
pt657b,"The difference boiled down to the focus after the 3rd semester. While the applied CS people went more towards Software-Dev stuff, my peers and I went more towards the hardware-software interface, with a bit more hardware. Think drivers, operating systems, controllers, and headless CPUs or microcontrollers. Also, a bit on the CPU from the logic gate perspective.",hdytapx,t1_hdyl0ty,1632406708.0,False
pt657b,"Ah ok, that is very in between Comp Sci and Comp Eng. I (Comp Sci) did have to do classes on operating systems, and what not, but no microcontrollers or CPUs. I did take a computer architecture class (basically high level CPU stuff) though as an elective and was the only Comp Sci person there.",hdyzpo1,t1_hdytapx,1632409443.0,False
pt657b,"Hence why I think the name ""Informatics"", which some degree programs call themselves, is a more apt name than ""computer science"".  We don't call ""astronomy"" ""telescope science"".",hdu8tb0,t3_pt657b,1632316067.0,False
pt657b,"Other alternatives: Computing Science, Computation Science.   
Edit: Or name it after ""Data"" like we do in Sweden and Denmark (Datalogi/Datavetenskap)",hduqk94,t1_hdu8tb0,1632323955.0,False
pt657b,"yes, I prefer these too - the difference is subtle*, but important.

*perhaps too subtle for most",hdv4vrm,t1_hduqk94,1632329768.0,False
pt657b,Computational Sciences?,hdv8kco,t1_hduqk94,1632331267.0,False
pt657b,"""Computational"" already refers to the use of computers in other fields, notably Computational Science, so that would be very confusing.",hdwbx77,t1_hdv8kco,1632351876.0,False
pt657b,Computational Science already refers to a specific discipline. It's basically about simulating processes in sciences and all of the challenges that arise doing that.,hdvg1le,t1_hdv8kco,1632334283.0,False
pt657b,"In Brazil it's called ""Ciência da Computação"" which translates to ""Computation Science"", but ofc to avoid confusion if someone translates this it would be localized as ""Computer Science""",hdvov0c,t1_hduqk94,1632337842.0,False
pt657b,"In Swedish it's called Datalogi (equivalent to ""Datalogy"") or Datavetenskap (Data Science)",hdwbowm,t1_hdvov0c,1632351772.0,False
pt657b,Datalogy. I like that.,hdxiksp,t1_hdwbowm,1632373899.0,False
pt657b,"As a Pole, I wholeheartedly agree  
^(although for most people here ""informatyk"" still means ""guy repairing computers""...)",hdufu1h,t1_hdu8tb0,1632319399.0,False
pt657b,"In Danish, CS is actually called *datalogi*. Similar.",hdv5bb4,t1_hdu8tb0,1632329942.0,False
pt657b,"Informatics is more algorithm related, like USACO. Most programmers I see here usually deal with business logic.",hdub7qy,t1_hdu8tb0,1632317252.0,False
pt657b,"Well yeah, but are those “most programmers” actually practising computer science (or whatever we choose to call it)?",hduxrmx,t1_hdub7qy,1632326884.0,False
pt657b,"it's a semantics game, I guess the question is if would you call what those programmers do ""computer science"", or would it be more ""software engineering"", or ""programming""?

Not a value judgement, all of these are valuable roles in society, with overlapping skills, but different focuses.

To me, ""computer science"" == ""informatics""",hducc8t,t1_hdub7qy,1632317789.0,False
pt657b,Your right - It really is a semantics game after all. Personally I just call it computer science.,hdudcbv,t1_hducc8t,1632318262.0,False
pt657b,"I try to not use the word ""programmer"" for me it's like calling a Carpenter a Sawer. The actual activity a widely named ""programmer"" does, is software engineering. So they are software engineers. Software engineering and computer science are different fields. But they are just categories. You are not able to separate both. It's like trying to create or build something without knowledge and understanding. Engineering is the process of creation while science is understandment resulting in knowledge.",hdvj0rw,t1_hducc8t,1632335487.0,False
pt657b,What are the morphisms in the category of computer scientists?,hdwo4hd,t1_hdvj0rw,1632357676.0,False
pt657b,"I had a professor that always said Computer science was the study of problem solving, and math was the study of truth.",hdxjxdy,t3_pt657b,1632374839.0,False
pt657b,I learnt about telescopes in my astronomy course,hdya9nk,t3_pt657b,1632396804.0,False
pt657b,"Astronomy -> Telescope Science

Biology -> Microscope Science

Chemistry -> Beaker Science


OK, it's fixed. /s",hdyg7mr,t3_pt657b,1632400429.0,False
pt657b,[deleted],hdu9tqa,t3_pt657b,1632316576.0,False
pt657b,https://en.m.wikipedia.org/wiki/Formal_science,hdue72l,t1_hdu9tqa,1632318657.0,False
pt657b,"> Science is about about The Scientific Method...

I'd argue that only applies to the natural and social sciences. Computer science  is a formal science, like mathematics.",hdue8y2,t1_hdu9tqa,1632318681.0,False
pt657b,[deleted],hdufgsq,t1_hdue8y2,1632319236.0,False
pt657b,So you are saying social sciences aren’t actual sciences? What is your definition of a science?,hduz9au,t1_hdufgsq,1632327495.0,False
pt657b,[deleted],hdvqaei,t1_hduz9au,1632338421.0,False
pt657b,"Uhhhh, things like psychology and sociology definitely use the scientific method.",hdvs0pj,t1_hdvqaei,1632339116.0,False
pt657b,[deleted],hdw4p8t,t1_hdvs0pj,1632348640.0,False
pt657b,"Formal sciences are still the same type of science as natural sciences; a science doesn't necessarily mean it has to have the scientific method to it, that's just one definition of science.

When we say \[topic\] science, we don't necessarily mean ""\[topic\] with the scientific method"" - that's just false. Here's one definition of science:

""A systematically organized body of knowledge on a particular subject"".

There is no mention of the scientific method. Computer science is essentially a systematically organized body of knowledge regarding computation. This includes math, or social sciences, or criminology, etc.

Natural sciences just naturally (lol) use the scientific method due to the nature (lol) of how they work. Whereas formal sciences like math don't necessarily need the scientific method but rather proofs instead.

Edit: words.",hdwi4m4,t1_hdw4p8t,1632354818.0,False
pt657b,"Why do people argue so much about the meaning of words? Words are just conventions made by men, and many times these conventions aren't consistent or differ among groups of people.

Should I care if I call such or such a field ""science"" or something else? Ultimately, it's just a label.",hdxy7ax,t1_hdwi4m4,1632386784.0,False
pt657b,Ok Wittgenstein.,hdzf2de,t1_hdxy7ax,1632415845.0,False
pt657b,"I thought you were calling me names but apparently Wittgenstein is a philosopher.

Which means that you are quite literally calling me names.",hdzwxqx,t1_hdzf2de,1632423186.0,False
pt657b,"A philosopher who famously said that there were no true philosophical problems, that they are all simply problems of language. 

You might like his concept of a [language game](https://en.m.wikipedia.org/wiki/Language_game_(philosophy)).",he021ad,t1_hdzwxqx,1632425342.0,False
pt657b,that's super cool,he2v30z,t1_he021ad,1632483340.0,False
pt657b,"The existence of a replication science extends to far more fields than just to psychology and sociology. Equally affected is e.g. medicine. But even physics is. 

This has nothing to do with a lack of scientific method in either field - but rather with a refinement of it over the decades. Human biases and limits of knowing are the determining factors here - not some supposed lack of scientific thinking in psychology or other fields.",hdxj9ww,t1_hdw4p8t,1632374381.0,False
pt657b,"I mean, I definitely say that social science is rarely scientific. Most of the field is summed up in analysis of data from online polls given to sets of students. I'd laugh, but it's far from amusing. Science needs more than statistics, especially stats almost no one attempts to corroborate. Inb4 we all pretend social science fields aren't in crisis over bad research.",hdvavi6,t1_hduz9au,1632332201.0,False
pt657b,I can see where he comes as he came up with the foundational network routing Algorithms,hduhsec,t3_pt657b,1632320271.0,False
pt657b,"More than that - he was the guy who fought to get Comouter Science recognised as a new academic discipline worthy of being studied at universities, with its own standards and methods. Before that, any work with computers was done in Electrical Engineering.",hdv8wxn,t1_hduhsec,1632331410.0,False
pt657b,"Did not know that, now I do thanks",hdv9v8i,t1_hdv8wxn,1632331795.0,False
pt657b,"Same statement in so many sub-reddits. Either I am in too many similar subreddits or someone is farming karma.

Kindly remove the post.",hdxqwft,t3_pt657b,1632380182.0,False
pt657b,"It honestly should be called computer engineering, in fact, that's what it's called in my country. There's no ""computer science"" here. And engineering is about technology and problem-solving, so it perfectly fits.",hdugkug,t3_pt657b,1632319731.0,False
pt657b,"I suppose it depends on the location and the curriculum, but in general, the two fields are not the same. While codependent, Computer Engineering and Computer Science generally have differing focal points in the academic, industrial, and institutional realm.",hdw36zy,t1_hdugkug,1632346716.0,False
pt657b,Just fix my printer bro,hdw2zbk,t3_pt657b,1632346536.0,False
pt657b,"Well, take a look at sensitive dependence on initial conditions, aka chaos.  Basically would not have been discovered without computers, although the general notion was known to mathematicians of the late 19th century like Poincare, they just didn't have the tools to investigate it.  So is that pure mathematics, or is that computer science?

I understand why academic institutions created 'computer science' and 'computer engineering' departments, but they could easily be grouped under applied mathematics and applied physics instead.  A lot of that is just about how to distribute resources and jobs in the best manner - the universe cares not at all about the political and financial (and social) dividing lines within academic institutions, but professors looking for resources care a lot about them.",hduogdy,t3_pt657b,1632323076.0,False
pt657b,"Astronomy is about the stars. Optics is about telescopes. You might very reasonably call it telescope-science. This isn't a real statement, and it is not deep. Its a boring, tedious fuzzing of what words \*actually\* mean.

Sick algorithm though.",hdwbrpl,t3_pt657b,1632351808.0,False
pt657b,"If you need to put ""science"" in the name, you're admitting it's not really a science. Same as putting ""democratic"" in the name of a country.",hdwtzij,t3_pt657b,1632360476.0,False
pt657b,How bout computation science?,hdusppa,t3_pt657b,1632324836.0,False
pt657b,Very true. I don’t care what tool a mechanic uses to fix my car. I just want it to work.,hdv7mq9,t3_pt657b,1632330887.0,False
pt657b,"Why do some people need to separate ""engineering"" (ie the practical aspects) from ""science"" (ie the theory)?

Using the same analogies, astronomy would barely exist without telescopes, we would not know much of biology without microscopes.   
Without these tools, we would know little more than the Mayas about astronomy, and even them were using (crude) tools.

Scientists need tools to practice science, to experiment, to measure, to test hypothesis.   
Both are intertwined and complimentary, one is not better or purer than the other.

Our natural senses are too limited and subjective to be used as reliable measure systems, which is why we need tools.

Many scientists ultimately design and engineer their own tools to push their research further, using both theoretical and practical knowledge.",hdxp0vz,t3_pt657b,1632378666.0,False
pt657b,"Tbh there should only be computer science and engineering schools. You can't abstract computer science away from its comouter roots, and when you do it a terrible thing called python emerges.",hdui44y,t3_pt657b,1632320414.0,False
pt657b,">You can't abstract computer science away from its computer roots   
  
No, but you can abstract it away from physical computer machines.",hdur0k4,t1_hdui44y,1632324137.0,False
pt657b,Yes but I have to doubt weather that is beneficial or not,hdurmcp,t1_hdur0k4,1632324385.0,False
psxdq3,"Heck yeah!  We had OOT (Object Oriented Turing) in my high school and it provided excellent foundations for both procedural and object oriented programming.  I found it to be a super accessible language with simple syntax that could yield exciting results for a new coder - like easily getting user input, drawing graphics/animations to the screen, etc.  I think these days something like [processing.org](https://processing.org) could provide a similar experience, but has the advantage of many powerful libraries available.",hdt25oy,t3_psxdq3,1632283956.0,False
psxdq3,"To this day Turing remains my only real experience with graphics. Although that’s liable to change given my status as a Grade 12 taking high school CS who doesn’t even have an actual teacher for the class yet.  University soon. Graphics in other languages seems to be much more complex though. Now it’s all about Java 

One day I want to learn something in the C family. Maybe C#. The two seem oddly similar",hdt2g43,t1_hdt25oy,1632284135.0,True
psxdq3,"For what it's worth, C# is closer to Java than C.",hdtxwa2,t1_hdt2g43,1632309717.0,False
psxdq3,"Yep, Turing was how I got started with programming back in grade 11. Now I am half way through a masters in CS :)",hdt6hg0,t3_psxdq3,1632286717.0,False
psxdq3,"Where you are is where I hope to be some day. Hopefully when I get there I still remember my roots. 

Step 1: Get accepted to university first. Surprisingly difficult task at least I think it’ll be. I have good marks but are they good enough? 77% in math and a suggested average of 75% to really be considered. I might get slaughtered by the genius crowd who casually talk about how they “only” have 90s

But it’ll happen eventually and then I can embark on my CS adventure",hdt6raw,t1_hdt6hg0,1632286902.0,True
psxdq3,"A CS degree is hard work, but if you are dedicated you can and will do it!   
  
Are you looking at Canadian universities? I can maybe give some insight for ontario.",hdt7znf,t1_hdt6raw,1632287748.0,False
psxdq3,"Yeah Canadian universities in Ontario. I’m thinking York because it’s decent and it isn’t a 2 hour commute. UFT is cool but I’m not good enough to go there. Ryerson I can get into probably but it’s far as hell. I’ve also seen colleges like Sheriton advertise CS programs. What’s the difference between college and university in this case? Is university what you really need to get hired? Also one of the requirements for York is no math grade below a 65% but my grade nine math grade is 63%. Really hope they ignore that one 

In my case I’d be gunning for a Bsc degree",hdt88l8,t1_hdt7znf,1632287923.0,True
psxdq3,"I don't think grade 9 math will be an issue at all for you, if your grade 11/12 averages are fine. The difference between college and university is that college is often shorter and less involved frankly. You'll learn to do specific things like web development, and its not a terrible way to go. That being said if you *can* go the BSc in CS route, it is the better thing to do. You are simply more hireable.
  
Generally speaking unless you plan to do grad school, you should just pick the university you like best. Personally, I didn't like the UofT campuses and the culture. Just to make my bias known, I did my undergrad at YorkU. But I think regardless of what you choose, you will get a just fine education. (You're gonna hear people bitch about things at every uni lol, and every uni thinks their problems are unique to them).   
  
I reccomend visiting the campuses for open house if thats still a think considering covid, and see if talking to faculty and taking in the school environment helps you choose. I remember I was gung-hoe about going to UofT, got accepted for the Co-op program I wanted, but I really didnt like the downtown ""campus"" or the scarborough campus. Considering the commute/expense of residence, the not so-great interactions I had with faculty, it ended up being my least favourable option. So go to open houses and see what speaks to you!  
  
Of course... Then again that depends if you get into the universities you apply for! York is great in that it gives people opportunities to succeed even if they didnt do the best in highschool. Some people hold that as a negative against the school but I never thought so.",hdtbmq1,t1_hdt88l8,1632290383.0,False
psxdq3,"Ah great. Maybe I do have a shot. It’ll be tough considering all the people with ridiculously high grades that are applying as well but it’ll work out somewhere *eventually*. I’ll look into that while open house thing.

And thanks for clearing up the college/university difference. Good to know in case my only 77-80% math grades aren’t good enough for university expectations when they have the high-90s club available",hdtbyzu,t1_hdtbmq1,1632290641.0,True
psxdq3,"Ya, I think you have a pretty decent shot at getting into York frankly (especially since admissions have dropped due to covid). If you don't get into any universities its not like going the college route is bad, in fact you'll probably still due well for yourself, especially in IT if you are interested in that sorta thing. But its just a different route in life.",hdtc62z,t1_hdtbyzu,1632290788.0,False
psxdq3,"Cool, great. Thanks! It’s interesting to see a fellow Canadian here. We’re not exactly rate but it’s interesting especially considering you’re from roughly the same area as me and considering we’re both CS types you were able to help answer some of my big questions as I move forward.

Thanks very much. Have a good day and I wish you the best in life",hdtdoav,t1_hdtc62z,1632291972.0,True
psxdq3,"Yes, actually! I suppose it is the fate of most unsupported languages to fall away but at least it left its mark.",hdt3s2b,t3_psxdq3,1632284958.0,False
psxdq3,">Usually at the grade 10 level before real languages are introduced.

This was my very first introduction to programming. Exactly as you described: grade 10 at a Canadian high school.

Edited to add: I enjoyed using Turing more than Java, which immediately turned me off programming when I ran into it at grade 11. It wasn't until I finished university and had little career prospects in what I studied that I decided to turn back to programming.",hdt68km,t3_psxdq3,1632286553.0,False
psxdq3,And it was mine as well. Looks like the curriculum hasn’t changed all that much in that regard. To be fair it’s not a bad intro at all. Simple syntax yet you can do a lot with it for what it is,hdt6cw7,t1_hdt68km,1632286631.0,True
psxdq3,"At some more opportune moment (i.e. when I'm not feeling lazy), I'd definitely like to revisit it, just to get a feel of what can really be done in it, now that my understanding of programming in general is way more advanced compared to my old teenager self.",hdt6r80,t1_hdt6cw7,1632286900.0,False
psxdq3,"You probably already know this but you can use processes to make two things happen at the same time. They’re like functions but have that unique trait. Really saved my life during the final project where I couldn’t figure out how to make it work without having two things happening at once or a mindboggling complex nested loop.

Also you can make custom text with custom fonts and sizes but it’s complex(at least for a grade 10 student)",hdt75gf,t1_hdt6r80,1632287167.0,True
psxdq3,Yeah I programmed in turing at my CS classes in highschool. One of my CS teacher's professor developed turing!,hdu2b5r,t3_psxdq3,1632312526.0,False
psxdq3,Turing was taught as my high school's programming class (this was in 2009-2013),hdu3zut,t3_psxdq3,1632313505.0,False
psxdq3,Still used today afaik,hdu422v,t1_hdu3zut,1632313540.0,True
psxdq3,I didn't take that class but I had friends who did and I got the sense that it made graphics really easy. Probably why it's so popular as a teaching language.  Like turtle graphics libraries.,hduy8ke,t1_hdu422v,1632327080.0,False
psxdq3,The integrated graphics were indeed a very useful educational tool. They’re quite limited but if you use them right you can pull a lot off. I managed to make a simple game where you drive a “car” down a road and avoid obstacles. The road isn’t animated because that was too difficult to do with the languages limitations but… whatever I’m still proud of it for what it is and when I did it,hdvex23,t1_hduy8ke,1632333826.0,True
psxdq3,"Yeah, I learned some Turing in high school for controlling breadboard circuits",hdubjjn,t3_psxdq3,1632317410.0,False
pst2jb,ask him why latex is so slow. its been like 200 years since they made it and yet it has not gotten any faster (in fact its slower).,he043pq,t3_pst2jb,1632426205.0,False
psqdx8,"I took those classes in the philosophy department at my college. Start off by Googling for some entry PDF course material.

The book we had for Critical Thinking was called ""Critical Thinking for College Students"". Can't remember the Logic book.",hdslzj9,t3_psqdx8,1632275417.0,False
psqdx8,"No, it's impossible. All those people just pretend they know anything about it",hdrew8h,t3_psqdx8,1632255662.0,False
psqdx8,"You are a realist,But we shouldn't just sit. we have to keep trying",hdrjcl0,t1_hdrew8h,1632257525.0,True
psqdx8,Idk why you say that. But I have personally felt I have become far better than when I started with programming just by practicing.,hdtmshe,t1_hdrew8h,1632300139.0,False
psqdx8,And I thought the sarcasm was obvious...,hdu0acx,t1_hdtmshe,1632311293.0,False
psqdx8,Look at gamedev videos on YouTube. Sebastian league is top tier,hds8edp,t3_psqdx8,1632268921.0,False
psqdx8,I used Udemy for math and discrete mathematics,hdrivk6,t3_psqdx8,1632257331.0,False
psqdx8,Can you reccomend a course. Thank you,hdrj0ry,t1_hdrivk6,1632257391.0,True
psqdx8,"For discrete math

https://www.udemy.com/share/1022nG3@ZKx7STDF7zwCYB7y7Dmdaoe4OAPEARuYt8x4qDljdOCf55tSeQio5cO_knVNzfM=/

For other mathematics check Krista King courses",hdrrr8w,t1_hdrj0ry,1632261169.0,False
psqdx8,You could use Kenneth H Rosen if you prefer textbooks,hdu4w3q,t1_hdrj0ry,1632314006.0,False
psqdx8,Just math,hdrtwxt,t3_psqdx8,1632262143.0,False
psqdx8,Discrete maths,hdt34ph,t3_psqdx8,1632284554.0,False
psqdx8,Logic and critical thinking can absolutely be taught - in fact it’s been actively taught for hundreds of years. Here’s a great foundational book on logic http://web.mit.edu/gleitz/www/Introduction%20to%20Logic%20-%20P.%20Suppes%20(1957)%20WW.pdf,hdt9mpm,t3_psqdx8,1632288910.0,False
psqdx8,"Not everyone will agree to this, but logic and critical thinking are not skills that can be taught, per se. You can take up courses but they only give you techniques to solve a certain problem which doesn't do anything to improve your skills. That said, these are skills that can be developed using some habits and techniques and a lot of practice. The more you think critically, the better you become at it, you start to see more scenarios right away. I can recommend a book that helped me a lot to understand this concept. [https://www.google.co.in/books/edition/A\_Mind\_For\_Numbers/Sb1iAgAAQBAJ?hl=en&gbpv=0](https://www.google.co.in/books/edition/A_Mind_For_Numbers/Sb1iAgAAQBAJ?hl=en&gbpv=0)

Let me know if you try this and proved to be helpful.",hdsn4vi,t3_psqdx8,1632275967.0,False
psqdx8,https://www.logicmatters.net/tyl/,hdt1oyj,t3_psqdx8,1632283676.0,False
psqdx8,no you are born either as a phd level logician or as a mere mortal. your fate is already decided,hdt8nf4,t3_psqdx8,1632288211.0,False
psqdx8,"my kids have been learning comp sci basics [from this youtube channel, its pretty fun](https://www.youtube.com/channel/UCvIzIe8bIMkAkblXz4aQ-Pw)",hdrs01l,t3_psqdx8,1632261278.0,False
psqdx8,"It's mostly just how your brain is wired, I think. Everyone's got their own little algorithms going on inside their heads, some more efficient than others and they're all hardcoded.",hdsonw4,t3_psqdx8,1632276709.0,False
pspap8,"Big O notation isn't about actually calculating exact run times based on *specific* inputs. It's about the *worst* case scenario given some really big random N value (best case is Big Ω, and average case is Big Θ).

In your first block of code, your *worst* case is 10,000 \* N, where N is really big, or O(N), because the first 10,000 is hardcoded, and is independent of input.

In your second block of code, your *worst* case is N \* N, where N is really big, or O(N^(2)). If N happens to be 10,000, the run time happens to match the first block of code. But then, that's not really a *worst* case scenario, considering N can potentially go to infinity, right?

The point isn't to calculate the exact amount of 'work' that needs to get done. I remember doing some exercises in data structures and algorithms that had us calculate that, but ultimately, it's a fruitless endeavor because the computer is always running more than 1 thing at once, so run times tend to be an inconsistent metric of how optimized a program is. We instead use Big O to classify algorithms according to how the running time or space requirements of an algorithm grow as its input size grows, as it better captures the *general* idea of how any given algorithm will function.",hdrhk5b,t3_pspap8,1632256777.0,False
pspap8,"I understand, thank you.",hdri08i,t1_hdrhk5b,1632256964.0,True
pspap8,"Big O notation is not about worst case asymptotic growth. Big O is used to denote an upper bound on asymptotic growth. You could be talking about an upper bound for the worst case, best case, average case, and so on.

Big Theta is used to denote a tight bound and big Omega for a lower bound. These are orthogonal to best, worst, average case.",hdt554p,t1_hdrhk5b,1632285834.0,False
pspap8,"I mean, technically yeah, but the difference doesn't really matter in this context. The comment above was right to say that the upper bound of asymptotic growth for a given algorithm is its worst case.",hdt91ls,t1_hdt554p,1632288489.0,False
pspap8,"No, that's not correct. For example O(n!) also correctly describes both algorithms best, worst, and average cases, since big O is an upper bound and n! is an upper bound for all of those (it's just not a tight bound for any of them).",hdtaj5y,t1_hdt91ls,1632289571.0,False
pspap8,"Ok, it seems like I might have a misconception about how it works then. Can you explain in a bit more detail?",hdtbc8e,t1_hdtaj5y,1632290165.0,False
pspap8,"Sure, it's just in the mathematical definitions.

Suppose that we have two functions f and g. When we say that f is (in) O(g(n)), we are saying that there is a constant c and a natural number n\_0, such that for all n >= n\_0, f(n) <= cg(n), i.e. an upper bound.

When we say that f is (in) Omega(g(n), we are saying that there is a constant c and a natural number n\_0, such that for all n>= n\_0, f(n) >= cg(n), i.e. a lower bound.

When we say that f is (in) Theta(g(n), we're saying f(n) = cg(n) for all n >= n\_0, i.e. a tight bound or upper and lower bound are equal.

What's orthogonal to these complexity classes is the fact that some algorithms  behave differently when the input is different and some don't. For example, the best algorithms that search for the max element of an n-length list must make n-1 comparisons no matter whether the max element is in the first position, somewhere in the middle, or the last. (Of course, there are worse algorithms that solve this problem, but don't confuse that for best versus worst case for this particular algorithm.) In the best case, it makes n-1 comparisons. In the worst case it makes n-1 comparisons. We say that this algorithms is Theta(n). It's also O(n) and Omega(n). However, we can also say that it's in Omega(1), since they don't do better than constant time, i.e. constant time is a lower bound or for some n >= n\_0 f(n) >= c(1) .

In other words, let n\_0 = 1 and c = 0, then f(n) = n-1, the number of comparisons to search for the max element of an n-length list, c(1) = 0(1) = 0.  And we have, for all n >= 1, n-1 >= 0. That proves Omega(1) is a lower bound for our algorithm. We can also say that this algorithm is in O(n!), since it doesn't do worse than factorial time, i.e. factorial time is an upper bound or f(n) <= c(n!) for n >= n\_0.

Theta(n) is a tight bound, and it's generally what we're looking for.

In the other case where the input makes a difference, such as linear search for a specific element in an n-length list, the best algorithms will find the element in the first position after 1 comparison and will need n comparisons to discover that the specific element doesn't exist in the list. These are the best and worst case scenarios for the algorithm respectively. In the best case, we can say that the algorithm is (in) O(1), Theta(1), and Omega(1). But we could also say that it's (in) O(n\^2), i.e. f(n) <= cn\^2 for n >= n\_0. Feel free to do the math.

In most cases, it's pretty clear what to say about the time or space complexity of an algorithm without digging into the math. We often care about the tight bound for the worst case. But the definitions are unambiguous mathematical definitions.",hdtepre,t1_hdtbc8e,1632292815.0,False
pspap8,"Well, yeah, but now you’re just being nit-picky. 

Could we say something is O(n^n) and technically be correct since it’s an upper bound and is contained within that? Sure, of course we can, but if you wrote that as an answer on a test, you’d get points off bc it’s just a guess versus doing the math to calculate the lowest possible upper bound. That’s typically what the worst case is.",hdvvvuw,t1_hdt554p,1632340842.0,False
pspap8,"You're right that we typically are looking for lowest upper bound (i.e tight bound) but that's not the same thing as worst case scenario for an algorithm, and that wasn't my point. The worst case and best case might have the same lowest upper bound asymptotic growth or they might have different lowest upper bounds.

In other words big Oh does not refer to the worst case scenario. You can talk about the big Oh complexity of an algorithm's worst case scenario and you can also talk about the big Oh complexity of an algorithm's best case (or average case or amortized average or so on). These do not necessarily have the same big Oh complexity.",hdvy245,t1_hdvvvuw,1632342404.0,False
pspap8,"If n is upper-bounded by a constant (in this case 10^5), then the running time is upper-bounded by a constant as well - i.e. the worst-case running time of both will be O(1).

So yes, as they stand, they do have different asymptotic running times in n - but, as n = O(1), the running time still comes be O(1). :)",hdr8hzq,t3_pspap8,1632253032.0,False
pspap8,"Okay, thank you.",hdr8zjx,t1_hdr8hzq,1632253235.0,True
pspap8,"No prob 😁 Should you have more questions or if something is unclear, feel free to ask!",hdr9vn2,t1_hdr8zjx,1632253603.0,False
pspap8,"As someone who is subscribed to all the running subreddits, this got me temporarily excited that there was some cool crossover between running and CS.",hds372l,t3_pspap8,1632266466.0,False
pspap8,"my kids have been learning [comp sci basics from this channel,](https://www.youtube.com/channel/UCvIzIe8bIMkAkblXz4aQ-Pw) its pretty fun",hdrrutw,t3_pspap8,1632261213.0,False
pspap8,"If it's more than a marathon, we say Big-Oh. Otherwise, we just say Oh.",hdrhpc9,t3_pspap8,1632256838.0,False
pspap8,Okay,hdri255,t1_hdrhpc9,1632256987.0,True
pspap8,"Big O doesn't concern itself with constants, that's why some algorithms that are classified as O(log n) can run slower than O(n) algorithms when there's enough constants.",hdsamk8,t3_pspap8,1632269998.0,False
pspap8,ok so this is definitely your homework.,hdt8s1g,t3_pspap8,1632288299.0,False
pspap8,also you are correct at 10\^5 they are equal if n is smaller than 10\^5 it will go faster. if n is bigger than it will go slower. big o is for seeing what happens n goes to infinity. in this case you can sort of reason out when each will be faster but in other cases there are hidden constants that make it pointless to calculate.,hdt95yx,t1_hdt8s1g,1632288577.0,False
pshyvz,"Depending on the implementation either one.

Normally when working with GAs you also add a mutation step. This randomly changes some Genes/Attributes (however you want to name it) therefore you can also find fitting solutions even if the didnt have been in the original set.

If you do not add this step then depending on your data size you may or may not find an optimal solution. But even if you can’t find the theoretical optimal solution, in reality even an close to optimal solution will suffice.

So in the end, it depends.
Hope that explains some things.",hdqgyva,t3_pshyvz,1632241728.0,False
pse2bo,[deleted],hdp4ayk,t3_pse2bo,1632214766.0,False
pse2bo,Thank you for your answer :) that helps.,hdp6k7e,t1_hdp4ayk,1632216936.0,True
psdy59,"In simple terms, the same way the parser works with user-defined function names or variable names.  There will be a set of characters that are allowed in a custom operator, and that gets pulled out via regex just like the rest of the tokenization process.",hdptcxq,t3_psdy59,1632231735.0,False
psdy59,"Yup, it's pretty much the same as a normal function/method call. The dot and parens are just optional (from a parsing perspective, and possibly from a syntax perspective) and the operator has to be a unary method.

Eg.  https://docs.scala-lang.org/tour/operators.html",hdqc0yg,t1_hdptcxq,1632239716.0,False
psdy59,"You can see Swift's grammar here:

https://docs.swift.org/swift-book/ReferenceManual/zzSummaryOfTheGrammar.html

The general idea is the same as how it handles variables names, with the difference of how you set up the first character
operator-head → / | = | - | + | ! | * | % | < | > | & | | | ^ | ~ | ?


Although the grammar here doesn't seem to describe precedence rules, unless I missed it.",hdpnzxv,t3_psdy59,1632229098.0,False
psdy59,"Many mainstream compilers use a hand written recursive descent parser but switch to some version of precedence climbing or Pratt parsing for expressions, because they are exceptionally good at parsing expressions. I wrote an article about how to use these kinds of algorithms to parse languages in which the operators can be (re)defined at run time: [Making a Pratt Parser Generator](https://www.robertjacobson.dev/designing-a-pratt-parser-generator).",hdsuhox,t3_psdy59,1632279660.0,False
psdy59,Can you give an example of what language you are thinking of?,hdp6lwo,t3_psdy59,1632216982.0,False
psdy59,"I remember reading an article where the author defined weird operators in Swift. Here's the link, but it's alright not to click. https://abhimuralidharan.medium.com/how-to-create-a-custom-operator-like-operator-in-swift-55953c0c0bf2

These bizzare features really makes me curious lol",hdp968k,t1_hdp6lwo,1632219319.0,True
psdy59,"Hmmm. I don't know Swift *at all*, but the link you send just seems to be using [Operator Overloading](https://en.wikipedia.org/wiki/Operator_overloading), to overload an existing operator (`~>`) to perform a different function, just like you can in lots of other languages like C++.

In C++, the compiler already parses for the left and right-shift operators `<<` and `>>` which it expects to be used to shift the bits in a variable by a certain number of positions (e.g. `printf(""%d"", 5 << 1);` outputs `10` because 5 in binary left-shifted by 1 position is 10). But C++ also allows for those same operators to be overloaded and perform a *different* function, most obviously for outputting to stdout: `cout << ""hello world"";`

The main point here is that the parser already knows that << and >> are operators of some kind, and can therefore parse them appropriately. They aren't really 'new' operators at all.

If there is a language where you can truly declare you own operators (and there may well be), then the kind folks over in r/ProgrammingLanguages or r/Compilers will likely know the black-magic involved in parsing it.",hdp9z6t,t1_hdp968k,1632219997.0,False
psdy59,"Oh but the article does use some new operators, the square root one and the weird circle one. 

Thanks for the subreddit links, I was wondering if there's a sub for languages themselves. Maybe there is a subreddit for everything.",hdpa6lk,t1_hdp9z6t,1632220171.0,True
psdy59,"**[Operator overloading](https://en.wikipedia.org/wiki/Operator_overloading)** 
 
 >In computer programming, operator overloading, sometimes termed operator ad hoc polymorphism, is a specific case of polymorphism, where different operators have different implementations depending on their arguments. Operator overloading is generally defined by a programming language, a programmer, or both.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hdp9zz6,t1_hdp9z6t,1632220016.0,False
ps8py7,"Generally, I like it.

But really, in the industry, it's just a fact of life, I mean if you want to write iOS, Android, Mac, Windows apps, you're going to be using OOP.

If you do web front end, you're using OOP, like it or not, as that's what the DOM is.

Of course, there are areas in the backend, or embedded where OOP isn't used or required.",hdo4ipq,t3_ps8py7,1632190305.0,False
ps8py7,"I guess there's a reason for that, because in the industry codebase can be managed by thousands of people, or even more when people change and codebase stays, and OOP is the most maintainable and easy to learn",hdp6lvo,t1_hdo4ipq,1632216981.0,False
ps8py7,"Taking the [four](https://www.indeed.com/career-advice/career-development/what-is-object-oriented-programming) [pillars](https://en.m.wikipedia.org/wiki/Object-oriented_programming) [of](https://medium.com/nerd-for-tech/the-four-pillars-of-object-oriented-programming-39efe4e87afc) [OOP](https://www.freecodecamp.org/news/four-pillars-of-object-oriented-programming/) :

* _Inheritance_: This is taught to every student as `Dog` inherits from `Animal` and ""this models how the world works"", though in reality this often creates a convoluted mess of hierarchies. Nowadays people are generally saying ""prefer composition over inheritance"", so I guess it's not really a _pillar_ after all? 
* _Polymorphism_: Generally falls into ad hoc polymorphism (function overloading), parametric polymorphism (generics), and subtype polymorphism (inheritance). The first two are common outside of OOP and the last one is, well, related to the first ""pillar"" 
* _Abstraction_: A valuable concept but doesn't require OOP. Layers can be created using a variety of different concepts where lower level layers abstract implementation details away from higher layers.
* _Encapsulation_: This is really the selling point of OOP. Bundling together both data and behavior into a single unit is the main sticking point. When done poorly, it leads to an overcomplicated graph. When done well, it can create a clear system of responsibilities that are well-tested at the boundaries. It's easy to do poorly, which is why there are so many books and conferences about it.",hdobbie,t3_ps8py7,1632193535.0,False
ps8py7,"My take based on the points presented:

Inheritance:

While it works well in idea it does not work well in implementation, we get weird metaphors where something like car inherits engine, when car is not engine it has a  engine. Now I know this can be resolved by clearly defining ""has a"" vs ""is a"" but many times developers conflate these ideas. It also leads to behavioral hiding where one has to walk the tree of inheritance to figure out what is manifesting a behavior.

Polymorphism:

Nothing wrong with it, it is just a take on polymorphic functions, it is not a novel concept to OO. It helps us as humans reason about function naming and signatures.

&#x200B;

Abstraction:

Abstraction is better defined as loose coupling, OO tends to perform pretty poorly at this but so does procedural and functional is only slightly better than this, to truly move away from tight coupling one has to move away from graph based data structures, but this become increasingly difficult to reason about, and necessitates the utilization of languages based on first order logic, which have historically been extremely difficult to built complete systems in. While they tend to excel at black box abstraction they are very difficult to organize sequencing logic in, to comprise comprehensive systems.

&#x200B;

Encapsulation:

The absolute worst idea ever. While it seems easy to reason about packaging data with logic has causes all kinds of one off side effects as the logic specifically depends on the data it travels with. Further it promotes unnecessary mutation of internal state, so that any external reliance of that data cannot guarantee the sequence of events that transpired internally to arrive at the current state of the data. This is really where first order logic shines as with it, you do not tell the system the semantics of the graph nor the events to transform it, rather you specify facts and rules then the system can infer relationships and outcomes.

&#x200B;

The best recommendation I can give, if one really wants to understand the tradeoffs of programming paradigms, is to learn Java, Lisp and ProLog, once you have had exposure to those three you will understand the tradeoffs each group made the achieve the goal they were looking for. They represent classical examples of the three disciplines of graph/geometric, algebraic, and calculus based programming disciplines.

Side note, if you survive ProLog, you get some kind of merit badge. It was the one language that blew my mind, but to this day I cannot imagine the tediousness it would require to build an entire software system in ProLog. That being said, it will fundamentally change the way you view programming. Personally I found it far more of a profound WTF red pill, than Common Lisp or Clojure. To me it is the closest this to the theoretical question, if programming emerged in an alternative reality, what would it look like?",hdot7tk,t1_hdobbie,1632204741.0,False
ps8py7,"I think encapsulation can/could've worked well if it was commonly done at a higher level, to reduce the size of the graph. A `class` is far too small of a unit, creating such a large graph that no one can feasibly reason about. I've seen a Java project with 40,000 classes (actually required tuning the metaspace usage there were so many classes) and it's impossible to know how many of those classes have mutable state, and more importantly it's impossible to reason about how many _instances_ of those classes exist.

OOP encapsulation doesn't say it has to be a `class`, though that's of course the implementation in the ""enterprise"" languages (Java, C#, C++). There's other implementations of encapsulation like Javascript's prototype.

I am curious if there is a language with an implementation of OOP Encapsulation at a **module/package** level, where the module is the ""unit"" that has encapsulated data/behavior and reducing the graph to, I don't know, maybe 10 ""units"" on a medium project and 100 ""units"" on a large project.",hdpb3dw,t1_hdot7tk,1632220915.0,False
ps8py7,">I couldn’t imagine the tedious to build an entire system in ProLog

Checkout [terminusdb. these mad lads did something really interesting in Prolog](terminusdb.com)",hdq5gv0,t1_hdot7tk,1632236999.0,False
ps8py7,"I am familiar with Terminus and agree it is interesting, but data storage and retrieval is a natural fit for first order logic. This is where it really shines, that is why relation theory is based off of it.

I should have been more clear, modeling sequence based business logic is extremely difficult in first order logic. First order logic is good at accurate and predictable results like the following:

facts:

all living things breathe.

Men breath.

John is a man.

&#x200B;

where based on these facts you can say:

isAlive(John) and the response will be true.

&#x200B;

This is very hard to model, say an shopping cart where it is sequenced based:

&#x200B;

places item in cart -> checkout -> collect info -> collect payment -> produce order.

&#x200B;

But what a lot of developers don't realize is first order logic works well between every ->

This is where rules engines and workflow engines work well, as they provide fact based programming in between sequences.

&#x200B;

Let's take produce order as an example because many times it is easier to work backwards with first order logic:

We could say the facts are:

payment must be collected.

Item must be in inventory. (though this should also be performed in presenting the item as well)

Personal info must be collected.

Address must be validated.

&#x200B;

Then we can ask:

canOrder(invoice)

&#x200B;

That is why I personally think the trick is knowing the tradeoffs. Some of the most predictable and stable software in the world is built on languages that are based on first order logic, but man it has been my experience that trying to sequence in them is hell, tedious and nearly impossible. The person that cracks that nut and figures out how to make sequencing intuitive in a first order logic language, will certainly make a mark on computing. As it sits rules engines and workflow engines are really the best option we have at the moment, the issue is that first order logic programming is not well represented in CS curriculum and bootcamp training. So developers are unaware that their is a better way to perform some tasks.",hdqcrp9,t1_hdq5gv0,1632240023.0,False
ps8py7,[removed],hdp746r,t1_hdot7tk,1632217462.0,False
ps8py7,excellent points!,hdpagra,t1_hdp746r,1632220406.0,False
ps8py7,"Programming to interfaces/protocols/contracts is great. Love it. Use it all the time.

However inheritance is mostly pointless.  
I haven't seen a teaching example where either composition or just different data would've been a better fit.   
There are real use cases but they are very niche.

If given a choice id prefer first class mixins/composition rather than inheritance.",hdop2q8,t3_ps8py7,1632201741.0,False
ps8py7,"It's just my preference so I tend to avoid OOP since I see it as more cluttered than other paradigms. That being said, I stick with non-OOP languages such as Haskell, Julia, and Rust.",hdo5qg9,t3_ps8py7,1632190881.0,False
ps8py7,Do you think that it should stay relevant within the industry alongside non-OOP practices or do you think it should go away?,hdo8jag,t1_hdo5qg9,1632192181.0,True
ps8py7,"I don't think OOP should go away, for now. While I think it's cluttered, I find OOP languages easier (having used them in my internship and in school) and more maintainable in the sense that it's more intuitive for most people than FP. 

But as for example, I would like to see Haskell in more critical systems such as NASA Copilot. Also I would love to see Rust for more system software to reduce runtime errors such as null pointer errors.

Though, I like that the general public are being introduced to FP by lambda functions on Java and frameworks such as React. It's a start.",hdo9gsh,t1_hdo8jag,1632192625.0,False
ps8py7,"For me, I like OOP. I like the idea of having classes or objects to organize similar code into to have easy accessibility to it. Though I understand why some people dislike it.

I think this debate has become too focused on proving one side to be better rather than discussing each person’s different preferences. OOP and FP both have pros and cons and whether someone likes one or the other is entirely a preference and I think we should accept that and not attack each other.",hdoaqmj,t1_hdo9gsh,1632193249.0,True
ps8py7,"I avoid it where possible, but there's only so much that can be done to avoid it!",hdo7v5r,t3_ps8py7,1632191867.0,False
ps8py7,Why don’t you like it?,hdo8alb,t1_hdo7v5r,1632192071.0,True
ps8py7,"I think OOP is popular because it's neither particularly good nor particularly bad at anything. 
Data oriented code will pretty much always be faster as it should generally be more memory efficient, but many people find data oriented code more difficult to follow and it can be a bit tricky to do well.
Functional programming is more elegant, and once you're familiar with it, it is in many cases just better than OOP both in legibility and performance. However, many people find it difficult to learn, and without an excellent compiler you're likely to introduce many copy operations and checks that slow down your program if you don't know what you're doing. So while it has a lot of potential, it also features a lot of room to bugger it up.
Then there's aspect orientation and all that, but let's stick to data, functional, object.

OOP is easy to learn, reasonably efficient, and not too inelegant as long as you avoid silliness like factory factories.
Unfortunately, it is likely to introduce a significant amount of boiler plate, a lot of memory waste, many indirections, etc.
There's also something to be said about object oriented ""patterns"" being complicated ways to compensate for object oriented programmers not using or not having access to features like first class functions- a common debate.
It's particularly painful to see these patterns used in a language like C++, but a lot of programmers don't learn the tools they're using.
I think this contributes to OOP's bad reputation.
Since OOP is all many amateur programmers know, signs of excessive object orientation are indicative of lack of knowledge, whereas excessive use of, say, functional techniques is more likely to be considered a lack of wisdom.",hdob4ny,t1_hdo8alb,1632193441.0,False
ps8py7,If you do it while avoiding inheritance it can be bearable.,hdohep1,t3_ps8py7,1632196858.0,False
ps8py7,"""Object-oriented programming is an exceptionally bad idea which could have only originated in California."" Edgser Dijkstra",hdoeia2,t3_ps8py7,1632195227.0,False
ps8py7,"If you're going to be snarky and edgy at least be correct about it? The roots and origins of OOP hail from MIT in the 50's, not from California. 

 https://en.wikipedia.org/wiki/Object-oriented_programming

But sure, keep telling yourself OOP is some misguided product of silicon valley or whatever.",hdoguhv,t1_hdoeia2,1632196536.0,False
ps8py7,"Sorry, I forgot to properly attribute the quote to Edsger Dijkstra. Editing my original comment now.",hdohn9k,t1_hdoguhv,1632196999.0,False
ps8py7,"Ohh, I didn't know that was a quote from Dijkstra. You're correct that it is. My apologies.",hdoiqfa,t1_hdohn9k,1632197637.0,False
ps8py7,best exchange,hdonner,t1_hdoiqfa,1632200753.0,False
ps8py7,"I agree with your statement: sometimes it's really good, sometimes it isn't.

I personally love OOP in game development, cause  a lot of instances of classes will also be an actual object in the game, with possibly different physics, properties etc.

I do prefer non-OOP languages for quick fixes and small problems, cause it's just a smoother process.",hdp2f53,t3_ps8py7,1632212909.0,False
ps8py7,have you ever used ECS for game development? I find it fits the use case much better.,hdpb4jq,t1_hdp2f53,1632220941.0,False
ps8py7,"Nope, never tried it but it does seem handy. The fact that objects are defined by their properties and don't have a type (if I understand that correctly) could work very efficiently.",hdpbrlc,t1_hdpb4jq,1632221444.0,False
ps8py7,"I personally like and use java for backend web dev (hobby right now). I like using OOP to organize each aspect of the website like Users and Posts (if social media) into their own classes/objects. 

However, there are some downsides to OOP that one has to try to minimize, but every practice, including functional has some negatives. Ultimately, I would never use OOP for things like data science, but for other things it works quite well.",hdpczow,t1_hdp2f53,1632222385.0,True
ps8py7,I like it in squeak (smalltalk). Not so much in Java.,hdohqs8,t3_ps8py7,1632197053.0,False
ps8py7,"Everyone I've met who thought it a necessity seemed unaware that there are other ways to organize code. The few times I've been able to pick their brain enough to figure out why they thought so, it's been because they were too stuck in a Java-like mindset, where making objects and classes is the only way to get any sort of late binding, inversion of control, information hiding, subtyping, or even structured data. They can recite the ""encapsulation, polymorphism, inheritance"" mantra, but they don't know what capabilities OO languages have *that non-OO languages lack*.

That's more an opinion about how OOP is taught than about OOP itself. I don't have a particularly strong opinion on that. I'm not sure whether I'd rather need it and not have it or have it and not need it.",hdop8sr,t3_ps8py7,1632201857.0,False
ps8py7,"oo is a great way to make your program slow. that being said its also a great way to organize code especially when working with others. When making a game or a ui thing you should probably just use oo because it makes the code easier to write. When it comes to data science youre just going to write it as procedural (I mean you can use objects but its not in the correct style so it doesn't count) because thats just the nature of the problem. 

Theres a lot of hype around functional languages rn but honestly its more hastle than its worth to write a whole program in that style. Id say you can take the benefits without the hassle by writing parts of the code in that style. It helps out with not having to debug things later so thats nice I guess

every style has its place so just like keep in mind the problem parameters. think of different styles as tools in a toolset rather than a religion.

&#x200B;

additional note: just because a language has features of one paradigm or another doesnt mean all code written in that language is in that style. JS specifically supports a lot of functional programming tools (higher order abstractions) but most things are objects.",hdp1od2,t3_ps8py7,1632212187.0,False
ps8py7,"In theory, OOP is fantastic. In practice, I've never seen anyone get it right. Usually people get hung up on the OO part and lose sight of the overall goals.",hdo1s1i,t3_ps8py7,1632189045.0,False
ps8py7,I love it but I admit to the overhead it brings in compromising efficiency for readability/convenience/DX.,hdothkg,t3_ps8py7,1632204955.0,False
ps8py7,"It's a balance. Think of it this way: you can use loops and recursion interchangeably, but some problems are easier to write recursively and some are easier to write with a loop. It's the same thing here. Some projects are easier to write in an OOP style (games are the first thing I think of) and some aren't. As with everything, it's about choosing the right tool for the job.",hdph23s,t3_ps8py7,1632225122.0,False
ps8py7,Is this a homework question?,hdouked,t3_ps8py7,1632205813.0,False
ps8py7,"I think its a local maxima conceptually speaking, which works enough that most engineering management doesn't mind the pitfalls of its overuse.

In practice I think its overused because many languages, due to a lack of features, force you through class abstractions to achieve certain mechanisms. Then as a result of keyword/concept proximity, it becomes easy for anyone to mistakenly burden a class with unnecessary object orientated properties.

For example, try writing an AST datatype in Java (without spilling it across multiple classes/files). [It ends up looking something like this.](https://github.com/viskell/viskell/tree/master/Code/src/main/java/nl/utwente/viskell/haskell/expr). Now you have 16 files worth of surface area to ensure it does a specific job only.

Another example is ad-hoc polymorphism. In C++ you end up with a lot of class machinery and someone might have the wrong idea to add irrelevant state to the abstract base class that needs managing. Once people latch onto these object orientated implementations its hard to get away from them.             

OOP isn't inherently bad but you have to keep the nuances of the language and the goal of the objects totally in mind. In a team setting this gets hard.

An experiment I suggest is try writing [algebraic datatypes](https://en.wikipedia.org/wiki/Algebraic_data_type) (Product types & Sum types) by hand in C. Additionally try writing a [vtable in C](https://www.state-machine.com/doc/AN_OOP_in_C.pdf). Its laborious but it gets you to really question if these language features surrounding OOP in other languages are really benefiting you.             

These days I only use Haskell and find its community's preference towards category theory/math concepts over object abstractions really refreshing. If I do need to use objects I keep it minimal and as functionally pure as possible.",hdpcmsh,t3_ps8py7,1632222118.0,False
ps8py7,OOP is the harbinger of overengineering.,hdprc70,t3_ps8py7,1632230771.0,False
ps8py7,"It is a pragmatic approach. It has it's cons when it comes to performance, but that's just a cost that you pay for higher level abstractions. We live in a world where computing means more than crunching numbers for some weird Math question, so for all intents and purposes, OOP is usually the way to go",hdq5wq3,t3_ps8py7,1632237186.0,False
ps8py7,"It's fine, it has its place and like anything can be overdone. But it works pretty well in general. For the record, functional programming is also fine. Whatever works for you or, whatever matches the specific problem set.",hdqcr6o,t3_ps8py7,1632240017.0,False
ps8py7,"It has its use cases, but just like TDD, it has some cultist supporters.

One example off the top of my head would be game development, especially if it has a variety of playable characters and enemies.

When it comes to any well-established technology, I'm always neutral, realising that there is no such thing as a one size fits all solution/architecture, pick the right tool for the job and don't get attached to the technology itself.",hdqg5kx,t3_ps8py7,1632241402.0,False
ps8py7,"Encapsulation makes multi-threading difficult, either you add a mutex for every member or you dig in and read every method you use to see what side effects it has (which defeats the point of abstraction).",hdrk70e,t3_ps8py7,1632257879.0,False
ps8py7,Keeps things very organized with both heredity and polymorphism. There is no better way to store/use data imo.,hdrln8f,t3_ps8py7,1632258497.0,False
ps8py7,"my kids have been learning co[mp sci basics from this channel, its pretty fu](https://www.youtube.com/channel/UCvIzIe8bIMkAkblXz4aQ-Pw)n",hdrrwh7,t3_ps8py7,1632261235.0,False
ps8py7,"I don't particularly care for it, even in its purest forms like Smalltalk (Sorry Squeak and Pharo.) It gets more frustrating when ""object-oriented"" languages mix in every other paradigm and programming technique in existence. Its here to stay though, so most of us have to suck it up and use it.

Like someone else mentioned, I am glad to see functional programming getting more popular. Maybe it will replace OOP someday, but I highly doubt that.",hdoi7w4,t3_ps8py7,1632197329.0,False
prunhv,"Linux has the concept of standard input and standard output: stdin stdout. These are byte streams (like a socket, or a file). Every process created by Linux has a stdin and stdout. A process reads from its stdin and writes to its stdout.

What's on the other ends of these streams? It can be anything, depending on how the process is created. But for simplicity let's say both are connected to the process that launched it, which is usually bash, aka your console.

So we call:

    printf(""hello world"")

This is a function in lib c. If you look at the source code you'll see it does the equivalent of :


    write(stdout, text)

Write is a function provided by your Linux kernel, therefore it is a syscall. So we transition to the Kernel. Inside the write function there is code to take each byte of the strong and copy it to the given stream. It does so.

The kernel knows all processes associated with the stream, and it looks for any waiting ones and wakes then up.

So bash wakes up, sees it has data on a stream it used for your processes stdout. So it reads the bytes one by one and then puts them in a text buffer.

It then runs more. code to convert those bytes into visible glyphes on your screen.

It then calls another syscall telling the kernel to update the frame buffer at the relevant position.

This has the overall result of ""hello world"" being visible on your screen 


Note that a GPU isn't necessary here. Most Linux systems will have this done entirely on the CPU. The only thing done on the GPU is copying pixels to and from framebuffers.",hdlspnp,t3_prunhv,1632153982.0,False
prunhv,"I understand the concept more now. But I would like to know more about it from the CPUs perspective. You had me up to the point you said it converts bytes to visible glyphs. Everything until that point made sense from a CPUs perspective since it was just dealing with moving data in and out of a buffer. That can be easily done with assembly using LOAD/STORE instructions and using specific addresses. But what cpu assembly instruction triggers it to go, “hey, i need to take these bytes and convert them to glyphs on screen”? After all, the kernel is just a bunch of instructions in of itself running on the cpu. So someone had to have written really low level cpu instructions to make that part happen.",hdo0azv,t1_hdlspnp,1632188363.0,True
prunhv,"> what cpu assembly instruction triggers it to go, “hey, i need to take these bytes and convert them to glyphs on screen”? 

There isn't a CPU instruction to do this. It's all just software someone has written and a data table (a font map). The software will look the ASCII characters up in the fontmap and then write those pixels out to screen.

This is basically just moving more buffers about :)


Try it yourself. See if you can design a font map, then write some code to lookup an ASCII char in that map and write the data to a bitmap or PNG file. Then write an equivalent of printf that will print your string to this file.",hdo8n4s,t1_hdo0azv,1632192231.0,False
prunhv,"All of this depends on the operating system.

----

At the very *very* bottom of the stack is the hardware: the physical devices that comprise the computer screen, the keyboard and the like. The video card hardware which drives the display on your computer monitor, which is controlled through a device driver that knows what signals to send to that hardware to cause it to do things.

So, in the x86_64 architecture you may have a video card that, in a special start-up text mode, displays text on the display by specialized hardware that converts ASCII characters in a buffer of memory into video signals that draw those letters on a screen.

The driver software then knows how to send the right signals to draw the characters ""Hello, world!"" into the memory buffer, so the hardware will draw those characters on the screen.

(The same idea if the video monitor is in graphics mode, but instead of shoving ASCII in a chunk of memory, instead, certain memory blocks representing the pixels on a screen are set to represent the shape of the letters in ""Hello, world!"")

----

Now this device driver knows how to flip the bits in the hardware, and the operating system knows how to talk to the device driver: how to find the device driver on disk, how to load the device driver (which is its own program) into memory, how to run the programs in the device driver as they need to be called.

----

At a higher level, the operating system then provides API endpoints which eventually talk to this device driver, and programs compiled for that operating system use a standard library (like [glibc](https://www.gnu.org/software/libc/sources.html)) to talk to the operating system API endpoint.

And of course the operating system knows how to load your program into memory and run it.

----

So when you write a program like

    int main()
    {
        printf(""Hello, world!\n"");
        exit(0);
    }

Internally a call is made to `__vfprintf_internal`, which formats the string and writes the results to `stdout`, which is a FILE reference object that points to 'standard out.' Basically that object is a data structure which contains a reference to an operating-system supplied file descriptor which, before your program is started up, is connected the aforementioned device driver by default.

So when printf eventually pushes out the ASCII characters ""Hello, world!\n"" to the standard out stream, it writes those characters as a stream to a file reference, which goes to the device driver, that knows to take that string of ASCII and either put them in the text buffer (if your video card is in text mode), or paint them on the screen.

----

To make this even more complicated, there may be some program underneath that intercepts the standard input/standard output file references and does something else with them.

This is how a terminal program works on MacOS or on Linux; as the program starts up, it knows before starting your program up, to redirect the standard input and output files to itself. (See ['pipe'](https://www.man7.org/linux/man-pages/man2/pipe.2.html) and ['fork'](https://www.man7.org/linux/man-pages/man2/fork.2.html) for details in the POSIX standard.)

And so it receives the output of your program (rather than the underlying operating system) and the terminal program knows to then store the results in an internal array of characters, then redraw its window with the pixels representing ""Hello, world!""

A redrawing process that goes through a different API but eventually hits your video card device driver to flip the bits in memory representing the pixels on the screen that make up the letters ""Hello, world!""

----

As an aside, it's one reason why I wish more programmers would play with embedded hardware devices, like the Arduino. Because a lot of what's going on is made a lot more explicit without having an operating system in the middle: if you want to display ""Hello, world!"" on an Arduino graphics shield you wind up using a library (or writing your own) which fiddles the hardware bits on the hardware to cause the bits for ""Hello, world!"" to appear.",hdmjx76,t3_prunhv,1632165082.0,False
prunhv,"I think what you're getting at is the actual drawing on the monitor? I forget the exact details but the system call going to STDOUT is being translated to pixels and coordinates in a monitor screen relative to screen resolution, I have mad respect for the people that did all the work to abstract those steps away for the rest of us",hdlb3gi,t3_prunhv,1632146487.0,False
prunhv,"This is such a beautiful question. It's so simple, but the answer is about 10 million lines of code long, documented acros a couple tens of thousands of pages of specifications and interfaces.

The ""why"" of it buried in almost arbitrary decisions made in 1982 and before, based on what we thought was the cutting edge of technolog and implenentation ... but is not just seen as a burden.",hdo6zre,t3_prunhv,1632191465.0,False
prunhv,"absolutely! sometime we don't appreciate the things happened underneath.

Let me try to explain:

so for computer.. most important 2 things are , CPU and RAM, then I/O devices (KVM, harddisk, network etc)

for this case, CPU, RAM and Monitors are related. (GPU is optional and can be ignored)

if we passed OS/Kernel things. the CPU basically just need to put certain things (a few ASCIIs code into a certain position of VRAM) - assume it is TEXT terminal.

and DISPLAY card (read the VRAM regularly) would display the new TEXT in monitor.

btw, all these steps haven't been changed maybe 30+ years. or more. (last part actually is OS independent)",hdodhug,t1_hdo6zre,1632194666.0,False
prunhv,Its not a kernel function. Basically your message is written to the memory to a special address space or dedicated vram and from there read by the graphics card and displayed.,hdl6fwq,t3_prunhv,1632144302.0,False
prunhv,What? This does not sound correct. Stdout is not managed by GPU?,hdngntc,t1_hdl6fwq,1632179152.0,False
prunhv,"Depends. Is this system in text mode, or writing to a terminal window in a GUI?",hdo7342,t1_hdngntc,1632191508.0,False
prunhv,"So after the system call, the kernel takes control from the cpu and gives it to gpu?",hdl9sod,t1_hdl6fwq,1632145894.0,True
prunhv,"Not exactly. One of the CPU's main functions is reading and writing to memory, which it achieves by lighting up pins it's connected to. Memory is addressable, meaning we give every* combination of pins a number. Not every pin combination is wired to your RAM chips. By ""writing"" to memory addresses that are physically wired up to other physical components, such as the GPU, your CPU gives them work to do. The GPU is its own subsystem that doesn't necessarily need the CPU to do further work. It can translate concepts (strings, pixels, complex data types) to electronic signals that control your monitor.

Sorry if this is a little vague on details, computer architecture isn't my area of expertise. But if you want a recommendation for a fantastic (and accessible) video that might answer some of your question, search youtube for ""The World's Worst Video Card"" by Ben Eater.",hdlgdbd,t1_hdl9sod,1632148821.0,False
prunhv,"So there is no specific cpu assembly instruction that commands the gpu? The cpu just Loads data to a specific address field, and by loading data to that address the gpu knows it has some work to do since it is also connected to that address?",hdnyb8k,t1_hdlgdbd,1632187446.0,True
prunhv,"Well, it's via the PCI bus or whatever your system is using, but yeah, there's usually no specific instruction for it. Instead the GPU driver software runs in the CPU and talks to the GPU using the PCI bus. It'll stream the GPU commands/data over the bus.",hdo9iaj,t1_hdnyb8k,1632192644.0,False
prunhv,"it might be worth looking at a simpler computer to understand the hardware part before looking at what the OS adds on top of that. To that end, /r/beneater or his youtube with the breadboard computer, either the old one or the new one. He describes both video and text output.

But in short: put stuff in memory as a display buffer, tell the graphics hardware to draw that to the screen.",hdmsppn,t3_prunhv,1632168682.0,False
prunhv,"From the CPU perspective it's all memory and bits manipulation. Now there are some special memory addresses that are connected (wired) to a physical component, and this physical component will do the job of converting digital information to tangible physical stuffs.

How exactly? One example is imagine having a bunch of light bulb wired to bits in memory, and will light up when there is electron going though it (which corresponds to bit 1). Then patterns in the bits will correspond to light patterns of the bulbs.

For other components, it will depend on what kind of signal it receives. In the end there will always be some kind of digital to analog signal converter to make them all works.",hdpeh5h,t3_prunhv,1632223449.0,False
prunhv,"stdin and stdout are abstracted as file handles, as many things are in Linux. To output something you just write to stdin as you would to a file.",hdoqcp3,t3_prunhv,1632202629.0,False
prunhv,It is a sys call,hdl77gy,t3_prunhv,1632144669.0,False
prunhv,no dude its a CPU instruction,hdms02z,t1_hdl77gy,1632168386.0,False
prunhv,"my kids have been learning comp sci basics [from this youtube channel,](https://www.youtube.com/channel/UCvIzIe8bIMkAkblXz4aQ-Pw)its pretty fun",hdrs3c1,t3_prunhv,1632261318.0,False
prem7e,"There's not really much need. You can use n bits to represent 2^n. 2^32 is already about 4 billion, and 2^64 is that squared. These numbers already get so big. When you give up some bits for floating point, you'll lose accuracy but it's at a scale where it doesn't really matter. You could go bigger but you probably don't need to.

Edit: I also imagine your circuitry would grow a stupid amount, like a 1024 bit full adder would take way more gates to construct than a 64 bit full adder for probably no benefit because the 2^1024 is way bigger than anything you would likely want to compute.",hdi1nfw,t3_prem7e,1632079994.0,False
prem7e,"Because those sizes work well for nearly all tasks.

If you really want infinite precision you can get it was BCD types.

cryptography library’s would use much larger numbers; as would various scientific programs.",hdi255p,t3_prem7e,1632080183.0,False
prem7e,"You dont no just 1 or 2 numbers. You need many of them. This is why we dont use 1GB per number.

64 bit CPU can process a 64 bit number in 1 cycle. If you want to use 1024 bit numbers you need 16 cycles. Meaning 1/16 of the performance meaning 1/16 of the fps (this is 10 fps instead of 160). It will also take up 16 times more ram and disk space (Warzone would need ~1.8TB)*. Keep in mind there are many numbers to compute to create just one frame.

Increasing the precision will not solve the problem. The problem is that you have too much information on a small area.

(*) actually not, because for example image formats have other bit sizes for color etc.",hdic1hk,t3_prem7e,1632084139.0,False
prem7e,Doubles should do the trick,hdjlz5p,t3_prem7e,1632105218.0,False
prem7e,"> So i was searching a fix for my game bc textures and vertices on ground and models would start to jump around bc of floating point error.I searched how to upgrade from 32bit int-float limit to 64bit one but i couldn't find anything, even if i did convert it to 64bit limit there would still be this bug after certain distance.

I'm not a computer graphics person, so I could be wrong, but this sounds like Z-fighting. So maybe the problem isn't so much the lack of precision leading to loss of accuracy, but with the Z buffer implementation. Whether that's because of the SW or HW, I don't know.

> Why dont computers use 1024int numbers or even 4096/8192bit integers. Compared to normal hdd that have 500+GB of memory or even RAM memory if they are talking about it,its still a fraction of it and wouldn't take much space.

Ignoring the fact that very few people need the precision from a 8,192-bit integer, which others have pointed out, from the perspective of computer engineering, this is a terrible idea. Computers don't directly operate on data stored in the HDD or main memory. They move it into registers first. These days, even processors that implement out-dated CISC architectures (which can directly use operands in the main memory) move the operands into the registers first.

Moving 1 KB of data for one integer is terribly expensive in terms of bandwidth, latency, and power. The data path to main memory is 64 bits. In PC processors, you get like four or six of these channels. How many cycles is it going to take to transfer one of these 1 KB integers? (Not to mention that the current purpose of having four or six channels is not to transfer one big datum quickly, but for there to be concurrent accesses).

And power. Everybody agrees that we're using too much power moving data around. We would like the data to be closer to where its consumed. Hence the reason why we have HBM and the processor on an interposer: it's to reduce the distance, and hence latency and power. It's also the reason why there's research into alternative floating-point number systems like posits, which promise the same level of precision in half the bits (so a 32-bit posit can be substituted for a 64-bit IEEE 754 double). Posits would enable less power for data movement, allowing more for compute.

So far, we've viewed computers at an extremely high level. Data goes from one part to another, something happens, that's it. Modern computers aren't like this; or rather, modern processors aren't like this. Modern processors (and even very ones 50 years ago) have very heavily pipelined circuits, and the circuit blocks are decoupled with buffers. This is all so performance enhancing techniques like speculation, dynamic scheduling, and look-ahead can be used. Pipelining requires storage elements to hold state temporarily; buffers are small RAMs. If 8,192 bit integers are used, the storage elements for pipeline and buffers for decoupling and speculation will have to increase many times in size if there are to have the same amount of capacity in terms of entries. It's all infeasible in terms of area, delay, and power.

Also, while HDDs and the main memory aren't limited in capacity and could easily contain many of these big integers, the case isn't the same for the cache hierarchy. Your average L1 cache is several 10s of KB, your L2 is several 100s of KB, and your L3 is several 10s or a few hundred MB (for PC processors it's the lower end of that range).

> And CPUs today can do billions oc calculations per second so even if its a 8192bit numbers pc shouldnt have a problem.

Those billions of calculations per second are achieved on 32/64 data. If integers were 8,192 bits, I can guarantee you wouldn't reach anywhere near this level of performance. I would also make a distinction between throughput and latency. Billions of calculations per second is throughput; if you have a case where an operation depended on an earlier operation, performance would be hundreds of times lower, because doing arithmetic on 8,192 bits, from a computer arithmetic perspective, is *insane*.

Take an array multiplier as an example. Ignoring Booth re-encoding (which reduces the number of partial products we have to add), you'll need an 8,192 by 8,192 array of AND gates to generate the partial products: every bit in the multiplier is AND'ed with all those in multiplicand. 8,192 * 8,192 = 67,108,864. Each of these gates requires multiple transistors. A 64- by 64-bit multiplier only needs 4,096 gates. We haven't even gotten to summing the partial products yet, and we've already blown our transistor budget! There's techniques we can use to cut down the cost, but that's not my point. My point is that it's impractical to do 8,192-bit integer arithmetic in a timely fashion.",hdl8fv2,t3_prem7e,1632145262.0,False
prem7e,youre mostly right but floating point inaccuracy leads to z-fighting when the numbers get really big. 3kliksphilip should this in a recent video demonstrating its effects in the source engine.,hdm7i5w,t1_hdl8fv2,1632159976.0,False
prem7e,"When i said ""jumping around"",i wasnt thinking about z fighting but when the vector3 position of vertices is above the bit limit and theres less decimal places.Like (0,0,1.0000000000)in this case theres 10 decimal places and the precision is very high.But if i move the model far away from center,like to (0,0,1111111111.0) the precision is now very low.If i move the model 0.04 in z direction nothings gonna happen but if i move it by 0.05 its gonna snap to the closest possible position that is 111111111.1 and when the distance becomes great the precision drops even more and vertices cant rotate corectly,movement is snappy,and textures dont shoe propertly.The same thing happens in minecraft when you go beyond the border,see it yourself.
.
And thanks for the second part,made me realise how much i didn't know about computers.But 128bit int limit would be nice for game engines,especially with the ""infinite world"" generation or clicker games where values go beyond 32bit limit pretty quick.Thank you for spearing your time and teaching me why big numbers like that wouldnt be ""good"".",hdn9abx,t1_hdl8fv2,1632175749.0,True
pr87l2,"Caching is used as a term for storing intermediate data for fast access.

Cache is location, where those data are stored and where they can be accessed faster than in other locations.

In processor, it is as you said, a small but fast memory chip that procesor uses for storing intermediate data it currently works with. E.g. when doing some mathematical instructions with some numbers, having these numbers loaded from ram or SSD/HDD would take much much longer than loading them from cache memory right next to chip.

In software, caching data can be anything, that allow program access data faster than by other means. E.g. in some chat applications like whatsapp or messenger, all already received messages are ""cached"" in the local drive of your phone instead of loading them from server each time you open the app. The app only load new messages from server and stores them locally - ""caching"" them.",hdgqtwp,t3_pr87l2,1632060891.0,False
pr87l2,"Yeah, thanks for the explanation. Terms in CS sometimes have multiple meanings, and it can be a bit confusing when you are starting to learn them all.",hdgrcx3,t1_hdgqtwp,1632061140.0,True
pr87l2,"It is not multiple-meaning, both caches are still caches. They serve the exact same purpose: speeding up the program and could work perfectly fine without them.",hdl0jhd,t1_hdgrcx3,1632141254.0,False
pr5s8k,"‘Computer Science is to computers what Astronomy is to telescopes.“
-Edsger Dijkstra",hdgdz5i,t3_pr5s8k,1632054046.0,False
pr5s8k,"I know that guy, he made a cool algorithm or two",hdj53dl,t1_hdgdz5i,1632097091.0,False
pr5s8k,Elegant lol,hdgrjvu,t1_hdgdz5i,1632061228.0,False
pr5s8k,Holy deep,hdh0zwp,t1_hdgdz5i,1632065318.0,False
pr5s8k,Arguably modern theoretical CS is more doable without computers than modern astronomy is without telescopes.,hdjcrid,t1_hdgdz5i,1632100767.0,False
pr5s8k,"CS if considered in a very broad overview it's a mathematical tool to solve a mathematical problem. 

And it has its own beauty which right now is unrealised due to so much abstractions and ever growing new use cases. 

Also for astronomy you don't always rely on telescopes again it uses mathematics and other techniques to know about certain things. Telescope is a tool to understand and mainly to confirm rather than being used continuously as a tool. It's not attached to your eye and used , it's not that continuously used or critically needed.",hdjtx4y,t1_hdjcrid,1632109243.0,False
pr5s8k,"There were centuries of recorded astronomical observations before telescopes existed.  Sorry, try again.",hdjidam,t1_hdjcrid,1632103471.0,False
pr5s8k,Well put.,hdi9nf6,t1_hdgdz5i,1632083136.0,False
pr5s8k,Much more people confuse CS with printer and router maintenance. Especially my relatives.,hdhf5b7,t3_pr5s8k,1632071201.0,False
pr5s8k,That's why I never mention to my extended family that I study CS. I don't want my aunt to ask me to repair her dusty 2005 printer which stopped to function in 2011.,hdrtozi,t1_hdhf5b7,1632262041.0,True
pr5s8k,"I think you are half-right. The bit you’ve got wrong is to assert that CS is “the mathematical study of computation”. that’s absolutely a core field within CS,  but phrasing it as you do excludes a whole range of other legitimate CS sub-fields.

Also I guess you meant “nursing” not “nursery”",hdg7kdo,t3_pr5s8k,1632049806.0,False
pr5s8k,"This.

Computer science is like having a mathematician, a linguist and a physicist getting a rock to talk.

And each of those have respectable subfields that contribute to us being able to shitpost on reddit.",hdhmwra,t1_hdg7kdo,1632074188.0,False
pr5s8k,"I think one of the best examples of those sub fields excluded by OP's description is the study of UI design. UI design is arguably more of an art than any other sub-field, but I'd say it absolutely still falls under the umbrella of CS.",hdhhgzt,t1_hdg7kdo,1632072106.0,False
pr5s8k,"I, on the other hand, would argue that UI design doesn't fall under CS almost at all",hdhpgys,t1_hdhhgzt,1632075185.0,False
pr5s8k,"There are some applications that fall more under EE/CE than anything, especially at the lowest levels. Keyboards, mice, and monitors are a few examples. If we assume UI design is a subdiscipline of SWE, then UI design is the only discipline within CS where the end user's interaction with a system actually matters, and that's what sets it apart from all of the other sub-fields in CS. The exigency behind including UI design as a discipline of CS is to make it possible for the common person to take advantage of the power that a computer architecture has to offer. If we could actually do something with some of the models of theoretical physics that we have, then the subfield of implementation would probably still fall under physics.",hdhxdvh,t1_hdhpgys,1632078336.0,False
pr5s8k,"> If we assume UI design is a subdiscipline of SWE, then UI design is the only discipline within CS where the end user's interaction with a system actually matters, and that's what sets it apart from all of the other sub-fields in CS

UI design isn't subdiscipline of just one thing, but OK, let's assume it's limited only to SWE. It still doesn't fall under CS, because SWE isn't a subdiscipline of only CS. It's more complex. For example, human and task management is crucial for SWE, but those are concept from outside CS.

>The exigency behind including UI design as a discipline of CS is to make it possible for the common person to take advantage of the power that a computer architecture has to offer.

Could you reword it in some other way? I think I don't quite get the point behind this statement. How does end user (?!) matter for categorizing something as subfield of *scientific* discipline?",hdi15hd,t1_hdhxdvh,1632079802.0,False
pr5s8k,"Is your argument that UI design is one of the other disciplines of engineering as a general field? If that's the case, then I'd be willing to retract my statement that UI design is a subset of CS, because that would make a lot of sense.",hdi7ruf,t1_hdi15hd,1632082371.0,False
pr5s8k,"Basically yes, UI design is combination of art and engineering, but doesn't fall under either of them exclusively",hdido3u,t1_hdi7ruf,1632084814.0,False
pr5s8k,it depends on the interface used really. most of the time id say its not CS,hdi415v,t1_hdhpgys,1632080905.0,False
pr5s8k,"He means to say to those people who do the subfield only not even the core part and call themselves that they are computer scientist or know computer scientist. 

Starting with html css and learning web dev alone doesn't make you a computer science knowing guy or to be considered CS is that. 

That's his point.",hdju8pr,t1_hdg7kdo,1632109420.0,False
pr5s8k,"I think Computer Studies becomes Computer Science when you can define ""algorithm"" and ""turing complete"".

If you dont know what a turing machine is then its just computer studies imo.

to everyone downvoting: Sir Alen Turing is basically the founder of Computer Science, turing complete is used to define an algorithm as being *computational*",hdi3g0o,t1_hdg7kdo,1632080679.0,False
pr5s8k,"I don't know why you got downvoted but I totally agree.

I wouldn't call someone who can't explain what a differential equation is ""mathematician"" for the same reasons I wouldn't call someone who can't explain what s Turing machine is ""computer scientist.""

Of course, not explaining in its broad terms but having a decent amount of knowledge and expertise (if possible) in these major subfields of Computer Science.",hdrth6a,t1_hdi3g0o,1632261941.0,True
pr5s8k,"i was expecting to get downvoted, every person who downvoted me did not study this at uni. i shall forgive their ignorance.",hduhguj,t1_hdrth6a,1632320127.0,False
pr5s8k,"But nursing is also medicine. I completely see your point but at the same time, you need practical examples to learn concepts anyway. And the examples might as well be often used ones in real life. Also I don’t think it’s just computation. I took a lot of networking classes in my masters cs.",hdgbhne,t3_pr5s8k,1632052497.0,False
pr5s8k,"Oh oh, we had comments like this. Be aware that people will accuse you of gatekeeping.

We have a term for industrial coding: software engineering. It's a subset of computer science, though. The takeaway should be, that software engineering is not equivalent to computer science, much like physics and engineering.",hdgb39g,t3_pr5s8k,1632052230.0,False
pr5s8k,"Bingo.   This is one of the biggest differences I see in the software world.  There’s an enormous difference between writing code by yourself, for yourself, and writing code for commercial purposes.  Not using sound engineering practices kills projects, regardless of the brilliance of the coders.  

In my case I learned how to code when I was a kid, I learned CS as an undergrad, and I learned engineering at my first job job after college.",hdgctq1,t1_hdgb39g,1632053351.0,False
pr5s8k,"My degree program is technically software engineering.

They still call it a computer science degree.

This same computer science degree also ties into web development, and other subsets of information technology.",hdgrmha,t1_hdgb39g,1632061260.0,False
pr5s8k,"In Germany, we have universities of applied sciences, where I studied computer science to a bachelors degree. It was much more focused on the needs of the industry than the bachelors program at the university, which turns out to be quite a problem at the moment during my masters program at university.",hdgz24p,t1_hdgrmha,1632064480.0,False
pr5s8k,"That's the issue. I've seen a plethora of university and community college curriculums in the UK and the US being equivalent to a coding bootcamp, yet being named ""Computer Science.""",hdhkkwh,t1_hdgrmha,1632073300.0,True
pr5s8k,"I highly doubt most of my classes are coding boot camps.

Just talking to former grads, we use coding as a means to an ends, while exploring engineering concepts.

For example there's two classes I'm in now... One had NO coding, it's simply architecture and analysis of software design, basically teaching us how to plan out projects and work as a team utilizing different roles.

My other class is hardware design and architecture. It's teaching us how the hardware works, down to the logic gates. It has minimal coding, most of which is Arduino for weekly projects or assembly.",hdhltps,t1_hdhkkwh,1632073766.0,False
pr5s8k,I'm a bit confused how you would see a 3 month bootcamp as equivalent to a 2-3 year college program.  Can you elaborate on that?,hdhy7fh,t1_hdhkkwh,1632078656.0,False
pr5s8k,"I don't. Those small Colleges name their degrees that way, despite having a weak curriculum (which I compared to be equivalent to a bootcamp). I don't think ""Computer Science"" is the right term to call those degrees, but they do it anyway, probably to gain more applicants.",hdi5wru,t1_hdhy7fh,1632081634.0,True
pr5s8k,"Applied computer science is a thing m8, even if you consider theoretical CS better or whatever. There is a place for both and more. Keep an open mind and don't be too elitist about your field of study, just because one gets more recognition than the other.

Relevant XKCD? https://xkcd.com/435/",hdij3wq,t1_hdi5wru,1632087097.0,False
pr5s8k,"Would you call a whole degree of applied statistics ""Mathematics?""

If not, then why would you call a coding degree ""Computer Science?""

I am not saying one is better than the other. I am just saying that they should use the correct terminology for each domain. Theoretical Computer Science is, as the term ""Computer Science"" suggests, a science.

Calling a degree of web development and industry-related coding ""Computer Science"" is, I believe, simply incorrect.",hdik5kn,t1_hdij3wq,1632087550.0,True
pr5s8k,"No, you are using your own definition or what it should be. The definition of computer science, as others already posted, does encompass more than just the abstract part.

And maybe instead of telling others to differentiate, you should differentiate yourself? How about you specify your degree in theoretical computer science?",hdizinf,t1_hdik5kn,1632094405.0,False
pr5s8k,"Consider the following: 
  
  * clerk in the shop calculated your change
  * your neighbor calculated the square area of her living room
  * engineer calculated the differential to determine the best ratio of a metal cone to use

All those people used math, but none of them were mathematician. None of them did Mathematics, they did just simple math.

I believe that is the distinction u/pastroc is arguing for.",hdny8uj,t1_hdizinf,1632187414.0,False
pr5s8k,"Thank you, I understood what u/pastroc said without hyperbole. But the people we were talking about still study computer science. They are supposed to be able to read CS papers and keep up to a degree. I agree that there should be a distinction between programming and computer science, because the latter is a more general term. But stating that people who focus on programming during their studies are not supposed to be called computer scientists is weird. There are a lot of people who focus on any subfields of any science, are those people not practitioners of that science, just because they are specialized?

Edit: I think context is important as well. You can tell someone what you studied and be more general such that the other can get an impression on what you find interesting. During a job interview, there is a differentiation between programming and theoretical computer sciences, depending on industry vs academic, for example. So I wonder what we are trying to find out here.",hdp0y4a,t1_hdny8uj,1632211479.0,False
pr5s8k,"What you're thinking of is called Theoretical Computer Science. It's a subset of Computer Science, but not all there is to it. [The field of Computer Science is pretty wide.](https://www.flickr.com/photos/95869671@N08/36231833334)

It's true that Computer Science is not just programming. But the opposite is also true. Computer Science is not just theory.",hdgyft4,t3_pr5s8k,1632064203.0,False
pr5s8k,I agree. Code is just a tool for cs.,hdg6h7d,t3_pr5s8k,1632048963.0,False
pr5s8k,"Code is also a _product_ of computer science. Formal definition of languages and grammar is extremely mathematical, and core CS theory. 

The products of all that theory are programming languages - and if you keep pushing that application of theory you kind of naturally end up with compilers and interpreters, etc.

Everyone should go through the exercise of implementing a simple interpreter and/or compiler at some point - nothing hammers home so many core CS concepts with tangible code examples.",hdh487g,t1_hdg6h7d,1632066683.0,False
pr5s8k,This,hdhmuf1,t1_hdg6h7d,1632074162.0,False
pr5s8k,Computer Science programs pump out .NET developers here in OKC.  I have actually only met one good candidate that relied on college education for a web development job so far.  Same for the data science programs that refuse to teach SQL.  We will go with someone else that knows hot to do it and knows a little bit about why they're doing rather than someone who can shoot off theories about machine learning and what the best framework manufacturer is.,hdgz7ja,t3_pr5s8k,1632064547.0,False
pr5s8k,"Computer Science: the Science of Computing?

I honestly haven’t done much too much coding as soon as we were done Intro Comp and Data Structs, we started into the how computing does what it does (also Data Structs was more about the efficiency of storing and moving data in code). Honestly, COVID has left me with second thoughts if this was the right major for me.",hdguc4c,t3_pr5s8k,1632062474.0,False
pr5s8k,Yep. I switched. My university has a mix of coding and theoretical classes. But nah it’s not for me. But I switched to something where I’m able to still stay in the tech realm which is what I really want to do. I may still be coding or editing videos or doing 3D model etc,hdkjq6a,t1_hdguc4c,1632128707.0,False
pr5s8k,Someone posts this like every day.,hdgdrfh,t3_pr5s8k,1632053921.0,False
pr5s8k,I rarely check this sub out. I apologize for starting a thread on a topic that has been addressed countless of times previously.,hdgrbh9,t1_hdgdrfh,1632061122.0,True
pr5s8k,"A good old prescriptivist vs' descriptivist debate :)

My prescriptivist heart wants to agree with you (although I'd still say it's a little broader than just the mathematical study of computation). My descriptivist mind understands that CS and Software Engineering are often used interchangeably so I try not to hold it against people.

The only time I think the lack of distinction is harmful is setting expectations for CS undergrad education. Learning how to code isn't necessarily the goal but instead code is a medium to express underlying ideas. Of course, in practice this means people in CS tend to know a good bit about coding.",hdgd8sy,t3_pr5s8k,1632053608.0,False
pr5s8k,what’s the difference between computer science and。computer security？,hdh8s85,t3_pr5s8k,1632068579.0,False
pr5s8k,I believe one is a subset of the other?,hdhktot,t1_hdh8s85,1632073392.0,True
pr5s8k,"Computer security is the field of securing computers, that is less concerned about ""how"" as it is with ""why"".

Computer science is what you need to fully understand and implement the way computer security can be broken, the tools that reveal how it was broken after the fact, and improvements to make it better. It doesn't care about why, it only shows you how.

Comp Sci: ""FYI, you're using protocol X, it has vulnerability Y that I can exploit with tool Z. Here's a demo...""

Comp Sec: ""Dear Executives, let's not use protcol X, because reasons.""",hdknqz1,t1_hdhktot,1632132436.0,False
pr5s8k,I just enrolled in college from a long break to major in CS in hopes to learn/work with coding and make a living. Is what I’m doing by incorrect?,hdhla76,t3_pr5s8k,1632073563.0,False
pr5s8k,"(I don't understand the point of your comment? I may be misunderstanding you.) 

Majoring in Computer Science is a reasonable way to acquire an auspicious skillset to work with coding.

My concern is with educational institutions, or anyone, who label a typical software engineering bootcamp (popular framework in the industry, practical knowledge) as ""Computer Science."" A bit like labeling piloting as ""engineering.""",hdhlwof,t1_hdhla76,1632073797.0,True
pr5s8k,"Ah my apologies and I thank you for the clarification! My initial thought from reading your post was that the term or major Computer Science isn’t what it seems, but I understand what you were intending to say now. I was just concerned due to me signing up for CS as I’m looking forward to being prepared and learning more about the principles, procedures, etc. and didn’t want to mess up a chunk of my life signing up for something that I won’t need to learn. Thanks again!",hdhmt9c,t1_hdhlwof,1632074149.0,False
pr5s8k,"I think you are a little off in your understanding of what ""computer science"" is.  Computer science has both a theoretical and applied/practical element to it.  It's not as though people are studying and developing computers for no practical reason.

&#x200B;

>everything related to scripting, HTML, industry-related coding practices etcetera should have their own term

Typically it's called ""software development"" and it's very much a part of computer science.

The way I look at is that computer science *includes* software development, but software development is not all there is in computer science.

If you're looking for a term to distinguish the theoretical from the applied, simply saying ""theoretical computer science"" would be enough for most people to understand you're referring to the mathematical studies of it.  You might also be looking for the term ""data science"" which refers to the mathematical studies of data, and again ""data science"" is often included under the general term of ""computer science"".

&#x200B;

>To me, industry-related coding labeled as 'Computer Science' is like, say, labeling nursing as 'medicine.'

Nursing is part of medicine.  Seems very odd that anyone would think it's not.",hdhwp5l,t3_pr5s8k,1632078063.0,False
pr5s8k,"I’m still in high school, but in middle school I had a “Computer Science” class. Before 2020, I was in school and all we learned was how to touch type in the computer-lab. I don’t really know if that qualifies as the class",hdi489p,t3_pr5s8k,1632080982.0,False
pr5s8k,"Programming is a subset of computer science. It is both necessary for what you're wanting to call computer science and is also a product of it.

This quest for ""purity"" ends only in toxicity. There's proper terms to define these subsets. Theoretical CS and Software Engineering come to mind. Let's use those, instead of trying to push Software Engineering out of some imagined purely mathematical definition of CS.",hdjcnzg,t3_pr5s8k,1632100720.0,False
pr5s8k,“The quest for purity ends only in toxicity” is a great quote!,hdkiokh,t1_hdjcnzg,1632127701.0,False
pr5s8k,"I don't think it's really about purity. The current hype over coding has basically dumbed-down computer science to portray it as equivalent to coding. But it's so much more expansive, diverse, and deep than what the popular discourse would like it to be. I can understand the concerns over gate keeping and purity, but sometimes the insistence of some people that coding = computer science is merely populism, elitism's equally nasty polar opposite.",hdl92v5,t1_hdjcnzg,1632145559.0,False
pr5s8k,"""Popular discourse would like it to be""

Can you give me examples of this? Where is the public discourse rejecting anything that isn't programming as ""not CS""?",hdland8,t1_hdl92v5,1632146286.0,False
pr5s8k,"I'm not saying there is a campaign underway in the public discourse to sanitize computer science of anything that isn't coding; I'm saying that computer science is *more* than just coding, but this fact has been completely overlooked in the popular discourse because of the hype around coding.",hdldxll,t1_hdland8,1632147757.0,False
pr5s8k,I’m impressed that you actually typed etcetera and not “etc” 🙂,hdgr99y,t3_pr5s8k,1632061094.0,False
pr5s8k,*Et cetera* is two separate words.,hdgwtvn,t1_hdgr99y,1632063509.0,False
pr5s8k,My five years of Latin education paid off.,hdgrop4,t1_hdgr99y,1632061288.0,True
pr5s8k,"""et cetera""...",hdhcj4m,t1_hdgrop4,1632070121.0,False
pr5s8k,naww did i take up the wrong course  then,hdg6kmv,t3_pr5s8k,1632049041.0,False
pr5s8k,"the ""right"" course is the one that helps you accomplish your goals.",hdhyjme,t1_hdg6kmv,1632078794.0,False
pr5s8k,"Sorry for being a spoilsport, mate.",hdg6p3d,t1_hdg6kmv,1632049142.0,True
pr5s8k,[Wikipedia does an excellent job defining this field of study.](https://en.m.wikipedia.org/wiki/Computer_science),hdgh26n,t3_pr5s8k,1632055884.0,False
pr5s8k,"**[Computer science](https://en.m.wikipedia.org/wiki/Computer_science)** 
 
 >Computer science is the study of algorithmic processes, computational machines and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software. Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hdgh3c3,t1_hdgh26n,1632055901.0,False
pr5s8k,"Well, coding, if you want to do it well requires computer science, we could say that computer science it's a superset of well done coding",hdgkzpf,t3_pr5s8k,1632058037.0,False
pr5s8k,Most people confuse things about fields in which they have no part of. Not just computer science,hdgjbqf,t3_pr5s8k,1632057150.0,False
pr5s8k,"Sort of agree, most of people are just doing engineering work after cs work, well for the money",hdinb1w,t3_pr5s8k,1632088936.0,False
pr5s8k,"Computer science has always been the theory and application of computer technology, therefore it also includes web dev, any other form of programming too since you need to be able to program systems in order to apply them to some domain.

Computer Science not only considers current technologies, but also future “theoretical systems” that can be used to solve currently intractable problems.",hdj3q6u,t3_pr5s8k,1632096421.0,False
pr5s8k,"You can absolutely code without computer science, but just know computer science is why it works.",hdk65hj,t3_pr5s8k,1632117079.0,False
pr5s8k,"This is like the astronomy vs telescopes debate. The saying is that telescopes:astronomy::computers:computer science. But practically speaking, no one builds or fixes a telescope without knowledge of how to do astronomy. You could say telescopes are mere tools to support the practice of astronomy, and in some sense you’d be right, but more than half of the field of astronomy is owed to the existence of telescopes, and vice versa, so it’s not really fair to say they are entirely separate things.",hdgf00i,t3_pr5s8k,1632054671.0,False
pr5s8k,"This is Reddit gatekeeping at its best. This is like the people who say commujity managers arent game developers because they work with social media. 
Programmers, scripters, coders, silicon chip architecture engineers are all computer scientists. If you dont like it, hoes mad. Stay mad",hdh9wyu,t3_pr5s8k,1632069048.0,False
pr5s8k,I'd argue silicon chip architects are closer to computer engineers than computer scientists,hdhoxns,t1_hdh9wyu,1632074975.0,False
pr5s8k,"Indeed, most professionals in that line are EEs or CMPEs (if they came out of US universities). Not CS, relatively few CS programs cover the necessary prerequisites to go beyond VHDL/Verilog level design work. By the time you're actually designing it for a chip outside an FPGA you're dealing with physics that most CS graduates will have barely seen in one course their freshman or sophomore year of college.",hdhrmts,t1_hdhoxns,1632076041.0,False
pr5s8k,[deleted],hdhro2z,t1_hdhrmts,1632076055.0,False
pr5s8k,"Hey /u/GenderNeutralBot

I want to let you know that you are being very obnoxious and everyone is annoyed by your presence.

^(I am a bot. Downvotes won't remove this comment. If you want more information on gender-neutral language, just know that nobody associates the ""corrected"" language with sexism.)

_^(People who get offended by the pettiest things will only alienate themselves.)_",hdhrp4u,t1_hdhro2z,1632076067.0,False
pr5s8k,"Some of them could very well be computer scientists. The architecture level is where software starts, after all. As we go lower, towards organization, circuit design, physical design, then it's computer engineering. Physicists have also been known to work in these areas.",hdlacib,t1_hdhoxns,1632146148.0,False
pr5s8k,[deleted],hdhei6v,t1_hdh9wyu,1632070936.0,False
pr5s8k,"Nice strawman arguments there. And yes, everyone you mentioned is a game developer and IT professionals are in computer science. Stay mad.",hdi42s8,t1_hdhei6v,1632080922.0,False
pr5s8k,"Yes. It's frankly annoying when people don't let you decide for yourself what your job's title or your area of work should be called. 

I'm myself under investigation for saying I was a neurosurgeon because apparently I need the state's authorization to call myself that! How crazy is that?",hdhvi16,t1_hdh9wyu,1632077589.0,False
pr5s8k,"Better add this ""\s"" as I think for some the sarcasm may fly over their head",hdjk4xy,t1_hdhvi16,1632104324.0,False
pr5s8k,"Yeah, you don’t have to be a CS major to use CS in your job or hobby. Just like you don’t need to be a physics major to use physics. 

You’ll probably know significantly more if you are, though. And I’m neither…",hdkj26p,t1_hdh9wyu,1632128057.0,False
pr5s8k,"Computer Science is rather a study of ways, methods, and core ideas behind computers. I can't exactly phrase it but Computer Science doesn't even concern itself with coding at all. Coding or more correctly ""Computer Programming Languages"" are rather tools that help us communicate with the lower level of the hardware better.  
It is a science, the most important thing is Computer Science I believe is research and research of connections with other fields. Computers are capable of doing stuff that humans before thought impossible; so it is just like natural sciences.

Just like we say there is a difference between a ""DEVELOPER"" and an ""ENGINEER"", I don't think you need a degree to become either one of these but curiosity to explore and logic to assert and prove.",hdg8a8z,t3_pr5s8k,1632050334.0,False
pr5s8k,Programming Language theory is definitely a part of computer science.,hdgb1u6,t1_hdg8a8z,1632052205.0,False
pr5s8k,"whenever anyone sais they do computer science, ask them what a turing machine is.",hdi26dd,t3_pr5s8k,1632080196.0,False
pr5s8k,Finally!! Someone post which I can screenshot and share in my WhatsApp status and make all of my mates' ass light up on fire.,hdh0u0g,t3_pr5s8k,1632065249.0,False
pr5s8k,"Also, when choosing an engineering major, I think a lot of people do CS bc they like coding, but that’s not really how it works. Most engineering majors have jobs where your work is mostly coding, it’s just the type of coding that differs. You have to figure out what it is that you like about coding first",hdgjh27,t3_pr5s8k,1632057230.0,False
pr5s8k,[deleted],hditos3,t1_hdgjh27,1632091796.0,False
pr5s8k,"Mostly simulations and controls. Things like finite-element analysis and computational fluid dynamics are often handled by people who studied MechE and Aero. CS knowledge doesn’t cover everything there, though it’s still needed to build the platform. I’m sure there are controls jobs for anything from electrical power application to medical devices that would be handled better by subject-matter experts than what’s essentially a branch of applied math",hdiugt6,t1_hditos3,1632092140.0,False
pr5s8k,I think for practical purposes they are fairly interchangeable,hdgf6f0,t3_pr5s8k,1632054779.0,False
pr5s8k,"Whatever happens, correcting someone will make you seem like a giant nerd",hdjc8pa,t3_pr5s8k,1632100515.0,False
pr5s8k,Information Technology? Do people still call it that?,hdjd5sb,t3_pr5s8k,1632100955.0,False
pr5s8k,Fr like I am a cs major and I don’t think I’ve touched a line of code in months in my classes lol,hdjfom0,t3_pr5s8k,1632102171.0,False
pr5s8k,"""real computer scientists know their computer systems""",hdjlom9,t3_pr5s8k,1632105073.0,False
pr5s8k,"If all CS is only the mathematical study of computation then why do they teach operating systems, computer architecture, compilers, networks, etc. in a standard CS curriculum?",hdjzto8,t3_pr5s8k,1632112753.0,False
pr5s8k,"lol fun thread, I found [this youtube channel to teach comp sci t](https://www.youtube.com/channel/UCvIzIe8bIMkAkblXz4aQ-Pw)o kids. my own kids like it quite a bit!",hdrszr8,t3_pr5s8k,1632261723.0,False
pr5s8k,Who gives a shit?,hdkvhns,t3_pr5s8k,1632138199.0,False
pqwowk,r/WatchPeopleCode,hderk0b,t3_pqwowk,1632016488.0,False
pqwowk,"You should specify what type of project, this subject does not cover a few things.

I do not know any, it could be interesting but surely it would be an adventure of many hours and a waste of time for the one who sees. Well I remember a video game developer who was streaming how he was making the game.

There are people who explain well how it is done and its setbacks, surely the topic of your interest you will find one.

Possibly the one who makes a 30 minute video tutorial is a summary without the loss of time and laps that he gave, possibly it took him hours or days, especially if he was also learning, do you really want to see everything? things would be very difficult to capture.

From my own experience I know that in many subjects, basic concepts are omitted that make learning difficult for the initiate, maybe that's why you want to see in detail?

Part of learning is knowing how to investigate, learn yourself and make your mistakes not see those of others.

Of course it is also useful for someone to show you the possible problems and mistakes that were found,",hdebbo2,t3_pqwowk,1632009203.0,False
pqwowk,"Learning how to learn is a process which is I think the core of what makes me stop in the middle of doing projects when I feel like I'm going the wrong direction. I think a lot of people would benefit from seeing something like this.

If someone made like a daily journal etc it would be helpful, the wasted time specifically would be helpful to know what's normal wasting time and what's not normal XD What things are no use going all the way in to research how it works (so just copy paste it), what things are important to do on your own...etc",hdf2aet,t1_hdebbo2,1632021249.0,True
pqwowk,"There is not a normal wasting time and in my opinion look at the loss of time of others would not help you at all because that depend of your experience, your knowledge, your resources  and the difficulty of the project (and a bit of luck of don't screw it up)

and is not wasted time, really wasted time == experience.


About c&p. 

1º if you understand what do the code and you can modify it c&p. Reuse is normal and save time.

2º if you can't do better code, c&p even if you don't understand and have a specific function,    (normally code from stackoverflow and others are very reviewed from others users) 

I not good in maths and i always use 3rd party code/functions, c&p or asked to others with better knowledge, sometimes i really don't understand all the code for that function, just what it needs and what it returns.",hdgd90n,t1_hdf2aet,1632053611.0,False
pqwowk,"I've been building an OS in the browser for 38 weeks now and I've streamed every commit/refactor. If you're interested I'd be happy for you to check it out.

https://youtube.com/playlist?list=PLM88opVjBuU7xSRoHhs3hZBz3JmHHBMMN",hdfvd26,t3_pqwowk,1632040112.0,False
pqwowk,George Hotz.,hdej083,t3_pqwowk,1632012736.0,False
pqwowk,Remindme! 1 week,hde9nz5,t3_pqwowk,1632008435.0,False
pqwowk,"There is a 58 hour delay fetching comments.

I will be messaging you in 7 days on [**2021-09-25 23:40:35 UTC**](http://www.wolframalpha.com/input/?i=2021-09-25%2023:40:35%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/pqwowk/youtube_videoschannels_with_someone_starting_a/hde9nz5/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fpqwowk%2Fyoutube_videoschannels_with_someone_starting_a%2Fhde9nz5%2F%5D%0A%0ARemindMe%21%202021-09-25%2023%3A40%3A35%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20pqwowk)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hdkb5je,t1_hde9nz5,1632120942.0,False
pqwowk,RemindMe! 1 week,hdez8zx,t3_pqwowk,1632019873.0,False
pqkib8,"Having not heard the term before I did a quick search and came across this: [Lock Free Programming](https://www.baeldung.com/lock-free-programming).

I've used all these techniques before; I just hadn't heard the term used to describe these techniques. 

(Apparently ""lock free"" and ""wait free"" does not mean ""never locks"" and ""never waits."" ""Lock free"" means at least one thread can proceed, and ""wait free"" means any waits for a lock are well-defined and finite in time. Such as associating locks with individual data structures rather than having a thread ""lock the world"" to accomplish a lengthy task.)

----

My honest suggestion is to use a language you're familiar with that provides good threading and core locking semantics you're familiar with, and build some sample programs to test the ideas out. (My favorite here is Java, but YMMV.)

----

When I do multi-threaded programming I also keep two other elements in mind. Most of my development is with front-end stuff, like desktop or mobile development, so again, YMMV:

1. Keep all main interactions or main computational elements on a main thread. (With UI, that means make all UI elements on the main thread; for other processing tasks, that means the 'coordinator' of tasks is the main thread.) Use background threads for small computational tasks, and if necessary, use a thread pool or even a dispatch mechanism (like MacOS/iOS's Grand Central Dispatch mechanism) to dispatch finite duration background tasks.

2. Guarantee locking and unlocking order when a thread needs to grab multiple locks on a resource.

    In other words, if you have resources A, B and C, and a thread needs B and C, while a second thread needs A and C, always lock in the order A, B, C. (That is, the first thread *always* locks B before C, and *always* unlocks C before B. And the second thread *always* locks A before C and *always* unlocks C before A.)

    This helps prevent deadlocks, such as when thread 1 locks C then tries to lock A, but thread 2 has locked A and is waiting to lock C.",hdca3qj,t3_pqkib8,1631978715.0,False
pqkib8,I took a module at it and don't mind sharing my university resources on it via a PM. I found that section pretty interesting.,hdt2zf5,t3_pqkib8,1632284463.0,False
pqkib8,"Please go ahead, thanks.",hdt7yr9,t1_hdt2zf5,1632287731.0,True
pqkib8,"The youtube channel of jon gjengset has a few streams related to both lock free and wait free structures. There is a wealth of information in those streams, but the language used is Rust; if you have no issue watching material about construction of data structures in a language you may not be familiar with I recommend giving them a watch.",hdzql5v,t3_pqkib8,1632420569.0,False
pqhh7v,"I think you misunderstood what H+ is.H+ is simply:

`if ( H(input) )`  
`while(true){}`  
`else`  
`return;`

The only unknown thing about it is H, if H existed, than creating H+ would be trivial and would not break the laws of physics. So, let's assume that H exists and uses text files as input. We construct H+ in the above way, inlining H and putting the result in a text file. Now, we run H using the text file as both inputs. By definition, H is supposed to tell us whether the given program will halt on the input, so it is supposed to tell us whether H+ will halt on input H+. But the answer to that question contradicts itself, so there is no correct output H can give. So, H cannot exist.

At least at a glance, it is possible that H' exists, defined as:

* If the program halts on the input returns true
* If the program loops forever returns false
* If the program has weird contradictory shit throws an exception

However at this point we would have to narrow down exactly what ""weird contradictory shit"" is.",hdb3pp5,t3_pqhh7v,1631951734.0,False
pqhh7v,"Sorry, I'm not reading all of that, just wanted to point out that you can flip the sign with 

    outcome = not outcome",hdh7bdh,t3_pqhh7v,1632067970.0,False
pqhh7v,"The simple key to understanding the nature of the halting problem - is to realize that for any computing machines actions (outputs) to be predicted - it may at worse be required the running (simulating) of the program inside the computing machine who's actions your are trying to predict.

It's an obvious kind of thing, almost not worth mentioning and something which you can easily prove to yourself sitting in a chair.

The consequences of the halting problem are huge in the fields of P space complete (multiplayer) game theory - but the ways used to prove the halting problem (such as your referencing) are in my opinion a bit convoluted and outdated (involving printers etc).

The basic point they show in such videos is that it's necessary to know both a programs input and it's source code and most importantly, you must RUN that source code.

The correct perspective in which to understand the halting problem is algorithmic performance analysis, some algorithms can be verified in less time than it takes to actually run, for example if I tell you that I found some gigantic prime, then you can check if the number I found is indeed prime and in less time that in would take you to go looking thru a bunch of random huge (likely non-prime) numbers. 

The result was NO, you can't get away with any tricks, atleast in the worst case - there is no way to predict a programs output without actual execution of that program in the general case atleast.

In reality source code analysis is simple and effective, we most certainly can predict a programs output without actually running that program.

Though for some extremely high entropy highly optimized programs which generate extreme amounts of complex data (like video encoding and cryptanalysis resistant encryption) it is generally true that you cannot do better than to actually just run their source code.

IMHO Turning was a genius - because of his understanding that branching control flow was sufficient to move through process space, the church turning thesis and the nature of computation universality  were glorious but things like the halting problem - don't impress me.",hdb0fl6,t3_pqhh7v,1631948962.0,False
pqhh7v,"You've stumbled on something relatively common in impossibility results - the argument tends to create a very contrived object. This is just as true for Godel's theorems, where the statement that isn't provable or disprovable is some terrifyingly huge construction nobody would really care about. It crops up in cryptography, where the initial proof of vulnerability of a class of protocols is something nobody would actually design.

You can look at the halting problem as the first step of an exploration into uncomputable functions, rather than a finished result. We have proven that it is impossible to build a program that will tell you whether *any* program will halt - there is at least one that it will fail on. Now we need to move to the next step - can we build a halting decider for the programs that we actually care about, and shove these theoretical counterexamples into a corner?

In the case of halting and uncomputable functions, we have a pretty rich landscape of things that we know. We have a variety of prospective halting deciders that are quite dependent on the form of logic that we use to analyze programs. We've written programs whose halting behavior is dependent on unproven (and in some neat cases, unprovable from standard axioms) theorems. We know that for many bounded forms of computation (like linearly bounded automata) their halting problem is decidable. More broadly, we know a lot of things that are uncomputable. One quite annoying one is Kolmogorov complexity - the smallest compressed form of a string. This means our compression algorithms will always be heuristics that can only approach optimal compression.

Whether any of this really truly impacts practical software development is a little bit iffy. It definitely does if you are interested in automated code analysis, but for day-to-day work we tend to work in the loosely-defined space of programs that humans can comprehend, and the degree to which that set even has formal computability properties is kind of an open question.",hdrm3lp,t3_pqhh7v,1632258695.0,False
pqhh7v,"By the way, there is a form of the Halting problem proof that might be more to your liking, since it only refers to real objects.

Instead of proving ""there is no halting decider"", I will prove ""every program is not a halting decider"". There is no statement about hypothetical objects - I will only talk about actual programs that you can write down, and things they can or cannot do.

Consider a prospective halting decider H. Build the program

    def P {
      if H(<P>); LOOP
      else; HALT
    }

Since H is a real program, so is P. However, as you can see from its similarity to the example in the video, H will always be wrong about P - if H(<P>) is true, attempting to run P() will actually loop, and vice versa. So, P shows that there is at least one program every given prospective halting decider H will be wrong about.

These proofs are technically logically equivalent, but this one avoids talking about hypothetical objects that don't exist - it only talks about the somewhat strange program P that is contrived.",hdrmu5b,t1_hdrm3lp,1632259014.0,False
pqg2y4,Look in to iperf.,hdbuji0,t3_pqg2y4,1631971283.0,False
pqg2y4,Comes with graphes and statistics. Best. I think the gui version is jPerf.,hddhytu,t1_hdbuji0,1631996871.0,False
pqg2y4,My goto tool for this kind of thing is `socat`.,hdcwuoy,t3_pqg2y4,1631988214.0,False
pqg2y4,Not sure if possible on Mac but run a Kali Linux Vm and you have most of the pen testing tools all ready to go. If Low orbit ion cannon is still a thing there’s that too. Make sure you have permission and understand the scope of your permission,hdjxmkw,t3_pqg2y4,1632111394.0,False
pqffop,"I'd be pleased to hear from anyone if they have more up-to-date information, but as far as I'm aware there is no strong evidence in favour of any programming paradigm (on the grounds of reducing complexity or otherwise).

The area of research which looks into this sort of topic is *empirical software engineering*, which might help you narrow down any web or literature searches you do. 

There's a somewhat relevant [Software Engineering StackExchange](https://softwareengineering.stackexchange.com/questions/162789/empirical-evidence-for-choice-of-programming-paradigm-to-address-a-problem) question on this, and a [C2 wiki](http://wiki.c2.com/?OoEmpiricalEvidence) page it links to.

There are a few known *bad* papers comparing different software paradigms, that suffer from serious weaknesses in their methodology. Happy to dig some of these up if you're interested.

You may also be interested in the book [Leprechauns of Software Engineering](https://leanpub.com/leprechauns) by Laurence  Bossavit, which looks at myths found in software engineering, and the poor state of empirical research into many S.E. topics. (To be fair, it's a very difficult area to conduct research into; but that's no excuse not to think critically about it.)",hdamw9y,t3_pqffop,1631939137.0,False
pqffop,Excellent articles. I think a lot of the discrepancies are that we all might have different understandings of oop and the like.,hdf3288,t1_hdamw9y,1632021599.0,False
pqffop,I love it thanks,hdeu7on,t1_hdamw9y,1632017641.0,False
pqffop,"I can tell you from a programmers prospective, that compartmentalizing data into structures drastically reduces complexity when you are trying to read someone elses code. God.... What would I do if every single thing were procedural... I honestly just cant imagine. If I were trying to explain how structuring data changes the way its used. I would say, It drastically helps reduce the amount of lines of code you have to read on a single page. One of the biggest problems I faced when I started working for my company is that everything was just line by line by line by line by line by line by line for eternity, but after abstracting big chunks of code out into tools that can be reused, or chunking out big sections of code that only have use in once place, it makes the larger structure that you are working with more  easily managed/readable. Its the difference beteween reading 20\~ lines for a function vs 1k+ lines. The the code has all 1k+ lines in it either way, but, can I break that 1k lines up into 20\~ in a file, another 20\~ lines in a file, 100\~ lines in this file, 200\~ lines in this file. The whole idea is, can I break this IDEA down into smaller and smaller(if possible, reusable chunks that we turn into ""higher level"" functions).",hdapweh,t3_pqffop,1631941111.0,False
pqffop,What you're describing sounds like good practices in defining reusable functions more so than OOP specifically. There's a sliding continuum of hard-core OOP and just being organized with your code. I find many OOP specific coding patterns to make code more complicated (e.g. manager and factory classes).,hdazs51,t1_hdapweh,1631948436.0,False
pqffop,"OOP it's a tool, if you suck at using it it's not the fault of the tool in most of the cases",hdbwacf,t1_hdazs51,1631972181.0,False
pqffop,"In most cases!  Then there is Java.

I used to make everything into a class, but fuck that noise.",hdgdjfn,t1_hdbwacf,1632053787.0,False
pqffop,"I totally agree, but since I want to make some arguments in my essay about explanatory power of theories based on the premise of OOP reducing perceived complexity, I cant really cite your reddit post, as much as I wish I could though :)",hdau9vv,t1_hdapweh,1631944157.0,True
pqffop,"My take is that procedural is like building a factory where each line is duplicative in its component parts but outputs slightly different products at the end, whereas oop is like finding commonalities between the lines, combining like features then creating a complex framework that enhances the efficiency of the factory overall. 

The first one is straightforward and gets the products built, but the second one allows for scaling and additional modules to enhance or change the output. 

I think correct oop is terrifyingly difficult to build on a large scale but it is incredibly efficient and allows rapid expansion once it’s understood and implemented correctly. But also hard to consider the consequence of every change.",hdf3to2,t1_hdapweh,1632021954.0,False
pqffop,There's nothing specifically object-oriented about factoring out commonly-used steps in a process. Subroutines have been around much longer than objects.,hdm2beo,t1_hdf3to2,1632157855.0,False
pqffop,"I sat in on a lecture one time that has a profound effect on how I view programming language semantics and my personal development philosophy. Basically the summation of the lecture was every change in a software system increases entropy in that system. We all know this to be true when looking at the end users requirements, but we never evaluate, and sometimes we are not even aware of the requirements we introduce into the system as developers. One of the bigger examples that was used was the object-relational impedance mismatch. Which OO introduces, the mismatch is where the entropy manifested and because we as humans tend to grasp graph based data structures as easier to visualize in our mind. Thus we introduce concepts like ORM's to deal with that entropy, but many of us find the ORM becomes a beast of its own, most will overlook the fact that developer driven requirements introduced the complexity that manifested due to this.

Some will elect to throw out relational data structures all together, which proved to not work well as for most data to be of it's most value it must conform to first order logic and relational theory.  So what if we go the other way abandon graph based aesthetics in code and use first order logic. It's been done and it produces extremely difficult languages to reason about, ProLog would be one of them, and it is in my opinion very difficult to write a comprehensive system in it, and extremely time consuming to do so, but it produces far more accurate and predictable software (in my opinion again). Anyways to me, it's about finding a balance, OO reduces reasoning complexity but increases system complexity, but at the end of the day, we do have to work on these systems thus need to be able to reason about and visualize them in our mind. They key is knowing when you are making those tradeoffs.",hdbw1d3,t3_pqffop,1631972059.0,False
pqffop,"Thank you so much for the elaboration! Do you have - by any chance - any sources or key words I can google for to find citeable sources for the two statements below? (they totally make sense in the way you explain them, but quoting reddit is not really a thing at my university :D )

>and because we as humans tend to grasp graph based data structures as easier to visualize in our mind  
>  
>OO reduces reasoning complexity but increases system complexity",hdgbrf3,t1_hdbw1d3,1632052676.0,True
pqffop,"It has been a long time since I have been in acidemia thus do not have access for a lot of scholarly material, but you could start at wikipedia, which many times, will have white paper references, I would start with:

[https://en.wikipedia.org/wiki/Software\_entropy](https://en.wikipedia.org/wiki/Software_entropy)

[https://en.wikipedia.org/wiki/Algorithmic\_information\_theory](https://en.wikipedia.org/wiki/Algorithmic_information_theory)

[https://en.wikipedia.org/wiki/Object%E2%80%93relational\_impedance\_mismatch](https://en.wikipedia.org/wiki/Object%E2%80%93relational_impedance_mismatch)

Obviously you cannot cite them, but they may take you down the rabbit hole to some white papers that will bare fruit.

Also when I use the term OO, I use it loosely as the concept of representing data as graph based objects, technically interfaces and structs meet this definition, items like inheritance and polymorphism, etc. are questionable and really subjective as to how the particular developer reasons as to whether they reduce or increase reasoning complexity, but as a general rule of thumb, it seems that humans reason about groups of variables/data in a graph based structure.  I will see if I can dig up some material on this, but keywords you will want to search for are I/O psychology and programming language semantics / aesthetics / reasoning / complexity.",hdigat3,t1_hdgbrf3,1632085914.0,False
pqffop,"Personally I would be surprised if such a paper existed, for the simple reason that OOP is a tool.

Tools have their domains in which they work best. So for some class of problems OOP may be a better tool than (say) [declarative programming](https://en.wikipedia.org/wiki/Declarative_programming). (And in some domains we may not even know how to solve the problem using declarative programming--like, for example, building a functional user interface.)

But then, in other domains (like building a parser), OOP may not be the best tool for the job--at which point you want a declarative language, like YACC, to do the job.

And even other domains where a mix of tools is best--like building the user interface layout using a declarative system (say, in XML, like for Android) or even graphically (say, in Xcode for iOS), then building the functional aspects using OOP.

----

It's why I bristle when people talk about how their tool or their paradigm or their style trumps all else. It's like saying my hammer is superior to all the other tools in your tool box.

Best instead to learn the other tools (screwdrivers, chisels, wrenches, etc), learn when those tools are appropriate (like using a screwdriver rather than a hammer to drive in a screw), learning sometimes you need to make exceptions to these rules (like using a hammer to tap a screw in a tiny bit first into a wooden surface so you can drive it with a screw driver more easily without the tip slipping), and become a journeyman at using the full array of tools at your disposal.",hdbyn0g,t3_pqffop,1631973372.0,False
pqffop,"There are hundreds of them, all written in the 90's. Since then, not that much...",hddd68q,t1_hdbyn0g,1631994926.0,False
pqffop,"Don't you realise how personal that is? If you agree that no programming paradigm is ""unnatural"" and ""not suited for the human mind"", then whichever you choose to work with gets easier for you with time. In other words, I don't think there can be any such study, because the question doesn't make sense.",hdaulrc,t3_pqffop,1631944403.0,False
pqffop,"You write a function two ways. Ten people read function A, ten read function B. 8/10 people who can read function A can describe what it does, 4/10 people who read function B can describe what it does.

If speed is not a factor, and it often isn't because computing power is cheaper than developer time, then function A is indeed superior to function B and it isn't ""personal."" Now expand this concept to modules and entire programs.",hdbolcg,t1_hdaulrc,1631968002.0,False
pqffop,"Good luck escaping the bias for OOP due to its abundance in education and industry.

Edit: in other words, get a person to work with logical programming exclusively and see which code they understand. It changes with time and practice. You get better at each paradigm when you work on it and it becomes ""better"" for you. You can use your methodology to test if people play basketball or baseball better, it wouldn't mean anything.",hdc2fzk,t1_hdbolcg,1631975230.0,False
pqffop,"1. If basketball or baseball were activities that were used to accomplish some specific task, then your analogy would make sense, but they aren't, so it doesn't. Coding is not a skill in and of itself, you use to to \*do\* things.
2. If there is a ""bias"" towards OOP in the industry, that's likely because it is indeed an objectively superior choice for most tasks. This bias happened organically, it was not imposed by a regulation or some shadowy group. 

I'm glad you highlighted the ""bias"" as you put it towards OOP. You might also call that ""OOP being more popular."" Now why would OOP be more popular?",hdcatik,t1_hdc2fzk,1631979028.0,False
pqffop,I cant see where anything you say proofs the point you are trying to make. There can be studies that show that a hammer is a more suitable tool to build a shelf than a dead goldfish. Why would this be impossible for programming paradigms? This is the point you'd need to provide evidence for.,hday02s,t1_hdaulrc,1631947034.0,True
pqffop,"You are talking about which programming paradigm is easier to write programs. Not certain kinds of programs, but programs in general. So a more suitable analogy is ""what is a suitable size and color for a hammer to do general carpentry?"". That changes from task to task and personal preference.",hdb75tk,t1_hday02s,1631954672.0,False
pqffop,I think its a person by person basis. OOP makes it easier to work as a team since the kind of compartmentalization is enforced.,hdb8umx,t3_pqffop,1631956132.0,False
pqffop,"I'm a big fanboy of Lisp style OOP, specifically Objective C.

The strength of Objective C is that its easier to work with Apple's API. Much easier.

In fact, Objective C checks of objects existence at runtime for that reason (it enables your code to work with others' code better).

As far as making YOUR part easier? Nah. It doesn't.

But once you learn how Apple's UI kit and Sprite Kit works, its a lot easier to work with the GUI.

I am 38 and learned programming when I was 14, then gave it up for a bit. As a result, I learned Mac proramming when it was the Toolbox.

You had to create an event loop that handled... events and such. There were also handles (pointers to pointers) that were a big deal.

When I re learned Mac programming to create an app, it was significantly easier.

The main problem to me when I learned it is that it appeared to do too much for me.

You just override a class method (not declare you own) in the GUI to do what you want. Theres one for each event, like mouse up, mouse down, key down, etc. 

That being said, if you were a pure C programmer, eventually you'd create enough of your own code that when you went to start a new project, you'd just copy and paste your empty GUI handling code.",hdbqctx,t3_pqffop,1631969019.0,False
pqffop,I’ve never done old school Mac programming but I think that quirky model with event loops and handles was a symptom of the OS’s lack of pre-emptive multitasking.,hdcugtp,t1_hdbqctx,1631987228.0,False
pqffop,"Procedural programming it's eventually going to end up in at least the use of ADT if you don't want to get crazy

Like for a videogame

Try to handle a car without objects or ADT

to not talk about the fact that every time you are going to end up having to insert the parameter into the ADT's function, that's frustrating from a syntactical perspective

As programmer was taught to me to approach problems with a ""divide et impera"" thinking, that's what OOP allows even more, even the checks you can do when the language allows it via get and set (like in C# ) are awesome

Just look how awful the getter and setter functions in java and C++ look mixed in with other 100 different other functions


Edit: basically i think there is not that need to think about it, it might not be the best way to use a divide et impera approach but it is sure better",hdbvp6o,t3_pqffop,1631971888.0,False
pqffop,"What do you mean by awful ? 

In Java its just lombok + @Getter @Setter",hdbxztt,t1_hdbvp6o,1631973045.0,False
pqffop,"Last time i checked in java you had to use functions in order to make getters and setters, in C# you have dedicated keywords for it, it allows really quick and simple getters and setters to be made",hdby6nh,t1_hdbxztt,1631973141.0,False
pqenvl,">there's no actual way to create a random number using technology alone

There's no actual way to create a random number using **algorithms** alone

>There are so many places that ""random numbers"" are used in code, and as user privacy becomes more of a societal issue

Are you talking about encryption here? If yes, then take those quotes away. When I'm generating random numbers in such situation program takes uses external factors as the source of randomness, e.g. mouse movements.

> it makes sense that true random number generators are going to come about

If you think we will have algorithms which can produce random numbers, then you are wrong - deterministic system is unable to produce random outcome.  
If you are talking about generators, which output numbers based on physical phenomena (like e.g. atmospheric noise) then... we already have those.

>My thing is, why do we always say, ""random number"" when it never actually is, 

Not always =/= never. If pseudo-random numbers suffice, we use them; if true randomness is required, the true random numbers are used

>do you think we will get to a point where what we now call ""random numbers"" will become numbers through function as they currently are

_Pseudo-random numbers_ are called _random_ as mental shortcut, not because we pretend those are really random

>and we will have actual randomness based on physical phenomenon

We already have those!",hdajqah,t3_pqenvl,1631937179.0,False
pqenvl,"I understand we already have those, I'm talking about being a student in computer science, and the common words we use which are fundamentally wrong.",hdampaz,t1_hdajqah,1631939016.0,True
pqenvl,"Meaning, commonly we say ""random"" numbers when it's not, and asking if in the future people think we will stop saying random when we actually mean something different. I appreciate and agree with each of the points you made, rather I'm expanding on that assuming what you said is common knowledge in this community.",hdamviy,t1_hdampaz,1631939124.0,True
pqenvl,"Again, I completely understand random usually means pseudo random, but I'm talking about in this field people say ""random"" when they mean ""pseudo random"", that's my whole point. Why is that? Other than convenience it makes no sense and misleads people as to what the function is actually doing and creates confusion on the development of actually creating a widely used true randomization system.",hdanbu6,t1_hdamviy,1631939415.0,True
pqenvl,"So your issue is fundamentally one of language use: or rather, the imprecise way people use language. Well, guess what? Unless you can find a way to force everybody in the world to somehow converge on the same exact definitions for all words, this will be a pain point for you over and over. Or maybe you can tweak your expectations a bit.",hdb5tpl,t1_hdanbu6,1631953526.0,False
pqenvl,">there's no actual way to create a random number using  
>  
>algorithms alone

Am I wrong or does technology only work through algorithms? Is there a way to use binary to create a truly random number?",hdantu5,t1_hdajqah,1631939736.0,True
pqenvl,"> Am I wrong or does technology only work through algorithms?

Technology works purely through algorithms only in two cases:
  
  1. You define technology as being only computer related and ignore all technology which isn't connected to IT
  2. You believe that the whole universe is one big algorithm (but then there is no such thing like randomness at all tho)

>Is there a way to use binary to create a truly random number?

What do you mean by ""binary"" here?",hdbqado,t1_hdantu5,1631968981.0,False
pqenvl,"Technology can be random if you don't understand what's happening from a physics and logic perspective, but other than that in order for it to be functional you *have* to know how it's doing what it does in order for that to be executed.",hdao4b5,t1_hdantu5,1631939928.0,True
pqenvl,"Your computer generates\* countless sources of randomness every moment. We just need to harvest that randomness.

\*Computer Engineers would love to be able to eliminate them, but it's physically impossible.",hdbr30w,t1_hdao4b5,1631969420.0,False
pqenvl,">Are you talking about encryption here? If yes, then take those quotes away. When I'm generating random numbers in such situation program takes uses external factors as the source of randomness, e.g. mouse movements.

I'm talking about everything from encryption to online gambling. Also, there are still ways to attack a program which uses any sort of hardware/software variable which is itself defined and has a reversible input and output, which in my other point I mentioned by current understanding cannot be random.",hdaotcz,t1_hdajqah,1631940380.0,True
pqenvl,"Yes. Mental shortcuts are helpful and used in every field of study. Again. My point is do you think that the common term will change as physical phenomenon or other true random values become more wide-spread. Yes. There are true random number generators used today (my mind always goes to the lava lamp example of choosing the random variable based on the bubble size/timing), but give me an example of using that based on my pc. Give me an example that is wide-spread and useable by every developer and basic user, which can also be broken down using binary logic.",hdapu1z,t1_hdajqah,1631941068.0,True
pqenvl,"modern x86 cpus have 2 instructions that produce random numbers rdrand and rdseed. these instructions are based on ""entropy gathering circuits"" that produce a random number based on unpredictable thermal noise. This was implemented in Ivy bridge btw. rdrand is considered cryptographically secure while rdseed is ""non-deterministic"" which is what most people mean by random. 

here's a link for more information 

https://software.intel.com/content/www/us/en/develop/articles/intel-digital-random-number-generator-drng-software-implementation-guide.html",hdb9kli,t3_pqenvl,1631956764.0,False
pqenvl,"A lot of the time we use pseudo-random numbers to give predictable but random enough outputs. You give a random number generator a seed and it spits out a number, but if you give it that same seed again it's going to give you the same output. One of the best examples is how Minecraft worlds are randomly generated but if you use someone else's seed, you end up with the same world. Usually this is enough for what we want. 

To generate truly random numbers, you need a truly random seed. Radioactive decay is truly random, so you can collect data from it and use it as a seed. You could also do things like using the system clock as a seed, because it can be irregular but the point is you need a random seed for a random number.",hdb8b4n,t3_pqenvl,1631955666.0,False
pqenvl,"When we talk about random numbers being used in algorithms or mathematical functions we don't care about the implementation or source of the random numbers, we only care about the concept of randomness. Although if the topic of your study is randomness then you would talk about both the conceptual randomness and measures of randomness from various sources.

It's good to think about concepts and apply your knowledge of their implementation but getting caught up in the details will sometimes make learning and talking about ideas more difficult.",hdeswmo,t3_pqenvl,1632017076.0,False
pqenvl,"We use the simple term ""Random Number"" so the student, newbie, or layman quickly understands the intent of creating an unpredictable number. Many algorithms produce a suitably ""random enough"" number that may be used for example to create mock reports from static data, simulate a roulette wheel or dice thrown in a game, etc. For protecting sensitive information through encryption, however, a far more complex process or even use of a 'believed to be random"" external input may be desired. It's not ""incorrect"" to use the simpler expression, but in serious systems anything claiming to be random is typically subject to great scrutiny before put into use.",hdi0eaf,t3_pqenvl,1632079510.0,False
pqdhk9,"So you have to understand the inputs and outputs of each process.

 For example, you need data from company A. How do you get that data? Is it an API call based on member id? Is it a CSV file with data for all members sent daily? Do you need to integrate with company A in several ways?

Next, what is the desired output? Is it to display information, allow search, send a newsletter? Then depending on what the desired output/offering is you can choose an appropriate platform (website, backend Java application that sends emails, mobile app etc)

On you find the answers to these questions, you can do the programing required to automate going from company A output -> your final goal on a tech stack that integrates with both the input and output.",hdaka5p,t3_pqdhk9,1631937507.0,False
pqdhk9,Thank you so much! I can start searching the definitions and programs and frameworks that might help me for my questions because I had no idea where to start!,hdamhrn,t1_hdaka5p,1631938884.0,True
pqdhk9,"Short answer to your particular exampl is APIs

But if your interested in more broad terms how to get data from various source applications and into a data warehouse (ETL/ELT) look into data pipelines and the field of data engineering.",hdct22l,t3_pqdhk9,1631986653.0,False
pqdhk9,Thank you! Everyone here suggests API and It’s like a magic word or something that I never heard about lol! I am looking into it.,hdcu2e9,t1_hdct22l,1631987062.0,True
pqdhk9,"Okay, so... you really don't know enough to even ask a real question on the topic. Like, not to be rude, but you probably don't have all the required skills and knowledge to execute that project based on how you worded your post.

Just take a step back and focus on the basics.",hdadqv2,t3_pqdhk9,1631933873.0,False
pqdhk9,"I see and that’s not rude at all. Thank you, I’ll work towards it. I’m just curious and can’t find a way to understand it because I have no idea what I should search. I’ll stick to my basics for now",hdajqen,t1_hdadqv2,1631937181.0,True
pqdhk9,"Look into writing an api that queries a database you’re interested in. Just being able to go out and get the data and write it to a database will be enough to keep you occupied for months. And once you do that you’ll have the requisite skills to write another api that queries your database. There’s plenty of free, public, apis you can use to practice",hdap97r,t1_hdajqen,1631940671.0,False
pqdhk9,Thank you! That’ll definitely help me get closer to the topic!,hdarbku,t1_hdap97r,1631942070.0,True
pqdhk9,Absolutely. Feel free to dm me if you get stuck! Happy to help,hdardzr,t1_hdarbku,1631942117.0,False
ppozg9,[deleted],hd5dgzs,t3_ppozg9,1631839040.0,False
ppozg9,Great movie.,hd66fqq,t1_hd5dgzs,1631853622.0,False
ppozg9,What movie?,hd81o43,t1_hd66fqq,1631895505.0,False
ppozg9,Wargames.,hd8b7kb,t1_hd81o43,1631899494.0,False
ppozg9,A new science is born: Psychohistory,hd84p00,t3_ppozg9,1631896764.0,False
ppozg9,Upvote for foundation reference 😂,hd8mhwa,t1_hd84p00,1631904279.0,False
ppozg9,The USA is way behind China in the quest to get quantum computers.,hd6bnng,t3_ppozg9,1631857035.0,False
ppozg9,"In the hands of wise man, stick can do way more then gun in hand of child.

I am not saying USA is more wise compared to China. 

Just saying, technology level is not always crucial.",hd6dh32,t1_hd6bnng,1631858345.0,False
pphwcc,"I thing that in a world in which there is Linux Kernel, there are no reasons to begin from scratch making an operating system: 

1. Because you can use Linux Kernel and other things that make an OS to make your OS, 
2. Because you can start from scratch using “Linux from scratch” book, 
3. Because Linux is free and open source and is modern enough to fit the needs of every hardware, 
4. There are also Mac OS (not for all, I’m agree, but only for money side), 
5. I don’t think windows is so bad, I mean, for every day works is good enough;",hd3sh5t,t3_pphwcc,1631814213.0,False
pphwcc,Thank you for answering!,hd3y9r0,t1_hd3sh5t,1631816558.0,True
pphwcc,you are welcome :),hd3yfs4,t1_hd3y9r0,1631816625.0,False
pphwcc,I mean isn't Mac OS Unix Based?,hd4gg7g,t1_hd3sh5t,1631823962.0,False
pphwcc,"macOS is actual, trademarked, UNIX.",hd4yff6,t1_hd4gg7g,1631831764.0,False
pphwcc,It meets more of the POSIX requirements than Linux or BSD,hd4io0b,t1_hd4gg7g,1631824868.0,False
pphwcc,Based on BSD.  I think Unix is more of a philosophy than an OS these days.,hd5gwv0,t1_hd4gg7g,1631840669.0,False
pphwcc,Unix is more a set of specifications now,hd7lham,t1_hd5gwv0,1631888731.0,False
pphwcc,"Right, that's a better description.",hd8rj4o,t1_hd7lham,1631906438.0,False
pphwcc,"The Unix philosophy is still good to have.

Do One Thing And Do It Well.
https://en.m.wikipedia.org/wiki/Unix_philosophy",hd8uycv,t1_hd8rj4o,1631907901.0,False
pphwcc,"I mostly agree, but I think that ideology has been  used overzealously by the ideologues as far as Linux is concerned.  I like my command line utilities to do one thing well, but the Unix philosophy has led to some crazy patchwork solutions with a lot of technical debt like x11 and the plethora of init systems in order to support leagues of software and user choice.

This is to say I think systemd and Wayland are huge steps in the right direction for modern systems.  The direction systemd is heading is totally away from the Unix philosophy.  Wayland too depending on how generally you interpret ""do one thing and do it well"".  Hell, if you're super strict, anything other than a microkernel isn't following Unix philosophy.

I mean, if anything I think it should apply more to our functions and classes than entire applications.

[Edit]
I don't know why I told you all this but, uh... There ya go.",hd8xi75,t1_hd8uycv,1631908972.0,False
pphwcc,"**[Unix philosophy](https://en.m.wikipedia.org/wiki/Unix_philosophy)** 
 
 >The Unix philosophy, originated by Ken Thompson, is a set of cultural norms and philosophical approaches to minimalist, modular software development. It is based on the experience of leading developers of the Unix operating system. Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a ""software tools"" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software; these norms became as important and influential as the technology of Unix itself; this has been termed the ""Unix philosophy"".
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hd8uzvw,t1_hd8uycv,1631907919.0,False
pphwcc,"Yes, it is unix based, in fact I mean all operating systems, based on our specific needs.",hd4hgz4,t1_hd4gg7g,1631824379.0,False
pphwcc,"Microsoft tried in the early 2000's to build an OS from scratch with a different architecture and principles, the OS is called Singularity if you want to look it up.
They thought of productize this OS but the project was closed eventually.

I actually wrote a blog post about this OS, you can pm me I'll send you a link.

Another experimental OS is Determinator.",hd3yudq,t3_pphwcc,1631816789.0,False
pphwcc,I wanna write a sci-fi novel where there's underground agencies and hacker communities that use vaporware exclusively,hd4tc8w,t1_hd3yudq,1631829431.0,False
pphwcc,">I actually wrote a blog post about this OS, you can pm me I'll send you a link.

Can't you just share your blog post here?",hd5hh9d,t1_hd3yudq,1631840941.0,False
pphwcc,"sometimes people frown on self-promotion, they were trying to be polite I think",hd61tva,t1_hd5hh9d,1631850938.0,False
pphwcc,"I pmed you with the link.
I don't put it in this thread because I want to avoid self promotion here",hd68w0h,t1_hd5hh9d,1631855170.0,False
pphwcc,Really? Very interesting! But why they close the project? I will write to you for article.,hd3zfyk,t1_hd3yudq,1631817035.0,True
pphwcc,"I'm actually not sure why, I guess they achieved their proof of concept, and technically the project is still open source.
You might find more details about it here
 https://cs.stackexchange.com/questions/16844/microsoft-singularity-why-closed",hd40gsx,t1_hd3zfyk,1631817452.0,False
pphwcc,Thank you!),hd40v10,t1_hd40gsx,1631817614.0,True
pphwcc,"> Why there is no new OS that written from scratch?

There actually is one called [Google Fuchsia](https://en.wikipedia.org/wiki/Google_Fuchsia) which is currently being developed. It's now running their Nest product and it will likely end up replacing Linux/Android several years from now in their smartphones/chromebooks. They are also trying to make it binary compatible with Linux so I suspect they even want to replace Linux in servers in the long run. It's increasingly a Google controlled world.",hd4lq1o,t3_pphwcc,1631826134.0,False
pphwcc,I too heard about this. The very purpose of creating Flutter language is to write apps for that OS. Not just cross platform mobile apps.,hd5qch4,t1_hd4lq1o,1631845138.0,False
pphwcc,"> It's increasingly a Google controlled world.

My anti Google ass won't let that happen.",hd6fnn0,t1_hd4lq1o,1631860019.0,False
pphwcc,"New OSes are developed all the time, many of them do have some cool features that make them ""technically better"" than the competition. The problem is that OSes are _very_ sticky in that once an OS has established it's position in the market, it generally stays there. A rival OS would have to offer the end users a significant advantage over the established players to tempt people away - a small advantage probably isn't going to cut it.

Remember Windows phones? Microsoft is really good at designing OSes, and the actual OS of Windows Mobile was technically really good. It even had some neat features to ""outshine"" the competition. However, try as they might they just couldn't break through. This is an example of how it is _really difficult_ to make an OS and to win over developers, especially if the market has already established itself.

On the flip side of this, Android in particular was an OS in the right place ant the right time to take off. It managed to ride the smart device market and it managed to quickly establish itself partly because there weren't many competitors in that space at the time. Once Android had established itself with good developer support and crucially a larger user base, nothing was going to touch it.

Another key point in the question is developers. Unix is still popular in part because it is tried and tested, and there are developers who will ""instantly"" be able to use pretty much any Unix-like system. If you are making a new OS you can totally do your own thing and not be unlike everything else, but _every_ developer will have to scale a large learning curve to start writing code. Your OS might use a new approach which is technically better, but that doesn't matter if no one will use it. Time is money and if the same thing can be done on a Unix-like system quicker then they'll use Unix.

A good example of this is Plan 9. It has some really impressive OS features which are just different enough from ""actual"" Unix to confuse Unix developers. I would argue many of the concepts used in Plan 9 are better than old school Unix, but that doesn't matter when the end result is a program that would do the same thing.

Finally, if you're a company who wants to, say, host a website, are you going to go with Windows or Linux (or BSD if you like) which is well documented, well supported, well proven, pretty much guaranteed to work on your hardware and is the de facto option? Or are you going to use the new unproven 300450500350400550 OS™ which is up to 20% faster than the alternatives if used correctly, only supports intel processors from 2019 and none of your developers know how to maintain? Yeah, it's a no brainer for the company.

*tl;dr* - OSes are sticky. Once they are established they hang around for a long time and it's difficult for a newcomer to break through.

Edit: As pointed out if an OS does come up with something really cool, the competition will quickly implement something similar to compete. The Linux kernel is full of the ""best bits"" from other systems, further reducing the edge that a competitor might have.",hd4nw69,t3_pphwcc,1631827064.0,False
pphwcc,"> A good example of this is Plan 9.

And beyond to Inferno (1996). Plan 9 (1992) showed how to build a distributed OS. Inferno showed how to do it in a portable VM across microarchitectures. Among many other technologies. Engineers are still trying to shoehorn or replicate features of these OSs back into UNIX and Linux.",hd4zt82,t1_hd4nw69,1631832423.0,False
pphwcc,"You say old, but another way to see it is that they withstood the test of time. Windows provides the best portability across devices (and support for software) and Linux is the prime example of how collective cooperation is way better than profit-driven rivalry, as well as the cornerstone of stability. 

In terms of low level (kernel) features (e.g. file system, networking, process scheduling, multi-user handling), it simply makes no sense to rewrite. While it is conceivable for Windows to be scrapped for microsoft's new product, the GNU/Linux OS is unlikely to be replaced by anything new. The thing is, thousands of person-hours from some of the most brilliant coders have been allocated to their implementation and maintenance. Anything new would have to compete with that. And nowadays security is a huge implication, so something new would be way more vulnerable.

Though I would love a GNU/linux based OS in which the GUI of the programs would had OS-defined ""streams"" like stdin/stdout for controlling it through code. And also, the shell scripting language would had died with flames to be replaced by python syntax. Anyone with spare time up for the challenge??",hd4j6d3,t3_pphwcc,1631825080.0,False
pphwcc,"> The thing is, thousands of person-hours from some of the most brilliant coders have been allocated to their implementation and maintenance. Anything new would have to compete with that. 


Absolutely this. And it doesn't just apply to operating systems, or software in general. Often in life we are tempted to throw away an existing thing and replace it with something new, not realising that the existing thing has more value than the sum of its parts.",hd57jds,t1_hd4j6d3,1631836156.0,False
pphwcc,"> And also, the shell scripting language would had died with flames to be replaced by python syntax.

You can replace your shell.  Xonsh uses Python, I think.  Then it's just a matter of converting all of your scripts! :D",hd5hjyf,t1_hd4j6d3,1631840977.0,False
pphwcc,"The use a shebang at the top of your script pointing at you shell of choice 

#!/bin/bash",hd7mbcs,t1_hd5hjyf,1631889088.0,False
pphwcc,"There are experimental or research operating systems that have taken a more or less different approach, for example microkernel systems.

An important reason why new operating systems have not gained ground is inertia. In particular, if you tried to design a new operating system that doesn't provide an environment that's compatible with existing applications and operating systems, you'll miss out on having an application base for the operating system. That's going to be a showstopper for an OS becoming widely used in the vast, vast majority of cases.

You can only gain ground for a new operating system that's not at least somewhat compatible with existing ones if the new OS is part of a paradigm shift, or perhaps by providing a vastly superior experience to users. In both cases it still also requires good market sense and timing as well as luck.

Google was able to introduce Android as a new operating system [1], but largely because they did it on the emerging platform of smart phones. They did not replace an existing operating system on similar devices. Apple has been able to introduce incompatible changes in macOS, but that's partially because of their vertical integration, partially due to a dedicated following and successful branding, and partially because of providing a good enough (and well-marketed) product that people were willing to make the jump despite having to switch their applications.

In addition to applications, hardware compatibility is more or less tied to the operating system. Even if it's not tightly built into the core of the OS itself (as in Linux), the OS would still need to either support a vast variety of hardware by itself, or it'll need to attract hardware vendors to write drivers for the OS. The former is a formidable task for the OS developers, and the latter requires market share, which you generally don't have if you're starting out with a new OS. Or you'll need to be Apple who both write the OS and design and sell the hardware and select its components. If you're not controlling the entire technology stack from the hardware to the OS, it's massively difficult to build hardware compatibility that's anywhere close to what existing operating systems are able to provide.

On a related note, a clean slate operating system might be able to do some things better than existing systems simply by avoiding the baggage, but existing ones have solved lots of practical problems that a newly designed and written OS would need to solve again. [2] There's some merit to maturity even if it comes with not being able to be as flexible with trying new approaches.

That's not even getting into directly human factors such as people learning the new OS.


[1] Android has a Linux kernel, but the userspace is different from desktop or server Linux, making it essentially a different OS

[2] While not directly related to operating systems, Joel Spolsky's old blog post on rewriting software comes close: https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/",hd4rpd6,t3_pphwcc,1631828709.0,False
pphwcc,"Unix is pretty old now and obsolete. Although, there are many operating systems that have been inspired by the Unix operating system. We call these systems Unix like which are operating systems like the BSDs, MacOS, and Linux.

The Linux kernel can be thought of as a fine wine. It has evolved over the years with greater support for more hardware and more features. It’s easier to improve upon what you already have then to start a new operating system. Linux is a very different than it was when it was first release.

There have been a number of new operating systems over the years that have been built from scratch like ReactOS or Temple OS.

Edit: Also, there is a huge cost and development time to create a new OS. It difficult to gain market share with a new operating system. Applications would need to be ported to the new OS which takes a lot of time and resources which is not something that most developers will want to do.",hd3wtmq,t3_pphwcc,1631815976.0,False
pphwcc,"I think your last point is very important.  The amount of time and resources already devoted to the three main PC OSes, linux, macOS, and Windows, is huge - thousands of people working over decades.  There would need to be a very compelling reason to start a new OS from scratch, and it would be an astronomical investment.  However, in the case of mobile, there was a compelling case, and both Google and Apple have invested untold resources over the last decade building Android and iOS into mature platforms.",hd46yae,t1_hd3wtmq,1631820112.0,False
pphwcc,"MacOS is a certified UNIX. So no, UNIX is not obsolete. Especially considering iOS/macOS share the same core",hd4m762,t1_hd3wtmq,1631826334.0,False
pphwcc,"As are AIX and HP-UX. I don't know how widely those are used nowadays but I assume there are significant (probably corporate or other large organization) users considering that those systems are still in business.

The term ""unix-like"" is more commonly used for Linux and other systems that aren't certified UNIX.",hd4npy9,t1_hd4m762,1631826989.0,False
pphwcc,">As are AIX and HP-UX. I don't know how widely those are used nowadays but I assume there are significant (probably corporate or other large organization) users considering that those systems are still in business.

AIX is still being used. We have 50 or so machines running AIX on prem. Super stable. We had one AIX Box that hadn't been rebooted in 10 years. But expensive",hd5i7ln,t1_hd4npy9,1631841284.0,False
pphwcc,"Certified Unix is cool and all and I bet a few Linux and BSD distros would meet the criteria, but it costs money to be certified and you'd have to certify every version and architecture.  

It's a little silly to be honest, but from a business-standpoint, running legacy software, you'd likely want to be sure you get one of the 9? certified Unix systems.  Anyone not running legacy software dating back to the days of Unix probably wouldn't care.",hd5ixad,t1_hd4m762,1631841613.0,False
pphwcc,"FreeBSD absolutely a UNIX. Linux? Absolute not, it doesn’t fully the UNIX philosophy at all.

Yeah the paying for certification is stupid I admit, but that’s probably due to someone actually analyzing the structure of the OS, but in sure just having a community verify it would suffice (to offset the cost of someone going through the code).

And Cert is more so focused on the OS follows the UNIX philosophy rather than a some random spec. As UNIX now runs on many different kernels.",hd7ygbm,t1_hd5ixad,1631894155.0,False
pphwcc,"A few Linux distros might though.  When you talk about Linux, you're generalizing down to the kernel because that is what unites all Linux distros.  I wouldn't make the claim ""Linux? Absolutely not"" when there is at least one Linux distro certified Unix while FreeBSD isn't.

Truth be told, many Linux distros are only a few optional userspace packages away from it.

Also, I am pretty sure no one has to go through the code to be certified Unix.",hd80m2m,t1_hd7ygbm,1631895059.0,False
pphwcc,"GNU = GNU Is not UNIX.

Please know your history.

Just like WINE is
WINE is not an emulator.",hd8alvf,t1_hd80m2m,1631899244.0,False
pphwcc,"Whoa now, no reason to be rude.

Who said GNU/Linux?  

Next check out EulerOS, which is Unix certified and uses gnu c compiler.  K-UX as well.",hd8p55h,t1_hd8alvf,1631905411.0,False
pphwcc,"Alright, but you said Linux, if anything uses Linux or the GNU subsystems it’s not a UNIX.

So let’s not move the goal posts here.

And telling someone to check up on there history isn’t being rude, it’s being polite by filling in gaps in their knowledge.",hda8tot,t1_hd8p55h,1631931310.0,False
pphwcc,"You didn't fill in gaps in my knowledge.  You provided nothing of significance.  It was just rude.

You should learn what makes a Unix today.  You should probably learn the history.

Take note that both EulerOS and K-UX are Linux systems considered Unix systems.  Proof by contradiction that anything using Linux or GNU subsystem in not a Unix?

It is true: I did say Linux.  I did not say GNU/Linux--the two are not inseparable.  That's what you're not getting.  Additionally, you'll find that many Unix systems incorporate GNU in part or in whole.  

Shoot, I challenge you to find the Single Unix Spec that Linux doesn't adhere to other than the core utilities having extra functionalities.  Or you can keep pretending to know what Unix is based off the name of something unrelated--meanwhile you thought it required people to rummage through entire OS codebases (like that wouldn't be some massive undertaking) to get certified.

Even Dennis Ritchie thought Linux was a continuation of Unix and what made Unix great.  He would probably know better than you, wouldn't he?

Keep downvoting and keep your fundamental misunderstandings and mental fragility.  It was civil and delightful before you decided to be a condescending prick--I always wonder: what causes people like you to double down?",hdajhoh,t1_hda8tot,1631937037.0,False
pphwcc,"I didn’t read your post.

You were wrong, deal. It’s no big deal, calm down on the programmers ego. There is enough in the industry.",hdan7ts,t1_hdajhoh,1631939344.0,False
pphwcc,"I didn't read your post.

Take a look in the mirror.

Lol?",hdbi7rb,t1_hdan7ts,1631963869.0,False
pphwcc,Thank you for answering!,hd3y8cx,t1_hd3wtmq,1631816543.0,True
pphwcc,You’re welcome!,hd3zpor,t1_hd3y8cx,1631817147.0,False
pphwcc,Temple OS lol,hd6bmw6,t1_hd3wtmq,1631857021.0,False
pphwcc,I am not an expert but I think because the at the basic level they are still same as old computers. Like their fundamentals and way of working is still same.,hd4g61b,t3_pphwcc,1631823846.0,False
pphwcc,"There's nothing fundamentally wrong or limiting about current mainstream OSs. Small kernel, big kernel or medium kernel. What else are you gonna do? Any other innovations can be patched into current OS kernels and shells. Hardware is fundamentally the same as it was in the 90s, just incrementally improved. We're all still basically using Pentium IIIs on our desks and PowerPCs in our pockets. 

Even Google Fuchsia is not really a new OS. It's a new OS made using the same old ideas as before but with added syntactic sugar and compartmentalization in an attempt at freedom from Linux and stronger vendor lock-in. Been hearing about it for almost 4 years now and still haven't seen anything with real results. If Google with their huge brainpower, wallet and clout can't pull it off, forget about any smaller players. You could do new OSs 30 years ago but that ship has sailed and we're standing on the shoulders of too many giants to try and reinvent the wheel now. 

When we get quantum coprocessors, holographic displays, mind-machine interfaces and million-core processors, maybe there will be a need for a fundamentally new OS and one will arrive. 

What people often think of as the OS is the shell, i.e. the part that you interact with directly. If your question is why don't today's computers look like they're from the 'future', like in the movies, the answer is that all that razzle dazzle isn't easy on the eyes in a system you use for hours daily. We have more than enough graphics processing capability to make OSs that look super cool. Just look at any modern game. If anything, modern OSs attempt to look cleaner and simpler to be easier on the eyes. Look at the resurgence of dark mode, i.e. how computer interfaces looked 30 years ago.",hd5044y,t3_pphwcc,1631832565.0,False
pphwcc,"If you don't have a radical new idea, rewriting will get you to the same product that already exists. Even if it's better, there are so many costs to replacing os in prod, which means that even if the new os is better, nobody will adopt it. Most people don't have radical new and good ideas and thus keep improving existing products, which is also fine.",hd5utxr,t3_pphwcc,1631847362.0,False
pphwcc,"Fun fact: Android OS is only 13 years old (its birthday is in around a week actually), compared to 30 years for Linux, 35 for windows and 50 for Unix, and it is the [most used operating system in the world right now](https://en.wikipedia.org/wiki/Usage_share_of_operating_systems), owing to the huge number of phones in circulation.",hd4mr8i,t3_pphwcc,1631826571.0,False
pphwcc,"I guess because there are no new groundbreaking ideas in OS design.

Edit: ... That can not be added to existing architecture with a new version",hd4utkv,t3_pphwcc,1631830097.0,False
pphwcc,"There are tons (like Linux and FreeBSD)- but I guess they're all Unix-like, since unix is more like a choice of design principles nowadays than an actual OS.",hd4lu4w,t3_pphwcc,1631826181.0,False
pphwcc,There are look into plan 9 from bell labs and inferno,hd4ujb2,t3_pphwcc,1631829969.0,False
pphwcc,"Writing an OS from scratch frankly is hard. You have to write your kernel and scheduling algorithms and disk management and networking and stuff, then a nice UI to go with it, and probably more I'm forgetting. Why bother? We have the Linux kernel with plenty of distros and enough open source software behind it to make a good user experience, and your increase in computing power doesn't change much, since you want your OS as light as possible (the more my OS used the hardware, the less my applications get to). 

Your ideal OS is something that can run on a toaster and is friendly to use, so your Unix like operating systems (mainly Linux and Mac OS) and windows already do the job very well. There's just no point reinventing the wheel",hd51z0c,t3_pphwcc,1631833449.0,False
pphwcc,"form my humble point of view the tendency of things to stay the same in this sector is basically because no one wants to break existing code for the sake of new innovation without really being funded by someone how really want to monetize the output really big to accustom  for the expenses, which is basically prohibited by anyone who's sake is money. 

so basically i think money is the reason.",hd6egxy,t3_pphwcc,1631859093.0,False
pphwcc,"Because it’s still based on von Neumann architecture. There’s not really much to gain unless you have a super unique paradigm in how you organize files. The OS can’t improve beyond the con Neumann bottleneck so it’s not you have to revolutionize computational mathematics to build a new way to consume discrete math in a way that can communicate with what exists and allow multiple transactions in parallel somehow. 


Check out the von Neumann bottleneck to understand the limitation of general computers.",hd7wpd9,t3_pphwcc,1631893433.0,False
pphwcc,I have wondered the EXACT SAME THING!!! this drove me to form a group of people who share the same thought and are looking to build the very OS your asking for. Anyone who would like an invite to this group please feel free to message me and I'll get you an invite link.,hd4jnv9,t3_pphwcc,1631825278.0,False
pphwcc,But what exactly do you want to improve? How do you want to compete with the maturity and great hardware compatibility of for example GNU/Linux or Windows that have thousands or even millions of hours being put into by many many developers?,hd4upun,t1_hd4jnv9,1631830050.0,False
pphwcc,"There is an attitude that ""old"" == bad. It's a right of passage for any young engineer to look at something and think ""I could do better!"" The mark of an experienced engineer is one who can see the man-hours put into getting where we are and be thankful they don't have to burn them for themselves.

It's like keyboards. We have decades of people trying to re-invent them, yet they persist on mostly as they have been. A lot of that is because they meet the need at a reasonable cost in terms of design, purchase, and training to use, and efficiency to input with. Other designs are ""better"" in various ways but haven't been better in enough ways to take over.

It's going to be the same for almost all hobby OS projects. They'll meet some narrow design criteria to be ""better"" in initially, but then feature creep sets in and before anyone can actually use them to do what they need to do, they will have re-built most of what the existing OSs have already been doing.",hd5104s,t1_hd4upun,1631832987.0,False
pphwcc,"Variants are built all the time to serve special purposes, but you have to understand that building a new OS not based on unix or windows, is a herculean task that no one needs to take on, so no one does.",hd4lrt0,t3_pphwcc,1631826154.0,False
pphwcc,"The fact that they they are older also means that they are probably a lot more efficient. If you look at current technologies they are often wasting performance/memory for simplicity.

So maybe beeing older is not even a bad thing. Also they matured a lot over time. It's not that easy to build a new operation system from scratch without many bugs and compatibility issues.",hd4txb9,t3_pphwcc,1631829693.0,False
pphwcc,"There are actually several new OS's being written from scratch. Most of them are unix-like and pretty much just kernels at this point. There's fuschia, redox, and more that I can't think of off the top of my head.",hd56x55,t3_pphwcc,1631835856.0,False
pphwcc,"Software don’t magically become better just because they are new. Radical new changes are often caused by some huge improvements that are completely incompatible with the old software. However one of the purpose of mainstream OS is to provide an abstraction that are broad enough to allow almost all the new technology. The concept of driver provide a layer of abstraction for hardware, and OS itself provides another layer of abstraction for the apps, which is really just UI, or input/output broadly speaking. The model is so powerful that it’s probably difficult to have a piece of hardware that doesn’t fit this model. 
Also mainstream os also go through huge code change on every new version release, not just adding new code, but also deleting obsolete old code. So they are technically new. There are many old components in windows. They are still there not because no one delete them, but because someone is still using them.",hd64mmd,t3_pphwcc,1631852523.0,False
pphwcc,"The problem is not creating a new OS, the problem is the applications. Creating a new OS means you need to either create all the applications again or import them from another OS which probably needs another layer of software to translate the os calls of the original OS to the new OS and this lower the performance. Always remember what killed the windows phone/mobile was only lack of applications.",hd6i6x4,t3_pphwcc,1631862052.0,False
pphwcc,"For the same reason why Walmart is still kicking around, Jeff Bezos might change that but… I guess that’s what it’s all about.

There’s no new os because IT hasn’t HAPPENED YET. By it I mean the big technology revolution that changes everything forever.",hd6qe7s,t3_pphwcc,1631869595.0,False
pphwcc,"There have been many but there is currently very little user demand for one.  Most are made by students to see what they can do.

The reason for low user demand is Unix and Linux are still being developed with new features and functionality.

The reason most fail to make a new os from scratch is how much would need to be rewritten.  From network stack to drivers to process management.  And this is expensive in time and people.",hd7kys2,t3_pphwcc,1631888508.0,False
ppcsj7,"Terminator. That said, I've always preferred a different separator: parentheses.",hd2rt8s,t3_ppcsj7,1631799189.0,False
ppcsj7,I think this question is just asking how did the first language you learn/one you use most use a semicolon. I learned C and Java first so that should give you the answer.,hd2vikh,t3_ppcsj7,1631800858.0,False
ppcsj7,"Separator. ML gets it right; it mirrors the natural language use more closely. The terminator should be ""."" like in prolog. That said, it doesn't matter.",hd3pd8v,t3_ppcsj7,1631812952.0,False
ppa590,"How SHA-256 works isn’t ‘hidden’ you can find perfectly legal, freely available, source code for it all over the internet",hd265zi,t3_ppa590,1631785744.0,False
ppa590,yes.,hd265o0,t3_ppa590,1631785736.0,False
ppa590,Kerckhoffs' Principle states that the security of a cryptosystem must lie in the choice of its keys only; everything else (including the algorithm itself) should be considered public knowledge.,hd2883r,t3_ppa590,1631787548.0,False
ppa590,https://en.wikipedia.org/wiki/SHA-2#Pseudocode,hd36pyq,t3_ppa590,1631805500.0,False
ppa590,"AFAIK the SHA algorithm was a public contest like with AES, jpeg, ... and therefore it is completely public.

But I guess wikipedia knows best.",hd3ibgk,t3_ppa590,1631810151.0,False
pp6jgs,"Dynamic vs static typing can easily turn into flame war...

When your codebase is small, it doesn't matter too much, but when it's bigger, with multiple people working on, then static typing is a no-brainer.

When I was much younger, and just starting out, I often gravitated towards dynamically typed languages, now, I don't go anywhere near them.

I can see the appeal, they *look* easier, but in reality, are probably harder.

I think if we look at more recently created languages like Swift or Kotlin, they are statically typed, but *look* dynamically typed, that seems to be the way the industry is going right now.

Dynamic typing is one of the reasons Python is slower, but it's really the lack of a JIT that does the real damage.",hd1me0u,t3_pp6jgs,1631769018.0,False
pp6jgs,I just see them as hiding information. Data is almost always supposed to be a type and with typed languages you just keep track in code of that. Being more explicit usually makes things more readable and unlike boilerplate types don't really add bulk.,hd1ten0,t1_hd1me0u,1631774105.0,False
pp6jgs,what is JIT?,hd2hz3k,t1_hd1me0u,1631794147.0,False
pp6jgs,"""just in time"" compiler",hd2xq50,t1_hd2hz3k,1631801801.0,False
pp6jgs,"Jit is like interpreted langues who store like binary chunks make it faster each time you use it, whituot a big ovhead the first time you compile it",hd8vegd,t1_hd2hz3k,1631908088.0,False
pp6jgs,"You can give python type hints though like in function arguments:

Say you have a function that will accept a name and age and then return the name of the oldest person:

def funct(name : str, age: int) -> str:

In this case you are telling python that the name argument will be a string, the age argument will be an integer and the output of a function will be a string.",hd3onis,t1_hd1me0u,1631812670.0,False
pp6jgs,"Correction: you’re telling MyPy or other tooling that. The Python interpreter doesn’t care, doesn’t check, and doesn’t use the type hints to optimize the code at all. It’s just window dressing for linters.",hd3z4hh,t1_hd3onis,1631816903.0,False
pp6jgs,"Ahh. I didnt catch that in the documentation, that makes sense though",hd3zuau,t1_hd3z4hh,1631817199.0,False
pp6jgs,"An argument in favor of dynamic type languages (though, I am not in favor of one over the other in all cases) :

**The duck test:**

>If it looks like a duck, and quacks like a duck, then it is a duck.

In dynamic type languages you can use objects by knowing only a part of their attributes, while disregarding their class (unless you would like to check). If the object you are using has the appropriate member, then the rest can be literal dragons for all client-code cares. This is somewhat similar to Java's Interfaces, but way more powerful.

The fact that complete knowledge of the type of an object is not a prerequisite means that you can design more flexible APIs that play nicely with third parties. This is a huge plus for adding to open source libraries, that's mostly why languages like Python and JavaScript are so popular.

Case point: Think of Python's `numpy` library for arrays, on top of which Google's `Tensorflow` and Facebook's `PyTorch` machine learning libraries are built, as well as the CSV handling library `pandas`. Now imagine having all of those teams needing to include each other's header files and static types. You would never even manage to install them.

Some minor points:

* Conventions over rules: Dynamic type languages rely on conventions to make things work (e.g. you expect a collection to have the language's form of `.length` attribute). This is somewhat stronger because you are free to break them and/or introduce new ones when you really need to.
* The real issue is rarely about primitive types. A decent IDE will mark those. The real problems arise when you have no idea what kind of objects you are using. Especially in generic collections.
* With the wide adoption of Docstrings/javaDocs etc and proper documentation in industry code, ""type hints"" can be given in dynamic typed languages so their problems are becoming much less of an issue.
* I would say that dynamic typed languages have a steeper learning curve for newcomers, as they need to learn the conventions along with the language.
* Stability vs ""API power"" is a tradeoff: I work in Academia and I need to use experimental libraries, tweak their source code or build on them. I would instantly quit if I had to even type the name of every class that I am using. On the contrary, I would never even dare to dream of building an Android app without static typing.",hd1wyjp,t3_pp6jgs,1631777099.0,False
pp6jgs,"Rich Hickey, inventor of the Clojure language, describes his justification for dynamic typing in [this video](https://www.youtube.com/watch?v=2V1FtfBDsLU) (most of his videos are pretty insightful and entertaining). TLDR: the world and other people's code are too complicated and fast-changing to be captured by type systems, and typed languages spend a lot of time converting from one type to another instead of actually doing real work.",hd1t3qd,t3_pp6jgs,1631773866.0,False
pp6jgs,"One benefit of dynamically typed languages is it enables type polymorphism without the learning curve. Languages like C++ try to give people templates and Java gives you interfaces, but its much easier to just think ""x + y"" and have the language figure out what that means when it encounters a float, or an integer, or a pair of strings. You know what it means, the language should, too!

That said, to me the best way to deal with types is something more in the ML/Haskell family, and I don't think Java and C++ are terribly good examples of type polymorphism done right, even if you're not optimizing for ease-of-use.",hd1q2wa,t3_pp6jgs,1631771582.0,False
pp6jgs,Unless I'm missing something isn't that just operator overloading? I.e. syntactic sugar for calling specific methods,hd4a38k,t1_hd1q2wa,1631821379.0,False
pp6jgs,Sort of? Operators are functions and an operator that works over many different data types would have type polymorphism.,hd4aw8x,t1_hd4a38k,1631821702.0,False
pp6jgs,"The prevalence of dynamically typed languages can be explained because the vast majority of professional software engineering these days is in support of building web applications.

Lets say you're writing the server-side of some big web application.  You probably aren't storing objects in your database, you're using SQL, and as a result have to pack and unpack your objects into Strings, ints, Dates, etc. on the way in and out of the database.  The structures your code emits are all text in the form of html, css, javascript, and json, to be interpreted by the browser on the receiving end.  You are casting everything you work with constantly to make the type-checking compiler happy, and in the end, you still need to defensively code against type-coercion problems so you aren't seeing the benefit of the type-checking compiler proving some level of correctness... so the only benefit you can claim is 'speed'.

But in web development, we don't scale the app by making each request/response cycle faster, we scale the app by making more parallel request/response cycles happen at the same time.  We do this first by launching many application processes, then by launching many machines and load balancing between them.  We get speed through horizontal scalability, and eventually the bottleneck is the database (which is supported by a vendor, and written in, you guessed it, a type-safe language).

The cost of the compute time is dwarfed by the cost of the army of developers... so instead of optimizing for the cost of the compute cycles, we optimize for the cost of the developers.  This means ""lets make cheaper developers more productive"".  That again trends toward dynamic languages.

Add to that the prevalence of Javascript.  No matter what you do, you aren't going to get away from the need to use dynamic languages when building web applications.

I still don't understand how Python became the go-to language for data science... probably because it's easy to learn and has great support for fast C++ libraries.  I wish Ruby had won in this space.",hd3gzbd,t3_pp6jgs,1631809616.0,False
pp6jgs,"python isnt slow because its dynamically typed, its slow because its interpreted not compiled and because everything is a name of a reference to reference to the actual data. Usually people prefer these languages because the dynamic typing allows flexibility in the code logic. In python you can use a number as a boolean or even a boolean as a number. additionally it makes certain code simpler. python does not care if you are multiplying 2 ints together vs multiplying 2 floats or even multiplying a string by an int. this makes any function that takes 2 ints work with 2 floats so you dont have to write it twice.",hd1ksza,t3_pp6jgs,1631767969.0,False
pp6jgs,You're mixing up strict vs loose typing with dynamic vs static typing.,hd1tfqq,t1_hd1ksza,1631774129.0,False
pp6jgs,oh shit youre totally right,hd23l0b,t1_hd1tfqq,1631783343.0,False
pp6jgs,"Though C++ can convert bool to int and back as well. I feel like being able to multiply objects like that would cause even more issues when debugging, as it would not yell at you if you accidentally multiply the wrong variables. This is probably just personal preference though, and my static typed programming background speaking.

I think I watched a video a while back though that showcased a python runtime which could ""rival speeds of C"", where it tried to decide what type each variable was before running it, so the first time calling a method would be slow, but subsequent times would be much quicker. I may have completely misunderstood the video though.",hd1lgb6,t1_hd1ksza,1631768397.0,True
pp6jgs,"so first of boolean is a fake data type (except of old arm, itanium and gpus) so a boolean is an int. I think in plain c there is no boolean and you just do if(int)

it does make debugging harder but usually these languages are not used for long complicated programs. it makes development fast but ONLY if you arent debugging a ton of stuff.

what the video was probably talking about is called ""just in time"" compilation where everything is interpreted and compiled simultaneously so the next time you run it the function has been compiled and runs very fast. Since its now compiled it can run as fast as the underlying language that python is converted to which is C. Javascript interpretation is implemented by the browser and so all (modern) browsers try to use this to make javascript run faster on them.",hd1mb5m,t1_hd1lgb6,1631768965.0,False
pp6jgs,">In python you can use a number as a boolean or even a boolean as a number.

I guess in a lot of commercial environments, that's considered a flaw, not a feature.

If 0 means false, and 1 means true, what does 2 mean, what does -1 mean? What about 0.4?

When 1 can turn into 1.0 without a developer realising it, that's when you end up with a pretty bad bug on your hands. I say this as someone who used Python to build a banking system many years ago...

This is why dynamic typing leaves a bad taste in the mouth for a lot of developers.",hd1mpod,t1_hd1ksza,1631769238.0,False
pp6jgs,"so this is based on a concept called ""truthiness"". so None, zero, """", non-initiated variables, empty data structures, and terminated iterators are considered false and anything that has a value thats not zero is considered true. You can test if a parameter was passed or not using this (although its not recommended) and stuff. As for floating point numbers, I think 0 and -0 are false while everything else is true. 

treating ints/floats as Booleans and visa versa is not actually dynamic typing. In c there is no Boolean because its much closer to machine language where there is only sort of booleans. A boolean ""variable"" only exists on cpus that use predication registers and gpus. for most other hardware you use flags. and so a boolean is always stored as a number. (also no one uses bitmask members in an if statements)",hd1o1t9,t1_hd1mpod,1631770160.0,False
pp6jgs,"Oh, I'm well aware of truthiness, it's just a really bad idea.

C introduced booleans in the standard library in C99.",hd1pa4r,t1_hd1o1t9,1631771008.0,False
pp6jgs,cringe. booleans arent real,hd1qhef,t1_hd1pa4r,1631771873.0,False
pp6jgs,im jk i didnt know that booleans were added,hd1qido,t1_hd1pa4r,1631771893.0,False
pp6jgs,Only 0 mean false others mean true. That's how C works,hd1urqk,t1_hd1mpod,1631775225.0,False
pp6jgs,"As stated in another subthread, the person you're replying to was mixing up static vs dynamic typing and strict vs loose typing. Python specifically doesn't even permit the part in your quote: 4 + True is a type error. Many other languages do allow this though so the rest of your post is absolutely valid.",hd24xxu,t1_hd1mpod,1631784627.0,False
pp6jgs,"It’s compiled *and* interpreted: CPython compiles to a bytecode and the runtime interprets that. Some CPython alternatives include a JIT, broadly similar to Java or C# (although I have no idea how *good* their JITs are and I’d be surprised if it got anywhere near Java’s hotspot)",hd1u8cs,t1_hd1ksza,1631774787.0,False
pp6jgs,"i dont think other implementations of python syntax count for whether python itself is interpreted or compiled. 

also its not really settled whether java ""compiling"" to the jvm counts as a true compiled language and pythons bytecode is even further from machine language.",hd242fu,t1_hd1u8cs,1631783812.0,False
pp6jgs,"CPython is the official reference implementation. It isn't an ""other"", anything else is ""other"".",hd2pa7r,t1_hd242fu,1631797995.0,False
pp6jgs,"One argument is that if you do TDD then your dynamic variables and function arguments/returns are tested for types as you code, so having types doesn't add any bug protection. Any typing bug will not pass the unit tests.

Types are nice code documentation though. Most large size javascript projects that choose not to use typescript usually document their types with jsdoc and also have 100% test coverage that will catch any typing bug.

For example the javascript IPFS implementation mentions they chose javascript over typescript to reach a broader audience of contributors. But they document their types with jsdoc and also have a lot of documentation for public apis. And a lot of unit tests.",hd1plq9,t3_pp6jgs,1631771240.0,False
pp6jgs,Just know both and use the best tool for the job,hd3v7k3,t3_pp6jgs,1631815317.0,False
pp6jgs,"Industry perspective on this should be noted. And, despite all the talk about how Real Work™ is done statically typed, the stats speak for themselves - two most used programming languages are now Python and Javascript.

The business case is obvious.

It's impossible to know enough upfront for Big Design Up Front type of development to be viable. 

Business requirements change frequently, so APIs change frequently, and thus (structural) data types change frequently. And they typically change im ways that are really, really well served by duck typing.

It's extremely hard to prototype in statically typed languages. Time to market laws simply pressure businesses to ship prototypes. Languages that provide production quality prototypes win.

That's why Node.js is growing, Javascript is everywhere, Python is the fastest growing language and Go (with it's `interface{}` pseudo-void type and garbage collection) is fastest growing compiled language.",hd3x326,t3_pp6jgs,1631816083.0,False
pp6jgs,"It's hard to do this comparison because it's not like we're only comparing dynamic typing vs static. We're also comparing so much more. For me, I don't mind static vs dynamic. As an example I like typescript and javascript about the same amount each. That's the closest I can make that comparison.

The reason I like both and don't like java has nothing to do with dynamic typing, rather, I dislike dogmatic oop languages that make duck typing hard. Both of typescript and javascript have solid ways to implement duck typing.",hd4xwew,t3_pp6jgs,1631831517.0,False
pp6jgs,"Python and JavaScript aren't just dynamic typed; they're duck typed.

That is, types guarantee basically nothing. You could have two objects `my_obj_1` and `my_obj_2` both of type `Duck` with *completely* different attributes. This is because an object's attributes can change at runtime (i.e. existing attributes can disappear and new attributes can be declared). Functions can also be redefined on-the-fly. The type of an object really only tells you how it was *constructed* rather than how it's defined *now*.

So just because `isinstance(my_obj, Duck)` is true, that doesn't mean `my_obj.looks_like(Duck)` or `my_obj.quacks_like(Duck)`. The reverse is true as well, which puts the ""probably"" in the statement, ""if it looks like a duck and quacks like a duck, it **probably** is a duck"".

As such, you can never really guarantee anything about the implementation of an object, neither by type checking nor attribute checking. As such, duck-typed languages rely heavily on **convention** and **trust**; I trust you not to change the definitions of my objects so that my convention remains consistent and reliable. If you break my trust and start altering my object's definitions, my convention falls apart, and I can no longer guarantee anything about what my objects really are or how they behave.

In languages with dynamic typing but not duck typing, the type is still meaningful and thus can provide guarantees; it is simply that a name may refer to any type of object, and the exact type of object to which it refers is not determined until runtime (imagine polymorphism in Java if everything was declared as type `Object` and was automatically downcasted to its dynamic type whenever using it).

As to why people use it? It's because 1) if you're writing a small project or script by yourself and you don't need to put trust in other dependencies not to break your conventions, you can often write code more quickly in duck-typed languages than in static-typed languages; and 2) the learning curve is low.

Python is used a lot in machine learning today because a lot of the machine learning community consists of researchers; researchers tend to disregard code quality in favor of speed of implementation since their code is only intended to be run once in an isolated experimental setting. I am a machine learning researcher, and I unfortunately have to deal with other researchers' terrible code every damned day.",hd35d0k,t3_pp6jgs,1631804948.0,False
pp6jgs,"In the codebases , I have worked with - good developers can manage both dynamic and static typed languages. Bad developers mess up dynamic typed languages. Most of the junior devs don't have good practices and the code they write is bad. So it's a good thing to start with static typed and move on to dynamic typed ones.

In college, I learnt C, Java and prolog ( functional ) and in my first job - it was Nodejs. It felt great to learn about even driven architecture and you can solve most the common IO use cases with it. But the codebase is horrible.

I moved on to use Java and Golang ( learning slowly ). It's nice and relaxing as of now. 

As you become mid devs, you will realise - most of the stuff doesn't matter, just the system and framework architecture matters.

I prefer functional + oop + static typed language.",hd1sqm0,t3_pp6jgs,1631773578.0,False
pp6jgs,"One minor correction.

Prolog is logical and declarative. I've learned it too in college. Pretty interesting language.

You might enjoy logtalk, a prolog & smaltalk based logical & OOP language.",hdal7up,t1_hd1sqm0,1631938081.0,False
pp6jgs,Ah... forgot about it. It's been 4 years since last usage.,hdalr21,t1_hdal7up,1631938414.0,False
pp6jgs,"I do agree though that it *feels* like a functional language. For me it was easier to go from prolog to a functional language like scheme. Once you get recursion, it's pretty much the same I guess :).",hdalwl8,t1_hdalr21,1631938510.0,False
pp6jgs,"In college it was an assignment to implement academic course dependencies and validate whether a given set of courses.

Yeah, once you get it - maybe an hour or two - it's easy to use and addictive as well.",hdams46,t1_hdalwl8,1631939064.0,False
pp6jgs,You can always act as if your language is statically typed.,hd2bpwq,t3_pp6jgs,1631790206.0,False
pp6jgs,It's also worth noting that the opposite isn't true. This becomes especially limiting with rigid statically typed languages with rudimentary type systems like Go or pre-generics Java.,hd3vmlq,t1_hd2bpwq,1631815486.0,False
pp6jgs,"Arguably the biggest dynamic programming language is JavaScript. 

As a whole the community has been trying to move away from it's dynamic features for years with the development of really good IDE intellisense to at least warn you when you're doing something silly. 

Recently the popularity of TypeScript has somewhat solidified this trend. A lot of web devs I know (myself included) go out of our way to at least treat JS as a statically typed language even when we're not able to use something like TypeScript (via lots of commenting at a minimum).

To be fair, JS is probably the most insane example of a dynamicly typed language out there. There are a ton of hilarious examples here https://github.com/denysdovhan/wtfjs",hd2dcqe,t3_pp6jgs,1631791311.0,False
pp6jgs,"I have dynamically typed languages to thank for getting me into programming. My first language was python, and I had a hard enough time figuring out the basics of control flow and writing basic code.

I think if I'd have picked up C#, Go, Java or something like that, I might have given up. We see static typing as crucial information but when you're just starting out, every extra symbol contributes to information overload.

So maybe it's a maturity thing, or a task based thing. If I'm just doing a quick project over a couple of days, I don't always want to whip out Rust and spend ages fighting the borrow checker. I can make a Node or Flask API in a day instead and it'll probably be fine.

On the other hand if it's a large project then the more guardrails, the better frankly.",hd2vnmy,t3_pp6jgs,1631800919.0,False
pp6jgs,"I like doing quick little hacks in python like 

`number += boolean_flag`

 lol",hd3lso8,t3_pp6jgs,1631811521.0,False
pp6jgs,"To throw into the great discussions elsewhere:

One of the advantages of static typing is that we make potential errors **compiler errors** that can be caught at compile time, rather than run-time errors which can only be caught at run-time.

For small programs, that's less of a concern, since often for small programs typing can become a detriment in getting something up and running quickly. 

(So to answer your question: if you're building something relatively small and need to do it relatively quickly--a dynamically typed language can be a big win in terms of quickly writing code.)

But as complexity grows, you really want to make more and more problems 'compiler errors' rather than 'runtime errors.' That way you can validate more and more of your code at compile time, catching problems right away rather than having to exercise all of the code paths to make sure there are no potential problems.

----

Type errors are just one category of errors that can be caught at compile time or deferred to run time. There are other categories of errors which can similarly be caught early or deferred (requiring more complex unit testing to validate).

For example, Objective-C (and Swift, which inherits many of the problems of Objective-C) does not have a notion of 'abstract methods.' Nor is it necessarily a compiler error for a class to inherit an interface (or '@protocol') but not define several methods in the interface. (For missing method calls, which Objective-C calls 'selectors', if the method could not be found, the NSObject's 'resolveClassMethod' and 'resolveInstanceMethod' methods are invoked, giving the class a way to dynamically handle these calls.)

But that means if you have a typo in your method declaration, you won't know it until you try to exercise that bit of code.

Meaning a missing abstract method is a run-time error.

In C++, on the other hand, it's a compile-time error; the compiler will complain if there are any abstract virtual functions not declared in a class that is instantiated.

----

I like pushing all errors onto the compiler, and I hate languages which defer checking for relatively easy-to-catch errors to the run-time environment. That just means you have to write more unit tests--and unit tests cannot really guarantee you've thought through all the corner cases and covered them correctly.",hd43jyk,t3_pp6jgs,1631818698.0,False
pp6jgs,"For a newbie programmer static typing is better, as the knowledge expands you are more concious about the software you write and so you may want to take more control over your code and program in general (memory management etc.). This is the tradeoff which exists within high level languages (Java and C++ compared to the abstracion level of python are more like low level languages even though formally it isnt right) where you reduce complexity of your code but at the same time your freedom decreases. Basically, Python is more prototype-friendly and C++ or Java are better choices for the final product. Finally, not to forget about readability, huge codebase may be hard to maintain with dynamic typing, so you save time writing it but at the end of the day you loose more to maintain it and as many companies dont rewrite their software often (unless they are forced to) lower level languages are more suitable in the long run.",hd4pxov,t3_pp6jgs,1631827938.0,False
pp6jgs,"python is great when your application is like 100 lines of code, its an absolute nightmare after that.
Without the help of the compiler i have to wait 30 minutes for my spark jobs to run just to get an error telling me that an int should be a float",hd5i560,t3_pp6jgs,1631841251.0,False
pp6jgs,Because they are bad programmers,hd205m4,t3_pp6jgs,1631780051.0,False
powghp,"Imagine the recursion tree for merge sort. 

In each call, you divide the input in two halves. Therefore, the depth of the recursion tree will be log(n). In depth d, there are 2^d nodes (the root being at d=0).

Now, merging happens in each call, ie. in every node of the recursion tree. However, the size of the input in a node depends on the depth of the node (because we split the input in halves). Particularly, the input size for a node in depth is n/(2^d). Merging is linear in the size of the input. In summary, n/(2^d) operations in each of 2^d nodes in depth d means n operations in depth d.

And since the tree depth is log(n), the complexity is n*log(n).",hczlgka,t3_powghp,1631734489.0,False
powghp,"Nothing to add regarding merge sort, but know that it's not really a thing to write O(a + b + ...) because the convention is to omit all terms except the one with the highest time complexity. So, O(n + logn) should be written as O(n)",hczyh75,t3_powghp,1631739673.0,False
powghp,"Merging is n but there is more than one merge. There is one merge per logn, thus nlogn",hczkw4r,t3_powghp,1631734261.0,False
powghp,"This is not accurate. Merge operation is used more than log(n) times, but with smaller and smaller inputs than n.",hd2gv21,t1_hczkw4r,1631793491.0,False
powghp,Each merge happens in log n. Times n merges,hd0pqeq,t3_powghp,1631752038.0,False
popiv4,I would assume that that is set notation and the square brackets indicate that lowval and highval should be included.,hcy3c64,t3_popiv4,1631711787.0,False
popiv4,">that lowval and highval should be included.

I am talking about the array. Not in terms of closed intervals. I want to know if they should be present in the original array.",hcy3xqb,t1_hcy3c64,1631712081.0,True
popiv4,"Not necessarily, in any case the program check for minor or major values of the given interval. lowVal and highVal may be contained in the array, you could check for numbers ≤ or ≥ of that values. It doesn't matter, you could do whatever you want :)",hcyj1q4,t3_popiv4,1631718780.0,False
poogz8,logic and discrete math,hcy3ski,t3_poogz8,1631712011.0,False
poogz8,"Depends on if they’re planning on getting a CS degree, then they will have to learn calculus for some reason. Discrete math is definitely the most important though.",hcycrxb,t1_hcy3ski,1631716132.0,False
poogz8,You need calculus for algorithms,hcyvrbg,t1_hcycrxb,1631723986.0,False
poogz8,And neural Networks which are a pretty hot topic atm,hcz9l56,t1_hcyvrbg,1631729667.0,False
poogz8,I thought neural networks was more of a linear algebra thing rather than a calculus thing?,hcze8e0,t1_hcz9l56,1631731576.0,False
poogz8,You'll find plenty of derivatives on ML.,hczeg55,t1_hcze8e0,1631731663.0,False
poogz8,Oh great now I have to actually try to learn calc 😔,hczfa8s,t1_hczeg55,1631732003.0,False
poogz8,"If you are truly interested in machine learning and neural networks then you shouldn’t be afraid of learning advanced mathematics, including calculus.",hcztwvy,t1_hczfa8s,1631737835.0,False
poogz8,"Calc is the least of your worries, trust me",hd0kwn1,t1_hczfa8s,1631749728.0,False
poogz8,I figured :/,hd0o2jj,t1_hd0kwn1,1631751253.0,False
poogz8,What aspect of algorithms are you referring to?,hczh1hc,t1_hcyvrbg,1631732712.0,False
poogz8,"You may need some knowledge of sequence, series for basic algorithms. Advanced algorithms involving optimization would require some other basic concepts like newton method, gradient, taylor expansion, etc.   


If you are interested in signal processing then fourier analysis would be helpful.",hd0u1yu,t1_hczh1hc,1631754064.0,False
poogz8,Calculus is only needed for certain subfields of CS (manly machine learning) so if you are not interested in that no need.,hd0rhfs,t1_hcycrxb,1631752862.0,False
poogz8,"Calculus can be used when reasoning about the growth rate of algorithms. Say, for instance, you have two algorithms, y=2^n  and z=n^2. You want to find which one has the slower growth rate. So you differentiate with respect to n for each function and plug in a value for n and use the one with the smaller slope.",hd1mu1g,t1_hcycrxb,1631769321.0,False
poogz8,"I don’t think you need to differentiate anything to know that exponentials grow way faster than polynomials, but I do see your point. I probably picked up more from calculus than I realize.",hd2ooao,t1_hd1mu1g,1631797703.0,False
poogz8,"Yup Discrete Math is something you need you understand. Plus it's really find. ""Truth tables and Logic gates"" and some other fun stuff.",hd1iffr,t1_hcy3ski,1631766463.0,False
poogz8,"Exactly. I'd say discrete is just barely, slightly more important than logic depending on the specific field. Linear algebra is a very close third place depending on whether or not you plan to pursuit graphic design or other related sub-specialties.",hd1zppn,t1_hcy3ski,1631779628.0,False
poogz8,"Linear algebra. Try to actually remember it too. Linear algebra is so widely used in computer science that cpus have highly optimized routines for them called BLAS - Basic Linear Algebra Subroutines - which make code run using linear algebra much faster than code without it. It shows up everywhere, and while its not a defining feature that makes a good developer great, I haven’t met a great developer that isn’t very familiar with linear algebra. 

Its the basis of almost all machine learning algorithms, and used anytime you want to process a lot of data very efficiently. Also used as key building blocks for efficient game engines and efficient code in general.

Edit: Also I noticed your question was worded in a way that you might be holding off on learning computer science while you learn math. My advice is to not hold off, and just start learning to code or taking a comp sci course.

Edit: For those asking for specific examples:

Matrix multiplication - everywhere in machine learning, almost every algorithm, in the old way of using tensorflow you’d have to multiply all the matrixes manually (using tf.matmul) but now its taken care for you in Keras. Fast forrier transform uses matrix multiplication behind the scenes. Used all the time in data analysis.

Matrix add/subtract/divide - also used all the time - ie kernels, data analysis, tensorflow, etc.

Matrix math is also used in pandas a lot. Creating masks with its super useful.

Dot products - used in data analysis for correlation matrices, used to get similarity scores between vectors. Good to know, but probably wont use too frequently, though you might.

Cross products - used much less but good for checking how different two vectors are.

determinants - underutilized imo! Theres this thing called determinental point processes which have a lot of really awesome, look up a youtube video lecture by Jessie Dodge on this (its about open loop hyperparameter optimization). Its really awesome!

These are just a few examples, but there are definitely more",hcyl4xp,t3_poogz8,1631719656.0,False
poogz8,"Always been interested in that math-y side of computer science, so interesting how that works. Only in Calc 2 but fascinating, can’t wait to get higher up there",hd0od78,t1_hcyl4xp,1631751393.0,False
poogz8,Tbh Calc 3 is definitely easier than Calc 2 in my opinion.,hd1ta65,t1_hd0od78,1631774007.0,False
poogz8,"We haven’t even gotten to the hard part yet. Only things we’ve done for I think 5 weeks so far was review integration techniques, area/volumes of revolution and then learn arc length and work. Just barely getting to new integration techniques 😬",hd1tx9c,t1_hd1ta65,1631774531.0,False
poogz8,"Discrete mathematics,
Linear Algebra,
Calculus,
Probabilities and Statistics.
There are more but at undergrad level those are the big 4",hcyol0p,t3_poogz8,1631721072.0,False
poogz8,"Agreed. No need to dig deep in any area, but the basics in these above areas really make a difference",hcyqb2d,t1_hcyol0p,1631721773.0,False
poogz8,"Agreed! And MIT has some good, free resources available to follow along, OP!",hd03jfd,t1_hcyol0p,1631741807.0,False
poogz8,Also linear algebra is very useful.,hcyhyrh,t3_poogz8,1631718317.0,False
poogz8,"Getting a Bachelor's in CSci right now, I read the course description for my linear algebra class, I'm excited and terrified at the same time!",hczeexe,t1_hcyhyrh,1631731649.0,False
poogz8,"Don't be. I took it and it really isn't that bad. And I'm not some super genius, lol.      
It's pretty much just algebra but with matrices.",hd0rgij,t1_hczeexe,1631752851.0,False
poogz8,"Alright lol, I just saw the word ""eigenspace"" and sorta got a little nervous, haha",hd14s5r,t1_hd0rgij,1631759127.0,False
poogz8,I’d say linear algebra and discrete maths. I’m not sure about calculus I’m just a junior in college so I haven’t seen calculus yet to be applied in the cs classes I’m taking but linear algebra and discrete maths I’ve used in some of my cs classes. Also linear algebra is everywhere. It’s is one of the most important math classes as a cs major.,hcyzvr0,t3_poogz8,1631725678.0,False
poogz8,Which cs class did you use linear algebra? Apart from ML of course,hd09es7,t1_hcyzvr0,1631744364.0,False
poogz8,"Coding theory, linear programming, robotics, graphics programming, probability, statistics, graph theory, signal processing.",hd0uneu,t1_hd09es7,1631754338.0,False
poogz8,"Not sure about OP, but linear algebra is used a lot in graphics programming",hd0la8j,t1_hd09es7,1631749910.0,False
poogz8,Computer graphics and game design,hd0ncwj,t1_hd09es7,1631750906.0,False
poogz8,"A computer science degree is pretty much an applied math degree: calculus, linear algebra, discrete structures, combinatorics, graph theory, and then whatever extras your concentration requires.",hczqc2k,t3_poogz8,1631736412.0,False
poogz8,"I love coding maths theorems and do lots of machine learning. If you need any tips, let me know :)",hd03oi7,t3_poogz8,1631741868.0,False
poogz8,There is a website called “Project Euler” that has a bunch of math coding challenges. You should check it out! :),hd0qs2d,t1_hd03oi7,1631752528.0,False
poogz8,Thank you so much for your recommendation! I have been doing some of the challenges on there and love it :),hd9ee1d,t1_hd0qs2d,1631916374.0,False
poogz8,"You’re very welcome!
I try to do a few here and there after work. Let me know if you find any that are really interesting! :)",hdf8yft,t1_hd9ee1d,1632024503.0,False
poogz8,"I would recommend calculus, probability and statistics and linear algebra",hd03jmx,t3_poogz8,1631741810.0,False
poogz8,"Personally, I have taken three semesters of calculus, linear algebra, differential equations, discrete math, and engineering statistics. 

Of course, my major is computer *engineering*, so it's going to be different. However, I have used my math skills to solve many problems related to programming, particularly when it comes to processing large arrays or structuring data.",hd0j6qx,t3_poogz8,1631748905.0,False
poogz8,"By your question, I am assuming you mean computer science and not simply programming.

Go look at some  number of universities with CS programs and look what classes you need to take to get the CS BS.",hd0537n,t3_poogz8,1631742468.0,False
poogz8,Discrete math(s) and a lot of calculus and linear algebra.,hd05t4q,t3_poogz8,1631742772.0,False
poogz8,"Focus on algorithms and data structures. Math isn’t usually something I whip out while programming most things (it has its uses but just learning Linear Algebra won’t make you a better programmer by default). Instead, learn about ways of organizing data and the trade offs with each. I use a hash table every single time I build something. It’s worth knowing about time and space complexity as well, so maybe that’s a good starting point if you’re math oriented. 

For example, if I gave you a list of numbers and asked you to search for some number N, what’s the fastest way you could do that? Can you save time anywhere? The base solution would take about as long as it would take to read the list, but you can do better with smarter data organization! 

YouTube is your friend. I started on the same journey 5 years ago and it was worth every second! Good luck!",hd1d5et,t3_poogz8,1631763413.0,False
poogz8,"I would primarily focus on discrete math and algorithms, linear algebra, and statistics. That seems to be the most used in my opinion. Not to say calculus isn't important, because it definitely is (especially for machine learning), but the most I used it for was to do basic integration or doing derivatives in most classes. Only recently have I been using it for ML and but at the same time, this isn't my research area so I don't really care all that much about it right now.

I'll share all the ""math"" classes I had to take so far and attempt to categorize them into three sections. Discrete (which should ideally be your first introduction to the type of math most of your ""math classes"" in your degree will consist of) will be the logic section and arguably the most important since it contains the foundation of ""how to think like a programmer/computer scientist"".

Anything under the discrete section are based on logic or critical thinking.  Stats and Linear Algebra may be small but those classes that use these concepts are still important and all tie in together. 

If you actually want to learn the math before jumping into CS then you may do well by going through a formal mathematics route in that case Discrete/Proofs/Graph Theory/Game Theory/etc would interest you more. Hopefully, you or others find this useful! :)

Discrete Math

* Algorithms & Data Structures, Advanced Algorithm Design
* Theoretical Computer Science
* Compiler Design
* Computer Networks
* Databases? Kind of a stretch but it uses relational algebra
* Statistics (set theory portion of discrete math)
* AI

Statistics

* Machine Learning

Linear Algebra

* Computer Graphics
* Statistics
* Machine Learning",hd1o4gy,t3_poogz8,1631770210.0,False
poogz8,"If you haven't even started your computer science journey, no math is required. Just begin to program for fun, learn a bit as you go. Math is not the first step I'd say. If you're getting your degree you'll learn plenty trust me",hd27sru,t3_poogz8,1631787190.0,False
poogz8,You don’t need math…just logic skills. There are libraries for math stuff.,hczpeh2,t3_poogz8,1631736041.0,False
poogz8,"you very much so need math for computer science.  


for *programming*... perhaps not.",hczqzmf,t1_hczpeh2,1631736675.0,False
ponstr,"I'm not aware of a working implementation, but you could build a postprocessor for https://github.com/xoreaxeaxeax/movfuscator (relevant talk: https://youtube.com/watch?v=R7EEoWg6Ekk), which is a c compiler that can compile to only mov instructions. There are already post processors that generate among others only xor instructions.
(Whiles you are at it, a fractran postprocessor would be quite cool as well)

Although, you'd have quite a lot of trouble computing anything with this, since even a simple program would take a humongous state.",hcykgjx,t3_ponstr,1631719375.0,False
podbzs,"Oof, Turing got Schmidhuber-ed.

He's right of course, regarding Turing's 1936 work and its relation to Godel and Church's earlier work. However I still feel like Turing's formulation was more ""correct"" than Godel's general recursive functions or Church's lambda-calculus, but that might just be my CS bias. Certainly it's a better model for thinking about the limits of computation in terms of resources, than either of the other two formulations. I suspect Godel also agreed, since as mentioned: 

""Nevertheless, according to Wang,[WA74-96] it was Turing's work (1936) that convinced Gödel of the universality of both his own approach (1931-34) and Church's (1935).""",hcw18p7,t3_podbzs,1631665152.0,False
podbzs,Really the issue is that people have only so much space for the minutia of history. Like yes as a computer scientist those guys are kinda important. But the average person is lucky to remember one person in CS. It's inevitable that someone wins the heroization lottery.,hcwt2w3,t1_hcw18p7,1631678555.0,False
podbzs,"I agree that people have pretty limited capacity for the subtlety of these things, but I don't know if I think the conclusion is inevitable. We could, for example, teach students that scientific discoveries are a process by a community of people, and while some individuals have a disproportionate impact, no person is an island and every scientist builds on the work of others. Thus no single scientist should be aggrandized, since the truth is often much more complicated.",hcxt6fl,t1_hcwt2w3,1631706133.0,False
podbzs,"yes. also 'we cant give credit to 5 people, lets give all the credit to 1' is not the right way of celebrating science.",hczjid2,t1_hcxt6fl,1631733707.0,True
po6xdm,"USB keyboards do not use ASCII at all. They have their own system of scancodes (or ""usage codes""), with entries for ""Volume Up"" etc.

Here is the list of USB keyboard scancodes.
https://deskthority.net/wiki/Scancode",hculyf9,t3_po6xdm,1631643482.0,False
po6xdm,"Thank you very much! This is exactly what I was looking for!

I was pretty sure keyboards use ASCII to encode keys before sending signals to the computer.",hcussrd,t1_hculyf9,1631646164.0,True
po6xdm,"If you want to know exactly how keyboards transmit data, you'll probably want to look up Human Input Device (HID) Protocol. I suspect there's more to it than just transmitting the letter 'A' in ASCII, as computer-side software can detect when the key is both pressed and released.

I haven't used [WireShark](https://www.wireshark.org/) in quite some time, but I seem to recall that it came with something called USBCap. Typically WireShark is used to view inspect network traffic (e.g. your TCP/IP packets going between your machine and the web server), [but it can also be used to examine USB data](https://isc.sans.edu/forums/diary/Wireshark+and+USB/23457/). If you install WireShark and filter it down to USB data, then start pressing keys on your external keyboard, you should see the packets appear in WS, which can then be expanded to see what each field means.

Finally, it's worth noting that keyboards aren't 'output-only' devices. Or in other words, they don't just send data to your computer, but they can also receive it. For example, there was a thread on StackOverflow Code-Golf where users made programs which caused the Caps-Lock button to blink: [https://codegolf.stackexchange.com/questions/110974/blink-the-caps-lock](https://codegolf.stackexchange.com/questions/110974/blink-the-caps-lock)",hcuewqr,t3_po6xdm,1631640639.0,False
po6xdm,"Thank you very much for the useful information! I will try the apps you suggested. I already have wireshark, but I didn't know it can also examine usb packets. 

And yes, of corse there is more to it tham just transmitting 'A', there are sync signals and other kind if signals both before and after the letter itself, but I want to know what binary code is used for command keys.

I will try wireshark to see if it works.

Thanks again!!",hcuiexu,t1_hcuewqr,1631642035.0,True
po6xdm,ASCII is just an encoding standard.  Anything can be encoded as long as both parties agree on how.,hcv5vby,t3_po6xdm,1631651328.0,False
po6xdm,This is covered here. https://youtu.be/wdgULBpRoXk by ben eater.,hcwytak,t3_po6xdm,1631681974.0,False
po6xdm,"I'll check it out, thank you!",hcxlxmc,t1_hcwytak,1631700917.0,True
po6um9,"1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)
7. Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms)

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)

And finally, [play the long game when learning to code.](
https://stackoverflow.blog/2020/10/05/play-the-long-game-when-learning-to-code/)",hcushr2,t3_po6um9,1631646042.0,False
po6um9,I think the last one is the hardest. There are so many tutorials that promise you mastery in 30 minutes it's easy to get entangled in bad practices and forget to learn what good code should be like.,hcuy01g,t1_hcushr2,1631648224.0,False
po6um9,Is this a sidebar comment?,hcwu8yu,t1_hcushr2,1631679219.0,False
po6um9,No.,hcxaut4,t1_hcwu8yu,1631690933.0,False
po6um9,Should be,hcxvgmu,t1_hcxaut4,1631707537.0,False
po6um9,"Google free college classes. Edx and Coursera are some of them, they'll have CS courses. Codecademy + w3schools for programming languages. Have fun dude",hcufwap,t3_po6um9,1631641029.0,False
po6um9,"Lol the computer science subs have a huge section (see the sidebars) on resources on computer science.

EDIT: Okay, maybe not this one but check out /r/programming , /r/learnprogramming , /r/cscareerquestions , and /r/csMajors",hcumip9,t3_po6um9,1631643703.0,False
po6um9,"[https://teachyourselfcs.com/](https://teachyourselfcs.com/)

Take a look at here. See what grabs your attention.",hcv3e1q,t3_po6um9,1631650351.0,False
po6um9,"I'm coming from a math background here, but this is one thing you can start doing: Read some of Epp's book on discrete mathematics, and then read some of the ""algorithms bible"" aka CLRS, aka *Introduction to Algorithms* by Cormen and others.  If you take on these (very large and difficult) texts, you'll probably want to have a tutor or teacher who can help you through them.  But discrete math comes up in a lot of different CS classes, so it could be especially good to have a head-start on that.",hcuncxb,t3_po6um9,1631644037.0,False
po6um9,Will Susanna Epp's Discrete Math book be enough to give pre-requisite math background for CLRS?,hcux9d1,t1_hcuncxb,1631647934.0,False
po6um9,"Not for the entire book, but if the kid can get through the first several chapters (which is already more than most college undergrad courses in algorithms get through), he's a genius and doesn't need our help.",hcva5ep,t1_hcux9d1,1631653040.0,False
po6um9,"Thank you, i was just asking for myself (not OP). Wanted to start learning algorithms and i often see Epp's and Rosen's book being recommended as pre-requisite. Thats why i wanted to check if this will be enough to start learning algorithms from CLRS. 
Also if you have red Epp's book, would you recommend any pre-requisite for starting. Also do you recommend doing all questions from the book?",hcvnqmn,t1_hcva5ep,1631658893.0,False
po6um9,"Thing is, CLRS contains some stuff that you won't fully understand until you take something like Real Analysis.  But if you're taking Real Analysis you're probably a mathematician, or something close to it.  So I wouldn't worry about going this far out in the textbook--it's not meant for the typical undergrad.  

So even though Epp won't prepare you for everything in CLRS, don't worry about everything.  Epp's book prepares you for the parts of CLRS that you should study in an intro class.  

If you want to read Rosen as well as Epp, or instead of Epp, that's probably fine.  I personally feel like if you just read one of these books that's enough.  But it's hard for me to say because I already know all this material, so to me it all looks easy and you don't need to worry about it.  But I know that past-me who didn't already know this stuff, might not be able to just read one book and be ready to move on to algorithms.  

So just pick a book, start reading and doing exercises.  If you don't like how they do things then start reading the other book.  Hop around to various books until you find one you like best.

Do lots of exercises.  I personally do WAY more exercises than I read.  Exercises teach you more than reading does.",hcvorjn,t1_hcvnqmn,1631659361.0,False
po6um9,Thank you so much buddy for sharing your learning experience.,hcxhoxd,t1_hcvorjn,1631697143.0,False
po6um9,"What I’m about to say isn’t “resources” exactly or is it “a good way to learn”. But here’s the college courses I took, that I feel will bring so much value to anyone interested! 
I comment this with the hopes it gives you a “Template” of what to learn and what’s next for you!

1. Programming basics: variables, if statements, loops, very basic things. 

2. Object Oriented Programming: what objects are, how to use them. 

3. Data structures and algorithms: sets, trees, queues etc. sorting/search algorithms (Everything from simple Bubble Searches to ‘A Star Searches’)

4. Database Management: how to design databases, how to get information from databases and use them accordingly. 

5. Object Oriented Design and Implementation: learn things like Design Patterns (Model View Controller (MVC), states)

6. Parallel Algorithms: Multithreaded programming. Essentially how to run multiple tasks at the same time. 

7. Networks and Security: this is more of an IT course I took. But it gave me a good idea of how the internet works, how machines communicate, etc.",hcwomaw,t3_po6um9,1631676160.0,False
po6um9,Khan acdemy has a CS section.,hcxpah3,t3_po6um9,1631703515.0,False
po6um9,"[https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/)   
Still a place I go to as a software dev. It has everything from basics to advanced topics. Most of the articles are very well written and clear to understand. A good starter set of languages would be python, java, or C#. You could start with a more complicated lang like C or ++, but they can be overwhelming to start with. Other than that, just try to code something. Practice will take you the furthest.",hcv91jh,t3_po6um9,1631652599.0,False
po6um9,people say geeksforgeeks is full of misinformation,hcvbjk0,t1_hcv91jh,1631653612.0,False
po6um9,"To be fair in my experience everywhere has misinformation. When self learning, figuring out what works, what doesn't, and what works but is technically bad practice, takes a lot of patience and checking several different sources.",hcx35rw,t1_hcvbjk0,1631684858.0,False
po6um9,Are you taking high school courses in the subject?  What state and county are you in?,hcv9ktn,t3_po6um9,1631652810.0,False
po6um9,"Come up with some personal projects. If you have an idea related to technology, see it through. Want to make a website for example? Follow web dev projects until you start coming up with/refining your own ideas.

Remember it’s important to be ambitious (there’s a lot to learn/master in CS) however, set realistic goals. You’re not gonna make a polished RPG mobile game on Unity in a day. Especially if you play video games half the day, and have no experience. Give yourself a couple days/weeks for a serious project. 

Reach out to other people practicing. Pick their brain.

My biggest suggestion: never give up, and don’t take the easy way. Copy/pasting somebody else’s work is only passively learning, like listening to a lecture. Do your own work, your mind will start racing with the possibilities you can accomplish.",hcvbpee,t3_po6um9,1631653681.0,False
po6um9,reading.,hcvk6tm,t3_po6um9,1631657296.0,False
po6um9,"Hmm, not really.",hcvutln,t3_po6um9,1631662161.0,False
po6um9,"This is a great intro course. Doing the assignments/homework is what will gain you the most. It's challenging as a beginner, but so satisfying as you complete them. 

https://online-learning.harvard.edu/course/cs50-introduction-computer-science?delta=0",hcwmka7,t3_po6um9,1631675112.0,False
po6um9,Look for university courses' books and read those. Try to stay away from pop science book as they will more than likely give you a false sense of understanding of a subject.,hcx3f4r,t3_po6um9,1631685043.0,False
po6um9,The internet is overflowing with information,hcx8v0r,t3_po6um9,1631689250.0,False
po6um9,freecodecamp,hcz6icg,t3_po6um9,1631728394.0,False
po6um9,"I can vouch for Udemy, learnt a lot through the courses on that website",hczevo5,t3_po6um9,1631731838.0,False
po098u,Are you asking about [non-binary computers](https://ddg.gg/?q=non-binary+computers)?,hct3ebd,t3_po098u,1631619274.0,False
po098u,Are you asking about [Quantum Computing?](https://en.m.wikipedia.org/wiki/Quantum_computing),hctetzk,t3_po098u,1631625661.0,False
po098u,"**[Quantum computing](https://en.m.wikipedia.org/wiki/Quantum_computing)** 
 
 >Quantum computing is the exploitation of collective properties of quantum states, such as superposition and entanglement, to perform computation. The devices that perform quantum computations are known as quantum computers. :I-5 They are believed to be able to solve certain computational problems, such as integer factorization (which underlies RSA encryption), substantially faster than classical computers. The study of quantum computing is a subfield of quantum information science.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hcteve1,t1_hctetzk,1631625680.0,False
po098u,How about “multi bit transistor”? Mubistor if you will.,hcuh9wg,t3_po098u,1631641577.0,False
po098u,That's just an amplifier really.,hcx99oi,t3_po098u,1631689578.0,False
pnwtqa,"Tip: organize a truth table for any gate or logic combo you are trying to come up with a solution for. As someone said, it's an xor gate. I've totally been victim to this before, and it really helps to see the truth table yourself and just go ""oh durr that's an xor"" or whatever.",hcsfy1t,t3_pnwtqa,1631599482.0,False
pnwtqa,It's amazing how far you can bake things down that way sometimes.,hcsh3vr,t1_hcsfy1t,1631600369.0,False
pnwtqa,That is called an XOR gate.,hcsemk8,t3_pnwtqa,1631598511.0,False
pnwtqa,Thanks.  Now I realize this was kind of a dumb question,hcsewn1,t1_hcsemk8,1631598711.0,True
pnwtqa,"Not at all, XOR is sneaky like that. You know most programming languages don't have what they call a ""binary XOR"" operator? Because it's hiding as ""!="".",hcsphwy,t1_hcsewn1,1631607564.0,False
pnwtqa,"That's really interesting! I never even thought of it like that, but it makes perfect sense",hcu0ut3,t1_hcsphwy,1631635020.0,False
pnwtqa,xor IS sneaky. I was looking at an article about assembly code for nvidia gpus and I noticed theres a fused load-and-xor instruction. This confused me until I realized its for testing equality,hcx87xi,t1_hcsphwy,1631688729.0,False
pnwtqa,"That's interesting. I know the xor instruction is used on x86 to set a register to 0. `xor eax eax` is a smaller instruction than `mov eax 0`, since it avoids the 32-bit zero literal. And no matter what value was in the `eax` register initially, xor'd with itself becomes 0.",hcxcfwk,t1_hcx87xi,1631692337.0,False
pntwgy,Hint: build an adder and use a negative number as one of the inputs,hcrtiac,t3_pntwgy,1631586816.0,False
pntwgy,"Have it run on 2s compliment, and the subtraction would be straight up.",hcti1qu,t1_hcrtiac,1631627165.0,False
pntwgy,That's correct. I wanted them to see that though.,hcut9m3,t1_hcti1qu,1631646349.0,False
pnsaul,It’s really just for interpreters. Compilers have a whole different execution model,hctl052,t3_pnsaul,1631628475.0,False
pnsaul,Yeah it touches on three address byte code so that’s why I included compilers. Theoretically you could plug in LLVM or something like that and compile but yeah you’re right.,hctm1ju,t1_hctl052,1631628915.0,True
pnp559,"I'm not sure what you mean by ""skip code"". Let's start with some definitions: a ""branch"" is any time the program decides to go to one of multiple choices. If statements are branches, but so are loop conditions or switch statements (python doesn't have these). I'll talk about branches in general.  


If you mean running only one side of the branch, there are a few reasons a language would do that. The most obvious is performance. If the user only asked you to do one side of the branch, why would you waste time doing the other? The other reason for this is correctness. The user probably put the condition in there for a reason. The if statement might be checking to make sure you didn't do some illegal operation or violate some safety condition (see below for a fun example of when this goes wrong).  


If you mean running only some of the checks in the condition (like when you have an ""and"" or ""or"" statement), there are the same two reasons: performance and safety. Most languages will only evaluate a condition until they can be sure of the outcome, but no more (this is called ""short circuiting""). If I have ""if f() or g()"", the program will only run g() if f() is false. This means we don't have to do extra work. If g() is only meaningful when f() is false, then it's also a safety concern.  


To your top-level question: branches are somewhat inefficient, but not to a degree that really matters for the vast majority of code (certainly not something like python). The reason comes down to how CPUs work. CPUs try to run lots of stuff at once. They usually are half-way through processing a bunch of instructions at any given time. There are a few forms of this, but pipelining and out-of-order execution are the big ones if you want to Google more. Basically, if you ask the CPU to do a bunch of unrelated things in a row, it can do some or all of them at the same time instead of waiting for one to finish before starting the next one. The bottom line is that when you branch, the computer can't do anything until it knows the result of the branch. It looses all that nice parallelism.  


Mitigations and why this isn't really all that big of a problem: High performance CPUs do something called ""speculation"". They do stuff that they aren't sure is legal until later. They have a bunch of predictors (e.g. 'branch predictor') that tells them what the most likely outcome is and they do that until they know for sure. If they were wrong, they just undo everything up until that point and try again. Branch predictors are really really good, for most code they almost always guess right. Some types of code are fundamentally unpredictable and that can be a problem but in general speculation is so good and Python has so many overheads already that if you are worried about the cost of branches, you should probably be writing it in C++ or something.  


Fun fact: This speculation violates all the performance and safety stuff I mentioned earlier! The performance part is easy: they CPU wouldn't have anything to do anyway so it may as well run part of the branch. The safety part is more interesting. CPUs are pretty fancy and they keep track of everything that's going on, so when they go down the wrong path they're pretty good at undoing everything. They try to avoid stuff that might cause side effects (e.g. none of your changes to memory can be seen by anyone until the checks are done). Unfortunately, some recent attacks called Specter and Meltdown showed that we weren't as good as we thought. It turns out really clever hackers can leave 'bread-crumbs' behind after the CPU cleaned up. All those tricks to guess what's going to happen? They learn from stuff that happened before, even on failed guesses! You can then use some clever performance measurements to see if they learned from the wrong branch. It's pretty wild and very hard to fix since those 'guessers' are so key to performance that you can't really just get rid of them.  


Anyway, probably more detail than you wanted but branches are surprisingly interesting!",hcr4zah,t3_pnp559,1631575479.0,False
pnp559,"Very interesting comment, thank you for taking the time to write it.",hct5k12,t1_hcr4zah,1631620656.0,False
pnp559,"In C, if statements can be considered inefficient because of cache misses. I’m unsure how caching works with Python’s virtual machine.",hcqymk8,t3_pnp559,1631572505.0,False
pnp559,"I'll add that it's not just cache misses that are a problem with branches. Compilers tend to try pretty hard to keep related code close together in memory to minimize that problem (though obviously it's not always possible). Another big problem is with flushing the pipeline. The CPU is always trying to run ahead of execution and have multiple instructions in flight at the same time. When you hit a branch, the CPU doesn't know what the next instruction is so it has to wait for everything to finish before starting something new. For really small, low-performance, CPUs this isn't really that big a deal. But for big fancy CPUs (most of the CPUs people think of), this can be a really big deal. Pipelines can have 10s of steps that can all be running at the same time (each instruction has to do 10 things, but 10 instructions can be doing one of the things at the same time). Beyond pipelining, they can be ""out-of-order"", which basically means that not only can it be running different parts of multiple instructions at the same time, it can be running the same part of multiple instructions simultaneously. At the end of the day, you might have hundreds of instructions in flight at any given time. Branches can throw all that parallelism out the window which hurts performance.",hcr7h3p,t1_hcqymk8,1631576666.0,False
pnp559,"Computer compilers are always trying to increase efficiency. And one common method is skipping sections of logical expressions. For example

Say you have the logical expression
(A and B)

If A is false, then the overall expression is false no matter what B is. Because (False and True) and (False and False) both equal false. When the program is resolving (A and B) and A is false, then it will stop at A and it wont execute B. This saves the time and resources that would have been used to compute B.",hcrisqs,t3_pnp559,1631581883.0,False
pnp559,"Programs that are ultra efficient I have seen try to eliminate the use of if statements, since a high amount of branches can cause your compiler or CPU to incorrectly predict the outcome of certain branches and can make your code slower.",hcrwsb4,t3_pnp559,1631588357.0,False
pnp559,"It depends on the architecture. In the past programmers tried to avoid ""branching"" (if statements) because it was very slow. On modern CPUs the branches are predicted and unless the branch the program takes is truly random you wont have any real slow down. 

on architectures that are not x86 it can be different. OLD arm cpus, intel itanium and a couple other architectures handle if statements without branching. This is handled through predication registers on the cpu that hold boolean values. If the predicate of an instruction is false then either the instruction isnt executed or the result isnt stored (depending on implementation). This is used in basically every gpu architecture btw. This handling of IF statements is very fast for very random conditions but is slower for most programs. 

if you are trying to optimize code by working around if statements by using math you are not doing yourself any favors. The interpreter knows how to optimize an if statement if it can be.",hcx9dj9,t3_pnp559,1631689669.0,False
pnners,"https://computerhistory.org/timelines/

https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo",hcqpzhv,t3_pnners,1631568660.0,False
pnners,Thank you,hcqq5nc,t1_hcqpzhv,1631568735.0,False
pnners,You’re welcome!,hcqqmlz,t1_hcqq5nc,1631568940.0,False
pnners,"I just started watching Lex Fridman's podcast where he is talking to Donald Knuth and it's really interesting listening to him talk about using computers way back in the 60's and 70's.  [https://www.youtube.com/watch?v=EE1R8FYUJm0&t=2332s](https://www.youtube.com/watch?v=EE1R8FYUJm0&t=2332s)

Knuth was also on an earlier episode too but I haven't seen that one yet. [https://www.youtube.com/watch?v=2BdBfsXbST8](https://www.youtube.com/watch?v=2BdBfsXbST8)",hcqvi5l,t3_pnners,1631571073.0,False
pnners,I didn't even think of looking into Lex's podcast. I know he's had some technological geniuses on there before. Thank you,hcqvy1p,t1_hcqvi5l,1631571274.0,False
pnners,Jack Kilby and ICs might be an interesting place to look.,hcr26uk,t3_pnners,1631574173.0,False
pnners,I was watching something about Ken Olsen yesterday and DEC. I think Jack Kilby will be the next person I look into. Thank you,hcr3khg,t1_hcr26uk,1631574817.0,False
pnners,"If you are interested in a light reading this is a nice book: Innovators from Walter Isaacson https://en.wikipedia.org/wiki/The_Innovators_(book)

To be honest neither cover nor title really gives it justice. It is a really good (although biased from USA point of view a bit) story/record of the events of how in general computer science/engineering shaped the last century and progressed.",hcqv3xx,t3_pnners,1631570893.0,False
pnners,"This sounds like an amazing read. Will definitely check it out, thank you",hcqvu65,t1_hcqv3xx,1631571226.0,False
pnners,"Building computer processors and and the software that runs on them are sort of two separate efforts.  Up until the 1990's, computer processors were primarily built by hand on drafting tables or using primitive CAD tools.  During the 1990's, the Hardware Description Language (i.e. Verilog) came about and revolutionized the semiconductor industry by allowing the process of ""synthesis"".",hcqxxt9,t3_pnners,1631572187.0,False
pnners,"No I understand the difference between a processor and the programs that run on them. I'm more so referring to the programs that were used to design them. Around the time that designing a processor in several programs, and bringing them together, replaced designing them in one huge program. For instance when programs were beginning to use boolean expressions as input and produce logic circuit designs as ouput",hcqz5cq,t1_hcqxxt9,1631572751.0,False
pnners,"I don't think there were any software tools specifically for designing hardware circuits, especially back in the early days.  They were still using [Karnaugh maps](https://en.wikipedia.org/wiki/Karnaugh_map) and that sort of thing to simplify complex boolean expressions into circuits that could be implemented easily in hardware.",hcr06nd,t1_hcqz5cq,1631573235.0,False
pnners,"Ok then at the risk of explaining something wrong, because I'm not that well versed on this subject, I think what I'm referring to are the tools that were created as the advancements in VSLI came to be. Looking back I mistakenly said that what I'm looking for are the programs that were used to design parts of a processor, but I meant the programs that were used to design parts of the tools that were then used to design circuits, not processors. I don't think so anyway

Edit: VLSI*",hcr1a0d,t1_hcr06nd,1631573744.0,False
pnners,"> The goal of this post is essentially to find any resources that could help me learn about computer science from roughly 1960 to 1980, and primarily how the science of processor building started to advance.

Do you mean computer architecture and organization? Or electronics engineering?

> As of right now I'm only aware that around 1970 with the advent of UNIX there was the beginning of a new level of computer processor building, in which they'd use computers themselves to build the processor because they were becoming more complex than the previous use of physical logic modules and all that stuff.

People have been using computers to design new computers since the late 1950s and early 1960s. IBM, for instance, modeled the architecture and organization of its Stretch supercomputer (early 1960s) to understand the performance impact of its design choices; and in the 1960s, when the several System/360 computers were being designed, they had primitive EDA tools for representing the circuit as a netlist and for managing bill of materials. I also vaguely recall reading somewhere that logic reduction tools started appearing around this time.",hculw67,t3_pnners,1631643456.0,False
pnners,"If computer architecture and electronics engineering were part of the process during that time then yes.

Yes I understand that computers were used in the building process of more computers. I was referring to the advancement of this. The newer tools that were being used, as the scale of integration of silicon circuits grew larger, and the creation of C and UNIX helping with this. I wasn't claiming to know much about that era, just that it's the era I had looked into.",hcup2rb,t1_hculw67,1631644723.0,False
pnners,"You could check the languages APL, J and the story of their creator Kenneth Iverson.",hcqf1jl,t3_pnners,1631564190.0,False
pnners,Awesome thank you,hcqflu4,t1_hcqf1jl,1631564416.0,False
pn4sj1,I have a textbook you can borrow from the computer architecture class im taking. JK you should go watch ben eater on youtube build a computer from scratch,hcx8dkk,t3_pn4sj1,1631688856.0,False
pn4sj1,by scratch I mean ICs not the coding block thing scratch,hcx8fgr,t1_hcx8dkk,1631688898.0,False
pn4sj1,Princeton University’s Computer Architecture course on Coursera. It is free.,hd1mags,t3_pn4sj1,1631768952.0,False
pn4sj1,"My advice isn’t very specific but supplemental YouTube videos to assigned readings really helped me in undergrad.

There are wonderful people on YouTube that make fantastic computer science videos. Supplementing your readings and lectures with independent research via YouTube goes a long way IMO. 

Good luck friend!",hcn4von,t3_pn4sj1,1631498890.0,False
pn4sj1,The greatest breakthroughs of humanity were never made alone. You've got to find people to share and explore with. That's the reason I quit pursuing math and science. There is no one that cares to walk with me there.,hcojb4o,t3_pn4sj1,1631534805.0,False
pn4gzx,"Yes someone can view you IP address when you are connected to the Internet without a VPN. 

The internet communicates via packets. Packets need to contain the source IP address and the destination IP address in order to route packets successfully. This information can also not be encrypted because how would you establish encryption between yourself and a server without first contacting the server to do a TLS handshake? Moreso each router your packet routes through would need to know how to decrypt that information in order to route packets successfully which would essentially defeat the purpose of encryption so the source IP address and destination IP address of a packet is always visible and not encrypted.

So people can “sniff” those packets and see what IP addresses are talking to each other using software tools like WireShark (you should download WireShark if you’re interested in this kind of thing and sniff packets running through the WiFi network you’re connected to).

However, the packets don’t contain any unencrypted information that says this IP address belongs to so and so. But IMO, people can easily socially engineer a scheme to determine this mapping. So, yes people can find your current IP address with some effort. 

I don’t think someone having your IP address is necessarily dangerous if you’re not doing anything illegal but I’d love to here someone else’s take on this. If you are concerned with privacy you should use a VPN which will mask your IP address with another IP address owned by the VPN so you’re essentially browsing the internet anonymously. 

Also, I’m not entirely sure what a private IP address in this context is? I’m aware of private and public IP addresses in the context of cloud computing but not in the context of how a laptop talks with the internet for example.

I believe everything I just said is correct but computer networking is not my strongest skill haha so if anyone thinks I misstated something or was not entirely correct please correct me. 

Hope this helps!",hcn6xn8,t3_pn4gzx,1631499892.0,False
pn4gzx,"you gave your best, that's what counts, iamthesexdragon",hd2o7xs,t1_hcn6xn8,1631797483.0,False
pn4gzx,"So your public up address is the IP address for traffic to reach your router. Your router then uses the NAT protocol to resolve a Private IP. Private IPs are the IP that your devices on your LAN know each other by. There is nothing inherently dangerous about someone knowing your public IP, if you had terribly configured security settings someone could flood your router with packets but this is basically a non issues because of default security settings. Knowing your public IP is how the internet functions, when you type in an URL your computer uses a DNS server to translate to an IP. If you want to try a little experiment try running nslookup www.google.com in your terminal, that will give you the public ip of google. Then you can type this into your browser and reach google.

EDIT
I looked at your post history and saw you asking about up logging on Wikipedia, so I figured I’d fill you in on Dynamic vs Static IPs, when you set up a domain to direct to a IP you need to use a static IP these are assigned by ISPs and never change. Your IP given to your router in a residential house is a Dynamic IP your router is given a lease on a IP this changes every 3 months ish depending on the policy set by the ISP. If for some reason you wanted to change your IP on your router settings you can force a lease renewal and get a new IP",hcn7omk,t3_pn4gzx,1631500268.0,False
pmw8rv,"Let's say you want to search an item in a linear array. That array has n elements. If you create a loop to search it linearly, the loop will go through the array 1 time. Meaning there will be n steps. So the complexity will be n. 

If you have some function with a 1-n loop inside a 1-n loop. Then there will be nxn steps. And this n^2 time complexity. 

It's the amount of significant steps performed.",hcnndo6,t3_pmw8rv,1631509068.0,False
pmw8rv,Im sorry if i dont understand your question but it looks like they explain it in the very next sentence,hclg99m,t3_pmw8rv,1631472651.0,False
pmw8rv,"An algorithm has to perform a number of steps to complete. The number of steps it takes the algorithm to complete is the time it takes. 

This abstraction is necessary because computer system will differ in various ways (cpu, OS etc.). In order to estimate the time complexity of any algorithm on any possible machine hence necessitates using a metric that is common to all available computers. That metric is steps. 

If an algorithm takes n steps to completion the time complexity is O(n). If it takes n \* n steps, for example with nested loops, the time complexity is O(n²) - because for each (n) step in the outer loop, (n) steps in the inner loop need to be performed. 

If this is still confusing you may want to watch this [FCC course on Algortithms and Datastructures](https://www.youtube.com/watch?v=8hly31xKli0). It does a great job of explaning time complexity and its calculation.",hcnyxm2,t3_pmw8rv,1631517919.0,False
pmw8rv,"What is ""complexity"" though?",hcnz1y6,t1_hcnyxm2,1631518026.0,True
pmw8rv,"Could it be that you are ovethinking the issue?

Do you have a legitimate problem to understand the metric of time complexity or are you hung up on the semantics of the term?

&#x200B;

>A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage.  
>  
>Wikipedia on [Computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)

Complexity in time complexity analyses is referred to in order to make meaningful decisions regarding the circumstances under which to use algorithms. Bubblesort for example is a slow algorithm for large quantities of data, but can be useful for small quantities because of low overhead. To arrive at that conclusion would necessitate analysing both *time* and [space complexity](https://en.wikipedia.org/wiki/Space_complexity) of the algorithm and possible alternatives.",hco4wo9,t1_hcnz1y6,1631523502.0,False
pmw8rv,"If my program runs in seconds, can I say that the time complexity is t seconds? Is it the execution time? I saw this [video](https://youtu.be/fZc3ijGM0aM), and at 1:27 he says that it's not time complexity. However, going by the Wikipedia article, I get a different sense that ""computer time"" refers to ""execution time"".",hcpkrjj,t1_hco4wo9,1631551853.0,True
pmw8rv,"Actual time (seconds, minutes etc.) is not meaningful in this regard. 

So, your algorithm runs in 1,25 seconds on your machine. How fast will it run on a university's supercomputer? How fast will it run on an old IBM T23?

This is why time complexity is estimated in meaningful steps and expressed in big-O-notation.

The video actually says exactly that. That it is a misconception to think of execution *time (as in seconds etc.)* as time complexity.",hcsexuq,t1_hcpkrjj,1631598736.0,False
pmw8rv,computer time necessarily scales with the theoretical function.,hcx9jbz,t3_pmw8rv,1631689802.0,False
pmw0cc,"Yes and No.

There are certainly approaches towards that way: Bayesian neural networks and active learning are designed to do exactly that (BNNs on their own are also very data efficient and have other advantages like uncertainty quantification). Also, there are paradigms like few-shot or one-shot learning, but I am not sure of their progress.

But keep in mind, ML is essentially statistics (on steroids). In its core, it simply uses neural networks to approximate probability density functions. So the very principles of statistics essentially dictate the more data the better the nodels.",hcmleb9,t3_pmw0cc,1631489607.0,False
pmw0cc,"The reason we train ML models with so much data is that the models get better at predictions the more you train them. Naturally, we want the models to be as accurate as possible, so we use a lot of data.",hcl747g,t3_pmw0cc,1631469165.0,False
pmw0cc,"Well said! Training data quite literally “trains” ML models. So, to develop the most accurate models, we like to train them with as much data as possible.

With that being said, some data scientists may chose to purposefully train their model with less data in order to decrease training time because the added accuracy associated with a larger training data set isn’t worth the extra compute time - totally depends on the model’s application.

Also, the above sentence implies that the more training data you have, the more accurate your model will be which isn’t necessarily true but the sentiment of the above sentence still holds true.",hclb1jp,t1_hcl747g,1631470655.0,False
pmw0cc,"I'm very new to machine learning. But I recommend looking up the curse of dimensionality. Also there are ways to measure your data to see what data is more useful than others so you don't have to use as much. I believe it's called covariance. In one example I remember from the wine data in scikit learn, a machine learning algorithm with the 13 dimensions dataset got over 90% of it's ""learning"" from just 2 of those 13 dimensions.",hclexyo,t3_pmw0cc,1631472140.0,False
pmw0cc,"No point in reducing the data you are using to train your model if you have a lot of data because as already said by other users the more data you give to the model the better it will be. 
Now, the problem arises if you have a small training set. In this case you will have a lot of variance (imprecision) which is not a desirable behaviour and to deal with it you have to use simple models thus introducing some bias.",hcnrfg1,t3_pmw0cc,1631511895.0,False
pmw0cc,"You can use Auto encoders or PCA methods. But they won't reduce the number of data points rather they will reduce the dimension of the each data point such that there is not much loss of information (typically we can tolerate about 2% loss in information, if the dimensions are reduced, but don't go below 2%, again this 2% is for information not the dimensions itself)",hcox3ln,t3_pmw0cc,1631541948.0,False
pmw0cc,Maybe with active learning.,hct6e2d,t3_pmw0cc,1631621164.0,False
pmv9wp,"""AI"" is very broad and starts at how you define intelligence and when you would call a computer ""as intelligent""
(Personally i feel like a lot of applications that are currently considerd as ""AI"" to be too dumb to be actually called like that, but thats just a whole other discussion)

The difference between ML and DL is that ML consist out of different algorithms like linear/logistics regression, K-nearer, decission trees, ... Generally every algorithm that makes a machine learn some set of rules (or thats ho i'd explain it) and also includes neural networks. DL mainly consists out of only (Deep) Neural networks (and ofc convolutional, recurrent, ... - neural networks)",hckrbhu,t3_pmv9wp,1631462743.0,False
pmv9wp,"I totally agree with your AI comment, I think the issue is, the industry cannot actually make AI at the moment, so we're constantly lowering the bar as to what AI actually is.",hcmgp4b,t1_hckrbhu,1631487512.0,False
pmv9wp,"I think so as well. Someone got a little carried away with the marketing and now everyone wants to have the buzzword on their product as well. What's next, AI vending machines?",hcmnsvk,t1_hcmgp4b,1631490763.0,False
pmv9wp,"I think it's mostly because of AI becoming a buzzword, and being a rather vague one at that. Since it became fashionable, everybody wants to splash AI around everywhere to look like they're at the bleeding edge or something. Since it's so vague, it's even harder to contain its use to something meaningful than some other term that technically might have a clear definition would.

The industry wasn't able to make AI (except in the limited sense of e.g. object recognition and other single tasks) ten or fifteen years ago either, yet AI was still not a fashionable word to use. The academic circles rather used more specific terms and I seem to remember AI being a bit of a bad and embarrassing word rather than something to make a buzz out of. The non-CS mainstream didn't care.

Also agreed though.",hco59j4,t1_hcmgp4b,1631523851.0,False
pmv9wp,"Yeah, I'd say that DL is a subset of ML that uses algorithms with ""hidden layers"" typically, meaning that some step with data-dependent tuning happens that is not the input or output layer",hckt7bj,t1_hckrbhu,1631463555.0,False
pmv9wp,"DL is just a rebranding of ANN, because ""Neural"" was a naughty word during the last AI winter.",hclnkke,t1_hckt7bj,1631475517.0,False
pmv9wp,"Also note that deep learning has the concept of a computation graph, which is how autograd is done and how we can define depth of a model as the depth of the graph. Where depth is the number of unique computation steps applied to some input.

Depth is also often said to be related to the number of layers in a model (particularly CNN), when it is directly related to the depth of the computation graph. However, in recurrent neural networks, the number of layers is much less than the depth of the computation graph.",hclxgib,t1_hckrbhu,1631479426.0,False
pmv9wp,"AI is problem to create a machine that can execute tasks that require human intelligence to do. There are many approaches to this problem like rule-based approach (like expert system, a simple regex chat bot,…), logic inference and machine learning. Machine learning is an approach to solve problems with data without explicitly programming. Deep learning is a subset of ML that use variants of Neural Network model. Other than deep network there are decision trees, linear regression, SVM,…",hckvgsx,t3_pmv9wp,1631464516.0,False
pmv9wp,"> that require human intelligence to do

I've never understood this bias. If I create an AI solution which emulates what bees or ants do, but humans would not be able to do, then it isn't AI?",hclnelr,t1_hckvgsx,1631475454.0,False
pmv9wp,"You have a great point. I think the bias come from the fact that AI is developed by human. If people can’t do some tasks, it is difficult for analyzing errors to improve performance, that make it difficult to develop a system. That make human-related tasks are more well-studied and biased toward.",hclzhnm,t1_hclnelr,1631480227.0,False
pmv9wp,"Examples of Machine Learning that are not Deep Learning:

\- Random Forests

\- K Nearest Neighbours

\- Support-Vector Machines

\- Bayesian Learning

They're ML because they are AI algorithms that improve with MORE data. They respond to it.

\--

Examples of Artificial Intelligence that are not Machine Learning:

\- Search algorithms, e.g. A\* search for shortest distance, min-max for playing simple deterministic games like chess. It basically searches a search space for the best solution.

\- Rules-based or Logic-based stuff, like what Prolog allows for. If you specify ""the Daughter of a King is a Princess"", ""Henry is a King"", ""Mary is Daughter of Henry"" it will tell you that ""Mary is a Princess"".

These mimic the intelligence humans are capable of and allow us to replace humans with computers. E.g., you no longer need a human friend to play chess, you can play against the computer.

\--

In other words, all DL is ML, but not all ML is DL. All ML is AI, but not all AI is ML.",hcl1ivn,t3_pmv9wp,1631467012.0,False
pmv9wp,">Random Forests  
>  
>\- K Nearest Neighbours  
>  
>\- Support-Vector Machines  
>  
>\- Bayesian Learning

Do you mind exampling what these are?",hcl3mci,t1_hcl1ivn,1631467833.0,True
pmv9wp,"Exampling what they are?  Do you want practical examples of where they are used? Those are Machine Learning algorithms. I think explaining them would be better than giving usage examples:

In [kNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) you look at the k nearest datapoints in your training data, then take a vote among these. Whichever is the most prevalent is what you classify your unlabeled datapoint as.

In [SVM](https://en.wikipedia.org/wiki/Support-vector_machine) you find the most influential datapoints of both labels then try to fit a line (hyperplane) which separates them.

Bayesian learning can be a number of techniques which try to estimate a probability distribution. [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) is a prime example.",hcln11r,t1_hcl3mci,1631475309.0,False
pmv9wp,"**[K-nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)** 
 
 >In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is used for classification or regression:  In k-NN classification, the output is a class membership.
 
**[Support-vector machine](https://en.wikipedia.org/wiki/Support-vector_machine)** 
 
 >In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al. , 1992, Guyon et al. , 1993, Vapnik et al.
 
**[Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)** 
 
 >In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis–Hastings algorithm.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hcln2v5,t1_hcln11r,1631475329.0,False
pmv9wp,"Example of something that is AI but not ML: https://en.wikipedia.org/wiki/A*_search_algorithm

Example of something that is ML but not DL: https://en.wikipedia.org/wiki/Support-vector_machine",hcll59y,t3_pmv9wp,1631474580.0,False
pmv9wp,"AI can include bots, like an enemy in call of duty. They have a predefined set of actions that a developer decided to use to approximate human behavior in a limited scope, but relies entirely on the intelligence of the programmer.

Machine learning is any algorithm that can find any amount of meaningful statistic. Regression is a form of machine learning, and in fact, deep learning is a specific form of auto regression.

Deep learning takes it a step further. Not sure about anything else that might be considered deep learning, but neural networks are a form of deep learning. You have a bunch of systems of a equations that approximate a bunch of linear regressions on a bunch of inputs.

These are much more complex and even explaining them broadly leads to complex descriptions, but each neuron is essentially a linear system, and if you have more than one layer, you have something called ""auto-regression"", since linear systems in the second layer have inputs that are the outputs of the first layer.

So if I can broadly, grossly describe the difference in a concise way, even if you can't understand it, I'd say that machine learning in any form of statistical regression of a linear system, and deep learning is a statistical, auto-regressive system of linear systems. While this might be an intuitive description, try writing it down, and if you end up understanding the true meaning of these two concepts, you can try to see if this description ends up making sense for you.

Hope this helps.",hcmn7lk,t3_pmv9wp,1631490475.0,False
pmv9wp,"Machine Learning is a sub discipline of artificial intelligence which has generally three categories. Reinforced learning, Supervised learning and unsupervised learning. Within unsupervised learning you find popular approaches like clustering and artificial neural networks. In simple terms; Deep learning is a artificial neural network with multiple layers/network of perceptions (think of them as functions).",hckyfln,t3_pmv9wp,1631465751.0,False
pmv9wp,"Machine learning is a wide class of algorithms that can ""learn"" things. For example, a simple linear regression algorithm is considered machine learning because it is ""learning"" a linear function that fits the data. Deep learning is another type of machine learning algorithm that uses a neural network to learn the structure of the data. Deep learning is particularly powerful, which is why it's so prevalent now. So if you need a diagram, deep learning is a machine learning algorithm, just like linear regression or K-clustering, but gets more attention because of how powerful it has proven to be",hckx78c,t3_pmv9wp,1631465251.0,False
pmv9wp,Deep learning in general consist of multiple layers and data transformation happens in the middle layers.,hcl0u6v,t3_pmv9wp,1631466739.0,False
pmv9wp,"I don't see the other comments point this out, but for me, this is actually the *main* difference between DL and other types of ML algorithms: DL allows you to combine the features engineering step in ML with the model training step, because with DL, you design layers that function as feature extractors and you train them together with the other layers that perform the actual modeling. 

Traditional ML algorithms usually require you to perform a feature engineering step first before training the model. The selected/used feature can really make or break your final model,.so it's very important to have a good set of features. DL reduces this requirement of having a ""manually tweaked set of features"", given that you have a large set of data from which you can derive your features automatically.",hclmecc,t3_pmv9wp,1631475067.0,False
pmv9wp,"DL is ML, but ML isn’t always DL.",hclqqp9,t3_pmv9wp,1631476755.0,False
pmv9wp,"Stop worrying about the ""terms"" and learn actual algorithms.",hcmyacj,t3_pmv9wp,1631495762.0,False
pmv9wp,"There are many many ways to do machine learning. SVM, decision trees, random forests, bagging, boosting, knn, k means, etc.

Deep learning is simply using neural nets with many layers.",hcnfooy,t3_pmv9wp,1631504459.0,False
pmv9wp,[deleted],hckt8zi,t3_pmv9wp,1631463575.0,False
pmv9wp,Yeah no,hckxa14,t1_hckt8zi,1631465283.0,False
pmlqqu,Yes that’s the way I read that. Imagine a computer with 2 hard drives that mirror each other. It is a fault tolerant system because either single hard drive could fail and the computer would still have all of the data.,hcix5r1,t3_pmlqqu,1631420988.0,False
pmlqqu,"Thanks for your response. I always thought ""fault tolerance"" as the way ""grace degradation"" is defined here (I didn't know the term ""grace degradation"" before), so it is so confusing to me.",hcj0iio,t1_hcix5r1,1631422954.0,True
pmlqqu,"To put it in other terms:

- Fault Tolerance: If there is a fault, the system tolerates it and continues functioning as it did previously.

- Graceful Degradation: If there is a fault, the system disables features that relied on the now faulty component. This gracefully prevents a full system crash, but degrades the functionality the system offers.",hcrciij,t1_hcj0iio,1631579027.0,False
pmlqqu,"Thanks, I get it.",hcrrym8,t1_hcrciij,1631586093.0,True
pmlqqu,Roy died on the way back.,hdqzpdk,t1_hcj0iio,1632249409.0,False
pmjoaf,"The very definition of independent:

>Two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds).

makes your conclusion **impossible**.",hcifqcp,t3_pmjoaf,1631412377.0,False
pmjoaf,"Oh ..... this bloody problem again. 

Watch this TED talk [https://www.ted.com/talks/peter\_donnelly\_how\_juries\_are\_fooled\_by\_statistics](https://www.ted.com/talks/peter_donnelly_how_juries_are_fooled_by_statistics)

The mathematics have been throughly explained here:

https://stats.stackexchange.com/questions/12174/time-taken-to-hit-a-pattern-of-heads-and-tails-in-a-series-of-coin-tosses",hcig0y7,t3_pmjoaf,1631412519.0,False
pmjoaf,[deleted],hcidjhv,t3_pmjoaf,1631411337.0,False
pmjoaf,"This is not at all similar to the Monty Hall problem, which only works because the host has knowledge of the prize location and deliberately chooses one of the losing doors. The flips in this example are absolutely independent, and conditional on a person having already flipped 3 heads, the probability of the next flip being heads is .5.",hciead2,t1_hcidjhv,1631411687.0,False
pmjoaf,"So, you using R?
Check binomial function, comes with base R.
[Binomial ](https://rpubs.com/DThurtleSchmdit/530416)",hciqufo,t3_pmjoaf,1631417692.0,False
pm4149,I'd suggest just applying the Master Theorem here,hcg5bjl,t3_pm4149,1631376267.0,False
pm4149,Maybe Wolfram alpha?,hcgdfpg,t3_pm4149,1631379805.0,False
pm3pc3,Why is a great scientist like him with groundbreaking inventions not talked about!,hcglrcb,t3_pm3pc3,1631383377.0,False
pm3pc3,"It's ok, we're talking about people important to our culture, like the ""cash me outside"" girl!

/sarcasm",hcgw4mk,t1_hcglrcb,1631387831.0,False
pm3pc3,Probably because he's black.,hcis70g,t1_hcglrcb,1631418364.0,False
pm3pc3,He's not white.,hcjb3r2,t1_hcglrcb,1631430344.0,False
pm3pc3,The tech came from a terminator.,hcgum5g,t3_pm3pc3,1631387181.0,False
pm3pc3,[removed],hcfhfeq,t3_pm3pc3,1631364728.0,False
pm38ze,"I'm probably missing context (haven't watched the whole video to see what he was talking about before) but it seems the professor posited that Bob had starting doing critical the section. And then showed the code and was trying to explain how this was Bob disobeying the logic in the code.

Spinning in this context means running a while loop without actually doing anything inside of it. This is also called busy looping.

I'm not sure what he means by ""in the loop"" unless the the critical section itself has a loop (again didn't watch the whole thing). The professor was going over that Bob read A_wants and turn==A therefore he should be ""spinning"" in the while loop waiting for those conditions to change so he can go move on to the critical section.

So, in this case, the professor was showing a logical disconnect. He probably said that Bob went on to do the critical section (an assumption) and then showed how the logic would not allow that.",hcf6wu3,t3_pm38ze,1631357706.0,False
pm38ze,"These are two entirely different things. 

Loops are logical constructs in high level programming languages that allow you to reuse code in memory by repointing the cpu back to the beginning of it.

Spinning is a concept in operating systems where two system tasks share the same memory. The point in each task where they access this memory is called the critical section because there can be huge problems if both tasks tried to use that memory at the same time. 

A common method around this is specifying the task to ""wait"" by jumping to another task until a ""lock""  is available to access the critical section. Only the task that has it can access it. Think of the lock as like a talking stick, only one person can talk at a time in a group.

When tasks that don't have the lock and just continue to wait for it, they are said to be ""spinning"".",hcfd53a,t3_pm38ze,1631362105.0,False
pm38ze,"To actually answer your question, the slide in your link demonstrates the problem with not using locks.

Take this sequence of events:

Alice runs
A_wants = true
Turn = A
Alice accesses critical section
OS context switches to Bob

B_wants = true
Turn = b
Bob accesses critical section

This is bad because you no longer can guarantee the state of the shared memory because it depends entirely on where the context switch happens. In the above example, if the context switch happens 1 line sooner, it changes Bobs outcome.

You need locks to ensure mutual exclusion.",hcff83j,t1_hcfd53a,1631363435.0,False
pm38ze,">When tasks that don't have the lock and just continue to wait for it, they are said to be ""spinning"".

doesn't this mean that the code is in the loop? if so does that mean that ""spinning"" = ""inside the loop""?",hcfn3nx,t1_hcfd53a,1631367870.0,False
plx224,I love Lexy's podcasts,hcel7gv,t3_plx224,1631339036.0,False
plx224,"Wasted? Far from it, I'd say",hcexpce,t3_plx224,1631349727.0,False
plx224,More like well invested time.,hcfueqa,t3_plx224,1631371496.0,False
plx224,ya but hes responsible for the complete mess that is latex. cool video tho,hcxcduu,t3_plx224,1631692286.0,False
plwjw8,"> basically this would entail every CLI program having one format as a common denominator

Stdout as a byte stream _is_ the greatest common denominator. You could use more structured data in many contexts, and some commands _do_ accept JSON or XML or postscript data on stdin, but finding a universally agreed upon format is going to be near-impossible.

For example, compression utilities like `gzip` accept arbitrary binary data from stdin and compress it. What if instead `gzip` expected to receive some JSON, containing metadata like the original filename, modification date, and the binary data to compress? Well now whatever utility is piping data to `gzip` needs to know what tags `gzip` will look for, and provide that metadata as JSON. That implies that any tool that's going to interface with `gzip` needs to be written _specifically_ with `gzip` in mind. With the current ""stdin is binary data"" design, I can pipe _any_ command to `gzip`, like `curl`, `tar`, some random python script, and it'll ""just work"", even if the program on the left side of the pipe was never built to connect to `gzip`.

If you want `gzip` and other compression tools like `bzip2` to be interchangeable in a pipe, then they need to use a common set of metadata fields, or else interoperable tools will have to be aware of whether they're piping to `gzip` or `bzip2` and support _both_ ""protocols"".

All in all, defaulting to ""stdin is a byte stream that might represent text or binary data"" gives the most flexibility, and lets us chain software together with the least amount of pre-planning required by software developers.",hcdusee,t3_plwjw8,1631323440.0,False
plwjw8,"You mean like this?

    STATFORMAT='{
        ""file"": {
            ""name"": ""%n"",
            ""size"": ""%s"",
            ""blocks"": ""%B"",
            ""ioblocks"": ""%o"",
            ""type"": ""%F"",
            ""device"": ""(%t/%T)"",
            ""inode"": ""%i"",
            ""links"": ""%h"",
            ""permissions"": ""(%a/%A)"",
            ""uid"": ""(%u/%U)"",
            ""gid"": ""(%g/%G)"",
            ""atime"": ""%x"",
            ""mtime"": ""%y"",
            ""ctime"": ""%z"",
            ""btime"": ""%w""
        }
    }
    '

    $ stat --printf ""${STATFORMAT}"" Table\ of\ Content\ -\ PowerVM.docx 5765-VE3 | jq '.file'

    {
      ""name"": ""Table of Content - PowerVM.docx"",
      ""size"": ""18538"",
      ""blocks"": ""1024"",
      ""ioblocks"": ""65536"",
      ""type"": ""regular file"",
      ""device"": ""(0/0)"",
      ""inode"": ""281474977302553"",
      ""links"": ""1"",
      ""permissions"": ""(600/-rw-------)"",
      ""uid"": ""(1000/linadm)"",
      ""gid"": ""(1000/admgrp)"",
      ""atime"": ""2021-09-11 02:35:26.971050800 +0200"",
      ""mtime"": ""2021-08-18 11:45:09.918067700 +0200"",
      ""ctime"": ""2021-08-18 11:45:09.973271100 +0200"",
      ""btime"": ""2021-08-18 11:45:09.668400900 +0200""
    }
    {
      ""name"": ""5765-VE3"",
      ""size"": ""0"",
      ""blocks"": ""1024"",
      ""ioblocks"": ""65536"",
      ""type"": ""directory"",
      ""device"": ""(0/0)"",
      ""inode"": ""1407374884145176"",
      ""links"": ""1"",
      ""permissions"": ""(700/drwx------)"",
      ""uid"": ""(1000/linadm)"",
      ""gid"": ""(1000/admgrp)"",
      ""atime"": ""2021-09-07 12:30:21.967662900 +0200"",
      ""mtime"": ""2021-08-20 13:23:53.512647600 +0200"",
      ""ctime"": ""2021-08-20 13:23:53.512647600 +0200"",
      ""btime"": ""2021-08-20 13:23:44.728045900 +0200""
    }

    $ stat --printf ""${STATFORMAT}"" Table\ of\ Content\ -\ PowerVM.docx 5765-VE3 | jq '.file.name'

    ""Table of Content - PowerVM.docx""
    ""5765-VE3""

    $ stat --printf ""${STATFORMAT}"" Table\ of\ Content\ -\ PowerVM.docx 5765-VE3 | jq '{ name: .file.name, size: .file.size }'

    {
      ""name"": ""Table of Content - PowerVM.docx"",
      ""size"": ""18538""
    }
    {
      ""name"": ""5765-VE3"",
      ""size"": ""0""
    }

It's possible, but not very common.
But I guess it would be welcome, if someone (like me) came up with a generic solution.
Like ... opening a new unused fd and writing to it in JSON while still outputting the regular stdout properly and then some function takes only that JSON fd handle and writes the content to a defined variable after each command, say JSON_LASTCOMMAND, or whatever...

Then you could write support functions or programs to make use of that JSON.

It would be nice.",hcdq3kh,t3_plwjw8,1631321056.0,False
plwjw8,"I wrote a tool called `jc` that allows this kind of thing by converting command output to JSON. 

Here is a recent HN thread on the concept: https://news.ycombinator.com/item?id=28266193

Here’s a similar discussion on r/Programming:
https://www.reddit.com/r/programming/comments/pa4cbb/bringing_the_unix_philosophy_to_the_21st_century/

Here is the GitHub:
https://github.com/kellyjonbrazil/jc",hcdzv1f,t3_plwjw8,1631326057.0,False
plwjw8,"I think I came across your project some time ago. Curious, have you seen the passage in Unix hater’s handbook about the string pipeline being limiting? 

It was obvious from the beginning that CLI as a text UI for executing programs is fine, but a typeless language is going to be fatally flawed.

Even Excel has data types like currency and time",hce6oar,t1_hcdzv1f,1631329686.0,True
plwjw8,"I’ve heard of it but haven’t read it yet. I’ll put that on my list!

There were many inspirations for writing `jc`, including a bit of PwerShell. I love the Unix pipeline and I see this approach as adding new possibilities for automation at the CLI without the overhead and idiosyncrasies of PS. 

`jc` already has evolved into some interesting use cases I didn’t foresee, including being used in server automation via Ansible plugin, etc.",hce86xz,t1_hce6oar,1631330542.0,False
plwjw8,"I’m reading your article. Could they have used CSV in the 70s, or is there an inherent limitation?",hcegju2,t1_hce86xz,1631335699.0,True
plwjw8,"CSV is workable for lots of things but breaks down when you need to describe more complex hierarchical relationships (nested objects) or attributes with multiple members (arrays). 

In writing dozens of parsers I’ve found that JSON was able to represent basically any output I could find, while limiting to a 2D table would be lossy or require cumbersome workarounds. 

XML and others can do the same or more than JSON, but my eyes always bleed looking at XML and JSON is not only more human readable, but it has a well supported ecosystem. All programming languages support it and there are many options like `jq` and `jello` for Bash.",hcg1uxb,t1_hcegju2,1631374778.0,False
plwjw8,[deleted],hceo35t,t3_plwjw8,1631341309.0,False
plwjw8,Not necessarily. You can have implicit conversion by an external hidden command.,hckuqvn,t1_hceo35t,1631464208.0,True
plwjw8,I believe there’s one or more projects to reimplement gnu coreutils in rust which have added —json arguments to the tools.,hcez0dj,t3_plwjw8,1631350894.0,False
plwjw8,"If I correctly understand what you mean, then check out [nushell](https://github.com/nushell/nushell), every output is a structured table.",hceqnzi,t3_plwjw8,1631343404.0,False
plwjw8,"Btw you should checkout nushell.

[https://www.nushell.sh/](https://www.nushell.sh/)

[https://github.com/nushell/nushell](https://github.com/nushell/nushell)",hche964,t3_plwjw8,1631395439.0,False
plf75c,"There are two concepts of computer. The physical device that you used to make this question needs electricity to light up pixels on its display, generate electro-magnetic radiation to communicate with the WiFi router, etc.

A computer is also a mathematical concept of any thing that takes a question, thinks for some time using pre-defined rules, and give an answer. Such a computer doesn't need electricity. It can be made with [dominals](https://www.youtube.com/watch?v=OpLU__bhu2w), [water in siphon](https://www.youtube.com/watch?v=IxXaizglscw&t=883s), or [Magic the Gathering](https://www.youtube.com/watch?v=OpLU__bhu2w).",hca5iry,t3_plf75c,1631254705.0,False
plf75c,"Thanks! I was talking about the first kind, the machine.

Why am i getting downvoted. oh well.",hca5mos,t1_hca5iry,1631254784.0,True
plf75c,In that case you're kind of answering your own question: an electrical device will require electricity to perform its function.,hcbsaik,t1_hca5mos,1631290095.0,False
plf75c,"I don’t know why you are getting so many downvotes for this comment. A physical implementation of a computer does not require electricity. You can build a completely mechanical computer that represents binary in some other way than electrical circuits either being low or high, you would just need kinetic energy to get everything moving.",hcd5rdg,t1_hca5mos,1631311098.0,False
plf75c,I remember there was a video of a guy who used water instead of electricity for a computer.,hchj03a,t1_hcd5rdg,1631397401.0,True
plf75c,Were you really asking how binary is represented in an electrical computer?  That's how I read it but everyone else seems to have taken it completely literal.  Just expand on your question a bit more so we know what you were thinking and wondering about.,hccnj3m,t1_hca5mos,1631303176.0,False
plf75c,Can you link the video for the MtG computer?,hcdsz3b,t1_hca5iry,1631322517.0,False
plf75c,"oops, seems like I had the wrong link, [here](https://www.youtube.com/watch?v=pdmODVYPDLA)",hce0y4p,t1_hcdsz3b,1631326619.0,False
plf75c,Look into Feynman's lectures on computations. He asked this question. He demonstrated that for thermodynamics reasons it takes energy to delete information. Logic gates take 2 inputs and produce one output therefore they lose information at every step. So for every computation there is a minimum energy consumption. He came up with the idea of reversible logic gates as a means of producing a device with no minimum energy consumption. They are called quantum gates and this how the idea of the quantum computer started.,hcbi03p,t3_plf75c,1631285925.0,False
plf75c,"Quantum gates are not the only kind of reversible logic gates. It's really the other way around, quantum gates must be reversible, but reversible doesn't necessarily mean quantum. The toffoli gate is perfectly acceptable classically and is reversible.

Also, the energy consumption due to Landauer's principle is just a lower bound. Practically speaking, we're consuming many many orders of magnitude above that.",hcd67yf,t1_hcbi03p,1631311313.0,False
plf75c,"Are you certain this was Feynman, and not Landauer?",hcc2cm5,t1_hcbi03p,1631294137.0,False
plf75c,yeah I'm pretty sure he just described landauers principle,hcczz6o,t1_hcc2cm5,1631308524.0,False
plf75c,"You don't need electricity. Babbage's difference engine was powered by a crank. His analytical engine, which was more general purpose, had a similar mechanical design and was considered Turing-complete.

https://en.wikipedia.org/wiki/Analytical\_Engine",hcbcw28,t3_plf75c,1631283801.0,False
plf75c,"Love this question! It brings up some really interesting computer science history.

In the early days of computers, people were still figuring out whether or not to use binary or decimal, and whether or not computers should be mechanical or electrical.

In the modern day we've mostly agreed on electric computers operating in binary, but that's a matter of convenience, not necessity.

You could totally make a binary computer with gears and cogs, it would just be very slow and would require a disproportionate level of effort to manufacture. (Any time that you can construct logic gates (ANDs, ORs, NOTs, etc) you can make a binary computer. Stand-Up maths has a great video about using dominoes to perform arithmetic using this principle: [https://youtu.be/OpLU\_\_bhu2w](https://youtu.be/OpLU__bhu2w))

And you could also totally have a decimal electric computer. ~~In fact, since decimal is so much more space efficient than binary you would save a lot of electricity! But squeezing that level of precision into your electronics would either require ludicrously high voltages or a level of precision that we still aren't able to reliably accomplish.\*~~ Computerphile has some great videos on the subject. The most relevant one is probably this one: [https://youtu.be/thrx3SBEpL8](https://youtu.be/thrx3SBEpL8)

Hope this answers the question!

&#x200B;

&#x200B;

\*EDIT: u/ iomet19 makes some great points below expanding on this and pointing out some flaws in my argument. i think some of what i said still holds, but theirs is by far the more accurate claim",hcbh4f9,t3_plf75c,1631285570.0,False
plf75c,"I would add that it is not really a matter of precision of the the voltage levels. It is about simplicity. The CMOS gates implementing boolean functions are incredibly simple, efficient and robust. Imagine designing adders for a multilevel logic system from Transistors... Assuming  you would want to use Transistors acting as a simple switch this would essentially boil down to decoding and performing the operation in binary anyway. The alternative is using the analog features of transistor circuitry, which would be so much more complicated and less robust.
Also it is not very clear that having more than two states leads to less power consumption. Operating Transistors like a switch is extremely efficient as there are essentially only switching losses involved",hcdaw3n,t1_hcbh4f9,1631313518.0,False
plf75c,[deleted],hcb5297,t3_plf75c,1631280312.0,False
plf75c,There are also examples like the https://en.m.wikipedia.org/wiki/Jacquard_machine (mechanical) or the https://www.census.gov/history/www/innovations/technology/the_hollerith_tabulator.html used in the 1890 US Census (electromechanical).,hccmpjx,t1_hcb5297,1631302824.0,False
plf75c,"Desktop version of /u/TheSkiGeek's link: <https://en.wikipedia.org/wiki/Jacquard_machine>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hccmr3e,t1_hccmpjx,1631302842.0,False
plf75c,"Somewhat unrelated but, Theoretically, One always need some source of power to perform ""computation"". see [Landauer's principle](https://en.wikipedia.org/wiki/Landauer%27s_principle). Electricity is one of the most convenient way to compute as other replies have already pointed out.",hcbs49x,t3_plf75c,1631290025.0,False
plf75c,"This is an interesting question with a fascinating answer.

The computers we are using to communicate right now use electricity to light up the screen, send signals long distances, generate the radio waves needed for wifi, etc...  but the physical act of *computation* requires none of that.  Here are real examples of physical computers that do not require electricity:

# Dr. Nim
Dr. Nim was a game from the 1960's that was sold as a real computer.  Here's a [YouTube video from Matt Parker](https://www.youtube.com/watch?v=9KABcmczPdg)

# DigiComp
In case you think Dr. Nim was just a one-trick pony, there was also a computer that could add, subtract, multiply, divide, and count in binary using nothing but marbles.  [Evil Mad Scientist makes a modern reproduction of it](https://shop.evilmadscientist.com/productsmenu/375)

# Turing Tumble
Sold as a modern game for kids to learn basics of computation, the [Turing Tumble](https://www.turingtumble.com/) is a computer in every sense of the word, and the game gives you puzzles to solve with falling marbles and various kinds of logic gates.",hcbf6ye,t3_plf75c,1631284775.0,False
plf75c,"Don't forget [Think-A-Dot](https://en.wikipedia.org/wiki/Think-a-Dot) by the same company as Dr. Nim and Digi-Comp, E.S.R., Inc. I owe it to these ""toys"" (and my parents!) to planting the seed in the late 60s of my love for computers and computation!",hcbudkm,t1_hcbf6ye,1631290935.0,False
plf75c,"I had no idea this existed!  I found this video that shows a 3-d printed replica and a link to the Thingverse file to print it.

I know what I'm doing this weekend!  Thanks random internet friend!

[https://www.youtube.com/watch?v=UyibzWVOTkw](https://www.youtube.com/watch?v=UyibzWVOTkw)",hcbx0l0,t1_hcbudkm,1631291982.0,False
plf75c,"You definitely need electricity for things like the screen, but I think you're asking about the core computer part (the CPU, or central processing unit) that does the actual computations.

No, you don't need power to build a CPU! Electricity can represent binary (on and off) and it can change state (on to off, or vice versa) super duper fast. Also, electronic circuitry can be very very small. This is why electricity is the preferred way to do it.

But you can (and people have) built computers out of other things, like water. Water flowing through a tube means ""on,"" and water not flowing through a tube means ""off."" But changing state (draining a tube or filling it up again) would be very slow compared to electricity, and also a water-tube-based CPU would be huge.

One of the first computers, [Charles Babbage's Difference Engine](https://en.wikipedia.org/wiki/Difference_engine), used a hand crank and gears to do computations.",hcbn42s,t3_plf75c,1631288002.0,False
plf75c,Note that any machine that does “computation” does in fact require *power*.  It just doesn’t have to be electrical in nature.,hcclx94,t1_hcbn42s,1631302486.0,False
plf75c,Abacus is good example of a computer that does not need electricity.,hcang7e,t3_plf75c,1631269867.0,False
plf75c,"An abacus is no more a computer than drawing 1s and 0s in the sand is a computer. It computes nothing. You compute things with your brain, using the abacus as a tool.",hcar1in,t1_hcang7e,1631272499.0,False
plf75c,People seem to disagree with this answer. A long enough abacus can perfectly be Turing complete. Just how writing 0s and 1s on the sand. The practicality of a computer is irrelevant.,hcb99en,t1_hcang7e,1631282222.0,False
plf75c,"Turing Completeness requires a rule set, not just a medium for data storage.

The fact that Magic the Gathering is Turing Complete, for example, is interesting not because you can write numbers on cards, but because the tournament rules for that game are reducable to an arbitrary Turing Machine.",hcbdnzn,t1_hcb99en,1631284131.0,False
plf75c,"That’s true, you are right. Maybe I was too quick to put that argument. 

Tho there are at least some level of arbitrariness on how to use a medium. I could stipulate a rule set on how an abacus should be used as a computer. Similarly I can use magic the gathering cards and cup holsters. Nevertheless both medium allow for a set of rules where a Turing complete machine can exist. 

I admit that I’m grasping at straws with that argument lol",hcbi0rb,t1_hcbdnzn,1631285933.0,False
plf75c,In Daniel Hillis’ book “The Pattern on the Stone” he describes a computer he made from Tinkertoys. He used the tension of rubber bands to hold values.,hcbjmre,t3_plf75c,1631286590.0,False
plf75c,"You have to be careful when using the term computer, it’s often confused with personal computer but they are not interchangeable words.

You have an electronic computer which uses electricity and transistors or in the olden days vacuum tubes to send signals.

Or you have an analog computer, whereas this might come as a surprise a simple thermometer is considered a computer, or even an hour glass. Additionally simple pressure gauges that don’t use electricity are considered computers. An analog computer computes it’s data by imitating or emulating the item or object tangible or on tangible that it is attempting to crunch data for. 

Analog computers are better at scientific and engineering problems whereas electronic computers are better at problem solving items such as finances and complex mathematical equations. Analog computers generally compute physical data such as temperature pressure change current and other miscellaneous things.

Computers have been used almost as long as human existence, just not the computers we modern humans think of. The earliest computer is tracked all the way back to Alexandria it’s name and purpose escape me right now I just remember my mind being blown when I read that in one of my computer science courses.",hccgfa9,t3_plf75c,1631300126.0,False
plf75c,ich Chào 50676 zzzz6zzzz666554555t6,hccwx40,t1_hccgfa9,1631307174.0,False
plf75c,No. Computers also need electricity to run the cooling system required because flipping 1s and 0s generates heat 😉,hccgllw,t3_plf75c,1631300203.0,False
plf75c,[A Bunch of Rocks](https://xkcd.com/505/),hcd0pyb,t3_plf75c,1631308847.0,False
plf75c,No. Computers use logic. Specifically Boolean logic. Logic is independent of electricity.,hcdzrwm,t3_plf75c,1631326012.0,False
plf75c,"High or Low is the most abstract idea of a binary computer.

Ex:

High or low [voltage]

High or low [pressure]

High or low [frequency]

High or low [light spectrum]

Ect.",hce7jgq,t3_plf75c,1631330174.0,False
plf75c,"Yes, a 1 is a certain voltage range and so is 0.

https://learn.sparkfun.com/tutorials/logic-levels/all

Edit:  lol wtf.  Reddit is getting dumber and dumber by the day.  He didn't ask if a computer had to be electric.  Also,  a monitor is not a computer.  Also,  a computer does not Inherently need a cooling system and even if it did, it doesn't have to be electric.",hcazib3,t3_plf75c,1631277516.0,False
plf75c,I think one other reason is the simplicity of reasoning with binary,hcbz5r3,t3_plf75c,1631292833.0,False
pl4uz4,"I imagine designing the basic gates would be much more difficult because you wouldn’t be able to use Boolean logic anymore which would completely change how computer architecture is made. Also you have to have tighter tolerances for the circuits. Right now it either on or off, but to do that the circuit would have to be able to have 10 states. 

I’m also not sure what being in base 10 would do that is better then base 2. Besides being a little more intuitive to the average person",hc7xqzz,t3_pl4uz4,1631216741.0,False
pl4uz4,"Just in theory being able to represent more than two symbols with a transistor would require less logic for the same application. 

Say an address bus to address 1024 words would need only 6 ""tri-bits"" and only 5 quad-bits instead of the now used 11 two-bits. 

Same for representing numbers. 

I don't know about ""combinatorial"" logic. Like we wouldn't have basic AND/OR gates. This would open up more possibilities for logic circuits, maybe someone would find an optimized multiplication algorithm, or a way to simplify logic diagrams and create more efficient muxes. 

However the reality is that we don't know how much is really saved if the physical implementation is way more complex. And by now I think someone would have already found one. 

Instead researchers are now trying to find new computational paradigms to drift away from the standard binary system based on transistors, think about photonics computing which embeds the numbers in some wave property (or smh like that), or quantum computing etc..",hc83jx6,t1_hc7xqzz,1631219084.0,False
pl4uz4,"IMO theoretically if you'd have base10 transistors, you could reduce the heat from computers by a lot.",hc8br2c,t1_hc7xqzz,1631222436.0,False
pl4uz4,"Transistors are inherently analog devices, actually all circuits are. Digital is just an abstraction you make by operating them at only 2 voltages. But transistors first came around because we needed a way to amplify analog audio for telephone lines. The more values you break your voltage range up into the more problems you run into, noise, resolution, complexity, speed, density, etc etc. Circuits went digital specifically to avoid these problems.",hc98r57,t1_hc8br2c,1631237180.0,False
pl4uz4,"how would that happen?

i'm thinking if anything the heat would increase as you needed higher and higher voltages to get to higher and higher states

0.7v for every step, probably higher because tolerances arent perfect",hc8kj2t,t1_hc8br2c,1631226204.0,False
pl4uz4,Fuzzy logic gates could work for some stuff but its useless for math afaik,hcaclrm,t1_hc7xqzz,1631260425.0,False
pl4uz4,"So I'm just reciting this from memory so I may have some ideas wrong. In the early days of computers, this was attempted. Not necessarily base 10 but other bases.I think the problem mainly has to do with difficulty dealing with sensitivity of transistors and the exactness of the electrical system.

In short, it is very difficult to engineer a cpu and voltage system that can deal with a large number of different volatages that are so close together. This can cause large errors in the system and the overhead for error checking is much larger than a binary system. The larger the base, the more complex the system becomes. Idk the growth but I imagine it is exponential.  


edit : much better answer [https://www.reddit.com/r/askscience/comments/254wpp/why\_do\_computers\_still\_use\_binary\_instead\_of\_a/chdqkl1?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/askscience/comments/254wpp/why_do_computers_still_use_binary_instead_of_a/chdqkl1?utm_source=share&utm_medium=web2x&context=3)",hc7ydxa,t3_pl4uz4,1631216996.0,False
pl4uz4,You can. It would be needlessly complex and thus horribly expensive. Also very prone to errors (voltages may fluctuate).,hc80rc4,t3_pl4uz4,1631217949.0,False
pl4uz4,signal noise would be constantly flip digits all over the place. processors don't want base 10. they have 1 finger and many hands.,hc7zdl0,t3_pl4uz4,1631217391.0,False
pl4uz4,If I remember right this was tried before with the ENIAC. The issue was that getting accurate readings for all the different voltages was very difficult as well as inefficient and as a result it was abandoned.,hc813vs,t3_pl4uz4,1631218092.0,False
pl4uz4,"If your desire is reducing floating point errors, why not base 120? Or some other highly composite number?

With binary the only thing that matters is if there's a path with all transistors open from the voltage supply to whathever device reads the cpu output. There's no ""reading"" of voltages involved. But you'd have to have some analog components to read the voltage levels if the base is bigger than 2.

Not an EE but the main issue afaik is that analog-ish (for n -> inf) systems are much more error prone or too slow.",hc7ys6t,t3_pl4uz4,1631217155.0,False
pl4uz4,Let's go base 5040 while we are at it.,hc8be4r,t1_hc7ys6t,1631222288.0,False
pl4uz4,"It isn't quite the same question as yours, but about 20 years ago I was involved in a project to perform high speed calculations using an op-amp based analog computer. It could have been tuned and gated to those step values. The primary problems we had were conventional RF issues. 

If I was given the same problem set today, I would spend their money attempting to solve their problems with an optical or quantum approach.",hc81bde,t3_pl4uz4,1631218177.0,False
pl4uz4,"> computationally-desirable   

Why is this computationally desirable? Anything that can be computed can be computed on a Turing complete binary computer.   
If you're thinking of floating point errors, you're going to get those with a decimal computer too: any finite precision representation has numbers it cannot represent accurately. For decimal, one example is 1.0/3.0.",hc9q39p,t3_pl4uz4,1631245173.0,False
pl4uz4,"This makes everything more complex, which is just the opposite of the basic design principle of systems that you should keep your designs simple as possible.",hc82x4m,t3_pl4uz4,1631218827.0,False
pl4uz4,"Here's some stuff written by Von Neumann 

from ""First Draft of a Report on the EDVAC""

https://web.archive.org/web/20130314123032/http://qss.stanford.edu/~godfrey/vonNeumann/vnedvac.pdf

Page 6 (Page 14 of the PDF)

> The element in the sense of 4.3, the vacuum tube used as a current valve or gate, is an all-ornone device, or at least it approximates one: According to whether the grid bias is above or below
> 
> cut-off, it will pass current or not. It is true that it needs definite potentials on all its electrodes
> in order to maintain either state, but there are combinations of vacuum tubes which have perfect
> equilibria: Several states in each of which the combination can exist indefinitely, without any outside
> support, while appropriate outside stimuli (electric pulses) will transfer it from one equilibrium into
> another. These are the so called trigger circuits, the basic one having two equilibria and containing
> two triodes or one pentode. The trigger circuits with more than two equilibria are disproportionately
> more involved.
> 
> Thus, whether the tubes are used as gates or as triggers, the all-or-none, two equilibrium,
> arrangements are the simplest ones. Since these tube arrangements are to handle numbers by means
> of their digits, it is natural to use a system of arithmetic in which the digits are also two valued.
> This suggests the use of the binary system.
> 
> The analogs of human neurons, discussed in 4.2–4.3 are equally all-or-none elements. It will
> appear that they are quite useful for all preliminary, orienting, considerations of vacuum tube systems
> (cf. {6.1, 6.2}). It is therefore satisfactory that here too the natural arithmetical system to handle
> is the binary one.
> 
> 5.2 A consistent use of the binary system is also likely to simplify the operations of multiplication
> and division considerably. Specifically it does away with the decimal multiplication table, or with
> the alternative double procedure of building up the multiples of each multiplier or quotient digit
> by additions first, and then combining these (according to positional value) by a second sequence
> of additions or subtractions. In other words: Binary arithmetics has a simpler and more one-piece
> logical structure than any other, particularly than the decimal one.
> 
> It must be remembered, of course, that the numerical material which is directly in human use,
> is likely to have to be expressed in the decimal system. Hence, the notations used in R should
> be decimal. But it is nevertheless preferable to use strictly binary procedures in CA, and also in
> whatever numerical material may enter into the central control CC. Hence M should store binary
> material only.
> 
> This necessitates incorporating decimal-binary and binary-decimal conversion facilities into I
> and O. Since these conversions require a good deal of arithmetical manipulating, it is most economical
> to use CA, and hence for coordinating purposes also CC, in connection with I and O. The use of CA
> implies, however, that all arithmetics used in both conversions must be strictly binary. For details,
> cf. {11.4}.

it keeps going, too",hc8u4hu,t3_pl4uz4,1631230560.0,False
pl4uz4,"Modern SSD can store a bunch of bits per cell by using multiple voltage levels.

For storage this work yet, but I am not aware of transistors beeing able to use multiple voltage levels.",hc828g1,t3_pl4uz4,1631218552.0,False
pl4uz4,https://en.m.wikipedia.org/wiki/Multi-level_cell,hc9tgr7,t1_hc828g1,1631246902.0,False
pl4uz4,"Desktop version of /u/vitiumm's link: <https://en.wikipedia.org/wiki/Multi-level_cell>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hc9ti2t,t1_hc9tgr7,1631246921.0,False
pl4uz4,"Base 10 is only more computationally desirable for us is because we have 10 fingers.  0 and 1 are irrelevant to a computer.  There is only a high state and a low state, either of which can represent a 0 or 1 (active high/low).  All outputs must be either pulled ""up"" to a high state or ""down"" (.1v) to low state otherwise the output is said to be ""floating"" and is undefined.  I can't imagine the complexity of having to build circuits with like 10 Zener diodes to represent 10 different states and then trying to do anything complex.  There were Analog computers which  can do some pretty interesting mathematical stuff in ways that discrete computers can't.",hc8t4ck,t3_pl4uz4,1631230102.0,False
pl4uz4,"Yes. But the most efficient way to do it would be to build a binary computer and use it to simulate base ten computer, which is essentially what we do now.",hc9fl1n,t3_pl4uz4,1631240187.0,False
pl4uz4,I don't see how base 10 is computationally-desirable,hc9hafg,t3_pl4uz4,1631240960.0,False
pl4uz4,"for finance applications, you would need floating point values accurate to the 10^-25 or something crazy because 1s and 0s can only represent a decimal so perfectly without slight deviation. when multiplied over hundreds of times, it can suddenly become a much bigger deviation.",hc9i2rn,t1_hc9hafg,1631241317.0,True
pl4uz4,"Technically it's doable, in one of my college courses we designed a non binary computer as a thought exercise (i did enough of a hardware experiment to prove out having multiple states works). It mostly relied on the fact that tech at the time was 5 volt and the ""0"" and ""1"" values used the voltage ranges that were defined in the 1950's. I forget the exact values, let's say it was 0-1 volt is a ""0"" and 3-5 volts is a ""1"". The gap was to allow for tolerances, circuits are never precise and you need a little gap to accurately determine that the value has changed. Needless to say that circuits are more precise than they were in the 1950s, so the tolerances could be tightened and more states introduced in between.

The hard part is how to use those extra states? All of the logic circuits and software have been designed around only having 2 states. Modern computers are built off of >70 years of research and development that is all based on the concept of there being only two states. True computers could be used to  speed up the process but it would take a significant estimate and effort.

A similar example to give some color, let's say it was decided that 26 letters in the alphabet were not enough and we need to add one more:

What logistics would be necessary to add it?
Which languages would it get added to?
How would we write it?
Where in the alphabet would it go?
Do the existing words need to use it?
How do we pronounce it?
Where does it go on the keyboard?
....

Sure adding the letter isn't hard, but getting it into the language and actually used correctly would take alot of effort and time...",hc93wz4,t3_pl4uz4,1631235026.0,False
pl4uz4,"Probably yes, but **why**",hc9ub7y,t3_pl4uz4,1631247364.0,False
pl4uz4,In theory. In practice it doesn't work out so well because its difficult to store and transfer a base 10 value. If it was electrical you would need 10 voltage levels instead of 2 (or 5 if its balanced base 10). Anyway you should google setun which was a ternary (base 3 ish its more like roman numeral base 2) computer,hcx8on4,t3_pl4uz4,1631689108.0,False
pl4uz4,investigate binary coded decimal,hc7yztp,t3_pl4uz4,1631217238.0,False
pl4uz4,I thought this was the concept behind neural networks.,hc7xods,t3_pl4uz4,1631216711.0,False
pl4uz4,Imagine you are building an adder circuit that adds together a single bit from two different numbers along with a carryover bit from the last bit addition of the two numbers. You can pretty easily draw out a quick flowchart on paper to see how your circuit would work. One branch if the first bit is 0 and one if the first bit is 1 then each of these branches has two branches for the second bit and each of those four branches has two branches for the carryover bit which gives you eight possible results. It might take you a couple tries but it's not terribly difficult to arrange some logic gates in a way that compares these three bits and and gives the correct result every time. A base 10 adder on the other hand is a lot more complex adding together two digits gives you one of 100 possible results on your flowchart (200 if you have a carryover of 0 or 1) or you would have to allow a greater range of values like 0.1-1.8 to allow 9+9 to be found and not only would your adder need to be sensitive enough to correctly detect the voltage level (and probably shielded from interference) it would need to be able to perfectly raise and lower the voltages.,hc8425m,t3_pl4uz4,1631219294.0,False
pl4uz4,"First your circuit design would be a nightmare. Logic gates in binary are straight forward and trying to build that in decimal would be harder. 

Then when you consider the actual electricity it gets worse. You have to have some tolerance for noise and as your voltage ranges get smaller, you get less tolerance for noise and therefore more computation error.",hc8898a,t3_pl4uz4,1631221001.0,False
pl4uz4,"I could maybe see a memory solution that yielded higher storage density, but I think logic will always be best organized in base-2 (or at least anything designed by our brains.) 

Binary is more than two numerical digits- it's 'yes' and 'no.' The true/false polarity is the atomic unit of all perception. Light and dark, good and bad, on and off, etc. You can model all information / decisions in true and false, but additional states only make sense in a small/specialized subsets of scenarios.

But idk anything.",hc8bc35,t3_pl4uz4,1631222265.0,False
pl4uz4,Just thinking of designing the firmware for this makes me wanna punch air,hc8bym0,t3_pl4uz4,1631222523.0,False
pl4uz4,"Can be done and such codes are used in some circumstances but for computers it's a stupid idea. Getting clean clocks etc. at high frequencies is hard enough you really don't wanna introduce more voltage levels and it'd make ALUs way more complex. Also why do you think base-10 is more computationally desireable? because you can express 0.1 etc. in it exactly? You'll run into the same problems just with other numbers and generally speaking base-2 isn't a problem from a numerical standpoint. For stuff like monetary values etc. you don't wanna use floats in any case, even if they were base 10",hc8l8l8,t3_pl4uz4,1631226519.0,False
pl4uz4,"Yes, the first electronic computer (ENIAC) was base10.

The obvious problem is that analog signals are harder to keep stable. If you have a base 2 5V system, you can have 1 anywhere from 3.5-5V and 0 from 0-2.5V. If you had 10 levels, any fluctuation in 0.2V would cause issues and then we haven’t talked yet about overflow. Also higher voltages don’t scale down very easy. Hence why some components are now down to 0.2V, having them at 0.5V would break down the small nm chip designs.

There are currently some ternary proposals. There are also encoding mechanisms that encode more data in a wave (typically optical, but 10G+ networking over copper is doing it as well) and can be considered analog base(n) devices.",hc8mi59,t3_pl4uz4,1631227091.0,False
pl4uz4,"Probably it would be very difficult technically 
 
I think (I not go too far on this topic) binary levels  0 and 1 are not always 0v and 1v (in a 0-1v scale) there are probably fluctuations, transistor leaks, signal degradation, and noises. 
I remember working in something (not remember what) where in a 5v scale,  <1v was 0 and >1 was 1 (or similar). 'On/1' was never trigger under 1v)

Probably there are a threshold in the ""binary computers"", 

Have a base10 computer with that class of exact voltages enough to be exact like a binary computer would be probably an big challenge and probably only will run good enough in scenario specially created for run that computer.",hc8n0aw,t3_pl4uz4,1631227320.0,False
pl4uz4,Complexity of computer circuits rises as the “base” rises.,hc8oivb,t3_pl4uz4,1631228008.0,False
pl4uz4,I believe babbage’s difference engine was in base 10. So yeah you could. Not sure how you would do this with transistors though.,hc8y9ga,t3_pl4uz4,1631232467.0,False
pl4uz4,On the transistor level how would this work?,hc9flmi,t3_pl4uz4,1631240194.0,False
pl4uz4,"You can build such a thing, but Claude Shannon proved that the most efficient way to encode information is by way of binary.  You could think of it as simplifying a fraction or expression; it's still the same value when it's not simplified, but there's a tighter, more elegant form for it, the simplified form, and when you're going to actually be building that thing in hardware which uses energy and produces heat, then you want it to be in the minimal form possible.",hc9wqig,t3_pl4uz4,1631248765.0,False
pl4uz4,Literally asked the same question in my CAO Class to my professor.,hcad5xj,t3_pl4uz4,1631260928.0,False
pl4uz4,"1. General problems with floating point numbers exist cause of your computer memory isn't infinite and can't store numbers those so huge that they are close to infinity, also your processor's clock rate isn't infinite too so it can't process huge numbers faster then seconds-minutes-millions-of-years-etc, and your life isn't infinite  too (eternal) to wait for it to be done.

2. Computer's memory and proceeding system doesn't work with bits with only numbers, they work with other data representations too (like strings, boolean and their children). And 10 based system won't help you to handle them better, moreover especially for boolean representation you will spend more memory/processing power in times then with 2 based system, and btw your program work with boolean values more often then you in your code (remember, every conditional based operation works through transformation to a boolean value).

P.s.: the only reason to think about using 10 based system as a superior way to work with numbers (and ofc with no care about other data representation forms) is that we, humans, as a species, have 10 fingers on our hands, and we used to use'em as a primary way to represent numbers in our life.",hcaec88,t3_pl4uz4,1631261995.0,False
pl4uz4,"The bigger problem I see is in transistors, they would become much more sensitive to quantum tunnelling, and we would need to make them bigger to increase tolerances which would beat the purpose. Also having a base 10 transistor would not benefit a lot of logics, so you would need the same number of transistors to achieve the same logical operation (with few exceptions), but your transistors are exponentially more power hungry, so you are losing a lot more efficiency and performance per mm^2 of silicon than you would be gaining.

I could see it maybe if we make optical transistors and use different wave lengths, but that would still be 2^n transistors, not a base 10. But I am not an expert in optics so don't quote me on that...

No idea why you would want a base 10 computer anyway. You can do any calculation with a base 2 computer, and you would be losing efficiency in more places than you would be gaining efficiency.",hcaf48n,t3_pl4uz4,1631262714.0,False
pl4uz4,"Its possible and its already used in many systems we use today, most of the high capacity high speed solid state storage systems use multiple voltage values in a single point in the semiconductor to store more values and access them faster. Its obviously much more complex than the simplified version but essentially they work in base n systems and transform the data to base 2

However using this in actual processor would be too error prone, hard to design and expensive, I don't event talk about architectural changes needs to be done to the software. Overall its just easier to make base 2 systems smaller rather then implementing base n systems",hcb2dnx,t3_pl4uz4,1631279004.0,False
pl4uz4,"Your guys sound so smart. I applaud you. I just started my cs major this semester, i hope my knowledge base expands to what you guys are talking about in 2 to 3 years",hcbex6s,t3_pl4uz4,1631284660.0,False
pl4uz4,"I think this is actually a very interesting question.

Let's assume we have a ""magic"" transistor that can give you very accurate readings from 0 to 9 with very low cost like what we have today. With that, instead of the Boolean operations we use today, we may have entirely different things. For example, an ""add"" gate which adds two readings, which is similar to OR in Boolean. A ""min"" gate and a ""max"" gate, maybe. And maybe we can use these to create a 10-base adder, and more. Although I am not sure if we can really build those gates with an accurate transistor, but it's fun to think about! But to be honest, using binary operations might just be easier.",hcbsu55,t3_pl4uz4,1631290317.0,False
pl4uz4,"This idea was also abandoned because of electrical degredation. Over time as electrons are pulled from any metal in the system the voltage very gradually diminishes the longer it travels through the system potentially changing ""numbers"" because of increased resistance from an older system.",hcbyvhc,t3_pl4uz4,1631292718.0,False
pl4uz4,"One of the biggest reasons computers use binary is to reduce noise errors. Due to external influences, sometimes bits can spontaneously flip. These are corrected by hamming codes but with higher bases dealing and correcting them becomes a nightmare",hccaxha,t3_pl4uz4,1631297782.0,False
pl4uz4,"Basically the reason we use base 2 is because a transistor is on (1) or off (0). For a base 10 PC, assuming no major redesigns of current architecture, there would need to be some kind of 'translator module' that takes the binary and converts it to base 10. Or: cut out the middle man and just do base 2.

TL;DR: Yes, you could make a base 10 computer. I could also start my own religion. Will l? Hell no.",hccdyxu,t3_pl4uz4,1631299078.0,False
pl4uz4,"Yes but you would have to overlay it to the existing binary system which already encompasses base 10 so it wouldn't be to any advantage.

Native base 10 would not work because that isn't how our computers understand voltages.",hc94bl6,t3_pl4uz4,1631235211.0,False
pkpdat,Have you taken languages and automata prior to reading it? I'd consider goving it a try but I'm sure it can get complex pretty quickly,hc5ckn7,t3_pkpdat,1631163354.0,False
pkpdat,"I took logic, discrete structures and theory of computing while I was in school. They helped, but I think you really only *need* high school level math to get the gist of what's going on. The proofs section goes crazy with predicate logic and theorems, and it really just came down to a lack of willpower on my part.",hc61p83,t1_hc5ckn7,1631185324.0,True
pkpdat,"In general you don't want to read any paper on formal logics written before (I guess) the 1970s or something like that. The typesetting is horrible, the symbols are confusing and it justs looks like a childs idea of complicates mathematics. The names of concepts are all very weird and so on

I'm lucky for decades of teaching experience making all of this much more presentable. Same applies to Computability Theory. Turing's paper is not that bad (you can recognize his definition of a TM) but try reading something like Church's original exception.",hc7waf8,t1_hc61p83,1631216144.0,False
pkpdat,I read Turing paper and had a similar experience. Reading the same page over and over again trying to under stand stuff. but yeah I skipped the Universal Turing machine construction cause it was too hard to understand.,hc6000p,t3_pkpdat,1631183995.0,False
pkpdat,"Yeah, that's why I'd recommend reading this book instead of the paper by itself. When he wrote the paper, Turing of course assumed that his readers would have a massive amount of background knowledge. Also he makes a fair amount of errors and unclear statements that can make things even harder. The author holds your hand through practically every sentence though and it really helps.",hc6286z,t1_hc6000p,1631185714.0,True
pkpdat,"I had the exact same experience. I got another book about theory of computation to presumably ease my way into the proof part but I ended up doubling the struggle. All in all, loved the book. The manner in which it introduces you to some concepts and works of some mathematicians (and of Turing's himself) is almost poetic at times.",hc8bsz1,t3_pkpdat,1631222458.0,False
pkpdat,"Man, thanks for mentioning it.

I must get this book so that I put some adventure and excitement in my life 😁.",hccl15x,t3_pkpdat,1631302108.0,False
pkpdat,"I know, I too live for the thrills that can be found in obscure, hard to read text books!",hcd0cih,t1_hccl15x,1631308686.0,True
pkn21x,"See if you can get a club started for FIRST robotics. I'd highly recommend it because it introduces a variety of ideas and knowledge to everyone with an engineering itch. You will have programmers, designers, 3D modeling, machining, electronics etc and everyone is simply taking the opportunity to learn and compete with other schools.

If you were looking for programming strictly, leet code sessions and mini projects like making your own lan chat, games, etc might be good.",hc4trsi,t3_pkn21x,1631152602.0,False
pkn21x,I wouldn't recommend leet code for high schoolers. It might be a little too much tbh,hc54ehu,t1_hc4trsi,1631158092.0,False
pkn21x,"I was that kid that ate everything CS so I always enjoyed the unnecessarily complicated challenge lol.

I understand leetcode is all over the place in difficulty so maybe you're right, I was thinking as a group or club, taking on a single problem that's intended for 10/15ish min or something like it might serve well.

I know there's programming competitions that are intended for HS students in a similar in idea. They basically have you do traversal algorithms, sorting, etc but in a more creative and interactive way - solving a maze, organizing cards etc. A whole booklet of problems that should be doable in a handful of hours.",hc5efpj,t1_hc54ehu,1631164671.0,False
pkn21x,"The easy problems arent too bad but yeah you're going to need some slightly more advanced data structures if you're going to accomplish them without brute forcing every single one. But I guess everyone starts with brute force (and it's actually a good way to start designing a solution before moving on to an advanced one)

Downside of LeetCode for a beginner is they might take a peek at a hard problem like Cherry Picker and choose to never write another line of code ever again lmao 

There are other sites that are better for beginners - my favorite when I was learning Java was a site called CodingBat, beginner problems with a super simple interface.",hc6oy8w,t1_hc54ehu,1631198082.0,False
pkn21x,Came here to suggest this. I’m a current student at FSU online and they have a robotics group. They sent me an Arduino kit and it’s been pretty fun so far,hc63oy6,t1_hc4trsi,1631186755.0,False
pkn21x,I got voted in as club president for promising everyone we could play LAN games. Then I hosted workshops to build the PC’s we needed so we looked legit. It was awesome.,hc4zh7z,t3_pkn21x,1631155474.0,False
pkn21x,This is honestly a pretty cool idea. The problem might be the budget,hc54k72,t1_hc4zh7z,1631158176.0,False
pkn21x,"We charged non-members to play games, and gave out game credits to anyone donating random PC parts 😀 out of a pile of trash we managed to get 4 working PC’s, enough for 2v2 Quake lol.",hc5qoz8,t1_hc54k72,1631175342.0,False
pkn21x,Robotics. Get an Arduino or some like thing and automate some stuff!,hc4u43h,t3_pkn21x,1631152767.0,False
pkn21x,"You could train for the USACO if you want to get into competitive programming. It's a nice resume booster for college/job applications, and for good reason

http://www.usaco.org/index.php?page=training",hc4v21u,t3_pkn21x,1631153231.0,False
pkn21x,Make a web app or a game or really whatever y’all are interested in that can keep you motivated to make sure the club survives. I’ve seen a lot fail because it wasn’t fun enough to keep interest. Adapt and focus on making it enjoyable,hc523qa,t3_pkn21x,1631156846.0,False
pkn21x,Write programs to play chess and compete them against the members of your schools chess club.,hc51fs9,t3_pkn21x,1631156499.0,False
pkn21x,"Ooh yeah, thats a great idea. Or even better, Play them against each other.",hc5gouq,t1_hc51fs9,1631166332.0,False
pkn21x,If you’re more interested in the cyber security side you could try and look at the PICOCTF challenges and try and compete in it.,hc4w9se,t3_pkn21x,1631153840.0,False
pkn21x,CTF maybe?,hc5j7ce,t3_pkn21x,1631168376.0,False
pkn21x,"You are going to get a lot of answers that say robotics, and or embedded and those are really good suggestions as they teach the principles of development without adding in all the layers of cruft that modern web dev and other disciplines bring to the table.

Other good ideas are Crypto dev, but that too is mired in steep learning curve to start accomplishing things and brings a host of unfamiliar concepts. Which creates a wall to understanding when introducing people to concepts you want to reduce that wall as the quicker they hit that ahah point, the sooner they go from just receivers of information, to organizers of information in a way that they can didactically learn.

Knowing that, my recommendation would be focus on hacking, why hacking because it inverts learning, you don't need to know everything about computing and programing and theory and best practices and ad nauseum to just make a todo list, you just need a computer, a vulnerability and a script. You just need to understand how one exploit works and that builds a deep understand of how the language, the compiler, memory, etc. etc. work. Understanding how to break things, only involves breaking it and then asking why. It makes it very digestible.

Other reasons for hacking:

\#1 you are talking about high school, social credit is huge in high school, how many movies have been made about robotics clubs? How many have been made about hackers? Who was cooler? Baseline is, you are going to be a more attractive club and that is what you want to do is attract people. Hacking will draw the people that would naturally be attracted to a robotic club, but it is also going to draw a more diverse set of people, this is beneficial because it will bring diversity of thought.

\#2 Hacking (as well a robotics) is easy to gamify, there are huge tournaments and war games that teams can compete in when hacking. Unlike robotics there are a lot of online events that hackers compete in, that do not require physical presence this gives more opportunities to compete in group events, rather than just the physical meetup opportunities.

\#3 there are hacker bounties which pay real money so there are endless sets of challenges to work on as your club levels up.

\#5 most CS disciplines never teach computing from this angle, if they do, they just glance over it. It inverts the way you think about computing and that can be very beneficial even if you pursue a normal career in computing.

\#6 it is by far the most approachable way to learn computing, you do not even need to know how to script to start. But will learn to script little by little, then how compilers and interpreters work little by gaining small digestible pieces of knowledge along the way, until one day, a full picture is developed.

Links:

[https://www.hackthebox.eu/](https://www.hackthebox.eu/)

[https://hackaday.com/](https://hackaday.com/)

[https://www.hackthissite.org/](https://www.hackthissite.org/)

[http://breakthesecurity.cysecurity.org/](http://breakthesecurity.cysecurity.org/)

[https://www.hacker101.com/](https://www.hacker101.com/)

[http://www.securitytube.net/](http://www.securitytube.net/)",hc69gcr,t3_pkn21x,1631190385.0,False
pkn21x,"Try building stuff with logic gates. It'd be a great learning opportunity too! You guys could also try programming, and projects in that language (make sure it's beginner-friendly, like Python or C).",hc4s5hu,t3_pkn21x,1631151825.0,False
pkn21x,"Depends on people's interests. If you're looking for something intensive, FIRST robotics is an option. It would take a lot of work to find a mentor, funding for parts, and learning not only programming but also mechanical, electrical, and business skills. 

Less intensive options are hackathons, building web applications, mobile apps, virtual reality games, normal video games, data science/machine learning competitions (i.e. kaggle contests), or smaller robotics projects like drones or arduinos.",hc4wf5k,t3_pkn21x,1631153914.0,False
pkn21x,[Build an 8-bit computer from scratch](https://eater.net/8bit),hc6xx5e,t3_pkn21x,1631201941.0,False
pkn21x,"Do you happen to be near Loudoun County, VA?  I run the non-profit LoudounCodes and volunteer in several high schools.  I’d be willing to help get you off the ground, even if is just to throw some curriculum your way and be a guest speaker for you.  I could do that over a video call too…",hc4uvl6,t3_pkn21x,1631153141.0,False
pkn21x,"I'm not sure if it exists, but are there contests where you compete against clubs from other schools?  It seems something like that could give a collective goal to work towards.

Does the club have a mentor or sponsor from the school faculty?  A teacher or someone who could help give encouragement and guidance.",hc506d4,t3_pkn21x,1631155840.0,False
pkn21x,"The National Cyber League is a good semi annual competition you could work towards. There are two individual rounds and a team round ever 6 months or so. Yhe best part for you is that it's all for students. They have tons of info on their page and I'm sure if you contact them they will be able to help you get off the ground even if it is in a different direction. I wish I'd had something like this jn my high school, GOOD LUCK!",hc5hy5r,t3_pkn21x,1631167331.0,False
pkn21x,"Make like a video game based on the school (like have the characters be teachers and stuff), and have the older members teach the newer member before they graduate so it can always be worked on. It’s more of a game dev club but it’s something to do. Maybe even find a way to get the game set up in an open area at school and let people play it in between classes. The only issue would be getting permission and following all the guidelines placed by the school",hc5lcj0,t3_pkn21x,1631170240.0,False
pkn21x,You can look into national competitions that can win you prize money. I know there are some organizations that offer these types of things,hc5lf04,t3_pkn21x,1631170303.0,False
pkn21x,Joined a club in a uni in Norway that runs weekly programming contests on kattis. You can use the problems on the site and make your own contests with it.,hc5ormo,t3_pkn21x,1631173422.0,False
pkn21x,"Do self-modifying MIPS code 

[https://inst.eecs.berkeley.edu/\~cs61c/resources/MIPS\_Green\_Sheet.pdf](https://inst.eecs.berkeley.edu/~cs61c/resources/MIPS_Green_Sheet.pdf)

[http://courses.missouristate.edu/KenVollmar/MARS/index.htm](http://courses.missouristate.edu/KenVollmar/MARS/index.htm)

have fun and make sure to check the ""self-modifying code"" setting in the settings menu",hc5qbsz,t3_pkn21x,1631174972.0,False
pkn21x,Consider a generative art club. Amazing stuff. When you feed real data into it you get fascinating things too. Will set you all up for careers in data visualization. Anyone who can do art and science at the same time is invaluable in the workplace.,hc5xq43,t3_pkn21x,1631182077.0,False
pkn21x,Help each other learn unity3d and collaborate on building a game.,hc6ag9b,t3_pkn21x,1631190952.0,False
pkn21x,"Could get a team together and learn cyber security
 through Capture The Flag.  Here is the link to high school tournaments https://picoctf.org/",hc6i4hv,t3_pkn21x,1631194979.0,False
pkn21x,"I recommend you avoid long lectures. No one is going to be interested in more school, in school. Keep things short, with very short, simple goals. Just get everyone to download a compiler and IDE, get them to copy-paste a ""Hello, World!"" program, and that alone is a victory for the day. The room is going to be loud, it's going to be busy, half the room is going to be there just to socialize. Work with the chaos, not against it. Those who bring their laptops and get their first programs to compile, they're going to want to know more - you're all going to be running around each others computers as you showcase some basic programming, and some of them may enthusiastically print stupid shit in a loop. Fantastic.

The point is not to teach a lesson, but to inspire the members to go off and self-study. Come back having learned something new since last time.

You can host [lightning talks](https://en.wikipedia.org/wiki/Lightning_talk). Typically the point is to showcase something you're working on and what you've learned from it.

For example, a friend of mine hacked his 3D printer firmware to lower the bottom-end temperature safety cut-off so he can print with some ultra-low melting point plastics - he had a problem that he couldn't get the original firmware to go that low, the software assumed the heating element was malfunctioning. Specifically, he had to write some [PID](https://en.wikipedia.org/wiki/PID_controller) code, which is a really awesome thing you're going to want in your life for future projects. Anything that needs to reach a steady state is controlled with PID, from engine idle, to cruise control, motor speeds, flow rates, temperatures, voltages, light intensities, you name it.

My favorite format works like this: you need a dry erase board with time slots. It's first come, first served - show up, and write your name and the title of your lightning talk. If the board fills up, that's all the talks for this meeting. There are some local quarterly industry meetups near me that does this. They rent out a space and setup multiple stations like this. Every half hour you gotta cruise the floor and see what talks are showing up and choose what you want to see. You should start with just one station and see how it goes.

I recommend you always know the format and content of the next meeting, this meeting. Don't plan a lightning talk session if you don't already know if there aren't going to be enough people to fill spots. Deep, focused dives and collaborative projects aren't for everyone. So if you've got a switching and routing project for everyone, some people might want to know not to show up ahead of time. And again, don't just plan it without knowing you have enough interest in it. Get everyone to vote on like a top 3 pick of ideas for the session after next.

I would recommend that club members invest ~$25 and pick up an [Arduino](https://www.arduino.cc/). Blinkin-lights is typically everyone's first project, but from there you can expand, tie in sensors, learn a little electronics, and control some more robust output. But it's also easy to turn a computer science club into an Arduino club, if you're not careful.",hc6ng8b,t3_pkn21x,1631197420.0,False
pkn21x,"**[Lightning talk](https://en.wikipedia.org/wiki/Lightning_talk)** 
 
 >A lightning talk is a very short presentation lasting only a few minutes, given at a conference or similar forum. Several lightning talks will usually be delivered by different speakers in a single session, sometimes called a data blitz. Some formats of lightning talk, including PechaKucha and Ignite, involve a specific number of slides that are automatically advanced at fixed intervals. Lightning talks are often referred to as ignite talks.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hc6nhox,t1_hc6ng8b,1631197438.0,False
pkn21x,"Hey hey heyyyyyy check out [https://hackclub.com](https://hackclub.com)   
I HAVE NEVER SEEN ANYTHING BETTER",hc78xw0,t3_pkn21x,1631206516.0,False
pkn21x,"Unrelated, but you can email companies in your area and see if they want to send the CS club study materials, licenses, or do events like a career day. It would look great for you on college applications as well.",hc7jay1,t3_pkn21x,1631210802.0,False
pkn21x,"Cybersecurity Awareness Month in October

[Cyberstart America](https://www.cyberstartamerica.org/)  \- CyberStart America gives all students in 9th-12th grade free access to the world-renowned CyberStart game.

[Cyber FastTrack](https://www.cyber-fasttrack.org/) \- Any college student in the US is invited to train and compete in the Cyber FastTrack program.

Some students move on to the scholarship competition:  
[The National Cyber Scholarship Competition for college students](https://www.ncs-competition.org/)",hccemo0,t3_pkn21x,1631299356.0,False
pkn21x,"If this is for people learning to code, follow a free program like Harvard’s CS50 or the courses on Khan Academy and work towards the certificate!

The final project for the course can be a group project for the whole club!",hc4xrfz,t3_pkn21x,1631154592.0,False
pkc72k,"Analog radio signals are _nomen omen_ analog, not digital, so there are no bits.  
But if there is a situation where digital information is transmitted through those mediums, then the similar mechanism as in other cases will be used - remember, in your PC there aren't really any 0 and 1, there are low voltages and high voltages.",hc2f5j9,t3_pkc72k,1631113802.0,False
pkc72k,"Specifically, we use [Frequency-shift Keying](https://en.wikipedia.org/wiki/Frequency-shift_keying) to encode digital signals into analog transmissions. This is the ""language"" of fax machines and modems.",hc2pn8d,t1_hc2f5j9,1631118203.0,False
pkc72k,[deleted],hc2nl55,t1_hc2f5j9,1631117350.0,False
pkc72k,In which case you have a quantum computer :),hc5lowe,t1_hc2nl55,1631170553.0,False
pkc72k,"Each uses a different technique. 

AM uses changing patterns of wave amplitude to represent individual bits
https://en.wikipedia.org/wiki/Amplitude_modulation?wprov=sfla1

FM works similarly but changes the wave frequency instead
https://en.wikipedia.org/wiki/Frequency_modulation?wprov=sfla1",hc2fnoj,t3_pkc72k,1631114018.0,False
pkc72k,Is it possible to amplitude modulate at the common FM frequencies and Frequency modulate at common AM frequencies?,hc3leuo,t1_hc2fnoj,1631131228.0,False
pkc72k,"Theoretically yes, but there'd be a lot of technical hurdles. Not sure what your application is, but sounds like it's time for some research - might be an easier way",hc3sug8,t1_hc3leuo,1631134906.0,False
pkc72k,"> Theoretically yes, but there'd be a lot of technical hurdles.

There's really nothing special about frequencies near 100 MHz that makes them more amenable to frequency modulation. The aviation band is right next to the FM broadcast band and uses amplitude modulation.",hc3tvzo,t1_hc3sug8,1631135422.0,False
pkc72k,"Very true - I was assuming this was for a hobby project, in which case standard frequencies are much more accessible",hc40mv5,t1_hc3tvzo,1631138570.0,False
pkc72k,Not sure what do you mean with 'common frequencies' but as far as I know you can perform these modulations with whatever frequencies. It's just that the commercial use (e.g. Radio) has specific frequency ranges.,hc3t43p,t1_hc3leuo,1631135042.0,False
pkc72k,"I think AM is typically 540-1700khz and the FM range is generally in the MHz frequcies.  

So what you're saying is that the FCC just allotted specific frequency ranges to AM and FM radio stations?",hc3tuaw,t1_hc3t43p,1631135400.0,False
pkc72k,"Yeah, these frequencies are typical because it's a convention to use them and as a result, the receivers and transmitters available are made to support these specific frequency ranges.

Other than that I can't think of a reason that these modulations would not be 'possible' with different frequencies. However, there would probably be differences in the transmitted signal characteristics (e.g. max distance travel etc).",hc3xbhb,t1_hc3tuaw,1631137026.0,False
pkc72k,AM and FM are analog radio modulation methods used by radio systems to transmit the radio waves over the air. Maybe you are asking about [HD Radio](https://en.wikipedia.org/wiki/HD_Radio)?,hc2l9xq,t3_pkc72k,1631116374.0,False
pkc72k,"I have made an example, that shows the modulated signal in time-space, where you can tweak the parameters. In these examples, the information that is going to be transmitted is a sine wave and it is doing so with the help of a higher frequency carrier signal (also sine wave).

FM: [https://www.desmos.com/calculator/ijokasntrf](https://www.desmos.com/calculator/ijokasntrf)

AM: [https://www.desmos.com/calculator/ltldhce6jn](https://www.desmos.com/calculator/ltldhce6jn)

sidenote: parameters for the FM signal are set just right in order to see the frequency shift, through time.",hc3v3cq,t3_pkc72k,1631136002.0,False
pkc72k,"This is an electrical engineering/signal processing question, not really comp sci imo.

That said, those are analog techniques. So strictly speaking, they don't. Of course you can embed ""information"" (e.g., some sequence of bits in this case) into any signal with a variety of techniques, but this has nothing to do with what amplitude and frequency modulation really mean.",hc3eyux,t3_pkc72k,1631128592.0,False
pkc72k,the radio wizards obv,hc5qdtz,t3_pkc72k,1631175028.0,False
pkc72k,"Uh oh, let us welcome another lifer to the ranks! Dangerous curiosity my friend, I started with this and now I spend my life implementing this stuff!",hc6ljce,t3_pkc72k,1631196563.0,False
pkc72k,"Binary isn't about the digits 0 and 1, it's about *two states*. You can build up binary numbers with any system that has at least two states, e.g. on/off, red/green, loud/quiet, voltage present/absent, finger straight/bent, etc. You just need something you can change, and both sides to agree what they're looking at.

That means there isn't ""a way it works"", there are lots. You could use FM with sound or with radio, where the core frequency is unchanged for 0 and modulated with anything for 1, or where the core frequency has 33Hz on it for 0 and 333Hz for 1, or where the modulation changes in a second for a 0 and stays the same for a second for a 1, or change the phase of the modulated signal, or anything else you can think up. They tradeoff how much radio/audio bandwidth it takes up, how much digital bandwidth you get from it, how easy/hard/costly it is to build the transmitters and receivers, commercial licensing and patents, how resilient it is against errors and noise, etc. 

How commercial systems actually work is by taking advantage of the analogue nature of systems to carry multiple bits, e.g. modulating the phase to indicate one of 8 bit patterns, doing that on multiple frequencies at one and then running them all at the same time.",hclwpaq,t3_pkc72k,1631479123.0,False
pk3f0c,"No, compilers are too complicated for 5 year olds.

A couple of good books to get you started, Writing an Interpret in Go and Writing a Compiler in Go.

https://www.amazon.com/Thorsten-Ball/e/B06XCKTCRW?ref=sr\_ntt\_srch\_lnk\_2&qid=1631110699&sr=8-2",hc28aih,t3_pk3f0c,1631110890.0,False
pk3f0c,"In one phrase it would “abstract syntax trees”. You create a 

1. lexer a class that handles opening and reading the physical text file your code or script is in. The output is a list of tokens. 

2. Parser, a class that takes either a stream or a list of tokens and outputs an abstract syntax tree(AST). How it does this is through the use of recursion. There are easy ways to design this beforehand that make coding one simpler. You can use a grammar language then almost directly copy that into code. 

3. Interpreter, tree walking algorithm that calculates the result and gives the user output without actually creating an executable program. Python and JavaScript are popular examples of this. 

4. Type checker, another tree walking algorithm but instead of computing the result it’s checking the code for errors and making sure all the types are correct, I.e. you’re not trying to use a string where you should be using an integer etc. 

5. Compiler, this one is the more mysterious one to me because I haven’t actually gotten this far myself but essentially it’s like the steps that came before. You walk the AST with a tree walker and at each point it changes the code from something you understand into something the computer understands, or it changes it to an intermediate language that then gets run through its own compiler later down the line.",hc20htr,t3_pk3f0c,1631107306.0,False
pk3f0c,https://craftinginterpreters.com/,hc2ffsv,t3_pk3f0c,1631113926.0,False
pk3f0c,RemindMe! 1 day,hc20au7,t3_pk3f0c,1631107211.0,False
pk3f0c,"I will be messaging you in 1 day on [**2021-09-09 13:20:11 UTC**](http://www.wolframalpha.com/input/?i=2021-09-09%2013:20:11%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/pk3f0c/any_eli5_articlesbooks_on_how_compilers_solvers/hc20au7/?context=3)

[**3 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fpk3f0c%2Fany_eli5_articlesbooks_on_how_compilers_solvers%2Fhc20au7%2F%5D%0A%0ARemindMe%21%202021-09-09%2013%3A20%3A11%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20pk3f0c)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hc20cmk,t1_hc20au7,1631107235.0,False
pjzv0m,You might want to talk to a real IP attorney who specializes in this area rather than rely on the advice of random redditors…,hc0n03o,t3_pjzv0m,1631071962.0,False
pjzv0m,Yes of course. That's the point with a license that includes Attribution.,hc13ofo,t3_pjzv0m,1631082752.0,False
pjzv0m,"What I got is ""I wanna steal others code on GitHub, and use it to make a course. But I wanna pass the code as my own"" 

That's asking to be sued.",hc17et9,t3_pjzv0m,1631085802.0,False
pjxvsi,"It's the language of the problem space. This language is not python, or Java, or even C. Its core principles of computer science. Its understanding how a computer works and the data structures and algorithms that are endemic to converting that which is in the problem space to the solution space. Regardless of programming language or operating system or hardware.",hc073vf,t3_pjxvsi,1631064096.0,False
pjxvsi,"Totally. Actually a lot of problem solving and planning can happen before you even decide on a programming language. Come to think of it, it may be good not to mention language details too early on as a general rule, when planning out a solution.",hc0z83i,t1_hc073vf,1631079428.0,False
pjxvsi,[deleted],hc29v3v,t1_hc073vf,1631111565.0,False
pjxvsi,">how do you know yours is quicker for everything and not just one example problem of the task?

Or, one algorithm is  better up to a certain size then it starts performing worse than the other algorithm which remains the same regardless of size.",hc2aclc,t1_hc29v3v,1631111772.0,False
pjxvsi,[deleted],hc2azfl,t1_hc2aclc,1631112045.0,False
pjxvsi,"One of the problems I use when mentoring junior engineers to describe the importance of algorithmic thinking and choosing the right data structure for the problem space is solving sudoku puzzles. The first approach most students come up with is the [brute force search](https://en.wikipedia.org/wiki/Brute-force_search) approach and quickly learn that approach falls down with even moderately harder puzzles to solve. With a bit of Algorithmic education many begin to see solving sudoku falls into a class of problems called [Constraint Satisfaction Problems](https://en.wikipedia.org/wiki/Constraint_satisfaction_problem). The go to solution for most students studying A&DS is [Bactracking](https://en.wikipedia.org/wiki/Backtracking) and for those paying attention the use of recursion fits nicely in this space. 

But, they begin to see the limits to this approach as we test this solution to much harder puzzles and/or different puzzle sizes (like 16x16.) With further studies in DS&A they begin to see the problem fall into other solution spaces like [Search and Constraint Propagation](https://norvig.com/sudoku.html) or [Exact Cover](https://en.wikipedia.org/wiki/Exact_cover). I then introduce them to [Donald Knuth's Algorithm X](https://en.wikipedia.org/wiki/Knuth%27s_Algorithm_X) and his very elegant implementation of that algorithm called [Dancing Links](https://en.wikipedia.org/wiki/Dancing_Links). 

Without this mathematically rigorous study of these and other Algorithms and Data Structures, they would never have been able to come up with these efficient solutions or even better, recognize future problems as one of these studied problem spaces and the associated solution spaces they fall in.",hc2godq,t1_hc2azfl,1631114443.0,False
pjxvsi,i just don’t understand why my coworkers who have such degrees struggle with code.,hc39hyx,t1_hc073vf,1631126351.0,False
pjxvsi,A degree is a piece of paper. Not hard to game the system and squeak by. Core principles of CS still hold water and mastering them is key to solving the tough problems in this field. With or without a degree.,hc46z9y,t1_hc39hyx,1631141590.0,False
pjxvsi,"One of the biggest things learning computer science had taught me (self taught) is what solutions are actually possible. It's easy to think that a computer can just do any sort of calculation or optimization, but there are definitely classes of problems that can't be solved.",hc0xjmt,t3_pjxvsi,1631078250.0,False
pjxvsi,Like centering a div,hc19mik,t1_hc0xjmt,1631087768.0,False
pjxvsi,Hardest part of comp sci is learning/using CSS change my mind.,hc1lo9i,t1_hc19mik,1631098515.0,False
pjxvsi,I agree,hc1qtp5,t1_hc1lo9i,1631102029.0,False
pjxvsi,*laughs in flexbox*,hc3j1gh,t1_hc19mik,1631130269.0,False
pjxvsi,I have a class in almost all my apps that’s “.flex-center” and it’s pretty sweet,hc41iv8,t1_hc3j1gh,1631138990.0,False
pjxvsi,I hate frontend development,hc5j3qj,t1_hc19mik,1631168292.0,False
pjxvsi,Sometimes frontend and messing with CSS can be fun but that's the tricky part: it's only pretending to be fun when it actually sucks.,hc5jgf1,t1_hc5j3qj,1631168591.0,False
pjxvsi,"The other day, my friend's girlfriend gave him and I a grocery list that involved a subset sum problem, and we had to explain to her that it wasn't trivial for us to get exactly 82 ounces of chicken breast when they're measured in hundredths of an ounce.

We were entertained. She was not.",hc2km1c,t1_hc0xjmt,1631116101.0,False
pjxvsi,"That's a very good question. Understanding what computer science is and why it's important is vital do anyone who means to do it. You'll probably get some downvotes for asking - people sometimes make claims that theory is unimportant, and a sub of people interested in that theory tend not to take kindly to posts that sort of resemble those - but don't worry about it. There are some great answers here.",hc0xfem,t3_pjxvsi,1631078169.0,False
pjxvsi,"The other reason it might see downvotes is because the post feels a little like it might be a homework question. Don't get me wrong - I LOVE to talk about CS theory, but the title is the question and there's not really much of an offering of discussion on OP's part. OP hasn't even replied to their own thread.

Again, most people are happy to help, myself included, but it seems to me like OP is trying to get answers for a homework question without engaging in meaningful discussion.",hc2ng8z,t1_hc0xfem,1631117293.0,False
pjxvsi,It’s not a homework question I’m still in high school and it’s summer vacation. I was just doing some research cause I like cs and I’m tryna dive deeper than just programming. And I didn’t get too much of what they were saying so I didn’t want to make myself sound dumb.,hc33j02,t1_hc2ng8z,1631123881.0,True
pjxvsi,"Gotcha! In that case I apologize for accusing you of trying to get cheap help on a homework problem - I was wrong. With that said, here's my contribution:

You've definitely got the right mindset for this stuff if you want to dive deeper than just programming. Theory is super cool because it more or less defines what is and isn't possible to do with a computer. Theory is also super broad - Compiler theory, automata theory, UI theory, the list goes on and on. You can really hone in on any aspect of CS and it'll all revolve around theory at the core. If you like math, you might  answer questions like ""Why is it easy for a computer to verify that a sudoku solution is correct, but hard for a computer to solve one?"" Hell, if you REALLY like programming languages, you can study those - there is theory for those, too.

Really, theory is the foundation of everything in any subject.",hc38iak,t1_hc33j02,1631125946.0,False
pjxvsi,It’s all good and thanks for the simplification it really helped me understand more :),hc3c9p9,t1_hc38iak,1631127489.0,True
pjxvsi,"It enables you to make simplified abstract models of complicated problems, and solve the simplified abstractions to get a desired result",hc10oqr,t3_pjxvsi,1631080482.0,False
pjxvsi,"Automata theory forms the basis of compilers. 

Asymptotic notation helps you compare algorithms and also determine bounds for your custom implementation.  


Even something as common as a build tool (like Maven or Gradle) has compilers and linkers inside it.",hc2a4pb,t3_pjxvsi,1631111679.0,False
pjxvsi,CS is not about a specific language. It's about theory. I still apply what u learnt 20 years ago in it in my job.,hc34917,t3_pjxvsi,1631124179.0,False
pjxvsi,"What kind of theory? Best practices can be considered theory, to some. In a practical sense, problems can be solved quick and dirty, or real effort can be used in the onset to make things easier to manage and test down the road. In my experience, theory comes into play most when you are being innovative and coming up with solutions that don't really exist. You can take a look at how they brought ray tracing to battlefield five, and what it took to optimize it. But it's all a cost-benefit relationship. Thinking and coding smart takes time. More for some, less for others, but if it can be done cheaper it often will, with a loss in quality.",hc1znwi,t3_pjxvsi,1631106898.0,False
pjxvsi,"Theory provides frameworks for understanding problems and solutions.

Imagine you wanted to build a search engine. How do you even approach a problem like that? Can you wait until the user types in their search before you do anything, or do you need to do some work beforehand? What kind of work? What sort of prep work will help with multiple possible searches? Can a computer actually do that work? Is there enough time to do it? Do you have enough memory? Do you have enough storage for the results? How do you even answer questions like that?",hc3c6wf,t3_pjxvsi,1631127457.0,False
pjxvsi,[deleted],hc0jy2e,t3_pjxvsi,1631070342.0,False
pjxvsi,"If the theory isn't important, you're not doing computer science, just programming, and probably pretty boring programming at that.",hc0v35g,t1_hc0jy2e,1631076616.0,False
pjxvsi,[deleted],hc0vmd2,t1_hc0v35g,1631076959.0,False
pjxvsi,"> I just finished an MSc

That must have been a truly terrible program to leave you so completely devoid of appreciation for the field

>especially in the current market

What on earth could the market possibly have to do with it? 

> old school considerations don't have to be accounted for.

That's simply not the case, and never will be. In the domain of computer science, the point of advancing hardware is to enable the advancement of software, not just to let us make the same thing over and over again, but worse and lazier each time.

You seem to lack a fundamental understanding of what computer science is and why we do it.",hc0wrhj,t1_hc0vmd2,1631077714.0,False
pjxvsi,[deleted],hc0x1tr,t1_hc0wrhj,1631077909.0,False
pjxvsi,I'm not sure you're using the right definition of theory here. How would you even design an algorithm without any theory?,hc0zes5,t1_hc0x1tr,1631079562.0,False
pjxvsi,[deleted],hc0zrjt,t1_hc0zes5,1631079811.0,False
pjxvsi,"It seems more like you're taking issue with the university practice, in general, of teaching way beyond what is needed for a given job field. Which is a whole other debate. For example in my mechanical engineering degree I rarely use the massive fluid dynamics equations they made me dissect each part of and derive by hand.

Some say it's for deeper understanding, some say it's an intelligence test, others say that it's a waste of time. For sticking to the purely applicable stuff there is always training courses and technical college. Though I realize those don't carry the same weight with employers as a university degree.",hc135cg,t1_hc0zrjt,1631082340.0,False
pjxvsi,"Yes,  you do.  You're looking like a fool.",hc1fy3b,t1_hc0x1tr,1631093663.0,False
pjxvsi,"It is impossible to have any sort of career as a competent developer making anything more complicated than a webfront without understanding and applying at least basic CS theory, _including how pointers work_. The fact that you're arguing otherwise blows my mind.",hc0xkm9,t1_hc0x1tr,1631078268.0,False
pjxvsi,Why would you need to know pointers when they get are hidden under a layer of abstraction by the programming language?,hc11n29,t1_hc0xkm9,1631081190.0,False
pjxvsi,Because they're not sufficiently abstracted that you can ignore them if you want to write anything performant.,hc36ej1,t1_hc11n29,1631125077.0,False
pjxvsi,[deleted],hc0xor2,t1_hc0xkm9,1631078346.0,False
pjxvsi,"This discussion isn't about whether or not you need a college degree, it's about whether or not you need to know and apply computer science theory to be decently productive in the field. The former is reasonably debatable; the latter is not.",hc0xvvi,t1_hc0xor2,1631078481.0,False
pjxvsi,[deleted],hc0y0hc,t1_hc0xvvi,1631078571.0,False
pjxvsi,"So your answer the question of ""why is computer science important"" is ""It isn't; don't bother""? WTF? Why are you even here then, if you find CS so pointless? 

 Not to mention the fact that you completely skipped the second part of the question.",hc0y8li,t1_hc0y0hc,1631078729.0,False
pjxvsi,[deleted],hc0ygsi,t1_hc0y8li,1631078889.0,False
pjxvsi,"The fact that people write incredible numbers of terrible web apps does not degrade the importance of computer science. The state of computing is what it is due to unceasing efforts to advance the field with cutting edge theory. It is this current state of computing that enables all those terrible web apps to be written in the first place, and the importance of practicing and advancing the theory is evident in this. The likelihood of a given programmer to need a deep understanding of that theory is therefore irrelevant. QED.",hc0z36t,t1_hc0ygsi,1631079329.0,False
pjxvsi,[deleted],hc10j6l,t1_hc0z36t,1631080370.0,False
pjxvsi,Pointers have very little to do with CS theory (they’re only relevant in one of the many computational models). Computational complexity would be a better example and the basics are useful in practice.,hc1cwej,t1_hc0x1tr,1631090801.0,False
pjxvsi,"As much as I dislike the reality of it, a lot of industry jobs don't require complicated theoretical expertise. That being said, there are a ton of jobs out there that require an immense amount of mathematical knowledge, logic, and CS. It just depends on the job. Don't expect to be making any ""sort algorithms"" or pathfinding algorithms as a front end dev. However in game development, these things are very important",hc11ckf,t1_hc0jy2e,1631080973.0,False
pjxvsi,[deleted],hc11jph,t1_hc11ckf,1631081118.0,False
pjxvsi,Weird how u got so many downvotes so I thought I'd give my 2 cents as well,hc12p50,t1_hc11jph,1631081998.0,False
pjxvsi,[deleted],hc12tgc,t1_hc12p50,1631082088.0,False
pjxvsi,"True. When these downvoters get out of school and find themselves in a cubical instead of a lab coat, they'll get it.",hc1jbe0,t1_hc12tgc,1631096622.0,False
pjxvsi,"There is also a lot of 'theory' and research behind stuff like design and architecture of software, how to reason about concurrency and how to prove absence of certain kinds of bugs, how to test, how to set up a good software proces, how to document software using software models, how to do requirements elicitation, etc etc etc. But you would get that from a software engineering degree, not per se a computer science degree. If you just want to be a developer, maybe it's the engineering degree you're after, not the science degree?",hc14sdk,t1_hc11ckf,1631083641.0,False
pjxvsi,"1: I haven't mentioned to be after anything.
2: coincidentally I know someone in the family who has been coding at companies like ibm and other lesser known worldwide companies and they all used some sort of automated testing software to track down a lot of bugs. I know someone has to write the software for this program, but the ratio of producers/users of this software is so tiny it's barely worth mentioning",hc1503o,t1_hc14sdk,1631083809.0,False
pjxvsi,"What is computer science besides theory? It’s like saying why is theory important for Physics… that’s kind of what the field is all about.

If you mean like why is theory important for software engineering or programming, that’s a different questions and one with conflicting answers. Basically, if you’re building a database, it’ll be helpful to know the theory, if you’re building a website, you probably don’t need it. 

But either way theory is something that should be fun to learn. If it’s not fun for you, and not necessary for you to do your job effectively, then don’t learn it. *shrug*",hc43nm9,t3_pjxvsi,1631140008.0,False
pjxvsi,"there's a fair number of algorithms you learn and and learn how fast they are. If you have a problem, *usually* you can fit this problem into a category of problems that have already been solved. This makes coding easier because you can use a preexisting algorithm rather than figuring it out yourself and potentially making really slow code. Additionally sometimes people try to find P = NP without realizing it and spend huge amounts of time looking for an algorithm to solve a known, difficult, open problem. So in a practical sense it can save you a lot of time.

just as an example: fast exponentiation is a known algorithm. so If you are coding in assembly or maybe for a microcontroller, you may want to implement n\^x. the easiest algorithm is linear and you can probably figure that one out in 2 seconds. A fast algorithm is lg(n) and that might take you an hour to figure out. and the FASTEST exponentiation is unknown so that would waste a lot of your time. Implementing this would take the least amount of time if you know the math behind it and research how to make a fast algorithm by knowing the correct terminology and what to look for.",hc5oylx,t3_pjxvsi,1631173614.0,False
pjlh5a,"I too have been studying compilers a lot in these recent months. I've read engineering a compiler, modern compiler implementation in ML, and part of the dragon book. My favorite so far has been the second, but I share your sentiments about engineering a compiler.",hbywjrf,t3_pjlh5a,1631043433.0,False
pj4mak,"Languages don't have to understand each other.  They just have to tell the computer what to do with the input, and generate output.

You wouldn't make a program with one class written in C++, and another written in Java.  But you could write an application in C to handle one aspect of the problem, and another application in Java that works with the C application to handle another aspect of the problem.

Edit:  also remember, at the end of the day, all the code will be compiled into machine code (with the exception of python - that runs thru an interpreter, and java - that compiles to java byte code and runs in the JVM)",hbu1fa2,t3_pj4mak,1630951181.0,False
pj4mak,"well thanks for expaining !

i think i got a bit more what's happening",hbuvpv0,t1_hbu1fa2,1630964666.0,True
pj4mak,"When you write a program they usually get compiled to some kind of binary file or intermediary byte code. You communicate with other applications via an [Application  Binary Interface](https://en.m.wikipedia.org/wiki/Application_binary_interface), [Foreign Function Interface](https://en.m.wikipedia.org/wiki/Foreign_function_interface), or [Application  Programming Interface ](https://en.m.wikipedia.org/wiki/API).",hbwr9wu,t3_pj4mak,1631003778.0,False
pj4mak,"**[Application binary interface](https://en.m.wikipedia.org/wiki/Application_binary_interface)** 
 
 >In computer software, an application binary interface (ABI) is an interface between two binary program modules. Often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. An ABI defines how data structures or computational routines are accessed in machine code, which is a low-level, hardware-dependent format. In contrast, an API defines this access in source code, which is a relatively high-level, hardware-independent, often human-readable format.
 
**[Foreign function interface](https://en.m.wikipedia.org/wiki/Foreign_function_interface)** 
 
 >A foreign function interface (FFI) is a mechanism by which a program written in one programming language can call routines or make use of services written in another.
 
**[API](https://en.m.wikipedia.org/wiki/API)** 
 
 >An application programming interface (API) is a connection between computers or between computer programs. It is a type of software interface, offering a service to other pieces of software. A document or standard that describes how to build such a connection or interface is called an API specification. A computer system that meets this standard is said to implement or expose an API.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hbwrbgy,t1_hbwr9wu,1631003816.0,False
pj4mak,Idk why my brain couldn’t understand thag,hcsb8uw,t1_hbwr9wu,1631596198.0,True
pj4mak,That*,hcsb9jp,t1_hcsb8uw,1631596211.0,True
pj4mak,"CS 101

A CPU takes a stream of data from memory (RAM).
The stream ought to contain instructions that the CPU can understand (Machine Code) and act accordingly to those instructions.
A program is located in memory as a block of instructions that the CPU will perform.
How you get the ""program"" to end up in memory, is usually by reading from disk to memory, by assistance of ""running"" a program through the kernel. Usually you have a user layer over the kernel, such as a terminal or a window manager (a UI / Front-end of the kernel).
A program is a side-effect (a function).

A programming language is just like english, syntax and grammar, etc. They usually represent a tree of content for parsers. How to perceive the language is up to the tools, and they do it properly when consulting the language specification.
A programming language may have tools associated with them, such as compilers, linters, JIT runtime engines, REPLs, etc. These tools are programs.

Proper compilers are programs/functions, that take code (in a certain language) and produce machine code (virtual or physical).
The resulting machine code may be run by a kernel like described previously.",hbynyca,t3_pj4mak,1631039877.0,False
pj4mak,Oooh okay. I think with all the answer I’m beginning to understand how it works thanks for your help everyone !,hcsb7g2,t1_hbynyca,1631596172.0,True
pj4mak,Its all just translations of binary.,hc4seaz,t3_pj4mak,1631151944.0,False
pj4mak,Yep I’ve seen other answers and it all seems to work un my head now,hcsbbcj,t1_hc4seaz,1631596243.0,True
pj4mak,another important and useful way programs in other languages can communicate is through an output text file. almost every language has some way to put output to a file and a way to read a file.,hcb3hlz,t3_pj4mak,1631279550.0,False
pj4mak,its a bit awkward communicating from one language to another. Python can have c and c++ modules that it can directly call. As for java you have to go through the os by using stdin and stdout to communicate between programs. If you know java you have probably seen stdin for user input. In python you can access these things in a java or c program using the popen module and assigning stdin and stdout. from java you can do this through Process class. as for c and c++ I have no idea.,hcxa1wj,t3_pj4mak,1631690243.0,False
pixs57,"I do not know for sure how Bitcoin operates but generally speaking:

I'd say it's no more trivial to find an IP address on a LAN than on the Internet.   
The methods are the same, only the size of the network varies.

You either scan the entire network, which will take time and which will be detected and flagged as an attack by advanced enough security systems; or you connect to well known hosts.   
The well known hosts act as a directory, they keep track of the currently available nodes and transmit the information back to the clients based on predetermined criterias (available bandwidth and/or resources and/or connection slots, and/or location, etc...). The clients then connect to the nodes.

Think P2P protocols: bittorrent, ed2k and such.

Edit: here's some useful information https://developer.bitcoin.org/devguide/p2p_network.html",hbsyro2,t3_pixs57,1630933613.0,False
pixs57,This right here 👆 A boarderline goldmine. Many thanks for the extension of knowledge.,hbt250v,t1_hbsyro2,1630935403.0,True
pivg21,"Your teacher isn't quite right.

The first and last address in a subnet are reserved - the first address is for the subnet itself, while the last is for broadcast.   In a /24 subnet, the first address is x.y.z.0. and the last address is x.y.z.255.  But for larger subnets, you can have legal addresses that end in .0 or .255.

And the .1 address is often used by the gateway, but that is a convention, not a standard.",hbsiv1z,t3_pivg21,1630922064.0,False
pivg21,thanks,hbsj8z0,t1_hbsiv1z,1630922414.0,True
pivg21,"Gotta remember that the reason subnets exist is to make small pools that can talk to each other, and then be able to tell whether another computer is in the same pool or not. If we just had 500,000 computers with their own number, to talk to one of them you'd have to communicate with all of them to find all their numbers, before you could get to the one you want and that wastes bandwidth like crazy.

Broken down into small subnets, you have only the size of the subnet to talk to, e.g. 64 computers *and* ""outside this subnet"" for everything else. That allows for routing, and it means the routing tables only have to go subnet->subnet, and not 500,000 different rules. Important, because early computers and routers and switches had very limited computing power and memory.

So the subnet address typically `192.168.1.0` (but not necessarily) is the one which tells you ""that other computer I want to talk to, is it in my subnet or not?"". You can't use it as a computer address because comparing IP address against the subnet address is the way to distinguish whether you can talk directly to a computer or need to route somewhere else.

The broadcast address typically `192.168.1.255` is the ""talk to every computer in this subnet"" address. It's not always supported, but if you try to ping that address you might get a reply from every machine which is powered on. Not all that useful, reserved by the design of networks.

`.1` is not anything special, but it comes in with the idea of ""outside this subnet"" and ""need to route somewhere else"". If you do the calculations and find ""that remote computer is outside this subnet"", how do you talk to it? You pass the traffic onto something else which can route closer to it. That has to be something you can talk directly to, so it has to be within your subnet. It's typically a device acting as a default gateway, and using the first or last address, e.g. `.1` just for convenience.",hclxrwt,t3_pivg21,1631479555.0,False
pivg21,Thanks for the detailed answer.,hcnnmbu,t1_hclxrwt,1631509228.0,True
pivg21,https://en.wikipedia.org/wiki/Reserved_IP_addresses,hbsecws,t3_pivg21,1630917962.0,False
pivg21,thanks for the input but I still step in the dark. I've already checked Wikipedia but it didn't helped me out at all. Thats why I ask on reddit.,hbsflb6,t1_hbsecws,1630919093.0,True
pivg21,"There's probably a more suitable sub for networking questions 🙂

Disclaimer: my networking courses were a long time ago and I never liked network stuff, so the answers won't be super precise.

Anyways, here it is.

Every network subnet is comprised of:   
- A network address.  
- A usable IP range.  
- A broadcast address.  

The network address is always the first IP address within the network IP range. It uniquely identifies the network segment and with the network mask (or CIDR), it serves as a reference to calculate the other possible IP addresses within it.

The usable IP range is the list of IPs that can be assigned to the network elements located within this subnet.

The broadcast address is always the last IP address, it's used when a packet needs to be sent to all the IP addresses within the IP range.

The network and the broadcast addresses are reserved and should never be assigned to a network element.

For simplicity's sake, your professor used type C networks (network mask 255.255.255.0, CIDR /24) to illustrate this.   
These networks' IP range is always exactly 256. Their first IP address is always .0 (the network address) and their last IP address is always .255 (the broadcast address). Their usable IP range is always from .1 to .254.

Note that your teacher told you that .1 was a reserved address. It is not, because it's actually within the usable IP range.   
By convention, many people (incl. me) will use the .1 IP as the gateway IP (the router that allows communication between several networks). I've seen some places/people use the .254 as the gateway IP.   
This is just a convention, that has nothing to do with technical requirements. Your gateway IP could be right the middle of your IP range if you wanted.

I wont go into the calculation, because there are tons of resources online and because the last I did was over 15 years ago 😅.

Hopefully this will help you.

Note: all of this applies to IPv4. IPv6 changes many things and the above is probably not applicable. I never really took the time to learn more about IPv6 so I can't get into more details.",hbsp0s1,t1_hbsflb6,1630927297.0,False
pivg21,"There are better subs for networking, such as /r/networking/",hbtr9uh,t1_hbsflb6,1630946786.0,False
pivg21,"Desktop version of /u/riggycat's link: <https://en.wikipedia.org/wiki/Reserved_IP_addresses>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hbsedqm,t1_hbsecws,1630917984.0,False
pipyku,"As someone who coaches others in Tech; It's amazing that you want to teach others!

Just wanting to make sure it's clear to anyone who wants your help, is this paid or unpaid?

Anything you can show people in terms of others you've helped or approaches to teaching might help them.",hbrfueq,t3_pipyku,1630895242.0,False
pipyku,"Thank you! 

This will be completely unpaid. I used to have a website but it isn't hosted anymore. 

My approach is purely application oriented. Before I proceed to ""what"" i emphasize on the ""why"" 

This, in my opinion, makes things interesting. Furthermore i try to instill coding best practices early on. A variable named ""result"" is so much better than ""x"". And so on. Please ask me more questions if you feel I've left anything out!",hbrgmco,t1_hbrfueq,1630895629.0,True
pipyku,"Hey OP, thanks for the initiative, I'd love to be a mentee if you can be a mentor? DM me please if you're interested

Edit: I'm a new grad with almost 1ish years of Internship YOE",hbs9r1h,t3_pipyku,1630913950.0,False
pipyku,Working on a course structure right now. I will keep you updated. Welcome onboard! :D,hbt7gox,t1_hbs9r1h,1630938052.0,True
pipyku,"I love this. I was an education major in college but switched to CS, so I have a passion for both. 20+ years later, I’m an exec in big tech (incl. ex-FAANG). If I can help in anyway, let me know. Career advice, technology choices, programming. I don’t have a ton of spare time as my family, job and personal hobbies take up a lot, but happy to help out where and when I can.",hbt6vrp,t3_pipyku,1630937776.0,False
pipyku,Thanks very much!,hbt763q,t1_hbt6vrp,1630937916.0,True
pipyku,"I really need help. First let me introduce myself I am from India I completed my engineering degree in 2015  then I learn manual testing and try to find a job but unfortunately due to lack of resources and guidance I couldn't find one then I move back to my hometown where I work with my friend's startup company it's kind of a free help for his courier company mean time I developed arthritis problem which I recently diagnosed with. So after working for 2 year in friends company I got an offer as a manual tester in 2018 in other state but by that time my sickness really got over me and I couldn't understand why it's so tough to get a head around simple code(I always like to read and watch about coding html css js python sometime c# java c) and doing some simple physical task I thought that might have something do with my depression. So I got a job as a associat tester profile I work for 1 year I build a test plan from scratch for a ERP product but by the end of the year my immune system became so weak that I had typhoid and jondis  together so I have to leave that job and move back to my hometown straight bed rest for 6 months it was year 2019 after that one of my relative suggest me to work in his company where I have to maintain a online store of electronics shop on different global ecommerce platform but then covid pendamic hit globally and I loose my job I again get into depression ....and lot of struggle and emotion from personal to professional but . I keep watching YouTube tutorials even though it feel overwhelming because of my health issues. 

I recently diagnosed by rheumatoid arthritis and lipoma so in this condition your whole body muscle became stiff and painfull and your whole body feel burden heavy to yourself.

for the cure I am taking medicine and doing meditation and exercise feeling better then before.


So I need your help. please guide me 🙏  what can I do to get my foot back to it industry because my experiences are quite irrelevant to what they require in it sector. So what I should I do now. I also want to learn coding properly where I feel confident . Thank you for your patience to read me and helping me🙏",hbspl3r,t3_pipyku,1630927733.0,False
pipyku,"> manual tester in 2018 in other state but by that time my sickness really got over me and I couldn't un

Thanks for writing all of that. I'd be glad to have you join the course. We can chat separately if you want career advice but regardless, I am confident you will benefit from the course. I am working on a course document now. I will keep you posted!",hbt8mn6,t1_hbspl3r,1630938601.0,True
pipyku,Thank you for your response mentor 🙏,hbtcswe,t1_hbt8mn6,1630940505.0,False
pipyku,"Taking CMSC 201 right now which is python based. Seeing as how Golang is like C i think it would be great practice. Would be interested to check this out, thank you for doing this.",hbtq7st,t3_pipyku,1630946326.0,False
pipyku,Thanks so much! Will keep you posted!,hbtqizp,t1_hbtq7st,1630946461.0,True
pipyku,[removed],hbsrsuu,t3_pipyku,1630929331.0,False
pipyku,"Sure! Working on a course structure right now. I will keep you updated. Welcome onboard! :D   


Not sure when we will begin but I bet it will be at least a couple weeks from now. So maybe that timeline will align with your availability!",hbt7sao,t1_hbsrsuu,1630938202.0,True
pipyku,"Hi, I'm interested in the course, I'm from Chile, South America. I studied ""Commercial Engineering"" (know as Business and Administration internationally) and found recent, after 1 year of unemployment, that I should study CS or something similar because I always waste my time reading or viewing Youtube videos of CS or related topics. Is for this that I recently enrolled on a master in artificial intelligence here in my country.

Recently I put an objective of getting a job as Data Engineer or as a Programmer to dedicate to the things that I love.

I know basic/medium of Python and SQL (I don't know if medium is the correct level, I felt more like beninger)",hbtwm5a,t3_pipyku,1630949092.0,False
pipyku,Thanks for reaching out! The course definitely doesn't have a pre-requisite. But any prior experience only helps. I am putting together a course document. Will send it out once ready! Welcome onboard!,hbtzwru,t1_hbtwm5a,1630950516.0,True
pipyku,"I would love to join this course ! Thanks for helping everyone.

I am 18, self-taught I would classify myself as an intermediate in React JavaScript and python.
Also learning clojure right now.
I have been looking to get into golang for quite some time ! Really excited for this.

Feel free to DM me anytime, also is it okay to DM you to ask for advice about learning programming, freelancing and careers etc sometime",hbu58qk,t3_pipyku,1630952841.0,False
pipyku,[deleted],hbu5r45,t3_pipyku,1630953064.0,False
pipyku,Thanks so much! Working on the course document now. Ill send it once ready,hbu6ru0,t1_hbu5r45,1630953512.0,True
pipyku,This seems amazing! I was a CS major but moved into education and haven't touched programming in a long time. I would love to be a part of an intermediate/dev ops group!,hbuc5tg,t3_pipyku,1630955885.0,False
pipyku,I will keep you posted! Welcome onboard!,hbucqia,t1_hbuc5tg,1630956139.0,True
pipyku,"Awesome, thanks!

On a side note, this seems like perfect fodder for being considered professional development that I could use. Would you be open to it if I looked into how I could make this a little more official and count towards hours of PD for licensure?",hbwfhzz,t1_hbucqia,1630994178.0,False
pipyku,I am not sure what PD for licensure means.,hbx3gfx,t1_hbwfhzz,1631013943.0,True
pipyku,I would be so interested im 17 got kicked out of my cs and programming class in high-school and am going to college soon looking to take some of those courses and get into this field,hbrufpp,t3_pipyku,1630903104.0,False
pipyku,Why’d you get kicked out?,hbs8hhv,t1_hbrufpp,1630912913.0,False
pipyku,Working on a course structure right now. I will keep you updated. Welcome onboard! :D,hbt7i0r,t1_hbrufpp,1630938069.0,True
pipyku,"Hey, I would like to join this course.
I have graduated from  Civil Engineering this year,
but my interest was always in Computer and Tech field.

Now, I am shifting my carrier in Programming and Software Development.
May be this course work as a Boost for me.

I would love to be mentored by you.
DM me if You are interested as a my Mentor too.",hbslp98,t3_pipyku,1630924573.0,False
pipyku,Working on a course structure right now. I will keep you updated. Welcome onboard! :D,hbt7j63,t1_hbslp98,1630938083.0,True
pipyku,"Hey OP , I'm 18 and I'm planning on taking a CS course in college but have very little to no knowledge in programming. So it would be amazing if I learn something from you",hbsnpvr,t3_pipyku,1630926261.0,False
pipyku,Working on a course structure right now. I will keep you updated. Welcome onboard! :D,hbt7juv,t1_hbsnpvr,1630938092.0,True
pipyku,"What do you have in mind when saying ""full blown app""?",hbsphlw,t3_pipyku,1630927658.0,False
pipyku,"We begin with requirements gathering then build a front-end, backend and put some infrastructure in place. Automate CI/CD make sure we have really good test coverage. Essentially making something production ready!",hbt7ogs,t1_hbsphlw,1630938152.0,True
pipyku,Alright then. Count me in.,hbteg48,t1_hbt7ogs,1630941230.0,False
pipyku,I'm interested! Where do i sign up?,hbt74cv,t3_pipyku,1630937892.0,False
pipyku,Working on a course structure right now. I will keep you updated. Welcome onboard! :D,hbt7sw9,t1_hbt74cv,1630938210.0,True
pipyku,Looking forward to it! Would you be interested in connecting and sharing your suggestions for an open source project?,hbt8x85,t1_hbt7sw9,1630938738.0,False
pipyku,"I'd be glad to connect. However I don't have any suggestions for open source contributions. My senior once told me to begin with Janitorial work - typos, documentation grammar etc. And I have never acted on it.   


But yes, let's connect!",hbt98wj,t1_hbt8x85,1630938891.0,True
pipyku,Alrighty! DM'ed you,hbtd5e8,t1_hbt98wj,1630940659.0,False
pipyku,"Quite interested to see your course of Golang :)  
I self-learned Golang a year ago, but I believe I still have some issues with some core features of the language. (Mostly because I use it rarely and for simple tasks)",hbtbd0s,t3_pipyku,1630939857.0,False
pipyku,Im working on getting a course document together. I will post it out once it's done. Let me know if it aligns with what you want to get out this. Happy to chat 1:1,hbtquej,t1_hbtbd0s,1630946599.0,True
pipyku,"Sure, I will happily comment over it once it's done",hbtz1dp,t1_hbtquej,1630950137.0,False
pipyku,"I am a beginner transitioning to intermediate software developer. I would like to get started with MERN stack. I know some html css and vanilla js and SQL based databases(MySQL) but nothing else from the MERN stack domain.

I see you would be going forward with golang but can you like roughly steer me towards this path with some guidance including but not limited to:
1. resources
2. best practices to follow
3. beginner projects to try, 
4. Even YouTube channels and blogs to follow for this (except freecodecamp and traversymedia, they teach really basic stuff and too quick. I beleive they are good for crash course/revisions mainly) would help
5. GitHub repos to follow for open source codes
6. Chrome extensions that ease the MERN dev
Etc
Anything else that you can think of. I would be really grateful if you can spare some of your time to guide me.
Similarly I am thinking of learning devops(web hosting, nginx servers or something, ci/cd and any other techs required). I know only the names. I know GIT only from this domain. I can google the ""what is <this_tech>"" and ""how to learn"" stuff but something more helpful/right direction would be great. I would be really grateful to you for your time. Nice initiative of giving back btw. Really appreciate",hbtdhj1,t3_pipyku,1630940807.0,False
pipyku,We should probably talk about this 1:1 Can you DM me please ?,hbtqmj4,t1_hbtdhj1,1630946504.0,True
pipyku,!remindme 1 day,hbtgpwa,t3_pipyku,1630942219.0,False
pipyku,"I will be messaging you in 1 day on [**2021-09-07 15:30:19 UTC**](http://www.wolframalpha.com/input/?i=2021-09-07%2015:30:19%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/pipyku/free_programming_lessons/hbtgpwa/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fpipyku%2Ffree_programming_lessons%2Fhbtgpwa%2F%5D%0A%0ARemindMe%21%202021-09-07%2015%3A30%3A19%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20pipyku)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hbtgu2o,t1_hbtgpwa,1630942270.0,False
pipyku,"sounds wonderful, count me in! Starting uni this october already studied some Java and c++ in highschool",hbtx7bv,t3_pipyku,1630949344.0,False
pipyku,Are you planning algo ds in golang? I'd love it.,hbu1s3j,t3_pipyku,1630951338.0,False
pipyku,Absolutely. I will be covering asymptotic complexities and a few algorithmic implementations. But this course will be geared towards writing clean code so not that algo heavy. If that makes sense,hbu204o,t1_hbu1s3j,1630951435.0,True
pipyku,"Current CS student, may be interested in the intermediate course, though I only am familiar with C++ and  some Java at the moment as far as languages go.",hbu3gyn,t3_pipyku,1630952075.0,False
pipyku,"Hi,
   Is this for experienced professionals as well?",hbu8xlz,t3_pipyku,1630954453.0,False
pipyku,"I am planning on running a separate batch of intermediate learners. You can join that if you want.! :D   


There we focus on building a front end, back end, infrastructure as code, automated CI/CD pipeline, test code coverage. Overall focussing on Software Engineering rather than Programming if that makes sense",hbu9ckq,t1_hbu8xlz,1630954639.0,True
pipyku,"Sounds great..please keep me posted..thanks again
For this opportunity..",hbubkzm,t1_hbu9ckq,1630955632.0,False
pipyku,What is golang and what are its benefits,hbskf08,t3_pipyku,1630923453.0,False
pipyku,Maybe Google that one 😉,hbsn2ej,t1_hbskf08,1630925724.0,False
pin2ke,"Foreign language, by a huge margin.

Depending on who you believe, the English language has between 500,000 and 1 million words.

C has 32.

Learning a programming language is trivial, it's *programming* that's hard.",hbqum1z,t3_pin2ke,1630884745.0,False
pin2ke,">Learning a programming language is trivial, it's programming  that's hard.

Perfect response.",hbrft1v,t1_hbqum1z,1630895224.0,False
pin2ke,Yes that's so true,hbu58g3,t1_hbrft1v,1630952838.0,False
pin2ke,Syntax is easy. Logic is hard.,hbrv8qx,t1_hbqum1z,1630903601.0,False
pin2ke,Exactly.,hbrz11v,t1_hbrv8qx,1630906029.0,False
pin2ke,Also architecture and good design.,hbsnuko,t1_hbqum1z,1630926364.0,False
pin2ke,Programming languages are small and tend to be logical. People languages are huge and full of bizarre oddities.,hbtzbfy,t1_hbqum1z,1630950256.0,False
pin2ke,Gendered words make no sense,hbut1kp,t1_hbtzbfy,1630963435.0,False
pin2ke,">Gendered words make no sense

Coming from an English speaker I presume :)",hbvi0nk,t1_hbut1kp,1630975592.0,False
pin2ke,Yup,hbvicwg,t1_hbvi0nk,1630975757.0,False
pin2ke,The syntax is easy; design and decision-making are complex.,hbudkn3,t1_hbqum1z,1630956512.0,False
pin2ke,A foreign language is many times more difficult.,hbqtjrn,t3_pin2ke,1630884217.0,False
pin2ke,a foreign language is so much more difficult,hbqpaiw,t3_pin2ke,1630882121.0,False
pin2ke,"Programming languages are much easier because programming languages are designed around a logical premise and, as such, all computer languages can be understood once you’ve understood one or two of them.

Human languages are the random accumulation of the results of historical events. So, in general, there is no reason for the concepts of any particular human language to resemble another one. (The exception being the Romance languages, since they all have a recent common origin.)",hbr6upg,t3_pin2ke,1630890768.0,False
pin2ke,There are many language families.,hbu164o,t1_hbr6upg,1630951068.0,False
pin2ke,"Foreign language

The great part about CS and languages is that the things you can do are generally the same (as is the syntax)

You just have to learn the differences and tricks unique to each language",hbqroab,t3_pin2ke,1630883287.0,False
pin2ke,A foreign language doesn't take just a couple of days,hbqvqvy,t3_pin2ke,1630885313.0,False
pin2ke,Definitely learning a foreign language by FAR. It's not even comparable to learning a programming language at all.,hbrt6vt,t3_pin2ke,1630902352.0,False
pin2ke,It’s not generally accepted and even expected to google everything about a foreign language lol,hbr5q1z,t3_pin2ke,1630890209.0,False
pin2ke,"Programming languages in general follow the same paradigms, semantics, and patters - you could probably write one yourself. Spoken, human languages on the other hand generally involve a lot more complicated syntax (like imagine writing an ast for English) and many more exceptions.",hbrkvp8,t3_pin2ke,1630897800.0,False
pin2ke,Dude foreign language is waaaaaaaaaaay harder you kidding me? Ive done both and programming doesnt even take a year but foreign language... that is several years...,hbrunq8,t3_pin2ke,1630903239.0,False
pin2ke,"As someone who's done multiple of both, the foreign language. I wouldn't consider myself fluent in any, but it's enough to where I could Google a new word and properly use it with my existing knowledge.

But I think it's more equitable to consider all programming languages dialects of each other, so once you have a good understanding of a single language, you can learn a new accent, or new words that get used in a particular region. One foreign language may not give you a leg up on other foreign languages.

The bigger hurdle I see with learning any language is use and practice. It's far easier to find material you can work on yourself with a programming language. You can take your time, deconstruct, and reconstruct new things as you come upon them and return to where you left off. In contrast, you can probably find various foreign media to practice your foreign language on, but the information will come very quickly and in methods you might not be able to easily look up. For example, hearing a new word in a film or show will not provide you with a spelling. Looking that word up isn't always an option as the media continues to play. A big part of this difficulty comes from the type of media each is normally expressed in (written programming vs spoken foreign).",hbrengk,t3_pin2ke,1630894632.0,False
pin2ke,"Learning a foreign language. When programming everything needs to be perfect but that's what makes it easy (im using this word liberally). As long as it's set up correctly and theres enough memory, it'll work. For spoken language you have to rely on context, and errors become a part of language aswell and it takes way more time to master. You have pronounciation, context, grammar, vocabulary, even cultural aspects that you have to learn and then coding is just grammar and vocabulary. But then again i only started coding recently and have been studying languages for half my life so I'm pretty biased. Maybe coding is way harder than I think.",hbrvcd6,t3_pin2ke,1630903664.0,False
pin2ke,"Once you learn one programming language, it's fairly easy to add another one to your skillset. But foreign languages require a lot of time every time you pick up one.",hbsdntr,t3_pin2ke,1630917333.0,False
pin2ke,Learning a foreign language is way harder. Learning a programming language what is there? just learn the syntax for loops and if/else. That's 90% of code anyway.,hbsgx7m,t3_pin2ke,1630920306.0,False
pin2ke,"Welp, I guess I'm the first one to go against the grain here and say programming languages are harder for me.  Several people have pointed out how much easier it is to learn the syntax of PLs.  That's true, and if that's all you consider knowing a programming language to be, then I agree.

For a little context on me, I have 2 Masters degrees, one in Applied Linguistics and the other in Computer Science.  I consider myself in the upper-intermediate / lower-advanced range in Spanish and Portuguese.  I'm about to start French 101 soon, because I enjoy learning human languages.  

Besides the programming experience from university, I had 1.5 years of part-time internship experience and almost 3 years full time as a software engineer, mostly in Java but this year mostly C# (and the syntax is extremely similar between the two).  So I've had a lot of experience learning both human languages and PL's.

At the risk of turning this into a semantics debate, I personally do not consider knowing the syntax of a PL as fluency.  I have made it my mission to understand design patterns in the language, clean code, etc., and I don't yet feel confident that I know those well.  Using crappy code but syntactically correct, I can generally get the program to do what I want.  But I also figured out early on in Spanish that I could say ""You know, the thing that does this in the car"" and convey the meaning I wanted.  I also don't consider that fluency.

My advanced Spanish comes mostly from a year traveling in Latin America over a decade ago, and sporadic practicing since.  Even when traveling, I spent a lot of time speaking English with other travelers in hostels.

I've been working full-time, M-F, for 3 years.  I've tied much of my self-worth to writing good code, started taking Adderall, etc., and again, don't feel great about it.  If I spent the same amount of timing practicing Spanish, well... I can't be certain, but given the time/energy investment rates between HL's and PL's, I would say I pick up HL's much faster.

Everybody's different, so maybe I'm an outlier in that sense, or maybe you don't agree with how I've defined learning a HL/PL, which is fine.  Just my thoughts.",hbs44h5,t3_pin2ke,1630909553.0,False
pin2ke,Upvoting only because I think it’s unfair that you’re getting downvoted just for sharing an unconventional anecdote lol.,hbsp7ba,t1_hbs44h5,1630927437.0,False
pin2ke,"Thanks Rokketeer, I appreciate it :) I'd be more interested to hear any counterpoints from people that disagree, but I guess downvoting is just easier.",hbti8at,t1_hbsp7ba,1630942873.0,False
pin2ke,"There's one other interesting anecdote I thought of.  Pluralsight is an educational website for programmers that many employers sponsor as a resource.  They offer a test to check your proficiency in various languages, e.g. Java.  I took it less than a year ago and I think I got upper-intermediate.  I've gotten that same outcome on Spanish proficiency exams.  Relative to how long I'd been studying both when I took those, I had put much less time/energy in Spanish at that point.  


Again, how you define proficiency/fluency in a language is subjective, just ask Linguists what it means only in the context of human languages and you'll start a debate.  But just thought that illustrated my point.",hbs4qcs,t1_hbs44h5,1630910004.0,False
pin2ke,[deleted],hbqyqmr,t3_pin2ke,1630886798.0,False
pin2ke,">Of course most people on this subreddit will answer foreign language. I too, would say that learning a foreign language is harder.

biased? maybe, but not without reason. i speak a few languages myself, it's much easier to pick up a programming language.

the keywords in programming are English, it's easier to pick up and understand.

the writing system (like Chinese) also makes foreign languages difficult to pick up. sure, programming has symbols, but they are countable. Chinese needs a thousand to understand [89%](https://studycli.org/chinese-characters/how-many-characters-do-i-need-to-learn/). and as a bonus, being able to recognise symbols doesn't mean to speak it aloud, you just have to know how it's pronounced.

the syntax of programming language is pretty much fixed, whereas languages are more fluid and is filled with exceptions.",hbrl4xv,t1_hbqyqmr,1630897934.0,False
pin2ke,I dont think the question can be asked to people with no tech backgrounds as they cant answer the question if they never learned a programming language. Just like how it doesnt make sense for someone that speaks only 1 language to answer.,hbrmo7v,t1_hbqyqmr,1630898750.0,False
pin2ke,"Probably learning a foreign language, but hard to know. If I spent as much  time learning a foreign language as I did programming, I would probably be good at that language.",hbshdwf,t3_pin2ke,1630920729.0,False
pin2ke,"Programming languages are easier because the number of subjects, objects, and verbs is orders of magnitude smaller than a written/spoken language, and they all share common constructs like looping, if-then, etc. The key words in a programming language could fill a couple pages, a spoken language dictionary stretches to thousands of pages. But knowing the words is only part of it … it’s putting them together coherently that matters.",hbsrz2w,t3_pin2ke,1630929452.0,False
pin2ke,Foreign language.,hbswg3z,t3_pin2ke,1630932302.0,False
pin2ke,I'd rather learn moo,hbt0zck,t3_pin2ke,1630934799.0,False
pin2ke,Learning foreign language IMO.,hbt5zec,t3_pin2ke,1630937336.0,False
pin2ke,"You should learn how to program well, you should not learn many programing language and your skills still suck.",hbtbd8x,t3_pin2ke,1630939860.0,False
pin2ke,"Consider the flexibility and malleability of the human brain when thinking about ways to communicate.  Human languages are born from that.

Consider the limited scope of all CPU instruction sets.  Computer languages are born from those.  

If you look at them as containers, one is a thimble and the other is an ocean.",hbtkq6o,t3_pin2ke,1630943952.0,False
pin2ke,Foreign language no doubt. And I’ve learned foreign and programming languages,hbtvgk4,t3_pin2ke,1630948595.0,False
pin2ke,Depends on the languages really... Chineses / C++? English or JavaScript? etc,hbtw02q,t3_pin2ke,1630948829.0,False
pin2ke,"Foreign language by far.

Learning a programming language is like memorizing puzzle pieces. Putting everything together is the hardest part.

Meanwhile learning a foreign language is like slowly finding the puzzle pieces over time, forcing them to fit together, and almost finishing the puzzle - only to realize there's 83 more sections of the puzzle to do.",hbtwqsw,t3_pin2ke,1630949148.0,False
pin2ke,Foreign language,hbuaiog,t3_pin2ke,1630955158.0,False
pin2ke,Foreign language by far. I struggled to learn German and Spanish,hbue8tl,t3_pin2ke,1630956815.0,False
pin2ke,"Programming is a skill you learn, new languages can be learned quickly in programming, Programming languages only really have different syntax",hbuerur,t3_pin2ke,1630957048.0,False
pin2ke,"I've been unable to learn Spanish in 16 years of being married to a native speaker, yet I can learn a programming language in a weekend.",hbufm95,t3_pin2ke,1630957424.0,False
pin2ke,"Learning a spoken language is about 100x harder than learning a programming language.

I speak arabic and studied greek latin german french japanese and turkish and i code in C Scala Python and R for reference.

It took ten years to become comfortable speaking Arabic, but it was probably more like a few months to learn all the python syntax i needed to know to develop.",hbui6gw,t3_pin2ke,1630958553.0,False
pin2ke,Foreign language,hbujq8u,t3_pin2ke,1630959256.0,False
pin2ke,"I'm doing both! 

Definitely the language, there's a lot of nuance in human language. 

Code is basically a smart way of saying do this do that. From my understanding.",hbulauh,t3_pin2ke,1630959961.0,False
pin2ke,"I’d guess jumping from one C-style language to another to be like jumping around latin languages. The more foreign the language or the paradigm, obviously harder. 

Just like language learning, knowing grammar well has always the benefit of passing over and helping. Sometimes you end up sounding like a foreigner by bringing over language styles, but that doesn’t make you wrong.",hbuqlqh,t3_pin2ke,1630962330.0,False
pin2ke,'Which of these skills is a more valuable/relevant asset in my life?' Is a more appropriate question.,hbur2xu,t3_pin2ke,1630962547.0,False
pin2ke,"Foreign language. Learning a programming language is the easy part, it's applying it that's the hard part.",hbut0ef,t3_pin2ke,1630963420.0,False
pin2ke,I've done both as an adult. Foreign languages are much harder and take much more time. There's really no comparison.,hbuw2ky,t3_pin2ke,1630964831.0,False
pin2ke,foreign language not even close,hbv0urv,t3_pin2ke,1630967083.0,False
pin2ke,The only one that's harder than a foreign language is binary.,hc4so7z,t3_pin2ke,1631152075.0,False
pilhhj,I dont think dsa will ever be obsolete,hbqf8so,t3_pilhhj,1630877315.0,False
pilhhj,"DS&A is probably the most important foundational CS course. I mean, you’re not going to be doing any machine learning without algorithms.",hbqg2zz,t3_pilhhj,1630877696.0,False
pilhhj,"> Bad programmers worry about the code. Good programmers worry about data structures and their relationships.  
  
- Linus Torvalds 
  
Data structures and algorithms (and how they interact) are literally the foundation of any branch of computer science *especially* ML/AI. Asking if they are obsolete is like an art major asking if learning color theory is obsolete.",hbqhm8q,t3_pilhhj,1630878418.0,False
pilhhj,"I will never dismiss the importance of DSA in computer science but it all depends.

If you simply want to create neural networks and do some machine learning using PyTorch, I totally think you could get by without having the most firm DSA understanding. However, if you want to be a sought-after data scientist/machine learning programmer that designs and implements machine learning models with much more control and granularity, then I believe DSA is a fundamental concept that you will need to have a deep understanding of to create the next generation machine learning models.

Tl;dr it is never ever ever a bad idea to study and get a better understanding of DSA because it will help in every aspect of CS but I think you could get by at a surface level without it using mature frameworks that implement those data structures and algorithms for you.",hbqljy8,t3_pilhhj,1630880297.0,False
pilhhj,"Obsolete? No, even if machine learning had no use for traditional data structures and algorithms, that wouldn't make learning them obsolete. Despite what all the hype would let you believe, ML is not all there is, and won't be.

Anyway, as for the first part of the question: basic data structures and common algorithms knowledge will probably come in handy when doing data preprocessing, and most of the programming work in machine learning is about managing and processing the data. The knowledge will also certainly be important to anybody who's implementing the actual ML algorithms, although few of the people working with ML are going to be doing that.

They probably aren't particularly central to ML specifically, but they're occasionally useful for many kinds of programming, including what you might be doing when pushing data around.",hbt4ygc,t3_pilhhj,1630936830.0,False
pigh0a,"You'd be surprised how quickly you can use 1TB of RAM. When I was at Intel, our GPU/CPU clusters each had 1TB of ram for running ML experiments. I was generating a huge amount of synthetic data to train our models, and was filling up about half the machine just with lists of triplets for our loss function.",hbpnssh,t3_pigh0a,1630864975.0,False
pigh0a,That's also a great example of how 1TB is an absurd amount for a consumer device but becoming increasingly common in industry,hbqhqiu,t1_hbpnssh,1630878475.0,False
pigh0a,"Really? Can you elaborate more on the list? Also, were you using python?",hbqfhm2,t1_hbpnssh,1630877427.0,False
pigh0a,"It was literally just a numpy integer array of size Nx3, where N was large.",hbqhan1,t1_hbqfhm2,1630878264.0,False
pigh0a,Images?,hbr2m48,t1_hbqhan1,1630888688.0,False
pigh0a,"No, we were doing contrastive learning for registration of 3d point clouds.",hbr2vfu,t1_hbr2m48,1630888812.0,False
pigh0a,"If that was training data, wouldnt you have needed it on gpu memory and not host memory?",hbsbiof,t1_hbpnssh,1630915463.0,False
pigh0a,"Training datasets are frequently larger than GPU memory. You only *need* to put the batch you're currently operating on on the GPU, the rest you can store anywhere and shuffle between the GPU and an alternative memory pool.",hbswrlm,t1_hbsbiof,1630932489.0,False
pigh0a,"Any processing that has a data set of that size could be done faster  than if the information has to be fetched from drives. So, AI, data fusion (e.g.: synthetic aperture radar). You'd probably run into data bandwidth problems but if you could parallelize the processing, it might not be an issue.",hbpi1e5,t3_pigh0a,1630862441.0,False
pigh0a,"I'm curious now what the effective limit is on something like a CPUs average time to iterate some computation over the entire percentage of the RAM memory space where the data is (or for simplicity just all of RAM). Maybe put limits like the code can't have pathological branch-prediction failure behavior, cache-thrashing access patterns,  sequential floating point divisions, etc. and then see how much RAM the CPU can use in the time it could take to load data from a top-of-the-line NVMe PCIe M.2 SSD at its max sequential read rate.

There's a WD Black SSD with advertised sequential reads up to 7 GB/s and random read IOPS of 1,000,000, and sequential writes up to 5.3 GB/s and random write IOPS of 720,000.  ([source](https://www.gamingpcbuilder.com/ssd-ranking-the-fastest-solid-state-drives/))

The Wikipedia page for DDR5 RAM has a figure of 51.2 GB/s per channel as the max supported bandwidth, with up to *6400 million* transfers per second.

So that's still 7-10 time faster if we just look at the GB/s values (ignoring all the complications added by transfer rates involving particular possibly varying size transfers, access patterns, read/write queueing, etc.). Taking AVX-512 instructions running at .25 instructions per clock-tick at a clock of 2.8 GHz ([source](https://travisdowns.github.io/blog/2020/01/17/avxfreq1.html)) that's a processing throughput of approx. 700,000,000 (2.8 GHz / 4 = 700 MHz) \* 64 (bytes per 512 bit register), or about 45 GB/s (not GiB/s but 10^(9)). 

So taking all my napkin-math for close-ish, you could process your way through all of that 1 TB of RAM (or are we talking 1 TiB of RAM?) every 20 seconds, in which time a single SSD would only be able to provide \~100 GB of newly read data.

What about multiple SSDs? If the data isn't being generated while the program runs, but read into RAM for processing, could multiple SSDs sharing the PCIe bus close the performance gap?",hbqq6np,t1_hbpi1e5,1630882558.0,False
pigh0a,I *think* solving this generally requires solving the halting problem. XD Good luck! XD,hbrucns,t1_hbqq6np,1630903051.0,False
pigh0a,i am not the expert but pretty sure someone will come up with a way to have more porn,hbqjje2,t3_pigh0a,1630879320.0,False
pigh0a,You can generate your own using a GAN,hbrrn3z,t1_hbqjje2,1630901445.0,False
pigh0a,Open a 1TB text file,hbqh4vx,t3_pigh0a,1630878191.0,False
pigh0a,Chrome wants in on the fun,hbt958b,t1_hbqh4vx,1630938843.0,False
pigh0a,so in case of txt files the size doesnt get divided into small pages to be allocated in ram?,hbrzu2h,t1_hbqh4vx,1630906573.0,False
pigh0a,Behind the scenes it probably does. My answer was more of a joke.,hbt8tk9,t1_hbrzu2h,1630938690.0,False
pigh0a,AA OK,hbtiwgx,t1_hbt8tk9,1630943165.0,False
pigh0a,"I'd configure the entire OS to be loaded into RAM at boot. That's actually how Distros like [Puppy Linux](https://puppylinux.com) are able to run so well on old hardware. I'd certainly have more space after that, so I'd probably mount everything else that I know I wouldn't worry about persisting if the computer randomly turned off. Things like temp tables and big caches for SQL DBs. I'd imagine Minecraft chunks would load super fast if the map was mounted to a RAMdisk first, but again there's a bit of a concern of losing data if any writes are made and not properly persisted to disk.",hbpyhsj,t3_pigh0a,1630869780.0,False
pigh0a,"For the persistence problem, you could just make half of it a ZFS ARC, so requests for e.g. Minecraft chunks are served from memory and not disk.",hbrujm3,t1_hbpyhsj,1630903171.0,False
pigh0a,[deleted],hbscclm,t1_hbpyhsj,1630916177.0,False
pigh0a,"Not that I know of, but I use Linux as my daily driver so I wouldn't know enough. 

I just of a way you could make it work, though I it would require a lot of set up. If you had a [QEMU/KVM](https://wiki.archlinux.org/title/QEMU) setup on  a computer, the host would be running Linux, but act as a hypervisor for any OS that you'd want to run top of it. Virtualizing like this gives you near native performance in the VM because the host machine is typically not running anything else. In this situation, you could configure the host machine to have a RAMDISK, which on Linux allows you to mount a certain path to RAM. If you had a Windows VM and configured the boot partition to be mounted to a RAMDISK at boot, you could have the entire VM OS in RAM when you boot the VM. 

You wouldn't get the same performance, but you could also try this same idea out with running a normal VM through VirtualBox or VMware on any Linux distro. You'd certainly be able to get idea of the performance difference though.",hbvt1rr,t1_hbscclm,1630981015.0,False
pigh0a,Run your entire application off of Redis.,hbpqenc,t3_pigh0a,1630866143.0,False
pigh0a,Use android studio without any issues,hbqi2os,t3_pigh0a,1630878633.0,False
pigh0a,Have 100 tabs on Chrome,hbpexlb,t3_pigh0a,1630861063.0,False
pigh0a,More like <50,hbpp2q6,t1_hbpexlb,1630865544.0,True
pigh0a,[Somebody found out](https://youtu.be/7iwgyzX-76g),hbq3akq,t1_hbpexlb,1630871892.0,False
pigh0a,I had 400 tabs on a 4GB Ram potato PC lol,hbrp3cm,t1_hbpexlb,1630900030.0,False
pigh0a,Having the entire bitcoin blockchain in memory in a faster querying format was something I did with an AWS instance. Took around half a TB. Was necessary for lots of analysis across the entire history.,hbr7wmg,t3_pigh0a,1630891301.0,False
pigh0a,"I’m curious, what kind of analysis did you do?",hbtq2xw,t1_hbr7wmg,1630946267.0,False
pigh0a,"finally be able to declare :

int array\[250000000\];",hbqunt2,t3_pigh0a,1630884769.0,False
pigh0a,"Nope, soft limits.",hbsep5t,t1_hbqunt2,1630918267.0,False
pigh0a,"two chicks at the same time, man.",hbqcd40,t3_pigh0a,1630875979.0,False
pigh0a,Don’t mistake “ram” meaning then,hbqhsyv,t1_hbqcd40,1630878508.0,True
pigh0a,Two *RAM sticks* at the same time man!,hbrvkw5,t1_hbqcd40,1630903808.0,False
pigh0a, Be able to run google chrome with 5 open tabs,hbqmikq,t3_pigh0a,1630880765.0,False
pigh0a,I process high-density terrestrial lidar data for my research. 1 TB ram would mean I could have a bunch of jobs running in parallel (each take \~100GB) which would reduce processing time significantly.,hbqy59j,t3_pigh0a,1630886506.0,False
pigh0a,"If you have a gigantic disk array, 1TB RAM is *enough* for the controller to manage and cache relatively large percentage of hits.

And yes, machine learning and matrices. Definitely matrices.

Lastly, SMP machines with 16+ sockets can make use of a lot of RAM. In that case, 1TB might not be enough, even.",hbs696c,t3_pigh0a,1630911146.0,False
pigh0a,Finally run Chrome.,hbpolbm,t3_pigh0a,1630865325.0,False
pigh0a,I could brag about how much ram ive got,hbrmas5,t3_pigh0a,1630898550.0,False
pigh0a,Open 3 chrome tabs,hbsrure,t3_pigh0a,1630929368.0,False
pigh0a,You could like load 1TB of stuff into it yo.,hbqlsho,t3_pigh0a,1630880412.0,False
pigh0a,"Have all your games entirely in ram and laugh at ps5's ""ssd technology"" :)",hbqol1h,t3_pigh0a,1630881773.0,False
pigh0a,Run chrome,hbr1fei,t3_pigh0a,1630888115.0,False
pigh0a,Nothing that you can’t do with 640K,hbr28tz,t3_pigh0a,1630888509.0,False
pigh0a,Run so many browser tabs,hbrgbo0,t3_pigh0a,1630895480.0,False
pigh0a,Minecraft,hbrh2o5,t3_pigh0a,1630895856.0,False
pigh0a,Use google chrome,hbrseo8,t3_pigh0a,1630901890.0,False
pigh0a,"Usually that amount of RAM is used for virtualization.  Also, some bare metal apps we use at work are semiconductor design simulations, AI research, etc...",hbrxtrh,t3_pigh0a,1630905238.0,False
pigh0a,Run Chrome,hbs3l70,t3_pigh0a,1630909166.0,False
pigh0a,Run my shitty code,hbs4z0f,t3_pigh0a,1630910183.0,False
pigh0a,We have a couple TB of ram in the datacenter at the city i work at.,hbs5rzl,t3_pigh0a,1630910786.0,False
pigh0a,RAM disk for Chia?,hbs8f7r,t3_pigh0a,1630912863.0,False
pigh0a,Hopefully play Saints Row 2 at max settings with a stable frame rate,hbs9h82,t3_pigh0a,1630913724.0,False
pigh0a,Maybe we are heading in this direction with non-volatile memory architecture...,hbsf3uf,t3_pigh0a,1630918643.0,False
pigh0a,Put more data in it. The end.,hbsh51n,t3_pigh0a,1630920505.0,False
pigh0a,I would opened 300 tabs in chrome 🙃,hbskefp,t3_pigh0a,1630923438.0,False
pigh0a,Use google chrome finally,hbskpn4,t3_pigh0a,1630923706.0,False
pigh0a,Open chrome and slack at the same time,hbsnrof,t3_pigh0a,1630926300.0,False
pigh0a,Two google chrome tabs,hbsqjfs,t3_pigh0a,1630928434.0,False
pigh0a,Idk RDP and stuff,hbsrk60,t3_pigh0a,1630929160.0,False
pigh0a,simulate windows 11,hbtmzyb,t3_pigh0a,1630944940.0,False
pigh0a,Our high-level quantum chemical calculations eat RAM for breakfast. A fancy coupled cluster calculation will hit  the 1TB mark somewhere around 20 atoms.,hbx17yl,t3_pigh0a,1631012334.0,False
pigh0a,run crysis,hbpfdas,t3_pigh0a,1630861257.0,False
pigh0a,Legit answer,hbpij94,t1_hbpfdas,1630862659.0,False
pigh0a,Multiple instances of Android studio with emulator !,hbqducz,t3_pigh0a,1630876666.0,False
pigh0a,I’d keep computing until it was all gone.,hbqfn3q,t3_pigh0a,1630877496.0,False
pigh0a,"Even I have a chance to have 1T ram on my computer, I think 90% will serve by chrome",hbqg5ev,t3_pigh0a,1630877728.0,False
pigh0a,More porn,hbpgl67,t3_pigh0a,1630861799.0,False
pigh0a,My SharePoint farm performance would be great if the databases could load up everything into memory.,hbqg41i,t3_pigh0a,1630877710.0,False
pigh0a,Calculate pi to infinity digits,hbqll0l,t3_pigh0a,1630880311.0,False
pigh0a,"Quake server!

(I'm old)",hbr2map,t3_pigh0a,1630888691.0,False
piejh5,"The lesson here is that you should not use ambiguous words like ""word"". Just say 32-bit int. ""Word"" originally meant the natural size of the GPU, but with x86 which was originally a 16 bit instruction set, this may still mean 16 bit. And it might mean 32 bit in gdb, for historic reasons again.",hbpguk7,t3_piejh5,1630861917.0,False
piejh5,"""word"" is the basic native register size of an ISA. It's the default and most efficient size. On a 32bit processor, a word if 4 bytes, it's 8 on a 64bit machine and 2 on a 16bit one. To answer your question, we would need to know which ISA you are using.

I'm not super familiar with x86 but I have this vague memory that they do something dumb and kept using ""word""  for the same size even after changing the ISA to use a larger size. Hopefully someone more familiar with it can help if you're using x86.",hbp0llk,t3_piejh5,1630854728.0,False
piejh5,"So i'm on a i686/32 bit Linux virtual machine running on a amd64 host. When I compile the following code on both virtual machine and host: 

    section .data
        var dw 0xaabbccdd

I get an error that says: 

    helloworld.nasm:25: warning: word data exceeds bounds

It compiles fine with two bytes of data. 

Let's assume that a word is 2bytes. Even then, executing the following command in gdb : 

    x/xw &var

prints 0xaabbccdd. that is, 4 bytes.

and 

    x/xh &var

prints two bytes.",hbp2zgt,t1_hbp0llk,1630855794.0,True
piejh5,"Ok, So I did a bit of research and found that on both x86 and x86\_64. A word is 2 bytes.x86 

>Architecture  
In the x86 PC (Intel, AMD, etc.), although the architecture has long supported 32-bit and 64-bit registers, its native word size stems back to its 16-bit origins, and a ""single"" word is 16 bits. A ""double"" word is 32 bits. See 32-bit computer and 64-bit computer.

source : [PCmag.com](https://PCmag.com)

But then the question remains, why is gdb's examine command treats a word as 4 bytes.",hbp6vy2,t1_hbp0llk,1630857520.0,True
piejh5,"It's definitely an ambiguous term. After making this post I went and had beers with my computer architecture friends and our consensus was that the most general definition is what I said (the native register size), but because ISAs change and documentation doesn't (especially for heavily standardized things like ISAs), it's really whatever the register size was when the documentation was written lol.",hc0p3dc,t1_hbp6vy2,1631073111.0,False
piejh5,"I’ve always heard of words being 32 bits. Fun fact, in early computing how many bits per byte varied, which may be the problem you’re running into",hbozkvb,t3_piejh5,1630854269.0,False
pi9l3k,"The engineer solution is ""use GPS""",hbobkj1,t3_pi9l3k,1630840728.0,False
pi9l3k,"I just linked a few in [a different comment](https://www.reddit.com/r/computerscience/comments/pi9l3k/time_sync_in_decentralized_network_where_time/hbohlzx/?utm_source=reddit&utm_medium=web2x&context=3), but I'm curious, do you have experience with any particular solution?

This also sadly doesn't ""prove"" anything, but it probably solves the practical problems as long as it's implemented properly and there is no significant hardware failure.",hbohqjc,t1_hbobkj1,1630845155.0,False
pi9l3k,That's what this is meant to solve: https://engineering.fb.com/2021/08/11/open-source/time-appliance/,hbo9t50,t3_pi9l3k,1630839258.0,False
pi9l3k,"Thanks, nice article. Atomic clocks are rare and too expensive currently to assume most of people at the planet will have at least one, I hope they will become cheap over the time.  
That component is using gps satellites as authoritative source of time. Which is what I would like for a network to have access to.  
But btw, since GPS satellites are very old technology,  
does they have protection from SEE (Single Event Effect) ?",hboeoh0,t1_hbo9t50,1630843070.0,True
pi9l3k,"If you read the article, one of the lower cost options instead of an atomic resonator is to use a TCXO (Temperature Controlled Crystal Oscillator), which are *surprisingly* affordable.

A great example is the [IQD E3199LF](https://www.iqdfrequencyproducts.com/products/details/e3199lf-5-30.pdf), which seems to sell for around $15 a pop, or down closer to $10 if you buy enough of them. The real cost is all the integration work, etc. There's even a [nice little TCXO module](https://www.newegg.com/p/2CS-0053-04Y71) available for HackRF, which might be easy enough to get working.

There's also COTS solutions like the [Masterclock PCIe-GPS Network Card](https://www.masterclock.com/products/pc-cards/pcie-gps), or the [GPS170PCI](https://www.meinbergglobal.com/english/products/pci-gps-clock.htm).

As long as you have sporadic GPS reception (maybe once an hour) you can keep drift down belong 1-2 ms with these inexpensive oscillators.",hbohlzx,t1_hboeoh0,1630845074.0,False
pi9l3k,You didn't put money as a constraint on the problem.,hbqcqpb,t1_hbohlzx,1630876152.0,False
pi9l3k,I think you responded to the wrong person.,hbr80oq,t1_hbqcqpb,1630891357.0,False
pi9l3k,"PTP is used for this where milisecond precision is required (for example frame level synchronisation in video processing).  It requires a very good time source and hardware (network switch) support.

https://en.wikipedia.org/wiki/Precision_Time_Protocol",hbpj6ek,t3_pi9l3k,1630862942.0,False
pi9l3k,"**[Precision Time Protocol](https://en.wikipedia.org/wiki/Precision_Time_Protocol)** 
 
 >The Precision Time Protocol (PTP) is a protocol used to synchronize clocks throughout a computer network. On a local area network, it achieves clock accuracy in the sub-microsecond range, making it suitable for measurement and control systems. PTP is currently employed to synchronize financial transactions, mobile phone tower transmissions, sub-sea acoustic arrays, and networks that require precise timing but lack access to satellite navigation signals. The original version of PTP, IEEE 1588-2002, was published in 2002.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hbpj7vj,t1_hbpj6ek,1630862961.0,False
pi9l3k,IEEE-1588,hbpci47,t3_pi9l3k,1630859991.0,False
pi9l3k,"The answer to this largely depends on the timing guarantees of the network. If you have no timing guarantees on the network, you'll need some sort of out-of-band communications which you've not specified here.

Also, are you looking for an algorithm that shows this result with mathematical \_certainty\_, or are you willing to accept some bounded probability that a misbehaving node will be accepted to the network.

Likewise, are you concerned about clock drift during the connection?

This smells a lot like an [XY problem](https://xyproblem.info/).",hboi14j,t3_pi9l3k,1630845344.0,False
pi9l3k,"For sure, network will probably have misbehaving nodes, so any node should not trust any other node out of air.  It is important for honest node to choose correct sources for clock set up before they join the network.

>Likewise, are you concerned about clock drift during the connection?

Once correct timestamp is established, monotonic clock should measure clock draft during connection for generating future timestamps which will be used for future needs.

As far I know, monotonic clock is good and precise enough for measuring elapsed time?
I hope it is not XY problem. 

In short, what I want to achieve is to have accurate time in trust-less decentralized network where is impossible to manipulate time accuracy or to be extremely difficult, like abnormal difficult.",hbonw5i,t1_hboi14j,1630848739.0,True
pi9l3k,">For sure, network will probably have misbehaving nodes, so any node should not trust any other node out of air. It is important for honest node to choose correct sources for clock set up before they join the network.

Maybe you want a [web of trust](https://en.wikipedia.org/wiki/Web_of_trust) where nodes can certify other nodes, and if a node has more than 60% of signatures of other nodes (or whatever value you pick) then it's time is ""trusted"".  


>Once correct timestamp is established, monotonic clock should measure clock draft during connection for generating future timestamps which will be used for future needs.

There be dragons. If you think you know how timing works on a computer, and you didn't both write the code and design the hardware, you're probably mistaken. I've mucked about with various low-level timing coprocessors and hardware-based profiling timing code, etc, but I would not consider myself competent to answer your question exactly.

Here's an interesting discussion of Linux CLOCK\_MONOTONIC that might be useful:  
[https://stackoverflow.com/questions/3523442/difference-between-clock-realtime-and-clock-monotonic](https://stackoverflow.com/questions/3523442/difference-between-clock-realtime-and-clock-monotonic)

IDK what operating system you're working with. If it's Windows, GFL. If it's Linux, well... GFL, but you won't need as much luck. CLOCK\_MONOTONIC is apparently not monotonic (NTP can affect it). If you used some kind of slow-sync monotonic adjustment scheme (you never adjust a clock backwards) to create your own clock backend on CLOCK\_MONOTONIC\_RAW where you'd sync with NTP now and then, but introduce those adjustments slowly (say, over the course of an hour or a day instead of all at once). Oh, and BTW if your system hibernates or is in a VM, that can affect it too. And CLOCK\_MONOTONIC\_RAW drift can be significant and might have other limitations/quirks depending on hardware. If you're not using specialized hardware and you actually care about the results of your code, (i.e. you're doing something important and irrevocable, like financial transactions or launching a missile or something) you're probably fscked without hardware support.

&#x200B;

>In short, what I want to achieve is to have accurate time in trust-less decentralized network where is impossible to manipulate time accuracy or to be extremely difficult, like abnormal difficult.

If you have an environment where you don't trust ANY node in the network (except your central server) then you can have temporal tokens introduced by the central server which allow a node to prove that a piece of data was sent *after* the time when the token was generated. You can also have a token-request system wherein one node can ask a central server to sign something (that something can be encrypted in a way the server can't read) to attest that it was received at a certain time.

If you don't trust \_any\_ node (no central server) including NTP, I mean... who can you trust?

Also you might look at [trusted timekeeping](https://en.wikipedia.org/wiki/Trusted_timestamping).

Sounds like what you're trying to do is difficult and may not be possible in principle without some compromises you haven't indicated you're willing to make yet. Interesting problem though, good luck!",hbqajja,t1_hbonw5i,1630875141.0,False
pi9l3k,"Your comment is very informative to me.

web of trust or similar would not fit into a vision.  
Network access must be equal for anyone (permission-less). So, central authority or fixed identity must not be involved, otherwise censorship would be achievable.

I started to think of time as only thing which is virtual-imaginary that affects our reality and at same time it is present everywhere. You can not touch it, control it, break it or be sure the time you think of is present or not.

All that is just because it changes its shape constantly.

I realized by now, compromise would be only way to achieve one step closer to what I want.

Thank you very much.",hbuaprr,t1_hbqajja,1630955247.0,True
pi9l3k,"https://en.m.wikipedia.org/wiki/Network_Time_Protocol

This might be relevant",hbomrjn,t3_pi9l3k,1630848125.0,False
pi9l3k,"**[Network Time Protocol](https://en.m.wikipedia.org/wiki/Network_Time_Protocol)** 
 
 >The Network Time Protocol (NTP) is a networking protocol for clock synchronization between computer systems over packet-switched, variable-latency data networks. In operation since before 1985, NTP is one of the oldest Internet protocols in current use. NTP was designed by David L. Mills of the University of Delaware. NTP is intended to synchronize all participating computers to within a few milliseconds of Coordinated Universal Time (UTC).
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hbomsr1,t1_hbomrjn,1630848143.0,False
pi9l3k,"I have considered NTP as one of alternatives/fallback, but I  see them as single point of failure which is easy to corrupt.Each system have single point of failure which needs to be secured.For decentralized network it is joining process which usually first step is to ask dns server where are the other nodes. By shutting down dns server, you will disable new nodes to join the network, old nodes will use already known node list until isp change their ip address, and finally, whole network is down.

Relying on NTP would be like adding one more single point of failure. Btw, there are more chances for corrupting ntp server then dns and isp together.Network joining process is one more problem to solve after this too.",hbor34b,t1_hbomrjn,1630850376.0,True
pi9l3k,"Desktop version of /u/whygohome's link: <https://en.wikipedia.org/wiki/Network_Time_Protocol>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hbomsq8,t1_hbomrjn,1630848143.0,False
pi9l3k,Ask the network what the time is and then return the answer?,hbohpsa,t3_pi9l3k,1630845141.0,False
pi9l3k,"GPS sync, it's how telecom has been doing it since forever.",hbphe95,t3_pi9l3k,1630862158.0,False
pi9l3k,Trimble gm200 and ptp,hbpwhxw,t3_pi9l3k,1630868889.0,False
pi22kr,"I’m not the best to comment on this subject but i’ll give it a shot.

Essentially GPUs are specialized processing units that are designed to handle highly parallelizable tasks. Often this means things specifically like matrix operations which are highly important for graphics. Of course GPUs can also be used for other brute force calculations, from crypto currencies to machine learning. 

tl;dr the GPU can accelerate some tasks by doing them in a highly parallel process rather than sequentially as might be done in a cpu.

Please everyone else correct me or add on to this explanation if you can. :)",hbmmlmg,t3_pi22kr,1630799355.0,False
pi22kr,"To add to this, to the amount of cores on a cpu is usually less than 8. But a GPU has hundreds, each that can handle thousands of threads. So you could imagine the necessity when it comes to high demanding programs like video games.",hbn8fs2,t1_hbmmlmg,1630810810.0,False
pi22kr,"Each GPU core can do much less than a CPU core, but in terms of the use cases stated above, it works way better, since no complicated single threaded instructions are required.",hboipjy,t1_hbn8fs2,1630845770.0,False
pi22kr,"gpu's are like cpu's but both are specialized for different tasks

cpu is optimized to perform a few low latency complex operations each time step

gpu is optimized to perform many simple operations each time step

cpu == complex, low latency, few threads

gpu == simple, high latency, thousands of threads

cpu's are great for performing a single very complex bit of math

gpu's are great for performing the same math to millions of data points (think high throughput)

this is why gpu's are primarily used for rendering, graphics, and machine learning",hbmzdnz,t3_pi22kr,1630805952.0,False
pi22kr,">What kind of instructions does it complete?

GPUs often run special programs called ""shaders"". These programs are designed to run on hundreds of computer cores simultaneously, all accessing and working with the same pool of memory.

&#x200B;

>What does it need to do for the display to work?

I'll take a very simple problem that GPUs are used to accelerate, scrolling a page up and down. A CPU is more than capable of preforming this task, but it doesn't do it well. This is because a CPU needs to iterate over each and every pixel of your window to ""draw"" the scroll animation one at a time. A GPU however would run the shader program on a bunch of cores at the same time to create the next frame of the scrolling window. 

The trick here is that it's very easy to know what each individual pixel needs to be set to, but you rarely NEED to know a pixels neighbor's color in order to find out it's own color. These kinds of programs work best when divided up and executed simultaneously.",hbndv8s,t3_pi22kr,1630813913.0,False
pi22kr,">but you rarely NEED to know a pixels neighbor's color in order to find out it's own color.

But you do need it when scrolling.",hbniyia,t1_hbndv8s,1630817122.0,True
pi22kr,"I think some more general examples would help here:

**Example 1:**

Let's imagine you want to invert all the colours in an image e.g. for each pixel you want to do something like this:

    output_colour.r = 1 - input_colour.r
    output_colour.g = 1 - input_colour.g
    output_colour.b = 1 - input_colour.b

This pseudocode is actually very similar to what you would write if you were implementing this in a real fragment shader. You write code that is only concerned with outputting a single pixel, the CPU tells the GPU to run it once over your input image, and the GPU runs it once for every pixel in the output

This sort of task is ideal for a GPU because you will always end up with the same output image for an input *regardless of the order the pixels are processed in*. The GPU can safely divide these thousands or millions of tiny tasks across all of its many, many cores

**Example 2:**

Now let's look at something that a GPU would be very bad at. Let's say for some reason you want to run through the pixels in the input image row by row, and for each one output the average of all the pixels that came before it. This would be simple to write for the CPU:

    num_pixels_seen = 0
    total_colour_value = colour(0, 0, 0)
    for x, y, input_colour in input_image:
        total_colour_value += input_colour
        num_pixels_seen += 1
        output[x, y] = total_colour_value / num_pixels_seen

However, you can't really write this as a shader, because you can't parallelise it. Each pixel relies on the output pixel directly to its left, which relies on the output pixel directly to *its* left, and so on. We have to process the first pixel, then the second, then the third etc. until we've done the full image, so you can only use one core at a time to run this program. CPUs are designed to be very good at this sort of thing - processing lots of elements of a data set one by one in a specific order

**So, going back to the scrolling example:**

To scroll in the way you're describing, you would have to take the output of the previous frame and render it again at a new offset on the screen. All your shader program needs to do is work out which pixel to read from the previous frame based on the old scroll offset, the new scroll offset and the on-screen position of the pixel it's currently outputting. There's nothing in there that depends on the output for another shader invocation within the same pass, so we can run this in parallel for every pixel",hbojnlg,t1_hbniyia,1630846339.0,False
pi22kr,Great response!,hbpgftd,t1_hbojnlg,1630861733.0,False
pi22kr,">To scroll in the way you're describing, you would have to take the output of the previous frame and render it again at a new offset on the screen. All your shader program needs to do is work out which pixel to read from the previous frame based on the old scroll offset, the new scroll offset and the on-screen position of the pixel it's currently outputting. There's nothing in there that depends on the output for another shader invocation within the same pass, so we can run this in parallel for every pixel

I don't understand that. 

To render something at a new offset, you need to offset the image, and that requires you to check other pixels' values.",hbqdn89,t1_hbojnlg,1630876574.0,True
pi22kr,"But not other pixels generated within the same pass. It's OK to read from the last frame you generated because you're not modifying it in your current pass, it's just a static image in memory",hbsbpb2,t1_hbqdn89,1630915619.0,False
pi22kr,Read up on openGL and it gets you a sense of what a GPU is supposed to do.,hbmuany,t3_pi22kr,1630803324.0,False
pi22kr,"In order to display anything on the screen you need to determine the RGB values for each pixel, right? And you need to do it at a certain refresh rate, i.e. several times a second.

So GPUs handle just that. Usually the information needed to decode to determine colors is in some geometric form, essentially coordinates / parametric forms of shapes. Those are in practice vectors and matrices

So the architectures of GPUs are optimised for executing vector and matrix operations very efficiently. Most importantly, matrix multiplicatios, dot products and element-wise operations.

To no one's surprise, GPUs are found to be very fast in executing code in various domains like machine learning, numerical analysis and crypto mining (hashing operations). This is because all such domains are based on vector/matrix operations. That's why they are treated as small supercomputers.",hbofn0o,t3_pi22kr,1630843740.0,False
pi22kr,GPUs have really specialized cache systems that handle operations related to graphics. Matrix multiplication is one of the main operations that they handle super efficiently and they can get “close” to O(n^2) with this problem in particular. They run a ton of similar instructions at once and are really built for “embarrassingly parallel” problems that can be multithreaded (even ray tracing is considered embarrassingly parallel at the thread level).,hbp3ep8,t3_pi22kr,1630855982.0,False
pi22kr,GPUs enhance the screen basically.,hbt6px4,t3_pi22kr,1630937698.0,False
pi22kr,"a gpu does SIMD multithreading. SIMD is Single Instruction Multiple Data. You can sort of do these kind of operations on a cpu with AVX and MMX instructions. Basically, every ""core"" in a group (on nvidia this is called a warp) must preform the SAME EXACT CODE but with different data. So if you want to add list a to list b (c\[x\]=a\[x\]+b\[x\] for all x) you can do that way faster on a gpu. The down side of this is that conditional statements (if/else/while/for) are very slow. On a gpu BOTH the if and else blocks are executed for every single data/thread/core but the result is not stored if its in the wrong block (this is handled through predication registers https://en.wikipedia.org/wiki/Predication\_(computer\_architecture)) . 

This makes GPUs ideal for graphics processing because you can assign (for instance but this is not everything a gpu does for graphics engines) one triangle to each thread and each core will fill in one triangle to draw the whole image. since all the data is different but the instructions are the same, the gpu does this extremely fast.",hc5pm7l,t3_pi22kr,1631174263.0,False
pi22kr,"**GPU** originally stood for ""Graphics Processing Unit"". Chips were dedicated to generating a video output signal and had fixed and limited ability to manipulate that signal.

Then at some point **GPU** became know as ""General Processing Unit"". These chips had more generic processing abilities that could be easily repurposed to other types of processing besides video display signal processing, but still offered somewhat fixed data processing capabilities.

Today it would be more accurate to call the high-end graphics cards from AMD and nVidia ""**GPGPU**"": General-Purpose Graphics Processing Unit. Such high-end cards contain thousands of small processors who's instructions can be quickly and easily updated to process data that's easily divided into independent chunks. The common usage is in high-end video graphic processing (such as video games), but can easily be adapted to other forms of data processing not associated with video displays (such as BitCoin mining).",hbn2w4g,t3_pi22kr,1630807806.0,False
pi22kr,"Your best path to learning the answer you want lies in the difference between integer and real numbers. Standard processors optimize for integer (whole) number operations. GPUs optimize for real, or floating point, operations. Even back in the 90s if you got a 486sx that is a processor that dealt with INT. If you got a 486dx that was a math coprocessor that handled real numbers. GPUs are just insanely advanced math coprocessors. What started off as a coprocessor that could handle visual geometry better ended up as a processor that could handle a lot of applications better.",hbnc114,t3_pi22kr,1630812837.0,False
pi22kr,"This isn't true any more, as far as I know there's no real performance difference between int and float calculations on CPUs or GPUs. GPUs do have specialised hardware for matrix and vector maths (Although apparently [even that might be coming to CPUs too](https://en.wikichip.org/wiki/x86/amx))

The big advantage GPUs have is that they're able to parallelise code much more effectively than CPUs",hbo41hn,t1_hbnc114,1630834120.0,False
pi1z15,"So if I understand your question. Big omega is a lower bound of a function rather then the more common bigO being the upper bound. Having the lower bound be constant time is meaningless because every function has that. I would say that O(n^2) is not meaningless because there are plenty of possible algorithms that could be over that. However something like O(n^n^n^n) would be meaningless because everything is under it. Omega(1) is meaningless in the same way, just on the other side.",hbnkai5,t3_pi1z15,1630818034.0,False
pi1z15,"Okay gotcha, I feel like I was thinking about it this way but couldn’t conceptualize the lower bound",hbpdwbh,t1_hbnkai5,1630860604.0,True
pi1z15,"What do you mean by ""meaningless""?

O(1) means that the algorithm runs in constant time, regardless of the size of the input (n).  Think about problems like: find the first element of a linked list!  Or, find the middle element of an array!  It doesn't matter how many elements are in your list or in your array, your algorithm will take exactly the same amount of time regardless.

O(n^2 ) means that as the size of the input (n) goes up, the number of steps to complete the problem go up much faster (n^2 ).

Obviously, we prefer to use O(1) operations basically whenever we can.",hbn747o,t3_pi1z15,1630810079.0,False
pi1z15,"I think you are referring to asymtotic bounds. If so it's like me telling that I'm to tell you how much I money I earn using the following upper bound and lower bounds.

My income is bound by the function big O(n^7) dollars.    
The lower bound is Omega(1) dollar.    

Is that information meaningful?",hbpbtbt,t3_pi1z15,1630859688.0,False
pi1z15,"Yeah, I just need think of it like the upper bounds but the opposite",hbpe4tz,t1_hbpbtbt,1630860708.0,True
pi1z15,"^(It's the lower bound so you wouldn't expect it to happen unless you already know something about your data. Sometimes it does mean something. Tim-sort, for example, has really fast lower bound and can achieve closer to it if the data ""looks"" a certain way (like the concatenation of some presorted data and some random data). ""Meaningless"" isn't the word I would use to describe it. The lower bound says very little about the quality of an algorithm WITHOUT FURTHER CONTEXT. If the context is given it can mean a lot.)",hc5q79i,t3_pi1z15,1631174847.0,False
phzbuq,"A computer doesn't ""know"" anything. To the processor, all numbers are just unsigned integers that it moves around and does operations on. Everything else is just a coordinated way to interpret those numbers.",hbm60d6,t3_phzbuq,1630791162.0,False
phzbuq,You have to tell the machine how to read it. This is why you have to specify a type for varibles in programming.,hbm2zj8,t3_phzbuq,1630789746.0,False
phzbuq,The processor's instruction set defines the endianness and binary representation of negative numbers used when making calculations.,hbm7ypj,t3_phzbuq,1630792079.0,False
phzbuq,"A cpu instruction knows whether it is reading signed or unsigned. It also knows how many bits it should read.

For example, there may be two different instructions called 'mul' and 'mulu'. First one multiplies assuming operands are unsigned, and later one assumes operands are signed. 

Instructions are defined in CPU's instruction set architecture.",hbmby3h,t3_phzbuq,1630793998.0,False
phzbuq,"It doesn't care if the number is negative or not. The first bit indicates whether it is negative or not. That way when there is an overflow you get 0 and when there is an underflow you get -1 (for signed numbers).

You don't need to know whether it is positive or negative because you do the same operation, flip the bits of the first number add 1 and add the second number. The only thing necessary is that they are allowed to overflow.",hboe7rd,t3_phzbuq,1630842735.0,False
phzbuq,"so there's no inherent meaning to the general purpose registers on a cpu. 2s compliment is pretty clever in that the adders dont have to do anything special to implement it. subtraction works out so that you can do twos compliment and then add. sign extension works with twos  compliment. also there are instructions for ""arithmetic right shift"" which is really just twos compliment sign extended right shift (so you can divide negative numbers by powers of two).",hd1oxar,t3_phzbuq,1631770758.0,False
phd414,Llvm? Then llvm to what ever? I’m not sure myself. Llvm has the ability to do low level optimisations that aren’t relevant for high level languages,hbhr95k,t3_phd414,1630702773.0,False
phd414,I guess LLVM is the way to go. It is useful to learn anyway if you are remotely interested in Compilers.,hbk2dr2,t3_phd414,1630754504.0,False
phd414,Have you tried posting in r/programminglanguages ? It’s pretty active and full of helpful fellow language writers.,hbk3eho,t3_phd414,1630755261.0,False
phd414,"oh my gosh I can’t believe this one exists, thank you so much!",hblcmqt,t1_hbk3eho,1630777810.0,True
phd414,Nice. What does the syntax look like?,hbimkma,t3_phd414,1630718090.0,False
phd414,"It's pretty neat, very curly-brace-esque (after learning a bunch of languages, I found I liked curly braces). It has types as well, and the ability to make custom types. This code is not a perfect representation, but it works and my lexer can go through it no problem. I plan to add template strings of some kind (the current thought is to use the $ character as a variable-specific escape, then use \\$ to escape that if necessary), but that can come later,

This is definitely not the most efficient way to write this code, but it excersizes praline semantics well and gives me a good test-case.

    //Praline beta test file, #1
    //This is the file used to test my lexer
    
    /*
    Multi-line comment
    */
    
    type tree_array = (arg){
        rule: arg.is(array);
        rule: arg[*].is(string);
        rule: arg[*].length > 8;
    }
    
    let tree_arr types = ['deciduous', 'coniferous', 'evergreen'];
    
    int function matchType(tp = ['']){ //it's optional to declare types
        for(let i in tree_types[*].lower()){
            switch(i){
                ? goal_type:
                  log('match: ' + i);
                  return 1;
                  break;
                ? default:
                  log('no match: ' + i + '\n');
                  break;
            }
        }
        return 0;
    }",hbip67v,t1_hbimkma,1630719431.0,True
phd414,Nice! looks very solid. I like it. So it compiles and runs on the browser right?,hbipf2p,t1_hbip67v,1630719560.0,False
phd414,"Glad you like it, but it currently doesn't. That's the issue I'm working out now. I need it to compile, but I'm not sure what I should compile it down to. Hopefully it will be easy enough to run in-browser though. I'd like to make a [replit.com](https://replit.com) language for it!",hbiujpc,t1_hbipf2p,1630722227.0,True
phd414,Ah I see so you’re just interpretting it on the fly for syntax and lexical analysis. Not bad. I’d like to see a full versiom of this someday.,hbiw82z,t1_hbiujpc,1630723125.0,False
phd414,"There’s a few books you should read, one being “the dragon book” and the other being “engineering a compiler”.

From what I can remember, you want to compile to 3 address instructions (pseudo-assembly), optimize that and then allocate registers.

This makes the compiler easier to optimize and a bit more portable.",hbl4y5r,t3_phd414,1630774414.0,False
ph8cmg,"Yes, the truth of the entire if statement can be seen as a linear time function of the number of conjunctions. Same goes for arithmetic operations but in usual algorithm analysis you assume the time taken for a single comparison or arithmetic operation to be constant. Though the bite-wise complexities are different for say a greater than and multiplication.",hbgs8ow,t3_ph8cmg,1630687578.0,False
ph8cmg,I don't think you mean to say conjunction - probably proposition or maybe condition.,hbiqtkl,t1_hbgs8ow,1630720290.0,False
ph8cmg,"I'm drawing a blank on the exact name but processes will stop a conjoined boolean statement on the first failed AND or first passed OR. For example:

if{FALSE && x=2}

The x=2 operation will not be evaluated",hbk98n9,t1_hbgs8ow,1630759192.0,False
ph8cmg,Short circuiting.,hbka7l9,t1_hbk98n9,1630759786.0,False
ph8cmg,"It depends on what is meant by time complexity, though. Asymptotically it makes not difference to the time complexity of the algorithm.

Although the time it takes to evaluate the conditions is linear in the number of conditions, the number of conditions in each full expression is fixed and doesn't depend on the size of the input, so it can be treated as a constant-time operation.",hbk2w01,t1_hbgs8ow,1630754883.0,False
ph8cmg,"look at the resulting assembly, count the number of instructions. that should give you a *very rough* idea of how much things slow down.

Also, a similarly wrong metric is that the more operations your code does, the slower it is. Expression evaluation works the same whether in an if or any other place where you must evaluate an expression.

Conditionals do not work by magic, they work by evaluating an expression and comparing it with 0. So their execution speed obviously depends on the time it takes to evaluate the expression.

In practice, the compiler will often optimize trivial (and some non-trivial) things away, and caching, OOO execution, speculation, vector instructions and so on make the above comparsions rather wrong in many cases.

From a theoretic perspective, your ""time complexity"" is a formal definition which is independent of actual runtime, actual machines etc. It also ignores constant factors. Thus, if you perform all computations exactly twice, your asymptotic complexity will not matter (be careful that you don't actually execute things exponentially often when trying to run everything ""twice""). So both your ifs take O(1) to evaluate the condition.",hbh47ag,t3_ph8cmg,1630692597.0,False
ph8cmg,"Couple good questions here. If you have a compound conditional such as two conditions joined by &&, put the condition you think will fail more often first. That way the compound condition can often check just 1 condition and fail (i.e., it will “fail fast”). For compound conditions involving ||, put the one that succeeds more often first, so this decision can also often be made after just 1 check. This technique is called “short circuiting” if you want to learn more.

Regarding the result of an operation as a condition, try to avoid making performing the same operation needlessly. In your example, rather than compute 31/2 every time, store 31/2 in a constant earlier in the code and then refer to the constant. That way the computer performs the division only once. The technique of “memoization” includes ideas like this.

Finally, you might explore writing code that times how long it takes to perform blocks of your code. You can take multiple measurements of how long an implementation takes, then refactor and measure again to make comparisons of code speed. (Beware that sometimes CPUs cache code and results, so that can muddy your attempts to take measurements.)

Good luck!

edit: realized you were asking specifically about time complexity. Sorry, I’m off topic.",hbgswu5,t3_ph8cmg,1630687852.0,False
ph8cmg,"No, it's all good, and helpful nonetheless. Also with the 31/2 I just put random junk there for hypothetical purposes. Thanks!",hbgvyiz,t1_hbgswu5,1630689125.0,True
ph8cmg,"Very interesting post! Learned a lot here.

Do you think those concepts are similar to static code blocks in Java? Heres an example:

public static final int FOO;

static {
  FOO = 5;
}

What are the purposes of these (often times massive) static blocks in some Java code, and are they similar to memorization and short circuiting as you described?",hbguqv2,t1_hbgswu5,1630688617.0,False
ph8cmg,"Actually:

`if (x == 1)`

and 

`if (x+31/2>=1)`

may execute in the same amount of time.  The second expression is actually:

`if x >= -2.5`

which (ignoring the floating point aspect) is the same as the first expression in terms of what typical assembly instructions are generated.",hbja20f,t3_ph8cmg,1630731220.0,False
ph8cmg,"Time complexity wouldn't really be affected as these are constant operations *however* if the choice of conditions allows or prevents an early exit from the code -- a classic case being palindrome checking where you return false immediately if any two pairs of letters don't match (comparing from ends towards the middle) then maybe

Generally the use of constant operations will only really affect true runtime but not scalability with data (which complexity represents)

So for instance (I'm about to give bad programming advice, please don't apply broadly as it will make your code very hard to read) if you minimised the number of variable assignments and did as much in a single line that you could you would save quite a few operations overall. An example might be

Define window(list L){
  L=copy(L)
  While (length(L)>3){
    X=L[0]
    Y=L[1]
    Z=L[2]
    Sum = X+Y+Z
    Average = Sum/3
    Floored = Math.floor(Average)
    If (Floored == Average){
       Return false
    }
    DeleteItemAtIndex(L,0)
  }
  Return true
}
Which could equally be

Define window(list L){
  L=copy(L)
  While (length(L)>3){
    Average = ((L[0]+L[1]+L[2])/3)
    If (Math.floor(Average) == Average){
       Return false
    }
    DeleteItemAtIndex(L,0)
  }
  Return true
}

Edit: Reddit hates code and this won't show properly

This difference would make a minor effect on run time as it would reduce the amount of saving and loading of variables

There are lots of other things you can do for constant level savings (like flipping an if statement around depending on whether the if or else block is more common.

To answer your original question though

Provided the individual operations in the if statement don't scale as input size grows then they don't affect complexity

Arithmetic in conditions will also generally not affect complexity unless again there's a scaling based on input size however in terms of actual run time they would compared with not doing them.

To understand you kind of need to know what's involved in different operations but essentially

If X==1

Vs if (X+32/3>=1)

The == is a single assembly level (what most coding languages turn into before becoming zeros and ones) operation

Greater than or equal to Is usually two operations (as would be <, >, or <=)

Addition is one operation

Division is... More complex, let's pretend the result is a whole number and you don't care about remainders then you can call it two operations

This is ignoring the need to move the values between registers (a kind of temporary variable to hold any value being used in an operation) which would add another two operations per task

So in total X==1 is 1 operation (3 if you count the registers)
Whereas (X+32/3>=1) is 5 operations (11 if you count register assignment)

So the condition with Arithmetic is maybe 3-4 times longer to compute but it's really more about doing more stuff than about Arithmetic specifically.

Generally speaking for the kinds of things most people choose you never need to worry about this as really computers are power houses these days

The reason this doesn't affect complexity though is that complexity is about the (for big O complexity) the upper bound of behaviour as input size grows.

For instance, code which calculated the sum of a list of numbers would scale linearly as that list grew (if you double the list size you double the time to complete) whereas the constant operations used in that don't really change the way it scales, they just change the true run time (e.g. maybe you have millions of elements to sum and one fewer operation could still save you time but doesn't cancel out that there's still millions to look at)

Complexity is usually more important as it gets you changing different aspects of code (usually changing the overall algorithm to suit the problem rather than fiddling with individual operations)

As an example, if we both wanted to find Rear Admiral Grace Hopper in the list of all humans who've ever lived and you could read twice as fast as me, it wouldn't matter at all if you started from the very start but I skipped to where I though H began and then I skipped to where I thought Ho, and kept skipping until I was very close to Grace Hopper",hbistuk,t3_ph8cmg,1630721336.0,False
ph8cmg,"depends on the operations.

if you call a function then obv the time complexity reflects the function you call

&#x200B;

as for operators:

generally equals and bitwise operators are fast addition/subtraction are slightly slower, multiplication is slower still, and division very slow comparatively. You really shouldn't worry about this unless you are making something where performance is the only concern.",hc5pwa6,t3_ph8cmg,1631174543.0,False
ph744s,[teachyourselfcs.com](https://teachyourselfcs.com/) \- I would recommend you to start with Structure and Interpretation of Computer Programs or SICP for short,hbggzcv,t3_ph744s,1630682934.0,False
ph744s,This looks pretty good!,hbgoj29,t1_hbggzcv,1630686055.0,False
ph744s,"+1, having read a majority of the book, I'd recommend taking several months to read through the book and doing the exercises.",hbjb3pl,t1_hbggzcv,1630731917.0,False
ph744s,"Thank you so much, I appreciate this!",hbwviha,t1_hbggzcv,1631007605.0,True
ph744s,"* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **Clean Code** by _Robert C. Martin_
* **The Art of Computer Programming** by _Donald Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to Automata Theory, Languages, and Computation** by _Hopcroft, Ullman_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Discrete Mathematics** by _Ross, Wright_
* **Introduction to Graph Theory** by _Wilson_
* **Software Engineering** by _Sommerville_
* **Design Patterns: Elements of Reusable Object-Oriented Software**
* **Fundamentals of Database Systems** by _Elmasri, Navathe_
* **Numerical analysis** by _Kincaid, Cheney_
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **The C++ Programming Language** by _Stroustrup_
* [**x86-64 Assembly Language Programming with Ubuntu**](http://www.egr.unlv.edu/~ed/assembly64.pdf) by _Jorgensen_",hbhxz5x,t3_ph744s,1630705866.0,False
ph744s,Thanks a ton!!!,hbwvj6q,t1_hbhxz5x,1631007622.0,True
ph744s,"One that I found really good, though kind of specific was this book: C Programming Language, 2nd Edition https://www.amazon.com/dp/0131103628/ref=cm_sw_r_u_apa_glt_fabc_KJNQEBWSXE63Z6HNWT2W",hbgiv1z,t3_ph744s,1630683715.0,False
ph744s,SICP,hc6fw7p,t3_ph744s,1631193873.0,False
ph744s,I think Introduction to Algorithms by Thomas H Cormen is pretty good. Im going to start it soon.,hbgo8ad,t3_ph744s,1630685934.0,False
ph1id6,"Yes. It is excellent at building a good CS foundation. After taking this course i would recommend a decent data structures and algorithms course or book. Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms)

data structures and algorithms are endemic to converting that which is in the problem space to the solution space. Regardless of programming language or operating system or hardware.",hbg10ii,t3_ph1id6,1630676127.0,False
ph1id6,Its Harvard University's intro to Computer Science.  Do you think you are too good for Harvard?,hbg7u0h,t3_ph1id6,1630679118.0,False
ph1id6,No,hbg7vzv,t1_hbg7u0h,1630679141.0,True
ph1id6,"Well then if you want to learn computer science then I recommend you invest the entire $0 that it costs to receive your education from one of the world's most prestigious universities.

That being said computer science is computer science. None of these programs are going to lie to you so it doesn't really matter where you learn it.  David Malan is probably one of the most charismatic people teaching computer science though.",hbgnadr,t1_hbg7vzv,1630685545.0,False
ph1id6,100% done. David Malan is a god and I don’t intend on wasting this resource. Doing the course rn,hbjvqy1,t1_hbgnadr,1630748881.0,True
ph1id6,"> Do you think you are too good for Harvard?

Well, their method of teaching C... let's say I'm not too fond of it.",hbicq56,t1_hbg7u0h,1630713056.0,False
ph0nk6,"After a while, it's not really about being interested in it. 

Relationships are the same way, you fall in love, build the relationship, infatuation fades and you live off the work you put in the relationship.

Trying something new helps in both cases though, I think. 

If you are a coder who's been doing the same shit for a couple of years then picking up an interesting new language of framework might do the trick. Or switching fields entirely and going from, let's say, web dev to embedded or game development.

Engagement seems to be greater if one's working on a project one cares about too. So starting a side project or changing jobs might help as well.

However, if you are still a student or are still learning how to code and you don't have any commercial experience then the only tip I have for you is to find a project you care about and try to implement it or contribute to it.

But don't ever bullshit yourself that the ""passion"" will last forever, humans just don't work this way.",hbfbuvo,t3_ph0nk6,1630660001.0,False
ph0nk6,So you’re saying I should be pegged by my wife?,hbgj8or,t1_hbfbuvo,1630683871.0,False
ph0nk6,Yes.,hbgyedv,t1_hbgj8or,1630690142.0,False
ph0nk6,An actual thought out answer to serious question about someone's possible career...... should I let my wife fuck me instead....,hbj2f1v,t1_hbgj8or,1630726546.0,False
ph0nk6,"Exactly that, I find that ""changing things up"" by working on a different project or different tech stack revitalizes my interest, but also I do not discount the impact working for the right company on the right team has.",hbfntk9,t1_hbfbuvo,1630669131.0,False
ph0nk6,Wise words,hbgkc76,t1_hbfbuvo,1630684324.0,False
ph0nk6,"🍍 If you love writing code at midnight, or getting caught on a bug 🍹",hbgyoqa,t1_hbfbuvo,1630690263.0,False
ph0nk6,"preciate this a lot, 

feeling like you're trapped in an office feels terrible. i hope i can find coding jobs that also allows me to step outside that box

I'm still a student so looking at a change in majors feels like I'm wasting a lot of my time",hbfc4o1,t1_hbfbuvo,1630660245.0,True
ph0nk6,"Depends on how deep you are in a major. 30 credits in a specific degree? Yeah maybe a waste of time. Only a few credits invested in X major and the rest are all gen eds, you're probably fine to switch. It also depends on what degree you are switching to. Lots of degrees are very open, you can take classes in close to any order. Other degrees are very rigid, the courses have a specific order.

As someone who didn't change his bachelors when he had the chance, I wish I had thought about wasting time a little less.

Even if you get a degree you hate in the end. Simply having a degree is valuable and you can always switch shit up if you pursue a masters degree.",hbhwtle,t1_hbfc4o1,1630705322.0,False
ph0nk6,"im 1 year in CS, my other option is human technology major, which is basically research on how i can match human issues with the right tech. Does anyone know if thats also worth getting into?",hbjltid,t1_hbhwtle,1630740052.0,True
ph0nk6,yeah also taking a long break can help. it can be hard to let go to something so formative but realizing that a break might be the best thing for you and that you can always pick up coding again makes it easier. you’ll get the urge eventually.,hbiwx3n,t1_hbfbuvo,1630723497.0,False
ph0nk6,Thank you very much for this comment.,hbsawth,t1_hbfbuvo,1630914934.0,False
ph0nk6,"Programming is just a tool, not everyone will find it inherently interesting. But if you find the project you’re working on interesting that might be enough. Try something different, maybe get an Arduino kit off Amazon and try making some electronics projects.",hbfbozq,t3_ph0nk6,1630659858.0,False
ph0nk6,I spiced up my relationship by getting a penetration testing certification.,hbfsd3h,t3_ph0nk6,1630671809.0,False
ph0nk6,"Username so, so checks out. Well played.",hbfunox,t1_hbfsd3h,1630673028.0,False
ph0nk6,"for some reason, reading this really feels weird 😂",hbh47ur,t1_hbfsd3h,1630692604.0,False
ph0nk6,"Learn that programming can also be a mean to something else, and try to learn about some other area of compsci as well.",hbfr2zg,t3_ph0nk6,1630671092.0,False
ph0nk6,Second this. You may enjoy applied coding more than pure coding. There are various specializations of programming.,hbgja2u,t1_hbfr2zg,1630683887.0,False
ph0nk6,Stopping for a while and coming back to it (obviously not the easiest option for everyone). I went to grad school for math for several years and I was pretty excited when I got back into coding afterwards.,hbfauia,t3_ph0nk6,1630659087.0,False
ph0nk6,"im an intern for the next 6 months, wish me luck",hbfc7je,t1_hbfauia,1630660318.0,True
ph0nk6,"If you're an intern and have already lost interest,  that doesnt bode well for your career. Try using your internship time to explore options other than sw dev. There is a lot out there in tech that doesnt require coding but still pays well.",hbgz0si,t1_hbfc7je,1630690403.0,False
ph0nk6,"Find something interesting to do that doesn’t involve work. Find a problem that frustrates you in real life, then solve it with code. I created a sudoku solver.

Also realize that it’s ok for passions and interests to change with time.",hbgr3zq,t3_ph0nk6,1630687113.0,False
ph0nk6,I love sudoku. I bet the arithmetic behind this was a lot.,hbhxxbt,t1_hbgr3zq,1630705841.0,False
ph0nk6,"I quit my last job cause I was kind of getting burned out by how bad and incompetent the project managers and customers were. I spent almost a month just doing not much, pretty standard programmer hobbies like video games and watching youtube videos. I was playing some video game and the AI in it was so bad I felt a need to reinsert myself back into the world. 

So yeah, here I am now again, doing my own project but just more of a passion project making my own video game and making sure to do a good job on the AI.",hbh7vtt,t3_ph0nk6,1630694201.0,False
ph0nk6,"It is not about the code, it's about what you do with it. - Yoda",hbi8jnr,t3_ph0nk6,1630710957.0,False
ph0nk6,"Never your mind on code you were writing, hum. What you were doing.hum..Always looking to that next project.

Code matters not.  There is no brainstorming. You simply do or do not.",hbjd7vk,t1_hbi8jnr,1630733384.0,False
ph0nk6,"Continuing to learn and challenge myself. It's when I get complacent is when I get bored and lose interest, often to other things that are interesting and challenging. Setting goals, learning new things or different ways to solve problems",hbgyca4,t3_ph0nk6,1630690118.0,False
ph0nk6,Went back to school for Electrical Engineering. Also recently started looking into game development (I work as a web developer so it's different enough to be interesting),hbhikud,t3_ph0nk6,1630698913.0,False
ph0nk6,"I think you might be asking the wrong question for the symptoms you’re having. You’ve not really told us any backstory context information so it just reads like I’m a depressed coder.

I’m generalizing but the code is just a tool it’s like saying i’m falling out in love with my hammer or my workbench or my welder or my stethoscope

You might need to focus back on the end result, building somebody a house, curing a disease convincing a patient to eat more vegetables so they don’t die.

If you want to go hard core getting your passion back go work at a food bank, volunteer at a battered women’s shelter, volunteer at the cancer ward at the local hospital.

There are so many people are suffering in the world that your ability to code would be a godsend. Maybe you shouldn’t be a programmer baby you should be one who teaches others how to. Etc",hbhuuyy,t3_ph0nk6,1630704416.0,False
ph0nk6,"Learn hardware. This may sound counter-intuitive, but If you take your understanding about hardware, you can make clearer connections with how software works on computers. Like learn about the Linux kernal and PC architecture. Or learn about 8 bit CPUs and work your way up and make a video game in asymbly for a z80 based console.(msx, zx spectrum, gameboy, etc)
Hardware and software should work together, not be so overly abstracted it becomes inefficient. - 

(1)- https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D - 


(2)- https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU - 

(3)- https://www.youtube.com/playlist?list=PLowKtXNTBypFbtuVMUVXNR0z1mu7dp7eH - 


I recommend watching these playlists in order if you are not familiar with hardware. Its a very fascinating subject and not as hard as it may seem on the surface. - 

At its lowest level hardware and software are the same thing. Its just switching transistors in the end.",hbiw3tx,t3_ph0nk6,1630723061.0,False
ph0nk6,There's a reason they typically have to pay you a lot of money to do it.,hbfr9qt,t3_ph0nk6,1630671200.0,False
ph0nk6,What got me interested in code was a book on hardware interrupts (spelling?) back in the 90s.  I just really wanted to see things happen.,hbjdiqe,t3_ph0nk6,1630733601.0,False
ph0nk6,Find out what I did not know.,hbjf15e,t3_ph0nk6,1630734683.0,False
ph0nk6,I started looking into 3D printing and laser cutting. Ended up buying one of each and have been making real world objects with them. Such fun. Combine 3D printing with some Adruino stuff and that's fun too. Try something new is all I'm saying.,hbkbxoa,t3_ph0nk6,1630760791.0,False
ph0nk6,I look at my paycheck.,hbmvzni,t3_ph0nk6,1630804195.0,False
ph0nk6,"As some people already commented on, some times it is not about the coding, but something else. Repetitive work, poor management, or just problems on your personal life.

In my case I just get bored with everything 😅. So I kept pivoting my career: web development, training, mobile development, system administration, now technical writer.

My strategy has been, dedicate a small part of personal time to something I really enjoy. Create a small pet project to learn a new technology, play with audiovisual stuff, draw, support classes to university students.

Eventually I got the chance to turn those hobbies into the next step in my career.

I acknowledge I was extremely privileged to be able to jump into these projects, and have the support from people around to give me chance to switch careers. So, although I don't recommend my path to everyone, I hope it helps a bit :).",hc2eqdv,t3_ph0nk6,1631113624.0,False
ph0nk6,Go into cyber,hbhzdny,t3_ph0nk6,1630706532.0,False
pgzg22,"Rendering may not be the biggest issue here if this was evaluated. A single server handling 31,000,000 connections at the same time is quite the workload.",hbf04k8,t3_pgzg22,1630650228.0,False
pgzg22,"any thoughts on what kind of server, or even just how much power it would need?",hbf0oa8,t1_hbf04k8,1630650641.0,True
pgzg22,"rendering 1000 players or more  that a player can have at sight will be a biggest issue too

ps: you can read about EVE game and the big battle https://www.youtube.com/watch?v=kavAF1fkEGU 
probably someone do some technical analysys

also now ""slow as fuck"" or ""the biggest lag""

>  but would it be possible? 

no actually and not in the near future 

ps: not need a single server for managing all connections there are already ways to avoid that . 

it's still a very difficult task but not that's not why.

Probably if client-side rendering were feasible, server problems would be avoided, it would be a matter of money.",hbf9zyl,t1_hbf04k8,1630658332.0,False
pgzg22,"are you talking about just rendering an imaginary battle where you see the results play out? if so see [https://en.wikipedia.org/wiki/MASSIVE\_(software)](https://en.wikipedia.org/wiki/MASSIVE_(software)), this was written about 20 years ago and was able to produce pretty realistic and good looking battles.

 If you're talking about actually playing an FPS with millions of other people connecting to a server and on screen at the same time, that's not happening any time in the near future. It's just a matter of scalability at that point and something would have to be built from the ground up with highly advanced hardware for every user and server connection capabilities.

For something similar to that, but with hundreds instead of millions of people, see Planetside 2.",hbfwn5z,t3_pgzg22,1630674034.0,False
pgzg22,"Like most things in games, you could probably fake it.",hbg3ajj,t3_pgzg22,1630677155.0,False
pgzg22,"This. Games are all about fakery. It’s not “cheating”, it’s the whole essence of what they’re about.",hbg5m3o,t1_hbg3ajj,1630678167.0,False
pgzg22,Destiny is p2p I believe,hbhe3q1,t3_pgzg22,1630696923.0,False
pgutbh,No you shouldn't be ashamed. People get exposed to things at different times. It doesn't mean you're incompetent. Take the offer and catch up on learning Linux and you'll be fine.,hbe1xea,t3_pgutbh,1630631537.0,False
pgutbh,"“Should I be ashamed as a computer programmer to be looking at <insert anything here>?”

Absolutely not. The pursuit of knowledge is worthwhile 100% of the time. Humility is arguably the most effective way to go about that, too.",hbecqid,t3_pgutbh,1630636644.0,False
pgutbh,"Are you kidding?  I’ve been a python developer for 15 years and I still google some very basic shit all the time.  Because honestly you just forget after not doing certain things for a few years.  

I also regularly watch tutorials for the same reason.  It refreshes my knowledge and exposes to to possible new techniques.

I can also say the exact same thing for Linux.  Currently working in Windows and will need a refresher if I ever go back to a Unix environment.",hbebh5c,t3_pgutbh,1630636042.0,False
pgutbh,"Linux is the damnedest thing: it's everywhere, but the barriers to entry are much higher than many Linux boosters are willing to admit. I've been using it for years, but it would've been much easier for me to not use it ever.

Your experience is normal, and it's your decision whether you go ahead with learning Linux.",hbfeoq8,t3_pgutbh,1630662464.0,False
pgutbh,"No, you should be proud.",hbf1biy,t3_pgutbh,1630651128.0,False
pgutbh,No shame at all.  We aren’t born with computer knowledge.,hbervca,t3_pgutbh,1630644710.0,False
pgutbh,"There is absolutely nothing to be ashamed of. In fact, you can get accustomed to most of what you’ll be doing in Linux right there in your Mac terminal. The learning curve is even lower for you compared to someone coming from say Windows. Up until two years ago when I joined my current company, windows was an alien concept to me because I hardly use it. Now I’m building applications on dot net framework. I had to learn a lot in a few days, but I told my employer right away that I’m not used to it but I’ll get used to it quickly.",hbfpsao,t3_pgutbh,1630670335.0,False
pgutbh,no through your career you will always eb looking at tutorials and learning bew skills,hbg03ux,t3_pgutbh,1630675701.0,False
pgutbh,"I have used linux but I don't anymore. Don't be ashamed. Linux is kind of a niche thing. MAC OS is based on openBSD which is even more linux than linux (well its unix but whatever). If your job or interests don't require you to learn linux then no one is gonna take your ""coder card""",hc5qkt7,t3_pgutbh,1631175223.0,False
pgp08d,[Try this](https://duckduckgo.com/?q=%22Linus+Torvalds%22+lecture&ia=videos&iax=videos),hbd095x,t3_pgp08d,1630614306.0,False
pgp08d,You beat me to it,hbdro5j,t1_hbd095x,1630626732.0,False
pgp08d,Check out Just for Fun: The Story of an Accidental Revolutionary by Linus Torvalds and David Diamond.,hbdzj24,t3_pgp08d,1630630435.0,False
pgp08d,Bro it’s safe putting this out so clearly,hbsrmjt,t1_hbdzj24,1630929207.0,False
pginug,I've been on my computer basically ALL day and have been for years. Now I'm thinking maybe I should start looking after myself haha.,hbbj6ub,t3_pginug,1630593001.0,False
pginug,[deleted],hbcdia8,t1_hbbj6ub,1630605256.0,False
pginug,Wait my eyes can get bad from not looking too far? Well shit.,hbezhjn,t1_hbcdia8,1630649758.0,False
pginug,[deleted],hbfo1ew,t1_hbezhjn,1630669266.0,False
pginug,"I'll start doing that, that's spooky.",hbg69o5,t1_hbfo1ew,1630678449.0,False
pginug,While also reducing productivity by 25% at the same time.,hbg7s92,t1_hbfo1ew,1630679097.0,False
pginug,"You tell me! I've used to set volume to -60dB (with -45dB being loud), now -30dB is the norm...",hbedtf3,t1_hbcdia8,1630637165.0,False
pginug,"I work behind a 55"" at home (LG C9), it stands in ~2-3 meters in front of me. Looking out the window occasionally also helps.",hbewq0k,t1_hbcdia8,1630647827.0,False
pginug,"I'm not in this boat but I do know that going outside and exposing yourself to natural light is MASSIVELY important. I'll add a link to an article from NCSU about this idea as well as the abstract to a scientific paper. Bottom line is if you're taking a break from looking at your screen, you might as well go outside or look out a window.

https://sustainability.ncsu.edu/blog/changeyourstate/benefits-of-natural-light/#:~:text=Natural%20light%20benefits%20vision,aids%20in%20healthy%20eye%20development.


https://scholar.google.com/scholar?hl=en&as_sdt=0%2C6&q=natural+light+exposure&oq=natural+light+#d=gs_qabs&u=%23p%3DMSHg8KMX0IUJ",hbboovx,t3_pginug,1630595329.0,False
pginug,Go Wolfpack!,hbc35pr,t1_hbboovx,1630601153.0,False
pginug,"The 20/20/20 rule is really helpful.

The three biggest things for me are:

* Make sure there's no glare on the screen.  I know tons of natural light is good for the spirit but if my screens have glare from a window behind me or whatever it wrecks my eyes by the end of the day.
* Similarly, I have to make sure there's not too much brightness contrast between my screen and the area surrounding it.  For me, that means don't back the monitor up against an open window.  The bright light around the edge of the monitor strains my eyes by the end of the day.  If you have to deal with this situation, try to match screen brightness closer to the stuff around it.  So maybe turn off dark mode and turn up the screen brightness.
* Utilizing display scaling.  Just because you have a hotness 4k monitor doesn't mean you should ogle in the fact that you can stick 3 windows side by side with all your new screen real estate.  If the text and buttons on the screen is too small then go into your OS settings and turn on display scaling to make it larger.  This sounds stupidly simple but I've seen many coworkers struggle with eye strain while have tiny text on their screen.",hbbrics,t3_pginug,1630596473.0,False
pginug,Totally agree!,hbc3pxu,t1_hbbrics,1630601375.0,False
pginug,Good stuff,hbdj8rq,t1_hbbrics,1630622741.0,False
pginug,"Take a 5 min walk every hour, take in a view if there is one.",hbbgwjy,t3_pginug,1630592040.0,False
pginug,I use stretchly for that in 30 minute intervals with a long 3 minute break every 90 minutes.,hbbnk83,t1_hbbgwjy,1630594867.0,False
pginug,"Not a programmer. But worked 5 years as an engineer spending most of my time on screen. And still do as a now Comp sci student. 

Every 20 minutes look at something that's 20 feet away for 20 seconds to help reduce fatigue with your eyes. 

Standing desks literally saved my back (I say that as a 29 year old that is relatively active) sitting is HORRIBLE for your body, got low back pain after 2 years of working. I now sit about 1/3 of my day or less. 

Definitely take walks, try to take a 10 minute walk after your meal. Helps me get past the post lunch siesta lol

Edit:
Grammar is hard",hbbomih,t3_pginug,1630595303.0,False
pginug,The 20/20/20 rule definitely reduces the number of end-of-day headaches,hbbt3o1,t1_hbbomih,1630597141.0,False
pginug,"Take breaks, go on walks, try to stand at least a few mins each hour, eat a healthy snack, drink water, and remember to blink. Desk/office jobs where you’re sitting all day have a more detrimental health impact than many realize. Take care of yourself!!",hbblxwl,t3_pginug,1630594171.0,False
pginug,"My biggest problem is actually the mental fatigue of trying to solve impossible problems all the time.
I feel like getting dumber (slow thinking, having a hard time to see simple solutions even if they are right in my face), always in a sleepy state, and suffering from increasing dyslexia. The only solution I've found up until now is to work less hours. :X",hbcvc3i,t3_pginug,1630612375.0,False
pginug,"See... I just lost the word ""screen"" in that sentence.... >.>
Oh my....",hbcvnw9,t1_hbcvc3i,1630612502.0,False
pginug,"that helps too! ive got the same issues

i just started an internship with 42 hours a week and it has me questioning if i really love to code sometimes.",hbcwck5,t1_hbcvnw9,1630612772.0,True
pginug,"I know nothing about you, just spit balling, but how’s your sleep in general? All those symptoms for me are consistent with periods when I’m not physically active and not sleeping well (think like, 5 or less hours a night for a week)",hbfpvkm,t1_hbcvc3i,1630670389.0,False
pginug,">I’m not physically active and not sleeping well

Actually, you're right. It gets really worse if I'm not doing any physical exercises. The sleep quality also degrades together. However, whilst the symptoms are way more pronounced in that period, even when I'm well active and sleeping properly, I still feel fatigued during high demand and high cognitive load periods. I guess it could not be different, although I really wish I could increase my endurance. You see, I really like coding and solving hard problems, but I'm also a math lover, and I really wish I could end up the work period and study category theory and game theory with a fresh brain as if I haven't worked at all.",hbfsam9,t1_hbfpvkm,1630671771.0,False
pginug,Yeah that’s fair enough. The stuff i said would only help a certain amount because solving hard problems all day is tiring no matter which way you spin it you know,hbfvgbq,t1_hbfsam9,1630673432.0,False
pginug,Dark mode on every program that has one. Also make sure you are sitting the right distance away.,hbc06rm,t3_pginug,1630599998.0,False
pginug,I avoid coding or staring at a screen for the full 8 hours. 4-5 is my sweet spot for programming. I try to map a lot of ideas out via pen and paper. Try to walk around every hour or so. Standing desks are a life saver. Blue blockers are great.,hbc2q6x,t3_pginug,1630600986.0,False
pginug,"i feel like i never wanna code again after 4-5 hours. my effectiveness just hits rock bottom.

also the standing desk is really interesting",hbc3hfl,t1_hbc2q6x,1630601281.0,True
pginug,I totally get the hours thing. I'm not the hacker type so if I can I like to whiteboard out the solution before I code. But thinking about all the dependencies and other things I get tired after 5 hours. The standing desk has made coding longer than sitting for me.,hbc47le,t1_hbc3hfl,1630601566.0,False
pginug,Blue light filter glasses,hbbms3e,t3_pginug,1630594528.0,False
pginug,"This one is important if you are in the late part of the day.

I assume we are 9-5 workers, no use for blue filter. Unless you love to stay till and beyond midnight.",hbbsuga,t1_hbbms3e,1630597032.0,False
pginug,Define love :(,hbbt4dj,t1_hbbsuga,1630597147.0,False
pginug,Baby don't hurt me...,hbgsocc,t1_hbbt4dj,1630687756.0,False
pginug,"Use Night shift/night light at home, though. Let your brain go into evening/night mode to help get to sleep. I have it enabled on all of my daily use devices, gaming desktop, laptop, phone. I've tended to feel tired and go lay in bed much earlier than I had for years before. Anymore I rarely make it all the way to midnight without feeling tired and ready to sleep.",hbcpl46,t1_hbbsuga,1630610092.0,False
pginug,"> night light 

I'll give it a try.

Do you use LED bulbs? They emit lots of blue. Tungsten bulbs have more natural emission spectrum with less blue than other types.

Laptops/desktops are not only things influencing us.",hbh3l9f,t1_hbcpl46,1630692337.0,False
pginug,"I wrote that as the built-in Night Light feature of Windows 10. macOS and iOS call it Night Shift. I generally prefer low-light anyway, so in my case the displays I am looking at actually are the main sources of light later at night. In my work office I have large windows so I leave the overhead off and work in natural light through the day.",hbhb2er,t1_hbh3l9f,1630695587.0,False
pginug,"No, they help reduce eye fatigue as well. I use them all the time.",hbc32ak,t1_hbbsuga,1630601116.0,False
pginug,"I had zero diopter ""computer"" glasses. They were designed to work with CRT displays (15 years ago). I stopped to use them with LCD as I didn't see difference and understand the principle they operate ))",hbcixic,t1_hbc32ak,1630607450.0,False
pginug,I notice a difference for sure. Everyone's different haha.,hbcxpdz,t1_hbcixic,1630613303.0,False
pginug,"As far as I know, they're just a marketing gimmick. Your eyes receive a lot more blue light from natural environment than from a screen.",hbc0gch,t1_hbbms3e,1630600101.0,False
pginug,"I heard the same thing. I also heard night mode, which many laptops have, takes away some or all of the blue light.",hbc0wga,t1_hbc0gch,1630600277.0,False
pginug,"Before buying them try, just blinking more, setting night time filter, and if in a dark room turn on a lamp.",hbc296z,t1_hbc0gch,1630600800.0,False
pginug,It’s that a lot is coming from the square you are looking at and not around it. Like burning an image into pixels.,hbc2y5l,t1_hbc0gch,1630601072.0,False
pginug,"that’s why i have a super ultra wide monitor. increase the square, increase the health",hbfphuo,t1_hbc2y5l,1630670157.0,False
pginug,"Nah as someone who used to have severe computer eye fatigue the blue light blocking on my glasses changed everything for me.

Nowadays I don't have any eye strain with it. I stopped wearing contacts or my normal pair of glasses period.",hbcbxl1,t1_hbc0gch,1630604633.0,False
pginug,"Bought blue light glasses on a whim, absolutely make a difference. Just try wearing them for a few days, then spend even an hour without and you’ll notice the difference.",hbessmg,t1_hbc0gch,1630645294.0,False
pginug,"Also have 20 sec break, in which you close eyes, this will help in reducing dryness in your eyes. Also if possible use artificial tears eye drops as they can provide hydration to you eyes. Never directly put water IN your eyes thinking of cleaning them, it can cause dryness issues. And just stay away from computer for sometime in between.",hbchs3h,t1_hbbms3e,1630606994.0,False
pginug,I feel like my blue light glasses also reduce glare from the screen into my eyes.,hbcxf6r,t1_hbbms3e,1630613192.0,False
pginug,Blue light cut at night is essential. Put this on your phone and computer (Flux for PC). Really improves sleep.,hbg0f65,t1_hbbms3e,1630675849.0,False
pginug,"Try using EyeLeo, a really cool app for this specific problem",hbbndot,t3_pginug,1630594791.0,False
pginug,"Get back in time to 2004,buy World of Warcraft, play 16h/day, do dungeons, raids, experimemt with PvP. Continue doing this for 10 years.

What is screen fatigue?",hbbwm86,t3_pginug,1630598579.0,False
pginug,"Having Windows in night mode, and... Nothing else. Trying to have the same room luminosity as the screen.

I'm on the screen for more that 10h every day, working or doing my things, without problem. It depends on the person though",hbbuz8k,t3_pginug,1630597930.0,False
pginug,Computer glasses with the amber tint. I like Gunnars.,hbc2piv,t3_pginug,1630600979.0,False
pginug,"2nd this, I got them for like 40 dollars on amazon and they have been great.",hbc4ag6,t1_hbc2piv,1630601597.0,False
pginug,"Same here, got me gunnars 12 years ago for work
and I still use them, invaluable piece of equipment I have at my desk.",hbcjibz,t1_hbc2piv,1630607681.0,False
pginug,F.lux,hbc8k3c,t3_pginug,1630603296.0,False
pginug,When I get real life fatigue I go programming...,hbbjld8,t3_pginug,1630593170.0,False
pginug,"Monitor with read mode

read mode - ""reducing blue color, the image becomes yellow as newsprint""

You should read about ""pulse width modulation""

Long story short ""pulse width modulation"" kill your eyes if you have small brightness",hbbmc7h,t3_pginug,1630594338.0,False
pginug,"Windows has this built-in; you can also download f.lux if you just don't want to use Windows' features for whatever reason. They did it first, but it's already built in to my machine so I just use that",hbbo5vz,t1_hbbmc7h,1630595116.0,False
pginug,"> You should read about ""pulse width modulation""

You should learn about LCD inversion: http://www.lagom.nl/lcd-test/inversion.php#invpattern",hbbt5cl,t1_hbbmc7h,1630597158.0,False
pginug,"Night light Mode on Windows. It will be a bit weird at the start, but after it will be really helpful",hbbzile,t3_pginug,1630599734.0,False
pginug,"Use the ""warm"" mode on your monitor and in your OS, nowadays windows and mac both has warm mode implemented and probably Linux too. And if you have glasses you can use blue light filter glasses. Other than that drink water, blink a lot and look around. Your eye is doing what its designed and won't be fatigued easily if you dont look at a single point for 8 hours",hbc3g4i,t3_pginug,1630601267.0,False
pginug,"I take regular pauses away from my screen every now and then, it also helps me think about solutions.

I work in a room with plenty of natural light, my desk is right next to a sliding patio door.

Every now and then, I just look outside the patio door to take on a bit more direct sunlight and to focus on distant objects.

I try to take a few short walks out, away from artificial lights. But I often skip them because of a heavy schedule.",hbc3nqr,t3_pginug,1630601351.0,False
pginug,Regular breaks and vary the lighting around my desk.  Having plants nearby also helps out.,hbc6i8u,t3_pginug,1630602472.0,False
pginug,"* Black themes
* Increase fonts and gui resolutions
* Do small breaks each 30/45 minutes to relax eyes and focus on distances",hbc6mx9,t3_pginug,1630602524.0,False
pginug,"Install f.lux to redden screen at evening, made my eyes so much more comfortable.
Also do eye exercises in the morning, right after awakening.
Get yourself a desk for wirking in standing position.",hbciuo0,t3_pginug,1630607418.0,False
pginug,Frequent breaks.,hbck13n,t3_pginug,1630607884.0,False
pginug,Nothing,hbck6ic,t3_pginug,1630607946.0,False
pginug,"Since working from home, I have a standing desk so I can stand up when I start to get aches from sitting. 

I have prescription glasses with the blue light filter which helps tremendously.

And lastly, I get up every few hours and take a break to make coffee or maybe look at something else for 15 minutes or so.",hbcw5jv,t3_pginug,1630612694.0,False
pginug,Blue light filter glasses,hbdr4rh,t3_pginug,1630626476.0,False
pginug,Blue light blocking glasses actually help quite a bit,hbecbuy,t3_pginug,1630636452.0,False
pginug,Dont spend your free time infront of a screen as well.,hbbudye,t3_pginug,1630597692.0,False
pginug,"Dark mode
Bluelight filter
Look 20 feet away from screen for 20 seconds, every 20 minutes.
Syntax highlighting",hbc0seo,t3_pginug,1630600233.0,False
pginug,20 feet is the same as 12.19 'Logitech Wireless Keyboard K350s' laid widthwise by each other.,hbc0tld,t1_hbc0seo,1630600247.0,False
pginug,Refresh eyedrops,hbc3qei,t3_pginug,1630601380.0,False
pginug,Playing videogames in the evening /s,hbc7kyv,t3_pginug,1630602904.0,False
pginug,"Well no one said this so ill add, after you steep some black tea, put the bags on your eyes, very comfy.",hbc9mrr,t3_pginug,1630603714.0,False
pginug,White mode if there is ambient light and dark mode if there is no ambient light.,hbciams,t3_pginug,1630607201.0,False
pginug,"I know others have said, but reducing blue light, setting as many apps to dark mode as possible, turning down the monitor brightness, and not having light behind the screen or glaring off the monitor is the way I can use the monitor for hours without eye fatigue",hbcj9wi,t3_pginug,1630607587.0,False
pginug,Yellow glasses.,hbclq1b,t3_pginug,1630608564.0,False
pginug,"this got a lot of attention, i appreciate all the answers. 

One thing i also strugle with is keeping up my coding motivation. What do you do when you start losing intrest in coding?",hbcnp9i,t3_pginug,1630609336.0,True
pginug,Meetings. Ugh,hbcruep,t3_pginug,1630610993.0,False
pginug,Every so often I take a break from looking at the screen to look at my phone… F,hbcsa3t,t3_pginug,1630611168.0,False
pginug,Stop and take a little stroll every once in a while. Trying to stick to pomodoro helps with this,hbcsn0v,t3_pginug,1630611310.0,False
pginug,"Dark mode everywhere I can. F. Lux or similar reader mode/night mode options. Antiglare glasses. Font sizes increased everywhere I can (8 or 10 is an annoyance, I use 12 for most things)

I should be taking more breaks but I forget to. Instead I go for a walk after work

PS. I spend most of my free time in front of the computer too since it's the only way for me to keep in touch with people. As a change, I read books for fun",hbcspxk,t3_pginug,1630611342.0,False
pginug,"colleague of mine has this app installed on his mac, that blinks a black screen every some minutes to remind him to look away. i can't remember exactly what the routine was, but you look at something 20ft away, and then look back at something closer etc...",hbcv9lg,t3_pginug,1630612347.0,False
pginug,"I use a program called eye Leo on PC , and one called time out on a Mac 
They blank out my screen for 20 seconds every 10 minutes",hbd7d0q,t3_pginug,1630617164.0,False
pginug,I play videogames for 8 hours more,hbd7tc4,t3_pginug,1630617356.0,False
pginug,Usually after a long day of work in front of a computer I like to play video games and relax. Pc master race.,hbd8nww,t3_pginug,1630617727.0,False
pginug,"Make sure the background is black. The letter should preferably be lime green.

&#x200B;

Look away from the screen when you don't need to. Often enough, you can type while closing your eyes. I sometimes type faster when I eyes are closed. 

&#x200B;

Every half hour to hour, get up and walk around, move a bit.",hbdgxer,t3_pginug,1630621680.0,False
pginug,"During the summer months, I go out to my patio and take meetings where I don't talk all that much. I have wireless headphones that allow me to leave my computer inside and just talk/listen to the meeting. It's a really great part of my day that I spend just watching the ducks or hummingbirds.",hbdq80u,t3_pginug,1630626037.0,False
pginug,developers who are only working 8 hours a day are already reducing their screen fatigue ...,hbdx7aw,t3_pginug,1630629355.0,False
pginug,"Light theme everywhere, dark themes hurt my eyes and give me a headache within the first 3-5 minutes, I'm in the 40% of the population that can't read dark themes easily. That means having plenty of surrounding light and sunlight coming in which I think also helps improve my mood compared to sitting in a dark area in front of consoles like I did in the service for 8-12 hours at a time staring at dark themed screens and gulping aspirins daily.",hbe8008,t3_pginug,1630634404.0,False
pginug,F.lux and pangolin - saved my eyes after all these years,hbedli6,t3_pginug,1630637059.0,False
pginug,Play video games for the other 8 hours,hbelfyx,t3_pginug,1630641018.0,False
pginug,"Blue filter glasses. Really helps. I wear them when I play video games as well. 

Invest in a pair with high reviews that are over 20 dollars. I want the ray ban ones but they are too fucking expensive but I bet they are awesome.",hbeneja,t3_pginug,1630642079.0,False
pginug,blue light filter and drink water,hbepdre,t3_pginug,1630643207.0,False
pginug,invest in blue light filtering glasses/contacts or if it exist screen cover,hbephut,t3_pginug,1630643274.0,False
pginug,"Not a programmer but I work on a computer most of the day. What works best for me is using windows night mode at around 20-30% to reduce blue light, take monitor breaks (such as the 20/20/20 rule) and lastly getting up and doing a bit of stretching regularly even if it's only for a few seconds",hbeq62z,t3_pginug,1630643671.0,False
pginug,Blue screen glasses,hbeq7w2,t3_pginug,1630643700.0,False
pginug,"At the office I never let someone get/pour me some coffee. I always walk myself to the machine to catch ‘mini-breaks’. This really helps with creativity too! At home I walk before work, in the afternoon too and in between I always get Coffee AND water to stay hydrated. Really helps for me, maybe for you too?",hber7pu,t3_pginug,1630644308.0,False
pginug,I installed some app that shifts color palette into red and reducing blue light. It helps me a bit. Plus everything is in dark mode.,hbeslkg,t3_pginug,1630645170.0,False
pginug,"This may go against what a lot of the other people have said, but if you're in a room with a lot of florescent lights, you might have an easier time using a light theme on your IDE with high contrast fonts. If the room is too bright you might end up straining your eyes staring at something with a dark theme. 

If you have an office with no windows, get a nice lamp with a full spectrum bulb and you'll have an easier time running a dark theme. 

Last thing would be a monitor light. I know BenQ makes one, and they're a game changer.",hbettxl,t3_pginug,1630645942.0,False
pginug,"If you're on a windows machine, turn on the night light. I have mine set at like 45% and it makes it so much nicer to look at. I can't even look at the screen without it anymore",hbevan4,t3_pginug,1630646883.0,False
pginug,"I make sure that none of my devices are flickering (seriously, PWM is a brain-killer for me).",hbexa1t,t3_pginug,1630648210.0,False
pginug,"144 Hz, night theme, yellowish light in system settings, no direct sunlight into the monitor and look away sometimes",hbeywn4,t3_pginug,1630649343.0,False
pginug,"I work from home and my dog helps a lot, it's as if he knew when I need a break, without him I'd probably forget to take my eyes off the screens but luckily he takes care of that and jumps on my lap every hour or so, that's when I take a small break.",hbf1re4,t3_pginug,1630651466.0,False
pginug,Flickerfree monitors. Tired of screens rarely now.,hbf4hqp,t3_pginug,1630653610.0,False
pginug,"I wear glasses, which have  slightly blue filter",hbf5ows,t3_pginug,1630654589.0,False
pginug,"Dark theme

At least 60 fps

Looking out of a window

Reading glasses",hbfj0uk,t3_pginug,1630665870.0,False
pginug,"I have really painful dry eye spells that I have been able to practically eliminate even if I have to work 12 hours occasionally. I do a bunch of stuff, some works, some I don’t know if it works but doesn’t hurt.

1) 20-20-20 rule: I use this alarm (I prefer seagulls or thunder strike): https://ergonomictrends.com/20-20-20-rest-eyes-health-tool/

2) lubricant drops at night. My dry eye attacks happen at night. I put these in a few times a night usually.

3) prescription for Restasis. Put this in before bed, sometimes in the morning.

4) bluelight glasses

5) lutein supplement and vitamin E

Not sure if the last 2 things are doing anything but I am a desperate man.",hbfn4ye,t3_pginug,1630668706.0,False
pginug,Program with sunglasses on,hbft4f5,t3_pginug,1630672224.0,False
pginug,Workout is the solution,hbfttqa,t3_pginug,1630672593.0,False
pginug,"Wireless headphones so you are not unplugging a cord to get up and streach / look away / get coffee for OSHA reasons I swear. Your more likely to stay staring at the computer if there's anything in the way.

Setting a silent reminder alarm on your watch also helps.",hbfumca,t3_pginug,1630673009.0,False
pginug,Honestly I was thinking the other day how everything is a screen now. What are these mega corps or scientists doing about coming up with something that's less harmful. I don't think we can survive without screen maybe just find a better a better alternative.,hbfvyeo,t3_pginug,1630673687.0,False
pginug,I set my code/terminal font to 14pt and this helped me a lot.,hbfw2j3,t3_pginug,1630673746.0,False
pginug,"Same as most comments with:

Set two alarms to force me to get up and walk away, one at 10:30 and one at 3:30. On a Mac, use the app In Your Face. Then go for a semi intense walk or if you have it, hit the gym for a super short workout, one set of a few exercises.",hbfxa6x,t3_pginug,1630674352.0,False
pginug,"I would buy a 32"" e-reader monitor to use for my ide if it was available.   It would be a huge qol upgrade.",hbfyky4,t3_pginug,1630674985.0,False
pginug,Outdoors and sports. Outside of work I have almost no desire to use a computer.,hbg0f79,t3_pginug,1630675850.0,False
pginug,Workhours ;),hbg1wnr,t3_pginug,1630676536.0,False
pg70sn,"It's generally looking for repetition of the same data, or similar data.

For example, take a single frame of a movie, the next frame is going to be damn near identical, so you record the \*differences\*, you don't just repeat exactly the same data again.

So, I could describe this data...

MMMMMMMMMMMMSSSSSSSSSS

like this...

Mx12Sx10

My 'uncompressor' would parse that, and assemble the data back together.

Same data, fraction of the size.

That's a very simple example, but it's the same principle.

So when you watch a .mp4 or whatever, it's piecing it back together again, it's 'uncompressing' without you even being aware of it.",hb9axlm,t3_pg70sn,1630544057.0,False
pg70sn,"The compression in this comment is called [run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding).

Another type of encoding used for compression is [Huffman Coding](https://en.wikipedia.org/wiki/Huffman_coding). I believe this is what is used for zip format. It works by finding the most frequently repeated little chunks of data & building a table of building blocks out of them, then compressing the data by writing compressed data using the chunks in the table as building blocks.",hba0j2l,t1_hb9axlm,1630556969.0,False
pg70sn,">used for zip format

great exploration of gzip file [here](http://commandlinefanatic.com/cgi-bin/showarticle.cgi?article=art001)",hbbjy2g,t1_hba0j2l,1630593320.0,False
pg70sn,"Thanks, that really cleared it up for me.",hb9hbw8,t1_hb9axlm,1630546996.0,True
pg70sn,"There's also such a thing called 'lossy compression' in which you can usually compress the data down even further but at the cost of losing some information on compression. This is often used with content made for humans, such as audio files and videos. MP3's are a lossy format which prioritize quality in the areas in which we can perceive a difference and throws away information that might be harder to discern from our perspective.",hb9r0bn,t1_hb9hbw8,1630551781.0,False
pg70sn,I sometimes wonder what MP3s sound like to cats and dogs.,hba4id1,t1_hb9r0bn,1630559424.0,False
pg70sn,"The same, probably. MP3 would discard anything inaudible to the human ear, and since dogs and cats' hearing range is strictly greater than ours, there's nothing we can hear that they can't, but there's nothing in the MP3 that we can't hear, so it should be the same.",hbak88p,t1_hba4id1,1630571725.0,False
pg70sn,Why? Do they perceive sound on a different wavelength?,hbad3i3,t1_hba4id1,1630565644.0,False
pg70sn,"The hearing range of dogs goes further than humans at least at the high-frequency end.

Quick random link: https://www.akc.org/expert-advice/lifestyle/sounds-only-dogs-can-hear/

To my understanding, though, a digital stream with a 44.1 kHz sample rate (same as on a CD) allows for losslessly representing analog signals with frequencies up to half of that, i.e. 22.05 kHz, so if human hearing goes up to around 20 kHz or so at best, a CD-quality digital representation might not contain that much higher frequencies in the first place. Maybe. I'm not an expert.

I suppose a lossy compressed signal could still sound different to dogs or other animals.",hbao3kd,t1_hbad3i3,1630575253.0,False
pg70sn,They have a wider hearing range than humans (hence dog whistles). So I wondered if the MP3 algorithm discards or distorts wavelengths that are inaudible to humans but audible to cats and dogs.,hbb4pcz,t1_hbad3i3,1630586404.0,False
pg70sn,I see. Thanks!,hbbbykl,t1_hbb4pcz,1630589876.0,False
pg70sn,Listen to a heavily compressed mp3 and you'll know. Although i don't think it works like that.,hbajm65,t1_hba4id1,1630571173.0,False
pg70sn,"Interesting, I didn’t know that. Thanks!",hbj4h7g,t1_hb9r0bn,1630727749.0,True
pg70sn,"> why do they need to be uncompressed

You can use API's that abstract away the fact that data is compressed - but it will incur some overhead behind the scenes. If you know you will read the same data several times, it might make more sense to decompress it once then work on it several times rather than decompress it every time it is accessed.

Another example is a case where you will read randomly from compressed data. It might make more sense to decompress all of the data into memory to enable fast random access, rather than incur the overhead of decompressing before every random read.

Another example is a case where you want fast random access across all of your data, but your data is too big to fit in memory. If it fits in memory compressed, you may see benefits from putting compressed data in memory and decompressing on the fly.

Fun fact, some operating systems even compress data in memory transparently to the user/program, see: https://en.wikipedia.org/wiki/Virtual_memory_compression.",hba1e6t,t3_pg70sn,1630557478.0,False
pg70sn,"https://en.wikipedia.org/wiki/Virtual_memory_compression

The last dot was giving a 404 error.",hbb4q5s,t1_hba1e6t,1630586416.0,False
pg70sn,So that would be why certain files open faster after previously being opened. Thanks for your explaination.,hbj5ngo,t1_hba1e6t,1630728453.0,True
pg70sn,Maybe - but that’s probably more likely due to caching.,hbm3zgl,t1_hbj5ngo,1630790216.0,False
pg70sn,"Ok so the simple answer is that machines can only read bytes at the lowest which is 8 bit. So when you compress you can actually parse stuff into bits and save a password to decode it later.

You can run a generalized algorithm that parses according to some previous observations fx: [http://www.thiagi.com/instructional-puzzles-original/2015/2/13/cryptograms](http://www.thiagi.com/instructional-puzzles-original/2015/2/13/cryptograms)

But usually you would like to scan your files and look for the most occurring patterns and then assign them to the lowest possible bit. 1 or 11 or 011 or 101 and so on. This way you can always know when the bit string will end because it will end in a signal bit.

Now that you have mapped all the words/letters/numbers/whatever to bits you create a password/alphabet you can later use to decrypt the compression.

Now as you can imagine this process can't be easily skipped. Since all compressed files(usually) have a unique password/alphabet. So running compressed files directly seems almost needlessly hard to implement.

also remember the lowest readable piece of data is a byte which is 8bit, but a compressed file is a string of bits, not bytes.

Edit: To elaborate on the string of bits. The most commonly used letter in the English alphabet is ""E"" which is ""0100 0101"" in bit. Take those 8 bit and make them into 1 by parsing them into a new byte like this: ""1000 0000"" then you can store ""EEEEEEE"" like this: ""1111 1110"" and have a null byte to signal terminate string. You just saved roughly 6 bytes of space out of 7",hbbd9l5,t3_pg70sn,1630590453.0,False
pg70sn,[Data Compression ](https://en.m.wikipedia.org/wiki/Data_compression),hbbj7j5,t3_pg70sn,1630593009.0,False
pg70sn,"**[Data compression](https://en.m.wikipedia.org/wiki/Data_compression)** 
 
 >In signal processing, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hbbj91d,t1_hbbj7j5,1630593027.0,False
pg70sn,"There are basically two types of compression, lossless and lossy compression. Typically, user files like exes and text files are compressed losslessly for obvious reasons. Video and sometimes sound are compressed and decompressed as they are played.",hbev5fv,t3_pg70sn,1630646787.0,False
pg5ij6,You might be better if just not trying to run a criminal empire online.,hb9kz9s,t3_pg5ij6,1630548747.0,False
pg5ij6,"I feel if he has to ask something this basic, he really shouldn't even try.",hb9woyo,t1_hb9kz9s,1630554806.0,False
pg5ij6,We all have to start somewhere.,hbb85wu,t1_hb9woyo,1630588135.0,False
pg5ij6,"Correct, it needs to be logged or current",hb9ceqw,t3_pg5ij6,1630544732.0,False
pg5ij6,"Your lan dhcp address doesn't matter. It's the public ip assigned to your router that will get you. Hence why hacking from Starbucks with a spoofed mac address and a black hoodie is a trope. Wouldn't trust doing that anymore though.

If one were to hypothetically network pivot over multiple hops and ingress via tor then having 'your' ip address wouldn't matter. That's something one can safely do from the comfort of their own home, just don't fuck it up!",hb9rft1,t3_pg5ij6,1630552006.0,False
pg5ij6,"DHCP has nothing to do with the IP that is to be traced.

The DHCP protocol assigns (private) IPs within your home network. When communicating with the internet, all traffic coming out of your router gets a single (public) IP address, assigned by your ISP. This is called NATting.

Granted, your public IP changes from time to time, though ISPs have their own policies on how to change them [1]. In almost all countries ISPs are required to log and save which IPs are assigned to which customers for some amount of time, exactly for tracing back illegal activity. 


[1] You may argue that each ISP is in fact using DHCP to assign public IPs to its users, but in practice they most certainly use more sophisticated strategies and commonly, DCHP is used when discussing private networks rather than the internet to avoid confusion.",hbao8wc,t3_pg5ij6,1630575383.0,False
pg5ij6,"DHCP (dynamic host configuration protocol) is used for local networks. It is are responsible for your private, logical ip address to make sure there are no conflictions. 

Unless there was a table or database somewhere that actively kept track of IP addresses that I knew of that would answer your question.",hbevwil,t3_pg5ij6,1630647280.0,False
pg2la7,I have no experience in this myself but [this](https://www.google.com/url?sa=t&source=web&rct=j&url=https://arxiv.org/pdf/1807.00050&ved=2ahUKEwj86ur44d7yAhVvhv0HHd9hCJ8QFnoECDMQAQ&usg=AOvVaw3BLoc79QhtRw0d7atODdzr) seems to be what you're talking about. Its a paper but its not too long,hb8oztk,t3_pg2la7,1630533419.0,False
pg2la7,"Thanks, I saved this. I am trying to find something that already exists, but, failing that, papers like yours and what the other Redditor mentioned will probably get me on the right path.",hb8z18x,t1_hb8oztk,1630538341.0,True
pg2la7,[deleted],hb8q0hy,t3_pg2la7,1630533905.0,False
pg2la7,">That wouldn't be made readily available by any big social media company

If you look at any major platform such as Twitter, Facebook, etc, you can easily use the UI to see who has liked a post, comment, etc. There are at least two ways to mine that data: 1) browser automation to scrub the HTML or 2) monitor outgoing HTTP requests to discover the platform's API and then replicate those calls in your own application to retrieve the raw data (this was my approach).

>As far as clustering, you could try using some variant of KNN with the data you mentioned as features.  


Thanks for this. I am going to see if I can find anything that already exists so that I don't have to write it myself.

And I do think I may be reinventing a wheel or two. I've already seen several of these types of social media cluster diagrams that people have developed, many of which end up being used as political hit pieces unfortunately, but I have no such intentions as this is just a personal project that won't be published.",hb8yuwv,t1_hb8q0hy,1630538251.0,True
pg2la7,"Ah, method 2 is interesting - I don't know how it would work with the one-time IDs though. Scrubbing the HTML should have ratelimiters, and besides that out of date/wrong info though, and even if getting past those (which are on backend) would not scale. I'd be interested in hearing more about method 2 (and maybe can provide some perspective), can I PM?",hbe9cyt,t1_hb8yuwv,1630635046.0,False
pf5ktp,"Last time I posted my CPU emulator (it emulates a CPU architecture inspired by Ben Eater's videos in software). It worked just fine, however it had hard coded instructions and wasn't very efficient to change the program running on it.

So I created an assembler for it. Here it is in action.

And here's the source code: https://github.com/dibsonthis/CPU-Emulator",hb1xpc8,t3_pf5ktp,1630415716.0,True
pf5ktp,have source? github or something like that,hb240ag,t3_pf5ktp,1630418592.0,False
pf5ktp,"Yep, here is the source code: https://github.com/dibsonthis/CPU-Emulator",hb26kdf,t1_hb240ag,1630419687.0,True
pf5ktp,"Nice to see that people are still doing this stuff. Wrote one for my degree, was interesting.",hb2vq6f,t3_pf5ktp,1630429915.0,False
pf5ktp,It's definitely a great learning experience.,hb9ak5h,t1_hb2vq6f,1630543888.0,True
pf5ktp,"I'm interested to hear some of the basic requirements to creating one. You would need a clock at some speed, registers (unions in C?), an instruction set parser, some input channels, but I'm interested in what else?",hbewaik,t3_pf5ktp,1630647537.0,False
pf5ktp,can you do self-modifying code?,hcxbx91,t3_pf5ktp,1631691883.0,False
peww3x,"Hell no — that’s honestly impossible over an entire CS curriculum. FWIW I work as a SWE (but mainly with research scientists now) at oneof(fb, g) and I forgot half of my cs classes honestly - graduated almost two yrs ago now (but hold on to high level concepts/takeaways from them) - I’ve talked to peers and senior colleagues (who have wikipedia pages for their achievements) about this and all of them look stuff up on a need to know basis when it’s outside of their domain - just like we don’t expect a single doctor to have every medical answer. That’s also a big reason imposter syndrome exists in CS. No one knows everything. If they did we’d prob know p ≠ np 😉 

What’s important is you grasp fundamental concepts and see recurring patterns/theory being applied in different areas. As an example, how knowledge of basic data structures & algos might be applied to networking algorithms, or transferring knowledge of the digital/boolean logic elements you mentioned into a deeper understanding of abstract math boolean algebra/theory. Lots of concepts in CS are if not identical, very complementary to each other. If you finish a class and 2 years later you can go back and read a book on the topic and quickly glean the information you need, I’d say you did a good job in that course. Overall, your critical thinking and problem solving abilities will be what matters most at the end of the day. Classes help to build and practice these abilities.
With that said, the above reasons are why I think it’s very valuable for someone to explore all the different areas of CS - from a semiconductor up to how the cloud works. It makes concepts click much more easily further down the line.",hb0eil1,t3_peww3x,1630377948.0,False
peww3x,"This is my experience too. I'm not an expert on a lot of what I learnt in IT/CS at University, but I understand the general concepts taught so I know where to look for deeper information.",hb0sy23,t1_hb0eil1,1630385955.0,False
peww3x,"Wow, great explanation, and so spot on.",hb0oesp,t1_hb0eil1,1630383192.0,False
peww3x,How much calculus and discrete math have you used during your career?,hb38a4c,t1_hb0eil1,1630434970.0,False
peww3x,"I think zero calculus (except that some ML stuff uses calculus but that’s at a lower level) and then lite discrete math. No proofs, but showing why/how some algorithm we want to implement will work with math/set theory can be useful. These were also the classes I disliked most in undergrad (discrete math/design of algos - the proofs aspect) though so grain of salt.",hbb9kvz,t1_hb38a4c,1630588807.0,False
peww3x,"We're paid to be problem solvers, not literal human hard drives. Remember the problem solving process, worry less about memorizing the details, and you'll do great :)",hb0jpb7,t3_peww3x,1630380581.0,False
peww3x,"Obviously, not everything will be remembered. But when you forget it, you know what to google. People without a CS background won't even know what they don't know, let alone what to google.",hb0knkv,t3_peww3x,1630381090.0,False
peww3x,"Depends on the kind of software you work on, but in general I would say: 

&#x200B;

Nooooo, but you should absorb a bunch of that info into an intuitive sort of level.",hb0d9z3,t3_peww3x,1630377336.0,False
peww3x,"No, you just google shit when you don't know it, which is most of the time. No one expects you to remember anything, but they do expect you to understand concepts and how systems work together. Like, if you're a professional DBA where all you do is work with databases all day, no one will think its weird you have to look up what the syntax is for the SQL statement you want. But they will look at you sideways if you don't understand the process of, I dunno, database sharding or some other advanced DB concept because you're the DBA.

Since you mentioned software engineering specifically and then listed a bunch of hardware stuff, let me tell you right now that you're *probably* never going to have to know that. They make you learn those things so you have a conceptual understanding of how a computer works. No one will ever ask you to design a circuit. Logic gates, I mean, yeah XOR operations show up and various other bitwise operations. Now I say that, but at some point some new technology is going to come along and you're going to have to learn it, and it might utilize different hardware. and you're going to have to learn how it works. And so its still useful, just not on at level of memorizing trivia and facts.

tl;dr in school they make you memorize to prove you studied the material. But in the real world no one cares what you remember, only that you understand what you're doing.",hb0fn0x,t3_peww3x,1630378503.0,False
peww3x,No. But you'll spend less time recollecting when required. The brain is like Priority Queue or Cache Memory. The unused stuff eventually gets paged and in worse case trashed. But still you can recover in less time than learning from scratch if you had learnt it properly.,hb0m0rl,t3_peww3x,1630381844.0,False
peww3x,"I forgot most of the things I have done in my previous projects, kind of why there is such a priority in comments and documentation in what we do.",hb0hce3,t3_peww3x,1630379357.0,False
peww3x,"Yes. There is literally nothing you can afford to forget at any time.

Jk, just like with most school, you can forget 90% of whatever you learn. The curriculum is to give you a broad appreciation and introduce you to many different specialties so you can choose what you want to go into.",hb0k81g,t3_peww3x,1630380862.0,False
peww3x,"Definitely not. It depends where you end up. If you’re a developer, than you’ll benefit from your algorithm knowledge, data structures, Big O notation, probabilities, etc. Abstract stuff like that.

If you end up working with like embedded systems than the stuff that you mentioned will probably be more relevant.",hb0dpxn,t3_peww3x,1630377556.0,False
peww3x,No U just revolve what u forgot,hb0ff7i,t3_peww3x,1630378394.0,False
peww3x,"I know what a logic gate is. I am deeply familiar with binary presentation. I rougly know how a basic ALU works and what other CPU components there are. I know what a transistor is. Note that you do not actually learn how a transistor works - that's for the electrical engineers to figure out :)

This does not mean I remember much of what I learned in university. And I still am in university. I feel like your examples are ill-chosen because at least some of them are actually important.",hb1bhiy,t3_peww3x,1630400968.0,False
peww3x,no you have google.  No other field has ti remember everything either its impossible,hb1y3eq,t3_peww3x,1630415907.0,False
peww3x,Absolutely!!!,hb0rdgs,t3_peww3x,1630384961.0,False
peww3x,no,hb0fpiz,t3_peww3x,1630378538.0,False
peww3x,No. You'll also find that you'll forget what you did in your old projects after a few weeks. It's not possible to remember everything.,hb0ggiy,t3_peww3x,1630378913.0,False
peww3x,You will learn what you need in the real world,hb0p6iy,t3_peww3x,1630383644.0,False
peww3x,"No, you're only expected to remember how to approach problem solving in your particular domain. Studying is just a good means to build this skill. You'll remember the things you need once you need them. Or at least what to look for and where to look it up.",hb0tlhb,t3_peww3x,1630386372.0,False
peww3x,I hope not! (recent Comp Sci graduate),hb0v1sk,t3_peww3x,1630387357.0,False
peww3x,"It's not the specifics that are important, it is the fundamentals. One cannot be taught without the other.",hb0wnbx,t3_peww3x,1630388478.0,False
peww3x,If you are interviewing yes. Day to day no. In a interview expect people to pull out whatever random little technology tidbit question that they want to.,hb13a8z,t3_peww3x,1630393639.0,False
peww3x,All these responses make me wonder what coders turned to when there was no internet…,hb14md6,t3_peww3x,1630394798.0,False
peww3x,"Reference books and personal journals, I'd bet.  I know every time I've forgotten how SVMs work, I have one page in one specific journal that I can look at",hb31l4e,t1_hb14md6,1630432244.0,False
peww3x,"You just need to know enough that you can find the details on Google/Wikipedia in a few minutes. Basically, know where to search for the solution",hb16053,t3_peww3x,1630396017.0,False
peww3x,"nope. we remember enough to put things together due to practice. when we come across a problem, which we often do! we google :)",hb179oh,t3_peww3x,1630397138.0,False
peww3x,No,hb1h4bg,t3_peww3x,1630405738.0,False
peww3x,"Yes, if you forget even one fact you're fired immediately. Of course not, we all live on google looking stuff up. No one remembers the details the learning process is what's important.",hb1h9i0,t3_peww3x,1630405853.0,False
peww3x,"To be honest,  most jobs in the industry require very little in the way of deep CS knowledge. The main difference that a CS background gives a person is the knowledge that there is an efficient way to solve certain sets of problems. Where a person that does not have the exposure to a CS background will many times brute force their way thru the problem. A CS background will give you the exposure, not to remember exactly how to solve it, but to say oh I remember seeing something like this, and I remember we applied this class of solutions to those problems, let me go back and brush up on that so that I can address it in the most efficient manner.",hb1jkkc,t3_peww3x,1630407561.0,False
peww3x,"Half the time i cant remember how to center a div on a webpage.  You arent memorizing stuff for immediate recll, you are building a mental working space so you have a vague memory of what you need to look up.",hb1jwmu,t3_peww3x,1630407795.0,False
peww3x,"IMO learning different concepts is quite important, and not their actual implementations. You need them for your problem solving skills and so you don't have to invent the wheel over and over again.",hb1k7xb,t3_peww3x,1630408008.0,False
peww3x,Nobody can remember everything. Although we have to make sure to keep the basics right. This will help much better then unnecessarily trying to remember everything.,hb1n89b,t3_peww3x,1630409953.0,False
peww3x,"Nobody has, in any job.

I can only think in some specific jobs that may require you to know ""nearly everything you should know about ut"", like pilot, maybe.

But no, for years existed books, and now there is internet, specially in our profession

Of course, the more you know, the faster and proficient you may get",hb1urnp,t3_peww3x,1630414246.0,False
peww3x,"As a run of the mill programmer who writes programs that don't need to milk every nanosecond out of the hardware, I rarely use my understanding of anything below the mechanics of the languages and protocols I'm dealing with. What I do spend a lot of time musing on is something Bill Kennedy has called \_mechanical sympathy.\_ I understand how these things work to the extent that my programs can run smoothly on them.",hb1vmjx,t3_peww3x,1630414686.0,False
peww3x,Yes absolutely everything needs to be remembered,hb1wdws,t3_peww3x,1630415069.0,False
peww3x,"No. But it actually becomes easier to remember and refresh your memory when you look back at some of it.

You need to understand logic and structure. Those can be applied universally. I often get presented with a new language to learn and code in. I know the language (most likely) must have constructs to do what I want based on the knowledge I gained previously. Experience. I can google the details.",hb1x5lp,t3_peww3x,1630415449.0,False
peww3x,"What you learn in CS undergrad is how to learn about CS in a way that allows you to learn as you go. As an example, my compilers prof told us that the purpose of his class wasn’t to learn how a compiler worked, or be able to answer interview questions about compilers. It was to teach us about how languages worked, and in doing so we would graduate with the ability to pick up any programming language within two weeks. I doubted this idea for almost ten years until I was forced to learn a new language for my career, Golang. Then everything hit me from finite state diagrams I stayed up to 3 am trying to solve, to fragments of the ray-tracing program I wrote for CS 215. No jk. But what I did see were pattens. I was able to see and say, “Oh this is just like Java here,” or “ oh I remember doing something similar in this class, it must use x behind the scenes.” Etc.  You might not realize you are even using the skill you learned in college, but you will. You go to college to learn the basics and how to learn and apply your skills to Cs. I would even go as far to say that college teaches you how to google effectively.",hb1y3bm,t3_peww3x,1630415906.0,False
peww3x,+1,hbyfgsj,t1_hb1y3bm,1631036353.0,False
peww3x,As you gain experience you will have deep understanding of one particular area. That should be enough.,hb1ztwd,t3_peww3x,1630416725.0,False
peww3x,"No just like any other major in your career you look these things up all the time when you need them. But it’s more about indexing the big picture in your head. So when you see a problem you can have that moment where you’re like didn’t we learn about doing this efficiently, or wasn’t there that thing we learned about how the processor works that may be the problem here. 
It’s also about getting your mind accustom to the CS way of thought and training your mind to handle holding big problems in your head. 

It also depends what job you end up going into. Most people aren’t sure yet so it’s good to be broad. You also may find yourself changing focus within CS. It’s pretty common. 

I will say, IMO the cs work seemed more relevant to actual work than many other majors.",hb20p8u,t3_peww3x,1630417125.0,False
peww3x,"Vernacular is also important. Being able to associate one word with the things you are seeing helps us solve problems, do research, and communicate with other engineers",hb20xup,t1_hb20p8u,1630417236.0,False
peww3x,"If you know what to google, is ok.
Manager: ""This process is taking a lot of time""
What you could think: Refactoring code, implement a queue process, separate components, uses different implementations of List in Java or consider another data structure, etc. Even this can be caused by a third-party software, it happens.

At least in my job position, I learned to debug even without a debugger. Just checking code and data flow expected (deploy this was a pain in the ass or we don't have access to the server). The other important thing is to get right logs. I've recently fix a bug that has been in codebase for 8 years. I only could get that because i got the log that i need, after that i could search a solution for that.

Of course if you know barely some topics, you can provide better ideas to solve something. Then you can search for further details.",hb2120s,t3_peww3x,1630417288.0,False
peww3x,Hahahahaha. Absolutely not,hb2cyn9,t3_peww3x,1630422339.0,False
peww3x,"Hardware stuff? Not really, for 99% of jobs",hb2jqsm,t3_peww3x,1630425088.0,False
peww3x,"Hell no, I didn't even study computer science.

Problem solving and logical thought are the key skills.",hb2nq7d,t3_peww3x,1630426695.0,False
peww3x,No. 2 words: stackoverflow and Google,hb3bchj,t3_peww3x,1630436276.0,False
peww3x,"No.

If you can look it up, why remember it?",hb0dyg8,t3_peww3x,1630377672.0,False
pewl46,How is this news?  We've known of the vulnerability of the power and communications infrastructures to CME's/flares for decades.,hb0h076,t3_pewl46,1630379186.0,False
pewl46,It would be quite a news if everything else beside the Internet have turned out to be prepared for such event.,hb0li0a,t1_hb0h076,1630381556.0,False
pewl46,[deleted],hb0qah5,t3_pewl46,1630384298.0,False
pewl46,That was my train of thought also. How fucked are we if tens/hundreds of meters of water are not able to absorb this radiation?,hb0yygc,t1_hb0qah5,1630390178.0,False
pef6an,This might help: [Crafting Interpreters](https://craftinginterpreters.com/),hax1nv2,t3_pef6an,1630323714.0,False
pef6an,"U can start with interpreters, I can suggest ""build your own lisp"" https://buildyourownlisp.com/",hayks37,t3_pef6an,1630348693.0,False
pef6an,"Engineering a Compiler by Cooper and Torczon is a good start and spends a good bit of time on Scanning and Parsing. I then recommend looking into ANTLR to do your scanning and parsing for you when you actually start trying to write your own toy language. You can look up the book The Definitive ANTLR 4 Reference by Terence Paul when you get to that stage. You could spend a lifetime learning compilers, especially optimization. But I recommend getting a basic grasp on all compiler phases before diving too deep into any one of them. Also, when you do start playing around with a toy language, I definitely recommend phasing your approach into Front End -> Intermediate Language -> Back End. Don't try to do everything at once.

Also, DO NOT skim over the Language theory section in Engineering a Compiler, really get a firm grasp on turning Regular Expressions into NFA and NFA into DFAs. It will help you immensely to build a foundation around these things. Do all of the practice problems in this and the following chapters on Context Free Grammars and Parsing.

Good luck! Compilers was one of my favorite topics in Grad School I think you'll really enjoy it. It is extremely rich in multi discipline areas in CS.",hayyvrr,t3_pef6an,1630354513.0,False
pef6an,"If you are interested in compiler fronend e.g. lexing or parsing, the dragon book is for you. If you interested in optimization pass, then some llvm materials are more suitable. If interested in type systems, tapl(types and programming languages) is the bible.",hax8dr4,t3_pef6an,1630327708.0,False
pef6an,"Beep. Boop. I'm a robot.
Here's a copy of 

###[The Bible](https://snewd.com/ebooks/the-king-james-bible/)

Was I a good bot? | [info](https://www.reddit.com/user/Reddit-Book-Bot/) | [More Books](https://old.reddit.com/user/Reddit-Book-Bot/comments/i15x1d/full_list_of_books_and_commands/)",hax8elf,t1_hax8dr4,1630327721.0,False
pef6an,Wrong bible,haxbonm,t1_hax8elf,1630329422.0,False
pef6an,make a wasm compiler. avoid the messy annoying stuff and learn it later,hc5qogc,t3_pef6an,1631175327.0,False
ped1c9,"This example is correct because it implicitly uses the recursion theorem (https://en.m.wikipedia.org/wiki/Kleene%27s_recursion_theorem).

Because this theorem is rather hard to understand, and requires some machinery (in particular, a universal Turing machine), one typically does not present the halting problem proof shown in this answer when introducing the halting problem. Rather, one uses the proof you gave in the post.

The actual problems are twofold: First, programs usually have inputs. While this is not necessary, for didactic reasons you should not suddenly stop talking about the input to a program.

The second, and much larger problem, is that `deceive` references itself. It is a function which has access to its own source code. Remember that the halting problem is for Turing machines, where code is not data, and hence programs can not access or manipulate their own source code. As mentioned, you need to invoke the recursion theorem if you want to rigorously construct that counter example.",hb006re,t3_ped1c9,1630371165.0,False
ped1c9,halting\*,hb29847,t3_ped1c9,1630420804.0,False
pecqes,[deleted],hawrez8,t3_pecqes,1630315961.0,False
pecqes,I believe what OP means is why is it impossible to copy a program's files and just run it like that without installing it again,hax2g3y,t1_hawrez8,1630324217.0,False
pecqes,It's not impossible though. What OP is missing are the data/files outside of the applications root folder that may be critical for the application to run. On a Windows system it's most likely in the Registry or in the AppData folder of the current user.,hax8way,t1_hax2g3y,1630327983.0,False
pecqes,Scrolled too long to see the mention of windows registry and appdata.,haxxh1j,t1_hax8way,1630339045.0,False
pecqes,"In some cases also the C:\Public Data folder, but not only. It's basically up to the devs decision where the data is going to be kept.",hazngyq,t1_hax8way,1630365200.0,False
pecqes,"Yeah, that's what I meant. Still, cool to read the answers, even though it just raises a million more questions lol. Computers seem to take longer than a lifetime to truly understand. Gonna keep at it though!",hax4t8j,t1_hax2g3y,1630325684.0,True
pecqes,"It really depends on the program, and what the developers choose to install during the installation. It is a very common scenario to just install everything in one folder, then it's possible to just copy it to another pc and run it, I've done it a lot of times.",hax8lcj,t1_hax4t8j,1630327821.0,False
pecqes,Interesting! That gives me at least a little more insight into what it takes for the computer to execute a program.,hax9dgw,t1_hax8lcj,1630328234.0,True
pecqes,"To execute a program, it only needs (in windows) the .exe file. The rest is just additional data that the individual program needs (.dll are pieces of code that aren't put in the exe file so they can be replaced later or shared with other programs). Maybe it is a game, so it will need images for the graphics (for example, idk). Maybe it needs to store data and creates a local database somewhere, or txt configuration files.",haxa6ia,t1_hax9dgw,1630328659.0,False
pecqes,"The only thing the OS needs to do when executing a program are:

1. Allocating the resources and calling the program itself

2. Ensuring the program has the appropriate permissions or the ability to request them.

Everything else is the program's responsibility. So when you see those errors about missing DLL files, it's usually because the program failed to ensure the files it needed existed during installation.",hayb7ao,t1_hax9dgw,1630344712.0,False
pecqes,"Linux distros tend to use plain text files as registration points. Windows uses Registry.

Sometimes configs aren't transferable even in Linux world. Imaging a record in `/etc/crontab` which is mixed with configs for other packages.

It is registrations (for drivers / services / schedulers / platforms like Java/Core#) that you miss when copy files. 

Next time copy appropriate Windows registry keys )) I think some of them are PC dependent and need to be recomputed for software to work.

And note that some copy-protection schemas bind an installation to hardware IDs.",hawrwrx,t3_pecqes,1630316378.0,False
pecqes,"How does the data get stored? Where does the data get stored? What environment variables, links, and registrys have been altered / created?

If you had an identical machine, you could copy it over. BUT, you’d have to repeat all the stuff the installer did to set up the environment. So, that’s why you just install a new version of it on the second machine and import your data over; a service that is often available.

A lot of times the installer isn’t compiling on your machine, especially in Windows. You’re getting the binaries, but the installer is just setting up the environment.

As a note, you seem to be mistaken on how compilation / binary works. Binary is not OS dependent, it is the most universally understood computer “language”. Windows machines can’t execute other Window machine’s compiled binary because they’re the same OS, it would be because they have the same architecture, hardware, and/or interpreter. Check out this recent post that could should a little more about [why compiled / interpreted languages exist](https://www.reddit.com/r/computerscience/comments/paqy2k/why_interpreted_languages_were_made_instead_of/?utm_source=share&utm_medium=ios_app&utm_name=iossmf)",haxa483,t3_pecqes,1630328626.0,False
pecqes,"A key factor that doesn’t appear to have been mentioned is the available api.  Executable format aside, which can determine a lot between each OS, the available libraries are a major area of incompatibility.  Win32 isn’t a standard api in the *nix world.  GNU is not a standard set of libraries in Windows.

If the question is purely a copy between different instances of the same OS, the Windows registry is one reason.  For Linux, it could be configurations missing from /etc or /usr.  Another is the data that lands in AppData or ProgramData in Windows (these are usually hidden directories off C: or the user folder).  The equivalent would be a .directory under the user home, which is also typically hidden.",haxm26x,t3_pecqes,1630334196.0,False
pecqes,This is the answer I found most aligned with my own thoughts on the matter,hay042o,t1_haxm26x,1630340133.0,False
pecqes,A simple counterexample: so-called [portable apps](https://portableapps.com/). These are standard desktop applications which have been modified such that everything they require to run is in a single directory. You can move them from one windows machine to another and they will just work. Often people install the app to a portable drive and just plug the drive into whichever computer they happen to be using.,hazbon0,t3_pecqes,1630359908.0,False
pecqes,"Not individual programs, but you can do a bit-for-bit clone of a hard drive. I did this once when I upgraded to a larger hard disk, but didn't want the hazzel of reinstalling windows.",haygv74,t3_pecqes,1630347059.0,False
pecqes,"It's not impossible. Programs work on a hierarchy, there are ""low level"" programs, or ""close to the machine"" which are things like assembly language which is executed on a CPU with the correct instruction sets. 

These can't be ""copied over"" per say as it's possible one machine might have different lower level instructions. 

Then there are high level languages, these are compiled or interpreted (essentially converted) into a low level version of themselves which can run on a specific operating system (which sits on top of the cpu). 

I can write something high level in python, and have the same code run on different systems because there are compilers/interpreters which convert them to the operating systems specific code - all the while taking advantage of operating system specific features like storing data in ""AppData"" for windows - linux has it's own nuances. 

Discord, Slack, etc are all high level applications, they are specifically designed to work on top of the main OS - windows, linux, ios etc. You can copy files from one windows machine, to another, and have it work - assuming you have moved all the corresponding files (eg not just the stuff in ""program files"" - but held elsewhere on the operating system or registry) but trying to do the same for windows to linux isn't really possible without using some kind of interpreter to convert what is currently stored over to the new operating systems.

So the long and short of it is, it's much easier to just think of them as different applications across different operating systems and you will have an easier time. 

tl;dr: different cpus have different instruction sets. OSs are designed to communicate between the program and the cpu. You can copy high level programs across the same OS type to different machines assuming you've got it all. But low level languages that interact closer to the CPU have instructions which may be different between CPUs and thus ""can't"" easily be copied.",hazc0dl,t3_pecqes,1630360051.0,False
pecqes,"First, many programs on OSX are installed by copying them into the applications directory.

For windows, linux, and other OSX apps, they install more than just the app binary, they might install or update library files, put data into various directories, put entries into a registry database, set permissions on other files, etc.

Short story, many applications shit all over your filesystem on install.  Not only cant you drag-install, you might never be able to properly undo that install.",hazjgee,t3_pecqes,1630363379.0,False
pecqes,"Used to do this with quake 3 arena, i burned the “installed” game to a cd and in computer lab you could just copy the disc locally to play it.",hb09h4f,t3_pecqes,1630375495.0,False
pe0kzr,175k of them are looking for tech support :),hau9nqs,t3_pe0kzr,1630265898.0,False
pe0kzr,Can anyone tell me why my PC shuts down when I swing it around the room by the AC cord?!?!,hav32pt,t1_hau9nqs,1630278991.0,False
pe0kzr,So it was you that punched me yesterday?,haukru0,t3_pe0kzr,1630270682.0,False
pe0kzr,Do we get free donuts or something? :P,havllvz,t3_pe0kzr,1630288069.0,False
pe0kzr,Ofc it did. The 5 programmers in there figured out how to write a subscribing bot...,haygdkw,t3_pe0kzr,1630346856.0,False
pe0j84,"The Z3 could get programs via punched tape, and was the first binary computer that worked and was also turing complete.

I'm not sure about the ABC's programmability (or lack of it), but it is defined as electronic since it used mostly vacuum tubes.

So you could see it as the first computer to use relays, though a lot of people seem argue that relays are electromechanical and therefore the Z3 wasn't the first computer.",hdz3y2r,t3_pe0j84,1632411226.0,False
pdvn89,"Please at least describe what kind of material or examples you've been trying to read, and where you're getting stuck.

Otherwise anybody answering your question will have to either guess what you already understand and what you don't (and they'll probably guess wrong), or they'll need to assume no prior understanding and explain everything from the ground up. Either way they'd have to do extra work because there's so little context in your question.",hatus35,t3_pdvn89,1630259550.0,False
pdvn89,I assume it’s the complexity and reduction problem maybe? Eg if we can map this problem to the halting problem we can prove this is unsolvable? At least that’s my assumption. u/Helasri ?,hau83u6,t1_hatus35,1630265236.0,False
pdvn89,You may want to do a search for *Rice's theorem*.,hbaffms,t3_pdvn89,1630567554.0,False
pddph3,"Strong consistency is like a formal dinner, it doesn’t end until everyone sits down and eats.


Eventual consistency is like a buffet style dinner. It can end when one person finishes eating, but at some point everyone will have the same meal.


Strong consistency, a server can not send a response to a request until it’s result is visible and saved on every server.

Eventual consistency, a server can respond to a request before all servers have the result if you can ensure at some point in the future all servers will have the same result.


Example, if I put a key/values K:V, on server A and request the value of K from server b afterwards. Strong consistency ensures that value will be the same as the one on server A, eventual consistency only ensures that at some point in the future they will be the same so it may not be immediately reflected when you make the second request but it will be at some later point in time.",hapg18p,t3_pddph3,1630172631.0,False
pddph3,🙌🏾,hapl2x1,t1_hapg18p,1630174764.0,False
pddph3,"changing the names makes everything much clearer

temporary inconsistency = eventual consistency

actual consistency = strong consistency


__


with eventual consistency we allow for ""temporary"" inconsistency in the case of a network partition, then we try to undo the damage of the temporary inconsistency when the network partition goes away

__

with strong consistency the data NEVER becomes inconsistent. in the case of a network partition, we stop accepting requests for fear of inconsistency

__

see chapter 10 of this textbook for the best explanation I've found 
https://ocw.mit.edu/resources/res-6-004-principles-of-computer-system-design-an-introduction-spring-2009/online-textbook/part_ii_open_5_0.pdf",har6a2f,t3_pddph3,1630201947.0,False
pddph3,"You have a ledger, which has credit and debit records. When adding all the debit records together, they must equal the sum of the credit records. The ledger doesn't make sense without this being true.

Given [strong consistency](https://en.wikipedia.org/wiki/Strong_consistency), this is always guaranteed to be the case. Given [eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency), you can potentially see a difference.

Formally speaking, strong consistency is a guarantee that all observers agree on the ordering of all the data accesses in the database. Some databases allow this ordering to be different than the ordering as seen by the clients. Example. A client enters two transactions, A and B. It's possible that a strongly consistent system returns these transactions in the order B and A.

So strong consistency itself doesn't guarantee the intuitive order as presumed by the client, which is why the [strict consistency](https://www.cs.colostate.edu/~cs551/CourseNotes/Consistency/TypesConsistency.html) model was introduced, which guarantees that these orderings also follow physical time order.

Eventual consistency just throws its hands up and says 'you get what you get' and serves the data it encountered until that point in an order determined by the circumstances.",hawk5p8,t3_pddph3,1630309618.0,False
pddph3,"**[Strong consistency](https://en.wikipedia.org/wiki/Strong_consistency)** 
 
 >Strong consistency is one of the consistency models used in the domain of concurrent programming (e. g. , in distributed shared memory, distributed transactions). The protocol is said to support strong consistency if:  All accesses are seen by all parallel processes (or nodes, processors, etc.
 
**[Eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency)** 
 
 >Eventual consistency is a consistency model used in distributed computing to achieve high availability that informally guarantees that, if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value. Eventual consistency, also called optimistic replication, is widely deployed in distributed systems, and has origins in early mobile computing projects. A system that has achieved eventual consistency is often said to have converged, or achieved replica convergence.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hawk6py,t1_hawk5p8,1630309642.0,False
pdc4en,"Computing power doesn't scale the same way that storage does. You cannot connect a bunch of computers together and make bigger computer that works faster for general everyday use. However, you can connect a bunch of computers together and see significant speed gains for some types of highly parallelizable, computationally expensive problems.

The question is, do you have a problem that is large and parallelizable enough that would benefit from being used in such a system? It won't be worth doing unless you can still see speed gains after factoring in the cost of breaking the problem into smaller units, distributing all the necessary information among all the computers, receiving the results, and assembling it into the final output.

A good example is raytracing for 3D animation: the assets (i.e., the scene, character models, and textures) are generally pretty small, but the computations are very expensive. It can take hours to complete a single frame of animation. Say you're making a CG movie that renders at about 5 hours per frame. On one computer, you would get 1 frame every 5 hours. However, if you had each of your 10 computers work on a frame, the cost of distributing the assets would be miniscule compared to the savings you get from having 10 frames completed in 5 hours.

A bad example is video games. Sure, the principles behind video games are similar to the raytracer example, and you could probably devise a system where each of your 10 computers is responsible for handling some component of the game. However, there still has to be some main computer that the player is interacting with. The latency introduced by having the main computer communicate with the others would overshadow any savings you got by parallelizing the game, and the game would be unbearably slow.",hap2u63,t3_pdc4en,1630167000.0,False
pdc4en,[deleted],hapxobr,t1_hap2u63,1630180319.0,False
pdc4en,"It's not necessarily a single ray. You can trace rays for every single pixel or for parts of the image and split these subtasks onto multiple machines. Furthermore, a single render image does not just consist of the image itself. Usually you have multiple passes for multiple components of the image, for example (from the top of my head; I'm not really a rendering pro) a light pass, an ambient occlusion pass, a color pass, and several others passes that can be calculated independent of each other, and just have to be merged into the final image. The passes can be split across multiple machines too. And last but not least for a render sequence it's possible to split the sequence (animation) into multiple subsequences (parts of the animation) that can also be split across the rendering machines.",hapyxfm,t1_hapxobr,1630180890.0,False
pdc4en,"My explanation wasn't super clear about this, but in my hypothetical raytracer, I was having each of the 10 computers render 10 different frames. So there wouldn't be any concerns around which computer is responsible for which ray. Instead, each computer is responsible for producing a single frame of output.

How you choose to split up problems in a parallel computing environment is a major consideration. My suspicion is that splitting a raytracing problem at the ray level instead of the frame level is too granular, and I am further suspicious that doing so would be too slow to be worth it. I doubt a single ray is very expensive to calculate, which means that the computer responsible for a ray may spend more time preparing itself for calculation than actually doing it, and would probably leave a significant amount of CPU and GPU capacity unused.

Having said that, I would totally buy it if someone involved in optics or materials science or whatever said that ray-level granularity is appropriate for advanced, detailed simulations of light traveling through some specialized plastic fiber-optic material. In this case (which I 100% pulled out of my butt), I'm imagining that the ray calculations are so difficult and time-consuming that it becomes worth it break up the problem at the ray level.",haqja80,t1_hapxobr,1630190345.0,False
pdc4en,"Parallel rendering using multiple GPUs has been tried before with Nvidias SLI and NVlink, and both have pretty much no use today due to the added cost and complexity and visual artifacts in early generations.",hapybdc,t1_hap2u63,1630180612.0,False
pdc4en,"I love the whole question because OP didn't realize they were asking a question about distributed microservice versus monolithic architecture, or sync versus async, but your explanation applies discussing those things too. 

It's amazing how the most 'naive' questions often come back to the core of the biggest intellectual problems of the day.",harvuto,t1_hap2u63,1630217525.0,False
pdc4en,"Yes

https://en.wikipedia.org/wiki/Computer_cluster

Depend of the use of your cluster you take more speed or nothing.

I think there is nothing specifically for ""a desktop cluster"", only for servers and related. Anyway if exists and you have old computers due power consumption and processor features of old vs new computers  is better buy a new computer with 10 cores or 20 cores than use a cluster of 10 computers with one/two core.

beyond the 'for fun/learn' I don't think it's worth it

cloud computing is similar, is a cluster running virtual servers/services",hapcss8,t3_pdc4en,1630171329.0,False
pdc4en,[PlayStation 3 cluster](https://en.m.wikipedia.org/wiki/PlayStation_3_cluster),haq57b6,t1_hapcss8,1630183724.0,False
pdc4en,"Yes and no. You can harvest some components like drives and maybe memory sticks and add them to one.  This assumes that the one has enough memory card slots and drive connections.

You could, potentially, harvest the CPUs but it is unlikely that the one machine you are trying to buff up has empty CPU sockets on the motherboard.

You could ""network"" them together into a ""cluster"" and use multiprocessing and parallel processing methods like MPI, openMP and other things.  You could leave them as-is and use a ""high throughput computing"" system like CondorHTC.",hap0tsm,t3_pdc4en,1630166120.0,False
pdc4en,"Your problem, i.e. program, has to be amenable to doing so.  In other words you can't chain a bunch of laptops and expect chrome on any one or all of them to run faster, but if you have a program which would benefit from breaking a problem up into little pieces and then dividing the work out, and you had the means of doing that, breaking it up, dividing the work, then bringing the results together then you could.  In some sense the OS is just a big program that runs big programs, so you could have a special OS that would allow for this in some cases.",hapgc6g,t3_pdc4en,1630172757.0,False
pdc4en,"Sure. Simple example: calculators are simple computers; if you need sum up a long list of numbers, you can split the list up and have your friends all use their own calculator to sum their part, then one of you sums all the results.   
In the modern day we've got programs so you don't have to enter the numbers yourself, and networking to communicate between computers. But the core challenges are the same. How do you split the list up? How do you decide who sums things up at the end? What if you have so many calculators that it becomes hard to sum all the results on one calculator? What if you're dealing with a more complicated equation with more variables that need to be communicated between people?",haqbd0k,t3_pdc4en,1630186556.0,False
pdc4en,Google “Beowulf Cluster”,haps9eo,t3_pdc4en,1630177874.0,False
pdc4en,"When you do distributed computing your trade off is lag. So you can take a job that would take 10 days to run on one machine and run it in 2 days on 10 machines (you don't get a 10x improvement)

But you can't run a game at 60 fps even with 4 computers that can do 30, the communication time isn't well suited to small tasks that require a low latency.

Likewise for some very rapid tasks like timing a bullet going through two light gates a microcontroller can beat a PC. It isn't nearly as powerful but it's simple and low lag.",haqaoq8,t3_pdc4en,1630186238.0,False
pdc4en,You can but it's not simple.,hapgwj6,t3_pdc4en,1630172989.0,False
pdc4en,"I would recommend getting a 10TB hard drive, a expensive (thousands of usd) CPU and GPU, good RAM, all in a desktop.",harlq3i,t3_pdc4en,1630210581.0,False
pdc4en,The most you could do is set up a distributed computing network over home wifi or a bluetooth ring.,hat2hcp,t3_pdc4en,1630247731.0,False
pdc4en,"depends on the application. 

For a game? no. Processing the separation of data would take more processing power than just rendering it on one PC, let alone the latency, every computer working at different intervals(I mean, bot every PC receives the task at the same time, and they start working at different times)

But there are applications where you can do such a thing. it's mostly for baking stuff(when you have a raw data, and you need to process it, in non-real time).

E.g. render farms for rendering 3D modeling and compositioning software like blender. It separates the required render data into kultiple segments and sends each to a different device. 

AI and machine learning are another example",hat5ruf,t3_pdc4en,1630249176.0,False
pd04of,"https://imgur.com/1EHwP8B
simply bit-shift the mantissa integer by the exponent integer.",han1t98,t3_pd04of,1630119981.0,False
pczjmr,"I’ve always heard it pronounced ’char’ but honestly, just go with how those your working with pronounce it, that’s really all that matters to avoid confusion.",hamodvu,t3_pczjmr,1630113082.0,False
pczjmr,Really? Because I always pronounced it ‘char’ but if you pronounce it ‘char’ I guess that also makes sense.,hamsr9p,t1_hamodvu,1630115250.0,False
pczjmr,"No, it's 'char"" actually. You have it both wrong",hanff26,t1_hamsr9p,1630128481.0,False
pczjmr,Really? Where I’m from we actually pronounce it ‘char’. You really pronounce it ‘char’? That’s so strange to me.,hany50c,t1_hanff26,1630144134.0,False
pczjmr,"I'm from Germany. Maybe it's an accent thing. Some people I know from other countries pronounce it ""char"" which I find pretty funny. I mean, I'm not sure if ""char"" is the correct pronunciate, but ""char""? Come on!",hanzl5d,t1_hany50c,1630145400.0,False
pczjmr,"No. It's char, as in... Char...",hao3wnc,t1_hanzl5d,1630148816.0,False
pczjmr,"I have heard people say it “char” (like charmander), “care” (like careful), and like “car” (like cardamom). I personally try to think of it / speak of it like a byte or string pointer rather than a character because saying it the other ways feels and sounds kinda of awkward.",hamqdd7,t3_pczjmr,1630114075.0,False
pczjmr,what about “shar” like chardonnay??,hamsxo1,t1_hamqdd7,1630115340.0,False
pczjmr,"Thanks, that brightened my day :)",hamupin,t1_hamsxo1,1630116247.0,False
pczjmr,Or like kkkrrraarrr,hanfh8o,t1_hamsxo1,1630128525.0,False
pczjmr,i feel like a lot of phlegm comes out in this pronunciation,hapqkzu,t1_hanfh8o,1630177129.0,False
pczjmr,"I'm German and this is how I pronounce it since this is how you pronounce it in German. At least that's how I originally learned it, and old habits die slowly.",hapnuxw,t1_hamsxo1,1630175943.0,False
pczjmr,thank you for chär-ing… hyuck hyuck 😁,hapqg4i,t1_hapnuxw,1630177070.0,False
pczjmr,"> like charmander

There’s a cultural reference that makes me feel old",hantsjj,t1_hamqdd7,1630140211.0,False
pczjmr,Charmander still comes in new episodes...,hap88sr,t1_hantsjj,1630169383.0,False
pczjmr,Haha no the whole of Pokemon is a phenomenon I missed out on / am too old for,hap8way,t1_hap88sr,1630169662.0,False
pczjmr,like charizard,hamv9is,t3_pczjmr,1630116530.0,False
pczjmr,"Cha arrr

Edit: be sure to assert your dominance and fully enunciate.",hamxxhy,t3_pczjmr,1630117916.0,False
pczjmr,"For full effect, wear an eyepatch and bring your wooden leg.",han5vdq,t1_hamxxhy,1630122297.0,False
pczjmr,care,hamwkjt,t3_pczjmr,1630117209.0,False
pczjmr,You monster.,hamzgng,t1_hamwkjt,1630118722.0,False
pczjmr,well it stands for character so thats what i thought,hba63ow,t1_hamwkjt,1630560475.0,False
pczjmr,Blasphemy,hapjvzw,t1_hamwkjt,1630174251.0,False
pczjmr,Ive pronounced it char in my head. I heard it said as ‘car’. But honestly it’s short for character so it should be ‘care’. Hope that helps. Lol,hamseyn,t3_pczjmr,1630115081.0,False
pczjmr,But the pronunciation of the last ‘re’ part of ‘care’ doesn’t really match character either…,hao4d1m,t1_hamseyn,1630149158.0,False
pczjmr,Could be an accent thing? Both sound like air to me.,hao4jbo,t1_hao4d1m,1630149283.0,False
pczjmr,"Wait do you pronounce character “cairacter”?
Weird! What accent do you have?",hao4utf,t1_hao4jbo,1630149518.0,False
pczjmr,Northeastern American. Not New York or Boston,hao50yn,t1_hao4utf,1630149638.0,False
pczjmr,I’m from Florida and also pronounce it as “care-acter”. I believe this is the standard American English pronunciation,hao8wcn,t1_hao4utf,1630152295.0,False
pczjmr,…how *else* could you pronounce “character”?,haq3oj6,t1_hao4utf,1630183040.0,False
pczjmr,"In my accent (from the UK), ""care"" and ""character"" sound absolutely nothing alike. Here's the closest I can find online: https://youtu.be/ntyF9O_NWdk",haqa64u,t1_haq3oj6,1630185997.0,False
pczjmr,"I’ll have to try to check on this later but dictionary.cambridge.org shows the same pronunciation for both “care” and the first syllable of “character” in both American and British English.

edit: it wasn't rendering well on my phone, they are indeed pronounced differently in British English.  ""care"" rhymes with ""air"" in both (with a slightly different vowel sound in American vs. British), but the first syllable of ""character"" in British is `kær` rather than `keər`.

https://dictionary.cambridge.org/us/dictionary/english/care
https://dictionary.cambridge.org/us/dictionary/english/character

has links to pronunciations in both.",haqubdi,t1_haqa64u,1630195801.0,False
pczjmr,"I’m from Denmark so as a non-native speaker, I have a bit of a mix of accents. But I generally speak more British than American, so that’s probably why I think ‘care’ is nothing like ‘character’.",has25y9,t1_haqubdi,1630222557.0,False
pczjmr,This is pretty much how I would pronounce it as well.,has1xc5,t1_haqa64u,1630222361.0,False
pczjmr,"Who are these people pronouncing the word character as: 

Ker ik ter?",hapk1s4,t1_hamseyn,1630174322.0,False
pczjmr,Confidently.  Show no hesitation in your pronunciation and anyone who disagrees will silently wonder if they've been saying it wrong.,haonnlb,t3_pczjmr,1630160252.0,False
pczjmr,Not like car.  Car is reserved for LISP.,hamlyyt,t3_pczjmr,1630111864.0,False
pczjmr,I cdr care less about that comment,hamw7a2,t1_hamlyyt,1630117020.0,False
pczjmr,lolz,hanc1pk,t1_hamw7a2,1630126157.0,False
pczjmr,Car and cdr are such stupid names,hapzrzw,t1_hamlyyt,1630181274.0,False
pczjmr,"Sounding like ""care"". It is shirt for ""character"" after all...",han38y2,t3_pczjmr,1630120790.0,False
pczjmr,That must be a cool shirt.,hao46fe,t1_han38y2,1630149022.0,False
pczjmr,Short sleeved one I think,hapae5d,t1_hao46fe,1630170313.0,False
pczjmr,"Crop top. Full sized shirt says the whole word ""character""",hapnrty,t1_hapae5d,1630175906.0,False
pczjmr,Careicter?,hapjtva,t1_han38y2,1630174226.0,False
pczjmr,I thought this was the Gundam subreddit for a second.,haorirn,t3_pczjmr,1630162011.0,False
pczjmr,"Hmm... ""Char"" to be honest, but by right it should be pronounced like ""care"" as in ""character"", right?",hamxdxe,t3_pczjmr,1630117632.0,False
pczjmr,It’s pronounced Char as in Char,hanbxae,t1_hamxdxe,1630126077.0,False
pczjmr,"The ‘re’ in care doesn’t really match character either. 

I feel like English is just so inconsistent that people should just pronounce it however they want.",hao4mhq,t1_hamxdxe,1630149347.0,False
pczjmr,Char is most certainly not pronounced like care.,havksvw,t1_hamxdxe,1630287671.0,False
pczjmr,"Even if it is short for character, I think it is perfectly finde to pronounce it \[tʃɑɹ\] or \[kɑɹ\], just think of how you say ""brother"" and ""bro"" (or ""fun"" and ""funeral"", even though those words have nothing to do with each other but I like the example). English is very inconsistent in its relationship between spelling and pronunciation. But it's not my native language so I have no idea and maybe you should not listen to me.",hanjwp7,t3_pczjmr,1630131829.0,False
pczjmr,"""care""

I've been told I'm too literal.",handvss,t3_pczjmr,1630127411.0,False
pczjmr,"I've always pronounced it like 'care', as that's similar to how the first syllable of 'character' is pronounced in American English. I most often hear it pronounced like 'char' as in 'char-broil' but I don't often get to talk with other people about programming, so whatev.",hanf7sg,t3_pczjmr,1630128336.0,False
pczjmr,"I pronounce it like ""care"" because that's the sound it actually makes in ""character"".",hangl8x,t3_pczjmr,1630129326.0,False
pczjmr,like charred without the -ed.,hanorxe,t3_pczjmr,1630135813.0,False
pczjmr,"Don't really care. It's probably clear from the context, regardless of if you say it like charcoal or character. English is feature degenerate enough that you'll find arguments for both.",haml6oq,t3_pczjmr,1630111472.0,False
pczjmr,"normal Pooh: char, car, etc.

fancy Pooh: *character*",hammmqv,t3_pczjmr,1630112199.0,False
pczjmr,char/car depending on how I feel,hamueap,t3_pczjmr,1630116086.0,False
pczjmr,"""Beep, beep, vroooom!""",hamzsc1,t3_pczjmr,1630118893.0,False
pczjmr,Charmander char,hanexjn,t3_pczjmr,1630128139.0,False
pczjmr,"I always say it in the same voice as Charmander from the Pokemon anime: ""Char!""",hangdm5,t3_pczjmr,1630129174.0,False
pczjmr,As in Charles,hanrggy,t3_pczjmr,1630138126.0,False
pczjmr,Char burger,haq9i00,t1_hanrggy,1630185687.0,False
pczjmr,I had a professor that would always talk about how there are two different types of people: those who say care and those who say char. Meanwhile I always say car.,hanrup7,t3_pczjmr,1630138477.0,False
pczjmr,"When coding ""char"".

When talking to others ""character"".",hanscmv,t3_pczjmr,1630138919.0,False
pczjmr,"If you pronounce “char” like car, how do you pronounce CAR and CDR?",hantqr4,t3_pczjmr,1630140166.0,False
pczjmr,"I am used to pronouncing it ""tsar"". But in my language, greek, if you pronounce ""putchar"" this way you get extremely close to the word for dick. So it's a nightmare TAing freshers while trying to avoid profanity. Recently I have been trying to relearn it as ""car"" to avoid this mess.",hao02uc,t3_pczjmr,1630145825.0,False
pczjmr,"Character has a hard c sound, so like “car”",hao2dwr,t3_pczjmr,1630147661.0,False
pczjmr,"If you pronounce ‘character’ starting with the same ‘ah’ sound as in ‘car’ then honestly you pronounce ‘character’ weird as well. 

I pronounce it like the beginning of ‘char’. To me that makes at least as much sense ‘car’. 

But in the end you can pronounce it however you want.",hao3rq2,t3_pczjmr,1630148716.0,False
pczjmr,"I've always pronounced it ""ch-are"". I don't think it matters much though",hao4nia,t3_pczjmr,1630149368.0,False
pczjmr,"Non-English speaker : since the dawn of my programming journey, I pronounce it char, as if pronouncing chair",hao4tlt,t3_pczjmr,1630149494.0,False
pczjmr,I pronounce char like “char aznable” 👀,haocbn7,t3_pczjmr,1630154353.0,False
pczjmr,lol,haoqun0,t1_haocbn7,1630161710.0,False
pczjmr,"""See,"" ""Eche,""  ""Eh,"" ""Are."" It's an acronym, no?",haomy4t,t3_pczjmr,1630159928.0,False
pczjmr,"I fully admit this is not logically consistent, but:

-	`char`: char (like charred meat, or charizard)
-	`VARCHAR` (sql): var-car (like varnish & an automobile- car)",haood6q,t3_pczjmr,1630160576.0,False
pczjmr,I've always heard it and said it like car so you're not alone lol,haory3u,t3_pczjmr,1630162206.0,False
pczjmr,like share,haovgdw,t3_pczjmr,1630163761.0,False
pczjmr,char because char star rhymes (char*),haox9p6,t3_pczjmr,1630164563.0,False
pczjmr,K-ahar,haoyef8,t3_pczjmr,1630165059.0,False
pczjmr,How do you pronounce character? Now remove “acter”. That’s how,hap4q95,t3_pczjmr,1630167842.0,False
pczjmr,"Hard 'ch', char.

This pronunciation works best for when referring to a char pointer: char star.  They rhyme.",hapaeqi,t3_pczjmr,1630170320.0,False
pczjmr,"I’ve heard it either : ch-ar (no pause, just writing this way for emphasis) or like “care”",hapclhz,t3_pczjmr,1630171250.0,False
pczjmr,Čar,hapcos6,t3_pczjmr,1630171285.0,False
pczjmr,HAR,hapg3jv,t3_pczjmr,1630172658.0,False
pczjmr,"Well, it's short for character. So I pronounce it ""Care""",hapgcd7,t3_pczjmr,1630172759.0,False
pczjmr,“Char”,hapi6f9,t3_pczjmr,1630173525.0,False
pczjmr,"I say it like car, with a silent breathy H, the same way it’s pronounced in its full form of character.",hapjmqr,t3_pczjmr,1630174142.0,False
pczjmr,Char,hapkqtx,t3_pczjmr,1630174619.0,False
pczjmr,Ch - arrrr,hapmgta,t3_pczjmr,1630175350.0,False
pczjmr,Jiff,hapn3pn,t3_pczjmr,1630175621.0,False
pczjmr,like someone in 1870s south dakota would pronounce chair,hapnits,t3_pczjmr,1630175801.0,False
pczjmr,"I pronounce it like ""charm"" but without m
Although i'm not even a native English speaker",hapol61,t3_pczjmr,1630176258.0,False
pczjmr,ciar,hapqrz0,t3_pczjmr,1630177214.0,False
pczjmr,It's nee-kol-aj !,hapwj8j,t3_pczjmr,1630179810.0,False
pczjmr,"I've only ever heard it pronounced as char (""ch"" like chew, and ""are"" rhyming with car or star)",hapzur5,t3_pczjmr,1630181308.0,False
pczjmr,"> Makes sense to me because character would be pronounce like car and because char is just abbreviated character it makes sense you would pronounce it like car.

You pronounce the first syllable of “character” like “car”? What kind of accent is that? I’ve only heard it like “care”.",haq08w5,t3_pczjmr,1630181487.0,False
pczjmr,uint8_t,haq2job,t3_pczjmr,1630182524.0,False
pczjmr,"If the person u r talking to understands it, then it does not matter how u pronounce it",haq2yqh,t3_pczjmr,1630182714.0,False
pczjmr,Car,haqb52v,t3_pczjmr,1630186453.0,False
pczjmr,Ki-ah-ree,haqdfrw,t3_pczjmr,1630187532.0,False
pczjmr,Charcoal minus coal,haqjynw,t3_pczjmr,1630190675.0,False
pczjmr,uint8_t,haqpi6h,t3_pczjmr,1630193404.0,False
pczjmr,ive heard it pronounced char with a “ch” sound,haqth2l,t3_pczjmr,1630195375.0,False
pczjmr,"i always just say character and don't try to shorten it, say/think character, type char",haqu1bl,t3_pczjmr,1630195659.0,False
pczjmr,Car,haqvz0i,t3_pczjmr,1630196646.0,False
pczjmr,"Not like ""car"" and not like ""care"". Like the fish.",har02lx,t3_pczjmr,1630198744.0,False
pczjmr,"I always said ""ch"", because the guy in the video said varchar, not varcar.",hardpz0,t3_pczjmr,1630205937.0,False
pczjmr,I pronounce it care like character but cut off the end. But really who cares there are so many engineers who speak different languages and pronounce things differently. The most important part is you understand what they're trying to say.,hartje3,t3_pczjmr,1630215783.0,False
pczjmr,"youve fallen into my trap. after i found out that character is long for char i started pronouncing it ""chair-actor"" and no one can stop me",hcxcotf,t3_pczjmr,1631692559.0,False
pczjmr,It’s car. It’s not even subjective. It just is. Whoever told you it was weird needs to stop using eMacs and tabs …,hamipua,t3_pczjmr,1630110261.0,False
pczjmr,Language is pretty subjective though,haojlrh,t1_hamipua,1630158313.0,False
pczjmr,It was dry humor but clearly this sub has none. Lmao at all the downvotes.,hapl8ph,t1_haojlrh,1630174831.0,False
pczjmr,Can’t you even tell when downvotes are sarcastic? Talk about no sense of humor.,hapniif,t1_hapl8ph,1630175797.0,False
pczjmr,"I pronounce it with a hard G like ""gift""...

G-iff",hanofo5,t3_pczjmr,1630135516.0,False
pczjmr,"40+ years programming: /kär/  
char\* (car star)  
varchar",hapk1tb,t3_pczjmr,1630174322.0,False
pcunb7,check out [Teach Yourself Computer Science](https://teachyourselfcs.com/) and look at their book recommendations.,haljz37,t3_pcunb7,1630094473.0,False
pcunb7,"I did check this guide and it's great, but they seem to advice video lectures over textbooks, especially for databases which doesn't list any full fledged textbook(s).",haog467,t1_haljz37,1630156498.0,True
pcunb7,Every single section in TYCS offers at least one book. While the database section recomends starting out with video lectures it does list textbooks near the end.,hb37b0v,t1_haog467,1630434575.0,False
pcunb7,"CLRS CLRS CLRS 

Can't go wrong with that for algorithms",hamo3as,t3_pcunb7,1630112934.0,False
pcq1om,"If it was integrated into a product I already use like Github or as a VS Code extension I would find it useful; otherwise it seems too inconvenient.

Creating some sort of image api to include readmes would be interesting too.",hakxp70,t3_pcq1om,1630085297.0,False
pcq1om,It is the most valuable feedback so far. Thanks,hal0cku,t1_hakxp70,1630086360.0,True
pcq1om,"I would like to see which parts of the code edited most. That can give the ability to increase complexity in least touched places and decrease complexity in most touched places. It can be a heat map of files and lines

Also I would like to see which developers edit which parts so a team can have a better distrubition of understanding of the code and if needed developers can switch around to touch places they didn't touch before, this can be a colored heat map of files, or a size based graph for each developer",hal10hh,t3_pcq1om,1630086626.0,False
pcq1om,"There was a project that visualized a git repos whole life in a really cool geometry wars way. I’ll try to find it.

Found it:

https://m.youtube.com/watch?time_continue=5&v=oIZuzOq-tPw&feature=emb_title

That’s the video

Here’s a site with info on it:

https://www.culturefoundry.com/cultivate/technology/using-gource-to-visualize-a-gitrepo/",hakzdg7,t3_pcq1om,1630085972.0,False
pcq1om,"Honestly, I expected a few charts. Now, I am awestruck!",hbkjbwa,t1_hakzdg7,1630764627.0,False
pcq1om,"I work in a startup, barely feel any need for this. Not for startups for sure.",haklev9,t3_pcq1om,1630080434.0,False
pcq1om,"Sounds like a fun data analytics project.  Fwiw, GitLab has some analytics built in.  I usually take git metrics for a grain of salt, but maybe larger teams or organizations could benefit from this? Not really sure how practical it is tbh.",hakzocw,t3_pcq1om,1630086094.0,False
pcq1om,"I used to have a chrome extension that did a kinda 3D bar chart view of your commits on your GitHub page.  It was kinda cool to begin with,  but I found my self striving for more activity on it.  
That itself isn't a bad thing, but commits for the same of commits is pointless so I got rid of it. 

Maybe your other points would make some more meaningful info,  but for some personal use, I know when I'm being unproductive and I know when I'm doing good work.  I don't think a graph is going to change anything for me.",haljajc,t3_pcq1om,1630094190.0,False
pcq1om,You should read this book [https://pragprog.com/titles/atevol/software-design-x-rays/](https://pragprog.com/titles/atevol/software-design-x-rays/) it’s talks a lot about using git to visualize tech debt,halsm3w,t3_pcq1om,1630098121.0,False
pcq1om,Maybe which weekdays you make more pushes etc. I've been looking for something like this.,hakksfy,t3_pcq1om,1630080188.0,False
pcp8xu,"Problem solving is the first thing you should be thinking about. My first cs class in college was two weeks of problem solving, and then programming, makes you think differently about the programming.",halswnw,t3_pcp8xu,1630098250.0,False
pcp8xu,Do you have to know programming before being accepted or they teach you from the basics?,hamvwiy,t1_halswnw,1630116868.0,False
pcp8xu,Basic data structures and algorithms. CS =/= programming.,hakuzx7,t3_pcp8xu,1630084225.0,False
pcp8xu,That's a fair point. My university seems to think that programming ability is a good filter for unfit students though,hakv5hd,t1_hakuzx7,1630084287.0,True
pcp8xu,"I see the problem with rejecting students completely based purely on lack of programming ability but I do think its fair to consider it. Lots programming concepts and languages can be learned from the internet nowadays and there is no shortage of people with passion who can do a lot from self teaching but might not excel at other areas considered in the college application routine.

Sorry, the previous poorly written paragraph is from by a bitter dyslexic programmer who went through university applications only a few years ago.",hama1te,t1_hakv5hd,1630106036.0,False
pcp8xu,"While I agree 100% with saying that CS is not programming, I have a tough time imagining ""complete beginners"" who'd have no background in programming able to fully grasp or appreciate data structures and to a different extent algorithms. Or rather, of you're working more on what those concepts look like but have a classroom full of people who don't know how to voice, what does the class _do_ with those ideas?

In my opinion the complete beginners class absolutely needs to include introduction to coding. It is a gaining to lack anything else, however.",haluaz8,t1_hakuzx7,1630098857.0,False
pcp8xu,Harvard CS50.,hakh7lu,t3_pcp8xu,1630078783.0,False
pcp8xu,This,ham2gnl,t1_hakh7lu,1630102483.0,False
pcp8xu,"A gentle introduction to the mathematical constructs that appear in foundational computing theory (e.g. sets, functions, automata) interspersed with illustrations of how they relate to decision making processes and machinery.",hal36wg,t3_pcp8xu,1630087512.0,False
pcp8xu,"the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)",hali7kb,t3_pcp8xu,1630093743.0,False
pcp8xu,YouTube has university level lectures posted from institutions like MIT.  The full course is recorded and online.,ham5xxu,t3_pcp8xu,1630104098.0,False
pcp8xu,"Honestly, programming 1 at my college was perfect, since I had no prior experience (took it as an elective before switching majors). No prior knowledge, no data structures, no algorithms, just Python and solving progressively harder problems with an enthusiastic and helpful professor.
Then slowly you peeled back the curtain of what your code was actually doing inside the computer. That’s what peaks interest in CS, at least for me.",hamh5kq,t3_pcp8xu,1630109500.0,False
pcp8xu,Cs50 from Harvard. Prof has awesome energy and it is a fun class.,hamk63i,t3_pcp8xu,1630110969.0,False
pcp8xu,"Understanding how to follow steps and describe steps. 

Like the video of the guy that asks his kids to write down the steps to make a peanut butter and jelly sandwich. 

Most people don’t know how to think in logical, concise terms. Vagueness is a pervasive problem with the layman in my experience so it needs to be rooted out and gotten rid of.",hamqb9f,t3_pcp8xu,1630114046.0,False
pcp8xu,"Read, write and critique algorithms for very simple things. Use Python. Make students code every day and learn something new. Teach the basics up to recursion.

A second semester course would go more in depth on compilers and source code, libraries and new languages.",hbex4hw,t3_pcp8xu,1630648105.0,False
pcng3a,"Technically you have to analyze the algorithm and determine what the Big O notation would be based on your implementation,

https://www.amazon.com/Introduction-Analysis-Algorithms-2nd/dp/032190575X/ref=asc_df_032190575X/?tag=hyprod-20&linkCode=df0&hvadid=312114711253&hvpos=&hvnetw=g&hvrand=7190859065682197009&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9033597&hvtargid=pla-487655701807&psc=1",hakl5ng,t3_pcng3a,1630080332.0,False
pcng3a,"Generally in cases like that we’d just look at the performance on large data sets, because on a small dataset the runtime is trivial and not too important to improve. Then you can simply to the fastest growing term, sounds like O(n log n) in your case.",hazdg0s,t3_pcng3a,1630360677.0,False
pcjfym,"1) The number of salespeople who use the term AI is too damn high.

2) I always go back to that awesome tweet:
“ML is written in Python, AI is written in PowerPoint”

3) Also, there’s a YouTuber called AvE who’s a classical engineer with some pretty unique perspectives on what AI is. One thing he said that stuck with me is that a corporation/large organization/business entity operates like an analog AI trying to maximize profit and we’ve been doing that for years. All the forms, protocols, business rules, meeting schedules and employees are all part of jimmy-rigged neural network of decision-making criteria for profit maximization. Something like that.

https://youtu.be/GsxLF-Db7uk",haj8s9y,t3_pcjfym,1630054497.0,False
pcjfym,The Media idea that the goal/future of AI is to build an artificial human with feelings and human behavior is ridiculous - why would anyone want to spent billions/millions when one could just grab a bugless human worker of the street?,hajxea4,t3_pcjfym,1630070506.0,False
pcjfym,"This is an excellent debate path. 

I found the video I mentioned in my comment:

https://youtu.be/uCB1xbkn1Ps

Near the end he makes a good point that relates directly back to your argument about bug-less humans vs AI. Basically that the ability to make human error through free will is our bug.

I agree with you though, popular media misses the point a 99% of the time.",hakeeco,t1_hajxea4,1630077652.0,False
pcezlq,"ELI5 version. 

When computers are determining if something is a 1 or 0 they use thresholds. Anything above .5 is 1, anything less than or below .5 is a 0. Therefore, there is no difference between something that is .97 or 1.

That’s really all it’s saying.",haij1oh,t3_pcezlq,1630036272.0,False
pcezlq,"Thank you, now I understand. So the notion of threshold is the key.",haijf2u,t1_haij1oh,1630036473.0,True
pcezlq,"Yes, because in a computer 1s and 0s are represented by voltages. For example processors using CMOS technology might allow 3.5-5.0 volts to represent a 1 and 0-1.5 volts to represent a 0. This gives you robustness to detect what bit value is being indicated with the voltage.",hainz4b,t1_haijf2u,1630038932.0,False
pcezlq,"Pretty good explanation (though the actual threshold will depend on the technology used - CMOS vs TTL, etc.)

The text quoted in the OP is really too wordy to understand.",hbjbcr2,t1_haij1oh,1630732084.0,False
pcesjr,"GPU is a number crunching machine that's it, no more and no less.

The GPU will make lots and lots of calculations in parallel  allowing faster operations like positional vector transformations, vectorial normalization and all that wobbly physics interaction",hain82o,t3_pcesjr,1630038514.0,False
pcesjr,The GPU processes vector calculation! So the CPU gives the GPU input data and certain operation and the GPU gives processed data to the computer monitor. Now it makes a lot more sense. Thank you.,hainv0o,t1_hain82o,1630038874.0,True
pcesjr,"Not necessarily to the monitor, it's usually called graphic processing unit because graphics use a lot of vectors but you can use it for anything like parallel training a neutral network, mining Bitcoin, brute force a password and of course process the math part of the graphics",haio86x,t1_hainv0o,1630039071.0,False
pcesjr,That's a good point! I might need good GPU if I want to study deep-learning.,haiouzq,t1_haio86x,1630039425.0,True
pcesjr,"I think you are jumping the gun a bit. GPUs are like CPUs. They have their own clock speed and special memory (caches). They do what they're told to do with numbers. The difference is GPUs have a lot of cores, which allows for a lot of math in the same amount of time a CPU would. Like @solrak97 mentioned, having a high amount of cores also increases parallelization, where instructions run at the same time at the same pace. 

GPU performance is sometimes measured in FLOPS, or floating point operations per second. GPUs are designed to be good with floating point numbers.

And also worth mentioning, the CPU does have an integrated graphics hardware in most architectures, but obviously it wont be as fast as a GPU.",hbexx5y,t3_pcesjr,1630648651.0,False
pcesjr,"Thanks for the details! I learned the CPU only by very simple model which is a 8 bit scott CPU.

>GPUs are designed to be good with floating point numbers.

And this point is really interesting.",hbnvqn0,t1_hbexx5y,1630826825.0,True
pcesjr,"SIMD operations. Single Instruction Multiple Data. so if you have a lot of the same thing to do then a gpu is going to chew through it. Drawing 10 bazillion trillion million triangles a second to you can play csgo is easy for a gpu because each ""core"" get to draw one triangle and they all finish at the same time",hc5qvqz,t3_pcesjr,1631175535.0,False
pc0vvm,"If you like this, take a look at NAND2TETRIS. Its hands down the best resource I've found for grokking CPU architecture. You build a computer from nand gates, then write all the software for it, then write and play tetris.",hafofam,t3_pc0vvm,1629991876.0,False
pc0vvm,I looked into that ages ago but never went ahead with it. Might give that a shot in the near future!,hafosyc,t1_hafofam,1629992030.0,True
pc0vvm,This is super fascinating. Thanks for posting,hafu3hq,t1_hafofam,1629994136.0,False
pc0vvm,"I second this! It gave me great insights into how a computer works, and de-mystified the world of embedded programming enough to jump-start working in that area.",hairh27,t1_hafofam,1630040949.0,False
pc0vvm,"Ah the flashbacks of computer architecture, I loved that class but was hard af",hafng8b,t3_pc0vvm,1629991477.0,False
pc0vvm,I've always wanted to demystify how computers worked. I'm not a compsci student so YT and hands on stuff was the way to go for me.,hafnrlq,t1_hafng8b,1629991608.0,True
pc0vvm,"Read ""Code the hidden language of computers"" it's insanely good and nevermind that it's written in 1990 or smth.
No knowledge needed beforehand. Gives a good overlook how computers work with a bit of history too",hahd3d5,t1_hafnrlq,1630016346.0,False
pc0vvm,Thanks for the suggestion!,hahimnc,t1_hahd3d5,1630018837.0,True
pc0vvm,"You're a decade off of the publication date (1999) but the suggestion is a bullseye hit: that book is outstanding.

At one point I was considering trying to get a university teaching position, and had dreams of building a freshman seminar course around that book.",hakbq90,t1_hahd3d5,1630076571.0,False
pc0vvm,"That's great! As long as you learn something new and interesting every way of getting knowledge is a valid one
I love the project btw",hafov4o,t1_hafnrlq,1629992055.0,False
pc0vvm,My thoughts exactly. Thank you!,hafp3hc,t1_hafov4o,1629992148.0,True
pc0vvm,"I was inspired by Ben Eater's videos on making a breadboard CPU from scratch and so in order to learn more about how CPUs work I decided to emulate one in software. Next step, writing my own assembler for it.

It's a fun and educational experience and I recommend it to anyone wanting to learn more about how hardware works. Also, definitely check out Ben Eater's YT channel. It's a goldmine.",hafhc26,t3_pc0vvm,1629988959.0,True
pc0vvm,This is awesome! Will be checking out Ben Eaters YT so I can do this too.,hafkss7,t1_hafhc26,1629990381.0,False
pc0vvm,"Ben is awesome. He doesn't quite show you how to do it in software, but following along his series really does put things into perspective. You'll love it for sure.",hafm3vb,t1_hafkss7,1629990924.0,True
pc0vvm,Thanks! Appreciate the info!,hagnimw,t1_hafm3vb,1630005772.0,False
pc0vvm,He shows you how to do it in hardware!!,hakk6g6,t1_hafm3vb,1630079947.0,False
pc0vvm,This is awesome! What is your work background and educational background OP? It seems you enjoy really low-level stuff like me.,hag4ili,t1_hafhc26,1629998203.0,False
pc0vvm,"I studied graphic design and currently work in marketing. Always enjoyed programming and computers in general though, so I code pretty much daily. Low level stuff used to be close to magic to me and so extremely fascinating!",hahijn6,t1_hag4ili,1630018799.0,True
pc0vvm,"""I used the CPU to emulate the CPU""",hahh4hs,t3_pc0vvm,1630018153.0,False
pc0vvm,Exactly!,hahiy9y,t1_hahh4hs,1630018983.0,True
pc0vvm,I just wrote an assembler for my custom risc CPU but now I wanna try making a software sim version of the CPU.,hafoi82,t3_pc0vvm,1629991909.0,False
pc0vvm,"Do it, it's a lot of fun. I want to build one out for real at some point too. But next step for me is an assembler for my sim.",hafoy9w,t1_hafoi82,1629992089.0,True
pc0vvm,This is literally the project we had to do for our Systems Software class in undergrad. And you're doing it for fun!,hagqy46,t1_hafoy9w,1630007146.0,False
pc0vvm,I should have done compsci instead of graphic design!,hahirbv,t1_hagqy46,1630018895.0,True
pc0vvm,I remember doing this in Systems Software with PM0.,hagqubs,t3_pc0vvm,1630007104.0,False
pc0vvm,Sounds like fun,hahisvp,t1_hagqubs,1630018915.0,True
pc0vvm,Awesome! Which architecture are you emulating?,hahd185,t3_pc0vvm,1630016320.0,False
pc0vvm,"None in particular, more of a software implementation of the CPU Ben designed with a few changes here and there.",hahiwxf,t1_hahd185,1630018966.0,True
pc0vvm,I find this hilarious. A CPU emulating a CPU. Amazing,hajuqbb,t3_pc0vvm,1630069248.0,False
pc0vvm,Valgrind,hakk9os,t1_hajuqbb,1630079981.0,False
pc0vvm,"Try emulating the MOS6502, it’s the basis for the Apple 2, Commodore and the NES.",hakqhma,t3_pc0vvm,1630082446.0,False
pc0vvm,Yeah always wanted to emulate the 6502.,hanrzli,t1_hakqhma,1630138597.0,True
pc0vvm,Take a look at [vemips](https://github.com/ameisen/vemips).,haikco3,t3_pc0vvm,1630036969.0,False
pc0vvm,"If you want something really advanced, consider using SimpleScalar or GEM5.",hajfrzv,t3_pc0vvm,1630060290.0,False
pc0vvm,Nice debuggee,haji5xp,t3_pc0vvm,1630062009.0,False
pc0vvm,What programming language was this made in c#?,hb8yrwu,t3_pc0vvm,1630538209.0,False
pc0vvm,C++,hb96q7a,t1_hb8yrwu,1630542190.0,True
pby682,"Start from first principles. 

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)",hafi2qt,t3_pby682,1629989261.0,False
pby682,"For 3 and 4, do a deeper research for every topic brought up in the course. This is probably implicit, but saying it wont hurt",hahqkol,t1_hafi2qt,1630022538.0,False
pby682,"I think the biggest thing that always helped me was implementing what was taught in code. The lower the level of code the better. For example, I had no idea how linked lists really worked until I made a linked list in C. 

If you’re trying to learn a programing language for the first time, a good teacher is very helpful. This doesn’t have to be your official teacher. Get a tutor or a friend that is able to help you. Then write code that uses the new concepts you’re learning. The. Debug the code. Expect to spend a massive amount of time debugging code, cause that’s how life works at every level.",haf32ws,t3_pby682,1629982591.0,False
pby682,"First thing you need to realize is that everybody is pretty bad when starting out in computer science, not because they are dumb or untalented, it is because computer science requires a lot of “thinking outside the box” which is nothing like everyday thinking and reasoning.

The key to getting good at it is practice; solve similar problems that apply the new concepts you have learned and never be afraid to write code and see it go horribly wrong - this is all part of the learning process.

Next, always ask questions not just for the why but for the how.

You’ll get there, be patient with yourself.",haf5gl7,t3_pby682,1629983749.0,False
pby682,"Since you're just starting out, It'd be nice if you could develop some habits

1. Pick a language ( whichever is in your syllabus ), get some good practice with it.

2.  Learn by doing, whatever you're learning, do it on the computer once by yourself.

3. DON'T touch the keyboard until you've figured out the 'how' of whatever you're going to do on paper.

This is all assuming you want to build a career in CS :)",hafapmz,t3_pby682,1629986148.0,False
pby682,"You'd need to share more information for anyone to be able to offer you real help:

1. What's your current level? HS? College? 
2. Are you taking a class? Is it your first class, introduction? 
3. What's the reason you're not doing well? Unfamiliar with a programming language? Unsure of how to solve a problem yourself?",haezjkx,t3_pby682,1629980749.0,False
pby682,Im in highschool,haf4rjk,t1_haezjkx,1629983410.0,True
pby682,"Have you talked with the teacher about it? I'm a high school CS teacher too, so I can tell you if he/she is a real teacher they'd only want to help. Never be embarrassed that you're doing badly or anxious about asking for help, all we teachers really want to do is help our students out.

Honestly whenever a kid comes to my extra help session I always feel jazzed by it. Find out how they can help you, get some one-on-one time to work with them, help them figure out where you've gotten lost.

Teachers love seeing that extra effort. I can honestly share that as a teacher over the years, the students I recall fondly were the ones that came to me for help. The superstars who aced everything without much effort are the ones you don't really remember that well because you didn't really get to work with them as much.",haf7tor,t1_haf4rjk,1629984855.0,False
pby682,Well I see reading comprehension and responses is subpar,hafhxlu,t1_haf4rjk,1629989202.0,False
pby682,That’s great. There’s no need to insult someone asking for help.,hah6jbv,t1_hafhxlu,1630013515.0,False
pby682,"It's going to be about weighting I think. My A level course (AP equivalent I think) was about 60% code, 40% theory. I was already good at the theory, so I put more time into the code to push my grade up. Think of how your course is weighted and where you're weakest, then go over those areas. If you know what you're struggling with I can probably point you towards materials to help",hai0ato,t1_haf4rjk,1630027141.0,False
pby682,"don't be afraid of asking stupid questions. 

always research a bit before asking anything else the community goes bully mode.

learn a programming language thoroughly. give it it's own sweet time. you can practice the language from these sources:

Hackerrank: https://hackerrank.com  
leetcode: https://leetcode.com  
DataCamp: https://datacamp.com \[Only for python and R\]  
codewars: https://codewars.com  
checkio: https://checkio.org/ \[Only for TypeScript and Python\]  
codingbat: https://codingbat.com/ \[Only for Java and Python\]  
kikodo: https://www.kikodo.io/ \[Only for Python\]  
codesignal: https://codesignal.com/  
practicepython: https://www.practicepython.org/ \[Only for python\]

&#x200B;

get yourself a smart CS community to stay updated on your particular field of interest in CS. keep exploring and never be too afraid.",hafgd7e,t3_pby682,1629988558.0,False
pby682,Don't try to understand or excel at everything.  Find a topic you love and follow through. This way you'll learn necessary knowledge on the way. For me it was computer graphics and image processing.,hafbp3s,t3_pby682,1629986576.0,False
pby682,"Spend a lot of time on basic subjects like OS, DS and start implementing while learning.",hafeoh4,t3_pby682,1629987860.0,False
pby682,"Basic tips, well, hard to say without knowing what's causing you troubles.

I'd say ask questions to your professor, look for resources/courses in the internet.

But also, not everyone is going to be good at CS, just like maths, learning languages, etc...   
If despite all your efforts you still don't improve, you might want to consider the fact that it's not a good fit for you.",hafewaj,t3_pby682,1629987951.0,False
pby682,"Make full projects, not just little school problems",hag6sbf,t3_pby682,1629999079.0,False
pby682,"When u study a lot and still feel like u r just scratching the surface, don’t give up, most the concepts are built over many years ,just keep practicing and u will get it eventually",hahg7fd,t3_pby682,1630017738.0,False
pby682,Happy cake day my dude!,hajh9wy,t1_hahg7fd,1630061384.0,False
pby682,Thx my dude appreciated,halocqk,t1_hajh9wy,1630096304.0,False
pby682,"* Don't panic.

* You have to slow down if you want to go fast.

* You can't get to where you're going until you understand where you're at.

* You're building a house. Do you want a foundation of sand, or cement? 

* Form a study group, 4-6 people (not more), 4 hours a day, every day. No chit chat, no phones, no snacks - only water, no distractions. The only conversation that should be happening is about the lesson material in front of you. Kick out and replace members who can't comply. You are all there to hold each other accountable, just by being there. Read. Get homework done. Write a paper. Whatever it is, you don't have to all do the same thing, and you don't have to all do it together. Help each other when asked, and take turns helping. Have a whiteboard handy.

* Sit in the front of the class. The front of the class has the best view of the material and the teacher, and controls the pace of the class. Those who sit up front are the ones the teacher is paying attention to. When you're sitting there with a corkscrew face going, ""Huh?"", that feedback informs the professor. Yes, he may call on you more often. GOOD. No one cares about how you answer the question, no one can be bothered to judge you. The teacher isn't looking for you to get it right - you're not being graded for answering the question, he's looking for feedback to see if he's doing his job. If you can't answer the question, GIVE HIM THAT FEEDBACK.

* Learn by teaching. If you can't explain it to someone else, then you don't understand it yourself. Explain your subject matter to your mother until she understands it. Help your study group by going over the lecture material at the white board - emulate a professor.

* Make friends with those who sit in the front of the class. They're the smartest kids there, the most eager, the most ambitious. Make your study group with them. The cool kids in the back are losers who will just drag you down. No cool kids. It's not about cool factor. This isn't high school.

* Talk to your TA's and professors after every class. You guys should be on a first name basis. Go to office hours, ask questions about the material you're struggling with. Come with something that showed you tried.

* Schedule your day. Get a daily planner. Stick to the schedule. You don't need breaks, you need sleep. Otherwise, keep studying. Breaks and snacking turn into escapism. This is why you need a schedule. Schedule meals and breaks so that you're accountable and they're visible. If you take breaks arbitrarily, your time will fly and you will have nothing to show for it.

* Journal, or keep a log. You need notes about your progress. Figure out a format that makes sense for you. This is - in part, another accountability thing. You can also use it to show your professor a record of your effort and your progress. When he sees it, call them notes. Journaling is also a medium in which you think to yourself, it makes you commit to the abstract notions in your mind - forcing them to become concrete by writing them down. Write about what you're learning about and what you think about it. Admit you don't understand something and ponder it in your journal. Mix in some bullet points and plans of action. This helps structure and discipline your mind. Don't just pick a journaling time, keep an open document and add to it throughout the day. Maybe make it a major part of your breaks.

* Cry a little bit. This is some of the hardest you will work in your life. Expect to cry a little bit. Then accept that this is all part of the process, no one is dying, it's just a grade, you can retake courses - and no one will judge you for it, and then get back on that pony.

* If you're already failing, you need to schedule a talk with your professor about it as soon as you can get him. You need to tell him you realize where you are and need help understanding where you might possibly end up. Accept now that you might just have to retake a course. Don't abandon your efforts now - continue to strive, because your work now will make retaking the course that much easier, and you will really appreciate it then.",hakb8s5,t3_pby682,1630076372.0,False
pbpexo,"Damn, good job. I think i'll read it

Suggest you put it on ResearchGate, just cus i like it there",haeeaw1,t3_pbpexo,1629964757.0,False
pbm9pa,"I'll try to lead you into finding a better answer without actually giving it to you.

Think about these questions, considering N := max{ |A|, |B| }:

1. Can you find the heights of each of the trees in less than O(N)? (remember that you have AVL trees).
2. With this information, can you ""find"" a spot on one of the trees where you could ""hang"" the other without violating the AVL properties?
3. How can you hang that tree without having somewhere to attach it? How can you ""place"" a node in that spot to be able to attach the tree? (this one is tricky to state without giving out the gold...)

This way, if I'm not mistaken, you'll find a O(log N) algorithm (remember who N is).

&#x200B;

PS: If you're not able to understand why any of these questions are important (sorry for the wording of the last one), tell me and I'll try to help a bit more.",had4w8g,t3_pbm9pa,1629938432.0,False
pbm9pa,"1. Probably not, because there is tolerance of 1 in difference in sub trees. From root I can not tell which side is deeper and this problem inducts throw whole tree.
2. If some subtree is smaller than second one. I can put two extra nodes there without violation of AVL definition.
3. Probably not without a lot of rotations.

grateful for your answer but I did not made much progress.",hb26dyu,t1_had4w8g,1630419613.0,True
pbm9pa,"1. The difference is of one, but you store it, together with the sign, don't you?
2. If you can calculate the height, you should be able to ind a subtree with the same height as the smaller one.
3. You should think about some node already in the tree you can take advantage of.

Let me know of any progress or doubts.",hb2d7yr,t1_hb26dyu,1630422443.0,False
pbm9pa,"1. I still do not see how you can find tree's higth in log(n) time. The only information node has is what is left and right offspring. Getting the {-1,0,1} data would last O(n) time. I feel stupid because I can not see how to get difference in size of subtrees from this information.",hb2eeau,t1_hb2d7yr,1630422925.0,True
pbm9pa,"Don't feel stupid. maybe you're just not with the right mind yet.

You see, the recursive definition of height is given by: height(n) := max{ height(n.left); height(N.right) } (base cases differ).

The, if you know the balance of a node is +1, for example, you know that the subtree that contributes to the result of max{ . } in that case is the one on the right (I think that balance(n) := | height(n.right) - height(n.left)|, but it may be the other way around. You should check it).

Thus, it has to be the case that if you go down on that path, and do this repeatedly, you must be going through a longest path in the tree, whose size gives you the height.",hb2ipm6,t1_hb2eeau,1630424670.0,False
pbm9pa,"Reading your answer again made me think. Don't you have access to the nodes' balances? Does the exercise say you cannot count on having that information at hand?

Because if you don't, I believe it is impossible to achieve the desired bound...

The availability of this information is crucial to, for example, be able to guarantee that the tree you end up with, whatever strategy you use, is indeed AVL.",hb2jhg3,t1_hb2eeau,1630424984.0,False
pbm9pa,"\> Because if you don't, I believe it is impossible to achieve the desired bound...

Literally that is what I was thinking the whole time and the reason why I asked this question. I have wasted so much time on this that from now on I will assume that balance info is here so I can move.

So ok,  Now I can find heights of these trees. I think that I want to hang smaller tree on bigger one. Is it possible to find place to hang it without need of any rotations and the result tree would be AVL tree ?",hb2ll9r,t1_hb2jhg3,1630425834.0,True
pbm9pa,"Sorry I didn't get to the root of your doubt before, but yeah, AVL trees with no balance info cannot be altered cheaply because you need to guarantee the AVL properties.

>Is it possible to find place to hang it without need of any rotations and the result tree would be AVL tree ?

Not necessarily without rotations, but definitely at most O(log n). You should just think about where to place the smaller one in order to cause the ""least trouble"" to the new parent. But, as maybe the node in the ""best place"" might have two children already, you'll need to extract some node from the tree and relocate it to aid with the merging.

But this relocation does not need to be done through rotations, although maybe you'll need some to fix its removal.

See if you can make some progress now.",hb2s95i,t1_hb2ll9r,1630428518.0,False
pbm9pa,"By intuition I would place smaller tree on leaf of bigger tree which is the closest to the root. But now there can be node with huge difference in subtree size.

Now I need to find out how to use fact that all items in one tree are after all items in other tree.",hb2uiby,t1_hb2s95i,1630429421.0,True
pbm9pa,"I don't think you can make it faster in the general case, but I don't have a proof. I have a feeling that you need to walk the entire tree in the worst case, which means you can't be faster than O(|A|+|B|)",hacs14t,t3_pbm9pa,1629932541.0,False
pbm9pa,"Sorry, I forgot important detail that All elements of tree A are before All elements of tree B.",hacspcz,t1_hacs14t,1629932854.0,True
pbm9pa,Ah. So you can just take the smaller AVL tree and attach it as the leftmost child in the greater one? Which should work in O(lg |B|) if B is balanced?,hacw9rv,t1_hacspcz,1629934492.0,False
pbm9pa,The problem is that he needs to guarantee that the tree he obtains is also AVL.,had52g1,t1_hacw9rv,1629938509.0,False
pbiva7,"* Python - Used to automate tasks and to create your own scripts so you can run a bunch of programs with the variables you want so you don't need to spend 20 minutes researching every time you want to run a scan.
* Networking basics - You are wanting to get into a network, you should know the basics of how they are setup so you can traverse.
* Writing Skills - You will be writing reports, some may be 2-5 pages, and others may be 25-100 pages. The worse your grammar, the less likely you will be in the field for an extended period of time.

You should also research which sub-set you are wanting to get into, like Governance Risk and Compliance (GRC), Cloud Security, Penetration Testing (Red Team), Defense (Blue Team), Hybrid testing (Purple Team), Sales, Security Engineering, etc..",hacb6i7,t3_pbiva7,1629925307.0,False
pbiva7,Thank you!,hagl42i,t1_hacb6i7,1630004805.0,True
pbiva7,"You're welcome!!

Also, little advice, don't think you need to memorize and retain every little piece of information. Utilize apps like OneNote or Evernote to keep detailed notes on tasks or skills, use Github for keeping track of your scripts and/or programs you create. Trying to retain everything leads to burnout since your brain just doesn't get the chance to dump some information to allow for new information to come in.",hak5xl8,t1_hagl42i,1630074203.0,False
pbiva7,"I’m not sure what you mean by before getting into cyber security. Learning the foundations of a topic is still learning that topic. When you learn basic networking in python, do it by building cyber tools you might need. Same with every other basic skill. It’s all cyber security if you look at it with a cyber focus. 

That said, what to learn as the fundamentals really varies over what you want to do with cyber. I’m best advice would be pick the thing you want to hack/secure and learn how it works. 

For example if you want to work on web apps learn how http/JavaScript and server/client work.  If you’re focusing on operating systems, learn some low level C and asm and how computers work. I could go on but hopefully you get the idea.",haczaim,t3_pbiva7,1629935886.0,False
pbiva7,Got it! Thank you very much!,haglac8,t1_haczaim,1630004875.0,True
pbiva7,"Great advice above, add to that:

* EQ/empathy, learning to work with people, etc. 

Nobody wants to work with “that jerk”, and definitely not with “that jerk from Security”!",haeun5y,t3_pbiva7,1629977902.0,False
pbew6d,"A long while ago I solved the problem of a fast and memory efficient trie in Java by using just three arrays to represent the trie and no objects or hash tables at all. I dug out the comment I wrote to explain my own code for you:

    /*
     * A node is indexed by an int, and it's properties stored in three arrays, 
     * nodeFirstChild[], nodeNextSibling[] and nodeLabel[]. An index of 0 represents
     * the null index.
     * 
     * The children of a node are represented by a linked list - the head of a node n's
     * children is nodeFirstChild[n], and then the next member of the list is accessed
     * from nodeNextSibing[].
     * 
     * nodeLabel[] is the letter at that node. A letter of '-' is used as an end of 
     * marker.
     */
 
    int[] nodeFirstChild;
    int[] nodeNextSibling;
    char[] nodeLabel;
    int rootNode = 1;

So to search it at a node you run along the linked list of siblings looking for a match, referencing the label array. If you match, use the first array to go down a level from that node. 

I would recommend giving this kind of design a go. Imagine you are writing in C and don’t have fancy objects to help. I was quite shocked how much faster and less memory this used in Java than an object based solution.",hac7mdp,t3_pbew6d,1629923873.0,False
pbew6d,"I appreciate the fact that you dug up your old comment for me. But it seems I going to need a little more help from you to understand this.
Could you please tell me what the following two rules translate to with your approach?
https://i.imgur.com/wIaRNNm.png",haec3x8,t1_hac7mdp,1629962959.0,True
pbew6d,"Ok so first thing to note I didn't realise this was for production before, figured it was for a university project. So given that I would try to do it in the database if you can, and only consider this if that still isn't fast enough. 

Anyway this is the basic idea. The trie for your example would look conceptually like this. I missed out one column of stars to true and keep things easy to follow:-

          ┌──────┐
          │64-200│
        ┌─┴──────┴┐
        │         │
      ┌─▼─┐     ┌─▼─┐
      │436│     │438│
      └─┬─┘     └─┬─┘
        │         │
        │         │
       ┌▼┐       ┌▼┐
       │*│       │*│
       └┬┘       └┬┘
        │         │
        │         │
    ┌───▼┐     ┌──▼─┐
    │64-A│     │64-A│
    └────┘     └────┘

That would be converted into a representation that looks like this:

                 ┌─────────────┐
                 │   Node 1    │
                 │             │
                 │ ┌─────────┐ │
                 │ │Sibling 1│ │                 
                 │ └───┬─────┘ │
                 │     │       │                 
                 └─────┼───────┘
                       │                         
                       │nextChild
                       │                         
      ┌────────────────▼───────────────────┐
      │                                    │     
      │               Node 2               │
      │                                    │     
      │ ┌─────────┐ nextSibling┌─────────┐ │
      │ │Sibling 2├────────────►Sibling 3│ │     
      │ └────┬────┘            └────┬────┘ │
      │      │                      │      │     
      └──────┼──────────────────────┼──────┘
             │                      │
             │nextChild             │nextChild
             │                      │
             │                      │            
      ┌──────▼──────┐        ┌──────▼──────┐
      │             │        │             │     
      │   Node 3    │        │   Node 5    │     
      │             │        │             │     
      │ ┌─────────┐ │        │ ┌─────────┐ │     
      │ │Sibling 4│ │        │ │Sibling 6│ │     
      │ └────┬────┘ │        │ └─────────┘ │
      │      │      │        │             │
      └──────┼──────┘        └──────┬──────┘
             │                      │
             │nextChild             │nextChild
             │                      │
             │                      │
      ┌──────▼──────┐        ┌──────▼──────┐
      │             │        │             │
      │   Node 4    │        │   Node 6    │
      │             │        │             │
      │ ┌─────────┐ │        │ ┌─────────┐ │
      │ │Sibling 5│ │        │ │Sibling 7│ │
      │ └─────────┘ │        │ └─────────┘ │
      │             │        │             │
      └─────────────┘        └─────────────┘

So the array values would be as in the tables below. You have a pointer that tells you the next sibling in a linked list to search for whether the current term matches. And then if you find a match you use the other array to tell you what node to go to next and start with its first sibling. You look up the values with a double pointer, so that each string only needs to be in memory once. And finally you keep track of accept states so that you don't accidentally match a substring of what should match. Every lookup in the array is of course super fast, no hash table needed.

    Index  nextChild   nextSibling   valuePointer   doesAccept
        1          2             0              1     false
        2          3             3              2     false
        3          5             0              3     false
        4          4             0              4     false
        5          0             0              5     true
        6          6             0              4     false
        7          0             0              5     true
        
    Values
         1   64-200
         2   436
         3   438
         4   *
         5   64-A

If you were being really fancy you could collapse matching branches. But that makes maintaining the tree much harder.

Another consideration is how big to make your arrays. The classical answer is to double them in size every time they run out of room, but at a certain size you might not want to be doing that. But finding an approach that works for the kind of size of data you are talking about shouldn't be hard.

Does this make sense?",haf63th,t1_haec3x8,1629984054.0,False
pbew6d,I could try this out. Thank you for take out time for this. I appreciate it.,hagljga,t1_haf63th,1630004977.0,True
pbew6d,No problem. Let me know if it works out for you.,haki4sr,t1_hagljga,1630079150.0,False
pbew6d,Are you sure that storing this info in a trie is the best option? Why not encapsulate this information in a class and store the rows as objects?,habmfjo,t3_pbew6d,1629915454.0,False
pbew6d,"Because I want fast lookups. Please see this [https://www.reddit.com/r/datascience/comments/p8d7ev/how\_to\_validate\_5000\_line\_items\_against\_2\_million/](https://www.reddit.com/r/datascience/comments/p8d7ev/how_to_validate_5000_line_items_against_2_million/)  
For more context.",habmm5m,t1_habmfjo,1629915527.0,True
pbew6d,"I agree that dictionary overhead may be a problem.  Hash tables have a fair bit of overhead -- I think that on creation they allocate enough memory for 10-ish elements -- and if most nodes only have one or two subnodes, that's a lot of space.

If instead you just stored an array of element/subnode pairs, you would be optimizing for the ""small number of subnode"" case which might make up the bulk of your tree.  Lookup speed wouldn't be too adversely affected, or may even be faster because of better memory locality.

Also if a lot of your nodes have only a single subnode, you could look into a [radix tree](https://en.wikipedia.org/wiki/Radix_tree) to merge nodes together.",habr7kt,t3_pbew6d,1629917375.0,False
pbew6d,"Are you saying crate Dictionary<string,string\[\]>() ? Will each array contain a row? Cell as items? Then what will be the key?",habrmzc,t1_habr7kt,1629917542.0,True
pbew6d,"I'd say instead of the Dictionary<string, string\[\]>, use Tuple<string, string\[\]>\[\] instead.  And instead of calling .get(key) just linear search through the array of key-value pairs and find one where the first element matches your key.   Sure it is an O(N) search instead of an O(1) hashtable lookup, but if N is small, it could still be cheaper than computing a hashcode and accessing more memory.

As for ""\*"" I don't think it would be too hard to have a special node type in your trie that just passes on to its child node unconditionally.",hac03p2,t1_habrmzc,1629920909.0,False
pbew6d,Also I dont think I can create a radix tree because of the \*,habv47o,t1_habr7kt,1629918932.0,True
pbeklm,"Yes. Initially interpreters and compilers for *very* simple language (think [Brainfuck](https://en.wikipedia.org/wiki/Brainfuck)) and then later for smaller 'toy'-languages of my own making, partly because I was interested in how compilers/interpreters work, and partly because I thought I might have a few ideas of how to create programs differently. 

Take a look at r/ProgrammingLanguages if you're interested in programming language design, and the implementation of interpreters/compilers. Also take a look at the [https://craftinginterpreters.com/](https://craftinginterpreters.com/) website. It uses Java IIRC to build an interpreter for a simple imperative language, but it could just as easily be done in Python/C#/JS etc..

Peter Norvig also has a Lisp interpreter on [his website](https://norvig.com/lispy.html) in Python (less than 150 lines of code..)",hab5s58,t3_pbeklm,1629908791.0,False
pbeklm,"\+1 on the lisp interpreter. It is easy to cut your teeth on LISP because of the sforms. Also take a look at some of the internals of a transpiled language like TypeScript:

[https://basarat.gitbook.io/typescript/overview](https://basarat.gitbook.io/typescript/overview)

if you play around with some simple programs to parse the transpilings it will really help you understand what is going on, without having to eat the whole elephant in one sitting.",hab9tvs,t1_hab5s58,1629910400.0,False
pbeklm,"Yep. Lisp is a good first project because it's super-easy to parse (whitespace and brackets, basically) and prefix notional means you don't have to get into Shunting Yard territory as you would if you were trying to parse '`(x + 5) * (2 + (3 -y))`'.

[Forth](https://en.wikipedia.org/wiki/Forth_(programming_language)) is also a good candidate for a first language project for the same two reasons.",habbe4n,t1_hab9tvs,1629911020.0,False
pbeklm,"Had to build a toy language in my grad school compilers class called Tiger. Pretty basic, but we had to build the backend using a global registration allocation scheme using graph coloring. So just the act of doing both the front and backend of a compiler will teach you MANY CS algorithms and principles. I highly recommend the book Engineering a Compiler by Cooper and Torczon. Really good book for learning fixed point algorithms and how they apply to scanning parsing and other things.",hadaf9v,t3_pbeklm,1629940909.0,False
pbeklm,"I'll also add, I thought building the symbol table code from scratch was one of the best exercises to help understand lexical scoping and hash tables.",hadamx7,t1_hadaf9v,1629941006.0,False
pbeklm,"Did this as a college class. Built an interpreter rather than a compiler. Used BNF to describe the grammar, then used this to tokenize the raw code,  and then built the interpreter on top of this. 

For fun,  I played with a proof of concept where I built an addin for excel that would extract all the formulas in a worksheet in dependency order and then transpiles the formula into C#. It only supports numeric formula, but it was a fun time.",hadcp0q,t3_pbeklm,1629941931.0,False
pbeklm,"At college we build a C compiler using C. First using Bison for syntactic validation and Yalex (or some programming named like that) for lexical validation. I don't remember what we use for semantics. After this introduction, we wrote in C the lexical, syntactic and semantic processes for the C compiler. The optimization part were introduced as theory only. Unfortunately, I didn't wrote the semantic in C. In the time I was worried finishing others classes. I regret a little :/ but is definitely a very cool experience! I think it was the only class in college that really went deep on build programming using a lot of data structure in practice. I did some others classes that we build some algorithms or use some DS, but compilers opened my eye about how deeply we can go using some DS and algo (as recursions, graphs, trees, stack and automata) to build something useful.",had72zd,t3_pbeklm,1629939413.0,False
pbeklm,no,hc5qx6p,t3_pbeklm,1631175574.0,False
pbeklm,https://cs.lmu.edu/\~ray/notes/howtowriteacompiler/,hablpvu,t3_pbeklm,1629915164.0,False
pbeklm,"It says URL note found

Edit: Working link: https://cs.lmu.edu/~ray/notes/howtowriteacompiler/",hadt6qg,t1_hablpvu,1629950014.0,False
pbeklm,It's an uncompiled link,haem22n,t1_hadt6qg,1629971547.0,False
pbeklm,I would recommand using ISL.,haevci7,t3_pbeklm,1629978330.0,False
pbde2s,"""Deterministic state machines"" each block on the etherium chain was always going to be that. so a block is the like reverse hash of the previous block. That hash always existed we just used 10 bazillion watts of electricity to find it. 

""practically unbound"" means that although its deterministic the solution code be ""practically"" any number 

""singleton state"" no clue. youre on your own now",hc5r635,t3_pbde2s,1631175827.0,False
pb9mni,i passed it by watching Neso Academy on YouTube,habt2ih,t3_pb9mni,1629918113.0,False
pb9mni,Thanks 😭🙌🏻,hac3qem,t1_habt2ih,1629922339.0,False
pb9mni,Just wait till you get to dfas and cfgs. That stuff is actually really fun!,haf9j82,t3_pb9mni,1629985630.0,False
pb9mni,I have started the journey. Hope your words come true 👐🏼,haj2q13,t1_haf9j82,1630049256.0,False
pb9mni,Interesting. Must see it myself. Am curious.,hae0sh7,t3_pb9mni,1629954599.0,False
pb9mni,"step 1:

look up a picture of Alonzo Church's adorable smile

[https://upload.wikimedia.org/wikipedia/en/a/a6/Alonzo\_Church.jpg](https://upload.wikimedia.org/wikipedia/en/a/a6/Alonzo_Church.jpg)

next step:

watch weird animated videos of the halting problem until your brain melts

step 2: 

code autocomplete in 7 different ways including a prefix trie

step 3:

try to solve P vs NP with your newfound knowledge

&#x200B;

in all seriousness most resources are very boring. try reading the bible (""concrete mathematics a foundation for computer science"" second edition). It has funny comments in the margin to keep you entertained.",hc5rop8,t3_pb9mni,1631176346.0,False
pb14j9,"Alright, this is pretty neat. Have you considered making the DOT language used by [Graphiz](https://graphviz.org) one of the export formats?",ha903zc,t3_pb14j9,1629862199.0,False
pb14j9,Yes! I only just heard about that format the other day and now I've heard like 5 people tell me about it. Thanks for trying out the tool and let me know if you have other thoughts!,haamxz3,t1_ha903zc,1629901064.0,True
pb14j9,"Just added it!

Edit: For some reason the copy clipboard doesn't work if you don't have ""https"" in front, so it'll only work for this link: https://tranquil-oasis-04623.herokuapp.com/tools",haue59p,t1_ha903zc,1630267815.0,True
pb14j9,It would be nice to see the node numbers used for the adjaceny list / matrix on the nodes themselves.,ha91ahr,t3_pb14j9,1629862829.0,False
pb14j9,"That's an excellent point. Thanks for checking it out and let me know if you have other thoughts

Edit: just to add another thought, I've been thinking of making an ""annotate"" tool where you can click nodes/edges and it opens a little text box to type labels. Maybe it would default to being the node indices but then you can type any label you want.",haamn76,t1_ha91ahr,1629900935.0,True
pb14j9,Nice start. A tool to be able to move placed nodes would be helpful.,ha984s4,t3_pb14j9,1629866775.0,False
pb14j9,Thanks for checking it out! Other people have given similar feedback so that's definitely something I'll implement soon. Let me know if you have any other thoughts!,haamjfo,t1_ha984s4,1629900891.0,True
pb14j9,Just added it!,haue7jo,t1_ha984s4,1630267842.0,True
pb14j9,[deleted],haacue2,t3_pb14j9,1629896527.0,False
pb14j9,">https://github.com/dreampuf/GraphvizOnline

Thanks for checking it out! I've heard of Graphviz before and it does do a great job of visualizing a graph given the raw data. However I actually really enjoy making the graphical part and thinking about the UI/UX, so that's  why I've been doing that. 

There are other tools out there but many suffer from feature bloat. For this project I wanted to limit its scope: not visualizing raw graph data (graph drawing algorithms like in Graphviz are also not trivial to implement), not on heavy-duty analysis (there's lots of tools for that), but instead on a quick user friendly way to build a graph and export it for use. My inspiration for this came when I was trying to make test cases for a previous project and couldn't find anyway to do it except work it out on paper.",haam4d4,t1_haacue2,1629900711.0,True
pb14j9,yes plz,haa07kc,t3_pb14j9,1629889072.0,False
pb14j9,Thanks for checking it out. Let me know if you have any ideas for use cases or features to implement!,haan22k,t1_haa07kc,1629901111.0,True
pb14j9,noice,haa19h4,t3_pb14j9,1629889807.0,False
pb14j9,"https://csacademy.com/app/graph_editor/
Check this out. see it it helps",haaq7v9,t3_pb14j9,1629902435.0,False
pb14j9,"Haha, I intuitively knew how to draw a graph with your tool, thus I didn't need any docs anyway! Great work!",habcuom,t3_pb14j9,1629911606.0,False
pb14j9,I didn't inspect a lot but ui seems nice,hacklbk,t3_pb14j9,1629929262.0,False
paqy2k,"Interpreted languages are more portable, typically.

EDIT: I guess to answer your question more accurately, they were made to be more portable. A compiled language is compiled on the target machine and must be recompiled on each different machine. An interpreted language allows a middle ground I suppose where it’s not source code but it’s not processor-specific binary.

I’m also no expert. Could be some misinformation. You can use what I said and research a bit deeper on your own to figure it out better.",ha6stbw,t3_paqy2k,1629827670.0,False
paqy2k,"Some nuances. 

C is more portable as in, can be compiled for almost any processor. 

Interpreted languages are more convenient. If an interpreter exist for the target CPU architecture it's write once and run ""anywhere"".",ha98dgq,t1_ha6stbw,1629866924.0,False
paqy2k,"On top of the previous answers, interpreted languages have some pros and cons if we compare them to compiled languages.

Yes, an interpreter language might be slower but also easier to debug as you can track when, where and why it fails, also portability is a huge thing (java is in the middle as a compiled yet interpreted language and is widely used for portability) 

Also a fun fact on interpreted languages, I've seen people write code that modifies itself on runtime and it's pretty cool to see that kind of behavior, a difficult task on any compiled language",ha7ett2,t3_paqy2k,1629836370.0,False
paqy2k,"Originally? Lisp is usually cited as the first interpreted language - first Lisp interpreter written in 1958, first Lisp compiler 1962. This is because it's much easier to write a Lisp interpreter than compiler, because a Lisp program is a function that takes Lisp expressions as input and returns Lisp expressions as output.

Beyond that, one major motivation for computers in the 1950s and 1960s was memory. There was only enough memory to keep the source code and data at a time - no room for the object code. For the same reason, the home computers of the 1980s had a BASIC interpreter built in instead of a compiler for some language.",ha71vp9,t3_paqy2k,1629831218.0,False
paqy2k,What makes interpreted langs portable,ha72vmk,t1_ha71vp9,1629831602.0,True
paqy2k,"Because your program is text, so the hardware doesn't matter as long as you have the interpreter. Once a program has been compiled, it is converted to code that runs on a specific platform. To run on another platform, you need to recompile the code.

It's more complicated in reality, since some languages compile to a byte code which is then interpreted. And for interpreted languages to be portable, the target platform needs to have the interpreter installed. You can compile code to an executable that will run on a target that is too limited to even run the compiler.",ha8rqlx,t1_ha72vmk,1629858045.0,False
paqy2k,"Interpreted languages provide portability for software written in them, but that had absolutely nothing to do with why interpreted languages were produced. They provide portability because instead of porting every program in the language to a new machine, you instead port the interpreter -- a single program, and now you can run all the programs the interpreter is able to understand.

But, again, portability had nothing to do with it, and no one even expected interpreters to really exist. A lot of engineers referred to them as ""the dumbest possible way to calculate on a machine."" The interpreted languages existed on paper first, and then Steve Russell wrote an implementation of an evaluator for such a language, and folks were off to the races.

Interpreted languages were made because they provided abstraction consistent with the way mathematicians thought about things. Programming was very concrete and literal at the time. With interpreted languages you could reason about the application of recursive functions to perform computation -- exactly how a lot of mathematicians already thought in their own heads about programming. The inventors wanted to think about ""mathematical computation"" the same way they thought about mathematical physics, not about registers and switches and relays, but about applications of functions and operators.

The implementation really is stupid, in terms of being incredibly wasteful -- But it provided a playground where the best ideas in computation emerged, from garbage collection to continuations. Modern programming owes more to early interpreted languages than anyone could have guessed at the time.",ha8b9x1,t3_paqy2k,1629850434.0,False
paqy2k,"> instead of porting every program in the language to a new machine, 
Clarify that",ha8d7xa,t1_ha8b9x1,1629851326.0,True
paqy2k,"You can port Python.exe to a new type of computer you just made yourself in secret, and my Python source code could be evaluated by it without me knowing anything about how you constructed your computing machine.",ha8eqah,t1_ha8d7xa,1629852019.0,False
paqy2k,Thank u all,ha80pg4,t3_paqy2k,1629845623.0,True
paqy2k,"Interpreted languages may have risen from shell script programming. Often, I use interpreted languages to quickly automate something.",ha8b1ef,t3_paqy2k,1629850325.0,False
paqy2k,You wouldn't wanna write java / C for a simple utility script / data exploration projects,hab400p,t3_paqy2k,1629908074.0,False
paqy2k,"each of theme have pros and cons and it depens on the general usecase of that language

with a little search you would find this article:

https://www.google.com/amp/s/www.geeksforgeeks.org/compiler-vs-interpreter-2/amp/",ha6k57f,t3_paqy2k,1629824247.0,False
paqy2k,That doesn't answer my question Im asking why interpreted langs were made,ha6ny8g,t1_ha6k57f,1629825762.0,True
paqy2k,"Other than portability, with interpreted languages you get elasticity; read-eval-print-loops (repl), ability to alter parts of a running programs live, better visibility, traceability without using external debuggers (eg. simple built-in stacktraces). Ability to embed interpreter in your program and run immediately user supplied code without carrying compiler and other tools (like lua or even python).",ha74215,t1_ha6ny8g,1629832061.0,False
paqy2k,What makes interrupted langs portable,ha749ww,t1_ha74215,1629832148.0,True
paqy2k,you have to port only the interpreter - not all of the interpreted code. There's additional abstraction layer that moves the code further from hardware/environment - making it easier to write portable high-level code.,ha74pn6,t1_ha749ww,1629832322.0,False
paqy2k,I guess you’ve never made an interpreter or a compiler?,ha6j4ml,t3_paqy2k,1629823845.0,False
paqy2k,I may be wrong but I believe compiled languages can easily be ran on any computer or device while interpreted languages can run into problems depending on the architecture of the computer.,ha6oren,t3_paqy2k,1629826081.0,False
paqy2k,You are having it the wrong way around,ha6rsbb,t1_ha6oren,1629827272.0,False
paqy2k,Why interpreted languages were made instead of making just compiled languages,ha6uy8i,t3_paqy2k,1629828515.0,False
paqy2k,Whqt make interpreted language portables,ha72liy,t1_ha6uy8i,1629831493.0,True
paqy2k,What,ha72n2g,t1_ha72liy,1629831510.0,True
paqy2k,Why interpreted languages were made instead of making just compiled languages,ha73r3n,t1_ha72n2g,1629831942.0,False
paqy2k,Main reason is it is easier. You also don’t have to wait for them to compile.  There are also some features you can do that would be really hard to do in a compiled language.  Downside is if your code goes out to the world then every time it is run time and electricity are wasted.,haa2pjc,t3_paqy2k,1629890789.0,False
paimzm,"So modern CPUs are electric circuits and I’ll introduce a few concepts first

Voltage: voltage is a physical quantity which you can think of as the electrical analogue to pressure. Each point in 3d space has its own voltage at any time and all points connected by a continuous conductive path such as a wire are at the same voltage.

Power rails: the power rails are generated by the power supply, one is the ground wire, which carries a voltage of 0 which represents digital 0. The other is the positive wire which carries a voltage higher than 0 which represents digital 1. These are distributed all throughout the CPU.

Signals: these are wires which carry a time varying voltage, they could be low or high at any given time.

Transistors: a transistor is an electrically controlled electrical switch. It has 3 terminals, a voltage at one of the terminals controls whether or not the other 2 are electrically connected. Multiple transistors can be controlled by the same signal wire.

Logic gate: this is an arrangement of transistors which performs some logical function such as and/or/not on its input signals and outputs the result on its output signals. For example a not gate can be made by having a transistor switch the output wire to the ground power rail when the input wire is high and another transistor switch it to the positive power rail when the input wire is low.

Clock: the clock is another wire which carries a voltage generated by a physical device called an oscillator which is constantly alternating between 0 and 1. This too is distributed all throughout the cpu.

Memory cell: this stores a single bit of information for later use. On each rising edge of the clock signal the value of the input wire is stored, and on each falling edge of the clock signal, this value is reflected on the output wire. These too can be made by special arrangements of transistors. Registers and RAM are made of these.

Inputs/outputs: these are additional digital signal wires coming from input devices such as mouses/keyboards and additional output wires going to devices such as monitors.

So how’s it all work?

At the beginning of each clock cycle, the outputs of the memory cells and inputs of the CPU propagate through the network of logic gates, switching various signal wires to the low or high power rails, the final results find themselves back at the inputs of the memory cells and the outputs of the CPU. The results are stored in the memory cells and the whole thing happens again on the next cycle. It’s one big sequential state machine.

To answer your question about the endless chain, it’s a loop.

The inputs, memory cells and power rails control the logic gates.

The logic gates, power rails and clock control the memory cells and outputs.

The clock ultimately drives this endless loop of processing in the logic gates and storing in the memory cells.

If you want to study this in more depth you could start with the abstract theoretical side:

Digital logic: describes how you can take basic logic gates and memory cells and combine them to create more complex building blocks and functionality

Computer architecture: describes how you can take the building blocks from digital logic and create cpus and memory systems

Assembly programming: this is the end goal of it all, to create a circuit which can perform the basic  functionality of a computer which you’ll learn here

As for real world implementation:

Electromagnetism: the physics behind circuits

Circuits: self explanatory

Semiconductors: the physics of transistors

There are many more components to a computer, for example the hard drive, which has a different physical principle and is treated as an input/output device rather than a fundamental component.

And I’ll end by saying that circuits and transistors are not the only way to achieve digital logic in the real world, you could use relays, gears, vacuum tubes or really anything that allows you to construct networks of logic gates.",ha5386p,t3_paimzm,1629796234.0,False
paimzm,Your word 'loop' is intuitive. And the notion of 'Clock' is very useful to understand what I asked. So there is no magical unit but an oscillator just constantly bounces between 0 and 1 and this leads loop and each cycle CPU processes certain task! Thank you very much.,ha5544e,t1_ha5386p,1629797898.0,True
paimzm,"If I am understanding your question correctly, you are asking where does the initial information come from, to trigger the very big chain of digital electronics that will give the result to the user.

The answer is, it is stored in the hard drive of a computer. If you want to run a program (basically a sequence of bits) you have to store it in the hard drive first, then load it into the ram, and the ram will feed it into the processor that will do its magic. 

In earlier times, you could literally hard code a program's 0 and 1 into a magnetic tape and put it into the computer by hand. 

Also there is the case of rom memory, which feeds some data (instructions) to the processor during the computer start-up. This data is hard coded into the rom (that's why it is called read-only) by the manufacturer. The rom is basically like a very big lookup table, or you can think of it like a mathematical function from {0, 1}\^m -> {0,1}\^n.",ha53jzj,t3_paimzm,1629796525.0,False
paimzm,"In fact, I overlooked there could be initial input. I just imagined CPU creates from nothing to something. It was helpful that hard drive can give information to ram and finally to the processor. Hard coding through magnetic tape and rom memory are also nice example! Thank you.",ha55gtm,t1_ha53jzj,1629798204.0,True
paimzm,"Adding to what the other commenters said, think of a CPU as something with some state (the output of the registers, tiny bits of memory) that, whenever the clock sends a new signal, calculates the next state of the registers and updates their input. When the clock ticks the registers copy their current input to their output and hold it stable until the next clock tick.

So ""who controls bits(electricity) in each unit be on and off"" is the current output state of the CPU/registers.

For example: if the current state of the registers is that the next operation is to add register A to register B's output and store the result in register X, that makes the CPUs arithmetic unit (a small circuit that can do some basic math operations) read A and B and output the result of their addition to the input of register X. This takes some time to happen and propagate, so the clock speed is chosen such that the slowest operation the CPU supports always finishes within one clock cycle. CPU designers avoid supporting operations that are too complex and would take too long to propagate throughout the circuit, requiring a slower clock, as those can usually be replaced by a series of simpler operations anyway.",ha56yp7,t3_paimzm,1629799483.0,False
pagsk7,"“”https://minersutep.sharepoint.com/sites/CS53506350/SiteAssets/CS53506350%20Notebook/””  
  
this is a link from UT El Paso (UTEP) CS department and you can find lecture notes and discussions on Advanced Algorithms as the professor i took with before graduating last spring did a good job",ha4mo9x,t3_pagsk7,1629782597.0,False
pagsk7,Needs a student account from UTEP to access.,haeyu8l,t1_ha4mo9x,1629980362.0,False
pagsk7,"[https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/)

MIT 6.851 - Advance Data Structures.",ha59goa,t3_pagsk7,1629801483.0,False
pagsk7,interesting ..thanks  I will also check it out as I’m interested in algorithms and data structures,ha4mv8v,t3_pagsk7,1629782731.0,False
pagkn4,"If you're looking for practice, a very popular source is [Project Euler.](https://projecteuler.net/) This is a bank of hundreds of math/programming challenges, and the numbers are large enough that brute force usually won't cut it.",ha5o2fs,t3_pagkn4,1629810400.0,False
pagkn4,"you already know how, if you know how to find a repeatable method of making a process that solves a problem or accomplishes a task, than congrads, you already know. 

there are definitely books or youtube series or brilliant class on it.",ha4n3tp,t3_pagkn4,1629782902.0,False
pagkn4,"Thanks. What I seem to be worried about is that I can't seemingly solve problems on my own, for example the other day I got stuck on karatsubas mult, and my only way forward was to look up for the solution. Upon looking at it I could understand the solution perfectly. Any suggestions to improve upon this.",ha4puig,t1_ha4n3tp,1629784910.0,True
pagkn4,"Experience (is the best way to improve).

With growing experience, you should find it easier to figure out algorithms for yourself without looking them up. On the other hand, any new problems that you encounter are likely to be harder than the ones you already know, so the need to look it up at least sometimes might be there for a long time still.

You could try to rehearse and become better with the basics:

* is the series of steps consistent, and the number of steps neither too low or to high?
* does every step have one determinate way forward?
* does the series contain infeasible steps?
* can you make the algorithm loop forever? (You shouldn't.)
* what happens if you extend the set of inputs? (For each set, always a result, always the same result, always the correct result.)
* if given a class of input sets, can you exhaust it and reach the end each time?",ha53dvq,t1_ha4puig,1629796372.0,False
pagkn4,"Thanks, that makes sense and gives me hope xD. I'll keep on trying.",ha57kak,t1_ha53dvq,1629799980.0,True
pagkn4,"Patterns. Always look for patterns, and do some two dimensional problems like “least amount of moves” or “battling queens”. I found that doing multi dimensional thinking in math was really really helpful in solving all other problems. Gets you thinking about meta math rather than hard math, also build a calculator in  marie architecture, that helped me understand how computers think and really was the click moment during my education",ha82y28,t3_pagkn4,1629846638.0,False
pagkn4,"Here's an algorithm I think every one runs, ""what do I want to see?"" Ask it often for the short term, semi often for the medium term and periodically for the long term. Answer it for the time context you've chosen and course correct your routine accordingly.",ha7myr9,t3_pagkn4,1629839688.0,False
pagkn4,"look up golomn rulers (or sparse rulers) and then join a cult. 

Get through the course first. There are lots of recourses online for testing and improving your skills (these have been mentioned by other comments).

In the end you will give up because LUTs are always fastest, always O(1) or O(n) and never have edge cases",hc5rx5b,t3_pagkn4,1631176575.0,False
pa6i0q,"It's not reall that hard, once you know the theory. Our university's compiler course basically consists of you implementing a (basic) C compiler in C++. Of course, C++ is significantly harder to compile than C. However, this just means that you should stick to writing your compiler in a simpler language so that you can later compile it more easy.

Writing lexers and (LL) parsers is actually rather mechanical and a task I find fun (I have weird hobbies, I know). Coming up with a type system for your language (trust me, you will want one) and implementing a type checker is anywhere between a textbook algorithm (ie algorithm W), something a bit more involved (inheritance?) or something undecidable (Java, surprisingly..).

Writing the backend also is easy, as long as you don't care about optimizations: Just translate by recusing on the AST.

If you want to feel like your compiler is completely freestanding, use x86 as a backend (or ARM, or RISCV, or whatever architecture you use). You will still want to output plaintext assembly, to be assembled using an assembler of your choice. If you want to learn a useful skill that will significantly help you if you ever want to do anything involving a compiler again, choose LLVM as a backend.

The fun exercise when trying to write a simple compiler that can compile itself is figuring out a language that makes writing a compiler easy without that language being hard to compile.

Functional languages need GC, while plain C makes switching over AST structures a pain. I guess you may want something similar to unsafe, simplified Rust, where you are still writing C-like code managing its own memory, while having first-class support for switching over inductively defined datatypes. This at least seems like a reasonable middle-ground language to me.

Of course, if you also design your own language, you will need to (initially) maintain two compilers: one written in that language itself, and one written in some other language for which a compiler already exists. This is the approach I would pick, though.",ha3knfq,t3_pa6i0q,1629763258.0,False
pa6i0q,[deleted],ha4qx9x,t1_ha3knfq,1629785739.0,False
pa6i0q,"it's easy to parse. Comiling LISP without a GC is possible but has memory leaks.

Making your own ISA is orthogonal to writing a compiler.",ha5rqgn,t1_ha4qx9x,1629812194.0,False
pa6i0q,"Check out “Modern Compiler Implementation in X”, it covers basically everything.",ha2j8an,t3_pa6i0q,1629747232.0,False
p9yzww,Clickbait title imo. The article is just a rant about how the competitive programming is wrongfully used as only metric in interviews. Competitive programming is not useless.,ha1dx3p,t3_p9yzww,1629730174.0,False
p9yzww,"What is it actually useful for? Competitions aside, ofc",ha1h22l,t1_ha1dx3p,1629731519.0,False
p9yzww,"Critical thinking, time allocation and, obviously you get to know the tricks of whatever language you are using.",ha1iyux,t1_ha1h22l,1629732323.0,False
p9yzww,"Aside from the other comment, other benefits are team work (if you do ICPC or other team based competitions), a better understanding of time complexity which can be very useful in swe, knowledge of data structures or algos that may otherwise not get taught in schools, and it also gears people up for algo based cs academia (which seems like a big oversight in this article).

Edit: I also think the author has a bit of an overestimate as to how hard interview problems are. I think they are harder than they were 10 years ago but interview problems for junior devs aren’t near as difficult as hard cp problems.",ha1qwsd,t1_ha1h22l,1629735634.0,False
p9yzww,"> Edit: I also think the author has a bit of an overestimate as to how hard interview problems are. I think they are harder than they were 10 years ago but interview problems for junior devs aren’t near as difficult as hard cp problems.

I think part of this is because people who don't have a CS background or haven't done Mathematical problem solving type of work are not guaranteed to have expertise in the sort of thinking that these puzzles test. Problem solving as in experimenting, making conjectures, using the knowledge you possess in creative ways is something that you can in occasion sidestep learning how to do especially if you are learning how to code on the job (where you have things like google/stackoverflow/ask a coworker). 

The good thing (IMO) is that these skills are sort of like riding a bicycle, once you acquire them you can view different domains with that same lens and continuously re-apply them. Presumably why folks who talk about having worked on Olympiad style problems in high school talk about how familiar that process is when compared to ICPC or even some interviews.",ha3lvgr,t1_ha1qwsd,1629763832.0,False
p9yzww,"I personally think it's useful because one can practice the skill of thinking, programming and problem-solving in general. Also, programming is my hobby so I find stuff like [codingame](https://www.codingame.com/home) to be fun and a good way of practicing a syntax of a new language.

I actually agree with the article itself I just don't think the competitive programming is useless.",ha1rhh2,t1_ha1h22l,1629735870.0,False
p9yzww,"I practiced competitive programming for a while, but soon (a few years) I got bored with it. The last match I was so bored I couldn’t code at all.

I suppose I find more joy in the “creative” aspect of programming instead of the problem solving itself",ha1wzjq,t1_ha1rhh2,1629738120.0,False
p9yzww,It always seemed to me that the creative aspect and the problem solving aspect were the same aspect. You feel otherwise?,ha347sr,t1_ha1wzjq,1629755835.0,False
p9yzww,"Perhaps? I think it’s the difference between making/solving something that doesn’t actually “make” anything else

But that’s just my opinion, I’m a bit odd",ha34fiy,t1_ha347sr,1629755929.0,False
p9yzww,"To add on to other comments, many useful things in life are the things not deemed useful by some bullshit measure, like money or a job. There are many useful thing competition provides:

Enjoyment, challenge, measure your progress (both self and against others), mental health reasons, self improvement, self satisfaction, self esteem, anti-depressant, social reasons like friendship, companionship, team work, 

Etc...",ha2bd0b,t1_ha1h22l,1629743986.0,False
p9yzww,"Intellectual benefits aside, it's like football or other sports. Entertainment and time consume",ha2xxth,t1_ha1h22l,1629753189.0,False
p9yzww,Removing candidates you don't want to hire for other reasons.,ha3o2m3,t1_ha1h22l,1629764836.0,False
p9yzww,It's fun.,ha3u0h0,t1_ha1h22l,1629767491.0,False
p9yzww,It helps you develop problem solving skills.,ha4h8fe,t1_ha1h22l,1629779055.0,False
p9yzww,Probably lost a lot,ha1fjmr,t3_p9yzww,1629730869.0,False
p9yzww,Lmao idk why downvoted but true.,ha1kdao,t1_ha1fjmr,1629732911.0,False
p9yzww,Probably written by someone who's never done competitive programming,ha1qn7t,t3_p9yzww,1629735524.0,False
p9yzww,"I wouldn't say its useless, but the comments here in this thread are way off. Leetcoding should be a minor part of interviewing. It says nothing about 95% of what constitutes a good programmer. And its certainly true that many recruiters are caught up by 'code metrics', which wont actually get you good hires.",ha4yyxr,t3_p9yzww,1629792434.0,False
p9yzww,"„uber-coding-lord status on codeforces regularly fail the interview process because EVERYONE around them is at that same level!“

Uh, no. This level of skill is really rare and even at top companies most people are not at that level (and don’t need to be). But just because someone is great at CP, they can still fail interviews for many other reasons: failure at system design questions, really bad social skills, bad luck, bad coding style, misunderstanding a question, etc.",ha29ot4,t3_p9yzww,1629743294.0,False
p9yzww,"This shit again?

I keep coming across rants about competitive programming and the people who write them are invariably one of two types:

a) Young software engineers who have bad problem solving skills and can't pass interviews as a result. They blame the system instead of working to improve themselves.

b) Older engineers who come from a time when ""knowing someone who knows someone"" was enough to get a job and are salty about the fact that they have to pass technical interviews to get a job now. They want to go back to an era of nepotism.

Competitive programming IS useful. It gives you a better understanding of algorithms and data structures, pushes you to develop problem solving skills to a level beyond what a CS degree requires and shows that you are smart, which is why big companies put so much emphasis on it.",ha58o96,t3_p9yzww,1629800866.0,False
p9yzww,If these people put as much effort into leetcode as they do into complaining about leetcode they’d all be employed by now,ha42och,t3_p9yzww,1629771429.0,False
p9yzww,competitive programming is fun af idc if its useless,ha4vej1,t3_p9yzww,1629789355.0,False
p9yzww,Yes because efficiency and runtime don't matter at all in coding.... I too like to take things out of context,ha3mx7o,t3_p9yzww,1629764315.0,False
p9yzww,"I can appreciate the emphasis on why people do not think competitive coding is sufficient for coding in a company but by no means is it useless. It is a mind sport by nature and develops  critical, creative and resourceful thinking. I would love to see more of this encouraged in curriculums as (imo) schools teach CS way too broadly and the problem solving aspect becomes a lesson in wrote memorisation, where only a handful of algorithms must be memorised via pseudocode.",ha498n9,t3_p9yzww,1629774633.0,False
p9pg7g,"It’s not a general purpose algorithm, but some architectures have a native instruction for finding the position of the most significant bit.

[A relevant stack overflow question with various answers](https://stackoverflow.com/questions/671815/what-is-the-fastest-most-efficient-way-to-find-the-highest-set-bit-msb-in-an-i/673781#673781)",h9zg9qd,t3_p9pg7g,1629684819.0,False
p9pg7g,Thanks!,h9zkfdg,t1_h9zg9qd,1629686946.0,True
p9pg7g,"If there was a general purpose algorithm counting these bit sequences that would be faster than O(N), you would have a contradiction:

Consider such an algorithm exists. Imagine it running on the input 000...00100...0, i.e. an input of length k with a single 1 somewhere in it. Choose k such that the algorithm computes in less then k steps - such a k must exist since the algorithm has sublinear time. If we look at what parts of the input the algorithm considered, we realize that it can not have ""looked at"" all parts of the input, since that consists of k bits, but we did less than k steps. Thus if we modify our input in a place where the algorithm did not look, we get an example where our algorithm is wrong. This is a contradiction.

So since an O(N) algorithm exists, and an algorithm faster than O(N) does not, O(N) is the optimal time.",ha3m9rj,t1_h9zkfdg,1629764016.0,False
p9pg7g,">Thus if we modify our input in a place where the algorithm did not look, we get an example where our algorithm is wrong.

I think you are talking about the general case where the input doesn't have to be one hot. I think I agree in that case, but still not sure because there are bit twiddling tricks that isolate the lsb (rightmost ON/HOT/SET/HIGH bit) of a binary sequence in O(1) (\`b & \~(b - 1)\` zeroes everything out except for the lsb) and maybe this can be used somehow to get a really fast algo. Just a gut feeling.

\---

On the other hand, I am now wondering if there can be a O(lgN) algorithm that binary searches for the single 1 IF we assume the input is one-hot.

    # Assuming one_hot is a power of two equal to 2 ** e, finds e.
    # Precondition: one_hot is a power of two.
    def find_position(one_hot):
        # Initially there is no upper bound. We indicate this with r == None.
        l, r = 0, None
        # Initial guess: guess that one_hot == 1 << 7 == 2 ** 7
        g = 7
        g_val = 1 << 7
        while g_val != one_hot:
            if g_val < one_hot:
                l = g + 1
            else: # g_val > one_hot:
                r = g - 1
            if r is None:
                g <<= 1 # double the guess
            else:
                g = (l + r) >> 1 # guess the midpoint between l and r
            g_val = 1 << g
        return g
    
    # Tests
    for e in range(100):
        assert(find_position(1 << e) == e)

If we consider \`1 << g\` to be O(1), then the entire algorithm runs in O(lgN) where N == e. But if we consider \`1 << g\` to be O(g), then the algorithm runs at like O(N).

\---

Going back to the general case where the binary sequence can have more than one ON/HOT/SET/HIGH bit, we could find the positions of the SET bits as follows:

    def find_positions(num):
        positions = []
        while num: # len(positions) iterations
            lsb = num & ~(num - 1) # isolate the lsb
            # lsb is one_hot
            position = find_position(lsb) # O(lg(position)) assuming 1 << g is O(1) instead of O(g)
            positions.append(position)
            num &= (num - 1) # zero out the lsb
        return positions

The while loop has L iterations where L == len(positions) == the count of SET bits in num. Each iteration runs in O(lg(position)) == O(lgN) (N is the maximum position, so O(lgN) is the worst case). Therefore the entire thing runs in O(L \* lgN)  which for many cases will be much smaller than O(N), especially if the binary sequence is sparse (i.e. L << N). Ofc, this whole thing hinges on \`1 << g\` being O(1) instead of O(g).

Edit: A previous version of this replay had the following incorrect sentence. ""M is the position of the MSB of num, which implies that M is on the order of lgN where N is the bit length of num."" What's incorrect about this is that if M is the positions of the MSB of num, then M is on the order of lg(num), not lgN. Indeed if M is defined this way, then M == N. Corrected this and the propagation of this error. The conclusion is still the same though.",ha3rke3,t1_ha3m9rj,1629766400.0,True
p9pg7g,"Those tricks only work on finite sequences, where any algorithm is O(1) because the input space is finite.",ha5rv43,t1_ha3rke3,1629812256.0,False
p9pg7g,"I was kinda thinking the same way what you have mentioned.

Using some sort of hashing to find the correponding index afer converting ""num"" to a power2 number using  ""unset all except rightmost""   and later removing the rightmost and hashing again.  
[https://youtu.be/raYgCcAGduU](https://youtu.be/raYgCcAGduU)  
[https://youtu.be/RkJuSlIeGFU](https://youtu.be/RkJuSlIeGFU)

But the problem I think is the hashing itself.  If we try doing hashing, we might end up doing division/multiplication/branching which are anyway much more expensive then our simple right shifts.

In real implementations, we usually have intrinsics available to count the leading or trailing zeros which do the counts in one go. And combining that with the unsetting righmost,  we would need ""M"" unsetting operations and ""M"" intrinsic zero counts.",hmtjbb8,t1_ha3rke3,1638380266.0,False
p99xqz,"It must repeat, since there are a fixed number of possible values. This is the [pigeonhole principle](https://en.wikipedia.org/wiki/Pigeonhole_principle).

Calculating the longest and shortest cycles becomes intractable for any realistically sized hash. A good hash function acts random, so [63% of them have a minimum cycle length of 1](https://crypto.stackexchange.com/questions/19493/is-there-a-string-thats-hash-is-equal-to-itself)",h9wardj,t3_p99xqz,1629630178.0,False
p99xqz,[deleted],h9wgtp8,t1_h9wardj,1629634687.0,False
p99xqz,"Repeatable, yes. For any given input, it always gives the same output.

Predictable, no. Given an input, a good hash function makes it essentially impossible to predict the output without actually running the function. Essentially it seems random.",h9wj6fd,t1_h9wgtp8,1629636228.0,False
p99xqz,[deleted],h9wjr7k,t1_h9wj6fd,1629636576.0,False
p99xqz,"Yes, that's exactly why I used the word ""seems"" and why OP said ""acts random"". So, literally no one here said a hash function *is* random.

As for ""no academic would ever..."". Ignoring the fact that neither of us *did* refer to it as random. There is actually quite a bit of research into the randomness properties of various hash functions:

From a [paper](https://people.seas.harvard.edu/~salil/research/streamhash-soda.pdf) by a Harvard Professor of CS and Applied Math:

>it is commonly observed that simple hash functions, including 2-universal hash functions, perform as predicted by the idealized analysis for truly random hash functions. In this paper, we try to explain this phenomenon. We demonstrate that the strong performance of universal hash functions in practice can arise naturally from a combination of the randomness of the hash function and the data.

Another commenter says that I am confusing chaotic and random. I am not. Yes, the chaotic properties of hash functions are what make them essentially impossible to predict for a given input. The fact remains that the output of many hash functions, especially those that are most commonly used, *appear random* which means that it is very difficult to distinguish their output from actual randomness and for many (not all) purposes they can be thought of as random.",h9wx11q,t1_h9wjr7k,1629643388.0,False
p99xqz,[deleted],h9y71rl,t1_h9wx11q,1629663389.0,False
p99xqz,"Do you have a critique of the paper or my comment? 

You said no academic would use random in the way we were. 

I said, I wasn't saying something was random, I was saying something seems random and posted a link to an academic paper discussing research into exactly how close to random certain hashing functions perform and for which conditions they do behave similarly to a truly random hash function.

You then questioned whether I read the article without providing any critique of my comment or the paper.

If you'd like to discuss the content of the paper, let's do it! Otherwise have a great day and try not to be so angry all the the time. It's probably not good for your health.",h9ymiyk,t1_h9y71rl,1629670332.0,False
p99xqz,"
>No academic would ever refer to random to something in the way you’re referring to, the same with the word predictable.

A cryptographic hash function MUST be indistinguishable from a **random oracle**. It's in the cryptography academic litterature and specifications.


>(cpu needed cheap enough to be used a lot in different applications)

No, slow hash functions are useful to hash passwords. That's the whole point of construct like PBKDF2, scrypt and Argon2.

>The real power of hashing algorithms stems from the impossibility to reverse them. They are one way, it has nothing to do with randomness.

The real power of cryptographic hash functions is that they are indistinguishable from random **and** that they are trapdoor functions (irreversible).",h9wvnqi,t1_h9wjr7k,1629642759.0,False
p99xqz,"Why is this downvoted? There's no randomness in sight. Maybe people are confusing ""random"" and ""chaotic"". These are two entirely different things. A good hash algorithm will be chaotic i.e. outputs from two similar inputs are not similar. But not random, that doesn't even make sense.",h9wq9j9,t1_h9wjr7k,1629640170.0,False
p99xqz,"It's downvoted because it's factually wrong. Hash functions in cryptography MUST be indistinguishable from a **random oracle**.

The fact that it doesn't make sense to you is irrelevant.

Furthermore that post makes grand claim that you won't find any mention of randomness in academics but this is just plain wrong as well, all papers about cryptographic hash function mention the indistinguishable from random property. Non-cryptographic hash functions similarly have plenty of statistical analyses done to ensure that the hash produced looks random enough.",h9wweku,t1_h9wq9j9,1629643102.0,False
p99xqz,It means that you cannot get any feasible information about the input from analyzing the hash. There is no relation between the hash and the input other than that fact that the input put through the hash function creates the hash. The hash generated is essentially random to an attacker unless the attacker knows the input.,h9wjrj6,t1_h9wgtp8,1629636582.0,False
p99xqz,[deleted],h9wkcqo,t1_h9wjrj6,1629636941.0,False
p99xqz,"Pseudorandom may be a better word for it, though “theoretically random” works as well. Notice the parent comment you originally replied to said hashes “act” random. And i said its “essentially random”. So i think the proper terminology is being used. 

But it sounded like you had a problem with something predictable being random? If you give a random number generator the same seed, it’ll produce the same random number every time, which would make it both predictable and “pseudo” random, no?",h9wlwuf,t1_h9wkcqo,1629637845.0,False
p99xqz,"Nope, it's not even pseudorandom. Pseudorandom outputs must follow a certain distribution (such as binomial, gaussian, uniform, etc). Each output must appear to be generated with a certain probability. This obviously does not happen with a hash function.

That guy is completely right. The only way a hash function is ""random"" is in the non-academic sense of the word. In reality, hashes are chaotic. A small change in the input space creates a large change in the output space. The algorithm for generating these outputs, however, is deterministic. It would be pointless for a hash function to be random. It's vital that the same string always maps to the same hash in a manner that is not feasibly reversible.",h9wr6xg,t1_h9wlwuf,1629640627.0,False
p99xqz,"Hash functions have a uniform distribution. Each output must appear to be generated with a certain probability **if you don't already know the input that caused the output.** An attacker who is trying to guess which input caused a specific hash has an equal chance to guess correctly, which is one out of the total number of possible inputs

>The only way a hash function is ""random"" is in the non-academic sense of the word.

I don't think you know what the academic definition of randomness is. What say you about pseudorandom generators? they're deterministic and usually chaotic. Do you think pseudorandom generators aren't even pseudorandom? What do you think the difference between a pseudorandom generator and a hash function is?",h9wyx8c,t1_h9wr6xg,1629644247.0,False
p99xqz,"Well, I stand corrected, I see what you're saying. To be quite honest I'm not entirely sure what I was thinking this morning.",h9xei8x,t1_h9wyx8c,1629651009.0,False
p99xqz,"No problem, randomness for these cryptographic function is a bit confusing.",h9xfyz9,t1_h9xei8x,1629651631.0,False
p99xqz,Good on you! It’s so rare to see people change their minds in online debates. This is how it should be done!,h9xxqeo,t1_h9xei8x,1629659289.0,False
p99xqz,Thanks u/billy_buttlicker_69 !,h9yrewh,t1_h9xxqeo,1629672635.0,False
p99xqz,"It is predictable if you have the function, but if you don’t have the function it should appear to be random, changing a single bit should result in a completely different hash.",h9wjd0v,t1_h9wgtp8,1629636344.0,False
p99xqz,"That's not random, that's chaotic. Random means non-deterministic and obviously it's deterministic if there is a predictable generating function.",h9wrfmv,t1_h9wjd0v,1629640747.0,False
p99xqz,Then you create Proof of History and design a 25 billion dollar cryptocurrency: Solana,h9xii3g,t3_p99xqz,1629652710.0,False
p99xqz,"It will repeat, because of the fixed size.

A ""good"" hash function has a long cycle for any input. For a better understanding linear hash functions are a good example to see what ""good"" and ""bad"" functions look like if applied repeatedly and cycle through the hash-space.",h9wiiw2,t3_p99xqz,1629635808.0,False
p99xqz,"If the hash function forms a permutation cipher, the cycle will have on average (N+1)/2 hashes. If the hash function is just random, the cycle will have on average sqrt(PI\*N/8). 

As others have said, iterating the hash will always create a cycle, but if it's not a permutation cipher, there might be a 'tail' of hashes that don't repeat... eg A -> B -> C -> D -> C -> D -> ...",h9xir9p,t3_p99xqz,1629652819.0,False
p99xqz,"Idk what exactly a hash is, but this reminds me of the Collatz Conjecture.",ha0icwo,t3_p99xqz,1629711629.0,False
p97tg1,"Downvotes me if you will, but I don't think this type of content belongs on /r/computerscience.",h9wg718,t3_p97tg1,1629634256.0,False
p97tg1,I’m curious to know why you think this?,h9wjre4,t1_h9wg718,1629636579.0,False
p97tg1,"TLDR: BDD is a SWE practice, not a computer science principle.

BDD/TDD is firmly in the realm of SWE, i.e. tools to support the development of software. This isn't a compsci topic *per se*, and doesn't involve any computer science formally. It's a best practice thing, and in my realm of bioinformatics, only highly mature suites/tools/algorithms have comprehensive test suites, let alone any hint of a BDD development style. BDD doesn't guarantee success, and similarly, it's absence does not preclude the use of tools that don't conform to it's dogma.

I've been developing software for the last 10 years, and I've never taken an undergraduate or graduate course in compsci that would mention or emphasize the importance of a testing style like TDD. TDD and BDD practices vary by institution/company and it doesn't *always* lead to better quality code.

To me this is just self promotion and not an interesting compsci topic that would be helpful for early career professionals looking to read and discuss computer science.",h9wkjds,t1_h9wjre4,1629637053.0,False
p97tg1,"Oh cool. I myself is very interested in computer science and don’t know much about it, but I want to learn as much as I can. Thanks for the insight :)",h9wl889,t1_h9wkjds,1629637457.0,False
p97tg1,"BDD is great but, I do find some difficulty with it. My tests often become deeply nested in contexts, rely on shared state, and can be very difficult to read, any tips?",h9vvok4,t3_p97tg1,1629617105.0,False
p97tg1,"I think as with anything in life, you build up the knowledge and capabilities to do it yourself, unfortunately. Can’t offer the specific advice you’re looking for cause this is outta my field (scrolled the website and I think I got the gist), but what I can say is after taking a step back and focusing on the big picture, that I have always been skeptical on relying on frameworks others have built (other than conceptual frameworks).  It does seem like a good framework, but anything more complex or user-specific requirements, options are: (1) spend more money to get the company to do the work for you, (2) spend the money to hire people to build your own framework, (3) spend the time (and effectively money) to learn it and build it yourself. In the long run, amassing the resources (knowledge and skill) yourself is the best option. But another counterpoint is that it’s a balance between resources: time and money. So it could be the best option to buy in. Just thought I’d reply with something that came across my mind to add any discussion points even though probs not helpful! Sorry!",h9wgo13,t1_h9vvok4,1629634581.0,False
p972zx,"Yep there are multiple ways to do partitioning. But the end goal is always to separate the array into 2 sections - one section with elements < than pivot and the other > than pivot.

One motivation behind different partitioning processes is to reduce the likelihood of a worst case scenario where pivot is the largest / smallest element.

Sometimes this involves randomly picking multiple elements and choosing the median as the pivot. We can even use k pivots to partition the array into (k + 1) sections.

For general usage it doesn't matter how you partition the array as long as the end goal is met . So just go with whichever you can understand and confidently implement.",h9vtnyi,t3_p972zx,1629615491.0,False
p972zx,"This might break it down for you.  Select the sorter you want to learn, pick any color, select 50 elements, click the button at the bottom of the screen, then it will move to the main screen, press the play button, sit back and watch it go to work. There are a couple different ways to have it sort the elements.

https://sorting.at/",h9w3f4k,t3_p972zx,1629623769.0,False
p972zx,this is usually for in place quicksort so first try to understand the logic behind the algorithm and then see why these kinds of implementations are equivalent.,hc5s0pq,t3_p972zx,1631176675.0,False
p8qm7y,The book Code by Charles Petzold. It’s more about hardware than software.,h9v89pe,t3_p8qm7y,1629602004.0,False
p8qm7y,I think Iowa states CPRE 281 class which is hardware focused has a lot of stuff online,h9u2o5f,t3_p8qm7y,1629581533.0,False
p8qm7y,There's a couple of crashcourse computer science videos on youtube that are pretty good for computer hardware.,ha2dvc1,t3_p8qm7y,1629745017.0,False
p8qm7y,Try the first part of this course https://www.nand2tetris.org/,h9t3uym,t3_p8qm7y,1629565547.0,False
p8msdc,"This news could mean nothing imho. IIRC, IBM announced similar breakthrough in chip size few years back but no white paper / further evidence  was released by them 🤷‍♂️",h9rv31h,t3_p8msdc,1629540914.0,False
p8msdc,Theoretically they should have already reached the physical limits. I would love to read about what this means going forward,h9s0cto,t3_p8msdc,1629545268.0,False
p8msdc,"Well, from one hand 2 nm is far below tunelling limits for popular semiconductors, from the other hand I'm almost certain that similar to previous breakthroughs with all the recent *small_number_in_nm"" technologies there is no single element with the mentioned size, that's just a new transistor packing scheme, probably in some weird way in 3D.",h9t0w40,t1_h9s0cto,1629564228.0,False
p8msdc,"Right, I'm more interested in their touted 75% power savings.",hg8pgxw,t1_h9t0w40,1633967900.0,False
p8msdc,confidentially incorrect,h9sqqyp,t1_h9s0cto,1629559673.0,False
p8msdc,"I'm not sure what you mean, can you elaborate?",h9swet7,t1_h9sqqyp,1629562220.0,False
p8msdc,"nobody has hit any limit yet, and nanometer numbers are marketing numbers so we can’t know if they had hit a limit even if we knew what the limit was",h9swlpy,t1_h9swet7,1629562307.0,False
p8msdc,"We're reaching physical limits all the time. As for semiconductors the true physical limit is the speed of light. But the challenge is usually finding a way to get around a physical limitation by changing the design. 
This is not a design change. It's basically just a small optimization of a current design. Which means we're still operating under the current limit.",h9sx839,t1_h9swlpy,1629562590.0,False
p8msdc,"Isn't the true physical limit of semiconductors the size of the physical atoms that make up the silicon? The diameter of a single atom measures somewhere between 0.1 and 0.5 nanometers, so there is a finite limit to how small a single transistor can become.",h9t10co,t1_h9sx839,1629564279.0,False
p8msdc,"Yes it would probably be tough to fashion a tube that's only 1 atom wide, but it would definetely be as small as it can get without coming up with another approach and Yes, transistors are also a great example of one of the more obvious limits. Following Moore's law we can see we're closing in on the threshold for transistors.",h9t2mzi,t1_h9t10co,1629565001.0,False
p8msdc,Moore’s law is hardly a law. It was an observation about the number of transistors in silicon increasing at a certain rate in the 90s -00s. I’m pretty sure that observation ceased to be valid a long time ago.,h9t3fsz,t1_h9t2mzi,1629565359.0,False
p8msdc,"Moore's law was about the limits of economy at scale. In effect, Moore’s Law predicted the increasing affordability and accessibility of computer technology, as the cost per transistor decreases when there are more transistors available on a single chip. This translated to the doubling of transistors every 2 years maintaining their affordability. 
We have pretty much reached that limit with 7nm and to most applications, 10nm.",h9t5e02,t1_h9t3fsz,1629566236.0,False
p8msdc,"The tech started to outrun Moore's law around 2010, but since 2018 leading semiconductor manufacturers have developed IC fabrication processes in mass production which are claimed to keep pace with Moore's law.

From wikipedia",h9t40e6,t1_h9t3fsz,1629565614.0,False
p8msdc,"Until they remove the nucleus of the atom, replace it with the transmitted, and leave the electron cloud essentially turning it into a tube allowing for speeds beyond the speed of light. Thereby turning each atom into both container and contents at the same time.",h9tndqq,t1_h9t2mzi,1629574454.0,False
p8msdc,"That sounds crazy, but necessary lol",h9toh4h,t1_h9tndqq,1629574961.0,False
p8msdc,lets talk in incoherent circles!,h9t0slz,t1_h9sx839,1629564184.0,False
p8msdc,You are the confidently incorrect one lol.,h9uyphp,t1_h9swlpy,1629597179.0,False
p8msdc,"Please feel free to indicate specifically what I said that was incorrect, and I will reflect and apologize if I have been so.",h9waw3q,t1_h9uyphp,1629630282.0,False
p8msdc,"No I won't because you didn't, and other people in the comments have already pointed it out to you",h9wj6dk,t1_h9waw3q,1629636227.0,False
p8msdc,it is a fact that the nanometer claims are marketing figures that have bo relation to any physical dimension on the chip.  it is a fact that nobody knows where the limit actually lies yet.,h9wpt2z,t1_h9wj6dk,1629639941.0,False
p8msdc,Ignore this person,h9t0vvo,t1_h9sqqyp,1629564225.0,False
p8msdc,"the nanometer numbers companies use now dont even mean anything anymore so no, nobody can say.",h9sqzf8,t3_p8msdc,1629559781.0,False
p8msdc,"This literally doesn't mean anything, the mm used in silicone industrie kind of lost its meaning and every manufacturer uses the term according to them

But being able to put smaller and smaller transistors helps with efficiency and size, so if this process somehow exported to other manufacturers we can see better performance, as expected",h9s7mg9,t3_p8msdc,1629550072.0,False
p8msdc,"To add to what others have said, ""#nm"" is pretty arbitrary and up to the manufacturer. Some ""7nm"" chips are really more like 12-14",h9u4rzy,t3_p8msdc,1629582513.0,False
p8msdc,"Yep, unlike the days of planar CMOS … there’s just not anything “X” nanometers in size in state-of-the-art FinFET nodes.

That being said … a 2nm transistor is -- with some marketing spin applied -- roughly 2x better at *something* compared to a 5nm transistor, the current flagship mass-production node. It could be power efficiency, it could be speed, it could be density, it could be a little of all three. It’s also arguably 2x more expensive to manufacture.",h9uqmn9,t1_h9u4rzy,1629593233.0,False
p8msdc,"in depth article

""Today’s announcement states that IBM’s 2nm development will improve performance by 45% at the same power, or 75% energy at the same performance, compared to modern 7nm processors.""\[1\]  
\[1\] [https://www.anandtech.com/show/16656/ibm-creates-first-2nm-chip](https://www.anandtech.com/show/16656/ibm-creates-first-2nm-chip)

article about intel's nanoribbon 

 [https://www.anandtech.com/show/15865/intel-to-use-nanowirenanoribbon-transistors-in-volume-in-five-years](https://www.anandtech.com/show/15865/intel-to-use-nanowirenanoribbon-transistors-in-volume-in-five-years)",h9uq342,t3_p8msdc,1629592960.0,False
p8msdc,RemindMe! 1 day,h9rnetz,t3_p8msdc,1629534080.0,False
p8msdc,"I will be messaging you in 1 day on [**2021-08-22 08:21:20 UTC**](http://www.wolframalpha.com/input/?i=2021-08-22%2008:21:20%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/p8msdc/a_record_breaking_2nm_process_developed_by_ibm/h9rnetz/?context=3)

[**6 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fp8msdc%2Fa_record_breaking_2nm_process_developed_by_ibm%2Fh9rnetz%2F%5D%0A%0ARemindMe%21%202021-08-22%2008%3A21%3A20%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20p8msdc)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",h9rnfv6,t1_h9rnetz,1629534104.0,False
p8msdc,Does IBM manufacturer chips? I thought they were a consumer and manufactured mainframes or servers,h9s0ojw,t3_p8msdc,1629545516.0,False
p8msdc,"Although IBM doesn’t manufacture chips, they still perform advanced research, including transistor process development. Year after year, IBM is #1 in US companies filing patent applications.",h9upv6y,t1_h9s0ojw,1629592850.0,False
p8msdc,IBM designs and uses the Power line of CPUs for their mainframes. Idk if they manufacture it themselves but computer hardware is definitely one of their research areas,h9t6juo,t1_h9s0ojw,1629566758.0,False
p8msdc,They are more of a software company nowadays.,h9vcrsk,t1_h9s0ojw,1629604395.0,False
p8msdc,soo will we be seeing 32 core cpus for consumers for cheap ?,h9s39uj,t3_p8msdc,1629547359.0,False
p8msdc,[deleted],h9s3kyr,t1_h9s39uj,1629547559.0,False
p8msdc,"Ah that's sad :(

I always dreamt of running more than 3 VMs at once lmao",h9s3v0y,t1_h9s3kyr,1629547746.0,False
p8msdc,[deleted],h9s3zr3,t1_h9s3v0y,1629547831.0,False
p8msdc,"Well wouldn't that be slow af

I have a 4C/8T CPU and using anything less than 2 cores for a VM makes it lag like hell

I use VMs for a whole variety of things

Some of them for testing if a said program is a malware or not

others for Trying Different Distros Of Linux",h9sunj7,t1_h9s3zr3,1629561435.0,False
p8msdc,[deleted],h9suyeb,t1_h9sunj7,1629561571.0,False
p8msdc,"For Testing Malware, Windows 10 

Since I'm learning stuff like C++ I have a VM with Endeavour OS (Linux) with an IDE so that VM could make use of as many cores as I can give it",h9sva0c,t1_h9suyeb,1629561715.0,False
p8msdc,[deleted],h9svppw,t1_h9sva0c,1629561908.0,False
p8msdc,Xfce (because I wanted linux to be as light as it can so that more resources can be devoted to the IDE if that makes sense ?),h9sw7o8,t1_h9svppw,1629562131.0,False
p8msdc,[deleted],h9swijr,t1_h9sw7o8,1629562266.0,False
p8msdc,"Good Question

It all comes down to the Compiler

In Windows I gotta spend a lotta time configuring the Compiler (Although theorotically the IDE should come configured with it)

&#x200B;

On Linux 90% of the time gcc/g++ is already installed so the IDE just uses that instead of downloading its own so Linux saves me time and it's basically much easier to deal with tbh",h9sxsgy,t1_h9swijr,1629562842.0,False
p8msdc,[deleted],h9w5rm2,t1_h9sxsgy,1629625880.0,False
p8msdc,"Oh That's cool

Soo since your VM doesn't have a GUI, you don't even need to allocate a lot of RAM to it Right ?

So that means that you could keep it running in the background forever without it having a major performance impact on your Host

Also how exactly did you setup SSH ?

does it require some special software ?",h9wk9ee,t1_h9w5rm2,1629636884.0,False
p8msdc,"Not much.

If you look at what faster computing has effectively meant for the common person, its meant better targeted ads. If you think about it, all the bright minds at Google and FB are there to  make better classified ads. Top companies are ad companies, yay. Truly we live in an amazing age.

Its kindof a disappointment to me that is what the main takeaway of the computer revolution so far.

For the desktop computer, most people wouldn't even notice a 10x increase in CPU single core performance because thats not the bottleneck on what they do.

Sure, we will get pi calculated to higher digits.",h9tdoya,t3_p8msdc,1629569966.0,False
p8msdc,"> Sure, we will get pi calculated to higher digits

WORTH IT",h9vbkfo,t1_h9tdoya,1629603741.0,False
p8msdc,Call me when they are under 1 nano meter. Pico ?,h9txinf,t3_p8msdc,1629579136.0,False
p8msdc,"That is super cool, I always wonder how they can manage to make them smaller and smaller commercially when (at least to my understanding) physics tells them no.

... 👀 ... nice Notification bar",h9uqg0z,t3_p8msdc,1629593140.0,False
p8msdc,How do you even build that?,h9v8sut,t3_p8msdc,1629602286.0,False
p8msdc,"Eh, as a Computer Engineer, the fabrication process isn’t much of a concern to me. Truth be told, it’s a marketing strategy. 

The real advancement in processing performance (vague term) will be derived from the implementation of hardware acceleration cores. This is already being done with current trends in ASIC design.",h9vh3mv,t3_p8msdc,1629606804.0,False
p8msdc,I'm no expert on processors. Can this technology improve quantum computing? If not I don't see how this could be a huge breakthrough for computing when quantum is the up and coming technology. At least not from a standpoint where some huge physics problems are solved opening up the mysteries of the universe.,h9volxe,t3_p8msdc,1629611707.0,False
p8msdc,ah gay...,h9t87u0,t3_p8msdc,1629567513.0,False
p8msdc,"Look up Sam Zeloof. Moore's law ain't shit, homie.",h9sgyez,t3_p8msdc,1629555072.0,False
p8m6g2,"An example is the Z-machine, a virtual machine for interactive fiction (text adventures) invented by Infocom in 1979 and still used today by the Inform design system.

https://www.inform-fiction.org/zmachine/

I remember Level 9, another 1980s software developer formed by 3 guys (the Austin brothers) working from their home, also developed their own virtual machine (The A-machine) as a way of getting text adventures to fit into the home computers of the time, when 16K of memory was considered a lot.",h9rl5xz,t3_p8m6g2,1629532119.0,False
p8m6g2,Awesome info. Thanks!,h9rlqan,t1_h9rl5xz,1629532609.0,True
p8m6g2,"It’s not a stupid idea.

You can easily imagine a VM that offers support for specialized hardware.  If the hardware is present, the VM uses the hardware. If the hardware is not present, the VM emulates the missing hardware, and the application software runs fine, either way.

Most VMs are general purpose, but there’s no reason why you couldn’t have a special purpose VM.",h9rldnm,t3_p8m6g2,1629532304.0,False
p8ltws,"Professor of CS here and I can tell you that the solution I would use is a spreadsheet (Or even just a text file) and Ctrl+F. That will be a faster interface than anything I code by hand.

If I discover I want to do more in future - like collect statistics on how often each one is used - I can code something then and import the data from the spreadsheet.

If you want to do this just to practise coding, it can be done in any language. You can do this in Java, sure. Pick a language you know - you don't want to learn a new language just for this project 

Myself, I would pick a scripting language like Python over Java, because this is a simple program and Python will have much less setup time.",h9rcws0,t3_p8ltws,1629525558.0,False
p8ltws,"I agree with this. I am currently a CS student in their junior year at uni. I would use a CSV (comma separated value/ excel spreadsheet) to do this. Additionally, Python also has a useful library called Pandas that is used for easily displaying data/ statistical data in a descriptive way (i.e bar graphs, histograms, pie charts). Be very careful with this though, because personal information about patients is heavily protected by HIPAA.",h9ugsvi,t1_h9rcws0,1629588360.0,False
p8ltws,"Assuming that you don’t have to worry about regulatory compliance (e.g. HIPAA), you can complete those requirements fairly easily with pretty much any programming language. Depending on the number of samples, some choices might be better than others. For example, if you have a billion samples, a database and the use of SQL might be a better choice than a local flat file (e.g. CSV, JSON, or XML). Assuming you don’t have that volume of records to store and search, Java should work just fine. Read/write to a local file and back it up periodically.

I get that you want to test your skills and this is a good project to work on. However, sometimes we developers can spend too much time reinventing the wheel. A spreadsheet might also fit your task just as easily.",h9rdh04,t3_p8ltws,1629525974.0,False
p8ltws,The big problem is practicallity.   It will be pretty onerous to type in the ID #s and location as you store every tube.   Bar code scanners could help but will not be as easy for inputting the location.,h9ubypw,t3_p8ltws,1629585952.0,False
p8copp,"Your question needs to be a lot more specific before anyone can answer that question. What are the inputs and outputs? You question is so general that a sorting algorithm could suffice, or a simple if statement comparing two prices. Maybe you need memorization or dynamic programming.",h9pl77e,t3_p8copp,1629493256.0,False
p8copp,The fact you are looking for a ranking algorithm seems to imply there are multiple options to rank but you mention only two - flat fee and percentage. Do you want to do a simple check if one number is bigger than the other? If not can you explain how it’s more complex than that?,h9pmgj4,t3_p8copp,1629493789.0,False
p8copp,Did you mean to Google this homework question?,h9q8lke,t3_p8copp,1629503806.0,False
p8copp,quicksort will do trick,h9q3zdi,t3_p8copp,1629501639.0,False
p8copp,a line graph i think,hc5s21r,t3_p8copp,1631176713.0,False
p7x4gh,Fun and Regex do not go together,haah540,t3_p7x4gh,1629898546.0,False
p7nprt,"There really are a lot of answers to this question as I'm sure you can see from the comments.  Depending on what kind of code you're dealing with, computers ""understand"" it in a variety of ways.

At the very lowest level, there is binary code.  The computer ""understands"" this by sending each bit of the code through the processor as either a high-voltage or low-voltage signal.  The machinery inside the processor then processes these using what are essentially convoluted configurations of transistors.  Every processor is built to interpret certain instructions, which make up its *instruction set*.

The level above that is assembly language.  While binary code is stored in zeroes and ones, assembly programs are at least somewhat human readable.  However they still have basically a 1:1 correspondence with the binary instruction set.  A computer ""understands"" these by reading the assembly program line by line, resolving a few memory locations and such, and packing them into binary code so that they can be executed.

From there it really goes all over the place.  Some languages, like C, are *compiled*, meaning there is a program that reads them and converts them into binary code to be executed.  Others, like Python, are *interpreted*.  This means that rather than be converted into machine code, Python programs are read and executed in real time by the Python interpreter.  And there are languages that use combinations of both techniques.  One reason to choose between different programming languages is because of the pros and cons associated with each of these approaches!",h9lwosu,t3_p7nprt,1629422856.0,False
p7nprt,So the processor is the piece of hardware that converts physical inputs into digital outputs in the form of binary? It does the same thing in reverse as well? I've always wanted to know this!,h9p5bt1,t1_h9lwosu,1629486756.0,False
p7nprt,"The processor receives physical inputs and executes some action based on what it receives.  So, imagine there's some binary instruction that corresponds to ""load whatever value is in memory location X and store it in register A"".  That series of electrical impulses will enter the processor and percolate through its circuitry, eventually hitting the memory controller, which then sends a message along the memory bus to the actual physical RAM, which does its own processing... Obviously, it's complicated.  But at the end of the day, a value is sent back to the CPU from memory which is eventually stored in one of its registers.  Voila!  an instruction has been executed.  Now (at least in theory; modern processors are highly optimized so sometimes they cheat) the processor will wait until the next clock cycle before pulling in another instruction and executing that one.

Instructions can do a variety of things but really what they come down to is manipulating the state of the processor itself, and exchanging data with peripheral devices like memory or a keyboard.",h9pb4wp,t1_h9p5bt1,1629489018.0,False
p7nprt,"Yes, in a nutshell. The CPU contains registers to store bits and an ALU (arithmetic logic unit) to perform operations with those bits. It also contains circuitry that interprets and executes instructions. I would recommend nand2tetris if you want to take a course about the subject.",h9p7s6h,t1_h9p5bt1,1629487614.0,False
p7nprt,"If you want to know what is going on inside the cpu on a very basic level then I suggest watching this series by [Ben Eater](https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU)

In this series he builds a simple 8 bit computer from simple components.  During this you will come to understand how machine code works.

All programming languages first started out as machine code.  Somebody was back when sat down and wrote tge first assembly language using machine code.  Somebody then came along and used assembly language to develop a higher level language like C. Then C was used to develop languages like python.  

When higher languages are compiled they are being translated into machine code. Some languages require that the program be compiled before you run it.  Python does the translation from python code to machine code at run time.",h9nche4,t3_p7nprt,1629457973.0,False
p7nprt,Watching Ben Eater's videos is what made everything finally click for me. His explainations and demonstrations are top notch.,h9oska8,t1_h9nche4,1629481470.0,False
p7nprt,"I know enough programming to entertain myself.  Got into computers in mid 80s.  Learned Fortran, basic, c, and pascal.  Tried to teach myself assembly and I just couldn't get it.  It seemed unnecessarily complicated.  Years later I started looking into how a cpu was made and I finally understood why assembly language is the way it is.  I believe if you really want to understand how programs work you should start with how a cpu is made.  It's the difference between being able to drive a car and how to build a car.",h9otphj,t1_h9oska8,1629481934.0,False
p7nprt,"Compilers. They test for syntax, lexic and semantic. If the code pass to these 3 algorithms, you have a code that is readable by the machine. The code result from the compilers will be mount in a data structure (like a tree) that the language can run through it and understand it easily. 

For machine language, like Assembly, you are using a language very close to the functions that a CPU has and understand, so the conversion is much more simple: basically map 1:1 the assembly command to the correspond binary sequence understood by the machine.

Edit. Reading OP comments, just want to add to give more insight 

A CPU is a piece of hardware that is very good at counting. It does it very fast and precisely. It works consiste of counting from binaries data saved in memory. Binaries are number represent in base 2. E.g you may represent a space in memory as the value 12. If this is the last space used on the memory, the CPU will calculate the next free space (which would be 13), so the CPU increase this number and save it on memory. That's all about the base theory for computers: count binary data saved in memory. (That is why you call computer of computers, because they count! They count-puter hehe). The binary that saved in memory is a numerical representation for our real problems! 

There is one last piece for this puzzle: Automata & formal language. I'm calling automata here, but I'm not sure about the name. But I will describe here so you understand what it is or someone correct me the name. Automata is a algorithm that describe what a computer or computer language can understant. It's basically a function that receive a input (e.g a binary data) and tells if that binary will be accept and understood by the machine or not. That means, it describe all the 'words' a computer will understand. That is how a compilers can tell if you misspelled a keyword by the language you are using. There are a lot of CS theory to develop a language. This will be applied even for a binary language! Now that you defined how your language will be, you can map the 'words' with functions. This functions can be the low level language (machine language) or a Python function mapping a C functions.",h9l1suw,t3_p7nprt,1629408791.0,False
p7nprt,"So compliers and interpreters work based off of automata theory?

Also, what is formal language?",h9lwzr6,t1_h9l1suw,1629423002.0,True
p7nprt,"I think I went to far on the explanation. What I called as automata is the algorithm used on the lexical process of the compiler (but this probably not the right name for the algorithm). ""Automata and formal language"" is the name of the theory that study the process of creating computer languages. There are a lot of terms and ideas around computer language. I got an example from wiki, just so you can have an idea that the rabbit hole for the theory is very deep:

""A formal grammar that contains left recursion cannot be parsed by a LL(k)-parser or other naive recursive descent parser unless it is converted to a weakly equivalent right-recursive form. In contrast, left recursion is preferred for LALR parsers because it results in lower stack usage than right recursion.""

You probably may not get what is writing above but I will give one example for context.

You may create a language that the computer will only understand if the words are symmetric (the automata algorithm would only recognize symmetric words):

See that I'm using binary, so you can understand that a language can be made for machine only use.

    0101 - ok, because 01|01
    0110 - not ok, because 01|10
    010010 - ok, because 010|010
    011101 - not ok, 011|101 

So, every symmetric word the computer would understand, which could give a infinite number of words for this language. You may use later the word '0101' for this language to map some specific function when the CPU reads it.

This may be a fictional example, but it works very close to that. Depends on how you structure the words of the language, you will have different behavior, performance, different algorithm to understant it, different number of possible words on the language, etc. Conclusion: do a master in compilers and you may will see a lot of this.",h9m41iv,t1_h9lwzr6,1629426403.0,False
p7nprt,"You might take a CS class called language processing. It’s where I learned some automata theory, context free grammars, formal language - and essentially how the programming language is processed using tokens.",h9ozezb,t1_h9lwzr6,1629484281.0,False
p7nprt,"Shortly, every byte inside a code is converted sequentially into an another code, in the case of Python, usually is C, so when you call a sort() function in Python, the code to this function created in C is mapped and starts to run, after the C, depending on compiler, the code is mapped to assembly so it can execute the built in functions of the language, and after that, finally comes to the binary code which is nothing more than the same code built until here, but written in a way that processor can translate this to electrical signals to execute some action, if you study some digital electronics you'll see that you can perform all operations that we use (like sum or division) only using 0s and 1s as input.

Hope that clarifies a little",h9l0q40,t3_p7nprt,1629408339.0,False
p7nprt,Thanks for the great explanation and happy cake day!,h9mjzfi,t1_h9l0q40,1629435229.0,False
p7nprt,"There's a few layers down where things are implemented in code, but at the bottom layer the code runs on logic gates. When it comes to logic gates there are a few basic operations available OR, XOR, NOT, and AND. These operations are combined to make more complex operations (i.e arithmetic).

You can even have a couple of them loop back on each other in such a way that it can be set to always output a charge or to never output a charge without having to rewire anything. This is called a flip flop.  It can be used to make CPU registers so that the computer can remember things and use the previous outputs as inputs to the next operation.

Logic gates are implemented using transistors. At this point understanding how it works is no longer in the domain of computer science, it's chemistry.",h9li0tg,t3_p7nprt,1629415989.0,False
p7nprt,Do you mean physics?,h9nrbzz,t1_h9li0tg,1629466214.0,False
p7nprt,"Every instruction is given a number and these numbers are designed to work with what is called an instruction decoder in your cpu that uses that number to turn on and off wires that control the other parts of the CPU. So an add instruction is a number that controls the wires in the CPU such that the arithmetic parts of the CPU do their job correctly.

http://static.righto.com/images/ARM1/2-chip_labeled.png
https://cdn-blog.adafruit.com/uploads/2014/09/z80-labeled-bus.jpg

Grab a copy of the book Digital Computer Electronics by Albert Paul Malvino, Jerald A. Brown, and Stephen Page, it covers everything you are curious about.

Python works similarly, but in a simulation: The Python program simulates a type of computer called a stack machine. It isn't at all like the hardware equivalent because that would be exceptionally slow -- it's rather similar to how emulators work. https://github.com/python/cpython/blob/2f180ce2cb6e6a7e3c517495e0f4873d6aaf5f2f/Python/ceval.c#L1645 That's the C code that evaluates python bytecode.",h9lznxg,t3_p7nprt,1629424282.0,False
p7nprt,"The 0s and 1s are literally translated to on/off input sequences to the CPU chip. Different processors (like AMD vs Intel) have different specifications, and the same combination of 0s and 1s that makes sense to an AMD chip will not make sense to an Intel chip. This is referred to as binary input.

The ""higher"" you go away from the chip, the more human-readable the instructions become. After 0s and 1s there is Assembly, which essentially provides common shortcut commands for actual sequences of 0s and 1s for things like ""remember this"" and ""multiply this by that"" etc. that are specific to the processor type.  This lets a developer interact with any chip type (AMD or Intel) using the same commands, and translates those commands into the appropriate combination of 0s and 1s appropriate for the type of chip.

On top of that is C, which introduces logical concepts and control structures like loops and variables, as well as making it easy to import code from other people who have already solved common issues, like showing stuff on a screen and sending data to a printer. Above that is everything else. 

You may have heard the term ""compile"", which refers to translating instructions (code) from a higher-level language like C to the lowest level language, binary aka 0s and 1s -- and if you are understanding so far, you will deduce that the same C program compiled into AMD binary code will be different than the Intel binary code! 

You may wonder ""hmm the same EXE file in Windows works no matter the chip"" and that is true, because EXE files are not true binaries but rather Windows executables, where Windows actually interprets instructions from the program to the chipset on the fly! (no wonder all the slowness and crashes, right!? :) )

Also note that the terms higher-level and lower-level are relative... even binary 0s and 1s are technically higher-level than the actual electrical impulses they represent... and though I refer to C here as higher-level than binary and assembly, C is actually considered a low-level language because so many other languages were actually programmed using C! So, common languages like JavaScript and PHP AND Python are high-level languages, and C is relatively low-level compared to them!

Hope that helps!!",h9mecje,t3_p7nprt,1629431858.0,False
p7nprt,"Maybe this is a better way to put it. The code is compiled into binary, and these 1s and 0s translate to a high voltage or a low voltage, a low voltage opens a switch so electricity doesn’t get through, while high closes the gate and allows electricity through (this can be flip flopped but that doesn’t matter). This causes other switches to open/close and the resulting data is a series of switches that are read by the computer, and returned to the program. It’s pretty hard to picture how just that can create whole programs, I didn’t understand until I got into a class about how just these on/off switches can be organized into AND, OR etc, then that organized into things that can add, multiply etc, which keeps getting more sophisticated and further from binary, but at the very bottom, binary is all it is. Pretty crazy we can do so much just opening and closing switches.",h9lx6ct,t3_p7nprt,1629423090.0,False
p7nprt,[deleted],h9m3wwy,t3_p7nprt,1629426339.0,False
p7nprt,Came here to suggest this as well.,h9opdxy,t1_h9m3wwy,1629480191.0,False
p7nprt,You should read this book: Code by Charles Petzold,h9m99fu,t3_p7nprt,1629429067.0,False
p7nprt,This book is fantastic.,h9oqf6k,t1_h9m99fu,1629480608.0,False
p7nprt,"Code is built upon each other. Binary data is just a signal being off, 0, or on, 1. You. An perform and save calculations with these signals like adders and save memory states. A computer system is abstractions constantly built on top of each other.",h9n0icx,t3_p7nprt,1629448012.0,False
p7nprt,"I think the place to start is ""high and low level language"". Programming languages come in levels. The lowest level is assembly, which the CPU knows how to run but is hard to write for a human. A high level language, like python, is easy for a human to write and read, but can't be run directly on a CPU. It must be translated by a program called an interpreter or compiler to assembly for the computer to know how to deal with it. 

When you run your code with your interpreter, it turns it into these assembly instructions and loads that into the system memory. The CPU goes to the memory for the next instruction or piece of data that the program says it needs and the CPU handles the rest.",h9l11m1,t3_p7nprt,1629408475.0,False
p7nprt,"To be pedantic - the CPU does not run assembly.  An assembler is needed to convert assembly code into the proper zeroes and ones that the CPU operates on.

Compilers and interpreters sometime generate assembly, but most often they generate binary code directly, or some intermediate form.",h9lm8yr,t1_h9l11m1,1629417981.0,False
p7nprt,"Yes, sorry of course.",h9lmkyz,t1_h9lm8yr,1629418141.0,False
p7nprt,"So, an interpreter would translate the code into binary?",h9l1ynh,t1_h9l11m1,1629408857.0,True
p7nprt,Yes. Your interpreter or compiler creates the machine code in binary that runs your program,h9l7bdb,t1_h9l1ynh,1629411129.0,False
p7nprt,"Ok, so it ""translates"" the code into binary, great. but how does it read that binary and ""know"" that 01000001 means a, or 00111101 is an equal sign? and then output it into a screen?",h9l9nsu,t1_h9l7bdb,1629412155.0,True
p7nprt,"It matters which binary you mean. So your source code is text, written in a format called ASCII. What's important there is that each character has a number tied to it. We just decided that 65 (or 01000001 in binary) was a lower case a.

Now the important thing is after it's been translated, you begin to create instructions and addresses. So I want to create a set of operations, like input, output and so on. So when I'm designing my assembly, I might say an input instruction is 100, and the last 2 digits can be the address my input is stored in. So 123 would store an input in address 23. These decimal numbers are converted to binary and stored like that in memory. Your CPU them knows what to do for each instruction.",h9lgn8y,t1_h9l9nsu,1629415343.0,False
p7nprt,"Characters are mapped to numbers by the ASCII standart. All your computer really sees is the number. In order to turn this into an A, you need a font, which contains the image of the actual A your computer will render. An image, of course, is also just a sequence of bytes. The computer then looks at the number, looks at the table mapping these numbers to images, and then sends this to the graphics card, which in turn converts this into a HDMI signal your monitor is able to decode, making it display the image. The one responsible for reckognizing this image as an A is your brain.",h9ldd62,t1_h9l9nsu,1629413828.0,False
p7nprt,"Every computer program is a sequence of instructions, so to say. The computer needs to read those instructions and make sense of it, considering that at a very low level, it only knows how to perform arithmetic and logical operations, so to speak. The missing link chain in this process is the compiler, which acts as a very big dictionary capable of translating your code into a tree-like structure, which, in turn, is ""very easily"" turned into computer instructions.

As very well put in the first chapter of the ""Structure and Interpretation of Computer Programs"", the key to understanding computation as a whole is the concept of abstraction. You create a procedure (mechanical or otherwise), name it and use it to build something more complex.",h9lryuu,t3_p7nprt,1629420649.0,False
p7nprt,"Check out Ben Eater on YT. He builds a basic computer from individual components, and demonstrates how the circuits are configured to 'understand' the instructions that are contained in the code programmers write.",h9ltmom,t3_p7nprt,1629421423.0,False
p7nprt,"I will give you a short answer. Imagine a simple computer with a simple processor. Nothing else.

Everything is going to be handled in ""words."" In old days, it was shorter. Now our ""words"" are 64 bits. So its hardwired to take in some words and auto send them to the program counter (PC). It doesn't know anything about those words and what they mean. It just knows at start up ""this word goes into PC.""

Controlling the PC is very important, because hardware doesn't know data from instructions.

Now, when a word is read from the PC register, it is hardwired to go into an instruction unit and it interprets it as a command. It looks up the 0's and 1's (which it can determine with logic gates) and then uses a hardcoded instruction set to do pre set actions based on that.

The instructions then are used to determine what the next words are, whether they are inputs to the instruction or not.

Control of the PC is  having ultimate control of someone's computer. If you look at architecture details, there is a lot of hardware stuff on privileges for this reason. 99% of software devs will never need to worry about that, but at that level, having the correct instruction get to the PC is important. You don't want everything able to have control of the PC, just the OS.",h9nh4vq,t3_p7nprt,1629460944.0,False
p7nprt,"Everyone has some good answer, but I just want to add it also has to do with digital logic as well. Voltages are turned into 1s and 0s and depending on your k-maps it will create a circuit of some sort.",h9o42mq,t3_p7nprt,1629471638.0,False
p7nprt,"The computer uses context to do the stuff you want it to do. If you press a key on your keyboard, the keyboard generates a number (the keycode) and sends it to the program, which knows that it is a keycode because it came from the keyboard.

The keycode then gets converted (by just mapping it with a dictionary) to another code ([ASCII](https://smeretech.com/wp-content/uploads/2014/02/better_ascii_table.jpg) for ease of explanation) and with this code (which your program knows is a character because it just converted it) you can do string operations.

With the print instruction, you ask the terminal to display it on the screen. The terminal knows how to display what you send it, because it assumes it to be a string with a specific encoding (ASCII again, or hopefully UTF-8). It then looks up the Typeface for that code and renders it to the screen by turning on some pixel on your screen.

So, context is everything. Otherwise, it's just 0s and 1s.",h9o6gob,t3_p7nprt,1629472616.0,False
p7nprt,"I recommend the crash course in computer science from PBS:

https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo

It goes over how we got to digital computing and is pretty entertaining to boot.",h9ob0c1,t3_p7nprt,1629474447.0,False
p7nprt,Check Ben eater vid https://youtu.be/yl8vPW5hydQ,h9oojk9,t3_p7nprt,1629479854.0,False
p7nprt,"There is a youtube playlist (probably many of them now), that start all the way from the bottom to the top.",h9liigi,t3_p7nprt,1629416219.0,False
p7nprt,"Check out Ben Eaters breadboard computer, specifically the episodes about the CPU control logic.",h9lx3qr,t3_p7nprt,1629423054.0,False
p7nprt,"The code you write gets translated all the way down to ones and zeros. The ones and zeros are then fed as electric signals to a cpu which is basically a chip that can perform the most basic operations like adding, subtracting, AND, OR, etc. 

See “Functions” in: https://en.m.wikipedia.org/wiki/Arithmetic_logic_unit. (the ALU is a core component of CPUs)


It is from these basic operations that everything you can do on a computer is built upon.

If you want a deeper understanding of how all of this comes together I highly recommend the following course: 

Build a Modern Computer from First Principles: From Nand to Tetris
https://www.coursera.org/learn/build-a-computer


Its a great resource to help demystify what the hell is going on inside a computer. It really helped me get a better intuition for how a high level language gets translated all the way down to binary and how the computer interprets binary and produces outputs in binary that are then translated all the way up to something we can understand. Its also a fascinating journey through the many layers of abstraction that are necessary for computers to feel so “user friendly”. Your question is a fascinating one. Happy learning!",h9m2tfh,t3_p7nprt,1629425801.0,False
p7nprt,"This is literally what a cs degree answers. Tough eli5. But we made a rock do things when we shoot it with electricity(cpu). We organized the things into patterns(circuits) and we designated certain patterns of things into actions(add this and this, store here).

Now we say do this billions of times per second, react to what humans do to you, and show the patterns on the screen. 

So human input (INput devices) create many many patterns of 1s & 0s in the cpu, which does things based on how we put the circuits together and then the (OUTput devices) screen, lights, motors which also have cpus in them react to the patterns of bits being supplied to them. 

It’s like if I say “first get bread, second go to the fridge, third get the jelly….” Same thing with computers we have just boiled down every possible action into some kind of binary math pattern, much like Morse code. It just happens incomprehensibly fast.",h9mdu68,t3_p7nprt,1629431568.0,False
p7nprt,That's the beauty of computers. They don't know what any of it means. Computers don't know anything about the code you write. It is simply a series of switches (transistors) firing off that produce some output that we then give meaning to. Same thing as all these letters and words you are reading. They have no meaning in and of themselves until an intelligent being sees them and gives them meaning.,h9mjpbb,t3_p7nprt,1629435050.0,False
p7nprt,"your computer has memory that stores data as 1s and 0s (high and low voltage states) 

there are electronic devices called multiplexers which take a combination of 1s and 0s as input and output a single 1 or 0 ( high low voltage) 

example (1001 0010) - goes to -> 1 (made up for demonstration)

there are electronic devices called de-multiplexers that take a single input 1 or zero and output a combination of 1s and 0s 

example 1-> goes to -> ( 1101 0001 ) 

the multiplexer is how the CPU ""reads"" a multiplexer (more than one actually) is used on a section of memory in the cpu if it ""recognizes"" a instruction it outputs a 1 

the output of that multiplexer is then fed into a de-multiplexer which operates actual electronic devices like turning on a printer or a screen vibrating a speaker ect 

humans don't read 1s and 0s very well or at least most don't seem to like to so they assigned words to patterns of instructions which is as I understand it what assembly code is. 

the code you type in to a computer to program is read by another program a compiler to take the instructions you gave into predefined patterns of assembly code and that is what your computer actually ""understands""",h9mkjdf,t3_p7nprt,1629435585.0,False
p7nprt,"
Hello! You have made the mistake of writing ""ect"" instead of ""etc.""

""Ect"" is a common misspelling of ""etc,"" an abbreviated form of the Latin phrase ""et cetera."" Other abbreviated forms are **etc.**, **&c.**, **&c**, and **et cet.** The Latin translates as ""et"" to ""and"" + ""cetera"" to ""the rest;"" a literal translation to ""and the rest"" is the easiest way to remember how to use the phrase. 

[Check out the wikipedia entry if you want to learn more.](https://en.wikipedia.org/wiki/Et_cetera)

^(I am a bot, and this action was performed automatically. Comments with a score less than zero will be automatically removed. If I commented on your post and you don't like it, reply with ""!delete"" and I will remove the post, regardless of score. Message me for bug reports.)",h9mkkge,t1_h9mkjdf,1629435603.0,False
p7nprt,ect,h9volc7,t1_h9mkkge,1629611695.0,False
p7nprt,The format the binary file is expected to be in is defined by whatever machine code that computer uses. The instruction decoder then copies the binary into the registers of the CPU depending on what instruction it is.,h9mu6ob,t3_p7nprt,1629442688.0,False
p7nprt,"1) you should probably read the book Code by Charles Petzold. Very approachable. 

2) you should probably read the About section of any subreddit, so you don’t accidentally post learning questions in a subreddit dedicated to advanced CS journal research.",h9lep20,t3_p7nprt,1629414437.0,False
p7e35j,"Well... yes... If you wrote a program that generated every possible 600x480 picture, then yes, sure, some of those would be pictures of you from the future. Some of those pictures would also be of you from the past, and of some of the those pictures would be of the birth of Jesus, and some of those pictures would be of the birth of Jesus but with Mary wearing a sombrero, and so on and so on. Every single possible arrangement of colours on the 600x480 canvas.

It's not terribly different to [infinite monkeys sitting at typewriters](https://en.wikipedia.org/wiki/Infinite_monkey_theorem)..",h9iznqz,t3_p7e35j,1629377646.0,False
p7e35j,"Certainly possible. 

Similarly, you could produce the next Kanye West album by randomly arranging bits. 

Extending the idea to atoms - assuming there is a finite number of ways to arrange atoms in a certain volume, there must be some arrangement that produces the fastest CPU physically possible.",h9j04l5,t3_p7e35j,1629377887.0,False
p7e35j,so this is why Kanye keeps pushing the album back over and over...,h9kgi21,t1_h9j04l5,1629400122.0,False
p7e35j,Probably fucked with a bit without knowing it and is struggling to figure out in the sea of 1’s and 0’s where the fuck he went wrong,h9m50u4,t1_h9kgi21,1629426891.0,False
p7e35j,How do you discretize space?,h9nimj4,t1_h9j04l5,1629461808.0,False
p7e35j,If it's a 1X1 image I bet I could do it for you 😉,h9j0vel,t3_p7e35j,1629378266.0,False
p7e35j,Yeah sure 😂,h9jbcxw,t1_h9j0vel,1629383094.0,True
p7e35j,"This has been done already in the form of The Library of Babel. Initially starting out as random text hashing, it now works with images.

https://libraryofbabel.info/

http://babelia.libraryofbabel.info/",h9kne3n,t3_p7e35j,1629402935.0,False
p7e35j,Yeah checked it out its cool AF,h9kycpd,t1_h9kne3n,1629407341.0,True
p7e35j,"For reference, there are less than 10\^100 atoms in the observable universe. If we assume a 600x480 image with 8bit rbg colors, that gives use 8x3x640x480 \~= 7.4 million bits. So there are \~2\^(7.4 million) \~= 10\^(2.2 million) possible images. So if you had every atom in the observable universe produce one image every femtosecond for 100 billion years you could make less than 10\^(130) images, leaving you with about 10\^(2.2 million) possible images left.",h9kpgcg,t3_p7e35j,1629403763.0,False
p7e35j,Sounds like this: https://en.m.wikipedia.org/wiki/Infinite_monkey_theorem,h9ki4td,t3_p7e35j,1629400795.0,False
p7e35j,"**[Infinite monkey theorem](https://en.m.wikipedia.org/wiki/Infinite_monkey_theorem)** 
 
 >The infinite monkey theorem states that a monkey hitting keys at random on a typewriter keyboard for an infinite amount of time will almost surely type any given text, such as the complete works of William Shakespeare. In fact, the monkey would almost surely type every possible finite text an infinite number of times. However, the probability that monkeys filling the entire observable universe would type a single complete work, such as Shakespeare's Hamlet, is so tiny that the chance of it occurring during a period of time hundreds of thousands of orders of magnitude longer than the age of the universe is extremely low (but technically not zero).
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",h9ki6pe,t1_h9ki4td,1629400817.0,False
p7e35j,"Beep. Boop. I'm a robot.
Here's a copy of 

###[Hamlet]( https://snewd.com/ebooks/hamlet/)

Was I a good bot? | [info](https://www.reddit.com/user/Reddit-Book-Bot/) | [More Books](https://old.reddit.com/user/Reddit-Book-Bot/comments/i15x1d/full_list_of_books_and_commands/)",h9ki7t8,t1_h9ki6pe,1629400829.0,False
p7e35j,Yes,h9kv8qi,t1_h9ki7t8,1629406079.0,True
p7e35j,"New insult acquired: ""A monkey has a higher chance of typing Hamlet than you _______"".",h9m9atg,t1_h9ki6pe,1629429088.0,False
p7e35j,It’s big brain time,h9kasw1,t3_p7e35j,1629397746.0,False
p7e35j,Indeed it is🤯🤯🤣🤣🤣🤣,h9kv2m7,t1_h9kasw1,1629406011.0,True
p7e35j,It’s basically that “if you give a monkey a typewriter he will eventually write all of Shakespeare’s plays”’or something like that.,h9ktdea,t3_p7e35j,1629405334.0,False
p7e35j,"It is possible to build a program that randomly outputs pixels on a 600x480 png. Only thing is that it would take a long time to output all the images. A 640x480 png image weights ~73,774 bytes. Doing some quick maths (and please correct me if Im wrong!) there are 2^586,672  possible ways to arrange all the bits of such png. If each second a computer would create and load each png file, it would take 2^586,619 ages of the universe for the program to complete its task. Again, if I did any midtake in this comment, please let me know! Did some quick maths plus some rough estimation on this one. ;)",h9kiyki,t3_p7e35j,1629401135.0,False
p7e35j,"> A 640x480 png image weights ~73,774 bytes.

If you're enumerating all arrangements of pixels, you'd want to consider the uncompressed size. Picking a handful of compressed examples is likely to underrepresent the size averaged over *all* possible images. For 24-bit color, 640*480*24 = 7,372,800 bits (921,600 bytes).",h9sdfpz,t1_h9kiyki,1629553271.0,False
p7e35j,Omg I had this same thought 2 years back at Uni and discussed with my professor.... the possibilities of generating my future partner to my rest is pretty incomputable I guess haha,h9l0ewh,t3_p7e35j,1629408207.0,False
p7e35j,"Here ya go

Each typical color can be defined as 0-255, 0-255, 0-255. Therefore per pixel, any given color has a 256^3 chance of occurring. A quick google search told me a 1080p monitor has roughly 2 million pixels. So, randomly generating any specific 1080p image would be (256^3 )^2mil . That’s gonna look something like 2.8 * 10^14444444. For reference, there are somewhere between 10^78 to 10^82 atoms in the known universe. But yes, every conceivable image is in there, most of it would be incoherent,  but a future you on a bear surrounded by a burning pile of money would indeed be one.",h9l6x6m,t3_p7e35j,1629410957.0,False
p7e35j,YEAH THATS A LOTTA OF NUMBERS!!!! I GUESS WE WOULD NEED LUCK THAT AMOUNTS TO ALL THE atoms in the OBSERVEABLE UNIVERE TO GET A PIC OF ME ON A BEAR SURROUNDED BY A PILE OF BURNING MONEY. AND EVEN THEN THERE IS A CHANCE THAT IN MY FUTURE I WILL BE RIDING A LION INSTEAD OF A BEAR!!!!!,h9ogk6h,t1_h9l6x6m,1629476665.0,True
p7e35j,"You don't need a random generator. Take a binary representation of π. Decide how many bits you want in an image (a frame), copy the bits inside the frame and open as an image. Then shift the frame one bit, copy the bits inside the frame, open as image. (Note that at some point you get the same image as before with just the first pixel missing and a new pixel at the end.)",h9jqprv,t3_p7e35j,1629389438.0,False
p7e35j,where is pi required? Sorry I am not that much familiar with the algorithms of computer science,h9jrbvh,t1_h9jqprv,1629389687.0,True
p7e35j,"π is simply a long string of non-repeating substrings of values. If you want to try out what kind of variation you get, π is a cheap way to get it.",h9jsv20,t1_h9jrbvh,1629390313.0,False
p7e35j,Oh I see thanks 😊,h9jzmkw,t1_h9jsv20,1629393074.0,True
p7e35j,"So has anyone ever tried writing this kind of program….. if we could make it like that everyone could volunteer and their computers would keep processing the images and save the best ones and send it to a central computer the time of getting a non random image would increase. Like while the volunteers computer worked it would passively keep producing the images and an ai would keep filtering out the totally ridiculous images and any sensible image would be sent to a central computer. Of corse the order of arrangements of all the pixel in all the images produced by the computer would be recorded so as to prevent the production of the same image on any other volunteer’s computer. 


I know it’s a really ridiculous idea but what do u think 🤔",h9jc9xf,t3_p7e35j,1629383487.0,True
p7e35j,"Why would humanity waste the massive amount of energy to randomly produce images to see what happens? And if you had an AI that could determine what is a sensible or not photo, why not use that AI to create the photos?

In actuality, there are dozens of AI programs out there that create faces that never have existed and some that can try to make a version of you. It’s because in computer science we rely on algorithms that make steps towards a goal, and in some cases use heuristics to get closer to that goal. In computer science, using randomized algorithms is nonsense (see bogo sort…).

Your idea of using idle time to not be idle was done long ago, though. A program called SETI used volunteer’s idle CPU time to analyze signals they got from space or something along those lines in the 80s. The goals was to find aliens. Pretty sure that’s still around.",h9jiv6c,t1_h9jc9xf,1629386262.0,False
p7e35j,Yeah I took the inspiration from seti program….. also if you didn’t see I already said it’s a ridiculous idea. I just wanted to know your opinions….😊,h9jpaxh,t1_h9jiv6c,1629388869.0,True
p7e35j,"All great accomplishments or impressive programs were once ridiculous ideas. Even if you never use this exact idea, doing some research into computer science and it's many faculties will only serve to make you a smarter person.💡",h9jgdt4,t1_h9jc9xf,1629385222.0,False
p7e35j,Haha thanks,h9l5kkt,t1_h9jgdt4,1629410373.0,True
p7e35j,Sounds like the most brute force thing ever,h9joyra,t1_h9jc9xf,1629388734.0,False
p7e35j,YES,h9kysl5,t1_h9joyra,1629407528.0,True
p7e35j,Check blood pressure. Sounds high.,hbvnqsn,t1_h9joyra,1630978403.0,False
p7e35j,"You should look into generative adversarial networks. It's kind of like what you're wanting, but more efficient than brute forcing every single possibility.",h9kikg5,t1_h9jc9xf,1629400974.0,False
p7e35j,I think that this idea is basically implemented in Generative adversarial networks.,h9k95nt,t1_h9jc9xf,1629397051.0,False
p7e35j,I’ll check it out,h9kv02p,t1_h9k95nt,1629405983.0,True
p7e35j,I think that's what Entropy is.,h9lojpf,t3_p7e35j,1629419063.0,False
p7e35j,"Entropy is going from order to chaos, soo yeah we could say its going from a frame of a perfectly ordered single colured pixels to a chaotic mixture of pixels",h9ofykf,t1_h9lojpf,1629476427.0,True
p7e35j,"The problem is that there are so many permutations within that photo.

In order to find a photo like that, you would need something that could search that enormous domain of every possible photo to find one that you are looking for, which sounds crazy but is actually possible. One way is to train a neural network to do this for you.

[Here is a video you might find interesting.](https://youtu.be/p_7GWRup-nQ)",h9kups1,t3_p7e35j,1629405869.0,False
p7e35j,OK Thanks a lot for the video recommendation,h9kykbw,t1_h9kups1,1629407430.0,True
p7e35j,"This is more probability and the definition of infinity, rather than computer science specific. If you gave an infinite number of monkeys with keyboards an infinite amount of time, eventually one of the monkeys will write all of Shakespeare.  Same with if you generated an infinite amount of “binary” strings, eventually when converted to png, you’d find every possible png combination in the set. The problem is in practice we don’t have an infinite amount of time to create binary strings for the image you want. Theoretically there’s a chance, but in reality your time on this earth to observe the perfect binary string is the limiting constraint.",h9lhpfi,t3_p7e35j,1629415842.0,False
p7e35j,"Wow didn’t read the other comments, looks like everyone thought of the typewriter monkeys.",h9livpm,t1_h9lhpfi,1629416392.0,False
p7e35j,Yeah everyone saying that .😅😅😂😂😂😂😂😂😂,h9lkk34,t1_h9livpm,1629417183.0,True
p7e35j,"Look at the concept of the library of Babel. However, it would take an eternity before successfully doing something like that, no matter how powerful your computer was.",h9lx3np,t3_p7e35j,1629423053.0,False
p7e35j,That’s sad…. Could quantum computer change this,h9mfybs,t1_h9lx3np,1629432774.0,True
p7e35j,No. It’s simply too much.,h9mi4xz,t1_h9mfybs,1629434076.0,False
p7e35j,I feel cosmic sadness. Whatever that means,h9mk3g5,t1_h9mi4xz,1629435300.0,True
p7e35j,Wow so many people replied thats amazing! i never thought so man people would consider giving opinion on something i wrote!!!!!!!!!! Thanks for giving your opinions ......!,h9ofq0f,t3_p7e35j,1629476336.0,True
p7e35j,"The probability of this is very very small, but it is definitely possible. But you can significantly improve the chances if you switch from a code based approach to data based approach. I.e. Make a database of current object and their future counterparts, and train a ML model ( CNN models will be ideal for this case). It would significantly better for your objectives.",h9uvooe,t3_p7e35j,1629595728.0,False
p79wtb,"All information is data.

Behaviours, relationships/communications and everything else you can imagine is data.",h9i90dh,t3_p79wtb,1629357736.0,False
p79wtb,"Data is just information. It can be browsing history to use to more accurately suggest search results the person is interested in. It can be information about how likely someone is to pay a loan back or how likely they are to fall through. It can be how often they are in car accidents to predict their risk in car insurance. It can be the items people buy on a weekly shop (if they buy cat food, they probably have cats, so you can then target cat-care products at them more successfully). Companies like Elliptic use data about distribution of cryptocurrency transactions within the blockchain to attempt to detect laundering and thus infer the likelihood that a crypto payment is funded by illicit activity.

Data is literally anything you can use to make suggestions or potential correlations to other information, facts, trends, or anything. It entirely depends on the company you work with. Fintech and insurance is going to massively differ to that for a massive supermarket chain, and that is going to differ to companies like Facebook, Google, Netflix who are more focused on making the content of their platforms more relevant to each individual using it.

Pretty much anything where you can make assumptions about something else using information.

Like, example. You work for BP or Shell, you can infer that someone drives an HGV by the fact that they repeatedly refuel with 100L of diesel  fuel each week. You can then target other products and offers towards them more suited to HGV drivers, such as food, drink, and money off future transactions to reward them being a regular returning customer. Doing this makes the likelihood of such campaigns much more likely to provide a positive additional income for the company as opposed to targeting a 70 year old who only gets petrol for their lawn mower once a month, where it would be less likely to be cost beneficial to invest in.

Data can also be used in security. You can make preemptive predictions that a DDoS attack is ramping up by analysing transactions-per-minute in a system. This needs to be carefully compared to just regular spikes in transactions that occur throughout the day or week (e.g. a bank is more likely to get internet banking traffic around 12pm each day rather than 12am). Doing so allows you to lock down certain gateways to protect vital services ahead of time before you get an outage. In fact, Cloudflare does this already.

I've worked on projects before where we used data regarding the electricity draw of household appliances to attempt to monitor the power supply to houses to help detect things like thermostats in refrigerators malfunctioning, which increases yearly energy bills and contributes to the climate crisis.

The possibilities are endless.

Disclaimer: I am not a data scientist, so my examples are not the best, but hopefully it gives some insight?",h9i9avx,t3_p79wtb,1629357981.0,False
p79wtb,"Literally everything. The foundation of CS is “data” structures and algorithms. How we hold and handle data, how we use it.

I’m terms of google, I guess websites browsed, search history, preferences and tendencies. What does data mean for a software engineer? Anything. Once again, literally everything you do is working with data.",h9kbs7m,t3_p79wtb,1629398154.0,False
p79wtb,"It means these companies extract and store user data. Their purchasing/browsing patterns, the information they put in while making their account, etc.

And then they use that data to improve their products. Like amazon comparing your purchasing habits with others and suggesting you products that someone with similar choice as yours purchased.

These companies also provide services to store and manage data too that people can subscribe to.",haa6cx1,t3_p79wtb,1629893047.0,False
p77d1u,[removed],h9iua3z,t3_p77d1u,1629374654.0,False
p77d1u,Thank you very much for your input. I shall have a look at those. 🙏,h9iz6hy,t1_h9iua3z,1629377394.0,True
p70cg9,No. A decentralized system is just a distributed system in which no one machine is the ‘leader’.,h9gdgch,t3_p70cg9,1629321272.0,False
p70cg9,"Layman here. Thinking distributed system means processing happens at node level, whereas decentralized just speaks to the overall admin of the system.",h9j1nog,t3_p70cg9,1629378657.0,False
p6w7dz,"Given the limited amount of info, it's hard to say. Performance can drop if you're spawning too many threads. Also if the amount of data items you're giving to each thread is too small, the overhead of spawning each thread may eliminate any possible gains that you could be getting. It could also be something funky with your code. One thing you can do is to try different thread counts and see which one yields the best performance gain.",h9frzzo,t3_p6w7dz,1629311639.0,False
p6w7dz,"Probably cache locality, but it's hard to say without profiling",h9wmw3g,t3_p6w7dz,1629638395.0,False
p6v7qh,"Technically the purpose of any network protocol is to transfer data, but I feel it isn't what you are asking about. Typical applications of SSH include remote command-line, login and command execution. For example, I sometimes want to use the software installed on my institute computers, so I use SSH to login into university network and remotely use the resources as if I was using my local terminal.",h9fg7z8,t3_p6v7qh,1629306418.0,False
p6v7qh,"Okay that definitely paints a better picture, so it’s basically used almost like a cloud platform, but it’s allows you to access software installed on another PC from your own PC to use?",h9fh0t0,t1_h9fg7z8,1629306773.0,True
p6v7qh,"More that you use that software remotely. If you use something like AWS and have a cloud server, you will connect to that server from your own computer using SSH to use commands in the terminal on the server",h9fqugh,t1_h9fh0t0,1629311120.0,False
p6v7qh,"I'd say its more like using a remote control for your TV (SSH connection to the device) vs pushing the buttons on the side (Using the device locally). Everything command you execute is being executed on the machine you're connected to, as if you were there using it.",h9ftd51,t1_h9fh0t0,1629312257.0,False
p6qfmu,I think doing the attacks is way more useful then learning the theory (and it helps the theory stick). I would suggest setting up some virtual machines to attack or going on hack the box. The OWASP dvwap was where I started and I’ve heard good things about vuln hub.,h9el8wd,t3_p6qfmu,1629292028.0,False
p6qfmu,"Yea I see where you're coming from, but I'm also talking about more technical stuff like, setting up a NAT network using Cisco commands, troubleshooting stuff, things like that.",h9ensv8,t1_h9el8wd,1629293375.0,True
p6qfmu,"Fair, I don’t really know the Cisco stuff and all of my experience with NATs and firewalls have been from messing with my home lab. You could always try to implement the network protocols if you really wanted to. Otherwise I don’t think I have great suggestions. Maybe some of the certs, but idk. 

Good luck",h9eog5d,t1_h9ensv8,1629293702.0,False
p6qfmu,Look at the CompTIA network+ book. It's not real expensive but covers all that technical ground. Also check out the MIS classes over in your business school (if you have one).  Networking is typically covered there.,h9f2c1q,t3_p6qfmu,1629300263.0,False
p6qfmu,This is also a well-regarded course:  [https://www.professormesser.com/network-plus/n10-007/n10-007-training-course/](https://www.professormesser.com/network-plus/n10-007/n10-007-training-course/),h9o9w2z,t1_h9f2c1q,1629474001.0,False
p6qfmu,"You can either go on netacad.com and buy the course, there you have a lot of packet tracer exercises which helped me a lot or you can go to itexamanswers.net where you can get them for free.
All in all just do packet tracer exercises. It's kinda hard to play alone with it because you might not know everything and how to setup certain protocols. 
Ps: sry for my english, not a native speaker",h9eqls9,t3_p6qfmu,1629294772.0,False
p6qfmu,"Looks helpful, ill check it out. Thanks.",h9f3o9j,t1_h9eqls9,1629300873.0,True
p6oybv,"Think of it like this, your car has an overall architecture. It has wheels, it has an engine, it has seats, all of which have their own designs. The engine of the car itself also has its own architecture, maybe it's a boxer engine, or a v8, etc... but these two different engines are still in an architecture of a car overall. 

These are totally different levels of design. Von Neumann describes the overall computer build, whereas zen refers to the CPU architecture.",h9ezm0f,t3_p6oybv,1629299024.0,False
p6oybv,"No, these architectures are all Von Neumann Architectures, but there's a lot of stuff on top of that these days.

For technologies like AMD Zen, architecture includes what pipelines, caches, instruction registers and buses exist (and much more).

Compare this:
https://en.wikipedia.org/wiki/Von_Neumann_architecture#/media/File:Von_Neumann_Architecture.svg

To this:
https://en.m.wikipedia.org/wiki/Zen_(first_generation_microarchitecture)#/media/File%3AZen_microarchitecture.svg

These days, to computer hardware engineers it's often the question not only where and how conceptually the data goes through the integrated circuitry of a processor, but also what happens inside with it. Things like microcode and branch prediction, as well as memory, caches and instruction prefetch strategies are highly relevant for performance and also security, but technically still are all Von Neumann Architectures.

Marketing and giving things fancy names also plays a role.",h9edtv2,t3_p6oybv,1629287472.0,False
p6oybv,"The term ""architecture"" in computer hardware can be a bit confusing because it can mean a number of different things, on different levels (as u/suckmacaque06 said).

The von Neumann architecture describes a rather general structure of a computer that all current computers (that I know of) follow. It doesn't go into the details of a CPU at all. It doesn't even tell what a CPU is capable of in terms of the code it can run.

Then there's something called an instruction set architecture, which defines the kinds of data types, registers visible to programs etc. that a CPU knows, and the kinds of instructions the CPU can execute and how they behave. This is what you'd write or generate machine code for. Examples of instruction set architectures would be x86, the 64-bit x86-64, or the ARM instruction set architectures. The ISA defines the external functionality of a CPU and what kind of code the CPU can be expected to be able to execute.

The AMD Zen would be an example of what's often called a microarchitecture, and it's basically a vendor-specific general design of *how* a particular physical CPU implements its instruction set architecture. The microarchitecture is internal to the CPU in the sense that regardless of microarchitecture, for example a specific instruction from the x86-64 instruction set should still get you the same result and behave the same way whether you run it on a processor with Zen microarchitecture, an older AMD CPU that has a different design, or an Intel CPU that might have an entirely different internal design. However, the microarchitecture can have significant effects on the performance of a CPU, which is why there's a big buzz when a CPU company announces or starts using a new promising one.",h9fhd5b,t3_p6oybv,1629306927.0,False
p6oybv,"In the sense of your original question, it is a modified Von Neumann, but so are all x86 CPUs.    


Von Neumann, IIRC, just indicates that the Programs and Data storage share the same memory space, unlike the Harvard Architecture which essentially had a ""Program"" and ""Data"" storage separately.  


As noted above, This is more a minor technical & marketing scheme.  An 8088/8086, 286, 386, 486, Pentium, iSeries on the Intel side of things indicate various evolutions of this general design.    


One thing to note, is that the modern systems aren't ""pure"" Von Neumann, however, as the NX Bit/Flag (No Excecute) provided a way to mark data as ""data only"" (and not as a ""program""), that is, it created a sort of hybrid Von Neumann/Harvard Architecture, that helps prevent data from being executed (and thus a source of an exploit).  A pure Von Neumann architecture that doesn't do that would allow random data to be executed like a program.  


Anyways, AMD chips, whether K6, Athlons, Threadrippers, Zen what have you are also, because they're modified x86 chips, also feature similar technologies and architectures.  


In the end, the features added over the years (whether MMX (another marketing term for Intel's version of ""Single Instruction Multiple Data"" features - or ""Vector"" processing). or other enhancesments through the years, also play a role.  But these aren't necessarily related to whether something is Von Neumann or not (I don' think?)",h9ff49c,t3_p6oybv,1629305928.0,False
p6oybv,"so amd zen is an implementation of the ""x86\_64"" instruction set. This instruction set can be interpreted as a ""Von Neumann architecture"". An ISA or Instruction Set Architecture is a set of instruction a CPU must understand to be compliant. Code is compiled to these instructions so that the cpu can execute them. AMD Zen is amd's implementation. They make certain tradeoffs in order to hopefully execute more of these instructions per second. Intel CPUs understand the same instruction but calculate the result differently. 

&#x200B;

All of these CPUs can be understood academically through the lens of a  ""Von Neumann architecture""",hc5sp5t,t3_p6oybv,1631177349.0,False
p6bc0r,"If you have the center of the circle and the point, you can use the Pythagorean theorem to get the distance between them and make sure that is less than or equal to R.

Unless I'm misunderstanding and your question actually is ""Where do I place the circle on the grid to capture the maximum number of points""",h9bwi2x,t3_p6bc0r,1629231983.0,False
p6bc0r,"I think the author is asking, given a disk radius and an evenly spaced grid, to find the maximum number of grid points fitting in such a disk (not necessarily centered at the origin)",h9bzq9b,t1_h9bwi2x,1629233317.0,False
p6bc0r,"Well, the place where the circle is **does** determine how many points are there

But the question really is ""what's the maximum amount of points i can fit within a circle of radius R?""",h9c0lw7,t1_h9bwi2x,1629233679.0,True
p6bc0r,"Can the optimal center not fall in one of those:
- on a point
- between two points 
- between four points (middle of a square) ?",h9c1ha7,t1_h9c0lw7,1629234041.0,False
p6bc0r,I did some visual test on MS Paint and it seems the optimal center is on top of a grid point,h9c3uv0,t1_h9c1ha7,1629235054.0,True
p6bc0r,"For instance, with a radius 1/2 you can encompass 2 points (middle of them), and with a radius sqrt(2)/2 you can encompass 4 (center of a square)",h9c4d33,t1_h9c3uv0,1629235272.0,False
p6bc0r,"Ok so the best way to do that is to make the circle a square. Given a square that's 2R on each side assume that the first point in the grid is on the corner and then plot all the other points from that. If your square is 2 units long (R=1) and points are 3/4 units apart then you can fit 3 points on one side, so you get nine points total in the square. 9 in the most points you could ever fit inside the circle. But it's unlikely that you get all 9. The question is now how do I optimally place the point grid, because you have .5 units of room in each direction that you can move the grid. My guess would be that you always center the grid of points in the square, having .25 units of extras room on each side in this example. Generalizing this, if you can fit an odd number of points inside 2R the circle will be centered on a point, and if an even number of points then the circle will be in the middle of 4 points in the grid. Then you just need to use the distance formula to figure out how many of the points in the square are inside the circle.",h9cnq6l,t1_h9c0lw7,1629244439.0,False
p6bc0r,Looks like this problem: https://en.wikipedia.org/wiki/Gauss_circle_problem,h9d64al,t3_p6bc0r,1629253846.0,False
p6bc0r,https://en.wikipedia.org/wiki/Gauss_circle_problem,h9d95t9,t1_h9d64al,1629255468.0,False
p6bc0r,https://en.wikipedia.org/wiki/Gauss_circle_problem,h9dtz11,t1_h9d95t9,1629270027.0,False
p6bc0r,"Holy shit this seems to be the exact problem

And the solution seems to be way too complex and not even solved

Maybe ill write a hacky way, to solve through interations, or approximate the solution

&#x200B;

Thanks man!",h9g4c25,t1_h9d64al,1629317181.0,True
p6bc0r,"Well, you can get an approximate answer in O(1) time since the number of grid points will be very close to the area of the circle for large radius.

You can get an exact answer by iterating from 1 to + floor(r) and summing (4 \* floor(sqrt( r\^2 - x\^2)) + 1) .   I've left out the zero term, since it doesn't get doubled, but you have to add that back in.

I'm not sure if there's an O(1) way to get the exact answer.  It seems there should be, but I don't see it.

EDIT: I've assumed that the circle is centred on the origin and the grid points are those with integer indices, since your question sort of implied that.",h9bwppk,t3_p6bc0r,1629232071.0,False
p6bc0r,"I am pretty sure i can get the answer quickly, just by having the radius R size and origin, and the space X between the points. So the cost doesn't scale with R nor X

Not sure if there has to be a point within the center of the radius, all i need is to fit the **maximum** amount of points within a radius",h9bzwlc,t1_h9bwppk,1629233388.0,True
p6bc0r,I compute circle points using a modified bresenham algorithm and compute the interior of the disk by performing scanlines of half the disk size. Seems to work for all diameters.,h9dtkhy,t3_p6bc0r,1629269673.0,False
p6bc0r,"When you find your method, you could consider using quad trees for speeding up the querying. I've done something similar in the past.",h9e23fi,t3_p6bc0r,1629277669.0,False
p6bc0r,[deleted],h9bwxch,t3_p6bc0r,1629232160.0,False
p6bc0r,"This has nothing to do with the question.
It was about a grid of points.",h9bz2u3,t1_h9bwxch,1629233048.0,False
p6bc0r,"Yeah but these are distributed like a sunflower, not a grid

Still useful tho, ty",h9c3epa,t1_h9bwxch,1629234860.0,True
p68ws6,"Have you ever taken a computer architecture class? If so, what assembly language did you guys learn?",h9bg863,t3_p68ws6,1629225163.0,False
p68ws6,"No I did not, unfortunately. I have one next year though lol",h9bhmcc,t1_h9bg863,1629225766.0,False
p68ws6,I didn’t even learn an assembly language in my computer architecture class or any programming language. I’m only a sophomore so maybe it’s later down the line.,h9buxfh,t1_h9bg863,1629231341.0,False
p68ws6,Yup. Assembly language for 8086 and 8085 micro processor. I am planning to learn nasm prob after 6-7 months.,h9f0e66,t1_h9bg863,1629299386.0,False
p68ws6,"It's kinda hard to say without more context from the project requirements. This would be a good question for your TA.

Usually when people talk about computer architecture, they mean the CPU and possibly some details about it. It's entirely possible they just want something like ""experiments were done on an Intel I9 at 3.2 GHz with 8 hyper threads"" or something like that. If it's relevant to the project, you might mention cache sizes, how much DRAM you had and maybe some other details like that. even for academic research, if we aren't modifying the CPU or relying on some special feature, we don't go into much detail beyond the model and memory capacity.

If you aren't being taught computer architecture stuff in this class, it's unlikely they want more details than the CPU model, clock rate, and DRAM size. But you should definitely clarify with the course staff.",h9bn3yu,t3_p68ws6,1629228108.0,False
p68ws6,"At the minimum, CPU model, number of cores, GPU, memory, hard disks, ssds used by the computer you used to run the programs. Etc...",h9brb9e,t3_p68ws6,1629229869.0,False
p6864d,That covers most of approximation algorithms known. The question is too generic.,h9bqzww,t3_p6864d,1629229740.0,False
p6864d,Well if u can give the names of some of the ones you know that would be great,h9bzkw2,t1_h9bqzww,1629233257.0,True
p6864d,Use a search engine...,h9c9fbi,t1_h9bzkw2,1629237516.0,False
p5zcf0,"I hope someone disagrees with me on this. 

I hate, hate htdp. I felt like it was a huge waste of time for me personally and I went thru both the simple and complex data lectures. 

Everything about htdp is needlessly verbose and most of it is common sense. I would recommend just watching the lectures on 2x playback speed. There is also a cheat sheet on the course site that condenses the information. I wouldn't recommend touching the book. I'm not sure I would even recommend doing the programming challenges in the course. Just watch the videos.

If this is your first programming MOOC, your time would be better spent on CS50 and MIT 6.00.1x & 6.00.2x.",h9b0hia,t3_p5zcf0,1629216704.0,False
p5zcf0,I’m currently working on CS50 and it has been  really interesting.,h9bvcqo,t1_h9b0hia,1629231514.0,False
p5zcf0,"I'm glad you are enjoying it! David Malan changed my life haha.

If you like CS50, you are going to love 6.00.1x which I highly recommend you take after CS50. It's not a repeat of CS50. It is more in depth and equally fascinating.",h9gb3o3,t1_h9bvcqo,1629320196.0,False
p5zcf0,Awesome! I will have to check it out.,h9gd9zj,t1_h9gb3o3,1629321192.0,False
p5zcf0,"I have been working through the book (I suppose half an hour a day or so semi-consistently) for the past couple of months or so. I am just linearly reading through the book, and working through all the exercises, making notes on my thoughts. So standard stuff (no videos cause I just don't have the patience for videos). 

I have professionally programmed for several years (mentioning this because it biases my review). My goal in reading this book was to learn a Lisp dialect and also figure out what all the fuss was about in these books (HTDP, SICP etc) as a non CS major. 


I am currently at exercise 109 (so roughly 1/5th of the exercises) and in Chapter 6 (so roughly 1/6th of the chapters).  It has unfortunately, most recently been a bit of a slog. The exercises are not hard, they are just tedious. Also for whatever reason there is a ton of emphasis on graphics (so far) which for me are not my cup of tea. 

The material (and this might be a function of my background) varies from being truly enlightening (in that a lot of cruft in several programming languages, e.g. optionals, type safety etc. start making way more sense) and boring. From that perspective, I think I wish this had been my first programming book, I spent so much time learning programming via learning programming languages and it is easy to fall into the trap of confusing the presentation that a programming language imposes on you with the concepts that are language independent (i.e. syntax vs semantics).  From that perspective, this (perhaps it is true for all Lisp based books) style of teaching is refreshing. 


Let us see..I will try to get through more of the book and see how it changes my thinking or I might switch over to SICP.",h9bolx8,t3_p5zcf0,1629228744.0,False
p5zcf0,I think that SICP will be a better choice for you since you already have experience in programming in the real world.,h9durq8,t1_h9bolx8,1629270749.0,True
p5pekj,I miss devon crawfords vids :(,h9924h6,t3_p5pekj,1629174690.0,False
p5k0ns,"Jump on geeksforgeeks and start reading some code and introduction to Computer Science topics. Once you have some basic concepts down and have read through some code head to leetcode and start tackling some easy problems. 

Wish I took CS in high-school! I didn't learn to code until my 2nd year of college.",h96zm3u,t3_p5k0ns,1629141166.0,False
p5k0ns,"That https://www.geeksforgeeks.org/java/ looks like a really good place to dig in. Thanks for the pointer there. 

I guess from there https://www.geeksforgeeks.org/data-structures/ would be good. Then maybe poke around in the easy section and move up here?: https://leetcode.com/problemset/all/?page=1&sort=ASCENDING&order=DIFFICULTY",h97nv58,t1_h96zm3u,1629150813.0,True
p5k0ns,Absolutely. Start with arrays and strings to get a good grip on algorithms... don't be afraid to look up answers and always revisit old problems! Slowly move up the ladder in difficulty! No rush!,h982aec,t1_h97nv58,1629157155.0,False
p5k0ns,try to steal chairs from the vacant desks around you. My record was 4 but I only did the test twice,hc5st8d,t3_p5k0ns,1631177457.0,False
p5k0ns,"as far as I remember you implement a few classes and functions in java so you can practice that. also i think theres a fair number of ""gotcha!"" questions about syntax in for loops and stuff so practice that too",hc5swfv,t3_p5k0ns,1631177547.0,False
p5i8du,"Wow, this was… a lot. I don’t know if you quite comprehend the behemoth of a task that is operating systems development. Many have tried it and almost all have failed. There are countless examples of operating systems being created and fizzling out, starting in the 70s and 80s and continuing to this day. There is a reason MacOS and Windows dominate the desktop market, they have multi billion dollar companies behind them. And Linux, by virtue of its popularity in the server space, also has multiple multi billion dollar companies working on it (not to mention the humble OS devs contributing around the globe).",h96p5c4,t3_p5i8du,1629136963.0,False
p5i8du,So you’re saying a person without million dollar backing couldn’t create a hot item from the ground up ? U mean million dollars for research and development or just that some company is “backing them” what exactly do you mean by backing them ?,h97cb0u,t1_h96p5c4,1629146164.0,True
p5i8du,"No, you definitely can. Projects like SerenityOS have proved that. 

But if you wanted to compete with Windows, MacOS, and even Linux, you would need a tonne of money to pay for development costs.",h97ed7l,t1_h97cb0u,1629146967.0,False
p5i8du,Why tho - if it’s just an OS? And not a computer to go with it ? Well I mean Windows isn’t a computer with it it’s an OS installed on 3rd party ones but Apple products carry the Apple OS. Is what I meant . But why would this cost so much? Why could a programmer not accomplish this? Without millions? Just one person themselves… or a team … why would it NEED millions ? I just don’t understand what that would be needed for that’s why I’m curious,h97fxbw,t1_h97ed7l,1629147578.0,True
p5i8du,"Again, you absolutely can accomplish that. Look into a project called SerenityOS.

That being said, there is a reason you’ve never heard of SerenityOS. I don’t think you understand how complicated an operating system is. In order to compete with Windows, MacOS, or Linux, you will need millions of dollars to pay for development, testing, marketing, legal fees, support costs, management, etc. One person, or even 100 people, will never be able to compete on Microsoft or Apples level. It just won’t work. 

Also, I recommend studying what an operating system actually is. It goes way deeper than a desktop environment and some tooling. Saying it’s “just an OS” is a huge understatement.",h97gxpu,t1_h97fxbw,1629147976.0,False
p5i8du,"I know what an OS Is… by definition… I may not grasp the complexity of it but I’ve read and know what it is, I self study computers, but I’m still disabled so sometimes my mind doesn’t integrate things so well, although ya I def no wat it is.",h99f40v,t1_h97gxpu,1629183630.0,True
p5i8du,"Well self studying is definitely a good start. I recommend reading “Operating Systems: Design and Implementation”. And check out SerenityOS’s main developers YouTube, Andreas Kling. I think you’d be interested in his content and seeing how much work building an OS is. Get some different virtual machines running and pick apart different systems to see how they work, what breaks them, etc. There is a lot to wrap your head around there but it’s a fascinating subject.",h9asidp,t1_h99f40v,1629213549.0,False
p5i8du,You is gotta be compatible with al the diff computer requirements now think about how many computers are out there and how much they differ. Do this task for each,h9agh9l,t1_h97fxbw,1629208474.0,False
p5i8du,"The real question is why would anyone build a new OS from the ground up?

This battle is lost before it's even started.

To have a slight chance, an OS must have both quality applications and hardware to run on.

Why would companies and developers build softwares for an OS that no one uses (yet)?   
Why would a company start building an OS without any applications to run on it?

A new OS would not be lucrative at all. Why does Apple give the OS for free and Microsoft for less than 150$? (but really, who buys a Windows license for a personal computer?)

Because their business model is not selling OSes.

Apple sells/rents contents (Apple Music, Apple TV),  applications (Appstore) and hardware.   
Microsoft sells software to companies and bundles cheap OEM Windows licenses with computers. (And cloud and gaming)

A new OS would make money from, what?

I would not buy another OS that doesn't support all of my hardware, that doesn't allow me to play the few games I play, that doesn't let me install the softwares I want/need.   
Would you? How much would you be willing to pay for that?


If there were any profit to be made from it, someone would have already done it.


Think about Linux.   
The applications are pretty much usable on any distro, there are hundreds (probably more) of distros, the vast majority are free, easy to install and use.

What's Linux marketshare on PCs?   
About 2%

If it was easy to make an alternative to Windows and Apple, Linux would have more users.",h98h6gf,t3_p5i8du,1629164051.0,False
p5i8du,"Well our current operating systems are so counterintuitive our government is counterintuitive the way I see it the majority of society has lives that are complicated because they’re not smart enough to see how simple it is I’m not saying that applies here but I’m saying that it applies in almost every aspect of life that I’ve been aware of since the beginning I’ve seen it recurring lately that things are so simple but everybody else thinks it’s complicated and I’ve heard other smart people say the same thing so you know maybe it’s true. And maybe no one has thought of a new OS yet, cuz they just never had that idea. just like our government hasn’t realized many flaws in what they’re doing that could easily be altered just a bit, to make society so much better and our country so much more resources SIMPLY — I’m not gonna get specific cuz it would me require explaining alternatives thoroughly and I don’t have time but 

Yeah I’ve looked into the government things I haven’t looked into the OS thing yet .. But I am on well on my way. I’m in school - self study for various subjects including computers.. Anyways, I’ve looked into many things and what I said seems to be true. About simplicity, But I have no one to verify it to so I just have to see for myself. Cuz it sure seems to add up, but I don’t kno until I prove it myself 

Cuz the “facts” I was given in analysis may not be actual facts or were written by someone who’s mistaken / not thinking clearly PLUS , the main factor is: I still have a disability that distorts my perception at time , but the facts and deductive reasoning with many things looked into seem to indicate that things are not that complicated , 
People make it complicated 
The answers / solutions are usually simple .
The harder part is working with others if collaboration or negotiation is necessary

My disability is healing , it’s almost gone .. so I’ll know better soon, but even then until I see changes happening I haven’t seen proof (of what I’m saying) in other words unless i prove it for myself after my disability heals it’s JUST A THEORY however it seems entirely Obvious 

But when I say prove for self I’m not saying I’m design OS I’m talking about testing other things
And maybe an OS, but I don’t have more time to explain but Ya I g2g .",h99gqnj,t1_h98h6gf,1629184908.0,True
p5i8du,"Here's an alternate way of thinking.

It is estimated that about 100 billion humans have lived since the advent of humanity, about 52,000 years ago. They've lived and built societies that are thriving in many places around the world.

You are alive so you are less than 100 years old. You, a minuscule fraction of humanity, have figured out pretty much everything that the your fellow humans, both contemporary and past, have not. Do you truly believe so?

I dare say you are exhibiting the classic Dunning-Kruger Effect bias. I summarize it as ""you don't know enough to realize you know nothing"" and hence, you believe that your have found solutions to problems that you can't even comprehend.

That doesn't mean you're stupid, not at all. We all suffer from this bias, including me. I sometimes believe I've figured out better ways to work in my company, despite being but a tiny cog in a 150 years old, 50,000 employees community. But sure, me with my very simple and limited view on things, I must have figured out what no one else has because I'm so much smarter. Right?   
I wish but no, it doesn't work that way.


Humanity and the societies it built are much more complex than any of us can comprehend. Beware of yourself when your mind tells you you know better and you have figured it out.   
I don't even need to know your solutions to know that they are bad, or rather, that they have an incredibly high probability of being so.

Nothing is simple on this good old planet Earth, absolutely nothing. Believing otherwise is a mistake.   
But you're young, you'll learn otherwise as you grow up 🙂",h99jdhy,t1_h99gqnj,1629187042.0,False
p5i8du,As I said there’s no proof until I prove it for myself,h99js5u,t1_h99jdhy,1629187373.0,True
p5i8du,"Well, at the time I started writing you had not yet edited your message 😛

But regardless, proving it to yourself doesn't mean much.   
What you must do is print others, people whith more knowledge and experience then you. When you've proved several of these people, then maybe you have something that makes sense.",h99kazg,t1_h99js5u,1629187802.0,False
p5i8du,And I’m not young btw,h99k5zl,t1_h99jdhy,1629187689.0,True
p5i8du,I think you hit the nail on the head with “not knowing how much you don’t know”. The day you overcome that and realize you actually have no clue what’s going on is when you can finally begin to learn.,h9asz25,t1_h99jdhy,1629213735.0,False
p5h6qh,Cool! Thanks for the read!,h96bbds,t3_p5h6qh,1629131620.0,False
p5h6qh,I only see a single photo instead of an album. Is there more?,h96tjqn,t3_p5h6qh,1629138730.0,False
p5h6qh,sorry no I originally was just sending it to a friend who also does programming. The conclusion was basically saying how you shouldn’t charge based on CPU seconds since they’re inconsistent. I’ll try to find the article again when I go back to his house,h96unu5,t1_h96tjqn,1629139181.0,True
p5etup,"Depends on what you mean by ""CS"".

For various reasons, lots of people equate everything related to programming, software development and basically any non-layperson computing knowledge with computer science. Without more context it's hard to guess what you'd actually like to learn.",h96szjz,t3_p5etup,1629138505.0,False
p5etup,"From total basics how exactly computer works, networking, data bases etc.. i really enjoy learning to code but i want to know more about computers to not hit a wall after a while :)",h97hsxm,t1_h96szjz,1629148325.0,True
p5etup,The Crash Course computer science series on YouTube is also pretty good as an introduction.,h98t81j,t1_h97hsxm,1629169840.0,False
p5etup,If you want to try a book get one called “ A c++ crash course “ I’ve thoroughly enjoyed it even though it’s not a fun book it definitely goes through what each step/thing means and example problems to do with the book and on your own. I’d definitely give it a shot,h95cfzy,t3_p5etup,1629116373.0,False
p5etup,Harvard's CS50 course on edx.,h95gpmm,t3_p5etup,1629118665.0,False
p5etup,One stop shop = https://teachyourselfcs.com,h95kgvo,t3_p5etup,1629120512.0,False
p5etup,Jump on geeksforgeeks and start reading some code and introduction to Computer Science topics. Once you have some basic concepts down and have read through some code head to leetcode and start tackling some easy problems.,h96zvmh,t3_p5etup,1629141276.0,False
p5bcx1,"Robert.Sedgewick Philippe.Flajolet - Analytic Combinatorics is a good reference on the topic. It structures/unifies complexity theory.

You need high math proficiency to work through book.

And you have to practice exercises, the book has MOOC and there are exercises: https://www.coursera.org/learn/analytic-combinatorics",h94xnyi,t3_p5bcx1,1629105865.0,False
p5bcx1,"All I can think of right now.

https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/lecture-3-insertion-sort-merge-sort/",h952765,t3_p5bcx1,1629109561.0,False
p5bcx1,https://runestone.academy/runestone/books/published/pythonds3/AlgorithmAnalysis/toctree.html,h95lykx,t3_p5bcx1,1629121222.0,False
p5bcx1,"https://youtube.com/playlist?list=PLDN4rrl48XKpZkf03iYFl-O29szjTrs_O

Try this!",h96gwzl,t3_p5bcx1,1629133780.0,False
p55w4v,"What you said is true, if the number of edges is greater than or equal to the number of nodes in an undirected graph, there is a cycle. If you make such an observation in any given graph, you can confidently say there is a cycle. 

However, note that the reverse is not true - number of edges being less than the number of nodes does not imply the absence of a cycle. As an example, take a disconnected graph where node A and B are connected, and node C, D, E are connected. The number of nodes is 5 and the number of edges is 4, but there is still a cycle (connecting node C, D and E).

What this means is that, if you were asked to design an algorithm to detect a cycle in an undirected graph, whenever you see that the number of edges is greater than or equal to the number of nodes, you can say there is a cycle, but it's ambiguous otherwise. This is why you would need disjoint sets or some graph traversal algorithm to detect a cycle in a graph - your observation alone isn't sufficient.",h9dn4z7,t3_p55w4v,1629264344.0,False
p55w4v,Aha! I knew there was something I was missing. Thank you.,h9g8dg9,t1_h9dn4z7,1629318975.0,True
p55w4v,quickunion and a hash table,hc5t0tm,t3_p55w4v,1631177670.0,False
p50qk0,"In short, everything uses pixels for display. Rasterized images, however, use fixed pixel colors and locations, while vector drawings use math to draw a shape. This means as a vector image is increased/decreased in size, it can be redrawn to produce a more accurate rendition. 

Rasterized images can’t be resized as easily",h92n4tv,t3_p50qk0,1629059360.0,False
p50qk0,"> In short, everything uses pixels for display

This isn't entirely true. A lot of the early vector graphics were drawn directly to the screen via the cathode ray, much like an oscilloscope, there was no attempt to rasterize /quantise the vectors 

It's only on modern screens based on pixels do we need to convert the vector lines into raster ones.",h9394al,t1_h92n4tv,1629069174.0,False
p50qk0,"Correct. For the curious, check out the [Vectrex console](https://en.wikipedia.org/wiki/Vectrex?wprov=sfti1).",h94p09b,t1_h9394al,1629098576.0,False
p50qk0,"This is technically correct, but for all intents and purposes totally irrelevant.

Nice work, and I genuinely mean that.",h950zcb,t1_h9394al,1629108604.0,False
p50qk0,"I'm just a big fan of Battlezone and won't have people erasing it from history!

https://youtu.be/zdfKy4c7yuc&t=400",h955gye,t1_h950zcb,1629111994.0,False
p50qk0,"The responses here aren’t wrong but just to elaborate a bit further beyond the short answer of: ‘Vector graphics ultimately use “math” instead of pixels to describe the shapes.’

You’re likely familiar with the math necessary to describe a straight line in 2D space: ‘y = mx + b’. If you were a program, you could be given the parameters of this equation and be able to reproduce the line that it describes.

Diving deeper: higher power Bézier curves are more complicated than a simple line and allow for more intricate shapes. A vector graphics program like Adobe illustrator provides “bezier handles” that you can visually manipulate to adjust the parameters of the curve you’re trying to describe. It does all of the math for you in the background and when the image is saved, what it’s saving is all of the parameters and math that describes the image. When opened by a program that can interpret this data, the exact same shape(s) can be reproduced. 

Additionally, because these shapes are represented by math and not pixels, linear scaling can be applied to these mathematical parameters to “theoretically infinite” scales without loss of detail in how the image is described. So a program can reproduce the image at any given scale and “zoom in” infinitely and it will be able to show finer and finer detail ‘endlessly’.

In practice there could be limitations in the scalability due to floating point precision capabilities of the software but that’s another discussion and unrelated to the soundness of the mathematics that describe the image.",h92rixe,t3_p50qk0,1629061216.0,False
p50qk0,Mathematical fills and strokes.,h92r6xs,t3_p50qk0,1629061076.0,False
p50qk0,"Vectors. :)

They can be thought of as an arrow. It has a size and direction.

Edit: it's a math concept, usually encountered for the first time in calculus/physics",h92md1o,t3_p50qk0,1629059034.0,False
p50qk0,"Vectors, You could think of a vector image to be a bunch of information bout the placement of various lines relative to each other.",h92mtql,t3_p50qk0,1629059230.0,False
p50qk0,"Several people have said vectors, however “primitives” may be more accurate since there are several shape primitives that can define the image along with the vectors that provide position data.",h94j5ur,t3_p50qk0,1629094116.0,False
p50qk0,Vector= coordinates. See Postscript (which is actually a language). Download Adobe‘s Bluebook. There the Vector and drawing on/ to a „canvas“ concept / which can be a screen/ is very well described.,h94vwe9,t3_p50qk0,1629104351.0,False
p50qk0,"I made a video on this very subject!

https://youtu.be/HAXVcAiLlJE",h92sg09,t3_p50qk0,1629061601.0,False
p50qk0,"In the d3 introduction documentation on observable, the fundamental building blocks / ""composable primitives"" of SVGs are ""shapes"" and ""scales""",h934yhg,t3_p50qk0,1629067223.0,False
p50qk0,"Well, every image is made of pixels

But the data used to make vectorised graphics is, well vectors. Math functions. Therefore you can know the exact color at that point, you just gotta solve the equation using the pixels position value

Vectorised images don't go nearly as far as rasterised ones tho, in terms of usage",h93zhwp,t3_p50qk0,1629082182.0,False
p50qk0,"Probably  vectors (curves, lines)",h941gz4,t3_p50qk0,1629083187.0,False
p50qk0,I have heard they use mathematical equations to protect the curves and lines.,h9dzdqb,t1_h941gz4,1629275052.0,False
p50qk0,Yeah  so when the vectorised images are upscaled  the quality of images doesn't  alter .,h9e4njk,t1_h9dzdqb,1629280071.0,False
p50qk0,Math,h949ab6,t3_p50qk0,1629087541.0,False
p50qk0,"If you need to visualize it the best way to think of it as points. Every shape or graphic can be broken down into a series of points on a grid, a square has four (I know, obvious example).

Then between those points we have our straight lines that we can then apply a curve to. So in many cases when creating a complex image it's generally done as almost a low poly type design as a series of points and then after those are set the curve is applied, that's if you're designing it with a GUI like Adobe Illustrator. 

I know this description doesn't get into the math or CS of it but I hope it helps give you a better understanding of it as a high level concept.",h955qoq,t3_p50qk0,1629112183.0,False
p50qk0,"lines. 

vector images usually use mathematical functions to define an image

you want a circle? just draw a circle!

this is svg code -> <circle cx=""50"" cy=""50"" r=""50""/> 

&#x200B;

most vector images heavily use bezier curves to draw things because they are very fast for a computer to make. a vector image is made of shapes and lines and gradients

&#x200B;

in most vector formats you can define a simple math function for coloring. this can include gradients and blurs and stuff. 

&#x200B;

take a look at the SVG standard. if you can read html you can read this and it will start making sense",hc5t9xx,t3_p50qk0,1631177921.0,False
p4vyt5,"Well, Compilers put strings in interesting places. Here, the strings are placed in `.rdata`, also sometimes called `.rodata`, which is like `.data` but read-only. You need to disable filtering directives in GodBolt to see this.

Does not really matter, though.

Apart from that, C code usually uses the C library to interact with the OS, instead of executing syscalls directly, for a variety of reasons (mainly that syscalls are not part of C, also for good reason).

Finally, neither `li` nor `la` are actual instructions, but rather pseudo instructions, so you won't find them in compiler output.",h91rqs5,t3_p4vyt5,1629045981.0,False
p4q4jp,"I suggest all of the articles in this course: 

https://pdos.csail.mit.edu/6.824/schedule.html",h90ppy5,t3_p4q4jp,1629026686.0,False
p4q4jp,"Check out the various categories of award winning papers across the years:

https://jeffhuang.com/best_paper_awards/",h92dofo,t3_p4q4jp,1629055276.0,False
p4q4jp,"I loved the book The Innovators by Walter Isaacson. Fantastic history of hardware/software from mechanical calculators, early electro-mechanical machines, through digital programmable computers, through the internet, web, etc.",h935605,t3_p4q4jp,1629067319.0,False
p4q4jp,"I second this. It's been a long time but if memory serves me right, you get a detailed view of not just historic developments but also the context in which it became necessary. 

What I really liked about it was how in history it shows that there were multiple innovators but only some got the spotlight- sometimes by being 'inspired' by the inventions by the lesser known. Reasoning being that it's better to serve humanity than to waste time in finding the right credit to give.",h94iy1d,t1_h935605,1629093955.0,False
p4q4jp,"Thank you Sir Over Board 98. 

Isaacson is one of my grandfather's all-time favorite biographers (a quick search will show you why) and he tells such inspirational stories, like how Woz and Jobs used to hang out while Woz hacked the At&t phone lines with fake tones to prank the Vatican and almost got all the way through to the Pope pretending he was Henry Kissinger. 

For me, it's some of the coolest and most inspirational stories about most of my favorite inventors, companies, labs, technologies, languages, etc. It reads so quick.

Absolutely love Alan Turing, JCR Licklider, Alan Kay, Douglas Engelbart, and also fascinating recent history of the open-source movement and even the Grateful Dead are in the book! Incredible graphical timeline & photos in the middle too!",h95qm4w,t1_h94iy1d,1629123304.0,False
p4q4jp,"I’m subscribed to ACM they publish monthly issues , give you access to Safari Books and Papers",h92jtm8,t3_p4q4jp,1629057942.0,False
p4q4jp,Yup I agree it's a good investment though at times little unsure how much of it am I leveraging,h94j4uv,t1_h92jtm8,1629094097.0,False
p4q4jp,Really like the other comments. Just my addition would be to consider the Papers We Love Community- and their github repo of papers. There is another site called papers with code where you get exactly what it promises if you want to get hands on.,h94j8gp,t3_p4q4jp,1629094168.0,False
p4q4jp,A personal favorite of mine is the Dynamo paper describing the principles behind DynamoDB: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf,h91ifi9,t3_p4q4jp,1629042093.0,False
p46y49,https://online-learning.harvard.edu/course/cs50-introduction-computer-science?delta=0,h8wnnu0,t3_p46y49,1628946619.0,False
p46y49,This is what you need,h8yc36k,t1_h8wnnu0,1628974123.0,False
p46y49,the solution to all your problems : https://www.reddit.com/r/learnprogramming/wiki/faq#wiki\_getting\_started,h8wsfj0,t3_p46y49,1628949203.0,False
p46y49,"https://arghbox.wordpress.com/2014/04/25/minecraft-pi-recipe-cards/

also ask in r/learnprogramming",h8won63,t3_p46y49,1628947176.0,False
p46y49,"thank you, now i know it exists",h8wthyz,t1_h8won63,1628949745.0,True
p46y49,"https://teachyourselfcs.com

Fantastic site. Thorough.",h8xqd0x,t3_p46y49,1628964265.0,False
p46y49,"The o rilies books (for dummies series) was always good for learning. 

Youtube is where I lean...it is also where I teach basic it skills :).",h8x0bde,t3_p46y49,1628952966.0,False
p46y49,[removed],h8wvyga,t3_p46y49,1628950953.0,False
p46y49,Low viewer count.,h8xpswj,t1_h8wvyga,1628964024.0,False
p46y49,Help me make it bigger!,h8xwpft,t1_h8xpswj,1628967071.0,False
p46y49,"For a good overview of everything these is the classic Art of Computer Programming ->

https://www-cs-faculty.stanford.edu/\~knuth/taocp.html",h8x97wz,t3_p46y49,1628956875.0,False
p46y49,Crash course computer science. Youtube it.,h8xotus,t3_p46y49,1628963598.0,False
p46y49,"Not a book but what I did when starting would be I’d set myself fun projects and research as needed to complete it.

Things like creating a simple platformer shooter in Unity. Was my first introduction to game design, C#, and OOO programming.

Honestly though things like that really helped me as I was making things I found fun which motivated me to do it for longer; whereas reading a book I’d get bored pretty quickly when I first started.

Alternatively you can find tutorials on YouTube where you can download project files and watch the videos which teach you how to build the project yourself.",h8xv2mh,t3_p46y49,1628966330.0,False
p46y49,"If you want a person to talk to, feel free to DM me",h8ynqe0,t3_p46y49,1628979548.0,False
p46y49,"What interests you? This is such a big field. 

Ai?
Algorithms?
Computer arch?
Operating systems?
Programming languages?
Computational theory?
Systems programming?
User interfaces?

I have found that there is so much to do and learn that it can be overwhelming to such a broad question. I have asked myself the same thing and have perused interests in all of the subjects listed above.",h8yuuzl,t3_p46y49,1628983049.0,False
p46y49,"for big-picture history of CS & Internet: Walter Isaacson's book The Innovators. my grandpa gave me that when I was in college for CS and it changed my life. but I didn't learn about the web that well in that major. decent, but not great.

for an overview of the web and the browser that is the ""computer"" of the web: [https://github.com/kamranahmedse/developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)

lmk if you want more :)",h935r6y,t3_p46y49,1629067594.0,False
p46y49,"(copying this from another post I made)  
I'd study the history of JavaScript, the major techs and frameworks over the years and how teh language evolved. React is the latest evolution of the 20 years of the web and JavaScript.  
Keep in mind, the browser engine is its own 'computer' with its own runtime (the event loop) and you need to understand how software (technologies written in JS) run on it.  
Some of my favorite Web fundamentals resources:  
Weird history of JS: https://www.youtube.com/watch?v=Sh6lK57Cuk4  
Event loop: https://www.youtube.com/watch?v=8aGhZQkoFbQ  
Web APIs (standards across all browsers): https://developer.mozilla.org/en-US/docs/Web/API  
How browsers work: https://www.dailymotion.com/video/x3qs3lz  
Here's a developer roadmap of the frontend/backend/DevOps so you have a good big picture of how all the Web technologies fit together and what you need to use to make React work: https://github.com/kamranahmedse/developer-roadmap",h95thbb,t1_h935r6y,1629124513.0,False
p3qigz,"I found ""Designing Data-intensive Applications"" by Martin kleppmann very useful, it's very well organazied book full of great ideas, however, you should know that distributed systems and distributed computing are very wide area and you cannot find all of the details about every algorithm in one book, but if you get interested in one specific subject you can find more implementation details about it from the book references. It has a very good list of references for each chapter.",h8u4xpz,t3_p3qigz,1628891274.0,False
p3qigz,">however, you should know that distributed systems and distributed   
computing are very wide area and you cannot find all of the details   
about every algorithm in one book

I was just hoping for the well-known ones that are relevant to modern swe now, paxos, raft, etc.

I read the beginning of DDIA a while back and got the impression it was a book that just kind of sets the landscape rather than talking about implementation. I reckon I'll give it another shot.",h8v4wlg,t1_h8u4xpz,1628908296.0,False
p3qigz,"You're right that it does not include implementation details, I think for implementation details you should refer to specific articles and source codes, by the way, I am also really interested in implementing these things and I am implementing an specific database which entails some of these concepts, if you are interested we can work on it both. https://github.com/esahekmat/dalvdb",h8vtr3z,t1_h8v4wlg,1628923502.0,False
p3qigz,"That's cool your implementing a db, once I build myself up more, I might have to try jumping in. Thanks for showing me the github link!",h8xjzsh,t1_h8vtr3z,1628961522.0,False
p3qigz,"Oh BTW, If you are really into implementing. I remember this fantastic course. [https://pdos.csail.mit.edu/6.824/schedule.html](https://pdos.csail.mit.edu/6.824/schedule.html)

I highly recommend reading each session paper first before watching the lecture",h8y4l4p,t1_h8xjzsh,1628970691.0,False
p3qigz,This. I miss implementations as well.,h9eah09,t1_h8vtr3z,1629285038.0,False
p3qigz,"While not a book, I would keep a tab on an in progress article on Martin Fowler's website about Patterns of Distributed Systems. 

I was in charge of improving my system's availability last year and the page on HeartBeat helped me out. https://martinfowler.com/articles/patterns-of-distributed-systems/

I actually just really like his writing. Tons of resources to learn on refactoring and system architecture on there too.",h8u4rth,t3_p3qigz,1628891206.0,False
p3qigz,"*Distributed Algorithms: An Intuitive Approach (Fokkink)* is recent, but *Distributed Algorithms (Lynch*) is a classic. If you want a primer on the higher-level concepts and architecture, start with *Distributed Systems (van Steen, Tanenbaum).*",h8wlc6v,t3_p3qigz,1628945318.0,False
p3qigz,">Distributed Algorithms: An Intuitive Approach

I think I'll give this one a shot. Judging by the book summary, it sounds like exactly what I want.",h8xjsl9,t1_h8wlc6v,1628961436.0,False
p3qigz,[Distributed Services with Go](https://pragprog.com/titles/tjgo/distributed-services-with-go/) is a practical book to writing distributed systems. Along with the suggested Kleppman book for the theory.,h8yv478,t3_p3qigz,1628983175.0,False
p3qigz,"As other have suggested, Lynch's ""Distributed Algorithms"" book is great, but more theoretical. ""Distributed Systems"" by van Steen and Tanenbaum is also great. It's more practical too.

Finally, I found this series of distributed systems labs to be tremendously helpful. They gave me a feel of distributed systems beyond what books / paper reading had done for me:
https://github.com/emichael/dslabs",h93k6y0,t3_p3qigz,1629074548.0,False
p3pkz0,"CRDTs, hamming codes (error recovery), Swift introduced a bug in its language semantics which was proven to be a major issue using abstract algebra.",h8tgxo9,t3_p3pkz0,1628881806.0,False
p3pkz0,Do you know of any articles talking about that Swift bug?,h8tk5sb,t1_h8tgxo9,1628883104.0,False
p3pkz0,"https://forums.swift.org/t/swift-type-checking-is-undecidable/39024

Yup, found it^

Technically type system, d'oh on my memory's part.",h8tkk0y,t1_h8tk5sb,1628883257.0,False
p3pkz0,This is impressive,h8va92s,t1_h8tkk0y,1628911020.0,False
p3pkz0,Thanks!,h8tknhg,t1_h8tkk0y,1628883295.0,False
p3pkz0,"I feel that programming abstractions in general have a correspondence to principles from abstract algebra. Design patterns, interfaces, observers, flow of data, etc. Very deeply-connected and elegant. But few people in either the academic sphere or in industry seem to understand the connection.",h8ugrig,t3_p3pkz0,1628896513.0,False
p3pkz0,"Agreed. Offhand I can think of two books on the connection: Richard Bird's and Oege de Moor's *Algebra of Programming* (but that is maybe more about category theory?), and Stepanov's *Elements of Programming*. Stepanov supposedly had algebra in mind when designing the C++ STL.",h8v1rff,t1_h8ugrig,1628906747.0,True
p3pkz0,"Reed-Solomon codes are constructed using polynomials over finite fields. Quite a lot of coding theory can also be approached via linear algebra (lots of codes can be usefully treated as vector spaces over finite fields).

Lattices come up in a few places (e.g., program analysis, CRDTs).

The paper ""Fun with Semirings"" is a good read if you can get a copy.",h8thrxg,t3_p3pkz0,1628882152.0,False
p3pkz0,"Babai’s graph isomorphism algorithm reduces to abstract algebra. 

Shortest paths can be solved by tropical matrix multiplication.",h8unioo,t3_p3pkz0,1628899705.0,False
p3pkz0,Teaching abstract algebra,h8t6ojw,t3_p3pkz0,1628877641.0,False
p3pkz0,And bragging about knowing abstract algebra!,h8tho93,t1_h8t6ojw,1628882110.0,False
p3pkz0,"Not sure it's what you're looking for but I find Group theory to make my Computational Theory class easier and found connections (which I was able to confirm via lit search) between regular languages, DFAs, etc and groups.",h8wu9jz,t3_p3pkz0,1628950123.0,False
p3kby3,"Depending what you mean by that, not within any current living being lifetime.

Disney is/was working on a technology to turn basic text inputs into video: https://studios.disneyresearch.com/2019/06/01/generating-animations-from-screenplays/

But, a book is much more than a screenplay description and a movie is much more than just animating entities.

Just random thoughts about this: 
* In books you can read the thoughts, not in movies
* In movies, you can completely change a scene by tilting the camera. Point it downwards, the characters will be smaller, weaker, oppressed, etc... Point it upwards, the characters will be powerful, dominating, etc...

AIs assisting movie makers, yes more than likely. Making (good) movies by themselves, we're far from that.",h8s7wn7,t3_p3kby3,1628862763.0,False
p3kby3,"Thank you, that link is very helpful.  I look forward to reading through the paper to see the limitations at this time.",h8u1mg5,t1_h8s7wn7,1628889886.0,True
p3kby3,"I think it will be hundreds of years minimum, if it ever even comes about. It seems like you are greatly underestimating the challenges you face in video production.",h8rwtws,t3_p3kby3,1628856518.0,False
p3kby3,"You're greatly underestimating the progress of this field, check out the link posted above that the Disney research team has done.  It's actually a really interesting read to see where this technology is at.  10 years ago, we didn't have voice assistants (all that data is being used to train natural language models).  I can go on and on citing examples, but you can look up for yourself where any technology was a decade ago and see where its at today to see the rate of progress.  That paper published by Disney is from 2 years ago...   


In hundreds of years we won't be watching movies.  Neuralink, crispr , and other similar technologies are going to enable us to instantly downloads thoughts/ideas.  Movies will be an old art practice.",h8u5ypu,t1_h8rwtws,1628891708.0,True
p3kby3,See above.,h8u67gn,t1_h8u5ypu,1628891810.0,False
p3kby3,I’ve read some fantastic stories that are fundamentally impossible to turn into movies.,h8tlevd,t3_p3kby3,1628883595.0,False
p3kby3,"I agree.  Many book's I've read have been turned into movies and ""don't do it justice"", but that didn't make the movies bad.  There are many books that won't get read, and the only way to convey those stories/ideas to a larger audience is through a different medium (movies).",h8u7zy7,t1_h8tlevd,1628892587.0,True
p3kby3,I don’t just mean they made bad movies: I mean they are so tied to the medium they are expressed in that they fundamentally couldn’t be filmed. Stuff like there being some information hidden from the reader (like perhaps the sex or skin colour of a character) that would be revealed immediately if you could see them. And similar such tricks.,h8vuuf0,t1_h8u7zy7,1628924363.0,False
p3kby3,"Movies, music, paintings are human art forms. Humans express feelings, emotions, perspectives and world views through them.

What you propose should eventually be possible. But humans woulds still have to sift through all the different outputs and decide on what is actually worth showing to viewers.

The main question is: Who would want to watch it?

AI is basically pattern recognition and replication. The reason however a director selects a particular angle for a given scene is not a statistical approximation but there is meaning behind it.

One part of the fun of art is to guess or feel why a choice has been made. That would be missing with any AI produced ""art"".

But sure, if you just don't want to read the book and want a visual presentation of the plot line that might work. But I wouldn't mistake that for actual art.",h8rt0p5,t3_p3kby3,1628854192.0,False
p3kby3,"Very well said, I agree.  I wouldn't label most movies as ""art"" though lol.  Seems like streaming services are just churning them out at a fast rate for profit.  As for the question ""who would want to watch it?"", I would say everyone.  Reading is a slow way to download a story into our minds, it's a ancient technology.  Sure there are advantages to reading, but you have to think what is the most efficient way to get a story into the minds of the masses.",h8u7kl9,t1_h8rt0p5,1628892399.0,True
p3kby3,"I agree that there qualitative differences - and differences in taste - when it comes to forms of art.

Reading may be an ancient technology. But consider what reading - of especially prose and novels - offers in comparison to a movie:

While reading \*\*you\*\* create the scenes, the world, the characters and are oftentimes more engaged in the story than when someone else explicates every last detail in the way they see it or are able to condense it into a movie. In other words: Reading novels will never be superseded by movies. Reading a book and watching a movie or series are too separate experiences.",h8vi0km,t1_h8u7kl9,1628915412.0,False
p3jxr5,"I'd assume so, because GPT-3 is being used in GitHub Copilot to do code generation. Since it can do that accurately, it can *at least* write an `if` statement and you could go from that and build a circuit off of it",h8s6pjo,t3_p3jxr5,1628862128.0,False
p3jxr5,I'd definitely look into making it write SPICE input files. It's a standard format for describing a circuit and netlist in text that can also use SPICE software to then verify/check/simulate your circuit. If GPT-3 can be used to write code then I don't see why it can't be used to write simulation input files.,h8sk2vd,t3_p3jxr5,1628868685.0,False
p34us9,"The MIT OCW site is a good source.

https://ocw.mit.edu/ 

Introduction to Algorithms -> https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/

Introductory Programming Courses -> https://ocw.mit.edu/courses/intro-programming/",h8p1rbo,t3_p34us9,1628798509.0,False
p34us9,"I recommend Goodrich's Data Structures & Algorithms book. He offers editions in C++, Java, and Python.",h8q8qwl,t3_p34us9,1628817239.0,False
p34us9,"Also these two:
- Introduction to Algorithms, 3rd Edition by Thomas Cormen & co.
- Algorithms, 4th Edition by Robert Sedgewick & Kevin Wayne",h8s91ij,t1_h8q8qwl,1628863369.0,False
p34us9,"Links:

GFG Data Structures - [https://www.geeksforgeeks.org/data-structures/](https://www.geeksforgeeks.org/data-structures/)

GFG Algorithms - [https://www.geeksforgeeks.org/fundamentals-of-algorithms/](https://www.geeksforgeeks.org/fundamentals-of-algorithms/)

You will find enough theory & practice in GFG.

Video Tut:

Data Structures from mycodeschool - [https://www.youtube.com/playlist?list=PL2\_aWCzGMAwI3W\_JlcBbtYTwiQSsOTa6P](https://www.youtube.com/playlist?list=PL2_aWCzGMAwI3W_JlcBbtYTwiQSsOTa6P) (Learn Pointers in C++ before starting this)

Algorithms by Abdul Bari - [https://www.youtube.com/playlist?list=PLDN4rrl48XKpZkf03iYFl-O29szjTrs\_O](https://www.youtube.com/playlist?list=PLDN4rrl48XKpZkf03iYFl-O29szjTrs_O)

Books:

\- Introduction to Algorithms CLRS (More Mathematical)

Practice from leetcode, hackerrank, etc.. This is all you need.",h8okbh8,t3_p34us9,1628791035.0,False
p34us9,I've studied algorithms at university and didn't know cpp - why do you suggest it?,h8qm3sn,t1_h8okbh8,1628823676.0,False
p34us9,Faster and more efficient.,h8qp0ws,t1_h8qm3sn,1628825177.0,False
p34us9,"As a language yeah, I imagine algorithms can be studied regardless of language.",h8rjnct,t1_h8qp0ws,1628847079.0,False
p34us9,You DO NOT need to waste your time learning a new programming language if your goal is to learn about data structures and algorithms. You can use basically any language you already know for this.,h8re1iq,t1_h8okbh8,1628842233.0,False
p34us9,[deleted],h8rgsbr,t1_h8re1iq,1628844579.0,False
p34us9,"So you think he was trying to learn data structures and algorithms as the first thing without knowing a single programming language beforehand? A weird assumption to make, I would say. Also, you probably know that HTML is not a programming language, and from the context of my message it should be pretty clear that 'language' was referring specifically to a programming language.

If the case is that he does not know any language, then sure, C++ is a perfectly valid first language to learn, but from OP's post I would assume he already knows or is somewhat familiar with some programming languages.",h8rhscc,t1_h8rgsbr,1628845465.0,False
p34us9,"HTML is a mark up language not a programming language. 

You can learn algorithms with Python, just as effectively in most cases too. For instance I learned algorithms with C#.",h9havmr,t1_h8rgsbr,1629336921.0,False
p34us9,I was joking mate.,h9j8ya3,t1_h9havmr,1629382055.0,False
p34us9,If you say so...,h9r23z3,t1_h9j8ya3,1629518523.0,False
p34us9,"Thanks. I know Cpp, guess I'll get started with the rest.",h8olpzh,t1_h8okbh8,1628791590.0,True
p34us9,Abdul also has  a really nice data structures and algorithms course OK Udemy,h8p3h3f,t1_h8olpzh,1628799174.0,False
p34us9,Why do you suggest C++ as opposed to something like Java?,h8p9zw7,t1_h8okbh8,1628801731.0,False
p34us9,Pointer manipulation really helps you get at the heart of making efficient (in terms of memory and not only speed) data structures and using them,h8pk7v3,t1_h8p9zw7,1628805905.0,False
p34us9,I have to agree that starting with Java is better. You want to learn how the datastructure works not sit around worrying about pointers.,h8qsw3i,t1_h8pk7v3,1628827299.0,False
p34us9,"A good data structures class teaches you how to properly implement the structures from scratch, not just how to import and use them",h8qtf3c,t1_h8qsw3i,1628827597.0,False
p34us9,"Yes, which you can do in Java and not have to get bogged down with pointers",h8qtmbr,t1_h8qtf3c,1628827711.0,False
p34us9,"but process of learning, should comprise of as much technical details necessary as to completely comprehend the topic. DS will be at very high level of understanding without the concept of pointer. Cpp seems to be best way to take the holistic view. Atleast it was for me",h8r23dv,t1_h8qtmbr,1628833013.0,False
p34us9,Exactly.,h8rshsq,t1_h8r23dv,1628853850.0,False
p34us9,Amazing comment thank you sir,h8pe1ws,t1_h8okbh8,1628803361.0,False
p34us9,"There are free MIT courses on YouTube. As for books, there is the classic CLRS.

But I strongly suggest Algorithms by Dasgupta et al.",h8p21rk,t3_p34us9,1628798624.0,False
p34us9,good luck,h8uoezu,t3_p34us9,1628900144.0,False
p34us9,"This two part course from Robert Sedgewick is really worthy. His use of visualization and animation for describing algorithm is really helpful to understand them. 

Note: He uses Java to implement algorithm

[https://www.coursera.org/learn/algorithms-part1](https://www.coursera.org/learn/algorithms-part1)

https://www.coursera.org/learn/algorithms-part2",h8y38ph,t3_p34us9,1628970077.0,False
p34us9,Thanks,h8y3dek,t1_h8y38ph,1628970136.0,True
p34us9,"If you learn well from videos, check out these channels. If you don't know how you learn best, try to learn from different formats and see what works best for you.

* [Abdul Bari](https://youtube.com/channel/UCZCFT11CWBi3MHNlGf019nw) 
* [Back to Back SWE](https://youtube.com/c/BackToBackSWE) 
* [Tushar Roy](https://youtube.com/user/tusharroy2525)",h8p7o8v,t3_p34us9,1628800815.0,False
p34us9,"Everyone always seems to suggest C/C++, or Java, yet the MIT course uses Python! 

Speaking of which, MIT also has discrete math as a pre-req for DS&A. I’m curious how crucial that is in successfully understanding the concepts of DS&A. Any thoughts from anyone?",h8rqgsr,t3_p34us9,1628852465.0,False
p34us9,"Discrete was an immense help when I studied Algorithms. It covers ground that Algorithms treated almost as assumed knowledge. I imagine you don't HAVE to know it, but I'm glad I had done it before going in.",h9hcsjp,t1_h8rqgsr,1629337822.0,False
p34us9,"Thank you all for your advice. For now I have started with the MIT OCW Intro to Algo course, along with the book by Thomas Cormen.",h9mvok3,t3_p34us9,1629443920.0,True
p2x1fr,[deleted],h8oag3u,t3_p2x1fr,1628787098.0,False
p2x1fr,Interesting I was not aware of the fact 😅,h8obk7a,t1_h8oag3u,1628787541.0,True
p2x1fr,"All of the major social media have hundreds of engineers and researchers working on these type of algorithms and AIs.

They are amongst the most secret algorithms these companies own.

You'll only find very general information, mostly based on what people deducted from the algorithms. All the good stuff is basically industrial secrets.",h8p2z1b,t1_h8obk7a,1628798982.0,False
p2x1fr,True. I look on the internet for it only think I can find is few beginning Crouse and article,h8qmeri,t1_h8p2z1b,1628823827.0,True
p2x1fr,Do you Know any Crouse where I can start to learn.,h8obzr0,t1_h8oag3u,1628787714.0,True
p2x1fr,[deleted],h8ocd96,t1_h8obzr0,1628787866.0,False
p2x1fr,Oh . By the way thanks for your time. I will dig into it.,h8ofh0y,t1_h8ocd96,1628789114.0,True
p2x1fr,"AI is such a broad term that you might want to look for something more specific unless you also want to wrap your head around a crapton of other stuff.

Recommender systems might be more specifically what you're looking for. Some AI courses probably cover them, but there might also be some more specific resources. For example, some data mining courses might cover recommender systems as well, and it's a narrower and more specific field than AI.

I can't vouch for any particular book, though.

Of course actual social media companies probably also have lots of other stuff in their recommendation systems beyond just the common basic algorithms, so I'm not sure how representative a course or textbook would be of real-world social media systems, but it would probably teach you a lot more than nothing.",h8pjqkq,t1_h8obzr0,1628805705.0,False
p2wszh,"Like:

Ban, Can, Dan, Fan, LAN, Man, Nan, Pan, Ran, Tan, Van


But with a G at the beginning.",h8n2chp,t3_p2wszh,1628765974.0,False
p2wszh,"Ah, so like GIF, eh?",h8qyv7a,t1_h8n2chp,1628830880.0,False
p2wszh,"Yes, the only way of pronouncing GIF.",h8r1u9h,t1_h8qyv7a,1628832841.0,False
p2wszh,I wanna know how people pronounce it in the privacy of their own home,h8n9k7e,t3_p2wszh,1628770629.0,False
p2wszh,"\*whispers\*

Jan",h8nvfr1,t1_h8n9k7e,1628780971.0,False
p2wszh,Gin,h8nhf8b,t1_h8n9k7e,1628774781.0,False
p2wszh,"GAN like can, ban, man... and if someone gets confused, just say generative adversarial network",h8ndrnt,t3_p2wszh,1628772939.0,False
p2wszh,But is it G as in Gay or G like G,h8of9pu,t1_h8ndrnt,1628789032.0,False
p2wszh,G as in gatling gun,h8ok7sp,t1_h8of9pu,1628790995.0,False
p2wszh,G as in gif ( ͡° ͜ʖ ͡°),h8pp6ce,t1_h8ok7sp,1628808169.0,False
p2wszh,That's damn right,h8pwffo,t1_h8pp6ce,1628811713.0,False
p2wszh,You mean like g as in gift right?,h8r1yc5,t1_h8pp6ce,1628832917.0,False
p2wszh,"No no, as in GIF",h8rz90v,t1_h8r1yc5,1628857921.0,False
p2wszh,Which is the same for g in gift😏,h8s00ja,t1_h8rz90v,1628858364.0,False
p2wszh,"* Gähn

(Sorze: äm Dschörmän)",h8nu80y,t3_p2wszh,1628780460.0,False
p2wszh,supercalifragilisticexpialidocious,h8n037a,t3_p2wszh,1628764251.0,False
p2wszh,Lmao this reminds me how my previous employer who had a PhD in explainable AI used to call it generative BS.,h8nhr2i,t3_p2wszh,1628774937.0,False
p2wszh,2,h8mz1pf,t3_p2wszh,1628763419.0,False
p2wszh,I‘d pronounce it either g.a.n. or gan with the a pronounced as in adversial,h8n3842,t3_p2wszh,1628766602.0,False
p2wszh,"There's two A's (with different pronunciations) in 'adversial'. Also, ""adversial"" isn't a word; I think you mean ""adversarial"", in which case, there's 3 A's, each with a different pronunciation.",h8nojqf,t1_h8n3842,1628778036.0,False
p2wszh,Option 5,h8n10r8,t3_p2wszh,1628764985.0,False
p2wszh,G.A.N,h8nfmu1,t3_p2wszh,1628773903.0,False
p2wszh,Like pronouncing GIF,h8ohmbu,t3_p2wszh,1628789967.0,False
p2wszh,Like gift without T.,h8r2069,t1_h8ohmbu,1628832953.0,False
p2wszh,BatGAN.,h8ooj6g,t3_p2wszh,1628792765.0,False
p2wszh,Like the 2nd syllable in Polygon,h8oqey4,t3_p2wszh,1628793636.0,False
p2wszh,GAAAAAAAAAN,h8p9bnm,t3_p2wszh,1628801464.0,False
p2wszh,How old is your team leader? Sometimes older people like to pronounce things like they don’t care what it really is. I know some older people that watch the news all day long and still pronounce it COVIS Instead of covid.,h8qqhlb,t3_p2wszh,1628825965.0,False
p2j4zt,"[https://scholar.google.ca/scholar?as\_ylo=2017&q=optical+computing&hl=en&as\_sdt=0,5](https://scholar.google.ca/scholar?as_ylo=2017&q=optical+computing&hl=en&as_sdt=0,5)  


Still very much a thing. And yes it could.",h8kjgl5,t3_p2j4zt,1628709807.0,False
p2j4zt,"Optical computing, that's why I wasn't finding anything, much better results with the correct terms. Thanks a lot!",h8lpya2,t1_h8kjgl5,1628728263.0,True
p2j4zt,My pleasure :),h8lq12b,t1_h8lpya2,1628728300.0,False
p2j4zt,I have been hearing about photon computers for over 40 years.  Where is it?,h8ld74h,t3_p2j4zt,1628722356.0,False
p2j4zt,Did you steal the idea from my brain? lol I was literally thinking about this so much the past week.,h8krzpu,t3_p2j4zt,1628713382.0,False
p24bw0,"Python is the better teaching language I think.

JS just has so many edge cases and weirdness that just doesn't suit learning logic at all.

https://github.com/denysdovhan/wtfjs",h8hpk3p,t3_p24bw0,1628651080.0,False
p24bw0,[removed],h8i19da,t1_h8hpk3p,1628658073.0,False
p24bw0,"Second Zelle's book, I think it's great for beginners",h8i9ehx,t1_h8i19da,1628664244.0,False
p24bw0,"In terms of programming exercises I've always found 2 topics grab high school students' attention:

1. Code that does their math homework.  (their attention perks up and they learn why computers can save them time)
2. Games. Teach them how to write very simple games (start with hangman and move onwards to graphics)

If you can work your teaching topics into these areas you'll get more engagement.

Also,  use analogies that are meaningful for them when explaining concepts.  For example,  when discussing lists you can explain that the weapons in the GTA V weapon wheel are stored in a list, or positions of the players in FIFA are stored in a list of players.",h8in6cy,t3_p24bw0,1628676454.0,False
p24bw0,"python is a great place to start.

theres lots of 'real life' tasks and applications to tackle and a ton of solid online resources.  

Javascript can be a nightmare hahaha",h8i367v,t3_p24bw0,1628659411.0,False
p24bw0,Look for the AP Computer Science requirements .,h8hobxi,t3_p24bw0,1628650424.0,False
p24bw0,"Thank you! I actually have and really want to avoid teaching it in an AP style (previously taught AP calculus). I also saw they teach Java which I have never used. I also am teaching two other subjects and will not have to time to prep in the way that AP styled courses require. I was looking for a book that I can use to teach out of for a semester course that they will benefit from, but won't have the level of rigor an AP course comes with.",h8hoynv,t1_h8hobxi,1628650759.0,True
p24bw0,You could just use it as an outline and go at whatever pace you want. But another thing you might look at is https://scratch.mit.edu/.,h8j25y9,t1_h8hoynv,1628686743.0,False
p24bw0,"Woah, less than 48 hours isn’t much time. If the book is from 2013, consider that that was around Python 3.3/3.4 era, and that anything that you’d want to teach to students almost certainly is not relevant in newer Python versions (we are currently on 3.9). I have a feeling you might be talking about “Learning Python” 5th Edition, by Mark Lutz. If so, that book will be just fine for teaching.

As far as a language goes, I highly recommend sticking with Python. Consider the following scenarios:
- Your students walk in on the first day. You tell them to download Python. CPython installs, along with IDLE, a place that Python can be run. You tell your students to open IDLE, and can immediately start instructing them on programming. Or…
- Your students walk in on the first day. You’d like to start teaching them JavaScript, but instead you first need to teach them how HTML works and what script tags are, and how browsers work, unless you plan to set them up with a runtime environment for JS. 


Even then, JavaScript can be weird. Imagine trying to explain to kids with no programming experience how = is different from == is different from ===. Without first explaining how JavaScript is a weakly typed language, the difference between == and === is going to make no sense. Even then, explaining data types is going to be a little harder in both Python and JS, as they are both dynamically typed. My first language was C++, and the transition to Python and other languages from there was a breeze because I had been “raised” with a static, strongly-typed language.

Bottom line: go with Python; the book from 2013 shouldn’t be much of an issue, but newer doesn’t hurt obviously.",h8ihr27,t3_p24bw0,1628671561.0,False
p24bw0,"This would be Python and not JS, but there’s a great Interactive Version of “How to think like a Computer Scientist” that IMO would be great for the classroom! We used this website for my CS 101 class(albeit a different course on the website), and there’s lots of control for the teacher to assign stuff to students with a deadline, etc. It’s entirely free, so your admin also wouldn’t need to shell out anything.

https://runestone.academy/runestone/books/published/thinkcspy/index.html",h8iziqx,t3_p24bw0,1628685275.0,False
p24bw0,"Between the two, Python definetely.

Generally speaking, I consider C to be the best intro programming language, not because it's easy but because its the basis of most modern languages and its close enough to the hardware that it gives a more intuitive understanding of what's going on behind the scenes. Also it's important to use a compiler manually and not hide everything with an IDE. 

Python might be better for highschool students though, especially if they're not all interested in the subject. It's very easy to get started with and start making useful projects, and this will encourage more kids to learn it and be creative with it.",h8izy3k,t3_p24bw0,1628685519.0,False
p24bw0,"I assume most of the recommendations here will be for Python and for good reasons. However, I think it's very captivating to be able to quickly make things that behave like programs they're used to using. Being able to make and use a button is going to capture more imaginations and it's simpler with JS/Web. (this argument could also apply to a raspberry pi with one of those electronics kits, but that involves a lot of other variables and funding)

Someone mentioned the difference between setting up Python and IDLE vs learning about HTML to get started and it's an important consideration but because any computers in a school's computer lab surely have a browser installed already, there's nothing to install to get started. Fire up the browser, press F12 and start typing commands into the console. Or just have them navigate to codepen or jsfiddle.

If you are going to do Python, I'd look into Jupyter notebooks because you can get straight to coding without worrying about files and the command prompt. I mean, I love files and the command prompt, but it'd be overwhelming to start there.

Good luck!",h8isa48,t3_p24bw0,1628680599.0,False
p24bw0,"Look into ProcessingJS its what they taught in my high school before transitioning to python and it really helped rather than starting on python straight away.


ProcessingJS is a JS library. Its also what they use on khanacademy.",h8k6q89,t3_p24bw0,1628704646.0,False
p24bw0,"I’d stick with Python (or move to Java) and teach some of the basic primitive types then move up to some OOP. Avoid spending too much time with language-specific knowledge unless where necessary. Spend a little time on tooling like the IDE, debugging via IDE, and Git. You can finish the class by having your students write John Conway’s Game of Life. That was basically my first intro to programming class in community college.",h8jsve8,t3_p24bw0,1628698724.0,False
p24bw0,"Here is a free AP CSP e-text for use with Python

[https://runestone.academy/runestone/books/published/StudentCSP/index.html](https://runestone.academy/runestone/books/published/StudentCSP/index.html)",h8k1i5i,t3_p24bw0,1628702386.0,False
p24bw0,Python.,h8kg7o8,t3_p24bw0,1628708470.0,False
p24bw0,"My Ryerson Chang School intro Python materials, free to use and adapt:

https://github.com/ikokkari/PythonExamples
https://github.com/ikokkari/PythonProblems",h8kj15t,t1_h8kg7o8,1628709631.0,False
p24bw0,I prefer [Introduction to Computer Science and Programming Using Python](https://www.edx.org/course/introduction-to-computer-science-and-programming-7) and [Introduction to Computer Science with Python](https://online-learning.harvard.edu/course/introduction-computer-science-python?delta=0) and [How to Think Like a Computer Scientist](https://openbookproject.net/thinkcs/python/english3e/),h8kk0ju,t1_h8kj15t,1628710037.0,False
p24bw0,"Python is easier, teach them about basic programs like a GPA calculator, BMI calculator and then go for more complex things like a hang game. 

Complex in the context of HS students who don't really have any experience programing.",h8llnyn,t3_p24bw0,1628726254.0,False
p24bw0,"Please avoid JS. More than half of the students would hate it. Maybe start with html to make them comfortable with editors etc since it provides instant ""gratification"" and then move on to python?",h8lsmai,t3_p24bw0,1628729516.0,False
p24bw0,Not super practical advice sorry but make sure it's fun for their age. I hated IT in school purely because they taught it in a horrid way.,h8qmb57,t3_p24bw0,1628823776.0,False
p24bw0,"If you want to prepare students for college CS, consider something from left field - Scheme or Haskell. These languages are ""weird"" for someone with preexisting knowledge of traditional imperative programming, but very intuitive for a beginner with a blank slate. Both have excellent books for beginners.

""Structure and Interpretation of Computer Programs"" is a great choice for Scheme. For Haskell, there are many options; Paul Hudak is a good author to look into.",h8t0p3u,t3_p24bw0,1628875260.0,False
p24bw0,Check out this nice article about microlearning! You can use this technique to teach your students: https://home.osc.school/introduction-to-microlearning/,h8t360h,t3_p24bw0,1628876239.0,False
p23d5b,"People have worked on this:

https://mathoverflow.net/questions/269786/what-computational-problems-would-be-good-proof-of-work-problems-for-cryptocurre

https://www.google.com/amp/s/spectrum.ieee.org/amp/making-blockchain-bitcoin-computing-more-useful-2653906658",h8hr4dc,t3_p23d5b,1628651936.0,False
p23d5b,Maybe look into how Folding at Home or SETI approach things?,h8l2ztt,t3_p23d5b,1628717935.0,False
p23d5b,"My understand with proof of work is that it has to be easily verified. If you’re having nodes doing some research task, how can you verify if it’s correct?",h8hhxup,t3_p23d5b,1628647172.0,False
p23d5b,CPU utilization is may be difficult but ML training should be easy by testing the model accuracy against a validation set,h8hm9ft,t1_h8hhxup,1628649351.0,True
p23d5b,"Technically, the work bitcoin does is not all wasted. It keeps sha256 secure. Everything in todays world is built around that algorithm and with Bitcoin we would instantly know if it somehow failed.",h8isncn,t3_p23d5b,1628680866.0,False
p1vqbh,"so now you can recurse it, and make conways game of life in conways game of life in conways game of life etc.",h8g1zsn,t3_p1vqbh,1628623199.0,False
p1vqbh,"Do that 42 times and the answer to life, the universe, and everything gets simulated. 

Also, RIP Conway (COVID-19 death).",h8g2twh,t1_h8g1zsn,1628623554.0,False
p1vqbh,yo wtf he died? 😕😔,h8giqfa,t1_h8g2twh,1628630393.0,False
p1vqbh,"Which is as ironic as it is sad, if you think about it...",h8i1a0d,t1_h8giqfa,1628658085.0,False
p1vqbh,yep,h8i17nl,t1_h8giqfa,1628658040.0,False
p1vqbh,"The XKCD was pretty cute https://xkcd.com/2293/

Still sad, though 😢",h8i62o3,t1_h8g2twh,1628661593.0,False
p1vqbh,"yo dawg, I herd you like conways game of life",h8gbn7j,t1_h8g1zsn,1628627317.0,True
p1vqbh,and fractals,h8gwq9l,t1_h8gbn7j,1628636772.0,False
p1vqbh,That's definitely paywall DLC.,h8irsj4,t1_h8g1zsn,1628680235.0,False
p1vqbh,Pretty awesome. [Here's more information](https://www.conwaylife.com/wiki/OTCA_metapixel).,h8gba4q,t3_p1vqbh,1628627164.0,False
p1vqbh,"hey, is there source code for this?",h8g56v3,t3_p1vqbh,1628624565.0,False
p1vqbh,"I don't think you want source code. There are plenty of options for that, if you want it. I'd imagine conways game of life has been implemented in just about every language out there.

Instead you want the initial starting conditions.",h8gm1d9,t1_h8g56v3,1628631851.0,False
p1vqbh,you are correct,h8gm6f2,t1_h8gm1d9,1628631914.0,False
p1vqbh,"Comment below me linked to this: https://www.conwaylife.com/wiki/OTCA_metapixel

Seems to cover the implementation.",h8gmul6,t1_h8gm6f2,1628632213.0,False
p1vqbh,thanks!,h8gmxrv,t1_h8gmul6,1628632253.0,False
p1vqbh,"Potato, potato",h8gsnio,t1_h8gm1d9,1628634866.0,False
p1vqbh,Who produced this video?,h8gklos,t3_p1vqbh,1628631215.0,False
p1vqbh,Wasn't me.,h8gvccq,t1_h8gklos,1628636123.0,False
p1vqbh,But she caught me on the counter,h8i1c0m,t1_h8gvccq,1628658122.0,False
p1vqbh,It's from a fantastic mini-documentary: https://youtu.be/Kk2MH9O4pXY,h8horfq,t1_h8gklos,1628650651.0,False
p1vqbh,"nop, it was from this video : https://www.youtube.com/watch?v=xP5-iIeKXE8",h8icclk,t1_h8horfq,1628666719.0,False
p1vqbh,"Thank you! I loved Conway’s Game of Life in high school, I wrote several versions in C, Pascal, and x86 assembler, and did a project on cellular automata for Biology. It’s neat to see how far the computing work has gone!",h8iis10,t1_h8icclk,1628672500.0,False
p1vqbh,Nice!,h8ictx1,t3_p1vqbh,1628667138.0,False
p1vqbh,"Well TIL about Conway's Game of Life. Also have never heard of a zero player game. 

Question: does the term ""seed"" from randomly generated video games derive from Conway's Game of Life?",h8hbgks,t3_p1vqbh,1628643959.0,False
p1vqbh,"To your question: No. Generally, all pseudo random generators need a seed (initial position). Most of the time the seed is the only way to get true randomness into the generator.

GOL just looks random because it's extremely visual. In fact the cellular automaton is still deterministic. It's just that you see it's configuration the whole time while you need a debugger to do the same for the usual programming languages.",h8i6pdb,t1_h8hbgks,1628662083.0,False
p1vqbh,Alright. That’s pretty neat.,h8hbbk9,t3_p1vqbh,1628643891.0,False
p1vqbh,"Implementing John Conway's Game of Life in whatever programming language you are learning is a great programming practice exercise for beginners. Much more interesting than your standard accounting problem a lot of beginner books throw at you.

I've just programmed this and first time I got it working was very cool.",h8i3ofg,t3_p1vqbh,1628659781.0,False
p1vqbh,So you invented multi cellular life forms 😂,h8ifqaq,t3_p1vqbh,1628669730.0,False
p1vqbh,"I heard you like conways game of life, so I put a conways game of life in your conways game of life.",h8ioxy7,t3_p1vqbh,1628677979.0,False
p1vqbh,Whoa,h8jje7j,t3_p1vqbh,1628694749.0,False
p1vqbh,Game of Life is Turing complete. You could play whatever you want on it so long as you're able to handle player input.,h8jl66c,t3_p1vqbh,1628695497.0,False
p1vqbh,THIS... is the type of thing that make me happy that I took computer science.,h8jzfdr,t3_p1vqbh,1628701492.0,False
p1vqbh,Love it,h8l35qo,t3_p1vqbh,1628718004.0,False
p1nmmd,"I'm not sure, but this paper may be helpful as it describes how to ""delete"" edges to produce a cycle. So you could hypothetically start with a graph with many edges, and then delete them as described in the paper.

&#x200B;

https://springerplus.springeropen.com/articles/10.1186/s40064-016-2746-8",h8kn01j,t3_p1nmmd,1628711265.0,False
p1j516,Wtf is a midwit?,h8ds7aa,t3_p1j516,1628577504.0,False
p1j516,the biggest point on the bell curve,h8dxwdu,t1_h8ds7aa,1628582377.0,True
p1j516,What kind of data? Text from the image? And also what’s the end goal?,h8dqu6w,t3_p1j516,1628576410.0,False
p1j516,Yeah its just text from image. End goal is to be able to quickly pull the text from an image once or several times a day and dump it in a spreadsheet.,h8dy202,t1_h8dqu6w,1628582514.0,True
p1j516,What kind of text?,h8dzbt1,t1_h8dy202,1628583636.0,False
p1j516,Its just white text over a black background. Numbers and letters. Similar to receipt data only theres an actual template thats consistent and the only thing that changes is the numbers.,h8dzo5t,t1_h8dzbt1,1628583931.0,True
p1j516,There are out of the box text recognition solutions but your coding skills are going to be a limiting factor. It  can give you all the text on the image but first you’ll have to implement it in your code base and then you’ll need to run your own algorithm to organize the text so you can put them in the programs you mentioned,h8e0rrh,t1_h8dzo5t,1628584921.0,False
p1j516,I might be able to get some of it done and then have a friend or someone else do the rest. What are the out of the box solutions? matlab?,h8e2h8v,t1_h8e0rrh,1628586476.0,True
p1j516,"For phones, Android has Firebase and Apple has ML Kit. Matlab is its own library I’m not familiar with. And for sure good luck",h8e4aqy,t1_h8e2h8v,1628588144.0,False
p1j516,thanks i will check those out,h8ecugp,t1_h8e4aqy,1628595037.0,True
p1chqf,"There are languages based on different type theories (e.g., Coq, Agda), which on some level trace back to Russell for their core ideas.",h8cj9yg,t3_p1chqf,1628552290.0,False
p1chqf,"> There are languages based on different type theories 

Isabelle engine has different (plugable) logics inside: https://isabelle.in.tum.de/dist/Isabelle2021/doc/logics.pdf",h8ea0wb,t1_h8cj9yg,1628592930.0,False
p1chqf,"In my impression, the biggest problem is that all representable sets and functions need to be redone as predicates:
.
> https://math.libretexts.org/Bookshelves/Mathematical_Logic_and_Proof/Book%3A_Friendly_Introduction_to_Mathematical_Logic_(Leary_and_Kristiansen)/05%3A_Syntactic_Incompleteness__Groundwork/5.03%3A_Representable_Sets_and_Functions

> For the sake of discussion, suppose that we let  f(x)=x2 . It will not surprise you to find out that  f(4)=16 , so we would like to write  N⊨f(4)=16 . Unfortunately, we are not allowed to do this, since the symbol  f , not to mention  4  and  16 , are not part of the language.

> What we can do, however, is to represent the function  f  by a formula in  LNT . 

The canonical language of PA is very limited. It does not even represent numerical constants efficiently. The formulas are even more (incredibly) inefficient. 

This translation into the very limited language of PA is needed for theoretical reasons, but you probably do not want to compute with it on a real-world computer.",h8cljfz,t3_p1chqf,1628553397.0,False
p1chqf,">Z-notation

Could the square be a function to call from the library? Or would that not be a good idea?",h8d1uu0,t1_h8cljfz,1628561287.0,True
p1chqf,"As far as computer languages go, ones that fall under the logic paradigm are supposed to be at least based on formal logic and set theory which is pretty much what this is. Prolog is the most famous language of this sort I believe. 

Formal specification languages (such as Z-notation) look a lot closer to this though. However, these languages are meant to model safety critical software to prove that a system works BEFORE actually writing it as code. So part of the software development life cycle, but not technically code.",h8csk0l,t3_p1chqf,1628556775.0,False
p1chqf,"I don't think that would be particularly useful. In the decades since PM was released, we massively simplified the syntax of first- and second-order-logic into something you would recognize today, introducing symbols for the quantifiers, implication arrows, etc.  


Judging from [https://plato.stanford.edu/entries/pm-notation/](https://plato.stanford.edu/entries/pm-notation/), PM seems to use a somewhat restricted version of first- or second-order logic. It seems like Coq or any other language at the core of a proof checker is a good template for this.  


That being said, logic is not a programming language. It has no executable semantics. It has ""truth value"" semantics, which are a tremendously useful concept but uncomputable. So you might end up building a proof checking system, which is something that already exist. Additionally your proof checker would use a lot of weird notation nobody understands and no one would bother with.",h8cuhxl,t3_p1chqf,1628557709.0,False
p1chqf,"PM's syntax was an infinite-order logic (or higher-order logic, in modern terminology). First- and second-order logic are both subsystems of PM.

 Each individual natural number was a second-order predicate - for example, the number 2 was defined to be the predicate

2(P) = there exist x,y such that Px, Py, x=/=y, and for all z, if Pz then z=x or z=y

Or in short 2(P)= ""Exactly two objects satisfy P"".

So the predicate ""n is a natural number"" was a third-order predicate. I think ""x is a real number"" was fourth-order (I would need to check).",h8fdcmx,t1_h8cuhxl,1628612871.0,False
p1chqf,Thanks for the clarification. That still means it would be rather easily adaptable to current theorem prover's specification language.,h8fynd2,t1_h8fdcmx,1628621782.0,False
p1chqf,"I don't mean necessarily useful for people, since it is hard to read at first glance. But for machine learning applications?",h8d103i,t1_h8cuhxl,1628560866.0,True
p1chqf,"Like the other person said, this doesn’t really make any sense. Principia Mathematica (assuming you mean Russell’s book, not Newton’s, though both are irrelevant) is a philosophical work in the field of number theory and mathematical logic, and doesn’t really have any relevance to machine learning.

Source: Have degrees in both philosophy and CS, so I’m finally qualified to use them both at once, haha.",h8dyzla,t1_h8d103i,1628583339.0,False
p1chqf,"Meaning: use the language, give it traveling salesman or some increasing difficulty. Maybe better symbols can be found? I think humans would review input and output, but the layers would be difficult.",h8d19aa,t1_h8cuhxl,1628560994.0,True
p1chqf,What? None of what you said made sense.,h8dx8tj,t1_h8d19aa,1628581792.0,False
p1chqf,Perhaps look into the history of logic? The proof checkers came from somewhere...,h8e1yrh,t1_h8dx8tj,1628586010.0,True
p1chqf,"Wikipedia and YouTube about primitive logic representation seems to have piqued your interest.

We gather a lot of data these days, and Machine Learning is great at navigating the data based on grading criteria we configure. A reasonable (and free) course to look at this subject is:

https://www.coursera.org/learn/machine-learning#about

It can turn sample data into a developed data model (decision trees, neural networks). This model would represent some logic it “learned”. Then, use that model to process data it has never seen before.

The traveling salesman problem (TSP) you mention is a problem specifically b/c it is too computationally-intensive to brute-forced by trying all possible combinations/paths. If you want to pursue solving this type of problem better, using AI there is Simulated Annealing. But that method can’t be sure it has the best answer to the problem or guarantee to approach it quickly.

Quantum computing, which has been developing gradually, could be a better approach. It may be able to find a guaranteed best solution to a given instance of the TSP in non-polynomial time (NP). 

Keep your curiosity going.",h8dpcq2,t3_p1chqf,1628575262.0,False
p1atyg,"I love this book, it’s an absolute gem. That being said, a lot of the chapters aren’t incredibly easy to understand and only clicked on my second read. However, pushing through and just reading the book will still get you a lot of knowledge and is definitely worth it in my opinion. 

Side note: if your end goal here is to learn to write software (in anything above C), then this book might not be what you want. It focuses mostly on the hardware and extreme low level software.",h8c4j6s,t3_p1atyg,1628545418.0,False
p1atyg,Nand To Tetris: https://www.nand2tetris.org/,h8cwp92,t3_p1atyg,1628558782.0,False
p1atyg,"Try the [*Write Great Code series*, by Randall Hyde](https://nostarch.com/writegreatcode1_2e)",h8cxime,t3_p1atyg,1628559169.0,False
p1atyg,"I should add if coding is the best way to learn Petzold’s concepts, it would be queued behind several other projects I have planned for my time in front of a laptop…",h8c1rwt,t3_p1atyg,1628544194.0,True
p162z2,"Glasses with anti-glare coating.
Your eyes will thank you later!",h8b8akf,t3_p162z2,1628531659.0,False
p162z2,"Take breaks, and dont work more than 8 hours a day.

I take 15min break at 10.00, 45min lunch, another 15min break at 15.00 with colleagues and a fruit/coffee",h8bn2i2,t3_p162z2,1628537902.0,False
p162z2,"I'd say listen to how you're feeling and learn when and how to take breaks so that it suits you. How humans work (especially in knowledge work) doesn't necessarily follow a fixed schedule.

I used to take walks or stretch or just stare into nothing (or all of those) in the middle of the day somewhat regularly, especially when I felt like I was getting too tired to work effectively, and it worked quite well. I didn't care if I took one or five breaks a day, because in the end what matters is getting things done with the time you work, not whether you take a few minutes more or less for breaks.

Listening to how you're feeling doesn't mean you should just walk away every time you feel a bit stumped or frustrated, of course. Knowing when you should just push through and when you should perhaps take a break is part of knowing yourself.

Perhaps some people function well on a fixed schedule of work and breaks, but I've found that's not what works best for me, so I wouldn't follow it dogmatically.",h8btpi6,t1_h8bn2i2,1628540718.0,False
p162z2,"That's a very objective way of looking at it, thank you.",h8c1ws2,t1_h8btpi6,1628544254.0,True
p162z2,"Standing up, streching, walking around, maybe have a chat with some kollege.

Moving is the most important thing.",h8b93qw,t3_p162z2,1628532000.0,False
p162z2,"You aren't getting head-aches, its actually eye-strains. And they are there because during night time (or a dark enviroment) your eyes aren't used to red green and blue, the colors on your computer. So try to do it during the day, and if you have to program during the night, take breaks, and i recommend stretching too.",h8dstjz,t3_p162z2,1628578014.0,False
p162z2,"Get your eyes checked, take breaks.",h8bxi4z,t3_p162z2,1628542329.0,False
p162z2,"/Not a professional but this is what I think\

Frequent breaks, hydration(water) and 20-20-20 -> Look far away (20 feet/6 meters+) every 20 minutes for 20 seconds",h8b5cyf,t3_p162z2,1628530433.0,False
p162z2,"I just don’t look at my laptop all day.

Instead I spend 50%+ of my day on my phone looking at Reddit, and my migraines are usually polite enough to hold off until Sunday...

Lol don’t look at your laptop all day. Walk away. Go outside. Take a walk. Talk to people. Take a lunch. Go to the gym.

Oh yeah also install f.lux",h8cibd4,t3_p162z2,1628551820.0,False
p162z2,"I do a few things to prevent that. Never using screen at full brightness, I always use around 70% or if the screen is too big, even less. I do a stretch and walk every hour or when possible ( fuck back to back meetings ). And of course I have glasses that protect from blue light.",h8e7akv,t3_p162z2,1628590758.0,False
p162z2,"First, figure out why you are getting the headache. Is it eye strain? Muscle fatigue? Something else?

Eye strain can be helped with glasses, anti glare coatings, etc. Also look at something far away whenever you break focus, this is to help the lens in the eye remain flexible as you age (slowing near sightedness). 

Headaches from muscle fatigue usually come from the muscles in the neck and shoulders. You can use massage, and exercise (isometrics, or weights) to keep your muscles flexible and break up the knots. I will sit with a tennis ball in my shoulder blade for 20 minutes while coding (move it around a little against the back of the chair) to encourage blood flow in that area.",h8f5643,t3_p162z2,1628609438.0,False
p162z2,Check out this nice article about microlearning! You would no longer have headaches if you follow the proper technique: https://home.osc.school/introduction-to-microlearning/,h8t4kh4,t3_p162z2,1628876794.0,False
p162z2,.,h8uokpo,t3_p162z2,1628900222.0,False
p10x5m,"For starters there is not ""standard"" alu design, you can find many different implementations, secondly you can build an 8 bit alu from 2 4bit alu elements.",h8a55x8,t3_p10x5m,1628514069.0,False
p10x5m,"> can build an 8 bit alu from 

Thanks a lot for the feedback!",h8a7tra,t1_h8a55x8,1628515506.0,True
p10x5m,Honestly it would take maybe half an hour on logisim to make one. If the operations arent too complex,h8aey8s,t3_p10x5m,1628518972.0,False
p10x5m,"Here's a very simple one from Ben Eater's 8-bit breadboard CPU series, though it can only perform addition and subtraction so that might not be all the functionality you're looking for
https://youtu.be/mOVOS9AjgFs",h8cf31o,t3_p10x5m,1628550271.0,False
p0vvh4,"I enjoy [Creel](https://www.youtube.com/user/WhatsACreel). A lot about assembly and optimizations close to the hardware.

[Jeff Geerling](https://www.youtube.com/user/geerlingguy) also has well made videos. A lot of raspberry pi cluster videos lately.

[LiveOverflow](https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w) has hacking and computer security videos. Here is one of his videos where he recommends other channels with similar contents: https://www.youtube.com/watch?v=GSraDuD4ziQ

[Sebastian Lague](https://www.youtube.com/user/Cercopithecan) for fantastic game making videos with wonderful graphics using creative approaches.

[Two Minute Papers](https://www.youtube.com/user/keeroyz) presents new research in computer graphics and simulation, most often including machine learning.

[stacksmashing](https://www.youtube.com/channel/UC3S8vxwRfqLBdIhgRlDRVzw) has hardware hacking videos.",h89yj11,t3_p0vvh4,1628510047.0,False
p0vvh4,"I love Creel, though he covers pretty intense topics",h8dmxbh,t1_h89yj11,1628573463.0,False
p0vvh4,"Two CS channels I really enjoy watching are:
• Computerphile (totally recommend, they talk about a wide variety of things relating to computers, programming, tech and security into detail, can sometimes go pretty deep on a topic and can be hard to follow if you dont have some knowledge on the certain topic but is usually a fun watch regardless)
• CrashCourse: Computer Science (talks about the history of computer science in general but does amazingly explaining concepts on how the computer reads code and how far technology and our understanding has evolved)

Some others:
• ModernVintageGamer (more gaming/homebrew related vids with CS talk but goes in depth on how retro consoles work like the hardware the console runs on, the challenges programmers had to face when developing a game or dealing with security in a game or console, homebrew, emulation, leaks, etc.). 
Some great vids for example (grab some popcorn): 
'How graphics worked on the game boy advance':  https://youtu.be/mpNWEbZdXNw
'How did Street Fighter Alpha 3 manage to fit on a single game boy advance cart' (fav): https://youtu.be/V1uCqk0DqBE
and 'Doom runs on everything': https://youtu.be/ZU4-7jltj0o

• TheHappieCat (havent watched enough but has videos explaining how games work)
• javidx9 (mainly in depth c++ programming tutorials, very friendly and nerdy. I get a lot of inspiration with the way he thinks and tackles problems and also gives good advice on what to look for and be cautious of when programming)

Hope this helped and wonder what other people like to watch",h89sdx9,t3_p0vvh4,1628505520.0,False
p0vvh4,Crash course: computer science is seriously good,h8afwfs,t1_h89sdx9,1628519413.0,False
p0vvh4,Agreed on computerphile will check out the other one too,h8dn1u0,t1_h8afwfs,1628573554.0,False
p0vvh4,"Gotta give another shout out to Sebastian Lague, who's already been mentioned here.  It's game-dev stuff, mostly, or maybe you could see it as general programming projects presented in game-like style just to have a more visual product which makes for better YouTube.  His videos are watchable without understanding a lick of code but he gives an overview of how the stuff is meant to work and the videos themselves will give you some really cool, extensible ideas to try your own hand at.  I watch every new video of his.",h8ah7o9,t3_p0vvh4,1628520019.0,False
p0vvh4,"Not a channel, but just found this and it's AWESOME: [Game Programming Patterns](https://gameprogrammingpatterns.com/contents.html). Just trust me and read the first two pages. I'm confident they'll convince you to read further!

Even if you have zero interest in game development, this guy's explanations/examples of design patterns are far and away the best I've ever seen.",h8c469p,t3_p0vvh4,1628545258.0,False
p0vvh4,"My favorite new channel if you like 3Blue1Brown is Reducible

https://youtube.com/c/Reducible",h8d80br,t3_p0vvh4,1628564382.0,False
p0vvh4,CS Dojo! They have how tos on learning different languages and also how to do various projects with those,h8bekj7,t3_p0vvh4,1628534310.0,False
p0vvh4,Big +1 for Code it Yourself channel: https://youtube.com/c/javidx9,h8bhaxl,t3_p0vvh4,1628535460.0,False
p0vvh4,"Maybe not what you’re looking for, but I really like Iain explains. He has helped me a lot with my digital communications class: https://youtube.com/channel/UCrltzuSvRbL3rpsvLDnFkuQ",h8a51q5,t3_p0vvh4,1628514005.0,False
p0vvh4,Where can I find some good hacker documentaries?,h8c61a1,t3_p0vvh4,1628546085.0,False
p0vvh4,"YouTubers like Nexpo, Barely Socialable, Disrupt have some great videos!",h8c66t7,t1_h8c61a1,1628546153.0,True
p0vvh4,"Awesome, thank you!",h8c6acc,t1_h8c66t7,1628546197.0,False
p0vvh4,Techlead is based,h8btsbm,t3_p0vvh4,1628540751.0,False
p0itk7,">Sex, age, ethnicity

Surely the eye is not the best feature to determine this.

>Drug consumption habits, emotions, health

I could see a potential application here.

>Fears, interests


Eye tracking alone won't work, additional context as to what someone is looking at is required.

>Everything else

Not buying it.",h87nd4t,t3_p0itk7,1628457023.0,False
p0itk7,">Surely the eye is not the best feature to determine this

The article includes the area surrounding the eye in the definition of ""eye tracking"", so we're also looking at wrinkles, skin colour, and eye shape.

>Eye tracking alone won't work, additional context as to what someone is looking at is required.

The article mentions this, it's achieved via use of heatmaps and a known view to gauge what the user is looking at. Also mentions these are combined with other factors, such as pupil dilation, to achieve recognition of interests & fears.

Don't have time to read all the references, so I'm still a little sceptical as to what extent of certainty this can really all achieve, but sounds like an interesting application!",h87q4sa,t1_h87nd4t,1628458320.0,False
p0itk7,"This is more of a review paper (consolidating tons of related studies already published) than a ""finding"". 

There are a lot of mights and maybes. The purpose of this paper is to urge researchers to explore stated connections.",h89gnzf,t3_p0itk7,1628495110.0,False
p0itk7,"I mean, I remember there were (and are?) doctors who claim (and whose claims are usually confirmed by their patients) to be able to diagnose patients by just looking them in the eyes. The human subconcious is extremely sensitive to sensory inputs we're not aware we actually perceive, and still our body and mind respond somehow, sometimes just in the shape of a subtle feeling. Maybe those doctors who claim to have mentioned ability have just learned to build their diagnoses based on the subconcious responses to these nearly imperceptible anomalies in the seccades of the eyes of their patients...",h89mgim,t3_p0itk7,1628500381.0,False
p05ywc,"Short answer, no. When you're addressing a specific memory location (eg via pointer/reference), the CPU is able to load it directly from memory in constant time. It's a function of the hardware at that point.",h84j9md,t3_p05ywc,1628389517.0,False
p05ywc,Is this an assumption in the RAM model?,h84n572,t1_h84j9md,1628391732.0,True
p05ywc,"RAM stands for “random access memory”, by its very design it’s meant to be able to access any memory address in roughly the same amount of time*

*For simplicities sake I’m omitting caching which can have a significant impact on run time but is rarely asymptotic on data size.",h84o5uq,t1_h84n572,1628392323.0,False
p05ywc,"I accept that you can access in constant time but you still need logarithmic time to read the address right?  


If I have one node of my tree has its children stored at location m in memory I need at least logm time to read m and if this occurs often enough this adds a factor of logm to the total run time.",h84t06l,t1_h84o5uq,1628395157.0,True
p05ywc,Your nodes point to addresses. If you look up n nodes you need O(n) time to do this,h84tv3y,t1_h84t06l,1628395684.0,False
p05ywc,"Is this an assumption in the word RAM model, it takes constant time to read a word?  Is this different in the RAM model?",h852j45,t1_h84tv3y,1628401542.0,True
p05ywc,"I see your point.

Given any particular RAM, the time it takes to access a node  in that RAM will not depend on on the number of nodes in your tree

However, across multiple machines, access time trends to increase with log(s) where is the size of that machine's RAM.

If part of your algorithm is responsible for dynamically choosing which RAM scheme to access and if the time each RAM scheme takes to accesss increases by log(s) with s being the RAM's capacity size, then yes, the log(m) would actually factor into the analysis and tree navigation would truly be log^2(m).

With that said, most algorithm analysis either just assumes an infinite RAM or just limits n's max to be whatever causes their particular machine to run out of memory . The type of dynamically increasing RAM size required to add log(n) to the analysis is almost never analyzed when studying Algorithm complexity. 

BTW, using smaller memory with faster access times like this is called caching and is something that is sometimes studied in algorithms, however caching systems aren't considered a RAM because all the accesses aren't in the same constant time.

Basically, if your algorithm is dynamically switching between memory types, then that memory can no longer be considered RAM.",h865ayp,t1_h84t06l,1628431607.0,False
p05ywc,"The size of the address is fixed. In modern computers that means every pointer will be 64 bits, whether you've got 1 GB of memory or 100 TB of memory. Small or old machines will have pointers fixed at 32 bits.",h851aao,t1_h84t06l,1628400616.0,False
p05ywc,"I meant as a theoretical model of computation that is given input size... and a computer with suitable amount of memory how much time does it take?    
Although somewhat unrealistic we assume input size can be arbitrarily large number n and express computation time in terms of n.",h853hof,t1_h851aao,1628402259.0,True
p05ywc,"Memsize is not a function of n, it's a function of the machine, so even if access is log(memsize) across various machines with different memory layouts an sizes,, that doesn't change running times in terms of n.",h84ljmo,t3_p05ywc,1628390809.0,False
p05ywc,"No, thats why its called RANDOM access memory. It may take different amounts of times for different memory regions, but it doesnt change a lot with memory size",h856gh5,t3_p05ywc,1628404555.0,False
p000om,"Perhaps in your browser.. type lcto:#ggj2023@globalgamejam.com to go straight to the channel using your preferred ""lc"" (live communication) client..? Or to send a message to ""john"" whose given you their ""lc"" address.. type lcto:[johnjohnson@hotcomm.com](mailto:johnjohnson@hotcomm.com) which would give you a pop up similar to what you would expect when you type [mailto:johnjohnson@hotmail.com](mailto:johnjohnson@hotmail.com).",h839xw3,t3_p000om,1628365988.0,True
p000om,"Could you express ""matrix"" to me from a more grade school elementary perspective? I have recently become a software/tech enthusiast and this is too interesting to put down but too complex to know in one short read.",h83wjw0,t3_p000om,1628377259.0,False
p000om,"As far as I understand it, matrix is a protocol for communication via the internet.

A bit (or maybe a lot) like SMTP (simple mail transfer protocol) enables mail to be exchanged between two servers, and each server to have multiple users (e.g. [user1@server1.com](mailto:user1@server1.com), [user2@server1.com](mailto:user2@server2.com), [user1@server2.com](mailto:user1@server2.com), [user2@server2.com](mailto:user2@server2.com) etc.).

&#x200B;

A matrix user has a ""matrix address"" (that is how I call it atleast), which e.g. may look like @user1:server1.com. Also.. there can be spaces (i.e. in a space multiple users can have text conversations, or ""do"" video calls/audio calls). A ""space address"" e.g. may look like #space1:server1.com. The official site is [matrix.org](https://matrix.org).

&#x200B;

What I am trying to express in this post.. I don't think that matrix is ever going to catch on. I don't find it realistic that people are going to refer to a mainstream protocol by an abstract ""nickname"" like matrix. I posted this to discuss what ""people"" think of a name closer to SMTP (which I believe served as inspiration for matrix, and has been adopted ""mainstream""). Hence.. SLCP (simple live communication protocol). Trying to explain what SLCP is overseas to someone unfamiliar with mathematics (i.e. doesn't know what a matrix is) might be more likely to succeed than trying to explain what ""the matrix protocol"" is.

Also.. with SLCP.. you could say that you have a live communications (LC) account AND an email (""MT""/email) account.",h83zhf5,t1_h83wjw0,1628378770.0,True
p000om,"I don't know why would I explain some random protocol to ma grandpa. Also, I don't see how ""SLCP"" is a better name than ""Matrix"", except that it's already used in other contexts.
If someone wants to use it... They will. But don't expect the ""mainstream"" communication providers to implement it just because the name. Nothing goes mainstream because of the name, and if people doen:?'t want it to be implemented, it won't be.",h85d6o5,t3_p000om,1628410242.0,False
p000om,">If someone wants to use it... They will.

&#x200B;

I am not intending use for independent projects. I think that the name matrix is not suitable for an international standard.

&#x200B;

>Nothing goes mainstream because of the name

Nonsense, without an ok name, you have no way to communicate what the technology is (e.g. if I say RFC 524, it might be very hard to explain that with RFC 524 you can send messages back and forth, but if I say email, then the explanation isn't necessary).",h8e1vxo,t1_h85d6o5,1628585938.0,True
p000om,"Do you think an ""international standard"" is born without projects using it?",h8eibd9,t1_h8e1vxo,1628598556.0,False
p000om,"This is no longer on topic.

&#x200B;

The title of this post includes the word ""hypothetical"".",h8emao2,t1_h8eibd9,1628600769.0,True
ozwved,"http://garden.irmacs.sfu.ca/category/theoretical_computer_science

This collection lists two of its problems as appropriate for an undergrad.",h83hkei,t3_ozwved,1628369751.0,False
ozwved,"Generally speaking, open problems are things for phd level and above.",h82ryjk,t3_ozwved,1628357521.0,False
ozwved,I would appreciate if you can share any such collection as well.,h82vakb,t1_h82ryjk,1628359033.0,True
ozwved,I’ll get you started with the Goldbach’s conjecture and the factoring problem. Basically find any problem in number theory.,h83a12f,t3_ozwved,1628366031.0,False
ozwved,"Another good collection

https://faculty.math.illinois.edu/~west/openp/index.html",h9xs3ao,t3_ozwved,1629656836.0,True
ozwved,prove that finding optimal golomn rulers is NP hard,hc5tk12,t3_ozwved,1631178198.0,False
ozvr09,"I will first repeat your argument to check whether I understood it correctly, after that I will point out were I think your argument breaks down.

1. We will proof that P = NP.
2. SAT3 can be solved via resolution in polynomial-time.
3. SAT can be reduced to SAT3.
4. All NP problems can be reduced to SAT.
5. Hence all NP problems can be solved in polynomial-time.
6. Which means P = NP.

I do not think step 2 is correct. I think all SAT3 can be solved via resolution however I do not think it would necessary be in polynomial-time. Could you think of a proof that resolution solves SAT3 in polynomial-time?

In step 3,4 and 5 I make statements without providing proof. I am certain that these statements are true because I saw them during my lectures. Send me a message incase you would like to see a proof.",h82vj2i,t3_ozvr09,1628359142.0,False
ozvr09,"I agree with you on all the above, and my question is indeed, why it isnt the case that, Sat3 can be solved in polynomial time, via Resolution? In my mind its essentially a nested loop over the amount of clauses and so its not exponential, so its polynomial... what am I missing?

Thank you",h82xzlo,t1_h82vj2i,1628360284.0,True
ozvr09,"I do not have a proof but I will try to make an argument that resolution is not polynomial. I would be interested in a more precise proof.

You think that resolution are two loops over the clauses. The algorithm tries to clash each clause against all the other clauses. However a clash would generate a new clause. This clause would need to be checked against the existing clauses and new clauses. This is why two loops over the clauses is not sufficient.

I do not think the argument is as strong as it could be. It may be easier to find the mistake if you could provide pseudocode.

(I want to mention that I am a computer science student and not a teacher)",h835jji,t1_h82xzlo,1628363868.0,False
ozvr09,"Same here so no worries, and I meant that the two loops would loop across the new ones as well, in pseudocode for ex

For i=0 i<clauses.size i++(

For j=0 clauses.size j++(

C1=clauses.get i

C2=clauses.get j

If C1 contains -V and if c2 contains V

Clauses.add ((C1.remove -V)+(c2.remove V))

))

Where v and -v are the interpretations of a single variable, ofcourse this would have to make sure of other things, for example that the resultant clause doesnt have any contradictions, i.e. that its only one variable that is contradicting and not more, since if more, for ex 123 and 1-2-3 could give 12-2, which I dont really know its interpretation but its not right and ofcourse just crashes everything",h83mbii,t1_h835jji,1628372089.0,True
ozvr09,Thank you for the pseudocode. I will try to analyze the complexity of your pseudocode. Your algorithm does use for loops however this does not mean it is polynomial. Because you add new clauses to clauses.size can grow way larger then n. It might be n!. I do not know. You would have to proof that clauses.size is also polynomial.,h85hnj6,t1_h83mbii,1628414308.0,False
ozvr09,"I guess thats part of my question, is proving that the number of clauses added is polynomial, the missing piece? Or if that it is polynomial in the first place? Or is there another piece missing? And thank you for your help",h85i64y,t1_h85hnj6,1628414778.0,True
ozvr09,"My intuition says that it is probably not polynomial. Because if that were the case then we would have proven P=NP which is unlikely. 

I invite you to prove that clauses.size is not polynomial. I think a counterexample could work",h85iew4,t1_h85i64y,1628415004.0,False
ozvr09,But is that the only thing hindering it from being a solution to p vs np though?,h85jkm9,t1_h85iew4,1628416082.0,True
ozvr09,"Remember that resolution proves a formula through contradiction. With resolution, you negate your original formula and convert to clauses (disjunctions of literals), which is a representation of the conjunction of all the disjunctions.

Then, we attempt to derive the empty clause from our set of clauses. If we are able to derive the empty clause, then our negated original formula cannot be true, and this our original formula must be true.

In this case, the clause {A, B ~B} would not be a contradiction because this clause represents A or B or ~B, which is always true, because clauses are disjunctions and not conjunctions.",h83vxzn,t1_h83mbii,1628376950.0,False
ozvr09,"I am not sure we are talking about the same resolution, somehow. What I know and mean by resolution is, for ex if we had

1OR2

And 

1OR!2

Then applying resolution to these two would give me 
1

So as you can see my clauses are in CNF and remain in CNF format and I dont change them or convert them to DNF. Am I missing something? or are we indeed not talking about the same thing? Please let me know if i made any mistakes.

Thank you",h84z60k,t1_h83vxzn,1628399098.0,True
ozvr09,Nobody understands why resolution should help and why it should work on polynomial time. either you write your argument down or people don't understand what you mean,h83j2aw,t3_ozvr09,1628370481.0,False
ozvr09,"Its written in the book that I am reading that it doesnt work in every instance with polynomial complexity, I dont understand why, which is what I am asking, why is it so, I am not arguing for something specific as much as I just want clarification and someone to explain to me what I am missing.",h83kztg,t1_h83j2aw,1628371435.0,True
ozvr09,"Are you trying to solve SAT in polynomial time? If so, please provide an algorithm, because I can't understand what you are trying to tell us.

If you're unsure whether you are clear enough, just write it in python. Or actual pseudocode.",h82v34q,t3_ozvr09,1628358937.0,False
ozvr09,"I mean I would love to believe that this would solve sat in polynomial time, but the book I am reading already says that it isnt polynomial for all instances, which is mainly my question, which instances are those and how or why do they cause it to not run in polynomial time.

A simple pseudocode 

For i=0 i<clauses.size i++(

For j=0 clauses.size j++(

C1=clauses.get i

C2=clauses.get j

If C1 contains -V and if c2 contains V

Clauses.add ((C1.remove -V)+(c2.remove V))

))",h83nxi4,t1_h82v34q,1628372890.0,True
ozvr09,He is telling us he knows more than the experts on the subject.,h8328h0,t1_h82v34q,1628362278.0,False
ozvr09,"I am not, I am actually stating a question, but thank you for your input",h83ng6m,t1_h8328h0,1628372648.0,True
ozvr09,"I'm not sure I understand your claim. It doesn't look like you've given a full algorithm for solving SAT and you didn't mention complexity (after the introduction to what the P vs NP problem is).

Is there a particular algorithm that uses resolution which you're thinking solves SAT in polynomial time? The Davis-Putnam algorithm is one example of an algorithm for solving SAT that uses resolution, but its worst-case time complexity is exponential.",h82vuzt,t3_ozvr09,1628359297.0,False
ozvr09,"I was talking about resolution it self as an algorithm, basically a nested loop over the clauses and applying resolution over the two in question and so on. I haven't read about the davis putnam algorithm could you explain to me why its worst complexity is exponential?",h82xk8t,t1_h82vuzt,1628360081.0,True
ozvr09,">I haven't read about the davis putnam algorithm could you explain to me why its worst complexity is exponential?

I doubt that I would be able to give a good full presentation of the proof without putting a decent amount of time into it myself. However, there is already a proof that there exists an infinite family of SAT instances whose resolution refutations are of exponential size relative to the formula size [here, starting on page 10 (Theorem 4.1)](https://homes.cs.washington.edu/~beame/papers/resj.pdf). There is another proof of the same result in Theorem 15.1 of Section 15.2.1 in *Computational Complexity: A Modern Approach* (2009; Arora, Barak).

Taking this as given, I can try to sketch the rest of the reasoning.

The particular family of SAT instances used in those proofs are statements about the pigeonhole principle: each SAT instance in the family corresponds to the statement that, with n pigeonholes and m pigeons, (i) each pigeon is in a pigeonhole and (ii) there is at most one pigeon in each pigeonhole (for some specific n and m for each instance in the family). They've named each SAT instance in this family ¬PHP^(m)\_n, with natural numbers m, n. However, this family is also restricted to only m and n values where m > n (that is, there are strictly more pigeons than pigeonholes). As a result, the SAT instances in this family are all unsatisfiable by the [pigeonhole principle](https://en.wikipedia.org/wiki/Pigeonhole_principle) (hence why each one has a resolution *refutation*). These SAT instances each have m + (m choose 2)n clauses, which is <= m^(3). The above proofs show that the resolution refutations for ¬PHP^(n)\_(n-1) have a size of at least 2^(n/20) when n >= 2, which is exponential in n. This means that the size of the resolution refutations grow exponentially relative to the number of the clauses of ¬PHP^(n)\_(n-1), as each SAT instance of that form has <= n^(3) clauses which is polynomial in n.

Note also that an algorithm's time complexity is always lower bounded by its space complexity. As a result, any computation of the resolution refutations of ¬PHP^(n)\_(n-1) will require (at least) exponential time relative to the number of clauses.",h83855a,t1_h82xk8t,1628365118.0,False
ozvr09,I think you have a misconception of how resolution actually works. Resolution is worst-case exponential time.,h839dtf,t1_h82xk8t,1628365718.0,False
ozvr09,"I have read so in the previous comment but I still dont know why, could you explain?",h83d5z5,t1_h839dtf,1628367583.0,True
ozvr09,"I can explain this, actually!
Resolution is not polynomial for 3-sat
It is only polynomial for 2-sat because you eliminate a clause in every step (notice this doesn’t happen with 3sat)
So maybe try to prove that resolution is polynomial for 3sat and see where it goes wrong :)",h84kmiq,t3_ozvr09,1628390287.0,False
ozvr09,"So the only problem/difference is that it doesnt add a clause each time? I mean by intuition, that would seem like a benefit since the clauses arent increasing at all, let alone in polynomial or exponential time, right? And I guess I'll try to do so. Thank you",h850gq8,t1_h84kmiq,1628400014.0,True
ozvr09,"I’m sorry I’m actually dumb and explained it wrong

You can actually reuse clauses in resolution which i’s forgot (for example, solve (-p, q)& (p, s)&(p, -s)&(-p, -q))

However, resolution does work in 2 sat because every solvent has size at most 2, therefore there are at most n^2 of them (with n the number of literals)

In 3sat see that this argument fails instantly. For example the solvent of (-p, q, r) and (p, s, t) is (q, r, s, t)

Iirc from there you can prove that it can be exponential.


Sorry for the confusion!",h86dc6u,t1_h850gq8,1628435636.0,False
ozvr09,Ask here https://cstheory.stackexchange.com,h845l0n,t3_ozvr09,1628382025.0,False
ozvr09,"Because P and NP are sets ?? If one element is in both sets it doesn't mean ALL elements are in both set
Sorry if i didn't understood your question correctly",h82i0ry,t3_ozvr09,1628352903.0,False
ozvr09,"I understand what you mean, but I think you're missing that every NP problem can be converted/rewritten into a 3SAT instance, I think its the Cook-Levin Theorem, and so, if 3SAT is proven to be in P, then it won't be in both, it would show that all NP problem are in P, and NP problems would cease to exist. That's what I understand, if there is any mistakes in what I am saying please let me know. 

But regarding resolution do you have any idea why it isn't a polynomial time algorithm?",h82inl2,t1_h82i0ry,1628353197.0,True
ozvr09,"Not Every NP problem, but yes any NPC problem (where C stans for complete). If you solve a NP problem in polynomial time, but the problem isnt NP-complete, then the only thing you did was to prove that only that problem is in P. But if you find a polynomial solution to a NPC problem, then yes, you would prove that P = NP since every NP problem can be reduced to a NPC problem

But apart from that, now i’m wondering too why the algorithm of Resolution couldn’t be a polynomial algorithm for solving every instance of SAT, it seems logical",h8313y4,t1_h82inl2,1628361748.0,False
ozvr09,Yes youre correct. Do you have insight regarding my question?,h831s93,t1_h8313y4,1628362068.0,True
ozvr09,"No  :( im sorry, im as intrigued as you now. In a few weeks i'll be starting the semester at my university and i'll be in touch with my professors again, perhaps i could ask one of them abouth this problem and i could give his/her answer to you.  


I did the subject of Logic for CS a few years ago so i don't remember very well and i may have some mistakes, but maybe in order to apply the Resolution algorithm you need that the wff (well formed formula) respects some kind of normal form, in spanish (my native language) this is called ""forma normal conjuntiva reducida"". So perhaps the time needed to bring any wff to this form takes a non-polynomial time, so in that case the Resolution algorithm wouldn't be able to decide in polynomial time if any wff is satisfacible or not. Again, I'm not sure if that's true so I may be wrong.

&#x200B;

I recently reviwed my Complexity classes and 3-SAT is NP-Complete, so if you solve 3-SAT in polynomial time, then P = NP.",h845lzz,t1_h831s93,1628382040.0,False
ozvr09,Yes you are right. But don't cook's theorem say that for all SAT problem? Then it has to be proven that unrestricted SAT are P,h82o7w7,t1_h82inl2,1628355787.0,False
ozvr09,"Same thing? As far as I know, what it says is that if a problem can be reduced to SAT then it is in NP, in result if SAT is proven to be in P, then all NP problems are in P as well.",h82pj71,t1_h82o7w7,1628356400.0,True
ozp0pa,We're getting X Æ A-12 levels of naming here,h82behc,t3_ozp0pa,1628349765.0,False
ozcs9n,"Google has a technical writing course:

https://developers.google.com/tech-writing",h7zi9ue,t3_ozcs9n,1628286466.0,False
ozbr22,"Proof writing is the fundamental skill to tackle courses of this kind. More specifically, familiarity with Predicate Logic, basic Set Theory, Relations, Functions and Proof Techniques are the mininum to know I think. Take a look at the first chapter (chapter 0) of Introduction to the Theory of Computation, by Sipser. It covers all these things briefly. If you struggle with some topic covered there, that's where you'll need to search for extra material to study. Best of luck!",h7zq9nh,t3_ozbr22,1628290238.0,False
ozbr22,Discrete math which will help you with proofs.,h80zmpv,t3_ozbr22,1628315947.0,False
ozbr22,"Just going by the name, learning how to write a formal proof might be helpful. Apart from that: what will the course even cover?",h7yuuml,t3_ozbr22,1628276099.0,False
oz3bq1,You should look at Khan Academy and Scratch,h7xgz0d,t3_oz3bq1,1628254336.0,False
oz3bq1,"Start them off with advanced data structures and algorithms. Tell them if they get the time complexity wrong that their recess and lunch are going to be taken away for the rest of the year and they’ll have to stay after school for 3 hours everyday until they get it right


Probably start them off with an intro to Python course and show them real world applications of programming",h7ym061,t3_oz3bq1,1628272303.0,False
oz3bq1,Scratch would be fantastic for that age group,h7ztjpp,t3_oz3bq1,1628291851.0,False
oz3bq1,I’m UK but I’m a CS teacher. How old is 7th grade? I’d happily share resources with you,h7x4bli,t3_oz3bq1,1628245766.0,False
oz3bq1,Hello! Yes please! Your a blessing! They are between 12 and 13 years of age.. I’ll direct message you. Thank you.,h7xf6oi,t1_h7x4bli,1628253363.0,True
oz3bq1,Yeah feel free! I’ll need an email address and I can share a one drive with you,h7xgjkk,t1_h7xf6oi,1628254110.0,False
oz3bq1,https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU,h7zas1v,t3_oz3bq1,1628283086.0,False
oz3bq1,"If you are in the U.S. some states have Computer Science standards for different grade levels. For instance, I'm from Indiana and ours are [here](https://www.google.com/url?sa=t&source=web&rct=j&url=https://www.decaturproud.org/academics/files/documents/IndianasAcademicStandardsforComputerScience.pdf&ved=2ahUKEwidtNrxyp3yAhU_hf0HHaLMCSMQFnoECBsQAg&sqi=2&usg=AOvVaw3Zyy89_c43B7q09qKCU_76). That will give you a decent high-level overview.",h7zxsla,t3_oz3bq1,1628294011.0,False
oz3bq1,"When I was in middle school I took a CS class and we did GameMaker. The programming is similar to scratch. It was a good way to learn about basics such as: input and output, loops, and if statements.",h7zy5sr,t3_oz3bq1,1628294200.0,False
oz3bq1,"I haven't done a lot of research but I know Google had some programs you can use. Might even be able to get yourself some certifications.

https://edu.google.com/code-with-google/",h7zz8le,t3_oz3bq1,1628294755.0,False
oz3bq1,"Scratch, and make sure you start with the basics, please dont give them anything too complicated and make them hate the subject",h80ls79,t3_oz3bq1,1628307121.0,False
oyunkm,I want to see the back.,h7vegyj,t3_oyunkm,1628206788.0,False
oyunkm,"Are you sure you are ready for this. Once seen, you may never go back. This is your final warning.",h7vembr,t1_h7vegyj,1628206863.0,True
oyunkm,I am ready.,h7vet5y,t1_h7vembr,1628206960.0,False
oyunkm,[The back](http://imgur.com/a/dexrkhu).  You may never be the same again.,h7vf3bo,t1_h7vet5y,1628207103.0,True
oyunkm,[deleted],h7vfidf,t1_h7vf3bo,1628207311.0,False
oyunkm,Just needs some tomato sauce and it'll be perfect.,h7vk5wh,t1_h7vfidf,1628209610.0,False
oyunkm,If he downloaded more ram it might fix it,h7zues3,t1_h7vk5wh,1628292285.0,False
oyunkm,haha cool,h7vkpo3,t1_h7vf3bo,1628209881.0,False
oyunkm,I like it.  For your next build I would definitely consider the color ones though.,h7wbhms,t1_h7vf3bo,1628223364.0,False
oyunkm,^ w ^,h7wbpg0,t1_h7wbhms,1628223496.0,True
oyunkm,So you specialize in assembly and firmware?,h7zqnec,t1_h7wbpg0,1628290422.0,False
oyunkm,I read up on the manual for the z80 and have been goofing with its ASM. I'm a bit new but I'm getting pretty familiar with how ask and firmware works.,h7zrx76,t1_h7zqnec,1628291042.0,True
oyunkm,"I’m actually more impressed after seeing the back. You really put a lot of effort into this clearly, I’m amazed that you were able to keep track of everything.",h7wfvcg,t1_h7vf3bo,1628226120.0,False
oyunkm,Learn color wire.,h7vi3p9,t1_h7vf3bo,1628208585.0,False
oyunkm,I ran out ;n;,h7vi5yv,t1_h7vi3p9,1628208616.0,True
oyunkm,which color did you run out of? it wasnt white,h7vlhxh,t1_h7vi5yv,1628210273.0,False
oyunkm,"He ran out of color, like in general.",h7wgjxy,t1_h7vlhxh,1628226578.0,False
oyunkm,Or did he just put every color into every wire...,h7wgncx,t1_h7wgjxy,1628226643.0,False
oyunkm,"i've also labeled mine with scotch tape

i have some passive components that i measured labeled with scotch tape as well. small simple thing to keep track of what i have on the board",h7wh98l,t1_h7wgjxy,1628227066.0,False
oyunkm,"""Ain't nobody got time fo dat!""",h7wiu9c,t1_h7wh98l,1628228181.0,False
oyunkm,What before you started?,h7wzrh5,t1_h7vi5yv,1628242037.0,False
oyunkm,Nice spaghetti,h7x23fz,t1_h7vf3bo,1628243994.0,False
oyunkm,my eyes!!!,h7wwqr9,t1_h7vf3bo,1628239411.0,False
oyunkm,It's not that bad.,h7wzptn,t1_h7vf3bo,1628241996.0,False
oyunkm,And that is why I do CS and not ECE,h7ze8wy,t1_h7vf3bo,1628284644.0,False
oyunkm,I expected more.,h7wnc3p,t1_h7vf3bo,1628231535.0,False
oyunkm,Reminds me of the reactor scene from Chernobyl.. A beautiful disaster,h7z7xbk,t1_h7vf3bo,1628281822.0,False
oyunkm,That checks out,h7zkfsh,t1_h7vf3bo,1628287457.0,False
oyunkm,When will it run doom.,h7vydud,t3_oyunkm,1628216594.0,False
oyunkm,"All joking aside, the ti84 plus uses a z80 for its CPU. And there is a port(kinda) of doom for the ti84 plus. So, that's not completely outside the realm of possibility. Hmmmm. You might be onto something.",h7w0pw1,t1_h7vydud,1628217734.0,True
oyunkm,So how to you write/upload programs ?,h7voamm,t3_oyunkm,1628211652.0,False
oyunkm,"You write the program you want in the assembly language of the target CPU, the Z80. Pull the eeprom chip out of the zif socket. Assemble the program it to hex code, then flash that file to the eeprom chip with an eeprom programmer. I use the tl866 eeprom programmer and Oshonsofts Z80 IDE to write, test, and debug my programs. https://www.oshonsoft.com/z80.html",h7vq5a8,t1_h7voamm,1628212564.0,True
oyunkm,I was expecting wire wrap not solder,h7vod3d,t3_oyunkm,1628211686.0,False
oyunkm,Is this computer science or electrical engineering?,h7vjelv,t3_oyunkm,1628209232.0,False
oyunkm,"They are two sides of the same coin. The more you know about the hardware, the better you can program. Its good to know how hardware works at a low level. Software and hardware should go hand and hand and work together. Computers aren't a magic box, its the sum of its parts.",h7vjvxo,t1_h7vjelv,1628209472.0,True
oyunkm,"Absolutely agree!! Not completely essential to know for PC dev but it *does* help. At the Z80 level it kind of is essential to know both sides.

I did something similar with 7-segment displays and hex keypad for readout and program input to the ram. Back in early 80s, 2K static ram and 16K eprom was a decent system. 8255 is killer interface chip with that processor!

Nice job with wiring! Ignore the haters. You learn more trying to find wiring errors this way. Couldn't afford multiple spools of wire when I did mine either; was wire wrapped with all yellow wire. Always laugh at movie bombs - mine would be impossible to know which wire to cut.",h7w351n,t1_h7vjvxo,1628218922.0,False
oyunkm,"Thanks! Haven't even finished college yet! Turns out you can buy a the 8255 (82c55), z80, eeprom (SST39SF010A), and SRAM (62256) all manufactured brand new from Mouser and digikey. And the 6502 brand new too! (WDC65C02S)

 Zilog still makes them. Crazy.",h7w3voe,t1_h7w351n,1628219295.0,True
oyunkm,So computer engineering?,h7w07e9,t1_h7vjvxo,1628217479.0,False
oyunkm,¯ \ _ (ツ)_/¯,h7w0bqp,t1_h7w07e9,1628217538.0,True
oyunkm,Next you're going to tell me that the math reference was an accident but math is the edge of this 2 sided coin.,h7vzx0z,t1_h7vjvxo,1628217338.0,False
oyunkm,"I absolutely agree with you. I'm an electrical engineer and i've been doing little Z80 projects for years and Z80 machine code was actually the first programming language i learned. Even though it's a very old processor it really helped my understanding of how computers actually work and give me an understanding that i wouldn't have gotten if i'd just played around with C or python.

And I completely agree that knowing more about hardware will make you a better programmer. When i finally learned C at university a lot of the optimisations quirks made sense to me from my assembly programming. And more than that, it was so much quicker to program and so much more user friendly.",h7zv41l,t1_h7vjvxo,1628292639.0,False
oyunkm,"Thanks! I'm glad you see where I'm going from. Fun fact, you can buy a z80 in original DIP packaging from mouser.com manufactured brand new. Zilog still makes them. Same for the 8255 and 65c02.",h7zvh3t,t1_h7zv41l,1628292823.0,True
oyunkm,Yeah i literally just bought a couple from mouser a couple days ago. Check out my post from a few months ago of my then half complete breadboard z80 computer. I've designed a PCB for it now and i've just sent it off for manufacturing.,h7zw01e,t1_h7zvh3t,1628293089.0,False
oyunkm,I plan on making one too! I started goofing with it in kicad and gonna send it to OSHpark eventually,h7zw5ny,t1_h7zw01e,1628293170.0,True
oyunkm,"Yeah i use kicad too and usually use JLC pcb for my projects but this time i had to use PCBway because it was a 4 layer PCB with tiny via's. Would have cost too much. 

But yeah definitely design a PCB for it. It's not as hard as it looks.",h7zwomu,t1_h7zw5ny,1628293441.0,False
oyunkm,Computer engineering.,h7w85w3,t1_h7vjelv,1628221517.0,False
oyunkm,You get it.,h7xqdkl,t1_h7w85w3,1628258907.0,False
oyunkm,This guy computer engineers,h7xs6qs,t1_h7xqdkl,1628259713.0,False
oyunkm,Why not both?,h7wnnhl,t1_h7vjelv,1628231778.0,False
oyunkm,Yes,h7wtm26,t1_h7vjelv,1628236715.0,False
oyunkm,"This is firmly Computer Engineering though this is more electrical engineering than computer science. CompSci is usually categorized as more theoretical. The other pillar of CE is software engineering, specifically lower level operating systems. Think instruction sets and kernels.",h7xr24j,t1_h7vjelv,1628259214.0,False
oyunkm,(Artificial) Life in the wilderness. It will find a way to survive.,h7vljwp,t3_oyunkm,1628210300.0,False
oyunkm,"I built an 8 bit CPU from 74 series TTL components in college.  This is not from scratch, young grasshopper.  I had 2 4-bit ALUs as the largest pre rolled components.  Nowadays you can just FPGA everything.  Still, keep the DIY alive.  I built a 4-bit ""computer"" with just a PLD, EPROM, and 8 bit latch, if you want to do what you have here with only 3 chips.",h7whpnz,t3_oyunkm,1628227382.0,False
oyunkm,Would a 74181 be too integrated then :p,h7wi8hk,t1_h7whpnz,1628227749.0,True
oyunkm,I've been thinking about implementing a simple CPU on a CPLD for a while now. Can you run me through the architecture of your one? I'm interested to see the corners you cut in order to fit it onto one PLD.,h81rrqv,t1_h7whpnz,1628339121.0,False
oyunkm,"Wire wrapped it all, I assume?",h7vt97v,t3_oyunkm,1628214075.0,False
oyunkm,https://m.imgur.com/a/dexrkhu,h7vthgd,t1_h7vt97v,1628214183.0,True
oyunkm,"Bravo, can’t wait for my CS1104 Computer Systems course to be started. Basically my school is using Virtual Lab to build computer.",h7w62ul,t3_oyunkm,1628220439.0,False
oyunkm,Then ur gonna love this: https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU,h7w68tb,t1_h7w62ul,1628220528.0,True
oyunkm,Thanks.,h7w6dca,t1_h7w68tb,1628220594.0,False
oyunkm,This is the kind of stuff I came to see here. Well done!,h7wkilj,t3_oyunkm,1628229403.0,False
oyunkm,"Super cool, dude.",h7x4noi,t3_oyunkm,1628246030.0,False
oyunkm,You should see if you can get zephyr running on it,h7xssvk,t3_oyunkm,1628259986.0,False
oyunkm,What's that?,h82tc54,t1_h7xssvk,1628358144.0,False
oyunkm,This is super cool.,h7ysvkm,t3_oyunkm,1628275259.0,False
oyunkm,Lol 😭,h7zluea,t3_oyunkm,1628288121.0,False
oyunkm,Good job. Always a fun project to build a retro computer.  I've done a a few 6502 based builds,h8bnbnu,t3_oyunkm,1628538010.0,False
oyunkm,That’s amazing good job,h7vsati,t3_oyunkm,1628213613.0,False
oyunkm,Thank thank ^ w ^,h7vsc9i,t1_h7vsati,1628213633.0,True
oyunkm,"For a moment here, I had thought you had built a simple Turing machine with logic gates on a breadboard. Then I saw that the computer *you made* targeted the Z80 architecture - with over 256 opcodes. That’s a very complex system to implement using logic gates on a breadboard. Now unless you’re telling me that you have a billion dollar clean room that can manufacture precise z80 architectured computers, *you didn’t build that computer*. From what I can glean, all you have done is plugged a processor and a few other components and passed it out as a computer *you built*. While I am not trying in any way to detract from your creation, I think it would do us all, especially those who lack a fundamental understanding of computer architecture, that OP did not build a computer. OP merely coupled a pre-built Z80 architectured chip and coupled it with some memory and code.",h7vv2wa,t3_oyunkm,1628214956.0,False
oyunkm,Bro. Shut up.,h7vvgt8,t1_h7vv2wa,1628215148.0,True
oyunkm,"If you didn't form all the required matter from nothing by using your very own big bang, you didn't *build* anything at all. Just assembled some ready-made components. Check mate OP",h7w5546,t1_h7vvgt8,1628219949.0,False
oyunkm,"Is this any way to address legitimate criticism? A simple “shut up” certainly doesn’t reflect well on you, and on the very merits of your post.",h7w6pos,t1_h7vvgt8,1628220770.0,False
oyunkm,"Minus 5 votes (and counting) from the community doesn't throw any red flags that it might not be socially acceptable to post that? You stated the obvious In a condescending manor. If you legitimately think that was okay to state what you did, then you have a serous social disability. Think before you speak.",h7w7cmn,t1_h7w6pos,1628221102.0,True
oyunkm,[deleted],h7wf8wu,t1_h7vv2wa,1628225708.0,False
oyunkm,[deleted],h85dz2c,t1_h7wf8wu,1628410938.0,False
oyunkm,[deleted],h85rhek,t1_h85dz2c,1628422914.0,False
oyunkm,"From on CE to another, niiice.",h7xrdwj,t3_oyunkm,1628259362.0,False
oyunkm,I read CE as Chip Enable >.>,h7yaao4,t1_h7xrdwj,1628267403.0,True
oyunkm,Did you remember to invent the universe?,h7y0gc6,t3_oyunkm,1628263248.0,False
oyunkm,Can you share which resources (theoretical) you used to build this. I want to build one someday. Will help if you can share the resources so that I can learn that.,h7yfv6n,t3_oyunkm,1628269734.0,False
oyunkm,Go on hackaday read through a few of the projects there and have a go. It's not as hard as you'd think.,h83u8v9,t1_h7yfv6n,1628376092.0,False
oyunkm,How difficult is this?,h7zk9oy,t3_oyunkm,1628287380.0,False
oyunkm,"So honestly that depends on your methods of making it. dont be fooled into thinking there is one way and one way only of doing these kinds of things. if you realy want to get into these kinds of things, here are some YT channels and playlists that i think that you might like.  [https://www.youtube.com/channel/UCvfTzrGE1VhDpxYnvqGQWQQ](https://www.youtube.com/channel/UCvfTzrGE1VhDpxYnvqGQWQQ)   [https://www.youtube.com/watch?v=HyznrdDSSGM&list=PLowKtXNTBypGqImE405J2565dvjafglHU](https://www.youtube.com/watch?v=HyznrdDSSGM&list=PLowKtXNTBypGqImE405J2565dvjafglHU)   [https://www.youtube.com/watch?v=LnzuMJLZRdU&list=PLowKtXNTBypFbtuVMUVXNR0z1mu7dp7eH](https://www.youtube.com/watch?v=LnzuMJLZRdU&list=PLowKtXNTBypFbtuVMUVXNR0z1mu7dp7eH)   [https://www.youtube.com/watch?v=3iHag4k4yEg&list=PLFhc0MFC8MiCDOh3cGFji3qQfXziB9yOw](https://www.youtube.com/watch?v=3iHag4k4yEg&list=PLFhc0MFC8MiCDOh3cGFji3qQfXziB9yOw)   [https://www.youtube.com/watch?v=K658R321f7I&list=PLFhc0MFC8MiD2QzxJKi\_bHqwpGBZZpYCt](https://www.youtube.com/watch?v=K658R321f7I&list=PLFhc0MFC8MiD2QzxJKi_bHqwpGBZZpYCt)   if you are committed, binge the shit out of these.",h80yr3t,t1_h7zk9oy,1628315304.0,True
oyunkm,Yes,h7zkjg9,t1_h7zk9oy,1628287504.0,True
oyunkm,did u make the chips too,h7zrpah,t3_oyunkm,1628290936.0,False
oylndl,Yes some capacitors hold residual charge and some system utilize that to cache short term states. Letting all the capacitors drain sometimes does rid electronics of some intermittent issues. Usually it's just more of a make sure everything is back to baseline step than it actually fixing the issue. But I have had not letting them drain bite me on a monitor before. Could not figure out for the life of me why the system could not recognize it. Left it unplugged for a while and like magic it worked.,h7tnnsc,t3_oylndl,1628180174.0,False
oylndl,That makes sense! So is “the longer the better” true or is it only relative to the capacitor size?,h7todf0,t1_h7tnnsc,1628180465.0,True
oylndl,"It depends there are capacitors that can hold a charge for a long, long time and then there are some that only hold it for a few seconds. Most that you find in consumer electronics are of the type that within 20 - 30 seconds they are completely drained. Hence the advice to wait 1 minute to be sure.",h7tovjh,t1_h7todf0,1628180672.0,False
oylndl,"I thought it was unplug, or turn off the power supply switch if you have one. Then push and hold the power button for about 20 seconds after the light goes off. This is supposed to clear any residual power that was stored",h7tnnxw,t3_oylndl,1628180176.0,False
oylndl,"How important do the steps matter?
Sometimes I forget to shut off the power switch after turning it off and before unplugging and always feel a panic that I messed the computer up (it’s home built with no coverage anymore)
I almost never hold the power button, I thought that would mess up the computer system more than a quick push.",h7to3xx,t1_h7tnnxw,1628180359.0,True
oylndl,"Quick push tells the OS to shut down, holding the button cuts power to most parts regardless of if the OS is ready or not. Data can be lost/go corrupt if the OS wasn't ready.

If it's not booted, and not plugged into a power source (and battery too if it's a laptop), holding the power button should simply flush remaining power.

Holding the power button is really only for 2 things:

1. Flushing the power after power sources have been disconnected
2. Forcing the PC to turn off if the OS freezes without having to get to the back of the PC and disconnect power.

Also just as a side note:

If someone reading this is planning on opening their PC, unplugging the PC and flushing the power is not enough to keep from shocking and ruining the parts. Be sure to ground yourself before touching any parts in the PC. It takes less static discharge than you can even feel to destroy a component.",h7u50g9,t1_h7to3xx,1628187264.0,False
oylndl,"Too many I don’t knows for me.
Not all power supply units have a switch on them so you have to pull the plug. I have my PC plugged into a surge protector with an on/off switch so I never technically pull a plug. 

If you have both, I think switch first and then remove the power or pull the plug.

Now your actual PC power button. I wouldn’t do that step until you know there is no more power going to the PC. All you’re wanting to do is let any power that could be saved in those fancy capacitor and ohms things discharge.",h7u3ycm,t1_h7to3xx,1628186836.0,False
oylndl,"It has to do with the capacitors in the power supply, motherboard, and other components of the computer.

Capacitors are basically like water tanks, they are filled with electricity, and then release it at a set rate. It’s the reason why when your power flickers, your microwave might need to have its time set again, while your PS4 stayed on without issue. The capacitors held enough electricity to keep the PS4’s system running stable during the hick-up in the power.

So by removing the main power source from the system for a set time, it drains the remaining charge in the capacitors and allows all volatile memory storage to completely lose power, thus removing any set information from lower-level components in the system.",h7ufmec,t3_oylndl,1628191536.0,False
oylndl,"So far no one has brought up standby power. There is a small microcontroller that handles turning the full system power on and off. It is fed via a dedicated 5v line on a desktop with an ATX power supply. On a laptop it's more tightly integrated, but works the same way only there's also the battery involved. Pulling all power and holding the power button removes the power from this system management controller and makes it reboot when standby power is reconnected.",h7vi4as,t3_oylndl,1628208593.0,False
oylndl,capacitors or smthn,hc5tqhy,t3_oylndl,1631178370.0,False
oyl8yl,"You can check out open dsa stuff

https://opendsa-server.cs.vt.edu/OpenDSA/Books/Everything/html/

Super helpful as just a reference",h7tojsw,t3_oyl8yl,1628180538.0,False
oyl8yl,"Have you tried ""Grokking Algorithms""
 https://www.amazon.com/dp/1617292230/ref=cm_sw_r_cp_awdb_imm_GKWZYAQD5BEGWAG8N48H",h814hy2,t3_oyl8yl,1628319703.0,False
oykwud,Thank you people who sort by new!,h7u2f00,t3_oykwud,1628186218.0,True
oykwud,"actually it showed up under Best! gg  


Will give it a read, just fyi saw this typo on first page https://rentruewang.github.io/learning-machine/intro.html#befor-we-start",h7uac3g,t1_h7u2f00,1628189395.0,False
oykwud,"Oh, wow thanks! I guess that's one fewer bug before release (-‿◦)",h7uby1q,t1_h7uac3g,1628190041.0,True
oykwud,Thank you.,h7uvjra,t3_oykwud,1628198181.0,False
oykwud,You're welcome :),h7w1aih,t1_h7uvjra,1628218020.0,True
oyk7q7,"Have you heard of ""set theory""? As a child I was always wondering why we had to learn it. Out of context it did not make much sense. Only when I started my ET training and later in EE it became clear that (basic) set theory is just boolean logic. If you don't code, never work with digital electronics or go into pure maths you'll probably never us it again. But as a stepping stone into CS, set theory could serve like an intro to boolean logic, because it's quite palpable, easy to understand.",h7tizq1,t3_oyk7q7,1628178241.0,False
oyk7q7,"The only reason I didn't quite start out with raw set theory is because many of the students had dealt with set theory in their math classes. However, that means it would be great to tie into. Thank you for the recommendation!",h7ur6hc,t1_h7tizq1,1628196326.0,True
oyim21,[https://imgur.com/2sJ7mgw](https://imgur.com/2sJ7mgw) hmmm k,h7tc6v9,t3_oyim21,1628175324.0,False
oyim21,Just use React + Bootstrap. It's super easy and you can think of it as a text based website builder. It also gives you the possibility to customize anything you want about your website.,h7vwa48,t3_oyim21,1628215548.0,False
oyim21,Also pls delete your other identical posts. It is probably the reason why no one else answered this question.,h7vwjav,t1_h7vwa48,1628215676.0,False
oy5miv,"Find and compare the major requirements for both at your school as it will vary. At my school the only overlap is the first intro to programming course then another course over assembly language (+ calculus & other general engineering stuff). In terms of jobs CS is the straight forward path for ""software engineer"" or ""software developer"" titles whereas CE I'm not so sure since I'm not studying it.",h7qrynj,t3_oy5miv,1628121901.0,False
oy5miv,Check out schools and compare the programs there to see what they share and where they differ.,h7qx7v6,t1_h7qrynj,1628124388.0,False
oy5miv,"Technically, computer science is actually theoretical fundamentals of computation in general. But that generally means that when going for a bachelor's you take programming and software engineering classes with a couple of ""language theory"" and ""boolean algebra"" classes thrown in. You can use this for essentially any programming/coding job. 

Computer engineering, as you say, is more focused on hardware. Though you probably would learn some assembly and probably a compiled language like C. I never took computer engineering, so I don't fully understand the field, but your career path would definitely involve more electronics.",h7qycl1,t3_oy5miv,1628124925.0,False
oy5miv,"Depends on the university. In some universities computer science is mainly programming, but that will often be called Software Engineering, and in others it's focused more on the theory of computation (like where I'm currently studying). Regardless there will always be some work on hardware, but that varies from university to university.

Computer Engineering is always more focused on hardware but to what degree varied by university.",h7vgd39,t3_oy5miv,1628207727.0,False
oy5d26,The absolute best of the best out there is [Teach Yourself C++ in 21 Days](https://i.redd.it/1fl5yi6bbpzz.png),h7qqklu,t3_oy5d26,1628121236.0,False
oy5d26,Had me in the first half ngl.,h7qqpzc,t1_h7qqklu,1628121308.0,True
oy5d26,"Honestly, any of the 2 famous Bjarne Stroustrup books on C++ are the best way, IMO. 

The C++ Programming Language or A Tour of C++. Choose the latter if you already have some experience with programming.",h7ro5r3,t3_oy5d26,1628138254.0,False
oy5d26,I'll  be sure to check both out.,h7s2v6i,t1_h7ro5r3,1628148772.0,True
oy5d26,I wouldn't stress too much about this. The course you've got is probably fine. No course will cover absolutely everything you will ever need anyway. You'll be looking things up for your entire career. What course you take won't play much of a role in getting a job. Being able to put together a decent portfolio of projects and do well in interviews is what is most important.  Much of what you learn that is actually useful will come during the time you spend working on projects.,h7s2w6v,t3_oy5d26,1628148795.0,False
oy5d26,"Thanks for the reply, this is some great advice. I'll be sure to take it on board.",h7ve9w8,t1_h7s2w6v,1628206690.0,True
oy5d26,I think the difference in time is because java is a higher level language and have more tools and stuff you’ll need to learn unlike c++ which is low level and doesn’t have much of an abstraction you kinda have to do everything yourself,h7rpm6q,t3_oy5d26,1628139132.0,False
oy5d26,"Oh right, that makes sense. Can't wait to start C++, from what I've seen the syntax looks identical to java.",h7s2x5v,t1_h7rpm6q,1628148817.0,True
oy5d26,"Also keep in mind to use c/c++ you need to have an understanding of how does the computer specifically the memory works bec these languages have direct access to the memory and what are pointers and hex values etc. I recommend giving this video a watch 

https://youtu.be/NKTfNv2T0FE",h7s36y9,t1_h7s2x5v,1628149042.0,False
oy5d26,Yeah I'll be sure to keep these in mind. Tbh the course I bought covers all those as well.,h7s3al9,t1_h7s36y9,1628149127.0,True
oy5d26,"By no means in the world C++ is low-level language. Heck, C isn't low-level despite having having even less abstraction. Both are capable of low-level programming though.",h7s37cn,t1_h7rpm6q,1628149052.0,False
oy5d26,"*C and C++ are now considered low-level languages because they have no automatic memory management. ... The only true low level programming is machine code or assembly (asm).*

I guess you are right but in contrast to java they aren’t the same at all unlike java and c#",h7s3czr,t1_h7s37cn,1628149184.0,False
oy5d26,"The lack of automatic memory management isn't the sole factor determining the level of abstraction.

I do agree, C is completely different league (I like to call it _mid-level_). But C++? It's just a bit lower than Java.",h7s47dx,t1_h7s3czr,1628149896.0,False
oy5d26,"I didn’t write that answer I copied it from google I still don’t get why c and c++ are considered different in that way 


“C++ is made to be a superset of C” - the creator of c++ 

I’m myself is relatively new to all this and it’s just confusing why c is still being used also the same thing with JavaScript and TypeScript",h7s4hku,t1_h7s47dx,1628150136.0,False
oy5d26,">I didn’t write that answer I copied it from google

Yes, I know. Don't trust Google's instant answers too much. At the very top of the search result you won't find the best resources, but sites with the best SEO.

>I still don’t get why c and c++ are considered different in that way

The level of abstraction is determined by the (reasonably) maximal abstraction offered by the language.

C and C++ operate also under two different paradigms - C is procedural, while C++ objective.

>“C++ is made to be a superset of C” - the creator of c++ 

C++ began its life as superset of C (at the beginning it was called _C with Classes_), but now it's completely separate language (though highly compatible with C). The misconception of C++ being superset of C often bites inexperienced programmers in the hand.

And why is C still used when we have other languages? Maybe not the best analogy, but why are bikes still used when we have motorbikes?",h7s5i12,t1_h7s4hku,1628150991.0,False
oy5d26,"There are jobs that require C++ skills? From where I come, students learn it just to clear some competitive programming questions for interviews and then go on to web dev/ app dev using stacks.",h7rtsmo,t3_oy5d26,1628141861.0,False
oxtgso,Cannot read this on darkmode :(,h7ox9p6,t3_oxtgso,1628093271.0,False
oxtgso,"If you click it you might get a white background!

https://i.redd.it/ekgnlp0amcf71.png",h7oyca3,t1_h7ox9p6,1628093711.0,True
oxtgso,This makes a lot more sense than seeing only the components on black background :D I feel stupid now for wondering what was up,h7pwrb1,t1_h7oyca3,1628107960.0,False
oxtgso,"You shouldn't feel stupid... I was also wondering what was up. 

... But then again, maybe we're both just stupid ¯\_(ツ)_/¯ ",h7rjbg2,t1_h7pwrb1,1628135429.0,False
oxmmta,"1. Measure temperature with great precision
2. Use it as random number or use it as seed to PRNG
3. Profit",h7nvx4l,t3_oxmmta,1628074295.0,False
oxmmta,"ahh, i thought it would be more complex",h7nzfqb,t1_h7nvx4l,1628076882.0,True
oxiqfu,"Some ideas:

* How the Internet Works
(You need to think hard about what about how the Internet works you really want them to learn, since fully covering it for a non-technical audience is probably about 3-4 hours.)

* Basic Principles if Cybersecurity
(Maybe think of a flashier title to catch their attention, though!)
Again, you’ll need to figure out what pieces of cybersecurity you really want them to learn.  I highly suggest tying the concepts to practical things they can actually use.  In fact, I think the ability to do that is a possible strength of this topic.",h7mt4ph,t3_oxiqfu,1628045512.0,False
oxiqfu,"The internet is a really cool thing to talk about.

Do you have any resources that show the connection between cybersecurity concepts and things that end-users can do? Or can you give an example of what you mean?",h7mw1d4,t1_h7mt4ph,1628047084.0,True
oxiqfu,"One example is talking about authentication, the “something you know, something you have, and something you are”.  They’ve certainly used passwords before.  The fingerprint or Face ID many people have on their phones is “something you are”.  It’s likely many of them have seen 2-factor authentication codes, and that’s “something you have” (theoretically, you can’t get the code unless you have your phone).

If you want to go another step, you can discuss the difference between AND two factors (“true” two-factor) which is more secure, and that is why they should use it on all important accounts (and consider it in all accounts, period).  In contrast to OR two factors, which is less secure, so it’s important to be sure two-factor is being done right.  Although this could be a user convenience - like your phone unlocking with your fingerprint OR your passcode - the fingerprint is convenience, not increased security.

If you want to go a hair more technical, you can talk about approaches to cracking passwords (assuming the authentication system itself is well designed).  What “brute force” is, and why long, random passwords are harder to brute force.  And then you can talk about dictionary attacks, common password lists, and other shortcuts crackers use to get non-random passwords more easily.  And the dangers of using the same password for more than one thing (the Kinked-In password breach is a good story to tell as an example).

If they have some math ability, you could have them figure out how many random passwords are possible (would need to be brute forced), depending on the length and character set.  You could even give them a rate of passwords guessed per minute and have them figure out how long brute forcing would take.  If they don’t have any background in probability, you’ll have to explain the calculation to them (don’t sidetrack into a probability lesson, just stay focused on the number of password combinations).  If they’re not going to understand _why_ the calculation produces the number of possible passwords, then I’d just skip this.  Learning to crunch some numbers without understanding why they work isn’t useful - the “why” is the important part.  So this really depends on their math level….

Those are just a few ideas, but there are many more possibilities.  Anything they are likely to come across as an ordinary user, decisions they might make, or things that directly affect them would all work well.",h7n0rzw,t1_h7mw1d4,1628049753.0,False
oxiqfu,"Honestly? I’d try to get across the difference between computer science and software engineering, or IT, or a career in programming, etc.",h7ndmzm,t3_oxiqfu,1628058444.0,False
oxiqfu,"i would personally set up some type of GUI for a simple game snd then tesch people if statements, variables, and for loops",h7mq0jr,t3_oxiqfu,1628043925.0,False
oxiqfu,"Maybe a graphical programming language, like Scratch, would be good for that?",h7mt8bg,t1_h7mq0jr,1628045563.0,False
oxiqfu,I'd first have to convince them that these concepts are worth studying. I'm not sure yet exactly how computer literate the people will be. I might have to explain what a program is before I try teaching them to write their own.,h7mvsbz,t1_h7mq0jr,1628046946.0,True
oxiqfu,"I’d explain what a computer is and how it works. Memory, storage, cpu, north bridge, south bridge, etc … hopefully something will get triggered and they will want to learn more. I’d use racetrack or multi lane highway to describe lanes for pci and memory width. Etc etc",h7mufwh,t3_oxiqfu,1628046220.0,False
oxiqfu,A really really simply crash course in computer architecture,h7n3wyv,t3_oxiqfu,1628051640.0,False
oxiqfu,Teach them how to build a circuit with a switch and led. Then maybe logic gates,h7odxsu,t3_oxiqfu,1628084948.0,False
oxiqfu,golomn rulers,hc5tstw,t3_oxiqfu,1631178434.0,False
oxgwkg,"They were used a lot, I think around WWII. They are cool, but you can't get more than about 3-4 significant figs out of them. It does seem fun designing the cams and linkages and stuff..... But digital computing is much faster and more flexible and more accurate, and hence how everything is done nowadays.",h7mcvmc,t3_oxgwkg,1628037473.0,False
oxgwkg,"If there is a way to produce very, very small ones, why not? We are rapidly approaching the limits of how small we can make devices with silicon. So, if you can still make them smaller by using something else, I think you find users for that.",h7mr1vh,t3_oxgwkg,1628044445.0,False
oxgtba,Good work! Now try to build one using only Nand gates :D,h7mdfvw,t3_oxgtba,1628037752.0,False
oxgtba,Build one using only transistors,h7ndg5h,t1_h7mdfvw,1628058301.0,False
oxgtba,This guy adds,h7nws3y,t1_h7ndg5h,1628074951.0,False
oxgtba,Now build it getting an education,h7o7per,t1_h7ndg5h,1628081878.0,False
oxgtba,Now build it giving birth,h7ozj0n,t1_h7o7per,1628094188.0,False
oxgtba,"So the story about why this calculator is built this way is because I wanted a few things for this. I wanted to be able to put the bigger number on either input, and specifically for this, I didn't want to use any not gates, so when there are no inputs, the entire calculator is off, no outputs at all. I wanted it like that just for the aesthetics. So I probably won't be doing it with Nand gates. But it does sound fun",h7me2ul,t1_h7mdfvw,1628038067.0,True
oxgtba,"Well, that does not stop you from trying with NAND gates? All boolean logic can be implemented by NAND 😁👍",h7nazzh,t1_h7me2ul,1628056417.0,False
oxgtba,"Very cool, I didn't actually know that.",h7oa71s,t1_h7nazzh,1628083152.0,True
oxgtba,"There’s an online course you can take called NAND-Tetris which I enjoyed! You build an entire computer off of nand gates and use it to eventually play Tetris on it, gives you a good understanding of how everything works!

I believe it is free but if not you can always audit it",h7q00vq,t1_h7oa71s,1628109336.0,False
oxgtba,And NOR too.,h7rtyrj,t1_h7oa71s,1628141976.0,False
oxgtba,Talking about aesthetics in the context of logic gates. Get a load of this nerd! Haha jk I feels that. When I did this stuff in school it does feel like you're an artist.,h7ontlc,t1_h7me2ul,1628089342.0,False
oxgtba,Ngl I kinda obsessed with this circuit. But yeah I kinda felt that way too.,h7oouqm,t1_h7ontlc,1628089780.0,True
oxgtba,What program did you make this with?,h7otd81,t1_h7me2ul,1628091666.0,False
oxgtba,"I made this image with Logic.ly 

Don't use it. The full version which unlocks SAVING costs 60$ when there's better programs for free.",h7ovs0w,t1_h7otd81,1628092655.0,True
oxgtba,What free ones do you recommend?,h7pgktc,t1_h7ovs0w,1628101160.0,False
oxgtba,There's a few options in the other comments. The one I downloaded was Logisim,h7pgx1z,t1_h7pgktc,1628101303.0,True
oxgtba,"Every. Single. Time. We would build a system then the professor says ""now with nand gates only!""",h7o6otw,t1_h7mdfvw,1628081327.0,False
oxgtba,"How is this an interesting problem though, surely you just replace all the gates with the nand gate variant, like if I wrote out the function definition for every time it's used inside a piece of code.",h7otptw,t1_h7o6otw,1628091810.0,False
oxgtba,"You should check out the book *Code:* *The* *Hidden* *Language* *of* *Computer* *Hardware* *and* *Software*.

If you enjoyed that project, I think you'll really enjoy the book. It's an easy read that is a lot of fun. It isn't super technical and more about history and *why* programming languages are the way they are.",h7mdkdl,t3_oxgtba,1628037812.0,False
oxgtba,"Love that book. A couple of more resources I have loved when I studied CS from first principles:

1. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
2. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)",h7mnp3u,t1_h7mdkdl,1628042768.0,False
oxgtba,What application did you use for this?,h7mmrm0,t3_oxgtba,1628042310.0,False
oxgtba,"I used the browser demo version. Of logic.ly

I'm not paying 60$ for it.",h7moblk,t1_h7mmrm0,1628043082.0,True
oxgtba,Try logisim!,h7njv4g,t1_h7moblk,1628063813.0,False
oxgtba,"No one should, it's a shit program lmao. See my other comment.",h7msc1v,t1_h7moblk,1628045104.0,False
oxgtba,logic circuit is free and has more features for a slightly more outdated look (still looks better than digital),h7nitp5,t1_h7moblk,1628062897.0,False
oxgtba,Digital is free and has more features. What are you talking about.,h7o8x18,t1_h7nitp5,1628082502.0,False
oxgtba,"This took me probably around 9 hours to figure out over the course of 3 days. And maybe my too high dosage of my prescription Adderall helped a bit.

The inputs go from right to left, which translates to: down to up. In this case. If you rotate it to the left 90 degrees you would get the right orientation.",h7mbrqj,t3_oxgtba,1628036922.0,True
oxgtba,[deleted],h7nz6y0,t1_h7mbrqj,1628076715.0,False
oxgtba,"Gotcha, thank you",h7o1dtx,t1_h7nz6y0,1628078186.0,True
oxgtba,"If you enjoyed putting this together, I think you might enjoy poking around with Verilog or SystemVerilog.",h7n5jlo,t3_oxgtba,1628052671.0,False
oxgtba,That subject is so fun! Cant get much closer to hardware without wiring up physical circuits.,h7qfudp,t3_oxgtba,1628116224.0,False
oxgtba,If you want a high performance simulator use this: https://github.com/hneemann/Digital,h7msa53,t3_oxgtba,1628045077.0,False
oxgtba,How does it compare to logisim?,h7nt041,t1_h7msa53,1628071933.0,False
oxgtba,Orders of magnitude faster with HDL support.,h7o6iiz,t1_h7nt041,1628081229.0,False
oxgtba,I'll check it out!,h7o8nxl,t1_h7msa53,1628082370.0,True
oxgtba,"AND/OR/XOR, carry the 1… Yeah this checks out.",h7nw5m8,t3_oxgtba,1628074476.0,False
oxgtba,This one actually uses only AND and XOR gates,h7nwumr,t1_h7nw5m8,1628075005.0,True
oxgtba,"Ok, but can an OR gate not be made of two XOR gates and an AND gate?

a OR b = (a XOR b) XOR (a AND b)",h7nzubf,t1_h7nwumr,1628077164.0,False
oxgtba,"Probably, I'm too tired to actually check rn lol",h7o1d1r,t1_h7nzubf,1628078172.0,True
oxgtba,Psst… the answer is yes. Lol,h7o1pti,t1_h7o1d1r,1628078401.0,False
oxgtba,I figured haha,h7o1z6k,t1_h7o1pti,1628078566.0,True
oxgtba,this is digital logic design?,h7nx505,t3_oxgtba,1628075220.0,False
oxgtba,I just looked online for a free logic gate simulator and made it.,h7o954a,t1_h7nx505,1628082618.0,True
oxgtba,Oh gosh thanks for making me relive the trauma of having implemented an 8 bit multiplier with logic gates.,h7oo2na,t3_oxgtba,1628089449.0,False
oxgtba,8 bits? Jeez sounds rough. I did this because it sounded like a neat challenge. A test of my logical reasoning. I kinda obsessed for a few days,h7oojcy,t1_h7oo2na,1628089647.0,True
oxgtba,"Multipliers are tougher than adders. If you've got the time and want to try, it's a great project. Very challenging.",h7oqz0y,t1_h7oojcy,1628090674.0,False
oxgtba,I'm actually thinking of doing a subtractor first,h7or7ff,t1_h7oqz0y,1628090769.0,True
oxdvfw,"Every problem facing our species will be, in some way, confronted with technology. No it's not dying... quite the opposite",h7lverj,t3_oxdvfw,1628029117.0,False
oxdvfw,"It’s very much not true. 

What do you mean by a computer science job, though? I’d imagine entry-level software developer positions will be in lower demand than they were, say, in the 00’s. This is mostly due to the proliferation of the modern bootcamp developer. Many people can now do your basic developer work. Add in the fact that many engineers who are not software engineers also pick up a language or two to pad their resume and the bar has definitely been raised for entry-level positions. 

However, I can tell you that it’s NOT easy to find people who are competent or that actually understand the field enough to go past a tier 1 developer position. There is a high demand for those who have the drive and the passion to stick to ‘it’. 

However, just to address the question in the way you phrase it: to think that computer science is dead is a laughable notion.",h7lw4g5,t3_oxdvfw,1628029446.0,False
oxdvfw,"I'm curious why you think the bar has been raised for entry level positions?

I've been working for 20 years, and found the bar is pretty much on the ground at this point for entry level, the standard of applicants has gone through the floor compared to 20 years ago.

I totally agree, finding competent people is becoming very difficult indeed.",h7m4tyg,t1_h7lw4g5,1628033545.0,False
oxdvfw,"I think that the bar has been raised in comparison to the wages being earned. That is to say that I think that there are now many “Do you know what a for loop is? Can you implement a hash table?” dev positions that pay less on average. So these people are getting dumber and dumber every year. However, I do think that software engineers proper have a lot more fire on their asses to be better, so I think tier one engineers are getting better. Or at least, they are being told to be full-stack engineers at an entry level (which is wild, to say the least) so they try harder. However, what might be true is that everyone is spread so damn thin that they have become more incompetent at specific technologies on average.",h7m5p5b,t1_h7m4tyg,1628033960.0,False
oxdvfw,"That's an interesting point of view, maybe it's a cultural difference.

For example, in my experience, 20 years ago, 'full stack' would be a bare minimum, and not really very impressive.

Maybe the problem now is that concept of a 'stack' is much more prevalent, i.e. 20 years ago, you just had to know how cgi-bin worked, and that was about it! Now, the 'stacks' are much more like black box and are skills in their own right.

I agree on the 'spreading themselves thin', maybe that's the problem, they try to be good at 10 things and end up being competent at nothing?",h7mfep3,t1_h7m5p5b,1628038722.0,False
oxdvfw,Boot campers,h7m67bd,t1_h7m4tyg,1628034199.0,False
oxdvfw,"I'm not sure I can follow your metaphors. What happened to ""the bar""? What happened to ""the standard of applicants""?",h7m7u1u,t1_h7m4tyg,1628034996.0,False
oxdvfw,"Sorry, by 'the bar', I mean the 'standard' expected of programmers for entry level positions, in my experience, the standard expected now, is very low compared to 20 years ago.

It may just be anecdotal, but the ability of entry level devs seems to be much worse now that it was when I first enter the industry 20 years ago.",h7meuic,t1_h7m7u1u,1628038444.0,False
oxdvfw,Lmao no,h7luiw5,t3_oxdvfw,1628028708.0,False
oxdvfw,reason being?,h7lvexx,t1_h7luiw5,1628029119.0,True
oxdvfw,Go to major corporations career pegs and see how many software dev and related jobs they post.,h7lxgmb,t1_h7lvexx,1628030065.0,False
oxdvfw,"People who don't have a job after graduation means they didn't use their student time properly. Focused exclusively of school but didn't do any internships so they didn't get any practical knowledge. After you graduate it's a whole new difficulty in getting a job. You can get a FAANG job pretty easily IMO before graduation just take advantage of internships and learn all you can. Else you are going to have a really hard time on interviews and play the interview system of leet code hell.

Sorry for going of a tangent but there are plenty of new problems and new jobs for a lot of areas. Be it just design, management, software development. There will be a lot of good fits depending on your aptitudes.",h7m08zt,t1_h7lvexx,1628031368.0,False
oxdvfw,">You can get a FAANG job pretty easily IMO before graduation just take advantage of internships and learn all you can

Which FAANG company would you say is easiest to get a job in, from your experience?",h7m7jv3,t1_h7m08zt,1628034859.0,False
oxdvfw,"If you're motivated by FAANG clout, Amazon hires SDE's at an insane rate, I'd apply there. If you're concerned with human rights and work/life balance I wouldn't. Download the app Blind. You'll get a good idea.",h7p3k72,t1_h7m7jv3,1628095824.0,False
oxdvfw,"No idea tbh it depends, I recently got hired at a FAANG but for me I think it was sort of situational (?). 

Most of the time it seems like a combination of skill and luck, turns out a team had to hire somebody asap and they needed somebody with front-end experience, and I did well in the interview. I have only worked at Amazon.",h7ms4na,t1_h7m7jv3,1628044999.0,False
oxdvfw,Why this one question is asked literally every day here recently?,h7lybks,t3_oxdvfw,1628030465.0,False
oxdvfw,Because the mods of this sub are beyond useless,h7m6h6u,t1_h7lybks,1628034332.0,False
oxdvfw,What mods could possibly do to prevent this question from being asked?,h7mnroo,t1_h7m6h6u,1628042805.0,False
oxdvfw,Develop a bot that filters repeated posts,h7n643h,t1_h7mnroo,1628053036.0,False
oxdvfw,"But I'm not wondering why I see those posts, but why somebody sits down and types them in the first place.",h7nvcwu,t1_h7n643h,1628073858.0,False
oxdvfw,"This is literally the golden age for Computers, we're all born in the perfect time for this lol",h7lxjlm,t3_oxdvfw,1628030103.0,False
oxdvfw,"Lol whoever is saying this is either a self-taught dev who went to a boot camp and couldn't find a job (boot camps can definitely help people, this is about bad personalities) or someone who is ignorant to the job market. Or both.

First jobs are always the hardest. After that it's pretty easy when you realize what interviews you excel at and which ones you don't.",h7m17ib,t3_oxdvfw,1628031820.0,False
oxdvfw,"Take a look around this website:

https://www.onetonline.org/",h7lv800,t3_oxdvfw,1628029030.0,False
oxdvfw,"I’m sure, with tens or even hundreds of unsolved problems that could change the way we approach every scientific field, that it’s dying.",h7ltxq7,t3_oxdvfw,1628028438.0,False
oxdvfw,"Dead or dying? No. Just No.

Over saturated?

This is a bit more complicated.

No, it's not over saturated, there are a lot of jobs out there, however...

There are a number of people graduated, who have minimal interest in the subject, and even less ability.

These people will struggle to find work, and maybe impossible to actually keep a job.

There will be a lot of graduate developers who struggle to find work, but it's not because of over-saturation, it's because they simply aren't up to it.

It's also worth remembering there are just about fuck all 'Computer Science' jobs, but lots of software development jobs, it's not the same thing.",h7m35f8,t3_oxdvfw,1628032738.0,False
oxdvfw,We still cannot write software without bugs. I am very doubtful that CS is anywhere close to dead.,h7mg3qk,t3_oxdvfw,1628039062.0,False
oxdvfw,It’s the compete opposite. I think these people are just mad they didn’t pursue CS.,h7mh4sa,t3_oxdvfw,1628039563.0,False
oxdvfw,"I read this today and it was eye opening : https://spectrum.ieee.org/the-stem-crisis-is-a-myth

Also entry level jobs are extremely extremely saturated at this point and it is insanely hard to get a job with no experience.",h7lso4p,t3_oxdvfw,1628027874.0,False
oxdvfw,"This is from 2013. I imagine some things have changed since then, for better or worse. There are thousands of job titles and areas of expertise within STEM. To assume there's a generalized shortage or abundance is reductionist and false. It's a highly complex issue even within computer science. I got a CS degree from a somewhat standard public institution and was hired by the first company I applied to. I know this is not the case for everyone but I don't know of a single person in my program without a job.",h7lt29p,t1_h7lso4p,1628028050.0,False
oxdvfw,"yea honestly has gotten worse, especially in anything not tech since we had a mini boom",h7ltdxp,t1_h7lt29p,1628028194.0,False
oxdvfw,"Anyone who says this has absolutely no idea what they’re talking about. 

Just go to any job board and see how many openings there are for software engineers. No other profession even comes close.",h7m1wrw,t3_oxdvfw,1628032148.0,False
oxdvfw,Literally the third time this question has been asked in the like 48 hours,h7m5u3j,t3_oxdvfw,1628034025.0,False
oxdvfw,"really? didn't notice, sorry",h7ng822,t1_h7m5u3j,1628060608.0,True
oxdvfw,Computer science is constantly evolving so some skills become obsolete and others become essential.,h7m75rb,t3_oxdvfw,1628034666.0,False
oxdvfw,"""Many people"" either naive or jealous...",h7mm4he,t3_oxdvfw,1628041998.0,False
oxdvfw,Yeah this tech fad will fade in a few months. Can’t wait to throw my iPhone away.,h7n2qel,t3_oxdvfw,1628050916.0,False
oxdvfw,"I wouldn't say it's dying but your not going to make huge money as entry level except at big companies like you could in the 1980s. 

back in the 1980s you could make effectively 200k Inflation adjusted doing basic dev work. and it was a lot easier back then.  not tons of JS frameworks and automated deployment...ECT 

Today you have tons of people with basic knowledge but managing a software project effectively still demands a lot of pay.

for example in the 1980s only people with CS degrees or really techy knew programming.  today anyone can watch a 30 minute YouTube tutorial and be at beginner level in python / JavaScript...ect",h7oa18f,t3_oxdvfw,1628083071.0,False
oxdvfw,"
Hello! You have made the mistake of writing ""ect"" instead of ""etc.""

""Ect"" is a common misspelling of ""etc,"" an abbreviated form of the Latin phrase ""et cetera."" Other abbreviated forms are **etc.**, **&c.**, **&c**, and **et cet.** The Latin translates as ""et"" to ""and"" + ""cetera"" to ""the rest;"" a literal translation to ""and the rest"" is the easiest way to remember how to use the phrase. 

[Check out the wikipedia entry if you want to learn more.](https://en.wikipedia.org/wiki/Et_cetera)

^(I am a bot, and this action was performed automatically. Comments with a score less than zero will be automatically removed. If I commented on your post and you don't like it, reply with ""!delete"" and I will remove the post, regardless of score. Message me for bug reports.)",h7oa2jr,t1_h7oa18f,1628083089.0,False
oxdvfw,"its funny how other industries have been around for thousands of years and people are worried about an industry that doesn't even have 100 years to its name is dying. Like if you are asking a question about saturation just go somewhere else. I am tired of lazy people wanting an easy path in life. Life is a competition, saturation does not matter if you strive to be better than others. You are comparing yourself to people that do the absolute minimum to get a job.",h8d9qfb,t3_oxdvfw,1628565315.0,False
oxdvfw,"I did NOT mean it in that way. I am in no way comparing my self to anyone. And no i do not want an ""easy"" job. That stuff only exists in magic land. This is earth we are talking about, if you don't do your best you ARE gonna get replaced. I was just asking to see if it is still a viable career choice, and not an industry that is on the way out.",h8e41uc,t1_h8d9qfb,1628587921.0,True
oxc1b5,This is what FFTs were made for.,h7lravr,t3_oxc1b5,1628027266.0,False
oxc1b5,"Yep. Fast Fourier Transformation is the way.  It can be a bit processing intensive sometimes though.

A more rudimentary approach for rough BPM detection is analyzing the average signal for volume changes, usually with some parameters to set the trigger range.  This is usually less accurate than FFT though.

It's an interesting case to balance efficiency with real-time accuracy.  It should help some for optimization that it's designed for a solo musician rather than an undefined input.",h7lzrh2,t1_h7lravr,1628031136.0,False
oxc1b5,"That's actually not an easy project. While beat detection has been done in various ways it's not a perfect solution and generally doesn't account for tempo changes and the like. Here's a link to a matlab project that does beat detection with explanations.

  
[https://www.clear.rice.edu/elec301/Projects01/beat\_sync/beatalgo.html](https://www.clear.rice.edu/elec301/Projects01/beat_sync/beatalgo.html)

&#x200B;

What languages are you familiar with?",h7m3n4g,t3_oxc1b5,1628032969.0,False
oxc1b5,"Any background of sound processing?
I think you can find papers/researches, but I can’t find a specific topic or sub-field.",h7lhxxx,t3_oxc1b5,1628023248.0,False
oxc1b5,haha nope ive never done sound processing,h7lp4ht,t1_h7lhxxx,1628026310.0,True
oxc1b5,"I've been searching for an app like this for my personal use. It's a great idea, if you manage to get it working please let me know",h7lnjb5,t3_oxc1b5,1628025628.0,False
oxc1b5,will do!,h7lowb7,t1_h7lnjb5,1628026212.0,True
owqfmg,"You must look up explanation for the 'IEEE754 norm'. It's not as complicated as it sounds. In nearly every langages, floating points will be encoded that way.",h7hljbm,t3_owqfmg,1627946703.0,False
owqfmg,Thank you. Will have a look.,h7hmd11,t1_h7hljbm,1627947092.0,True
owqfmg,"The bits of a floating-point number would be stored like this:

    seeeeeee emmmmmmm mmmmmmmm mmmmmmmm

Here `s` is the sign (a 0 bit is positive, a 1 bit is negative), `e` is the exponent and `m` is the mantissa.

The above example is for a 32-bit `float`; a 64-bit `double` would be similar, but have more bits for the exponent (11) and the mantissa (53).

To compute the numeric value of the floating-point represented by the bits above, you would take the equation:

    sign * pow(2, exponent - 127) * (1 + mantissa * pow(2, -23))

(the constants `127` and `23` depend on the number of bits; they'd be different for a 64-bit `double`, for example)

So for example, if you had a `float` with the bits

    seeeeeee emmmmmmm mmmmmmmm mmmmmmmm
    11000000 01000000 00000000 00000000

        sign = ""bit 1"" = -1
    exponent = 0b1000000 = 128
    mantissa = 0b10000000000000000000000 = pow(2, 22)

You would get the number

    -1 * pow(2, 128 - 127) * (1 + pow(2, 22) * pow(2, -23))

    == -1 * 2 * (1 + 0.5)  == -3

Aside from the above, there are also some special bit representations to represent numbers like `+0`, `-0`, `+Infinity`, `-Infinity` and `NaN`.",h7hnzb0,t3_owqfmg,1627947858.0,False
owqfmg,The Wikipedia article on Double-precision floating-point format is approachable too.,h7hod10,t3_owqfmg,1627948041.0,False
owqfmg,It sounds like you have the right idea - it's just the binary equivalent of scientific notation.,h7hsqzy,t3_owqfmg,1627950129.0,False
owhg74,"CODE by Charles Petzold is what you're looking for.  Starts with the idea of a switch turning on a lightbulb, and walks you through building that idea up to a general purpose CPU.",h7fycg6,t3_owhg74,1627920574.0,False
owhg74,Is this a book?,h7jcudd,t1_h7fycg6,1627986558.0,False
owhg74,"Basically, data is represented in binary which is 1s and 0s in RAM.

The CPU is a bunch of circuits to do specific things e.g you can make a circuit that adds numbers together, you can make a circuit to write to an address in RAM etc. So there's a whole load of circuits, and you can number them and give each circuit a binary code. E.g If there were 8 circuits in total you might say 010 is add and 011 is subtract etc.

You can then write in RAM something like 010 10 10 (add 2 2) meaning 2+2. There is another circuit that reads specific memory addresses, which is basically like: if it starts in 010, send the arguments to the add circuit; if its 011, send them to the subtract circuit etc.

So programs are basically a list of binary numbers corresponding to instructions.
A special register is set to the address that instructions start at, say, 0. The CPU then reads address 0, executes it then the register is incremented then address 1 is read and  executed etc on and on. There are also special instructions that change the line number register so that it doesn't always have to go line by line. They are called jumps and they are what let us do loops and stuff.

Obviously as this is a Reddit comment its not in full detail just a high level overview. I may have simplified some things for understanding and I havent.really gone into.exactly how circuits work.",h7g5k5z,t3_owhg74,1627923639.0,False
owhg74,"Thank you, this makes a whole lot more sense to me now.",h7gkzbi,t1_h7g5k5z,1627930312.0,True
owhg74,Also Id suggest playing www.nandgame.com to get a better idea of how circuits build on each other to form a CPU,h7gqm36,t1_h7gkzbi,1627932765.0,False
owhg74,"If you can accept the idea of a NAND gate, the NAND-to-Tetris site is helpful. https://www.nand2tetris.org It builds a CPU from basic gates and then runs a game on it. 

If you struggle with the idea of electricity to a NAND gate, a general physics book and / or circuit simulator is helpful. RTL transistors are easier to grok than TTL transistors, so study them first. Wikipedia is also helpful here.",h7gcz65,t3_owhg74,1627926829.0,False
owhg74,"Have a look at Ben Eater's ""How to build a 8-Bit Computer"" from scratch: https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU",h7gnm8j,t3_owhg74,1627931462.0,False
owhg74,Hennessy and Patterson’s Computer architecture book might help shed some light. That is fairly advanced but a great read.,h7x8khv,t3_owhg74,1628249153.0,False
owhg74,Find a copy of the book Digital Computer Electronics (DEC) by Albert Paul Malvino and Jerald A. Brown. It covers everything you want to learn and more.,h87rmn8,t3_owhg74,1628459015.0,False
owhg74,"look up ben eater making a computer from scratch. this is what made it click for me

https://www.youtube.com/watch?v=HyznrdDSSGM&list=PLowKtXNTBypGqImE405J2565dvjafglHU",hc5twrz,t3_owhg74,1631178542.0,False
owhg74,There is a really good book on this but I can’t remember what it’s called... I will try to find it,h7fwymr,t3_owhg74,1627919977.0,False
owhg74,"Two words: Fetch, execute.",h7ggd8u,t3_owhg74,1627928317.0,False
owg4zz,"The site is live here: [https://app.learney.me/](https://app.learney.me/).

  
When learning ML online I found it difficult to find the right content at the right level for me and understand how concepts fitted together. Now I'm a ML researcher I'm trying to help others in the position I was in a couple of years ago!  


  
[Join here](https://join.slack.com/t/learneyalphatesters/shared_invite/zt-tf37n610-x8rIwDk6eeVctTVZqQkp7Q) for access to the quizzes through Slack (currently no access as the quiz bot is being upgraded, but you'll hear when it's next available by joining the Slack!)  


  
Hope you like it! What do you think? Would love to hear your feedback & suggestions :)",h7foaws,t3_owg4zz,1627916120.0,True
owg4zz,Amazing and speechless....,h7iyjxn,t1_h7foaws,1627974181.0,False
owg4zz,Thank you.,h7ns03y,t1_h7foaws,1628071084.0,False
owg4zz,"thanks for this, saving for future use :D",h7fr0kh,t3_owg4zz,1627917353.0,False
owg4zz,Thank you for the kind words!! :D,h7frjhc,t1_h7fr0kh,1627917592.0,True
owg4zz,"This is neat! Although certainly missing a bunch of stuff... There a A LOT that is in the machine learning box that isn't there. Trees, for example? Similarly, there seems to be quite a number of missing connections. E.g. PCA and eigen vectors? I like the materials listed upon click and the toggle buttons.",h7fpv9g,t3_owg4zz,1627916830.0,False
owg4zz,"Thanks for the feedback and suggestions :)

You're totally right - it misses out A LOT of stuff from the 'ML' bucket! We didn't try to map *all* of ML yet. We're going to add community contribution to help the map to scale up and cover things more thoroughly

Good point about PCA & eigenvectors! Will add that!",h7frh02,t1_h7fpv9g,1627917561.0,True
owg4zz,"I'm taking this month to study AI before returning to classes, since due to the pandemic the quality of my classes dropped dramatically, and I feel like I don't know enough. I am absolutely amazed by your work because it's precisely what I was looking for to help me visualize such a complex topic. Thank you so much.",h7g5i8m,t3_owg4zz,1627923617.0,False
owg4zz,Woohoo!! Glad you find it useful!! :D,h7g66y5,t1_h7g5i8m,1627923910.0,True
owg4zz,What's the front-end stack?,h7g8o7t,t3_owg4zz,1627924973.0,False
owg4zz,"It's just Vanilla JS (+HTML & CSS). 

Going to migrate to react.js once I spend enough time learning it (maybe I'll add a map to learn react.js when I'm done :P)",h7glhaj,t1_h7g8o7t,1627930529.0,True
owg4zz,Are you using something like D3.js for visualization?,h7ighpm,t1_h7glhaj,1627961926.0,False
owg4zz,Ah I should have mentioned I'm using [cytoscape.js](https://js.cytoscape.org/) for the network visualisation!,h7j5ybu,t1_h7ighpm,1627980588.0,True
owg4zz,Nice,h7j5zjx,t1_h7j5ybu,1627980618.0,False
owg4zz,Well a lot of stuff is still missing but the idea has a huge potential. I'll follow this project for sure! Keep doing it.,h7linbh,t3_owg4zz,1628023548.0,False
owg4zz,Ironic. Where's the node on Knowledge Graphs? ;),h7fuij2,t3_owg4zz,1627918913.0,False
owg4zz,Haha!! :D,h7fv16s,t1_h7fuij2,1627919138.0,True
owg4zz,Wow! you did a great job. I liked your learning map. This is cool.,h7gly5m,t3_owg4zz,1627930733.0,False
owg4zz,Thank you so much! :D,h7gmm1i,t1_h7gly5m,1627931023.0,True
owg4zz,Can you tell me what languages you used for front-end ?,h7gobnu,t1_h7gmm1i,1627931773.0,False
owg4zz,"Just Vanilla JS

The name of the graphing library I used is cytoscape.js",h7gpo5x,t1_h7gobnu,1627932358.0,True
owg4zz,"woah...cool map! 

maybe consider adding something on ""genetic algorithms""?",h7gna4t,t3_owg4zz,1627931314.0,False
owg4zz,Great idea!! Thank you :),h7gq9ji,t1_h7gna4t,1627932614.0,True
owg4zz,!remindme 6 hours,h7grjqx,t3_owg4zz,1627933176.0,False
owg4zz,"I will be messaging you in 6 hours on [**2021-08-03 01:39:36 UTC**](http://www.wolframalpha.com/input/?i=2021-08-03%2001:39:36%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/owg4zz/i_built_an_interactive_map_for_selfteaching/h7grjqx/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fowg4zz%2Fi_built_an_interactive_map_for_selfteaching%2Fh7grjqx%2F%5D%0A%0ARemindMe%21%202021-08-03%2001%3A39%3A36%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20owg4zz)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",h7grmvb,t1_h7grjqx,1627933214.0,False
owg4zz,"I have a similar idea of creating an interactive map but doesn’t limit to Machine Learning, do you mind sharing some tips while doing these kind of maps?",h7hebqu,t3_owg4zz,1627943309.0,False
owg4zz,"Ours isn't limited to ML either!

We're expanding through moderated community contribution, although this hasn't been implemented in the prototype yet!",h7jhigb,t1_h7hebqu,1627990090.0,True
owg4zz,Is there a way that I can also contribute to the project? I want to learn more about this!,h7jmjml,t1_h7jhigb,1627993378.0,False
owg4zz,"Absolutely!!

Join our Slack [here](https://join.slack.com/t/learneyalphatesters/shared_invite/zt-tf37n610-x8rIwDk6eeVctTVZqQkp7Q) to contribute :)",h7k2ntz,t1_h7jmjml,1628001548.0,True
owg4zz,"OHMYGOD THANK YOU!!! I’ve been searching online for over a year to figure out where/how to start learning ML on my own and decided to start with Naive Bayes, since I want to learn the actual algorithms and logic behind everything AI/ML related and that seemed like a good place to start. This is gonna be a HUGE help!! Thank you!!!",h7hen9x,t3_owg4zz,1627943457.0,False
owg4zz,WOO HOO!!! SO happy you find this useful :D,h7jgtkv,t1_h7hen9x,1627989598.0,True
owg4zz,"Okay I know I commented before but I just wanted to mention something on the topic of Optimization! I have a couple of simple optimization algorithms code and visualized for people/students that have trouble with only auditory learning (for visual learners). It’s a personal project of mine that I’m still working on. If you ever have need for a visualization in that area, or a way to make something interactive for the users of your site, I could port all of them from Processing into p5.js so you could use them too. So far the algorithms I have coded up and visualized are: Genetic Algorithm, Particle Swarm Optimization, Firefly Algorithm, and Differential Evolution.",h7hhkof,t3_owg4zz,1627944821.0,False
owg4zz,"This sounds AMAZING!! Yes please!! :D

We're huge fans of more interactive, engaging and intuitive explanations that are possible online (like [Seeing Theory](https://seeing-theory.brown.edu/#firstPage) or 3B1B)\* but seem to be reasonably rarely done well

**Please** [join our slack](https://join.slack.com/t/learneyalphatesters/shared_invite/zt-tf37n610-x8rIwDk6eeVctTVZqQkp7Q) so we can keep in touch. Would love to include this!

\*We WISH these were incentivised online - currently they're 1-offs and we really think there's so much to be gained from these types of explanations being incentivised.",h7jhy32,t1_h7hhkof,1627990391.0,True
owg4zz,"Aaaa!! Sorry for the late reply! Had some personal stuff going on so I completely missed this! 
Thank you for the slack invite! I’ll join up to see if you’d be able to include this! I got 2 examples readied up that I could show you to see if you’d be interested (video format), so I’ll see if I can find a spot to post in on your slack, or if not, find a way to show you some other way. c:",h8e2sii,t1_h7jhy32,1628586766.0,False
owg4zz,"Amazing, would really be helpful. Thanks for building this!",h7ivvk2,t3_owg4zz,1627972027.0,False
owg4zz,added to bookmarks,h7jtycw,t3_owg4zz,1627997427.0,False
owg4zz,the god himself,hbtayq7,t3_owg4zz,1630939677.0,False
owendc,"I think it depends on what you consider computer science. Simple website design and other things that can be thrown together really simply could be outsourced or even automated (see GitHub copilot). However the really hard stuff that hasn’t been solved 100 times already probably won’t be outsourced or automated. Long story short, if you can solve hard problems in CS I wouldn’t worry too much.",h7fkccp,t3_owendc,1627914285.0,False
owendc,"This is something that I have been warned about for my entire career (which I am several decades into) and if anything outsourcing is less common now then it was then.  It *is* harder to make a living out of the ""simple"" stuff, like basic HTML/CSS, but in turn there's a whole bunch more ""complex"" stuff that requires specialist knowledge to implement effectively.

I think a crash in computing salaries is a persistent fantasy among MBAs who run businesses where those salaries are a large percentage of expenses.",h7g92uy,t3_owendc,1627925148.0,False
owendc,"No. Simple economic principle will always apply: you pay for what you get, you get what you pay for. 

I’ve heard so many nightmare stories about people outsourcing to countries like India.",h7fr9ah,t3_owendc,1627917464.0,False
owendc,"With all the info available on the internet, i do not think the gatekeeping will last much longer.

I would assume it will eventually crash and then come back in a new and improved form.",h7fk8q5,t3_owendc,1627914238.0,False
owendc,[deleted],h7fiszu,t3_owendc,1627913560.0,False
owendc,Can you explain?,h7fivmn,t1_h7fiszu,1627913595.0,True
owendc,I think that time hasn't come yet but maybe oversaturated in next 5-6 years.,h7gpm82,t3_owendc,1627932334.0,False
owendc,"Wait, what does ""outsourcing"" mean in the this context?",h7h2pru,t3_owendc,1627938078.0,False
owendc,the people that fear of saturation are those that have no clue what they are doing and have no understanding of actual computer science.,h8da49q,t3_owendc,1628565524.0,False
ow9nem,"One of the fundamental theorems of computer science is that any sufficiently powerful system for computation is equivalent to every other sufficiently powerful system for computation e.g. they can all simulate each other and they can all compute that which is computable. Your recipient system sounds sufficiently powerful, so any system (lambda calculus, Turing machine, text editor macros) would work. 

It’s kinda of funny to apply some CS concepts to your system though.

Infinite loops: 1. Add milk 2. Add eggs 3. Go to step 2.

Recursion: Directions for use 1. Open packet 2. Use

Code / data equivalence: write the recipe on rice paper. Throw the recipe into a pot of boiling water. 

Emulation: recipe includes some python rules, directs you to write down a python program on the back of the page line by line, then execute it. 

Non-determinism: 1. Throw the bottles of spice cabinet on the floor 2. Put them back on the shelf. 3. If Thyme is not the first bottle, go to step 1.",h7evawd,t3_ow9nem,1627898381.0,False
ow9nem,"This depends on what you want your language to be able to do.  


Most of these tasks are linear, so having a simple list of instructions should suffice. For routing, I guess you can have a list of instructions like  
""Turn right in 300m. Turn left in 500m. Drive straight on in 1830m"" or something like this. This gets more complicated, of course, if your instruction are supposed to include lane changes and so on, but modern routing software has mostly figured this out.

&#x200B;

For cooking, you have a set of possible actions, and the language would look roughtly similar. Named binders might make sense.

""Let X := 300g flour; Y := 5 eggs. Let Z := extract egg yolk from Y. Let A := Mix X and Y. Stir A. Bake A at 250°C"".",h7exfi5,t3_ow9nem,1627900147.0,False
ow14p2,"Decimal precision occupies memory space in a conputer.  the more precise a fraction or decimal is the more memory is needed.  The maximum number of decimal spaces a number can hold depends slightly on the architecture of the machine you are using, but nowadays memory is so big that the answer depends mostly on the coding language that is used to allocate the memory space needed for the number being represented.  

For example in c++ on a 32 bit or 64 bit machine declaring a double precision floating point number will allocate you 8 bytes of memory space for that number.  This is enough space to store approximately15 digits worth if precision.  i.e. 
.123456789012345

Someone please correct me if I have made an error in my explanation.",h7d52dt,t3_ow14p2,1627857787.0,False
ow14p2,"IEEE single precision floating point can hold the maximum value of:

(2 − 2−23) × 2127 ≈ 3.4028235 × 10\^38. 

So anything more than that would round to infinity.

[https://en.wikipedia.org/wiki/Single-precision\_floating-point\_format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)",h7dupuz,t3_ow14p2,1627871351.0,False
ow14p2,thank you :) have a great day,h7g0oqn,t1_h7dupuz,1627921566.0,True
ow14p2,If you wanted to know the maximum number a computer can hold well it depends on the data type holding the number.,h7czf7l,t3_ow14p2,1627854972.0,False
ow14p2,"No it makes no sense. Infinity is a concept, Infinity - 1 = Infinity. There is no number n which when added to another number equals infinity.",h7czb7o,t3_ow14p2,1627854919.0,False
ow1336,"So synchronous in tech means that it happens not one after the other but rather clocked to a source. This source, in electronics, could come from the processor clock or another oscillator. Shift a bit on every rising edge of the clock source for example. Asynchronous in electronics would mean that a signal, data bus, whatever, is not clocked to a separate source but can happen with a variable timing.

 In computer science, one would consider instructions happening within a thread to be synchronous as they happen sequentially because they are not interrupted by an event such as an exception. Instructions at hardware level from this thread are interpreted one by one as the processor is clocked. Every rising edge makes the core execute an instruction (this can vary from architecture to architecture).

Asynchronous does not necessarily mean parallel processing. Take a single core processor as an example that is running a thread making an led blink. An interrupt is enabled on an external gpio on the processor chip. Whenever this input receives a logic high signal, the thread making the led blink is suspended while an interrupt service routine gets executed and treats whatever it is we wanted to happen when that signal was received. The interrupt was completely asynchronous because its timing could not be anticipated.

Does that clear it up? Please feel free to ask Follow up questions :)

Edit: the definitions are indeed correct, however an understanding of the electronics behind the processor is needed to realize it",h7d1h9d,t3_ow1336,1627855982.0,False
ow1336,"Oh, I see. So I think what you're saying is that synchronous means instructions can be completed in a fixed/predictable amount of time(or on a constant period) and asynchronous means instructions can have a variable/unpredictable time that it takes to complete the instruction(so for example, an interrupt or like, a server request or something). I must have misunderstood the meaning since I was thinking of it from a web programming perspective(like, Promises in Javascript) when you make an asynchronous request to a server(or create something on a database) and that request gets processed at the same time as when the rest of the code runs. I think my question arises from using the wrong English definition of ""synchronous""(The second definition was like, ""recurring or operating at exactly the same periods"") and ""asynchronous"" since there's multiple definitions of it or something.. I guess this is why I'm not an English major haha.. Thanks to your response though! I learned something today!

edit: so synchronous means happening in a predictable/set amount of time, and asynchronous means happening in a variable/unpredictable amount of time or something like that",h7d555v,t1_h7d1h9d,1627857826.0,True
ow1336,"Haha don't be hard on yourself, it takes years to learn and understand everything. 

Synchronous does not mean an instruction takes a certain amount of fixed time. This concept is called Real Time. Arm cortex R processor cores are purposefully designed for this as they give of a promise of something happening within x amount of time by having predictable operations. For example, removing memory management units to no longer have translation table walks in virtual memory means accessing memory happens in a precicely known and predictable amount of time. This is a discussion for another time.

 An instruction in a processor will always take the same amount of clock cycles, whether it's called from an interrupt service routine or from your main thread. It means that it happens at the same time as another event, in this case the clock source as it ticks from a low logic voltage level to a high logic voltage level. Here is a timing diagram for an SPI interface: 
https://images.app.goo.gl/1AsKBUsAC6qMFX768
You can see that the bits are read synchronously to the clock edges. An asynchronous signal would have no clock signal and would be read when the data comes in, whenever it comes in. But that data may have a constant bit rate, it is just not referenced to another signal.

Now, to be clear: this is the strict definition of synchronous and asynchronous. There are programming languages that will bend this definition to mean exactly what you were talking about before and asynchronous in those cases would mean to be running in parallel. Multiple threads running concurrently to produce data, etc. I suggest reading definitions for every key word you come across, Wikipedia is your friend. And remember that these words may mean different things in different contexts. Read as much as you can as often as you can. I've been in the game for several years now and I've only begun to scratch the surface.",h7d9f7o,t1_h7d555v,1627860022.0,False
ow1336,"intresting related topic:

&#x200B;

to speed up calculations modern processors seek out for instructions which can be made ""asynchronus""

i think is called parallelization, basically whatever can be done on other cores for example or when the processor it's in idle whithout depending much on the previous input and calculations or on the need to output gets done",h7figmr,t1_h7d555v,1627913394.0,False
ow1336,"Almost. In a practical sense, synchronous basically means that both sides of an interaction are actively participating **at the same time** (it's not about *how long* an interaction takes). In my opinion the best examples of this are related to communication between two parties:

For example a *phone call* is a **synchronous** interaction. To have a phone call, both parties have to be present on the call together at the same time for it to work. If there is only ever one person on a call at a time, no communication could actually happen. It simply cannot work with 1.

An  **asynchronous** example is a *text message*. Say someone sends you a message while you're in the shower.  That's okay, your message is received and stored for whenever you're ready. You dry off, get dressed, and read the message. The act of sending the message and the act of processing the message happen independently. It works with 1.

The idea is the same regardless of application. 


Threads operating synchronously perform their interaction at the same time. (Ex. multiple threads process data to be written to different parts of a single file and write data as they go at the same time)

Asynchronous threads perform their actions independently. (Ex. Multiple threads process the same data to be written to different parts of a single file but each writes data one at a time)",h7hd2u6,t1_h7d555v,1627942736.0,False
ow1336,Yea as many posts here have clarified. The meanings aren't opposite. Sync is in time. Async is out of time. And by time i can also say sync = in order and async = out of order. So it's the same meaning.,h7dip12,t3_ow1336,1627864986.0,False
ow1336,"That's not what synchronous and asynchronous mean in the context of computer science. Synchronous means well do something right now and wait for it to be done (happening at the same time). Asynchronous means we'll eventually do it but we'll get on with other things for the time being (not simultaneous). For example, a request to a backend endpoint makes a synchronous request to a database (happening at the same time). But when you click on a button in an app an asynchronous request is made and something only happens on your screen when the request is completed. In the mean time, the app continues to render it's current state. (Not simultaneous)",h7d1euf,t3_ow1336,1627855949.0,False
ow1336,"I think OP is fully aware of this fact (they used word ""usually""). The question was _**why**_ is it different than in the English dictionary.",h7d1pet,t1_h7d1euf,1627856097.0,False
ow1336,"It's not different is my point. Synchronous means at the same time in both contexts and asynchronous means not simultaneously in both contexts. Not in the sense of executing steps in series or parallel but in the sense that making a request asynchronous means we'll eventually get to processing the job, it will happen in a different time series.",h7d22um,t1_h7d1pet,1627856284.0,False
ow1336,"Are the following commands executed at the same time?

    echo ""foo""
    echo ""bar""

Is this code synchronous or asynchronous?",h7d2uxl,t1_h7d22um,1627856668.0,False
ow1336,"These commands aren't executed at the same time, but rather sequentially. They are also synchronous; i.e. they will execute in order without interruption.",h7deaai,t1_h7d2uxl,1627862608.0,False
ow1336,That's exactly my point,h7demhz,t1_h7deaai,1627862790.0,False
ow1336,[deleted],h7d44u5,t1_h7d2uxl,1627857313.0,False
ow1336,"Of course the example is garbage at showing difference between sync/async, since it's not even attempting to!",h7d98hj,t1_h7d44u5,1627859927.0,False
ow1336,[deleted],h7ee3kw,t1_h7d98hj,1627883410.0,False
ow1336,"That ""synchronous"" doesn't always mean ""at the same time"" in CS?",h7f0w3w,t1_h7ee3kw,1627903030.0,False
ow1336,"> in the context of programming, asynchronous usually means that instructions are executed in parallel

Uhh... What? That's not at all true. Who told you this?",h7dblzv,t3_ow1336,1627861174.0,False
ow1336,What a helpful comment.,h7dohb1,t1_h7dblzv,1627868063.0,False
ow1336,"Surely pointing out that the question at hand does not make sense because of initial false knowledge is by itself an answer?

If it needed more clarification, I would happily discuss.",h7gg06w,t1_h7dohb1,1627928159.0,False
ow1336,"I really don't know this topic well, so I could be wrong. But reading through the answers it seems like there's a simple connection that very few people are saying outright.

It seems that the word synchronous as used in computing talk about code execution and clock ticking happening at the same time, and doesn't refer to two different lines of code executing at the same time.

When we talk about synchronous code, it feel like the words should be synchronized and asynchronized rather than synchronous and asynchronous. However being synchronized does mean that you're synchronized with something, which makes you and that something synchronous. I guess since that's how computers started, with execution being synchronized with a clock ticking, that's the phrasing that has persisted with more high-level use cases like we see nowadays. My mental picture synchronous code means its execution order is synchronized with the order of the lines of code, so the line number and execution number would be what are being referred to as synchronous.",h7ew3dw,t3_ow1336,1627899047.0,False
ow1336,"Just to add, there is a different definition again when you consider distributed algorithm settings.

Asynchronous means that communication is unbounded in terms of delivery time.

Synchronous is bounded.

This has implications for foundational theoretical findings such as FLP; it is not always possible to reach consensus in the presence of one faulty processor when in an asynchronous network.",h7d7gsp,t3_ow1336,1627859005.0,False
ovpox0,"just a guess...the data isnt ""read in"" since it computes it for all possible data and then constrains the result to what we want. Since the data isnt read in the same way then i think its different",hc5u1gi,t3_ovpox0,1631178669.0,False
ovkp1p,Link to online demonstration: https://telvannichad.github.io/TravellingSpellwrightProblem/TSP.html,hlxttzu,t3_ovkp1p,1637782692.0,True
ovkmr7,[deleted],h7a8a3n,t3_ovkmr7,1627795887.0,False
ovkmr7,A drawback of all the picture-based approaches is the camera’s lens features being static,h7ag1iz,t1_h7a8a3n,1627802164.0,False
ovkmr7,[deleted],h7agq7a,t1_h7ag1iz,1627802747.0,False
ovkmr7,"You misunderstand. Every camera lens has a unique set of properties and imperfections that leave a “fingerprint” on the pictures captured with them. In some cases this is sufficient to be able to link the picture back to the camera. For cryptography, you don’t want a static, predictable element being fed into your entropy pool.

This isn’t a new observation: I remember lavarand being dismissed for this reason 20 years ago.",h7ah1ny,t1_h7agq7a,1627803017.0,False
ovkmr7,[deleted],h7ai0ig,t1_h7ah1ny,1627803854.0,False
ovkmr7,Sorry you still don’t get it. This isn’t Kerchoff’s principle. The model of camera is irrelevant. It’s putting derivable information into the entropy pool that’s the problem. It’s equivalent to knowing the dice are loaded.,h7aiby9,t1_h7ai0ig,1627804129.0,False
ovkmr7,[deleted],h7airw8,t1_h7aiby9,1627804511.0,False
ovkmr7,"If that’s the case, how come it didn’t take off? Nobody uses this because it’s an expensive to set up gimmick with serious theoretical concerns and a really low bitrate. 

The design of hardware RNGs is really cool. A friend of mine was involved in the design of this product: http://web.archive.org/web/20160108044945/http://www.entropykey.co.uk/tech/",h7aj2y9,t1_h7airw8,1627804786.0,False
ovkmr7,"The background is also going to be largely static, why would the camera lens be the limiting factor? Isn't the key the position of the ants?",hh7ybd7,t1_h7aiby9,1634643005.0,False
ovkmr7,Would part of the problem also be that ants follow certain rules in their behavior e.g follow pheromone trails of other ants to find food? That would add some predictability.,h7cgz68,t1_h7ah1ny,1627846092.0,False
ovkmr7,[deleted],h7d0p69,t1_h7cgz68,1627855592.0,False
ovkmr7,Ok then,h7d6lle,t1_h7d0p69,1627858560.0,False
ovkmr7,[deleted],h7d8abq,t1_h7d6lle,1627859429.0,False
ovkmr7,"Ahh, I see. Gotcha.",h7dij59,t1_h7d8abq,1627864900.0,False
ovkmr7,"In my opinion, the core part here is generating truly random numbers. I had worked on a similar project some time ago, and it was pretty fun.

Essentially, computers cannot generate truly random numbers. It is their only problem, they just aren't good at making (truly) random decisions. You need an external source for that.

The source must be sufficiently random and not predictable. This is done so as to prevent someone from representing it as a function, approximating it, and guessing the workings. For example, iirc the Nazis code was cracked by Turing as they used to say heil hitler at the end of every message, which left an artefact.

Now, you might wonder, stock market rates are excellent? They're always changing, and being a class 2 chaos system, they're trivially unpredictable. However, they are freely available all over the net; this makes brute-forcing for results much easy. 

Hence, you need to consider two things while selecting the source, sufficient randomness and secrecy of the source. Ant farms are better than anything over the internet, with respect to the second aspect. 

If you find a suitable source, you need to do further processing. For example, a small difference in the source quantity should generate a huge difference in the output, called as avalanche effect. Most of the later stuff can be done by hashing and encrypting algorithms, which are readily available to learn from the internet, and whose choice is largely dependent on the problem at hand.

Hope you will find this useful!",h7a394q,t3_ovkmr7,1627792323.0,False
ovkmr7,So could I generate continuous random numbers by measuring the temperature of a room to a precision that detects subtle random changes (to the thousandth of a degree or something) and use the last digit(s)? Or is there some pattern that permeates deep into high precision measurements of physical phenomenon that makes discrete random data preferable?,h7a9d7k,t1_h7a394q,1627796711.0,False
ovkmr7,"Yes, measurement noise is usually considered unpredictable.",h7bl4j2,t1_h7a9d7k,1627831199.0,False
ovkmr7,"Well, I believe the random numbers aren't supposed to continuous; they will generate a continuous output, and to randomize them an algorithm will be needed, but unfortunately computers are not good at that...
The pattern will be random, but it won't be sufficiently random.

The last digit trick is cool, but I guess it boils down to least count of the instrument. Again, temperature won't vary by that much, and in any case, the last digit will be constrained to ten possibilities, so I don't think it will be really good.

However, your idea of measurement can be really good. For example, weather systems are random, and with sufficient combinations of different parameters (humidity, pollution, any kind which varies largely and quickly) and relative privacy of the dataset could yield awesome results.",h7abfz3,t1_h7a9d7k,1627798336.0,False
ovkmr7,"laval lamps and ant farms are a finite arenas. There is only so much non repeatable information to reference for it to be viable on an enterprize level as a replacement for current encription techiniques based upon prime numbers. Ants and lava have a range in which they will operate, and an area where that expression makes sense (ant collonies are only so big and so complex, lava lamps have to repeat patterns in the confined space).",h7bxiyj,t3_ovkmr7,1627836925.0,False
ovkmr7,"What about the ""true static"" of old tvs?",h7c22af,t1_h7bxiyj,1627839032.0,False
ovkmr7,"What's the problem with finite states? A perfect coin will land only on 2 states, yet is still capable of giving you an infinite random sequence right?",h7g0nvq,t1_h7bxiyj,1627921556.0,False
ovkmr7,"Can someone explain how these “real” random keys would differ from ones generated by a computer? I understand the method of getting the keys is much different, but does it matter if both are “random”?",h7ccfyp,t3_ovkmr7,1627843926.0,False
ovkmr7,[deleted],h7ct1o4,t3_ovkmr7,1627851846.0,False
ovkmr7,Ants have antenna,hh6431y,t1_h7ct1o4,1634599760.0,False
ovfapd,Fun fact: The leader Fugaku is ARM based unlike others.,h78y4j8,t3_ovfapd,1627769088.0,False
ovfapd,With over 7 million cores,h791tlu,t1_h78y4j8,1627771012.0,False
ovfapd,finally a stable 30fps in tf2,h7bobdd,t1_h791tlu,1627832628.0,False
ovfapd,over 7 million? wtf lol,h79y74f,t1_h791tlu,1627789056.0,False
ovfapd,"Quick reminder to anyone looking at this list for the first time: these are only supercomputers that have been authorized to submit their HPL results publicly to the TOP500. There are many more massive, off-the-grid HPC deployments out there that aren't on this list.",h79rtoa,t3_ovfapd,1627785253.0,False
ovfapd,Things like owned by companies like Google or used by agencies like the NSA?,h7cpt6z,t1_h79rtoa,1627850301.0,False
ovfapd,Why is there such a high demand for super computers world wide? What are they being used for? For evil or for good?,h7ee67t,t1_h79rtoa,1627883466.0,False
ovfapd,I would love to see a normal gaming PC at the end for comparison.,h79j8fq,t3_ovfapd,1627780406.0,False
ovfapd,"~~Tom's Hardware estimates a RTX 3060 at \~100 Tensor TFLOPS so, generously, a gaming PC would have a rpeak of around 1/5000 of Fugaku.~~ u/LemonXy has a better comparison below.  Thanks u/TCGG-",h79msc4,t1_h79j8fq,1627782385.0,False
ovfapd,That can't be right,h7b2n3l,t1_h79msc4,1627821418.0,False
ovfapd,That's tensor performance. Completely different thing.,h7b44f4,t1_h79msc4,1627822395.0,False
ovfapd,"The countries that have [created the most](https://www.hp.com/us-en/shop/tech-takes/fastest-supercomputers-ever-built):  
United States: 16  
Japan: 10  
France: 5  
Germany: 5  
Italy: 3  
China: 2  
Australia: 1  
Russia: 1  
Saudi Arabia: 1  
South Korea: 1  
Spain: 1  
Switzerland: 1  
Taiwan: 1  
United Arab Emirates: 1  
United Kingdom: 1",h78t31w,t3_ovfapd,1627766488.0,True
ovfapd,"The Super-MUC claims to be the most energy efficient HPC. I dunno if its true but they told us the city of Munich won't provide bigger cables so they are more or less forced to find more energy efficient ways of competing. Which I find kinda nice.  


Its amazing, they renew halve of their processors every 4 years in a way that the second batch has the same power as the first, and the area used by the second batch is fairly consistent a quarter of the first batch. I guess Moore was right \^",h7azv57,t3_ovfapd,1627819442.0,False
ovfapd,"The green500 is for top energy efficient supercomputers.

[green 500](https://www.top500.org/lists/green500/2021/06/)",h7b22h8,t1_h7azv57,1627821025.0,False
ovfapd,"Ah that's nice to know thanks.

Nice that its not even on the list.",h7b8zgr,t1_h7b22h8,1627825088.0,False
ovfapd,But can they run Crysis?,h7b09lc,t3_ovfapd,1627819739.0,False
ovfapd,"Japan got the fastest supercomputer, fastest internet, what will they do next?",h7bdyms,t3_ovfapd,1627827724.0,False
ovfapd,"First battle-ready Gundam.

/s",h7bl1e5,t1_h7bdyms,1627831161.0,False
ovfapd,no need for the /s they WILL DO IT,h7bog6k,t1_h7bl1e5,1627832690.0,False
ovfapd,"Wouldn't be effective in actual battlefields, but they would be amazing for entertainment battles.",h7bqqcl,t1_h7bog6k,1627833743.0,False
ovfapd,I was thinking sword art online,h9qohz2,t1_h7bl1e5,1629511413.0,False
ovfapd,"I would guess that the US would have it first, what with Neurallink being the current leaders of brain interfaces. Facebook will also be working on this heavily as well, at least according to an interview with Mark Zuckerberg. I am assuming Google and Amazon won't be leaving this opportunity behind either.",h9rj2ed,t1_h9qohz2,1629530350.0,False
ovfapd,I don't get it... Alot of them were made by nvidia and I heard top 50 supercomputers use Linux. Am I missing something?,h78vkm3,t3_ovfapd,1627767763.0,False
ovfapd,"I’m not sure what’s not to get?

Systems with Nvidia GPUs running Linux. They’re running the proprietary drivers.",h78zaz4,t1_h78vkm3,1627769697.0,False
ovfapd,"Sure?

I mean, they say, that there are over 7million cores at the top places HPC. 

So how could over PC have 1/5000 of that power?",h7acjhh,t1_h78zaz4,1627799216.0,False
ovfapd,"The RTX 3060 that comment about 1/5000 of performance refers to has 3584 cores, top super computer has 7,630,848 cores, 7,630,848 divided by 3584 comes to about 2129 so one would ""only"" need 2129 RTX 3060 GPUS to have same amount of cores as top supercomputer.

Worth noting that GPU and CPU cores are bit different performance wise but TOP500 adds the numbers together for the list. Also normal GPU operates with 32-bit floats where as TOP500 lists 64-bit FLop/s performance (and the 100 TFlop/s number that person found for RTX 3060 was for 16-bit floats so not really comparable)",h7aiu35,t1_h7acjhh,1627804563.0,False
ovfapd,"GPU and CPU cores aren't ""a bit different performance wise"", they are VERY different. I don't know why people are making comparisons like this - doesn't make any sense.",h7b4g4l,t1_h7aiu35,1627822605.0,False
ovfapd,"I agree that comparing RTX 3060 core numbers to Fukagu core numbers is silly but comparing FP64 numbers between CPU and GPU is fully logical.

If the task is 64-bit floating point performance like TOP500 does CPU and GPU look a lot more similar than otherwise. Floating point operations take about same time no matter what is executing the operations (as long as the hardware has native support for FP64, RTX 3060 does not), yes GPUs do have more cores but each core is about as fast as CPU cores what comes to floating point performance.

Just for fun RTX 3060 has 199 FP64 GFlops which would mean that to match Fukagu one would need 2221155.8 RTX 3060 GPUs and that comparison actually makes sense FP64 Flops are FP64 Flops no matter what is executing the operations.

Worth noting that Fukagu doesn't have any GPUs just CPUs with a vector extension to handle more 64-bit floating point math where as Summit does have CPUs and GPUs the CPUs in Summit act as bridge between the GPUs mostly but also do a portion of the calculations.",h7bk0yo,t1_h7b4g4l,1627830685.0,False
ovfapd,Yeah I comparing their FPxx or INTxx performance makes sense. Just not the cores themselves (like some people have been doing). Ofc they're vastly different and suited for different purposes.,h7brv2a,t1_h7bk0yo,1627834269.0,False
ovfapd,Make sense,h7akxsh,t1_h7aiu35,1627806453.0,False
ovfapd,teraflops per second?,h79y6pw,t3_ovfapd,1627789049.0,False
ovfapd,"smh my head, maybe the covid virus disease got them",h7b2jbc,t1_h79y6pw,1627821344.0,False
ovfapd,Tera (10^12) Floating Point Operations (for example the multiplication of two floats) per Second.,h7ao9ch,t1_h79y6pw,1627809440.0,False
ovfapd,"https://en.wikipedia.org/wiki/FLOPS

The S in ""flops"" is already ""per second"". The infograph thus incorrectly states ""per second per second"".",h7avbrk,t1_h7ao9ch,1627815758.0,False
ovfapd,"**[FLOPS](https://en.wikipedia.org/wiki/FLOPS)** 
 
 >In computing, floating point operations per second (FLOPS, flops or flop/s) is a measure of computer performance, useful in fields of scientific computations that require floating-point calculations. For such cases it is a more accurate measure than measuring instructions per second.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",h7avcwh,t1_h7avbrk,1627815785.0,False
ovfapd,Here is the current list. [top500](https://www.top500.org/lists/top500/list/2021/06/),h7b2oua,t3_ovfapd,1627821452.0,False
ovfapd,So what do they use them for? Space exploration?,h7bsflz,t3_ovfapd,1627834536.0,False
ovfapd,"Mainly ""simulation"". Either weather forecast, of nuclear reactions, or molecules folding, or anything where we know how a system works (in terms of physics equations), and these very complex systems are being resolved using the tremendous amount of computation power offered by these machines.",h7crtzu,t1_h7bsflz,1627851260.0,False
ovfapd,Yeah I'm curious also,h7bu0rz,t1_h7bsflz,1627835276.0,False
ovfapd,"I’m surprised there’s only 2 Chinese ones in the top 50, I thought they dominated the TOP500",h7c1z19,t3_ovfapd,1627838991.0,False
ovfapd,powerpc ebic tier still,hc5u5bs,t3_ovfapd,1631178774.0,False
ov8itn,"Superpower ""finish this task immediately"" could solve the halting problem - either the task finishes or you rip the the spacetime apart.",h77ifco,t3_ov8itn,1627743611.0,False
ov8itn,"Funny thing. I also posted this in r/math and someone there, at the same time you did, mentioned the Halting Problem too.",h77iwag,t1_h77ifco,1627743846.0,True
ov8itn,"I guess also the whole concept of the Big O would become irrelevant from the industry/business perspective (and consequentially P=NP too). If every algorithm finishes as soon as any other, then non-polynomial are as good as polynomial ones.",h77lfzb,t1_h77iwag,1627745119.0,False
ov8itn,And then you can kiss all of modern digital security and privacy goodbye,h77pic3,t1_h77lfzb,1627747110.0,False
ov8itn,"In the example, does the microwave that appeared to finish immediately while cooking/reheating for 3 minutes actual use the electrical energy as before, or do you get free energy?",h77mcu9,t3_ov8itn,1627745572.0,False
ov8itn,It uses the amount of energy that it would if the 3 minutes passed normally.,h77mjfv,t1_h77mcu9,1627745660.0,True
ov8itn,"More than what problems could be solved, what problems we would get.

As other user mentioned, complexity of algorithms would be irrelevant since every algorithm would finish in O(1), so class NP wouldnt exist. The main problem would be all the encryptions algorithms we use in daily life like RSA, whose decryption time without the private key is in the order of millions of years. If these algorithms could be solved in O(1), then encryption would be useless, so everyone could access people’s passwords whenever they want, of whatever they want, say social media, online banks, etc.",h7884mh,t3_ov8itn,1627755964.0,False
ov8itn,">What computer science problems could be solved with this power?

Time complexity wouldn't matter anymore -- this would give us the answer to any questions we knew how to solve, no matter how long it takes. The bottleneck then becomes the lifespan of the computing machine (some problems take so long that the computer would naturally break/degrade before the problem is solved).

>What if the superpower was instead “Finish this task immediately?”

- ""solve"" complex games like chess.

- break (most) modern cryptography.

- apply it to protein folding problems which would likely cure cancer.",h78fn9i,t3_ov8itn,1627759688.0,False
ou66j2,Most cs graduates do work as swe not as a computer scientist. And as I can see so far in swe math doesn't hold a great importance. If one wants to work in software companies i think that bachelor of mathematics would be overqualified.,h71n7z4,t3_ou66j2,1627618569.0,False
ou66j2,The math degree wants to solve math problems for its own sake. The CS degree wants to solve math problems to make money.,h70zwcb,t3_ou66j2,1627606716.0,False
ou66j2,"My two cents:

If you pursue mathematics as a study, you’ll likely utilize that skill set as a engineer. Theoretical to practical in the physical realm.

Those who go into computer science are more like to work on software. Theoretical to practical in the cyber realm.",h70alyp,t3_ou66j2,1627594952.0,False
ou66j2,"I was just talking about something like this with a friend the other day and we were talking about data science and the use of R and python, we where saying that R is better in statistics related stuff because its built by mathematicians which makes it more accurate unlike some stuff in python which are done by software engineers that are from research papers and sometimes there are assumptions and that causes inaccuracies.",h726xjo,t3_ou66j2,1627633162.0,False
ou66j2,Goedel was too meta for most mathematicians i guess?,h7et7je,t3_ou66j2,1627896531.0,False
ou66j2,[deleted],h7ucl9o,t3_ou66j2,1628190301.0,False
ou66j2,I agree. Today’s CS has roots mainly from Math and EE. But Theoretical CS was and can still be considered as a subfield of Math.,h89md2m,t1_h7ucl9o,1628500293.0,False
ou66j2,The wise person realizes that they are close partners and learns things from how to install an operating system as well as whether or not P = NP,h714n78,t3_ou66j2,1627608934.0,False
oty8fs,"What can companies learn about you by analyzing how you hold and move your mobile devices (e.g., smartphone/-watch)? In this thread, I summarize our study on the astounding privacy implications of accelerometer sensors \#privacy \#dataprotection \#machinelearning \#AI \#IoT 1/n 

***

posted by [@JL_Kroger](https://twitter.com/JL_Kroger)

Photos in tweet |  [Photo 1](http://pbs.twimg.com/media/E7c__NHXEAIpFSN.png) | [Photo 2](http://pbs.twimg.com/media/E7dABmAWEAIZ7e0.jpg) 

^[(Github)](https://github.com/username) ^| ^[(What's new)](https://github.com/username)",h6yhvh6,t3_oty8fs,1627567891.0,False
oty8fs,link to the paper: https://dl.acm.org/doi/pdf/10.1145/3309074.3309076,h6zfzwb,t3_oty8fs,1627582172.0,True
oty8fs,Plus isn’t accelerometer data unrestricted in iOS?😅,h735a3e,t3_oty8fs,1627655702.0,False
oty8fs,Doubt it,h6ykfgn,t3_oty8fs,1627569054.0,False
oty8fs,"I don't. Did you read [the paper](https://dl.acm.org/doi/pdf/10.1145/3309074.3309076)? It's not long.

- Location - This one seems obvious. This is just simple dead reckoning.

- Speech - They did not cite examples of reconstructing entire phrases, but indication of ""hotwords"" based on vibration patterns has been shown. Constructing words from scratch is suggested, but not fully demonstrated.

- Driving Style - Seems obvious. Lane changes, speed, braking distances, etc.. can all be correlated with known patterns to arrive at a categorization of ""driving style.""

- Intoxication - Researchers are referring to ""hand held"" devices (e.g. smart watches). Changes in arm/wrist movement patterns can be correlated to intoxication levels.

- Body Features/gender/age - The wearer's gait can be used to determine various details about the persons ""body type."" Obese people walk differently than non obese. As do females. As do children, adults, and the elderly. Additionally, physical activity can be used to assume a persons health. That one seems a bit obvious. They also called out sleep patterns, which seems fairly obvious as well given that some devices list this as a feature. More interestingly though, they found strong correlation between ""mood"" as well as ""conscientiousness, neuroticism, openness, and extraversion"" and steps taken per day. So this goes beyond just body features.

- Passwords - derived via vibrations & orientation patterns which correlate to characters being entered into the device. This is again for handheld/wrist worn devices.

The focus of this paper seems to be handheld/wrist-worn devices, which makes the conclusions much more plausible for me.",h6yya98,t1_h6ykfgn,1627574946.0,False
oty8fs,"nice summary, thanks!",h6zfgpe,t1_h6yya98,1627581950.0,True
oty8fs,">Location - This one seems obvious. This is just simple dead reckoning.

I'm probably dumb but I don't see how they would do it with just accelerometer data??",h721ve5,t1_h6yya98,1627628862.0,False
oty8fs,"I think you need an initial starting point, like either gps or WiFi or just knowing where person was at a certain time. I imagine it would be hard to pinpoint someone somewhere in the woods or countryside, but if the person is moving e.g. by car or train on a road (perhaps even by bike and by foot depending on how exact the data is) you can make out acceleration, decearation and turns. Compare that to a map together with some plausible assumptions (e.g the persons movement matched a train shedule so any movement between stops is just him moving in the train, not him getting off the train)
And you can probably get a pretty good estimate where the person was heading. Maybe not super exact but again with certain sync points again based on plausible assumptions you could probably narrow the search radius quite a lot. Then you look at what is in the radius that could be of interest for the persons beeing tracked and you could even tell it more exact.",h72p9x8,t1_h721ve5,1627647625.0,False
oty8fs,Oh yeah with a known starting point it makes more sense. Thanks for the answer!,h72ph9j,t1_h72p9x8,1627647744.0,False
oty8fs,Actually this is just known. It’s not even requiring research tbh. Google how smartphones receive different data forms. It’s all from the accelerometer and other built in components,h6z4vnm,t1_h6ykfgn,1627577649.0,False
oty8fs,"read the whole [thread](https://twitter.com/JL_Kroger/status/1420681035617116163). they say this, for example:

>Of course, drawing inferences from accelerometer data is not trivial & inference methods are never faultless. However, for many attacks and profiling purposes, 100% accuracy is not needed. Inaccurate methods will be used nonetheless, causing additional discriminatory side-effects.",h6zav9p,t1_h6ykfgn,1627580081.0,True
oty8fs,"It's simple statistics. As more dimensions are added, uncertainty goes down. This is why the ""it's just meta data"" argument is so naïve. If I know your weight, your gait, your gender, your current barometric pressure over the past 24 hours, and your approximate age, I'm really narrowing down the set of potential matches.

Let's take word matching for example. Figure out someone's social media accounts, scrape their history, train an AI model on common phrases/words, and then combine it with approximate accelerometer data and I wonder just how close we can get to guessing what's being typed.",h6zizdk,t1_h6zav9p,1627583408.0,False
oty8fs,"Maybe it can be used for determining these things, but it can't tell them apart so isn't useful. Also, it can't determine your location. Your change in location, sure, but not your location.",h729ubt,t3_oty8fs,1627635753.0,False
oty8fs,"You underestimate machine learning.

Of course, inference algorithms always have a certain error rate. As the researchs state in [the thread](https://twitter.com/JL_Kroger/status/1420681035617116163):

>(...) drawing inferences from ACC data is not trivial & inference methods are never faultless. However, for many attacks and profiling purposes, 100% accuracy is not needed. Inaccurate methods will be used nonetheless, causing additional discriminatory side-effects.

Regarding location: The organization tracking you could first have access to GPS (until you turn it off), and then continue tracking your location using accelerometer data from there. And: if you ride a car or train on known streets/tracks/routes, you can be located by matching your motion trajectory with existing routes on a map (read [the paper](https://dl.acm.org/doi/pdf/10.1145/3309074.3309076)).",h72b7k4,t1_h729ubt,1627636964.0,True
oty8fs,"Sure, your final paragraph is what I said, or at least meant and alluded to.",h72co5c,t1_h72b7k4,1627638271.0,False
oty8fs,"To back OP’s point, theoretically you should be able to determine when the device is stopped and only moving about the earth’s rotation. Accelerometers pick up that small variance and it would be possible to determine where you could be with some inaccuracy without check any networking information outside of some 9-axis accelerometer data tbh…",h72wmyv,t1_h72co5c,1627651621.0,False
oty8fs,"Sure, but actually purchase a few different brands and types of accelerometer, wire them up to a microcontroller, and write code for them, and you'll find that the drift is more problematic and difficult to derive any meaning from.",h730cu8,t1_h72wmyv,1627653422.0,False
oty8fs,Then you clearly have never touched machine learning…,h735e62,t1_h730cu8,1627655752.0,False
oty8fs,"No, I have. Still have several ML solutions in production.",h735zsi,t1_h735e62,1627656024.0,False
otqbu7,"1. You have historic data, choose some model, train it, get a predictive service hours and cost of the service.
2. Each mechanic will choose top 3 cars ( in whatever priority fashion ) based on less service hours and cost of the service - like a priority queue, greedy. 

You have the solution you need.",h6x4wvr,t3_otqbu7,1627533429.0,False
otqbu7,"thank you for your reply! but can this be successfully done when the ""true risk"" is unknown and must be estimated using other models? I have a feeling that this would result in too much ""error propagation"" and likely not work?",h6x8zzt,t1_h6x4wvr,1627536134.0,True
otqbu7,"If you have an expensive car, over time - the parts replaced and servicing charges would be higher than a average cost car. 

Maybe some more variations of this, but ultimately it won't get complex than this and it can be done using 2d - regression. Maybe you have to cluster them out based on different buckets of cost and then apply regression.

I think it would fit fine. Without data and testing, we can't conclude more.

I am not into machine learning but a normal sde. So yeah.",h6xj1gs,t1_h6x8zzt,1627543972.0,False
otqbu7,"Do you mean just that the model might perform poorly, or that the inaccuracy would somehow get amplified through the rest of the chain if you feed the output of the model into a more traditional algorithm that would then try to pick the optimal jobs within certain constraints?

If you just want the three jobs with the greatest dollar per unit of work (both estimated, of course), you don't need much of an optimization algorithm for that apart from the predictive model; just pick the three ones with the best ratio. If you also want to e.g. make sure the jobs all fit within a working day, you might have something resembling knapsack.

If you do need something like knapsack, I don't think the error would get amplified if your subsequent optimization algorithm is exact (i.e. not heuristic), because it would not introduce inaccuracy into the results themselves. The quality of the results would then depend only on the predictive model. I'm not sure if the error could somehow get amplified if your optimization algorithm is heuristic, though.

If the performance only really depends on the model (e.g. if you're using the output of the predictive model directly, or feeding it into an exact algorithm), I'm not sure there's much else you can do apart from trying to tweak your model or finding a better model if the only pieces of information the mechanics have are the ones you mention. It might well be that those two numbers just don't statistically contain enough information to make accurate predictions. Actually, while they might have some kind of a correlation with how long the job takes, as a layman I wouldn't expect them to be good predictors for that.",h6xpk7j,t1_h6x8zzt,1627549705.0,False
otb7nh,"Requirements for a system are generally whatever the client/user says they are, not the result of some philosophical analysis about moral obligations. If some company decides that having their web site respond within a fixed deadline is so important they're willing to accept the engineering cost of building a hard real-time system, then that web site is important enough for hard real-time.",h6ub8xv,t3_otb7nh,1627486845.0,False
otb7nh,"Allow me to quote the wiki article. 

> although the strict definition is simply that missing the deadline constitutes a failure of the system.",h6uwo97,t3_otb7nh,1627495643.0,False
otb7nh,"I also associate the term ""hard real-time"" with more serious consequences for failure than just  serving a 503 error to a web browser, but as the wikipedia quote notes the only strictly necessary aspect is that the deadline is enforced when counting errors.   I.e. a backend that responds successfully to 100% of requests, and is within deadline 80% of the time could be characterized as either a 100% successful backend, or an 80% successful hard real-time backend.

Services do definitely make a distinction between ""realtime"" backends that serve a direct user request (i.e. a flesh and blood human is actively waiting for a response that depends on this backend), vs ""batch"" backends that only feed offline processes.   Under overload conditions, batch backends can just respond slower, while for realtime backends it is preferable to fail some requests quickly so that the requests that are served are served within the normal deadline.",h6uuiss,t3_otb7nh,1627494732.0,False
otb7nh,"Amazon can deem these things imperative if they want, it's really up to them.",h6w6s7x,t3_otb7nh,1627515825.0,False
otb7nh,"I'd personally say the blog post definition of hard real time is pushing it beyond breaking point.

I've always considered Hard real time to be a system where the timing of the result is as important as the result itself. For example, an autonomous vehicle needs to be hard real time as \*when\* to turn is as important as the turn itself.

To put \*every AWS API\* in that bracket, i.e. S3, Elastic Transcoder, is, I think, objectively false.",h6vhqav,t3_otb7nh,1627504497.0,False
ot25cm,There are some platforms for connecting IoT devices to backend like KAA . Maybe this can help you.,h6sty75,t3_ot25cm,1627452374.0,False
ot25cm,"1. Without the user Web UI  
Your device can send a registration token to your backend.   
Just like a user signing up for a website. Exactly the same.  

2. With the user Web UI  
User can just simply enter the device's UUID through the web portal.   
Then your backend will start accepting requests from that device - the device will need to send its UUID in every request. There are better security mechanism than this.   


For large scale projects, I suggest you to look into Azure IOT Hub or AWS IOT  
https://aws.amazon.com/iot/  
https://docs.microsoft.com/en-us/azure/iot-hub/",h6t71kv,t3_ot25cm,1627463480.0,False
ot25cm,"I’m was involved in designing the z-wave SmartStart system for Zwave wireless devices. In our system the device each device has a ECDH key pair. At commissioning time the device will contact the gateway though broadcast messages containing a part of the public key. The gateway keeps a list of “pre-approved” public keys. If commissioning request matches a key on the list the gateway will perform the actually commissioning using the ECDH key. In this way we have identified the device and performed secure bootstrapping. Of cause this requires the gateway/backend to know the public keys of the devices to include. We do this by putting a qr code on the device with the public key. 

This same system could also be used by normal IP devices, here the device should just know the 
IP address or dns name of the backend in advance. 

Btw I know that some device uses Bluetooth to configure WiFi to get the devices in the internet in the first place.",h6vroo3,t3_ot25cm,1627508759.0,False
ot25cm,I've noticed when my Google Home loses internet (even for a minute or less) the device will blink and give me me warning. This would lead me to believe that the device is polling for some kind of message or action queue every so often. That would allow the web server to act as the middle man between the end user and the IoT device.,h6swzk6,t3_ot25cm,1627454741.0,False
ot25cm,"hash table?

ip address: MAC address",h7hzoed,t3_ot25cm,1627953445.0,False
ot25cm,Have you done any form of project management?,h6sj6t9,t3_ot25cm,1627445272.0,False
ot25cm,"Nope, just a student! I'm wondering about it from a technical perspective so that I may implement it. Becoming a PM later would be a cool career pathway however.",h6sjaqf,t1_h6sj6t9,1627445337.0,True
ot25cm,"The reason why I ask is that this type of project would require you to have some sort of idea of what the software engineering process is, not to say your idea is bad but you have to know each aspect of this project, from start to finish. Project management will teach you how to go about doing this because you will become confused. It will also teach you what research you need to do for something like this. Read this https://www.indeed.com/career-advice/career-development/what-is-software-development then you will understand how to go about doing something like this",h6sk1wh,t1_h6sjaqf,1627445782.0,False
ot25cm,"Thanks for the advice, however I'm purely looking for implementation specifics like what sort of protocols or design methods would be used. That's why I asked about big companies. It seems there must be some sort of industry standard method for achieving this functionality. If you've got any resources that specifically can help me with this question then that would be much appreciated!",h6skgwu,t1_h6sk1wh,1627446033.0,True
ot25cm,"Smart devices send information back and fourth via internet connection. Smart device (Alexa, for example) usually just has a microphone and internet connection, and sends the actual recording to their backend for processing off of the device.

Once the information has been sent via internet to the back-end, the data can be stored for later use and a response can be sent back to the smart device.",h6slauw,t1_h6skgwu,1627446532.0,False
ot25cm,"Would you happen to know specifically from an architectural perspective how this is achieved? How might a backend reach out to a device on your home network if the device doesn't have a publicly reachable IP? 

In the Alexa example, your phone works alongside this process. Does it do some sort of network discovery to find out where the Alexa device is/its address? Or does the Alexa device actually reach out?",h6slo7z,t1_h6slauw,1627446762.0,True
osxojq,"I learned Assembly language by the tutorial series of binary explotaition by ""LiveOverflow"" (Infosec youtuber). Even when those videos arent focused on teaching assembly I learned the basics of how a CPU works, the stack, registers, etc and after learning the basics I did the following:

- Code something basic in C like a variable initialization, Loops, conditionals, etc.
- Compile with and without optimizations (-O3)

- Use `obdjump -d -M intel FILE` to dissasmble a file.

- Try to understand everything in the output. If there is a mnemonic you dont know go to the Reference manual of your processor or search in google.

- And eventually do some low level projects with assembly like coding a bootloader.",h6rpxgi,t3_osxojq,1627430249.0,False
osxojq,"To expand on this quickly, compiling with debug flag (-g) will ensure that objdump has function names and debug info, and I believe using no optimisation will force it to follow the control flow of whatever program you write, instead of mucking everything up with the optimiser.

This assumes Linux environment using gcc.",h6sf4xr,t1_h6rpxgi,1627442982.0,False
osxojq,thanks for your comments!,haj2c2t,t1_h6sf4xr,1630048931.0,True
osxojq,thanks for your comments!,haj2bxl,t1_h6rpxgi,1630048928.0,True
osn6iv,"I don't understand how anyone could possibly answer this question.

The book is pretty well-organized. If you want to learn a certain topic, read about it. If you don't care to learn about that topic, don't read about it.

What more can be said?",h6qt9u7,t3_osn6iv,1627415362.0,False
osn6iv,"Had a bad day at work, huh?",h6qtmlo,t1_h6qt9u7,1627415519.0,False
osn6iv,"I'm retried.

Maybe reconsider your question: not essential *for what*? What are you trying to get out of [not] reading the book? How much time do you have? What percentage of chapters do you intend to skip? Just a couple? Some of them? Why not all of them?",h6qu0sq,t1_h6qtmlo,1627415694.0,False
osn6iv,"Since you are just looking to do a fly by, i would start from the beginning since the information builds off of itself. 

Virtualization lays the foundation for concurrency and persistence.

You can probably skip over the chapters that go over the API, since  they are very dense and you would need to be writing your own C programs alongside it to retain the information anyway.",h6r39bl,t3_osn6iv,1627419800.0,False
osn6iv,"Only “textbook” in college I read cover-to-cover. Don’t skip! If you only have time for one part though, virtualization is super important.",h6r3q0o,t3_osn6iv,1627420008.0,False
osn6iv,Thanks!,h6rby6c,t1_h6r3q0o,1627423635.0,False
oscvji,Buy an Arduino starter kit and start messing around,h6q6dsz,t3_oscvji,1627405399.0,False
oscvji,"You need to change your mindset. Everyone was terrible everything, until they weren’t. Without knowing engineering you will be stuck building pre-designed kits, which take no skill as you are following instructions. It will get boring and feel tedious and will not be any more rewarding than regular programming. 

You need to start small with something extremely basic. Like a digital clock, with an Arduino brain and display screen and a simple mechanical housing (a box). Learn the basics of the 3D modeling tool, 3D printing, wiring, and debugging on an embedded system to do the most basic thing of incrementing a counter inside of a box. 

It doesn’t have to be unique. It doesn’t have to be fancy. You can copy someone else’s design. Learn the tools so you can apply them to more complicated things later. You are capable. It’s a journey. Love the process not the product.",h7121fj,t3_oscvji,1627607708.0,False
