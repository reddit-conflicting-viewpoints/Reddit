post_id,title,score,upvote_ratio,subreddit,url,num_comments,body,created
sko30v,application with a database,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/sko30v/application_with_a_database/,0,"Hello! I am trying to create a dictionary application where you can enter a word and get a translation.(only 2 languages)
So my question is if I use a database like SQLite, when the user downloads my application they don’t have to download any additional things because the SQLite database would be packaged with the application, right?",1644008194.0
skm1fx,Starting classes for Computer science on Monday . Any tips/ Advices?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/skm1fx/starting_classes_for_computer_science_on_monday/,0,,1644003034.0
skltie,Confusion Between Different Types of Optimization Problems,8,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/skltie/confusion_between_different_types_of_optimization/,1,"I do not have a background in optimization and I am trying to teach myself more about this topic. I find myself having a lot of trouble understanding the different ""types"" of optimization problems that exist.

For example, I understand the idea of optimizing continuous functions (e.g. y = x\^2) - for example, we could be interested in finding out the value of ""x"" that results in the smallest value of ""y"". I also understand that continuous functions can be optimized subject to some constraints.

However, I find myself very confused when trying to sort through the following types of optimization problems:

* **Discrete Optimization**
* **Integer Optimization**
* **Mixed Integer Optimization**
* **Combinatorial Optimization**

When I think of these problems, the first thing that comes to mind is that they are fundamentally different from optimizing continuous functions. For instance, the inputs of the above list of problems are usually ""categorial"" in nature - this is why I have heard that problems belong to the above list usually require ""gradient free optimization methods"" (e.g. evolutionary algorithms, branch and bound, etc.) , since it is impossible to take the derivatives of the objective functions corresponding to these problems.

For example, if you take problems such as **""Travelling Salesman"" or ""Knapsack Optimization""** (note: I have heard that these problems belong on the above list, but I am not sure), I would visualize the objective function as something like this:

&#x200B;

&#x200B;

https://preview.redd.it/376tdicu9vf81.png?width=1581&format=png&auto=webp&s=d0d764cc25cbbf26b4422dd478a474dc6419e4dd

 

This leads me to the following question:

* Are 4 types of optimizations on the above list effectively the ""same thing""? The way I see it, all 4 types of these problems have ""discrete inputs"" and in a mathematical sense, ""integers"" are always considered as ""discrete"". In all 4 types of problems, we are interested in finding out a ""discrete combination"" of inputs - i.e. ""combinatorial"". Thus, **are 4 types of optimizations on the above list effectively the ""same thing""?**
* I have heard the argument that ""any optimization problem that can be formulated into a linear problem is always convex (because linear objective functions are always convex)"". If we consider continuous optimization problems - we usually say that ""convex optimization problems are easier than non-convex optimization problems"" because non-convex functions can have ""saddle points"" that can result in the optimization algorithm getting stuck in these ""saddle points"". Using this logic, I have seen the objective function of the ""Travelling Salesman Problem"" being written as a linear function and thus the ""Travelling Salesman Problem"" being considered as a convex optimization problem - I have also heard of the ""Travelling Salesman Problem"" is a very difficult problem to solve. **If the ""Travelling Salesman Problem"" is convex and difficult to solve - does this imply that there are non-convex discrete/combinatorial problems that are even more difficult to solve?**
* I have heard the following argument: Discrete/Combinatorial Optimization Problems are more difficult to solve compared to Continuous Optimization Problems. This is apparently because discrete/combinatorial optimization problems involve ""treating the problem as a continuous problem"" to first come up with a solution, and then determine if the solution lies within the feasible region - thus effectively solving two optimization problems in one. **Is this correct?**
* Finally, I have seen both ""Travelling Salesman"" and ""Knapsack Optimization"" being formulated as a linear problem and therefore as convex. **Are there any well known examples of non-convex discrete/combinatorial optimization problems?**

Thanks!",1644002463.0
sklhet,How to I get my code to a stand-alone computer chip?,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/sklhet/how_to_i_get_my_code_to_a_standalone_computer_chip/,9,How to I get my code to a stand-alone computer chip? The chip needs two usb inputs and two outputs to control two motors.,1644001631.0
skkubm,How is the computer science education quality in usa?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/skkubm/how_is_the_computer_science_education_quality_in/,7,"Im 26. As a candidate international student, are middle level universities good in computer science major? My preferred state is washington but im open to other states except cali, maine and other expensive states.",1644000052.0
skkkmf,Would you for sure get equal distribution of digits coming out of a TRNG?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/skkkmf/would_you_for_sure_get_equal_distribution_of/,4,"If we ran 10,000,000 cycles would we see a million of each digit, if we are just generating digits?",1643999377.0
skjr2f,Interesting breakdown about Web3,0,0.33,computerscience,/r/cardano/comments/skcoyc/web3_on_cardano/,1,,1643997424.0
skhmpy,Will software engineers become obsolete by 2030?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/skhmpy/will_software_engineers_become_obsolete_by_2030/,12,With growing technology like AI I fear I will never be able to experience the Field since my purpose with be meaningless. My goal was to innovate and create cutting edge technology but that’s hard to do when where literally coding our downfall!,1643992282.0
skd092,"Happy Birthday, Ken!!",0,0.5,computerscience,/r/unix/comments/skcziy/happy_birthday_ken/,0,,1643980456.0
sk80jv,Help with analysing time complexity of Karatsuba Algorithm.,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/sk80jv/help_with_analysing_time_complexity_of_karatsuba/,2,"I have done this much so far. I can’t proceed further. I computed the amount of work required for each step and added them all together. 
The no of steps are log2(n) and the summation of work done on each step forms a Geometric Progression.

https://imgur.com/a/U7OnMTs",1643962179.0
sk7puv,Behaviour of TCP and UDP traffic on the same network,21,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/sk7puv/behaviour_of_tcp_and_udp_traffic_on_the_same/,4,"So, as we all know , one of the main advantages that TCP has over UDP is its flow control mechanism, namely it'll reduce its packet ACK buffer if it detects a high number of packet loss, while UDP just doesn't care. 
 
How does this translate to a bottleneck situation where a router is overwhelmed with both TCP and UDP traffic? 

Will the TCP traffic reduce its transmission rate while the UDP traffic will then dominate the network?

e.g. Lets say router has a 1.000 Kbps bandwidth, there is an incoming 800 Kbps  of TCP traffic and 800 Kbps of UDP traffic, how will the network settle?

My intuition says that eventually the TCP traffic will be reduced down to 200 Kbps while the UDP traffic stays at 800Kbps",1643961054.0
sjuhh6,A computer's ability to reason,0,0.45,computerscience,https://www.reddit.com/r/computerscience/comments/sjuhh6/a_computers_ability_to_reason/,1,"Hello my fellow nerds.

I'm trying to track down an older article/study about a computer's ability to interpret a picture.

The picture was of a child in a cowboy hat with a lasso, a cat, and a broken vase. The computer was trying to determine how the vase was broken and was coming up with some strange conclusions.

Anyone remember that?",1643922709.0
sjn3gy,Estimating the Run Time of the Travelling Salesman Problem,34,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/,7,"The ""Travelling Salesman Problem"" is a problem where a person has to travel between ""n"" cities - but choose the itinerary such that:

* Each city is visited only once
* The total distance travelled is minimized

I have heard that if a modern computer were the solve this problem using ""brute force"" (i.e. an exact solution) - if there are more than 15 cities, the time taken by the computer will exceed a hundred years!

I am interested in understanding ""how do we estimate the amount of time it will take for a computer to solve the Travelling Salesman Problem (using ""brute force"") as the number of cities increase"". For instance, from the following reference ([https://www.sciencedirect.com/topics/earth-and-planetary-sciences/traveling-salesman-problem](https://www.sciencedirect.com/topics/earth-and-planetary-sciences/traveling-salesman-problem)):

&#x200B;

&#x200B;

https://preview.redd.it/oat1pzbl7nf81.png?width=753&format=png&auto=webp&s=e7342dde923fa64d8acf19874bee51d58ec06862

**My Question:** Is there some formula we can use to estimate the number of time it will take a computer to solve Travelling Salesman using ""brute force""? For example:

* N cities = N! paths
* Each of these N! paths will require ""N"" calculations
* Thus, N \* N calculations would be required for the computer to check all paths and then be certain that the shortest path has been found : If we know the time each calculation takes, perhaps we could estimate the total run time as ""time per calculation \* N\*N! ""
* But I am not sure if this factors in the time to ""store and compare"" calculations.

**Can someone please explain this?**

Thanks!",1643904849.0
sj2gsb,I found this awesome video about the TSP and how the Concorde solver roughly works. The speaker is one of its creator !,3,1.0,computerscience,https://youtu.be/tChnXG6ulyE,0,,1643842180.0
sjasw0,Is learning low-level engineering useful in building distributed systems?,15,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/sjasw0/is_learning_lowlevel_engineering_useful_in/,5,"Is learning low-level engineering like compiler engineering and other low-level stuff like CPU architecture, operating systems, and assembly any useful while building large-scale distributed systems and backend microservices. I am interested in learning about low-level stuff but I am doubtful whether it would be useful or not. Any examples where low-level knowledge can help in distributed systems would be great.",1643865444.0
sj5m8j,Any book recommendations on Networking?,9,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/sj5m8j/any_book_recommendations_on_networking/,8,"I’ve been working for an ISP for almost a year now as tech support (will be moving towards into a software developer position). My new position will mostly be concerned with building serverless applications, not really a “networking” position. However I’d still like to learn more about the fundamentals of networking! Any book suggestions?",1643850357.0
sj47go,How machines understand us?,0,0.17,computerscience,https://www.reddit.com/r/computerscience/comments/sj47go/how_machines_understand_us/,2,"Can you recommend  a source material on how computers understand what we want from them on most fundamental level of operations? I don't even know how to phrase this question. I have  a vague understanding of machine code, instructions, compilers, interpreters, etc. but I still don't understand what is the ultimate interface between the software and the hardware. Or maybe my question relates more to how circuit boards  and transistors work.",1643846612.0
sizzx9,Why do we assume the first value is sorted in an insertion sort algorithm?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/sizzx9/why_do_we_assume_the_first_value_is_sorted_in_an/,6,,1643836228.0
siw69v,"Theoretically, can you remotely re-program the system inside those micro/nano computers, so the devices run by them can just perform any tasks or move to specific environments, also upgrade itself as the programmer desires?",0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/siw69v/theoretically_can_you_remotely_reprogram_the/,0,"https://www.dailymail.co.uk/sciencetech/article-5875871/Worlds-smallest-computer-revealed-Incredible-image-shows-tiny-machine-dwarfed-grain-rice.html


https://www.makeuseof.com/nanocomputing-can-computers-really-be-microscopic/

> Nanocomputing involves using nanoscale structures to make computing processes. Nanoscale structures like protein and DNA (deoxyribonucleic acid) can be used to produce nanocomputers.
> DNA computing involves using DNA, molecular biology hardware, and biochemistry to perform computing processes instead of the traditional electronic computing which makes use of silicon chips. Information in DNA is represented using a four-character genetic alphabet (A [adenine], G [guanine], C [cytosine], and T [thymine]), instead of the binary numbers (1 and 0) used by traditional electronic computers.
> When applied to separate and non-sequential tasks, the DNA nanocomputer is better than the traditional electronic computer as it can store a larger amount of data in memory and conduct multiple operations at once. DNA nanocomputers are considerably faster than their electronic counterparts.

So according to these articles, micro/nano computers in that sizes are really able to perform any tasks and store datas just like the traditional computers do or even better? And it is indeed possible with our current technologies that people can remotely re-program the system inside those computers, so the devices run by them can just switch to perform different tasks or move to specific environments, or upgrade itself indefinitely as the programmer desires?

Is it possible that they are even able to perform tasks of building/creating a specific object by using the chemicals and substances they collected form the environment like engineering robots do?",1643827073.0
siux7r,What's the point of studying Group Theory ?,46,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/siux7r/whats_the_point_of_studying_group_theory/,25,"I'm doing my BTech in Machine Learning ( first semester ) and teachers often stress on getting good grasp on Linear Algebra concepts and I can see little bit where we are using Vector space concept, Matrices etc. But Professor did not say anything about Group Theory. I don't see it anywhere till now. But somehow we are studying it.




The thing is I'm loving Group Theory very much. I want to continue with this book of Joseph A. Gallian but I'm afraid if I'm wasting my time doing unnecessary. I've never grasped any concept of mathematics as quick as Group Theory, maybe it's easy whatever but I really like it. Will it be any beneficial for me ?",1643824133.0
siry8q,Confused in understanding pumping lemma - Theory of automata.,25,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/siry8q/confused_in_understanding_pumping_lemma_theory_of/,5,"I have just started out theory of automata in my university and we are studying pumping lemma. From what I have understood, the lemma states that y != λ (empty string) and that x y^i z (where i >= 0) so if ""i = 0"", then y^0 = λ and the string is still accepted.

How do these not contradict each other? If i = 0, y becomes the empty string that is previously stated cannot be empty? Can someone please clear this for me?",1643817117.0
sicshh,I don't understand how to find the Big-O of functions including multiplying multiple 'n'.,45,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/sicshh/i_dont_understand_how_to_find_the_bigo_of/,12,"For specifically, I don't understand how to find the Big-O of functions such as: 

log(2\^n) \* log(n\^2) =

n \* sqrt(n) = 

n \^ sqrt(n) = 

Any help or resources to look at would be appreciated.",1643768469.0
siaup5,What's something that has not (yet) been done with computers or programming?,2,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/siaup5/whats_something_that_has_not_yet_been_done_with/,8,"Something that you wish to see done that hasn't been done?

Something that you **do not** wish to see done that hasn't been done?",1643763172.0
si9m1h,Computer Architecture: A quantitative approach,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/si9m1h/computer_architecture_a_quantitative_approach/,2,I am going to start reading this book for a class and I was wondering if there are any resources or videos I can take a look at to be able to understand it. I think I lack the background to be able to understand the book. I already read computer organization by the same authors but had trouble with understanding it because I usually like to learn with videos.,1643759837.0
si23iw,The SRE Handbook via Google,43,0.95,computerscience,https://sre.google/sre-book/table-of-contents/,5,,1643740737.0
shzu1j,Algorithms where complexities similar to O(n/k * log(n/k)) appear,18,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/shzu1j/algorithms_where_complexities_similar_to_onk/,7,"I'm looking for any algorithms or data structure where a complexity of a form like O(n/k * log(n/k)) appears. In particular, I want cases where the complexity is normally considered worse than O(n). It looks like nlogn, so surely such a case exists?


One case I know of is the IO-model where we often have algorithms that perform O(N/B * log(N/B)) IO-operations, with B being the block-size of the IO-device. However, in this context, that complexity is usually considered _better_ than O(N) IO-operations.

(Strictly speaking the IO-model complexities are often more complicated, e.g. with an M/B in the base of the logarithm, but that kind of difference is OK for the kind of examples I'm looking for.)",1643735022.0
shfux4,Do a computer engineer have same software knowledge as a computer science software engineer?,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/shfux4/do_a_computer_engineer_have_same_software/,24,"Someone who studies computer engineering learns how a computer works in low level. If they know the hardware don’t they also know what software can be built on it and how?

For me it feels like computer science is an easier part of hole IT, CS, CE, EE field. They only know high level things for the most part. So the computer engineers have the same knowledge as a software engineer and the computer engineer do also have hardware knowledge. When does CS/IT education benefit from CE when the time to study CE and CS (both BSc and MSc) is mostly the same.",1643672113.0
shi03u,US Governors gathered in person for 114th annual Winter Meeting of National Governors Association. 'Governors will commit to strategies for expanding computer science education in public schools',35,0.86,computerscience,https://www.nga.org/news/press-releases/governors-convene-in-person-for-114th-national-governors-association-winter-meeting/,1,,1643678107.0
shath4,Implementation of Count Sort using a Sparse Matrix,7,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/shath4/implementation_of_count_sort_using_a_sparse_matrix/,1,"Hi, The count sort is only feasible for some inputs of 'k' that are not abnormally large since the space complexity increases arbitrarily. Can it be implemented using a sparse Matrix to counter that drawback.

I've tried but can't seem to get it work, can someone provide some insight on this. Thanks",1643659852.0
sh0rr0,Looking for online resources,22,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/sh0rr0/looking_for_online_resources/,7,Is there any free educational resources that are somewhat equivalent to the computer science bachelor program (like all the course pdfs of a certain program)? I am really interested in learning as much as I can and enrolling in a computer science bachelor program is not an option for me. I'm not interested in degrees or certificarites I just want to learn.,1643634195.0
sgzipv,"What is the best explanation you've ever read/seen on how computers go from bits to expressing logic. Still don't get it at its core unfortunately ;). And I don't only mean logic gates, I still don't get the big picture even with them.",64,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sgzipv/what_is_the_best_explanation_youve_ever_readseen/,26,,1643630008.0
sgvyvn,Overlaps in research between Computational Geometry and Graph Theory,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/sgvyvn/overlaps_in_research_between_computational/,2,"Hi, 

I'm currently a CS undergrad who is very interested in graph algorithms and NP-complete problems. I am going to start my grad school this year to study computational geometry (I only have basic knowledge about this such as polygon triangulation and partitioning.

While looking through some other applications of computational geometry, I noticed that there are problems that I am familiar with in a graph theoretic manner (such as the steiner tree problem and TSP as mentioned in [this 1976 paper](https://dl.acm.org/doi/10.1145/800113.803626) (Some NP-complete geometric problems). 

The school that I am going to mainly focuses on shape matching, casting, and dynamic convex hulls so I cannot see much intersections on Graph Theory here (I believe). 

My question is, are there current overlaps in research between geometry and graph theory (in the CS sense) that is active these days beyond TSP, geometric networks, and Steiner Tree? Thanks!",1643616183.0
sgh0h8,Everyone and their mother knows DSA is very important. But which side is more crucial?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/sgh0h8/everyone_and_their_mother_knows_dsa_is_very/,2,"There are much more data structures heavy topics and material that focus less on algorithms and then there are topics that focus heavily on algorithm design and advanced algorithms. 

Which is more practical to know better? I have a good understand of the basic ones learned in a DSA class, but I want to go ahead and study some more.


If you could be good at one and mediocre on the other, which would you choose to be better at?",1643571667.0
sg4epv,"In technical detail, how does the internet router work?",40,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/sg4epv/in_technical_detail_how_does_the_internet_router/,10,"I saw a comment on a programming subreddit that casually mentioned that they wrote code for an internet router, and I was mind blown. I decided to look up how they work because i was inspired to work on a project like that, but i didn't get any info. How do internet routers work?",1643530219.0
sfiktu,"Ever since I saw the graphs for the Lorenz attractor and logistic map in my first year CS course, I thought they would look cool as art prints, so I made some using Python and Photoshop!",307,1.0,computerscience,https://i.redd.it/pg40caf5ume81.jpg,19,,1643464477.0
sf7tjb,programming lang paradigm,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/sf7tjb/programming_lang_paradigm/,1,"Hi, what website has latest programming lang paradigm? I am thinking to create a new language and want to get some idea from the others. thanks",1643423943.0
sf7cyl,I'm confused? I can't tell the difference between a Software Engineer and a Programmer...,10,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/sf7cyl/im_confused_i_cant_tell_the_difference_between_a/,19,"I want to teach a computer to do things based on what I code it to do. What is this called? Is this a Software Engineer or a Programmer? I am confused and don't know where to go with this. I know you can code apps to pick out what the user wants. I'm doing research and the more I research the more confusing it gets. I don't want to be responsible for what things look like, I just want to do all the back end stuff. I just want to make things work. Make it make sense, please. I want to get a job, but don't want to learn the wrong stuff.",1643422532.0
sf5rtw,How does functional programming deal with memory management?,10,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/sf5rtw/how_does_functional_programming_deal_with_memory/,8,"In functional programming, we don’t create mutable data correct? I understand the usefulness of eliminating side effects for debugging, but doesn’t allocating a whole new data structure for every change gobble up memory? Or does this just depend on the compiler?",1643417759.0
sf5gj9,How do websockets and round robin LBs work together?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/sf5gj9/how_do_websockets_and_round_robin_lbs_work/,1,"Round robin LBs typically are TCP layer LBs which merely route incoming requests in RR fashion. Websocket protocol specifically needs that the client keep talking to the specific server with which it has opened the socket. How do client requests get routed to the same server by the TCP LB?
I tried looking it up online but couldn't find many resources",1643416854.0
sf4ab2,1982 'Twin Cities Computer User' Newspaper,66,0.99,computerscience,https://www.reddit.com/gallery/sewioz,3,,1643413550.0
seqqpv,Is Software Engineering a sub title to Computer Engineering?,2,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/seqqpv/is_software_engineering_a_sub_title_to_computer/,12," Is computer engineering consider top of the line or ""cooler"" than Software engineering? What I mean by this is that computer engineers often design embedded system or the computers/micro controllers and write low level software. After that the software engineers code och develop stuff on hardware that computer engineers made. For me it sounds like this: Computer engineering says to software engineer: ""Your not that good at computer so i'll build it for you and then you could program your software on my hardware with high level programing languages thats easier to understand"". Do you get what I mean, or am I wrong?   


Is there stuff that Software Engineers is able to make that the computer engineers aren't? My understandings is that because a computer engineer have made the computer they know everything about it and how to code and use it. And a software developer only learns how to use the tools that a computer engineer have made and dont need all that technical understanding. Is computer engineering higher prestige than software? Do a software engineer knows better way of making software than a computer engineer? This is my thoughts don't get offended by my thoughts instead describe to me how the job market works, im still a student.",1643376272.0
seq1gi,what is the purpose of the ISA Bridge in the following PCI diagram?,126,0.96,computerscience,https://i.redd.it/n90xtw3cdfe81.jpg,12,,1643374076.0
sepvgl,Learn Computational Economics (Free Course),18,0.91,computerscience,https://simulation.school/p/computational-economics,1,,1643373510.0
sed2hd,What are the best research papers/articles etc. for someone interested in zero-knowledge proofs and their practical applications?,6,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/sed2hd/what_are_the_best_research_papersarticles_etc_for/,0,,1643329084.0
secwcr,Do you need a theoretical understanding of all the math stuff for Computer Science?,98,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/secwcr/do_you_need_a_theoretical_understanding_of_all/,52,"I'm going through Precalculus II: Trigonometry, and I sort of do understand some of the stuff at a conceptual level but I'm kind of worried that I need to literally understand why everything works the way it does for Trigonometry, Calculus, and whatever other math is needed for Computer Science.

Like, I didn't sign up to be a mathematician, I'm fine with not knowing all the theoretical details, but I'm worried that I'm going to actually have to know them if I get into Computer Science.

So, is it a problem? (Note that this isn't about knowing the math - I'm going to learn Calculus and Linear Algebra Ok, that's not the problem, it's more about knowing how it works underneath)",1643328619.0
se2nq0,"Is there a ""trait theory""?",1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/se2nq0/is_there_a_trait_theory/,2,"I'm interested in getting into type theory, lambda calculus, etc. but I'm wondering if there's an equivalent model for *traits* rather than types.

For example, in C any non-zero value is True, so a pointer has the ""trait"" of being either true or false depending on whether or not it's null.

A `mod` type in Ada has the ""trait"" of being bounded between two values, and also the ""trait"" of overflowing to stay within those bounds.

This can allow you to express some very complicated things. The trait system in Rust allows you to say ""if I can do X, then I can do Y."" Using that, you can basically have behaviours ""implement themselves"" as long as there's a valid path through the trait implementations.

I'm wondering if there's a theoretical system similar to type theory that uses ""what you can do with something"" as the core idea rather than ""what something is.""",1643301893.0
sdxd4b,Data Structure & Algorithms Book for Beginners,10,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/sdxd4b/data_structure_algorithms_book_for_beginners/,6,"\[SOLVED\]

Hi all,

I know there are many posts here asking for book recommendations, particularly on this topic.

However I am looking for a recommendation of a good book for beginners with easy to comprehend language and structure. I agree that its a tough topic so all of them will be somewhat technical I am just wondering if anyone knows any that are both good but simple enough to follow!

Many thanks!",1643286803.0
sdur26,What is a good book for learning algorithms in programming?,84,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/sdur26/what_is_a_good_book_for_learning_algorithms_in/,29,"Hello there, i am a somewhat intermediate full-stack developer who is 100% self taught without any educational background in CS. but i want to up my game by learning some of the scientific fundamentals of computer programming, and i figured what better subject  to start with than algorithms!? so anyways let me know ur suggestions guys :D",1643276745.0
sdh6gc,Why doesnt multi-core processor like dual-core or quad-core perform 2x or 4x times faster?,61,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/sdh6gc/why_doesnt_multicore_processor_like_dualcore_or/,40,"Yes, its true, your program must be written in a way so it could be processed by multiple cores or processed effectively by them.

  
So to this question:  
Why dual/quad isnt 2x/4x faster then single core or why isnt quad 2x faster then dual core processor,

beside saying:

*  Due imperfect software algorithms and implementation. 

Are there any other facts/arguments we can add, like hardware ones, e.g.:

* They all share same resources, memory, bus etc etc

I know that there are some details that might need to be included in this like, brand, microarchitecture, speed etc etc... I am aware of that, its just a general question about multiprocessing.

Feel free to post any link where this might have been answered or it contain some explanation.

Thank you! :))",1643233106.0
sd0e5c,All the ways to model and deal with side effects in a program,18,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/sd0e5c/all_the_ways_to_model_and_deal_with_side_effects/,4,"I know about [monads](https://en.wikipedia.org/wiki/Monad_(functional_programming)) for dealing with side effects, by adding the ""world"" as an internal variable which gets passed around. And I know the basics of linear type systems (or more generally [substructural type systems](https://en.wikipedia.org/wiki/Substructural_type_system)), some of the ideas of which are used by Rust. Monads are used in Haskell or perhaps other languages, but these are purely for functional languages as far as I can tell (it is to make the functions pure again). I am not too sure about linear logic/types though.

But what other ways are there of dealing with side effects in terms of type theory, especially regarding to imperative or object oriented programming languages, dealing with pointers perhaps, or memory management, object creation and destruction, accessing and manipulating local/global state/context, etc.. It would be wonderful to paint a full picture of the possibilities the research has uncovered so far, so it is easier to make sense of how to model side effects (especially in imperative programs using type theory).",1643182419.0
scv94z,"Summary of the KindLang ""proof language"" theory?",3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/scv94z/summary_of_the_kindlang_proof_language_theory/,0,"Just stumbled across [KindLang](https://github.com/kind-lang/Kind), ""a modern proof language"", which looks amazing. I found some of the [old writing](https://github.com/kind-lang/Kind/tree/f70ed05356f4d6e88f4d28df3d1ddb0a3cad8c94), such as the so-called [FMC Specification](https://gist.github.com/lancejpollard/afa755498c6590c821290dc2b24a588b#file-fmc_specification-md), which describes some of the theory. Some important snippets from it are (bold added):

>Within the field of type theory, mathematical proofs and theorems are represented by functional programs and their associated types. This is called the Curry-Howard correspondence, and is the basis of modern proof assistants such as Agda, Idris, Coq and HOL/Isabelle. Sadly, those are complex software with monolithic implementations that are hard to audit and reason about. We're interested in a simpler alternative: a fully-featured proof assistant that can be turned into a small standard that is easy to analyze and implement independently.  
>  
>Under that perspective, one may be interested on the **Calculus of Constructions** (CoC), a small type theory that **easily fits 1000 lines of code** in a modern programming language. Sadly, that and similar languages **aren't capable of deriving mathematical induction**, an important proof technique without which any non-trivial theorem isn't provable. Moreover, it isn't capable of expressing efficient (constant space and time) pattern-matching, without which functional programming isn't viable. The usual solution to both problems is to supplement CoC with a native datatype system, but this results in the complexity explosion that is seen on the mentioned languages.

The same author made this [Calculus of Constructions repo in JavaScript](https://github.com/VictorTaelin/calculus-of-constructions/blob/5b4ede47dfcc408ce27903f78d29c81bf393a9d3/src/core.js), but it seems to have been replaced by KindLang.

So then it is stated:

>**A simple, clever alternative was proposed by** [Aaron Stump](https://homepage.divms.uiowa.edu/~astump/). With just one additional primitive, the ""**self type**"", **coupled with mutually recursive definitions, one can easily derive induction** for lambda encodings, i.e., a way to represent datatypes with native lambdas, subsuming the need for a separate implementation. This also solves the efficiency problem, as we're able to pattern-match in constant time with certain lambda encodings. The problem with that solution is that providing a semantics and, thus, proving the consistency of the resulting system becomes extremely hard.  
>  
>Aaron Stump **moved on from self types towards a very similar solution based on dependent intersections**. The idea is that, instead of relying on mutual recursion, inductive datatypes can refer to themselves in simplified, erased forms. This results in a comparably small language that is much easier to provide a semantics for, which Aaron Stump called Cedille-Core. In exchange, programming on it is hard. The simple task of defining a datatypes with efficient pattern-matching and recursion becomes a complex task involving non-trivial type-level manipulations and many intermediate proofs. While doable, programming in Cedille-Core without a supplementary language to do all that work for you is not practical.

Then it talks about ""consistency"" being a problem when modeling, which is a bit over my head. Then:

>Computationally, Formality-Core is just the lambda calculus.

Meanwhile, the latest KindLang [THEOREMS](https://github.com/kind-lang/Kind/blob/master/THEOREMS.md) doc describes how to prove things. Haven't read through all of that yet, though.

What I'm wondering is, what is the summary of the theory KindLang is built upon? What I shared above is pieced together.

I found this by landing on the [Calculus of Constructions](https://en.wikipedia.org/wiki/Calculus_of_constructions) (CoC) after looking at the [Lambda Cube](https://en.wikipedia.org/wiki/Lambda_cube). I found that from looking at [HoTT](https://en.wikipedia.org/wiki/Homotopy_type_theory) and [UniMath](https://en.wikipedia.org/wiki/Univalent_foundations), both of which are touted as ""foundations of math"". (UniMath is based on Calculus of Constructions as far as I understand). But as the above snippets show, CoC cannot model induction or recursion I guess, so I wonder how these can serve as foundations. So it looks like KindLang builds upon CoC like UniMath, but adds that ""self type"" that Aaron Stump talks about. *Does it add anything else?*

I was reading about [Cubical Type Theory](http://nlab-pages.s3.us-east-2.amazonaws.com/nlab/show/cubical+type+theory) (doesn't even have a Wikipedia page yet), but it was said it also had some key problems, and then landed on [this paper](https://arxiv.org/pdf/2101.11479.pdf) which states right up front:

>We prove normalization for (univalent, Cartesian) cubical type theory, **closing the last major open problem** in the syntactic meta-theory of cubical type theory.

*Maybe I misread that though.*

So I am wondering, what theory does KindLang use, and how does it relate to Cubical Type Theory, which seems to be the latest edition of Type Theory that is the most expressive.

Trying to figure out what the latest theory is for implementing a proof language like KindLang.",1643165204.0
scntb2,Resource recommendations for imperative algorithms that implement Type Checking and/or Type Inference,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/scntb2/resource_recommendations_for_imperative/,0,"What are some good books, journal articles, or even GitHub repos, which show how to implement algorithms for **type checking** and/or **type inference**? There appear to be several resources with implementations in functional languages like OCaml or Haskell, but I have yet to find anything which could be used to write a JavaScript implementation of type checking and type inference (i.e. algorithms defined in an imperative style). I would like to learn how to implement it in detail.",1643144450.0
scik77,A New Crisis in Mathematics?,27,0.97,computerscience,/r/math/comments/scamj7/a_new_crisis_in_mathematics/,0,,1643130626.0
sccyyv,"What's the most efficient way to keep yourself up to date in your field after working deeply focused in the lab all week (while experts in the field are non-stop posting amazing stuff in twitter, papers, podcasts, youtube, books, in-depth news, subreddits etc.)?",41,0.95,computerscience,/r/learnmachinelearning/comments/sc2w0e/whats_the_most_efficient_way_to_keep_yourself_up/,5,,1643114971.0
sbxbya,Max number of parallel http requests,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/sbxbya/max_number_of_parallel_http_requests/,3,"Hi all, is there a way to know/calculate how many parallel http requests a computer can send?",1643060451.0
sbw98k,How do general compression algorithms approach data redundancies in novel file formats?,23,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/sbw98k/how_do_general_compression_algorithms_approach/,7,"For example, if one had a source file containing 100 transformed instances of the same 3D model but that information is just vertices within the source file is a model built to recognize that data redundancy? Are algorithms for various types of information like this updated on a regular basis and integrated into the popular compression methods?",1643057763.0
sbk9aw,"Do you believe in cryptocurrency like Bitcoin and Ethereum, etc.? Why or why not?",0,0.48,computerscience,https://www.reddit.com/r/computerscience/comments/sbk9aw/do_you_believe_in_cryptocurrency_like_bitcoin_and/,34,"I would be asking this question in the crypto sub but that sub honestly just seems like an echo chamber. I myself have limited knowledge about cryptocurrency so I'm willing to learn more about it from some of you. Basically what I know is:

1. It acts as a digital ledger for a given set of programs.

2. There are limited number of Bitcoin and the more there are, the harder it is to mine them.

3. It can decentralise power and all computers acting as mining devices form part as a mini server for mining.

4. With all the talk about how Bitcoin is the future, I'm still skeptical yet to see mast adoption of this system even in the near future.

5. Many of the conversation about crypto (and Bitcoin specifically) seems and looks kinda scammy and I have also noticed many bots on such forums.

Apologies if there are some misinformed arguments here, as I said I myself have tried to understand it but still have a lot of confusion. So, what is your opinion?",1643024277.0
sa6sdw,Need recommendations for CS books and courses.,29,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/sa6sdw/need_recommendations_for_cs_books_and_courses/,16,"I will be starting my bachelors in CS in fall 2022.
Before the course begins , what Books/courses can I take/read for introduction on the topic.
Especially please recommend some basic beginner books for Linear algebra , Calculus and Discrete Maths.",1642870813.0
sasbd7,"In Dijkstra's Algorithm, if we assume that for a problem all input graphs are complete, the time complexity will be O(V^2) no matter the implementation, right?",5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/sasbd7/in_dijkstras_algorithm_if_we_assume_that_for_a/,3,"For example, the implementation with Fibonacci heap and adjacency list has a time complexity of O(E + V•log(V)). Because we assume the graph is always complete, the time complexity is O(V^2 + V•log(V)), which is O(V^2 ), right?",1642939758.0
saqw7i,Human Brain Cells From Petri Dishes Learn to Play Pong Faster Than AI,209,0.99,computerscience,https://science-news.co/human-brain-cells-from-petri-dishes-learn-to-play-pong-faster-than-ai/,29,,1642934253.0
s9gnb2,Maths for beginners book/resources recs,6,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/s9gnb2/maths_for_beginners_bookresources_recs/,11,"Hello, 
I'm planning on studying CS next year, and from what I've heard it's kinda maths heavy and I have forgotten everything since I graduated high school back in 2014, can you guys recommend any books or courses that would help refresh my memory and prepare me for Calculus and linear algebra? 
TIA.",1642787809.0
s9ffoc,What would you suggest ACM or OReilly?,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s9ffoc/what_would_you_suggest_acm_or_oreilly/,4,"I was looking to get a sub on OReilly Online Learning, but I noticed a lot of people suggest Association for Computing Machinery. I'm wondering how these compare for learning purposes and possibly certification, for Computer Science and Software Engineering generally.  


I also noticed that ACM offers OReilly material through their own subscription - what OReilly resources exactly does one get through ACM, is it books, etc. or live content and training as well?  


If you have any suggestions and tips on other similar platforms, please do share, I'd appreciate it a lot.  


Thanks!",1642784616.0
s9enpp,"Started learning ML 2 years, now using GPT-3 to automate CV personalisation for job applications!",259,0.99,computerscience,https://gfycat.com/snappysadichthyostega,43,,1642782622.0
s9c6ve,Are data types self-aligned or aligned according to the memory access granularity of the processor?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/s9c6ve/are_data_types_selfaligned_or_aligned_according/,1,"I was reading [The Lost Art of Structure Packing](http://www.catb.org/esr/structure-packing/) by ESR(Eric Steven Raymond) and he was written this:
> The jargon for this is that basic C types on x86 and ARM are self-aligned. Pointers, whether 32-bit (4-byte) or 64-bit (8-byte) are self-aligned too.

Now, he says that the data types are self aligned on x86 and ARM processors and pointers are self-aligned on 64-bit architectures too. Self-alignment means that data is stored at a memory address which is a multiple of the size of the data type of the data.

My question is that is data aligned according to its size(self-alignment) or according to the memory access granularity of the processor. There are hardly any cpu architectures which allow data types larger than their memory access granularity, so even it won't make a great difference if some data is self-aligned or aligned according to memory access granularity of the processor.

It would be great if you could specify some cpu architecture that allows data types larger than its memory access granularity. If self-alignment and alignment according to memory access granularity do not practically make a difference at the memory level, won't it still be better to say that data is aligned according to the memory access granularity rather than self-alignment.",1642775959.0
s95ek1,How are vectors used in Computer Science?,33,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/s95ek1/how_are_vectors_used_in_computer_science/,8,"I am working on a project and I want to explore how vectors are used in Computer Science.

Some articles/YT links on the same would be v helpful.",1642750830.0
s8kxq7,Name of Algorithm,7,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/s8kxq7/name_of_algorithm/,3,"Is there a name for an algorithm that assigns N outdoor people to N distinct houses such that the total distance they have to walk to their respective houses is minimized? (This is just another wording for assigning boxes to drop-off locations in Sokoban if you heard of that). Or is this an intractable problem where every permutation has to be tested?

Not sure if the Gale Shapley algorithm will work in this case??",1642690952.0
s8eq88,what book(s) would you recommend that could come across as motivational to a programmer? Something about the deep depths of the art of code and philosophy kind of thing?,80,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/s8eq88/what_books_would_you_recommend_that_could_come/,24,,1642669337.0
s8a2k9,Is cellular life itself an example of the P=NP problem?,0,0.36,computerscience,https://www.reddit.com/r/computerscience/comments/s8a2k9/is_cellular_life_itself_an_example_of_the_pnp/,11,"This is a little bit out there, but I think this is an interesting way to think about / apply the question of P=NP. 

Think about cellular life. Biologically, for many organisms (think simple multi cellular; plants, things that don’t have super complex brains) we understand the structure and function of literally every single atom and cell that they are made of. But we are unable to synthesize these organisms we know so much about ourselves in a lab despite knowing how to perfectly verify nearly every piece of their structure by looking at them. 

Translating to the P=NP problem, the “verification” of a problem can be thought of as examining every single piece of an organism and confirming that it is what we think it is. The “solving” of the problem would consist of creating such an organism from scratch, from non-living material. Despite knowing how every single piece of a multi cellular organism works and being able to verify them, humans cannot solve them by creating them in a lab. 

What is the only current known way to “create” life? By waiting for nature’s random iteration to try and fail over billions of years and trillions of “guess and check” iterations. If evolution can be thought of as the only way to achieve the “solution”, until we can figure out a way to synthesize complex multi cellular organisms from scratch, nature’s answer to the P=NP problem is that verifying a solution does not imply that it can be solved. Only guessed and checked over trillions of years.

This may not make sense but I think it’s an interesting way to think of how nature sort of answers one of humanity’s biggest questions.",1642652810.0
s82mjq,How did you learn about different protocols,4,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/s82mjq/how_did_you_learn_about_different_protocols/,4,"I always wonderd, how people can write programs, that interact with the web like its their harddrive, I always get confused what, how an why the different protocols do. My biggest problem is finding a good resource to learn about the stuff, often I find myself on websites, that are glossing over details and leaving out things. So my question is where did you start and where did you learn it ? 

Good book recommendations related to the subject are appreciated",1642631640.0
s7yrsa,Why is there nothing between 8bit and 16bit?,36,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/s7yrsa/why_is_there_nothing_between_8bit_and_16bit/,31,"For example, if i want to change the image specifications in photoshop, i can choose between 8 and 16 bit. But why is there nothing inbetween 8 and 16 bit? Since the difference between those to options is that high, 8 bits are sometimes too less and 16 bits are sometimes too high.",1642621874.0
s7wqyg,Will a RISC like instruction set ever be developed for QC?,5,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s7wqyg/will_a_risc_like_instruction_set_ever_be/,0,"Horray for quantum computing picking up more traction for the past few years. I've done a fair bit of reading and tinkered with things like qiskit  


but algorithm implementation on a per qubit level seems a little 😬

&#x200B;

What does a RISC-like instruction set look like for QC, is it even viable? Are circuit construction implementations here to stay?",1642616748.0
s7mb3k,Why is Knapsack NP-complete?,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/s7mb3k/why_is_knapsack_npcomplete/,3,"Here's what [wikipedia](https://en.wikipedia.org/wiki/Knapsack_problem) says:

> Even if P≠NP, the O(nW) complexity does not contradict the fact that the knapsack problem is NP-complete, since W, unlike n, is not polynomial in the length of the input to the problem.

`W` is the max weight capacity.
`n` is the number of items.

Why is `n` polynomial in the length of the input? 
If the input is binary, `n` is definitely exponential in the bit length.

Also, for problems in NP, we can verify them in polynomial time. However, here for a polynomial time algorithm in the numeric values `n` and `W` for knapsack, we still call knapsack NP-complete because of the length of the input. By the same logic, wouldn't the algorithm to verify a solution to knapsack **not** be polynomial (pseudo-polynomial) - and so, it's not even in NP?  
Could you please clarify this?",1642584461.0
s7li8v,Why is Internet/transmission speed in Bit and storage capacity/file size in Byte?,61,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s7li8v/why_is_internettransmission_speed_in_bit_and/,42,"I am always wondering about this. Why Internet/transmission speed is always in Bits (Kbits, Mbits...) and storage capacity/file size is always in Bytes (Kbytes, Mbytes...)? Can't we just have the same unit for both of them (to reduce the confusion of some people)? Is there a reason for the convention like this?",1642581032.0
s7h2vc,Can someone explain to me Big O notation like I'm dumb?,49,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s7h2vc/can_someone_explain_to_me_big_o_notation_like_im/,16,I've learnt it today in Java but can't seem to wrap my head around it. How can I learn to use it?,1642565300.0
s71762,Composite Images of Sphere into 3d object and more?,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/s71762/composite_images_of_sphere_into_3d_object_and_more/,3,"I have a question, which I believe is fairly well defined, but I don't have the knowledge to approach it.  If anyone has an idea of a better place that I can ask this question that would be excellent.

The problem :  I'm playing a game called Scam \*cough\* I mean Star Citizen and there are many large 3d worlds to explore but no coordinate system yet.  It does, however, give you the distance in kilometers you are from given hubs on said world as well as the center of the planet itself.

The hair brained scheme:  
Step 1 - take screenshots of the planet from X km away at each pole and then again from a few angles around the equator.  
Step 2 - stitch those together onto a 3d sphere to create a mapped globe  
Step 3 - take screenshots of interesting locations by flying to a certain altitude and looking straight down.  Then try to find software that can look at the screenshot and match it with a given location on the globe

Some problems I'll face:  
\- the screenshot will be a 2d image of a sphere and so will need to be reshaped as it cant just be put on as a skin ( there is a better mathy way of saying this right? something something projection? but im making sense yeah? )  
\- The planet will have lighting and cloud cover which change and could be confusing to any ML Algo.  
\- Mapping locations to the planet might be difficult as the planet may be too 'samey'.  A second option is to record the distance one is from several WP on the planet and use that information to, i believe the word would be, ""triangulate"" the position? yeah? Does that sound right?

This sounds very possible to me but might require some specialized ML magic that I'm not educated on? If anyone has any advice for how to better approach or frame the problem, or potential solutions for software or math that can be used to solve this, I would be stoked :)

In case it's relevant I'm fairly competent with JS and Node

Example Images

https://preview.redd.it/mv9r2kbk9hc81.png?width=898&format=png&auto=webp&s=94718778f56c9751b692874cbab53eb6f5019911

&#x200B;

https://preview.redd.it/ze65vktr3ic81.png?width=987&format=png&auto=webp&s=2f70d344c0756252f0f8f91aadff95052bc3dcbb",1642522761.0
s6zxpb,"Creating a scalable intent classifier with Elixir, Python and Tensorflow | Arjan Scherpenisse - ElixirConf EU 2021",1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s6zxpb/creating_a_scalable_intent_classifier_with_elixir/,0,"Arjan Scherpenisse gave an amazing talk with title ""Creating a scalable intent classifier with Elixir, Python and Tensorflow"" for the #ElixirConfEU conference in Warsow.   

Watch the video & learn more about the QnA ninja, a classifier service that recognizes text  https://youtu.be/U8c\_hsWC2jE",1642519300.0
s6v29r,Good resources to learn more about authentication and authorization concepts?,3,1.0,computerscience,/r/webdev/comments/s6ev5f/good_resources_to_learn_more_about_authentication/,0,,1642504022.0
s6mxkp,Need some Cache Memory analogies…preferably to a refridgerator. Thanks :),9,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/s6mxkp/need_some_cache_memory_analogiespreferably_to_a/,3,,1642475188.0
s6msij,Where can I get access to industry journals or scholarly articles free/cheaply?,11,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/s6msij/where_can_i_get_access_to_industry_journals_or/,4,"Having finished a masters a couple years ago,  I miss having access to journals and articles that my university library provided. Is there a free/inexpensive source of similar materials that I may be overlooking?

Thank you!",1642474798.0
s6ijx1,"""The early days of Unix at Bell Labs"" - Brian Kernighan (LCA 2022 Online)",5,0.86,computerscience,/r/unix/comments/s6iiyh/the_early_days_of_unix_at_bell_labs_brian/,0,,1642462826.0
s694uk,"First time posting here, hope this is the right kind of thing to put here. Can someone explain the basics of Bayesian Neural Networks to me? Specifically if they address the problem of overconfidence in AI systems when they encounter information outside of their data sets. Thanks!",5,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/s694uk/first_time_posting_here_hope_this_is_the_right/,4,"Bit of background, I come form the social sciences and very little computer science knowledge but I'm researching how the integration of AI will impact my field of study. 

One potentially large problem I've found is that AI can be overconfident when faced with information outside of what it has been taught to recognize. I've read that bayesian neural networks can mitigate this problem but my lack of computer science knowledge making me run into a wall when it comes to understanding exactly why it fixes this problem. 

I'm doing other readings on this topics but I thought I farm this out to Reddit and use this site for more than just watching people argue about literally anything. Thanks for any assistance or sources you can provide!",1642439716.0
s67zoq,Question about the correctnes of an algorithm,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/s67zoq/question_about_the_correctnes_of_an_algorithm/,4,"**Problem statement:**

>Write a function, which creates a random graph of a certain size as follows. The function takes two parameters. The first parameter is the number of vertices n. The second parameter p (1 >= p >= 0) is the probability that an edge exists between a pair of nodes. In particular, after instantiating a graph with n vertices and 0 edges, go over all possible vertex pairs one by one, and for each such pair, put an edge between the vertices with probability p. The graph representation is an adjacency matrix.

**Proposed algorithm:**

    1- Create a bool matrix (2d nested array) of size n*n initialized with false.
    2- Run a loop over the rows:
      - Run an inner loop over the columns:
        - if row_index != col_index do:
            - curr_p = random() // random() returns a number between 0 and 1 inclusive
            - if curr_p <= p: set matrix[row_index][col_index] = true
              - For an undirected graph, also set matrix[col_index][row_index] = true
    
    **Note:**
    Since we are setting both cells (both directions) in the matrix
    in case of a probability hit, we could potentially set an edge 2 times.
    This doesn't corrupt the correctnes of the probability
    and isn't much additional work. It helps to keep the code clean.
    
    If you want to optimize this solution, you could run the loop
    such that you only visit the lower-left triangle (excluding the diagonal)
    and just mirror the results you get for those cells to the upper-right triangle.

Now, someone told me that without the optimization the algorithm is wrong since it calculates the propability of edge existence twice.  In their words:

>With the current code in the undirected case, the probability that matrix\[0\]\[1\] will be set is the probability that the random number in the iteration with col=0/row=1 or in the iteration with col=1/row=0 is smaller than p. That probability is 1-(1-p)\^2.

&#x200B;

At first I thought they were right, but thinking more about it it occured to me that since the random number generations are independent of each other and since we don't need 2 false (or 2 trues) to set or unset an edge, the probability calculation of the algorithm is correct. If their argument was correct, it would be like saying when you throw a 6 on a 6 sided dice, your chances of getting a 6 next time are less than 1/6. In other words, it looks like the [Gambler's fallacy](https://en.wikipedia.org/wiki/Gambler's_fallacy) on their part.

&#x200B;

I hope I've managed to convey the problem and arguments in full. If not, please ask.

What I'm looking for is feedback on which argument is correct. Thank you very much in advance.",1642436886.0
s67l1c,What is Singular Value Decomposition (SVD)? A 2-minute visual guide. [OC],54,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/s67l1c/what_is_singular_value_decomposition_svd_a/,8,"&#x200B;

https://preview.redd.it/3a0ls71hv9c81.png?width=2048&format=png&auto=webp&s=fbf7e51bb04eb3215d08ff139b831a059e1a145a

https://preview.redd.it/lhtxvf3hv9c81.png?width=2048&format=png&auto=webp&s=0f072d9ab91e62c9dcb06b8a0a3ef342333d4105

https://preview.redd.it/6hswrj3hv9c81.png?width=2048&format=png&auto=webp&s=48a76af7b76ac6b4d1150b5e718bc82137ed29a9

https://preview.redd.it/ckrbdg1hv9c81.png?width=2048&format=png&auto=webp&s=8d441f775adde43a065b8e36403ac2cb514de0ef

https://preview.redd.it/zvtlv91hv9c81.png?width=2048&format=png&auto=webp&s=6b948bbe8b0ca2227ad370569f7b1be74efe72bc

https://preview.redd.it/hphsu02hv9c81.png?width=2048&format=png&auto=webp&s=74df09e30462215b4d2c0f203c772ba2b9d11824

https://preview.redd.it/hglrki1hv9c81.png?width=2048&format=png&auto=webp&s=9163169b5eacdafcca7ec3c61e31675f730791f1

🔵 Singular Value Decomposition 🔵

🪄 Remember from the [post on eigenvectors](https://www.reddit.com/r/learnmachinelearning/comments/ruf6eq/what_is_an_eigenvector_a_2minute_visual_guide/): eigenvectors of a matrix are special vectors that only get scaled when the matrix is applied to them. While EVD (eigenvalue decomposition) can only be applied to special matrices (diagonalizable matrices) SVD is a generalization of EVD and can be applied to any rectangular matrix.

🧩 Like prime factorization where a number is broken down to its prime factors (simpler pieces = prime numbers in this case); SVD factorizes the matrix into simpler pieces i.e. simpler matrices. This factorization or decomposition comes in handy for many applications some of which I briefly touch upon later.

🏥🦴 Time for (somewhat of) an analogy: You can think of SVD as an x-ray of a matrix. It provides us with simpler pieces that constitute the matrix and by looking at these simpler pieces i.e. simpler matrices we can say a lot about the matrix in question. Things like its (pseudo-) inverse, rank, null-space, range among other things can be easily determined the same way a doctor can say a lot with an x-ray scan of your body.

🔨 The Singular Value decomposition breaks the original matrix into 3 pieces: 2 unitary matrices and a rectangular diagonal matrix. You can draw a parallel with the Eigenvalue decomposition equation and see exactly how SVD has a more general form.

🔄 With the help of the Singular Value decomposition we can explain what happens to an arbitrary vector when a matrix M is applied to it. Since M can be broken down into simpler pieces, we can represent the matrix M by a rotation, followed by scaling (by the singular values) and another rotation. So if you had a circle it would first be rotated then stretched by the singular values (becoming an ellipse) and finally be rotated to end up as a rotated ellipse.

🗜️ There are numerous applications of SVD. Perhaps you could comment on the ones that I missed but you know about. One of the nicest things you can get is the pseudo-inverse of a matrix without doing crazy matrix inversion. This is because the simple pieces that SVD provides are extremely easy to invert making inversion of the original matrix child's play. SVD is heavily applied in solving numerical problems such as solving linear equations that so frequently pop up in engineering.

🎥 Data compression and computing low-rank approximations make it handy for compressing images and videos so you can watch cat videos without buffering. And of course, SVD has also been used for ranking web pages like Google's Pagerank algorithm does so you can find your favorite recipe with a quick search.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1642435873.0
s5vvf5,Initiating a study-group for MIT's Algorithms Course,60,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s5vvf5/initiating_a_studygroup_for_mits_algorithms_course/,7,"Hello,

I am looking for other students to join me in learning [Erik Demaine's algorithms course](https://stellar.mit.edu/S/course/6/sp15/6.046J/).

The course is mathematically matured and requires a background in discrete math, logic and proof techniques.

If you are already familiar with algorithms, Then you might focus more on advanced lectures. If you lack background in math, Then you might focus more on early lectures. Students coming from non-CS background are highly welcomed, including engineering, physics, and pure-math. Especially if they were interested in computational methods. We hope they contribute to CS students a new perspective about computation.

We hope to initiate a community, Where you can mutually contribute and benefit from others' experiences, skills, and backgrounds. You are welcome to ask questions, share your solutions to be reviewed by others, and even suggest further problems/topics.

We plan to complete two weeks of the course on one month, and have 1 hour meeting per week. There is no obligation to attend all meetings, solve the whole course's problem-set, or to respond to every question posed. Learn and engage with other members at your own suitable pace.

My availability is limited to 2 weeks per month, but other students are welcomed to collaborate at any time.

Join by submitting [this](https://docs.google.com/forms/d/e/1FAIpQLScDP0qFW6VhPMyD5eBfBfphYIfeq9WitJzfbvtzqJtHUhD5hQ/viewform?usp=sf_link&hl=en) google form.",1642396803.0
s5tfpw,Question about Neural Networks and Video Games,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/s5tfpw/question_about_neural_networks_and_video_games/,7,"I understand that neural networks have been trained to play games like chess pretty effectively. I've also seen some neural networks able to generate photographs based on a text description.

But has anyone tried training an AI to be a video game? In my mind it seems feasible because you could theoretically automate the training by making a bot play an existing game like Doom Eternal and then treat the bot's commands as input and use the resultant game state as the output that the neural network needs to fit to. And then if your network learned the game correctly, a human could give it input with their controller and it would return a close approximation of whatever game state the original game would have given. And then you'd only need a computer powerful enough to run the NN instead of the full game.

I'm curious because I'm 100% certain someone smarter and more experienced than me has thought of this before but Google isn't giving me any good articles on the subject",1642389192.0
s5swwz,Aren't software and hardware (kind of) the same thing?,0,0.28,computerscience,https://www.reddit.com/r/computerscience/comments/s5swwz/arent_software_and_hardware_kind_of_the_same_thing/,4,"I'll try to clarify. A software (like an OS), to exist, needs a hardware (memory). All the software information is stored in the hardware, which means it is a physical thing (like circuits). So, if we define a hardware as ""the associated physical equipment of a computer, directly involved in the performance of data-processing or communications functions"", turns out software is hardware.

The difference is a practical approach, where software is a set of instructions (programs), and hardware is the PC parts (CPU, GPU...).

It is like your brain and your thoughts (technically, thoughts exist inside the brain)

I'm totally a layman on the subject, so don't judge me if I'm saying bullshit lol",1642387620.0
s5r8t0,Where do you get your latest news and tutorials when you just want to browse what's new on computer science?,16,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/s5r8t0/where_do_you_get_your_latest_news_and_tutorials/,5,"I am interested in browsing new and exciting areas on computer science like programming, diy instructions on software, and reading informative articles on software or blogs. 

What are some sites every new techie should be on to read such material?",1642382740.0
s5i47n,stupid question,43,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/s5i47n/stupid_question/,11,I watched a video on how computers work and how it uses binary instead of base 10. But if computers had a way to use base 10 or like distinguish 10 different types of charges in a transistor or something wouldn’t that computer be like ultra super fast???,1642357905.0
s57kne,Making Your Game Go Fast by Asking Windows Nicely,41,0.87,computerscience,https://www.anthropicstudios.com/2022/01/13/asking-windows-nicely/,6,,1642322533.0
s4k68d,Any essential comp sci books that you wish you read earlier?,83,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/s4k68d/any_essential_comp_sci_books_that_you_wish_you/,31,"I am a high school senior who has a lot of free time and loves studying comp sci. I have read an introductory Java book, SICP(chaps 1 and 2), and the C programming language or K&R. I am interested in continuing with chap 3 of SICP but I am also looking for another challenge: think comp sci essentials. All suggestions are appreciated.",1642252786.0
s4h4a4,Difference Between an Implementer of C and a C Programmer?,47,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/s4h4a4/difference_between_an_implementer_of_c_and_a_c/,16,"(Brand new to CS so bear with me) I can't find anywhere online so far about what an implementer is and how that is different from a programmer. 

(My question comes from a C language book that talks about how part of the book is meant for programmers, not implementers.)

Thanks!",1642240851.0
s4frba,Are there any effective machine learning methods that aren't copied from nature?,13,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/s4frba/are_there_any_effective_machine_learning_methods/,6,"Recently have been struck by the fact that the two most powerful machine learning methods, neural networks and genetic algorithms, are partly just copied from nature (in concept at least, obviously was a ton of work by a lot of brilliant people). I guess there is a lot of machine learning that is basically just memorization or math but are there any other deep learning algorithm paradigms out there? Do you think there could be someday?",1642235250.0
s3yrrt,How does Concorde solver work ?,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/s3yrrt/how_does_concorde_solver_work/,5,"I'm looking for some resource clearly explaining how this solver for the tsp works.

I read that it uses the Lin-Kerighan heuristic and the branch and bound algorithm, but I didn't find a full explanation of how it works. The closest thing I read was [in the answers of this post](https://www.reddit.com/r/compsci/comments/8auwm9/how_does_concorde_claim_to_be_a_tsp_solver) , but it's not a full answer.

If you have the knowledge or any link/paper available online or on scihub that explains how it works in detail / in summary, that would be awesome",1642183919.0
s3wf1q,Nerves - An industrialization journey | Jean Parapaillon | ElixirConf EU 2021,6,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/s3wf1q/nerves_an_industrialization_journey_jean/,0,"By integrating buildroot, BEAM and erlang/elixir ecosystem into a comprehensive toolkit, Nerves has become a major framework in the IoT field. Watch the video and see Jean Parapaillon  talking about ""Nerves - An industrialization journey""  

https://youtu.be/6MDNm7k2OoI",1642177798.0
s3mbvt,Interesting Computer Science youtubers?,121,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/s3mbvt/interesting_computer_science_youtubers/,47,"I have been wanting to find some good videos that I can watch in my free time that are about cool computer science projects so I can learn more about new algorithms, and programs in a more leisure way instead of solely doing projects and reading documentation. 

&#x200B;

I'm interested in most anything related to Python, Data science, or back end development, but I'd really love to learn more about Machine learning algorithms if there are any good series about people working on machine learning algorithms.",1642144705.0
s3gau6,What makes cyclic tag systems Turing-complete?,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s3gau6/what_makes_cyclic_tag_systems_turingcomplete/,1,"I've been doing research on computational models, but I can't find anything on cyclic tag systems and their proof of universality. I want to know because it's supposedly what rule 110 can emulate (more specifically that's the proof that rule 110 is universal), however the fact that rule 110 can emulate a cyclic tag system means nothing to me if I can't find proof that cyclic tag systems are universal, let alone an implementation of rule 110 running a cyclic tag system. Does anyone know where I can find a cyclic tag system's proof of universality?",1642125936.0
s36y35,Novel view tennis from single camera input,241,0.95,computerscience,https://v.redd.it/v6xlgqq17ib81,9,,1642100761.0
s33v2n,"If Interpreters are used to run code and are written in another language, won't interpreters need interpreters?",1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/s33v2n/if_interpreters_are_used_to_run_code_and_are/,5,"Sorry if my question sounds dumb but I am a beginner to coding and have started with Python. Python is an interpreter-based language and not a compiler based language ( I know the difference between the two) 

As I was going through my online lectures, it is stated that python is interpreted with Cpython, a software written in C. Similarly, they have Jython (written in Java) and PyPy (written in Python). My question is is these interpreters break down the language into low level code like Assembly and machine code, then who helps interpret the software of these interpreters so that they run in the computer? 

Again sorry for the dumb question",1642092742.0
s3241k,Confirmation on if I am understanding hardware/software interaction correctly,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/s3241k/confirmation_on_if_i_am_understanding/,4,"Recently I have begun learning about how computers actually know to execute your programs, and I was hoping somebody could verify if I generally have the idea correct. 

- computers come preset with instructions on how to boot the OS located in the ROM. Once this takes place the OS controls the remainder of operations. 
- when you plug your computer into the wall, electrical signals are sent in clock cycles that run at some frequency all the time, either idle or active. These signals are always looking for data to transfer via the bus to read, but they are essentially always running (this is the part I'm shaky on I think) 
- when you write and compile a program say hello world, you have organized some high level code into what can be translated into machine code and binary. When you executed this is sent to the ram, where the clock frequency (always looking) activates logic gates based on what was sent, and tells you what the output it.
- if you wanted to use an embedded controller to run a motor the same applies where the programmer writes readable code which can be converted to binary, and is sent to the pinouts in such a way that causes some actuation to happen (such as alternate pulses to a stepper motor) 

Am I on the right track here ? 

Thanks 

Also, who sits around and actually designs computer architecture, I feel like that has to be one of the most complicated jobs on the planet. You've got millions of transistors to manage controls? Or is it more abstract than that.",1642087995.0
s2qf5f,This book demonstrates an infinite loop in a pretty cool way!,1271,0.98,computerscience,https://i.redd.it/jx92aw75udb81.jpg,33,,1642047968.0
s2nqom,Is there any overlap between computer science and foreign languages ?,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/s2nqom/is_there_any_overlap_between_computer_science_and/,6,"hey guys, basically the question. and forgive it is a noob question. I'm relatively new to computer science but have really started taking a passion to programming. I also have a deep love for studying for foreign languages. 

Is there any obvious / not so obvious overlap between the two?",1642040102.0
s2gdc0,Question: What does 'digital' mean?,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/s2gdc0/question_what_does_digital_mean/,5,"I've gone about my life for a while thinking that even 'digital' things have some physical correspondence. i.e. binary numbers are just surges of voltage and things of the such. My real concern for this idea turned up when I was learning about actuators and learned that to get it to receive some electrical charge it needed to pass through a DAC. Implying that the charge isn't a digital element and my correlation may have been wrong.

Are digital things not physical things? I'm pretty sure that's not true but I have absolutely no idea what to make of the idea now...",1642020128.0
s2ccci,A Guide to Combatting Human-Operated Ransomware: Part 1,2,0.67,computerscience,https://www.microsoft.com/security/blog/2021/09/20/a-guide-to-combatting-human-operated-ransomware-part-1/,0,,1642009924.0
s2ar5c,Just how powerful is an 8Bit CPU?,52,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/s2ar5c/just_how_powerful_is_an_8bit_cpu/,16,"By ""8Bit"" i mean a CPU that uses registers that are 8 bits wide and with an ALU that takes two 8 bit binary numbers and performs operations on them, for the sake of this post, lets say the ALU is capable of bit shifting, NANDing and Adding.
So just how capable this CPU will be, Will it be able to run a very simple version of tetris? or pong?",1642006016.0
s28p5d,Love Your Crash Dumps | Michał Ślaski | ElixirConf EU 2021,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/s28p5d/love_your_crash_dumps_michał_ślaski_elixirconf_eu/,0,"Livebook enabled a new level of interactivity with a running BEAM VM. Michal Slaski presented his talk ""Love Your Crash Dumps"" at #ElixirConfEU, describing the techniques for live analysis of VM performance.

Watch the video and learn more: [https://youtu.be/wyjWR731uSU](https://youtu.be/wyjWR731uSU)",1642000729.0
s20uj1,Adjacency List Representation of a Graph: Time Complexity to get the list of all neighbours/adjacent vertices of a particular vertex,0,0.5,computerscience,/r/AskComputerScience/comments/s20ofa/adjacency_list_representation_of_a_graph_time/,0,,1641974267.0
s1yfl6,Any ideas how hardware is physically-created? There are no comprehensible guides in the internet about the process.,63,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/s1yfl6/any_ideas_how_hardware_is_physicallycreated_there/,30,"By how is created I do not refer to a general theory like what's CPU/GPU or motherboard, but as if how they are physically designed and produced, when I was getting my degree they explained to us the basics (been years since then) but never the exact process.

I also saw some youtube videos but they seem more of hype inducers and they seem to be divided into scientists wearing anti-dust suits and working on the nano-world and people doing mechanical tasks. I'm looking for a more pragmatic and practical procedure about how hardware is truly created, anyone has any leads?",1641965454.0
s1h1gt,Subredditd and youtube channels for staying up to date,14,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/s1h1gt/subredditd_and_youtube_channels_for_staying_up_to/,2,"Not sure if this is the right subreddit for this, but im looking for a few subredditd and youtube channels for staying up to date and just general knowledge of the software development industry. Can anyone help?",1641918826.0
s1gfy8,What is Gradient Descent? A 2-minute visual guide. [OC],36,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/s1gfy8/what_is_gradient_descent_a_2minute_visual_guide_oc/,9,"&#x200B;

https://preview.redd.it/qa90r07j13b81.png?width=2048&format=png&auto=webp&s=7373c6bf3bcfec41bf1794f47317b637438027f9

https://preview.redd.it/4w1nhr6j13b81.png?width=2048&format=png&auto=webp&s=770a39f232e15e0e44407d8d5ea3ffa2b504cb12

https://preview.redd.it/zi4hqe7j13b81.png?width=2048&format=png&auto=webp&s=0085316ff44aaed6d78c07dc3c92a06635690472

https://preview.redd.it/ahkxq67j13b81.png?width=2048&format=png&auto=webp&s=42a6545dd07b141e2e2a7ebe92e4e203909107c2

https://preview.redd.it/nxiiny7j13b81.png?width=2048&format=png&auto=webp&s=6e5d77c26e6de57b25a7408a0b5d88f94a2b8ac5

[EDIT: Thank you to u\/antiogu for pointing out the error. The y-intercept should be 2 in my sketch.](https://preview.redd.it/chwmmn3j13b81.png?width=2048&format=png&auto=webp&s=5a5cc5127a2fa7d8bf3811b26e64ec1a77e405eb)

🔵 Gradient descent 🔵

💾 A more detailed post this time but I wanted to make sure I touch upon some basics first before diving into gradient descent itself. This is mainly so that it is more inclusive and no one feels left behind if they have missed what gradient is and if you already know what it is you get to brush up on the concept.

🏃 Although a relatively simple optimization algorithm, gradient descent (and its variants) has found an irreplaceable place in the heart of machine learning. This is majorly due to the fact that it has shown itself to be quite handy when optimizing deep neural networks and other models. The models behind the latest advances in ML and computer vision are majorly optimized using gradient descent and its variants like Adam and gradient descent with momentum.

⛰️ The gradient of a function is a vector that points to the direction of the steepest ascent. The length or the magnitude of this vector gives you the rate of this increase.

🔦 Time for an analogy: it is nightfall and you are on top of a hill and want to get to the village down low in the valley. Fortunately, you have a trusty flashlight that helps you see the steepest direction locally around you despite the darkness. You take each step in the direction of the steepest descent using the flashlight and reach the village at the bottom fairly quickly.

📐 Gradient descent is an optimization algorithm that iteratively updates the parameters of a function. It uses 3 critical pieces of information: your current position (x\_i), the direction in which you want to step (gradient of f at x\_i), and the size of your step.

🧗The gradient gives the direction of the steepest ascent but because we need to minimize we reverse the direction by multiplication with -1.

🎮 This toy example illustrates how gradient descent works in practice. We compute the gradient of the function that needs to be optimized i.e. the differentiation of the function with respect to the parameters. This gradient gives us the information we need about the landscape of the function i.e. the steepest direction where we should move in order to minimize the function. A point to keep in mind: gamma the step size (also called the learning rate) is a hyperparameter.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641917310.0
s14xir,Favorite CS books?,89,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/s14xir/favorite_cs_books/,49,"Basically title. I've been searching for some good CS books, but can't find much other than 1) the mainstream novels about ""AI"" that are of no use to someone who's studied CS and 2) the tutorial type books titled ""How to \[do something\] with \[insert popular technologies\]"". Looking for recs.",1641877819.0
s0vdjp,Hitting a wall between math and data structures,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/s0vdjp/hitting_a_wall_between_math_and_data_structures/,5,"Hello everyone,

I've been stuck in the classic problem of applying mathematics in the context of data science. However, I am much stronger in my math-side than in computer science. If you gave me a homework sheet of problems in stats, multivariable calc or linear, I'm happy. If you ask me to apply mathematics for machine learning, I'm drawing blanks.

Has anyone else struggled with this disconnect? If so, were there classes or concepts that helped build that bridge? Is this data structures with extra steps?

Thank you in advance",1641850841.0
s0jzi8,What are 1s and 0s in computers?,6,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/s0jzi8/what_are_1s_and_0s_in_computers/,21,"I have not learnt much electronics but I do kinda know how the CPU works and how logic gates and all the higher level more abstract things work in a CPU.

I‘m really confused by this, some say 1 and 0 is the state of the transistor but that doesn’t make sense to me. I‘ve also read that it’s a high vs low voltage, but than isn’t voltage the electric tension between two points, which points would that be? I have always imagined the 1s and 0s or asserted and deasserted signals as high and low amperage, is that wrong?",1641821166.0
s0ihgg,PIEEG: Turn a Raspberry Pi into a Brain-Computer-Interface to measure biosignals. arXiv:2201.02228,12,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/s0ihgg/pieeg_turn_a_raspberry_pi_into_a/,0," This paper presents an inexpensive, high-precision, but at the same time, easy-to-maintain PIEEG board to convert a RaspberryPI to a Brain-computer interface. This shield allows measuring and processing eight real-time EEG(Electroencephalography) signals. We used the most popular programming languages-C, C++ and Python to read the signals, recorded by the device. The process of reading EEG signals was demonstrated as completely and clearly as possible. This device can be easily used for machine learning enthusiasts to create projects for controlling robots and mechanical limbs using the power of thought. We will post use cases on GitHub (https://github.com/Ildaron/EEGwithRaspberryPI) for controlling a robotic machine, unmanned aerial vehicle, and more just using the power of thought.",1641816059.0
s0ficm,CS themed graduation party!,517,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/r89rlr/graduation_party/?utm_source=share&utm_medium=ios_app&utm_name=iossmf,32,,1641804617.0
s01ijj,A.I. Debate topics,32,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/s01ijj/ai_debate_topics/,29,"Hello! I'm a high school computer science teacher, and teach a course on computer ethics. One of my units is on A.I. and I want to conclude the unit with student debates on topics in AI. I'm struggling to come up with topic statements however. I know for sure I want one of the topics to be centered on whether A.I. at an advanced level should be afforded the same rights as humans.  


Any other topic statement ideas? Thanks!",1641762273.0
s011f6,Why did it take so long for consumer OS's to get UNIX-like features?,2,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/s011f6/why_did_it_take_so_long_for_consumer_oss_to_get/,9,"Forgive any and all ignorance as I ask this question please. As I understand it, Linux was the first open source UNIX-like OS, correct? And UNIX itself has been around since 1969!

My question is, if the robust stability and security features like protected memory, preemptive multitasking, secure user accounts/admin stuff and all that have existed for so long... why did Mac OS and Windows both lack any of that robustness until a full decade after Linux was created?

In other words, if we had a way to make an OS that would virtually never crash and had orderly memory management and security and the rest, why did Apple and Microsoft both ship such fragile OS's that could be toppled by a single rogue app and were generally terrible at security and for *SO* long?

Apple even had [it's own UNIX implementation](https://en.wikipedia.org/wiki/A/UX) for the early Mac computers complete with the System 7 GUI laid over top. Anyone care to speculate why THAT wasn't just the Mac OS everyone got? Microsoft also [licensed a UNIX version](https://en.wikipedia.org/wiki/Xenix) from AT&T in the early days. So it's not as if no one knew computers didn't have to suck. Why did it take so long to reach the average PC user?   Thoughts?",1641761027.0
rzqau2,Meaning/purpose of computer science,58,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rzqau2/meaningpurpose_of_computer_science/,53,"I am an undergrad studying computer science. While I enjoy coding (it is ludic) I can’t quite grasp its purpose/use in the bigger meaning of life.

Why do you study computer science and how do you find purpose in it?",1641729944.0
rzcexe,Can anyone explain NFTs from computer science perspective without the hype?,198,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/rzcexe/can_anyone_explain_nfts_from_computer_science/,119,"I searched and searched and still haven't been able to really figure out how NFTs are working behind the scenes. There is sooo much hype. 

Could a developer please explain. I'd like to know - 

1. How are they created? Using which popular frameworks (any code snippets appreciated)
2. Where is the media hosted? My understanding is that NFTs are really a record that your wallet owns this unique hash. But then how is that has linked to an image? And where is this image hosted? Follow up - can't anyone take this image host down and then that link between the hash and the image is lost.

Appreciate any elightenment without the hype. I am tired of 100th video about an influencer explaining NFTs as if they are the next gold with no context.",1641682303.0
rzc739,Best Way to Learn Programming theory Summarized,39,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rzc739/best_way_to_learn_programming_theory_summarized/,23,"I am new to coding. I have a math background (up to Diff EQ and LA). I want to learn the basic idea behind all programming languages. The idea of Syntax, how things operate, the different types of programs. Just a review of how programming works.

I know nothing. I need to understand it for my physics major. I learn best by theory and having a foundation of the most basic principles and abstract terms (nothing specific or applied at first).

Do any of you know any good websites or videos that describe the idea of programming? (I would prefer relation to math terms like “function” or “linear” or “transformation” or a set of axioms or system that can be diagramed).

I want to understand the process  and the terms before learning code.",1641681729.0
ryzj9x,Where is database research going these days?,46,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/ryzj9x/where_is_database_research_going_these_days/,10,"Hi!
I'm a data analyst intern who loves math and databases and I'm pretty sure that in the future I want to do something that uses both of these things.
For that reason, I've become very curious about where database research is going these days. I don't have much interest in software development per se but databases are just such mathematical marvels that I can't help but lean towards them.

So, what is currently being researched? User centered design?( i love this and it's very overlooked by companies)
New database paradigms? Graph databases? Etc...?

So what else is there?",1641647438.0
ryeny8,"Does the rise of no code, low code and AI coding tools, like Codex and Copilot, threaten developer jobs?",124,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/ryeny8/does_the_rise_of_no_code_low_code_and_ai_coding/,128,"A career counsellor said that I should teach math (my other possible career goal) rather than go into software development, since the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years. There is so much hype about this that I thought I'd ask the opinions of those here that know what they're talking about.

&#x200B;

Thank you",1641581316.0
rxuuqm,what would be the most efficient way to have a computer guess a number?,13,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/rxuuqm/what_would_be_the_most_efficient_way_to_have_a/,25,"say you picked a really large number at random and wanted to find it by repeated guessing, what would be the most efficient way to do that? 

i've been trying (really just out of curiosity) this and so far the fastest outcomes are usually from just incrementing the number one by one until it gets it but i don't feel like that's the most efficient solution. what would be a better way?",1641518891.0
rxn4al,Why is the castle clock a computer?,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rxn4al/why_is_the_castle_clock_a_computer/,3,"The castle clock of al-Jazari is said to be the first programmable analog computer. Now I get why it’s analog and also what makes it programmable (accounting for the length of day and night), but what makes it a computer? It gives the time, does that make all clocks computers?",1641498290.0
rxjst3,Parametric vs. non-parametric statistics. A 2-minute visual guide. [OC],62,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rxjst3/parametric_vs_nonparametric_statistics_a_2minute/,1,"&#x200B;

https://preview.redd.it/hqkf9w37q3a81.png?width=2048&format=png&auto=webp&s=f11e0afc13e872afd68e6150ed4972853a90c7b8

https://preview.redd.it/t7pbrm37q3a81.png?width=2048&format=png&auto=webp&s=a9895b8c904ce1802376adde7a79bf719c456499

https://preview.redd.it/l4tl8w37q3a81.png?width=2048&format=png&auto=webp&s=4f5c191f19c7b6fdd0a4750c3e6b7d63cfafde8e

https://preview.redd.it/ofi01147q3a81.png?width=2048&format=png&auto=webp&s=daaf40e3fd1a22c2b0cd661944faa3e3f5f7172e

🔵 Parametric and non-parametric statistics 🔵  


🧑‍💻 If you have written code before you have heard of a parameter. It is what a function takes as input to do some computation on and return an output. Similarly, probability distributions have parameters. These define the properties of the distribution.  


🔔 In the case of a normal distribution, the mean and standard deviation are its parameters. The mean controls the position of the distribution while the standard deviation controls the spread or ""peakiness"" of the bell curve.  


⛷️ In the parametric approach, the model consists of a finite set of parameters that characterize it. The big assumption that a parametric model makes is that the model will do well on the task if the underlying distribution from which your data is sampled matches the model. If that is not the case the model and data mismatch will hurt the performance on the task you care about.  


🤸 A non-parametric model does not rely on parametric assumptions and is generally more flexible. It is a good choice if you don't have any prior knowledge about what could be a good model that reflects your data distribution. A histogram is a good example that can model points sampled from an arbitrary distribution.  


🧐 However, there is no free lunch: you will need enough data points to get a good approximation otherwise the histogram will look nothing like the underlying distribution.  


\---------------------------------------------------------------------------------  


If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641489774.0
rx5u9h,What is the maximum # of guesses an reasonable Minesweeper player needs to take?,11,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/rx5u9h/what_is_the_maximum_of_guesses_an_reasonable/,4,"Not homework, just math.

Suppose there is an arrangement of X mines on an M x N grid. A player enters a sequence of clicks, which each ""flood fill"" outwards until adjacent mines are hit. They make logical deductions/inferences whenever possible, and if they need to guess, they will take any guess that is 50-50 or better. They don't have any intuition - i.e. they won't visually analyze the grid to see if one safe click is ""better"" than another.

What is the upper bound for the number of guesses such a player may need to take?",1641444320.0
rx5nj6,Turing Machine Exercises,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rx5nj6/turing_machine_exercises/,2,"**Brief: looking for more ideas on exercises for TM - share please if you had some in your CS course or seen somewhere.**

Always been fascinated by Turing Machine concept, decided to collect some puzzles to be solved with it on my site. However after adding first three ([these](https://www.codeabbey.com/index/task_list/turing)) feel lack of good ideas:

1st and 2nd are about making copy of a line of characters and incrementing binary value. There could be combinations and variations (copying line of different characters and incrementing ternary, octal, decimal value... or count characters in a line making result a binary value) - but after grasping principle these do not make additional fun (unless I'm missing some whimsical approach).

3rd is about implementing [Rule 30](https://mathworld.wolfram.com/Rule30.html) one-dimensional cellular automaton with TM, I'm somewhat glad with it (since I managed to solve it).

Have in mind something like checking bracket balance or wiping out comments in C++ / Python source. However these seem to be more about general sense of state automaton rather specifically TM...

So would be really glad for any suggestions! Perhaps you have come across something in university or played with TM for personal fun etc... thanks in advance!",1641443710.0
rx4z0z,Is there a name for 256 bytes?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rx4z0z/is_there_a_name_for_256_bytes/,7,"Im wondering if there is a name for 256 bytes, like how ~1000 bytes is a kilobyte, or ~1000000 bytes is a gigabyte?",1641441709.0
rx1mtm,Public key encription,32,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/rx1mtm/public_key_encription/,30,"I have been trying to wrap my head around it, but i dont get how the publick and private key are combined.

Like, if i only i know the key, and i use it, the receiver wont be able to decript it, or will it?

And if we both have it, it means it was shared before being aplied, and i sent it to the receiver through simetric enciption, thus defeating the purpose of it existing since everyone with the public key (wich i am assuming is public) can decript the message and find what the private key is?...

What am i missing here? I cant seem to find any explanations that made me understand it, if you know any good explanations plz share!

Edit: The problem is that i thought that the public key was  some preexisting key, when in fact it is generated by me at the same time I generate the private one, I share the public one, te receiver uses it to encrypt stuff, and only the private key can decrypt what was encrypted with the public key, and, it is very hard (close to impossible rn) to get the private key by looking at the public one!",1641429660.0
rwv1vj,"How to aim towards publishing theoretical computer science papers regarding computer/IT systems for space travel, exploration, and survival?",12,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/rwv1vj/how_to_aim_towards_publishing_theoretical/,2,"Hello redditors, I humbly ask for your assistance...

Here's the elaboration: I'm a computer science undergraduate with a relatively decent amount of pre-existing background knowledge in computer systems and some programming, and I have an interest and some supplemental education in astronomy as well as a semi-professional observatory which I use actively. In my future academic career (I intend on pursuing a M.Sc. and Ph.D.) I'd like to focus my research and studies around theoretical systems which could help humans and machines travel, explore, and survive in space. 

The question is... What would actually be the best way to start tackling this in order to one day be worthy and knowledgeable enough to pursue this? There is no subject such as ""astrotechnology"" so I am a bit at a loss. Also, how do ""theoretical"" papers consisting more of science/evidence-backed ideas (rather than experimental results) even get accepted and published by journals???

I would be grateful for any pointers. <3  
P.S. - Not sure if it matters, but I doing a degree in health science alongside computer science. Also - due to where I live and financial constraints - my resources are limited; I have only my mind, my computer, and the internet.",1641411326.0
rwnjr8,What is k-means clustering? A 2-minute visual guide. [OC],51,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rwnjr8/what_is_kmeans_clustering_a_2minute_visual_guide/,1,"&#x200B;

https://preview.redd.it/l885cgy6lv981.png?width=2048&format=png&auto=webp&s=67b8b1f40e1ec3a919810758aa8d606dd8b639f9

https://preview.redd.it/mpr2ufy6lv981.png?width=2048&format=png&auto=webp&s=3b5e1f964c06ab3ff7a8de9f66c94073c093c0d1

https://preview.redd.it/grnekly6lv981.png?width=2048&format=png&auto=webp&s=ce49caa99da92e45c02e4325fe138501d66af9ba

🔵 k-means Clustering 🔵

🔱 The k-means algorithm divides N data points into K disjoint clusters. It is a well-known unsupervised learning model. This means that the k-means algorithm only requires the data points and does not need the corresponding cluster that each point belongs to as this is what the algorithm figures out.

✨ The clusters are found by allocating points in such a way that the total variance of the points within each cluster (intra-cluster variance) is reduced or minimized. Although it iterates quite fast, the k-means algorithm can have varying cluster formations based on the initialization. It has been widely implemented in many software packages. The scikit-learn package for python is the one that I have used most often.

🔁 The most common method to perform the clustering is iterative. It alternates between two steps: (1) assigning each point to a cluster based on the point's closeness (or distance) to the cluster center and (2) updating the cluster center (called mean or centroid; hence the name k-means one for each of the k clusters) based on all the points that belong to this particular cluster. The iterations are usually performed until the centroids stabilize (converge) between consecutive iterations.

🤓 K-means is popular in scenarios where the data is known to consist of multiple groups (distributions) but it is unknown which point belongs to which group (cluster). It can be used for data analysis and splitting data that comes from multiple distributions, image segmentation, color quantization among other things.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641391224.0
rwly7h,Books that Changed My Career as a Software Engineer,145,0.96,computerscience,https://julianogtz.github.io/my-personal-blog/posts/five-books-that-changed-my-career-as-a-software-engineer,4,,1641386187.0
rwgzqx,"Is there a ""synthetic"" Universal Turing Machine? Where can I find it's rule set?",2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rwgzqx/is_there_a_synthetic_universal_turing_machine/,0,"I've been on the lookout for a UTM description, and I've found a machine with only 2 states and 3 symbols. I thought, how neat, it's so simple! I put the description into a TM simulator, realized I had no idea how to use the rule set, and took to the internet again to figure out why. Turns out, it simulates a 1D CA, which I know to be terribly time and space inefficient, and very hard to understand. It makes me wonder if there's a larger, synthetic ruleset, one with more internal states but only 3 symbols (1, 0, and empty symbol), one that can reasonably be experimented with, one that can be programmed more easily?",1641367089.0
rw99z6,What is the proof that a 2-state Turing Machine with a 3-state alphabet can simulate any arbitrary Turing Machine?,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/rw99z6/what_is_the_proof_that_a_2state_turing_machine/,1,"This is a follow-up to my question of how many states and symbols a TM would need to simulate a TM with an arbitrarily large number of states and symbols, that was answered for me, the answer is 2 states and 3 symbols. But I can't find a proof for this, or even better, an example. Also, is there a proof that a 2-state 3-letter TM is the simplest universal TM? Could a 2-state 2-symbol simulate bigger TM's, or could a 1-state 2-symbol TM work?

Edit: I found a [source](https://mathworld.wolfram.com/UniversalTuringMachine.html) for the description of the (2,3) UTM, as well as some larger UTM's. However, the description of the Turing Machine does not prove anything to me, because it's only half the story. I'd also need to know what the tape looks like while it operates to gather anything.

E2: I've now learned that the (2,3) TM is universal because it simulates a specific 1D CA that has already been proven to be universal, however this pushed the question out for me, so I looked up a proof that 1D CA are universal, and I found that it can simulate some other thing that's been proven to be universal, and soon enough the question was put back onto the UTM and why it is universal. Either I'm stupid or the internet is.",1641342841.0
rw311s,Memory and pointers: where does the actual memory address come from?,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/rw311s/memory_and_pointers_where_does_the_actual_memory/,4,"When, say, in C++, we say `void* ptr = &my_var`, we set ptr equal to the memory location of my\_var.  Okay.  But where does this memory location number come from, and why/how do we actually access it?  

Say we have our memory as 5000 bytes of data, in 1D.  My understanding is that each byte in memory has its own address, but I don't understand the mechanism for how.   I understand that it's like a ""house address"" and we need it in order to find a value stored at a given location.  But at the most basic level, where are these addresses coming from? The hardware? Or is this just something we are adding to that byte of memory (sort of like metadata in Numpy arrays)? If that's the case, then does this not take up some of the memory? 

E.g. we have a memory location of 13413241 let's say.  Where is this value stored? I would assume that all the memory locations, all these integers, are inherent to the computer we are working on, and thus always exist.  Is this entirely separate from ""memory"" in terms of how big our program is (if they always exist anyways)?  It just seems like we can pull these memory addresses out of thin air.  If I define say `x = 5`, is it always the case that I have also implicitly defined `some_pointer = &x`, and is this now in my program's ""memory""?   Unless the memory location is not ""bundled"" to that 5, I don't see how the pointer ""knows"" the memory location.  Maybe all of this is happening at the hardware level and I'm too focused on the software level.  I haven't found any articles that explain what I'm looking for, only what a pointer is.  Maybe this is more of a question about what the `*` and `&` operators are actually doing.

It might be evident already, but I almost exclusively use Python.  In Python, we can define a variable, which will always be equal to some object, and then we can set that variable equal to other things, namely we are just telling that variable to now ""point"" at a new object: but the previous object still exists in memory, it just isn't referenced.  However, my understanding is that Python doesn't actually have pointers, so my understanding may be tainted pretty heavily by the Python id() function which returns the location in memory, and also by how variables and objects interact in Python.",1641325885.0
rw2dl6,What's the difference between a Python ID and a pointer in C++?,1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/rw2dl6/whats_the_difference_between_a_python_id_and_a/,3,"My current understanding is that Python does not support pointers because it is against the ""Zen of Python"" but it does have id(my\_obj), which returns ""the object's address in memory.""  How is that not a pointer and what is the difference between the Python id() function and a pointer in C++?  Are both not just an integer representing the object's location in memory?",1641324167.0
rvy218,What is an eigenvector? A 2-minute visual guide. [OC],195,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/rvy218/what_is_an_eigenvector_a_2minute_visual_guide_oc/,19,"&#x200B;

https://preview.redd.it/t23amo984p981.png?width=2048&format=png&auto=webp&s=564718c960ea7754ef20081d0374b58902605668

https://preview.redd.it/mcvd1q984p981.png?width=2048&format=png&auto=webp&s=5ef0fcb88fc8af53cd6ba6d21e1789ee8b6cccec

https://preview.redd.it/2pohyo984p981.png?width=2048&format=png&auto=webp&s=cd07e5c504eb15e5e07642c51e0f20c5401433d1

https://preview.redd.it/fn2yxo984p981.png?width=2048&format=png&auto=webp&s=4a263659688b553b9463d5a6df32fc8a9d7220e1

🔵 Eigenvectors 🔵

🚀 An eigenvector is a special vector associated with a linear transform. It's special in the sense that after applying the said transform it does not change direction but only gets scaled (multiplied by a scalar value) by the eigenvalue.

🔨 Each eigenvector comes with a corresponding scalar called the eigenvalue. Breaking a matrix M into its eigenvalues and corresponding eigenvectors is called eigendecomposition.

🔭 The word ""Eigenvector"" comes from ""eigen"" in German where it means ""its own"". It was originally used to study rigid body motion and in the discovery of principal axes. However, nowadays it has found its way into a wide array of applications from my favorite: principal component analysis, differential equations, and problems in physics and chemistry relating to wave transport and molecular orbitals.

🎭 Another one of the classical applications is the Eigenfaces project for facial recognition. Eigenfaces decompose a face as a composition of face templates (basis) called eigenfaces. Imagine N eigenfaces E\_1, ..., E\_n when given a new face F it can be written as a composition of each of these N eigenfaces for example: F = (10% of E\_1 + 55% of E\_2 + 35% of E\_3). Each eigenface would represent a meaningful feature in the high-dimensional space of faces.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",1641312934.0
rvrtx4,"The frequency that we see in the CPU, is the frequency at which the single bits are read by the CPU?",1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/rvrtx4/the_frequency_that_we_see_in_the_cpu_is_the/,4,,1641294168.0
rvq0w4,Have you ever used what you learned in chemistry classes in your computer science career?,24,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/rvq0w4/have_you_ever_used_what_you_learned_in_chemistry/,31,?,1641286813.0
rvfquw,Why did the Microsoft date bug happen in 2022?,61,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rvfquw/why_did_the_microsoft_date_bug_happen_in_2022/,12,"So Apparently Microsoft was trying to store date in a 32 bit signed integer. 

But the max value of that is 2147483647, so why can’t January 1 2022 be stored?",1641253825.0
rvc4y3,Minecraft to teach software architecture?,50,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rvc4y3/minecraft_to_teach_software_architecture/,17,"Hey guys I just thought about teaching Classes, Inheritence, Dependency Injection etc. (you get my point) with Minecraft. Nearly everyone knows the game and a block serves as a great example for a class. It has specific attributes like texture etc. and there are lots of subclasses. 

Additionally you could discuss how those blocks could be stored, which sort of database would be suitable etc. 

What do you think about this?",1641244232.0
ruwflx,Question about installation software,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/ruwflx/question_about_installation_software/,8,"Bit of background: I am a Computer Science student and a software developer, have been programming for several years now.

I have been wondering about this for a while now, but how come the Installer of a software is sometimes much much lighter than the software itself? As I understand it, the concept of an installer generally is to compress the necessary files for a software, and then distribute the files in the PC + configure whatever settings required to operate the software (add registry records etc.).

But sometimes the weight difference between the installer and the software is really big, like several GBs big, and for games it's sometimes up to a couple dozen GBs even when the installer is an offline one (as in does not have to download extra data in order to finish).

I don't see how these variations can be achieved using conventional compression, especially when the compressed files are not simple textual data.

Am I missing some key point in how installers work? Is it really just simple compression and decompression? Am I just exaggerating the weight differences? (I really don't have a concrete example of an installer that behaves that way at the moment, but if memory serves me right that was the case with many of them)",1641196518.0
ruum67,What Every Programmer Should Know About Memory by Ulrich Drepper[PDF],11,1.0,computerscience,https://akkadia.org/drepper/cpumemory.pdf,5,,1641189641.0
rutayn,How does a quantum computer reduce time complexity?,109,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rutayn/how_does_a_quantum_computer_reduce_time_complexity/,21,"When I studied algorithm, people talked about time complexity and bigO notation. What I want to know is that if given an algorithm whose running time is O(n\^2), if I run the algorithm on a quantum computer, does it necessarily reduce the running time to O(n), that is, the sqare root of n\^2?

And if given an algorithm whose running time is O(n\^3), does a quantum computer reduce it to O(n\^(3/2)), that is, the square root of n\^3 ?",1641185362.0
rushx3,How is memory aligned data good for caching?,13,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rushx3/how_is_memory_aligned_data_good_for_caching/,12,"I read some articles about memory alignment and while I was reading about the advantages of memory alignment I read about caching. It was not very clearly explained that how memory alignment helps in caching. I would be delighted if someone could explain this to me.

This [Stack Overflow answer](https://stackoverflow.com/a/381368/14951775) mentions that memory alignment is helpful for caching. I know that aligned memory makes the fetching of the data faster because the CPU does not need to shift the bits in two reads and then combine the results in the register. I got to know about this from this [IBM Developer article](https://developer.ibm.com/articles/pa-dalign/).

Thanks!",1641182808.0
rue0sq,"What does it mean for the CPU to ""decode a command""? I mean i get the general idea but how exactly it works?",50,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rue0sq/what_does_it_mean_for_the_cpu_to_decode_a_command/,20,"Pretty much title, i know that CPU fetches a unit from memory and then it decodes it, but i dont know what exactly that means or how it works",1641143248.0
rubwjz,How does computers with different systems communicate ?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rubwjz/how_does_computers_with_different_systems/,5,"how can computers with different systems that have different data type representation communicate properly? what's the concept behind it, any links or articles would be appreciated.",1641137406.0
rubju8,Resources for time complexities of common data structures across multiple programming languages,14,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rubju8/resources_for_time_complexities_of_common_data/,11,"In **JS**, array.push() is always O(1) since even if need to 'extend' the current array as the reference to the last item is simply removed.

In **Java**, there's no array.push() and O(n) as need to shift all the elements to a the new array.

Is there any resources available that show different time complexities for different programming languages?   


Happy 2022!",1641136358.0
ru6jx7,I would like to learn how a database is encrypted.,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ru6jx7/i_would_like_to_learn_how_a_database_is_encrypted/,1,"Hello, I understand a database containing passwords should be hashed using some sort of key, but I understand that each entry should be encrypted again in some other way. What I can't understand is how that works. I'd love a few pointers to some resources if anyone knows of any. Thanks :).",1641117296.0
ru5qo2,How many internal states are needed in a Turing Machine?,10,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/ru5qo2/how_many_internal_states_are_needed_in_a_turing/,12,"I haven't personally experimented with a Turing Machine implementation myself - mainly because building my own implementations makes learning about the concept easier and more fun - but I know enough to know that TM's can simulate TM's, even if at a slower and smaller scale, which makes me wonder.


If a Turing program exists that simulates the entirety of a Turing machine inside a Turing machine, how many internal states would it need to run such a program? Would this number vary depending on the size of the tape's alphabet, or the number of states the simulated Turing Machine can be in?

EDIT: I found the answer, though I don't yet understand it. a 2-state, 3-letter Turing Machine can do this. I'm searching the internet for proof that that's true.",1641113742.0
ru3c37,Computing Knowledge Itself?,8,0.65,computerscience,https://www.reddit.com/r/computerscience/comments/ru3c37/computing_knowledge_itself/,18,"So before I start I literally had the idea 5 minutes ago and I hope to explore it more thoroughly with you guys by finding possible problems and possible solutions. So I encourage you to have a think before you comment so we all get the maximum benefit. Let‘s get started!

So knowledge is something that people love. Rightly so! Knowledge helps solve all sorts of problems. In fact, it is a very valuable resource in any given arena. However knowledge also has its own statistical topology. There’s ways in which you can get to the next theory quicker and more fundamentally. So why don’t we try and teach the AI this!?

We define a computable theoretical topology with a small amount of variables. Mathematics itself would be preferable because it is relatively data minimal. Then we define the axioms of our current understanding of mathematics. Then we let the AI invent knew theories and check it against our current ones. Where am I dreaming?",1641103921.0
rtv1bn,Are there other machines exactly as powerful as the Turing Machine?,10,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rtv1bn/are_there_other_machines_exactly_as_powerful_as/,10,"I'm playing around with manually operated mechanical computers (think the abacus, crank calculator, etc.) and I'm wondering if there's other deterministic architectures exactly as powerful as the Turing Machine?

&#x200B;

Edit: I've done some hours of research and discovered an online copy of Stephen Wolfram's ""[A New Kind of Science](https://www.wolframscience.com/nks/)"" that includes some mechanisms that match closely what I was looking for, such as [tag systems](https://www.wolframscience.com/nks/p93--tag-systems/), [register machines](https://www.wolframscience.com/nks/p97--register-machines/), and [substitution systems](https://www.wolframscience.com/nks/p82--substitution-systems/). I don't know if any one of them are Turing complete yet, but something tells me at least some of them are.",1641078155.0
rtu2da,Why is time stored as a signed integer?,70,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/rtu2da/why_is_time_stored_as_a_signed_integer/,18,"Someone was talking about the y2k bug, whrich reminded me of the 2038 bug and I pulled up the Wikipedia page to read up on it again.  I was under the impression that it would flip down to all 0s and move us back to 12/31/69 but I found out I was wrong and that it will just flip the sign when that happens and move us back to 12/13/1901.

But why was time stored as a signed integer anyway?  Was there an expected need to work with negative time?  If it was unsigned wouldn't that buy us a few more decades to procrastinate before needing to fix it?",1641075398.0
rtkdm7,Mind reading algorithms,67,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rtkdm7/mind_reading_algorithms/,25,"Good day everyone

Recovering sz here, on meds and in therapy

Iv been researching the incidences you hear about where, social media,google or the internet seems to read peoples minds. I think its quite common and alarming.

You often hear folk say they don't mention aloud what their thinking let alone have it in their search history or have any record of it, they simply thought it and received an ad or a search suggestion.

My question is this ;

How is this possible? Iv heard its a predictive algorithm but how does it do it exactly? Is it really an algorithm if its right time after time

Iv heard of confirmation bias as well but i don't believe thats what this is


Any help appreciated

Have a good day",1641047052.0
rstlls,Why is RAM called random?,176,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rstlls/why_is_ram_called_random/,33,"Good day!

I've been wondering, what's so random about memory?",1640956234.0
rslk6w,"I know I'm toeing the line between science fiction and reality, but what is the current state of artificial general intelligence? Is there any literature on the subject worth reading?",8,0.7,computerscience,https://www.reddit.com/r/computerscience/comments/rslk6w/i_know_im_toeing_the_line_between_science_fiction/,3,,1640925537.0
rsjxaz,Are there any real-time trackers of Moore's Law?,22,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/rsjxaz/are_there_any_realtime_trackers_of_moores_law/,6,"I'm thinking some sort of website that automatically plots released processors vs their number of transistors or transistors per dollar against time.

It feels obvious enough that either someone's already done it, or it wouldn't work, but all I've been able to find is a [static graph on the Wikipedia page](https://en.wikipedia.org/wiki/Moore%27s_law#/media/File:Moore's_Law_Transistor_Count_1970-2020.png).",1640920271.0
rs9uqq,"This may be a dumb question, but how would one scientifically develop AI?",76,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/rs9uqq/this_may_be_a_dumb_question_but_how_would_one/,16,"I’ve been following a YouTube channel called Two Minute Papers for some time now and it’s awesome stuff. I’m a computer science major currently in college (senior) and I was wondering how I could get started or have some fun learning and training my own algorithm based AIs?

Also, I have done research online with regards to this but I have not found what I’m looking for. My real question is, how do AI “scientists” get their start? Are they all geniuses that are self taught or is there actually a major/education route you can take to become one?",1640892280.0
rs0z7i,What is the BEST way to find which agents are the best with minimum games between them (genetic algorithm),12,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rs0z7i/what_is_the_best_way_to_find_which_agents_are_the/,9,"Hi, I am trying to make a genetic algorithm to play chess. My problem is with finding the fitness value for each model. Currently, I just pair each agent with a random agent and the winner adds to his score while the loser subtracts from his score. But I find it not very effective because while having a generation of 10 I only play 10 games and it's not enough to evaluate which agents are best. Moreover, I can't match every agent with every other agent since it will take fo ages to complete training. 

What do you think is the best way to know which agents are the best with minimum games between them?",1640868325.0
rrvks1,Solving Logic Puzzles Using Computers and Brute Force,19,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rrvks1/solving_logic_puzzles_using_computers_and_brute/,5,"I came across the following logic problem:

&#x200B;

&#x200B;

https://preview.redd.it/hsy6iegqrm881.png?width=1344&format=png&auto=webp&s=583fcf8d3514d80d7abaae1e266f5f704328a990

In this problem, you are required to match the real names of basketball players to their nicknames, and sort the basketball players by their heights. Normally, this problem would require you to manually enumerate different combinations of names-nicknames and names-heights, until there are no contradictions according to the conditions below.

&#x200B;

I was wondering if these kinds of problems can be solved by brute force using programming languages such as R.

&#x200B;

For example, the code below lists every possible combination of basketball players by height:

    
    my_list = c(""Bill"", ""Ernie"", ""Oscar"", ""Sammy"", ""Tony"")
    
    d = permn(my_list)
    
    all_combinations  = as.data.frame(matrix(unlist(d), ncol = 120)) |>
      setNames(paste0(""col"", 1:120))
    
    
    data_frame_version = data.frame(matrix(unlist(d), ncol = length(d))
    
    matrix_version = matrix(unlist(d), ncol = length(d)) 
    
    #first 20 rows of matrix version:
    
         [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]    [,10]   [,11]   [,12]   [,13]   [,14]   [,15]   [,16]   [,17]   [,18]   [,19]  
    [1,] ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Tony""  ""Tony""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Bill""  ""Tony""  ""Tony""  ""Sammy"" ""Sammy"" ""Sammy""
    [2,] ""Ernie"" ""Ernie"" ""Ernie"" ""Tony""  ""Bill""  ""Bill""  ""Tony""  ""Ernie"" ""Ernie"" ""Ernie"" ""Sammy"" ""Sammy"" ""Sammy"" ""Tony""  ""Bill""  ""Sammy"" ""Tony""  ""Bill""  ""Bill"" 
    [3,] ""Oscar"" ""Oscar"" ""Tony""  ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Tony""  ""Sammy"" ""Sammy"" ""Ernie"" ""Ernie"" ""Tony""  ""Sammy"" ""Sammy"" ""Bill""  ""Bill""  ""Tony""  ""Ernie""
    [4,] ""Sammy"" ""Tony""  ""Oscar"" ""Oscar"" ""Oscar"" ""Sammy"" ""Sammy"" ""Sammy"" ""Tony""  ""Oscar"" ""Oscar"" ""Tony""  ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Ernie"" ""Tony"" 
    [5,] ""Tony""  ""Sammy"" ""Sammy"" ""Sammy"" ""Sammy"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Tony""  ""Tony""  ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar"" ""Oscar""

And this code below records every possible combination of name-nickname:

    
    list.a <- as.list(c(""Bill"", ""Ernie"", ""Oscar"", ""Sammy"", ""Tony""))
    
    list.b <- as.list(c(""Slats"", ""Stretch"", ""Tiny"", ""Tower"", ""Tree""))
    
    result.df <- expand.grid(list.a, list.b)
    result.list <- lapply(apply(result.df, 1, identity), unlist)
    result.list <- result.list[order(sapply(result.list, head, 1))]
    
     head(result.list)
    [[1]]
       Var1    Var2 
     ""Bill"" ""Slats"" 
    
    [[2]]
         Var1      Var2 
       ""Bill"" ""Stretch"" 
    
    [[3]]
      Var1   Var2 
    ""Bill"" ""Tiny"" 
    
    [[4]]
       Var1    Var2 
     ""Bill"" ""Tower"" 
    
    [[5]]
      Var1   Var2 
    ""Bill"" ""Tree"" 
    
    [[6]]
       Var1    Var2 
    ""Ernie"" ""Slats"" 

The way I see it, the two objects (""matrix\_version"" and ""result.list"") should contain the right answer to this logic puzzle - I just don't know how to extract the correct combination from these two objects such that the logical conditions are respected.

Can someone please show me how to do this?

Thanks!",1640848635.0
rrsvuo,"CPU instructions, OS and applications",19,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rrsvuo/cpu_instructions_os_and_applications/,5,"Hello,

I have a hard time picturing how the CPU instructions, the operating system, and applications work together.

What I mean by that is the following, we have an application that is compiled, now it's converted to CPU instruction (I guess ??), but then this app is using system calls to interact with some I/O devices, how does this system calls go through the OS? And if it's just an instruction that invokes an OS function that makes that system call, what prevents the application from calling the I/O directly or ""hacking"" that function by pretending it's another process ( as an example a process reading data from a socket owned by another process).

Thanks",1640839846.0
rr75ns,It would be really interesting to research nature's sorting algorithms to see if there's one better than the ones we've found so far. Does anyone know of any research like that? Also I guess this is Crab insertion sort haha,628,0.96,computerscience,https://i.redd.it/gdzbgny17h881.jpg,28,,1640781086.0
rqysm5,How do we know when real “AI” is achieved?,0,0.45,computerscience,https://www.reddit.com/r/computerscience/comments/rqysm5/how_do_we_know_when_real_ai_is_achieved/,8,"I see all kinds of marketing about machine learning and AI but I don’t think any of it passes the Turing test. Are there other tests that are used to determine when artificial intelligence is achieved? Maybe I am missing something but I don’t think AI exists yet, thought?",1640751083.0
rqqd61,Question about storing variables in memory in programming languages,1,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/rqqd61/question_about_storing_variables_in_memory_in/,3,"Lets say i declare int x and x's adress is 1 for example. Since x is an integer it will take adesses 1-4. Then lets say i declare a float and its adresses are 5-8 

How does the computer know that the data type of adress 1-4 is int and 5-8 is float? Does it also have another memory address that points to those 4 memory adresses that says that those are integer and float types? 

I know ints and floats are represented differently in binary but in the end its all 32 bits so the computer could read it however it pleases. 32 bits can be read as an integer or as a completely different float. Thats why i dont know if the computer leaves another address just to say what data types are on certain adresses or does it work differently?",1640727368.0
rqeudi,What are your resources to understand theory behind programs?,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/rqeudi/what_are_your_resources_to_understand_theory/,4,"Hello! I am somewhat new to programming. I am wondering what you're recommendations are for resources (books/YouTube videos/websites etc.) that explain theoretical concepts behind programming and programs. One example of what I mean by ""theoretical concepts"" would be explaining what data structures are, from their smallest components to abstract ideas. 

The goal is to understand what exactly is happening when programming, not just learning which statements and syntax to use for any particular language. 

Thanks a lot!",1640694726.0
rq60d4,Rules of Programming,168,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rq60d4/rules_of_programming/,45,"Rob Pike's 5 Rules of Programming

&#x200B;

\#Rule 1.

\*You can't tell where a program is going to spend its time.

Bottlenecks occur in surprising places, so don't try to second guess and

put in a speed hack until you've proven that's where the bottleneck is.\*

&#x200B;

\#Rule 2.

\*Measure. Don't tune for speed until you've measured, and even

then don't unless one part of the code overwhelms the rest.\*

&#x200B;

\#Rule 3.

\*Fancy algorithms are slow when n is small, and n is

usually small. Fancy algorithms have big constants. Until you know

that n is frequently going to be big, don't get fancy. (Even if n

does get big, use Rule 2 first.)\*

&#x200B;

\#Rule 4.

\*Fancy algorithms are buggier than simple ones, and

they're much harder to implement. Use simple algorithms as

well as simple data structures.\*

&#x200B;

\#Rule 5.

\*Data dominates. If you've chosen the right data

structures and organized things well, the algorithms will

almost always be self-evident. Data structures, not

algorithms, are central to programming.\*

&#x200B;

\*Pike's rules 1 and 2 restate Tony Hoare's famous maxim

""Premature optimization is the root of all evil."" Ken

Thompson rephrased Pike's rules 3 and 4 as ""When in doubt,

use brute force."". Rules 3 and 4 are instances of the

design philosophy KISS. Rule 5 was previously stated by

Fred Brooks in The Mythical Man-Month. Rule 5 is often

shortened to ""write stupid code that uses smart objects"".\*",1640663682.0
rpr7gj,Book recommendations on algorithms related to chess programming?,55,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rpr7gj/book_recommendations_on_algorithms_related_to/,14,I’ve recently taken an interest in chess engines.  I’m looking for seminal books that discuss the algorithms used in modern chess programming. There are lots of algorithm books out there. Recommendations?,1640622649.0
rp30oe,Where does one find out what the latest tech is?,64,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rp30oe/where_does_one_find_out_what_the_latest_tech_is/,11,"Hello everyone,

We all know how Computer Science is such a fast moving and evolving field, but where do you even find out what the next big thing is in the field? Youtube/TikTok tech influencers? Blogs? News? 

And say you find out that X, Y and Z are three new things in Computer Science. Does this mean we should learn all of X, Y and Z if they relate to our field of interest?

Thank you!",1640544158.0
rp1tzo,How does the c++ std rotate algorithm impl work?,21,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rp1tzo/how_does_the_c_std_rotate_algorithm_impl_work/,7,"Impl in referring to is here:
http://www.cplusplus.com/reference/algorithm/rotate/

I can't find any intuitive explanation online that motivates or proves the correctness of this algorithm.  It doesn't appear to be the ""clever"" triple reverse method for implementing rotate with O(1) extra space.",1640540786.0
rop984,K+1 databases more secure than K?,14,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rop984/k1_databases_more_secure_than_k/,10,"I'm just now, starting to learn about security as a full stack engineer.

In terms of security, does it make sense to separate the database for user authentication w/ the db that stores user data? My approach for a project I'm doing is to salt and hash raw passwords (unique salt per password) server-side, and compare that hash output with the stored pre-computed hash output, to do user authentication.

What happens after? I feel like if you can protect the integrity of user->pass and therefore user->userData, it doesn't matter whether userAuth db and userData db are separate or not, b/c it would be a pain for a hacker to determine data ownership (assuming data does not hold identifying info). Is there a principle I can follow about separating data/processes into different databases?

Also, are there any resources one would recommend for application-focused security practices? I don't want to be the security guy, I just want to learn some basic high-level understanding.",1640492946.0
rolb3t,Patterns in Prime Factorisations (and an alternative algorithm for prime factor generation),11,0.79,computerscience,/r/math/comments/rok78o/patterns_in_prime_factorisations_and_an/,4,,1640479238.0
ro43co,NeurIPS author qualification,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/ro43co/neurips_author_qualification/,0,"Who can submit papers to NeurIPS? 

Are there any specific requirements or qualifications for authors?

I looked in their website and couldn't find an answer.

Thanks!",1640413417.0
rnyhfs,What happens if a component doesn’t do its job before the successive clock tick?,56,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rnyhfs/what_happens_if_a_component_doesnt_do_its_job/,11,I’m new to this stuff so maybe my question is not phrased correctly...,1640392031.0
rnvod9,Was the Apple I computer an example of SoC?,14,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rnvod9/was_the_apple_i_computer_an_example_of_soc/,3,,1640382608.0
rnuf9s,Book Recommendation For a Student,15,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rnuf9s/book_recommendation_for_a_student/,7,"Hi everyone,

I want to read some books related to mathematics and computer science. I am not looking for a book to learn a topic I don't understand; I am just trying to find a book about computer science and/or maths to read to have fun with and to learn something new.

To clarify, I will give an example, although the math is more basic than what I am looking for: The book of numbers by John Conway

Thanks in advance",1640378438.0
rnt4bg,Introduction to Hash Tables,7,0.77,computerscience,https://bytethisstore.com/articles/pg/implement-hash-table,0,,1640374275.0
rnt040,Operating systems,29,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rnt040/operating_systems/,18,"Hello,
Is there any latest research going on in OS? 
I find OS specifically very interesting and wish to do research on it.",1640373904.0
rnmo0w,Help me understand assembler/assembly language,42,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rnmo0w/help_me_understand_assemblerassembly_language/,10,"So, I know that a compiler takes the code that you write and processes it into machine code. But what's the role of assembler/assembly language here?",1640353595.0
rne5t2,what is the big O for this simple problem?,69,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rne5t2/what_is_the_big_o_for_this_simple_problem/,86,"public static int recursiveFunction (int n ) {  
   if (n==100) return 606;

   if (n == 99) return 600;

   return recursiveFunction(n+2) -12; 

}

&#x200B;

it seems in the worst case scenario this runs forever so I thought it is O(infinity) but my Prof put O(n) in the exam as the answer. can someone tell me if my answer is acceptable?",1640320743.0
rnc33s,What is your best critique of decentralized public blockchain technology and why it won't play a major role in the future of society?,7,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/rnc33s/what_is_your_best_critique_of_decentralized/,18,"I'm doing a personal project where I will steel man the arguments for AND against a decentralized blockchain led future, and then write a formal dialogue between the two perspectives with citations.

I've done a fair bit of research thus far and have found it much easier to find pro-blockchain future articles/resources (this is not surprising to me), but am still looking for more of both. However, I would be really happy with some solid bear arguments because the ones I've found aren't especially convincing. They do more to explain the limitations of blockchains and call out the flaws in maximalist thinking.

Can anyone link some really interesting/compelling content for either side? I would really appreciate it, and will absolutely share my work here when it's done!",1640313890.0
rmtdj4,"A fascinating read about ELF ..take a peek , if it's interesting to you or importantly, have time!",74,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/rmtdj4/a_fascinating_read_about_elf_take_a_peek_if_its/,5," Part One: [https://kestrelcomputer.github.io/kestrel/2018/01/29/on-elf](https://kestrelcomputer.github.io/kestrel/2018/01/29/on-elf) 

Part Two: [https://kestrelcomputer.github.io/kestrel/2018/02/01/on-elf-2](https://kestrelcomputer.github.io/kestrel/2018/02/01/on-elf-2)",1640257630.0
rmewjn,Web pages indexing and ethics for search engine results ?,16,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/rmewjn/web_pages_indexing_and_ethics_for_search_engine/,1,"Recently, I've started looking into search engines and some algorithms that get used for crawling and ranking the pages on certain factors.

I wanted to understand different perspectives on ideal ethics we should adopt while creating something similar of our own. I very much know that the combination of keywords generate different results but my question is how to go about some of these keywords which can be borderline non acceptable. For example, someone recently pointed this out to me. The word voyeur is not shown in search recommendation although some movie with same name recently came out and is available on Prime. So this got me curious and i searched for the actors in the movie with that movie's keywords and results were not according to my expectation to say the least. (search for Sydney Sweeney voyeur and go to images/videos tab and you'll understand my point)

This also leads to another question regarding the most popular search engines that we use everyday...so suppose some particular word is not ""acceptable"" according to societal norms but gets very popular due to certain pop culture catalyst. How does these search engines work around that to handle these cases ?  [Because any 12 year old who uses Amazon prime will see the movie Voyeurs there on the home page and will go to Google to search for actor etc and my good it can be daunting]

P.S. I hope i haven't derailed too much in the description but the title should give away the gist. And sorry for the typos, typing from mobile.",1640207457.0
rma55n,Question about ALU Design,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/rma55n/question_about_alu_design/,10,"Hi , I am a bit confused about the ways in which an ALU is designed , So and ALU has mainly 2 operations: Arithmetic And Logic , The Arithmetic part is mostly achieved through Adders but what about the logic part?
I saw an online article and they used AND , NOR , XOR gates , but why those gates specifically? , Wouldn't it make more sense to use AND , OR , NOT gates as they are the basic logical gates? I am extremely new to this stuff , Am i missing out on something?",1640194155.0
rm2a30,How is long distance network communication done?,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rm2a30/how_is_long_distance_network_communication_done/,3," I just started learning networking and I was playing with tracert.

One  of the outputs got me wondering. Im in Europe and trying to access New  York server. 

&#x200B;

    Tracing route to 104.21.24.154 over a maximum of 30 hops
    
      1     4 ms     1 ms     3 ms  192.168.0.1
    ...
      5     9 ms     5 ms     6 ms  xe-8-1-3-3180.bar1.Sofia1.Level3.net [212.162.46.177]
      6     *        *       33 ms  ae-2-3216.ear4.Frankfurt1.Level3.net [4.69.210.66]
      7    41 ms    33 ms    36 ms  195.122.183.210
      8    33 ms    30 ms    38 ms  104.21.24.154

 

The 7-th address is in  Germany  and  the 8-th (destination) is in New York. So how is the communication  between the two routers performed?

Are  there routers between the last two points that tracert cannot find  information for or a satellite communication or a really long cable?",1640168240.0
rm1nxm,is there a book which takes me through the process of how computers and/or programming languages were first invented?,104,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/rm1nxm/is_there_a_book_which_takes_me_through_the/,22,"

id love a book thats like a history lesson but also showing the technical details that the founders struggled with and eventually solved. basically a book where, if i was teleported in time and space to the lunch room of the inventors i would understand the conversation they have about what theyre trying to figure out. 

I know it sounds a bit vague since its a very big topic but im interested in both the hardware and the software, like anything from 1930s to 1970s.",1640165632.0
rls9mc,Can nodes in balanced binary search trees contain parent node field?,26,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/rls9mc/can_nodes_in_balanced_binary_search_trees_contain/,9," The background of this question comes from me the fact that in Skiena's book ""The Algorithm Design Manual: Second Edition"" page 85:

https://preview.redd.it/sc6mewa9lz681.png?width=961&format=png&auto=webp&s=893bdc9adb5bd9b79ce1f17cf2fdcb0c63741169

He claims that the delete-minimum operation for the priority queue when implemented with a balanced search tree will have O(log n) time complexity. But, I disagree with that statement IF the nodes in a balanced search tree have a parent node field. Because since we have a pointer to the minimum it takes constant time pointer manipulation to unlink the minimum node and since the new minimum node will be the parent node, setting the pointer to the new minimum also takes constant time. Unless my assumptions on how a balanced search tree is misguided (I assume a balanced search tree is functionally equivalent to a binary tree, except its self-balancing process), then the delete-minimum operation should have O(1) time complexity. Where have I gone wrong?",1640132590.0
rlgtu5,Concept of Delayed abstraction,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rlgtu5/concept_of_delayed_abstraction/,0,"Hi,

I am trying to understand the concept of delayed abstraction from the following paper at:

[VerX](https://files.sri.inf.ethz.ch/website/papers/sp20-verx.pdf)

     To verify a Past LTL specification of a bundle of contracts C
    we apply abstract interpretation over a symbolic domain.
    We employ predicate abstraction [35] but without the usual
    conversion to boolean programs. Our approach is similar to
    that of Flanagan and Qadeer [32] where two transformers are
    alternated: precise symbolic transformers to handle individual
    commands, and an imprecise transformer to ensure convergence.
    In contrast, classic abstraction applies an imprecise
    transformer at every step. Hence, we call the precise/imprecise
    approach delayed abstraction.

Somebody please guide me.

Zulfi.",1640100001.0
rlfsh9,How does data structure alignment help in efficient reading of memory?,50,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rlfsh9/how_does_data_structure_alignment_help_in/,9,"I know that data structure alignment is done to prevent a data structure being stored across two memory blocks. This was explained by u/AuntieSauce in another post I created regarding [the internal working of structs](https://www.reddit.com/r/computerscience/comments/rj8mre/how_do_structs_work_internally/).
> Now, why does the second rule ensure we don’t have one element of a struct stretched across two blocks? Here is an example: lets say we have a struct that an 8 byte element and a 1 byte element:
> 
> | 8 bytes | 1 byte
> 
> Now, lets say we want to make an array of structs. Each struct is 8 bytes in size, and arrays are stored contiguously in memory, so we have
>
> 8+1 bytes | 8+1 bytes | 8+1 bytes | etc…
> 
> If our array is large enough we will eventually approach the end of a block. Will a block size ever be divisible by 9? Like, almost definitely not. So you’ll get the end of the block with less than 9 bytes of space left, and the struct will get chopped!

In the Wikipedia article about [data structure alignment](https://en.wikipedia.org/wiki/Data_structure_alignment), it is written that CPU can efficiently read memory when it is data structure aligned.
> The CPU in modern computer hardware performs reads and writes to memory most efficiently when the data is naturally aligned, which generally means that the data's memory address is a multiple of the data size

So my question is that how does data structure alignment actually help in the efficient memory reads by the CPU.

Thanks!",1640096846.0
rkr2j4,Andrew Tannenbaum,67,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/rkr2j4/andrew_tannenbaum/,26,"Hello, I have gone through Tanenbaum's books on OS. They are very informative and interesting. So much that I googled about the author and his. In his wiki I came to know that he has got degrees in physics and astrophysics from MIT and Berkeley. 
I'm curious as to how he got the expertise to write books and do research on OS? He has other books also like Structured Computer Organization, computer networks etc. It seems like he has studied CS in his physics courses. I dig into his career but couldn't find any info related to when and where he learned CS. 

Kindly address me the missing link if anyone knows, as his wiki says he's a established physicist while his books and his career speaks he's a computer scientist.",1640017594.0
rkr18d,Looking for textbook recommendations for algorithm analysis/proofs?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rkr18d/looking_for_textbook_recommendations_for/,3,"Any recommendations? Looking for a book that will go through the analysis of algorithms, giving proofs by induction/contradiction etc. 

Is there well known go-to resource for what I'm looking for? Or any suggestions that you just found to be great in general?

Thanks in advance.",1640017494.0
rkfmao,I'm making a web app and I'm hashing the passwords with a salted bcrypt after that I hash the hashed value with md5 then store it in my DB. Do you guys think this is secure or do you have any recommendations?,2,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rkfmao/im_making_a_web_app_and_im_hashing_the_passwords/,23,"My thought is that even if the DB leaked and even if they got the salt by hacking into the server they would still have to deal with the md5 they wouldn't be able to decode it without knowing the original value is because first, they would have the guess the md5 hashed value that bcrypt generated then decode it with the salt.

What do you guys think? I hope that hacking won't be an issue though as my production environment will be locked down pretty well.",1639978028.0
rkf6jh,I really want to design a single board computer. I have a lot to learn. Advice of specific resources?,25,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rkf6jh/i_really_want_to_design_a_single_board_computer_i/,17,"I am aware this is not an easy task but I don’t care. I want to do it. I am an electronic engineer and have experience with electricity, testing and troubleshooting pcbs. I have never designed anything besides simple circuits with lights and switches.. lol. 

Currently studying software development for a bachelors and I’m comptia a+ certified.

To be able to design an SBC I definitely need to improve in designing pcbs. I need to improve my understanding of computers so I can have a better comprehension of how some of the components on an sbc will go together.

I feel a bit overwhelmed to be honest and wanted to consult you guys so I can define a learning path for me so I can actually start considering designs.

If you have ideas or specific resources that’d be helpful for me and my intentions I’d appreciate it.",1639976497.0
rk78ya,Reentrancy Detection Technique by Zeus tool,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rk78ya/reentrancy_detection_technique_by_zeus_tool/,1,"  

I was reading the detection of same function reentrancy by Zeus as discussed in \[[Zeus: reentrancy detection](http://pages.cpsc.ucalgary.ca/~joel.reardon/blockchain/readings/ndss2018_09-1_Kalra_paper.pdf)\]. They have done cloning. I can’t understand this method. Is this a valid method of testing a SC because we are modifying the SC. Can we modify the SC during testing?

The paper says:

    Reentrancy in Solidity can happen via the call method. send only invokes the default function with limited gas for logging purposes. ZEUS handles same-function reentrancy by first cloning the function under consideration, and inserting a call to the clone before the invocation to call. Fig. 14 shows the patched function for the example in Fig. 2. Note that ZEUS ensures that the patch is done within the same basic block so as to ensure that if the cloned function is called, then the invocation to call is also made. Further, we also assert false before the call code. If the verifier finds a path leading to this assert, it indicates a bug.

Somebody please help me to understand .

Zulfi.",1639952159.0
rk5sms,"Is there a ""perfect"" random event?",40,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rk5sms/is_there_a_perfect_random_event/,18,"I dont mean a coin toss or such thing, as its technically determined as soon as you flip the coin ( gravity, wind etc.), but rather a totally unpredictable event like a programmed coin toss, but I'm not sure wether its actually totally random. (sorry for bad my bad english)",1639948067.0
rjwxfl,Why does SSH use symmetric encryption,22,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rjwxfl/why_does_ssh_use_symmetric_encryption/,5,"Hello everyone! 
I've been reading up on SSH and am trying to figure out why SSH uses symmetric encryption for communication when asymmetric encryption is used for authentication and key exchange.
The only reason I can think of is symmetric cryptography allows for fast encryption and decryption. Is there any other reason?",1639921080.0
rjvbrs,Looking for programming challenges/projects related to randomization/approximation algorithms.,62,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rjvbrs/looking_for_programming_challengesprojects/,17,"Hello, I have been reading on randomized/approximative algorithms books, and while Math is aesthetically pleasing, I wish to get my hands dirty with practical coding challenges or projects.

Any recommendations?",1639915018.0
rj8mre,How do structs work internally?,66,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rj8mre/how_do_structs_work_internally/,35,"How do structs work internally in memory. I know that an instance of a struct is a pointer to the first field of the struct. I also know that all the fields of a struct are contiguous to each other in memory so the memory address of the second field of a struct can be accessed by adding the size of the first field to the memory address address of the first field.

I am failing to understand that how do we access the consequent fields of a struct with just the memory address of the first field. We can do it in arrays by jumping x bits ahead according to the data type of the array, we can only do this in arrays because the values in a certain array have the same data type. My question is that how do we navigate through the fields of a struct by only knowing the memory address of the first field of the struct.

Thanks!",1639837811.0
rj7x0t,Can someone explain how I would calculate the Big-Theta notation for this bit of code please? (Python),6,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/rj7x0t/can_someone_explain_how_i_would_calculate_the/,7,"    specifiedItem = 'item6'
    lists = [['item1', 'item2'],
                  ['item3', 'item4'],
                  ['item5', 'item6']]
    
    wasItemFound = False
    
    for list in lists:
            for item in list:
                if item == specifiedItem:
                    freeFromIngredient = True
                    break

Please don't just give me the answer, if you could explain how I would calculate it that would be great! Thanks :)",1639835496.0
riwlmc,Brian.W.Kernighan will give a keynote at linux.conf.au 2022,46,0.95,computerscience,https://twitter.com/linuxconfau/status/1472020904687198208?s=20,1,,1639792616.0
risxvd,Uptime difference between servers and computers?,23,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/risxvd/uptime_difference_between_servers_and_computers/,13,"I was curious about the differences in how we treat servers and computers. So for example, if my computer is on for several weeks, it starts to get slow and weird things start happening until I eventually shut down or restart and it comes back up peforming normally. Why is it that servers can stay up for months or even years and not experience the same flakeyness as regular computers. I imagine it has some to do with the OS and some to do with the hardware, but wanted to see if anyone could enlighten me. Thank you.",1639781308.0
rip3jw,Question: Alan Turing’s approach to decidability problem,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rip3jw/question_alan_turings_approach_to_decidability/,7,"A question: Alan Turing’s approach to Decidability problem(Can we know beforehand if certain numbers and theorems are calculable and provable) was that if there existed a decidable program D that takes another program as input and decides if it will finish, We can encode the program itself and there should exist a decidability program that decides on the program D. So is Decidability Decidable. Well now my question is can’t the answer be yes. That would make an infinite chain of decidability programs but that doesn’t mean it is logically incoherent as suggested by the analogy used that this problem reduces to logical problem that arise from:“this statement is false”.Why is the answer No?",1639769913.0
rinkr5,How far can a one terabyte file be compressed?,27,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/rinkr5/how_far_can_a_one_terabyte_file_be_compressed/,36,"Does anyone know how far a one terabyte file can be compressed? What’s the limit of today’s technology compared to 2000 and 2010? Regarding the compression of a file. 

If one terabyte holds 1,000,000,000,000 bytes, what is the utmost limit of compression?

If data loss will occur, tell me the limit for both. With and without data loss.

Edit: Let’s say the data is an entire computer full of word files, photos, and videos. I know it’s basically impossible to state an exact amount of word files, photos, and videos, however, I’m stating an example. One terabyte of your entire computer. Going off the assumption that your computer is exactly one terabyte of data.

Edit 2: If someone has an exact example, let me know. For example, your own computer. How much would you be capable of compressing? Let me know the beginning size and then the compressed size.",1639765489.0
rihkz0,Software Architecture Readings,30,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rihkz0/software_architecture_readings/,7,"Hey all, I'm looking for some software architecture design books or websites that can help me devlop and understand large and scalable projects. It's difficult trying to find readings that aren't aimed at newbies. Really just looking to advance my skills. Java is my primary language if it matters.",1639748149.0
ri9bkh,Advent Of Code: My Best Hobby For 2021,4,0.68,computerscience,https://theabbie.github.io/blog/advent-of-code-2021,0,,1639716383.0
ri6duj,Question about the time complexity of a specific algorithm,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ri6duj/question_about_the_time_complexity_of_a_specific/,3,"I was watching a youtube video about coding interviews and after looking in the comments, someone was claiming that [this](https://gyazo.com/fa08cd2babb00bead656e52bf93d0a0c) algorithm had a time complexity of O(n) where n is the size of the array.

I have more of a technical background rather than a CS one so this is my thinking when approaching this:

Because he's taking each integer in the array and brute force testing it against all other integers in that array and then moving onto the next integer if there is no match, the algorithm will take exponentially more time as the array size increases and therefore has a time complexity of O(n^2).

For example, an array with 2 integers will have 4 (2^2) total pairs: 00, 01, 10, 11,

While an array with 4 integers will have 16 (4^2) pairs: 0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111

So which one is it? O(n) or O(n^2)?",1639706958.0
ri2yda,Question about a possible attack on WEP,41,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/ri2yda/question_about_a_possible_attack_on_wep/,10,"Hello everyone, I'm learning about the Wired Equivalent Privacy (WEP) protocol. I thought about a possible attack and wanted to know if it is possible:

An adversary intercepts a packet/frame that has the initialisation vector (in plaintext) and the ciphertext. They then replace the ciphertext with some other data, and send it to the intended recipient.

Is it possible to perform this attack without the recipient detecting that this has happened and without finding the value of the secret shared key? 

Thanks in advance!",1639696553.0
rhnc1k,[Discrete Maths] Surjective functions,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/rhnc1k/discrete_maths_surjective_functions/,3,"So if asked to prove that f(x) = y is surjective, we check if all the elements in the co-domain(y) have an element in the domain(x), so does this mean that we can have an element in x that doesn't map onto any value in y. i.e
for the domain {x1, x2, x3} and the codomain {y1, y2}

f(x1) > y1
f(x2) > y2
f(x3) > blank

Is this Surjective? Since I've heard that when mapping, all elements in x are used, but when we check if a function is surjective we only use elements in y to check for x, so there might be some elements in x that don't map to y",1639647549.0
rhll1c,"If a text message held 64 characters, would that equal 64 bytes?",20,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/rhll1c/if_a_text_message_held_64_characters_would_that/,28,"I’m not sure if this is the right place to ask, however, I’m gonna ask anyways. I’m pretty sure one byte equals eight bits. If that’s correct, am I correct in assuming that one byte equals one character? Are all characters the same amount of bytes? Like, numbers and letters. Example being; 7 compared to H. They’d both equal one byte? Separately, of course. Not together. 

Also, is a space considered a character byte? 

Lastly, is there a difference between a email message versus a text message? Pertaining to byte size per character. 

If this isn’t the right place for this question, could someone point me to the correct area? If this is the right area, mind answering these questions?",1639639943.0
rhhyf3,I have questions about the physical nature of files,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rhhyf3/i_have_questions_about_the_physical_nature_of/,6,"Will downloading files wear on a computer? 

The processes of downloading files, storing information, sending information, etc.

Does the process destroy wires, chips the memory drive, etc?

Does a file physically imprint itself in such a way that it can never be completely erased?

Is there a field of study that relates to my specific interest?

Thank you",1639627270.0
rh7r2v,Bresenham's Line Drawing Algorithm is... confusing?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/rh7r2v/bresenhams_line_drawing_algorithm_is_confusing/,1,"Forgive me if this is the wrong sub for this, but I'm kinda getting desperate at this point.

Can anyone explain in a dumbed down manner how the following lines from [Bresenham's Line Drawing Algorithm](https://medium.com/geekculture/bresenhams-line-drawing-algorithm-2e0e953901b3) make sense? Essentially, this is used to calculate where the next pixel should be drawn when attempting to draw a line between two points in a 2D space:

>We use dx and dy values to calculate the initial decision variable (d). value of the decision variable is changed on each step.  
>  
>d=2dy-dx  
>  
>Similarly, we need to calculate △E and △NE values. After the first initiation, these values are not changed.  
>  
>...

Like, where does the multiplication come from and why is it arbitrarily on dy and not dx? Not only that, most of this method seems to only account for positive increases, and although it does mention them, it doesn't give a proper solution to other cases.

Also, a ""slope error"" doesn't seem to be something google wants to tell me about. So, what's the deal with that?",1639598063.0
rh4amb,"Does the programming language type system spectrum (such as below) exist anywhere in academia? I'm writing my dissertation and would really like to include it somewhere, if anyone knows an academic paper it's in that would be incredible, been searching for hours!",280,0.92,computerscience,https://i.imgur.com/DxhVCU5.png,45,,1639589143.0
rgyayz,System Design Interview – An Insider's Guide | Any difference between the book and the online course,4,1.0,computerscience,/r/csMajors/comments/rguyh0/system_design_interview_an_insiders_guide_any/,0,,1639572303.0
rgnbmf,I wrote code the stupid way,142,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rgnbmf/i_wrote_code_the_stupid_way/,33,"I've been working as a software developer for 6 months and during this time I had to write a certain query and did it in the absolute stupidest and unnecessarily complex way that I thought was genius and was very proud of.
Meanwhile a coworker had to solve the same problem and I came across his solution. I was embarrassed by it's simplicity and now I feel like a moron.
Has anybody had similar experiences?",1639533028.0
rg53gw,Is there a website or atleast a document that lists common optimizations done by compilers or interpreters?,65,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rg53gw/is_there_a_website_or_atleast_a_document_that/,5,When watching tutorials I often notice them sometimes saying one way is better due to compiler optimizations. I know they treat it as a black box for time reasons but it would be nice to have a compilation that lists what these optimizations are and when the compiler does them. Even without looking at the assembly or atleast an explained version of it.,1639479581.0
rfovp2,What's the best way to encode SHA-2 Hash?,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/rfovp2/whats_the_best_way_to_encode_sha2_hash/,3,,1639426374.0
rfmupc,How should I introduce a kid to programming?,136,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/rfmupc/how_should_i_introduce_a_kid_to_programming/,74,"Hello everybody! 

I am a 4th year Computer Science student. My fiancé’s 10 year old sister has expressed some interest to me about the work I do and said she wants to learn how to code. 

Does anybody have any resources or advice on how to introduce kids at this age to programming? She’s interested in games like Roblox. I just feel like the approach I took to start learning might not be appropriate since I started as an adult.",1639421131.0
rfa9t6,Will we ever reach a point were no more malware can be created.,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/rfa9t6/will_we_ever_reach_a_point_were_no_more_malware/,12,"Will there ever be a point were every possible malware type will be detected by antivirus software, making malware nonexistent?",1639381039.0
rf81l6,Viruses- mutation?,22,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/rf81l6/viruses_mutation/,14,So seeing as various types of colds and viruses can autonomously mutate- is it possible for a computer virus to mutate by itself?,1639372760.0
rf03ai,Understanding NP Completeness,53,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/rf03ai/understanding_np_completeness/,15,"Can you share a good website, book or other resources where the ideas related to np complete and np hard complexity classes are explained intuitively?

I read Cormen and Wikipedia but feel like I want something more. 

Thanks.",1639348304.0
rey8v8,Widespread Exploitation of Critical Remote Code Execution in Apache Log4j,9,0.84,computerscience,https://www.rapid7.com/blog/post/2021/12/10/widespread-exploitation-of-critical-remote-code-execution-in-apache-log4j/,0,,1639343004.0
reoznh,Do you think neural networks could be used to recognize murderers through the handwriting in the notes they sometimes anonymously send to the police? How?,34,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/reoznh/do_you_think_neural_networks_could_be_used_to/,18,,1639315566.0
reinb8,"Is the ""mod"" in Modular square root same as the other mod (e.g. 5%2 = 1)?",20,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/reinb8/is_the_mod_in_modular_square_root_same_as_the/,5,"Hey, I am not from a computer science background but I do software dev. I was trying to understand VDF and how it works - Verifiable delay function. I came across 'Modular square root' in [https://www.youtube.com/watch?v=\_-feyaZZjEw](https://www.youtube.com/watch?v=_-feyaZZjEw). I was not able to interpret how ""mod"" was being used here. 

Tried this link: [https://www.rieselprime.de/ziki/Modular\_square\_root](https://www.rieselprime.de/ziki/Modular_square_root), its even confusing. 

Any resources or material to understand the background will help. 

Thanks in advance.",1639289343.0
rdr5st,Coding path to learning guidance,3,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/rdr5st/coding_path_to_learning_guidance/,5,"Hi, I have 0 experience in coding (went to school for Finance), and my end goal is to learn solidity. I want to start with JavaScript but should I learn about Data Structures and Algorithms first or first learn the basics of JavaScript. I also saw that there are courses of Data Strucutres and Algorithms specific to JavaScript. Any guidance would be helpful.",1639195644.0
rdo7ab,Confusion on adder-subtractor circuits.,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/rdo7ab/confusion_on_addersubtractor_circuits/,2,"&#x200B;

https://preview.redd.it/6omanltxet481.png?width=613&format=png&auto=webp&s=14a787509b92bbf1d6fcc17ce86c61b5cad6286a

So in an adder-subtractor circuit the output for addition will be as an unsigned integer while the output for subtraction will be in two's complement form right?  


If so does the circuit not work when the difference is less than minimum two's complement value? For example in two's complement a 4-bit integer can range from -8 to 7 in decimal. But you can input 0000 - 1001 for example. The difference of those two terms would be -9 in decimal which is less than -8 the lowest possible value. So are adder-subtractors unable to work when the difference of the input is less than minimum two's complement value?  


Sorry if this makes no sense. I am very confused.",1639186172.0
rdlhtp,Computer science riddle: monkey and 10 cages,45,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rdlhtp/computer_science_riddle_monkey_and_10_cages/,25,"There is a monkey and 10 cages.
Each cage has a door and there is a passage between each adjacent cage, not including 1 and 10.

The monkey starts at a random cage. Your goal is to find him. You may open and close as many cage doors as you want, but each time you close a cage the monkey must go through a passages to a different adjacent cage. 

The monkey can enter a previously opened cage (for example, he was in cage 5, you looked inside cage 4 and once you closed it he moved into cage 4).

Example:

Monkey at 6, you opened 8, you closed 8, monkey went from 6 to 5. You opened 4, you closed 4, monkey went from 5 to 4. Repeating.

The goal is to find a method that ensures you'll find the monkey in the least amount of doors opened.

Me and my friends had a go on this one for about 2 hours, we came up with a 17 moves solution that's I'll describe below in the first comment.

The person who told us about it said there is a better solution with less doors to open.
I hope you can help me find it. Good luck!",1639178190.0
rdc453,Why do computer scientists (and mathematicians) use the worst variable names in proofs and papers?!?,212,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rdc453/why_do_computer_scientists_and_mathematicians_use/,83,"Sorry if this post doesn’t quite fit this sub but this has been bothering me for ages and I’ve recently hit a breaking point for my frustration. 

It just pisses me off that there are 26 letters in the alphabet and 325 pairs of letters you can choose and for some reason the pair that we all seem to use is (i,j).  
There are literally no two letters that look more alike and that could get more easily mixed up but for some reason whether it be in code are in my case here in proofs and papers these two letters are always used together.  
Why should I have to use a magnifying glass to read a damn proof from a paper that couldn’t have the decency to at least use a better font?

And it doesn’t stop at (i,j). 
We also love to use (m,n), (p,q), (u,v). 

Like I get it, it’s more clear that the variables are related when they look alike, but it should certainly be more important that people don’t mix them up???",1639152919.0
rcla2n,So what do computer scientists think about NFTs? Cool tech with real world application? Or just a new way for rich people to launder money?,102,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/rcla2n/so_what_do_computer_scientists_think_about_nfts/,83,"Seems like everyone is talking about NFTs in some capacity but I haven't seen a lot of opinions about them from tech literate people, just wondering what the general consensus on them is from a comp sci perspective.",1639067420.0
rcj9zu,How does a cpu process information,0,0.45,computerscience,https://www.reddit.com/r/computerscience/comments/rcj9zu/how_does_a_cpu_process_information/,4,"I heard about something turning on and off, I don't know anything more",1639061548.0
rcip1t,How do you think should Calculus be taught to Computer Scientists?,69,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/rcip1t/how_do_you_think_should_calculus_be_taught_to/,32,"Do you think that it's better to teach it with an emphasis on applying calculus to actual problems instead of the conventional way of integrating this ugly monstrous function? If you were to teach it to undergraduates doing CS, then how would you do it?",1639059771.0
rcfj0u,"Don't Just Track Your Machine Learning Experiments, Version Them - Managing ML Experiments as Code with Git and DVC",2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/rcfj0u/dont_just_track_your_machine_learning_experiments/,0,"ML experiments often get split between Git for code and experiment tracking tools for meta-information - because Git can't manage or compare all that experiment meta-information, but it is still better for code.

The following guide explains how to apply DVC for ML experiment versioning that combines experiment tracking and version control: [Don't Just Track Your ML Experiments, Version Them](https://dvc.org/blog/ml-experiment-versioning) - Instead of managing these separately, keep everything in one place and get the benefits of both, like:

* Experiments as code: Track meta-information in the repository and version it like code.
* Versioned reproducibility: Save and restore experiment state, and track changes to only execute what's new.
* Distributed experiments: Organize locally and choose what to share, reusing your existing repo setup.

Experiment versioning treats experiments as code. It saves all metrics, hyperparameters, and artifact information in text files that can be versioned by Git, which becomes a store for experiment meta-information. The article above shows how with DVC tool, you can push experiments just like Git branches, giving you flexibility to share experiment you choose.",1639048353.0
rc9jzq,Can AI theoretically solve the halting problem or at least most of it?,0,0.22,computerscience,https://www.reddit.com/r/computerscience/comments/rc9jzq/can_ai_theoretically_solve_the_halting_problem_or/,15,"Imagine you have an neural network system, and its a classifier. It takes in some code and spits out whether or not it will halt or not. You could imagine in a simple case that you train the neural network with code that has a while (true) loop. It clearly halts and the AI can report that it halts with some trivial training examples, probably easily, without the need to do an infinite amount of computation.

So theoretically as you expand the sophistication of the AI system and incorporate more training and more examples it should be able to determine whether code halts more and more. It seems obvious, at minimum, that the AI should be able to determine whether a much larger range of code either halts or not without doing any traditional computation. Have there been any cs papers exploring whether AI could expand the reach of solvable verifiable problems traditionally thought to have been out of computational reach e.g. in the np space?",1639023865.0
rc955j,Time Complexity,23,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/rc955j/time_complexity/,13,"I have this question where it asks what the time complexity of this piece of code is. I know that the solution is O(n), but I can't wrap my head around the ""why"" of it. I was wondering if someone could help explain it to me.

https://preview.redd.it/992i6gk00g481.png?width=1202&format=png&auto=webp&s=887f13656915bea3909b15678ce44e5315b559b1",1639022484.0
rc3wsu,"You are tasked with creating a ""library"" computer which can survive societal collapse and remain operational for decades, centuries, or longer. How would you go about it? What kind of hardware and software would it need?",32,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rc3wsu/you_are_tasked_with_creating_a_library_computer/,26,"Perhaps this isn't quite the right subreddit. I'm doing some research for a bit of fiction I'm working on. 

Let's say it's built using current or very-near-future tech. The group making it has the resources of a large nation. Their aim is to preserve human knowledge even if we get knocked back into the stone age, and for the device to remain operational for long enough to be used to help rebuild society.

What sort of information would it store? What sort of hardware would be able to store and retrieve data for a very long time? What kind of software would run on it? What can be done to make sure it's stable? What kind of software would be most useful for this purpose? What and how do we teach a user who might not even know how to write? How would we power it?

How long, realistically, could we make such a device remain operational? What kind of peripherals could this device use? (communicate with a paired satellite in a geostationary orbit, antenna on top of a mountain, etc)",1639006059.0
rc1j2y,"Difference between data type, structure, schema, model",4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rc1j2y/difference_between_data_type_structure_schema/,4,"What is the difference between data type, structure, schema and model? And what's the relationship between each? Any examples?

New to CS and would love to learn more. Thanks!",1638999111.0
rbz19p,Programming permissions and privileges?,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rbz19p/programming_permissions_and_privileges/,2,"The previous company I worked at and the current company I work at both have implemented some sort of permission system so that end-users or even different organizations could not access data that they were not allowed to.   


This sort of feature is actually really important when selling a product. Both implementations have also been absolute cluster-fucks, with a ton of spagetti, edge cases, weird states, un-flexible for future feature requests, etc. 

&#x200B;

In both cases I believe this is due to the original implementors, while smart people, were trying to design a complex system from scratch using their own intellect without taking a look at the problem from a foundational or theoretical perspective.   


I might have an opportunity to right some wrongs in the coming year, and I was wondering if there's any material/information out there on how to properly design security/privileges. If i model a proven solution, i think we might end up in a better place.",1638992000.0
rbwnot,How do I get better at coding?,120,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/rbwnot/how_do_i_get_better_at_coding/,44,"I'm taking computer science in school and I'll admit, I'm not the best at it and I want to change that, I want to get better at coding but I'm not sure how I should go about it because I've heard that practise is the best way of getting better at it but I'm not sure where I should practise (We're studying python btw)

thanks in advance!",1638985373.0
rbng33,Etymology of screen dump,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rbng33/etymology_of_screen_dump/,5,"I'm posting here because internet is for once proving useless in answering this question, what is the etymology of screen dump?

Over the years the word, screenshot, has taken over for the use of 'dump', but in my native language the word still carries this legacy meaning. I've got a master in informatics and currently doing my phd in information systems research, but can't for the life of me find the answer. The old-school 70s and 80s professors in my research group, who lived through the nascent computer science days, doesn't have the answer either.  


any old school computer scientists here that might know the reasoning behind this naming? What is the reason for using dump? I've been speculating that it has something to do with how computers back in the day had to dump the memory and not make a screen photograph, but something else. Would love to hear your speculations, or possible answer!",1638955440.0
rb9f0g,Good books on optical/photonic computing?,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/rb9f0g/good_books_on_opticalphotonic_computing/,2,\^,1638910927.0
rb71fe,How long did it take for you to realize that the dynamic programming taught in algorithms is the same as the one in RL?,0,0.3,computerscience,https://www.reddit.com/r/computerscience/comments/rb71fe/how_long_did_it_take_for_you_to_realize_that_the/,5,,1638904548.0
rawuw6,For the computational scientists and AI guys here,95,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/rawuw6/for_the_computational_scientists_and_ai_guys_here/,42,Tell us about some cool projects that you've worked on.,1638875623.0
rawmbc,What are the real life applications of Taylor’s theorem?,0,0.33,computerscience,/r/maths/comments/rawm02/what_are_the_real_life_applications_of_taylors/,0,,1638874746.0
ram49s,Everything's a Provider?,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ram49s/everythings_a_provider/,4,"I showed a junior engineer that you can reduce dependencies by using inheritance and only using the parts of the interface you need.

For example,

class Circle : Shape, UIElement {}

And later, if all you need is shape properties, then use a Shape ref to point to a circle, don't require circles.

But instead, he's now broken *everything* down into single field classes/structures. E.g.

class NameProvider { string name; }

class AddressProvider { string address; }

class AgeProvider { int age; }

class TitleProvider { string title; }

class RankProvider { int rank; }

class HeightProvider { int height; }

...

class Thing : NameProvider, AddressProvider, AgeProvider, TitleProvider, RankProvider, HeightProvider, ...

I find this level of breaking things ridiculous, and it's become impossible to work with his code. It takes forever to figure out what anything is and where it is defined. But he's in love with this new approach and won't compromise. Is there a formal reason to not break things out so much? Or better yet, is there a test for when you've broken things out the right amount?",1638837044.0
rajca8,"I’m a first year computer science student with a lot of free time, I would like to study topics that aren’t included in my program that would help me when i look for work, what would you suggest?",30,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/rajca8/im_a_first_year_computer_science_student_with_a/,41,,1638829038.0
rahxnv,Formal Automata Theory next steps,10,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/rahxnv/formal_automata_theory_next_steps/,6,"I just recently finished studying Automata. I found it very interesting and enjoyed the intuitive process, even though I struggled a bit.

I was wondering what topics or subject matters would be best to study next?",1638825302.0
raflir,Question on malware.,11,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/raflir/question_on_malware/,4," 

How is it that malware can run on computers that dont have programs that run the language installed? or even certain libraries?

Like if i dont have python installed and i get a python virus, how does it run?

or if i did have it, but didnt have the right libraries, how could it function?

sorry if this is a stupid question, i am relatively new.

&#x200B;

I just really dont understand how code can even run without the right stuff installed.",1638819261.0
rabwtu,Does it make sense to study something from 2007 and will it really make me a better engineer?,1,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/rabwtu/does_it_make_sense_to_study_something_from_2007/,4,"Hi. I'm a self-taught developer who studied CS50 and full-stack web development and is solving coding challenges daily. I'm trying to become a better developer by deeply understanding the fundamentals of CS but I'm not really sure if I'll become a better developer by doing so. I came across that website: [https://teachyourselfcs.com/](https://teachyourselfcs.com/)

And I found it has old books and 2007 MIT courses. I want to know from more experienced developers out there if it is worth it to study these old heavy textbooks and courses? I'm not trying to learn the theoretical side of computer science or something I'm just trying to become a better developer who can provide better value for businesses.",1638809925.0
raadci,Computation tree,6,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/raadci/computation_tree/,3,"Was watching presentation by The Guy Steele

[https://www.youtube.com/watch?v=dPK6t7echuA](https://www.youtube.com/watch?v=dPK6t7echuA)

He describes computation as computation tree.

I'm trying to find if there are research (books) that focuses on computation trees specifically?

Take code -> unfold its execution into computation tree 

Found this: [https://en.wikipedia.org/wiki/Computation\_tree\_logic](https://en.wikipedia.org/wiki/Computation_tree_logic)

This is close but I was wondering is there something that applies ""computation tree"" in compiler/JIT/interpreter optimization...?

Thanks!",1638805950.0
ra4e6d,Linear algebra and matrices in computer science,101,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/ra4e6d/linear_algebra_and_matrices_in_computer_science/,23,"I am really struggling with the first course on linear algebra and matrices and I find it hard to motivate myself, as I don't know many real-life applications to the things I am learning, or am trying to learn.

I know these things have many applications in cs, but since I don't have a clear vision on what I'd like to do in this field yet (so far I've enjoyed web deving and I am interested in audio programming and graphics, although I haven't gotten into them yet), I don't really know why these things are relevant for me.

Any pragmatic examples on why I really should learn this stuff? Thank you.

edit: thanks to everyone who has responded, it is greatly appreciated!",1638787049.0
r9qvnk,How do I lead a project?,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/r9qvnk/how_do_i_lead_a_project/,8,"Alright, to give some background on myself, I'm a junior in High School who has been learning to code and programming for the past 2 years in just one language. Anyways, this year, I decided to join my school's CS club where they said they were taking applicants for team leads. I didn't really know what a team lead was and they left it pretty vague so I decided to sign up because I felt like why not and that this would probably be a good opportunity for me to learn some leadership skills. This being said, I really didn't know what I was getting into. For one thing, I assumed that we would be having team meetings during the club at lunch on Friday when we usually meet, however, in reality I'm supposed to schedule meetings on my own. Anyways, this isn't really too much of an issue for me however, my largest issue is the fact that 9 people signed up to be in my group, most of them arguably better programmers than me, and others, quite beginner. This is an issue for me because I've never actually coded something not by myself so I really have no idea how to divide up the work into 9 parts given how interrelated everything is in the project. In the same sense, I really have no idea how often I should meet with my group. I feel like the project is fairly simple and I'm afraid if we meet even just an hour a week we'd probably finish within a month. If you want more details on what the project is, feel free to dm me, however, if anyone could give me advice on how I should proceed given how lost I am that would be great.",1638742267.0
r9bx1d,Computer Networking Basics Every Developer Should Know,259,0.97,computerscience,https://iximiuz.com/en/posts/computer-networking-101/,7,,1638695607.0
r95hk3,How do computers render perfect circles given things like SVG images?,18,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/r95hk3/how_do_computers_render_perfect_circles_given/,10,"I don't know if this is the right place to ask but I couldn't think of any other subs that might be appropriate. Basically, how does the computer take an svg image, and create a perfect circle that is perfectly circular no matter how ""zoomed in"" you are. It seems like its either a lot less complicated than I'm assuming or a lot more complicated than I'm assuming, but it seemed like an interesting topic.",1638671737.0
r8z2ei,"Does publishing an app on the Apple store constitute as ""commercialization""?",5,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/r8z2ei/does_publishing_an_app_on_the_apple_store/,8,"I'm working on an iOS application for my portfolio. I'm a frontend guy so I'm using an open API as the backend. It's free to use but strictly mentions that the data must not be ""commercialized"". 

1. Does this mean I can only use the API as a personal project, and not publish? 

2. Or does it mean I can publish my application - I just can't monetize it?",1638651761.0
r8upnl,How did early computers understand input?,6,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r8upnl/how_did_early_computers_understand_input/,8,"I know they used punch cards and ASCII, but without programming languages like we have nowadays how did the computers understand what was on the cards? How can a machine understand input without a language allowing it to do so? It would be like talking to a rock and expecting it to do something. I’m really curious about how this worked, how the first communication with machines began. Can’t seem to find an answer on google",1638639539.0
r8rl8f,What Part of Computer Science Does Computer Graphics Fall Under?,79,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r8rl8f/what_part_of_computer_science_does_computer/,21,"I'm really interested in creating software like game-engines or 3d modeling software, mainly i like to deal with the graphical aspects of the software like reflections or lighting. What would be the name of the course that i will study?",1638630602.0
r8mrol,Are pointers useless in the same scope?,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r8mrol/are_pointers_useless_in_the_same_scope/,8,"Pointers are obviously very useful when they point to values across function calls but are they actually useful while pointing to values in the same scope. For example if a variable `age` stores the value `114`, I can update the the value of `age` by creating a pointer to that memory address, but why would I do that? I can simply change the value of `age` directly because I have access to it as I am changing from the same scope.

A reason I could think of for creating a pointer to a variable in the same scope is that it might be useful to have pointers point to a variable in the same scope because it would make the naming easy and referencing to the value in huge codebases would be made easier, but I do not think that this is a good use case. 

The main purpose of pointers is to stop the wastage of memory by copying of the values stored in a variable to another variable but if we are creating pointers, we are of course storing them in a variable which is taking memory so we are back to square one. This might not be the case where variables store a lot of data, this is true for variables that store less data than the memory address itself.

My question is that is there any use for creating pointers that point to variables in the same scope or they are never used like that. Stating any examples would be very helpful!",1638613113.0
r8f02f,How does firmware get updated?,27,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/r8f02f/how_does_firmware_get_updated/,16,"So if firmware is on an ROM chip soldered to the motherboard, this should mean it is impossible to change what is on the chip because it's read only. But firmware updates happen over the internet, so how is that possible?",1638583707.0
r81i62,Any good resources to learn Boolean Algebra?,18,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r81i62/any_good_resources_to_learn_boolean_algebra/,11,I've my exams in a few days and I need to learn about Boolean algebras and logic gates. Can you guys please help me with some resources?,1638545028.0
r80r4b,Any good resources or books for learning Lambda Calculus?,49,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/r80r4b/any_good_resources_or_books_for_learning_lambda/,11,"As in title, I would appreciate any reccomendation.",1638543014.0
r7x6t1,What is the difference between AI specialists and computational scientists?,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r7x6t1/what_is_the_difference_between_ai_specialists_and/,2,I'm currently an UG student in Comp sci and want to PG in a field that contributes more to scientific research. The fields I've found that do this are AI and computational science. They seem pretty close to me so what's the difference between them and which is better wrt to my goal?,1638531200.0
r7nt4h,Help with Asymptotic Growth,16,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/r7nt4h/help_with_asymptotic_growth/,5,"EDIT: For people asking the same question as me, I have found an INCREDIBLY useful video that explains Asymptotic Growth and Asymptotic Bounding. Please watch this here: [https://youtu.be/0oDAlMwTrLo](https://youtu.be/0oDAlMwTrLo)

Hello. This post is very simple: I am struggling to understand Asymptotic Growth. It just seems like this pointless silly abstract mathematical concept that doesn't even make sense when its applied to things like computational complexity. I can write down definitions of what it means for f to equal O(g), but I don't understand the problem, I don't understand how this is the solution, and I don't understand how the solution couldn't be something else. Basically, I need a resource that can really comprehensively teach what this thing is in a way that makes sense for it to exist.

&#x200B;

RANT-- Skip to TL;DR if uninterested

Like if you already have a function that perfectly describes your algorithms time complexity, what's the point in simplifying it in a weird way that breaks algebra rules? If you're trying to compare it to another algorithm, then why not just... compare it to another algorithm? Grab two functions, from two different algorithms, come up with some reasonable input sizes, and evaluate. Simple as that. But then there's this best case worst case thing? And I don't get how what comes out of the definitions is the best or worst case... The way that O(n) was defined to me just made it seem like any arbitrary function that was larger than f(n) could suffice in defining the worst case scenario...

&#x200B;

TL;DR please give me resources on Asymptotic Growth that go deeper into explaining why its a thing and how its actually useful",1638497963.0
r7gaav,Halting Problem & Quantum Entanglement,45,0.9,computerscience,https://www.youtube.com/watch?v=2H8629BCbkM,2,,1638476316.0
r7fkp9,Official client verification,19,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/r7fkp9/official_client_verification/,9,Is there a way I can have a client send something like a hash of it's executable to confirm the client has not been modified to a server?,1638474405.0
r71nbu,VMs and Executing Machine Code,8,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/r71nbu/vms_and_executing_machine_code/,2,"Sometimes when trying to run a VM, you need to enable virtualization in the BIOS. I always presumed this meant that the hypervisor was sending machine code directly to the CPU, which in turn interacts with all the peripherals for the VM, and that the host OS was ""allowing"" the VM's machine code to pass by.

However, I just watched [this video](https://youtu.be/wX75Z-4MEoM) where he says that the hypervisor (or at least a type 2 hypervisor) is making a virtual representation of the hardware. When I use something like VMware (type 1) desktop, is it actually interacting with bare metal? Whereas something like Virtual Box (type 2) goes through the host kernel?  Any insight is appreciated.",1638429089.0
r70ia7,Algorithm Time Complexity,12,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/r70ia7/algorithm_time_complexity/,3,"I was given a function T(N) = 5N + T(N-1)

My understanding is the algorithm will always have a Big O time complexity of of O(N) because there isn't any multiplication to increase N's power, but the answer that was expected from me was O(N\^2). Even in words, I could say that the function calls itself 5N times plus another N minus 1 times, so the time complexity should still be linear with respect to N inputs. Why is this O(N\^2)?",1638424964.0
r6vpdh,Encrypted dead man's switch,88,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/r6vpdh/encrypted_dead_mans_switch/,51,"I want to make a dead man's switch to release all my code publicly and my master password to my family, but I want it to be encrypted beforehand, and have a low chance of being cracked or destroyed before deployment, I don't have anyone trusted to maintain it and I might end up in a psyc ward for long periods of time

any way set up a dead man's switch with these conditions?",1638410229.0
r6qahx,I'm a hard sci-fi writer looking to write about cyborgs that edit their RNA with the help of nanites. How do i find the processing power to do this effectively?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/r6qahx/im_a_hard_scifi_writer_looking_to_write_about/,2,"I'm fully aware that controlling the many variables that go into genetics is a difficult task. Previously i had the computers that controlled the nanites linked to a massive, planet-wide supercomputer, but realized this connection would be impossible to maintain on earth (the cyborgs are also aliens). Is there a way I can fit the needed processing power into a small package? ",1638395644.0
r6f0od,Learning OS after learning C,53,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/r6f0od/learning_os_after_learning_c/,12,"Hey guys I wanted to know. If someone knows C and wanted to get into the world of operating systems and know all the round robin, multiprocessing stuff should they:
a) Read an entire book on OS first, before getting their hands dirty with real code 
                                                 OR
b)Should read books, alongside getting their hands dirty while following the documentation of the OS they have chosen.
I wanna know you all thoughts and opinions.",1638366123.0
r6eo9a,Getting into computational modelling ?,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/r6eo9a/getting_into_computational_modelling/,8,"Hi guys, I'm an undergrad cognitive psychology student and recently I came across the existence of the use of computational models to study phenomena.

Since I've always been pretty keen on computer science I wanted to dive more into it but I couldn't find much material on the web about what knowledge you need to start developing these programs.

I'm particularly interested in computational linguistics and computational biology.

Do you guys have any suggestions on what mathematical and CS knowledge is needed in order to have a solid background for starting to build these models ? I have some programming skills but only with Python and Java and i don't know what language is used for models (C++ maybe?)

I've already asked some professors but i still have some doubts about it.",1638365054.0
r6ei7a,Network simulator,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/r6ei7a/network_simulator/,4,"In Uni are currently using Modeler of riverbed, which was created by opnet in case you know it. I basically hate that tool to the core, it makes our lives misserable! ..mostly due to the lack of documentation.  
It is basically a tool where you can create Logical or Physical networks to simulate, with many choices for mediums, settings, protocols, profiles e.t.c.  


 I was wondering, what are the similar alternatives out there for IT in big companies?",1638364521.0
r6bl5i,Does Concorde algorithm really give an optimal solution to the TSP ?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r6bl5i/does_concorde_algorithm_really_give_an_optimal/,9,"Recently, I played around with Concorde algorithm [available here](https://www.math.uwaterloo.ca/tsp/concorde/downloads/downloads.htm).

It should return an optimal solution, however some choice he made in his optimal path are counterintuitive.

For example, I would have thought that green path would be more optimized than the red one proposed by Concord :

[Red path is suboptimal ?](https://preview.redd.it/el0vbb9s6y281.png?width=325&format=png&auto=webp&s=6aac58adabee8a54644470bf4127535cc306115d)

&#x200B;

Is this caused by some rounding approximation ?",1638353634.0
r639yl,Resources to learn about the inner-workings of a computer at a technical level?,52,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/r639yl/resources_to_learn_about_the_innerworkings_of_a/,14,"Hi all, IT professional here. Recently I have become interested in learning more about how the components inside computers work. For example, I know what CPU, GPU, RAM, etc are...but how do they actually operate?  I'm talking at an extremely technical level. Does this fall under computer science? Thanks all.",1638323120.0
r5yufr,Why might changing process priority and forcing a single CPU thread to focus on a program improve the stability of that program?,57,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/r5yufr/why_might_changing_process_priority_and_forcing_a/,22,"This is kind of a specific question but, I formulated it in the most general way possible so anyone can benefit from any answers.

I have a very lightweight visual novel video game called *Shiny Days*. For some reason, the game runs just fine with minimal bugs in windowed mode. However, when in fullscreen mode, the game will freeze at a very specific moment in the game every time without fail.

The fix for this endorsed by the community is to change the process priority to ""Above Normal"" and change the processor affinity to only ""CPU 0"" or ""CPU 1."" This is to force the program to only use a single thread.

&#x200B;

The question I am asking here is, why does that fix work? Why would increasing process priority and limiting the program to a single thread help anything? I know what the fix does but, I don't get why it stops the freezing of the program in such an effective way. What do you guys think is going on here? I'm curious to hear any insights computer science experts would have to say on WHY this fix actually works.",1638310213.0
r5w1xe,"Advice on studying virtualization (material, learning path, ...)",2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r5w1xe/advice_on_studying_virtualization_material/,2,"Hi guys, 

This semester I am taking a course named ""virtualization techniques"". It is very interesting and also daunting to me when it involves a huge amount of low-level understanding of the system. I don't have a good background in this and I think I can get some good advice from Redditors :D Like good textbooks to read, basic articles to grasp the idea, ... I think it would be very helpful to refresh my computer architecture knowledge, but it would be nice if I could get a list of worth-studying contents :D As of now, I am trying my best but kinda lost in the lecture (I feel the professor is quite bad at eli5)

Don't tell me to quit, I picked this course because I found it fun to learn, and also I intentionally spent this semester studying shit I have almost zero knowledge of. I don't care about my grade at all.

Thanks in advance!",1638302695.0
r5rwl8,How does a web server talk with an API at a low level.,26,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/r5rwl8/how_does_a_web_server_talk_with_an_api_at_a_low/,4,"Hello, I'm trying to get a deeper understanding of networking, specially low level network programming. I've been reading about sockets and how to write server programs using socket libraries like winsock or socket.h, but I am kind of confused about how the server then talks with other programs. I think the best way to explain it, is for me to lay out what I THINK is happening when a simple GET request hits an HTTP server.

Assume a client and server socket are already connected using TCP.

Data Arrives to the network interface for the machine, the driver for network interface card tells the OS that it has data (through an interrupt?). The OS then looks at the IP:Port# and if there is a connected socket listening with that IP and Port, it then (through the socket library) passes the data to the process that is the ""server"". The server has the payload with the HTTP headers, metadata, body, etc. Now lets say I have written an API using C# using the webapi framework in dotnet. Does the server open a socket with the process running the API in order to pass the HTTP request info to the correct endpoint? Is the server and the API the same process, and the dotnet framework (or any API framework) is baking in the socket code and acting as a server as well? or is it something I'm completely missing?

&#x200B;

Let me know if this makes sense at all. Please correct and incorrect assumptions or missing steps in my walkthrough. I'm trying to remove as much abstraction as possible and understand how something like this works at a low level.

Thanks for your time!",1638291958.0
r54to6,(HELP) Learning Networking,26,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r54to6/help_learning_networking/,11,"Hey guys, i just wanted to see if any of you know any good resource because so far i have read many books but none of them helped me (Head first, Cisco , Comptia)  i know those are good books but i need something that explains EVERYTHING, im a complete beginner.. for example they start talking about protocols without explaining what  a protocol is and a lot of things like that they just say words without explaining

i need a book or course that gives a deep introduction.",1638217686.0
r4y6sf,Researchers Defeat Randomness to Create Ideal Code,38,0.98,computerscience,https://www.quantamagazine.org/researchers-defeat-randomness-to-create-ideal-code-20211124/,3,,1638199555.0
r4xo16,Liskovs Substitution Principle Question,5,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r4xo16/liskovs_substitution_principle_question/,10,"My question is about why LSP requires contravariance in the parameters of the subtype's inherited methods.

And more to that, if following LSP means that I can pass in a subtype wherever a parent type is expected, then why couldn't the inherited methods of the subtype be covariant in the parameters?

Does the fact that you can't be more specific when passing parameters to a subtype's inherited methods itself violate LSP?

Edit: Thanks to u/Probablynotabadguy I think I understand this.

If you have a type T and a subtype S, then for all intents and purposes, S IS a T.

And so of course the substitution portion holds that if you have a function `foo(T)` that you should be able to pass in an S - because we just said an S IS a T, right?

That part I understood. However, because anything provable of type T should be provable for type S, that includes it's methods. And so the contravariance of the subtypes methods is precisely because it's trying to preserve the provability of the subtype relationship.


My apologies, I know that's probably a lot of the same words.

[Code](https://carbon.now.sh/?bg=rgba%28171%2C+184%2C+195%2C+1%29&t=vscode&wt=none&l=text%2Fx-php&ds=true&dsyoff=42px&dsblur=68px&wc=true&wa=true&pv=56px&ph=56px&ln=false&fl=1&fm=Hack&fs=14px&lh=133%25&si=false&es=2x&wm=false&code=class%2520Speaker%2520%257B%250A%2509public%2520function%2520speak%28string%2520%2524word%29%2520%257B%250A%2509%2509return%2520%2524word%253B%250A%2509%257D%250A%257D%250A%250Aclass%2520LoudSpeaker%2520extends%2520Speaker%2520%257B%250A%2520%2509public%2520function%2520speak%28string%2520%2524word%29%2520%257B%250A%2509%2509return%2520strtoupper%28%2524word%29%253B%250A%2509%257D%250A%257D%250A%250Aclass%2520ScreamingSpeaker%2520extends%2520LoudSpeaker%2520%257B%250A%2520%2509public%2520function%2520speak%28string%2520%2524word%29%2520%257B%250A%2509%2509return%2520strtoupper%28%2524word%29%2520.%2520%27%21%21%21%27%253B%250A%2509%257D%250A%257D%250A%250Aclass%2520Animal%2520%257B%250A%2520%2520%2520%2520public%2520function%2520speak%28LoudSpeaker%2520%2524speaker%29%253A%2520string%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520return%2520%2524speaker-%253Espeak%28self%253A%253Aclass%29%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Aclass%2520Cat%2520extends%2520Animal%2520%257B%250A%2520%2520%2520%2520%250A%2520%2520%2520%2520%252F%252F%2520Why%2520am%2520I%2520not%2520allowed%2520to%2520accept%2520ScreamingSpeaker%2520here%253F%250A%2520%2520%2520%2520%252F%252F%2520According%2520to%2520LSP%2520I%2520can%2520only%2520accept%2520either%2520LoudSpeaker%2520or%250A%2520%2520%2520%2520%252F%252F%2520it%27s%2520parent%2520class%2520Speaker.%250A%2520%2520%2520%2520public%2520function%2520speak%28LoudSpeaker%2520%2524speaker%29%253A%2520string%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520return%2520%2524speaker-%253Espeak%28self%253A%253Aclass%29%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Afunction%2520makeAnimalSpeak%28Animal%2520%2524animal%29%2520%257B%250A%250A%2520%2520%2520%2520%252F%252F%2520According%2520to%2520LSP%2520I%2520shouldn%27t%2520pass%2520in%2520LoudSpeaker%2520here%253F%250A%2509return%2520%2524animal-%253Espeak%28new%2520Speaker%29%253B%250A%257D%250A%250AmakeAnimalSpeak%28new%2520Cat%29)",1638198036.0
r4v7w6,Operations of a memory stack and how it is used to implement function calls on a computer,29,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r4v7w6/operations_of_a_memory_stack_and_how_it_is_used/,10,"Hi everyone, I have recently been doing some personal research on data structures and algorithms, one of the things I want to look into is a memory stack. 
I have already written about how a Stack data structure works in C# (what is does, what the operations are and some examples) but I also need to talk about a memory stack.

Would anyone be able to explain to me in simple terms what a memory stack is and if there is any different between a Stack in C# and a memory stack and also how it is used to implement function calls in a computer?

Everything I have found online is either too complicated or is just telling me you can Push and Pop like you can in the C# stack so I don’t understand the difference.
Thanks :)",1638190442.0
r408nu,"Computer Science was always supposed to be taught to everyone, and it wasn’t about getting a job: A historical perspective",12,1.0,computerscience,https://computinged.wordpress.com/2021/11/26/computer-science-was-always-supposed-to-be-taught-to-everyone-but-not-about-getting-a-job-a-historical-perspective/,2,,1638090600.0
r3rold,Creating hash based verification of links.,18,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/r3rold/creating_hash_based_verification_of_links/,6,"Hi geeks,

How do we verify whether a link or a github profile that was provided in Resume belongs to the same person as the Resume? I am thinking of having a hash based addressing to solve such problem. Any leads? 

&#x200B;

For detailed explanation: Let's say in my resume I put [https://github.com/randomuser](https://github.com/randomuser) and no one knows If I am the random user and github user names are strange most times. What can be a possible solution to the problem?",1638060862.0
r3exb5,LC-3 memory size question,11,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/r3exb5/lc3_memory_size_question/,10,"https://justinmeiners.github.io/lc3-vm/index.html#1:3

> The LC-3 has 65,536 memory locations (the maximum that is addressable by a 16-bit unsigned integer 2^16), each of which stores a 16-bit value. This means it can store a total of only 128kb, which is a lot smaller than you may be used to! In our program, this memory will be stored in a simple array

Can someone help me to understand why and how the size of the memory is 128kb (kilobits)? I really might be this dense, but I don't get it.

Sorry in advance if not correct sub, I thought this would be the closest one. This is not my home-work, I'm old dude just doing this on my freetime.",1638024227.0
r3cbfk,Can you recommend any free courses/lecutres on fluid simulation?,26,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/r3cbfk/can_you_recommend_any_free_courseslecutres_on/,5,"Basically, question is in the title  
UPD: found a great book on the topic:  Fluid Simulation for Computer Graphics by  Robert Bridson ",1638014898.0
r39ah9,Why do we use texture atlases?,4,0.7,computerscience,https://www.reddit.com/r/computerscience/comments/r39ah9/why_do_we_use_texture_atlases/,4,"If it's all in ram already then it shouldn't add any additional time to skip around for textures, right? And even if it's some buffer shit then isn't that openGLs job?",1638002216.0
r2ye5m,Most Used Software Engineering Stack,23,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/r2ye5m/most_used_software_engineering_stack/,33,In your opinion what is the most widesbread stack used in software development feild today? I’m cs student and am trying to get an idea of how people build software in the real world.,1637965523.0
r2sjef,How a video player plays the video file instantly but not a video editor?,22,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/r2sjef/how_a_video_player_plays_the_video_file_instantly/,7,"When we open a video in any video player, it opens it almost instantly and plays in full resolution with 30fps. We can also skip to a particular section (time) and player instantly jumps both frame and audio to that section.

But when we open it in video editor, it does operation like caching frames or drop frame rate while playing inside editor. Also when we zoom a particular length inside editor it takes time to make thumbnails for that section while in video player it resizes to resolution of windows in real time.

It seems video player can decode the video file instantly but not video editor.

So whats different between how a video player handles a file and how a video editor handles a file. Also why it takes 5 minutes to export (write) the video while it can read it instantly.

I'm not a computer science student. Sorry if question is poorly phased.

Thanks.",1637948259.0
r2rbee,book recommendation,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/r2rbee/book_recommendation/,1,"hi
i studied computer science and worked in security for 20 years, but now I have changed and work in communications ( datacenters architecture, Ironport, bluecoat, Cisco, F5, infoblock, dns, firewalls, mpls, wan, etc) does anybody know of a good book to refresh my knowledge in communications in general. not too basic as i have good background but a good digest. 
thanks",1637944740.0
r2e13s,"If an NP-complete problem X reduces to Y, is Y also NP-complete?",24,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/r2e13s/if_an_npcomplete_problem_x_reduces_to_y_is_y_also/,7,"My only exposure to NP is a very well written stack overflow answer and I want to see if I understand it. If X is in NP-Complete (so all NP problems reduce to X), and X reduces to Y, then Y is also NP-complete right? Or is it not transitive?",1637898299.0
r2cwfc,Quick Vocabulary Clarification,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r2cwfc/quick_vocabulary_clarification/,2,"Are ""Data Width"" and ""Bandwidth"" interchangeable phrases? This assignment I've been tasked with uses the term ""data *width*"" in the context of data transmission and I don't think that it's really commonplace. I'm just curious if it has any other probable/possible meanings. Thanks.",1637894489.0
r2a6yz,How do operating systems get loaded onto computers?,75,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/r2a6yz/how_do_operating_systems_get_loaded_onto_computers/,20,"When there are the components of a computer assembled into an otherwise perfectly functional PC, how is the operating system loaded onto the computer? It's not like they can code the computer by just typing on it because with no software it can't interpret the text and it wouldn't even know how to make the text because Unicode is software too.",1637885515.0
r27hds,Lowest wattage to read text file,10,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/r27hds/lowest_wattage_to_read_text_file/,7,"There’s work being done on a diamond nuclear battery that can last hundreds of years, but it’s only 100 microwatts. How many of these would you need to read a basic text file? It could be as simple as having an LED light up above a set of printed alphanumerical stickers, but the easier it is to read, the better. The idea here is to be able to download Wikipedia and access it at any time without recharging.",1637877306.0
r27cn1,Mathematical Logic Gates,58,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/r27cn1/mathematical_logic_gates/,26,"Any letter here is used to represent a binary input (1 or 0) and then you just follow the basic of principles of maths, oh and work in **DENARY.** The idea is that you could punch one these formula into a standard school calculator without changing any settings and have it working.

Heres the basic ones:

1. NOT = 1-x
2. AND = xy
3. NAND 1-xy
4. OR x+y-xy
5. NOR 1-x-y+xy
6. XOR x+y-2xy
7. XNOR 1-x-y+2xy

Heres an adder that takes a carry and 2 bits and outputs the binary in:

* x+y+z+8xy+8xz+8yz-16xyz

Sorry I didn't create a half adder as my main source for logic gates was a video series by Sebastian Lague so I just followed that along .

Anyway just for fun and to prove how utterly useless this whole thing is here is an untested 4 bit adder:

z+a+b-2ab-2az-2bz+4abz+10((ab+az+bz-2abz)+c+d-2c(ab+az+bz-2abz)-2d(ab+az+bz-2abz)-2cd+4cd(ab+az+bz-2abz))+100(e+f+(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef-2e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)-2f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)+4ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+1000(g+h+(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2gh-2g(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2h(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+4gh(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)))+10000(gh+g(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+h(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2gh(ef+e(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))+f(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz))-2ef(cd+c(ab+az+bz-2abz)+d(ab+az+bz-2abz)-2cd(ab+az+bz-2abz)))

z is a carry value (if you want to string together multiple adders) and a adds to b c adds to d etc

Heres an interesting question, how far can we take this? Could we write a Turing Machine like this?

Mistakes have now been corrected Thanks everyone who pointed them out and suggested solutions",1637876920.0
r20mys,Where to start as a dev who wants to adapt a CS paper to code?,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r20mys/where_to_start_as_a_dev_who_wants_to_adapt_a_cs/,3,"As the title says, I want to find out if I can implement an algorithm described in a research paper, or one research paper out of a specific family of research. I think a great achievement would be to understand a more basic paper in this family of research, implement a basic working form of the algorithms presented, and have them working together.

The parts of any of these papers I am unfamiliar with is the more complex math and symbols, and how these different described algorithms should fit together. Most of my relevant experience is as a software engineer, if that matters at all.

What are the steps I can take to move towards this goal, as someone unfamiliar with the research and language. Who can I reach out to?",1637858536.0
r1xcw9,Artificial super intelligence (ASI),47,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r1xcw9/artificial_super_intelligence_asi/,22,"Good day everybody,insight here (worried)

1.The supercomputer aurora21 is nearly finished and been used to map the human brain/connectome, they say it could only take three years to map it

Source:https://www.pbs.org/wgbh/nova/article/brain-mapping-supercomputer/

2. Im also worried about artificial super intelligence and artificial general intelligence already been used


My delusions are now furthered thinking Aurora21 and ASI already exists and are been used to read/implant thoughts (and making people hear voices)

Can someone in the know tell me this isn't possible or the details on how it works/or doesn't

I dont know anything about computers so im turning to you for insight again

Again,on meds,in therapy. Just want to know your insights which i struggle with due to schizophrenia",1637849376.0
r1rdzl,Edsger Dijkstra's Turing Award Speech - Part 1 of 8,5,0.86,computerscience,https://www.linkedin.com/posts/unixbhaskar_edsger-dijkstras-turing-award-speech-part-activity-6869541920759156736-C2GQ,0,,1637826714.0
r1lsow,Analyze Algorithm Complexity which has Distinct Scenarios,14,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/r1lsow/analyze_algorithm_complexity_which_has_distinct/,3,"I have an algorithm which will check if any given number is prime. I've included it at the bottom of the post. It makes use of a few optimizations, mainly the 6n+-1 optimization, where numbers which are not modulus of 6 + 1 or 5 are not considered prime.

I can analyze the complexity of the algorithm to be ``O(sqrt(n))`` if n % 6 === (1 or 5) and ``O(1)`` otherwise, but I am not quite sure how to assess the 'overall' complexity.

My first though is to average them out, such as: ``(4 * O(1) + 2 * O(sqrt(n)))/6``, but I believe that would still equal ``O(sqrt(n))``. Is that correct, or have I taken the wrong approach?


```javascript
/**
 * Check if a number is prime
 * 1. Check if number is a base case
 * 2. If not, check if num is of the form 6n +- 1
 * 3. If it is, then call helper method
 */
function isPrime(num) {
    //handle special base cases
    switch (num) {
        case 1:
            return false;
        case 2:
        case 3:
            return true;
    }

    //primes are never even (except for 2)
    if (num % 2 === 0) {
        return false;
    }

    //determine if num is of the form 6n - 1 or 6n + 1
    const sixMod = num % 6;
    if (sixMod !== 1 && sixMod !== 5) {
        return false;
    }

    return _determineIsPrime(num);
}

/**
 * Helper method to determine if a number is prime
 * If the ""isPrime"" method is inconclusive, this will be called
 */
function _determineIsPrime(num) {
    //optimize by only searching up to sqrt of num
    const limit = Math.sqrt(num);

    //iterate over numbers
    //return false if any divides
    for (let i=3; i<=limit; i+=2) {
        if (num % i === 0) {
            return false;
        }
    }

    //if nothing was divisible, return true
    return true;
}
```",1637807749.0
r19gul,Tree/Graph Diagram program,8,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/r19gul/treegraph_diagram_program/,1,"Friend is asking me if I know any program where you can drag and drop nodes in order to visualize Trees/Graphs.  
Additionally, is there any program that could diagram your code ? ( like graphviz, but for algorithms instead of structures  )",1637772506.0
r199w0,"Publishing, The Choice and The Luck",3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/r199w0/publishing_the_choice_and_the_luck/,0,Publishing research results is Computer Science is a process with no simple solutions. In this blog post I try to present some common strategies and learn from recent insights into the random effects that are present in the review process. [https://cacm.acm.org/blogs/blog-cacm/256994-publishing-the-choice-and-the-luck/fulltext](https://cacm.acm.org/blogs/blog-cacm/256994-publishing-the-choice-and-the-luck/fulltext),1637771971.0
r189ka,Do you know any good pop-sci books on the topic of NSA backdoors in Encryption Algorithms?,24,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/r189ka/do_you_know_any_good_popsci_books_on_the_topic_of/,2,"I've seen mention of the supposedly backdoored  Dual\_EC\_DRBG  a few times.

I'm interested in reading more about the background and known facts, any other relevant cloak and dagger shenanigans would be appreciated also.

I am not a mathematician, so I'm looking for a pop-sci book, or something digestible for someone technologically literate but non-expert.

Books are preferred, but good quality articles also appreciated.

Thanks

&#x200B;

EDIT: Any good books on encryption and randomness would be great actually, too.",1637769198.0
r0ss7l,"Why does Rosetta run x84 apps at basically native speed, but running Linux on an M1 is borked as hell?",52,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/r0ss7l/why_does_rosetta_run_x84_apps_at_basically_native/,15,"Why? Isnt rosetta essentially a CPU emulator? How can it run apps at normal speed, but running Windows or Linux (the real versions, not the ""M1"" versions) is unusable? Dont even know if this is the correct sub to ask.",1637717136.0
r0eadx,"Parsons puzzles (create, save, modify, delete puzzles)",21,0.94,computerscience,https://www.codepuzzle.io,3,,1637677285.0
r07asz,How do data cables index files for large hard drives?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/r07asz/how_do_data_cables_index_files_for_large_hard/,2,"Specifically with PATA data cables, I've been looking through their specifications and noticed they have 16 data lines but only 3 address lines. Hard drives that support PATA seem to be able to support hundreds of gigabytes, but directly addressing even just 4 GB would require 4 bytes. How does a computer send data to the hard drive about which data it wants read/written?

My hypothesis would be that either it has something to do with paging and lookup tables, or the data lines are repurposed to tell the microcontroller within the hard drive the area in memory to be accessed, or the computer tells the hard drive which block to start at and it copies over the entire block, one word at a time, or the 3 address lines control the read/write head directly and reading the correct block is timing based, similar to how DVI doesn't use coordinates for pixels.",1637650582.0
r02lfl,Resource for practicing algorithms without programming? (migraine sufferer),6,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/r02lfl/resource_for_practicing_algorithms_without/,2,"Does there exist a resource for exercises of the form ""design a program with the following specifications in time/space O(f(n))""?

I am aware of competitive programming websites but i can't currently program due to migraines. I can't practice off these websites because often the best solution is some exponential time algorithm which i will have no way of knowing whether it will work without implementing it.

CLRS is a good resource for problems but there aren't enough to regularly practice on. I would like a resource preferably of the computer contest flavor but where i can practice on pen and paper rather than programming it.

edit: why is this getting downvoted",1637634523.0
qzwduc,How do I practice my DS Algo concepts?,23,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qzwduc/how_do_i_practice_my_ds_algo_concepts/,10,"Before you ask me to create project, I am asking for questions to practice the individual concepts like linked lists, stacks, sorting algos etc. so I can get comfortable with their implementation and the various problems they can solve, before moving on to building projects and integrating them together.",1637616562.0
qzwb8e,Any advice on building a search engine?,68,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qzwb8e/any_advice_on_building_a_search_engine/,37,"So I have a DS course and they want a project that deals with big data. I am fascinated by Google and want to know how it works so I thought it would be a good idea to build a toy version of Google to learn more.

Any resources or advice would be appreciated as my Google search mostly yields stuff that relies heavily on libraries or talks about the front end only.

Let's get a few things out of the way:
1) I am not trying to drive google out of business. Don't bother explaining how they have large team or billions of dollars so my search engine wouldn't be as good. It's not meant to be.
2) I haven't chosen this project yet so let me know if you think it would be too difficult; considering I have a month to do it. 
3) I have not been asked me to do this, so you would not be doing my homework if you give some advice.",1637616378.0
qzo8bt,Resources or practice problems for software architecture or design patterns,26,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qzo8bt/resources_or_practice_problems_for_software/,2,"I am not sure if software architecture or design patterns are the right terms. But I am looking for resources to read up on or practice problems for ways to approach coding problems and implement solutions in an optimal way. Like how to break down a problem in smaller chunks and modularize it. Or even when you should use procedural vs functional vs oop approach.

The reason is that for smaller projects I tend to just do what works, which is manageable, but as it gets bigger I quickly get spaghetti code. Then I start to have analysis paralysis on how to solve certain problems, because it feels like there are so many different ways. I want to practice scalability so that I can keep my projects manageable. Likewise, something that bothers me is that a lot of basic online examples (particularly in OOP) give very trivial problems (e.g. squares being a type of shape, or cats being animals), which is easy to visualize, but the problems I am dealing the objects are very ambiguous (for my feeling). To the point where it is even the question if objects are relevant, with the exception of being able to separate code/data from each other.",1637595660.0
qzkz6g,"2nd to Last member of the ""Traitorous Eight"" passed away. Rest in peace Dr. Last.",2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qzkz6g/2nd_to_last_member_of_the_traitorous_eight_passed/,0,"The New York Times: Jay Last, One of the Rebels Who Founded Silicon Valley, Dies at 92.
https://www.nytimes.com/2021/11/20/technology/jay-last-dead.html",1637586259.0
qzkegn,Books on IOT,19,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/qzkegn/books_on_iot/,5,"I recently got interested in IoT. My background is computer software and programming. My interest has always been on the different mediums for  communication between human and computers. 
I would appreciate if you can suggest books that covers the basic concepts of IoT and their application on communication and collecting data.",1637584284.0
qytazd,Where to find practical exercises about IPv4 addresses?,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/qytazd/where_to_find_practical_exercises_about_ipv4/,1,"I'd like to train/practice on IPv4 tasks, such as:

\- what number of addresses there in this address and mask?

\- which range does this address belong to?

\- calculate this, find out that

Where to find such? Thanks!",1637494991.0
qz9jwv,"Why are errors ""close"" to the kernel so irreparable?",7,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/qz9jwv/why_are_errors_close_to_the_kernel_so_irreparable/,1,I have so many questions to ask about kernels and this is one of them.,1637544058.0
qz4cwb,Genetic programming: tree GP vs. linear GP,3,0.64,computerscience,https://www.reddit.com/r/computerscience/comments/qz4cwb/genetic_programming_tree_gp_vs_linear_gp/,0,"I'm writing an algorithm to predict global average temperatures. I read an academic paper that used tree GP to solve the same problem. I already have a linear GP algorithm I wrote for something else that I'd like to recycle, but I'd also like to get the best performance possible out of the temperature algorithm.

Is there an advantage to using tree GP vs. linear GP and vise versa? Are there certain types of problems that are better suited to one representation over the other?",1637528545.0
qz023c,Genetic Algorithms| Bayesian Optimization | Reinforcement Learning,27,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qz023c/genetic_algorithms_bayesian_optimization/,7,"Hello all,

I'm interested in learning more about Genetic algorithms and Bayesian optimization in the context of Hyperparameter tuning in Machine Learning and Operations Research. Not interested in medium articles, I want to dive and understand the Math. I am also intested to get a good introduction to Reinforcement Learning.

Could you suggest good books/ pedagogical articles about these three subjects?",1637516416.0
qyyi89,Is computer vision is a part of engineering or computer science,1,0.53,computerscience,https://www.reddit.com/r/computerscience/comments/qyyi89/is_computer_vision_is_a_part_of_engineering_or/,3,"I wonder if computer vision consider for computer science or engineering or both
As my knowledge that Computer vision is  An Ai but many ppl tell mostly for engineering than science",1637512069.0
qyv2sd,Good introductory book on formal specifications?,22,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/qyv2sd/good_introductory_book_on_formal_specifications/,3,"I am looking for a good introductory book on formal specifications. This is a new field for me so I haven't the slightest clue where to begin. Any help is greatly appreciated.

Thanks",1637501642.0
qys9kx,Exam question format: Consider an x-stage pipelined CPU. How much time does it take to execute n CPU instructions where each stage is y nanoseconds.,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/qys9kx/exam_question_format_consider_an_xstage_pipelined/,3,"So I've been doing some past papers recently and this format or something amounting to the same thing keeps cropping up.

The only problem is, if your stages dont divide neatly into your instructions and leaves a remainder.

For example a 3 stage piplined CPU with 8 instructions, will initilise as such:

You subtract the triangular number of 3 from the instructions: 8 - 6 = 2

And that is how many instructions will be carried out while the program is operating at full efficiency, so you can divide the 2 by the number of stages you're operating at any given time so 2 / 3 = ...

Oh... rem. 2?

Trying to visualise what happens this is my best guess at the answer:

 [https://imgur.com/WyVTjCN](https://imgur.com/WyVTjCN)

As you can see you would just have a wind down stage which last 1 cycle? But that doesnt make sense, because on the x axis the third stage of one of the instructions doesn't get carried out?

I think I just fundamentally dont understand.

Thanks a million in advance for any assistance c:",1637490467.0
qys6ai,"To get passed binary input, do we need to drop conventional sourcing? Fibre optic?",0,0.22,computerscience,https://www.reddit.com/r/computerscience/comments/qys6ai/to_get_passed_binary_input_do_we_need_to_drop/,6,"So I am very new to coding, so apologies if this is completely impossible or even incorrect.

But I do believe that current systems run off CPUs measuring the amount of 0s and 1s through voltage.

However if we were to change this process to fibre optic where the process is still assigned to an on or off process.

However if we introduced colour into fibre optics we could expand the possibilities. For example if white was 0-1 blue 1-2 yellow 2-3 and so on so forth we could expand the possibilities.

We can't expand so far, on the 0 being off and 1 being on, so to introduce colours into fibre optic we could essentially assign colours a number to achieve different out comes.

We know fibre is faster than conventional powered connections. So this could increase performance also.",1637490058.0
qyo107,How does a computer know how full memory is? (Should have asked in my previous post),58,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qyo107/how_does_a_computer_know_how_full_memory_is/,13,"Okay, Another memory related question, I wish I thought of this with my previous post to save a bit of time, but anyway. Another thing I have thought of is how does a computer calculate how full Memory is? Is it fully software based, or does hardware handle it? If its hardware based is it in the CPU Memory controller, or the RAM Sticks themselves?",1637472565.0
qylubl,"Build a simple ad hoc network, with small computers.",0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qylubl/build_a_simple_ad_hoc_network_with_small_computers/,0,"Hi, I programmed a small computer based on von Neumann architecture with a few set of instructions on binary and now I'm asking to design and implement an ad hoc network with some of this emulated computers, so, do you have any advice or documentation for this purpose? Thanks",1637464746.0
qykxoh,How would a computer calculate how much RAM it has to work with?,1,0.55,computerscience,https://www.reddit.com/r/computerscience/comments/qykxoh/how_would_a_computer_calculate_how_much_ram_it/,7,"Hi! I am designing a computer just for fun, I learned the basics and what not, and I am building one in Logisim (I know, not the best software, but works for now) And it is a decently complicated CPU and stuff. (Still trying to work out how the stack works, and a couple of other things before it can technically be deemed a proper computer) And this thought crossed my mind: ""How do computers calculate how many bytes of Memory it has?"" I have a working prototype of the whole computer and am working on a simple BIOS for it. And while I was making a prototype of the BIOS I thought of ways the computer detects memory and how much there is, but cant think of any. I tried looking it up but cant find any good explanations. Anyone know how this is done? And my apologies if I picked the wrong flair and if this breaks any rules, I have read them and it doesn't seem to break any. Any help is appreciated!

Edit: Thanks to raedr7n and NamelessVegetable for the help! Found out how it works",1637461704.0
qyiizq,How exactly would you go about converting a truth table into a circuit diagram?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qyiizq/how_exactly_would_you_go_about_converting_a_truth/,10,,1637453711.0
qyeqvw,Recommendations for a basic high-level language encryption / obfuscation algorithm?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qyeqvw/recommendations_for_a_basic_highlevel_language/,7,"Hi I am making a game with a high-level scripting language that has pretty minimal support for bitwise-manipulation (if any). Think something like Lua. And I am making a game where the game has a secret message that is defined as a string literal, ""secret message"". This script can be scraped by data miners, string literals are not obfuscated at all, so if they got a hold of of the sourcecode, they would be able to read ""secret message"". What is a basic obfuscation algorithm I can use here to store the string literal in a scrambled-up format so that dataminers will have a harder time interpreting this message? I'm sort of looking for something one step above a caesar cipher. Any algorithms come to mind?",1637442185.0
qyaklu,Does anyone know of any good podcasts that cover computer science or programming topics?,118,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qyaklu/does_anyone_know_of_any_good_podcasts_that_cover/,13,"Basically what the title says. There are podcasts about space that aerospace engineers can listen to, for example, so I was wondering if anyone knows of any comp sci podcasts",1637429966.0
qy9ke0,"If data packets can collide and can be affected by interference, does that mean they are physical bodies?",13,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/qy9ke0/if_data_packets_can_collide_and_can_be_affected/,14,,1637427093.0
qy5tvy,French supercomputer Adastra with AMD EPYC Genoa and Instinct MI250X to become one of the most powerful in Europe,26,0.93,computerscience,https://www.ryzencpu.com/2021/11/french-supercomputer-adastra-with-amd.html,0,,1637415696.0
qy34w1,Does Encryption work both ways?,7,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/qy34w1/does_encryption_work_both_ways/,19,"Here's an example. If a folder is encrypted, it's obviously protected from outsiders trying to get in. But let's say there was a virus inside of an encrypted container, would it be able to escape this container or does the encryption keep it ""imprisoned""? My understanding is that encryption garbles all of the data so surely protection DOES work both ways?",1637405141.0
qy32gm,Why does the routing table system work?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qy32gm/why_does_the_routing_table_system_work/,1,"Hi all, I’ve recently been looking a bit into networking and trying to understand it a bit more but routing tables have me a bit stuck. I think that I understand how they work by I don’t have the faintest idea why.

So as I understand it, a packet comes into a router and the destination IP address is AND’ed with each of the subnet masks in the table and the result of each of these is compared with the destination IP address of each entry in the table. If there are any matches between the result of this AND calculation and destination IP addresses then the packet is forwarded to this destination. If there are no matches the packet goes to the default destination and if there is more than one matches then the packet goes to the destination with the highest subnet mask.

My question is, why does this work? How does this form of selecting destinations make sure packets make it to their destination. How does this stop data packets going round in circles and taking ages to make it to their destination.

I hope that I’ve been clear, but please let me know if I should clarify anything.

Thank you so much for any help here :)",1637404847.0
qxxaab,Do you guys refer to yourself as computer scientists,82,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qxxaab/do_you_guys_refer_to_yourself_as_computer/,62,,1637380818.0
qxv7ln,"Why cant a RAID 1 (mirroring) have twice the write speed of a single drive, the way it has twice the read speed?",7,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/qxv7ln/why_cant_a_raid_1_mirroring_have_twice_the_write/,7,"I understand how it currently works, and subsequently why it cannot write twice the speed. My question more specifically, is why doesnt it work a different way where that is possible. For example, if there is a 100mb file, instead of writing the 100mb to both drives simultaneously, why doesnt it write 50MB (different halves of the data) at the same time, and then virtually treat it as one complete file as it then continues to write the full file on both drives in the background after done? Does writing 1 bit take up 1 bit of potential reading or something? Even then, the background writing can resume after reading is done at a later time if that is the reason. The only downside I see to this is that you risk losing that particular file if one drive fails before the completed write takes place, but that seems like a minor sacrifice for the improved performance.",1637373882.0
qxrdbd,"May be a stupid question: Why are computer programs that modify themselves so uncommon? I can't really think of a use case, but does the phenomenon have a name?",75,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qxrdbd/may_be_a_stupid_question_why_are_computer/,62,,1637361444.0
qxl1o1,How is data stored in a DHT?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qxl1o1/how_is_data_stored_in_a_dht/,3,"I’ve been learning about DHTs (distributed hash tables), and I understand that data is distributed across many nodes (or peers in p2p networks). I can’t seem to find any information on how data is stored in each node though. Is data stored in blob stores in each node? Or SQL databases? Am I understanding this correctly?",1637342787.0
qxixkk,Why are some people so excited about functional programming?,61,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/qxixkk/why_are_some_people_so_excited_about_functional/,60,"It seems like FP can be good at certain things, but I don’t understand how it could work for more complex systems. The languages that FP is generally used in are annoying to write software in, as well. 

Why do some people like it so much and act like it’s the greatest?",1637336708.0
qxh10d,Learning C again,14,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/qxh10d/learning_c_again/,11,I have learned C previously from Dennis Ritchie book. But as now I have learned the concepts of Compiler Design  ( and other theoretical CS subjects ) I again want to learn C but with some compiler design concepts( and or other CS concepts ). Like when I am doing something I want to know whole thing behind it relating to compiler design concepts. I don't know whether I have asked clearly or not. But if you get it please tell the resources for it. I want to know whole process that follows running a code.,1637331102.0
qx827n,What sort of memory is used on a simple Casio calculator or any calculator in general ???,43,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qx827n/what_sort_of_memory_is_used_on_a_simple_casio/,4,,1637295608.0
qx22or,Are LeetCode questions poorly grouped in their difficulty tiers?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qx22or/are_leetcode_questions_poorly_grouped_in_their/,3,"I have done my fair share of LeetCode and cannot help but notice that ""easy"" questions are sometimes quite difficult. Medium questions are generally more difficult but many of them just use slightly more advanced data structures and the actual concept behind the question is very straightforward. 

I had never even attempted a hard question since many mediums rack my brain for an hour or two, most of the time I need to look solutions up because there is a new approach to learn. I tried my first ""hard"" question the other day and I swear it was mis-labeled and could definitely be placed in ""easy""

I just solved this graph-ish question and it was labeled as easy but I had to use an iterative breadth-first search approach? That seems slightly more advanced than inverting a binary tree or reversing a string.

The hard question I am referencing is [Median of Two Sorted Arrays](https://leetcode.com/problems/median-of-two-sorted-arrays/).

This was my solution for the ""easy"" question, IMO far more complex than the median of two arrays.

    class Solution {
        public int[][] floodFill(int[][] image, int sr, int sc, int newColor) {
            int row = image.length, col = image[0].length; // STORE GRAPH BOUNDS
            int target = image[sr][sc];                    // STORE VALUE WE ARE AFTER
            Queue<int[]> traverse = new LinkedList<>();
            boolean[][] visited = new boolean[row][col];   // VISITED NODES
            int[][] directions = new int[][]{
                {0,1},{1,0},{0,-1},{-1,0}                  // TRAVERSAL DIRECTIONS
            };       
            int[] tmp = {sr,sc};
            traverse.offer(tmp);                           // PUT START NODE IN TRAVERSAL QUEUE
            
            while(traverse.size()>0){
                int[] node = traverse.poll();              // GRAB NEXT NODE
                int m = node[0], n = node[1];
                visited[m][n] = true;                      // VISITED = TRUE
                image[m][n] = -1;                          // SET VALUE TO CHANGE COLOR
                
                for(int[] dir : directions){
                    int a=m+dir[0]; int b=n+dir[1];
                    
                    if(   a<0 || a>=row
                       || b<0 || b>=col
                       || visited[a][b]==true
                       || image[a][b] != target )          // DISQUALIFY NODE FOR TRAVERSAL
                        continue;
                    else{
                        int[] add = {a,b};
                        traverse.offer(add);              // SATISFIES CONDITIONS PUT IN TRAVERSE QUEUE
                    }
                }
            }
            
            for(int i = 0; i < image.length; i++){
                for(int j = 0; j < image[0].length; j++){
                    if(image[i][j]==-1)
                        image[i][j]=newColor;
                }
            }
            
            return image;
        }
    }",1637276565.0
qwtqeb,RNG algorithm that just flips bits randomly,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/qwtqeb/rng_algorithm_that_just_flips_bits_randomly/,8,"I was thinking about RNG algorithms. And most of them are pretty complicated, using math. Middle-square, Mersenne Twister, etc.

But has anybody come up with an algorithm that just randomly flips binary bits of a variable you want to randomize? What would be the advantages and drawbacks of this method? I guess it would depend on the implementation. 

Anyway, I just wanted to get opinions of people smarter than me.",1637253610.0
qwqv67,Can someone help me understand the computer science of what exactly is a database?,3,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qwqv67/can_someone_help_me_understand_the_computer/,9,"So this topic that might be a little closer to computer science, but could someone explain to me what happens on a computer/server when a database is created? Like a high-level overview of the computer science behind the creation of a database and it's properties(how to interact with it)? 

Secondly, if you could also briefly talk about the computer science of database management systems?

Please, also let me know if I'm thinking about this incorrectly. I am a self taught person with a lot of black boxes.",1637245557.0
qwnv1m,Deadlock vs Deadly Embrace,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qwnv1m/deadlock_vs_deadly_embrace/,8,What's the difference between a deadlock and deadly embrace in concurrent programming?,1637235350.0
qwegv6,IDE Features - Statically vs. Dynamically Typed Languages,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qwegv6/ide_features_statically_vs_dynamically_typed/,4,"I am trying to find what additional features IDEs provide for statically typed languages that it doesn't for dynamically typed ones for a project. I've tried researching it online, but couldn't come up with anything clear. So, I would appreciate if someone could help with this.",1637199526.0
qw8yg4,A tech journalist just wrote a small article about a project I did. I'm geeked to the max.,72,0.97,computerscience,https://www.hackster.io/news/tyler-jacobs-logic-gate-display-is-a-desk-toy-for-learning-the-basic-building-blocks-of-computing-38ed6ae6cdb6,10,,1637183137.0
qw8tom,"Is possible too have static typed, interpreted languea",0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qw8tom/is_possible_too_have_static_typed_interpreted/,2,"Type checking insted of hints, and force you to use types",1637182707.0
qw5zd4,How to read the optimal makespan,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qw5zd4/how_to_read_the_optimal_makespan/,2,"Hello I'm having trouble reading the optimal makespan of job scheduling algorithm. In particular, what does it mean for max to have index i here?

https://preview.redd.it/pir2dkccb7081.png?width=988&format=png&auto=webp&s=05a4c7ad5b716ad7d71419862a04dd142512bd69",1637174685.0
qw5g3c,Are there any good websites with quizzes where you can check your knowledge on a certain topic and see if you are ready for exams?,89,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qw5g3c/are_there_any_good_websites_with_quizzes_where/,10,"Currently studying Object Oriented Programming which is a course of the 3rd semester in my uni. Im pretty sure I have a good understanding of the concepts but I'd like to confirm that somewhere somehow. There are many times where I wish I had something like that in courses like data bases, networks, data structures. Any help is appreciated!",1637173180.0
qvrimr,What is a DAG (Directed Acyclic Graph) and how are they represented in memory,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/qvrimr/what_is_a_dag_directed_acyclic_graph_and_how_are/,8,"I have an interview tomorrow. 

This question was recently asked to one of my friend who interviewed at the same company. 

I searched really hard but couldn't find the answer to the latter part of the question. 


Even for the first part, I need a simple answer and not a wiki kinda definition. 

Any help is appreciated.",1637125263.0
qvi861,What is the different between multi-tasking and parallel processing?,31,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qvi861/what_is_the_different_between_multitasking_and/,8,"Hi,

While revising for my mock, I have unexpectedly became confused about the difference between multi-tasking and parallel processing.

At first, I though multi-tasking is the simultaneous processing of multiple tasks, while parallel processing is the simultaneous processing of the same task but broken down in chunks.

However, after reading about SIMD and MIMD, I realised that parallel processing could also process multiple task at the same time, just like multi-tasking.

Thanks in advance :)",1637097069.0
qvhov8,Bit Manipulations You Like,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qvhov8/bit_manipulations_you_like/,8,"Bit manipulations is one of the most interesting subjects I ever studied. It really blows my mind.

I'm wondering if you use those occasionally, and if so what are your favorite tricks?",1637095647.0
qvgwfj,What's the most difficult part of writing documentation for you guys?,74,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qvgwfj/whats_the_most_difficult_part_of_writing/,23,"Recently started working on a large project which requires I document my code.

Feel free to share which part about writing documentation (writing comments, official documentation, things like that) you find the most annoying or difficult.",1637093532.0
qvgo6i,How does the communication between devices with cables work?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qvgo6i/how_does_the_communication_between_devices_with/,2,"I would like to get a Raspberry Pi in the near future and a receiver and transmitter for electromagnetic waves. I have to connect the receiver and the transmitter to the Raspberry Pi with cables, but now I'm wondering how exactly this works, for example how the transmitter knows what data I want to send?",1637092938.0
qvcvqa,"Books on bios, uefi, smbios and acpi?",17,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qvcvqa/books_on_bios_uefi_smbios_and_acpi/,4,"I would like to learn a bit more about firmwares and their interactions with operating systems, especially on everyday computers.

After reading, I would like to have a good overview of what types of firmwares are common on the market, how they interact with with the OS and how they are structured. Ideally without reading all the specs, I just want go get an overview at first :) 

Any recommendations?",1637083224.0
qv9yun,Does anyone have tips for understanding code?,16,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qv9yun/does_anyone_have_tips_for_understanding_code/,11,I just started studying computer science and have trouble understanding code. We've started with java and python and even though I have some experience with java I still have a hard time reading and understanding the code. Every time I look for tutorials they always go directly into coding and creating an algorithm (which is understandable - learning by doing) but I'd like to understand how to learn how to read and understand an algorithm first. Does anyone know a good platform on how to learn exactly that?,1637075641.0
qv9g3m,Can't find recent research paper over data structure,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qv9g3m/cant_find_recent_research_paper_over_data/,2,"Hi everybody, for my enflish class, I need to read 7 articles over data structure (it was imposed to me) with a restriction of 500/600 words and  should be published after may 2021. I don't know where to search, the most part of articles that I found on internet is over neural structure in the brain or is too old or too long (+-4500 words). Thank you for reading.",1637074201.0
qv3ej2,quantum computing,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qv3ej2/quantum_computing/,0,"Watching the xponential rise of technological development, I havent heard about quantum computing for quite a while. What is the progress atm? And are just the big companies working on it? Wondering if there are small startups as well?",1637052222.0
quwl9r,"Why do applications always download so fast in the start, say the first 99%, and the last 1% take so long?",0,0.44,computerscience,https://www.reddit.com/r/computerscience/comments/quwl9r/why_do_applications_always_download_so_fast_in/,2,,1637028143.0
qunqn2,"50 Years Ago Today (November 15, 1971) Intel releases the world's first microprocessor, the Intel 4004.",711,0.99,computerscience,https://i.redd.it/opv4njiy4tz71.jpg,14,,1637003033.0
qulcc1,Display and Keyboard (DSKY) used on the apollo spacecrafts,7,0.82,computerscience,https://i.redd.it/2q09n3wslsz71.jpg,0,,1636996589.0
quk6my,what’s the difference?,0,0.29,computerscience,https://www.reddit.com/r/computerscience/comments/quk6my/whats_the_difference/,3,"are there any differences between computer science, programming, software engineering, and software developing? i’ve been using google and doing my own research, but each website i look at gives me a different answer and leaves me more confused",1636993510.0
quec2x,Can you recomend papers about reinforcement learning applied to games?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/quec2x/can_you_recomend_papers_about_reinforcement/,1,Can you recomend papers about reinforcement learning applied to games?,1636975603.0
qu8lzi,Good resources to learn about time complexity?,6,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/qu8lzi/good_resources_to_learn_about_time_complexity/,2,Does anyone have a good resource to learn about time complexity? I am struggling with the material and can't wrap my hear around anything besides O(1) and O(n).,1636952514.0
qu4c71,I've always loved the concept of instant learning and I want to study it more.,6,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/qu4c71/ive_always_loved_the_concept_of_instant_learning/,8,"One of the things that fascinated me the most in the Cyberpunk genre, was the ability to have instant learning, such as when Neo learns how to fight in Matrix, or when in Neuromancer you had those chips in the back of your ear, that made you a instant fluent Chinese speaker. Knowing the existence of biocomputing, and those biomechanical arms that responds to brain impulses, I started asking myself if there wouldn't be any papers that could help me grasp an idea of real life applications between computing, the human body and the learning process... Is there any book or paper recommendation where I could follow?",1636938467.0
qu426c,Why is LaTeX still used academically?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qu426c/why_is_latex_still_used_academically/,15,"I can understand the usage of it for the time it was created, but surely in recent years with the introduction of many other word processors IE Word and Gdocs, it would be made redundant? The inclusion of both equation and formatting tools in Gdocs is really good, so why do academics still use LaTeX?",1636937585.0
qttk9c,Early programmers were reluctant to switch from binary programming to an assembly language,75,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qttk9c/early_programmers_were_reluctant_to_switch_from/,14,"I'm currently reading ""The Art of Doing Science and Engineering: Learning to Learn"" by Richard Hamming. It's a great book about computer history and programming in general.

One thing that strikes me is the omnipresent status quo bias.  Early programmers were reluctant to switch from binary programming to an assembly language:

https://preview.redd.it/vos58v5f9lz71.jpg?width=2787&format=pjpg&auto=webp&s=a555d1663ead77991c4e2f1cceeb825d8caa774d

The same happened during the transition from assembly to FORTRAN, one of the first high-level, compiled programming languages:

https://preview.redd.it/gkxkw4ji9lz71.jpg?width=2910&format=pjpg&auto=webp&s=7a76bc3ea7597597d8fc9cc562785a5e3c28b3b8

Have you experienced a similar thing when new frameworks, tools, or languages were introduced?

&#x200B;

>Source: [https://twitter.com/krebs\_adrian/status/1459909375427059715](https://twitter.com/krebs_adrian/status/1459909375427059715)

&#x200B;",1636907791.0
qtrrml,My Programming prof is an absolute Chad,513,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qtrrml/my_programming_prof_is_an_absolute_chad/,96,"He programs everything using the text editor and he doesn't even prepare the code before the lectures. He just quickly programs it all on the spot in front of us. And not just a few lines but often around 20 for each example. And even if a student asks him to show a very specific example that guy just immediately knows what to do and starts typing without mistakes. He also opens every document and application over the terminal instead of clicking on it. 

&#x200B;

He honestly inspires me to actually take my assignments seriously. It seems like he always immediately knows the solution to any coding task in his head. That guy must be a genius.",1636902666.0
qtl1ir,Approximating T(n-2) and T(n-1),2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qtl1ir/approximating_tn2_and_tn1/,1,"Based on what I've read to get the closed form of the Recursive Fibonacci Algorithm, you have to approximate `T(n-2)` to `T(n-1)` from `T(n) = T(n-2) + T(n-1) + c` and vice versa to eventually reach O(2^(n)).

The thing that's confusing me is **why** we should approximate it. The only thing I know so far is that T(n-2) will take lesser time than T(n-1), but I'm just not sure of the mathematical explanation behind approximating both times to make them somewhat equal.",1636876550.0
qtkimi,"Best books to understand Computer Architecture, threads, concurrency, processes and everything else that goes under the hood",2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qtkimi/best_books_to_understand_computer_architecture/,5,"Hi!   
I'm looking get a strong grip on the inner workings of CPUs, threading, concurrency, instructions , SIMDs and etc..  


Right now I have a shallow understanding of the matter, Im looking for a new intro book into the whole thing, nothing very very technical - as I am starting to program with RUST and get to grips with Assembly language.  


Basically, how does programming gets executed on the physical level, and how it all comes together to create these fascinating machines. Thank you!",1636874256.0
qtg8ok,Is there a formula to calculate the magnitude of precision loss with respect to the magnitude of a number presented by a float?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qtg8ok/is_there_a_formula_to_calculate_the_magnitude_of/,0,"We know that a float will lose precision as it's value increases. But is there a way to calculate when floats will be come, for example, at least one whole unit off? 

For example this algorithm would tell me that numbers of magnitude of 10\^9 will likely be off by a magnitude of 10\^-2.",1636857763.0
qtg70j,"I heard that if the first digit of a hexidecimal number is 8,9,A,B,C,D,E or F (and it’s a multiple of 4) then it’s negative. Is this true? And if so, why?",1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qtg70j/i_heard_that_if_the_first_digit_of_a_hexidecimal/,10,,1636857609.0
qtet0i,What does K mean in O(nk) or O(n+k),1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qtet0i/what_does_k_mean_in_onk_or_onk/,2,"**So for the context:**

If i'm assuming right the ""n"" stands for input or size of the array and when one says O(n) it means the statment will take ""**n""** number of times to complete it....

but what does ""**k""** mean?

[here](https://stackoverflow.com/q/27301287/5630533) op said this ⬇️

&#x200B;

[https:\/\/stackoverflow.com\/q\/27301287\/5630533](https://preview.redd.it/jmrx4r07ogz71.png?width=1272&format=png&auto=webp&s=fdc3bd65cf25418a5eaa893c1f28f4fcf0ffd352)

Does that mean ""**k""** is just an another (secondary) notation for ""**n""**

&#x200B;

&#x200B;

[sorry, couldn't find the original link](https://preview.redd.it/0kxd4sr6mgz71.jpg?width=1010&format=pjpg&auto=webp&s=16283f1b1acb8e318c710fae010e53fccac5aebc)

&#x200B;

Referencing the above table, how much would be the difference between cubic and polynomial if n is 10 and k is 0(can we even assume 0 for ""**k**"" value)?",1636852787.0
qtapfy,Any goods websites/newsletters for IT/CS news and research papers?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qtapfy/any_goods_websitesnewsletters_for_itcs_news_and/,1,"As the title suggests, as an undergraduate CS student, I'd like to really improve my knowledge on the ongoing research, hardware improvement, Linux news, security, etc. by reading more articles. I was wondering if some fellow redditors had anything to share?

I may also consider creating an RSS stream.",1636839302.0
qt4qqy,(1964) Engineer Karen Leadlay working on the analog computers in the space division of General Dynamics,587,0.99,computerscience,https://i.redd.it/c1lkr0t5s8z71.jpg,26,,1636821044.0
qt3e8y,How do computers know which binary values represent which numbers?,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qt3e8y/how_do_computers_know_which_binary_values/,11,"In computer science, we learn that computers only understand base 2, essentially binary. But how do computers actually know which numbers are which? For example, the number 00000011. How does the computer know the the 1st '1'(from the right to left) represents 1(in denary), and the second '1' represents 2, which adds up to 3? I'm hoping for a low level explanation. Thank you!",1636816896.0
qsn40p,What is the difference between synchronous and asynchronous sequential circuits?,27,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/qsn40p/what_is_the_difference_between_synchronous_and/,3,"Hi everyone! First of all, I'm sorry if this post can be a little bit confusing, I'm trying to gather all I have understood so far to make it as clear as possible.

So, I am attending a Logical Networks class, and we started some days ago talking about sequential circuits. Today, our professor introduced us to synchronous and asynchronous circuits, but not in the same way I found online.

He introduced them by saying this:
Basically an asynchronous circuit is a circuit that whatever the state and the input (q1,I) you consider, will always end up in a steady state (qn, I), where if you keep I as an input you'll keep staying in the qn state.

And, contrary to this, he told us that synchronous circuits are circuits where not all states and inputs have this property.

So what does that specifically mean? I struggle to understand the concept, as some of the example he gave us for synchronous circuits (one of them was a machine that was able to recognise if there was a 101 sequence in the input) seemed to have the steady state property I talked about before, so I think I might be missing something.

That said, he then explained us that for that reason (not every state with an input I will end up in a steady state) we need an external synching (a clock) and a memory to make a state steady. Why is that? And why don't we need that for asynchronous circuits?

Lastly, he talked about how some of these machines need just an impulse or a longer signal to work (and for example an asynchronous machine cannot recognise a '00' input as it operates when an input changes, so it can only tell that the last input was a (for example) 1 and this one is a 0). How do you tell that? 

Hope this was clear, thank you for everyone who chooses to spend their time to help me understand :)

Oh and, don't take my words for something so accurate of what our professor told us, since I am trying to rephrase these things from what i have understood.",1636757693.0
qsis3v,"How are supercomputers utilized and how does one present a usecase for the machine(not that I want to, just curious)?",17,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qsis3v/how_are_supercomputers_utilized_and_how_does_one/,4,"I've been in a development capacity at my job for about 4 years. I work on the bulk of the SQL and c# interactions on our flagship products. So I have a pretty good understanding of what a supercomputer is and what they are good at. (Not saying my job gives me that knowledge, just saying I work in the industry and pay pretty close attention to these topics for fun) So, what I'm most curious about are the NVIDIA, INTEL, IBM, etc companies that build these things and how they decide what application the machine is used for. Like radio telescopes, for example, I know universities, companies, really any organization can apply for dedicated time with the telescopes. Is it similar to that? Can someone submit a proof of concept to these corporations and then get approved or denied? Or do all projects that use the computers just come from their internal teams? I recently saw a future growth model of earth coming out NVIDIA's supercomputer. It showed weather patterns, climate shifts, all sorts of really cool things. So I just kind of got to thinking about the origins of these projects and the process. I checked the rules out and I think this is an okay question for this sub, if not, I can post it elsewhere. Thanks!",1636745237.0
qsg140,Are personal IP addresses stored on DNS?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qsg140/are_personal_ip_addresses_stored_on_dns/,4,"I have a very superficial understanding of how DNS works. If you type ""[google.com](https://google.com)"" into the browser input, the computer asks a server in the DNS for google's IP address, the server either answers or asks another server for help, and eventually tells your computer where to go. 

So in my mind, these servers have a sort of ledger like ""[google.com](https://google.com) : [216.58.223.255](https://216.58.223.255) "" 

But what about my own public IP, for example? Is that stored in there as well? If so, what information is associated with it? In other words, how the server be asked to look it up?",1636737568.0
qs8gkb,How do you read your books?,12,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qs8gkb/how_do_you_read_your_books/,12,"Cs has a lot of great material, so i thought of figuring out a strategy to learn it better, do you read a book like a novel for the first time, and then take notes? How often do you repeat your material etc",1636713262.0
qs8dso,What’s the difference between programming and computer science?,87,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qs8dso/whats_the_difference_between_programming_and/,38,I’m going to take introductory classes at my uni and there’s two diff options,1636712906.0
qrlsrw,Everyone learning programming/computer science how do you deal with the fact that AI may be making you obsolete in a few years as you are busting your ass learning technologies and tools that may turn out to be almost useless?,0,0.17,computerscience,https://www.reddit.com/r/computerscience/comments/qrlsrw/everyone_learning_programmingcomputer_science_how/,28,"I’ve been self teaching over a year, typical front end shit, but I do a lot or reading from programmers about AI tech replacing the future of coding jobs-makes me almost wish I chose a field that is not stable like plumbing or health.",1636638817.0
qr8cpv,SWE influencers/pages you follow on social media?,0,0.31,computerscience,https://www.reddit.com/r/computerscience/comments/qr8cpv/swe_influencerspages_you_follow_on_social_media/,8,"Who are the some social media influencers/pages (could be a meme page even) for SWE? Or maybe some of your favorite influencers for SWE that you follow?

Could be IG/TikTok/Youtube/LinkedIn/etc.

Thanks!",1636590290.0
qqxc02,Can someone point me in the direction of some sort of algorithm that converts noisy analog data into binary,9,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qqxc02/can_someone_point_me_in_the_direction_of_some/,10,"I am working with a cheap EEG sensor that outputs attention levels. This data has some noise (fluctuations). I want to turn a LED on and off using this data. I am currently considering taking a mean of x values and if the mean is higher than a threshold, it's a 1 else 0. This is kind of naive and I was wondering if there are better algorithms out there for such problems?",1636559365.0
qqvz15,Information on a Conference,3,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/qqvz15/information_on_a_conference/,3,"Does anyone know if the SocProS conference is a respectable conference?
I know that the submitted papers are published by Springer.",1636555474.0
qqpsqd,A terrible schema from a clueless programmer,0,0.5,computerscience,https://rachelbythebay.com/w/2021/11/06/sql/,0,,1636533030.0
qqpf3o,What field of CS deals with cutting-edge research in creating better document formats like pdf?,55,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qqpf3o/what_field_of_cs_deals_with_cuttingedge_research/,19,"In particular, is there a journal analogous to ACM Transactions on Graphics (which publishes cutting-edge discoveries in the field of computer graphics), which publishes articles related to more portable, robust, efficient file formats for viewable documents like pdfs? 

Is there any chance new discoveries are made (e.g. more efficient algorithms to compress documents), or is this only a purview of companies like Adobe? I hope I'm making sense.",1636531399.0
qpu9g8,How do CPUs actually work ?,140,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qpu9g8/how_do_cpus_actually_work/,54,"I'm curious as to how CPUs actually communicate with the outside world.

I know it's electric currents flowing through transistors at the lowest level.

I know you can run compiled programs.

But where is the transition layer from one to the other ?

For example, Intel creates a CPU and I write a program for it, let's say in Assembly. I write the command: mov eax, \[ebx\] .

Who translates 'mov' into what it actually does and who knows how to interpret that command ? Is there a dictionary somewhere that says mov in binary is 0011010010011 (made up obv) and when you encounter that string of binary code you perform this function ?

Even more, how can this be hardwired onto a CPU ? Is mov a physical circuit that gets called whenever that command string is encountered ?

Sorry for the avalanche of questions but this particular topic is very hazy to me. I can't picture how a written command can direct electric current down in the CPU to do certain things.",1636426967.0
qpr8lq,Trying to read server memory bytes via postgresql vulnerability in Kali-Linux VM,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qpr8lq/trying_to_read_server_memory_bytes_via_postgresql/,1,"
Basically I found this vulnerability and I want to perform it on my system. I installed this virtual machine (64-bit for VirtualBox):    https://www.kali.org/get-kali/#kali-virtual-machines      and here is the vulnerability: https://security.snyk.io/vuln/SNYK-DEBIAN11-POSTGRESQL13-1292197         I’m really confused how is even do this, so far I know how to start postgresql and I can create a table in a database. If anyone could assist me in anyway I’d really appreciate it.",1636417260.0
qpl4wt,Impact of computational science in research?,21,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/qpl4wt/impact_of_computational_science_in_research/,13,Can anyone provide me with some examples of the impact computational science has had on science and research? How many advancements have been made with direct help from CS in thr past decade?,1636399760.0
qpjwrs,Question about the debate of how to end a string?,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qpjwrs/question_about_the_debate_of_how_to_end_a_string/,5,"I am an IT student, but [I'm watching this video](https://youtu.be/0fw5Cyh21TE) for fun and he explains the way developers can end a string (if that's the proper verbage). He explains a couple of different ways, the first being that the developer tells the computer how many characters they're looking for. He also explains that this is not efficient use of memory and then explains the second method, where developers would simply stick a \[NULL\] character at the end of the string. He also says that there are debates as to which method is better to use, but to someone with a pretty solid education with computers, I don't see why we don't just use a \[NULL\] character at the end of every string if it's so much more efficient than the alternative.",1636396422.0
qp5vod,"Can someone help better explain what’s going here? (From the book, “The Art of Computer Programming” by Donald Knuth",110,0.97,computerscience,https://i.redd.it/2dkaibv8tay71.jpg,16,,1636345321.0
qoyr6v,External version of RamDisk,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qoyr6v/external_version_of_ramdisk/,6,"I've been thinking about starting a project to repurpose old RAM from previously upgraded laptops and desktops. I'd like to see if I can create an enclosure for holding multiple RAM chips to be connected to a computer via USB or thunderbolt, and configuring them as an external disk. Using software similar or even the very same as this below. [http://www.radeonramdisk.com/software_downloads.php](http://www.radeonramdisk.com/software_downloads.php)

Can anyone point out the burdens and obstacles that I may have to cope with and overcome in order to make a idea like this a reality. 

Note: If your going to tell me this isn't possible please don't bother, I need constructive ideas here only, the fact that this software exist is proof that a ram disk is possible, I just want to make it external.",1636321907.0
qosr3u,Standardized Methods for Monte Carlo Markov Chain (MCMC) Sampling Using Computer Language,13,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qosr3u/standardized_methods_for_monte_carlo_markov_chain/,0,"Hello!

I have been trying to research standard ways to perform sampling (MCMC) high density regions of multidimensional probability distributions using computer programs. As far as I can find, there do not seem to be any standardized ways of performing this sampling - and often requires the user to translate the mathematical algorithm into computer code.

I tried to attempt the implement this algorithm over : [https://stackoverflow.com/questions/69849254/r-standard-method-for-mcmc-monte-carlo-markov-chain-sampling](https://stackoverflow.com/questions/69849254/r-standard-method-for-mcmc-monte-carlo-markov-chain-sampling)

Can someone please take a look at it and let me know if this is correct?

Thanks!",1636304620.0
qoqmin,"China's New Quantum Computer Has 1 Million Times the Power of Google's, the world fastest.",0,0.44,computerscience,https://interestingengineering.com/chinas-new-quantum-computer-has-1-million-times-the-power-of-googles,4,,1636298416.0
qoqb6q,Non-computable processes or functions,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qoqb6q/noncomputable_processes_or_functions/,3,"Are there any resources on what are non-computable processes or functions and how do they look like. 

I am trying to search for them, but more often than not, something on the topic of Consciousness or Roger Penrose comes up.

I want to understand what is not a computation and I'm pretty sure there is a huge amount of research on such a topic.

Thanks.",1636297414.0
qon555,Computer Science in pop culture,21,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/qon555/computer_science_in_pop_culture/,12,"Hello, people. English is not my first language, so, sorry for any grammatical mistake.

&#x200B;

Have you ever saw any tv show, or movie, or video game, with any direct reference to a subject from computer science?

&#x200B;

I see a lot of references to Artificial Intelligence and Hacking, but few to other areas of CS. I'm curious to see if there are references to other areas as well.",1636286480.0
qoj64n,Can the internet exist without wired data transfer?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qoj64n/can_the_internet_exist_without_wired_data_transfer/,5,"I know how Bluetooth works. It is completely wireless. In the case of the internet, let's say we are talking about Google servers. They have an IP address and some information stored in them. My PC has an IP address. Can it somehow get the information from the Google servers without any wired communication between intermediary devices?",1636268103.0
qoj0r4,Is it possible to encrypt an SSD/HDD/USB or even a whole PC based on geographical location?,17,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/qoj0r4/is_it_possible_to_encrypt_an_ssdhddusb_or_even_a/,10,"So for instance if I wanted to encrypt one of the above or multiple based on a specific location so it would only be unlocked if the password was entered while at a specific location such as let’s say altitude, so the PC would have to be at a certain height to then be able to be unlocked with the password, is this possible?

Because if it is, this would mean that essentially you can completely insure that even if they got the password right it still would not work, because they are not at the location which would be required. 

I’ve tried to find out by doing a quick internet search but no results show. 

Any information would be helpful. 

(This is just for my personal curiosity.)",1636267434.0
qoifaz,Why do we initialize the table to 0 in the knapsack problem at the start?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qoifaz/why_do_we_initialize_the_table_to_0_in_the/,2,"Video: https://youtu.be/PLJHuErj-Tw
Why do we initialize the table to 0 in the knapsack problem at the start?",1636264749.0
qoh47t,Theoretical Proofs for Evolutionary Computing,3,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/qoh47t/theoretical_proofs_for_evolutionary_computing/,2,"Has anyone ever heard of ""evolutionary computing"" and ""genetic algorithms""? They are computer algorithms used in optimization that were inspired by phenomena occurring in nature.

I posted a detailed question on stackoverflow relating to their theoretical justifications : [https://math.stackexchange.com/questions/4295279/does-the-following-computer-science-optimization-theorem-have-a-proof](https://math.stackexchange.com/questions/4295279/does-the-following-computer-science-optimization-theorem-have-a-proof)

Can someone please take a look at it? 

Thanks!",1636259294.0
qo5xff,What algorithm could I use to find optimal selection of sets to maximize unique elements,41,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qo5xff/what_algorithm_could_i_use_to_find_optimal/,31,"Hello there!

Could someone tell me which algorithm should I use in order to do the following:

I have X groups with different number of elements in them and I want to find the best combination of n chosen groups in order to maximize the number of unique elements (alternative version maybe even choosing which elements).

Should I use some optimization algorithm or something or is it easier than that?

Thanks in advance!

**Edit (to add maybe a more accurate description of what I'm searching for):**

Parameters:

I) collection of sets

II) n - size of group to consider

III) (Optional) - list of elements

Output: which n sets from collection should be selected in order to maximize unique elements (if parameter III is given, consider only elements from that list) considering all sets from collection.

**An interesting case I think worth mentioning** (made me wonder it might be optimization problem and not a brute force that would check for set with more unique elements and go next for  set with more unique elements disconsidering previous added):

Set 1: {A,B,C,D,E,F}

Set 2: {A,B,C,G,H}

Set 3: {D,E,F,I,J}

for n = 2, optimal collection here would be sets 2 and 3.",1636222460.0
qnl3xe,How does the ML research done in Stats departments differ from the ML research done in CS departments? What areas are empathized more/less in either one?[D],52,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qnl3xe/how_does_the_ml_research_done_in_stats/,6,,1636147706.0
qnj83v,"Is ""memoization"" also called ""memorization"" in computer programming?",47,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/qnj83v/is_memoization_also_called_memorization_in/,25,"Is ""memoization"" also called ""memorization"" in computer programming?",1636142182.0
qnduwn,Is embedded DRAM the future?,40,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qnduwn/is_embedded_dram_the_future/,10,"With the Applr M1 processor using integrated DRAM on the SoC, with affords faster memory access, do you think this will be adopted by other cpu companies? Is it likely x86 cpus will be manufactured with embedded dram in the next few years?",1636126949.0
qna9yb,computer engineering vs computer science,10,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/qna9yb/computer_engineering_vs_computer_science/,63,Most of people will become engineers tbh. Few people will invent a new algorithm or prove a computing theory. Most people will deal with writing simple CRUD code in certain business. Why do we keep calling CS not CE?,1636116142.0
qn6kqf,What happens when you open a connection?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qn6kqf/what_happens_when_you_open_a_connection/,4,"I’m curious what actually happens when I open a connection from a client to something like a backend or database. Like, what is a connection actually? I’ve been struggling finding out how to understand the nitty gritty details of how a connection actually works. Thanks",1636101113.0
qn56a6,"Question in regards to stored memory. Was curious about this for a while, if I were to copy paste a photo or just a folder or a notepad document, when I make a duplicate, is it the exact same data down to the T with every 0 and 1 being the same? Are they truly identical in this manner?",25,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qn56a6/question_in_regards_to_stored_memory_was_curious/,13,,1636094567.0
qmvvyw,Whats the highest level programming language you can program an operating system in?,18,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qmvvyw/whats_the_highest_level_programming_language_you/,12,"Is the go-to language C? or has it been upped? and do you think this will change in the future? or does it even matter what language is used? Can we build an OS from Python?

I guess I'm asking programming languages that best communicate with modern hardware architectures",1636063345.0
qmuxrz,Do all compilers output the same machine language?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/qmuxrz/do_all_compilers_output_the_same_machine_language/,10,"Are all machine laguages the same on a single computer architecture even when written in different programming languages?

Like is the instruction of if (x==2) then print(x) the same machine language for C and python?",1636060799.0
qmtz3q,OS Installation Sequence,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/qmtz3q/os_installation_sequence/,1,"When you install an operating system from a USB. Am I correct that it installs it from the USB then copies itself over to the harddrive? Or does it copy itself to the harddrive first then install the operating system from there?

Second question is, when the OS is copies over to the harddrive is that piece of hardware essentially your whole computer? Like if I swapped out two laptops harddrives from one to another, would they both just boot up as if they were switched?",1636058232.0
qmil8h,"Been reading Computer organization and design, can't understand a sentence",6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qmil8h/been_reading_computer_organization_and_design/,2,"As the title says, i've read some chapters of a book called **Computer Organization and Design: The Hardware/Software Interface,** finished the part about variables, registers and memory interactions. But this sentence completely staggers me, can someone elaborate/explain(english is my 2nd language  )  


The sentence:  


**Many times a program will use a constant in an operation—for example, incrementing an index to point to the next element of an array**",1636025659.0
qm9wc6,Travelling Salesman of world map visualisation,86,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/qm9wc6/travelling_salesman_of_world_map_visualisation/,13,"&#x200B;

[This uses the simulated annealing algorithm - source code in comments](https://i.redd.it/9fjlbchahhx71.gif)",1635990296.0
qm0yhw,How are emails sent if they are stored as hashes in a database?,23,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/qm0yhw/how_are_emails_sent_if_they_are_stored_as_hashes/,16,"Let's say I have a newsletter people can sign up to. When a user signs up, I hash their email and store it in the database. What I don't understand is how I would then use the hash (I no longer have access to the actual email as I've hashed it) to send my newsletter to them.

Also sorry if this is the wrong sub, I'm not entirely sure where else to post this. Thanks!",1635964423.0
qm0bpt,Thoughts on analog AI accelerators?,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qm0bpt/thoughts_on_analog_ai_accelerators/,4,"It's almost time for me to select a master's program within Computer Engineering/Computer Science. I want to do something valuable and so I've been thinking of going into high performance computing and doing research on applying analog computing to AI hardware. 

Does anyone have any similar experiences or thoughts on the subject?

Edit: For those that have never heard about the concept before, [here's](https://www.youtube.com/watch?v=owe9cPEdm7k) a good introductory video",1635962672.0
qlzztd,transfer protocol rdt2.1 related question,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qlzztd/transfer_protocol_rdt21_related_question/,5,"Does 1 sequence bit really suffice for a reliable transfer?  
Are there still no scenarios of endless loops or corrupted( and accepted ) packages?  
I'm confused because my intuition tells me it reduces the risk of failure but does not eliminate it.

Edit: Adding reference images.

https://preview.redd.it/z6pubzgr9fx71.png?width=1205&format=png&auto=webp&s=6d4ca9f2d379d36304471d96cbd78b4e15b36bc9

&#x200B;

https://preview.redd.it/re3n52iu9fx71.png?width=1200&format=png&auto=webp&s=cd4941c5aa72cd683339176285d23a1f96aedfbd",1635961753.0
qlvaqy,How does one go from working on console programs to making things like apps/games/etc?,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/qlvaqy/how_does_one_go_from_working_on_console_programs/,7,I am a second year CS major and the language I have the most experience with is C++. How do I begin making stuff with visuals? Are there any online courses that can help me?,1635948582.0
qlj8jr,Looking for an obscure language.,11,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/qlj8jr/looking_for_an_obscure_language/,1,"A few years ago I worked on a project with a friend of mine and I remember we used a language based off futurama, I believe the language was some kind of fork of Ruby. Not sure though. Can't seem to find it anywhere, anyone know what that could be? Thanks! (It was a web app if that is of any use!)",1635901899.0
ql9rw5,CLRS,39,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/ql9rw5/clrs/,16,"Any tips about needed math or study methods that can help me to finish this book faster, and get as much as possible of the content? And for those who've already read, what is your opinion about the book?

https://preview.redd.it/oeq7bem5y7x71.jpg?width=1536&format=pjpg&auto=webp&s=95f151988914e7e222476c7d7e76e6882760d47d",1635875109.0
qkkwwt,Why is the following code O(n)?,45,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qkkwwt/why_is_the_following_code_on/,23,"    def f(n):
        c = 0
        for i in range(n):
            j = i 
            while j < n:
                  j = 2*j
                  c = c + 1

 

My professor tells me this is O(n), but his explanation did not make sense to me. The way I see it, the complexity boils down to log(n) + log(n-1) + log(n-2) + ... log(n-n) and I don't see how that is asymptotically n rather than nlog(n).",1635792943.0
qkc8u6,Build an 8-bit retro computer powered by a Z80 !,95,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/qkc8u6/build_an_8bit_retro_computer_powered_by_a_z80/,10,"Hello computer sciences lovers,

Let me join your community by presenting to you a project I've always wanted to make (or simply have) since my young age but never tried until now. I am currently designing an **8-bit computer** with Zilog **Z80 CPU**. **It is not an emulation, and not powered by MCU**. I hope that it will be both entertaining and educational: ideally it would be used to learn **hardware, programming in assembly, Basic and even play 2D games** etc.

[Zeal 8 bit computer prototype](https://preview.redd.it/aedi2s0xyyw71.jpg?width=1080&format=pjpg&auto=webp&s=13bafe0aa61808c9389aaaf5f6a41cecc72259f7)

**Now the key features already working:**

· Native OS fully written in Z80 assembly

· ROM and RAM support with banking (both internal and external)

· Support external extension card (for adding RAM, ROM, Flash, EEPROM, and so on...)

· PS/2 keyboard support (targeting a full 104-key keyboards support)

· 16 GPIO pins (some used by the system)

· Software I2C

· Software UART

· VGA graphics support (powered by an **FPGA**)

· Sound support (powered by the FPGA)

Here is my video showing the design process:

[https://www.youtube.com/watch?v=n\_eEDAQWMdY&t=25s](https://www.youtube.com/watch?v=n_eEDAQWMdY&t=25s)

Feel free to give me your feedback or remarks",1635767382.0
qk9vxt,Why Embedded Software Development is Harder,3,1.0,computerscience,https://beza1e1.tuxen.de/embedded.html,1,,1635757149.0
qk5g35,How do I get machine learning to be gamified and interesting?,4,0.65,computerscience,https://www.reddit.com/r/computerscience/comments/qk5g35/how_do_i_get_machine_learning_to_be_gamified_and/,1,Are there any websites like hackthebox for Machine learning which make it fun and challenging? I want to constantly do projects and get my hands dirty in it while having fun.,1635737407.0
qjz6f7,Any Really Good Computer Science or Coding Channels on YT?,147,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qjz6f7/any_really_good_computer_science_or_coding/,55,"Any good YouTube channels for new people learning coding and coding fundamentals. I watch lots of math videos on YT and if anyone where to recommend me for math channels I would say 1blue3brown, Veritasium (sometimes). I was wondering If anyone knows any good channels that doesn't sticky teach how to learn a certain langue step by step but more deep understandings and good advice that I will keep back in my head as I keep learning to code. Interesting topics as well, like those math channels. Thanks",1635715720.0
qjwzwd,Can someone help understand this algorithm?,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/qjwzwd/can_someone_help_understand_this_algorithm/,1," 

    M ← ∅        ▷ Initialize with an empty matching P ← getAugmentingPath(G,M) 
    while P is not empty do 
    M ← M ⊕ P 
    P ← getAugmentingPath(G, M)

 

This is an algorithm for maximum  matching. however, what I don't understand is that before the iteration  loop starts,

 what exactly is P getting intialized with? 

Since M is set  to nil and we are passing that as a parameter.

Also can someone give an example output of what kind of return value we expect from the getAugmentingPath function?

&#x200B;

For example:

&#x200B;

https://preview.redd.it/0ma28i369uw71.png?width=457&format=png&auto=webp&s=37067d91a4322c56a61fe503e397675ffa32ace2

What would be the output of the augmentingPath function in the first few iterations? Someone please help as I can not understand this at all.",1635709068.0
qjrkb9,Is compression just a way to express the same data in a much much simpler/cheaper way?,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/qjrkb9/is_compression_just_a_way_to_express_the_same/,3,Title says it all.,1635693250.0
qjntyw,Chinese Researchers Built a Quantum Supercomputer 10 Million Times Faster Than Previous Systems,3,0.64,computerscience,https://science-news.co/chinese-researchers-built-a-quantum-supercomputer-10-million-times-faster-than-previous-systems/,5,,1635680652.0
qjk3eb,What is this data structure called?,112,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/qjk3eb/what_is_this_data_structure_called/,35,"Imagine you have a List or Array like this:


`[1,1,1,1,9,9,9,9,9,5,5,5,6,6,7,7,7,7,7,7,7,0,0,0,12,12]`

Then it may be beneficial to display it like this, e.g. for compression purposes:

`[(1,4),(9,5),(5,3),(6,2),(7,7),(0,3),(12,2)]`

or even like this:

`[1,4,9,5,5,3,6,2,7,7,0,3,12,2]`

On the left side of the tuple you put the element on the right side how many times it occurs in a row. What is this structure called?",1635663569.0
qjk2pi,How do you even solve programs?,1,0.55,computerscience,https://www.reddit.com/r/computerscience/comments/qjk2pi/how_do_you_even_solve_programs/,3,"I have recently passed my algorithm course where I learned about 15 algorithms. 

My professor has suggested me to solve problems in online judges. Now that's where the problem begins.

When I was in the course, the processor would provide exercise related to a topic he had just taught. So every time exercises were given, I knew which algorithm it is relevant to. But now when I pick a random problem from an online judge I'm lost. I don't know what algorithm might it be relevant to, I can't write a single line.

I was kind of ahead of my class in some of the programming courses but now I'm losing interest in programming because I can't identify the algorithms needed to solve problems. Should I give up programming(which will be a disaster)? 

Any suggestion on getting out of this situation? Sometimes people say ""keep practicing"" but it's not helping. How can I even keep practicing if I have to look up the answer of every problem I pick?",1635663479.0
qj9666,Feel a bit incomplete on insertion sort,21,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qj9666/feel_a_bit_incomplete_on_insertion_sort/,11,"Was reading the CLRS book section regarding insertion sort, and it gives the following code for insertion sort:

    INSERTION-SORT(A)
    for j = 2 to A.length{
        key = A[j]
        i = j-1
        while i > 0 and A[i] > key{
            A[i+1] = A[i]
            i = i-1
        }
    A[i+1] = key
    }

CLRS gives the first loop invariant saying that A\[1..j-1\] are in sorted order.  But I'm not entirely convinced that's the case from a mathematical standpoint.  Intuitively looking at an example it is pretty clear that A\[1...j-1\] is in sorted order, but I'm wondering what could be the loop invariant for the inner while loop, since I'd assume its why the loop invariant CLRS gave holds.  Not the most useful question from a practical standpoint but I really like math and am curious what the loop invariant is for the inner loop.  Also not sure why CLRS would leave it out.

EDIT: formatting",1635623589.0
qiu1wc,Programming Languages are overrated,0,0.37,computerscience,https://www.reddit.com/r/computerscience/comments/qiu1wc/programming_languages_are_overrated/,12,"I have been thinking about this from a long-long time and thought of sharing this here. Programming Languages are given a lot of importance than they deserve in the programmer community. I have seen a lot of people flexing that they know x number of languages which is incredibly shameful. Most of the programmers(90%+) would always give programming languages a lot of importance. Programming languages do not matter much. 

The core concepts of programming matter a lot more than programming languages but still a lot of people want to learn more and more programming languages. Senior software engineers know this that programming languages don't matter as you can learn a programming language thoroughly well in a night if you know the concepts well. By concepts I mean all the things that are applied while making a product(e.g. a web app), if you know compiler design, it would be incredibly easy to grasp a language, that is where concepts pay off.

This problem's cause is bad platforms and bootcamps but I would not name them. They teach the programming language and nothing else. That does not pay off as you might get a freelance job for 10$/hour but you won't be able to land a good 100k+ job at a good company. And I won't even like to call you a programmer if you just know the programming language.

You actually get the taste of core concepts when you go to get a CS degree from a formal college. Yea, I hear you saying that ""Duh, I can't go to college"", then learn it yourself. I myself am 14, started to program seriously since 13 and I can't go to college too. So I started learning the concepts myself. I do not know a lot of programming languages and I just know JavaScript that too not to a great level but I have come to learn system design and databases pretty well and I can make a web app easily. 

Programming Languages are just a way to implementing our programming thoughts, it is just a tool. Just like english is a language to express our thoughts about philosophy, science, etc. Do tell me your thoughts about this, I would love to read them. I have been writing and encouraging this on other platforms to like YT and dev.to.",1635568580.0
qiq0jd,What software process model uses component-based development?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qiq0jd/what_software_process_model_uses_componentbased/,1,What software process model uses component-based development? What software process model can be used for developing a new software using some components that already exists?,1635553461.0
qidftd,Is there a most recursive function?,0,0.38,computerscience,https://www.reddit.com/r/computerscience/comments/qidftd/is_there_a_most_recursive_function/,5,And doesn't make sense to measure the recursivity of a function?,1635515916.0
qibyr4,Why the development of brand new operating systems has stagnated in the last 20 years?,114,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qibyr4/why_the_development_of_brand_new_operating/,59,"Almost every OS we use today was conceived and it's development started in the 80's or the 90's and since the 2000's no significant new OS's pop-ed up. Obviously the major OS's were developed and upgraded further while new technologies were incorporated in them, but yet again those OS's are based on 90's concepts and technologies. So why no brand new OS's were created since then? Were those OS's designed to be future-proof? For example was Linux/Unix so advanced that it could support every breakthrough in computer science with just minor updates ,or nowadays every company/organisation has figured out that it's not worth to write something new from scratch?",1635511404.0
qi8v4v,How did large & detailed games like God of War store game files in an 8 MB memory card in play station 2?,23,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/qi8v4v/how_did_large_detailed_games_like_god_of_war/,16,"Found my Play station 2 recently and remembered it had a 8 MB memory card for all my games. God of War used the most space in the memory card with 550 KB. How could such detailed game files could be stored in an 8 MB chip (other than the CD) but now everything needs 50-100 GB, for only slight improvement in graphics?",1635499469.0
qhohv1,AI consumes a lot of energy. Hackers could make it consume more.,19,0.85,computerscience,https://www.technologyreview.com/2021/05/06/1024654/ai-energy-hack-adversarial-attack/?utm_source=Facebook&utm_medium=tr_social&utm_campaign=site_visitor.unpaid.engagement&fbclid=IwAR2jjhiI2CfI2qT0-IAyulThAM2cr5PiZPy1DJRGNJ6EQFBjOKnNCx3ejrg,4,,1635431906.0
qhhz7s,Is there an order of operations for Boolean logic?,8,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/qhhz7s/is_there_an_order_of_operations_for_boolean_logic/,6,"Like how mathematicians have the convention that ""2 + 4 x 2"" means ""2 + (4 x 2)"" and not ""(2 + 4) x 2"". If I had ""A and B or C"" does that mean ""(A and B) or C"" or ""A and (B or C)""? Does it change with NOR, NAND, XOR, NOT etc?",1635408068.0
qhcmwq,Question About LinkedList,17,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/qhcmwq/question_about_linkedlist/,7,"Does the head node store a value, or only the address?",1635387514.0
qgzhy7,Processing Order of Operations,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qgzhy7/processing_order_of_operations/,2,"From an IDE’s perspective, does and expression with an equation, that requires order of operations, take longer to process than code that has the order written separately to its own defined register?
e=y’+(y’-y)+x’+(x’-x),
Vs.
ey=y’-y,
ex=x’-x,
e=y’+ey+x’+ex

E: Carriage return did not function; insert commas.",1635349133.0
qgoxsp,I have a question about predictive text engines,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/qgoxsp/i_have_a_question_about_predictive_text_engines/,5,"What exactly are they? I know that they're AI's that take information and create new text similar to it, but if I wanted to code one in Python, how would I do it? Some help would be appreciated.",1635310700.0
qg1ucj,Post Office Horizon Scandal - Computerphile,0,0.5,computerscience,https://www.youtube.com/watch?v=hBJm9ZYqL10,0,,1635236773.0
qgc7oj,First Search Algorithm,13,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qgc7oj/first_search_algorithm/,6,Does anyone know the name of the first break-through search algorithm for classical computing was? I’ve done some research on it but nothing seems very definite. Any ideas?,1635271256.0
qgbxhk,Minimal motivation on partly known studying topics,36,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/qgbxhk/minimal_motivation_on_partly_known_studying_topics/,9,"As an computer science student with an programming apprenticeship done, some topics seem very hard to get motivated on, since I know half of the lectures but am not an Expert. 

Where does one take the motivation from to do for example Javascript / Html exercises, when you already worked 4 years in Web development?",1635270486.0
qg6bvz,Tesla Dojo Whitepaper,3,1.0,computerscience,https://tesla-cdn.thron.com/static/SBY4B9_tesla-dojo-technology_OPNZ0M.pdf?xseo=&response-content-disposition=inline%3Bfilename%3D%22tesla-dojo-technology.pdf%22,0,,1635254670.0
qg1nhc,Would an adaptive processor ever become a thing?,0,0.38,computerscience,https://www.reddit.com/r/computerscience/comments/qg1nhc/would_an_adaptive_processor_ever_become_a_thing/,2,"Ok, so, knowing that AI is a series of logic gates, I theorize that programming is going to go extinct, and be replaced with AI training.

Instead of writing a program, the program writes itself over time, based on your needs. The logic gates assemble themselves after getting a base neutral network from a trainer.

Building off of that theory, processors can be much more like the human brain, taking input data, and learning to route the output to the appropriate place, while learning what output is to be expected.

In this scenario, a single piece of hardware could theoretically become whatever it was needed to be, in the same way software defined radio works. The processor would be a hardware neural net, with variable values. External hardware would automatically train the net when it was plugged into a processor port.

Are there any known concepts like this?",1635235825.0
qfwfbn,Are power demands the main reason Raspberry Pi doesn’t incorporate a better CPU?,4,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/qfwfbn/are_power_demands_the_main_reason_raspberry_pi/,7,"Of course form factor is a big deal for the raspberry pi. Desktop CPUs really aren’t very big and for a slightly larger raspberry pi it could have it. The only restriction on that is the power demand. Newer CPUs consume 65W whereas the RPi4 consumes 4-5W. The heat dissipation required as a result of the power demand could be an issue without added cooling.

Is there another reason why a better CPU isn’t used in a raspberry pi?",1635214851.0
qfwbyw,what math do I study for AI,74,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/qfwbyw/what_math_do_i_study_for_ai/,21,"I haven't touched math in 12 years. I am starting from the bottom at basic algebra.

Can anyone recommend a path starting from algebra to be successful in the AI field?

I am not looking for a job in tech, I'm doing this for me because AI fascinates me. So time is no obstacle for me, I could spend the next 5 years learning all this math If have to. 

Thank you for any help yall can give!",1635214522.0
qftnw9,what does model and design mean in Computers science/software engineering?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qftnw9/what_does_model_and_design_mean_in_computers/,2,"I read this in some book, "" Instead of focusing first on programming, we can learn how to model and design. We can also learn how to use different modeling and design approaches together and in context to determine what a system will do and how it will do it. ""

What does model and design mean computer science and software engineering in general?",1635205690.0
qfdf0t,"Prof. Niklaus Wirth, 1984 ACM Turing Award Recipient",3,1.0,computerscience,https://www.youtube.com/watch?v=SUgrS_KbSI8,0,,1635156360.0
qfnew0,Personal Project Impediment,8,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/qfnew0/personal_project_impediment/,4,I have a lot of ideas for personal projects I would like to work on. The problem I keep running into is that I need data from various places that either don't offer it up or using their APIs are extremely expensive (i.e. various MLS APIs). What is the best approach to obtaining data you need? How do you guys get data for your personal projects?,1635187433.0
qfhy4b,What makes an algorithm 'good'?,74,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qfhy4b/what_makes_an_algorithm_good/,33,"Hi all

In an effort to became a better programmer I wanted to check what actually makes a given algorithm 'good'. e.g. quicksort is considered a good algorithm - is that only because of average-case performance? 

Is there a community-approved checklist or something like that when it comes to algorithm evaluation? I tried looking on my own, but the deeper I dig the more questions I have instead of answers.

P.S. If you know any papers or articles that go in depth about the topic that would be great",1635172110.0
qfhaf7,Purpose of OR gates in bit-shifter,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/qfhaf7/purpose_of_or_gates_in_bitshifter/,4,"I can't find the answer anywhere to this question, but I've been looking into the circuit diagram for a bit-shifter and I can't figure out what the purpose of the OR gates are. I've tried putting in sample values for the inputs and working it through for left and right shift, with and without the OR gates at the bottom and I get the same values with or without the OR gates. If that's the case, then why are they in the circuit?

You can see a diagram of the circuit I'm talking about here: [https://www.101computing.net/binary-shifters-using-logic-gates/](https://www.101computing.net/binary-shifters-using-logic-gates/)",1635170157.0
qfgg3g,What is the Algorithmic Complexity of Euclidean Method of GCD Calculating?,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qfgg3g/what_is_the_algorithmic_complexity_of_euclidean/,1,"Hi folks. In short, the Euclidean method of calculating GCD says that;

    suppose a>= b and rem = remainder, gcd(a, b) = gcd(rem(a,b), b)

Implementing this method is also easy. However, I do wonder what its algorithmic complexity is. Or does it exist in terms of known notations? I know some basic ideas behind categorizing algorithms such as recursivity, loops, binary searches, etc, but I couldn't categorize that. Thanks in advance.",1635167560.0
qfah2t,Youtube channels to get in touch in basic computer terminologies,43,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/qfah2t/youtube_channels_to_get_in_touch_in_basic/,11,,1635142580.0
qeyub5,What is a an example of OSI layers in daily life?,13,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/qeyub5/what_is_a_an_example_of_osi_layers_in_daily_life/,10,I understand sending and receiving letters is similiar but I'm just curious if there is a similiar process in daily life ?,1635102571.0
qewt85,Counting the number of unique syntax trees of a grammar,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qewt85/counting_the_number_of_unique_syntax_trees_of_a/,3,"Lets say we have a arbitrary grammar for which we would like to know how many syntax trees does it generate.

We know that there are two always two derivations and 2021 leafs.

How to start?",1635096645.0
qebyt6,What is THE stack?,35,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/qebyt6/what_is_the_stack/,17,"Hello,

I understand what a stack is as a data structure; it's like a stack of plates at a buffet, where you only have access to the top plate. 

However, people seem to talk about THE stack of a computer, and the stack overflowing. For example, I think I've heard that it's bad to write recursive functions, because it can cause the stack to overflow.

Can someone please explain what this is?

Thanks!",1635018169.0
qebwa6,Happy Saturday! I was wondering how a social media feed is usually programmed?,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qebwa6/happy_saturday_i_was_wondering_how_a_social_media/,6,Is it a data base that uses the observer design pattern to refresh and post new entries into the database? That's what I'm thinking in my head but I'm not quite sure. Just trying to learn on Xamarin got a good bit of experience with C#. Thank you so much for your time and I'm sorry if this isn't the proper way to ask.,1635017950.0
qebtrg,What algorithms every one should know?,197,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/qebtrg/what_algorithms_every_one_should_know/,45,,1635017726.0
qe36q2,"For generations capable of facing the development of technology and information security, the UAE launches a training program to enhance youth skills in cybersecurity",5,0.67,computerscience,https://i.redd.it/toff45alr6v71.jpg,1,,1634988793.0
qe1vmc,"Difference between ai,ml,dl & ds(data science)",13,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/qe1vmc/difference_between_aimldl_dsdata_science/,11,"Hi guys, I wanted to know a brief difference among these four fields in computer science.so please any kind of link to the difference or giving a brief difference in this post will be very helpful. Thanks in advance",1634982628.0
qdmby0,"Website to build logic gates from simpler gates, allowing you to 'unlock' and use these gates to build more complex one",4,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qdmby0/website_to_build_logic_gates_from_simpler_gates/,3,"Hi all, I remember using a site that allowed you to wire logic gates to construct more advanced ones - it was really helpful for me but I can't remember the name of it for the life of me.

I am almost certain that you only started out with a NOT gate and I think it there was an AND gate. I think you could choose the number of inputs to create the gate and had to wire them to enable the truth table to operate in the correct way, allowing you to use the gate - for example, after an AND and NOT gate you you could make an NAND, OR gate, then an XOR ... 

If anyone can knows the name of that website I would be really grateful !",1634925063.0
qdkg3r,"I would like to start a study on quantum computer (not asking for help in the subject itself) but I don't thing I have a strong base in this topic, what studies and topic should I practice and study before moving to the main topic",1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/qdkg3r/i_would_like_to_start_a_study_on_quantum_computer/,3,,1634919659.0
qdgm1o,"Understanding algorithms and data structures, but not being able to implement them?",82,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/qdgm1o/understanding_algorithms_and_data_structures_but/,45,"Just a bit of background information: I'm currently in high school, and I'm taking a course about algorithms on Coursera. I do have previous programming experience.

I'm able to understand the concept behind algorithms and why and how they work, how efficient they are etc... 

However, when I try to implement or code those algorithms, I get stuck. I know that to solve this problem I should practice more, and I do try, but for some reason, I just can't seem to ""translate"" the algorithm into code. 

This is really affecting me cause I really enjoy computer science in general, and I understand the concepts, but I just can't seem to find a way to transfer my thoughts into code, and it kinda discourages me. However, I'm not gonna give up anytime soon. 

What can I do to solve this problem? Any advice is greatly appreciated! Thank you so much :)

Sorry if this post doesn't belong here, I'm not sure where to post it.",1634908201.0
qd6bj4,Donating computers to schools in Central America. Recs for educational centered software (computer coding for kids maybe).,44,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/qd6bj4/donating_computers_to_schools_in_central_america/,9," I'm donating a couple of computers to an elementary school in El Salvador and Mexico. I was hoping to also pack them with useful educational software that the kids can utilize (typing games, English learning games, match games, etc.) Anyone have good recommendations for this? I'm anticipating that the place won't have internet connection, so I want to make sure they can use them without an internet connection.",1634866249.0
qd407r,How does the use of a register make this feedback logic-gate diagram work?,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qd407r/how_does_the_use_of_a_register_make_this_feedback/,3,"Here is what we want:

    s = 0;
    for(int i = 0; i < n; i++){    // n could be any number
        s = s + x
       }

With this digital logic diagram, it is impossible:

&#x200B;

[No Register](https://preview.redd.it/tgc9butftvu71.png?width=896&format=png&auto=webp&s=3cf5c0936ec8756ae3c9e1285fb547847396c77c)

The above digital logic diagram is a logical circuit with feedback. The combination logic unit is an accumulator (with the '+' on it).

Using this diagram Suppose we feed a 1 into the accumulator (assuming we are working with 2 bits). This is the progression.

1. We begin with the circuit above. With the inputs of the accumulator being x and y

&#x200B;

https://preview.redd.it/cr2u5whwwvu71.png?width=773&format=png&auto=webp&s=d4cbec178fb49ba146c26cd85ad83d3bfcec4fd8

2. We feed 01 (assuming we are working with two bits of data) into x

&#x200B;

https://preview.redd.it/zyt951m3xvu71.png?width=1920&format=png&auto=webp&s=c240fa17f3aee1ad53f10b1fc384ba629f61b54e

3. 01 is fed into the accumulator, and the output is 01 because we added nothing to 01

https://preview.redd.it/sdiqo8raxvu71.png?width=1920&format=png&auto=webp&s=98cb3a624bb7348ea4509f7bf696a226934597ec

3, We begin the feedback. x still remains 01 because it had not been updated. y is now 01 because we fed the output into y. Sowe feed 01 and 01 into the accumulator

&#x200B;

https://preview.redd.it/8w46jzyjxvu71.png?width=1920&format=png&auto=webp&s=50a5fbfe425070f23902e4f13beaf10f15f96efb

4.  We add 01 and 01 into the accumulator. So we add 01 + 01, which is 10

&#x200B;

https://preview.redd.it/9v0v51hxxvu71.png?width=1920&format=png&auto=webp&s=13d98e3cb662622328b9ec5482a35feb84374a5b

5. 10 is the output. We then want to feed 10 into the accumulator.

&#x200B;

https://preview.redd.it/snx0coz4yvu71.png?width=1920&format=png&auto=webp&s=8ebf6207677c63ce40ca61487280edabc5f12c7e

6. x still remains 01 because nothing has updated it. y is the previous output of x and y.

7. (this continues infinitely)

So I understand that this loops forever and thus, an incomplete solution.

This problem is completed with the addition of a register.

&#x200B;

https://preview.redd.it/egp8l95nyvu71.png?width=1182&format=png&auto=webp&s=0e542594e08b8266e250600bb9b86404b4d395e8

This fixes the problem. To preface, I am familiar with registers on a software level and had an internship doing reverse engineering so I conceptually understand how registers work. In this example, how does a register help? How does it add the control? How does it make it so that s is initialized to 0?

I would appreciate this a lot.

EDIT: Solved

Summary as to why the register

I understand registers on a software level. I've interacted with them a lot when I had an internship doing reverse engineering. But, I've wanted to up my knowledge so I'm resisting the digital logic implementations of them. I digress, but it was to illustrate I have somewhat of an idea of what I'm doing.

The reset signal from the register is sent indicates the end of the loop.

As for the initialization, the load signal indicates to replace the state of S. In other words, initialize S.

So, to recap:

The register does the following ( slight over simplification):

1. CLK = State Change
2. LD = Initializes
3. Reset = Control",1634858345.0
qczvme,Why is While(1) an endless loop,21,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/qczvme/why_is_while1_an_endless_loop/,54,Hello everybody may I ask on why is while(1) an endless loop?,1634845790.0
qcxk48,Why I cant run a program in Hard Disk,5,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/qcxk48/why_i_cant_run_a_program_in_hard_disk/,7,"I have searched in on google. The reason they say is 

1. It is too slow.
2. It is not byte addressable.

Lets keep that slow factor aside , why cant they make it byte addressable?",1634839200.0
qcxh83,what is a hard disk,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/qcxh83/what_is_a_hard_disk/,17,I have read that HDD/SSD's are secondary memory and they are not counted in either RAM or ROM. So Is there any ROM in my laptop or not? If it is then how to check? Unlike in mobile phones we are provided with 32/64 GBs of RAM.,1634838977.0
qcjqwn,Data type vs Data structure,37,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/qcjqwn/data_type_vs_data_structure/,26,"Whats the difference between them? Also, is an array a data structure or a data type.",1634791317.0
qcddhf,Drawing Datapaths,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qcddhf/drawing_datapaths/,0,"Hello there. Today in class, my professor began going over datapaths, and there was one example he didn't have time to finish that I was curious about. The example was suppose to be a multiplier (64 bits) that used a 64 bit adder. He went over 32 bits, and said he would get to this one in the next lecture, but ""in the meantime, try and figure it out"". I can't figure it out, but want to know how to make one like this. Can anyone help? Thanks.",1634770795.0
qc7295,What are the current Operating System Textbook used on best Computer Science Universities in USA ?,77,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qc7295/what_are_the_current_operating_system_textbook/,18,"HI there, 

as I have studied Operating System subject more than 20 years ago with amazing 

[Operating System from Andrew S Tanenbaum](https://www.amazon.com.br/Operating-Systems-Implementation-Andrew-Tanenbaum/dp/0136374069/ref=sr_1_29?dchild=1&qid=1634753181&refinements=p_27%3AAndrew+Tanenbaum&s=books&sr=1-29&ufe=app_do%3Aamzn1.fos.25548f35-0de7-44b3-b28e-0f56f3f96147)

What are the current Textbooks used in USA universities ?

Best Regards",1634753345.0
qc6rth,Are there cloud database services for specialized binary trees?,2,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/qc6rth/are_there_cloud_database_services_for_specialized/,1,"Title. When I mean by specialized binary trees, I mean like heaps, segment trees, etc.

I feel like major cloud providers would have this kind of thing, but I'm not sure if I'm looking in the right direction because I can't find any results. If there isn't, can it be explained why? Thanks.",1634752594.0
qbv5z3,How can I improve?,53,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/qbv5z3/how_can_i_improve/,15,"Almost every single time I see a question I don’t know where to start from to solve it. But every time I see the answer I understand every line of the code made and why it was this way to solve the problem.
My only concern is how can I improve my self and look at the question then solve it immediately without seeing the answer and realizing that I overthinks?
Is it the lack of practice or understanding or problem solving skills (which is also practice)?",1634714065.0
qbre5v,Regular expressions and DFA,23,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qbre5v/regular_expressions_and_dfa/,0,"Recently, I watched this video [here](https://www.youtube.com/watch?v=N_rkHzhXueo) about  algorithms using to match regular expressions. I learned that the string `.*a.{n}`  produce number of DFA states proportional to the exponential of n.

Since GNU Flex tool using DFA for its search engine, I tried a simple code as below:

    %{
    %}
    %%
    .*a.{16}  { return 0; }
    .         { return 1; }
    %%

At n = 16, Flex produce more than 100K states. At n = 20, my computer run out of memory and kill the process. It's really  nice to see theory work in real code.

Anyone know any simple grammars which break Bison?",1634698509.0
qbl118,Data structures and algorithms,25,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/qbl118/data_structures_and_algorithms/,10,"Hello, is there any resources out there that teach data structures and algorithms (books,Ecourses,etc.) I feel like it’s really important to understand this topic. Any suggestions?",1634677729.0
qbkvnl,"What are ASCII and Unicode, and the differences/pros and cons?",1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/qbkvnl/what_are_ascii_and_unicode_and_the/,3,,1634677304.0
qb5blt,"From a memory/efficiency point of view, is there any reason a banking app can make locking of cards only available for credit cards?",0,0.29,computerscience,https://www.reddit.com/r/computerscience/comments/qb5blt/from_a_memoryefficiency_point_of_view_is_there/,2,"I lost my wallet recently, containing my debit and credit cards. I locked my credit card directly from the banking app. 

But I have to call or come to a branch to lock my debit card. 

It's not like there'll be a whole different implementation for it, right? Credit cards usually have more security than debit cards so won't the code (or methods) used be almost identical?

Is there a (programming) related reason why both cards can't be locked from the app?",1634623571.0
qb4bof,THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER RECOMMENDATION SUB. Please visit one of the following subs if you need either of those things,412,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/qb4bof/this_is_not_a_tech_support_sub_or_a_computer/,56,"Tech Support: /r/techsupport

Computer Recommendations: /r/SuggestALaptop or /r/buildapc

Sorry if this comes off as rude, but despite the previous stickied post people keep making posts asking for tech support or asking which Mac Book they should buy for college. This isn't the subreddit for those posts. Computer Science != IT ~~despite what all my relatives think~~. This thread also is not the place for those questions. Use one of the above subreddits.",1634619389.0
qb42z7,ELI5 : term rewriting,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qb42z7/eli5_term_rewriting/,1,"It seems related to metaprogramming , was wondering if someone could break this down for me ?",1634618388.0
qauqn2,Why does Strong Induction exist?,46,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qauqn2/why_does_strong_induction_exist/,9,"In math for cs lab, the instructor stated that there are 2 types of induction. However, if I grasp correctly, they must be equivalent.

    Strong induction: (P(0) ∧ P(1) ... ∧ P(n))   ==> P(n+1)
    Induction: P(n) ==> P(n + 1). 

So how do they differ? If I just make all logical operations from 0 to n in induction, don't I reach the exact same statement with the strong induction? Thanks in advance.

Edit: Thanks, everyone!",1634587288.0
qap6xe,An Unbeatable ModifiedTuring Test,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/qap6xe/an_unbeatable_modifiedturing_test/,1,"I’d like to propose a modified version of the Turing test that I believe no computer program could be written to pass. I believe this modification makes the test only passable by humans because it makes purpose-driven seeking of information the independent variable and not the provision of answers to pre-defined questions. This lack of having a pre-defined nature is the heart of the claim because, though artificial intelligence (hereafter “A.I.”) can be self-aware, it cannot be self-defined.

In the modified Turing test, there are two (2) apparent Turing tests taking place in two separate rooms. Finally, there is a third room that will be explained at the end. 

In the first room, a typical Turing test is run. This means there are two humans and a computer in the first room. The “investigator” human asks the questions and receives the answers, thus, we can generate two transcripts that have identical questions and are only different in their answers. This is standard. 

In the second room, there are two computers and one human. The “investigator” is itself a computer that has already passed the standard Turing test. Thus, in this second room, it is the computer program that is actually serving the role of trying to distinguish between the answers of a computer and human. Again, we can receive two transcripts of the Q & A.

The human investigator and the A.I. investigator In both of the first two rooms must be provided sufficient time for their questions and interactions with the computer and the human that they will end up judging. They must feel satisfied that they can make an intelligent guess in their own Turing test attempts. 

Finally, in the third room, we have our real investigator, This investigator will compare the four (4) transcripts and judge which of the two rooms had a human questioner and which of the two rooms had a computer program questioner. Thus, the true object of the examination is to see whether a computer program could match a human in asking the questions in a Turing test as opposed to answering them. 

Assuming a large enough amount of data, i.e., sufficient questioning and a long enough time given for the “interview process,” I do not believe a computer program can be designed that could pass this modified Turing test. My hypothesis is that because the “investigator” side of the traditional Turing test will be filled with questions that will make it apparent that a human is doing the asking. Importantly, the human investigator and the computer investigator in the first two rooms must NOT know that they are actually the subsections of the third room test. This means that the A.I. Investigator (who already passed the traditional Turing test) will now know that it should be trying to imitate a human.

The point of the modified Turing test proposal, in summation, is to show that the A.I that passed the Turing test can only do so when it is, knowingly, the object of the study. Providing the second layer into the experiment would turn the A.I. into the apparent “studier” and, yet, unbeknownst to the computer, still actually the object of the study.

This proposed modified Turing test will allow for a better barrier to defining humanity from computer programs should the hypothesis hold true that artificial intelligence will not be able to pass this modified version of the Traditional Turing test.

Please feel free to consider how this modified Turing test would work should the experiment ever take place.",1634571553.0
qakzly,Wrong Algorithm ? I found this algorithm on geeksforgeeks but I think its not completely correct,3,0.64,computerscience,/r/AskComputerScience/comments/qakxks/wrong_algorithm_i_found_this_algorithm_on/,5,,1634557440.0
qai7co,How to learn the working of computer to its basic electric circuits and how they harmonize together to a working machine?,62,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/qai7co/how_to_learn_the_working_of_computer_to_its_basic/,26,I've been learning C.S and quite confused about binary and how it's processed in CPU and hardware and turned into software. I mean entirety of it and I am quite confused where to start. Anyone experienced willing to help me out?,1634544251.0
qah5lk,Do computers calculate irrational and rational exponents using the same algorithm?,12,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/qah5lk/do_computers_calculate_irrational_and_rational/,9,,1634539258.0
qabacx,Extended use of math in computer science?,73,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/qabacx/extended_use_of_math_in_computer_science/,33,"Hey I’m kind of new to computer science learning, one thing that I keep on hearing though is the use of heavy mathematics in it such as calculus. To people who have a knowledge beyond mine can someone please give me a summary of what that’s all about as I’m just learning calculus right now too and I’m not the biggest fan so far…",1634516590.0
qa3kdv,Comprehensive books for computational methods,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/qa3kdv/comprehensive_books_for_computational_methods/,2,"Hi, it would be great if people could suggest some books on the subject of computational methods. Our lecturer gave a list but I didn't find them simple to understand. I want a book that explains the theory like the reader is coming out of school.

Edit : I think I gave the wrong idea, computational methods doesn't mean DSA. It means mathematical methods for solving differential equations (time, space equations) in computers using approximation

The list  : 

 K.A. Hoffmann, S.T. Chiang, Computational Fluid Dynamics,Volume I, 4thEdition, 2000

P. Moin, Fundamentals of Engineering Numerical Analysis, CUP,2001

C.A.J. Fletcher, Computational Techniques for Fluid Dynamics 1:Fundamental and General Techniques, Springer-Verlag, 1991

P.K. Kundu and I.M. Cohen, Fluid Mechanics 2ndedition,Academic Press, 2002

J. Glyn, ""Advanced modern engineering mathematics"", PrenticeHall, 2004

J.D. Anderson, Computational Fluid Dynamics: the basics withapplications, McGraw-Hill, 1995

E.F. Toro, “Riemann Solvers and Numerical Methods for FluidDynamics. A Practical Introduction”, Springer 1997 ",1634492439.0
q9x87e,Alan Kay on the context and catalysts of personal computing,10,0.92,computerscience,https://www.notion.so/blog/alan-kay,1,,1634471550.0
q9waxg,Peer to peer networking on the internet,5,0.78,computerscience,https://www.reddit.com/r/computerscience/comments/q9waxg/peer_to_peer_networking_on_the_internet/,2,"What is the state of the art of peer-to-peer networking on the internet? Hosts behind (possibly carrier-grade NAT, ideally no hosts involved other than the peers. How close are we these days? Will IP6 make a big difference or are too many deployments still done in a way that doesn't give each device a routable IP address?",1634467168.0
q9u6uz,Operations with floating point numbers by hand on paper,20,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/q9u6uz/operations_with_floating_point_numbers_by_hand_on/,1,"Can anyone explain to me how exactly that works? Yes, it’s an assignment but I somehow keep getting the wrong results over and over again. Sorry if this isn’t the right sub for this. The question is as follows: Subtract the number 1100011101011011 from the number 0010010001100101. The format is F(2, 11, -14, 15, true) IEEE 754-2008 with half precision (16 bits). Use „round to nearest- round to even“.

My result : 0100100000010001

Their result : 0100011101011111

What I did first was make the exponents the same and adjusted the value to the right (mantissa we say, not sure in english). I then added the two numbers since we have a -(-number) which should be the same as +. At the end I rounded using the guard, round and sticky bits. I really don’t see the problem and this is an example in a long series I’ve tried solving , getting none correct. If someone can explain what I’m doing wrong, I’d be very thankful.",1634456652.0
q9mje2,Digital Modulation Techniques,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/q9mje2/digital_modulation_techniques/,2,"Hello,

I am currently struggling to find a book that goes in-depth in the explanations of digital modulation techniques like BPSK, FSK, Differential PSK signals, QAMs, Spread Sprectrum and so forth. Are there any books you guys recommend for digital modulation techniques? Thanks!",1634424948.0
q9exqs,"I've made a feed of podcasts, articles and videos from the best thinkers in Computer Science. But I'm missing a ton of people. Help me discover them?",92,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/q9exqs/ive_made_a_feed_of_podcasts_articles_and_videos/,14,"There is too much noise and sensationalism on traditional social media. So I want to curate the best podcasts, videos, and articles in computer science into one place. The goal is to improve the signal-to-noise ratio, You can visit the feed as it is [here](https://app.cicero.ly/topics/Computer%20Science).

Here are the people I have found so far whose content I'm curating:

**Jeremy Kun, John D. Cook, Scott Aaronson, Lex Friedman, Donald Knuth, Dick Lipton, Jelani Nelson, and Andrew Ng.**

I'm sure I'm missing a ton. Who are those people?",1634400427.0
q9e5oo,Computer Science is not a science?,50,0.84,computerscience,https://m-cacm.acm.org/magazines/2013/1/158757-computer-science-is-not-a-science/fulltext,59,,1634397941.0
q9c7cp,Designing own compiler vs using a pre-built compiler,0,0.41,computerscience,https://www.reddit.com/r/computerscience/comments/q9c7cp/designing_own_compiler_vs_using_a_prebuilt/,4,"In current times there are compilers that exist for every language. This helps automate the process of writing code for many programs. What kinds of programs require completely original compilers? Is this for mainly academic purposes or are there other reasons to consider a new compiler? Such as a program made for a specific purpose. 

Thanks",1634391585.0
q95xo2,Data analysis program for beginners,14,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/q95xo2/data_analysis_program_for_beginners/,7,"Hello. So as of now I have zero experience with programming. I have just decided I am going to start learning how. Currently I'm working as a tutor at a school through an AmeriCorps service program. I noticed during the last staff meeting when they were talking about compiling data to determine which students need the most extra help (basically where they want to send me and the other support staff) that a lot of the teachers were pretty frustrated with the amount of effort it takes to get all that information organized. I'm supposed to be doing a project for AmeriCorps to help the school and system out, and I was thinking it would be a good idea to try to develop a program that could streamline this process of data collection/analysis. I would have until the end of summer 2022. From zero experience with coding other than a little bit of HTML in high school, which I don't really remember, do you think I'd be able to get the knowledge and experience in order a write a program to do this? Assuming I don't get terribly bored with it or burnt out, I plan on spending a few hours a day learning. 

Also, if this does seem feasible, what language should I be primarily focusing on? 

And lastly, if anyone has any advice for someone starting to learn to program, that would also be welcome. I am most interested in the interactive online classes like [freecodecamp.com](https://freecodecamp.com), as I tend to get distracted during lectures. I can definitely see myself sitting in on lectures and paying attention/enjoying them once I start getting the basics down. 

Thanks :)",1634363220.0
q8w84n,Simulator or emulator?,22,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/q8w84n/simulator_or_emulator/,8,"At what point does a simulator become an emulator 
/ What are the absolute requirements for something to be considered an emulator, and would the following program (little man computer)  be considered an emulator?


https://www.peterhigginson.co.uk/LMC/",1634328016.0
q8rs2z,"Luddite here. I really want to get into computer science but I am overwhelmed on where to start. If I wanted to become an expert, is there a good place to begin?",31,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/q8rs2z/luddite_here_i_really_want_to_get_into_computer/,27,,1634314575.0
q8kgky,Some other books like 'The Art of Computer Programming' by Donald Knuth?,18,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q8kgky/some_other_books_like_the_art_of_computer/,4,"The Art of Computer Programming by Donald Knuth is famed artwork of a literature on Computer Science.

What are some other books in Computer Science which goes to similar depths and breadths like Donald Knuth??
(Could be in any field or domain relating to Computer Science)",1634288620.0
q8b9g3,3 party transaction problem,3,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/q8b9g3/3_party_transaction_problem/,11,"Does a solution exist to what I will term the ""3 party transaction problem""?

Let the problem be defined as follows...

There are three people: A, B, and C

It is common knowledge among all parties that A starts with $100. This $100 is in a vault and cannot be shown to B nor C.

Let's say that C is approached by A who offers C a $100 IOY (I owe you) in exchange for some widgets that C owns.

At the time of the transaction, how does C know that A hasn't already given B a $100 IOY?

What evidence might A be able to provide to C to prove that A has not already given B a $100 IOY?

note: C does not know how to contact B so cannot ask B any questions directly.",1634253592.0
q87ymv,Will we ever see AI split from the video card?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/q87ymv/will_we_ever_see_ai_split_from_the_video_card/,7,"So right now, high end (and soon enough low-mid ends as well) video cards come with AI accelerators, like nvidia's tensor cores.

But as we've seen in the past, whenever a technology emerges in one hardware component (like graphics in CPU in the older days), it eventually gets it's own dedicated hardware, like graphics splitting from CPU.

Do you think this will happen to AI acceleration in the near future too? Like have a dedicated AI card?",1634242937.0
q87ayn,Coding time / testing time ratio,19,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/q87ayn/coding_time_testing_time_ratio/,14,"Hello,

I am interested in the stats of the coding time / testing time ratio. If you are working as a professional developer and you are using tests (unit, component, contract, etc.) what is the time spent on coding / time spent on the development of the various tests.

Example: 1 day coding 2 days testing (unit, integration)",1634240947.0
q805c4,Agent based model of gender bias (nyt / arxiv),0,0.42,computerscience,https://www.nytimes.com/interactive/2021/10/14/opinion/gender-bias.html,1,,1634219530.0
q7mriy,Community evolution on Stack Overflow,33,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/q7mriy/community_evolution_on_stack_overflow/,1,"**Corresponding paper:**  
[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253010](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253010)

**Abstract**

Question and answer (Q&A) websites are a medium where people can communicate and help each other. Stack Overflow is one of the most popular Q&A websites about programming, where millions of developers seek help or provide valuable assistance. Activity on the Stack Overflow website is moderated by the user community, utilizing a voting system to promote high quality content. The website was created on 2008 and has accumulated a large amount of crowd wisdom about the software development industry. Here we analyse this data to examine trends in the grouping of technologies and their users into different subcommunities. In our work we analysed all questions, answers, votes and tags from Stack Overflow between 2008 and 2020. We generated a series of user-technology interaction graphs and applied community detection algorithms to identify the biggest user communities for each year, to examine which technologies those communities incorporate, how they are interconnected and how they evolve through time. The biggest and most persistent commu-nities were related to web development. In general, there is little movement between communities; users tend to either stay within the same community or not acquire any score at all. Community evolution reveals the popularity of different programming languages and frameworks on Stack Overflow over time. These findings give insight into the user community on Stack Overflow and reveal long-term trends on the software development industry.",1634165720.0
q7dw91,"Suggestions for entry level articles on Fuzzy Logic, please?",32,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q7dw91/suggestions_for_entry_level_articles_on_fuzzy/,4,"Hello,

I'm taking an intro class to Fuzzy Logic, and I know nothing about the subject. I'm supposed to read some articles on the subject and explain them in paper for the mid-terms but since the subject is wide, I'm afraid I might end up trying to understand rather complex and upper-level articles. This is not my native language either so that might be double the problem. Could I ask for some suggestions on the articles to read, please? Thank you in advance.",1634139748.0
q79tbx,Is there a audio or video version of the Tanenbaum-Torvalds Debate on microkernels and monolithic kernels?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q79tbx/is_there_a_audio_or_video_version_of_the/,0,,1634127165.0
q71mpj,Is the full computing power of modern equipment being bogged down by something?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/q71mpj/is_the_full_computing_power_of_modern_equipment/,11,"There seem to be many factors that counteract the hardware and computing advances of recent decades: bloat, inefficiency, etc? Are there any people or projects working on this? From outside the field it seems things such as software quality have been a wet blanket on Moore's Law. I mean, even as a layperson, Windows seems like poo.  


Any insight or thoughts on what's going on here or solutions? I'm technical enough to have a dual boot and use terminal on ubuntu if that means anything. Please be as detailed and technical as you like.

Edit: hardware has become way better and software has become worse. I can't help but wonder how much the latter is negating the former and if there is anything currently being worked on or explored to help.",1634092371.0
q71moh,Can anyone learn to code well?,8,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/q71moh/can_anyone_learn_to_code_well/,13,"Even though coding fascinates me, computer science just doesn't come naturally to me. The only reason why I did well in my introductory computer science class was that I kept bugging the person next to me for help. I'm frustrated because being a software engineer interested me (in fact, I loved going on Scratch and looking at peoples' programs on Khan Academy when I was a kid), but now I'm not sure anymore.",1634092369.0
q6so82,"I feel as though the pseudocode for the RB-Insert-Fixup operation given in ""Introduction to Algorithms"" (Cormen) is incomplete... Thoughts?",0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/q6so82/i_feel_as_though_the_pseudocode_for_the/,1,"The code is as follows:

    while z.p.color == RED
        if z.p == z.p.p.left
            y = z.p.p.right
            if y.color == RED
                z.p.color = BLACK                         //case 1
                y.color = BLACK                           //case 1
                z.p.p.color = RED                         //case 1
                z = z.p.p                                 //case 1
            else if z == z.p.right
                    z = z.p
                    LEFT-ROTATE(T,z)                      //case 2
                z.p.color = BLACK                         //case 3
                z.p.p.color = RED                         //case 3
                RIGHT-ROTATE(T,z.p.p)                     //case 3
         else (same as then clause
                      with ""right"" and ""left"" exchanged)
    T.root.color = BLACK

My problem is this:

Case 2 and 3 are given within the same else if statement, with the difference being that, for case 3, only the last 3 lines run, but for case 2, all of the lines run.  But if I were implementing this code, all 5 lines would run all the time.

To correct this, I would change the 'else if' statement to an 'else', then use another if statement inside, such as `if z == z.p.right`, where I would have the two case 2 statements therein.

I just feel as though the pseudocode is not *difficult to understand* as others have stated, but rather that it is either incorrect, incomplete, or badly written.

Why would you include every other if-else and while structure only to leave out another?",1634063863.0
q6lq9i,"I've often heard that Math is an important part of CS, but I am curious about the real world (on-the-job) applications of Math for Computer Scientists.",6,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q6lq9i/ive_often_heard_that_math_is_an_important_part_of/,6,"It'd be great if you could point how/where in particular do you use the following:

1.  Discrete Structures
2. Linear Algebra
3. Calculus",1634043867.0
q6gpe2,"Today but 10 years ago, the creator of C and Unix; Dennis Ritchie passed away.",1569,0.99,computerscience,https://i.redd.it/r9a1yqed1zs71.png,46,,1634023583.0
q6g6ho,Maximum flow in flow network,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q6g6ho/maximum_flow_in_flow_network/,2,"Let's say you have a maximum integer flow function in a network with 7 edges. The capacity of each edge is increased by one. The capacity of all edges is always an integer. Is there an algorithm that can find a maximum flow in this new network with complexity O(|E|), where E is the set of edges? If so, what would this algorithm look like?",1634021385.0
q6ejfh,Optimal cache management,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q6ejfh/optimal_cache_management/,5,"If we know in advance the exact set of keys, it is possible to construct a perfect hash function. If the same vein, let's suppose that we wanted to build a key-value store to act as a cache. If we knew in advance the classes of objects that we want to store, the distribution of their sizes and of the read requests, is it possible to build an optimal data store that minimizes read latency and variance, as well as storage overhead ? Is there research to this effect ?

This question was vaguely inspired by the article referenced [here](https://www.reddit.com/r/programming/comments/q5pk4c/memcached_vs_redis_more_different_than_you_would/), comparing Memcached to Redis.",1634014612.0
q61r8g,Has anyone else experienced this?,21,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/q61r8g/has_anyone_else_experienced_this/,12,"I'm working on a group project at school for a software development class. We're developing an app that a person on the team pitched.

Problem #1: There's barely any communication within the team, just the bare minimum. If I ask the team for feedback via Slack for the design of a page in the app:  there's no response. I ask the team if we should meet in person to prep for our presentation: 0 response.

Problem #2: One person is taking it upon themselves to do all the work. They've assigned themself as the project manager, but hasn't done any PM work of creating detailed stories, making sure there's 1 person per story, and facilitating communication across the team. There's 3 people per story, and for the ones they assigned themself to, they did literally all the work after ignoring message I sent asking how we should split up the work. To top it off, they didn't meaningfully respond to my messages but sent a message to the channel that includes the professor to say hey guys I finished the design and code for all three of these pages, I'll be ready to demo them during our presentation. 

To add on, I messaged to say I researched some things and I have some code so I can help with integration (I spent two full days researching this stuff) Again, 0 response so now it'll look like I did literally nothing for the project.

Problem #3: We grade each other for coding and general business development. I'm obviously going to be scored lower and they'll justify themselves getting the highest score.

I'm completely lost. This is my first semester as a grad student and was excited to work with a team having heard how fun it can be. Has anyone experienced something similar? How did you get through it?",1633975416.0
q61kx5,What's the difference between declarative and imperative programming languages?,63,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/q61kx5/whats_the_difference_between_declarative_and/,14,,1633974968.0
q5y2n6,Not everyone can do computer science,0,0.21,computerscience,https://www.reddit.com/r/computerscience/comments/q5y2n6/not_everyone_can_do_computer_science/,24,"Coding isnt just about learning a language like many people say. You need to have actual analytical and problem solving skills.

You can definitely develop your thinking skills but not everyone can learn above basic logical programs. I believe the best coders are born with analytical skills.

When people struggle with coding others always tell them to grind it out and to keep practicing. Some people simply arent smart enough or arent wired for coding.

If you're a comp sci major because you like spending time on your computer and want a ""guaranteed"" nice paying job, maybe you should reconsider. If you notice a majority of students in your class are a lot smarter than you, reconsider. If you're already struggling early on then reconsider.

I'm not saying this to depress people or discourage them. I'm just trying to save people time and stress. As a failed comp sci major I knew the cards were never in my favor to begin with.",1633965381.0
q5ryuw,How does md5 work in very simple terms?,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/q5ryuw/how_does_md5_work_in_very_simple_terms/,6,"I get that it produces a hash value for a password, but, what inputs does it take in?

if it takes no inputs other than the password itself (e.g. another word to make the hash unique for a set of data), isn't it completely useless?

what I was wondering is, does it do this:

md5(""password"")

or something like this:

md5(""password1"",""dog"")

md5(""password2"",""dog"")

where ""dog is a unique key that whoever in charge has decided to use in this specific case?",1633943942.0
q5qcer,What is the time complexity of sudoku algorithm?,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/q5qcer/what_is_the_time_complexity_of_sudoku_algorithm/,15,"Link: [https://www.geeksforgeeks.org/sudoku-backtracking-7/](https://www.geeksforgeeks.org/sudoku-backtracking-7/)

Here, it is given that it is 9\^(n \* n). But we are always using the same 9 x 9 board. Shouldn't the time complexity be O(1). It doesn't depend on the input size. Please clarify.",1633935988.0
q5q3qo,C++ Books For Systems,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q5q3qo/c_books_for_systems/,1,"Hello everyone, I'm currently a 3rd year EE student. I got very interested in Computer Architecture and other Systems topics in my 2nd year. I'm decently comfortable with C and can do some basic stuff in systems using it. I've gone through the ""Computer Systems: A Programmers Perspective"" book and completed most of the labs using C. I got to know that most of the Computer Architecture Research is done using simulators that primarily use C++, and profeciency in this language is a must. Can you suggest books relevant to this, which can teach me C++ with the Object Oriented Programming Concepts, programming using threads and other Systems related things which can help me with what I have mentioned above ? Any help would be appreciated.

Thanks In Advance!",1633934887.0
q5oy2m,How to improve object oriented and system design,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q5oy2m/how_to_improve_object_oriented_and_system_design/,0,"Hello everyone,
I want to get better at object oriented design and system design. In order to get better at designing software, I would love to read book/ take a course to get better at designing systems. 
Please suggest me any books or any good practice which can make me better at designing",1633929817.0
q5ou0f,How do Computers Calculate and Apply Probability?,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q5ou0f/how_do_computers_calculate_and_apply_probability/,2,"If I want a certain result to appear n% of the time, how would I apply this in a program? The research I've done only leads me to what probability is, rather than how it's applied in, let's say, games and the percent chance a certain item can appear.

I assume the computer has to generate some sort of random number and then based on that make the calculation to determine if the desired result will appear.

    let chance = 0.5;
    calculateChance(chance);
    
    function calculateChance(chance) {
        let x;
    
        // probability code using the chance value
    
        if (x == true) {
            print('true');
        } else {
            print('false');
        }
    }

*Javascript, why not?*",1633929319.0
q5mjcb,Most of computer science seems to include the tradeoffs as there is no absolute best way usually. I dont see this with containers (kubernetes) everything seems to point to the pros of containerization (microservices) what are the cons? what are the tradeoffs?,3,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/q5mjcb/most_of_computer_science_seems_to_include_the/,6,,1633920393.0
q5lf6c,Computer disease. Richard Feynman on his first computer experience in the 1940s.,204,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/q5lf6c/computer_disease_richard_feynman_on_his_first/,11,"I'm reading Richard Feynman's book ""Surely You're Joking, Mr. Feynman!"". There is a chapter on working on the first atomic bomb (The Manhattan Project) and how the first computers hit the scene. I was amazed that despite the past 80 years, the attitude towards computers has not changed at all.

>Well, Mr. Frankel, who started this program, began to suffer from the computer disease that anybody who works with computers now knows about. It’s a very serious disease and it interferes completely with the work.  
>  
>The trouble with computers is you play with them. They are so wonderful. You have these switches—if it’s an even number you do this, if it’s an odd number you do that—and pretty soon you can do more and more elaborate things if you are clever enough, on one machine.  
>  
>But if you’ve ever worked with computers, you understand the disease—the delight in being able to see how much you can do. But he got the disease for the first time, the poor fellow who invented the thing.

[computers were like that at the time](https://preview.redd.it/eecicf3lyqs71.png?width=1280&format=png&auto=webp&s=50603dbe26aa2d7895e1b7ff6fea1674302f3d29)",1633916240.0
q5993u,How do i learn to solve this type of problem?,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/q5993u/how_do_i_learn_to_solve_this_type_of_problem/,15,"Im a first year computer science student and i was watching someone win a google kickstarter and attempted to complete the problem myself but the whole thing was completely foreign to me. I understand these problems are probably very advanced but i was wondering what approach you guys think would be best to try and learn how to solve problems like this one.

&#x200B;

There are **N** houses for sale. The i-th house costs **Ai** dollars to buy. You have a budget of **B** dollars to spend.

What is the maximum number of houses you can buy?

### Input

The first line of the input gives the number of test cases, **T**. **T** test cases follow. Each test case begins with a single line containing the two integers **N** and **B**. The second line contains **N** integers. The i-th integer is **Ai**, the cost of the i-th house.

### Output

For each test case, output one line containing Case #x: y, where xis the test case number (starting from 1) and y is the maximum number of houses you can buy.",1633876668.0
q54tls,Cybersecurity – Put to the test,1,1.0,computerscience,https://www.ornl.gov/news/cybersecurity-put-test,0,,1633858289.0
q53ako,What's a computing term for 'first'?,6,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/q53ako/whats_a_computing_term_for_first/,22,"Firstly, I'd like to admit that I'm not at all techie, hence the newb question. Basically, I'm looking for a term that implies 'first', or perhaps a more computing appropriate wording, 'zeroth'. Like the first block in a chain or the first file on a database. The terms in my head are along the lines of root, core, etc. But that's as far as I can get as a non techie. 

Throw at me any term that suggests 'first' in a computer sciencey way.

Thanks!!",1633850601.0
q50xlp,I don't get password hashing and salts.,122,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/q50xlp/i_dont_get_password_hashing_and_salts/,29,"Ok, so I understand that storing passwords in plaintext is bad, and encrypting passwords just means that now we have to keep a secret safe, and that isn't ideal either.

So the answer is to hash password values to some fixed-length value using a hashing algorithm. 

A frequently cited problem with just hashing a password is that a hacker could use common passwords and employ the same hashing algorithm and essentially dictionary attack a resource. 

But something I don't understand is this: if hashing algorithms are deterministic, that is, given the same input they always produce the same output, and the algorithms themselves are known, then couldn't a hacker essentially reverse the steps taken to hash values and produce the original input? Why is the rainbow attack method even necessary?

That's my first question.

I also know that salting hashes introduces randomness into the hashed values. I get how this means that an attacker can't carry out a rainbow attack using common hashes to guess passwords - but then how the heck is the password later verified? If I've randomized the hashed password, how can I check it against credentials I get from the user which will also be salted randomly and hashed?",1633839458.0
q4fa3b,Creating Decentralized Websites?,20,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/q4fa3b/creating_decentralized_websites/,12,"I have been doing small web development projects for the last couple of years and have been wondering how I could create a decentralized dynamic webpage.

I am wanting to have nodes that contain the website source code and has a signature of the source with it that then gets compared to other nodes source signatures to prevent unwanted modifications just like how a cryptocurrency would work, but instead of transactions it's the source. I'm not sure how possible this would be with a dynamic website.

I am wondering if there are any places that have information on creating this type of node network. I can't seem to find anything as I don't exactly know where to look or what to search for, this is very new to me.

I am trying to learn more about decentralization so I can eventually move to working on more advanced projects, I know I am probably talking straight gibberish right now.

Edit: I unfortunately don't have the time to reply to everyone, but thanks for all of the info about this, you've all bet a major help!",1633759067.0
q4dei9,Can operating system be known as an algorithm?,18,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/q4dei9/can_operating_system_be_known_as_an_algorithm/,18,"It's stated in my algorithm lecture that

""An operating system can be taught of as an online algorithm that never terminates, rather than a  finite algorithm with input and output.""  


Every time I try to wrap my head around this statement my brain goes blank with a static noise screen, Can someone bother to explain this?

&#x200B;

https://preview.redd.it/4wr3uylphcs71.png?width=888&format=png&auto=webp&s=1ea6d65f6993e7b63e6b9039d16c7d5181247178",1633750642.0
q4bz42,Run length encoding (Java),10,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/q4bz42/run_length_encoding_java/,2,We just started learning about this in my programming class and I am sooo confused. Does anyone have any resources or recommendations to better conceptualize this topic? Thank you!!,1633744819.0
q478w4,Inhouse centralized computing centers,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/q478w4/inhouse_centralized_computing_centers/,4,"Why we do not use centralized inhouse&public computing centers that would support smart fridges, tvs, laptops and other electronic devices that is using software and ton of unnecessary hardware?

Majority of inhouse electronics could run with an assitance of inhouse computing center. Additionally it would boost up computing power of other devices. Laptops, tablets would still need some internal hardware to be mobile but it could boost their capacity in home while playing videogames or at work while working on ie. Video editing. 

Wouldn't such solution save a ton of materials in manufacturing and a lot of energy while using? It will keep one device complicated (central station) so ""upgrading"" your fridge would be no longer necessary. If you need more power- you simply upgrade your station. 

All ""go green"" tech companies are blind for such solution or is it more complicated than I am thinking?

I run few small/medium companies in chemical industry and I am always looking to optimize certain processes and this just hit me while day dreaming. 

TLDR- Why we do not use one computing center for many devices?",1633728431.0
q45e9c,What kind of game trees can be alpha-beta pruned?,14,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/q45e9c/what_kind_of_game_trees_can_be_alphabeta_pruned/,4,"I know for sure zero-sum game trees in minimax can be, but for games which are not zero-sum games, (e.g. utility is represented by A,B, player a maximizes utility A, b maximizes B), what conditions are necessary to make alpha-beta pruning possible?",1633722752.0
q3tlim,Do i need to actually know how to paint in order to get into generative art? Or programming knowledge is sufficient?,4,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/q3tlim/do_i_need_to_actually_know_how_to_paint_in_order/,2,"Title, i was always interested in art, mainly paintings, but i dont how to draw for shit. Dont know anything about the theory either.

 Is programming enough for this? I dont want it for anything professional, just some fun projects, but with certain decency",1633681926.0
q3kkxr,is it technically impossible to beat a computer at checkers?,7,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/q3kkxr/is_it_technically_impossible_to_beat_a_computer/,4,"https://www.wired.com/2007/07/the-game-of-che/

I was reading this article where it says that computers have memorized every possible game of checkers that could ever be played. 

Does this mean that is impossible to beat a computer at checkers? Thus, you can only ""tie a game"" of checkers with a computer? 

Thanks!",1633646633.0
q3jwi3,15^n vs n^15. Which function grows faster?,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/q3jwi3/15n_vs_n15_which_function_grows_faster/,7,"I read somewhere that exponential functions grow much faster than polynomial ones. But when I plot these two on Desmos, it seems that after a certain value, n\^15 grows steeper than 15\^n. So can we say that O(n\^15) > O(15\^n)? Or am I doing something wrong here?",1633644433.0
q3gkgx,Union Find tools,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/q3gkgx/union_find_tools/,0,Does anyone know of any tools that can do union find.  Looking for something that will also do percolation.,1633634746.0
q3dv02,how does a computer understand the concept of time ?,142,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/q3dv02/how_does_a_computer_understand_the_concept_of_time/,42,When i tell my program to print a text after 5 seconds how can it know when 5 seconds have passed and what's happening in the cpu.,1633627037.0
q2qkic,Any good resources??,1,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/q2qkic/any_good_resources/,4,"Anyone know of a good book or tutorial on YouTube to learn computer organization, in my case computer arithmetic and number representation???",1633545427.0
q2pjf4,Exam-style questions,8,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/q2pjf4/examstyle_questions/,4,"Where can i find exam style questions answers for a level cambridge computer science, i am studying privately and figuring out these answers without help is a pain. So if any knows the source to these answers would be great help. :)",1633542285.0
q2oi78,"Anyone here, who loves programming but not maths?",166,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/q2oi78/anyone_here_who_loves_programming_but_not_maths/,100,,1633539229.0
q2libt,"Testers’ ideas flow of the week: TDD, QA taking work of IT support and bad reputation",3,1.0,computerscience,/r/softwaretestingtalks/comments/q2lfnj/testers_ideas_flow_of_the_week_tdd_qa_taking_work/,0,,1633530211.0
q2h7gz,Will web apps dominate in the future and making all software requiring installation obsolete?,5,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/q2h7gz/will_web_apps_dominate_in_the_future_and_making/,12,"So I was thinking the other day, if could computing can be just as fast if not faster compared to most personally owned hardware, will there eventually be a day where almost everything would be cloud based?

For example, there wouldn't be a need to download apps on your phone, instead you just have a browser or have a customized browser for each app, eliminating the need for downloading apps.",1633514117.0
q1txed,"“My phone is listening in on my conversations”: not paranoia but a legitimate concern, researchers find. Eavesdropping may not be detected by current security mechanisms, and could even be conducted via motion sensors (accessible to mobile apps without request/notification to the user).",92,0.97,computerscience,https://twitter.com/JL_Kroger/status/1445333142869577733,6,,1633431143.0
q1swpn,Is space complexity finally just too simple?,35,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/q1swpn/is_space_complexity_finally_just_too_simple/,7,"Behind the rhetorical question, do you have examples of algorithms with a non-trivial (heap-)space complexity? For instance, algorithms heavily relying on a GC to achieve a particular bound, or with a smart amortized analysis?

Classical algorithms (like sorting, or simple graph algorithms) are often either in-place or clearly linear in the size of the input. Analyses of more refined algorithms seem simply not to take into consideration heap space complexity and focus on time.

Where is the Union-Find of space complexity?",1633426433.0
q1g1gh,Calculating network packet route?,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q1g1gh/calculating_network_packet_route/,3,"Is there are a known way by knowing my ip address and geolocation, to calculate to what destination ip address packet must be sent to pass specific route and to what addresses to send packet to be sure my packet will pass trough real target, but end up on ""fake"" target?

For example:

Having 5 devices/modem connected in network.device number 4 is my real target, but I am sending packet from device 1 to device 5 in a way so device 4 will need to forward packet to device 5, like router will do. But packet will contain interesting information only meaning to device 4, but rest of the network will think that device 1 is communicating with device 5 instead of 4, because in ip header destination is ip of device 5.

&#x200B;

EDIT:   
I am aware how hard is to gain access directly to internet at 0 hops ( being router instead of router)  
Hypothetically speaking, assuming ""real target"" is a router a like device with modified firmware to look into rest of a packet to see if packet has content of specific format...  
Important:   
By knowing own ip, own geo location  and ""real"" target ip, calculate ""fake"" target ip?  
Is that even possible mathematically?  
",1633383768.0
q1f8t3,"Understanding ECDH, ECDSA & HKDF",1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/q1f8t3/understanding_ecdh_ecdsa_hkdf/,0,Can someone explain these to me in a way that makes sense. I have to combine all of these examples into one. I understand that ECDH makes the secure connection and ECDSA is what looks at the secure connection and makes sure it is from Bob or Alice. I just want to create a discussion on how these go about. Thanks!,1633381634.0
q16owj,Address mapping in cache,22,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/q16owj/address_mapping_in_cache/,10,"I am studying address mapping in cache. In direct mapping( or in general any ) when they divide physical address space into blocks and cache into blocks and each block in cache can have any of the block from physical address space. So if PAS = 2\^8B, cache is 2\^4B and block size is 2\^2B.

no of blocks in cache = 4( 0 to 3)

no of blocks in PAS = 64(0 to 63)

I want to ask is there each memory location assigned a number or what? Or its just how we refer to some memory location? I am totally confused. How memory location is pointed even in the case without being divided into blocks. What is the role of word size? Like if a memory is byte addressable then if we have some data at 0000 0000 ( assume PAS = 8 bits ) then when we are going to read the data at 0000 0000 then we read all the data from 0000 0000 to 0000 0111 i.e., 1 byte?",1633358223.0
q15vs5,How to go about studying Port usage,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/q15vs5/how_to_go_about_studying_port_usage/,1,"I know there are various ports used in computer networking, and they have their own protocols. Is there a good system to study each port individually to learn in greater detail how to access or use each port for specific uses. Thanks for any information.",1633355609.0
q10w5o,I'm confused about mod calculations,38,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/q10w5o/im_confused_about_mod_calculations/,9,"How is mod(10,-4)=-2?

I get that 10/-4 gives you a remainder of -2 if you take the quotient as -3.

But when you take the quotient as -2, mod(10,-4)=2

So I'm so fucking confused here.

Is there some sort of rules for mod calculations that I didn't understand?",1633333662.0
q0ta23,difference in hard vs soft transfer function (AI),1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/q0ta23/difference_in_hard_vs_soft_transfer_function_ai/,1,,1633303206.0
q0m3n0,GUI talk with Shell or Kernel?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/q0m3n0/gui_talk_with_shell_or_kernel/,1,"Is it right to say that the Shell directly talks with the kernel?

If this is the case, does the GUI talk with the Shell or directly with the kernel?",1633281184.0
q0j4vh,Yann LeCun's Paper Gets Rejected From NeurIPS 2021,65,0.9,computerscience,https://www.theclickreader.com/yann-lecun-paper-gets-rejected-neurips-2021/,17,,1633272254.0
q0j1h2,Is it possible to make a fake “randomizer” using two clocks?,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/q0j1h2/is_it_possible_to_make_a_fake_randomizer_using/,3,"Let’s say we have two clocks that turn on for a certain amount of time then turn off for a certain amount of time and they cycle through these ons and offs. I am able to set the on and off time for each of these clocks.

Is it possible for me to set the on and off times for the two clocks such that they have random looking periods in which both clocks are on at once?

Of course it’s timed so it random at all, but just so it looks kind of random.",1633271976.0
q0if47,(Beginner’s Question) If your RAM uses 256 bit memory modules wouldn’t you need more than 8 bits for your data?,24,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/q0if47/beginners_question_if_your_ram_uses_256_bit/,5,"𝗤𝗨𝗘𝗦𝗧𝗜𝗢𝗡 𝗦𝗨𝗠𝗠𝗔𝗥𝗬:

If your RAM uses 256 bit memory modules wouldn’t you need more than 8 bits for your data? Because if you only have 8 digits wouldn’t they all have to be used for the 256 addresses leaving no digits left to use as the opcode? This question stems from confusion on two Crash Course YouTube video.

-------------------------------------------------------------------------------------------------------------

𝗙𝗨𝗟𝗟 𝗤𝗨𝗘𝗦𝗧𝗜𝗢𝗡:

I am confused by two crash course videos. In the first video they created the RAM using 8 256-bit memory modules (16 latches x 16 latches). So an 8 bit number can be broken up into one bit in each memory module allowing for 256 memory addresses for 256 bytes.

In the second video they break the 8 bit number at an address into 4 bits for an opcode and the other 4 bits for an address. They reduced the number of addresses to 16 which makes sense since 4 bits can only produce 16 numbers.

This is why I am asking how it would work if you wanted 256 addresses. If you still want 16 instructions you would still need a 4-bit opcode, but wouldn't you also need 8 more bits for the 256 addresses bringing your data at each address to 12 bits? Wouldn't this mean you need 12 memory modules instead of 8?

-------------------------------------------------------------------------------------------------------------

𝗟𝗜𝗡𝗞𝗦:

https://www.youtube.com/watch?v=fpnE6UAfbtU&list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye&index=7

https://www.youtube.com/watch?v=FZGugFqdr60&list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye&index=8

-------------------------------------------------------------------------------------------------------------

𝗗𝗜𝗦𝗖𝗟𝗔𝗜𝗠𝗘𝗥:

Sorry if this question makes no sense. I am a beginner and I may not understand the concepts or know proper terminology.",1633269982.0
q05rtf,Memory address space calculation question assumes 1byte word length,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/q05rtf/memory_address_space_calculation_question_assumes/,9,"If you have a 32 bit CPU. The text books say that the calculation for maximum amount of memory would 2\^32 bits. =  4294967296. 

Why does the text book assume the word length would be 1 byte? i.e 2\^32 gives \~4GB of memory space.

Why not assume the word length is 32bits? Which would be  4294967296 x 32 bits /1024 /1024 /1024 = 128 GB

I know that Windows XP 32bit had 4gb limit, so its not wrong, but looking for a reason.",1633216793.0
q00zo3,Data Structure and Algorithm,6,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/q00zo3/data_structure_and_algorithm/,4,"Hi,

I've been hearing about this topic a lot. What's the best book to learn this that's very beginner friendly and preferably in python? Or course.",1633200582.0
q00w48,Must dynamic routing algorithms be distributed or could dynamic routing algorithms be centralized?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/q00w48/must_dynamic_routing_algorithms_be_distributed_or/,2,Title really says it all,1633200267.0
pzvpje,Finding matches between an input string and a fixed set of strings,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pzvpje/finding_matches_between_an_input_string_and_a/,2,"I need to compare an input string to multiple strings which I'll refer to as fixed strings, and you can assume the latter won't change. Comparison disregards letters with special characters, only letters from A to Z, but case insensitive. I need to try to match my input string to the start of every one of the fixed strings, even if not all characters match, not unlike [approximate string matching](https://en.wikipedia.org/wiki/Approximate_string_matching). For example:

If my fixed strings are ""`apple`"", ""`bullcrap`"", ""`tomato`"", ""`apricot`"", I should be able to search for ""`Applecrab`"" and get the following matches:

    (Apple)crab	App(l)ecrab	Applecrab	(Ap)plecrab 
        
    (apple)		bul(l)crap	tomato		(ap)ricot

I also should be able to do multiple input words, as in:

fixed strings: ""`i love books`"", ""`book fair`""; input string: ""`I love`""; matches:

    (I) I    I      |  love  (love)  l(o)ve
    (i) love books  |  I     (love)  b(o)oks
       
    I     I     |  l(o)ve love
    book  fair  |  b(o)ok fair

Since the number of fixed strings isn't very small, the solution should employ a decent data structure. I was thinking about trees: a [prefix tree](https://en.wikipedia.org/wiki/Trie) seems to be a good fit, but it wouldn't help with loose matches in the middle of a string, e.g. `l(o)ve` and `b(o)oks`, neither would the [Aho–Corasick algorithm](https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm). I don't think [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) can help either. What are some data structures/algorithms I could use to achieve this?",1633183271.0
pztodf,How much calc will I use on the programing side,50,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/pztodf/how_much_calc_will_i_use_on_the_programing_side/,46,,1633175338.0
pztda7,Book/lectures/video recommendations to get me interested in Computer Science?,4,0.62,computerscience,https://www.reddit.com/r/computerscience/comments/pztda7/booklecturesvideo_recommendations_to_get_me/,7,"Hi guys, do you have any book/lectures/video recommendations to get me interested in Computer Science? I'm not asking for a how to code or anything too technical like that but more of a theoretical/hypothetical book or something that makes me be intrigued to learn about the technical and gets me interested in the subject if you get what I mean?",1633173811.0
pzrkv4,Principal Component analysis explained?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pzrkv4/principal_component_analysis_explained/,3,"Hey!

I'm getting into PCA and find that it has really useful applications. However i never want to use new methods of analysis, without getting af mathematical and intuitive grip on them.

Also i dont have a CS background (currently studying industrial engineering), but am very familiar with linear algebra. 

I have watched loads of videoes about the topic and understand the principle as being the following;

You use PCA to lower the dimensionality of a dataset, thereby making it easier to get an overview of which datapoints are related and not related. 

We do this by standardising and centering all the data (all variables thereby having the same weight) and putting it in a matrix = A, which has standardized variable 1 values in its first column,  standardized variable 2 values in its second column etc.

It is then possible through basic calculations and matrix operations to make a covariance (((x-X)\*(y-Y))/n) matrix, which has all covariation factors symmetrically distributed throughout it like so;

&#x200B;

https://preview.redd.it/abcqglzd30r71.png?width=404&format=png&auto=webp&s=9295628778768cc62f49c7032edeebce2ca49b7e

However the next step does not make sense to me.

It is said, that the eigenvector of this matrix is a principal component, and that the bigger the eigenvalue, the more important this principal component is for the dataset (the more variance it encapsulates). How is this possible? Is there an intuitive way of understanding this?",1633164916.0
pzqak4,"Let's say I'm trying to store this ""graph"". I have studied Linked Lists but haven't studied graphs yet, does it make sense to store it in this way in C++, or is there a more clever way to do this? (Each root node may have varying number of branch nodes)",77,0.95,computerscience,https://i.redd.it/1rwxakc3kzq71.png,23,,1633158221.0
pzmk2y,Anyone have the old craig and dave a level comp sci playlists?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pzmk2y/anyone_have_the_old_craig_and_dave_a_level_comp/,0,"OK so this is a bit of a silly question but craig and dave videos used to be flat out better than their new ones and they have removed the playlist they had?!?

Here is an old 2017, not 2020 video:

[https://www.youtube.com/watch?v=yyHFI5juppA](https://www.youtube.com/watch?v=yyHFI5juppA)

Hopefully someone knows where a full playlist would be? Thanks a million!",1633141529.0
pzfvrt,How would you write an algorithm that brute forces a problem like the four 4s one?,6,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/pzfvrt/how_would_you_write_an_algorithm_that_brute/,2,"The 4 fours problem: https://en.m.wikipedia.org/wiki/Four_fours

I'm a first year CS student, i just heard about this problem but I already knew about other problems of this class, and i always wondered if it's possible to write and algorithm that basically brute forces these problems, in order to find all solutions for a certain number/s, if yes, what would the structure of such an algorithm be? In particular, how do you make the machine execute random operations?

I'm apologize in advance if this is a stupid question, and thanks for your time",1633118432.0
pze8dj,Space Complexity How to Compute?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pze8dj/space_complexity_how_to_compute/,1,"Q1: Do the arguments of a function get incorporated into the space complexity?

Q2: If I created a temporary array in a function but then it gets automatically deleted upon function exit, would the array still be incorporated into space complexity?

Q3: Same as Q2 except I manually delete the array within the function before it exits. Would the array be incorporated then?

Thank you so much for your help!",1633113467.0
pze835,Recommended reading for database engine internals?,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pze835/recommended_reading_for_database_engine_internals/,2,"I've recently been reading [Martin Kleppmann's](https://www.amazon.co.uk/Martin-Kleppmann/e/B00Q43XKD6/ref=dp_byline_cont_pop_book_1) book and have read through chapters on storage engines and encoding e.g. SSTables, B-trees, LSM-trees, OLTP, OLAP, Data Warehousing, Log structured storage engines, but most of it is flying over my head. Any suggestions on additional material to study?

Also have read through [Alex Petrov's](https://medium.com/databasss) medium posts on Database Internals but still not satisfied that it's covered the fundamentals...",1633113441.0
pzcvjq,Best site for execution time challenges?,7,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/pzcvjq/best_site_for_execution_time_challenges/,4,"I'm looking to improve on writing code that is fast and efficent, however all the sites I can find score you entirely based on passing test cases rather than speed. Is there a site where I can be tested on my code being fast?",1633109459.0
pzbae2,What are some prerequisites for learning Computer Networks?,65,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/pzbae2/what_are_some_prerequisites_for_learning_computer/,16,"I am following Andrew Tenenbaum's book and it started out well. But as soon as the book jumped into the physical layer and it started getting mathy(Fourier series, etc.) I felt lost.

Now I have taken a discrete math course. So what other prerequisites are needed to learn networking?",1633104825.0
pzb24x,Can anyone suggest some resources for AI,15,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/pzb24x/can_anyone_suggest_some_resources_for_ai/,15,"YouTube content, websites anything!",1633104158.0
pz9k60,Computer Fundamentals by P. K. Sinha,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/pz9k60/computer_fundamentals_by_p_k_sinha/,5,"hi

Thoughts about this book? Seen the preview and it seems really a book full of cs concepts, from the basic to higher level concepts.

As I see on Amazon is such a popular book  in Indiaz.

Similar or better book with the same concepts and structures?

&#x200B;

Thanks",1633099702.0
pyybaf,Anyone know of a good professor on YouTube for computer data representation???,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/pyybaf/anyone_know_of_a_good_professor_on_youtube_for/,4,,1633054197.0
pywerf,How many hours do you spend a day coding?,90,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pywerf/how_many_hours_do_you_spend_a_day_coding/,40,"I’m in my second year of CS classes, how many hours do you guys spend coding a day? I’m currently learning Java,CSS & HTML. To my surprise CSS is so much easier to understand. Any tips/advice would be appreciated.",1633047486.0
pyrzy8,"Each term of a regular expression (term being defined as something in the expression that can be fully enclosed in its own parantheses) can AND WILL only produce one symbol in a string, unless it is enclosed with a Kleene star, correct?",0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/pyrzy8/each_term_of_a_regular_expression_term_being/,2,"Just did...poorly on my first assignment for my programming paradigms course.

    (a + b)* = all strings of a and b, any order.
    (a + b) = {a, b} which means a OR b, one per evaluation, not both and not neither?
    (ab) = {ab}

Is that correct? If so, how for instance would you know what order to generate from this regular expression  **(a + b + ba)?**  Does the order not matter? When *does* the order matter in a regular expression (i.e you need to know what the third symbol will be in each string, and that symbol needs to be constant)?

NOTE: None of what I just asked directly corresponds to any actual class problems, I just want to ensure I actually understand this material, this is not a homework-help thread.",1633033193.0
pyr63w,At what age did you have a good grasp on programming?,5,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pyr63w/at_what_age_did_you_have_a_good_grasp_on/,11,"A good grasp would be understanding a language well enough to execute what you intended to do, with your knowledge.",1633030669.0
pylxxq,Is there a mathematical way to pick the best SOLID color background for an image?,13,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/pylxxq/is_there_a_mathematical_way_to_pick_the_best/,3,"Obviously *best* is subjective. I'm using *best* here to mean **highest contrast** and **most striking**. Generally speaking, that's complement colors.

I had a random thought today when trying to pick a solid color background for my picture on my portfolio website. My picture has a lot going on: trees, a pink shirt, blue sky, light complexion, and I couldn't really decide a ""perfect"" color to complement it. So, I threw the photo in [a color averager](https://matkl.github.io/average-color/) and just got blah (#6f665d if anyone is wondering). Using a color wheel, the complementary color for blah is... well, blah (#5D666F). Just for shits and giggles I put the two beside each other, and it's painfully boring.

&#x200B;

This got me thinking: striking color contrast is *everywhere*. In most cases it's easy -- a model is wearing **orange?** Give them a **blue background!** But what about if they're wearing blue *and* red? What if they have a dark complexion and are wearing something bright?

&#x200B;

For example, a model below has an average wine color (#5f2d37), so theoretically, a strong complement would be a dark green (#2D5F55)

&#x200B;

https://preview.redd.it/husydiakqnq71.png?width=131&format=png&auto=webp&s=2cc752953629e51236bf08903c6d14080b5aad0a

https://preview.redd.it/grtwpd0lqnq71.png?width=131&format=png&auto=webp&s=db39f9089b7b4e58def445d5630ef16d2eb79a90

&#x200B;

The problem is that **orange** is a hell of a lot better background color than dark green:

&#x200B;

https://preview.redd.it/9h4bj6nlqnq71.png?width=450&format=png&auto=webp&s=c5cb59dfe09a44b10e618ac926f3c733a5a42205

Why is that? Is there that I could make the process objective and come up with orange every time?

&#x200B;

Thanks for entertaining my curiosity",1633015121.0
pyjzd4,"Machine Learning: Overfitting Is Your Friend, Not Your Foe",2,0.58,computerscience,https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/,0,,1633009162.0
pyj6av,Some random questions that haunt me for a while,64,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pyj6av/some_random_questions_that_haunt_me_for_a_while/,15,"Is it theoricaly possible to make an .txt file that contain the md5 hash of an .rar in which there is the said .txt?

And

Is it theoricaly possible to have an .rar file that contain itself in?",1633006465.0
pyiccs,Floating point representation,7,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/pyiccs/floating_point_representation/,5,"I had an exam and one of the questions was some binary (mantissa and exponent) and we were asked to convert the binary to decimal and then write its floating point representation or smth.
 
I was very confused and my question basically is how do we write the floating point representation of a number?",1633003386.0
pye2ol,A question about logic gates,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pye2ol/a_question_about_logic_gates/,0,"I have recently in my CS classes learned about the different types of logic gates, the and or nor xor NAND etc.
My question was:
All of these logic gates seem to have a completely different design, how do they account for this in chip manufacturing?
Like do they have a ratio of how many NAND-AND gates they have or XOR-or ?",1632983391.0
py9o7e,Skew Random Function Python,3,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/py9o7e/skew_random_function_python/,7,"So recently, I was looking for some coding challenges to do with python, and I came across this one that intrigued me ([https://www.geeksforgeeks.org/intermediate-coding-problems-in-python/](https://www.geeksforgeeks.org/intermediate-coding-problems-in-python/)).

The problem itself is pretty simple, and I went through two iterations of it just for the practice (once where the program literally just made random guesses until it got one (took like an hour just to repeat a 4-letter phrase) and then another one where the program looped through an array with the characters of the alphabet in them, which was able to get even entire paragraphs in thousandths of a second).

I'm not sure if this even exists, but I've been thinking of an idea on how to make it even faster: using a skewed random method (I'm guessing with some kind of machine learning model, or maybe that's not even necessary). The idea I had was for the model to study a bunch of data from random words and see, for example, what percentage of the time the average word will have either a consonant or a vowel at each index, if some letters are more popular than others, etc.

I know that this is really redundant, and there are faster ways to do this (I'm not sure if this pertains, but I'm actually just starting a course on Data Structures and Algorithms, with the advice of you guys on Reddit), and that this program really has no point in the real world (after all, where will basically rewriting a string letter by letter actually show up practically), but I thought it would be a cool little challenge, and wanted to know if any of you guys could help me out or give me some advice. (I would prefer to work in Python, since that what I'm most comfortable with, but pseudocode also works)",1632966139.0
py0ewf,Advice About Computer Science,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/py0ewf/advice_about_computer_science/,1,"Do You Know Some book where i can read about this detailed with examples

https://preview.redd.it/v89k0420ahq71.jpg?width=4128&format=pjpg&auto=webp&s=f4616aaea413e52a1212c29b039de3271901234d",1632936834.0
pxx9fk,Why do JWT’s need a signature?,5,0.68,computerscience,https://www.reddit.com/r/computerscience/comments/pxx9fk/why_do_jwts_need_a_signature/,6,"I understand that the signature is there for security purposes but I don’t understand why it is necessary.

The way I understand it when a jwt is generated there is a header and a body and then a signature. The signature gets verified by the server. But why can’t the body be verified by he server.

In other words if the header and body are encoded as a1a1 and the signature is b2b2 the server is checking that b2b2 is a valid signature.

Why can’t it just check that a1a1 is a valid body?",1632927864.0
pxl0k4,Could someone explain to me what does it mean that an algorithm is asymptomatically or polynomically larger or smaller than another like I am five? (Sorry if the way I wrote it is confusing),53,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pxl0k4/could_someone_explain_to_me_what_does_it_mean/,8,,1632879645.0
pwwkmh,How come graphics cards are barely utilized for ultra high res videos?,33,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pwwkmh/how_come_graphics_cards_are_barely_utilized_for/,16,"Did a little experiment today, opened up an 8k YouTube vid. My 2060 usage only went up slightly but cpu (3700x) usage shot up 40%. Wouldn’t it make more sense for the gpu to do that work? And this is also only a 1440p screen, so…I guess it’s rendering but just isn’t displaying it? I’m curious how this works",1632796164.0
pwwjqp,How computers are made? It is like we coded something into hardware? How do softwares are created? How did humans code a program for coding?,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/pwwjqp/how_computers_are_made_it_is_like_we_coded/,15,"im just really curious lol, can someone answer me with not so technical answer?",1632796076.0
pws5tm,"With the knowledge we currently have, can we rethink the basics of computer systems and make them better?",97,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pws5tm/with_the_knowledge_we_currently_have_can_we/,23,"I am taking a computer systems course right now and I was wondering if we could take a look again at the basics of the computer, I mean the very fundamental basics of a computer, and rethink how they work and possibly make them better? In other words, are Von Neumann and Alan Turnings' implementations perfect, or could we make them better?",1632781443.0
pwfr13,How do I learn about computer architectures?,133,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/pwfr13/how_do_i_learn_about_computer_architectures/,45,"This seems like an obvious question (I can just download a book and start reading), but I want to make sure I’m asking to learn the right thing. Basically, I really don’t know how computers work. I get the basics (kinda), but I don’t know how everything connects at all. Will reading a computer architecture book help me understand the OS, kernel, compilers, CPU, etc. or do I have to read a bunch of different books to understand all these things? I’ve heard of nand2tetris, but does that cover everything? Is there one source I can use to understand “everything” about a computer?",1632744798.0
pwahyf,Is theoretical CS dead?,36,0.73,computerscience,https://www.reddit.com/r/computerscience/comments/pwahyf/is_theoretical_cs_dead/,28,"With the amount of hype that AI and machine learning has seen in the past decade, I've seen many students flock towards that instead of theoretical computer science. Has theoretical CS taken a backseat? Are there professors who still focus on theoretical CS like Dr. Jelani Nelson or Dr. David Karger?",1632720199.0
pvx5dq,I want to self teach myself computer science so that it would help me in AP computer science. How?,44,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/pvx5dq/i_want_to_self_teach_myself_computer_science_so/,19,I am currently in 9th grade and completed algebra 1 and in enrollment in geometry. I am looking forward to going into AP CS. I can hopefully take the java course that my school offers.(completion of java course is one of the pre requisites for AP CS). but the enrollment numbers are so high that I cant garuntee my enrollment in that class. I want to self prepare myself for AP CS. I can dedicate a lot of time upwards to 3 hrs a day. How should I do this?,1632673658.0
pvo5ag,"In IPV4 addresses, it is said that there are 2^32 possible address, but why 2 is used as the base?",70,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pvo5ag/in_ipv4_addresses_it_is_said_that_there_are_232/,15,"Sorry for being naive i just started out on computer science


Edit: thank you for everyone's explanations, i understood it now! Have a nice day!",1632636680.0
pvmav0,What field of study in Comp Sci do things like hashmaps and binary trees fall under?,65,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/pvmav0/what_field_of_study_in_comp_sci_do_things_like/,25,"So lately, there have been a lot of meme programming videos on YouTube by people like JomaTech and Frying Pan about ""First Day as an Intern"" or ""If Programming was an Anime"" and most of them tend to not be too serious. They usually make a joke out of things like hashmaps and binary trees, but I can’t help but wondering where these show up and how I should study them.

As some background context, I’ve completed Comp Sci courses in Java and  Python, as well as done some front end development in ReactJS. I have also completed math through BC Calc. Every time one of these videos show up, I always get interested because it combines the best of math and Comp Sci, but I can't figure out where I should find them.

I'm guessing that the fundamental idea stays pretty constant, no matter which language you choose, but is there a preferred language to do these things in? I'd prefer to implement them in Python, just because that's my strong suit, but it would be interesting to see what everyone else thinks.

And finally, are there any good free courses that anyone would recommend? Once again, if they work well in Python, then by all means I'd prefer to work with that.

TL;DR: Which branch of CompSci do things like hashmaps and binary trees fall under, where should I study them, which language do they work best in?",1632628546.0
puwwd6,CS Beginner/Noob,32,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/puwwd6/cs_beginnernoob/,20,"
I really want to get into CS but I don’t know where to start. Any tips, book recommendations, youtube videos that help?",1632533363.0
pup5rl,How is the compiler/assembler able to make the distinction and conversion of commands understandable to us into the binary form that's accepted by the CPU?,76,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/pup5rl/how_is_the_compilerassembler_able_to_make_the/,31,"I understand all the machine knows are the electrical signals of 1s and 0s, I also understand well the fetch execute and decode cycle. But my main issue is getting past that layer of abstraction where what we type is seemingly converted into a bits that then go through the necessary logic gates and circuits understood by the machine.",1632506477.0
pu4kv8,Big-O Was A Lie,0,0.2,computerscience,https://calebwrites.substack.com/p/big-o-was-a-lie?r=sm7ry&utm_campaign=post&utm_medium=web&utm_source=reddit,2,,1632430757.0
pu0sj1,"Best modern books for Comp Architecture & Operations, DB Project Dev & Implementation + Computer Networks?",2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pu0sj1/best_modern_books_for_comp_architecture/,2,,1632419710.0
pu0btd,What are the differences between classical and generalized computability theory?,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pu0btd/what_are_the_differences_between_classical_and/,0,"What are the differences between classical computability theory and generalized computability theory? I have seen them in the titles of some computability books or their chapters, but I am not sure what their scopes are, and how they differ.
Thanks.",1632418391.0
ptzbw3,Graph Machine Learning in Industry: YouTube,51,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/ptzbw3/graph_machine_learning_in_industry_youtube/,0,"When I was a CS student, I always wondered how graphs that we study in the courses are used in real applications. Now, we organize a live workshop about applications of graphs and machine learning in the real world. Please join us for a live discussion from research engineers at Google, Amazon, and other companies. 

&#x200B;

YouTube link: [https://www.youtube.com/watch?v=bLN1V5fZD2g](https://www.youtube.com/watch?v=bLN1V5fZD2g)

&#x200B;

https://preview.redd.it/jytgorq28ap71.png?width=1113&format=png&auto=webp&s=bc984390391da4830e32b67965dd1cba241f6d81",1632415610.0
ptptpz,LUTs are underrated,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ptptpz/luts_are_underrated/,15,I took the data structure and algorithms classes last year ad they just didn't mention LUTs at all. ok so then i go look up how to make your code fast (given you dont have a potato tier alg) and i find out the best way to get the constant multiple down is to use a look up table to do stuff. is a look up table not considered a data structure/alg?,1632381228.0
ptc09c,Is C really a not low level programming language?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ptc09c/is_c_really_a_not_low_level_programming_language/,28,"https://m-cacm.acm.org/magazines/2018/7/229036-c-is-not-a-low-level-language/fulltext 

The author of this article claims that not only is C not really a low level language because it's not really very close to the hardware, but that people trying to force it to be one is the reason for the spectre and meltdown security vulnerabilities from a few years ago.  Is he right?  I don't know that much about C myself (I've had an intro to programming course that used C++, a little Matlab programming for a math class a couple years ago, a few MIPS assembly code projects for a computer organization class, and I'm learning some Python for a data science class; that's the extent of my programming knowledge/experience) but these seem like some rather wild claims and I'm interested to hear what other experts have to say about them.",1632330343.0
pt657b,"Computer science is no more about computers than astronomy is about telescopes, biology is about microscopes or chemistry is about beakers and test tubes. Science is not about tools. It is about how we use them, and what we find out when we do. — Edsger W. Dijkstra",561,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/pt657b/computer_science_is_no_more_about_computers_than/,65,,1632312455.0
psxdq3,Has anyone heard of an old Canadian programming language called Turing?,46,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/psxdq3/has_anyone_heard_of_an_old_canadian_programming/,23,"It’s old enough. It first appeared in the 1980s but stopped being developed by its makers and they released it as freeware.

It’s used to educate low-level high school students with an interest in computer science. Usually at the grade 10 level before real languages are introduced. It’s a good way to test if you’re really interested in programming and if you understand the very basic idea behind coding. To print a string for example it’a just 

put “words and stuff”.  Like Python v0.01

Very simple, no braces or static public mains or anything. Good introduction.

It has a built in documentation with example programs for  and integrated basic graphics that allow you to make shapes on a coordinate grid using a long list of coded colours. So if I wanted to make a coloured in box it would be simply drawfillbox(x,y,colour). The graphics could be used to make games but in my experience it was very hard since you couldn’t have a separate box entity above the others. You had to draw and erase the box to move it every time and that would wipe everything under it.


Despite it being very limited and very basic some of my favourite programs were made on that thing using its simple syntax and integrated graphics. It used to be quite popular in the Canadian CS community. Compsci.ca had a whole section dedicated to it and the programs hobbyists would make with it before the place died. Alas with the death of an organized Canadian CS community the death of Turing as a known language will follow. Well not exactly they still use it as an educational tool but I doubt even I’ll remember Turing in a few years.

[The wiki page is probably the best source of info. apparently there are other variants that aren’t totally dead ](https://en.m.wikipedia.org/wiki/Turing_(programming_language))",1632275619.0
pst2jb,What would you like me to ask Leslie Lamport (2013 ACM Turing Award) in my video interview tomorrow?,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pst2jb/what_would_you_like_me_to_ask_leslie_lamport_2013/,1,"As the title says, tomorrow (September 22nd), I will be recording a video interview with Leslie Lamport and would love to be able to ask him some of your questions. I am a mathematician, rather than a computer scientist, so please keep them as understandable as possible - or at the very least provide some links where I can find out more about a particular topic - but otherwise I'd love to hear what you would be interested to ask him - thanks!

Full bio (courtesy of ACM):

2013 ACM Turing Award for fundamental contributions to the theory and practice of distributed and concurrent systems, notably the invention of concepts such as causality and logical clocks, safety and liveness, replicated state machines, and sequential consistency.

*If we could travel back in time to 1974, perhaps we would have found Leslie Lamport at his busy local neighborhood bakery, grappling with the following issue. The bakery had several cashiers, but if more than one person approached a single cashier at the same time, that cashier would try to talk to all of them at once and become confused. Lamport realized that there needed to be some way to guarantee that people approached cashiers one at a time.* *This problem reminded Lamport of an issue which has been posed in an earlier article by computer scientist Edsger Dijkstra on another mundane issue: how to share dinner utensils around a dining table. One of the coordination challenges was to guarantee that each utensil was used by at most one diner at a time, which came to be generalized as the* ***mutual exclusion*** *problem, exactly the challenge Lamport faced at the bakery.*

*One morning in 1974, an idea came to Lamport on how the bakery customers could solve mutual exclusion among themselves, without relying on the bakery for help. It worked roughly like this: people choose numbers when they enter the bakery, and then get served at the cashier according to their number ordering. To choose a number, a customer asks for the number of everyone around her and chooses a number higher than all the others.*

*This simple idea became an elegant algorithm for solving the mutual exclusion problem without requiring any lower-level indivisible operations. It also was a rich source of future ideas,* *since many issues had to be worked out. For example, some bakery customers took a long time to check other numbers, and meanwhile more customers arrived and selected additional numbers. Another time, the manager of the bakery wanted to get a snapshot of all the customer numbers in order to prepare enough pastries.* *Lamport later said ""For a couple of years after my discovery of the bakery algorithm, everything I learned about concurrency came from studying it.""*

*The Bakery Algorithm and Lamport's other pioneering works -- many with amusing names and associated parables -- have become pillars of computer science. His collection forms the foundation of broad areas in concurrency, and has influenced the specification, development, and verification of concurrent systems.*

Key headings summarising his work:

**1. Mutual Exclusion solutions and the Bakery algorithm**

**2. Foundations of Concurrent programming**

**3. Foundations of Distributed Systems**

**4. Fault tolerance and State Machine Replication**

**5. Formal specification and verification of programs**

**6. LaTeX**",1632261109.0
psqdx8,Is it possible to learn logic and critical thinking If so how? What are some good resources you reccomend,28,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/psqdx8/is_it_possible_to_learn_logic_and_critical/,18,,1632253520.0
pspap8,Question regarding running time and big oh,27,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/pspap8/question_regarding_running_time_and_big_oh/,19,"Hello, if i have a piece of code like this:

    for(int i = 0 ; i < 100000; i++){
       for(int j = 0; j < n ; j++){
         \\-------------}
     }

i can say that it is O(n), right?

so another one which is like this:

    for(int i = 0 ; i < n ; i++){
       for(int j = 0 ; j < n ; j++){
          \\-------------}
          }

here i can say that is O(n\^2)

My question is that if n in the first code block is equal to 10\^5 and n in the second block is equal to 10\^5 also, which are maximum values n can take then the running time is equal for both even though they have different big oh?",1632250374.0
pshyvz,Question on genetic algorithms.,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pshyvz/question_on_genetic_algorithms/,1,"I am intending to optimise a process using genetic algorithm. 

I intend to build a neural network model of the process and optimise the output attribute using GA.

My tutor is adamant I do not need to build a model and should run GA on the raw data.

My belief is that not using a model would only allow me to find the optimal parameters which have already been used as GA would not be able to find the output of attribute combinations which are not in the dataset.

A model would allow all combinations to have a predicted output so an optima may be found that has not been tried in the real world.

Am I correct in my thinking or is my tutor correct.

Thanks!",1632228241.0
pse2bo,What could be the research parameters for an It project? Let's set the scale of the project to Cloud but features Should be new? How can a team define the research metrics for the project? How can one plan it out?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pse2bo/what_could_be_the_research_parameters_for_an_it/,1,,1632210152.0
psdy59,Compiler Design: How does the parser work if the language supports user-defined operators?,15,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/psdy59/compiler_design_how_does_the_parser_work_if_the/,9,"I've been diving into parsing and language design recently, and I see that a lot of programming languages use a sort of grammar with hard-coded operators. How do some languages have the support for custom operators? How does the parser work? Would the grammar then still be definable by EBNF / PEG? Or would user-definable operators mean that they would need to use a different system together?",1632209567.0
ps8py7,Object Oriented Programming,102,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/ps8py7/object_oriented_programming/,44,"What is your opinion on object oriented programming? 

From what I’ve seen, people either love it or hate it, but I believe in a middle ground where you think it’s useful for some things and not so much for other things.

What are your thoughts?

*Edit: Why do you or why do you not like it and do you think that the dogmatic “it’s always bad” or “the industry should get rid of it completely” are good?",1632188410.0
prunhv,"How Does the CPU Output “Hello, World!” To the Command Line",84,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/prunhv/how_does_the_cpu_output_hello_world_to_the/,20,"I understand the part where the cpu does an interrupt (system call) and the PC is set to point to the beginning of the kernel function that implements whatever specific system call was invoked.

But the kernel function (write in this case) is just a bunch of instructions that get executed by the cpu as well. I’m confused as to what happens next since I’m certain there are no specific cpu instructions to write to standard output. We always relied on the OS to handle it for us.

We are at the kernel instructions running on the cpu at this point. So what cpu instruction actually get us from cpu to writing a message on screen?

* If it helps, assume x86_64 architecture on Linux .",1632143699.0
prem7e,Why don't integers or float take up more space in computer memory?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/prem7e/why_dont_integers_or_float_take_up_more_space_in/,7,"So i was searching a fix for my game bc textures and vertices on ground and models would start to jump around bc of floating point error.I searched how to upgrade from 32bit int-float limit to 64bit one but i couldn't find anything, even if i did convert it to 64bit limit there would still be this bug after certain distance.
Then i did a bit of research on google bc i was wondering why were computers limited/only use 32/64/128/256bit int and unity uses 32bit floating point presicion and it only works well till 100 000 units from Vector3.zero.
After that u get error/warning in inspector and the models and textures are glitchy.
So after googling i found that:

Double-precision floating-point format (sometimes called FP64 or float64) is a computer number format, usually occupying 64 bits in computer memory

Only 64bits in memory?
Why dont computers use 1024int numbers or even 4096/8192bit integers.
Compared to normal hdd that have 500+GB of memory or even RAM memory if they are talking about it,its still a fraction of it and wouldn't take much space.
And CPUs today can do billions oc calculations per second so even if its a 8192bit numbers pc shouldnt have a problem.",1632079586.0
pr87l2,"What is the difference between cache in the processor and ""cached data"" on mobile phones?",0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/pr87l2/what_is_the_difference_between_cache_in_the/,3,"As far as my knowledge goes, the cache data in the processor is the fastest memory in the memory hierarchy, with the cost of it being so small (usually in kilobytes).

  
My question is, if the cache memory is so small in the CPU, how can android store ""cached data"" that is up to 3-4 GB and sometimes even more, when the cache in the CPU is 1000x smaller in size?",1632059108.0
pr5s8k,"Many confuse ""Computer Science"" with ""coding""",475,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/pr5s8k/many_confuse_computer_science_with_coding/,98,"I hear lots of people think that Computer Science contains the field of, say, web development. I believe everything related to scripting, HTML, industry-related coding practices etcetera should have their own term, independent from ""Computer Science.""

Computer Science, by default, is the mathematical study of computation. The tools used in the industry derive from it. 

To me, industry-related coding labeled as 'Computer Science' is like, say, labeling nursing as 'medicine.'

What do you think? I may be wrong in the real meaning ""Computer Science"" bears. Let me know your thoughts!",1632048819.0
pqwrgv,Any good books on different types of computing systems??,16,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/pqwrgv/any_good_books_on_different_types_of_computing/,0,"I've heard of things and watched several youtube videos of quantum computing, optical computing, neuromorphic computing, on and on. Is there any books or information on these?",1632007155.0
pqwowk,Youtube videos/channels with someone starting a project and going through the process?,36,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/pqwowk/youtube_videoschannels_with_someone_starting_a/,9,"I don't mean a tutorial of a project, I mean including the steps of research and redoing parts of the code, the mistakes and learning, the whole process.

I know it's different for everyone but I feel much more reassured with seeing what someone else's process is. Preferably on a project that's not like 'hello world', like their own application or something. But also not someone toooo advanced who doesn't make mistakes for us to see and already knows what they're doing what direction they're going before they start?

&#x200B;

Thanks!",1632006881.0
pqkib8,Resources about lock-free data structures,3,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/pqkib8/resources_about_lockfree_data_structures/,4,"Hello all,
I'm looking for some good resources to learn about lock-free data structures.

Do you guys have any resources to point out?
Thanks!",1631963411.0
pqhh7v,CMV: The Halting Problem doesn't make sense,0,0.42,computerscience,https://www.reddit.com/r/computerscience/comments/pqhh7v/cmv_the_halting_problem_doesnt_make_sense/,5,"I'm going to be referring to the problem as its framed in this video: [https://www.youtube.com/watch?v=macM\_MtS\_w4](https://www.youtube.com/watch?v=macM_MtS_w4).

My problem with it is twofold: one, its an entirely immaterial program, and arguably doesn't 'do' anything (in a sense, it can't be run--not can't finish, it can't even *begin*), and two, it violates the laws of physics. Nobody was ever expecting a general solution to physically impossible or instubstantive programs, and for that reason I find the halting problem entirely vacuous.

Its very similar to the following 'contradiction':

1. Statement 2 is a lie.
2. Statement 1 is true.

Many people have seen a problem like this before, and its relevantly similar to how the halting problem is laid out. If you accept statement 1, then that implies statement 2 is false about 1 being true, meaning you've contradicted your initial assumption. If you assume statement 1 is false, then that means statement 2 is true, which means statement 1 is true, so which contradictions the initial assumption.

My problem with this 'contradiction' is that its entirely insubstantive. It doesn't bottom out in the real world anywhere, and I'm not convinced the problem can exist anywhere other than pure linguistic pedantics.

To demonstrate this, see a similar problem:

1. Statement 2 is true.
2. Statement 1 is true.

Here's the question: how do you evaluate these two statements? Are they both true? If you assume statement 1 is true, then that means statement 2 is true, which confirms your assumption, no contradictions. So they're true then? But wait, you can do the same by assuming they're false. If statement 1 is false, then statement 2 is false, meaning statement 1 is false, just like your initial assumption.

Technically you can do it with just one statement, to make it more like the halting problem:

1. Statement 1 is true.

So there's absolutely no way to evaluate these statements. My argument is this: 'truth' does not make sense unless you bottom out in the real world somewhere. Truth, accuracy, correctness, all it really means is that there's a 1 to 1 correlation between a representation and reality, but there *is no reality being represented by these insubstantive statements.* It leads to an infinite regress, which is why its unsolvable--you have to know the truth of statement 1 to know the truth of 2, and you have to know the truth of 2 to know the truth of 1.

The contradictory version is the same--also leading to an infinite regress of insubstantive questioning, we just don't notice because we get caught up in the contradiction.

Notice how a series of statements that bottom out somewhere substantive can be solved:

1. Statement 2 is true.
2. Statement 3 is true.
3. Statement 6 is true.
4. Statement 5 is true.
5. Statement 6 is true.
6. Statement 9 is true.
7. Statement 8 is true.
8. Statement 10 is true.
9. Statement 10 is true.
10. I ate eggs for breakfast this morning.

This time, we *can evaluate the statements*, no matter how long it is. This is because we bottom out at something we can empirically evaluate, and then derive the truth value of the other second-order statements (statements about other statements). But when a set of propositions contains *only second order statements that don't refer to anything except each other, you can't evaluate it.* But if you break the infinitely regressing cycle with a substantive statement, the problem disappears. No matter how long the series of meta-statements goes, as long as they bottom out somewhere substantive, you can solve it.

This is relevantly similar to the halting problem, because in the same way that we're evaluating the truth of an insubstantive statement, we're also evaluating the haltability of an insubstantive program. In order for the program to be run, we'd have to know the results of the program, which depend on the results of the program, which depend on the results of the program... ad infinitum. It only becomes a problem because we've made up a completely meaningless and immaterial thing. How would such a program even *start*? Where would it get the inputs? In the same way that circular sets of statements can't begin evaluation, this program can't begin to run.

That's not to say that someone couldn't *attempt* to write such a program. Only that they would fail. The program they generate would be a real thing that exists in the actual world, but it would never be relevantly similar to H+ (as described in the video). To demonstrate, lets try it, and showing why it can't be done will be illuminating.

Suppose you created a Python program that looks like this (ignore boolean/string conversions for the moment):

    # read output from file
    outcome_file = open(""outcome.txt"",""a"")
    outcome = output_file.readlines()
    
    # flip the sign
    if outcome == True: 
        outcome = False
    elif outcome == False:
        outcome = True:
    
    # write to file
    outcome_file.write(outcome)

Also stipulate that the only program capable of creating the outcome.txt file is this program. No other program or user has done so. What will happen? The program will crash immediately if there's no pre-made outcome.txt file (which you can't do, because the program needs to be evaluating the outcome of something it itself generated).

Why'd the program crash? Well, you might say its because I made a mistake, but think about what would be required not to: the outcome.txt file in the second line would have to be ***THE EXACT SAME*** both in contents and time as in the one in the last line. You would literally have to go backwards in time to send the outcome of the program to itself in order for it to run. Turing is correct that this program can't be evaluated, but only because its the kind of thing that violates the laws of physics.

Some people might continue to object: ""That's exactly what Turing was getting at!"", but that's a sleight of hand--the question wasn't whether *H+* can exist, its whether *H* can exist. Sure, we've shown that the program doesn’t work when its own output is required to run it, turning it into H+, but that's only because H+ contradicts the laws of physics. H+ may fail to exist, but that doesn't demonstrate that H can't exist, any more than saying ""you can't run H on a computer that's in the shape of a square circle"" demonstrates that H can't exist. When you artificially construct a contradiction in the scenario, its not a sign of the other entity being flawed, its a sign of the scenario being flawed.

So brilliant. If you know a Python library that can send booleans back in time or perform other logical contradictions, then perhaps this will have practical significance to you. For those of us that want to evaluate real programs that can exist, the halting problem isn't meaningful.",1631947663.0
pqg2y4,Is there a way to download/upload a ton of random data?,1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/pqg2y4/is_there_a_way_to_downloadupload_a_ton_of_random/,4,"I am wondering if there's a way to download/upload random data as fast as your computer can handle to stress test a network. I know you can set the packet size of the ping command, but the bit limit is very low. How could I do something like that but with a ton of data? I am on a Mac btw. Thanks!",1631940798.0
pqffop,Are there Papers that show that OOP helps with reducing perceived complexity ?,97,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/pqffop/are_there_papers_that_show_that_oop_helps_with/,28,"Hey everyone, 

I read about the no-silver-bullet paper which tells us that we can not reduce the complexity of a problem in general. I am looking for a paper though, that investigates if modelling a problem as a system of classes is less complicated for the programmer and other people reading the code, compared to procedural code. Some psychological or empirical data on this would be awesome.

Any good sources, or is this actually a myth?",1631937942.0
pqenvl,Random as Hell (pun intended),2,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/pqenvl/random_as_hell_pun_intended/,15,"This is a very broad question, but I am confused about how random number generators work (edit: how we talk about random generators, and if the word random generator will eventually mean true randomness) . Rather, I'm irritated by people saying, ""random number"" when as far as I'm aware there's no actual way to create a random number using technology alone. There are so many places that ""random numbers"" are used in code, and as user privacy becomes more of a societal issue it makes sense that true random number generators are going to come about. My thing is, why do we always say, ""random number"" when it never actually is, and do you think we will get to a point where what we now call ""random numbers"" will become numbers through function as they currently are and we will have actual randomness based on physical phenomenon which may or may not develop into a new binary phenomenon? (ie. random != random, will it in the future?)",1631934671.0
pqdhk9,How hard it is to manipulate data between apps (e.g third party software)?,3,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/pqdhk9/how_hard_it_is_to_manipulate_data_between_apps_eg/,9,"So the thing is. I’m a very new CS student, I’ve learned programming languages but I have no idea how it is applied in real world. I’m thinking about this thing where I take data from a third-party software and connect things and make sure it works on my website too. Example is below:

1. Let’s say I use a company’s (named A) software to accept orders from customers as well as their delivery address etc.
2. Somehow I will have to collect those data from A and go to my own website, put them on there for other purposes(maybe like deliver or something like that).

When I’m asked to write a programs or functions to do certain things then maybe I can, but ask me about something like above, I’d be dumb. I mean we can do it manually like entering data into our system, but is there/should there be an automatic way?

So, can you explain it to me like I’m very retarded? Thank you so much.",1631929865.0
ppyncv,British computing inventor Clive Sinclair dies at 81,101,0.99,computerscience,https://www.nbcnews.com/tech/tech-news/british-computing-inventor-clive-sinclair-dies-81-rcna2064,0,,1631879412.0
ppozg9,US Military Wants to Predict the Future With Artificial Intelligence,5,0.73,computerscience,https://science-news.co/us-military-wants-to-predict-the-future-with-artificial-intelligence/,7,,1631836227.0
pplt1u,The World’s Largest Computer Chip [2:29 audio],1,1.0,computerscience,https://www.ausum.io/s/O8OaXTn49w7Huwz9d_CeA6lZj1F6UN5q0z95SdmBaU0,0,,1631825369.0
pphwcc,Next level OS,86,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/pphwcc/next_level_os/,68,"Hello! Unix and Windows are old. Computers now faster, stronger, etc. Why there is no new OS that written from scratch? There are some little projects written on rust language but they are only for developer like people. So, the question is, why we still use things older than many of us? :) 

P.S. I am beginner in all this and only want to make things clear.",1631813392.0
ppcsj7,Should a semicolon be terminator or a separator?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ppcsj7/should_a_semicolon_be_terminator_or_a_separator/,3,"Some programming languages (i.e. ALGOL and Pascal) have used the semicolon to separate statements, while Java uses it to terminate statements. Which of these, in your opinion, is most natural and least likely to result in syntax errors?",1631797058.0
ppa590,Are there insiders who know how the SHA-256 hashing algorithm works,0,0.14,computerscience,https://www.reddit.com/r/computerscience/comments/ppa590/are_there_insiders_who_know_how_the_sha256/,5,"There must be some insiders who know how to implement this algorithm at whatever agency (I think NSA) it was created. Since this algorithm is used for mining new blocks for a blockchain, what is stopping these insiders from winning Bitcoin all the time?",1631785597.0
pp6jgs,Why do people prefer Dynamic typed programming languages over typed programming languages?,123,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/pp6jgs/why_do_people_prefer_dynamic_typed_programming/,50,"My go to programming languages are Java and C++. They are what I know best, and am best a coding with. I've tried to dabble in Python and other ""easy"" languages, but the dynamic typed variables make life so much more difficult for me. I was wondering, why do people prefer dynamic typed programming languages over static typed programming languages? From what I've heard, the dynamic type part of python is part of what makes it so slow compared to other programming languages, and I just don't understand why people like dynamic types if they make life harder, and your code run slower. Please enlighten me, why do people prefer dynamic over static typed?",1631767234.0
poys2z,Unsigned and signed saturation of hex addition?,4,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/poys2z/unsigned_and_signed_saturation_of_hex_addition/,1,"So when you normally add two hex values, (for example, 0x610 and 0x A80) you will get 0x090 with 1 bit being a carry and overflow. So the hex value is realistically 0x090 (b0000 1001 0000). How does this value change when it comes to signed and unsigned saturation? We are learning about it in class but I am confused with the process",1631739322.0
powghp,Why is the time complexity of merge sort nlogn and not n+logn,6,0.81,computerscience,https://www.reddit.com/r/computerscience/comments/powghp/why_is_the_time_complexity_of_merge_sort_nlogn/,5,"Why are we multiplying n and logn , the way I understand is log n is the number of times division occurs and n is for merging process.",1631732155.0
popiv4,Three way partitioning of an array around a given range,4,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/popiv4/three_way_partitioning_of_an_array_around_a_given/,3,"See this [article](https://www.geeksforgeeks.org/three-way-partitioning-of-an-array-around-a-given-range/)

 \[**lowVal**, **highVal**\]: Should these two values belong to the array? In other words, should the elements for range be present in the given array?",1631710347.0
poogz8,Maths for CS,51,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/poogz8/maths_for_cs/,48,Which math concepts do you recommend someone should learn before learning computer science?,1631706237.0
ponstr,Rule 110 Cellular Automata Compiler Implementation?,8,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/ponstr/rule_110_cellular_automata_compiler_implementation/,1,"I am fascinated by cellular automata, especially rule 110 which creates complex patterns using very simple rules and can, amazingly, also be used to emulate a universal computer!

[Rule 110](https://en.wikipedia.org/wiki/Rule_110) cellular automata was [proven](https://wpmedia.wolfram.com/uploads/sites/13/2018/02/15-1-1.pdf) to be turing complete. The problem is finding a compiler for it that is at least somewhat practical. There exists a solid [paper](https://arxiv.org/pdf/0906.3248v1.pdf) providing the theoretical recipe for just such a compiler which should output polynomial time/space rule 110 programs. However, I could not find any concrete implementations of said recipe on the internet. The closest candidates are:

* [Machine-110](https://github.com/slightknack/machine-110),
* [c110](https://github.com/jonahs99/c110),

both of which are utterly unfinished and only provide a, more-or-less, useful starting point.

&#x200B;

So... does anyone have any additional references I could look at or, even better, a link to a working compiler? It would be great to be able to play with such a compiler and not spend countless hours designing it myself :).",1631703409.0
podbzs,Turing Oversold - Jürgen Schmidhuber,12,0.83,computerscience,https://people.idsia.ch//~juergen/turing-oversold.html,4,,1631658945.0
po6xdm,USB Protocol question.,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/po6xdm/usb_protocol_question/,7,"Hi!

I studied the way keyboards and other usb devices work, but there's something I still can't understand: if  a keyboard encodes a normal character such as 'A' is encoded using ASCII, then how are commands like 'shutdown' or 'volume up' encoded? They must be encoded in binary, but are there actual lines of code sent by the keyboard for the computer to execute, or is there some pre-defined chart, like ASCII, but for special commands? I thought it might have to do with the driver, but I am using a keyboard that does not seem to use any special driver and yet it has such commands (volume up/down; shut down etc) that actually work.

Thanks!",1631639062.0
po6um9,Know any good ways to learn more about computer science,72,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/po6um9/know_any_good_ways_to_learn_more_about_computer/,29,"I’m in high school and I want to learn as much as possible!
Thanks in advance",1631638834.0
po098u,what to call multi-bit transistor,1,0.56,computerscience,https://www.reddit.com/r/computerscience/comments/po098u/what_to_call_multibit_transistor/,5,my question is we all know computers have 1-bit 1 or 0 but what do they call a computer that have 1-bit that goes through 0 to 4 or something like that,1631615980.0
pnwtqa,Does anyone know how to form a NOT gate that only outputs the NOT value if a second I put is positive.,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pnwtqa/does_anyone_know_how_to_form_a_not_gate_that_only/,9,"Sorry if this is obscure, but I know this must exist.  I am thinking of a logic gate with 2 inputs.  Output would be NOT input 1 if input 2 is positive, otherwise output would just be input 1.",1631598228.0
pntwgy,How do I build an 8-bit Subtractor circuit?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/pntwgy/how_do_i_build_an_8bit_subtractor_circuit/,3,"I've been building some circuits and whatnot in Logisim, and I've looked and looked for a good subtractor circuit but I cant find one. Any sources or recommendations? I've built many but none of them worked.",1631586282.0
pnsaul,I just wanted to share this free online book (physical book is on Amazon) about interpreters and compilers since people ask about this topic from time to time - Crafting Interpreters,76,0.96,computerscience,http://craftinginterpreters.com/contents.html,2,,1631580414.0
pnp559,Are if statements efficient,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/pnp559/are_if_statements_efficient/,7,Why does Python skip code while evaluating if statements? What is more efficient: to skip code or to evaluate for both true and false statements?,1631569405.0
pnners,Computer Science in the 1960s,57,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/pnners/computer_science_in_the_1960s/,17,"Hello all 👋 I'm not sure where else to post this and thought this would be a good place to start. The goal of this post is essentially to find any resources that could help me learn about computer science from roughly 1960 to 1980, and primarily how the science of processor building started to advance. As of right now I'm only aware that around 1970 with the advent of UNIX there was the beginning of a new level of computer processor building, in which they'd use computers themselves to build the processor because they were becoming more complex than the previous use of physical logic modules and all that stuff.

I'm really hoping someone can point me in the right direction as to how that came to be, the languages they were using etc. So far I know that C and Yacc were big parts of the building process around 1971 but I'm not sure where to look to find anything else out that came before that.

Thank you to anyone who can help!

Edit: I noticed in the rules it said no exam questions or something. This isn't for an exam or school related. This is my own personal curiosity",1631564010.0
pnf2d1,Understanding random order hardness result,15,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pnf2d1/understanding_random_order_hardness_result/,0,"Currently studying the following paper:

[""Fair Allocation in Online Markets"" - Gollapudi and Panigrahi 2014](https://dl.acm.org/doi/10.1145/2661829.2662011)

In which they present Theorem 2 as a hardness result for online maxmin matchings (without proof). My thinking for a counterexample which demonstrates that a random ordered stream can give an arbitrarily bad result say with a standard greedy algorithm (and thus verify this theorem):

&#x200B;

Assume we have 𝑛n agents to which we must match a good as they arrive online in a random order (assume we implement a greedy algorithm which always allocates to the least satisfied agent, breaking ties arbitrarily). Additionally, assume an adversary has selected 𝑛 goods with the following properties: 𝑛−1 goods have value 1 for exactly one agent 𝑖i and 0 for all other agents 𝑗 (𝑣𝑗(𝑔𝑖)=1⟺𝑗=𝑖    vj(gi)=1⟺j=i) meaning there is a leftover agent who does not have a specified good, and there is one good of value 1 for all 𝑛 agents. Thus, if the item of equal value to all agents is assigned improperly the maxmin value is 0.

My questions are

1. Does the above accurately capture the hardness of online matching with random order?
2. I am not sure how to approach a probabilistic argument that with high probability, the worst case scenario will occur. My thinking is that we can say the probability the good of value 1 to all arrives in the t-th time is 1/n and the probability it is matched correctly is 1/(n-t). Therefore, the probability it is matched correctly is \\Sum 1/(n\*(n-t)) which is O(logn / n)?",1631538471.0
pn4sj1,Computer Architecture,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pn4sj1/computer_architecture/,5,"I'm in this online class for computer architecture. The course content is literally ""read the textbook"". The assignments are equally as lazy but able to do well with help. I have never learned well from textbooks. I'm not learning a damn thing.

Does anybody know any good novice resources on computer architecture?",1631493606.0
pn4gzx,Question about Computer terms (Public IP address),0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/pn4gzx/question_about_computer_terms_public_ip_address/,4,I'm learning about IP addresses and computer science. Can anyone view your **public IP** address? I know that there are two types: Private and Public Ip addresses. But what does the Public IP address do? Is it an harmful if someone has your public IP address?,1631492408.0
pmw8rv,A bit of confusion regarding time complexity.,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pmw8rv/a_bit_of_confusion_regarding_time_complexity/,8,"From Wikipedia:
https://en.m.wikipedia.org/wiki/Time_complexity

>In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm.

What does ""computer time"" mean here? I thought that time complexity is a theoretical concept, but this seems to suggest that it is a practical one as it refers to ""computer time"". I hope someone will clarify. Thanks in advance!",1631465183.0
pmw0cc,Is there a way to reduce the training data used by machine learning programs?,10,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/pmw0cc/is_there_a_way_to_reduce_the_training_data_used/,7,Ive seen a couple posts about ML recently and how the amount of training data used is so astronomically large. Is there a way to reduce it and would there be a point in doing so?,1631464448.0
pmv9wp,I do not understand the difference between machine learning and deep learning,95,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/pmv9wp/i_do_not_understand_the_difference_between/,25,So I know that deep learning is a subset of machine learning which is a subset of AI. Can someone explain what the difference is. I think I need examples of something that is AI but not ML and something that is ML but not DL,1631462162.0
pmlqqu,Graceful degradation vs fault tolerance,14,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pmlqqu/graceful_degradation_vs_fault_tolerance/,6,"I am reading the book ""OS Concepts 9th edition"" by Avi Silberschatz. There is a paragraph which give brief definitions of graceful degradation and fault tolerance:

""The ability to continue providing service proportional to the level of surviving hardware is called graceful degradation. Some systems go beyond graceful degradation and are called fault tolerant, because they can suffer a failure of any single component and still continue operation.""

So what does the authors mean by ""go beyond"" ? Does it mean fault tolerant systems' performance isn't affected by a single component failure at all ?",1631419478.0
pmjoaf,Writing a Program that Simulates Coin Flips,0,0.37,computerscience,https://www.reddit.com/r/computerscience/comments/pmjoaf/writing_a_program_that_simulates_coin_flips/,4,"Here is a problem I thought of:

&#x200B;

\- Suppose I am watching someone flip a fair coin. Each flip is completely independent from the previous flip.

\- I watch this person flip 3 consecutive heads.

\- I interrupt this person and ask the following question: If the next flip results in a ""head"", I will buy you a slice of pizza. If the next flip results in a ""tail"", you will buy me a slice of pizza. Who has the better odds of winning?

&#x200B;

I wrote the following simulation using the R programming language. In this simulation, a ""coin"" is flipped many times (""1"" = HEAD, ""0"" = TAILS). We then count the percentage of times HEAD-HEAD-HEAD-HEAD appears compared to HEAD-HEAD-HEAD-TAILS:

&#x200B;

    #load library
    library(stringr)
    
    #define number of flips
    n <- 10000000 
    
    #flip the coin many times
    flips = sample(c(0,1), replace=TRUE, size=n)
    
    #count the percent of times HEAD-HEAD-HEAD-HEAD appears 
    str_count(paste(flips, collapse=""""), '1111') / n
    
    0.0333663
    
    #count the percent tof times HEAD-HEAD-HEAD-TAIL appears
    str_count(paste(flips, collapse=""""), '1110') / n
    
    0.062555

From the above analysis, it appears as if the person's luck runs out: after 3 HEADS, there is a 3.33% chance that the next flip will be a HEAD compared to a 6.25% chance the next flip will not be a HEAD (i.e. TAILS).

&#x200B;

**My Question:** Thus, could we conclude: Even though the probability of each flip is independent from the previous flip, it becomes statistically more advantageous to observe a sequence of HEADS and then bet the next flip will be a TAILS? Thus, the longer the sequence of HEADS you observe, the stronger the probability becomes of the sequence ""breaking""?

&#x200B;

Thanks",1631410714.0
pm4149,Is there an online calculator which solves recurrence relations?,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/pm4149/is_there_an_online_calculator_which_solves/,3,Is there an online calculator which solves recurrence relations like T(n) = 2T(n/2) + O(n)?,1631350930.0
pm3pc3,Mark Dean: The man that created the first colored PC monitor and the first gigahertz chip,95,0.93,computerscience,https://blackdoctor.org/mark-dean-computer-engineerings-hidden-figure/,7,,1631349172.0
pm38ze,Difference between spinning and being inside the loop,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/pm38ze/difference_between_spinning_and_being_inside_the/,5,"Hi, 

I was going through an MIT [lecture](https://youtu.be/5sZo3SrLrGA?t=1754) Synchronization without locks and came to the part where Peterson's algorithm is explained and proved. However, I came across a statement that mentions, it should be spinning here, it shouldn't be in the loop - [here](https://youtu.be/5sZo3SrLrGA?t=1754). Can somebody help me out with this?

&#x200B;

All helps are appreciated :)",1631347019.0
plx224,"Well, wasted almost entire Friday evening by watching this legendary man unveiling truth!",101,0.95,computerscience,https://www.youtube.com/watch?v=EE1R8FYUJm0,5,,1631319957.0
plwjw8,"Unix Shell pipelines that use structured data like CSV, JSON, etc",5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/plwjw8/unix_shell_pipelines_that_use_structured_data/,12,"We know that you can have a REPL that has a full on type system (Powershell) but what about enhancing Unix shells by outputting stoud into a structured format, so that at least you have value pairs instead of scraping giant strings.

The example that got me thinking was when someone used a CSV output of a command, just to import it and use one field.

I’m not PL theorist, so I don’t know if it’s feasible, but basically this would entail every CLI program having one format as a common denominator.",1631318013.0
plf75c,Is the only reason computers need electricity because 1s and 0s are on and off electric charges?,95,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/plf75c/is_the_only_reason_computers_need_electricity/,38,,1631253509.0
pl4uz4,Is a base 10 computer possible?,113,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/pl4uz4/is_a_base_10_computer_possible/,50,"I learned computers read 1s and 0s by reading voltage. If the voltage is >0.2v then it reads 1 and <0.2v it reads 0. 

Could you design a system that reads all ranges, say 0-0.1, 0.1-0.2....0.9-1.0 for voltage and read them as 0-9 respectively such that the computer can read things in a much more computationally-desirable base 10 system (especially for floating point numbers)

What problems would exist with this?",1631216062.0
pks795,Question about Data Sets and Machine Learning (NMT),2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pks795/question_about_data_sets_and_machine_learning_nmt/,0,"So I am reading up on training neural machine to do translation. Given a pair of languages and corresponding datasets, it will turn roots suffixes, etc into numbers and learn from that. Very simplified I know. But I read about how this can take a lot of GPU and RAM, but I have a department at my school that will allow use of some higher powered GPU farm. So i can build the model. My question is how are these kind of applications served? not just Google Translate API calls, but once I have the model, does it no longer need the data set it just went through? Otherwise wouldn't I be storing tons and tons of language pairs in a DB and serving those as well? I downloaded an app off Windows Store out of curiosity and turned WiFi off, but when I went to translate it, it asked me to buy premium. So I am wondering how offline would work for an app like this. It says it is GPU intensive so without network or serving this as a web application where the web server is doing the computing, how would this be distributed?",1631168500.0
pkpdat,"I just finished reading ""The Annotated Turing""",34,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pkpdat/i_just_finished_reading_the_annotated_turing/,8,"If you don't know, this book is basically a walkthrough of Alan Turing's breakthrough paper on the Entscheidungsproblem, in which he proves that there cannot exist a general algorithm for determining the solvability of any arbitrary problem (or something like that), and simultaneously invents the famous ""Turing Machine"".

Damn, was it a doozy. I read and understood roughly the first 250 pages, but once I got to the proofs section, it became incredibly hard to follow. After a while I just skipped to the last two chapters for the conclusion. Not proud of it, but I still feel like I got something out of it. Turing machines, and especially the universal Turing Machine, are amazing. It's crazy that he wrote this paper when he was just 24 years old. I also loved the background on Turing's and other mathematicians lives that the author gives you as he guides you through the paper. A solid read, although difficult.

Any of my fellow computer scientists ever give this one a read? How did you find it?",1631156666.0
pkn21x,Any ideas for High School Computer Science Club,86,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/pkn21x/any_ideas_for_high_school_computer_science_club/,35,"I’ve really wanted to make a computer science club due to the fact that we do not have one at my school. Now, a couple of my friends are really interested in the club, but we have no idea what we would do in it. I’ve always wanted to inspire other people with programming because I like it a lot, and I’ve always wanted to share my passion with other people. Any code-related ideas or projects to kick start our new club? Any advice? Thank you!",1631148233.0
pkd3th,What is the impact of polyhedral compilers on numerical computation/ compiled code?,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/pkd3th/what_is_the_impact_of_polyhedral_compilers_on/,0,"If I write performant software, why should I care about polyhedral compilers? Let's suppose I don't care about the details of how the compiler works.

Will it make compilation faster under certain conditions?

Will it make new optimizations possible that weren't possible before?

Will it make compilers easier to write, but not impact me as a downstream user?

&#x200B;

Why does the world need polyhedral compilers?

Don't mean to be snarky, I genuinely think the technology is cool, but I haven't anywhere seen an explicit discussion of impact in the ""market"" of compilers in terms of things users would care about, and have just seen discussions of how the technology works.",1631115988.0
pkc72k,How does AM and FM transmit 0's and 1's?,30,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/pkc72k/how_does_am_and_fm_transmit_0s_and_1s/,17,,1631113205.0
pk3f0c,"Any ELI5 articles/books on how compilers solvers, type checking.etc?",28,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/pk3f0c/any_eli5_articlesbooks_on_how_compilers_solvers/,5,"I want to contribute to the Swift open source language's compiler one day, but I'm trying to learn. I'm looking for relatively in depth readings that explains compilers, especially how type checkers and solvers (words that I see thrown around a lot but don't understand deeply) work to efficiently find all the syntax errors, assuming little to no existing specific compiler knowledge.",1631075004.0
pjzv0m,Using Apache 2.0 licensed GitHub code examples in a commercial online course,4,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pjzv0m/using_apache_20_licensed_github_code_examples_in/,3,"Not sure if this is the right place to ask but if I want to create an online course to sell like on Udemy and I will be referring and copying a lot of the code examples from the public GitHub repo for code samples (demonstrates sample ways the library can be used) by the company who created the software/library that has licensed it under Apache 2.0, do I need to actually mention them and include the NOTICE in any code files I write that have copied code from there?

Also would the answer differ if I just show and explain how to write the code as opposed to sharing actual code files or are they considered the same thing",1631061618.0
pjxvsi,Why is theory important in CS and how is it used to solve problems?,72,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/pjxvsi/why_is_theory_important_in_cs_and_how_is_it_used/,44,,1631054579.0
pjlh5a,"How does ""Engineering a Compiler"" (by Keith Cooper and Linda Torczon) compare to the Dragon Book (Principles of Compiler Design by Alfred Aho and Jeffery Ulman)?",74,0.99,computerscience,https://www.reddit.com/r/computerscience/comments/pjlh5a/how_does_engineering_a_compiler_by_keith_cooper/,3,"Hey everyone, So I have been thoroughly and religiously studying Compilers for a couple of months now. In addition, to the Dragon Book, I have been taking video lectures and exploring through slides online that have been compiled (no pun intended) by people that have worked with the compiler technology for years. So anyway, I have familiarized myself with all the basics (Scanning, Parsing (including generating parse trees and ASTs), and even code generation, as well). I have a good understanding of the complex algorithms involved in some of these stages, and I have a good intuition on the complexity involved in all this (in particular, I understand what problems are undecidable for a programmer. What are the pros and cons of different techniques and so on). 

So anyway, one of the lecture series that I watched had taken upon ""Engineering a Compiler"" as their reference book, and I have explored some portion of this book (of course, I am still at the very beginning), and my initial thoughts are: 

\- This book comes across as very beginner-friendly 

\- I like how it builds from the basics. So, for example, while constructing a scanner, we get to see the application of finite-state automata in action. So the book tells us how to go from a regular expression to non-deterministic finite automata using Thompson's construction algorithm and how all this process leads us to recognize words. We get to learn about this process within the first 50 pages of the book. In contrast to this, the dragon book jumps straight into the more complex stuff (such as writing context-free grammar, building a syntax-directed translator, and some other notions from a parsing point of view). In other words, it seems to skip a lot of stuff at the beginning that should have been introduced before we had jumped into the more complex stuff. 

\- As mentioned above, ""Engineering a Compiler"" takes a very programming-oriented approach. It is easier to comprehend things from a coding point of view, and it is convenient to map the theory into actual practice.

\- The notion of ""Classic Expression Grammar"" is introduced in a very beginner-friendly manner in that book. 

Anyways, I am still in the process of learning and exploring as much as I can, so I am not sure how much weight and value my opinion carries. I am reading up on both texts. In my opinion, the Dragon Book is very detailed, in the sense that each sentence can be a book on its own at times, but ""Engineering a compiler"" seems more beginner-friendly. One more thing that I have heard people say is that some of the optimization techniques presented in the dragon book are a bit outdated, but I am still looking forward to reading about that. 

Anyways, these were just my two cents on both these books. Now, why do I ask this question from you guys? I would love to get some insights from a professional or someone more educated on this stuff. Also, if you find any misconceptions in the above passage then feel free to correct me on that. I come here to learn more.",1631015265.0
pj4mak,Hey !,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/pj4mak/hey/,12,"So, I’m learning to code and I had a huge question

How does multiple language understand each other

How can a server or application be made in python, c, c++, java and other languages 

Thanks in advance !",1630950925.0
pixs57,How exactly does the communication layer of a given blockchain work?,2,0.62,computerscience,https://www.reddit.com/r/computerscience/comments/pixs57/how_exactly_does_the_communication_layer_of_a/,3,"For the most part, I understand that in any system that requires communication between two or more devices, you really just need the IP of the device in order to connect and send data. 

Taking from that above concept, being able to create a blockchain that run in a LAN environment for the most part is easy, as getting access to the IP of a device over a local network is pretty simple and sending data between the two is straightforward. For the most part, online tutorials can show how that can work. 

What I don't understand is how to create or convert an existing blockchain that runs in a LAN environment to one that can accept nodes from outside a local network? How do you make a blockchain be accessible globally, and not just accessible within a local wifi network? 

How exactly does that initial connection get established to a given blockchain and what happens when that new node needs to be  indoctrinated? 

I've been looking all around to find information that talks more deeply about the actual communication between nodes and how the actual network layer works, but I've come across very little. 

I've also looked at the source code of bitcoin to see if that could give me any clues but it just left me more confused. 

If anyone has knowledge on how this works, or any resources that can point me in the direction of greater understanding, it would be greatly appreciated.",1630928425.0
pivg21,Question about reserverd IP-adresses,29,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/pivg21/question_about_reserverd_ipadresses/,9,"recently in class I came across the subject subnetting. My teacher told me that in a local area network the IP-adresses .0, .1 and .255 are unavailable. Unfortunately he could't elabroate why these adresses are already taken and which services run under these adresses.

Could anyone give me a brief explanation?",1630917153.0
pipyku,Free Programming Lessons,97,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/pipyku/free_programming_lessons/,54,"Hey everyone,

I've been a software developer for about 6 years now and i love teaching. I have taught or rather coached programming to more than 100 hundred students over the last past few years.

I moved to the States not so long ago and ever since i started my full time job here I've been thinking of teaching programming again. Ideally I'd like to teach beginners and intermediate folks separately - focusing on fundamentals for beginners while maybe building a full blown app with dev ops and what not for intermediates. Given the pandemic and delta variant I'd like to do it virtually but hit me up if you'd be interested. 

I've always seen programming as a welcoming discipline and this is my way if giving back! 

Cheers!

Update:
I'd like to use Golang as the language of choice!

Update 2: 
Thanks for the overwhelming response everyone. I am currently putting together a course document outlining what we will study over the next few months. It is a google doc. I will paste the link here once it's ready to get your inputs. Excited to be doing this again!

Update 3: I have created a discord server - everyone interested please join it. I will keep posting updates over here. I am fairly new to discord and don't completely understand how to use it. Any help is appreciated. See you guys there
https://discord.gg/HaY9ttDX",1630893078.0
pin2ke,"I'm curious, what sounds harder to you: Learning a foreign language or learning a programming language?",80,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/pin2ke/im_curious_what_sounds_harder_to_you_learning_a/,55,,1630881887.0
pilhhj,"Data Structures, Algorithms and Machine Learning",3,0.71,computerscience,https://www.reddit.com/r/computerscience/comments/pilhhj/data_structures_algorithms_and_machine_learning/,5, How important do you guys think that the knowledge of data structures and algorithms is for machine learning or learning ds&algo is obsolete now?,1630876332.0
pigh0a,What could you do with 1TB RAM?,120,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/pigh0a/what_could_you_do_with_1tb_ram/,71,,1630860160.0
piejh5,How much is a word for a 32 bit linux ?,7,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/piejh5/how_much_is_a_word_for_a_32_bit_linux/,6,"I'm learning 32 bit assembly on linux and the instructor taught that to define a word which is ""two bytes"" we use dw which stands for define word. The nasm compiler produces error when we try to define more than two bytes with ""dw"".

But, the output of the examine command in memory suggests otherwise, there are ""bytes"" , ""halfword"" which is two bytes, ""word"" which is 4 bytes and ""giant"" which is 8 bytes.

So, is ""word"" 2 bytes or 4 bytes. I'm confused.

&#x200B;

EDIT:

Ok, So I did a bit of research and found that on both x86 and x86\_64. A word is 2 bytes.

>Architecture  
In the x86 PC (Intel, AMD, etc.), although the architecture has long supported 32-bit and 64-bit registers, its native word size stems back to its 16-bit origins, and a ""single"" word is 16 bits. A ""double"" word is 32 bits. See 32-bit computer and 64-bit computer.

source : [PCmag.com](https://pcmag.com/)

The question remains, why is the gdb's examine command treating a word as 4 bytes, and not two.",1630853757.0
pi9l3k,"Time sync in decentralized network, where time accuracy is one of most valuable component.",45,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/pi9l3k/time_sync_in_decentralized_network_where_time/,21,"Let say we want to have decentralized network(over internet) where you must provide proof if you are aware of current accurate time with tolerance of few milliseconds to max 1 second.

As an independent node you are free to use any sources to modify/set/fix your time accuracy before you join the network.

Now, let say we want to make sure our time accuracy is not compromised/corrupted/censured, etc..

What would be your approach to solution for this problem?",1630831948.0
pi22kr,What exactly does a gpu do?,29,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pi22kr/what_exactly_does_a_gpu_do/,18,"What kind of instructions does it complete? What does it need to do for the display to work? I couldn't find any info online, the only thing they say is ""It conpletes simpler instructions and faster than the cpu.""",1630798263.0
pi1z15,Big Omega,1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/pi1z15/big_omega/,6,"Hello! I was wondering why having a running time of big-omega(1) of an algorithm is practically meaningless? I understand why having a running time of O(n^2) of an algorithm is meaningless, but confused on why big-omega(1) is meaningless. Thanks!",1630797869.0
phzbuq,How does the machine know to use two's complement? How does it know it's appropriate?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/phzbuq/how_does_the_machine_know_to_use_twos_complement/,7,"There is something I never understood with two's complement. 

&#x200B;

Let's say the *machine* sees the line: 1111

&#x200B;

How does it know if it is dealing with a negative number or not(is it 15 or is it -1)? I remember asking the professor this, but he just kind said ""there's a flag"" but I never got any more details on this. I am not in college anymore but I've been taking up hobby projects lately and this came up and now I wanna know lol.",1630788381.0
phd414,Working on writing a compiler [JS],10,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/phd414/working_on_writing_a_compiler_js/,10,"Hello reddit,

I've been working for the last few weeks on writing my own programming language.

I've written it so far in non-proprietary javascript, and I've finished the tokenizer (lexer) and am just getting to the Abstract Syntax Tree. However, I'm considering making it a compiled language (at this point I can still change that), and I'm looking for what I should be compiling it into.

I know I don't want to use the JVM or the C# machine, but I'm a bit stumped on just *what* I should compile it into. I was thinking either C or straight up Assembly (probably through another intermediary, the way Node.js does). 

TL;DR: I want to find a language to compile my own custom programming lanugage to, but I need help finding/weighing the options.",1630700641.0
ph8qzn,Reference for Cook Levin theorem by reducing RAM model to SAT,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/ph8qzn/reference_for_cook_levin_theorem_by_reducing_ram/,0,Looking for a reference for Cook Levin theorem by reducing RAM model to SAT.  I'm trying to understand the Udacity video,1630686934.0
ph8cmg,Does the number of checks in an if statement affect time complexity?,15,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/ph8cmg/does_the_number_of_checks_in_an_if_statement/,12,"For example do these statements have the same time complexity simply by both being ifs:
If (x==1)
If (x==1 && y==1)

Also do arthemetic operations slow things down in If statements:
If (x==1)
If (x+31/2>=1)

Not hw related, just curious since I never learned about these cases.",1630685702.0
ph744s,Book Recommendation,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/ph744s/book_recommendation/,9,"I've been watching a lot of videos, and reading a lot of articles, but I feel like a good introduction book for computer science. Even if it isnt an introduction, any good recommendations??",1630681941.0
ph1id6,Cs50 courses advice,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/ph1id6/cs50_courses_advice/,6,Just wanted to ask if someone has taken this course in the past? How was your experience with Harvard’s cs50 introduction to computer science course? Does it build a good base for learning advanced Cs?,1630660104.0
ph0nk6,"Programmers who lost interest in coding, what do you do to regain passion?",161,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/ph0nk6/programmers_who_lost_interest_in_coding_what_do/,37,,1630655595.0
pgzg22,extremely large battlefields in games question,17,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/pgzg22/extremely_large_battlefields_in_games_question/,7,"what would it hypothetically take to render in a battlefield (specifically in destiny 2) but any fps shooter in general, where everyone in the server could all be on at one time at one point fighting against one giant army with maybe double or triple the ads? or have it be half half, a battle that you couldn't even replicate in real life with so many people. i realize that you would probably need each person to have like a super computer, but would it be possible? destiny 2 has  **31,000,000** players for reference sake. oh and the internet would probably need a ""bit"" of an upgrade.",1630649681.0
pgwidh,Difference between the domain-specific language and problem-oriented language,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pgwidh/difference_between_the_domainspecific_language/,0,Hello! What's the difference between the domain-specific language and problem-oriented language?,1630637448.0
pguxfc,Resync of Huffman codes?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pguxfc/resync_of_huffman_codes/,0,"Can anyone point me to any academic papers concerning the resyncing of Huffman codes?  Specifically, I have a case where I am observing a session and may occasionally lose the dictionary but want to try and restart/resync with processing the data.  It's fine if I have to discard for a while until I can rebuild the dictionary, just not sure if this will work in practice.  Any pointers would be greatly appreciated.",1630631474.0
pgutbh,Should I be ashamed as a computer programmer to be looking at Linux tutorials?,3,0.58,computerscience,https://www.reddit.com/r/computerscience/comments/pgutbh/should_i_be_ashamed_as_a_computer_programmer_to/,9,I studied computer programming for 5 years and have had a great high paying job as a computer programmer for 10 years with barely any Linux experience. I took a basic course my first year so I know a little bit of bash but not enough to know or remember what these certain errors mean. I Someone recently quit and they were sr programmer so my boss wants to promote me but sr programmer at this certain job MUST know Linux.Should I be ashamed? Or should I just say I’ve used Mac ever since I was a child and that’s the only thing I’m used to.,1630631037.0
pgp08d,Is there footage of linus torvalds getting technical?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pgp08d/is_there_footage_of_linus_torvalds_getting/,4," recently saw a youtube video title that captured my attention ""it said something like ""mark zuckerberg lecture cs34"" and i thought he was actually teaching a cs course. Turns out he was just giving a speech.

But that peaked my curiousity, are there any videos/blog/text of linus torvald giving a technical overview of something? Or a video of him coding something? Just  curious, and thank you in advance for answering",1630611292.0
pginug,"Programmers who work 8 hours a day, what do you do to reduce screen fatigue?",232,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/pginug/programmers_who_work_8_hours_a_day_what_do_you_do/,117,,1630591824.0
pg70sn,How do computers compress and uncompress files?,68,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/pg70sn/how_do_computers_compress_and_uncompress_files/,21,"How do computers represent the same amount of data with less of it? If computers are only shortening the code / number of bits taken up, why do they need to be uncompressed, such as converting to a .zip file, and then extracting the data from it?",1630543505.0
pg5ij6,I watched a documentary about the Silk Road and they were able to find homie off his IP. Got me thinking…,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pg5ij6/i_watched_a_documentary_about_the_silk_road_and/,7,"If DHCP leases IP address and they change over X days/weeks, can an old IP address be traced back to you? Unless it’s static, current, or logged by ISP or etc. there’s not much you can do with it right?",1630538117.0
pg2la7,Looking for a program that can crunch some social media platform data into a social/cluster diagram,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pg2la7/looking_for_a_program_that_can_crunch_some_social/,4,"I have mined a bunch of raw data from a social media platform and now I want to analyze it all and draw relationships between users. My goal is to be able to generate a social/cluster diagram that visualizes the strength of relationships between users based on their various interactions with one another through likes/votes, shares, comments, etc.

I've come up with my own rudimentary ideas, one of which is to calculate an ""affinity"" score between users and then use all of these scores to create clusters of users, but it's getting really complicated and I figure I am just recreating what someone else has already done.

Is there any software that will allow me to plug my data into it? Restructuring my data for input wouldn't be an issue at all. I prefer software/online service that is 1) free with no paywalls to unlock features and 2) doesn't require registering with any personally identifiable information. This is just a personal project of mine, so I don't need anything state-of-the-art.",1630528826.0
pf5ktp,I created an assembler for my CPU emulator,73,0.97,computerscience,https://v.redd.it/1bdbp3sv0pk71,7,,1630415562.0
pf13ka,Breaking the software licensing of early-2000s abandonware: reverse engineering for software preservation,2,1.0,computerscience,https://yingtongli.me/blog/2021/08/29/drm5-1.html,0,,1630395291.0
peww3x,Do software engineers have to remember everything learned in Computer Science?,136,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/peww3x/do_software_engineers_have_to_remember_everything/,49,"Examples: logic gates, binary representation, ALU, CPU components. transistors and more. When you work, are you expected to remember all of these and how they work?",1630376963.0
pewl46,Computer scientist warns global internet is not prepared for a large solar storm,12,0.73,computerscience,https://insidermag.net/computer-scientist-warns-global-internet-is-not-prepared-for-a-large-solar-storm/,3,,1630375833.0
pef6an,How to get started with Compilers?,21,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/pef6an/how_to_get_started_with_compilers/,7," I am a ML engineer who has been doing hardcore ML stuff since undergrad and in 2 years of work experience as well. Lately I have developed interest in Computer Systems, understand how computers work from ground up. Particularly, I'm super interested in compilers and want to do some projects to gain experience in it.

Can someone suggest some track (courses and projects) to get somewhat proficient in compilers for a beginner? (Similar to what [deeplearning.ai](https://deeplearning.ai/) or FastAI is for ML)",1630318354.0
ped1c9,Inputs to Program to Illustrate Halting Problem,6,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/ped1c9/inputs_to_program_to_illustrate_halting_problem/,2,"Is this illustration of the haling problem correct: https://stackoverflow.com/a/20231503/3042018

```
function halts(func) {
    // Insert code here that returns ""true"" if ""func"" halts and ""false"" otherwise.
}

function deceiver() {
    if(halts(deceiver))
        while(true) { }
    }
```

If so, why do so many descriptions involve a function which takes two arguments, where one is the program and the other is the same program as program input?

E.g.:

*Assume we have a function Halts(P, W) that halts if program P halts on input W. Then we write this perfectly-valid Python function:*

```
def K(P):
    if (Halts(P, P)):
        while True: pass # run forever
    else:
        return 42 # halt!
```
*Now, what happens when we call K(K)?
If Halts(K,K) is True, then K(K) hangs (runs forever)
If Halts(K,K) is False, then K(K) halts
So either way, Halts(K,K) is wrong.
Thus, the Halts function cannot exist!*",1630307466.0
pecqes,Why is it Impossible to Copy Over Programs to Another System?,31,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/pecqes/why_is_it_impossible_to_copy_over_programs_to/,18,"If you have all of the data required to run the application (ie: Discord, Spotify, Slack, Age of Empires II), then what makes it so that it makes it nearly impossible to copy this data to another computer? I assume that it must be that there is a LOT of data of MANY various types that must be allocated to memory where the CPU knows it is located in. 

I'm a Windows and Linux user, and with Windows I understand more. There are so many hidden files and a registry, and complexity - but with Linux it seems that all of a program's binaries are present within a folder ready to be run alongside config files. 

Overall, any input on the way that an application is installed and executed would be very insightful and awesome!",1630305969.0
pe60nt,Adaptive KD trees,9,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/pe60nt/adaptive_kd_trees/,0,"Good (whatever time),

I'm looking for good references for recent work done creating KD trees from embedding spaces, particularly if those embedding spaces were then reshaped in the experiments using a control agent.

I've found a few sources that discuss building and using trees for nearest neighbor search, but none that discuss actively maintaining and refitting the trees to a non-stationary space.

Thanks for any help you can give!",1630278699.0
pe0kzr,/r/computerscience hit 200k subscribers yesterday,84,0.93,computerscience,https://frontpagemetrics.com/r/computerscience,5,,1630261170.0
pe0j84,Question about the ABC and Z3 computer,7,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pe0j84/question_about_the_abc_and_z3_computer/,1,Without getting too technical can someone explain to me what is meant by the z3 being the first programmable computer. What exactly did it offer that was programmable and how did the ABC not have it? Also was the ABC computer fully mechanical? Was the Z3 the first computer to use relays? Thank you!,1630261004.0
pdvn89,Can someone please explain to me how we use the halting problem to solve other undecidable problems ? I have many examples but I just don't understand how we reduce ...,0,0.47,computerscience,https://www.reddit.com/r/computerscience/comments/pdvn89/can_someone_please_explain_to_me_how_we_use_the/,3,,1630245276.0
pdfd04,Mechanism of Howard's algorithm,1,0.67,computerscience,/r/compsci/comments/pdfbif/mechanism_of_howards_algorithm/,0,,1630176001.0
pddph3,ELI5: What is the difference between strong and eventual consistency ?,20,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/pddph3/eli5_what_is_the_difference_between_strong_and/,5,Analogy from real life would help as well,1630170551.0
pdc4en,Can you combine computers?,44,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/pdc4en/can_you_combine_computers/,16,I don’t know much about computers so i figured i’d ask the community. Say I have like 10 average power Dell work computers. Can I take the hardware from all of them and chain them together to get a better computer? Similar to how flash memory is additive ex: plugging in an additional flash drive means more overall storage,1630165410.0
pd4qp6,Tree data structure and multiple traversal approaches,2,0.67,computerscience,https://stackfull.dev/series/tree-javascript?ref=reddit_cs,0,,1630132151.0
pd04of,I don't understand how to find the maximum value of a 32 bit floating point signed number,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/pd04of/i_dont_understand_how_to_find_the_maximum_value/,1,"I understand 2^x gives you the max number for x unsigned bits. but say we're using IEEE-754 32 bit floating point number. Shouldn't the maximum value be 10^256 x 2^23? 

[This site](https://www.sounddevices.com/32-bit-float-files-explained/) says:

> The largest number which can be represented is ~3.4 x 1038, and the smallest number is ~1.2 x 10-38. Doing the math:

> dBnoise = 20 x log (1.2 x 10-38) = -758 dB

> dBmax = 20 x log (3.4 x 1038) = 770 dB

Yet the [wikipedia page on bit depth](https://en.wikipedia.org/wiki/Audio_bit_depth#Quantization) says there's only 192dB of dynamic range with a bit depth of 32

Where does this extra dynamic range come from and how do you actually calculate the maximum number a 32 bit floating point signed number can be?",1630112387.0
pczjmr,How do you pronounce char?,124,0.96,computerscience,https://www.reddit.com/r/computerscience/comments/pczjmr/how_do_you_pronounce_char/,115,"I personally pronounce it like car, but I have recently found out that this is weird. Makes sense to me because character would be pronounce like car and because char is just abbreviated character it makes sense you would pronounce it like car. But I guess this isn’t how most people do it, what’s everyone else’s opinion?",1630110091.0
pcunb7,Textbook Suggestions for Core CS Subjects?,14,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pcunb7/textbook_suggestions_for_core_cs_subjects/,4,"I am going to be buying textbooks for some CS fundamentals like OS, DBMS, Networks and some other subjects. 


I am looking to buy textbooks that give both a solid base for theory and also practical implementations and examples (code and algorithms) of the the chapters/ topics. **I may not get the latest or even the newer editions for these textbooks, is that OK?**

* For Operating Systems, I was thinking of *Operating System Principles by Abraham Silberchatz, Peter B. Galvin, Greg Gagne* or *Modern Operating Systems by Andrew S. Tanenbaum* I can't decide which one to get, I heard they are both good for OS theory. Looking for suggestions for a practical/code-oriented OSdev book.


* For Database Management Systems, *Database System Concepts by Silberschatz, Korth* is the only one I know but if there are better texts available, let me know.

* For Computer Networks,  I have read *Computer Networks by Andrew S Tanenbaum, David. J Wetherall* and liked it enough to consider buying it. But again, lmk if there are better alternatives.



* For Compiler Design, I read a bit of *Compilers: Principles, Techniques and Tools by Alfred V. Aho, Monica S. Lam, 
Ravi Sethi, Jeffry D. Ullman* but apparently it is outdated. So I would love some newer, more practical/implementation oriented book recs.


* Probably the two **most important of ALL** subjects, **Data Structures and Algorithms**. Need suggestions for the best books and MOOCs/courses out there for DS and A from both a compsci/academic and a Leetcode pov. I really need to learn problem solving and how to implement DSA and get better at leetcode and codechef et al.


* For Discrete Mathematics, I heard both good and bad things about *Discrete Mathematics and its Applications with Combinatorics and Graph Theory- Kenneth H Rosen*. Apparently, it's a bit hard to understand and I am not the sharpest tool in the shed lmao. So I need a really good book or course for this as I have already failed this class once. Something focusing on Graph theory would be great too!


* I also need recommendations for Computer Architecture too (or should I just do nand2tetris?), Distributed Systems and Theory of Computation?


I would really appreciate your help and suggestions here for these and any other subjects or textbooks (like SICP, should I buy it?) you consider would improve and expand my engineering know-how and make me a better engineer/developer.

Thanks in advance.",1630093737.0
pcq1om,"Visualize your git repo: most frequent edited files, most active contributors, commits count per weekday. Interesting?",37,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/pcq1om/visualize_your_git_repo_most_frequent_edited/,10,"I would like to validate the idea of a side project. You know how it is, you start and never finish unless you are convinced that it really makes sense. That's why I need your feedback, if you are interested, others also might be. 

If you don't like the idea, downvote immediately. It is also valid feedback.

Thanks in advance 🙏

Imagine a web app where you login via GitHub or paste a link to a public repo.

 In seconds, several interactive plots appear. For instance:

1. Bar chart: what average count of commits in each week day
2. Bar chart: average file size in for each extension
3. Histogram of lines of code
4. Time series graph: repo size in LOC over time
5. Time-series: count of bugs over time

&#x200B;

What information you would like to get about the repository?

Does it interest you at all?

Do you know existing online services like that?",1630079557.0
pcpmzd,Attack analysis: How the Sysrv-Hello Botnet compromises a WordPress with default credentials and uses it for crypto mining - New malware version found,5,0.86,computerscience,https://sysdig.com/blog/crypto-sysrv-hello-wordpress/,0,,1630078273.0
pcp8xu,"What would a comprehensive introductory ""assuming you have no prior programming experience"" CS class look like to you?",16,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pcp8xu/what_would_a_comprehensive_introductory_assuming/,15,,1630077051.0
pcng3a,"How to analyze mixture of algorithms? Merge + Insertion sort, theoretical analysis",5,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/pcng3a/how_to_analyze_mixture_of_algorithms_merge/,2," 

I am trying to analyze the complexity (both theoretical and empirical analysis. I read that in the real world use-case (by my prof) that we use a mixture of merge and insertion sort. When the sub-arrays are *small enough* (how small?), we will use insertion sort on the smaller sub-arrays.

I understand that for Merge sort, it is O(n log n) for general cases whereas for Insert Sort, it is O(n) for best case, O(n\^2) for worst case.

How can I do a theoretical analysis on this mixture of algorithms?

As I understand for empirical analysis, I can simply feed the program a collection of dataset and measure the number of comparisons & swaps.",1630071206.0
pcjfym,Asking General Public Responses of AI,0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/pcjfym/asking_general_public_responses_of_ai/,3," Prior, currently I enrolled in Philosophy of Computer Science course. And for introductory, we asked to gather public opinion about current AI (whether about ethics or development).

So I need your opinion, anything, about that. One simple sentence would be enough. For example: ""Peoples exaggerating and overestimating what AI can do. In the end, its just some simple program designed to help us human.""

Thanks before.",1630053929.0
pcezlq,A question concerning bit loss while traveling through a wire,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pcezlq/a_question_concerning_bit_loss_while_traveling/,4,"At ""But How Do It Know?"" by J. Clark Scott 198p(with my bold)

>Enter digital technology to the rescue. When you send a digital code over long distances, the individual bits are subjected to the same types of distortion and noise, and they do change slightly. However, it doesn't matter if a bit is only 97% on instead of 100%. A gate's input only needs to 'know' whether the bit is on or off, it has to 'decide' between those two choices only. **As long as a bit is still more than half way on, the gate that it goes into will act in exactly the same way as if the bit had been fully on.** Therefore, the digital pattern at the end is just as good as it was at the beginning, and when it is converted back to analog, there is no noise or distortion at all, it sounds like the person is right next-door.

I don't understand the bold part. One bit loss can cause different result in many logic gates. If states of lost bits are the same as before, the result should be the same as intended. But generally the result would be changed if bits are interfered. Why does the author say like that?",1630034121.0
pcesjr,What kind of task GPU does?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/pcesjr/what_kind_of_task_gpu_does/,7,"What's the meaning of having a GPU between a CPU and a display screen? I think a display screen has its memory and fetches 3 byte RGB value from the CPU, then it will use that 3 bytes per one pixel and next 3byte and next sequentially. And it seems there isn't any opportunity for GPU to make this process faster and better.

I googled a little and found the GPU is good for efficiency and it's electric circuit is designed for that purpose. But what task it optimizes? I think enhancing CPUs hertz or display screen's hertz or widening bus width will be enough. And also transferring bytes directly from CPU to screen should be faster. So, why is GPU needed? What is the job GPU does?",1630033391.0
pc17ro,Clip: David Patterson on Tesla's D1 Chip - Dojo Project,1,1.0,computerscience,https://soundcloud.com/ieeeras-softrobotics/clip-david-patterson-on-teslas-d1-chip-dojo-project,0,,1629989935.0
pc0vvm,Emulating a CPU in software,618,0.99,computerscience,https://v.redd.it/73nsrb46spj71,37,,1629988878.0
pby682,Basic tips to be good at computer science for absolute beginners,28,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pby682/basic_tips_to_be_good_at_computer_science_for/,20,Im terrible at computer science and am failing i dont wanna fall behind,1629979691.0
pbxe4z,Replacement of CSI in new Wi-Fi standard,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pbxe4z/replacement_of_csi_in_new_wifi_standard/,0, I am new to data collection using Channel state information but I have heard that there is a new Wi-Fi standard replaces the use of channel state information and/or breaks it down to the V value after performing Single Value decomposition. I am completely unsure of this and would like to request more information regarding this. Anyone knows what it is or where I can find more information about it? I am reading about 802.11ax but I think it uses the same base only now its upgraded and uses MU-MIMO. Any help is appreciated!!,1629976494.0
pbpexo,A Neural net to refine the Doom guy ....,39,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/pbpexo/a_neural_net_to_refine_the_doom_guy/,1,"Wrote a paper of how to refine synthetic images to give a photo-realistic presentation

&#x200B;

&#x200B;

[ Original Doom avatar - original vs refined ](https://preview.redd.it/uauh20b6wlj71.png?width=228&format=png&auto=webp&s=9b3a12ba87e86ed879d623a8cdb1bfae375f9e82)

&#x200B;

[ Pixel-Me generated avatar - original vs refined ](https://preview.redd.it/hbhm1mxawlj71.png?width=228&format=png&auto=webp&s=c0258bbcab730762a39c70ffc3170698dfb5bc48)

&#x200B;

[ Fallout 4 character - original vs refined ](https://preview.redd.it/rd2fy1acwlj71.png?width=228&format=png&auto=webp&s=955ecaedd36c55eae197f1716e912c6e64a9baf8)

&#x200B;

* Git: [https://github.com/consequencesunintended/RefinementGAN](https://github.com/consequencesunintended/RefinementGAN)
* paper: [https://arxiv.org/abs/2108.04957](https://arxiv.org/abs/2108.04957)",1629941837.0
pbm9pa,Merging two AVL trees in to one AVL tree,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pbm9pa/merging_two_avl_trees_in_to_one_avl_tree/,13,"I am trying to figure out algorithm that can merge AVL tree A with AVL tree B into AVL tree C. 

All my solutions have time complexity O(min(∣A∣,∣B∣)). But somehow there is way to do it faster.

How would you approach this problem to make asymptotic time complexity better ?

EDIT: All elements of tree A are before All elements of tree B.",1629931121.0
pbiva7,Cybersecurity,7,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/pbiva7/cybersecurity/,6,"For your guys, what are the main themes to learn before get into cybersecurity? Like the fundamentals...",1629920964.0
pbew6d,How to save space in a Trie composed of words?,16,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pbew6d/how_to_save_space_in_a_trie_composed_of_words/,11,"I have a giant Trie which I have created using C# Dictionary (Hashtables).I have an excel sheet with 1 million rows and roughly 10-15 columns,Each cell has a value  as shown below:

https://preview.redd.it/r0kv2w4e6jj71.jpg?width=488&format=pjpg&auto=webp&s=720ce821973cd23e44c17d079bf756fa7bcb6c67

Instead of storing characters as value, I am storing it as follows:

https://preview.redd.it/ag77s28r6jj71.png?width=909&format=png&auto=webp&s=0caf8187cd50a4f48bbf09e0a7a49b526d712c9c

Now the problem is after loading it into the memory this occupies roughly 3-4GBs.It could be the overhead of creating so many dictionaries. The performance is awesome. But the space it occupies is pretty expensive. Is there a way to optimize it?

I will be having a collection of roughly 5000 lineitem rows which I will have to validate against these 1million rows that had been inserted. The row has to match in its entirety.

Is there a way I could optimize this for space?  


**One important piece of information:** \* means that the lineitem could have ANY value for that property. Had that not been the case I could have just concatenated each cell of a row and stored it in a dictionary. 

&#x200B;

Then concatenated lineitem and looked up in the dictionary. Since \* means any value. It won't work.",1629909240.0
pbeklm,Have you created a programming language?,27,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/pbeklm/have_you_created_a_programming_language/,12,"Thinking of designing and implementing a language these days. Ambitiously, I wanted to use it (probably as a DSL), so wondering what other folks have been up to.

&#x200B;

Have you created a programming language? Do you develop in it at all?",1629908293.0
pbde2s,Could you please help me clarify this definition?,1,0.57,computerscience,https://www.reddit.com/r/computerscience/comments/pbde2s/could_you_please_help_me_clarify_this_definition/,1,"Hello, I am trying to become more knowledgeable on CS and current topics and I was recommended Mastering Ethereum. While the first chapter is pretty clear and easy to follow, this definition has many words that aren’t entirely clear for me. 

“From a computer science perspective, Ethereum is a deterministic but practically unbounded state machine, consisting of a globally accessible singleton state and a virtual machine that applies changes to that state.”

- Deterministic state machines: I can’t say I totally understand. Except maybe it seems from Wikipedia that all deterministic state machines are also FINITE, meaning they have a predetermined number of states?

- practically unbounded. Does this have to do with the finite number of states? Is this saying that Ethereum is a finite state machine but “practically infinite” ? How does that work?

- state : I understand this. The states are the possibilities of the machine. In the classic turnstile example the states are locked which only allows the input (coin) and unlocked which only allows the input (push).

- Singleton state: I tried to read bout it but I still don’t get it. Any help?

Could you maybe go word by word and help me clarify that initial statement?

Even the concept of a virtual machine changing the state… I know VM from my days running windows on a mac, so I understand the concept, but when it comes to modifying the state of Ethereum… does that simply mean adding a new block/transaction/smart contract? 

If states have to be predefined, how can my contract fit in?

TLDR:
Help me understand:
“From a computer science perspective, Ethereum is a deterministic but practically unbounded state machine, consisting of a globally accessible singleton state and a virtual machine that applies changes to that state.”",1629904770.0
pb9mni,Sources to learn Theory of Computation?,11,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/pb9mni/sources_to_learn_theory_of_computation/,6,"I have Theory of Computation as a subject this semester and the course work is very dry and boring. I have tried putting in efforts but that stuff is just too abstract for me. I have talked to others and they have more or less said the same thing.

Are there any books/courses that explains this subject in an interesting way? What did you do to get through it? I dont want to fail or fry my brain.

Thanks!",1629892250.0
pb14j9,"Graph theory anyone? I'm making a tool to build and export graphs, would love feedback!",52,0.95,computerscience,http://tranquil-oasis-04623.herokuapp.com/tools,15,,1629855137.0
paqy2k,Why interpreted languages were made instead of making just compiled languages,28,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/paqy2k/why_interpreted_languages_were_made_instead_of/,25,Why interpreted languages were made instead of making just compiled languages,1629823087.0
pakhjg,The Personal Cybersecurity Checklist - find out how to protect yourself online,40,0.93,computerscience,https://tsh.io/blog/personal-cybersecurity-best-practices/,0,,1629800267.0
paimzm,What lets a computer work from a single electricity?,0,0.38,computerscience,https://www.reddit.com/r/computerscience/comments/paimzm/what_lets_a_computer_work_from_a_single/,5,"I have just started learning basic principles of the computers. There are bits, bytes, nand gates, memories, registers, buses, etc. Many units exist and they do what they're supposed to do repeatedly and fast.

What I wonder is that who controls bits(electricity) in each unit be on and off? No matter how basic stuff each unit does, that behavior should be controlled by something other. It may depend on A which depends on B which depends on C and so on. As a result there may be intelligent one which knows how to operate computer units in order to cut this endless chain. If there really is such a thing, how can it know? Or am I missing a point?",1629790720.0
pagsk7,Advanced Algorithms CS graduate course,31,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/pagsk7/advanced_algorithms_cs_graduate_course/,5,"Does anyone have relevant course materials on Advanced Algorithms CS graduate course.??. like textbook, course videos or lectures. id really appreciate.",1629781674.0
pagkn4,Algorithmic Thinking,13,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/pagkn4/algorithmic_thinking/,9,"Hello guys. So it has been almost 2-3 weeks since I started a course on Data structures and Algorithms. Although, I have implemented and can understand the working of the algorithms I have covered, I would like to develop a better and keener sense of algorithmic thinking and design. Is it too soon, or is there a particular method I should follow during the course.

&#x200B;

Thanks alot!",1629780709.0
pa6i0q,Has anybody created a bootstrapping compiler before?,3,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/pa6i0q/has_anybody_created_a_bootstrapping_compiler/,3,"As an ambitious project, I wanted to develop a language, and compile a compiler in itself.

I have some background in PL, but this still seems like a daunting task. I'm not too worried about the frontend, because it will be relatively straightforward, even if not having access to lexers or parsers will be annoying. I'm more worried about runtime or memory management.

There's a lot of resources online on building a compiler, but a dearth of tips on how to build a bootstrapping compiler.

Any suggestions or tips from experience would be highly appreciate. Currently thinking of having a rather minimal subset of C and compiling it down to LLVM.",1629745468.0
p9yzww,Competitive programming is useless,95,0.73,computerscience,https://kislayverma.com/organizations/competitive-programming-is-useless/,24,,1629722411.0
p9pg7g,[Bit Manipulation] Fast way to find the position of the ON/SET/HIGH bit in a one-hot binary sequence?,7,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/p9pg7g/bit_manipulation_fast_way_to_find_the_position_of/,6,"Given a binary number/sequence, find the positions/bit indices of the 1/ON/SET/HIGH bits. As an example, given the binary number:

      76543210 # positions
    0b10010110 # binary number
      ^  ^ ^^

the positions would be \`\[1,2,4,7\]\`.

I have the following O(N) algorithm (where N is the bit length of the binary number):

    def get_on_positions(num):
        res = []
        pos = 0
        while num:
            if num & 0b1:
                res.append(pos)
            num >>= 1
            pos += 1
        return res
    
    assert(get_on_positions(0b10010110) == [1,2,4,7])

But I was wondering: is there was a faster O(M) algorithm (where M is the count of ON/SET/HIGH bits in the binary number). A particular case where the difference between N (the bit length) and M (the count of ON/SET/HIGH bits) can be significant is when the binary number is one-hot. A one-hot binary number has exactly one of its bits ON/SET/HIGH). For example \`0b10000000\` (or any power of 2). In the one-hot case, is there an O(1) algorithm for finding the position of the single ON/SET/HIGH bit? As a side note, the case where \`num\` is one-hot case is equivalent to finding \`log\_2(num)\`.",1629681906.0
p99xqz,What happens if you apply a hash continually on itself? Will it eventually repeat? If so what are the shortest longes cycles?,120,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/p99xqz/what_happens_if_you_apply_a_hash_continually_on/,21,,1629626961.0
p97tg1,"An overview of BDD testing, and frameworks worth considering",13,0.75,computerscience,https://www.perfecto.io/blog/bdd-testing-frameworks,6,,1629615110.0
p972zx,"Trying to understand quicksort, it's making me go crazy",0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/p972zx/trying_to_understand_quicksort_its_making_me_go/,3,"I have watched countless quick sort videos, and it seems to be that there are many ways to do the partitioning?

First video ([https://www.youtube.com/watch?v=COk73cpQbFQ&t=1071s](https://www.youtube.com/watch?v=COk73cpQbFQ&t=1071s))

`Method 1:`

`One video shows picking the the right most (end) element as the pivot. We also create a variable`

`tracker = left;`

`Then we iterate/for loop through from the left using i, and check if the element is smaller than the pivot.`

`If it is smaller, then swap that element at i with element at tracker. Increment both i and tracker.`

`Else just increment i.`

&#x200B;

`At the end of this for loop we swap the element at tracker with the element at the end, which is the pivot.`

&#x200B;

This makes sense to me, I am able to visualize it.

&#x200B;

Another video ([https://www.youtube.com/watch?v=7h1s2SojIRw&t=281s](https://www.youtube.com/watch?v=7h1s2SojIRw&t=281s))

`shows picking pivot as most left element.`

`Then use two variables i and j, one is most left and the other most right.`

`i will search from the left for element larger then pivot, and j will search for element from the right smaller than pivot. When both is found, swap them.`

`If i > j, we have to stop, as we finally found a position for the pivot, that is, there are no more elements`

`on the left that is larger than pivot. So we swap and put the pivot element into j.`

&#x200B;

&#x200B;

It seems that picking pivot from the left and picking form the right inherently modify in partioning process. Am I understanding and stating it right so far?

&#x200B;

Another method is to pick element from the middle. Then move that element to the end. This is much more confusing for me, as it involves moving the pivot element at the start, which is not what the other 2 methods mentioned earlier does.",1629611451.0
p8qm7y,Learning about hardware,5,0.86,computerscience,https://www.reddit.com/r/computerscience/comments/p8qm7y/learning_about_hardware/,4,"Hi , 

Just wanted to know if there are any books/ free courses / resources I could use to learn more about computer hardware from a beginners point of view.",1629549041.0
p8msdc,"A record breaking 2nm process developed by IBM, can someone give me an idea about the capabilities and potential that a CPU of this efficiency process can produce?",304,0.96,computerscience,https://i.redd.it/ls3bops2sni71.jpg,48,,1629528749.0
p8m6g2,Does a very specialized language and bytecode VM make sense?,4,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/p8m6g2/does_a_very_specialized_language_and_bytecode_vm/,4,"Okay, so I am reading Crafting Interpreters and some other books on compilers, just from a personal hobby perspective. And I like it a lot.

However, it got me thinking. I understand why a compiling to a (bytecode) VM makes sense. A good example here is the Another World game from the 90s where the creator compiled it to its own VM and you “only” needed to port the VM in order to run it.

However, all the “custom” languages I am reading in the books are general purpose. Just like the commercial VMs like JVM etc. 

But, I can imagine there being very specific use cases that can warrant a specialized language and VM. E.g. a 2D game engine might benefit a lot by specialized routines for managing a lot of entities or rendering pixels to screen etc. I can imagine if the language, compiler and VM are built for a specific use case a lot of assumptions can be made compile time.

Are there any examples in the real world of this? Or is it a stupid idea?",1629525658.0
p8ltws,What language should I learn to create a system that organizes samples that can be retrieved later.,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/p8ltws/what_language_should_i_learn_to_create_a_system/,4,"Hi guys, I volunteer in a hospital lab and one of the most tedious tasks they tell me to do is retrieving a blood sample that I have to go hand by hand looking at every individual sample and it’s name and medical registration number to find the sample they’re looking for.

I’m new to coding and I want to test my discipline to start something and finish it, while also testing my abilities in programming. In this project I want to create a simple system that’ll allow me to input their name and mrn number and put it into a machine that catalogs the slot I put it in, like slot 1,2,3.. so on. Then when I want to retrieve it, I just input the name and mrn number and it’ll tell me the slot it’s located in, but if it’s not their it’ll just say that it can’t be found.

I was thinking Java would be the best to do this but I also wanted to see if there are any other better options that I can use to do this as well. The only relevant coding experience I have is my AP Computer Science class that I took in my junior year of High school but I ended up failing the exam with a 2.

Also, any recommended software or websites that I can use to complete this project would be very helpful as well :)

Appreciate the help guys :)",1629523944.0
p8copp,what is a ranking algorithm that compares a flat fee and a percentage fee on a delivery service given a budget?,5,0.66,computerscience,https://www.reddit.com/r/computerscience/comments/p8copp/what_is_a_ranking_algorithm_that_compares_a_flat/,5,,1629489452.0
p7x4gh,Fun with regular expressions: part I,0,0.5,computerscience,https://yurichev.com/news/20210819_RE1/,1,,1629443454.0
p7nprt,"How does a computer ""understand"" what code/binary means?",91,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/p7nprt/how_does_a_computer_understand_what_codebinary/,47,"So I know that at the very very basic end of cs, a computer works by receiving binary data and turning it into some sort of output, But how can a computer ""know"" what I want it to do when I type this, for example. Same with programming languages, how can it know that print(""Hello World!"") on python actually means, ""put these words on the screen?""

Anyway, hopefully that sort of makes sense. I don't know much about cs, so basically anything helps.

Edit: Thanks for all your answers. I think I'm beginning to understand it more, but this is still insanely complicated to me lol.",1629407311.0
p7e35j,Future photo?,81,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/p7e35j/future_photo/,51,So….. I’m a kid who is interested in computers. Recently I was thinking about how machines work entirely on binary code and even your photo you see on a phone is just a bunch of binary code. So I thought what if we had a program that produces random binary code of varying lengths and converting them into .png form . Would it randomly one fine day end up producing a image of me in future cuz it’s also just a bunch of binary code. I know it’s highly unlikely and it might take more than the lifetime of universe but if we just get get extremely lucky then is it possible?????,1629377004.0
p79wtb,What does data really mean in computer science?,0,0.31,computerscience,https://www.reddit.com/r/computerscience/comments/p79wtb/what_does_data_really_mean_in_computer_science/,4,"Presently as a computer science student I always hear that big tech companies like Google , Amazon etc companies are based on data but by data the only things which come in my mind is basic information of a person like : age , gender etc so what does data means what does a software engineer works with in a big tech company?",1629356950.0
p77d1u,Pixel mapping/tracking? Or maybe scene mapping or panoramic stitching?,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/p77d1u/pixel_mappingtracking_or_maybe_scene_mapping_or/,2,"I was wondering if anyone was aware of software that can be used to track a stationary cursor (crosshairs in game) within a scene that moves and trace its pattern (the crosshairs movement relative to the scene) in an output?

The idea being to track the cursor movement and look for “inhuman” movements.

I know that certain softwares exist for similar uses, I’m just unsure as to the names of them or the genre of software I need to be looking at.

I have tried a few different approaches that I’m still tinkering with. The first being using a panoramic stitcher program to see if that will help me work put coordinates of movement etc but I run into the issue of the program (Huggins) requires a focal point and FOV from lense, when I’m feeding it still frames captured from a video.

And the other which I’m yet to go down the road of is some software used when people take pictures with their telescope to keep the telescope aligned.

I’m not a programmer but have messed around and surprised myself before, so a point in the right direction is all I’m after really?

I hope the question and my requirements made sense? Am happy to explain further.

Thanks in advance.",1629345373.0
p70cg9,Is a Distributed system and a Decentralized system the same thing?,2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/p70cg9/is_a_distributed_system_and_a_decentralized/,2,,1629321055.0
p6w7dz,Why are sequential programs faster for larger datasets,2,0.63,computerscience,https://www.reddit.com/r/computerscience/comments/p6w7dz/why_are_sequential_programs_faster_for_larger/,2,"Hey everyone! I'm currently working on a program that looks for patterns in an array of data and I have two implementations: a totally sequential implementation and a parallel implementation. For smaller data sizes (such as 100 or 1000 elements) the parallel implementation is much faster but when we move to larger data sizes (in the millions), the parallel implementation is much slower. What could be the cause of this?",1629308652.0
p6v7qh,Question,9,0.68,computerscience,/r/computers/comments/p6v5r3/question_from_26_year_old_just_getting_into_using/,5,,1629305688.0
p6qfmu,I'm more than halfway into my second year of CS and I'm still kind of a dumbass with networking,4,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/p6qfmu/im_more_than_halfway_into_my_second_year_of_cs/,7,"It's my dream to get into the pentest/ethical hacking/network admin route, but damn is this field hard to become an expert in. I find that even my  basics aren't that great, like building a LAN in Cisco Packet Tracer, things like that.

I find it difficult to have a proper ground up way to learn all the protocols, acronyms, configurations, things like that. Is there any good way I can build a good foundation for this kind of stuff? I barely made it through Data Communications and Network Security, and I feel that I didn't actually retain anything at all. It's so all over the place.",1629290746.0
p6oybv,Some questions about CPUs...,9,0.77,computerscience,https://www.reddit.com/r/computerscience/comments/p6oybv/some_questions_about_cpus/,5,"I recently started learning about CPU architectures, and I have a question:
What does the zen architecture in an AMD CPU mean? Is it just a modified version of the Von Neumann architecture?",1629284842.0
p6bc0r,"Number of grid points within a radius, any help pls?",30,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/p6bc0r/number_of_grid_points_within_a_radius_any_help_pls/,18,"It's a personal problem i'm having, if i can figure it out, it would be nice

So i am given a grid of points, evenly spaced by X. I am given a radius R. How can i figure how many points can i fit within that radius?

I can do it manually, but if i could find a function to solve it, it would be great

&#x200B;

To make it clear:  if each point is 1 meter (x=1) from each other, and the radius is 1 meter in size (r=1), i can fit a max of 5 points within that radius. One in the center, and four above below and to the sides, tangently.",1629230775.0
p68ws6,[Question] Discussing computer architecture,1,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/p68ws6/question_discussing_computer_architecture/,6,"Hey everyone. I'm currently busy with an undergraduate research project and one of the things we need to discuss in our report is the computer architecture we ran experiments on. Even though I'm a computer science major I have never had any interest in computer architecture so I don't even know where to begin the discussion. What *things* about my computer would be good to mention under computer architecture? (CPU, etc)",1629223619.0
p6864d,[Question] What are the known(polynomial) approximation algorithms to get MIP(Mixed-Integer Programming) solutions from LP relaxation?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/p6864d/question_what_are_the_knownpolynomial/,4,Thank you all in advance.,1629220577.0
p5zgqj,[N] David Patterson: The future of computer architecture,1,1.0,computerscience,/r/MachineLearning/comments/p5pr5e/n_david_patterson_the_future_of_computer/,0,,1629187734.0
p5zcf0,Reviews on the book How to Design Programs 2e,20,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/p5zcf0/reviews_on_the_book_how_to_design_programs_2e/,6,"People who have read this book what was your strategy to complete this giant? It has close to 500 exercises. I am talking about www.htdp.org . 

I have started but it looks like jumping into a huge ocean. In some portions it seems dry. 

There are two courses on edX: 

How to Code: Simple Data
How to Code: Complex Data

But the courses don't cover the entire book. 

I am looking for review and experiences from people who have read and liked or disliked it. How did the book help you in your CS journey?",1629187102.0
p5pekj,"Looking for ""coding project"" YouTube channels.",2,0.75,computerscience,https://www.reddit.com/r/computerscience/comments/p5pekj/looking_for_coding_project_youtube_channels/,1,"Hey, I'm looking for YouTube channels that talk about their projects like Devon Crawford or [https://youtu.be/oq1gHjRO4ic](https://youtu.be/oq1gHjRO4ic)",1629148698.0
p5k0ns,"High school student getting ready to start AP CS, any suggestions?",0,0.33,computerscience,https://www.reddit.com/r/computerscience/comments/p5k0ns/high_school_student_getting_ready_to_start_ap_cs/,5,"Hi folks! I have a high schooler that's getting ready to start the AP computer science course. They've been programming for a couple years, but struggle with confidence and recall due to some life issues.

As you might know, the course work uses Java. I'd like to find a course that might help them with feeling more prepared for the course and subsequently familiar with the concepts. Perhaps even doing some various smaller exercises that give some context.

Unfortunately the school systems have created a problem where a ""quest giver"" is essentially needed so I think the latter part of smaller exercises would be especially helpful. I think it could produce some feeling of success and accomplishment that would instill some confidence and emotional memories to push through the possible upcoming doldrums of a longer course (depending on the teacher's style). I've always found that preexisting familiarity make the initial learning and course adjustment cleaner.

Any suggestions, links, videos, or so forth?",1629132657.0
p5i8du,Why have no new “big”operating streams been made ?,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/p5i8du/why_have_no_new_bigoperating_streams_been_made/,16,"They came out with Apple and Windows. We got a few others not commonly used and we got Linux sort of.. in the background. But why no new closed-source OS that sells and is really popular? Something that combines the freedom and capabilities of Linux with the “user friendliness” of Apple & Windows. It’s in quotations because I find both OSs to be a real hassle. Automatic updates mandatory, all sorts of crashes, bugs , errors … it’s beyond me Apple & Windows continues to overlook very serious problems with their OSs with all the updates they come out w.? So why hasn’t anything new dropped in the mainstream besides these clowns? I know there’s Android and chrome OS but those are Linux based and pretty similar to each other .. I’m talking about like the next big thing a really nice OS like I described that has the best of both worlds (like mentioned above) and is fast & efficient? I mean Linux uses so much less system resources than Windows. Apple, I’ve had Apple desktop computers and the more they update the slower they get I tried to prevent auto updates on my Mac Mini and thought I did that computer sat upstairs for years and one day I had finally decided I’m either gonna use it or lose it (“sell it”) and I went wit selling it cuz when I’d left it , used it the last time it was ok. Just “ok.” … a lot of apps I didn’t find useful and I wouldn’t at it was nearly as fast or error free as Linux at all but when I came back to use it a few years later f had installed all sorts of updates and was so slow and tedious. I had it set to a sleep cycle to preserve its “health” and everything I was trying to preserve it as best I could even tho I never used it. And ya the thing was “fried” thru no fault of my own. By fried I mean it was just a paperweight to me. My Windows lap stop was still slower but I installed Linux one day and it was super fast. On one of the Cheapest laptops available. I sold it and had the Government buy me a high end laptop for my schooling and it’s been great with Linux (on it) but there are issue with Linux sometimes things certain actions can ruin your whole computer. I had a big problem when i was new to Linux I won’t get into it cuz I’ve already kind of rambled off-topic enough but something that combines those advantages “mentioned” and maybe ads some new stuff too sounds like… revolutionary in that the current mainstream OSs especially the mobile ones are so far behind and atrocious (in my eyes) by that I mean the same serious issues are still there in every new update. There’s a lot I’m currently on iPhone 11 it’s got a lot of issues that affect 55% or more of what I use it for. They just don’t fix it . Android doesn’t seem much better either . It seems we need a great new OS that revolutionizes the industry and has (of course) a workstation version (desktop/laptop.. I think that’s the proper term “workstation” in my book(computer book) they refer to it as that) AND mobile versions. …. But why do u think that hasn’t happened ? A new mainstream OS -It sounds totally lucrative and logical for me. Any ideas/ thoughts on this ?",1629127340.0
p5h6qh,Found this in my grandpa’s attic who worked on ATMs for Y2K (May 1989),326,1.0,computerscience,https://i.redd.it/1efyukx5dqh71.jpg,3,,1629124211.0
p5etup,Best ways to learn CS?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/p5etup/best_ways_to_learn_cs/,8,"\#education

\#books

\#learning

Hi all. Looking for some resources to enhance my knowledge in networking and CS. Any books, sites, articles recomended?",1629115986.0
p5cu9s,User Study on Debugging Behavior,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/p5cu9s/user_study_on_debugging_behavior/,0,"Are you familiar with python programming? Are you interested in solving programming problems? Do you want to support software engineering research? [https://vrt-21-0007.cispa.de/](https://vrt-21-0007.cispa.de/)

https://preview.redd.it/b0myl43zxoh71.png?width=940&format=png&auto=webp&s=39c6155b3dc72d6bb3771b35ebc45f2a24505ec6",1629107120.0
p5bcx1,Any good resources to understand complexity analysis of algorithms?,16,0.95,computerscience,https://www.reddit.com/r/computerscience/comments/p5bcx1/any_good_resources_to_understand_complexity/,4,"I am currently learning algo designs and analysis, and I'm having a tough time trying to analylse its complexity. For example merge sort complexity, it is n \* log n, but looking at the steps of the recurrence relation got me so lost, furthermore watching different videos on the same topic, they are doing it differently",1629099396.0
p55w4v,Disjoint Sets and Cycle Detection,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/p55w4v/disjoint_sets_and_cycle_detection/,3,"I'm going to preface this with this is probably going to be a stupid question and I'm hoping I'm going to get an answer that is so obvious that I'm going to hide from the world until my beard reaches the floor and I've made friends from the local wildlife.

However, I've been following this article here:

https://www.techiedelight.com/union-find-algorithm-cycle-detection-graph/

And it states that Disjoint Sets can be used to determine whether there is a cycle in an undirected graph. Great. However I notice that no matter how I count it up, wouldn't you know that there is a cycle in an undirected graph if the number of edges equals the number of nodes in the graph?",1629076142.0
p50qk0,If rasterised images use pixels / What do vectorised images use?,40,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/p50qk0/if_rasterised_images_use_pixels_what_do/,20,"Is there a word for it?

Rasterised = Pixels

Vectorised = ?

I hope that makes sense",1629058125.0
p4vyt5,Assembly Language doubt,0,0.4,computerscience,https://www.reddit.com/r/computerscience/comments/p4vyt5/assembly_language_doubt/,1,"**Backstory:**

I   am an Intermediate programmer in C/C++ and Python. Assembly Language  is  currently part of my course (B. Tech). It started with MIPS. I am   tinkering with Godbolt compiler explorer, to know Assembly code of   simple C programs.

**Doubt:**

Below things are taught till now.

* Strings are stored in `.data` directive
* `syscalls` are controlled with `$v0` register
* `li` instruction loads value into a register
* `la` instruction loads address into a register

Here's the Godbolt Link: [https://godbolt.org/z/1q4f485xE](https://godbolt.org/z/1q4f485xE)

I   written a very simple C code which takes 2 integers as input and   calculates sum & difference and prints them out. The corresponding   assembly code DIDN'T have `.data` directive at all. It didn't control syscalls with `$v0`  
register. So, What is happening here?",1629042650.0
p4q4jp,Papers and articles about computers,29,0.92,computerscience,https://www.reddit.com/r/computerscience/comments/p4q4jp/papers_and_articles_about_computers/,9,"So I am into that rabbit hole of reading research papers and articles about computer/software engineering, if you have any recommendations please drop them below",1629018288.0
p4hr9p,Student researcher urges natural language processing research focus on signed languages,21,0.79,computerscience,https://www.reddit.com/r/computerscience/comments/p4hr9p/student_researcher_urges_natural_language/,0,[https://techxplore.com/news/2021-08-student-urges-natural-language-focus.html](https://techxplore.com/news/2021-08-student-urges-natural-language-focus.html),1628980965.0
p46y49,Anyone have recommendations for books/videos/anything that could help a beginner learn about computer science/IT? Any guidance on where to start to learn would be appreciated,31,0.85,computerscience,https://www.reddit.com/r/computerscience/comments/p46y49/anyone_have_recommendations_for/,17,,1628943317.0
p3qigz,Distributed Computing Book Recommendation,19,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/p3qigz/distributed_computing_book_recommendation/,11,"Do you all have any book recommendations for learning distributed computing? Something that's fairly accessible and language agnostic. I want to learn about common algorithms/protocols and try implementing them myself, rather than just get high level descriptions. Something that hopefully strikes a balance between practicality and not getting too lost in the weeds with white paper stuff (but that just might be the nature of the beast).",1628877562.0
p3pkz0,Practical uses for abstract algebra?,59,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/p3pkz0/practical_uses_for_abstract_algebra/,12,"I am curious what applications for abstract algebra exist in computer science. I have read about group theory and ring theory being used in cryptography. Functional programmers seem to use monoids and semigroups as library design inspirations - although this blurs into category theory? And I would imagine (though I don't know) that coding theory intersects with algebra - is this so?

What other areas of CS, applied or theoretical, make use of abstract algebra?",1628874847.0
p3lucx,What I Learnt From Going Head to Head with Codex,3,0.71,computerscience,https://ooshimus.com/what-i-learnt-from-going-head-to-head-with-codex,0,,1628855603.0
p3kby3,Books to Movies,0,0.38,computerscience,https://www.reddit.com/r/computerscience/comments/p3kby3/books_to_movies/,11,"TLDR  
Input: Any book

Output: A movie  


I couldn't really find any information online about this ""idea"", and I think it's because the software needed is 10-15 years off in my opinion.  Essentially, I would like to take any book and have some software turn it into a movie.  This seems inevitable to me the way deep fakes, face generators, text to speech, text analysis, tone sentiment, CGI, etc., are progressing.  We can already generate new images based on a given string, how far off can interpreting a book into a film be?  We have a giant collection of movies to train a model on to learn camera angles and such.  


I guess what I'm looking from you (the reader) is how long do you think this will take to become a reality? Post up some links if you can find anything similar to what I'm talking about too maybe?",1628848581.0
p3jxr5,What kind of approach can be used for designing electrical / electronic circuits using GPT-3?,11,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/p3jxr5/what_kind_of_approach_can_be_used_for_designing/,2,"Is it possible to make a ‘text-description’ to ‘circuit’ system using GPT-3?
If yes, what approach / model you will suggest?",1628846526.0
p34us9,Getting started with algorithms and data structures.,80,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/p34us9/getting_started_with_algorithms_and_data/,30,"Hello guys. I would love some good resources on algorithms and data structures. Some good courses, books, etc that will be suitable for someone novice in the field and would provide a well hands on approach. 

Cheers!",1628790132.0
p2x1fr,How are social media algorithm made ?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/p2x1fr/how_are_social_media_algorithm_made/,6,"I have been wondering how does a social media algorithm work so incredibly in promotion posts , showing feed and people recommendation, and all other stuff . As a computer engineer I an curious how can one make something like this.

If anyone know anything and would like to share knowledge that will be very grateful.",1628763868.0
p2wszh,How do you pronounce GAN at the workplace,49,0.88,computerscience,https://www.reddit.com/r/computerscience/comments/p2wszh/how_do_you_pronounce_gan_at_the_workplace/,28,"My team leader calls it gæn. What is the appropriate pronounce in your team or lab?

(1) gæn
(2) gaaaaan
(3) gee-ei-aen
(4) generative adversarial network",1628762754.0
p2j4zt,Could a non-quantum photon-based processor/computer outperform a classical computer?,10,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/p2j4zt/could_a_nonquantum_photonbased_processorcomputer/,5,"A while ago, I thought of this idea and looked on the internet to see if it was a thing. At the time I saw some articles that talked about some attempts. Today I just tried searching for that again but was unable to find anything since all results were about quantum computers.

From my understanding, a photon-based computer could work at vastly higher speeds, there would be no translation between the information received from optical fiber to electrical signal, and the energy costs and the cooling should also be lower.

My main question is about whether photon-based computers are still a thing that is being researched and if they could outperform a normal computer. My secondary question is if I'm wrong on any of the benefits for any reason.",1628706914.0
p289kp,When Zero Cost Abstractions Aren’t Zero Cost,1,1.0,computerscience,https://blog.polybdenum.com/2021/08/09/when-zero-cost-abstractions-aren-t-zero-cost.html,0,,1628667125.0
p24bw0,What should I teach my high school students in a 'programming' class left up to me?,33,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/p24bw0/what_should_i_teach_my_high_school_students_in_a/,23,"I got hired as a teacher (previously only taught math) to teach an intro to python programming class this year. I was given a python book from 2013, and I asked the admin if we could get a better book. They said sure, get whatever book you want and we will pay for it. These admin don't have tech backgrounds at all and essentially left the entire course makeup to me. I think I can switch the language from Python as well if I want. I'd like to change it to JavaScript and am thinking of getting an intro JS book instead to teach from. What do you think? Python or JS? I really need book recommendations though as students will be entering the class in less than 48 hours.

tldr;

Hired to teach intro programming to HS students. Curriculum left entirely up to me and I need recommendations for books, Python or JS or any advice whatsoever. Thank you all in advance.",1628650172.0
p23d5b,"Is there a more generalizable version of ""proof-of-work""?",8,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/p23d5b/is_there_a_more_generalizable_version_of/,6,"One of the major detractors of traditional proof-of-work algorithms is that the node is doing ""useless"" work i.e. performing hash calculations. 


Has anyone developed a consensus algorithm like ""proof of CPU utilization"" or ""proof of machine learning training iteration"" where the work being performed is either generalized enough it can be any useful task or a specific useful task?",1628646655.0
p1w3qi,I Remade Github Copilot - PyOoshi - Browser Based Alternative - In under 6 hours,2,0.75,computerscience,https://ooshimus.com/i-remade-github-copilot-pyooshi-browser-based-alternative,0,,1628622802.0
p1vqbh,Conway's Game of Life emulated in Conway's Game of Life,715,0.99,computerscience,https://v.redd.it/nmwq780xukg71,34,,1628621715.0
p1nmmd,"Building example cases for NP-Hard problems with a specified ""hardness""",2,0.76,computerscience,https://www.reddit.com/r/computerscience/comments/p1nmmd/building_example_cases_for_nphard_problems_with_a/,1,"Hello,

I'm a game developer and I want to make a game that's based on Rook's Path / Finding a [Hamiltonian Path](https://mathworld.wolfram.com/HamiltonianPath.html) (or Cycle) in a directed 4-(or 5)-connected graph.

Now, the player is the person supposed to solve this, so the computer has to provide the problem.

To use the vernacular: the player needs to visit every square on a checkerboard with obstacles, board has varying dimensions, but the graph's order is likely always <100, and often <50.

Notable examples are the ""[Flow](https://www.youtube.com/watch?v=ujZp1HGP124)"" mobile game, in a way, which does so but allows many 5-edge vertices. Long ago, I myself worked on a similar game in the past, called ""[InfeCCt](https://www.youtube.com/watch?v=kfnB8G_WaBU)"", which is based on paper puzzle books used for cognitive training. We were given the puzzles, but no insights into their MATLAB-based generator, or its runtime characteristics.

The ""magic"" of the dataset used for InfeCCt was that *most puzzles were quite hard, yet at the same time compact and had only one solution*. The sweet spot was somewhere at 1 to 3 solutions, while maintaining many false choices. Given the sheer number of puzzles they had, and the limited available computing power, they did something much, much more efficient than just testing a lot of cases. In fact, others ripped off the InfeCCt game - and ended up copying the puzzles, or merely mirroring them as obfuscation. Which leads me to my...

# Question: What are efficient approaches to generate ""nice"" example cases for this problem?

My research mostly yields me better algorithms to solve the problem, but for me, formulating and selecting the problem from a vast number of possible puzzle board configurations is hard. I've seen a few Hamilton path [generators](https://clisby.net/projects/hamiltonian_path/), and am currently picking these apart; but they care more about the intersection of the path rather than missing (inaccessible) vertices of the graph.

My initial approach was to randomly generate puzzles, check how many Hamilton Paths exists (e.g. naïve backtracking, parallelized across cores), and grade the puzzle on their length and number of choices. This requires searching the entire problem space, which - given the problem - is not very efficient. In fact, outputting higher order puzzles quickly slows to a crawl. I also experimented with pre-determined shapes and cutting these out of a trivial graph.

I currently test finding the first solution in a depth first search, and counting the number of decisions on the way, discarding anything that has too many decisions (e.g. an empty chess board has a gargantuan number of choices for a rook to travel and visit all the squares, but many trivial solutions are immediately obvious)

Worse, some problems with exactly 1 path are extremely hard (and often intuitively doesn't look like it), and some with 1 path are trivial (think a maze with no forks or intersections). For the former, the search could be extremely hard for whatever algorithm I use, but easy for humans who will use heuristic search, and occasionally even vice versa (but this is beyond the scope of my question, and goes into my ultimate goal of researching cognitive and neurological effects on people learning to solve these puzzles).

Thank you for your input, I greatly appreciate any ideas or pointers where to look.",1628596340.0
p1j516,Image processing as a midwit?,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/p1j516/image_processing_as_a_midwit/,10,"Long story short im a bit of a midwit and im trying to collect a ton of digital photo data that can be dumped into something like Power BI for data visualization. My coding experience is also extremely minimal. I could do this all manually by typing out the letters and numbers or possibly even use M Turk, but I think image processing might be a better way.

Does anyone have a solution for a layman? The Letters and numbers should always be pretty organized and they will always be the same color, background, font size, and font type. Are there out of the box software solutions or is image processing a lot more difficult than I think?

Basically I want to know what the best way to get data off a jpeg and into an excel sheet are.

edit: So I was able to figure out I could do some janky OCR through google drive. It recognizes the text in the picture and it spits out the data. Now I just need a way to bulk harvest and collate the data. It appears gyazo has OCR capabilities so maybe I will check that out.",1628574488.0
p1chqf,Computer language built using approximations of Principia Mathematica?,50,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/p1chqf/computer_language_built_using_approximations_of/,14,"Hi, I was watching a YouTube on ""Math Has Fatal Flaw"". It talked about the Principia Mathematica, which looks like this:

[Excerpt from Principia Mathematica \(https:\/\/en.wikipedia.org\/wiki\/Principia\_Mathematica\)](https://preview.redd.it/m37r7yoeueg71.jpg?width=400&format=pjpg&auto=webp&s=54b3b7bd484934b26ff0095c24283e35fc6b3cf1)

Wikipedia: ""PM, according to its introduction, had three aims: (1) to analyze to the greatest possible extent the ideas and methods of mathematical logic and to minimize the number of primitive notions and axioms, and inference rules; (2) to precisely express mathematical propositions in symbolic logic using the most convenient notation that precise expression allows; (3) to solve the paradoxes that plagued logic and set theory at the turn of the 20th century, like Russell's paradox.\[1\]""

So I was wondering, is there a programming language built using approximations of the syntax used within the Principia Mathematica? Machine code? Maybe a 'native' implementation?

This is important work, since I think it is good to have a singular syntax to represent logic and mathematics. If it were programmed, well I think that is awesome.

I have only a little experience with computer science, so I don't know a ton of languages out there, so I figure I'd ask the hive mind. Thanks!",1628549392.0
p1atyg,Is there a softer alternative to Petzold’s CODE?,23,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/p1atyg/is_there_a_softer_alternative_to_petzolds_code/,4,"I am halfway through and whilst the basics of how a computer works was built up nicely, the learning curve has ramped up massively with no exercises (code to logic and gates and suddenly to 8-bit adders feeding 8-bit latches).

Is there an intermediate or exercise-supported alternative to this book that builds up to the modern computer?",1628544131.0
p162z2,How do professionals deal with computer-induced headaches?,12,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/p162z2/how_do_professionals_deal_with_computerinduced/,13,I have been programming for maybe five years now - but only recently did I decide to pursue programming for full-time. I find myself getting a headache after 8 or 9 hours behind a computer screen. Do professionals have any advice on how to avoid/deal with computer-induced headaches? I'd really appreciate it.,1628530001.0
p10x5m,Looking for a schematic diagram of an 8-bit ALU,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/p10x5m/looking_for_a_schematic_diagram_of_an_8bit_alu/,4,I was going through the internet looking for schematic diagrams of an 8-bit ALU with logic gates but could only find a 4 bit one. Does anyone know or have a reference of it?,1628513822.0
p0vvh4,Any cool CS channels?,84,0.97,computerscience,https://www.reddit.com/r/computerscience/comments/p0vvh4/any_cool_cs_channels/,16,"I enjoy watching hacker documentaries that go into detail on the actual processes of the scenarios, but I'm curious as to any other CS channels that also explain their doings? Could be anything. Documentaries, making games, etc. I enjoy Code Bullet too.",1628490712.0
p0itk7,"Researchers find that eye-tracking can reveal people's sex, age, ethnicity, personality traits, drug-consumption habits, emotions, fears, skills, interests, sexual preferences, and physical and mental health. [March 2020] [PDF]",96,0.93,computerscience,https://link.springer.com/content/pdf/10.1007%2F978-3-030-42504-3_15.pdf,5,,1628443468.0
p05ywc,Does it take log(memory size) to read an address in RAM model/model for algorithms?,11,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/p05ywc/does_it_take_logmemory_size_to_read_an_address_in/,11,"Does it take log(memory size) to read an address in RAM model/model for algorithms?  


If so are we just omitting a logn factor all the time?  e.g. does it take nlogn time to traverse a linked list   
(logn)\^2 time to search a binary search tree?",1628387623.0
p000om,So I thought of a name for a (hypothetical) protocol inspired by matrix.,5,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/p000om/so_i_thought_of_a_name_for_a_hypothetical/,7,"Hello,

*I am not quite sure if this post will ""abide by the rules"" of this subreddit, but if not I will likely know quite quickly.*

&#x200B;

**Why this post?**

So..  I have tried matrix (the protocol by vector) for a while, and I can only draw one conclusion, which is that it will not go ""mainstream""  (likely ever). I think that the initiative is good for developing  interesting technology, but the name ""matrix"" is abstract and not  specific.

I  then wondered.. if there was a protocol called e.g. SLCP (simple live  communication protocol), and there were mainstream service providers (e.g. *hotcomm.com* from microsoft, *gcomm.com* from google), would that be something that you think you could ""explain to e.g. your grandma/grandpa""?

&#x200B;

Also.. I was also thinking of an address format inspired by the matrix ""address"" format, but a little bit  different so that people unfamiliar with the technology might not get as confused. What I thought of is..

&#x200B;

|<name>@<host>|i.e. ""identical"" to SMTP (could be e.g. [info@hotcomm.com](mailto:info@hotcomm.com))|
|:-|:-|
|\#<channel>@<host>|i.e. specific for SLCP (in matrix each ""space"" has an address)|

&#x200B;

So.. e.g. #[ggj2023@globalgamejam.org](mailto:ggj2023@globalgamejam.org) for people to discuss/follow the global game jam of 2023.

<name>@<host> would not conflict with email any more than the current matrix ""address"" does, as if you try to fill in an email address in your matrix login form you SHOULD get an error,  and if you try to fill in a matrix address in an email login form you  SHOULD also get an error.

&#x200B;

**To be extra clear.. SLCP is not an actual protocol, but an idea for a name that accurately describes what I think ""matrix is about"". I think that for ""mainstream"" adoption, it is essential that business can name  their services accurately as well. I think that matrix is never going to catch on, hence this ""proposal"".**",1628365673.0
ozwved,Open CS Problems for Undergrads,15,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/ozwved/open_cs_problems_for_undergrads/,7,"I was wondering if there was a collection of open problems in theoretical computer science, specially in graph theory, combinatorics, approximation algorithms etc. suitable for undergraduates to explore. 

There must be many many problems out there. I am looking for a collection.",1628355231.0
ozvr09,How/Why is Resolution not a proof that P = NP?,46,0.91,computerscience,https://www.reddit.com/r/computerscience/comments/ozvr09/howwhy_is_resolution_not_a_proof_that_p_np/,33,"So to prove that P = NP, we need an algorithm/method/way to solve an NP problem, such as 3SAT, in polynomial time or less, for all possible instances of the problem, with no regard to the number of clauses, or the number of variables, or the application of the instance in real life.

Now resolution, as I understand it is, if a clause is of the format: P(t),x and another is Q(t),-x, then the resolution of these two clauses is P(t),Q(t). and the instance is UNSAT, if there exists a clause where P and Q are empty, and is SAT if all possible P's and Q's are generated and none of them satisfy the condition that both are empty in the same clause, i.e. the empty clause.

So why/how/where is this wrong? and Pardon me for any mistakes I made.

Thank you",1628351545.0
ozpqiz,Encoding,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/ozpqiz/encoding/,0,What examples of benefits from different encoding ways like utf and it's offshoots examples,1628325884.0
ozp0pa,Is GPT-3 still King? Introducing GPT-J-6B,22,0.85,computerscience,https://ooshimus.com/is-gpt-3-still-king-introducing-gpt-j-6b,1,,1628322001.0
ozcs9n,CS writing/documentation,2,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ozcs9n/cs_writingdocumentation/,1,"Where do I go to learn more about good technical documentation of system, and writing proposals for new systems.

While the approach internally at the company I work at is working, I'd like to get some external exposure.

If specific help, good standards for Confluence documentation of systems, the creation of proposal for new systems are the two things I want to improve on the most at the moment",1628275295.0
ozbr22,What should I review before taking Intro to Theory of Computation?,8,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/ozbr22/what_should_i_review_before_taking_intro_to/,3,I’m coming up on Fall semester and this is one of the harder courses in my schedule. I was wondering what steps I could take to prepare for this course. Any advice is greatly appreciated!,1628272129.0
oz3bq1,Need Computer science course for 7th grade,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/oz3bq1/need_computer_science_course_for_7th_grade/,11,"Hello, I’m a new teacher and I am going to be teaching CS to 7th grade. Does anyone have any recommendations on what programs I should purchase for my 7th grade class? If I can buy three programs or more for under 10 k that would be useful. I’m looking for something the kids would enjoy… any ideas on how I can teach this subject?",1628242447.0
oyunkm,"Built a computer from scratch. A Z80 running at 2mhz, 32k ram, 32k rom, an 8255 for IO, port A of the 8255 connected to the LEDs. You don't want to see the back of it trust me.",1000,0.99,computerscience,https://v.redd.it/qti2kz1rjmf71,82,,1628206454.0
oyrbdp,computer hacking for beginners,7,0.82,computerscience,https://www.reddit.com/r/computerscience/comments/oyrbdp/computer_hacking_for_beginners/,0," 

Before I begin - everything about this should be totally and completely ethical at it's core. I'm not saying this as any sort of legal coverage, or to not get somehow sued if any of you screw up, this is genuinely how it should be. The idea here is **information security.** I'll say it again. **information security.** The whole point is to make the world a better place. **This isn't for your reckless amusement and shot at recognition with your friends.** **This is for the betterment of human civilisation. Use your knowledge to solve real-world issues.**

There's no singular all-determining path to 'hacking', as it comes from knowledge from all areas that eventually coalesce into a general intuition. Although this is true, there are still two common rapid learning paths to 'hacking'. I'll try not to use too many technical terms.

The first is the simple, effortless and result-instant path. This involves watching youtube videos with green and black thumbnails with an occasional anonymous mask on top teaching you how to download well-known tools used by thousands daily - or in other words the 'Kali Linux Copy Pasterino Skidder'. You might do something slightly amusing and gain bit of recognition and self-esteem from your friends. Your hacks will be 'real', but anybody that knows anything would dislike you as they all know all you ever did was use a few premade tools. The communities for this sort of shallow result-oriented field include [r/HowToHack](https://www.reddit.com/r/HowToHack) ~~and probably~~ [r/hacking](https://www.reddit.com/r/hacking/) ~~as of now~~. ​

The second option, however, is much more intensive, rewarding, and mentally demanding. It is also much more fun, if you find the right people to do it with. It involves learning everything from memory interaction with machine code to high level networking - all while you're trying to break into something. This is where Capture the Flag, or 'CTF' hacking comes into play, where you compete with other individuals/teams with the goal of exploiting a service for a string of text (the flag), which is then submitted for a set amount of points. It is essentially competitive hacking. Through CTF you learn literally everything there is about the digital world, in a rather intense but exciting way. Almost all the creators/finders of major exploits have dabbled in CTF in some way/form, and almost all of them have helped solve real-world issues. However, it does take a lot of work though, as CTF becomes much more difficult as you progress through harder challenges. Some require mathematics to break encryption, and others require you to think like no one has before. If you are able to do well in a CTF competition, there is no doubt that you should be able to find exploits and create tools for yourself with relative ease. The CTF community is filled with smart people who can't give two shits about elitist mask wearing twitter hackers, instead they are genuine nerds that love screwing with machines. There's too much to explain, so I will post a few links below where you can begin your journey.

Remember - this stuff is not easy if you don't know much, so google everything, question everything, and sooner or later you'll be down the rabbit hole far enough to be enjoying yourself. CTF is real life and online, you will meet people, make new friends, and potentially find your future.

What is CTF? (this channel is gold, use it) - [https://www.youtube.com/watch?v=8ev9ZX9J45A](https://www.youtube.com/watch?v=8ev9ZX9J45A)

More on [/u/liveoverflow](https://www.reddit.com/u/liveoverflow/), [http://www.liveoverflow.com](http://www.liveoverflow.com/) is hands down one of the best places to learn, along with [r/liveoverflow](https://www.reddit.com/r/liveoverflow/)

CTF compact guide - [https://ctf101.org/](https://ctf101.org/)

Upcoming CTF events online/irl, live team scores - [https://ctftime.org/](https://ctftime.org/)

What is CTF? - [https://ctftime.org/ctf-wtf/](https://ctftime.org/ctf-wtf/)

Full list of all CTF challenge websites - [http://captf.com/practice-ctf/](http://captf.com/practice-ctf/)

\> be careful of the tool oriented offensivesec oscp ctf's, they teach you hardly anything compared to these ones and almost always require the use of metasploit or some other program which does all the work for you.

* [**http://pwnable.tw/**](http://pwnable.tw/) (a newer set of high quality pwnable challenges)
* [**http://pwnable.kr/**](http://pwnable.kr/) (one of the more popular recent wargamming sets of challenges)
* [**https://picoctf.com/**](https://picoctf.com/) (Designed for high school students while the event is usually new every year, it's left online and has a great difficulty progression)
* [**https://microcorruption.com/login**](https://microcorruption.com/login) (one of the best interfaces, a good difficulty curve and introduction to low-level reverse engineering, specifically on an MSP430)
* [**http://ctflearn.com/**](http://ctflearn.com/) (a new CTF based learning platform with user-contributed challenges)
* [**http://reversing.kr/**](http://reversing.kr/)
* [**http://hax.tor.hu/**](http://hax.tor.hu/)
* [**https://w3challs.com/**](https://w3challs.com/)
* [**https://pwn0.com/**](https://pwn0.com/)
* [**https://io.netgarage.org/**](https://io.netgarage.org/)
* [**http://ringzer0team.com/**](http://ringzer0team.com/)
* [**http://www.hellboundhackers.org/**](http://www.hellboundhackers.org/)
* [**http://www.overthewire.org/wargames/**](http://www.overthewire.org/wargames/)
* [**http://counterhack.net/Counter\_Hack/Challenges.html**](http://counterhack.net/Counter_Hack/Challenges.html)
* [**http://www.hackthissite.org/**](http://www.hackthissite.org/)
* [**http://vulnhub.com/**](http://vulnhub.com/)
* [**http://ctf.komodosec.com**](http://ctf.komodosec.com/)
* [**https://maxkersten.nl/binary-analysis-course/**](https://maxkersten.nl/binary-analysis-course/) (suggested by [/u/ThisIsLibra](https://www.reddit.com/u/ThisIsLibra/), a practical binary analysis course)
* [**https://pwnadventure.com**](https://pwnadventure.com/) (suggested by [/u/startnowstop](https://www.reddit.com/u/startnowstop/))

[http://picoctf.com](http://picoctf.com/) is very good if you are just touching the water.

and finally,

[r/netsec](https://www.reddit.com/r/netsec) \- where real world vulnerabilities are shared.",1628196056.0
oylndl,Is there any science behind unplugging your computer for a specified time to fix it?,25,0.9,computerscience,https://www.reddit.com/r/computerscience/comments/oylndl/is_there_any_science_behind_unplugging_your/,10,"I understand why a hard reboot helps the computer repair itself but why am I always counting to 1 minute? I was told by my cousin in IT to always wait at least 60 seconds and his brother said longer the better so is there any science behind leaving your computer (or any electronics) unplugged for a specified time and then plugging it back in or is it just something you do just cause?


(This is a genuine post. I really feel like there’s logic behind it but to me it seems like magic)

Edit: r/computerscience and the people on it are great, nothing more to add",1628179876.0
oyl8yl,Book for Algorithms,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/oyl8yl/book_for_algorithms/,3,"I have a respectable amount of experience with a couple of programming languages, basic Searching & Sorting algorithms, and Asymptotic Notation. Which Algorithm book between The Algorithm Design Manual by Steven Skiena and Algorithms by Robert Sedgewick & Kevin Wayne would be better for the sort of pseudo-mathematical introduction to the subject? I don't want a book that delves too deeply into the mathematics of everything single thing (though too little amount of mathematics would not be good) since I will get that in my upcoming classes.",1628178755.0
oykwud,"Open Sourced a Machine Learning Book: Learn Machine Learning By Reading Answers, Just Like StackOverflow",92,0.98,computerscience,https://www.reddit.com/r/computerscience/comments/oykwud/open_sourced_a_machine_learning_book_learn/,5,"We made a compilation (book) of questions that we got from 1300+ students from this [course](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html).

We believe that stack-overflow-like Q/A scheme is perfect for learning, so we made this.

[Project Repo](https://github.com/rentruewang/learning-machine)

[Website](https://rentruewang.github.io/learning-machine)

The website is hosted on GitHub, automatically built from the repo.

Please tell us what you think.

Any suggestions are welcome!",1628177758.0
oyk7q7,Teaching Boolean Logic Early,4,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/oyk7q7/teaching_boolean_logic_early/,2,"I've recently found an opportunity to come in and teach lessons on python programming to a local high school's introductory CS course. I've always been interested in finding the most effective and efficient way to teach programming as I believe it is often done poorly. I once had the idea that teaching boolean logic as early as possible could be beneficial, and I have implemented this idea with this class with very promising results. Students are understanding how comparative statements work and take a genuine interest in exploring the many possibilities of if statements and, perhaps more uncommonly, the idea of returning a boolean value from a method. What are your thoughts on this? Do you wish you were taught boolean logic earlier on?",1628175747.0
oyim21,Is combining hardcode with website builders possible?,0,0.44,computerscience,https://www.reddit.com/r/computerscience/comments/oyim21/is_combining_hardcode_with_website_builders/,3,"I am a complete beginner at web development and code in general. I understand the website would take me about a year to build, probably more so I am simply looking for better solutions. Could I use a website builder and add a combination of my own code to implement unique features. I'm looking at web builders such as square space or Wordpress. Also I've seen a lot of cry about how these website builders are often subject to malicious hacks. Is it not possible for a user to change also the website builders security code and implement their own unique security system or such. Thanks",1628172810.0
oycadb,Bye CUPS: Printing with netcat,22,0.93,computerscience,https://retrohacker.substack.com/p/bye-cups-printing-with-netcat,0,,1628147367.0
oy5miv,How much of cs is computer engineering?,0,0.25,computerscience,https://www.reddit.com/r/computerscience/comments/oy5miv/how_much_of_cs_is_computer_engineering/,4,"As far as my understanding goes, cs is more focused on coding, while ce is focused on hardware.
I know that it can differ across different paths, but dont know which ones and by how much. I am starting as a cs major this month, so would just like to seek out some things I dont know. 


Also, if there is anything i can add to help you help me, let me know and I can add.",1628121403.0
oy5d26,Best online courses to learn C++ to get a job and certification.,8,0.7,computerscience,https://www.reddit.com/r/computerscience/comments/oy5d26/best_online_courses_to_learn_c_to_get_a_job_and/,16,"I was just wondering which C++ course is the best to land a a job and also to pass the certification exam.

I bought the C++ udemy course from beginner to beyond since a lot of websites listed it as either 1 or 2 on the list.

Its by Tim Buchalka whose Java course I did which was 80 hours. The C++ one is 45 hours though which is significantly less so I was wondering if maybe it doesn't teach everything, though everywhere I've read said it more or less covers everything you need to know.

So, tl;dr what's the most complete and comprehensive course which teaches most, if not all there is to know about C++ and will land you a job as well as allow you pass the certification exam.",1628120499.0
oy4v7j,Can we create an Effector based Computer System?,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/oy4v7j/can_we_create_an_effector_based_computer_system/,0,"Here are the details on the Effector:

[https://drive.google.com/file/d/1m39FZ-NZ-kJHV5kAPrTSFBe8V\_KO6DK4/view?usp=sharing](https://drive.google.com/file/d/1m39FZ-NZ-kJHV5kAPrTSFBe8V_KO6DK4/view?usp=sharing)",1628118914.0
oxtgso,I made a comparison between traditional networks and SDN,55,0.91,computerscience,https://i.redd.it/ekgnlp0amcf71.png,4,,1628086125.0
oxsyna,Technical debt is not debt; it’s not even technical,0,0.5,computerscience,https://markgreville.ie/2021/07/23/technical-debt-is-not-debt-its-not-even-technical/,0,,1628084602.0
oxmmta,how do temperature based TRNG work?,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/oxmmta/how_do_temperature_based_trng_work/,2,I wanna learn about that but I keep running into paywalls,1628058393.0
oxiqfu,What to teach in a one-time one hour lecture?,3,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/oxiqfu/what_to_teach_in_a_onetime_one_hour_lecture/,12,"If you had a one hour lecture to teach something about computer science to a demographic who can't be assumed to have more than basic computer literacy, what would you teach them?

Some thoughts I had:

* What is a computer?
   * Basic definition: device for storing/processing data
   * You use more computers than you realize. Tablet, phone, car keys, traffic light system, etc.
   * Talk about the basic components (CPU, RAM, etc.)
* What is a program?
   * Instructions written by a programmer carried out by a computer
   * You use GUIs that programmers made",1628043349.0
oxgwkg,Is there any future for mechanical computing?,4,0.84,computerscience,https://www.reddit.com/r/computerscience/comments/oxgwkg/is_there_any_future_for_mechanical_computing/,2,"I find mechanical computers kinda fascinating and intriguing. Is there any benefit to them over silicon based ones? I imagine they are waaaaayyyyyy easier to manufacture, but there's like no point because all software is built for silicon based ones and is based around it. I mean, like, you don't write C# on a mechanical or electromechanical computer do you? I mean maybe there's a way to translate C# into mechanical computer code, but I am not aware of it (that sounds like a fun diy thing if you're super nerdy like me).

I've heard some interesting discussion on Rod Logic. Do you think there's any future there or nah?

I'd love to hear your thoughts on mechanical computers. Thanks!",1628037111.0
oxgtba,"4 bit adder I poured so much time into a while ago. Sorry it's sideways, it was easier to work with.",409,0.97,computerscience,https://i.redd.it/481kpi5sj8f71.jpg,46,,1628036812.0
oxdvfw,Is computer science dying?,0,0.42,computerscience,https://www.reddit.com/r/computerscience/comments/oxdvfw/is_computer_science_dying/,40,I have heard many people saying that computer science is dying/dead or it is over-saturated and you won't ever find a job if you go with that field. Is this true or not?,1628027503.0
oxc1b5,Determine BPM from live audio input?,12,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/oxc1b5/determine_bpm_from_live_audio_input/,7,"Hi everyone, I’m an undergraduate cs major and I was brainstorming ideas for a project. I was thinking of making app where the user is practicing an instrument and the app detects if they’re off tempo. 

I searched around and didn’t find anything on how to detect BPM from live input. Any ideas on how I could do this? Thanks!",1628022104.0
ox53qk,What's the Relationship Between Innovation & Open Source?,5,0.69,computerscience,https://www.reddit.com/r/computerscience/comments/ox53qk/whats_the_relationship_between_innovation_open/,0,"Here is what we have found: Open source framework adoption can increase the amount of innovation possible compared the proprietary, closed source code.

&#x200B;

On a smaller scale, utilizing open source frameworks allows innovators to have more help with their projects, to be able to more swiftly fix problems in their code, to reduce the amount of unnecessary work that has been solved previously, and to increase overall efficiency in the creation of their products. These small scale improvements allow projects to be better for everyone.

&#x200B;

On a larger scale, utilizing open source frameworks establishes an incentive for more people to participate in technology, teach other up-and-coming developers and to create thriving, symbiotic communities that are constantly changing and growing as technology grows. This increases the level of competition and quality of projects across the industry.

&#x200B;

What have we missed? How else can open source communities enable change? Let us know your experiences and thoughts down below in the comments.",1628002409.0
owzsfc,questions about complexity of Howard's algorithm,1,1.0,computerscience,/r/algorithms/comments/owzgzv/questions_about_complexity_of_howards_algorithm/,0,,1627982562.0
owqfmg,C coding,9,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/owqfmg/c_coding/,5,"In my textbook it says that doubles (data type) are stored in C as an 'integer exponent and a fractional mantissa'. What does this mean? is someone able to provide an example of an 'integer exponent and a fractional mantissa'.

Is it just like (some number in binary - encoded for a by a certain number of bits e.g. 0001100 \* 10\^n, where -1000<=n<=1000 (approx))",1627946179.0
owhg74,How a CPU REALLY functions?,4,0.83,computerscience,https://www.reddit.com/r/computerscience/comments/owhg74/how_a_cpu_really_functions/,12,"After searching the internet forever to find an answer, i can’t find what i’m looking for.

I understand how a CPU is made, and how a CPU runs in a system. What i can’t wrap my head around is how we go from bits of silicon from sand suddenly knowing how to read a task from RAM and apply instructions to output a different electronic signal.

I understand the basics of a motherboard sending signals to represent 1s and 0s using metals, but i just don’t get how specially cut/printed/whatever pieces of silicon are able to process something. If someone has any articles/simple explanations on this this would be cool :)",1627919856.0
owgkm9,Statistical Physics for the Knapsack Problem,1,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/owgkm9/statistical_physics_for_the_knapsack_problem/,0,"For those with an interest in the overlap between statistical physics and computer science: A paper (with code) showing how an analytical approach to the knapsack problem yields a new approximation algorithm for the problem. [https://arxiv.org/abs/2107.14080](https://arxiv.org/abs/2107.14080)

The motivational idea is that while optimization problems become more difficult as N increases, statistical physics calculations become more accurate as N increases.

[Accuracy and Complexity - Physics and Optimization](https://preview.redd.it/674c0ev6oye71.jpg?width=3044&format=pjpg&auto=webp&s=2d126e9a567d05c3c0590c6f704586207a80aa14)",1627917271.0
owg4zz,I built an interactive map for self-teaching online. It's a skill tree for learning. First map of machine learning,651,1.0,computerscience,https://v.redd.it/7rxkepksjye71,37,,1627915999.0
owendc,Is it true that computer science is going to be oversaturated or outsourced to other countries like India?,3,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/owendc/is_it_true_that_computer_science_is_going_to_be/,8,"Thanks for answering, I appreciate it",1627911323.0
ow9nem,How to define a formal language for describing procedural activities,3,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/ow9nem/how_to_define_a_formal_language_for_describing/,2,"I do not have a formal computer science background here so I am looking for pointers.

How would you advice I go about describing a formal way to describe procedures like cooking recipes, manufacturing process, driving to a location etc.

Given the fact that these types of process does feel like algorithms, but they feel more open ended than normal algorithm represented by programming languages. For example a cooking recipes does not have to be 100% identical to result into the same dish. Also describing a step in a cooking recipe could be expressed in various ways since natural language is being used. 

This same process can be made for manufacturing process, driving to a location etc.

What concepts or tools should I be looking at if I want to achieve this kind of things?

Would DSL? Do the job? Or would DSL be too restrictive? Because I am thinking how can one encode the various near infinity steps/procedures involved in an activity like cooking or manufacturing.

Pointers would be appreciated",1627889021.0
ow14p2,not sure if this is the right place but what is the smallest number a computer can round to infinity (if that made sense),7,0.89,computerscience,https://www.reddit.com/r/computerscience/comments/ow14p2/not_sure_if_this_is_the_right_place_but_what_is/,5,,1627854518.0
ow1336,"Why are the words ""synchronous"" and ""asynchronous"" have the opposite meaning in CS than in English???",118,0.87,computerscience,https://www.reddit.com/r/computerscience/comments/ow1336/why_are_the_words_synchronous_and_asynchronous/,19,"I didn't realize this until now when I searched up the word ""synchronous"" on Merriam Webster, but the definition of the word in English is:
> happening, existing, or arising at precisely the same time

However, in the context of programming, synchronous programming usually mean that instruction are executed one at a time(so not happening at the same time). 

Conversely, the word ""asynchronous"" in English, according to Merriam Webster, means:
>  not simultaneous or concurrent in time 

Again, in the context of programming, asynchronous usually means that instructions are executed in parallel, which is the opposite of what the word means in English.. 

My burning question is, why is this the case? I'm I just bad at English or do you guys find this weird too? Thank you.

Sources: https://www.merriam-webster.com/dictionary/synchronous https://www.merriam-webster.com/dictionary/asynchronous https://stackoverflow.com/questions/748175/asynchronous-vs-synchronous-execution-what-is-the-main-difference",1627854367.0
ovy5vn,WebP lossy/lossless compression techniques straight from Google,6,1.0,computerscience,https://developers.google.com/speed/webp/docs/compression,0,,1627844771.0
ovpox0,Why is a quantum computer not a von neumann computer?,2,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/ovpox0/why_is_a_quantum_computer_not_a_von_neumann/,1,Most quantum computers look like ALU or probabilistic highly parallel von Neumann computer. All the quantum computers I've seen cannot function without von neumann computer to process the result as if it's ALU.,1627813138.0
ovkp1p,TSP with AI Tutorial,2,1.0,computerscience,/r/artificial/comments/onlp5b/tsp_with_ai_tutorial/,1,,1627787871.0
ovkmr7,Using an ant farm to generate encryption keys?,104,1.0,computerscience,https://www.reddit.com/r/computerscience/comments/ovkmr7/using_an_ant_farm_to_generate_encryption_keys/,17,"I was recently sent [a post](https://www.reddit.com/r/udub/comments/oucofr/is_there_enough_space_in_a_dorm_for_a_massive_ant/) about a guy talking about using an ant farm to generate random numbers for encryption keys, which he could supposedly sell to companies for a profit. I know there was that company that [did a similar thing with lava lamps](https://en.wikipedia.org/wiki/Lavarand). Is this viable? If so, what kinds of algorithms would I need to use? How much do companies pay for random numbers like this?",1627787601.0
ovfapd,The fastest supercomputers ever (and who built them),344,0.98,computerscience,https://i.imgur.com/GtXmazS.png,42,,1627766419.0
ov8itn,What problems could be solved with this superpower?,2,0.6,computerscience,https://www.reddit.com/r/computerscience/comments/ov8itn/what_problems_could_be_solved_with_this_superpower/,10,"Let’s say you had the ability to manipulate time. Here’s an example of how it works: Suppose you’re microwaving noodles for 3 minutes. You don’t feel like waiting, so you make 3 minutes pass for the microwave, essentially finishing the task immediately. Basically, the superpower is making it so that a certain amount of time passes for a certain object. You can do this as many times as you want (until the object expires from whatever) and however much time you’d like.

Now applying this to a computer, what computer science problems could be solved with this power?

Below is a side question that I thought about while making this one:

What if the superpower was instead “Finish this task immediately?” What’s the difference and which superpower would be better for this application?",1627743132.0
ou66j2,Philosophically - how is the mindset different of someone who pursues computer science different than that of someone interested in mathematics?,8,0.8,computerscience,https://www.reddit.com/r/computerscience/comments/ou66j2/philosophically_how_is_the_mindset_different_of/,7,"Just as the title states, what's the difference in mindsets and philosophies? Ultimately, both disciplines are broad, and CS grew out of mathematics at some point.",1627592253.0
ou2yn8,Help with haar wavelet transform,1,0.67,computerscience,https://www.reddit.com/r/computerscience/comments/ou2yn8/help_with_haar_wavelet_transform/,0,"Does anybody know about implementing the haar discrete wavelet transform on a 2d matrix, I understand the 1d transform but I’m having trouble with finding any explanation online for the 2D one",1627582470.0
oty8fs,"Researchers found that accelerometer data from smartphones can reveal people's location, passwords, body features, age, gender, level of intoxication, driving style, and potentially be used to reconstruct words spoken next to the device.",200,0.96,computerscience,https://twitter.com/JL_Kroger/status/1420681035617116163,19,,1627567873.0
otqbu7,Has anyone ever worked on an optimization problem like this before in CS?,3,0.72,computerscience,https://www.reddit.com/r/computerscience/comments/otqbu7/has_anyone_ever_worked_on_an_optimization_problem/,4,"Here is a problem I thought of:

Suppose there is a car repair shop and 5 mechanics work there. Everyday, new cars arrive at the shop and each mechanic has to choose 3 cars to work on. In short, the mechanics ideally want to choose cars that they think will be both :

\- Easy to work on (i.e. require fewer hours)

\- Pay them well (e.g. suppose the mechanics are paid 50% of the total price the customer pays)

The only problem is: the mechanics have no idea how much time any given car will require for repair (let's assume that no one knows this information exactly), nor do they know the amount of money the customers were charged (e.g. let's assume that the owner of the repair shop and the owner of the car negotiate the price in private). When making a decision on which cars to work on, the mechanics only have access to the following information:

\- Total Mileage each car has driven

\- Price that the customer originally purchased the car for

However, the mechanics have access to historical data. The mechanics have a dataset that contains all 4 of these variables - for all cars that all mechanics at this shop have serviced since the shop has opened, they have: Total Mileage, Original Price of Car, Number of Hours that were required (can consider this as a ""supervised label""), Total Bill that the customer was charged (can consider this as a ""supervised label"").

On first glance, this problem sort of looks like the ""Knapsack Optimization Problem"" ([https://en.wikipedia.org/wiki/Knapsack\_problem](https://en.wikipedia.org/wiki/Knapsack_problem)) - however, in the ""Knapsack Problem"", we know in advance the ""value and cost"" (i.e. the ""labels"") of each potential item we would like to consider for the knapsack. In this car mechanic problem, we do not know the ""labels"" - information that will eventually be used for defining/calculating the costs and utility function.

**Question:** Can the mechanics train two separate supervised models (e.g. regression, random forest) on the data that they have, e.g.

Model 1: hours\_car\_requires = f(mileage, original\_price)

Model 2 : total\_bill = g(mileage, original\_price)

Then, if these models are able to perform well on the training data - they can then use them to predict the ""total bill"" and the ""hours required"" for each new car that comes to the repair shop. From here, they could then turn this problem into a ""multi objective optimization task"" and use optimization algorithms to select cars to work on?

Can someone please tell me if this approach that I have described makes sense? Or are there already well established algorithms designed for these kinds of problems? My analogy was that on some abstract level, the mechanics selecting desirable cars to work is the same as choosing portfolios with low risk and high return.

Thanks",1627532456.0
otitnh,What are some applications of image segmentation in robotics?,0,0.43,computerscience,https://www.reddit.com/r/computerscience/comments/otitnh/what_are_some_applications_of_image_segmentation/,0,"I am a student interested in exploring computer vision and machine learning in robotics. I want to get a better understanding of the practical applications of real-time image segmentation.
What is the data acquired from real-time segmentation used for in robotics or computer vision? How is the data processed and what algorithms are used to process the data? What are the applications of semantic/instance/object segmentation in robotics? Explanations, interesting articles, research, and other resources are appreciated :).
I have also been exploring Tensorflow & TFLite recently. Are there any TFLite compatible instance/object segmentation models available (DeepLab seems to only be for semantic segmentation)? Thank you in advance.",1627505755.0
otb7nh,"Question about AWS blog post's use of ""Hard Real Time""?",25,0.93,computerscience,https://www.reddit.com/r/computerscience/comments/otb7nh/question_about_aws_blog_posts_use_of_hard_real/,5,"In [this AWS Builder's Library post](https://aws.amazon.com/builders-library/challenges-with-distributed-systems/?did=ba_card&trk=ba_card), in ""Types of Distributed Systems"", the author has this definition for ""Hard Real Time distributed systems"":  


""At the far, and most difficult, end of the spectrum, we have *hard real-time* distributed systems. These are often called request/reply services. At Amazon, when we think about building a distributed system, the hard real-time system is the first type that we think of. Unfortunately, hard real-time distributed systems are the most difficult to get right. What makes them difficult is that requests arrive unpredictably and responses must be given rapidly (for example, the customer is actively waiting for the response). Examples include front-end web servers, the order pipeline, credit card transactions, every AWS API, telephony, and so on. Hard real-time distributed systems are the major focus of this article.""

&#x200B;

That goes against my previous understanding of what ""hard real-time"" meant. The definition on [wikipedia](https://en.wikipedia.org/wiki/Real-time_computing) mentions this:  


""Hard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system)""

&#x200B;

I wouldn't consider a front-end web server to be a ""great loss"" in the same sense as some of the other examples in the wikipedia article (car engine control system for example). Is the author of the blog post overreaching when using this term? Or is it OK to use in this way?",1627483103.0
ot25cm,How do smart devices get registered to a backend?,42,0.94,computerscience,https://www.reddit.com/r/computerscience/comments/ot25cm/how_do_smart_devices_get_registered_to_a_backend/,11,"Hi all, I don't know if this is the right place to ask this question, so if it isn't I hope you can help me narrow that down. 

If it is, then my problem is that I'm trying to design a system that is a cloud device tracker / manager. What are the common implementation choices for achieving device registration? How do large companies like Apple/Amazon/Google handle their smart device registrations?  

One pathway I have thought of is to have the device reach out to the backend, but another use case I have might be that I want the user to be able to do it from a web UI. Would that be possible? Would there need to be some sort of unique code already on the device like a serial number?

Any insights appreciated! If you think my question is confusing or need clarification please feel free to ask.
Thank you!",1627444122.0
osxojq,how to efficiently learn machine language or command line,0,0.44,computerscience,https://www.reddit.com/r/computerscience/comments/osxojq/how_to_efficiently_learn_machine_language_or/,4,"I am assuming there are many ways to learn the machine language, however, I don't know much about it. I learn everything right from the campus, and I forget about it soon as I finish the course. therefore, I wondered if I can keep practice it even after the course finished. 

I had learned C Sharp/ C++

and I forget most of them because I did not have any CS courses during the summer semester. shame for myself. 

please share your experience if you were on a similar path as me. how did you overcome it

please share some helpful websites that could help me with practicing it?",1627427836.0
osn6iv,Which chapters should I skip for OSTEP (Operating Systems: Three Easy Pieces)?,0,0.31,computerscience,https://www.reddit.com/r/computerscience/comments/osn6iv/which_chapters_should_i_skip_for_ostep_operating/,6,"I will soon start my graduate software engineering role; it would be standard backend so I will not *need* Operating Systems knowledge much. I am mainly self-studying Computer Science to see the big picture.

After completing nand2tetris, I am now reading **Operating Systems: Three Easy Pieces**. 

I don't have a lot of time before my role starts and I was wondering which chapters/sections/parts are non-essential from the book? Which ones can/should I skip?",1627393707.0
oscvji,Any projects that I could do with physical visual results.,0,0.5,computerscience,https://www.reddit.com/r/computerscience/comments/oscvji/any_projects_that_i_could_do_with_physical_visual/,2,"Hey, I'm a programmer. Been programming in Python for 3 years now. I've been doing alot of projects. Calculating pi. Discord bots one time I made a pong game that calculates where the ball will land and moves the paddle there so the bot can never lose. But all that is me essentially just seeing things on a screen. That's cool but its summer and I want to do something different and code something in the physical world. But I am a programmer, not an engineer. I'm not like mark Rober who can do  both. I tried engineering and I was terrible at it. So I was wondering if there is anything I can program in the real world. Like a robot or something. I dont mind there being alot of maths since I'm pretty good at it and I also dont mind learning a new language",1627348756.0
