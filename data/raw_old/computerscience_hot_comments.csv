post_id,comment,comment_id,parent_id,created,is_submitter
n2n0ax,How late is too late to start a career in programming? I’ll be 40 when I’m done with my degree.,gwkmb17,t3_n2n0ax,1619895202.0,False
n2n0ax,You’ll be 40 in a few years anyways. Why not try and get a degree on the way.,gwlbcha,t1_gwkmb17,1619905980.0,False
n2n0ax,that is like the smartest thing Ive read in a long time,h06cu2y,t1_gwlbcha,1622540856.0,False
n2n0ax,Hang in there!!!,h9d1o4m,t1_h06cu2y,1629251547.0,False
n2n0ax,I'm so glad that I'm still seeing people repeat this years after I first read it on Reddit. They way I first saw it was phrased: you'll be 30 anyway; do you want to be 30 with a degree or 30 without a degree?,h4oqtdr,t1_gwlbcha,1625910955.0,False
n2n0ax,"My dad didn't when to university until he was in his late 40s and walk out with a degree when he was 52. so, you not too old to do anything.",h84xnb2,t1_h4oqtdr,1628398097.0,False
n2n0ax,I have a full time career and make plenty enough money from it. I have 2 children under 2 and a wife. I’m 27 and just started my computer science degree. It’s never to late.,hgrin8t,t1_h84xnb2,1634319728.0,False
n2n0ax,I’ll be 30 when I get my computer science degree…,h9d1mrk,t1_h4oqtdr,1629251527.0,False
n2n0ax,At least you’ll have one.,h9db1sr,t1_h9d1mrk,1629256520.0,False
n2n0ax,31 here… Cheers Mate🤓,hor5cdv,t1_h9d1mrk,1639639020.0,False
n2n0ax,"dude, fuck the system! even if society told you that you´re ""too old"" go ahead and say ""fuck you"" and make a way for yourself.

That being said, i believe that if you take into consideration the fact that people keep dying at an older age and the idea of retirement at 60 is obsolete because of that fact, you may have AT LEAST 20 years left in the workforce.

I'm starting school next year at 31 after being done with restaurant/customer service jobs. I deserve a job I *enjoy,* not just one that i can *stand.*",h36iiam,t1_gwkmb17,1624768581.0,False
n2n0ax,"i'm 33 years old and i resigned my customer service job too to pursue a real career,, i'll be 37 when i get my computer science degree,, getting out of comfort zone is the best thing you do.",hgu593y,t1_h36iiam,1634369977.0,False
n2n0ax,"Whoa! Im about to be 40 myself, and I wish I got into programming a lot sooner. But as we all know its never too late. Only thing is, I dont know where to start so Im here.",h0k6ise,t1_gwkmb17,1622810127.0,False
n2n0ax,"[https://github.com/ossu/computer-science/blob/master/README.md#core-cs](https://github.com/ossu/computer-science/blob/master/README.md#core-cs)

Try this.",h5rhxbk,t1_h0k6ise,1626709314.0,False
n2n0ax,I wish I earn more money so that I could buy an award and give you one,hfahfez,t1_h5rhxbk,1633305476.0,False
n2n0ax,You don't need to do anything to thank me. I had no part in forming my curriculum. But I wanted to do my bit to show that I am grateful for the people behind it by spreading the word and you can do that too :),hfax0sj,t1_hfahfez,1633313137.0,False
n2n0ax,"Wow, what a cool resource. Thank you for sharing that!",hcvoqwt,t1_h5rhxbk,1631659353.0,False
n2n0ax,I don't mean to be rude but are there parts of computer science that you dont necessarily need to know if you were simply taking them for personal knowledge instead of gettin the degree?,heg2aah,t1_h5rhxbk,1632725789.0,False
n2n0ax,"Well I am not sure I am the right person to ask as I too am following the same curriculum but you can start with the ""Introduction to Python"" course and see where you can go from there. You can probably skip core theory and math if it doesn't interest you.",hfaxdx5,t1_heg2aah,1633313319.0,False
n2n0ax,"This is exactly what my husband needs.... Now I sincerely hope that he has the dedication it will take to complete the courses without it costing him something.

I have been looking for online colleges and courses for him to take the last little while as I have more free time than him at the moment and MOST of the ones I have found appear sketchy.",hijg469,t1_h5rhxbk,1635529653.0,False
n2n0ax,Ho-le shit. I might just try to finish this. Looks interesting as hell!,hseg1nr,t1_h5rhxbk,1642022281.0,False
n2n0ax,404 ?,husthm3,t1_h5rhxbk,1643509507.0,False
n2n0ax,"Damm... I'm 22, just graduted (No job offers though) & I thought I was late.",h8p167o,t1_gwkmb17,1628798279.0,False
n2n0ax,"To be fair, it isn't my first degree either. And 22 is a pretty normal time to graduate.",h8v073m,t1_h8p167o,1628905970.0,False
n2n0ax,"Hmmm atleast in my country we go to high school until 19, a computer science bachelor degree takes 3 years so normally if you're not a genius and skip grades, 22 is the ""earliest"" age you could get a degree in computer science.",hglnocy,t1_h8p167o,1634212907.0,False
n2n0ax,Where are you located? I could hire you.,hhw9ty6,t1_h8p167o,1635102667.0,False
n2n0ax,By looking at your history you're incredibly stupid 😛,hhxevoi,t1_hhw9ty6,1635120909.0,False
n2n0ax,LMAO,hhyijz3,t1_hhxevoi,1635144582.0,False
n2n0ax,So True. I'm 22 but I just started computer science. Please give me some advice as to will I be able to find an appropriate job and have a stable future in this career.,hk27rtc,t1_h8p167o,1636548132.0,False
n2n0ax,"my mom got her PhD in psychology in her early 20s. she started her career in the psychology field (marriage & family therapy) less than a year ago, and she's 54 years old. you're never too old to start doing things you love",he5a0rw,t1_gwkmb17,1632521517.0,False
n2n0ax,"i will be 30 next year, i thought my am too late to learn it, but now I think the age does not matter. isn't it? LOL",h63latb,t1_gwkmb17,1626935662.0,False
n2n0ax,"I met a senior in Boston who was probably around his 60s from North Africa studying computer programming, so no.",hdg6x25,t1_gwkmb17,1632049315.0,False
n2n0ax,I’ll be 31 when I get my degree!,hdyk3a5,t1_gwkmb17,1632402476.0,False
n2n0ax,Ignore the number,heevjtf,t1_gwkmb17,1632701335.0,False
n2n0ax,Definitely not! You’ll be able to make your own projects within 12 months of evening study. It’s a competitive industry but your unique background will be valuable in the right project.,hf31ueg,t1_gwkmb17,1633172467.0,False
n2n0ax,I'm going through something similar. I'll be either 40 or late 30's when I'm done with my degree in computer science. I also think I'll be 40 or 50 either way so why not live my life and do it.,hfzycgs,t1_gwkmb17,1633797746.0,False
n2n0ax,"You will live till 80 and will probably work till 60, coring is the easiet/safest way to reach seven figure per year income right now( apart from IB or PE, those are out for you). Worth it bro,  actually better if you don’t have anything else distracting and if you can do it. Believe me no one gives a shit in valley how old you are , or what ivy you went to. But if you want yo get into actual -ai roles you need to get into phd, no other way, other than that you are good.",hn4vxtb,t1_gwkmb17,1638577528.0,False
n2n0ax,I have a career in software and I’m in the middle of my degree. If your employer sees dedication and genuine interest you’ll get a job,hr2tngs,t1_gwkmb17,1641221638.0,False
n2n0ax,"I am 24 turning 25 and just starting. its literally just a number. If you think you'd be passionate, who cares!",ht8vrkx,t1_gwkmb17,1642548887.0,False
n2n0ax,I’ll be 47 when I’m done with CS. Age is just a number. I have 15-20 years left in the workforce after that. Don’t keep thinking about it. Do it.,hut9twc,t1_gwkmb17,1643517188.0,False
n2n0ax,"I am a freshman at a university and haven't been able to work on any side project or learn anything besides classes. I see many other students doing amazing projects and grinding leetcode. I have no idea how they do it. My gpa is perfect but I am worried that if I start spending less time for classes, my gpa may drop dramatically, which I don't want because of my scholarship and department requirements. What techniques do you use to do classes well and do side projects/grind leetcode?",gwkoyl5,t3_n2n0ax,1619896412.0,False
n2n0ax,"Hey Wolverine, I'm a CS grad, then spent 3+ years working as a software engineer and currently running a tech startup and i'm constantly mentoring and interviewing  students in your position.

1. I definitely believe there's a strong benefit in spending time on side projects, it allowed me navigate the career ladder 4x faster whilst forcing me to apply my learnings in CS in providing actual value to others.
2. Most employers prefer project based resumes! I do agree being able to communicate a solid CS understanding when combined with practical project experience is the sweet spot. But solely focusing on academics is going to make it harder for you to stand out from the crowd, you're more than your GPA score.

I see you have some grade requirements which you should not discard but definitely try to find the balance, even occasionally participating in 24hr hackathons where you have to build a real project goes a long way! I'm actually writing a short guide on this, feel free to DM more for more info, resume advice or any other way I can help! Balancing your studies ain't easy, but its super possible",gx767xb,t1_gwkoyl5,1620335721.0,False
n2n0ax,"Hi there, I'm an incoming CS freshman. I'd definitely want to do side projects if my time in uni allows it, esp since we're still in an online setup. What side projects would you recommend? and, if it's okay to ask, exactly how much time do you allot to these side projects and how do you manage your time for it? Thanks! :)",h5wb52u,t1_gx767xb,1626801493.0,False
n2n0ax,"Hey! I’d recommend choosing an area that interests you in software engineering.

For me in 2013 it was mobile app development! 
I just started building mini apps that I though me and my fellow students could use. (News readers, social apps, games etc.) you could try front end/back end or even smart contract development which is hot right now. It’s up to you! 

My favourite type of side projects are ones that someone could potentially use. Look at current trends/problems and build towards that. 

But when you’re just starting out building anything from start to finish should be the goal. My first app was a Snapchat clone(following an online video tutorial) probably took two weeks to finish. After that I could more or less figure out how to build my own projects. 

The more time you spend, the faster you’ll learn! Aim for at least 10hours a week to see significant progress but that’s down to you! Best of luck.",hbvu9jj,t1_h5wb52u,1630981627.0,False
n2n0ax,What projects make a candidate standout more than another? Also what are good projects to start learning to apply recently learned cs skills?,h5x8qr9,t1_gx767xb,1626815694.0,False
n2n0ax,"Depends on what area of CS your interested in! 

Best case scenario: anything that was used in the real world gets bonus points, I.e something you could get your fellow college mates to use ;) but that’s not the only way. 

Depends what area/job roles you’re trying to go for I guess but try to get your problem solving hat on.",h5x93l9,t1_h5x8qr9,1626815850.0,False
n2n0ax,"For me, it was effective time management. I worked 40 hours and did my entire CS degree online. I still had time to pursue other things like side projects or teach myself iOS development.

I worked 9 to 5:30 and was home by 6. Between 7 to 1am, I was doing my CS work. I had 90% of weekends free to do whatever I wanted.",h4ovmce,t1_gwkoyl5,1625915517.0,False
n2n0ax,Wow hats off to you. Your day sounds really busy but u gotta manage stuff,h4tz9tx,t1_h4ovmce,1626027766.0,False
n2n0ax,"How do you teach yourself more about CS? Sorry I'm only a starter and wants some advice. I tried Github, but I don't understand anything from it.",h5x6qqx,t1_h4ovmce,1626814820.0,False
n2n0ax,I don’t understand why you would go to GitHub? It’s not a learning center. Have you been to /r/learnProgramming?,h665pia,t1_h5x6qqx,1626986699.0,False
n2n0ax,what online school?,h8sn67v,t1_h4ovmce,1628869923.0,False
n2n0ax,"Dakota State University, but I think they require an associates degree",h8t4hmb,t1_h8sn67v,1628876762.0,False
n2n0ax,This is me right now and I feel like I’m not learning shit,h9i47u1,t1_h4ovmce,1629353870.0,False
n2n0ax,"I'm a CS student (Junior) and I'm gonna tell you not to worry about it. You want to learn the fundamentals of Computer Science as that will enable you to develop successful projects and succeed at technical interviews.

1. I've seen some work done by people who are still early in CS and have decent projects. Reality check, the code is average, sometimes the code is being copied from various sources without citing (not an issue if it's for personal use, but you should really cite it if you're putting it out there as your own work). It might *seem* good, but a little digging often reveals the reality of the situation.
2. Grinding LeetCode is stupid if you haven't studied Data Structures & Algorithms yet. It's almost like learning how to do brain surgery without understanding the **why** behind it all. Data Struct & Algor teach you the core of all relevant topics you need to know, the theory behind each, and when/how to implement them (e.g. Search Table Map vs a Skip List Map when processing millions of similar data or MergeSort vs Insertion Sort when the data is already almost sorted). LeetCode is significantly easier after you take this course as you will have the toolset to approach a variety problems.

My advice, master the core material of Computer Science and when you have spare time, work on those other things such as projects or interview prep.",gwm3u11,t1_gwkoyl5,1619921087.0,False
n2n0ax,Got it. Thanks a lot.,gwm5lik,t1_gwm3u11,1619922121.0,False
n2n0ax,"Besides, Software engineering is not the single possibility for CS grads. Research is an option too, if you love the theoretical aspects of CS but are not too keen on the practical implementations. Research in CS afaik always requires a high GPA, so you will always have the option of taking it up. 

Besides, why don't you consult your professors regarding this same question? Since you're a student, it will help you build a rapport with the professor, and you will also most likely learn a thing or two.",h1t84wd,t1_gwm5lik,1623729012.0,False
n2n0ax,"Thanks! Since the time posted the question, i emailed a few professors and got a research position xD. It is so much harder than I thought but let's see how it goes lol",h1uovyu,t1_h1t84wd,1623767806.0,False
n2n0ax,"In my opinion (and experience) internships mean more than anything when hiring fresh grads.  You can get school credit for internships and the professional experience is invaluable.

I didn't have any personal projects when I was in school.  I did push all my school projects to my personal GitHub to fake it a little lol.  But I did two internships and my interviewers brought those up more than anything.  

I also participate in interviews now and having experience in a professional environment is high on our list of expectations.

Hope that helps :)",hibfh5b,t1_gwkoyl5,1635379633.0,False
n2n0ax,"I'm still in highschool but really interested in computer science. I'm currently trying to find out how computers work from the base up. I already know the basics now, like how transistors work on a molecular level, how transistors form logic gates, and how to compute with logic gates. 
There is one thing I can't seem to find any good explanations of, and that is how can a logic board be made so it changes based on the instructions you give it (like a universal turing machine AKA a modern computer). Does anyone here have a good (video) explanation for me? 


I'd really like to do computer science as a minor on physics/engineering in university and i'd like to know the basics before starting.",gwl9g0v,t3_n2n0ax,1619905226.0,False
n2n0ax,Have you watched Ben Eater’s Youtube videos? He has a very high quality series where he builds an 8-bit CPU using just integrated circuits.,gwliwte,t1_gwl9g0v,1619909512.0,False
n2n0ax,"I've seen one about transistor logic gates, i'll check out the series.

And the 3d to last video (link: https://youtu.be/AqNDk_UJW4k ) is probably the one i'm looking for. (Haven't watched yet as it's getting too late in europe to get sucked in a YT rabbithole.)",gwlllxh,t1_gwliwte,1619910975.0,False
n2n0ax,Codd by Charles Petzold is an awesome introduction to computer science from the ground up. Worth every penny,gynzujh,t1_gwl9g0v,1621404781.0,False
n2n0ax,"You will have to work hard so if you don't have a passion its gonna be 10x harder

Passion makes it easier to work hard",h1i4ep1,t1_gwl9g0v,1623493816.0,False
n2n0ax,Check out [pathfinder.fyi](https://pathfinder.fyi) if you want to understand diff CS/Tech major career paths,hovejco,t1_gwl9g0v,1639712471.0,False
n2n0ax,Nand2tetris? I'd recommend you check it out!,hugdcpc,t1_gwl9g0v,1643300094.0,False
n2n0ax,"Logic boards don't really change. They have circuitry that allows them to read flash and storage so that it can fetch, decode, execute, and store the results of the instructions. You should check out Ben Eater.",hurbtsg,t1_gwl9g0v,1643486153.0,False
n2n0ax,"This is probably a common question, but how well do those coding bootcamps work for someone who has been out of the work force for nearly a decade? By 'work' I mean get you a stable middle class job. I have almost nothing on my resume, just a little math tutoring (my undergrad was in math). I'm confident I can learn the material, my question is really about how well the bootcamp will get me employed. Also, I'm in my mid 30's, I'm not sure how employers will view that.

A related question, which bootcamps should someone like me be looking at? Besides C++ I have relatively little CS background.",gwkgfeu,t3_n2n0ax,1619892670.0,False
n2n0ax,"Bootcamp is all about getting you a job. The number or percentage of people who got a job after the bootcamp is their main selling point. I think you'll be fine if you do well there, except looking for a bootcamp with good reputation might be a good idea? 

\> middle class job  
I don't know what's your definition of that, but I don't think you'd be earning 6-figures right at the start.

\> 30s  
They won't care.",gx5rb40,t1_gwkgfeu,1620314638.0,False
n2n0ax,My brother-in-law did coding bootcamp at the UofM and got hired at 78k 3 days after finishing. Seems decent to me.,h6yar1k,t1_gwkgfeu,1627564461.0,False
n2n0ax,"Any idea if that's the norm? I'm in St. Louis and starting a six month boot camp (unfortunately have to keep working at the same time). I keep being told to either take an entry level or apprenticeship when its over and not to expect much as far as pay 

But that pay you mentioned would actually be the same as I make now working HR! So that would be awesome!",hpuq0hk,t1_h6yar1k,1640382295.0,False
n2n0ax,"I have a buddy who did the same boot camp as my BIL and it took him a year to land a job, though the pay is around the same. Seems to depend on a good amount of luck and who you know, but if you land something the entry level pay seems to be quite good. I personally don’t know though.",hpvwqaw,t1_hpuq0hk,1640407359.0,False
n2n0ax,UofM as in Minnesota? I’m contemplating one of their coding boot camps.,hjbxhtx,t1_h6yar1k,1636059138.0,False
n2n0ax,Yes!,hjcbq2l,t1_hjbxhtx,1636064904.0,False
n2n0ax,Did he have any degree or experience other than the boot camp?,hq6opm1,t1_hjcbq2l,1640635200.0,False
n2n0ax,"He had experience just doing stuff in a hobby level, but no degree",hqhebj3,t1_hq6opm1,1640824684.0,False
n2n0ax,"I am planning on starting a CS major this fall. I am an academically gifted student, but feel that I could never be too far ahead. What skills or topics should I work on or attempt to learn before then?                
Btw, I’ve done AP Computing Science for 3 years, CalcAB and BC, and have learned a bit of HTML, and am okay at Java.            
 I was thinking of trying out Python",gwm8fam,t3_n2n0ax,1619923788.0,False
n2n0ax,"python is a great versatile language and i'd recommend being familiar with it, but i would also encourage you to focus less on language and more on writing good code! skills you learn in any language are transferrable to other languages, but writing good, clean code is hard to come across. the book clean code will be helpful, but also look into learning about design patterns. 

topic wise, look into data structures and algorithms. build different data structures and algorithms on your own and you'll gain a great understanding into the topic and it'll prepare you well for interviews as well!",haq7bqw,t1_gwm8fam,1630184682.0,False
n2n0ax,"Sounds good, and thank you for replying (after 118 days!!!).              
I actually start on Monday! And thanks for the tips",haqc506,t1_haq7bqw,1630186923.0,False
n2n0ax,how was your first semester?,hs6hgrc,t1_haqc506,1641885602.0,False
n2n0ax,"My good sir, I don’t know who you are or why you remembered to reply to me, but I appreciate the fact that you did.             
It went really well!! We ended up using Python, and though learning some introductory Python didn’t do that much to get me ahead, it was actually pretty nice to have in those first 2-3 weeks.          
Academically, all went great! I ended up with a 3.94GPA average, just getting my last mark back today actually. Dang English class pulling me down.           

What about you kind redditor? How has the last few months been to you? Dealing with your world well?            

PS: I was browsing your profile, and how TF does a real estate manager from San Diego stumble upon a (prospective) CS Student’s post about learning Python? I don’t see a single reason you would have any reason to comment, let alone see my post, let alone be on this subreddit. Man, the world is wide.",hs6jesj,t1_hs6hgrc,1641886994.0,False
n2n0ax,"I hope you get an answer. My son is a gifted student about to enter 5th grade and already wants to study at MIT. He has a passion for computers and wants to know more about how they work. He wants to get into coding and such and I have no idea how any of that works. For my age, I am technologically inept. I know a lot about things that have held interest to me but the tech world is beyond my knowledge. I would love ideas on how to support his passions to lead him where he wants to go. 

What helped you, outside of school, to learn wjat you have so far?",h67dm91,t1_gwm8fam,1627008347.0,False
n2n0ax,CS50 from Harvard on Coursers. Free intro course. Go from there,hcwu5m7,t1_h67dm91,1631679166.0,False
n2n0ax,"4 months late but meh. I think python would be good for your son to learn being so young. Imo its quite a bit easier than most other languages, and you can do some really fun things with it that I imagine a 10 year old would enjoy. For example I've made some mini games such as Flappy Bird, which have not been too difficult, but will teach your son a lot. Happy to help you and your son setup a development environment, or anything like that if you have not done so already.",hlzrzvm,t1_h67dm91,1637813025.0,False
n2n0ax,[deleted],houpk7s,t1_gwm8fam,1639701259.0,False
n2n0ax,"I appreciate the comment. I ended up starting python. It’s fun. Weird to type dynamically, but real simple. Have any project ideas?",houptpp,t1_houpk7s,1639701377.0,False
n2n0ax,[deleted],hourf7g,t1_houptpp,1639702098.0,False
n2n0ax,Thanks!!,hous0sr,t1_hourf7g,1639702363.0,False
n2n0ax,Check out pathfinder.fyi if you want to understand diff CS/Tech major career paths,hovemxp,t1_gwm8fam,1639712517.0,False
n2n0ax,"If you like computer science topics try the O'Rielly Safari (not sponsored to say this lol). I got it for free from my university and oh man it has all of my topics of interest in comp-sci. Just wanted to mention it, I dont know how expensive it actually is. Full of informational reads on all hot programming topics (comp sci fun)",gwlk8q0,t3_n2n0ax,1619910233.0,False
n2n0ax,"What I am asking is probably too long for a reddit response, so if you have an article or book to recommend, I'd love to hear it.

I know how to use computers and can find my way through most issues I face. However I am an intellectually curious person and I like to know the roots of how things work.

I find myself looking up how computers work and being faced with a plethora of other terms I don't understand. My interest in technology has recently been sparked again by all the talk about bitcoin and cryptocurrency.

How do computers work on the smallest and most basic level moving up? Perhaps it would be useful to start by exploring how electricity and electrons work? How does coding work? what is a programming language and how do computers understand them? What is a server? what is a blockchain? Where is all of the data of the internet stored? What is the ""cloud."" I have a hard time conceptualizing things that aren't manifested in the physical world. I hear terms like ""nodes, XML, SQL, CSS stack"", and I feel like there is just too much to learn and it's overwhelming. Can someone help me find a bottom up approach to understanding all this tech and how it first came about?",gwtj4po,t3_n2n0ax,1620070498.0,False
n2n0ax,"I suggest the book *Code: The Hidden Language of Computer Hardware and Software*. It's a really enjoyable and careful exploration of computation from the ground up. It doesn't answer all of the questions you've listed (many of those are very big questions which can have their own books if you want to go deep into how they work), but it will give a good foundation from bits and bytes onward.",gxape74,t1_gwtj4po,1620408805.0,False
n2n0ax,I'm a second year Computer Science major. I chose this degree because I'm very interested in tech but I'm not looking forward to a career where I have to code all day. I'm considering technical writing because my writing skills are pretty good. What other career options do I have that include minimal coding?,gwr7svs,t3_n2n0ax,1620025328.0,False
n2n0ax,"hey there! i really want to put out there are TONS of different careers in the technical industry that don't leave you writing code for the rest of your life - in fact, i would highly encourage you to look into product management as a role. i have a youtube channel where i create videos around CS and career advice and i've covered both [different tech roles](https://www.youtube.com/watch?v=_1o_aq0pdJY) and a briefer on [product management](https://www.youtube.com/watch?v=aQZWVHr2h-Q).

that being said, if you think computer science isn't the right fit then there's never too late to find something more suitable for you.

happy to answer more questions if you have any! :)",gys40ir,t1_gwr7svs,1621481008.0,False
n2n0ax,"Any word of advice for cs novice? Beginning local community school next month, working full time in food & hospitality…your product management video gave me some insight usung social skills…however I have none with coding but still interested",h58xufj,t1_gys40ir,1626331607.0,False
n2n0ax,Wow your comment has inspired me too!!,hew09qq,t1_gwr7svs,1633030852.0,False
n2n0ax,"Hi, I’m Japanese.

I will be going to a community college in January.

In the future, I would like to work for a company outside of Japan.
For a computer science bachelor’s degree in California, does it matter if you go to UC or CSU?",h0a7ggf,t3_n2n0ax,1622612713.0,False
n2n0ax,No it doesn’t matter at all what school you go to!,h5t3r7o,t1_h0a7ggf,1626734454.0,False
n2n0ax,Do you have any book recommendations for a beginner guide to coding and computer science?,h11o4za,t3_n2n0ax,1623170190.0,False
n2n0ax,"I recently watched a video that recommended the book, ""C Programming Language, 2nd Edition"" - by Brian W. Kernighan. I'm working my way through it at the moment and I would recommend it as it is basically a crash course of the C language. It's very informative and you can easily find PDF's of it. On top of all of that, C is a very good foundational language to learn.",h89lfbu,t1_h11o4za,1628499432.0,False
n2n0ax,Thank u for this,h8db13q,t1_h89lfbu,1628566026.0,False
n2n0ax,"I’m a cs freshman at another country (not america) and I want to continue my education in America. Since I’m an international I know I’d have to have a good list of impressive extracurriculars and mine aren’t too bad but I don’t have any work experience yet (which is extremely normal in my country bc university is pretty hard in my country so students generally aren’t expected to work until after university). I know C and Python and like I said I’m only a freshman and have finished 1 semester, but where could I apply to? Could someone guide me on what to do?? I really want to study at america :(( I just need more experience.",gwl3gdj,t3_n2n0ax,1619902807.0,False
n2n0ax,"I would really love to study a MSc in CS, but I am about to graduate in Mechatronics Engineering. Most univeristies require a degree in CS *or similar*. Anyone knows if Mechatronics is close enough? No university has given me a straight answer.",gwsutz5,t3_n2n0ax,1620060285.0,False
n2n0ax,Check out pathfinder.fyi if you want to understand diff CS/Tech major career paths,hoveqjs,t1_gwsutz5,1639712562.0,False
n2n0ax,"I need to interview a programmer for an essay due Monday.

Feel free to comment/dm me your answers to the following questions:

1.	Why did you decide to become a programmer?

2.	What does your typical day look like?

3.	What qualities set good programmers apart from great programmers?

4.	What is your favorite programming language and why?

5.	How did your expectations differ from the reality of working in your field?

6.	Do you have any pet peeves regarding management?

7.	Who should not become a programmer?

8.	What normal challenges do you and your colleagues face and how do you overcome them?

9.	If you had all the time in the world, what types of projects would you pursue?

10.	Do you have any final advice for the readers out there looking to get into this field?",gxfppmz,t3_n2n0ax,1620513912.0,False
n2n0ax,Hey all! (asking for a friend) Is it possible to become a data scientist without a computer science degree? Do you know if employers value bootcamp certifications specifically in data science area?,h0gcjlc,t3_n2n0ax,1622736129.0,False
n2n0ax,"you could do a masters of data science, i know most universities in australia are offering",h2il0wo,t1_h0gcjlc,1624262118.0,False
n2n0ax,What kind of math courses will I be studying to get my computer science degree?,h0hv1yo,t3_n2n0ax,1622759123.0,False
n2n0ax,"Calculus 1 and 2, groups and linear algebra, probability, statistics, discrete maths",hggd2f4,t1_h0hv1yo,1634106300.0,False
n2n0ax,"you'll need calculus 1 and calculus 2, and also a discrete math course for the most part. discrete math is more logic oriented - uc berkeley has great course syllabus on this!",haq73p1,t1_h0hv1yo,1630184578.0,False
n2n0ax,Thanks for this! I'm living in SoCal so looking at Berkeley’s course might be a good thing,hbs6kwy,t1_haq73p1,1630911396.0,False
n2n0ax,"I've been looking for this CS course I once saw.

The course teaches you electrical circuits (I think it was Arduino but might have been pie) to assembly to lower layer programming to upper layer programming(I think it was web server by python).

It was all in one course named ""Computer Science 101"" or some generic title and each topic was not separate, as in, some classes teach you based on this magical tool called ""python"" and somehow it all works, but in this course, it went all the way down to circuit to learn how all of it works and then up one step at a time.

&#x200B;

&#x200B;

The reason I'm looking for it is that I've been doing web programming professionally for some years and I believe I'm getting moderately good at it, but I still have this complex that ""I have NO idea what I'm doing once the assumption that this upper layer language works somehow is gone"".

&#x200B;

So if anyone has suggestions on how I should learn ""systematically"", as opposed to ""learn what you don't know right now; lather rinse repeat"", to become proud to say ""I'm a computer science engineer"" (just to clarify; I don't aim to be an academic in CS), I would appreciate it a lot, too.",gx5mvb9,t3_n2n0ax,1620312729.0,False
n2n0ax,"Please let me know if you found anything useful. I’m also immensely interested in knowing how computer science works on the smallest and most basic level moving up,but I dont know where to start.",hpd0zr5,t1_gx5mvb9,1640045707.0,False
n2n0ax,"I found something. It doesn't quite look as I remember, but it's basically it.

The website is [Nand2Tetris](https://www.nand2tetris.org/)  
It has a book ([The Elements of Computing Systems](https://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686/ref=ed\_oe\_p)), It has a [coursera course](https://www.coursera.org/learn/build-a-computer).",hpgaeuk,t1_hpd0zr5,1640110239.0,False
n2n0ax,Thank u so much. I appreciate ur help.,hps3hp1,t1_hpgaeuk,1640326532.0,False
n2n0ax,"I was curious in the way CPUs and other components in a computer are designed or work. Like I know some basic stuff like building a desktop computer so it's not like I don't know anything about the inside of a PC case. I'm not sure if architecture is the word I'm looking for, but a book that explains more of an in depth look at the cores inside of CPU and the way they work.",h09inqy,t3_n2n0ax,1622598530.0,False
n2n0ax,"I'm not qualified to give you an in-depth answer, but you're right that 'architecture' is probably the word you're looking for. It's usually qualified somehow, e.g., microarchitecture, instruction set architecture, system architecture, software architecture, with all of these referring to (roughly) how a set of constituent parts is arranged.

Universities sometimes teach the discipline of developing computer (hardware) architectures under the title 'Computer Organization', so you might also look for resources on that. Good luck.",hacdpr3,t1_h09inqy,1629926362.0,False
n2n0ax,"You can look at the book ""But how do it know"". It is an awesome compilation on the inner workings of a computer.",hi6vr8u,t1_h09inqy,1635297256.0,False
n2n0ax,"I'm going to college for a degree in Computer Science and will start this Fall. I'm looking for a book for a beginner like me so I could learn the fundamentals to prepare myself.

Any good recommendations or advice?",h0fvbqh,t3_n2n0ax,1622728960.0,False
n2n0ax,"I'm working on a paper for my diploma, and I need some sources. Are there any good books on how encryption works that people know of?",h13ltqd,t3_n2n0ax,1623201795.0,False
n2n0ax,[Here's](https://imgur.com/a/W7VRYy8) the prospectus of the college where I'm going to take my bachelor's degree in Computer Science. I don't really know how I should prepare for it; I don't know where to start. I'd appreciate any sort of help or advice to figure things out with me,h5ovc8a,t3_n2n0ax,1626650774.0,False
n2n0ax,"From my personal experience, there’s really not a lot to do to prepare for new classes. Make sure you get all the necessary supplies you might need for each class, for starters. So basically, get some notebooks, a reliable flash drive to store/transport/transfer all of the files from each class you have each semester (make sure to keep those organized), and seeing as you might have calculus I’d recommend you brush up on the basics of algebra just to be safe (since I know some professors/teachers already expect you to be familiar with the basics when you start).
Some tips: 
1) before you register which professor you want to take each class with, background check the professors on a website called RateMyProfessor. You don’t want to be stuck with a bad professor for a whole semester unless there’s no other choice. 
2) Before buying any of your textbooks, wait until you’ve attended each class at least once!! Sometimes the professors have a free option for their textbooks so you don’t have to buy them at absurd prices from your school! OR sometimes one of the other students might have found somewhere where you can read or view your class’s textbook for free! Make sure to check online if there are free pdf versions of your textbooks available before considering buy them. ALSO!! If buying your textbook is the only option, check places like Amazon and other online stores to see if they have the textbooks you need at a cheaper price than your school, or if there are rent options available! 
3) For computer classes, if it’s the introductory courses on how to code and stuff, then you don’t have to worry too much, since the teachers/professors should guide you through the beginning courses at the very least. Hecc, in my University some of the teachers pretty much gave the answers away after they saw the students put in some effort into the assignments they were given, simply because they wanted more students to stay in the computer science department and not get scared off. 

I think that’s all the advice I could think of for now, based on the courses I saw in the prospectus you linked.",h6jxrad,t1_h5ovc8a,1627275068.0,False
n2n0ax,"Nice. If I remember it correctly, I think someone also mentioned calculus. Given that I'm not that good at math, I decided to dedicate more of my time to focus on the math-related lessons that I took in my senior years in high school to understand it a lot better—most of the lessons there were about the fundamentals and whatnot. And I assume you already know the CS50 course, it's actually helping me with the coding side as well. Anyway, the list of heads-ups you made will surely be pretty useful. Thank you!",h6koej8,t1_h6jxrad,1627297557.0,False
n2n0ax,"Of course! Just glad I could help 
And just on a side note:

(Something I just remebered) As you get more into computer science and the coding side of things, try checking out a website called HackerRank on your free time. It will help you strengthen the coding fundamentals you’ll learn in your classes through small problems that start as easy as solving 2+2 and get more complex later on. And that website also has ways to help you transfer your already-accumulated knowledge from one programming language to another if you want to learn more.",h6lqu8m,t1_h6koej8,1627317740.0,False
n2n0ax,How hard is it to be on the research side? I would love to be on the research team for CS like theoretical stuff and quantum and new innovations etc.,gwnq681,t3_n2n0ax,1619961584.0,False
n2n0ax,"are you currently in college or about to attend college? if so, most universities/colleges will have TONS of opportunities for you to participate in undergraduate research as a starting point. many schools will also have an honor's college where you can opt into writing a thesis, which is another great research opportunity and showing what you're interested in/capable of. 

in general, you'll need a PhD to be seriously considered as a researcher, academic or in industry (yes, companies like Google and Microsoft hire researchers and lots of them!) - building out connections during your undergraduate will be the best bet to write you recommendation letters when you start applying for graduate schools.",gys89gs,t1_gwnq681,1621483411.0,False
n2n0ax,"Some other guy said its really not that good, theres no glory and no money for researchers like that, what do u think",gysy2ts,t1_gys89gs,1621503699.0,False
n2n0ax,"i don't think that's true at all - i know a few people pursuing research and they really enjoy it. of course academia won't pay you as much as industry, but if you're pursuing research in industry your pay will rival that of software engineers at that company too.

you can also look to work as an [applied scientist](https://www.amazon.jobs/en/jobs/1530895/applied-scientist?cmpid=bsp-amazon-science) (just an example posting) that is somewhere between research and engineering.",gytwo99,t1_gysy2ts,1621523247.0,False
n2n0ax,"I want to start learning computers from the very basic. What is RAM? ROM? Network? LAN? Cloud? All of it. From the very beginning. I want to start from the basic and then go deeper into algorithms, computer systems.  Where can I start? Please point to me an interesting resource. Thank you.",gwxozp0,t3_n2n0ax,1620155112.0,False
n2n0ax,"Hello, admire the passion!  
You have a long journey, why don't you start with a nice CBSE or ICSE grade 8thstd or upwards Computer Science textbook, you'll skim through them pretty fast, and then you should preferably pick one starter language to learn to program.  
I always suggest 'C' because you'll learn a lot about how actual machines work on a low level and after that, you can pick any syllabus manual from any college of your choosing, like VTU or SRM, and follow their book recommendations from SEMESTER 1 till wherever you feel like your progression is taking you and adjust accordingly.   
Also, there are tons of online courses like the infamous Stanford CSE 101 and resources like khan academy and our ever-amazing youtube.  
([https://www.youtube.com/playlist?list=PLhQjrBD2T381L3iZyDTxRwOBuUt6m1FnW](https://www.youtube.com/playlist?list=PLhQjrBD2T381L3iZyDTxRwOBuUt6m1FnW))  
This will take a lot of time and patience as I do not know your current academic background nor the intent with which you want to pursue this, so your mileage may vary from other people, but keep on keeping on :)  
Sidenote: Computer Science requires you to appreciate mathematics as a tool, because you will be using so many novel techniques to program and understand various concepts, so don't be overwhelmed, just exposing yourself regularly to those concepts will take away the initial fear. I started programming in 2006 and I'm still getting the grasp of things every day and learning so much, so never get discouraged at any point.",gxj1tyb,t1_gwxozp0,1620588865.0,False
n2n0ax,"Easiest way to get basic knowledge of computer hardware is probably designing a pc build yourself. You don’t need to actually build one but doing research will teach you the job of every component and their purpose with how they interact. Main parts are CPU, GPU, RAM(memory), hard drives and solid state drives(storage), PSU, motherboard and arguably the case. This is probably the most fun context to learn about computer components in.",gxn4nua,t1_gwxozp0,1620673863.0,False
n2n0ax,"what's a good starting point for someone who's interested in computer science but doesn't plan to work in the field? i'm fascinated by everything computers do and want to learn more about how they work (i can't pinpoint a single part of it yet).

i took extra IT classes in high school and i really enjoyed them. i'm looking for resources to learn from as an adult",gxw494t,t3_n2n0ax,1620848240.0,False
n2n0ax,"I registered for a university a while back and am going in August. Degree program is computer science; I want to be either an application software developer or engineer. I just realized that the concentration I chose (Software Systems) sounds more like software for computer systems than applications. Which creates a problem. Software developers and engineers/ aspiring students, what concentration should I take? The only concentrations are software systems, cybersecurity, and data science.",h0v96fk,t3_n2n0ax,1623034685.0,False
n2n0ax,"Hi Everyone! I am going back to school to get an A.S> in computer science. I already have a B.S. and an M.L.I.S.. This last year+ working in public libraries during the pandemic really made me reevaluate what I want out of a career. The biggest thing I wanted was more flexibility to not have to work in-person. Also to not be at the whims of a city council and residents that want everything from you but don't want to pay you for it (we were open on 4th of July but didn't get paid for it as a holiday bc they closed on the 5th). Additionally, I would like to not to work weekends. Just normal stuff. 

In grad school I took a HTML/CSS class; javascript class; data management (info retrieval sys design) class; and beginning cataloging and classification class. I liked them. I wanted to do engineering when I was in high school but that kind of changed when I did undergrad. I've been kicking around the idea to get into CS since last year and finally took the plunge to take classes at a community college while I'm still working at the library. 

My program requires that I take either Java or C++, I decided on Java after talking to a couple friends. It also requires Calc 1 & 2; Calculus Based Physics in mechanical and electrical and magnetism; Computer Architecture/Organization; and Discrete Structures. Are there any other specific languages or topics I should consider for getting into computer science?",h5ugjp5,t3_n2n0ax,1626761149.0,False
n2n0ax,"Okay so this isn’t a specific language or topic or anything, but just a tip for fun: if you enjoy coding stuff and you like to solve puzzles, check out a website called HackerRank. 
It has coding problems that start from something as simple as solving 2+2, literally, to more advanced techniques and topics that you may find exciting, fun, or interesting! AND you can complete the problems in any language you like! Java is a really good language to start with! 
They also have sections that even help you *learn* other languages too.

Also! I just realized something: 
Make sure to narrow down what part of the computer science field you’ll like more and would want to work in, as you progress through your classes. There’s the research field, web development, desktop software development, mobile app development (you could narrow this down further to Apple, Android, or both), cloud development. I think there’s more parts to the computer science field but those were just the things I came across while I completed my C.S. degree. 
I hope that helps even a little bit!",h6jvoz1,t1_h5ugjp5,1627273687.0,False
n2n0ax,I just took a computer graphics course and I loved it. It feels like a career path I would actually be excited about. We did a 4 week long project building a CPU based rasterizer with a VTK reader which was a blast and did some work with OpenGL. I got some experience with calculating collisions and working with textures and physically based rendering but not a ton. I’m not sure where to go from there. Is there something I should look at learning to become more hireable in the field?,h9x25bc,t3_n2n0ax,1629645700.0,False
n2n0ax,I graduated with a science degree and I'm looking to work as a software engineer in the future. Is it worth going back to university for some of those 1-2 year accelerated cs degrees for graduates?,hbgvuzv,t3_n2n0ax,1630689083.0,False
n2n0ax,"If you already have a Bachelor's degree, you may want to consider getting your Master's instead. You should check out the MIU Master's in Sofware Development program. We accept any Bachelor's degree and you don't even need to have prior coding experience!

With your science background, you would likely do very well in this program!

You can learn more on our subreddit r/miu_msd!",hczcxyz,t1_hbgvuzv,1631731049.0,False
n2n0ax,Thank you I will check it out!,hczfpb4,t1_hczcxyz,1631732173.0,False
n2n0ax,"I graduated from college with a BA in Philosophy and Logic several years ago, and have found my career in sales relatively unfulfilling, but have always had a passion for technology and computers.

To that end, I wanted to investigate the cost and timeline feasibility of pursuing a CS masters to bridge my education and pursue such career options. However, I am unsure of where to start, what options would be possible for a non-CS major, and what I should expect should I pursue this option.",hbx5u8z,t3_n2n0ax,1631015541.0,False
n2n0ax,"You may want to check out the MIU Master's in Software Development program. We accept any Bachelor's degree, even if you have no prior coding experience.  


With your Bachelor's in Philosophy and Logic, you have the perfect foundational mindset for a computer science degree!

You can view our subreddit r/MIU_MSD to learn more.",hcyz6hf,t1_hbx5u8z,1631725388.0,False
n2n0ax,"Here's a sneak peek of /r/MIU_MSD using the [top posts](https://np.reddit.com/r/MIU_MSD/top/?sort=top&t=all) of all time!

\#1: [How to Prepare Your React Native App for iOS 15 and Android 12](https://reactnative.dev/blog/2021/09/01/preparing-your-app-for-iOS-15-and-android-12) | [0 comments](https://np.reddit.com/r/MIU_MSD/comments/pluktr/how_to_prepare_your_react_native_app_for_ios_15/)  
\#2: [How The Matrix Resurrections Is Using JavaScript And Nuxt To Generate Dynamic Teaser Trailers At Run Time](http://www.appocalypse.co/entertainment/the-matrix-resurrections-javascript-nuxt-generate-dynamic-teaser-trailers/) | [0 comments](https://np.reddit.com/r/MIU_MSD/comments/pl18dx/how_the_matrix_resurrections_is_using_javascript/)  
\#3: [Global Cloud Computing Services Market to Reach $810.8 Billion by 2026](https://finance.yahoo.com/news/global-cloud-computing-services-market-143500344.html) | [0 comments](https://np.reddit.com/r/MIU_MSD/comments/phaugg/global_cloud_computing_services_market_to_reach/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/)",hcyz7kf,t1_hcyz6hf,1631725400.0,False
n2n0ax,"Honestly, getting this BA in psychology degree was probably not a good idea since the quality of jobs I can get entry level don’t pay anything by livable, and the work isn’t what I want to do. 

I have always been tech each and very well with computers. I wanted to try code in HS but doubted myself at the time. I’m 21-22 and I think it’s ready to make the switch now than later.

What would you recommend my next steps be? Certificate? Another degree? Masters degree? Or something else? 

Thank you!",hcubv79,t3_n2n0ax,1631639431.0,False
n2n0ax,"You may want to check out the MIU Master's in Software Development program. We accept any Bachelor's degree so it is a great way to switch your career. Now is a great time to get into the field with more jobs than developers, and depending on where you plan to live and work many jobs are offering 6 figure salaries!

You can view our subreddit r/MIU_MSD to learn more.",hcyxyk4,t1_hcubv79,1631724886.0,False
n2n0ax,Thank you!,hd1vhj9,t1_hcyxyk4,1631775817.0,False
n2n0ax,"I just graduated with a B.A. in psychology and I’m starting to change my mind about what I want to do. I would like to get into programming but I have no experience whatsoever. I have been a psychology research assistant in a lab and we have been doing everthing remotely extracting data from articles etc. (if that would help me in any way). I am of course still interested in psychology, so are there any fields of computer science that would tie the two together? Most importantly, what would be my next step in terms of training/school? I am 100% open to going back to school to get a master’s degree but it seems like you have to have a solid background in computer science to be able to get in. I appreciate any suggestions. Thank you all!",hcvtey5,t3_n2n0ax,1631661511.0,False
n2n0ax,"You may want to check out the MIU Master's in Software Development program. We accept any Bachelor's degree, even if you have no prior coding experience. 

I'm sure that your psychology degree could provide some valuable insight into the field of software development, including user interface design.

You can view our subreddit r/MIU_MSD to learn more.",hcyxml8,t1_hcvtey5,1631724751.0,False
n2n0ax,"I have zero experience in Computer Science.  I am scheduled to take a hackerrank test to test my capable abilities in logic and problem solving. I have a week to prepare for this.  Can anyone give me some advice and where to turn for practice questions. 

The test is through LaunchCode",he111hc,t3_n2n0ax,1632441107.0,False
n2n0ax,Best QA analyst boot camps you can do online?,hhi7f4n,t3_n2n0ax,1634832541.0,False
n2n0ax,Most boot camps are not well respected. Never met someone in the industry who went through one.,hifvp1s,t1_hhi7f4n,1635460708.0,False
n2n0ax,"Ahhh, thank you. Yeah, I guess that makes sense. I don’t have any sort of knowledge in this field, though. What’s the best way to get into it, outside of technical college or 4 year school in your opinion?",higawbg,t1_hifvp1s,1635467796.0,False
n2n0ax,I’m thinking of getting into computer science. What languages should I learn?,hiarx2r,t3_n2n0ax,1635369487.0,False
n2n0ax,Any. Their all relevant. The real question is what do you want to do with it? If you want to build an app look into how to do that. If you want to make a website research tutorials on how to make websites. If you persue a goal the knowledge will come along the way.,hifu1qt,t1_hiarx2r,1635459963.0,False
n2n0ax,Does learning programming get more fun as you get more experienced? Right now I’ve done a little python and it feels a little boring.,hidd9wr,t3_n2n0ax,1635422840.0,False
n2n0ax,Try using it to achieve something that interests you rather than just programming for the sake of it alone. Set a goal. When I was younger I got into programming by using it to make mods for a videogame I played. The objective is always more satisfying than the programming itself.,hiftumv,t1_hidd9wr,1635459873.0,False
n2n0ax,"I’m a computer science major (freshman) at a uni. My classes are hard or anything but I feel like I’m a at a static point. I want to start training harder concept and start applying for internships( I know I’m a freshman, but trying don’t hurt). Any recommendation on what I should start doing? Any advice is welcomed.",hiejzca,t3_n2n0ax,1635441325.0,False
n2n0ax,"When I was a freshman in college I already had 8+ yrs experience coding and was passionate about the subject. I also struggled to feel challenged by the material. If you want to persue more you can always look into doing things in your free time such as develop websites, learn how to make apps, or any side project that you think making would be cool. If you want to get a head start job wise finding a job as an IT technician would be a good start. That's what I did and used the experience to later get an internship at a S&P500 tech company. As for your classes, they will get ALOT harder. If your not feeling challenged now just wait until your Junior year. You will feel the same pain we all went through. It starts off easy and then take a turn to ""ohmg how is this even possible"". 

Source: Current SWE for a major tech company. Graduated a year ago.",hiftmet,t1_hiejzca,1635459766.0,False
n2n0ax,Bet. I been trying to practice data structure and algorithms to get prepared. Do you know any courses good to take during the break for a head start.,hign3d5,t1_hiftmet,1635473376.0,False
n2n0ax,"I really hope if someone can help me with an internal conflict I'm having.

So I recently graduated with a degree in biology, but I'm having second thoughts on whether I should have done computer science instead; Therefore I wanted to ask you guys here if you could like give me a general idea on what a computer scientist does, and if it peaks my interest.

I've been very intrigued lately with programming and web design, and in my free time I'm watching a lot of videos concerning game design while also playing games. TBH I wanted to first enroll in a game design degree, but they don't offer it where I'm at, and I wasn't really sold on the idea whether a degree in computer science will help me with game design. 

So my question is, can someone like summarize to me what a computer scientist actually does? And would a degree in computer science help with starting game design? And if you personally know some people who were able to do both. I'm personally having a lot of second thoughts,  and it would be really appreciated if you guys can me a general idea.",hk79ftv,t3_n2n0ax,1636638435.0,False
n2n0ax,"How hard is it to get a CS job right out of college? I want to go into software engineering, and I've heard both that people are clamoring to get anyone with coding experience and also that you're lucky to get an interview at all so... What's the truth?",hlyaxie,t3_n2n0ax,1637789483.0,False
n2n0ax,"I went to a Big 10 university for CSE from 2016-2020. I got my first tech related job as a part time IT Technician in a industrial Physics laboratory at my school. My Junior year, after applying to about 100+ internships, getting a handful of interviews, and getting denied to all of them I started to get a bit discouraged. However, around December I got an email from a major tech company saying they found my resume on LinkedIn and was wondering if I was interested in a program of theirs because they still had seats for it to fill. After applying to it and a brief 15 minute interview (no idea how it was this easy) I got the position. For the next 8 months (long internship, most are 3 months. I took a semester off for it but hey, I made $25/hr so I wasnt complaining) I was a student developer on the Site Reliability Engineering team of the #1 Hard Drive/Data solutions company in the world.

After the internship I was in my senior year and started my job search. At first things seemed kind of hopeless. I applied to hundreds of positions and only got a handful of followups for interviews. The ones I did get however seemed more into me than before. I got 4 different 2nd round interviews, cancelled 3 of them due to lack of interest in the position/company (either I didn't like the company, the role, or the location), and nailed the 2nd round interview for the last company. They offered me a position but since they were a military contracter (one of the top military aerospace companies in the US) I had to get a secret level security clearance which required rigorous background checks and a drug test. I wasnt the biggest fan of this part, or where the job would have been, so I decided to try to find another job. After about a hundred more applications I found the job I currently work at full time. After a initial interview with a recruiter and a final interview with the team I had a job offer for more money than the previous offer and in a better location. I also really liked the position I would have had. I accepted it and about \~9 months later here I am. I am currently a Associate Software Engineer on the Site Reliability Team of the #1 Digital Marketing Company in the world. All it took was 4 years of wanting to cry myself to sleep, a 3.5 GPA at a major university, some extracurricular experience, and shitloads of applications/interviews.  


If you do well in school and take the subject seriously you will do well. May take a while to find an opportunity but they are out there.",hlz74ej,t1_hlyaxie,1637803363.0,False
n2n0ax,"I'm a CS major in a college a first year to be exact, in new to programming and I was looking for recommendations for books for beginners or sites for beginners in languages that are considered useful or marketable, that could possibly help me get an internship in my 2nd or 3rd year and give me a basic understanding of cs.",hp5qx0f,t3_n2n0ax,1639914686.0,False
n2n0ax,I'm a college freshman (CS major). What kinds of side projects would you guys recommend doing?,hqhasqj,t3_n2n0ax,1640823178.0,False
n2n0ax,"I was not actually interested in computer science but during my Undergrad years I got into this field. How can I get myself started?
Everytime I see some random teenager talking about coding... It scares the shit out of me... 😢 and I lose my confidence. So in order to be confident in my field what can I do??",hsbp0zp,t3_n2n0ax,1641974864.0,False
n2n0ax,"Any chance you could link to that thread? I was just thinking it might have some useful info, even if it's been archived. Thanks!",gwl0mqz,t3_n2n0ax,1619901638.0,False
n2n0ax,I do engineering in yr 12 and 13 but i want to do computer science at uni what can i put on my personal statment to stand out,gwmlvoi,t3_n2n0ax,1619932653.0,False
n2n0ax,"This is a bit of an interdisciplinary question. I am currently studying computer science at an undergraduate level and would like to start moving towards the scientific/research world that combine computing with science (non-medical).

What are some post graduate degrees and perhaps careers that make use of my Computer science knowledge/degree and incorporate subjects such as physics? I have recently been looking at computational physics but am open to more suggestions (even if they are specialized and not generic). Thanks",gwmym3a,t3_n2n0ax,1619943964.0,False
n2n0ax,"Physically based rendering might be up right your alley. It happens to be an area of computer graphics where you can program light models to produce photorealistic images.

[**https://en.wikipedia.org/wiki/Physically\_based\_rendering**](https://en.wikipedia.org/wiki/Physically_based_rendering)

It is somewhat niche, but certain people of an engineering, CS or physics/math background work their way into it. You do need a firm grasp of CS (algorithms and the likes, which you clearly have) and some mathematics used in geometric optics, namely calculus and linear algebra.

Maybe start with this book, and see whether this project interests you.

[Ray tracing in one weekend](http://in1weekend.blogspot.com/2016/01/ray-tracing-in-one-weekend.html?m=1)

The next steps would be to read about Physically Based Rendering by Matt Phar and tinker on OpenGL (path tracing maybe). Should it interest you enough, you may look into grad school for computer graphics research.

Link to the textbook.

[Physically Based Rendering](https://www.amazon.com/Physically-Based-Rendering-Third-Implementation/dp/0128006455)

I hope I  was able to help. :)",gxnkmnq,t1_gwmym3a,1620680682.0,False
n2n0ax,"Hi all,   


Firstly, thank you for this thread! When I was in college I was a CS minor and got to object oriented programming with C++ and intro to Python. I am currently taking a gap before I start full time in a technical business role. With that being said, I would like to improve my programming skills before I start work. 

I have tried using the great courses in the past, but the lack of homework assignments / projects didn't give me any opportunity to test my knowledge. 

Does anyone have advice on good platforms to learn? Good ideas for projects? Advice in general? 

Thank you in advance.",gwnr1vw,t3_n2n0ax,1619962003.0,False
n2n0ax,"a lot of the berkeley classes have their materials avaiable ([https://cs61a.org/](https://cs61a.org/)) - you can look at the other classes too (61b) and these all have homeworks and exam prep materials as well! 

also a side note - the best project is something that you're interested in continuing so think about any problems you personally would like to solve or things that you think might improve the quality of your life and continue from there!",gys80de,t1_gwnr1vw,1621483258.0,False
n2n0ax,awesome… thanks so much!!!,gyujbbw,t1_gys80de,1621532728.0,False
n2n0ax,"Sooo I recently started getting certified with CompTIA and started falling down the tech rabbit hole…now I want to go ahead and get a degree. It’s a little daunting when I’ll be 30 this year but I’m seeing that certs will only get me but so far. I will not be able to attend university in person, instead it will all be online and I’ll be starting the first 2 years in a community college. My 2 options are CompSci and Cybersecurity. I’m kind of on the fence as to which I should go for. I’m more interested in the latter, but the courses seem tougher (looking at you MATH) but it seems the former has a broader range. I’ve read people say to go for CompSci and stack electives for CYSE but this is all new to me. 

Does anyone have any experience/advice they could offer on this subject? I live in Virginia and Microsoft is investing heavily in our area with data centers, as well as a massive amount of DoD related jobs. I appreciate the help in advance! 

At the end of the day, I’m excited to go back to school but a little nervous would be an understatement.",gwoiny1,t3_n2n0ax,1619973794.0,False
n2n0ax,"hey there! i want to say that it's great to see you're interested in returning back to school. there are TONS of career switchers and adults interested in obtaining a cs degree now (especially at my university) and you are definitely not alone.

i would agree with the sentiment of a general computer science degree. if you're truly interested in cybersecurity, certifications aren't a bad way to go to obtain your own knowledge and show interest in the field. depending on how academic or theory-oriented you want to go, you may even need to obtain a master's in cybersecurity regardless.

i'm not sure if you already have a degree or not, but there are a few online CS programs that may interest you. this is [a generic list](https://www.bestcolleges.com/features/top-online-computer-science-programs/) i pulled up from Google, but i can personally say that Oregon State University's post-bacc degree program is ALL adults who have coming to get a CS degree in their late 20s or 30s/40s+. it's never too late - you'll be 40 someday regardless! 

cybersecurity is not always pure math in the sense of using calculus and such, but does involve a lot of logic-oriented math concepts (discrete math). 

happy to answer any more questions!",gys7t6s,t1_gwoiny1,1621483140.0,False
n2n0ax,Guys I’m new to computer science and I want to learn the basics of computer science and programming any advice on how to start ?,gwolce0,t3_n2n0ax,1619974843.0,False
n2n0ax,I don’t have the most experience with programming but I’ve gotten a healthy start with Java. I’m really enjoying it and my suggestion would be to learn how to implement blocks of code into your own programs. I find that it helps to both expose myself to new content to learn as well as practicing using the knowledge I have to solve problems I don’t know how to do. You will probably find that you will try to do a project and can’t figure it out. It can get frustrating but work in progress projects still teach plenty.,gxn3x0r,t1_gwolce0,1620673547.0,False
n2n0ax,"Is computer science like an umbrella term? Because I know there’s coders, programmers, data analyst, etc. so do they all fall under computer science?",gwpb5ag,t3_n2n0ax,1619984931.0,False
n2n0ax,I think it’s just if you tell someone what you do and they respond with “so you are kind of like an IT guy” then it’s computer science,gxn42id,t1_gwpb5ag,1620673611.0,False
n2n0ax,"yes, computer science is an umbrella term in some sense. the most common major that schools offer will be a computer science degree, where you learn about programming to computer architecture to operating systems and more. in terms of actual careers that are offered, nowhere will you really find a ""computer scientist"" opening. 

i have a [video where i explain](https://www.youtube.com/watch?v=_1o_aq0pdJY) some of the different roles available in the tech industry if you're interested in learning more.",gys4njb,t1_gwpb5ag,1621481359.0,False
n2n0ax,Hello! I’m a junior in highschool and I’m still thinking about my choices for college. I want to do a computer science major with a pre med program. Would this be too demanding of a workload? Any thoughts about it?,gwpsdl3,t3_n2n0ax,1619993492.0,False
n2n0ax,"pre-med tracks usually require a lot of science courses that you won't need for your computer science major, which would mean that unless you want incredibly packed courseloads every semester or quarter, you would need to extend your time to undergrad to 5 or 6 years to get through all the classes that you need.

that being said, computer science is a demanding degree as is pre-med, but it may be manageable if you are willing to spend a bit more time in college to spread out difficult courses.",gys76vj,t1_gwpsdl3,1621482775.0,False
n2n0ax,"Hello all,

I am a senior majoring in Electrical Engineering. I have one semester of school left. This past semester, I took an Embedded Systems course at my college and fell in love with the discipline. I really see it as my future career. The thing is, I do not have as strong a programming background as some of my peers do. Specifically, I haven’t taken two courses that they have: Object-Oriented Programming and Data Structures and Algorithms. Was just looking for some insight on the best way to learn and practice these two concepts. I did take the first Computer Science course offered at my school, which was programming in C. Appreciate any and all advice! Thank you!",gwqchb7,t3_n2n0ax,1620004264.0,False
n2n0ax,I'm currently eyeing a degree in Computer Science for college (I'm a HS senior). I originally planned to major in Chemical Engineering but decided to take Computer Science instead even though I have little knowledge about programming and robotics. Will it be a bad idea to major in ComSci because of that? I'm really interested in the course and I feel like I'm going to be the only person in my course who isn't a programming prodigy or didn't join hackathons. Am I about to make a huge mistake?,gwsphcd,t3_n2n0ax,1620058014.0,False
n2n0ax,Hey man you and I are pretty much on the same boat. I am also a HS senior.I have little to no experience in coding but I'm trying to learn. I think it's completely ok as long as you love computers. I've made it my goal to learn atleast 1 new thing about computers/coding everyday before freshman year of College. Mainly I've been Following along on YouTube videos on coding in different languages. I then use what I learn to try to code my own things such as discord bots.,gy1faun,t1_gwsphcd,1620948908.0,False
n2n0ax,Omg same! I acrually tried subscribing to Coursera and other online learning platforms for a month and testing the water if this course is really right for me. Thank you so much. I just felt like I was the only one feeling this way.,gy8zv5n,t1_gy1faun,1621109002.0,False
n2n0ax,"definitely not!! i've answered this before in the thread so you can see one of my earlier responses but i personally have never participated in a hackathon and landed internships and will be working at a big tech company full time :) everyone comes from different interests and backgrounds and if you are genuinely interested in learning computer science, you'll be able to succeed!

i went in college with 0 background knowledge and also did not spend the summer before learning anything about programming/cs.",gys64du,t1_gwsphcd,1621482175.0,False
n2n0ax,I’m brand new to coding and want to learn the basics of C as my first language. What text editor and compiler do you recommend for Mac?,gwxhe03,t3_n2n0ax,1620148649.0,False
n2n0ax,"i'd recommend [Visual Studio Code](https://code.visualstudio.com/) as your one-stop shop as an integrated development environment. they are plenty of plugins for you to customize your development environment and make life easier for you.

there are not really different types of compilers - there is really just gcc to compile C programs with. i might recommend learning a less complex language like Python first to get familiar with programming syntax as Python is a high readable language with tons of starter resources out there, but C will also teach you important topics you won't find in Python (namely memory management and pointers).",gys6xoo,t1_gwxhe03,1621482626.0,False
n2n0ax,"Don't start with C. It's syntax is so complex. I study it for national Olympics. For example: a[1] =1[a] = (a+1)[0]. These are all same statements and very confusing. It will teach you memory management but really not worth it at all. But you can learn it later as well. For now, go with simple language. I recommend you Python or Java. Python is easy-to-learn and use. If you use it, use VsCose or Python IDLE. Java is best for Object Oriented Programmimg and C like so you can then learn C. If you use it, use Netbeans or Eclipse.",gzveway,t1_gwxhe03,1622297243.0,False
n2n0ax,"Where do you even begin? There are so many ""boot camps"" etc....I know this can be a lucrative field and its always interested me, but is there any course/certificate or even part time online college to attend to take courses so you could get hired...

Any idea what the salary range is for someone in CS with a certificate/boot camp/ or associates degree?",gwxhfdn,t3_n2n0ax,1620148664.0,False
n2n0ax,"as long as you've been hired, your salary should not be different from any other recent college graduate if they were to be hired at that same company - if you're hired at the same level and performing similar tasks, you should be getting paid similarly.

it really depends on how much time you're able to commit and your financial situation. a full degree would be the best case scenario because you get a lot of time to learn a lot of things, and certifications would be less ideal since they only teach you a very specific subset of isolated skills.

my general ranking would be bachelors > associate > bootcamp > certification. there are plenty of online CS programs and bootcamps though if you can afford it that will likely serve you better than one-off certifications.",gys6g51,t1_gwxhfdn,1621482356.0,False
n2n0ax,[deleted],gx1nwsk,t3_n2n0ax,1620233561.0,False
n2n0ax,"hypothetically you could get a job with no degree as long as you can prove you're technically competent and get a recruiter interested in your resume/background. that being said, having a degree is one of the things that help grab a recruiter's attention, and a full degree will likely cover a lot more content and higher-level courses than just an associate's degree, as well as give you more project opportunities, connections, and time to find internships/full-time jobs/what have you.",gys6nqh,t1_gx1nwsk,1621482472.0,False
n2n0ax,What’s the difference between computer science degree and web full stack developer? And how to choose between those two?,gx2ifyk,t3_n2n0ax,1620246364.0,False
n2n0ax,"I haven't been in CS courses so can't really tell for sure, but looking at the curriculums and their topics for classes, it's all about how computers work; more on the software side since hardware stuff would be in Electrical Engineering, maybe.

A full-stack dev is someone who can do ""backend"" and ""frontend"" (and more) of web development. The backend is the server and stuff, and the frontend is mainly what you see on the web browser. The reason the term exists is that someone can make a decent living by just doing one of them, and it helps a bit to make a distinction between them and those who can do both. But the term is kinda loose in definition in that, one could claim to be ""full-stack"" because they watched 3 hours of youtube course and can manage to set up a server and do this and that on the frontend.

&#x200B;

The concept of ""choosing"" between them is kinda weird since those two are not equal alternatives.",gx5pmrq,t1_gx2ifyk,1620313924.0,False
n2n0ax,"Im a 3rd year in my Computer Science undergrad(I am not well versed in any particular language, but have learned Python, Java, HTML, and some C), how can I earn money to invest in crypto? I already am a part of a research project in Unity for the summer but was looking for something smaller on the side

Is freelancing a valid option? Ive heard its kind of hard nowadays to do freelance coding, where do you even find such work for my skillset?",gx7d75w,t3_n2n0ax,1620338757.0,False
n2n0ax,So I have heard all different things but I want to know what is more important: a degree or certifications? I want to go into computer programming but with my current job (Manager at Fedex) I work 60 to 80 hours so it would be really difficult to balance,gx7uil6,t3_n2n0ax,1620347216.0,False
n2n0ax,"I just finished a year learning Java at my high school. I'm really interesting in learning new languages and diving into the world of CS. I think in the course next year we'll be doing more projects in Python, but I wanted to try to learn a language this summer. I've been doing some research and I saw Kotlin, Scala, and C# pop up a lot. Any recommendations for languages to learn?",gx89t4v,t3_n2n0ax,1620355094.0,False
n2n0ax,[deleted],gxcm9wk,t3_n2n0ax,1620443552.0,False
n2n0ax,[removed],h0ix252,t1_gxcm9wk,1622777730.0,False
n2n0ax,"Hello, 
I’m planning to major in computer science in the fall at a community college and plan to transfer to a UC once I get my bachelors degree. Im fairly new to computer science, and would like to know before I go to college, is there anything I can do to prepare myself like learning how to code. If so how should I go about this and where to start.
Thanks in advance!",gxgmfhb,t3_n2n0ax,1620532034.0,False
n2n0ax,"So, I have my bachelor's in Mechanical Engineering. I came out of college actually disinterested in the subject matter, but thankful I have a degree. I am very interested in Computer Science at the moment, and am learning how to code well on my own. The job I have has a policy where they can pay for 90% of a Master's Degree, should I ever pursue one.

&#x200B;

I'm thinking of going to Master's for Comp Sci. But, I don't have a Bachelor's in Comp Sci, so what does that mean? Will I be unprepared, and/or turned down admission because I don't have a CS background? What courses can I do on my own to prepare myself for the necessary skills that a bachelor's in CS would provide, or am I just wasting my time and shouldn't study this on my own first and apply now?",gxgx4dr,t3_n2n0ax,1620538638.0,False
n2n0ax,[deleted],gxh5ukv,t3_n2n0ax,1620546039.0,False
n2n0ax,"html and css themselves are not complete programming languages in the sense that JavaScript or Python are. applying algorithms and data structures will be better served using JavaScript so you can work to understand the different concepts that build on top of basic data structures and algorithms. 

html and css are only used for web design, but not in relation to algorithms/data structures.",gys2ux8,t1_gxh5ukv,1621480409.0,False
n2n0ax,"I have a research about Operating systems. Can you send me good book, article or other resources about Operating systems.",gxh5ysb,t3_n2n0ax,1620546149.0,False
n2n0ax,"I am considering different options for an online BSCS degree. I have done extensive research and have pretty much narrowed my options down to three school. University of Florida, Regis University, and Colorado State University Global Campus.

UF is obviously the most recognizable school from the list. I am not worried about their quality of education or accreditation at all. My only concerns with this school is the out of state tuition, and the admittance process.

Regis University is also expensive, but I feel it would be easier to get into. What I like about this school is the 8-week rolling term, and ABET accreditation. I am concerned about this program due to the lack of any positive reviews about the quality of education and professors.

CSU Global is the most affordable option. They offer 8 week rolling terms as well, and seemed at first to be the best fit for me. My concern with this school is that I can't find many reviews or testimonials at all! I am worried about the quality of education and the way that class work is done. I am unsure if they have any lectures at all, and it seems to be an almost self-taught program. I am also worried about how this degree will look to employers. They are accredited by the HLC, and are a part of the CSU system. They are public and non-profit. I understand that a degree in CS just checks a box, and the really important aspect of a resume is experience and portfolios, but I do not want to be waiting over a year to land my first decent job after graduation. 

Any advice you could give me would be greatly appreciated. Any experiences from any of these programs would help me greatly in making a decision as to where I should begin my journey.",gxiyldf,t3_n2n0ax,1620587302.0,False
n2n0ax,"for what it's worth, accreditation (ABET or otherwise) is really not important. as long as it's a legit program with rigor and a full degree program, you'll be a-okay. 

i'm not sure if you've already vetted out this program, but the [oregon state](https://ecampus.oregonstate.edu/online-degrees/undergraduate/computer-science/) online program is decent as far as i know. OSU also offers a post-bacc program - if you already have a bachelor's, you can apply for the [post-bacc program](https://ecampus.oregonstate.edu/online-degrees/undergraduate/computer-science-postbacc/) to avoid taking many general ed classes, which may end up being much cheaper for you. 

i'm happy to answer more answers about the program but i unfortunately do not have much experience with the other programs you've mentioned.",gys2ojx,t1_gxiyldf,1621480312.0,False
n2n0ax,"I am 31 and returning to college for the second time. I have 4 years of college under my belt and was just about to transfer into a mechatronics engineering program before I had to take some time off (bad idea). Instead of going for Mechatronics, I have decided to go for machine learning/computer vision AI. I got up to calc 2, but hadn't started physics. 

Any tips on getting back into math, and to prepare myself for the core computer science classes? I have about 7 months to prep for my return to school. 

I have started on Khan academy from the basics to remind myself of all the rules, but I wonder if there is a better option. 

I also have the income currently to load up on CS/math books so if you have one to suggest - please do! 

Thanks!!",gxjem0v,t3_n2n0ax,1620595014.0,False
n2n0ax,"with machine learning/AI, understanding linear algebra will also be really important. the [3blue1brown](https://www.3blue1brown.com/) channel is really helpful in covering basic linear algebra concepts and was a resource i used heavily during my linear algebra class. his channel also covers calculus and physics as well which is a plus.

i'm not sure if you're looking to do a general computer science degree, but if so you'll likely also have to take a logic-oriented math course (discrete math - [helpful resource](https://math.berkeley.edu/courses/choosing/lowerdivcourses/math55)).",gys29b3,t1_gxjem0v,1621480091.0,False
n2n0ax,"Thanks for the reply! I will check out that channel. I hadn't heard of discrete math until recently and have watched a few videos on it. I actually think I will enjoy discrete math and look forward to it - I've already taken a intro to logic class and did well in that, so the application of that to math is super interesting.",gyse8o7,t1_gys29b3,1621486966.0,False
n2n0ax,"I’m wanting to study some type of computer science at university in 2 years but was confused at the different types. There’s multiple options like software developing, computer science, engineering and technology and game developing. Would anyone be able to sum them up and give a recommendation for what to do?",gxm0p1c,t3_n2n0ax,1620656898.0,False
n2n0ax,"yes! no worries. computer science / software engineering are both huge fields and there's a lot of barriers to knowledge here. 

**computer science** \- your most common degree program. computer science is pretty all encompassing and will teach you some history of computer systems (operating systems, computer architecture), how to program, data structures and algorithms (pre-defined ways to store data, handle data, move data around in the abstract), and apply programming to different topic areas of computer science (you can take game dev electives or web development electives, etc.) you will also usually have to learn fundamentals of software engineering (working in a team, project management, how to break up tasks).

**engineering and technology** \- this sounds like a very general engineering program so it's hard to say exactly what this means. i would assume this combines different types of engineering with some focus on tech literacy or programming, but it's hard to say without taking a look at the specific courses in this major at a particular university.

**software engineering** \- some universities will offer specifically software engineering as a major, which is more of an application of computer science. in general, i would say a major in software engineering focuses less on the theory and history behind computers/programming and more about applying programming to create software, project management, and program testing. 

**game development** \- this is a subset of computer science, and you may hear of different kinds of CS specializations/applications. other fields related to this include machine learning, data science, front-end/web development, computer engineering/systems, and so on. game development specifically will lead you down the road of graphics, simulations, and maybe some user interface/user experience knowledge. a lot of game development programs require calculus and physics to help you understand how to implement realistic physics in games. a very common programming language is C#, as the Unity framework is based in C#.

let me know if you have any other questions!",gys1yd6,t1_gxm0p1c,1621479931.0,False
n2n0ax,"Thank you very much this helps a lot :)

My original intention was to enlist in the RAF to do something using my degree, especially as new technology and systems are on the rise, so I would assume taking software engineering would be the best choice in that scenario? Im assuming because it’s more focused on the applying of the programming the other elements of the topic wouldn’t be as useful (history of it etc). 

And thanks again, it’s good to have someone with more knowledge on the topic to help out",gysm1b6,t1_gys1yd6,1621492699.0,False
n2n0ax,"i think it depends - i'm not really familiar with military applications of programming, but it's not like computer science means you will literally study how computer science meant to be. more so just small sprinkles in your classes to build knowledge about how systems have evolved to their modern day equivalents.

i think a general cs degree is better than a software engineering degree personally speaking - you can always learn software engineering practices and the CS degree will still cover aspects of software engineering, but it's harder to motivate yourself to figure out how/why things are happening under the hood when you're working as an engineer.",gytx2ix,t1_gysm1b6,1621523418.0,False
n2n0ax,"I’ll definitely lean more towards computer science as I’m still unsure about that being my career option and I don’t want to limit myself to that. Besides I have 2 years to decide 
Thank for all the advice :)",gyu6g5m,t1_gytx2ix,1621527369.0,False
n2n0ax,"I'm a new technology teacher at a high school, and the school year is almost over, so it's a weird time to take over a class. The students in the programming II class don't know who Charles Babbage, Alan Turing, or Ada Lovelace is. I was going to briefly mention them, and maybe make them bonus or trivia questions on a quiz, but I want to make sure I am relatively accurate. Yes, I've read the wiki pages on them, but they've done a ton of stuff so I'm just trying to give a one sentence explanation. What I'm going with...

Babbage invented the theory of how a programming machine would work. 

Turing created the computing machine later when the technology was available. 

Lovelace invented programming. 

Seem close-ish? Obviously these are super broad but I just want to name drop some important people so if these kids go on to college, they'll at least have heard of them.

Another sub(that is literally for asking questions) said this sound to much like a homework assignment so they wouldn't help me.",gxm2og1,t3_n2n0ax,1620657782.0,False
n2n0ax,"I was a solid math student in high school, now I'm in my 30s working toward my CompSci degree. I'll need to take Cal 1 & 2 for my degree. I'm not sure where I should restart my Math education at? I could go to College Algebra, Trig or Pre-Cal. I don't want to waste credits, but if it makes sense to start at College Alg I'll work my way up from there.

&#x200B;

Anyone else out who might've been in a situation similar to mine have a recommendation? It's been so long since I've needed anything beyond basic math skills I'm not sure how I'll fare at the college level. The most important thing to me is that when I finish my degree, I'm ready to start learning real-world applications of my college work and can work my way up to being a competent professional.",gxn1pzx,t3_n2n0ax,1620672631.0,False
n2n0ax,"Maybe jog your memory using Khan Academy. It helps to ease you into a subject as a support.

[https://www.khanacademy.org/](https://www.khanacademy.org/)

It is for free. The subjects you mentioned are all on there.",gxnm2k1,t1_gxn1pzx,1620681324.0,False
n2n0ax,Thank you I'll look into Khan academy! It'll help me see if I'm ready for higher level math.,gxo8x4m,t1_gxnm2k1,1620692347.0,False
n2n0ax,I’m just getting out of high school and am going with a cs major. I’ve been wondering what it’s like after learning a 4th or 5th language and the process of learning new languages later on. I’m assuming having experience with very diverse languages is the best option for the first three or so. I’m just curious about where I’ll end up before figuring out where to start,gxn2pvf,t3_n2n0ax,1620673046.0,False
n2n0ax,"i'm not too certain what you're asking - do you already know a few languages or are you starting out completely new? i'll try to answer on the latter.

the most common languages taught in schools are going to be Python, some flavor of C/C++, Java, and at least cover JavaScript (mostly used as a language for web development), all of which will teach you important programming principles. from there, you might learn a few more modern languages (such as Go, TypeScript, Rust, etc.) but for the vast majority of these languages, the only difference is syntax (how to write a loop, how to declare variables) less so understanding fundamentally how to do something. 

essentially, learning your 2nd or 3rd language will be exponentially easier than learning your 1st, but becoming an expert in any of them will require time and a good understanding of design patterns.",gys0cvf,t1_gxn2pvf,1621479104.0,False
n2n0ax,"How important is an active Twitter/Instagram profile for getting a job?

Long story short: I've realized that Twitter has a huge impact on my mental health, and I want to cut back on using it, which is constantly. Is it enough to just showcase projects on a GitHub or a personal website? Or do I have to cultivate an active social media presence to even get my foot in the door?",gxnk5o6,t3_n2n0ax,1620680472.0,False
n2n0ax,"You do not need twitter to get a job. Moreso linkedin and github. I find networking opportunities on slack as well, if you can find groups there. Don’t worry about Twitter or insta. Your mental health is way more important",gycep5k,t1_gxnk5o6,1621185632.0,False
n2n0ax,"Hello,  
  
I will be an upcoming freshmen at UC Berkeley intending to major in computer science. As someone will little to no experience, I was wondering what would be the most optimal use of my summer or if anyone could give me any recommendations on things I could do. The idea of internships was also thrown around but as someone with no experience, I doubt I will be a competitive applicant or benefit from such. I'm assuming internships wouldn't also be teaching me the fundamentals of coding, rather internships are meant to give me experience, but correct me if I'm wrong. I came across a couple of tech and web design internships but I doubt they would be helpful in the long run if I'm planning to pursue SWE. AS of now, I'm thinking about just learning coding through websites like codeacademy or watching lecture videos for CS61A. Is there a better use of my time professionally?",gxtwjw7,t3_n2n0ax,1620806485.0,False
n2n0ax,"hi! congrats and best of luck to you! 

i want to stress that you are not required to have prior experience jumping into cs programs/degrees (myself included - i came in with no knowledge!). it can definitely be stressful though because there are many people around you who are vocal about having prior background knowledge and knowing how to good, but i can promise you that you will catch up quickly.

i would encourage you to keep an open mind regarding ""software engineering"" - this is an incredibly large field with multiple different subsets. you could end up as a front-end software engineer, which is a fancier way of saying web design. 

casual learning is a safe and light way to introduce yourself and reading up on /watching the lecture videos is also great. this will be one of the few times you won't be hearing about internship grinding from your peers so go at your own pace! if you really wanted to get more familiar, i would look at creating small projects using whatever language CS61A is taught in, and even glancing at the data structures course (61B i believe?).",gyrylib,t1_gxtwjw7,1621478189.0,False
n2n0ax,"Hello all,  
  
I am graduating with my bachelor's in computer science soon and was wondering if having 5 years of IT experience in addition to my degree would at all help with my chances of getting a job right out of college. Any advice or pointers for someone just getting out of college with the degree?",gxvj3so,t3_n2n0ax,1620839687.0,False
n2n0ax,"How do I make code that interacts with the internet? I've been coding for 3 years but only within a certain program. How/where could I write code that could say, search something online or play an online game?",gxwy4q8,t3_n2n0ax,1620861343.0,False
n2n0ax,https://beej.us/guide/bgnet/,gy0ett0,t1_gxwy4q8,1620932304.0,False
n2n0ax,"Where would be the best sub to ask questions about specific, non-personal programming issues unrelated to one's own career? My question has to do with the 2038 rollover problem similar to Y2K.",gy4cbay,t3_n2n0ax,1621011867.0,False
n2n0ax,"If possible, I'm curious as to what are some up and coming CS jobs that are or ill be in demand in Europe.  Specifically netherlands or even Germany. I hope to move there one day but have no degree/portfollio or background in CS and was hoping to change that.  


Any specific suggestions/recommendations or does anything go?",gy8j9ix,t3_n2n0ax,1621100862.0,False
n2n0ax,"Hej! Hope to get some advice. I am planning to do a bootcamp, my only dilemma is how to choose between 2 schools. Thing is they are quite different in the syllabus, first one is ’just’ front end development while the other is full stack. For some reason i just cant decide, it is really a tough decision. Especially since i have to decide in the next 6 days or i loose my place in the first school. 
I understand bootcamp is just a foot in the ass but still i wanna get as much education as possible as a good base. 
Or is it possible to dive deep into front and and then eventually learn back end as i work?",gy963lv,t3_n2n0ax,1621112219.0,False
n2n0ax,"Hi there! I'm wondering if either of these are good online programs to take in Python:

[https://www.edx.org/professional-certificate/introduction-to-python-programming?index=product&queryID=7f16a894efe05d220a1a7c769179267f&position=11](https://www.edx.org/professional-certificate/introduction-to-python-programming?index=product&queryID=7f16a894efe05d220a1a7c769179267f&position=11)

[https://www.edx.org/professional-certificate/python-data-science?index=product&queryID=e120560c105e4919728f62f3857e1f79&position=2](https://www.edx.org/professional-certificate/python-data-science?index=product&queryID=e120560c105e4919728f62f3857e1f79&position=2)

I just recently graduated from university with a BSc in Chemistry, and am looking to top up my degree and gain data science/data analysis/computer programming skills as I don't want to work in Chemistry going forward. Does anyone have any advice?",gycxyzn,t3_n2n0ax,1621194269.0,False
n2n0ax,I’m about to graduate w a bs in biology. I have no cs experience but think I should learn it either as a supplement to my degree (computational bio/bioinformatics) or as a career change. I don’t have the best gpa but was looking into some masters programs for cs that have intro level courses looped into the beginning to get noobs caught up. How hard would it be able to get taken seriously at one of those as a career changer with a meh gpa do you think? Obviously I would be competing against others with CS degrees and better gpas and idek if that’s worth it. I don’t even know though if this is the best idea. What would you say is the best place to get started? A degree? Courses? Self-study? And what language? I honestly have no idea.,gyd9l68,t3_n2n0ax,1621199497.0,False
n2n0ax,"I'm finding it difficult to stay motivated in university studying CS, are there alternative ways of getting into the industry?",gyf5apv,t3_n2n0ax,1621236978.0,False
n2n0ax,When a process makes a system call how does the OS know where to return after? I guess this means that the operating system has to have its own stack?,gyflmpl,t3_n2n0ax,1621250771.0,False
n2n0ax,"this may be (somewhat to very) incorrect so let me know if this is unclear, but from my limited knowledge and vague recollection, system calls are essentially just API calls for your operating system. any event that forces a CPU to execute some code in the kernel can be caught by trap handlers. in setting up our trap handlers, we can save the execution context (state of the program such as registers, flags, etc.) of what's going on before we execute the system call. the CPU itself will store some things about the current context, and the operating system can store a user-defined object/struct that contains other relevant information about the state of the program.

once we execute the system call, we trigger a software interrupt and control is handed off to the kernel/hardware to do whatever is needed by the system call. our operating system can then pop the user-defined object/struct with all the stored information to know where to return to, and we can then return from the system call with another interrupt (iret or interrupt return) back to wherever the program was executing beforehand.",gyrx6ey,t1_gyflmpl,1621477473.0,False
n2n0ax,"I've been out of school for a few years, so my math skills are probably very dull. So I figured I should get prepared for CS by reexamining the content of relevant high school math subjects. But I don't know which subjects are needed. I assume all are.I've perused the Internet for info on what exact math courses I should excel at before starting  the CS program, but it only suggests what CS courses to take while at college. 

What high school math subjects should I go back to for practice and knowledge?",gyied4v,t3_n2n0ax,1621296982.0,False
n2n0ax,"are you looking to go back to get a full CS degree or just for self-studying? if you're looking for a full CS degree, most universities will ask you take a basic calculus course as well as some logic-oriented math (sample of such class + resources [here](https://math.berkeley.edu/courses/choosing/lowerdivcourses/math55) from Berkeley). 

do you also know if you're interested in any math-intensive subsets of computer science? machine learning/data science will require (or at least greatly benefit from) a heavy statistics and linear algebra background, while your average software engineer will rarely be pulling from abstract math concepts on the job.",gyrvffa,t1_gyied4v,1621476605.0,False
n2n0ax,"Can you recommend some examples of good software design to look at?

Books and courses give examples of bad design (code smells) and general principles. In trying to follow them, I have the feeling that I'm going too far the other way and overengineering my code most of the time.

Picking open source projects at random produces more examples of bad design. Any examples of good design that you can recommend?",gyj1ua5,t3_n2n0ax,1621309091.0,False
n2n0ax,"this doesn't exactly answer your question, but if you haven't read the book ""Design Patterns: Elements of Object-Oriented Programming,"" i'd recommend that as a good starting point. 

otherwise, you can look at open source Google or Microsoft projects (or other prominent software companies) - their design likely won't be 100% perfect but better odds than picking a random OS project off the street.",gyrv3sa,t1_gyj1ua5,1621476440.0,False
n2n0ax,"Hey, I've noticed a lot of jobs nowadays want you to have some programming knowledge. I know codeacademy is a thing want to know if that would give me any kind of leg up. I've taken a python course at the graduate level but otherwise have no programming experience.",gyk8hys,t3_n2n0ax,1621341477.0,False
n2n0ax,Hi everyone. I'm studying linguistics at university and I was considering several options for a masters program. I was looking for opinions on the computer science masters program. Is there any previous knowledge I should acquire before starting? Is it useful my linguistics University degree (a. K. Will I be able to find a job with those degrees and not starve?). Thank you for your answers :3,gyq006k,t3_n2n0ax,1621445980.0,False
n2n0ax,"Hello everyone, 

I've just recently started a master's in computer science and finding it especially tough at the moment. Was wondering if anyone has any forums or places for someone new to learn some programming ?

I don't know if anyone has experience with the book thinkpython2 but all we do is read a chapter and answer the questions at the end and I just feel that the chapter covers little to nothing on what the questions ask us to do.

Thanks for any help.",gyqgbey,t3_n2n0ax,1621452859.0,False
n2n0ax,"I am about to finish 11th grade in a high school that does not teach computer science classes at all. I am considering the possibility of applying for a computer science degree in uni eventually in a few months. I have 0 coding knowledge, as in I know *nothing*, and I don't know much about programming. I'm even uncertain using terms like ""programming"" because I simply don't know what they entail. Would I survive as a cs major? I feel like universities know that not all high schools have cs classes, so would they be expecting students to know a lot about computer science already, or is the fact that I know jack shit about the subject okay? Please be kind, I'm just trying to figure out my future lol",gyril9l,t3_n2n0ax,1621470121.0,False
n2n0ax,"definitely a valid question! and don't worry - there are plenty of people out there who go into college with no programming knowledge (myself included) and end up just fine. no schools expect you to come in background knowledge about your major - you'll take classes to build up knowledge from intro level classes and moving into more specific, focused classes. 

some universities will have different majors, but the most popular one will be computer science. computer science will generally teach you programming (writing code using a programming language), some hardware aspects (understanding basic computer architecture), some theory (how to build a programming language, types of programming languages), and project management/engineering (group projects, how to break up projects into different tasks, working with multiple moving components). 

there will be lots of people in your classes who don't have any programming experience, 100%. there will also often be boastful, loud people who do have programming experience - which is also okay, but important to not let their seemed confidence make you feel any worse about having no experience. 

let me know if you have more questions!",gyruvvu,t1_gyril9l,1621476332.0,False
n2n0ax,"Hi! Thank you so very much for this, I appreciate it a lot.

Since you offered, here’s a question for you: is math that important? I heard that math is essential for computer science but… math is a very broad term. I assume stuff like trigonometry for example will be very very useless with computers, right? I feel like maybe algebra might be important, since we’re working with data and algorithms? But what does “data and algorithms” entail? So many questions.

TL;DR what specific aspects of math are important in compsci?",gywgmsp,t1_gyruvvu,1621565545.0,False
n2n0ax,"this will really depend on what you want to do. if you want to go into data science and machine learning, then statistics and linear algebra will be particularly important (more so for machine learning) 

in a general note, you usually aren't doing anything more complicated than basic algebra in your cs courses, if any. ""data and algorithms"" is pretty abstracted away from you and you'll just be given data you have to manipulate using code but not perform any non-algebraic calculations or anything, 

the most important subject is probably discrete math, with works with logic-oriented math. see [berkeley cs55](https://math.berkeley.edu/courses/choosing/lowerdivcourses/math55)",gz3n3uz,t1_gywgmsp,1621721747.0,False
n2n0ax,"hello everyone, I am hoping someone can help me figure out a learning path for operating systems.

I have a technology background but not pure Comp Sci (BS in CIS, MS in SWE). To get into the CS PhD program I'm aiming for, I have to either take an undergrad operating systems class (it's junior level) or pass a departmental exam.

Since the class has a few prerequisites that I don't fulfil, I'm opting to take the exam. Given my education listed above, can you tell me the main subjects I need to study up on? I'd love book and blog recommendations if you have them, or a project you recommend I take a stab at. 

Mostly, I'm looking for something of a list of topics I need to know about and maybe some context so I know when I find the right resources.

Any help in this is super appreciated, thank you!",gysgrol,t3_n2n0ax,1621488670.0,False
n2n0ax,"so i have an associates in science and i have taken 3 very introductory coding classes in c++, python, and html. i really enjoy it and i think i would be good at it and it’s good instructors but i’m really wondering do i go for a bachelors or try and go the self thought way?",gyuqose,t3_n2n0ax,1621535830.0,False
n2n0ax,"Hey People,   
I am a computer Science Final Year Undergrad. I just want to admit that i do not know shit about coding. I have been trying learn reactJS nodeJS and express to be full stack developer but as much as i am trying i feel i can't learn it. Is there a perfect way to learn this before i give up. I am willing to put the time in and understand it but youtube tutorials are a blackhole. Can someone please guide me. It's a cry for help !  I want to learn !",gyur3ol,t3_n2n0ax,1621536005.0,False
n2n0ax,"I was in the same position when I graduated with a Comp Sci degree in 2018. Take these with a grain of salt as they are the things that helped me personally and are more geared towards ""How to get a job without as much experience as I hope to have"" lol. I'd love to hear your thoughts on them :) In no particular order:

* Try your hand at networking with people in areas of tech you are interested in. Regularly talking about the technologies you are learning can go a long way.
* Develop a support network of people who will not let you quit learning due to imposter syndrome. While you don't have the full stack developer position yet, you are still taking the initiative to learn the skills!
* With my machine learning background, I decided to apply for software developer/engineering positions. While I did not have all the listed skills and technology experience listed on some job postings, I always said during my interview that I am ""eager to learn to do the job as best as I can"". This allowed me to earn income, learn while at work with senior coworkers who are willing to give me some of their time, and continue learning via personal passion projects.
* Segue into... Personal passion projects! Make your hobbies do the work for you. At this point of your career, it would also be a good idea to make a website that functions as a resume. It can provide talking points during interviews. ""I'm pretty happy with how I did XYZ, but I would like to improve/implement ABC.""
* On social media, whether it's your personal account or a separate one, start following people who are active in any parts of tech you are interested in. This was tough, but I gradually found more people as I began networking on LinkedIn and various social tech events. Helps you keep up to date about what people are talking about outside of your region.

Lemme know your thoughts and questions! I'm down to talk about it.",gz3zzyf,t1_gyur3ol,1621728711.0,False
n2n0ax,"Thank You so much for taking time out and help me address the issues.   
I would say: 

* To reply to your first point. i can try networking but it makes me feel dumb and idk sometimes what to reply to certain topics.  
* My friends around me who had a cs degree are  rich and doesn't need a job in IT to begin with so they can't be my support system. The reason i made a reddit account(a cry for help in a way) was this to find a support system or guidance to somehow get out of this mess cause i refuse to believe that i can't make it, i will try and try until i find a way out but won't give up !  
* I did try that and idk why i stopped working on my personal website. I even learned how to deploy it on heroku. i have some projects under my belt. but i felt like i dnt have much to add and it looks lame.  and thn i stopped. Can you take a look at it and guide me how to personalize it ? Here's a link to it: [http://string-iamdbak.herokuapp.com/index.html](http://string-iamdbak.herokuapp.com/index.html)
* I have been reaching out to few people on linkedIn but the matter of fact is i feel i still need to learn a llot to even start working idk please help.",gzccefl,t1_gz3zzyf,1621906708.0,False
n2n0ax,I'll DM you and we can continue this conversation :D,gzcl1rd,t1_gzccefl,1621911295.0,False
n2n0ax,Hey everyone I’m currently gonna go back to school for my associates and I am stumped on wether or not I should do computer science or software development. Which is better if my main focus is on software but I’m not exactly sure exactly where I want to be just yet. Which would you say would pay more?,gyygvi3,t3_n2n0ax,1621613307.0,False
n2n0ax,"I have a mechanical engineering degree with 2 years professional experience as a design engineer and a a year as a project manager.
I realised that I don’t like what I have seen so far from the engineering industry and frankly it feels outdated. From the job that I do, to the company structure and operation. It seems like my degree hasn’t kept up with the now different society, where everything happens online, and me not being a part of this 4th industrial revolution really scares me. I don’t want to be left behind and be stuck with an obsolete set of skills.

I enjoy working on and solving difficult problems that make me think real hard to find a solution. I’ve always been good at maths and a logical thinker, but I went down a different path when I went to University and have not had any exposure to programming. 

Having said that, I have no skills what so ever in programming or computer science, but I just came across an MSC in software engineering, its a full time conversion course that welcomes people with no prior knowledge in CS, but with a degree in a closely related STEM subject. I understand I will have to get up to speed before the course starts. I’m gonna go to the open day and find out everything I would be expected to know before the course starts, and will teach myself that. 

How realistic would it be for me finding a job in the tech field with only having an MSC in software engineering? Is it worth it doing to get my foot in the door?",gz5j99d,t3_n2n0ax,1621771824.0,False
n2n0ax,How is a species that relies on commands supposed to invent anything? Something doesn't add up here at all.,gz5kj1g,t3_n2n0ax,1621772780.0,False
n2n0ax,"Hi everyone,  
Next year I'm doing studying accounting and finance at university in partnership with an accounting firm, so I sort of have my career mapped out for the next 3-5 years and I'm fine with that.  
However I'm very interested in computer science and I'd like to learn it. I don't want to work in tech but I want what I learn to compliment the work I do in finance, or if it doesn't then the skills will still help with starting a business etc.  
But I'm not sure how I'll go about getting the credentials considering I'm not studying CS at uni.   
https://www.youtube.com/watch?v=piSLobJfZ3c  
This TEDtalk is about a guy who studied the MIT CS curriculum using the open courseware platform. This really interests me because it's cheap (I don't have much money to be spending on courses) and covers a full degree.  
If I choose to do this, should I document my experience online so that employers can see what I've learned?   
More generally, how can I prove I know a fair bit about CS to an employer without a degree in CS? Build a portfolio of projects? Document my learning on a blog? Take an online qualification (if so, any examples?) that is somewhat reputable?  
Thanks",gz6dh8y,t3_n2n0ax,1621788005.0,False
n2n0ax,"What are all the mathematical rules and forms that i should learn after basic 10th grade math so that i can have enough background knowledge for computer science's math?

  
Also, I'm sorry if this question sounds weird, but a bit background so that you understand my situation a bit better, since I will have to drop out of school after 10th grade to support my family financially, I won't be able to attend school anymore, but in my free time, I still want to learn math to be able to understand linear algebra and calculus, since computer science has been my greatest passion up until now, I don't want to give up on them yet.  
  
My level at the moment is basic trigonometry, I can calculate Hypotenuse and draw lines and curves on graph with some easy given function.",gz7itx9,t3_n2n0ax,1621807518.0,False
n2n0ax,"My Introduction to Technical and Business Writing course at a community college has a majority of students in computer-related majors (programming, networking, cybersecurity, game design, etc.) I would like to make the assignments in the course more relevant, so I would appreciate your help. Would you please tell me what writing you do as part of your job? If you hire people for computer-related jobs, do you consider writing skills when you hire? Anything you can suggest to make the course more practical would be much appreciated. Thanks for your time!",gz9pa52,t3_n2n0ax,1621862044.0,False
n2n0ax,"Hi people, I will start in data science course and need to learn a language. What language is the best for data science jobs?",gzamsnl,t3_n2n0ax,1621877469.0,False
n2n0ax,"After a few career changes, I’m currently in my first semester of a university certificate in computer science to learn some of the basics. The university also has a 4 year degree I can transfer into afterwards if I want to. Is it worth spending the time on the 4 year degree or would it be more worthwhile to self teach from videos and online courses?",gzauhix,t3_n2n0ax,1621880838.0,False
n2n0ax,"Advice for mathematics major that is interested in CS PhD? I am an incoming senior mathematics major that plans on applying to CS PhDs this upcoming fall semester. My research interests are in theoretical CS broadly speaking: discrete mathematics, algorithms, computational complexity. Would I be better off taking graduate real analysis, differential geometry, or advanced graduate-level linear algebra?",gzbjr65,t3_n2n0ax,1621892121.0,False
n2n0ax,"Long story short, i had to drop college for a while and now i'm trying to get into the programming industry but i don't know if i should apply as self taught, invest in coding bootcamp or finish my degree which would take me 3 years to do. which option would be better in the long run taking into account that i am 26yo? Also which job should i aim for that would give me better career prospects down the line?

Thank you in advance!",gzce9mt,t3_n2n0ax,1621907679.0,False
n2n0ax,"Do you think Codecademy is a good way to start?

Other recommendations?

Thx :)",gzdcb5o,t3_n2n0ax,1621931342.0,False
n2n0ax,"I’m a bio major wanting to switch into computer science. Should I finish my bio degree the go for masters in CS for non traditional student or should I just get a 2nd undergrad in CS ? Also, does anyone have experience or recommendations for accelerated CS program ? 
Thank you in advance.",gzggx00,t3_n2n0ax,1621989286.0,False
n2n0ax,"Hello, I was wondering if anyone could give me any input on various career paths that could be used to help people like maybe those who suffer from poverty once I complete a bachelors in CS. Sorry if my question is maybe odd. Thank you!",gzhcdyn,t3_n2n0ax,1622006814.0,False
n2n0ax,"I am an employed Pega Business Analyst that is also a Certified Pega System Architect. I know how to make a websites with HTML/CSS/JavaScript, I know basics of Python + C++. 

However I am self taught. I am a Chemistry Junior drop out.

The question is: should I go back to school to finish a BS in CS?

Or keep trucking along as is?",gzis7af,t3_n2n0ax,1622041449.0,False
n2n0ax,Is [Code.org](https://Code.org)'s CS principles course good for a complete beginner?,gzl1wo8,t3_n2n0ax,1622077274.0,False
n2n0ax,"I'm going to start a programming bootcamp in August. I have no previous experience with programming. The bootcamp graduates will basically be fullstack developers. The idea is that after the bootcamp, the graduates can enter the job market confidently. 
What I want to ask, is after the bootcamp, will I be qualified to enter a master's program? What's your advice on this ?",gzlzc0b,t3_n2n0ax,1622098548.0,False
n2n0ax,.,gzooawa,t3_n2n0ax,1622150662.0,False
n2n0ax,"I'm looking to start a career path switch. I'll be 40 in few days but have had a growing interest in cyber security but I have almost no knowledge of programming, yet. What are some good resources to get me started in the right direction? Maybe books or apps to introduce me to some core concepts?",gzp0kbo,t3_n2n0ax,1622156513.0,False
n2n0ax,why computer science?,gzrzhff,t3_n2n0ax,1622220233.0,False
n2n0ax,"I derived a simple formula to help generate layouts for typographic work; it can be used to help determine the work-area dimensions and grid structure based on the type/leading size. I've always wanted to turn it into some interactive piece, where you enter in the numbers and it spits them out ([sort of like these phyics calculators](https://www.calculatorsoup.com/calculators/physics/velocity-calculator-vuas.php)).

What would be the best way to make something like that? I know a bit of coding (mostly C/Arduino), so I understand the structure of how to build a program. I could probably also scrape together some HTML if it came down to it.",gzup39l,t3_n2n0ax,1622275756.0,False
n2n0ax,"I am a high school student (currently second year). When I grow up, I want to get into top colleges like MIT, Standford etc. as a cs student. What would you study on if you were in my place. Maths, algorithms, ds, ML, Robotics, hardware, Software Development etc.? 

To state that, I think my Math is not bad. I am good at linear algebra but not calculus. I have studied on data structures and algorithms. I have been programming for a year. I well know programming concepts like; flow, OOP, functional programming. I am good at Java and have a Android App Development experience.",gzvd49s,t3_n2n0ax,1622296190.0,False
n2n0ax,[removed],h0jtnzx,t1_gzvd49s,1622801700.0,False
n2n0ax,Speak clearly.,h0jv4br,t1_h0jtnzx,1622802889.0,False
n2n0ax,"I just graduated from university,my major is computer sciences.However,The problem is I’m really confuse, I don’t know what to do in the future with all several type of thing wich I can do and I can become because of my major.
I feel like I don’t know anything and I’m really scared I don’t find any job my entire life because of this 😭💔
My GPA is 4.73 out of 5",gzvj1lj,t3_n2n0ax,1622299597.0,False
n2n0ax,"What is the best way to explore the different fields in computer science (i.e. software engineering, web design, etc)?",gzx6ygy,t3_n2n0ax,1622330793.0,False
n2n0ax,I would also like to know this,h02hw30,t1_gzx6ygy,1622457129.0,False
n2n0ax,"I don't know if this is the appropriate thread but I don't know where to ask. I have always wanted a job in the computer science field. But due to my bad grades in high school I am unable to receive financial aid for college. My question, is it necessary for me to have a college degree to get a job in the CS field?",gzz32y0,t3_n2n0ax,1622382384.0,False
n2n0ax,"I'm currently an undergrad student in engineering with very limited experience in CS. I want to do a CS masters. I've looked at a few programs, they say you at least need to demonstrate you are capable of handling the coursework. 

Aside from taking CS classes at my school, what others ways would you suggest demonstrating that I 'can handle the coursework'? What is a 'coding bootcamp'? Would  that be a good place to start, or is it something else? Are there any online programs that you'd recommend?",h03pccu,t3_n2n0ax,1622482585.0,False
n2n0ax,"So I jumped the fence and just finished my 1st year CS and am in my first internship at age 32!

I would like tips narrowing interests to find a 'niche' to get good in. ATM participated in two CTFs and really enjoyed it but not that good yet. Also there are these type of events for competitive programming style at my Uni I would like to participate but scared to spread too thin but these events really motivate me more than classes to learn stuff.

I can put around 1 hour per night to practice CS-related stuff (extra-curricular) .

At the moment I feel. overwhelmed with stuff I wanna learn, everything interests me (Security, algorithms in general, AI an machine learning, web-app and full-stack, etc)

I feel like it's a nice problem to have but I feel I could use some focus to learn more efficiently. Everything gets even more enjoyable with a general level of  competence in a subject.

Thanks for your guidance!",h03szdi,t3_n2n0ax,1622484356.0,False
n2n0ax,What does the day to day life of a coder or programmer look like. Looking into web development. Enjoying Python so far. I am hoping to live a more digital nomad/remote lifestyle. I am looking for a career with a high level of autonomy and freedom. Is this realistic in this career path?,h05tjiy,t3_n2n0ax,1622523897.0,False
n2n0ax,"Any dumb people good at cs? I feel my intelligence is less than those around me, but I'm interested in computer science.",h06ctc3,t3_n2n0ax,1622540838.0,False
n2n0ax,"I'm also new to this but I think the same rule appies to everything: of course ""raw"" intelligence helps but it's more about the effort you are willing to put into. When you are studying you have to sacrifice stuff like amount of time spent socializing or stuff like that.",h36jl9v,t1_h06ctc3,1624769343.0,False
n2n0ax,"I just graduated from college last month, but want to try and break into a career as a software developer or engineer. My Bachelor's was in electrical engineering and I have some relevant experience, but no internships however.

I've also got intermediate knowledge in Java and C++ and basic Python, but I'm not well versed in data structures and algorithms, which I anticipate as highly critical to the field. Do I need to get a decent grip on data structures and algorithms before applying to the jobs I want or should I just go for them right away?",h079czv,t3_n2n0ax,1622560550.0,False
n2n0ax,"What is the difference between ""computer & information science"" and ""computer programming & information systems"" degree?  I completed an AAS in information technology and the BS's I'm seeing at different schools are always called something different.  Are these the same?",h07dkav,t3_n2n0ax,1622562419.0,False
n2n0ax,"I'm nearly 20 and I am 3 semesters into college. I have been majoring in Accounting but it isn't my passion, it is mainly something I picked because my parents want me to.

I want to go into Computer Science, but I have Dyscalculia and I have trouble mastering middle school & high school level mathematics even though I have maintained a 4.0 GPA and excelling in other subjects. My biggest fear is being unable to pursue CS due to my disability.

I've heard CS uses a lot of math. Is it even worth it to pursue CS if I have major difficulty with math?",h08i8jj,t3_n2n0ax,1622580430.0,False
n2n0ax,I'm just finishing up my last week of classes for year one of my associates in computer programming. I've managed to get all A's this year but the more I learn the more I feel like I have no idea what's going on and I'm just going through the motions. When are things going to get easier? How do I help myself retain information during the summer? Also did anyone else cry a shit ton while learning all these languages?,h08jigp,t3_n2n0ax,1622581019.0,False
n2n0ax,"Hi! New to this sub, and I've got some questions. I'm a 2nd semester CS student, and next month I'll have a 3-month long holiday.  I want to spend it learning new things, but I don't know what should I learn. 

&#x200B;

Currently I can do C, Java & Java OOP (though still a bit shaky on multithreading and concurrency), Python, some bit of web programming (HTML, CSS, JS & jQuery), and a tiny bit of SQL and decent knowledge in data structures as well. Any suggestions on where I could go from there? I think web dev is cool, but I'm tremendously bad at css and the like. I enjoy coding in Python and C the most. Any suggestions and references would be awesome! Thanks in advance.",h0atgpi,t3_n2n0ax,1622631718.0,False
n2n0ax,"Hi! I have a friend who got a computer science degree from Cal State San Bernandino, and now he's graduated and has no idea what he can do with his degree. He has no certifications or money to get certifications. What can he do and who can he work for?",h0f2wcm,t3_n2n0ax,1622710984.0,False
n2n0ax,I’m gonna start going to uni soon in either computer science or computer engineering and I wanted to ask what will all the universities teach you in the beginning and what happens after and what languages you should learn beforehand,h0f7zzu,t3_n2n0ax,1622715523.0,False
n2n0ax,"If I want to go to university to learn how to design CPUs at Jim Keller level, with a focus on emerging tech like photovoltaic computing, what degree should I go for?",h0f9dfv,t3_n2n0ax,1622716619.0,False
n2n0ax,How far are we from using AI as a real research tool in liberal arts for finding relevant citations and sources and connecting new ideas nobody saw are linked before?,h0fh2py,t3_n2n0ax,1622721852.0,False
n2n0ax,I've had a rocky college experience and have been between schools. Was hoping to transfer to a 4 years school this fall but my no. 1 choice told me that there's no more room in the CEAS department but offered me a spot for other majors including Maths and Physics. I love Computers and want build a career in IT/IS so my question is if it's possible to do so with a degree in physics and what that path would look like?,h0gk2gx,t3_n2n0ax,1622739276.0,False
n2n0ax,Hi! I'm beginning my CS major this fall and have been learning more over the past year or two about programming and computers in general. I have been looking at minors for school that fit well with my major and have been split between an Electrical Engineering minor and a software engineering minor.Which would be more beneficial? I have learned Python and also am looking for a way to get into more practical projects. I have done small projects like make a calculator and even made a program with menus for storing passwords just in pycharm. What is some ways to get into more practical projects working outside of the IDE and more real world learning?,h0jfe7p,t3_n2n0ax,1622789336.0,False
n2n0ax,"Hey all, I am thinking about going to college for computer science. I've always wanted to do something in computers but never thought I was smart enough to do it. Now at 30 I'm going to be starting down this path and I was just wondering if anyone had any good videos or other learning tools that would be good to look into to help me ease into this field.",h0k8soh,t3_n2n0ax,1622811386.0,False
n2n0ax,"I'm 35 with a J.D. and a B.A., and I'm thinking about a M.S. in computer science.  It's always interested me, I think I could combine it with the J.D. in interesting ways, and frankly I need a break.

The problem I'm having is that I can't seem to find a solid list of prerequisite courses I should take.  I've screwed around with coding at a number of points, but I have no professional work product and no formal training. 

Any thoughts?

Also, if I want to avoid online, I'm likely looking at a school around 100 in the rankings (USF) or a new science and engineering school that isn't in the rankings yet but is a state school and presumably legitimate (Florida Polytechnic), so I would love thoughts on that as well....",h0l2tdq,t3_n2n0ax,1622824866.0,False
n2n0ax,What pay are you guys making?,h0lr6ur,t3_n2n0ax,1622835188.0,False
n2n0ax,"I'll be graduating with a B.S. from WGU soon, and I work full time and don't live with my parents. I've had a hard time trying to find internships near Nashville and it seems like every single ""entry level"" position requires minimum 3 years experience, so I'm completely lost on how to start my career coming out of college. I can't afford to stop working and move somewhere temporarily for something like Revature, but I could afford to do a Google internship or similarly paying position (assuming they'd hire me); however, I've had a terrible time trying to find internships outside of California, let alone near where I currently live and would really prefer to stay. Also, it doesn't seem like I could get away with applying to internships 3 years after graduating (with covid, I wasn't able to do any internships my freshman and sophomore years, and now I'm doing WGU and aim to finish my degree by the end of the year, so I'm worried how that will affect my program eligibility for internships going forward). Any advice?",h0msmbw,t3_n2n0ax,1622853131.0,False
n2n0ax,"My apologies if this post is in the wrong place or incorrectly formatted, as this is my first time using Reddit. I came here today to ask, am I a fool for not knowing anything regarding Computer Science before going to university? I just graduated High School roughly a week and a half ago, and am leaving for college in roughly 2 months, and I feel like a fool for not knowing anything about the field of CS. In the discord server for said university, I see every other CS major with crowning achievements, programs, custom bots they programmed themselves, and math AP classes that are leagues above my pre-calculus course from high school. Every time I try to prepare myself to learn about the field of CS in any programming language or program in general, I immediately feel lost and frustrated at my lack of understanding of even the most basic concepts. Is this normal, or am I doing something wrong? Any helpful advice or replies is much appreciated, thank you for taking the time to read my post.

(On a side note, I would like to mention that the high school I attended had no Computer Science courses available for us students. I figured I would throw that out there for context's sake)",h0q4y38,t3_n2n0ax,1622926617.0,False
n2n0ax,"Hello!  I am currently self studying to one day get a job in web development and I have been spending a considerable amount of time learning the typical web dev stuff (HTML, CSS, JS, Some frameworks, NodeJS) and I have found a lot of great content to help understand these topics on a deep level.  But, I keep wondering what are the CS topics I need to be learning to better understand CS concepts that one would learn in school.  Can anyone recommend topics to dive into other than Data Structures and Algorithms that would be beneficial for any programmer?  In case you do have a recommendation, do you know of any good resources to learn these topics?",h0t452z,t3_n2n0ax,1622996585.0,False
n2n0ax,"What's the best place for a working adult learner to finish their BS in math? I took a few classes in undergrad but I didn't complete the major and would really like to so I can work more interesting computer science roles like AI. Would you recommend that I also try to get a CS degree, or is a CS degree kind of null if you're already working in the industry(I just landed a role in an android dev firm after spending this last year building projects)? Again I took a few courses but never finished the major. 

&#x200B;

Or, is trying to get official accreditations kind of useless? Do employers usually not care so much? Is just doing stuff on coursera sufficient?",h0th52f,t3_n2n0ax,1623002773.0,False
n2n0ax,"Does anyone have any recommendations for books or other self learning material for C++ and Swift? 

I’m taking my second CS class this summer and I’ve always wanted to learn Swift so that I can build apps.",h0xaa8v,t3_n2n0ax,1623082845.0,False
n2n0ax,What's the best advice you would give a high school student looking into AI and Python. Start early? Practice more often? Build small projects? Launch a large project? Research?,h0xp1fz,t3_n2n0ax,1623088987.0,False
n2n0ax,"Hey, I am a CS major, looking to transfer to UC (UCI specifically). One question, UCI doesn’t require Calc3 but needs Linear Algebra. I can skip Calc3 completely. But is this required further into my CS? Would I be missing out on a lot if I don’t take this class? It’s gonna take a extra year if I include Calc3 in my course list. Otherwise I would have just taken it.",h0yn7ox,t3_n2n0ax,1623103740.0,False
n2n0ax,"Hi everyone! I'm about to enter my senior year at university, I'm going to graduate with a B.S. in exercise science. I intended to pursue a master's degree in exercise physiology and then get an industry job somewhere. However, after working in labs and doing research, I've come to the conclusion that I really don't want to pursue this path. I'm taking a CS introductory course next semester, and I have the option to complete a certificate of computing elements. If I decided to complete the certification, I'd have to stay an additional semester in college. Does anyone relate to starting a CS career later in college? I just need some advice... I feel intimidated by everyone that has years of coding experience! Should I even go to grad school if I end up loving CS?",h11kxcm,t3_n2n0ax,1623168850.0,False
n2n0ax,"I am 21 years old about to enter my junior year of college. I have taken a few basic CS courses and I am very interested in switching my major. I am currently a finance major but CS is far more interesting to me and I see the long term benefits of a CS degree. I’m in a position where if I continue my current path in education, I feel I will end up unhappy with my career choice due to the fact that I got my degree in something I found easy. Would it be worth my time to change my major to CS and transition my career path?  Also if I do, what are the most important foundations to learn about it?",h15mv4t,t3_n2n0ax,1623248681.0,False
n2n0ax,"Is there any programming book that teaches programming conventions (like constants are capitals, stay below 80 characters per line etc)? If there is, please recommend some",h19w23j,t3_n2n0ax,1623329835.0,False
n2n0ax,"the book clean code does! also look into design patterns, i think that's up your alley as well.",haq7ddj,t1_h19w23j,1630184702.0,False
n2n0ax,Thank you!,hawvp6i,t1_haq7ddj,1630319465.0,False
n2n0ax,"I just finished my undergrad with a violin performance degree from JHU, but I'm seriously considering a career change to become a web developer. I'm in quite a bit of debt, so I'm wondering how practical and how quickly one can actually dive into the field and find any job if self-taught. Is it possible go from a beginner>employed after 6-12 months of efficient/effective studying? Would a coding bootcamp be strongly recommended? Thank you guys for your insights!",h1f0n0p,t3_n2n0ax,1623427594.0,False
n2n0ax,"congrats on graduating! i would say that it's definitely possible and i've heard of similar stories. a bootcamp would be your best avenue if you want something structured and in a time crunch, but they definitely won't tell you the full story/depth of computer science and programming. i don't have any bootcamp recommendations though",haq7izn,t1_h1f0n0p,1630184774.0,False
n2n0ax,"Hello, I was just wondering what a programmer's typical day looked like and the responsibilities that are associated with the job. I am considering beginning my career in programming/ software development and don't know anyone who works in the field, so I thought that this would be a good place to ask.",h1f97c2,t3_n2n0ax,1623431295.0,False
n2n0ax,"there are a lot of youtube videos about a day in the life! for software engineers though, i've seen that most spend half if not less of their day actually coding. the other half is spent in meetings (standup, where you update progress), reviewing other coworkers' code, creating design documents, etc. it may not be exactly what you were thinking!",haq7o5x,t1_h1f97c2,1630184841.0,False
n2n0ax,If you were about to start a computing degree and had to focus on an area/stream of computing what would you learn now? What would be the most interesting/rewarding areas to learn about for the foreseeable future?,h1gryuk,t3_n2n0ax,1623457655.0,False
n2n0ax,"security is definitely a hot topic, as well as ai/machine learning. those are the ones that come to top of my mind, but if those don't get you excited you'll find it tough to work in these fields. i'd recommend exploring different topics on your own and find something that interests you, rather than the other way around. it'll be more natural.

also, not every niche has to be hot in the market. you can be at networking/streaming and end up at netflix, for example, even though networking/streaming isn't innately a ""sexy"" topic",haq7v8v,t1_h1gryuk,1630184931.0,False
n2n0ax,"Hi, I’m 16 and I was just wondering, how do I start my journey within computer science? I’m extremely interested in Machine Learning and Data science.",h1ow3rq,t3_n2n0ax,1623639369.0,False
n2n0ax,"Hi All,

I am 34 and currently working in operations for a pretty decent company. However I think I am worth more than my salary and have a skill set that would likely transfer nicely into computer programming. I also have a 2:2 business degree and am not willing to go back into full time education.

How easy would it be to transfer careers into computer programming? I am a novice but am willing to work hard to make my way into the field.

Also what sort of starting salary would I be looking at? I have two kids and a mortgage so it is also a factor that I need to consider.",h1piwzv,t3_n2n0ax,1623656857.0,False
n2n0ax,"working at a big tech company or fancy start up, you can easily make \~$100K+, although that trades off with potentially higher cost of living. career switches are always hard, but the good thing about software engineering if that if you can pass a technical interview, then the hard part is just about selling your resume (which you can do through networking).",haq8hnk,t1_h1piwzv,1630185221.0,False
n2n0ax,"I'm interested in an MS in Bioinformatics, but I don't have any background in coding. A few schools offer ""crash course"" preparatory courses, such as NYU's Tandon Bridge program. Ahead of enrolling in that program, they suggest completing some free classes from Coursera which cover some basic computer science topics.   


Are these structured courses for free online worth it, if you spend the time? Do you think purchasing a certificate means anything if you're only using it to get some fundamental skills? I feel confident I could learn a lot of the same knowledge piecewise from Youtube and such, but I lack the discipline to constantly seek out topics systematically. I feel like auditing some free classes teaching the basics is the way to go! What do you think?",h1qmgcc,t3_n2n0ax,1623684157.0,False
n2n0ax,"i don't think a certificate matters too much, but i would agree that spending time going through the free courses definitely won't hurt! go for it :)",haq8nuh,t1_h1qmgcc,1630185301.0,False
n2n0ax,"To start, I want to learn Java. And yes, I'm aware that I'll get nowhere in this field unless I build a portfolio of work using the language. 

I'm off to army boot camp in 4 months (I'm back after 6 months of training because I'm in the reserves), and I plan on attending a full-time in-person 3-month coding boot camp when I'm back. 

For the four months before my basic combat training though, I want to study Java as if I'll never be attending a coding boot camp. What's the best SELF-TAUGHT way to go about it? Any online courses/certifications I should pursue? Which website certifications are the worthy ones, and which should I avoid? Any resources you computer programmers could point me toward that could help my learning? Any help would be appreciated. Thanks!",h1sl2df,t3_n2n0ax,1623716571.0,False
n2n0ax,"I’m a Junior University Student majoring Business, I’m curious how much does it cost to built working app In one platform ? Possibly a price breakdown please thank you",h1yu8p6,t3_n2n0ax,1623851229.0,False
n2n0ax,"I was just wondering how much math and physics is involved in a university computer sciences degree. I'm quite into science and technology but don't find math in general very exciting, although i'm able to do it to a suitable level.",h26wm06,t3_n2n0ax,1624008420.0,False
n2n0ax,"You will take a lot of math. Programming is essentially applied math disguised to be human readable, Computer Science degrees aim to teach you not just how to write code, but how to understand what your code is doing at a fundamental level. I loved to complain about this approach as a student but that deeper understanding really does help in industry.

Generally I would say a CSCI degree will have little to no Physics. Some engineering schools require some physics classes no matter the major, and you may have a tiny bit of physics in a couple of mandatory classes, but unless you seek out a scientific computing class or something like it I don’t think you will have much.

Anyways, I wouldn’t let the math thing get you down too much. A lot of Computer science is a different kind of math, the big topics are things like discrete math (logic), linear algebra, and algorithmic complexity analysis (how efficient is this code I wrote) which is quite different from anything I did in high school and had me think of math in a totally different way. I was by no means passionate about math before school but I love my career now.",h298nba,t1_h26wm06,1624051705.0,False
n2n0ax,Hi guys! I’m currently finishing my junior year in high school and I’m planning on pursuing cs (probably not pure cs but a mix with another branch) and I’m looking for a good laptop. I’ve heard that the M1 MacBooks are insane and a new version is coming out this or next year. But I’ve heard that there will be some issues since they won’t be able to run programs I need on college. Should I get it (probably 13 inch) or should I go with an xps (13 or 15). I’m currently have an I phone which is also swaying my decision but I’m not 100% sure.,h2gjhj2,t3_n2n0ax,1624217466.0,False
n2n0ax,"i love my macbook (also in the apple ecosystem) and will be upgrading to the m1 macbooks in the future. i agree that the m1 macbooks have great performance, and the m1 macbook air is a great starter laptop with good performance, battery life, and can handle basically any non-gaming you need to do. 

i wouldn't worry about running programs. you'll mainly just need a integrated development editor (IDE), and ones like VS Code are m1-compatible already.",haq8w8p,t1_h2gjhj2,1630185410.0,False
n2n0ax,"Hey, I (26yr, Male, Australian) am about to make a career change into tech, I have only done introduction courses to CS and Code Academy/FreeCodeCamp courses. I have an undergraduate in biomedical Science, but now I am torn between 2 pathways:

Doing another undergrad in Computer science which would be 3yrs full time, but I would probably extend it to 4yrs because of work. I have friends who have just completed theirs and their careers are sounding promising.

Or

Do a Masters in Data Science for 2 years, then a bootcamp/self learn what I would need for jobs, friends in the tech industry recommend this route since it will utilise my health degree, its a really sort after skills etc. but I wonder if it would be useful in going into a dev/programming career",h2ikw35,t3_n2n0ax,1624261980.0,False
n2n0ax,"do you want to be a software engineer or a data scientist? both are sought after roles, but if you want to just go into software engineering, an MS in data science won't help you much. on the flip side, if you want to go into data science then the MS will greatly benefit you.",haq91um,t1_h2ikw35,1630185482.0,False
n2n0ax,"Hello, I am going to be starting CS in the fall. Looking at the curriculum the classes look very daunting. Especially the math side of it. It has never been my strongest subject. The last class I took was algebra 1 about 8 years ago and I haven't touched math since.

The other classes as look daunting as well. It seems like a lot to take in.

Any advice on how to get a good start? Thanks.",h2ki3hm,t3_n2n0ax,1624302834.0,False
n2n0ax,"the most common required classes will be calculus 1 and 2 as well as discrete math which is a logic-based math class. khan academy will be a great resource for getting started with calculus, while uc berkeley has a discrete math syllabus/resources available.",harmai9,t1_h2ki3hm,1630210926.0,False
n2n0ax,"31 year older, Graduated in Business, planning to make the shift.

I'm really keen on moving careers to IT, at present what my fields of interest are networks and servers... But I'm also trying to understand more about cloud computing.

1. How and where do I begin? Will certifications and boot camps be beneficial or will I have to go through a bachelors...?

2. I've always been an average student, GPA was never over 2.8, but I'm a quick learner and can implement with my understanding and have the bug in me which wouldn't lemme rest until an issue is solved.

3. Am I late at it? Can someone my age or older wjo has done it share their experience on how the switch was for them.

All help is appreciated ✌🏻️🙏🏻",h2mpp5b,t3_n2n0ax,1624349668.0,False
n2n0ax,"it's never too late!! plenty of people switch careers and are successful after the fact. certifications less so but bootcamps can be helpful. 

i would probably recommend an online bachelor's in CS - these will allow you to take classes part-time and give you time to work full time at the same time. many people are able to balance this (albeit with some stress) and start gaining internships under their belt over time, eventually quitting their jobs and moving into internships / full time work. specifically, of online BS programs, online post-bach bachelor's programs will have plenty of other career switchers to share advice and wisdom for you.",harmmjb,t1_h2mpp5b,1630211133.0,False
n2n0ax,Can i study computer science in 2 yrs after IB diploma,h2mpyir,t3_n2n0ax,1624349925.0,False
n2n0ax,plenty of people graduate in 3 years and i imagine the same could be true with IB exams. it can be done in 2 if you can are taking a lot of courses and summer courses as well.,harmr8x,t1_h2mpyir,1630211214.0,False
n2n0ax,"Hi, I’m looking to switch careers at the age of 30. Have been contemplating the route needed to take to become a software engineer. I have been coding on my own, and looking to pursue a degree now. Would it be better to go back to get a second bachelors in CS (First degree is in Health Science from ‘17) or masters in CS?
I’ve heard people say Masters is an overkill, just go for bachelors.

What do you guys think?",h2nmqt4,t3_n2n0ax,1624372837.0,False
n2n0ax,see my reply here! [https://www.reddit.com/r/computerscience/comments/n2n0ax/new\_to\_programming\_or\_computer\_science\_want/harmmjb?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/computerscience/comments/n2n0ax/new_to_programming_or_computer_science_want/harmmjb?utm_source=share&utm_medium=web2x&context=3) post-bacc programs would probably be a great resource for you!,harmtxi,t1_h2nmqt4,1630211261.0,False
n2n0ax,"Hi. I'm a CS student and I just skimmed through the first two years with nothing fruitful learnt. I'm only good enough in C++. with only two years left for my graduation i'm trying to get serious about learning the skills and make a good career path for future. I've been trying to figure out which domain i should make a career in and thus study accordingly.

I come from a financially weak family so I need to get the skills which can get me a good paying jobs later on. After researching for a while I was inclining towards Machine Learning/ Data Science but i've been reading that that field is already saturated plus the rising autoML is only going to make it tough so by the time I graduate that job might become a dead end. Other things i could find was a Web Developer or a software engineer in general.

It would be of great help if anyone could suggest me a career track which would be worth my investing the next two years in studying for it. Also it would be helpful if you could suggest me a starting point or like what all i would need to study for that field. Thanks.",h2ouwpy,t3_n2n0ax,1624392700.0,False
n2n0ax,"hi! i have a video exploring [different tech careers](https://www.youtube.com/watch?v=_1o_aq0pdJY&t=5s) and intend on doing a deep dive for software engineering and PM. you can also find some other PM videos. software development is still very much in demand - you don't have to specialize at all and many don't expect new graduates to have a particular specialization. 

you just need the basic BS in computer science, but passing the interviews is a question of your data structure and algorithms knowledge (leetcoding questions) plus the basic behavioral interviews, obviously.",harnen3,t1_h2ouwpy,1630211627.0,False
n2n0ax,"I have bachelor and master's degree in CS and almost 15 years of work experience. Many jobs, many technologies and different roles. I'm a jack of all trades who does not excel at anything. I am also always 1-2 steps behind of the latest and greatest stuff.

My main goal is to get a remote working position, due to health reasons, and keep it long term. What career path would be the most suitable? What are the latest hot topics in CS? I am happy to learn what is needed.",h2vzcio,t3_n2n0ax,1624544706.0,False
n2n0ax,"Hi! Freshman at community college, pursuing a computer science degree. I’m 25, finally getting around to getting a degree, and honestly no idea what field I am interested in. 

My question is, does anyone think those Google Coursera certifications are worth it? I have no idea if they would be helpful for my career, but it’s perfectly affordable for me so I’m kind of thinking why not. Just not sure if it would be a waste of time, or if it could look good on a future resume. 

I know they just came out a few months ago, but if anyone has any insight I’d appreciate it.",h2wl5it,t3_n2n0ax,1624554413.0,False
n2n0ax,"if you're already getting a degree, i would say it's not worth your time. you can audit the class, but don't bother paying for any particular certifications. experience and personal projects (such as participating in hackathons) can be your best path to success to move into software engineering! i would also encourage you to explore other career paths like product management, design, etc.",harnq39,t1_h2wl5it,1630211825.0,False
n2n0ax,"Just recently switched my major from History to Computer Science, What are some ways I can learn about programing before I take my first CS class? 

&#x200B;

What's the best thing to learn before taking a CS class?

Are there any programs or websites that can help me get a massive headstart?",h2xpu59,t3_n2n0ax,1624577539.0,False
n2n0ax,"your intro to cs class should give you everything you need. if you really want, CS50 by harvard is free and also many say that it's a good place to start learning computer science. otherwise, the best thing i can say is mindset and perspective. learning coding is hard, and you will likely also encounter many people who like to brag about knowing how to program already. ignore all of that and recognize that this is new to you and you'll be learning A LOT in your early classes especially.",haro47m,t1_h2xpu59,1630212073.0,False
n2n0ax,"Hi there,

I am 26 and am interested in switching careers from industrial hygiene/environmental to software development/engineering. Software no doubt has a promising future, I don't love regular 4+ hour commutes to job sites, and the money just is not in my field.  I would like to have a job in software/CS within two years. I'll gain direction as I dive in.

Please let me know of any **essential resources/free courses/books** \- I would love to know about them. 

I am just scratching the surface of intro to CS videos. Currently getting into CS50 2020 Lecture videos. Like any industry I'm sure it's vast and I understand course material is nothing without networking. I plan on youtubeing my way through for at least a few months. Looking like youtube and freecodecamp are my best friends for the next couple of years

Any **advice/tips** to save time? 

Thank you SO much for taking the time to read this!",h2y0y0t,t3_n2n0ax,1624583591.0,False
n2n0ax,"if you are financially able, i would encourage you to look into an online part-time bachelor's in CS. they'll provide structure, connect you with people going through the same thing, and share advice/tips/job listings, etc. the problem with free resources is that it's hard to know where to go next and how to build your skills. you could also look at degree programs and see what courses they have and when to provide some guidance.",harofo2,t1_h2y0y0t,1630212277.0,False
n2n0ax,"Hey all I'm a recent philosophy graduate with an interest in machine learning and software development. I have a very base understanding of computer but would love an opportunity to learn, I know how challenging it is but I would love the opportunity to challenge myself while I'm young. Any advice?",h31pq6i,t3_n2n0ax,1624660689.0,False
n2n0ax,"So I just finished my freshman year of high school and I’ve started to think about my future. I’m very interested in computer science. I’ve take two pretty beginner classes in my high school, intro to compsci and ap compsci principles. I just don’t know where to go from here. I want to start learning things now but I don’t know where to begin. I also want to take the best classes that will help me get into a good college. If anyone has any advice on where to learn some compsci and how to prepare for my future I would greatly appreciate it.",h33apsz,t3_n2n0ax,1624700175.0,False
n2n0ax,"that's great! you could go into learning data structures and algorithms, which is what makes the world go round and underlies a lot of fundamental operations. high school experience is already great compared to a lot of people (for example, i only started when i got to college)! 

you could look at joining high school-friendly hackathons too as a way to network and build different skills, as well as tangible projects! they would be great resume items too, even at a collegiate level.",haroy8a,t1_h33apsz,1630212614.0,False
n2n0ax,"Hi people! I’m a software engineer in a financial services company. I code in Java and of late I find my work is not that exciting. I tried applying to better roles and found my problem solving is just not up to the mark. I realize the CS community is grinding LC to pass coding interviews.

I thought I’ll take a different approach. Since I didn’t formally study CS in undergrad, I’m considering going back the fundamentals. I’m working my way through discrete math and solving problems. It’s a long and tedious road. You guys think it’s worth the time?",h34gqmn,t3_n2n0ax,1624727395.0,False
n2n0ax,i don't really see how learning the fundamentals when it comes to discrete math is going to be insanely helpful when that's not really the knowledge being asked of you in a software engineering environment anyways. i'm curious why wouldn't want to be practicing leetcode given that those are the most important types of questions to know when it comes to interviewing at software companies right now.,haros7p,t1_h34gqmn,1630212504.0,False
n2n0ax,"What's easier to learn on your own, cybersecurity or programming? 

Gonna start school next january and have to pick between software development or cybersecurity. Honestly I plan on learning both and if it were up to me I'd be completely self-taught (got a programmer friend that can help out), but   you have more chances of employment with a degree... So I plan on picking the hardest for school and the other to study on my own on my free time.",h36izby,t3_n2n0ax,1624768913.0,False
n2n0ax,"well, which one are you more interested in? if you're going to pursue a career in something, i would encourage it to be one that you're actually interested in spending years of life in. does the program not offer you the option to take security electives? that can help get your foot in the door and also doesn't pidgeonhole you otherwise.",harp6rf,t1_h36izby,1630212770.0,False
n2n0ax,"Hello! I'm 24 and currently hold a bachelor's degree in business administration. However, I recently learned python from [hyperskill.org](https://hyperskill.org) and developed a huge interest in programming and there are two things I want real bad rn :

1. A career in computer science
2. To move out of my country (India)

What options do I have?  (if any)",h3aiaye,t3_n2n0ax,1624858367.0,False
n2n0ax,Which laptop will y’all recommend for a college student who plans to major in computer science ?,h3dwq33,t3_n2n0ax,1624927193.0,False
n2n0ax,One thing that I'm sure of is go get a laptop that can run Linux,h489j36,t1_h3dwq33,1625573438.0,False
n2n0ax,"it doesn't really matter as long as it can run basic programs. the m1 macbook air is really great for the price and has great performance, portability, etc. other people also like the xps line by dell. like the other commenter mentioned, you can also choose to install a different operating system which lessens the differences even more.",harpd2y,t1_h3dwq33,1630212885.0,False
n2n0ax,I'm looking to do some competitive programming. What are the resources that I can used to learn? How well will it reflect on my resume?,h3fa47f,t3_n2n0ax,1624961718.0,False
n2n0ax,"competitive programming is pretty good, and people who are good at competitive programming also seem to be great software engineers. leetcode can help but i just googled ""competitive programming sites"" and it came up with decent results.",harpgr0,t1_h3fa47f,1630212952.0,False
n2n0ax,"I am doing my masters in Computer science but I am not good at programming at all. I am scared because I feel like I am already so much behind on everything even though I got good marks(partly because of the help from my classmates). Also the fact that I am 26 years old doesn't help as I am supposed to do my Thesis next semester (in 3 months) and I have no idea which field interests me or what I want to do after my studies. 
On the programming part, I know the basics of many languages but I am always scared to take the next step for some reason. For example, I can write basic programs in Python, C, Java but when it starts getting complex, I give up. 
What do I do? Where do I go from here? How do I get over this mental block ? Should I practice more on websites?",h3jemkf,t3_n2n0ax,1625040959.0,False
n2n0ax,[deleted],h3ky1z1,t3_n2n0ax,1625073273.0,False
n2n0ax,[levels.fyi](https://levels.fyi) outlines a lot of different compensation packages at different tech companies!,harppf6,t1_h3ky1z1,1630213114.0,False
n2n0ax,"I have been web programming for past 12 years now. I am pretty good and satisfied with what I have achieved till now. The issue is I have never planned anything so far in my life. I liked programming so I went into this field. ( I have a bachelor's in Computer Science ) .  But answering  what's next is getting difficult for me. I seem to be interested in everything - somethings related to computer science , somethings not. 
Currently my learning on the side  routine consists of  a massive to-do lists that I work on everyday - like reading on some topic - try to write a blog around the topic and move on the next interesting thing. I keep on adding new stuff when I see something interesting like on hacker news or here.
I feel that since there is no goal, I am not making any progress and sometimes I get demotivated. 
Please provide your  suggestions / advices .",h3ngx1r,t3_n2n0ax,1625117718.0,False
n2n0ax,"So my college is offering electives and they have given me a nuch of choices and I narrowed it down to 2 combinations those are
1. Data analytics, big data
2. Fundamentals of AR and VR, big data
This for my 5th semester, for 6th semester I have decided I will take natural language processing and deep learning. 
And the specialization I get after doing this is machine intelligence and data science
Please help me choose redditors",h3nl54o,t3_n2n0ax,1625120900.0,False
n2n0ax,"Hi all. Someone earlier I'd started a Post graduate course in Computer Application coming from a non-CS background. I approached my teacher explaining to him that I want to develop my skills in this field. He said that I first have to figure out what kind of real world problem I would want to solve and then he would help me advance in that. I don't understand how I can discover that. Where should I start looking for, and how?",h3nzt5p,t3_n2n0ax,1625133853.0,False
n2n0ax,"I was recently accepted to graduate school for an MS in Computer Science. I am taking the prerequisite courses first since I did not have a computer science undergrad. I’d like to spend the summer really digging in before Fall semester begins. Could you please offer me some advice on how I can spend my time preparing, pitfalls to avoid, etc.? I really want to crush every course, as having a degree (let alone a graduate degree) in computer science has been my dream and on my bucket list since I was 14 (now 34). Thanks all <3",h3ob1w6,t3_n2n0ax,1625142120.0,False
n2n0ax,"Hi So I am A rising senior in high school and my main career was to go to finance and go in the investing side, but I met a few people who were in computer science and was persuaded to do a bachelors in Computer Science. I don't know if it will work but what if I can do a computer science degree and enter the finance field with a CFA or masters in finance.",h3pdbx1,t3_n2n0ax,1625160165.0,False
n2n0ax,"Hello everyone, my question is simple. I am wondering whether or not should I keep doing whatever I feel like or should I stay with what I know best. I am a sucker for technologies and new stuff, so whenever I see something interesting I wanna do it. I specialize in Android Development and recently started working with Multiplatform development. Alongside that I've made servers for my apps by myself. Now getting more interested in startups and being alone I've found myself learning serverless approach and now I have to work with AWS. I don't mind all of this extra work and knowledge, I find it really fun, but is it advisable to do this as I am mostly looking for a career as Android developer? Anyone with similar experience who could shed some light on the situation?",h3sea7n,t3_n2n0ax,1625220862.0,False
n2n0ax,"Is there a way to combine computer science and either agriculture, geoscience, astronomy, or oceanography/ocean sciences?",h3vjhh0,t3_n2n0ax,1625276164.0,False
n2n0ax,"I’m a computational chemistry major (undergrad). I’m still deciding things career wise but I’m mostly between grad school or becoming a software engineer. Can I do this with just a computational chem degree, or would I need a masters in CS?

Edit: my degree plan includes about 18 hours of pure computer science courses (including 6 hours of software engineering courses), besides that the degree is about 30 additional hours of statistics/programming for science and chemistry, along with pure chemistry.",h3wjw6o,t3_n2n0ax,1625303570.0,False
n2n0ax,I'm thinking of getting a degree in Informatics from the FernUniversität in Hagen. Can anyone give me some insigth about the university? Will the fact that it's a remote university give me problems when looking for a job? How recognized is it's degree?,h41f6rw,t3_n2n0ax,1625417377.0,False
n2n0ax,"Hello everyone,  
After the summer, I'm a third-year student in Information & Communication Technology from the Netherlands and I have a question about my minor. I want to do my minor in Computer Science at a partner university in the US and I wanted to know what difference there is if I choose to do my minor in spring 2022 or fall 2022.  
The reason why I'm asking this is that I have to do an internship in my 3rd year and 4th year of college. I already got a Data Science internship for fall 2021 but I'm not sure if I would do my minor in spring 2022 or fall 2022 and what the advantages or disadvantages are. To clarify my career goals in the next years in college I will sum them up:  
Find a summer internship abroad (2022)  
Find the last internship also abroad (spring 2023)  
I would like to know if I would do my computer science minor in the spring of 2022 if that will give me any advantage like finding a summer internship in the US so that I have extra experience in comparison with the fall of 2022 where I have to search for a spring internship of 2023.  
Thanks in advance!",h44pelu,t3_n2n0ax,1625494560.0,False
n2n0ax,i hear alot that you dont need a CS degree to get a CS job how true is that? because im planning to get a CS degree does that mean its worthless?,h45nlig,t3_n2n0ax,1625512068.0,False
n2n0ax,Hi everyone! I would like to start studying computer science (i don't have any preparatio) I'm 19 just finished school (I'm from Italy) and next year I'm going to start the university in physics. I would like to start something new that is going to improve my skills and my preparation for a future. How would I start?,h48qc25,t3_n2n0ax,1625582756.0,False
n2n0ax,What's some good resources to start learning JavaScript?,h49keyc,t3_n2n0ax,1625596358.0,False
n2n0ax,"I’m a rising junior in high school and I just got into computer science like last year, and my goal is to get into a good CS program, maybe like Berkeley or UCLA. I’m taking a python boot camp on udemy and I took APCSP. What else should I do these coming 2 years to boost my chances as much as possible?",h4bscu7,t3_n2n0ax,1625638455.0,False
n2n0ax,.,h4ejqku,t3_n2n0ax,1625694681.0,False
n2n0ax,"Hi guys I've a question for you:

What is the most rewarding ($) career path in Computer Science? 

E.g. for: 

- Field: ML engineer, devops, security, sw developer.......

- industry (which and where?)? Research (is making pubblications profitable)?

Etc. Etc.

I'm wondering what do you think about because I could find a lot of different situations: e.g. a sort of admin/consultant without degree that get 110-120k in europe, while some phd in ML 55k only...",h4fwtro,t3_n2n0ax,1625720798.0,False
n2n0ax,Looking at going back to school for a bachelors and am very interested in Software Engineering after taking a front end web dev course. The most affordable college close to me has a CS course but not Software Engineering. Any thoughts on if this would be a good fit or if I should look at other schools? Thanks in advance,h4j343f,t3_n2n0ax,1625785705.0,False
n2n0ax,"I just started as a Business Analyst for an Application Development team at a large brokerage this week. I know NOTHING about computer programming (yet they assured me I don’t need to know, but I am lost in 99% of conversations and meetings). Am I fucked?",h4mhw91,t3_n2n0ax,1625858821.0,False
n2n0ax,I am heading to college but I suck at CS. I mean not really suck but I am below average. I wanna be the best. I wanna be the tony stark of the world. How and where do I start. I am focusing on everything from AI to ML to architecture. Any tips and tricks to help me,h4putxc,t3_n2n0ax,1625935697.0,False
n2n0ax,"So this year I’m going to start at a community college and most likely transfer to UIC in a year because I already have 25ish credits from AP and college classes. I feel like the new data science program with a specialization in computer science would interest me more than computer science bc I’m a big math person. The only part that’s making me reconsider this is because the cs program is older and more established. Realistically I would probably take the same courses with minor differences. I was wondering what you guys would recommend Bc I think sticking with cs and maybe getting a masters in data science if I chose to do so would be the safest option. Also the data science program is new and it hasn’t even been ranked. Although it is in the college of engineering, it is also mixed with other colleges and doesn’t have the same requirements which might give me room for a minor. Im open to getting a masters or phd. I’m still undecided which one I should do but I think I’d enjoy data science more.",h4qpm7w,t3_n2n0ax,1625951966.0,False
n2n0ax,What should i do i have choosen Computer Science in Grade 11 now the covid closed my college and my time is waste now and i havent learned anything,h4vkw0d,t3_n2n0ax,1626056446.0,False
n2n0ax,I am 19 and I want to run my own tech start up someday. I am a finance major right now and I know little about CS. What should I know before I change my major? Yes I am aware it is very hard and a lot of work.,h4xawhf,t3_n2n0ax,1626100893.0,False
n2n0ax,I just started data structures earlier last month. And wow. Hats off to you programmers. Always thought how hard could it be. Well it's as hard as you make it. And wowza it can be tough as nails.,h504c9u,t3_n2n0ax,1626150599.0,False
n2n0ax,"How important is a computer science degree if you already have a BS degree in a different discipline? I'm currently in the enrollment process for a computer science degree, but I do currently have a BS in Justice Administration with a concentration in Digital Forensics. Should I be spending my time and money on certs instead of formal education?",h51rebe,t3_n2n0ax,1626191545.0,False
n2n0ax,I know nothing about how to code or how computer science works but I have decided to dedicate my time to learning linux and python. Is there any youtube series or courses I can start with. Ive been using computers for years and built one in 2019 it currectly runs windows 10 and im going to do a dual boot with linux today. Thank you for any tips,h52pykv,t3_n2n0ax,1626206810.0,False
n2n0ax,I’m looking to become a full stack developer where can I find a group of people to study online for free?,h5325xy,t3_n2n0ax,1626212383.0,False
n2n0ax,"I have a large gap in schooling, I completed my associates in 2017 and I've had 2 kids since then. But looking into online universities to complete my bachelor's to get into this field. Does it *have* to be in CS? I was looking into WGU, they have a bachelor's in software development. Does anyone have any recommendations or experience with WGU?",h55iqmx,t3_n2n0ax,1626270037.0,False
n2n0ax,[deleted],h5curb3,t3_n2n0ax,1626406487.0,False
n2n0ax,"What is the best way for me to get in to this field? 

I’m 28, 4 years out of college and not able to use my graphic design degree. I’m not sure I ever really liked art. 

I maintain about $8,000 in the bank, with a car payment, and parents who let me live with them for free. Just not sure where to go..",h5f14fm,t3_n2n0ax,1626454901.0,False
n2n0ax,"I am going to college to study computer science,I haven’t decided on which college but my cousin is recommending one in America and my father is recommending one England and I wanna know  which one is better in all aspects of life from learning to working to settling down because I am considered a minority in both places but which one is better for me ? If you have some insight or some opinion please share it.",h5fwse9,t3_n2n0ax,1626468522.0,False
n2n0ax,I am about to be a junior in college. What are some good college jobs and internship places to look for to buff up my resume and get more field experience?,h5h7758,t3_n2n0ax,1626492063.0,False
n2n0ax,What would you recommend? The online OHSU Post-baccalaureate BS or an online CS certificate + online MS in CS? The OHSU program is significantly more expensive but I bet a MS in CS would cost similar.,h5i5jpi,t3_n2n0ax,1626519302.0,False
n2n0ax,"I'm a senior in majoring in computer science, I wasn't able to cop and internship this past summer and I feel like it will put be behind in terms of getting job at the end of the year. It's stressing me out a lot and I just don't know what to do to prepare... I'm nervous I won't even find a job",h5jnnk6,t3_n2n0ax,1626549179.0,False
n2n0ax,"So just want to get into programming to kill time as my profession can have a lot of down time. (Keep in mind I have absolutely ZERO experience when it comes to CS or programming or coding)

My question is: can an IDE negatively impact my device in any way? (I.e if I input a wrong string of code or close it incorrectly can it cause problems for my device?)",h5k1oqx,t3_n2n0ax,1626556284.0,False
n2n0ax,so I just graduated with economics and I’m not sure my degree will be that useful in getting a job. i want to learn computer science but i’m not sure if i want 4 more years of college. what is my best path for learning coding and computer science skills that can land me a job in the field?,h5lo3ci,t3_n2n0ax,1626588343.0,False
n2n0ax,"Hi, I'm deciding on a course for my college and chose computer science however there are so many branches and specializations in courses such as software, data analytics, etc. I was hoping to get some advice on which course is the best for future job opportunities",h5m28ik,t3_n2n0ax,1626599938.0,False
n2n0ax,"I’m going to be starting classes now in August as a computer science major and need a laptop, I was wondering if anyone had any recommendations for good laptops to use in computer science.",h5mysi1,t3_n2n0ax,1626620771.0,False
n2n0ax,This is my first summer as a computer Engineering student. What should i do during this time. I have no clue. I want to improve and learn more languages.,h5rwtld,t3_n2n0ax,1626715695.0,False
n2n0ax,Any OS text books by the end of which one can implement an OS similar to MINIX or L4?,h5uqa4v,t3_n2n0ax,1626769426.0,False
n2n0ax,"Who has disc, I need to talk vc",h5wxh2l,t3_n2n0ax,1626810889.0,False
n2n0ax,"I was initially (and currently) a Computer Engineering major, going into my Sophomore year of college. I had an internship over the summer with Lockheed Martin as a Data Engineer Intern, and the work was really not my cup of tea. I'd prefer to work with my hands or do design, and I'd really love to work with AI, Virtual Reality, or design. I don't know how much of the Computer science part of my degree I'd enjoy. At this point, I'm questioning if I should change my major to Electrical Engineering or Mechanical Engineering. Does anyone have any advice or resources to help me out?",h5xu9hr,t3_n2n0ax,1626825731.0,False
n2n0ax,Is the bachelor degree in computer science from university of the people worth it?,h5zettk,t3_n2n0ax,1626858430.0,False
n2n0ax,"I am a second-year Computer Science student and I am taking a semester course next semester that consists of independent research into any area in computer science I chose(provided it has been approved by the university). I've been interested in blockchain and cryptocurrencies for the past few years and also have a strong interest in business and the applications of technology in general on different businesses or as the basis for new business models (would like to focus on blockchain, AI and possibly machine learning).  
  
Unfortunately, I haven't had much exposure to many of the theoretical aspects of computer science as those are only covered in higher years. I need something that is on ""my level"" - I would obviously love to take this opportunity to learn new areas of CS but for the sake of the course, I shouldn't be too much of a learning curve - i.e. a project that I can tackle without taking any advanced courses, or a topic that won't take to much time to learn (I have to balance my research with 5 other courses over 4 months!). Do you perhaps have any ideas of research topics I could use?",h5zzjsy,t3_n2n0ax,1626873276.0,False
n2n0ax,Going to masters in computer science beginning this fall or next spring. I am a CPA and am curious about the fintech career for people with a computer science degree. What do people in Fintech do on the programming side?,h654uql,t3_n2n0ax,1626970887.0,False
n2n0ax,"Hey all, I've recently been considering returning to college in the near future to complete a CS degree. I left after COVID, but I recently rediscovered my love for programming and writing code. My experience with jobs/CS workers has always been in the infosec and cybersecurity worlds, but I have a huge love for low-level code and the nitty-gritty of developing hardware. I'm currently learning C as a hobby and plan on taking a stab at some kind of Assembly language soon, but I wanted to test the waters in terms of the future of such work. Would a CS degree be useful for something like that or should I consider engineering? Is there even a market for low-level programming anymore? My understanding is that a good portion of modern work/programming is derivative and there is little use for true low-level programming these days, though I have no people in the CS field I can talk to directly to confirm.",h6671po,t3_n2n0ax,1626987291.0,False
n2n0ax,Embedded programming is where low level programming is still useful. Ive seen tons of jobs near me offering embedded programming jobs where its all about optimization.,h6u75wq,t1_h6671po,1627485127.0,False
n2n0ax,What IDE should I use for coding Python on MacBook? I'm thinking of downloading Kite or Visual Studio.,h66msey,t3_n2n0ax,1626994498.0,False
n2n0ax,"I’m worried that the rate of people going into CS is going to saturate the field, making it hard to find a well paying job- the main thing being advertised as why it’s a good field to go into. I’m debating civil engineering or chasing the money and double majoring in CS/CE. I’m currently a senior in high school and worried that the job market will be saturated by the time I’d be able to graduate in 4-6 years. Thoughts?",h6apztf,t3_n2n0ax,1627076395.0,False
n2n0ax,"I am a fan of data analysis and I want to be a specialist in it. I want tips, how do I start learning data analysis, what should I learn first?",h6bni0o,t3_n2n0ax,1627093842.0,False
n2n0ax,"can someone tell me what a good computer is for computer science, i maybe want to major in it not sure yet but i want to know if i need a certain computer. i have an macbook air is that adequate?",h6g0wy6,t3_n2n0ax,1627190320.0,False
n2n0ax,Is Computer Science a good field to go in if you don't like math?,h6g3acw,t3_n2n0ax,1627192128.0,False
n2n0ax,"Hey Guys, Second Year comp sci student here. Basically in two months time I will be doing interviews for a work placement next year. I heard from one of my lecturers that it is good to have some side projects to show off in those interviews. For the sake of this, we'll say I don't have any notable ones. 

I'm contemplating between starting development of a web app or learning python and doing some smaller projects with that. Which do you guys think I should do.

Note: I'm going to do both anyways, I just want to know which one I should invest time into now. Thanks for your time!!",h6gunb5,t3_n2n0ax,1627216230.0,False
n2n0ax,Make a personal website. Couldnt get an interview for 3 months. Then i made my own site and hosted it on AWS and got 3 interviews in one day.,h6u6v0h,t1_h6gunb5,1627484996.0,False
n2n0ax,"So I am starting college in like 3 weeks (computer engineering/science), and I need a new laptop. From my researc I have heard that macs are really good for this sort of work, and the rumoured 14"" seems really appealing to me. Howerever it doesn't come out until september or october, so a month or 2 after I have started at college. I do have a pc from high school that I could use, I just don't know if it is good enough. Should I wait for the new macbooks and just use my old laptop for a month or two, or should I just buy an older mac or potentially a windows PC?",h6h7boy,t3_n2n0ax,1627223878.0,False
n2n0ax,"I’m not good at giving recommendations on which pc/mac/laptop is best for computer science/engineering but as a graduate of computer science I will say that the beginning courses of computer science are fairly light-workload based, so an old pc will be more than enough to hold you over for a couple of months while the new MacBooks are released, just in case you’re dead set on the MacBook you like.",h6hnlye,t1_h6h7boy,1627232021.0,False
n2n0ax,"Okay so I don’t know where else to ask this so here goes:
**Is it supposed to be this incredibly difficult to find a job in the software engineering/development field without any job experience or connection/contacts and only a B.S. in Computer Science??**

I’m asking because I’ve been fresh out of university, searching for a job for almost a year and a half now with no success and only a literal handful of interviews from over a 100+ job applications. Even when I apply for Junior entry level positions for software engineer and software developer, I’m disqualified from being considered for the job and I don’t get so much as a glance or response to my job applications.

I’m a bit shy, so making new connections with people already in the industry is really hard for me (plus I don’t even know where to look to meet new people or how to even approach them), and I didn’t get a chance to do any internships while at university because 
1) I was dumb enough to not even ask where to look for one while I was attending Uni (aside from the couple of internships offered at the university, which I applied for but didn’t get hired) *and* 
2) most of the companies that offered internships were in a city a 2 1/2 hour drive away from where I live (since I didn’t have any money to move there and only limited gas to get to and from Uni every day, commuting that far for an internship every day would have been difficult).
And my university focused mainly on backend development and languages like Java and C++, with maybe one class on web development and one on mobile development, so all the web and app development jobs that are hot or desired are pretty much out of my skillset (been focusing on interview prep and strengthening my skills in those languages while job searching instead of trying to spend time to learn new languages/technologies). 

Oh! And I’ve been mainly applying for jobs through LinkedIn, if that helps any in gauging my situation. 

I’ve asked for advice from friends of friends of my parents, who know someone somewhere who had the same problem, and they’ve all said things like: keep applying so that companies see you’re very interested in their job offers, work and post projects so that potential employers see you’re actively learning and doing something field-related, and someone even said that I should take my address off of my resume and/or ask a friend to use their address on my resume/job application if they live closer to the job I’m applying for just because some companies don’t want to pay for moving expenses and things like that. All advice which I’ve followed, to no success so far.

This is pretty much my last ditch effort to get more advice to help me land a job or at the very least get noticed.
Thank you so much if you’ve read all of that, I really appreciate any and all help and advice anyone has for me!!",h6hm1yj,t3_n2n0ax,1627231276.0,False
n2n0ax,[removed],h6n2s8w,t3_n2n0ax,1627339024.0,False
n2n0ax,"I recently graduated with a Psych degree due to do wanting to continue in the medical field, but those goals are not feasible anymore. I have always been interested in engineering and technology but I don't feel confident enough to be able to pursue a CS degree because the first time I dropped out was from not doing well in a biomedical engineering program. 

 **What are some pros and cons of the CS field?** 

I would love to get into software development in the game development world, but I feel like that is what everyone thinks or says when they just start out in this major. I have the fear of starting at a university again and not doing well and I guess I am just trying to figure out if it is worth it.  

&#x200B;

thank you to anyone that responds",h6pxdzl,t3_n2n0ax,1627401516.0,False
n2n0ax,I have six months off before I start a computer science degree. I normally read articles online but is there something I can do to prepare myself for university?,h6rxw73,t3_n2n0ax,1627434201.0,False
n2n0ax,"I start my college life in a few weeks and am going into the computer science field. I'm quite nervous as it seems most people in this field have either received or taken some kind of programming class before. I haven't and was curious if there was anything I should do before attempting this major, Thank you!",h6u37oq,t3_n2n0ax,1627483415.0,False
n2n0ax,Recently graduated with a CS degree here. Start learning about OOP(object oriented programming) its usually the first class you’ll take and really the only place others might have an advantage over you. Its a difficult first topic but also the foundation for most other learning you’ll do. Learn some basic C++ and try to familiarize yourself with the linux system if you haven’t yet.,h6u6ihj,t1_h6u37oq,1627484844.0,False
n2n0ax,"Awesome, thank you!",h6u9mxs,t1_h6u6ihj,1627486172.0,False
n2n0ax,"I just graduated with a computer science degree, and I'm headed for a career in the Air Force unrelated to Computer Science. This is great! However, the way the process works is they don't pay you until you start training, and there can be up to a year long wait before there are slots available in training. 

Are there any CS related jobs well suited to this situation out there? I don't really have a lot of ideas.",h6uhcop,t3_n2n0ax,1627489355.0,False
n2n0ax,"I am a student interested in exploring computer vision and machine learning in robotics. I want to get a better understanding of the practical applications of real-time image segmentation.
What is the data acquired from real-time segmentation used for in robotics or computer vision? How is the data processed and what algorithms are used to process the data? What are the applications of semantic/instance/object segmentation in robotics? Explanations, interesting articles, research, and other resources are appreciated :).
I have also been exploring Tensorflow & TFLite recently. Are there any TFLite compatible instance/object segmentation models available (DeepLab seems to only be for semantic segmentation)? Thank you in advance.",h6vkl3f,t3_n2n0ax,1627505690.0,False
n2n0ax,"Im a rising senior in college. I've had 3 internships with 4 years of industry experience in computer science. The job I've been offered is in the Chicago suburbs, and the total salary is 68,000 dollars. Is this a competitive offer? I dont have very long to accept it, so not enough time to get offers from other jobs before the deadline. I've heard of some college grads making 100k as developers, but I dont know if it's true or not. What do you guys think? Is this a good offer or no?",h73a8nh,t3_n2n0ax,1627657910.0,False
n2n0ax,"Any advice is appreciated. I am eligible for retirement in 10 years with my current career and I am 40 years old. I am currently in a Bachelor's program for cyber security to plan for a second career or a job to do on the side of my current main career. I still have time to switch majors and I have been back and forth between sticking with cyber security or Computer Science. 

What are the pros and cons of each degree in the career field? 

Any input is appreciated. Thank you.",h747vi6,t3_n2n0ax,1627672007.0,False
n2n0ax,"I'm upcoming college this year and I want to pursue Computer Science. 

I just want to know what should I study in advance and what topic should I start to study? 

I have no idea what should I start, So it will really helpful if you reply. Thank you in advance.",h74um27,t3_n2n0ax,1627681859.0,False
n2n0ax,I've been learning some stuff by myself and I'm at the point where I'm trying to decide whether I should learn a framework or not. For CSS I was deciding between Bootstrap and Tailwind but after watching a couple videos I noticed that I can do most of what these frameworks do with the CSS I know and CSS-grid and CSS-Flex for responsive design. Would you recommend that  I still learn a CSS framework or should I focus on a Js framework? Thanks for the advice!,h798xo4,t3_n2n0ax,1627774798.0,False
n2n0ax,Hi! I’m still a high school student but i’m thinking of what I wanna be in the future. I’m really interested in doing game arts and design and people tell me “Go com sci” or “Choose com sci” but I honestly have no idea on what com sci is and how it works. Does com sci really teach me how to do game arts??,h7a59zf,t3_n2n0ax,1627793709.0,False
n2n0ax,"Which is better, should i choose Computer Science or Biomedical Engineering?? I got accepted by college that has 2 majors: CS and Biomed. I should decide the decision that i'll make for my bachelor degree in the next year(because now i am in pre-college phase).

Reason why i choose CS: I want to develop game, learn math-related computation, make simulation, cryptography, and wanna to be a data scientist.

And these are the reason why i choose Biomed: interested in biomechanics(artificial arms, angiplasty, etc.), bioelectrical, neuroscience, MRI, and other med tools.

Both are interesting and challenging for me. I want your advice, guys. Sorry for my bad english....",h7bsy22,t3_n2n0ax,1627834776.0,False
n2n0ax,I'm a junior in high school but I can't take any actual coding or computer science classes here and it's what I want to learn and study in college. Where should I start to have some sort of basic knowledge of it for after high school?,h7gd30e,t3_n2n0ax,1627926875.0,False
n2n0ax,How many of you guys actually work from home or remotely? Is it possible to achieve a job with a company like that right out of college?,h7hbuke,t3_n2n0ax,1627942182.0,False
n2n0ax,I'll soon be turning 18 and I plan to get a Comp Sci degree. What are some difficulties I might encounter due to my lack of coding knowledge? What are some struggles I may face in general? What language would be the most advantageous for me to learn first? I've also heard that Codecademy is a good place to learn... is this true? Is there a better place I can learn for free? I want to learn languages that will be useful jobs I might encounter in the future. Thanks in advance! :),h7ifj4i,t3_n2n0ax,1627961396.0,False
n2n0ax,"Just started software engineering, python can be repetitive at times. What helps you code more effectively without it seeming like chore?",h7nds4n,t3_n2n0ax,1628058558.0,False
n2n0ax,"Greetings everyone , 

first of all tank you so much for making such a great platform beginners, I'm at the first steps of the way ( aka noooob ) although I have some intermediate C & C++ knowledge and I have good taste in Linux too , I would really like to know how can I really start to understand how computers exactly work , do I need to know assembly first , if yes do i nees to get an old PC ( affordable , I can't use rose berry pi , I just don't like it ) , if yes what cpu architecture should include , x64 / x86.

THANK YOU FOR YOUR TIME.

sincerely.",h7omaz8,t3_n2n0ax,1628088694.0,False
n2n0ax,"oh I'm sorry , after checking out the site I understood that I first have to master mandatory mathematics to do so , btw do I have to computer science to deeply understand the computer",h7onesb,t1_h7omaz8,1628089166.0,False
n2n0ax,"I made a few wrong study decisions and I'm most likely failing my entry exam to the university next week as I didn't realise English Literature would be part of the test. This means that this would by my third year without progress in the traditional education system.  


So I have decided that I will start to self study computer science if I fail this test. To lower my stress about this test a bit would anyone be able to give advice or share experience on self studying this field outside of traditional education?  


I have some knowledge, I know a bit about web development (react, nodeJS and all that) and already know very basic comp sci concepts. My mathematics needs some polishing but I still know the concepts from high school.",h7pccnu,t3_n2n0ax,1628099391.0,False
n2n0ax,What's the best language to learn for AI?,h7rwzo5,t3_n2n0ax,1628144114.0,False
n2n0ax,"Here I am sharing a wonderful platform for students in Australia , US and UK seeking for programming assignment help This is Programmingshark.com , world's no1 programming help company since 2014. They cover almost all wide range of programming subjects, here you go:  
  
C Programming Assignment Help  
at mail info@proassignmenthelp.com  
  
whtasapp +918299862833",h7sdwwm,t3_n2n0ax,1628158189.0,False
n2n0ax,"Hello, 29 year old guy here who works in healthcare. Occupational therapy specifically. I really dislike healthcare and I’m looking for a change. I’ve seen a lot of people talking about computer sciences and software development on Reddit and they claim to be making 100k-200k per year working from home.
I don’t necessarily have a strong interest in computer sciences or coding but I do have a strong desire to increase my earnings. I pull in roughly 80k pretax with my current profession and I want to earn more.

I guess my question is to get into computer sciences or software development would I be better off going back to school and getting a bachelors or would going through a coding boot camp be sufficient? Realistically can I expect to earn more than my current career (I live in a rural area that is LCOL and can’t relocate). Side note I suck at math but could probably learn.",h7spuzn,t3_n2n0ax,1628166403.0,False
n2n0ax,Is computer science a good career path? I'm in high school and I should rlly know what I want to do for a living,h7vhfht,t3_n2n0ax,1628208252.0,False
n2n0ax,Not sure how into high school you are but you should definitely see if your school offers any intro classes so you can see if it interests you also liking math is kind of implied . If it does it’s definitely a great career path for a lot of people. Me and a few of my friends graduated with CS degrees and got great jobs right after graduation.  Also you don’t have to be great at it right away I got like 80s in my highschool intro classes,h7zwl5f,t1_h7vhfht,1628293391.0,False
n2n0ax,"After studying construction management , what master can i study to change career?

Hello

I have finished my construction management degree in the uk . During my studies , I realized I really dont like it but I managed to get a first . After that I realized that Im more into IT-related field . For example , programming , revit , python . Please suggest me some master or anything to help me change career ,

&#x200B;

Thank you",h7ypfl3,t3_n2n0ax,1628273783.0,False
n2n0ax,I am a physical therapist working on an MBA to transition into administration. Some of the coursework I took was about business analytics with a heavy emphasis on R. I fucking loved it. What would you recommend as a source for further learning in data analytics or ways to get better at this kind of programming? Also what job titles would be appropriate for me to seek out as someone with no work experience with anything computer related?,h8292i9,t3_n2n0ax,1628348640.0,False
n2n0ax,How can i choose my CS field to start in?,h83lit1,t3_n2n0ax,1628371696.0,False
n2n0ax,"I am a BSc Statistics Student currently in third year rn, studying in india. 
I plan for Masters in CS in USA. 
what would be the appropriate prerequisites i do so that i would fit in well with the other CS students?",h89kji7,t3_n2n0ax,1628498617.0,False
n2n0ax,Any study tips ? What study schedule is good for a Uni student online and/or on campus? How to program and think faster for exams ? Thank you!,h89yksg,t3_n2n0ax,1628510079.0,False
n2n0ax,"Hi everyone,

Looking to enroll in to ASU’s online program with my Starbucks benefits. They have a handful of computer science degrees and really trying to figure out what the best option might be.

At first I was leaning toward software engineer, but then I’ve heard it can be extremely hard to get your foot in the door with a job. They also have computer science, I believe they also have a degree with databases, just looking for any suggestions.",h8as4x2,t3_n2n0ax,1628524825.0,False
n2n0ax,"Currently employed and looking to transition to a position with more professional development opportunities. I'm trying to figure out how to put the work I've done at the company I'm at on my resume with the best possible spin.

My job is to answer questions and coach clients on how best to use our company's product. Part of my responsibilities includes creating solutions (software and otherwise) to improve the work flow and reduce bottlenecks for me and my co-workers. To this end, I drafted a software proposal for one area of our workflow that could be greatly improved by a software solution (think manual sorting). My supervisor loved the solution and approved the proposal. Optimistic, I started development in my off time and created a functioning prototype in Python, learning a few new technologies along the way. After the prototype was completed, the CEO rejected the proposal before I ever got to demo the prototype, let alone get the go-ahead to develop it. So now I've got a semi-unsanctioned and non-trivial project on my hand that I'm not sure how to put on my resume.

The question then is this: how do I put this project on my resume in the least unflattering way possible, both to me and the company?",h8glpeu,t3_n2n0ax,1628631703.0,False
n2n0ax,"I just got into terminals and got Ubuntu on my PC but I hate how it looks, which terminals do you think are the best considering appearance and functionality? Honestly I'd like something that has VS Code like color, thanks for reading :)",h8hezjp,t3_n2n0ax,1628645716.0,False
n2n0ax,"Can i set up a wired connection from my pc to my playstation 5? They are both next to eachother and I have a long wire from my router going into the room but don't want to buy another long ethernet cable, can I connect a short etherbet cable from my computer to my ps5 so I can use them both at once?

Thanks in advance",h8hqc87,t3_n2n0ax,1628651503.0,False
n2n0ax,Does anyone have any suggestions of random crazy projects at the intersection of CS and Cognitive Science? THankss,h8n74xx,t3_n2n0ax,1628769171.0,False
n2n0ax,I’m a full time software engineer working since I graduated 4 years ago. I think I want to go back to school for a masters or something. Does a masters in CS have an impact career path? Is it necessary? Would an mba be a better option to be able to transition to a managerial role at some point?,h8o37cd,t3_n2n0ax,1628784181.0,False
n2n0ax,"Greetings!

I just enrolled in college and was enrolled in an ""integration elective"" may someone please explain what that is and how it's useful for CS?",h8os7ap,t3_n2n0ax,1628794469.0,False
n2n0ax,"Looking for a github (or gitlab) repository.

It was a literate programming book/research paper that dealt with intel assembly and vulnerabilities, it was written either in noweb or cweb. Unfortunately I can't remember much more than than. It was long and extremely well formatted.

Does anyone know what I'm talking about?

Edit: https://gitlab.com/osdevelopment-info/meltdown",h8s8fsw,t3_n2n0ax,1628863043.0,False
n2n0ax,"I am looking to get a job as soon as possible. If possible as a freelancer or remote with a salary in USD. What should I study? I have some basics knowledge in programming and cs. I am looking for a path to rush a work, any recommendation with books/courses from udemy or cheap are welcome. 

I would appreciate if you dont make comments about the 'perfect job for you' or what I like. My economic situation and the situation of my country is a complete mess. Im young i will have time to do sth different. Thanks 😊",h8saq2e,t3_n2n0ax,1628864256.0,False
n2n0ax,"Hi all,  sorry for the long post that follows:

I'm a journalist with about 20 years of experience looking to exit this industry and enter IT. I studied both a BA and masters in journalism. The MA had a tiny component for us to learn about online news and designing web pages. I turned out to be surprisingly good at coding, even if it was only a little bit of the course.  
  
I'm now looking to get into IT full time. However, there are some factors to consider and obstacles in my way. I've started following a YouTube Python tutorial and finding it really fun and interesting, I can see coding as something that I would like doing. I asked around and was suggested getting into applied data analytics. I could do that, but here is the problem- I'm a mum of 3, one child disabled and the youngest 3 months old. So my time is very limited which makes pursuing an actual course or degree impossible. I could self study and do shorter intro courses, like a 10 week course I've seen offered by City University, London.  
  
My biggest motivation apart from wanting a change of scene is money. We have a lot of expenses stacking up for our future due to our situation with our eldest, and I want to give my kids a good life. Right now they have a stressed, disillusioned mother who makes awful money in an industry where you can forget about a living income if you can't drop everything to follow a lead.  
  
I also have a background working in an investment bank for a few years and in business journalism lately and in both roles I noticed that while things went up and down, IT is a sector that has shown good growth so it has a future.  
  
My husband played devil's advocate while I was bouncing the IT switch to him - say I self train or do a short, manageable course. How do I then compete with all the fresh young things coming out of uni with 3 or 4 year degrees as an almost 40 yr old mother of 3? Is my plan realistic?  
  
Currently I'm following about an hour a day of self follow Python coding late at night (11, 12) while the kids are asleep. I'm optimistic but also worried my effort might be stupid considering the competition.  
  
If it helps direct your advice, I live in Europe (Greece) and plan on targeting remote work in other parts of Europe for better pay. Apart from applied data analytics, what else could I knit together with my background and a bit of IT training on top? TIA",h8wjfhx,t3_n2n0ax,1628944200.0,False
n2n0ax,"How hard is it just to get INTO computer science major in college(especially for CA residents). I saw people with perfect status and some even start coding when they are ten-yr old, yet rejected for CS majors by some colleges like UW and UCI.",h8x2nsz,t3_n2n0ax,1628954026.0,False
n2n0ax,"CS Major here, got an ipad pro 11"" recently, wondering if there are any apps that could help me greatly in this endeavor. I already have  main laptop for coding",h8zjbnj,t3_n2n0ax,1628995526.0,False
n2n0ax,"There are a lot of unknown things to me at this moment. Interesting apps  that I simply don't find out about,interesting features on windows 10 and its vast amount of settings that I do not know about. Where can I find such information? How do you guys get to know fresh information about computers everytime? I'd like to see a detailed list of resources or links about interesting things I could use my computer for,so I could have a more productive time using it.",h907vye,t3_n2n0ax,1629011447.0,False
n2n0ax,"Hello, so i’ve just completed my A levels and am considering getting into CS/IT. I’ve never seriously studied anything computer-related before. My O levels were all science subjects and my A levels were Maths, Biology and Law. So i’ve never really had any idea on computer stuff. However, throughout this year i’ve been watching tutorials on html, css on youtube and trying to understand the basics of programming just to see if i like it. i haven’t gotten too deep into it but so far i’m actually enjoying it. So my question is to all programmers/ software developers:
1. is maths used in a big part of programming? 
2. will the fact that i don’t really know much about programming affect my studies at uni? 
3. what’s an average day working as a programmer/software developer like for you? 
4. is a career in programming very time consuming or do u have holidays and time to spend on hobbies in your jobs? 
5. how hard was it to get a job? 
6. Are typically all jobs in an office setting or can you work from home? 
7. what’s the hardest thing working as a programmer? 
8. what kind of skills/qualities do i really need to be a good programmer?
9. How do i know from now if programming is for me? 

Thanks in advance!",h90b8su,t3_n2n0ax,1629014166.0,False
n2n0ax,"I chose a CS major for uni and will be starting in a few days, i’m not well aware with things related to CS but i wanted to study it because it seemed interesting, can anyone tell me what i should do before classes to be prepared and what things i should learn beforehand? thank u!",h9294yw,t3_n2n0ax,1629053292.0,False
n2n0ax,I just turned 24 last month and work in irrigation making $18.50 an hour. Was looking into this career mainly for more money and less physical stress on my body. What would be the best way to go about education? A bootcamp like flatiron or a online degree? Any advice is appreciated,h96fdgn,t3_n2n0ax,1629133178.0,False
n2n0ax,"So [levels.fyi](https://levels.fyi) states the salaries for computer engineers and data scientist. The money does not matter that much to me but I would like to know how much I could potentially make in the future. I am however thinking about getting my degree in computer science, but there are no salaries listed for computer scientists there. Is it just a terminology thing or are they completely different things?",h98t4bo,t3_n2n0ax,1629169787.0,False
n2n0ax,Computer scientist is a professor,h9at55p,t1_h98t4bo,1629213804.0,False
n2n0ax,Are there any good online courses to get started in computer science that are free online?,h98zt3f,t3_n2n0ax,1629173349.0,False
n2n0ax,"which one would be better? a career-technical program that would get you employed after you get your associate's degree, or a major that you would be able to transfer to a university for bachelor's?

I'm tied up taking Computer Science (bachelor's) or Computer Programming at a community college. Pros and cons? please help a lost girl!",h9b84d9,t3_n2n0ax,1629220049.0,False
n2n0ax,What must a programming language be capable of doing to be classified as a language? What is the criteria?,h9c1v3t,t3_n2n0ax,1629234201.0,False
n2n0ax,Best free coding bootcamp for beginners? I work in Finance currently but I want to move into a more technical / quantitative role,h9evv1m,t3_n2n0ax,1629297287.0,False
n2n0ax,"Is a degree/masters in CS worth it?
I’m currently in vetmed but want to change careers to CS, is continuing school for CS worth it or can I learn on my own and start a career from there.",h9ff6ex,t3_n2n0ax,1629305954.0,False
n2n0ax,"My local community college offers an AAS degree in Computer Technology, Programming/ Database. Between this and a coding boot camp, with which would I have an easier time finding a job? This is a big career change for me, so I am a worried about the initial job search that will follow whatever time I put into learning. I currently have little to no experience coding. I am thinking about the AAS degree because I may want to finish a computer science BAS in the future and this would be transferrable unlike the coding boot camp. I understand that a BAS in Computer Science is preferred, but would that also apply to an AAS degree?",h9fmkx1,t3_n2n0ax,1629309223.0,False
n2n0ax,"Hi i'm 15 years old and i'm looking into going further into computer science by becoming a software engineer. I have pretty novice knowledge of python and i understand pseudo-code. Right know i'm trying to learn python, html, java-script and the c's, however i feel like i'm doing too many things at once and i need advice from a software engineer on what i should do next or aim towards. 

&#x200B;

It would also be great if i could find someone who can mentor or tutor me.  :)",h9fow39,t3_n2n0ax,1629310248.0,False
n2n0ax,I have no formal experience with computer science only some self taught coding and an interest in it. I am afraid of not being able to do this.,h9gdtb7,t3_n2n0ax,1629321438.0,False
n2n0ax,"Hello,   
I have recently started learning AP Calculus AB at my highschool, which has an AP test that will allow me to earn credit for Calc 1 in college. However, I was wondering if I should switch to Calc BC as it will give credit for Calc 2 in college as well. Is calc 2 required for me to get a degree in cs?",h9gsrio,t3_n2n0ax,1629328366.0,False
n2n0ax,"There seems to be so many different ways of getting into computer science and coding that often times it’s confusing for me to know which way to go. I’m not sure if going to college, boot camp or receiving a multitude of certificates is the best way to start a career in software engineering. Is there even a possibility that you can obtain a software dev job without any merits and just solely on experience? I’m 29 and I really want to start a career in software engineering but I don’t necessarily know the best way to start. Is there anyone with some kind advice? I would very much appreciate it :)",h9kl0tn,t3_n2n0ax,1629401978.0,False
n2n0ax,"Is it worth it to move to a new area to get a graduate degree in computer science from a more prestigious university? My other option is to study at a local university with low living expenses, and either way my tuition is free. I guess the main thing I'm asking is how much does the university I studied at matter?",h9krozk,t3_n2n0ax,1629404662.0,False
n2n0ax,"i went to a state school and now work at a company with tons of other state school people but also tons of people from elite colleges. if you work hard and get lucky, you will get to where you want to be. i would recommend saving the expense and go for your local university! education can be expensive. if you can save that money and get an equivalent education, then do it!",hamn4t8,t1_h9krozk,1630112453.0,False
n2n0ax,"I majored in mechanical engineering and work in the aerospace sector, but since graduating have learned that I enjoy coding much more. I can get in the zone for hours on a specific project, which to me is a telltale sign that this may be the career for me. Unfortunately, all I really know right now is that I enjoy the practice of coding, and not a specific field. My experience so far has been Python. CodeAcademy to start, then a University of Washington foundations course, then going through How To Automate the Boring Stuff and How to Code with Fantasy Football (data science). What would be a good way to get quick exposure in different areas of computer science/data science so I can start choosing a path? Classes/certificates? I could go for a Master's but I'd rather have some idea of what I want to do with it before going back to school.",h9lfjz4,t3_n2n0ax,1629414838.0,False
n2n0ax,[deleted],h9mz4sx,t3_n2n0ax,1629446836.0,False
n2n0ax,"Hello. In order to promote inclusivity and reduce gender bias, please consider using gender-neutral language in the future.

Instead of **freshman**, use **first year**.

Thank you very much.

^(I am a bot. Downvote to remove this comment. For more information on gender-neutral language, please do a web search for *""Nonsexist Writing.""*)",h9mz5dp,t1_h9mz4sx,1629446849.0,False
n2n0ax,"I'm in high school currently, and i want to continue my education in business school, but i'm also interested in computer science especially about coding. If i want to get a job in computer science industry, do i need a cs degree? And if it can, is it difficult? Thank you",h9mzzii,t3_n2n0ax,1629447565.0,False
n2n0ax,"Hi, I’m a CS major college student( 1st year). Now I have 0 knowledge in computer science but I wish to land an internship ASAP, what are some nice lectures online that I can start with and teach me how to build projects?  Thank you",h9nwyl0,t3_n2n0ax,1629468698.0,False
n2n0ax,"Hello. In order to promote inclusivity and reduce gender bias, please consider using gender-neutral language in the future.

Instead of **freshman**, use **first year**.

Thank you very much.

^(I am a bot. Downvote to remove this comment. For more information on gender-neutral language, please do a web search for *""Nonsexist Writing.""*)",h9nwzvo,t1_h9nwyl0,1629468713.0,False
n2n0ax,Does computer science contain difficult mathematics? I love programming but I'm afraid I'm gonna hit a mathematics wall in uni and my grades are gonna suffer.,h9osqc3,t3_n2n0ax,1629481538.0,False
n2n0ax,"for the most part, no! if you want to go into cryptography for example, then you'll encounter some math. but most universities only require up to calculus 1/2 and discrete math (which is more logic-oriented than pure math). go for it!!",hammyl1,t1_h9osqc3,1630112365.0,False
n2n0ax,I'm so excited to finally have work and to make something of myself! :),hapts8u,t1_hammyl1,1630178563.0,False
n2n0ax,Is it worth doing a postgraduate degree in cs after doing a bachelor degree? Or should I just work?,h9sv2n5,t3_n2n0ax,1629561623.0,False
n2n0ax,"in my opinion, no. work experience will always trump further education when it comes to tech. the faster you gain experience as a software engineer, the better of a software engineer you'll be. by staying in school, you won't be working in industry and learning software engineering on the job.",hammv0w,t1_h9sv2n5,1630112315.0,False
n2n0ax,On a gap year right now after high school. I have a year to study and do basically anything. What should I spend my time on?,h9xns14,t3_n2n0ax,1629654962.0,False
n2n0ax,"How do you get started? As a gen Z kid I know the basics of computer things, but I never really did much of it at school (focused on other sciences) and took a degree, then a masters to avoid figuring out what to do with my life. I have always liked computers but I don't have any formal education beyond what was needed for my degrees, i've begun learning the basics of coding in python but the science is so vast I feel like i'm treading water in an ocean of information. How does one get established in ""computers"" what resources are good? How do you find the niche you enjoy? Sorry this is rambling, TIA!",h9y6c3v,t3_n2n0ax,1629663074.0,False
n2n0ax,I am planning on starting a career in computer science. Are online comp science schools like lambda worth my time or should I try to get into an actual physical college?,ha146u4,t3_n2n0ax,1629725757.0,False
n2n0ax,I really want to learn more about coding the basics anyway any helpful videos on YouTube?,ha1pwuj,t3_n2n0ax,1629735222.0,False
n2n0ax,"Yes good God, the Internet and YouTube have endless programming content. Find someone you like, and just watch their tutorials. If you don't know something or are unsure, Google, ask someone, post on Reddit. Theres so much information out there.",hbgub1s,t1_ha1pwuj,1630688430.0,False
n2n0ax,"What undergraduate classes are expected before applying to a theoretical CS PhD program?? CS and math specific, please 

Interested in things like graph theory and algorithms",ha43ns8,t3_n2n0ax,1629771892.0,False
n2n0ax,"Hey everyone, I really need some help….

I am currently a senior in college about to get my CS degree, and honestly I’m so lost lol. I currently work full-time as a sysadmin closing in on a full year at my current job. I am fully online as a CS major and even though I’m in school, most of my learning has been self taught. 

Currently I am mainly focused on front-end development with HTML, CSS, JS, and PHP. Although I do not really enjoy it and I am thinking of dipping my hands into the C# and Java aspects of development to see if I find more interest in that.

My issue I am facing is with working full-time and going to school full-time, I am not really learning whatq I want. I don’t have time for projects or to really teach myself/master a language.

Although the money is good, bringing in around $50,000/year, I am just not happy. I do not have many bills and currently have the thought of maybe finding a part-time early morning job, so I can focus on me and really work on projects and mastering my skills.

I recently started my own web development company and would like to spend more time on that as well. Any advice from you guys would be super helpful! Thanks in advance!",ha5wvx8,t3_n2n0ax,1629814542.0,False
n2n0ax,"hey there! that's great that you're a senior in college but already working a full-time job. i'm not sure if you want to move into software development in general, but you'll find that moving into software development roles is a very formulaic process, especially at the new grad level (SWE I). you've probably heard of ""leetcoding,"" which are data structure and algorithmic-based programming questions. if you spend some practicing, you could very easily find a more suitable role in development as opposed to your current role.

there a lots of videos on leetcode and problems on leetcode, especially breaking down solutions that i find helpful. i also have a basic video on [getting started with leetcode](https://www.youtube.com/watch?v=mAXOv_fZXt4).

i would encourage you to keep pursuing your web development company if you enjoy it, although it doesn't seem like it is given your comment amount lack of satisfaction with front-end development.",hammgke,t1_ha5wvx8,1630112111.0,False
n2n0ax,"I have recently found an interest in automation. Any info on where I could find good courses on Python? I’d love to become a backend dev using Python, SQL, and PHP.",handjba,t1_hammgke,1630127169.0,False
n2n0ax,Hi Im 20 years old and Im currently on my way to apply to computer science. I graduated high school late and was there for 6 years. I was never a good student in high school and I didnt take work serious. But now that I am out of there and its been a year since I grad. Im learning basic algebra at the moment cause I had horrible teachers in my math classes. I always hated math anyway and never was good at it but now Im giving myself a chance to learn. Just wanna know from any cs student do I need trig for calculus 1 and 2 ? Also do you think someone that doesnt like math can get good at it?,ha8rwhb,t3_n2n0ax,1629858122.0,False
n2n0ax,"hi! i'm excited to hear that you're taking charge of your education! i would say that trig is a touch relevant in calc 1 and 2 just for very certain types of problems you might encounter (you need to know how to evaluate trigonometric functions, for example). 

definitely - math is one of those subjects like many other technical subjects where the more you practice, the better you become. i also had low confidence in my math skills but i took the time to do extra practice problems all the time before exams to build that confidence up.",hamlvkh,t1_ha8rwhb,1630111818.0,False
n2n0ax,"thanks Laura. i saw this a while ago but never got to reply. Ever since Ive posted this , I been studying and learning algebra and also being taught by my friend who is a Math major. Im preparing for the SAT exam I need to take to get into college and Im aiming for a high score. Would you also saying there is a lot of coding? Im also learning python which is basic. thanks in advance",hejzx8n,t1_hamlvkh,1632800824.0,False
n2n0ax,"I don't know if my question has been already answered but hear me out. I am a Mechanical Engineer and aged 27. I have to say, I have always wanted to study Computer Science or programming instead but things did not go accordingly. Now I am working but I don't really like my work right now and I want to start studying for CS while I am working till I develop enough knowledge, experience to be able to find a job on this field. So my question is, how should I start? I have a low knowledge of programming but I am planning to study a lot, even during my work when I have free time. Is there maybe part time programs to take ? Could I take master degrees on CS without having a bachelor for it? What courses should I take or stuff to begin with. What should I know before getting into such field.",ha9vsd4,t3_n2n0ax,1629885635.0,False
n2n0ax,"Understand that programming is fundamentally about solving problems and if you have a job, it becomes a very large team effort to write and solve complex problems. You'd probably have to take and get a Bachelor's before getting accepted for a Masters program for most colleges.",hbgu0ih,t1_ha9vsd4,1630688309.0,False
n2n0ax,"Anybody here studied at Southern New Hampshire University's online Master's program for Software Development? Or the related specializations. I'm thinking about applying to that university since it fits my budget/schedule. And was wondering if anyone could comment about the difficulty, originality in their assignments, professors, or just anything really.",habyi1s,t3_n2n0ax,1629920278.0,False
n2n0ax,"I'm not in a position to go back to school right now. Anybody have recommendations for studying the math and logic behind parallel programming outside the academy?

I know it's a long shot, but I figured I may as well ask.",hacecva,t3_n2n0ax,1629926627.0,False
n2n0ax,"how to start to understand computer? hardware and software in basics? where to learn, what to read?",hagg96h,t3_n2n0ax,1630002860.0,False
n2n0ax,Hello! May I ask for advice? I'm an incoming computer science freshie next month and I have to get a grade of atleast 1.75 GWA so I can transfer to another university campus next year.  I'm really not sure what I'm getting into but do you guys have any tips and suggestions? I know I have a hard time understanding mathematics and I've heard algorithms is very important for this course. Can you guys give me a headstart with what I should focus on and study on? Do you guys have a book to suggest or an online course to learn? Classes right now for my country is strictly online and I know I have a hard time in this kind of learning system but I'd like to improve and I know that I'll have more free time. Notes and learning material is very much appreciated!,hamxcug,t3_n2n0ax,1630117616.0,False
n2n0ax,"So I am having a hard time thinking between civil engineering or computer science. I want to be a transportation engineer and also software engineer so it's really hard for me to choose.   
Please convince me to take Computer Science since I love them both. I just need reasons to choose CE more. Thank you!",han2lfs,t3_n2n0ax,1630120422.0,False
n2n0ax,I am 27 years old I have a bachelors degree in health science and I was thinking about going back to school and getting my masters in computer science i don't see the point in getting another bachelor's degree in computer science but before I go into the program to not get completely lost . Are there any recommend courses or certifications or boot camps before i joining  the masters degree program ???,han56w3,t3_n2n0ax,1630121902.0,False
n2n0ax,Networking certificates are very important.,hbgth1i,t1_han56w3,1630688083.0,False
n2n0ax,"I’m beginning a 4 years computer science degree next month and I am in need of a decent laptop that will help me with accessibility etc, any recommendations on specs or any advice?",haoqis2,t3_n2n0ax,1630161562.0,False
n2n0ax,"I recently got a computer for computer science so I’ll just tell you what I recommend for now (I am also a freshman). 16GB of RAM, SSD (SATA), a good processor so you can run more code at once (ex. icore7, RMD-7) however icore5 and RMD-5 will both work fine. And at least 14” screen because writing one a tiny screen is just Spain without the ‘S.’
Of course I recommend doing your own research, I work with Windows 10, but you’ll also have to consider if you want to do MacOS (more so for Apple apps). I believe most programming companies use Linux and GitHub so I’d also look into that as well.",hbgq71w,t1_haoqis2,1630686740.0,False
n2n0ax,"Is Computer Science generally a hard or frustrating subject? So far, I did good at one programming course and one introductory CS course. But, I’m not sure how much harder is it going to get when I move to second year and beyond",hapb7t9,t3_n2n0ax,1630170674.0,False
n2n0ax,"I just want some advice. I read many reviews that computer science is better than software engineering because computer science have many concentrations where you can work, computer science is really a degree that is mostly requirement for any job positions in tech, also there are some skills that software developers need from computer science (theory) The issue is that I really want to work as a software developer. I am not really interested in other concentrations unless it is cryptographic, so that’s why I choose software engineering because is more focused on what I want to do. My plan is to have a bachelor degree in software engineering then a master degree in computer science. Is this a good path?? Also I am learning by myself algorithms. Is not a requirement for my bachelor, but I think that it would be really helpful for any job. Data structures is part of my degree, so I don’t worry. What do you all guys think?? Or should I go for computer science bachelor degree??

I am 21 and I am in double major. My first major is website design. I am going to graduate next year for fall, and sometimes I think that is a waste of money going for a second bachelor degree(software engineering) , but I recently discover about programming, and I really love it. I was thinking of going to a bootcamp. For this second major, I am going to graduate in 2024.",haqnbce,t3_n2n0ax,1630192313.0,False
n2n0ax,"I would recommend looking at the differences between software engineering and computer science. From my understanding, software engineering is more hands on (building computers), whereas computer science is more programming computers.",hbgpbfb,t1_haqnbce,1630686378.0,False
n2n0ax,"I just started an apprenticeship  in IT as a software developer.

I feel somewhat comfortable when it comes to writing code etc. but during my apprenticeship (3 years) I'll have to learn a lot of things in which I don't feel that comfortable.. Gonna have to learn a lot about different aspects of computer science..

Are there some good sources where one can learn about many different aspects of IT?

I just want to learn as much as possible to be honest.",hawfjz7,t3_n2n0ax,1630305792.0,False
n2n0ax,Where is the best place to get homework help for CS theory? I'm kind of having a hard time.,hawuv6w,t3_n2n0ax,1630318800.0,False
n2n0ax,I graduated last week with computer science degree and applied mathematics minor. Any advice for the next steps I should take to get a good job?,hay1rjd,t3_n2n0ax,1630340813.0,False
n2n0ax,How many years would I have to do at university doing computer science to get a software engineering job.,hayi9ti,t3_n2n0ax,1630347641.0,False
n2n0ax,Probably 4 years or a Bachelor's.,hbgt44d,t1_hayi9ti,1630687934.0,False
n2n0ax,I am going to take Computer science engineering as my course at college and I have around 1 month of free time before the course [starts. Do](https://starts.Do) you suggest anything to learn before the course starts? I am bored in the lockdown and want to use my time on something productive,hb1vzi4,t3_n2n0ax,1630414867.0,False
n2n0ax,"Hey, 
I am 22 and  just graduated from Civil Engineering stream.
Now, I figured out that my interest/passion is in Computer Science.
Can I even start from the beginning in this field & is there any possible jobs I will get in this stream if learn coding & computer languages?",hb9nlsj,t3_n2n0ax,1630550048.0,False
n2n0ax,"Hi everyone , I'm new here Actually now I've finished my high school And I decided to have A major degree in chemistry and A Minor In COMPUTER SCIENCE what I want to know about it is Is it easy to combine between these subjects or not . Please I need your help to reply me about the below questions :

1. And is it useful to have a minor degree in COMPUTER SCIENCE or it is a waste of time ? (( In my uni we take only 8 classes in COPUTER SCIENCE IS THAT ENOUGH ? ))

2. Can I get started to programming and be ready to develop websites 

thank you all guys And I wating for your replies and I am grateful to you all .",hbaff2n,t3_n2n0ax,1630567541.0,False
n2n0ax,Computer science is not very different but indeed different than programming. Computer science has more theoretical applications than practical. A minor in computer science would only scratch the surface. If you want a more thorough answer itd be helpful to see the courses and course work your college offers for CompSci minors.,hbgsyq5,t1_hbaff2n,1630687873.0,False
n2n0ax,"How to apply for a position as a computer engineer abroad?  
21 this year, studying in China for an undergraduate degree.  
what kind of skills should I master? where can I get the imformation?",hbbocr7,t3_n2n0ax,1630595192.0,False
n2n0ax,"Anyone here with experience in CS Master Programs? I have a 3.504 gpa and good test scores, I'd be a domestic applicant. What are my chances at the top CS master programs in the US? CMU? I really want to study at CMU and I'm gonna apply anyways but I'm worried I won't stand a chance",hbdwdjx,t3_n2n0ax,1630628967.0,False
n2n0ax,"Masters programs, the ones I've looked into, have minimal GPA requirements of like 3.0s. Top colleges I would not know, but expect quite a heavy workload that comes with Masters programs from basically any school. 

Often times, you can get an employer that will pay off your entire financial debt to colleges when you want to achieve a Masters degree. It becomes solely a time, energy and mind investment which is a decent bargain.",hbgs4ye,t1_hbdwdjx,1630687535.0,False
n2n0ax,"I’m planning to go to university for a degree in computer science but I do have questions. Will computer science allow me to really do work in programming and software development or is that more of a software engineering thing? The descriptions from my local university are rather vague and confusing and are throwing me off. I still have time to change my prerequisite high school courses but I need to know. I like programming and I’m hoping that with my degree I can get into a career where programming is a thing that happens. Of course it definitely can’t hurt to get all the other very important parts of CS. I’m aware there’s more to it than just programming. All the theory and important essentials 

Kind of a dumb question I know but when I get thrown off at 3AM and hit by doubt I like to reassure myself that I’m not about to screw up my life",hbjo30m,t3_n2n0ax,1630741988.0,False
n2n0ax,"I am 16 and in my junior year in high school. Computer Science seems like a field i’d really be interested in going to college for, but i have no idea where to start. can anyone give me advice on what i should know going in/ what i should learn now?",hbmpe5w,t3_n2n0ax,1630800807.0,False
n2n0ax,"Administrative assistant wants to get into cloud computing ?

Hw and where should I start with getting cloud computing?  I come from a minority background family I don’t have money for a degree. I am trying to get myself out of poverty.",hbqv7t5,t3_n2n0ax,1630885049.0,False
n2n0ax,"Hello all,

I am 39, I have a career in an unrelated field but love ai, recognition programs, and modeling. I would love to pull my feild into the present and create a user friendly app for my profession. How do I start? I have built excel tools/calculators that optimize my work (the tools make me about 10x more efficient than my colleagues) but it's cumbersome and clunky. It takes some time to train people on how to use them. I would love to take these tools to the next level and create an app. Am I too old? Should I hire some pros? If so, how do I find the right people?",hbr512a,t3_n2n0ax,1630889866.0,False
n2n0ax,"Hey guys, I'm a sophomore in High School, and I have recently sparked and interest in computer science. I have no experience (yet) in computer science, other than a high school freshman computer tech class, which just went over the basics of computers. I just wanted to know a few of your guys' stories of when/how you began your education in this area. I am truly passionate about learning all sorts of new things in this area. I want to take every computer science class I can within the next 3 years of high school. So I just need some help getting myself going, and setting goals. As I have said, I want to know things such as; when did you decide to go into this area, when did you actually start learning to code and do other things, and any other information you might think would be relevant to a ""not sure what I want to do with my life high school student"" like me. Lol. Thanks For The Help!",hbriiy0,t3_n2n0ax,1630896604.0,False
n2n0ax,"Hi! I’m currently a math teacher (graduated summa with a BS in math and BS in education) but am considering going back for a masters in CS to work in Ed tech. I’ve used html briefly but have written a minimal amount of code. 

Where should I start to gain basics before considering applying for my masters? I assume I’ll still need to take a few pre reqs before getting into the CS program but I want to be prepared. Thanks!",hbteudo,t3_n2n0ax,1630941404.0,False
n2n0ax,"I just started my master programme in this field and damn I am taken aback by the sheer intensity of how much and how hard all of this is. 

Right now I am following courses AI Techniques, Linear Programming and Quantitative Performance Analysis and its soooo much and all of it is soooo hard to wrap my head around it.",hbtov16,t3_n2n0ax,1630945742.0,False
n2n0ax,is it possible to fully self-learn a computer science degree?,hbwr43f,t3_n2n0ax,1631003636.0,False
n2n0ax,I have ds&a this year in college. Could you guys suggest a book/site/channel where I could learn it from a very rudimentary level in a structured way?,hc1is6p,t3_n2n0ax,1631096173.0,False
n2n0ax,"I am soon starting my last year of high school and I am very interested in working in this field (CompSci, IT, software engineering). I am already learning to program by myself. My concerns are that the only university in my country does not have a CompSci or software engineering course, but it does however have an electrical engineering course where you can choose an extra subject to kind of specialize in. One of them being Informatics. I have talked to students doing the electrical engineering course and they told me that they have classes where they program in C++. Now my question: Will I be successful in getting a job/opportunity to work in the field of CompSci or software engineering with an electrical engineering degree where I chose informatics as an extra subject.",hc71pbf,t3_n2n0ax,1631203507.0,False
n2n0ax,I am majoring in computer science. I have the option to take Computer Science I and Computer Science II OR Java I and Java II. Which would be best? Thanks!,hc7j67n,t3_n2n0ax,1631210748.0,False
n2n0ax,Is it worth trying to get a CS degree if you are not good at math and what math should I expect to take in college?,hca02xa,t3_n2n0ax,1631250889.0,False
n2n0ax,"I want to enter into the computer science major as a freshman but I’m still unsure in what college I should join. 

Is it more better to study the program online or in person like on campus?

And what college do you all recommend I should join 🥲 or who is a graduate of a college that seems more suited to teach in this field for complete beginners with no experience",hceb747,t3_n2n0ax,1631332301.0,False
n2n0ax,I would love to do CSE course but i am weak in Maths. I lost my minimum grade C in Maths by 5 marks in A Level. I also hate maths so should i take CSE. Also how hard maths am i gonna face?,hcex2fv,t3_n2n0ax,1631349149.0,False
n2n0ax,"I'm a Math PhD, currently primarily teaching. I'm not sure I want to do it forever, partially because my salary is what professors actually make, not what people think they make. I have ZERO formal CS training, but I've written plenty of code in my life. I've done some math research using Magma, some in Mathematica. I've written an entire webpage using PHP and SQL for a few boardgames I wanted to play with my college friends. (And when GoDaddy got hacked, I wasn't worried because I salted the damn passwords in the DB.)

If I wanted to get into a CS career, what advice do you have on  formalizing my credentials and making myself an attractive hire? My math background btw is deeply theoretical - it has some relevance to cryptography/number theory stuff, but it's not really focused in that area.",hciyrpn,t3_n2n0ax,1631421916.0,False
n2n0ax,"Im so confused. I am looking to go to University of Otago for a computer science. What confuses me is that there is a Bachelors of Science and Bachelors of Arts and they both have the same courses. Also what the difference between BSc and BA?

Here is the link: [https://www.otago.ac.nz/courses/subjects/cosc.html#requirements](https://www.otago.ac.nz/courses/subjects/cosc.html#requirements)",hcjnpjs,t3_n2n0ax,1631440556.0,False
n2n0ax,"I am not in the USA, i believe thats a problem on the sites part, if you are getting your undergrad in a science major you get a BS when you graduate, and a BA in a art related major. You just cant get a BA in CS, its automaticly BS.",hcnkkh9,t1_hcjnpjs,1631507284.0,False
n2n0ax,"i have a laptop question for yall. Would you rather have an oled 15"" 4k screen or a 16-17"" ips running a 16:10 ratio like 2560x1600 for coding and general use/media consumption. both have similar internals",hcn54ou,t3_n2n0ax,1631499013.0,False
n2n0ax,if you are going to move it around 14-15.6 is perfect for me. both oled and ips is fine. So if u have classes and will move the laptop daily get the smaller one.,hcnkb4b,t1_hcn54ou,1631507125.0,False
n2n0ax,"I am currently in computer science. Info Systems seems to be difficult, database precisely. Any advice on how I can understand info systems??",hcpa66t,t3_n2n0ax,1631547480.0,False
n2n0ax,"Hi, I'm planning on starting a Master in computer science and was wondering where to get the relevant books to help me",hcq7512,t3_n2n0ax,1631561006.0,False
n2n0ax,"I've started the 100 Days Of Python Udemy course, is there anyone here who has completed the course or not completed it but started it? What are your thoughts  on it?

I'm 31 and have zero coding experience but hope to eventually make a career out of it.",hcqgzzy,t3_n2n0ax,1631564964.0,False
n2n0ax,"Been working as a software engineer for about 4 years, came out of a bootcamp and everything has been awesome....up until about now.  I just changed jobs to look for more of a challenge and I'm in way over my head.  Everyone here has at least a BS in Computer Science and I'm feeling very far behind when it comes to more advanced concepts of software development like server management, event-driven architecture, lambda functions/serverless computing in general, and the like.  I'm not against going back to school, but do you have any recommendations on youtube channels or udemy courses that would be good resources for me to really get into more advanced concepts of programming?  TIA",hcr57c1,t3_n2n0ax,1631575585.0,False
n2n0ax,"No CS degree but interested in becoming a front end developer. My degree is in Business Admin with a minor in marketing but I ended up not loving the field like I once did. I worked with a creative team in packaging design so I took the Google UX/UI certification but that’s obviously not enough. I need to learn to code, I’m able to take free Udemy courses (military spouse perk). Any recommendations? I’m starting with Python and moving to Java. Is this reasonable or should I head back to school for two more years?",hctmdhm,t3_n2n0ax,1631629058.0,False
n2n0ax,"Why can't I read code? I'm in my Sophomore year of computer science currently I keep finding my assignments lately very stressful simply because I can't follow my teachers code. Most assignments lately have been here's a huge portion of code and write one function inside it. For example, my most recent assignment was about Linked Lists. We were assigned to remove the end of the linked list. I completely understand the logic of linked lists and understand how to implement it but I just can't follow my teacher's code I constantly find myself confused as to what function leads to where and I feel like if I wrote it in my own way it would be incredibly easy but I just really struggle reading other peoples code.",hcuiosj,t3_n2n0ax,1631642143.0,False
n2n0ax,I’m in my first semester of community college in California and I do plan on hopefully transferring to a UC. Top choices are Berkeley and UC San Diego but I was wondering besides keeping grades up is there anything else I could do to boost my chances? Programming projects on the side help me at all? What kinds should I do?,hczxdk9,t3_n2n0ax,1631739230.0,False
n2n0ax,"Hey guys, I am a sophomore in college and this is my first time really going through the internship process. I am information systems major but am applying for software dev/engineering internships and just had a few questions about the technical interviews and types of topics they'd expect you to know. I know there will probably be some repeat questions that are already on this thread so sorry about that!!  
  
Some companies like Chase, for example, make you take a coding test(HireVue) before they even consider you for the job. What type of questions can I expect from these type of tests. I have heard they are mostly questions that involve arrays or string methods... is that correct?  
  
What can I expect from the technical interview? This is what I gathered from a little bit of research on this thread: Sorting, Hash Tables, Trees and graphs, recursion and induction, linked lists and maps. Is there anything else I should be studying?  
  
I know LeetCode has amazing sets of practice problems but there a quite a bit of them and I am kind of confused on what I should focus my time on. Anyone have a link to some good problems sets that really help them out during the internship process?  
  
When applying to an internship, I have heard mixed reviews of adding a cover letter. Does adding a cover letter really give you a higher chance of being interviewed than a person who didn't submit one.  
  
Thank you so so much even if you can answer one of these for me!!",hd04wa1,t3_n2n0ax,1631742385.0,False
n2n0ax,"I'm a mathematics grad who has just started working in fintech working as a Dev using a niche functional language. My thesis was in theorical computer science so I have some basis in algorithms but I have very little experience in actually implementing said algorithms. What resources would you recommend for me to learn algorithms (i.e not just describe a DFS but actually implement it)?

(Also any general recommendation for practical resources on fundamental topics in CS, I'd like to form a sound basis on how computers actually work beyond the model of a Turing machine)",hd3abzi,t3_n2n0ax,1631806938.0,False
n2n0ax,"I'm a sophomore computer science student majoring in software development. Ever since going to school, everyone around me is pressuring me to stay in software development because of the more stable income, higher income, and toxic working environments of game development. But creating games is what I find more interesting. I am conflicted on what to do, as if I change my major next semester I have to commit to it in order to graduate in 4 years. One idea I've had is to quad minor (I get IT/IS minors included with my major, + game design and cyber security minors) but I've been told that this would be a bad idea as I'm not really specialized in anything. Another option is I might be able to squeeze in is dual major in soft Dev and game design but I don't know how the course load would work out. 

I need advice on my options and what I should do.",hd4ggjf,t3_n2n0ax,1631823965.0,False
n2n0ax,"Could someone help me with my homework question?  
https://www.reddit.com/r/HomeworkHelp/comments/ppymhn/computer\_system\_analysis\_how\_to\_compute\_the/",hd72y41,t3_n2n0ax,1631879418.0,False
n2n0ax,I’m not that far into my CS undergrad but there are days I feel like I have no clue what the fuck I’m doing. I’ve never really struggled with learning but I have struggled with focusing so I often get this feeling I’m just following the book with no long term grasp of the content. From what I’ve seen employers are less likely to provide on the job training and expect you to know the content prior to arrival. Where I live internships are not an option. Any input?,hdhqjcm,t3_n2n0ax,1632075610.0,False
n2n0ax,"How can I distinguish myself from everyone else in the world studying computer science? I am in my second year in university and I am studying computer science. I was thinking about the future and getting in the job market. What would separate me from the thousands of other people with CS degrees?.Internships, certificates, side courses, etc? Thanks",hdkd48x,t3_n2n0ax,1632122593.0,False
n2n0ax,"Do multiple integrals have any applications in (commercial) computer science as a whole or in machine learning specifically and if yes, any examples?  
I went through all the big things in standard calculus like limits, derivatives, integrals and multivariable functions and loved it, second part of my book that I'm learning from teaches many physics oriented things (complex functions, differential equations etc) and one of them is double/triple integrals. I couldn't find any applications of that in computer science, so I thought I might as well ask just in case I don't waste my time right now learning something that's more for physics and engineering students.",hdkf0u0,t3_n2n0ax,1632124291.0,False
n2n0ax,"I'm starting computer systems in my collage and I'm just looking for tips maybe or things to look out for when starting my course, I have no real development experience or anything compared to that so I'm definitely starting from scratch.",hdlc9ni,t3_n2n0ax,1632147022.0,False
n2n0ax,I actually find coding really interesting and I kind of hate school besides that but I’m rlly good at it is it a good idea to go to college and get a degree to be a programmer or something in computer science or could you do it without the degree,hdlyqim,t3_n2n0ax,1632156407.0,False
n2n0ax,I am a civil engineer grad and I am working to do my masters in computer science because I want to change fields. I am unsure what fields I should consider. Any thoughts on this matter?,hdp6a8k,t3_n2n0ax,1632216674.0,False
n2n0ax,"Hey I’ve been really interested in computer science,software engineering for almost a month now and it’s the first thing school wise I’ve felt very excited, I wanna get into A.I machine learning,(Im thinking automotive like Tesla stuff) deep learning, would be my main focus for schooling, then I would do some course on stuff for like virtual reality creating new computer software creating a new banking stystem I don’t know if there close or not in fields so lmk :) thanks",hdqn9wl,t3_n2n0ax,1632244272.0,False
n2n0ax,"Searching for universities that I can study computer science affordably.

I don't have a specific country on my mind. My only necessities are that it has to be a country that speaks English and I can afford most of my expenses by doing part-time jobs.  
  
My qualifications are:  
\-I already know some coding languages (Python, Java, C++, C),  
\-I participated in various competitions (FLL, FRC, MUN's, debates),  
\-I have taken part in many community services,  
\-I passed with Distinction in BTEC Level 3 Certificate in Applied Science,  
\-I am a quick learner,  
\-I can speak English fluently,  
\-I am a 1-star CMAS scuba diver",hdquhkh,t3_n2n0ax,1632247252.0,False
n2n0ax,"I’ve been reading a lot and seeing lots of course and certificates I can get, would it even be worth it to go to uWaterloo for computer science or should I just do these courses then the AWS GCP exams?",hdrc2ul,t3_n2n0ax,1632254494.0,False
n2n0ax,just FYI my kids have been learning a lot from [this youtube channel](https://www.youtube.com/channel/UCvIzIe8bIMkAkblXz4aQ-Pw),hdrrp3o,t3_n2n0ax,1632261142.0,False
n2n0ax,"Recent college graduate with a BA in political science and Asian studies looking to diversify my skill set. Interested in cyber security but have basically 0 programming/cs experience.

Where do I start?",hdso1qj,t3_n2n0ax,1632276404.0,False
n2n0ax,"**TLDR:**

\------------------------------------------

I will be free for a whole year most likely until I'm eligible for an Access to HE in IT course. meanwhile, I want to build an ePortfolio and certificates seem to be a good choice (E.g. Microsoft's certificates/exams).

So what reputable programming certificates do you recommend for someone with minimal knowledge of programming (mostly python, I can build a simple calculator or a script that organizes files depending on their extension for example)

\------------------------------------------

Hello,

I live in London, Harrow (just moved to harrow) my current situation goes like this:

I'm 18, and when I was 17 I couldn't finish the BTEC level 3 extended diploma in IT, but still got a pass for the first year because of covid so now I have a subsidiary diploma (60 credit).

I got depressed for a little while, but when I got back on track, I was late to apply to another college and the same college wouldn't accept me for not doing enough work. So my only option at the time was Open University. I applied for the BSc in computer science, and it went well for the first few months. I was getting 80-90/100 on my first 2 assignments and I was pretty happy with it... except it was online so I was so bored and felt like sh!t.

At some point, some family conflict that was boiling reached its melting point, and the house then felt so different. I couldn't focus on anything, it was just too overwhelming to do anything. after some calming down, the family decided that moving to Sweden would be the best option, kinda like a reset button- it's too complicated to explain.

At the time, it was official, we were moving, so all my stress kinda dropped and since I was late for my assignments this time, I felt like there is no use getting back to studying as we will start a new life, with a different education system that is kinder and more forgiving (apparently).

After a month in a half in Sweden trying to find a place to settle, we just couldn't find any affordable place that is right for us- that and we didn't realize how political and different Sweden was in that period (Idk if still is rn), so we were pretty much forced to get to London and instead move to a different area, so we moved to Harrow.

Now I had to see what I can do in terms of education, I checked Open Uni, I was still enrolled but all the assignments are past due. I asked the teacher and they told me of the deferral system, where I can restart the year on a different date and keep some of my progress. now I'm still awaiting further information on this, but this is what I know so far.

I checked with a University that I was going to go to after one year in Open Uni, they told me they cannot accept me with what I have, even for foundation year, and told me that my only option is doing an access course and with it, I can skip foundation year, or re-doing Open Uni and getting accepted to the second year of the Uni I want.

I checked colleges in my area, one that has access course for IT and they told me I'm too young and advised that I redo the BTEC level 3, so I went to the second bigger one, and they told me because my subsidiary diploma is on the old qualification or something it would be ""difficult"" to enroll for a full 2-year BTEC level 3 extended diploma for some reason and that my only options are doing a different subject (hell naw) or finding a college that is still doing the old qualification, which there isn't as far as I know.

\------------------------------------------

**My Options TLDR:**

Unless the second college contacts me back telling me they were able to let me do BTEC level 3 IT there, my current options are as follows:

\- do a different subject BTEC level 3

\- re-do Open Uni first year

\- wait until I'm 19 to do the Access to HE course, and meanwhile, build up an ePortfolio for a better chance of acceptance, which is what this post is about- finding the right things to put on my ePortfolio, hence Certificates.

So, what do you think I should do? and if you have any experience with Programming Certificates (E.g. Microsoft's certificates/exams), please let me know. I have no idea where to start looking or what to do for them.

\------------------------------------------

Thank you, kind strangers.",hdvltgu,t3_n2n0ax,1632336619.0,False
n2n0ax,I've been wanting to go into software engineering would it be better to take a computer science class at a somewhat medicore community college near me or try online classes from something like code academy? I work full time in a factory and kind a not feeling like doing assembly forever. What do you guys think though what's your opinion?,hdxj805,t3_n2n0ax,1632374343.0,False
n2n0ax,When you received your degree was it hard to find a job right away?,hdyk7ku,t3_n2n0ax,1632402535.0,False
n2n0ax,"Hello.  
  
I'm a junior comp sci major - transferred with my associates, so currently still have to take a couple freshman classes that I never took, such as calc 1 , physics , etc..  
  
I do well in all my classes in school - even the entry level programming ones I've taken so far -  
  
But calc 1 is really an obstacle for me. The hardest part is I don't know what to study - the homework seems to be disjointed from the test and quiz - or maybe I'm just not following along.  
  
I understand the lecture in class - and I can read the textbook and even follow along in theory. I can discuss calculus in the abstract to an extent -  
  
But when I sit down to do my homework with pencil and paper - all knowledge and confidence drops out of my head.  
  
  
  
I get totally overwhelmed and discouraged.  
  
  
  
I have had my heart and mind set on being a comp sci major for years and I'm sort of panicking and stressing trying to pass calc 1.  
  
  
  
I know that I can do it - I just need to practice more problems and study more - I'm studying a good amount, but I can't seem to pinpoint the best way to study - and I am having trouble knowing which formulas to know when and so forth. Just generally disorganized mind when trying to solve problems.  
  
  
  
I know this is sort of a long post and a bit ranty -  
  
  
  
but I'm really desperate here seeking any help. I'm dedicated to passing and will use any resources to do so - if you have recommendations or any advice whatsoever it would be greatly appreciated.  
  
  
  
It also probably doesn't necessarily help that I have a crap professor (he is very smart but bad at teaching ) but that's no excuse and I know it. The math is the same no matter whos teaching it.  
  
  
  
Thanks !",hdz9lbq,t3_n2n0ax,1632413596.0,False
n2n0ax,"I prefer computer science, Because in computer science you will learn the necessary computer fundamentals beside programming.",he1vnjy,t3_n2n0ax,1632456296.0,False
n2n0ax,I'm looking to start a degree in computer science. Is there any jobs or anything someone would recommend that would help me in thid field while I am working towards my degree? I'm 32 and looking to switch from a blue collar job but I am trainable and I have always had a knack for computers,he3mzuj,t3_n2n0ax,1632496765.0,False
n2n0ax,"What kind of computer science job where its highly possible or at least almost normal for a computer science degree holder (fresh grad or experienced) to be required to move to another country and stay for, say, five years? I'm asking out of curiosity because heard from a now ex-co-worker of mine that he got a job to work in another country and has a cyber security based position for five years, and based on my research there's no definitive  answer related to it.",he4jiu2,t3_n2n0ax,1632510367.0,False
n2n0ax,"I’m currently going to school for Electrical Engineering. I am wanting to swap to a career in programming, and want to know if Computer Science or Software Engineering is the way to go? Are the degrees mostly interchangeable in the working world or will employers (predominantly in the engineering realm) look down on a Computer Science degree? The curriculum for the two seem similar but Computer Science takes less credits to complete. Any input would be awesome!",heara7s,t3_n2n0ax,1632623907.0,False
n2n0ax,"I have two main questions and it would be great if someone can help me:

1) How do I know if coding is for me? I have been doing several courses the last few months in python and java and I still do not know if i like to code it feels in an strange way and I do not know if it is normal to feel that. 

2) I am interested in doing a dual degree between computer science and statistics but I do not know if for example is more worthy to get a CS bachelor degree and later getting an Statistics MS. My main interest in a career is to have a lot of mathe like calculus, etc and I feel like these two complemente each other very well, so what are you thoughts about this?

&#x200B;

Thanks for your help!!",hedrrs8,t3_n2n0ax,1632683798.0,False
n2n0ax,"For someone who works as a software developer but has no formal training in computer science, what's the best resource for learning more about computer science without having to enroll in university?",heg6k5g,t3_n2n0ax,1632729729.0,False
n2n0ax,"How do I prepare myself for masters in computer science? I have a bachelors and masters in electrical engineering. 
I’m a beginner level programmer. Have some experience with leetcode but that’s all.",heh2471,t3_n2n0ax,1632751979.0,False
n2n0ax,"I have two semesters left until I receive my degree in general business practice and I am unsure of a career I want to pursue with this background. The knowledge I have obtained from this degree has helped me in pursuing my own business ventures, but they have not led me on a path to any specific careers. I have been interested in computer science for awhile now and am curious to see what others would do in the situation I am in. If I wanted to pursue a career in computer science would it be most logical to complete my business degree and take coding bootcamps, or would I have to get a degree in computer science if I wanted a real shot at a high paying career in this field?",heieolv,t3_n2n0ax,1632773659.0,False
n2n0ax,"I need advice on choosing a career field. My original plan was CS, but for me to get to that point, I would have to take a few more classes at the Comm. level that my own Comm. do not even have, and that would mean I would have to go to a different Comm. and I would have to stay at my 4-year a semester or two longer. On the other hand, I have IT to fall back on, but I would only have to take one class and then I could go onto my 4-year. The thing is, is that I want to be able to code, build programs, especially stuff dealing with A.I. and I know that I can learn this in my free time, but I also want to do it as a job. I am kinda at a crossroads and I would appreciate any and all advice.",hepq4n9,t3_n2n0ax,1632918251.0,False
n2n0ax,"For someone without any certificates in Computer Science, which would be the most valuable to get in the beginning? Looking to change career paths and want to get some insight on the field.   Thanks",heq7d73,t3_n2n0ax,1632926507.0,False
n2n0ax,What kind of jobs can I get in the field without a degree? I'm currently a student but I've lost all motivation to do college anymore.,heqa2j1,t3_n2n0ax,1632927645.0,False
n2n0ax,What’s the best accredited school to get an online bachelors/masters degree? Pros and cons?,heshl1k,t3_n2n0ax,1632961305.0,False
n2n0ax,"Im currently a junior majoring in Information Systems Management expected to graduate Fall 2022. Im taking intro to programming right now (C++) and I really like it. Im debating whether or not I should switch to Computer Science. Im only hesitant because Im bad with calculus (pre-calc was hard for me already even though I got an A). If I do switch to CS, I will graduate Spring 2023 instead of Fall 2022. Plan B is to just finish my MIS Degree and attend a coding bootcamp right after. Im from NYC and I heard good reviews about FullStack Academy. My college is not a top tier school or anything fancy (CUNY). Im worried that a MIS degree would not be competitive enough in the job market.",het4cru,t3_n2n0ax,1632972274.0,False
n2n0ax,"If I learn c++, instead of something like python or java, as my first language as per my college curriculum, will that make it easier to learn other languages?",hetplo1,t3_n2n0ax,1632986743.0,False
n2n0ax,Can I get an internship/job as a CS sophomore?,hetpo69,t3_n2n0ax,1632986807.0,False
n2n0ax,I am applying to do computer science masters. It’s a conversion course where you can have a background in anything. I am brand new to all of this and have only just started looking into the basics of html. The course is one year long. Is it possible for me to find a job once I finish the course?,heu2s95,t3_n2n0ax,1632998616.0,False
n2n0ax,"A college in my town is offering an associate of science for Cloud Networking & Cybersecurity. Would this be enough to land a decent job? Or, should i just accept that a bachelor will be better for me despite how much money they cost? I'm having a hard time deciding and feeling a lot of pressure.

&#x200B;

I want to work on cloud networking & engineering but atm can't find any specific degrees besides that one. I'm also open to software developing.

&#x200B;

Would anybody be interested in chatting for a little bit? I'm really nervous because it's a huge commitment at age 33 for me, and I'm about to get married.",hevjn9e,t3_n2n0ax,1633023877.0,False
n2n0ax,"Today I quit my job as a video editor and plan to pursue a career in computer science. I turned 29 years old a week ago and I am very excited but also nervous. I did mathematics 10 years ago in Grade 12 and passed by merely 55%. I genuinely don’t feel like it’s a true reflection of what I’m capable of understanding as I wasn’t motivated at all in math and hardly paid much attention, but now I am so stoked to learn mathematics again and understand it properly. How screwed am I???😂😂",hevzcio,t3_n2n0ax,1633030465.0,False
n2n0ax,What type of computer should I buy for computer science as a freshman?,hew0gmq,t3_n2n0ax,1633030934.0,False
n2n0ax," 
Hi, I'm an absolute beginner to computer engineering but I would really like to create my own budget application. I want to be able to track my finances everyday to the very last cent but I don't know where to start, hope you guys can give me a direction. 

  My father has recommended excell for doing this. I would like to have a diary style budget, where I can insert my monthly payments and costs. I would love if I could do some stats based operations like my average spending on X and how much cash I have left.


 Thanks for reading ❤️🤙",hf0xv2v,t3_n2n0ax,1633125514.0,False
n2n0ax,"Are there any books/lectures/videos you recommend to get INTERESTED in cs? I don’t want a book on how to code or anything too technical but maybe a theoretical side that makes me understand and potentially be intrigued more, to learn about computer science?",hf300ym,t3_n2n0ax,1633171086.0,False
n2n0ax,"**Need help switching from medicine to computer science.**  


Greetings, fellas!  
I'm from Brazil, 20 years old, and currently finishing the second year inmed school of Federal University of Paraná. Every new semester is such a pain for me that I end up anxious and frustrated.  
**Some background**   
My grades are good. I had created an extension project called ""Learning Support Project"", which aims to discuss evidence-based learning (through Anki) and other things with graduate students and professors. Also, the professor who helped me create it has already invited me to her research group. The point is that everything I'm involved has nothing directly to do with medicine.   
In my country the process to enter the college, although some are free, involves one taking a test about all the high school subjects. I passed the test in my first attempt, even though the college I applied to had huge competition. In Brazil, there is nothing like two years studying biology and after this entering medical school. Here, after one pass the entrance exam for medical school, he stays 6 years studying from the basics sciences to the clinical competencies. Here, medicine has the most difficult admission exam of all the other college courses.  
Other important thing, is that my parents are really unaware of how CS major is. They are reluctant ant think that only in medicine I would be successful (monetarily speaking).  
**The problem**  
After telling you my background, I wanna say that I'm not happy being a med school student. I'm worried about continuing the course and ending up frustrated and putting patients' lives at risk. Everyday I see my friends saying that they are loving the rotations, internships, the contact with patients (even though, because of the pandemics, the workload of practical classes has decreased), the clinical cases... I don't feel the love and the enthusiasm of them. I don't like the hospital atmosphere. I don't even see myself as a physician, regardless of the specialty. However, I get really excited every time I try to do some coding or deal with digital design. Everything I like to do, all the talent I have, doesn't seem to fit into the practice of treating patients with a disease. My reasons for taking the entrance exam for medicine were simply the fascination for the brain (which has more to do with psychology) and the huge cliche family pressure and judgement.  
I haven't quitted yet, but I'm very thoughtful and sad.  
**Can anyone give me some light telling about your experience? If you have lived in a situation similar to the one I am living in, please talk to me! I would like to know how are you dealing with it, the process, the judgements.** 

  
I'm very afraid of giving up and not getting support from my family, besides fearing that I won't find my way anymore. You know... Med school is very very difficult. And, if I don't love the profession, how could I pass through it?",hf95b59,t3_n2n0ax,1633285024.0,False
n2n0ax,"My son wants to study computer science at the University but we are also looking at military academies.  He is a 3.95 honors/AP student who aced the intro to CS (python) course at our local university. 

What is the reputation of the US Military Academies for CS/Cyber security?  USAFA & USNA

If not accepted (not gifted athleticly) does the University he chooses matter significantly for job recruitment?",hfb53w8,t3_n2n0ax,1633317228.0,False
n2n0ax,"I am in the field of computational mechanics and materials science. I have experience in writing codes for my research work and find it very fulfilling as it gives me a way to model the physics of a problem and can be validated through experimental research. However, I have been struggling to find an industry job in my field. 

I have been talking with PhDs and postdocs in my field who left their research career and are full-time software engineers in industry. They are earning 6 figures but they complain a lot about how boring their job is. How much they miss research work and compliment me for sticking around with research.

I loathe them. If I could get a job in software engineering I would do that in a heartbeat. I love my contractual postdoc research, but I hate to be poor and without much job prospects. I want to shift my career but I don't know if it's possible as I am already 32 and am not that adept of a programmer. I have worked with Fortran, Matlab, some Python and some C++. 

Just wanted to write this here.",hfb9rj8,t3_n2n0ax,1633319694.0,False
n2n0ax,"How many people in CS hate their jobs? as an extrovert, I can't imagine spending all day working solo, grinding on code.",hfdpme6,t3_n2n0ax,1633371445.0,False
n2n0ax,"I am going to start working on computer science come next semester. I would like to know what classes I need to start off with. You know, easy classes that help me open up to more advanced concepts. For example, I hear that taking a Visual Basic class can help open you up to more complex languages like Java.  
  
  
  
Thanks in advance.",hfenmm5,t3_n2n0ax,1633383953.0,False
n2n0ax,"I am a cs student and we always have like 3 weeks in the end of each semester where we can choose a course.
There is a course about Drupal and another one where we build 3 small application with react and im wondering which one I should roll in.
Is drupal still very relevant in the industry today and am I benefitting more on learning react than drupal and vice versa?
Drupal sounds exciting but I feel like react is also widely used and will definitely benefit alot from it.
Can someone elaborate with me on which one is better to learn for when I graduate and start job hunting?",hffafgn,t3_n2n0ax,1633394888.0,False
n2n0ax,"Hey everyone! I graduated from university almost a year ago with a degree in Information Systems and Finance. I work a non-technical business role at Big Tech company but I would like to become a SWE or Product Manager. I'd even like to work in AAA development sometime! But in order to do any of that, I need (and want) a strong CS background. I've found a couple Masters programs that seem to be a good fit for me. Namely, the LEAP program at BU in Computer Engineering and the MCIT at UPenn. (It is a Masters in CS but the name is Computer and IT, unfortunately)

The BU program is full time in person so I'd have to leave my current job, would be shorter to finish (1.5 years vs 2.5 years), and about 4 times as expensive as the MCIT (27k vs ~120k). I'd go into debt for a quality education however.

Factoring time, money, and quality of education, which program would open the most doors? Would a CE Masters be a good idea for someone with my career goals more focused in software? Also, would I be auto-dinged from job applications in the future if my degree doesn't say Computer Science? That's my biggest concern of UPenn's program.

https://online.seas.upenn.edu/degrees/mcit-online/academics/

https://www.bu.edu/eng/prospective-graduate/leap/planning/ (Core classes)

https://www.bu.edu/eng/programs/master-of-science-in-electrical-computer-engineering/ (Electives)

I put the course list down for both programs if you want to see what they offer.",hfffdsh,t3_n2n0ax,1633397382.0,False
n2n0ax,"We offer a 12-month and an 18-month Master's in Software Development program that sounds like it could be a good fit for you. Our 2021-2022 tuition cost for our 18-month track is only $48,795. We even offer a tuition refund guarantee. Our graduates are getting great jobs with an average starting salary of $91,000. 

You can learn more on our website [https://msd.miu.edu/](https://msd.miu.edu/) or our subreddit /r/MIU_MSD. 

Feel free to message me if you have any questions!",hfvak9n,t1_hfffdsh,1633709466.0,False
n2n0ax,"Help! Week 3 of CS degree , bought a Macbook Pro with the M1 Chip and my prof.s told us all to not use macs for the degree if we can because of some of the course content. Also with there being some slight differences it can make it hard for me to keep up when following demonstrations as i have to translate it as i follow. Im an older student and not a whizz like the kids in my group, i don't want to be hindered.

I went into the apple store and they said that i could trade in for a higher spec Macbook with an intel chip so i could dual boot but i wonder if this is worth it, will there be any compatibility issues or should i just sell it and buy a windows laptop! Only Have a couple of days to decide as we are about to start a project.",hffxppd,t3_n2n0ax,1633407002.0,False
n2n0ax,"Hi there! I graduated May 2020 with my bachelors in Computer Science. However, with the pandemic and all I ended up going back to my role as project Coordinator for the plants. Now that things are seemingly opening up again. I wanted to ask where is a good place to start as far as careers in Computer Science? Is there any particular items employers look for on a resume? Any help is much appreciated!",hfk1ryk,t3_n2n0ax,1633486683.0,False
n2n0ax,"I’m a project manager at a tech company with an econ degree. I have taken a few beginner crash courses on python and sql, but I really want to learn the architecture of how systems integrate with each other in all phases of the sdlc. Not really sure where to start tho, it all seems so overwhelming. Any advice?",hfm1r5p,t3_n2n0ax,1633533068.0,False
n2n0ax,"Are there computer science masters for non cs majors in europe? 

I studied bachelor in communication but have worked for a year as a developer and would like to get a degree in computer science but 4 years is too long and expensive, so I thought a masters could be a good idea.

I've seen some computer science for non cs majors in america, are there some in europe? Or that at least I can do some bridge course? Thank you!",hfo5dpy,t3_n2n0ax,1633566019.0,False
n2n0ax,"Hi everyone, I'm applying to UBC for computer science and hence can anyone tell me what the supplement essay prompts are for UBC?",hfp4mcc,t3_n2n0ax,1633585176.0,False
n2n0ax,"How can I get a tutor for multiple classes?I’m currently struggling with php,MySQL ,JavaScript and xml.Feel as if I should just switch majors because It’s really becoming to much for me.",hfrm25e,t3_n2n0ax,1633635398.0,False
n2n0ax,"What are the fields related to programming that someone needs to have a deep knowledge about technical, theoretical and/or stem related stuff to be able to perform appropriatley? Which of these do you think someone would regularly use this knowledge?",hful5t3,t3_n2n0ax,1633698102.0,False
n2n0ax,"I’m 19 and just started my freshman year pursuing computer science. I know this is going to sound like it sounds, entitled, but I’m an insanely smart kid. I came from a neglectful and abusive background and got a 1.7 GPA but I managed to get into college for 100% free due to my situation. I am currently passing all my classes with what would’ve been an A+ average (only one regular A in math) in high school. And I know that’s probably not enough, so let’s assume a hypothetical like Ben Shapiro.

If hypothetically, if I was within the top 15% regarding my intellectual capability, I’m genuinely mastering the science and am able to work my ass off like there’s no tomorrow, how much can I expect to make by the end of the first 5 years after graduating with a bachelors degree?",hfw6qmd,t3_n2n0ax,1633722811.0,False
n2n0ax,"I saw that you mentioned Ben Shapiro. In case some of you don't know, Ben Shapiro is a grifter and a hack. If you find anything he's said compelling, you should keep in mind he also says things like this:

>If you believe that the Jewish state has a right to exist, then you must allow Israel to transfer the Palestinians and the Israeli-Arabs from Judea, Samaria, Gaza and Israel proper. It’s an ugly solution, but it is the only solution... It’s time to stop being squeamish.


*****

^(I'm a bot. My purpose is to counteract online radicalization. You can summon me by tagging thebenshapirobot. Options: sex, patriotism, climate, dumb takes, etc.)



[^More ^About ^Ben ](https://np.reddit.com/r/AuthoritarianMoment/wiki/index) ^| [^Feedback ^& ^Discussion: ^r/AuthoritarianMoment ](https://np.reddit.com/r/AuthoritarianMoment) ^| [^Opt ^Out ](https://np.reddit.com/r/AuthoritarianMoment/comments/olk6r2/click_here_to_optout_of_uthebenshapirobot/)",hfw6rtt,t1_hfw6qmd,1633722825.0,False
n2n0ax,I was being sarcastic,hfw6vhz,t1_hfw6rtt,1633722866.0,False
n2n0ax,"Why won't you debate me?


*****

^(I'm a bot. My purpose is to counteract online radicalization. You can summon me by tagging thebenshapirobot. Options: dumb takes, feminism, civil rights, novel, etc.)



[^More ^About ^Ben ](https://np.reddit.com/r/AuthoritarianMoment/wiki/index) ^| [^Feedback ^& ^Discussion: ^r/AuthoritarianMoment ](https://np.reddit.com/r/AuthoritarianMoment) ^| [^Opt ^Out ](https://np.reddit.com/r/AuthoritarianMoment/comments/olk6r2/click_here_to_optout_of_uthebenshapirobot/)",hfw7rie,t1_hfw6vhz,1633723230.0,False
n2n0ax,"I kind of did debate you. I also think it’s a waste of time to debate a bot, but tbh I agree with you that he sees his perspective on things as the only correct perception.",hfw83lw,t1_hfw7rie,1633723369.0,False
n2n0ax,"*My only real concern is that the women involved -- who apparently require a ""bucket and a mop"" -- get the medical care they require. My doctor wife's differential diagnosis: bacterial vaginosis, yeast infection, or trichomonis.*


 -Ben Shapiro


*****

^(I'm a bot. My purpose is to counteract online radicalization. You can summon me by tagging thebenshapirobot. Options: patriotism, climate, sex, covid, etc.)



[^More ^About ^Ben ](https://np.reddit.com/r/AuthoritarianMoment/wiki/index) ^| [^Feedback ^& ^Discussion: ^r/AuthoritarianMoment ](https://np.reddit.com/r/AuthoritarianMoment) ^| [^Opt ^Out ](https://np.reddit.com/r/AuthoritarianMoment/comments/olk6r2/click_here_to_optout_of_uthebenshapirobot/)",hfw85kw,t1_hfw83lw,1633723391.0,False
n2n0ax,"I'm a senior in HS. I have almost zero experience in comp sci, but I really enjoy coding during ap comp sci. I was planning on applying for an engineering major, but I'm starting to have doubts that I want to. Is it too late to think about picking CS as a major? 
I'm a international student who hopes to go to school in the US. Preferably Tx.",hfzaumy,t3_n2n0ax,1633786642.0,False
n2n0ax,Probably not the best place to ask this but I'm not sure where is: are there bootcamps online for IT rather than just coding?,hg1j9v2,t3_n2n0ax,1633823465.0,False
n2n0ax,"is a computer science degree the same thing as a software engineer degree, I hear they are basically the same thing but in software engineering you learn how to use circuits, and I also had seen a job listing that stated a comp science degree or equivalent doe that mean its possible to apply for a job that requires comp sci with a software engineering degree, is it even called a software engineering degree or does it have another name.",hgb7z42,t3_n2n0ax,1634008561.0,False
n2n0ax,"Hey, so I absolutely cannot find a uni program that I can qualify for. I have done my graduation in management related field (business administration to be exact) then because I was intrested in web development...learnt it after graduation and have 1 year work experience as a web developer. I wish to get postgraduate education in the same tech field but I don't qualify for MS science or computer science programs. Help?",hgdh8xg,t3_n2n0ax,1634057678.0,False
n2n0ax,"Hello!

We offer a Master's in Software Development program that you might be interested in. We accept any US equivalent bachelor's degree with no prior experience necessary. Although, your experience as a web developer will certainly come in handy!

You can learn more on our subreddit r/MIU_MSD and on our website. Feel free to DM us if you have any questions!",hghwirv,t1_hgdh8xg,1634140784.0,False
n2n0ax,So I'm in 12th grade right now and interested on computer programming. What should I try to advance study? TIA,hggvz1k,t3_n2n0ax,1634122688.0,False
n2n0ax,If anyone could direct me to YouTube videos about logic gates and flip-flops for beginners I'll be immensely grateful.,hgiw3gl,t3_n2n0ax,1634155014.0,False
n2n0ax,"Are there programming-related jobs where a medical degree is useful? Im thinking of having a backup plan in case I get burnt out. Which language should I start with?

Prior programming experience was in high school but I’ve forgotten everything. I did pretty well with Java and C++. We tackled stuff from a Cisco certification course and MySQL too.",hgl49e5,t3_n2n0ax,1634196865.0,False
n2n0ax,"What are grades that would give me a good chance in being accepted to a computer science program? I am in grade 9, and am averaging 92 in Math, 94 science, 90 Language Arts, 85 Humanities, 100 in a course option called 'Coding'. Am I on the right track? I code in my free time and love web design as well. I'm not a top level student by any means, so I am a little worried.",hgnc5cf,t3_n2n0ax,1634239506.0,False
n2n0ax,"Should I pursue in Computer Science major even though my passion is art (concept art/digital art)?

I have always been passionate about drawing and games but I don't come from a well-off family, people have been telling me to just pursue what I love but it isn't realistic because of the financial status I'm in and the art industry here doesn't pay well and there's a high chance where I won't be able to pay off my study loan after graduating.  
  
However, I am good with Mathematics, I even work as a private tutor teaching high school mathematics and have a foundation in the engineering stream before, I had to drop out from my first year because I struggled to keep myself up with Physics and Chemistry, my brain doesn't work well with science.  
  
Would it be realistic enough for me to major in Computer Science with a specialization in Game Development in my situation, and perhaps doing art on the side with my free time? As far as I know CS major would bring in a stable career and better pay. I am just worried if I am making a mistake majoring in CS even though my passion is art. I really need some advice on this and would appreciate any kind of comments.",hgseb3c,t3_n2n0ax,1634333201.0,False
n2n0ax,"What are some things I can do to ""jumpstart"" my career or make myself look better to employers in the future? I am a first-year Computer Science student and have been stressed in finding a way to join extracurriculars, looking into creating projects, etc. pretty much things to add to my resume in a rush as I have nothing to show for the past, no extracurriculars, no coding experience/projects, etc. As a result, I'm trying to make up for it now. The problem with this is that I'm trying to look into teaching myself different parts of coding as in school they're only teaching us Python, and making projects, but I have no idea where to start as I've been intimidated by various fields of Computer Science in front-end, back-end, machine learning, and about a dozen other types of this field. Overall, I have no idea where to start and can't get into any extracurriculars that are related to my field as I lack the experience to do so. Any advice would be much appreciated as to where I could start or things I could/should do to help me out in some way as I just feel trapped. Thank you.",hgt7arq,t3_n2n0ax,1634347761.0,False
n2n0ax,I'm new to computer science but I see myself in the future successful with that career but right now I'm about to start college and I'm confused on which specialization should I take between Software engineering or cyber security if anyone can help me make my choice and tell me the difference between both I will be grateful 😄,hgu9f75,t3_n2n0ax,1634373641.0,False
n2n0ax,I'm choosing a degree to do that involves maths and ict. I feel like computer science would be the best option for that. I want to know if computer science is a good option for me if I really have an interest in maths. Any opinion would be much appreciated. Thank you,hgubk7g,t3_n2n0ax,1634375585.0,False
n2n0ax,"I am proficient in Java, and I have experience in Python. Only things I've done in coding are like problem solving, such as leetcode problems. I want to start creating an app but I don't know where to start. What program should I use? What other languages do I have to learn? Can I even use Java? Or do I need to learn a new language? I am currently a high schooler with no experience to creating an app. It will be great if someone can help me out.",hgxp1u0,t3_n2n0ax,1634436800.0,False
n2n0ax,How is the working environment/school environment for a female working in computer science? I heard from various sources that people tend to take women less seriously and might look down on them. Is this still true nowadays?,hh07yj8,t3_n2n0ax,1634491164.0,False
n2n0ax,I'm a non programming background student. I've taken up a project that deals with AIML. What courses and where can I learn from the very basics to the level eligible for employment? If possible in learning order. I know python a little bit.,hh16cuk,t3_n2n0ax,1634505404.0,False
n2n0ax,What is a easiest but reputed online master in computer science? I don't want to take time constrained proctored exams. I want to do assignments or do projects/exams at my own pace. I have 20 years of experience. I am already working at a good position in a fortune 500 company but thinking of taking director roles in the future. My brain is wired to complete projects and not motivated to score in any exams,hh1ezlm,t3_n2n0ax,1634509235.0,False
n2n0ax,"Hello,  
I was just wondering if it is possible to get the same jobs as people with computer science degrees, with a bioinformatics degree. I am unsure of what to go into and I choose my program soon. Any insight is helpful, I have done some searching online, and looked at job openings, but most require a masters in bioinformatics.  
Thanks",hh639i0,t3_n2n0ax,1634599367.0,False
n2n0ax,"Hello fellow coders. I am starting a bootcamp for full stack web development and was wondering if this will help me in the long run for working with data science. I plan to find a job as a web developer after i graduate and take some classes on data science(while still paying off the bootcamp). Do you think all this experience will be worth it in the long run or do you think im wasting my time? I know in a lot of businesses data scientists collaborate with web developers so I was thinking knowing both would make work a lot easier and maybe even help me score a job I wouldn't have been able to get.  
Thank you to anyone who reads this your responses are much appreciated.  
(Im only 21 years old so i figure I have a lot of time to keep learning)",hhcbo5f,t3_n2n0ax,1634721993.0,False
n2n0ax,Is seneca premium any good for gcse computer science?,hhmugwx,t3_n2n0ax,1634918286.0,False
n2n0ax,What knowledge is a must have before applying for undergrad degree. (I've currently started A-levels.),hhrcpbo,t3_n2n0ax,1635009110.0,False
n2n0ax,None,hifvlot,t1_hhrcpbo,1635460665.0,False
n2n0ax,Hey people of Reddit! I recently decided to go back to school for computer programming. Would I get more/better job opportunities with a post-baccalaureate or an associate’s degree?,hhxjwu6,t3_n2n0ax,1635123444.0,False
n2n0ax,"Hi,

I'm very interested in operating systems, viruses, programming, software, and computers in general. 

However, I really don't know exactly what degree that would be. Software engineering? General Computer Science degree? I don't know.

Sorry if this was vague, I'm just highly fascinated by computers but I don't know what direction to go.",hi06utz,t3_n2n0ax,1635180867.0,False
n2n0ax,"I’m a freshmen college student and currently have an assignment to identify an individual working in an occupation you are interested in (Computer Science) and conduct an informational interview with them. (This could be done through messages ) In the interview, I will ask questions and learn, first hand, about the career and the path that led this professional to their current position. Can anyone help me answer some questions about their career in ComputerScience? Like I previously mentioned it can be through pms. ThankYou",hi0f4mw,t3_n2n0ax,1635184257.0,False
n2n0ax,"I have a background in pure mathematics and I'm trying to learn about a few application. I enjoy programming from time to time and figured I could try to learn about statistics for computer science. Do you guys have any recommendation of ""must have"" books on the topic? I know a few good books that are specific to statistics, but I'd like to learn about   
applications for computer science.",hi1dfmx,t3_n2n0ax,1635198148.0,False
n2n0ax,"Evening, I've been fascinated with programming and software development for the past 10 years. I spent my entire twenties working odd jobs just trying to make ends meet and save a little on the side, I did enroll in school a few times over the past 10 years but I never completed my degree. In 2018 I completed a trade for computer information technology, the purpose was for me to get a job in information technology to get my foot in the door. Over the past 3 years since graduation I've had a few jobs since then and only two related to information technology, one which I'm currently working right now that I started during the middle of the pandemic. I've decided I want to pursue computer science, my main focus would be blockchain development. I don't have any children and I'm only looking out for myself right now as it stands, so I have the availability to go back to school. I recently got a part-time job outside of my full-time job that offers tuition assistance so I wouldn't have to worry about coming out of pocket for my school expenses if I took the traditional route of getting a degree. My question is is it too late for me as a 31-year-old to complete a degree in computer science? My goal is to stands right now is to enroll in a full stack development boot camp and the next 60 days. The overall goal is to get a job in blockchain development within the next 2 years, even if it's just entry level to help get my foot in the door. I'm just not sure what exactly I should be focusing on, should I go the route of certifications and dedicate personal time developing my skills as a developer? Or should I take the traditional route and get a bachelor's in computer science? I've been on the fence about how I should proceed for almost the past 2 years, I'm just looking for some inside on what path would be best for me or any suggestions as to how I should approach this. Thank you",hi2w6u0,t3_n2n0ax,1635225190.0,False
n2n0ax,I’ve heard that it is possible to be successful through certifications and independently studying but it’s harder to move up and be promoted without that bachelors degree in computer science,hi8o3uh,t1_hi2w6u0,1635338609.0,False
n2n0ax,"I hope my question could be answered here. I really don't know where to go and post it. I have two questions, pertaining to college and computer science.  


A: Computer science is a hard major. I'll be enrolling in university - part-time - so I can pay down my college loan while completing my degree. Before enrolling, should I study math on my own time to get ready for university math in computer science?

&#x200B;

B: How much math is used in computer science/programming each day in the workforce? I understand it's mostly coding. But do you have to be a math expert to succeed in computer science, programming, coding career at a company in the U.S?",hi52pqo,t3_n2n0ax,1635270208.0,False
n2n0ax,"Math is used in CS in a unique way. Anything you program that uses math will use that ofc but the fundamentals operate in what is known as discrete mathematics. It is a bit different from traditional math because instead of using numbers to portray exact things that exist, it uses signs and symbols to portray concepts that can be used to solve problems.",hifudv1,t1_hi52pqo,1635460115.0,False
n2n0ax,I need advice about what programming language I should start learning. I have no experience in coding and am trying to teach myself but there’s so many options I don’t know where to start,hi5gksw,t3_n2n0ax,1635275668.0,False
n2n0ax,"I asked my computer science department this yesterday and they said they will teach us everything, that’s at Hunter College",hi8nwji,t1_hi5gksw,1635338503.0,False
n2n0ax,Thanks for the advice👍🏽,hia1jfb,t1_hi8nwji,1635359313.0,False
n2n0ax,[deleted],hi7pbk0,t3_n2n0ax,1635312517.0,False
n2n0ax,"yes, take advantage of free coding courses like codecademy.com (i hope i spelled that right) google it, i’ve been using it for a while before switching my major to cs from pre-med",hi8ntys,t1_hi7pbk0,1635338466.0,False
n2n0ax,"How hard is CS, im planning on going back to college and CS is high on my list of choices but is it a course for A students or could an average joe do ok studying it ? Im terrified ill be 2.5 years in and fail and it will have been a complete waste of time for everyone",hi86nqt,t3_n2n0ax,1635326683.0,False
n2n0ax,"don’t think like that! have a growth mindset, not a fixed one. be motivated. i’m starting cs next semester too and i was a little worried about the same thing till I realized it was all a mindset thing.",hi8nouy,t1_hi86nqt,1635338390.0,False
n2n0ax,Is computer science not for me if I can’t think big? I solved a solution once and compared it to someone else and they used fewer lines of code than me.,higkfn4,t3_n2n0ax,1635472148.0,False
n2n0ax,"Fewer lines of code dont always mean better code. If you are able to solve a problem that is a great first step, optimizing that code and refactoring it to make it more efficient is another skill of its own that comes with experience. Do not feel bad if you do not solve a problem better, the fact your able to find a solution to a coding problem is better than getting stuck on it and not having it work at all.",higynjd,t1_higkfn4,1635478897.0,False
n2n0ax,"Im in a community college and I need two lab classes in order to transfer all of my credits to uni. I narrowed down my choices between general physics 1 & 2, or engineering physics 1 & 2. Is one of the two easier to take compared to the other? In terms of other math classes I need linear algebra and discrete mathematics left so I plan on taking discrete math with the first lab course and linear algebra with the second. Will these classes also make a difference in difficulty with one of the physics choices?",higyfkc,t3_n2n0ax,1635478783.0,False
n2n0ax,"Let me preface this by saying that I was a year ahead in math in highschool, and then I decided it would be a good idea to skip ahead another math class. I was completely lost in that math class, and I didn't put any effort into trying to learn the bits that I missed out on learning. 

This led me to grow an aversion towards a subject that I found rather interesting and fun. 

Fast forward to college, where I had to take calculus. I ended up barely passing (on my second time taking the class) but I never really had a deep understanding of what I was doing. That lack of understanding was partly because I, again, didn't put much effort into learning the material. 

I ultimately ended up dropping out of that college.

Fast forward a couple of years to now. I'm studying computer science and I'm actually putting in maximum effort towards my learning. I'm actually interacting with my professors and getting help when I'm stuck on something as opposed to just ignoring it as I have done in the past. 

Classes are going well, and I'm really enjoying the material. 

Even though it isn't required for my major (because I'm taking discrete math instead of calculus) I have a strong feeling that I will someday want to learn calculus as I've heard it's pretty much mandatory for machine learning, a subject I would love to learn. That and the need for calculus in other realms of computer science and science in general, seems rather apparent. 

The thing I'm stuck on trying to figure out, is whether or not I should retake college algebra (as I've forgotten a lot of important aspects of it over the many years I haven't been using it) even though I have a credit for it or if I should go back to trig or just the calculus that I wasn't willing to put the time into. Or whether or not I should do any of this and just focus on my computer science degree.

I've talked with my discrete math professor about this and she suggests not taking college algebra, as it's one of the most failed classes and if I really wanted to, I should just retake trigonometry or differential calculus. 

I feel like I should retake college algebra, because it should mostly just be review and it's basically fundamental for every math class after it. 

Thoughts?",hirzgob,t3_n2n0ax,1635696290.0,False
n2n0ax,hi I just joined this group and I want to ask for advice I am just entering university but in have not yet gotten admission to any school should I wait till i get admission or just start self-learning if that ca you help me with some resources,hitid7b,t3_n2n0ax,1635720096.0,False
n2n0ax,"-5.125 into 8-bit binary with a sign extension?

5.125 in binary : 101.001
In 8 bit : 00101.001
Invert (00101.001) + 1 : 11010111

What am I doing wrong?",hitjnp4,t3_n2n0ax,1635720728.0,False
n2n0ax,"Hi, I am a Computer Science major who graduated 10 years ago. Since graduating, I stopped coding as the job(s) I have held didn’t require me to continue programming and, unfortunately, not stay in touch. I find the world very different and I feel lost in trying to understand how things work. I need your help and guidance on how I can acquaint myself to the host of new paradigms, re-learn programming through one or two launguages, and build applications as hobby projects. 

I used to be familiar with C++/C# and OOP in general. I dabbled with some Python from time-to-time, but it was never serious. My objective is not to find a job or make a career switch. I work in start-ups and I want to be able to have intelligent conversations with engineers, understand & estimate actual technical complexity, and of course, build the hobby apps. 

Any advice would be truly appreciated and welcome. Thank you very much!",hivog9c,t3_n2n0ax,1635772031.0,False
n2n0ax,"Decided to take up Data Science. What are the key things to set myself apart from others and set myself up for success in the classroom (Junior year in uni)? I have fundamental knowledge of MySQL and a couple YT tutorial knowledge of Python, any suggestions?",hixivhk,t3_n2n0ax,1635800664.0,False
n2n0ax,"Should I get a BSCS in a year from WGU (online) or in 2.5yrs from University of Washington  (B&M)? 

33f, flexible work schedule, live near Seattle/Tacoma",hj1a9wu,t3_n2n0ax,1635874123.0,False
n2n0ax,"I need to learn data structures in c++ and some more coding for an app programming class I'm going to take in January, I am planning to take an online coding course. **Would a month be enough for me to get the right amount of knowledge?** I am skipping into app programming when I only took a beginner coding class. My advisor thinks it's risky but I need to do this in order for me to graduate in spring 2023.",hj1wure,t3_n2n0ax,1635882906.0,False
n2n0ax,"i graduate in a year, what is the fastest growing area i should focus on specializing in?",hj5ih8l,t3_n2n0ax,1635950258.0,False
n2n0ax,"How useful is it to have a degree in both CS and Math? I'm not taking math just cause I want to be extra, I genuinely love math. I was just curious how much it would benefit me in the professional world.",hjc9f99,t3_n2n0ax,1636063951.0,False
n2n0ax,"I have decided that I want to become a pentester. The first thing that I want to do is to get a CS degree. But I dont know which area is the one.

Maybe bachelor of science in Networking? or Bachelor of Science in Software Engineering?

I found one program that has the name Bachelor of Science in Computer Security. But i feel that the courses in the program only touches the basics on everything. It doesnt do the deep dive like Bsc in networking does.

I understand of course that no program in the world will cover everything that I need to become a pentester. But which foundation is best to choose?

Thanks",hjg3w2u,t3_n2n0ax,1636136792.0,False
n2n0ax,i'm 15 but i wabt to start a career in programming and hopefully get into computer science i have the simple idea of coding but i never realy got into it what do you recommend i start learning first i want a language that will help me later in computer science. thank you,hjg6pc8,t3_n2n0ax,1636137860.0,False
n2n0ax,"Working on my Bachelors in Computer Science, I should be done in July of 2023 if all goes correct. I'm having some blues lately and I keep coming back to doubting myself in this field. I have an understanding of computers and how they work but I have never written a line of code a day in my life. Am I wasting my time? I fucking love computers, artificial intelligence, and holy shit I love robots. I was even going to go to school part-time after graduation so I could work and get my master's with a specialization in AI and robotics. Am I shooting myself in the foot right now? Is comp sci just a trend that will fizzle out and die? please send me motivation and advice.",hjkae55,t3_n2n0ax,1636214087.0,False
n2n0ax,"Is a MS in computer Science worth it or should I just go for a bachelors in CS?
Just graduated with a degree in Exercise Science and want to get into the CS field. Figured might as well get a degree since I am still young (22) but not sure if I should just go back and get a bachelors in CS or if I should try to get a MS.",hjle31f,t3_n2n0ax,1636231262.0,False
n2n0ax,"Hello Cs reddit! I am asking for advice concerning my future as a Software engineer or the like. Currently I am a Freshman in high-school and am Taking the Ap CS A course, which is the last course offered as an option at my high school. I am considering taking an independent study to take some form of CS course. What course should I take? Depending how far I get in the next 3 years of independent study, is it really worth it to go to college? Or should I go right into coding interviews at 18? Knowing that I want to go into some sort of computer science field, and how far I can go with my life, what is the best course of action? 
Thank You.",hjn7c4d,t3_n2n0ax,1636263832.0,False
n2n0ax,"I'm currently a ""junior"" transfer student majoring in Math. My end career would be becoming a MD. However I want to have a reliable career to fall back on in case I change my mind with med school or whatever the case may be. I'm struggling with choosing whether to keep my major and minor in CS or switch to majoring in CS and then minor in math. Regardless of which I choose, their duration will practically be the same but I'm not sure which is more marketable and the best for me. I chose math because it was a requirement for ever college to satisfy. I was a US immigrant so I stopped pursuing education for a few years which forced me to start my math edu from the very bottom and that led to doing well in math classes so when the time to declare came I naturally chose math.

I was initially interested in actuary but since the time requirement to progress in that job is almost a decade(even though it is high paying from the get go) it did not align with my MD career goal. I then heard about data science but majority of what I've read from articles and personal experiences is that it mostly requires a master's which I'm not interested in pursuing. I am interested in programming and computer science in general but I would consider myself more of an analytical person(study numbers) rather than a creative(make programs/code).

Job-wise, would it be wiser to switch to CS major-math minor? Or will Math major-CS minor do me better since that makes me a little bit ""unique""? I know there are different factors to consider like internships, experiences, etc. but I'm currently a blank slate and part of the reason why I did not major in CS right away was because the community college I was at had terrible CS program and even though I got an A in Intro to Programming(the only CS course I took which is required for math majors), I did not learn a single thing after taking it. Sorry for the lengthy post, I just can't find anyone else to ask.",hjosd3i,t3_n2n0ax,1636301640.0,False
n2n0ax,"Hi y’all! 

I’m a 2nd year music education major at VCUarts and I’ve decided to change my major. Education wasn’t going to give me the financials I needed for my goals in life and, unless you’re living under a rock you know that the public education system is going to shit.

I’m looking at studying computer science and I’ve been doing a lot of research. I have no experience in coding.

I want to know how much having experience in coding matters to starting out in computer science. Can I start out with no coding experience ?

Also the upper math courses are a little intimidating, I’ve taken AP stats but I’ve never taken any sort of calculus. How hard are the math courses required? 

Also",hjqrfui,t3_n2n0ax,1636330378.0,False
n2n0ax,"I decided to study computer science and now I'm in my senior year. I have always loved art and animation as well as maths and nearly everyone I know kept telling me to pursue something art related because I was ""talented"", but there aren't any good art schools/programs in my country anyway.
So I wanted to study animation/computational design at first but it wasn't offered anywhere in my country, I had to choose the next best thing, and between architecture which I wasn't passionate about, and industrial design, which I had no clue where they work but actually seemed fun, and computer science, which seemed like a useful ""all rounder"" degree and would be nice to have and can be applicable to a wide range of fields, I chose computer science. 
Anyways, I liked the theoretical aspects of computer science but I absolutely hate programming. I don't like the prospects of me practicing coding questions on leet code daily, it makes me absolutely hate my life. Is there a more creative field in CS I can pursue? My university used to offer a computer graphics specialization but removed it as soon as I enrolled... We don't even have computer graphics as an offered course anymore. Is having a CS degree in case I wanted to pursue animation in the future useful?",hjrzx64,t3_n2n0ax,1636354466.0,False
n2n0ax,"So currently in my sr year of college. I have a passion for front end (started out as cpe, got a job in web app dev and fell in love) getting my degree in SE. My college does not have a specific ""front end"" degree and all of the higher level courses are all low level programming courses. I was wondering if any one with dyslexia has struggled with assembly the way I have and if any one has any tips. (Failed 2x and struggling this 3rd time still) at this point idk if I can pass and this class is required.  I have entirely lost motivation as iv tried hard to learn yet it just doesn't click. I think due to how abstract it is. I have asked for help from the teacher and dean but it has generaly been ignored and got the feeling they just think I'm lazy ( even though every teacher I have had loves me and I have worked my butt off taking grad classes but can't pass a sophomore class)",hjtnhcd,t3_n2n0ax,1636390813.0,False
n2n0ax,"Are there any degree programs that require computer science as a prerequisite?  My college doesn't let people study it unless it's necessary for their degree, and all the programs I find list it as optional...",hjwmsyb,t3_n2n0ax,1636438250.0,False
n2n0ax,"TLDR: I already have a comfy job in enterprise web app development. Should I pursue a masters degree?

About me: 39yo US citizen. 4 years ago I did a self-taught transition into a career in software development. For the past three years I have been doing web app development for an old, venerable enterprise software company (not FAANG, but close). As far as enterprise jobs go, its very good. Despite being beset by legacy code on all sides, I work mostly in greenfield projects using tools that I know and love, and that are in high demand (React/Redux/GraphQL/AWS/Git). I work with a fully-remote team and have a very flexible schedule. Salary is great, benefits are great, feedback from manager is great, its all great.

But I think about the future a lot and I wonder if I will plateau without a CS masters degree. I dont want to go down a managerial path. I know its ambitious (especially for my age), but if I had to set a target it would be Staff Engineer or Area Principal or something like that.

Now that I am firmly entrenched with my current company, the pace of advancement is slow. Anyone familiar with this career path knows that the quickest way to advance (in title and in salary) is to jump ship. But most jobs that would be ""better"" than my current job would probably be more attainable with a masters. I like the ""N"" in ""FAANG"", and that seems particularly unattainable.

Also not sure I want to stay in web dev. The future is data, machine learning, VR, etc. If I stay complacent, I worry my distant future job opportunities will be ""the only guy at the company who still knows how to get Webpack to build.""

I have a bachelor's degree in a humanities field. I am ""good at school"" (I hesitate to say ""intelligent"" or ""book smart"". Rather, I am disciplined, organized, have good judgment, good time management skills, etc.). I have no debt, not married, no kids, no plans for marriage or kids, no real responsibilities other than work.

So, if you were me, what would you do?

If I dont do a MS, I might get a dog or try to start a band or something. I dunno.",hjxheh9,t3_n2n0ax,1636462222.0,False
n2n0ax,"I am studying a completely different major, but what textbooks would you recommend for students in first year and second year of computer science? I want to learn the theory and then do the practice.",hjy7lul,t3_n2n0ax,1636474383.0,False
n2n0ax,"I want to go back to school to earn my master’s in computer science, I’ve been entertaining it for a couple of years now and I finally want to move forward. My bachelor’s is in education. I have the pathway all figured out, including prerequisites I would have to take. The only thing that gives me pause is my student loan debt will be brought up higher because I will have to borrow more loans in order to afford to go back to school. How much would you say is too much debt that would make it not worth it to go back to school and change careers. I really want to do this, but don’t want to put myself in a hole financially I can’t dig myself out of. Thank you all!",hk3l3q4,t3_n2n0ax,1636569043.0,False
n2n0ax,"I'm struggling to find my way...

  
I started developing for personal fun a few years back. Nothing fancy, just some web stuff. Actually, it made me really frustrated and I quit for a while (a bad habit of mine that it took nearly 25 years to break...) but I've gotten... Decent at it. I can do backend stuff with my eyes closed and I hate frontend with a burning passion.

To me, programming is more computational than some JavaScript web framework, CSS or some basic CRUD database access... It's number crunching, algorithms, it's figuring out how to best use the hardware you have in front of you to accomplish some task that barely fits in the available parameters.

These kinds of things get me excited. Have any of you ready about RICE university's SLIDE algorithm? It's a neural network that beats the same size network run on an NVIDIA Tesla V100 on a CPU! That's nuts!  


Anyways, not much I come across excites me except for the idea of these technologies that I have no use for. I'm like a walking solution with no problem, and it's really kicking my ass lately. I can't stick with a programming language, a project, or anything because, well, the things I want to build and work on I just simply don't have a use for. Part of this is likely depression and my lack of going out and enjoying life so I have nothing to apply my development to, but damn, it's hard battling those two opposing things at once.

I've come here for advice, not to whine, which I've already failed at (look at me go!). I'm looking for help figuring out what direction to take. What thing could I study or look into, what problems can I work on that incorporate my TRUE interests *(high performance computing, parallelism, clusters, super computers, just flat out burning hardware at the highest temp it will go and utilizing as many resources as possible at once. I like BIG stuff that I simply don't have access to*), but also without going to get a master's degree in something I don't *actually* want to study? I can develop neural networks, sure, I could explore data analysis and other ML algorithms, I could probably build image classifiers from scratch. Hell, I could even save up and build a full on Beowulf cluster out of NVIDIA Jetson Nanos and run all of these projects on them, but the sad truth is, all of that is useless and incredibly non-driving if you don't have something to apply it to, which I don't...  


I'm just lost, and looking for someone who will actually read this post and offer some real, actionable and thought out suggestions.",hk7cams,t3_n2n0ax,1636639812.0,False
n2n0ax,What are the best online bootcamps for a beginner?,hka20wq,t3_n2n0ax,1636681751.0,False
n2n0ax,"No experience but wanting to switch fields to CS from business, do I try for masters position or get another bachelors?

Job hunting for the past few months has been miserable, and im just tired.  
 :  / I want to switch to a cs field, and currently have 3 bachelors   
(all  in business), is it better to find a masters program or just go   
back and  pursue a bachelors?  
I pretty much only have SQL knowledge.  
Thank you!",hkdq2h1,t3_n2n0ax,1636751918.0,False
n2n0ax,Why not taking 1 year to learn by yourself and rhen apply to unternship/entry level jobs?,hlku1k6,t1_hkdq2h1,1637540833.0,False
n2n0ax,Received my BS in bio want to do data science.  can anyone tell me if it's better for me to just go straight for my masters in data science or go for a bachelors in CS before doing data science?  thanks,hkigvrc,t3_n2n0ax,1636843764.0,False
n2n0ax,"I am looking for a mentor in computer science.  I am relatively new to the field, but have high aspirations and propensity to learn.  Please reach out if you would be willing to hop on a zoom call.",hklt3vi,t3_n2n0ax,1636908592.0,False
n2n0ax,"Hey guys I’m having mad anxiety here

So basically I’m starting uni tomorrow 

I did not get my first choice which was software engineering but i got applied computing

I have no idea if I’ll be able to pursue my software engineer dream with this diploma especially if I’m from a poor country.

Any insight would be greatly appreciated",hkm13e5,t3_n2n0ax,1636911854.0,False
n2n0ax,"Have been wanting to switch careers for the last year or so. I’ve been looking at doing a MSCS, but don’t have the undergraduate background to get into most programs. Should I try to do an Associate in Comp Science at a community college then seek admission to an MSCS?",hkmp0y2,t3_n2n0ax,1636920800.0,False
n2n0ax,What would you guys recommend for a self taught/boot camp programmer who's just starting out their career to learn the theory/math behind what's going on? I want to learn some of the stuff you uni guys learned.,hknh03s,t3_n2n0ax,1636932051.0,False
n2n0ax,Any tips on how to start networking?,hkqksj1,t3_n2n0ax,1636995007.0,False
n2n0ax,I'm going into college to get my bachelors or higher in computer science. I'm going into Virginia Tech what can i expect from the classes? I feel like my knowledge of the subject is too thin to know what to do.,hkx7xlr,t3_n2n0ax,1637107568.0,False
n2n0ax,"Can I get a job with an Associates and A few Projects? I am A sophomore cs Student(Still in CC)   tried web development for a bit(prodject odin)  I liked working with linux alooot but not so much html,css, JavaScript. I wanted something a bit more like what i was learning in school(Java). I'm thinking  android development but also really intrested in Vr development.

I know im all over the place but bear with me pls, Im exploring but Getting tired but want to commit",hkxpe44,t3_n2n0ax,1637115250.0,False
n2n0ax,"Hello world, I am a middle school CS teacher in my first year. I have 0 background in CS so this semester has been difficult to say the least. I am looking for an unplugged activity that I can do outdoors with my students this Friday. They and me are burnt out from the screens and we need some fresh air. My students are grades 6-8 so I’m looking for something that won’t be to childish for them. If you have any suggestions please send them my way! Thanks!!",hl10wvf,t3_n2n0ax,1637178814.0,False
n2n0ax,"I enjoy the nit-picky things about writing--things like grammar, technical writing, and essay-writing. I've become curious about how that might translate into an enjoyment of and/or aptitude for coding. Admittedly math was never a strong suit of mine, though I did well enough to get by in high school, and I'd be willing to put the work into improving that if I need to. Are there any career paths in CS that could be good for someone like me?",hl2p7v6,t3_n2n0ax,1637205013.0,False
n2n0ax,"I’m 18 and will start college next year. I’m thinking of going into computer science? What will be my possible best career paths from here on? I’m talking about courses, internships, jobs, etc. Thanks!",hl2vgr6,t3_n2n0ax,1637208055.0,False
n2n0ax,"What programming languages should I learn if I want to apply for jobs in computer science industry? Do I need a degree, or can I self learn?",hl2xt88,t3_n2n0ax,1637209254.0,False
n2n0ax,Is it possible to get any training or certificate that may help me get a career in CS without a university degree?,hl7mv20,t3_n2n0ax,1637293692.0,False
n2n0ax,"What are some good free websites to use for learning intermediate level coding? I'm currently a college freshman - so far, I've tried ones like HackerRank and LeetCode, but those quickly became too hard (all I've taken was AP Computer Science A in HS).",hl81pi6,t3_n2n0ax,1637301786.0,False
n2n0ax,"Hello !

Hope I won't make it too long, but here I go. I'm 26 and I finished a master degree in CS and took another year for a master degree in video game programming. I had a 1 year experience in a web/mobile dev company, and right now I'm working in an indie game studio on our first project for the next year.

The thing is, I'm anxious about my future, especially in the video game industry : I feel like I'll be trapped there, without any career progress with an average wage. I also didn't really liked my experience in web dev.

So I'm wondering what other fields of CS I should look into and what skills I should work on for that. I'm thinking about following an engineer path, but it's so wide that I don't really know where I should start searching and what type of jobs I should look for as a ""beginner"".

I'll add that I'm also interested by other science fields, like physics, and innovations. I don't know if there is a demand for people working with research labs or advanced technologies in CS.

I'm kinda lost on my career and my future to be honest.

Thx for reading !",hlatnjv,t3_n2n0ax,1637353691.0,False
n2n0ax,"Hi, I’m new to Reddit and this subreddit I don’t know a lot about comp sci.

If anyone knows any books or websites that’s good for teaching how this tuff works I’d appreciate people telling em about it. 😁

Reposting here coz it was taken down on the original sub reddit",hlbfo8r,t3_n2n0ax,1637362578.0,False
n2n0ax,"My brother is in the military. 
He’s going to pursue a bachelors in computer science. He’s being deployed to Korea at the start of 2022 and he’s thinking about taking the bachelors program from University of Phoenix online so that he can go to school while he’s in Korea is it a good idea for him to do that? Is the University of phoenix a respected school in let’s say the software engineering field?",hlf90sj,t3_n2n0ax,1637438576.0,False
n2n0ax,"Hi,
I am a senior in high school and will be going to college next year. I’m thinking of applying to some CS internships but I know I need some projects under my belt. I know java and am currently taking college in high school. I am currently taking a data structures and algorithms class as well. 

What starter projects would you recommend that would look good on a job resume? I have never actually coded a project except for school projects. Other than that I have coded a website before by watching tutorials and learning as I go. 

Java is my main language but I am currently trying to learn python too. 

Your friend,
Venus",hlh19rr,t3_n2n0ax,1637469609.0,False
n2n0ax,"First off, I'm in high school at the moment and am interested in getting a computer science degree. The problem is that I have very little experience with computer science in general. I've tried a programming course online twice and it had to be abandoned due to other factors. I have about 9 months before admissions. I see a Khan Academy course on computer science and also a Crash Course on YouTube and I'm watching a few videos and starting to dip in. The question is that how do I translate this to something that college admissions would see? Are there any projects in computer science? Do I also need to learn to program (if I understand correctly, programming and science are different disciplines)?  
  
Second question, what does someone with a computer science major do in general? What jobs are open to such a degree?",hlhhmwk,t3_n2n0ax,1637480627.0,False
n2n0ax,"There is a lot to learn about what Computer Science is, but here is a short intro. 
https://m.youtube.com/watch?v=32_gTGL68XI&feature=emb_title

Science is a wide field that includes Computer Science, Math, Physics, Chemistry, Biology, Astronomy, Geology & more. 

You should not select a degree you want to have, but select a job you want to have and then work backwards from there. What degree do you need for the job you picked?

In general, CrashCourse is not a good resource to learn something new. Use that only if you want to review something you already know. Khan Academy is good!

Something that is more beginner friendly and will allow you to build projects and websites that you can show others is freeCodeCamp: 
https://www.freecodecamp.org/",hliii0b,t1_hlhhmwk,1637506036.0,False
n2n0ax,"Thanks, for replying, I didn't think I'd get an answer from this large of a question pile. In general, I'm just interested in science in general. Because I don't want to write research papers and journals, this is the best alternative. Also, I also have an interest that doesn't earn a lot, so in general, any job that earns a living is fine with me. Could I ask another question? Are computer science and computer programming two different disciplines? Should I learn both? Should I learn one by one? At the same time? Thanks.",hlmo13b,t1_hliii0b,1637582284.0,False
n2n0ax,"You can also work in engineering in science. Engineers don't write papers & journals, only researchers do. Engineers study in the faculty of Applied Science: electrical, mechanical, civil, manufacturing, mining, materials, geological, environmental, physics, chemistry, biomedical, aircraft etc. Engineers make good money, but there some of them don't have that many jobs. Others are in demand.

It sounds like you may want to see a career counselor. Is there one at your school? Do you live in the US? Here are some careers you can look at with videos: [https://www.careeronestop.org/Videos/CareerVideos/career-videos.aspx](https://www.careeronestop.org/Videos/CareerVideos/career-videos.aspx)

Many careers are not in demand, so it is important to check that what you select is in demand, and that you will be able to get a job once you graduate. If you are in the US, you can check this here: [https://www.bls.gov/ooh/](https://www.bls.gov/ooh/)

Computer programming is part of computer science. Being a programmer is highly sought out and highly paid, but it's not for everyone. Some people don't find it interesting or maybe too difficult. If you are willing to work hard, you can give it a try. But you don't have to be stuck with it if it's not working for you. There are plenty of alternatives out there.  


For example, if you like working with people, and you like Biology & Chemistry, all jobs in the Healthcare field are highly sought out and paid very well, too.",hlsnwor,t1_hlmo13b,1637690999.0,False
n2n0ax,hi am doing computer science for a gcse and we have been doing only python coding and i would like to become a game developer should i be learning c++ (any feedback would be greatly apreciated),hlrd9n1,t3_n2n0ax,1637670131.0,False
n2n0ax,Im starting soon as an entry level front end Temenos UXP developer. Can anyone tell me if this is going to lead to a lot of job opportunities down the road or is it more of a niche? Is it popular and will stay popular?,hluatg8,t3_n2n0ax,1637715315.0,False
n2n0ax,"Will I be able to do well in Comp Sci if I absolutely sucked at math in HS? I have no problem doing the work and putting in the effort, but my math skills were horrible and I was never really big on Comp Sci and I have absolutely no knowledge of it.",hlxzgn7,t3_n2n0ax,1637784916.0,False
n2n0ax,*Following*,hm41c5o,t1_hlxzgn7,1637896100.0,False
n2n0ax,"I mean, why not put in the effort to get *better* at math? And in some CS jobs, a strong grasp of math is a requirement and less so in others. And speaking personally, I found that I did better in higher-level courses  than I did in lower ones.",hm8ncp3,t1_hlxzgn7,1637988719.0,False
n2n0ax,"I just turned 30 and came back from a deployment overseas so I have access to some tuition benefits. I'm looking to pivot from my job on healthcare (medic in the army) to one in software engineering. I have a bachelor's in financial management.

What would be the best way for me to kick this pivot off? There's a lot to choose from, there's an OSU accelerated computer science degree that looks compelling, there are a million ""bootcamps"" which seem dubious to me, and there are all kinds of online resources that seem like very strong value propositions to me.

Given that my tuition will be completely covered by the army, though, I was hoping to get y'all's feedback. Bachelor's? Self-study? Bootcamps? Other option? My goal is to learn this stuff, not just land a job, but I also don't want to waste time learning stuff that won't be applicable to my job down the road.

Thanks!",hm020ti,t3_n2n0ax,1637818393.0,False
n2n0ax,Following,hmmmwj1,t1_hm020ti,1638249190.0,False
n2n0ax,"I wanna learn everything about a PC and wanna know where to start and where to learn, I assume Hardware be the easier starting point so yeah, No school programs if that can be avoided I don't fined most school education programs work for me their usually to slow and sometimes they end up being out of date and behind on their knowledge",hm3rapc,t3_n2n0ax,1637890777.0,False
n2n0ax,"Need advice.
Is paying for COMPTIA A+, Security+, and Network+ certifications worth it?
I've never worked in the IT field and am considering an offer for $4,000 for training and exam vouchers for all three online",hm54sic,t3_n2n0ax,1637923395.0,False
n2n0ax,"My University offers Cyber security ( with no minor) and Cyber Security Management with a Minor in Data Analytics , which one do you think I should choose ? And is there any difference between Cyber Security and Cyber Security Management? Which will land me a better job ? ( Please keep in mind that I am an international student so I will be required for the H1 Visa) . 
         
And if I choose Cyber Security Management with a Minor in Data Analytics, is there a  benefit in getting a certification in Applied Data Analytics?  Thank you for your time and consideration",hm9r1pp,t3_n2n0ax,1638018709.0,False
n2n0ax,"Hey everyone! I was looking to go to school in computer science. Particularly I want to learn how to work on blockchain technologies. Would you recommend going for a computer science degree initially? Are there any courses or specializations I should look into after getting my basic background in coding? Any help would be appreciated, thank you!",hme50yt,t3_n2n0ax,1638101287.0,False
n2n0ax,"I am currently in a part time position at a good company, but I feel like I do not have enough tasks to do and it is honestly stressing me out. I get less than 10 hours in a week due to the lack of actual actionable tasks that are available for a part time worker. I am part time because I am in school. Should I just apply for the internship program and quit my part time position? The manager has already put in a good word for me. It just sucks not feeling like I am really contributing to anything, and I wanted to do some research-adjacent things next semester anyway.",hmknjxi,t3_n2n0ax,1638217376.0,False
n2n0ax,"I know this is going to sound stupid, but I'm a high school senior and I'm in the middle of AP Comp Sci Principles and I've also taken a course about it at a local community college, but I feel shaky about the subject and I'm definitely interested about it and I would like to major in it, but other than those 2 classes I don't have much experience and I plan to start taking lessons at Khan Academy and from freeCodeCamp. I'm worried that I'm a bit behind in taking a CS major is there still hope for me?",hmmxqnx,t3_n2n0ax,1638256196.0,False
n2n0ax,"Hi all! 

I have a question about graduate programs and essentially what is the best way to get to where I want to be. I currently don’t have any formal education in computer science, so I have to be selective in how I go about this. Right now I’m leaning more toward software engineering. I have been learning to code for a few weeks now and I absolutely love it. 

Side note-In college I had a 3.27 and graduated with a communications degree. I have not taken Calculus yet-don’t know if I would need to-but I can take it if needed. 

First Option: UPenn MCIT Online Masters degree-Would start in Fall 2022
*Low acceptance rate
*Well regarded program 
*Would have to pay for it


Second Option: Cyber Technology Fundamentals Graduate Level certificate-Would start this spring
*Should have no issue getting in 
*Gives me the option of having automatic admission to a MS in cyber security management or IT Management
*Classes in IT Fundamentals, App Dev Fundamentals, Networks and Systems, Databases, Data Analysis and Data Structures, Cybersecurity and Information security, and enterprise IT. 
*I have a tuition waiver that would cover the cost
*Would take about 3 semesters to do
*Unsure if it would give me enough pre-requisite courses for a MS in Computer Science if that is the route I want to go

I’m really at a fork in the road, and I would love some help!",hmpazez,t3_n2n0ax,1638302024.0,False
n2n0ax,"Hey all, I’m trying to get some advice as far as a career path goes. Currently I’m working a full-time job as an aircraft mechanic (prior military, so i transferred right into a civilian job, don’t want to turn wrenches forever). As well as in college for a computer networks and cybersecurity degree (still doing gen ed classes). I know I need hands on experience and I’d like to get some done while I’m in school that way I don’t take 4 years getting a degree and still have to start from scratch. 

As of now, I have 0 experience in the IT field. I’ve built one PC and spend a lot of time on my PC, but nothing more than that. The field I’ve been looking into is networking, which I’ve heard is a broad one which encompasses a few other things. I would also be interested in cybersecurity but I’ve also heard it requires networking knowledge to even get into that. 

Clearly, I have no idea what I’m doing. I’ve looked online for career paths which suggest help desk positions but without some training I wouldn’t have any idea how to help anyone (do they train you for these jobs?) I have income aside from my current full-time job so taking a lower paying job for experience is something I’m willing to do if I gain valuable experience. 

I feel like I’ve rambled a little here so I’ll ask it a little more straightforward. How should a person with 0 IT experience (no certs, no degrees, not self taught) start a career in networking? 

One last thing, as far as self teaching goes, what would you recommend I do to gain some knowledge in the field? Anything helps. Books, videos, courses, whatever it is. 

Thank you!",hmqgmph,t3_n2n0ax,1638319477.0,False
n2n0ax,Hello so I’m 22 and want to enter the programming world and honestly I’ve been starting to learn python as my first language and prior to that I have no experience with syntax but I feel so lost on so many things like where I want to work in the industry(which field) steps to get there as I’m not in college as well as how to become better recognized without it if someone could advise me even just a little I’d appreciate it,hmqll6c,t3_n2n0ax,1638321699.0,False
n2n0ax,"So I finished my bachelors (not related to comp sci) a few months ago and computer science has sparked my interest for a long time, but at the same time I don't know anything technical or practical about it. But now that I have the opportunity to maybe do a masters in comp sci, I don't know if it's right for me.  
 So as a starting point, based on your experience what language do you recommend for a beginner and what type of class/course would help me get a glimpse of how comp sci is? :)",hmraxk6,t3_n2n0ax,1638333450.0,False
n2n0ax,I'm a first year medical student and considering a career change. Would it be wrong to consider computer science? I'm currently looking up videos on the basics of programming and javascript to see if I like it. Are there any other videos/general things I should consider about this field in the meantime?,hmrbes6,t3_n2n0ax,1638333694.0,False
n2n0ax,"One thing I didn’t keep in mind was how competitive the job market could be after graduation. While software engineers are in demand, lots of people are probably applying. Is it hard to get a job after grad?",hmwsxlo,t3_n2n0ax,1638434663.0,False
n2n0ax,"Hi! Newbie here wanting to start in the cs field afresh.

I want to have a really solid foundation over the core concepts in cs, so which one would be preferable, CS50 or CS61A from Berkeley?

Any advise would be appreciated!",hmx62ij,t3_n2n0ax,1638445518.0,False
n2n0ax,"On my 3rd year as a BSCS student, I still have no idea what path should I go on for my future career.",hmx6rvx,t3_n2n0ax,1638446017.0,False
n2n0ax,"I am a 3rd year BSCS student, I still dont know what path should I go for on my future career. On what things I should be studying extra right now",hmx6wkj,t3_n2n0ax,1638446108.0,False
n2n0ax,How flexible are most boot camps as far as the schedule? Would working full time and doing a boot camp be to much? I’m not able to work anything less than 40 hours unfortunately.,hmxc988,t3_n2n0ax,1638449546.0,False
n2n0ax,"I’m working full time in a factory making $60k+ but destroying my body. Any hopes on finding an entry level tech job making $60k+ with just an associate degree? My plan is get a cybersecurity applied science associate, hopefully find an entry level tech job and transfer to a university while getting my bachelors in CS.",hmxd456,t3_n2n0ax,1638450037.0,False
n2n0ax,I'm not good at maths. I'm like an average student of a class. can i study computer science in college?,hmxyjr7,t3_n2n0ax,1638459991.0,False
n2n0ax,"i am a mechanical engineering undergrad student, sophomore year. i am genuinely interested in computer science and curious about concepts. things like what is a computer, what is software, hardware; how do things work? also history of computer science. i am not really interested in programming, i took c# lectures (which at first time i failed FF then at summer school AA, still proud only because total 6 people did this in two terms, anyways) in the past but got no further interest to produce. just want to learn conceptual stuff in somewhat detailed way, i would prefer books but i am also open to other sources. but books being ordered comforts me rather than roaming internet. what would you advise me to do?",hmyhurt,t3_n2n0ax,1638467481.0,False
n2n0ax,"What are some computer science jobs one could find in the environmental/conservationist/animal protection fields? 

Ive been realizing I really don't care about working for a company that just creates business or medical products. I'd like to work somewhere that researches or benefits the environment or animals in some way, but I'm not sure where to look. Maybe some sort of data analyst job? Thanks!",hmyioz8,t3_n2n0ax,1638467799.0,False
n2n0ax,"So I don’t know if anyone is still active on this thread but I have a few questions

I got interested in code after seeing some work and understanding that behind everything is a code or a program to make it work and since then I wanted to do it to. I’m currently a senior in High School and I’m about to go to college, I’ve decided to major in CS, but other that that I don’t know what else to do, so does anyone have suggestions of things I can learn in the meantime, what types of things to learn where to start, how to learn where to start, what to choose as my future career for it, where to code for beginners things like that. I just wanna know where to start off right now and what things I should be doing",hn8jug9,t3_n2n0ax,1638650323.0,False
n2n0ax,"Im in a bit of a tough spot right now knowing my place in the universe...

I'm thinking of going to school for computers. What does a BS of Computer Science contain/entail? What kind of jobs can I do with that degree?

I have no idea what specifically I want to do with computers - will going to school for computer science help me find my niche (ie. Data analysis, game development, software engineering, etc...)?

What is the best schooling to do that: low tuition, online, self-paced, rigorous/easy, etc..? I've been thinking about WGU because they are super flexible/all online but they are not ABET accredited.

Does ABET accreditation matter? What does it matter for and what doesn't it matter for? What happens If I get my degree from a school that's not ABET accredited? 

Is there a way to know or to try out if I even like this kind of material enough to get my degree in it?

Thanks in advanced! Answers to any of the questions if not all of them are all appreciated. I would love to hear multiple perspectives too 😁",hnejq2g,t3_n2n0ax,1638754690.0,False
n2n0ax,"I am a non traditional student returning to school after 6years  to upgrade my A.S. in General Studies to a B.S. in Computer Science. My first classes begin January 3rd but I am anxious and would like to be doing something proactive in the mean time...
I'm looking for recommendations on resources, be it a book or a podcast or a website or even a YouTube channel, ANYTHING, that might help a complete novice feel a bit more prepared on that first day of class...
THANKS in advance!",hniw8pd,t3_n2n0ax,1638833647.0,False
n2n0ax,"Can't think of any project ideas to help reinforce concepts and generally just continue practicing. I'm only first semester to maybe I don't have enough of a foundation yet as we just started touching on classes and while I understand inheritance a little bit we haven't got there yet. 

I was considering readying the C# textbook in further detail or something but I'm not sure.",hnkbxku,t3_n2n0ax,1638858279.0,False
n2n0ax,"Advice on educational path?

I am currently a Junior in College studying data science. I am thinking about doing a double major in Computer Science because I like the problem solving involved with programming and I think that career would keep me more engaged as a person with ADHD. I also believe having more programming experience with my Math and Data Science degree would open more doors for me incase I decide I want to follow a different career path later on.

The issue I have is at my school most of the course work for the CS degree is low level programming languages and people I know in that major believe our school is not great for CS. I also want to move back home because I miss it. 

Are there online learning options people would recommend for me to learn enough to be able to pursue Software Engineering? Any other advice is also appreciated, thank you!",hnmtbr6,t3_n2n0ax,1638908353.0,False
n2n0ax,Is there a thing like domestic computing as field of study?,hnnweva,t3_n2n0ax,1638925795.0,False
n2n0ax,"I’m considering taking certifications in cybersecurity that will only take 10 months, is it worth it to continue on with my computer science bachelors degree? I just finished my GE and pre requisites to transfer to a 4-year. The cybersecurity program is tempting…",hnobieh,t3_n2n0ax,1638933423.0,False
n2n0ax,I’m in my second year of my comp sci major and I’ve been working at my dads party store throughout college so it’s made me neglect school quite a bit. I feel so insanely behind now that it’s insane. I feel like I can never catch up. Anyone have any tips on what I should do or how I can try to catch up on my own? I feel like after two years of classes I know practically nothing about coding. What do I do?,hnss2tj,t3_n2n0ax,1639015072.0,False
n2n0ax,"I'm a high school student who just finished AP Cs and would like to know what online courses I could take on my own. I studied python and C and covered pretty much similar concepts in them.  
  
I am not sure if i want to major comp sci but I would like to have a good foundation for most computer science fields. some of my interests are : web/app design , A.I. but have no experience in any of them.",hntffgy,t3_n2n0ax,1639026356.0,False
n2n0ax,"Hello everyone,

I'm currently an electrical engineer and i'm not so please with my daily tasks. (Too much drawing for my taste and not enough problem solving. (And other thing that I don't necessarily want to tell here))

I was told that I could change path to a computer science job by doing a master in that field. I've look it up and a master's in AI or in cyber security look interesting.

So, I was wondering what are the day to day task that people are doing in those fields? (I see the big picture, but the daily work can be quite different, as it is with my current job)

 If any of you did that switch in career, how did it went? Do you have any tips for me to ease that transition?

Ps. Sorry for my bad english, it's not my first language.",hntko77,t3_n2n0ax,1639029479.0,False
n2n0ax,"Programming career is great, I studied it",hnuafhh,t3_n2n0ax,1639049946.0,False
n2n0ax,"I'm 24 with 2 kids and severe ADHD. Medicated but still pretty hard to function at times. I really love the field and am interested in pursuing a degree. Can anyone link me to some resources where I can really study and prepare and practice for a long while before I commit to paying for school?

The required math and languages I should first learn would be most helpful. I dropped out of high school (got my GED but still) so I'm concerned about my ability to do what it takes to get a degree. I really want to throw myself into some free courses for now to both prepare myself and prove to myself that I can do it. TIA!",hnvu4hv,t3_n2n0ax,1639075358.0,False
n2n0ax,"I have an HR Bachelors degree, but have worked in software training for 3 years and have gotten to work a bit with developers. I’m now super interested in learning CS. I think I would have to go back to square one and get a Bachelors in CS. 
 Does anyone have experience with this?",ho01ss7,t3_n2n0ax,1639152673.0,False
n2n0ax,"Guys, I'm panicking. I hope this is ok to post here. I'm one of the last to sign up due to financial/scheduling issues, but I'm going back to school!! Advisors are cramming w/ multiple appts of other students like me, & I have to have everything scheduled & paid for by Thursday (although Spring tuition may be free, waiting to hear). Thank you for any advice!

**About me:**

* 32 yo female
* No CS experience or knowledge, only basic computer skills
* Currently have an Associates in Science 2015 from a community college, yet work in Accounting (lol)
   * thankfully degree takes care of a good portion of unrelated CS classes needed for CS degree like English, electives, etc

**Career goals:**

* Atm I'm aiming towards Cybersecurity, but I want to get my feet wet in the CS world (my goal may change if I enjoy something better)

**Questions:**

* OVERALL: Whether it directly goes toward my CS degree or not, what classes would you recommend beginning with for your average computer Joe?
   * Surprisingly, CPT 101 - Intro to Comp isn't a requirement. Is this something I should take w/ my minimal knowledge?
* Would it be better to learn this on my own until classes start on Jan 10?
* If my tuition is covered for Spring, should I take the max classes since I'm assuming the entry level classes will be easier than more advanced classes? More bank for my buck?
* If I take 4 classes for my CS journey, what 4 generic classes would you recommend to a noob whether they are CS related or not to be the most helpful for advancing my knowledge?

**Helpful links:**

* [My classes required for Associates](https://imgur.com/a/1vOArZf) (customized w/ what classes I've already taken)
* [Classes left to take for Associates](https://imgur.com/a/GjARYtm)

**Disclaimer:**

* I may only be taking courses at the community college to get into a 4-year, may not aim for an Associates.
* I've heard Python is helpful towards CyberSecurity. I'm also interested in Programming for personal use.

I know you are strangers on the internet, but I'm already doing this by myself & would love some advice based on your experience. Thank you so much again!",ho5d3oe,t3_n2n0ax,1639247037.0,False
n2n0ax,"Hello! I’m making a career change from food service to CS and am looking for any input or recommendations in choosing a program to enroll in. Please let me know if there’s a better place to post this. 

The vocational college I’ve been accepted to is offering fast-track programs (start in the spring and be done by the fall) and I’m trying to decide between two of the programs as a starting point:

A) Computer Diagnostics and CompTia A+ Certification Preparation, Certificate of Completion
- The computer diagnostics and CompTia A+ certification preparation certificate provides students with the skills necessary to pass the CompTia A+ Certification and to perform basic computer diagnostics and maintenance operations.

Or

B) Computer Networking and CompTia Network + Certification Preparation, Certificate of Completion
- The Computer Networking and CompTia Network+ Certification Preparation provides students with the skills necessary to pass the CompTia Network+ Certification and to perform basic computer diagnostics and networking operations.

My goal is to get my foot in the door starting a career in the IT field while I continue to learn. For someone who is just starting out and wants to hit the ground running, would you recommend one over the other in terms of usefulness in real world applications?

Any input is much appreciated. Thank you!",ho7ftcq,t3_n2n0ax,1639281477.0,False
n2n0ax,"If the thread is still alive. I have a Question. 

I want to do something different. Learn something new.maybe self teach. But I havent the foggiest of all these different languages and what they are for and where do I start. Im happy to read alot about it all. Before I start going into a certain one properly.  Is there anyway or any books thays cover the overview basics so I can decide which one. 
Thankyou in advance. 

By the way my knowledge is minimal if any at all in this. I want to start somewhere and hopefully self teach.",hodm61r,t3_n2n0ax,1639403182.0,False
n2n0ax,"Hello! I am interested in pursuing a degree in computer science, but I’m still on the fence about it. I currently work in digital forensics and love it, but I feel like I would have more options in relocating with a job in computer science. 

I haven’t taken a math class in 7 years and I don’t really know where to start.. any advice would be appreciated",hoej2kt,t3_n2n0ax,1639417164.0,False
n2n0ax,Ned help with a question!! Anyone willing to help me out? It’s a cs programming and problem solving class. Functions.,hoekemi,t3_n2n0ax,1639417691.0,False
n2n0ax,"I'm 29 and looking for a career change. I have my bachelor's and masters in piano performance, but that's a whole other story. I've been looking at the cybersecurity program at Kansas State University. It's 10 months long, online and the admissions team says they work with you to help get your first job. The whole program comes to about $15,670 USD. Is this worth it? Has anyone been through this program?",hoevs2w,t3_n2n0ax,1639422197.0,False
n2n0ax,I am leaving for college soon with an interest in electrical engineering but I am thinking about majoring in computer science. I have been around computers for a very long time but have minimal experience writing code. Would I be okay to peruse a computer science degree with no former programming experience?,hos001z,t3_n2n0ax,1639661202.0,False
n2n0ax,"Hey guys, just wondering. Is it possible to score an internship within the first year of a comp sci degree?",hot4opq,t3_n2n0ax,1639678073.0,False
n2n0ax,"Hello ladies and gentlemen, Out of curiousity, I'd like to ask before pulling the trigger on anything. I am trying to just get my foot in the door on any sort of IT profession. I'm 29 now and feel like I'm starting almost too late, I've started a software development degree at a uni a few years ago but 1. I can't afford it 2. ADHD makes traditional school very difficult for me. So I have been looking into just taking the CompTIA A+ 1001 and 1002 respectively. My questions are 1. Is there any online course that anyone could recommend that would be thorough enough that after completion I would be prepared for said exams? (The reviews of CompTIA's ""cert master course"" look bleak for almost $500) 2. Am I going about this all wrong. 
Thank you in advance!",hoygs3v,t3_n2n0ax,1639769478.0,False
n2n0ax,"I just took an online quiz to help narrow you interests and find compatible careers. My second top one was a computer science teacher for college. It's an idea that interested and surprised me greatly. So, I'm here doing some research. No need to answer all my questions just what you feel qualified to answer if perfectly fine.

What kind of education would I need?
Are there steps to get there? ie: teachers assistant, work experience, etc.
Is it a hard field to get in?
Are there any pitfalls I should be made aware of?
To get a job would I most likely need to move?
Any other helpful advice?

Thanks in advance! ❤ It really means a lot!!",hoziqjn,t3_n2n0ax,1639785184.0,False
n2n0ax,I am Senior seeking to major in computer science and want to become a product designer of computer software or something else of the field. What would you advise for the more interdisplinary worker for entrepreneurship.,hp2scum,t3_n2n0ax,1639853462.0,False
n2n0ax,"I just (probably) failed a CS course at my college for the first time. I know that I need to start programming more on my own and getting interested in the subject but don't really know where to start, both with getting interested and programming on my own. Anyone know where I could start looking?",hpb4vaq,t3_n2n0ax,1640017088.0,False
n2n0ax,"Hi all,  
  
I am a student in my final semester having a very hard time deciding between Data Mining class and Parallel Programming class. I know that I want to work in a field related to AI but I don't know if I want to be on applications side or data scientist side of things.  
  
Data mining seems like a crucial course for AI including topics such as pattern mining, clustering and Bayesian models. Whereas parallel programming involves writing efficient code and utilizing cache etc. which seems good to know for every programmer. I can also gain more exposure to C++ since I'm most familiar with Python and R only. What do you all think?",hpj0etg,t3_n2n0ax,1640154588.0,False
n2n0ax,"I’ve been out of highschool for 3 years job hopping from places like insurance to a basic apprenticeship job, I’ve never applied myself in school and got about average grades. (Most B’s and 3 C’s in regular classes) I was looking at a CS or CIS degree mainly the second because my girlfriend says CS is extremely difficult and she took all AP classes in highschool, so I’m worried the workload coupled with the difficulty of the classes will make me more likely to fail. My girlfriend just graduated with a bachelors in CS, and she says its hard work but anyone is able to do it. I know I have the potential to succeed my own expectations even though I’ve been too afraid to before, but I want to know how I can prepare and how people who received CS degrees or those who are excelling in college right now find their success with studying, retaining info, and coding. I’ve never coded before either, but I am an analytical person and very methodical.",hplnuv3,t3_n2n0ax,1640205884.0,False
n2n0ax,"I'm a college student interested in CS but if I enter the tech industry, I want to do it by working at startups (NOT through FAANG/big tech). How beneficial would it really be for me to get a CS degree as opposed to something like Math/Stats/Econ?",hpmbeor,t3_n2n0ax,1640215897.0,False
n2n0ax,Are there any work/internship opportunities for computer science sophomores? In my program we have been exclusively using C++ though. Our tests are effectively programming problems which we have to solve in less than an hour and make sure they're perfectly debugged. I was told they did it this way to prepare for job interviews in the future.,hpn4t8n,t3_n2n0ax,1640229922.0,False
n2n0ax,"Hi guys, I am not really new to computer science or programming. I am currently in the last semester of my education and I have to come up with a project that I will do on my own (under the supervision of my professor). This project should help me with writing my bachelor thesis, therefore it should be something ""measurable"" that can help me answer a specific scientific question? Unfortunately, I have no idea what to do and I was wondering if somebody has an idea, or has had the same problem like me and how he/she found the right project/thesis idea?

Thanks in advance.",hpo4xzp,t3_n2n0ax,1640257199.0,False
n2n0ax,"How do you get started with computer science? Are there any good fundamentals online, or perhaps a (couple of) languages I should learn first?",hpqewzt,t3_n2n0ax,1640296343.0,False
n2n0ax,"Is a CS degree from western governor’s university worth it? It’s all online. I have a business bachelors already, but am having heart burn about the price to go back to school ($400-$600/credit)",hprygcq,t3_n2n0ax,1640323428.0,False
n2n0ax,Thoughts on CS degree vs software programming? Both bachelors,hpryjjc,t3_n2n0ax,1640323480.0,False
n2n0ax,"What are some good resources to learn about AI, neural networks and computer vision? Been really interested about these topics",hps3cm5,t3_n2n0ax,1640326440.0,False
n2n0ax,Can anyone recommend masters programs for comp sci that aren’t too difficult to get in? I graduated with a gpa lower than 3.0 in a non comp sci degree. I dont think i have recommendations either,hpstawh,t3_n2n0ax,1640347240.0,False
n2n0ax,"We have thousands of original Windows 10 and Office 2019 keys for a great price with a 100% guarantee.  
.  
Go to this link 👇 to view our shops.  
https://linktr.ee/softech.shop",hq1zmrr,t3_n2n0ax,1640545440.0,False
n2n0ax,"Hello. I'm in my 30's. I want to get into programming. I didn't study very well at school, so math is a quite a problem for me. The question is: Does it worth to waste my time and learn Python, or I'm already too old cause I need to start with math, and Python without math won't work properly, and even if I learn programming, wihtout math I will always be a low level programmer?   
Thank you in advance.",hq31hm2,t3_n2n0ax,1640563174.0,False
n2n0ax,"Hello guys, I hope you are all  okay. I am new to the computer world and I was just wondering if there are any specific things to learn?",hq52ry7,t3_n2n0ax,1640608788.0,False
n2n0ax,"Wanting to finish my comp sci degree. My uncle who used to work in IT told me I should get certifications but I’m unsure what certifications are worth getting and would love to hear people’s opinions on what’s worth getting or if certifications are worth it at all. TIA!

For reference, I’m finishing up my associates in business management right now and then transferring those credits to a dual business/comp sci degree.",hq8ls70,t3_n2n0ax,1640665489.0,False
n2n0ax,Can I learn equivalent to a CS degree just by taking online courses from websites like edx and Coursera?,hqahsyl,t3_n2n0ax,1640708008.0,False
n2n0ax,"
Hey guys, so I’m actually just going to start my first introduction course to computer science. For someone who was decent at math in high school, will this course be a little more difficult than I should expect ? Also what are some careers I can find with a computer science degree? Sorry if no sound clueless; I just know that computers will always be the future and have a high demand for workers",hqcgz0u,t3_n2n0ax,1640737052.0,False
n2n0ax,"I am a nurse and no surprise, it's been an absolute shit show the last two years. I want to transition out of healthcare and into tech/computers. I thought there may be a need for someone who has a strong background in healthcare that can combine healthcare knowledge in tech knowledge together. My question... is there a certification or a few classes that I can take that could help me break into the industry? This is all very new to me.",hqr7c6b,t3_n2n0ax,1641001030.0,False
n2n0ax,How important is a Degree to landing a job?,hqx7ddr,t3_n2n0ax,1641120481.0,False
n2n0ax,"Is it better to learn based on tutorials or try and find a tutor? Right now I’m not in college, and my school doesn’t have any computer science classes that I can take, so I’ve resorted to self learning (mainly doing bootcamps on Udemy) But recently I was wondering, would it be better to stick with video learning courses, or try to find an in-person tutor / after-school class. While I can do online courses on my own time, and can look up questions, would it be better to have a person explain and talk through the material with me?",hr45ev7,t3_n2n0ax,1641239993.0,False
n2n0ax,"
I just finished my first semester of community college as a criminal justice major hoping to go into law enforcement. Although I realized that maybe that is not the best career for me and I am switching my major to computer science, get my associates from cc, and then transfer to a university to get my bachelors. I’m new to the world of computer science. Are there any jobs out in the computer science field  for students to get who are currently getting to school? If so, let me know because I’d love to get an entry level job that’ll not only help me with learning different coding languages but also get my career started!",hr6tb52,t3_n2n0ax,1641283200.0,False
n2n0ax,"Hi there! This looks like a pretty old thread, so I may be out of line commenting, but I figured I’d try.

I’m currently 5 years into a publishing career as an editor. I have two degrees in English Literature and Japanese. However, I can barely afford to scrape by. I’ve always had an interest in computers, and I’m wondering which direction I could go in to potentially get started in a CS related career. I’ve heard mixed opinions on boot camps, and given that I’m full-time I can’t exactly drop it all and do a full CS degree (though I could pursue online courses if they were flexible). Thoughts? Recommendations? All kindly appreciated.

For whatever it’s worth, I’m a 25 year old female living in Colorado.

Thank you!",hr7r8pa,t3_n2n0ax,1641305862.0,False
n2n0ax,"I’m currently working on a 2 year degree in Engineering Technology, but I want to obtain my bachelors after completing my associates. The issue I’m having is narrowing down a field of study! There are so many and it is a bit overwhelming, I prefer a position where I can work remotely, but I’m not sure what major is the best option. Any suggestions?",hrdb4xj,t3_n2n0ax,1641399428.0,False
n2n0ax,"Hello Everyone,  
  
I am a Civil Eng graduate and I work full time, 31 y old, Canada. I have no energy to deal with this industry anymore, the hours are long, the salaries are low and the work conditions will take a while to change. Work from home for example, it will take at least until the previous generation in top positions start retiring.  
  
That being said, I am looking for the quickest way to find me job in the IT field. I love computers, I have been around them since a young age. The smell of a powered up motherboard brings me joy, probably from the golden age of LAN houses playing CS and Warcraft.  
  
I have a basic knowledge of VBA, Python and SQL. I mean very basic. VBA is the one I know the most since I always try to write a script for things just to practice.  
  
Could anyone give me some advice, please?  
  
Thank,",hrj8ioz,t3_n2n0ax,1641498580.0,False
n2n0ax,"Hello,

I'm a math teacher enrolled in a Ph. D. program for Curriculum, Instruction and the Science of Learning. I have an original idea for an interactive math website, but I have zero web development/coding skills.

I'm interested in learning how to create my own website but I don't know where to truly start. I have been using the Mimo iPad application to start learning HTML with Codecademy sprinkled in.  I also started using DataCamp iPad application for Python Basics. Is this a good start? Should I be looking elsewhere? (I must stress that I prefer learning for free).

Suppose I am on the right track, where can I begin to practice these skills? Such as, what websites are out there to create this website I am imagining?

Are there any employers (USA) out there that hire people without coding/web development knowledge and teach them? (I'm assuming that's a huge stretch).

Any help, guidance, advice or positive conversation is appreciated.

Thank you!",hrknaim,t3_n2n0ax,1641517893.0,False
n2n0ax,"Hello, I am rather new to the computer science field. I took various courses at my local community college last semester to prepare me for a master’s degree in the spring. I have been accepted to Steven’s Institute of Technology Online Master’s in Computer Science and Case Western Reserve University Online Master’s in Computer Science. I am having a really tough time choosing. I understand CWRU is rather prestigious, which has its pros and cons considering I’m trying yo transition and I feel as though I might not fit into their “bubble” of the typical CS master’s student. The program is also approximately three years taking two courses each semester. Steven’s on the other hand, their program is approximately 1.5 years taking 2 courses each semester. Any advice is TRULY appreciated. Thank you so much.",hro43sj,t3_n2n0ax,1641580046.0,False
n2n0ax,I’m 24 and looking for a career change. I just started applying for an associates program in CS and I was wondering how realistic it would be for me to get a job with just an associates degree. Any advice would be greatly appreciated.,hro86ph,t3_n2n0ax,1641581519.0,False
n2n0ax,"Already a bachelor degree in another field, so hesi stand to go back for another degree. Is it really possible to get a job at FANNG with just a boot camp or self taught? Which one would you recommend?",hrr9uyo,t3_n2n0ax,1641632046.0,False
n2n0ax,I am a CS student. Pursuing 2nd year of my BTech. How to get a remote internship?,hrrtbfb,t3_n2n0ax,1641646726.0,False
n2n0ax,"Looking for a book that covers the very basics of computer science. 

Most of the book that keep coming up are about coding in specific languages but I'm looking for a beginner friendly book that covers the very basics. 

Recommendations?",hrwbvl1,t3_n2n0ax,1641722762.0,False
n2n0ax,What do you enjoy most about computer science?,hs0s4tx,t3_n2n0ax,1641790309.0,False
n2n0ax,"Hello folks,  
  
I am currently facing a huge dilemma regarding the course that I would take for college. I am currently undecided as whether to take Computer Science or Entertainment and Multimedia Computing (Game dev) for a 4-year course. I have no one to ask, so I thought of asking on reddit.  
  
I literally have no experience in coding, game development, game design or whatsoever, although I am familiar with computers in general. I suck at math, but I guess I have no choice but not to suck at it. Whatever code I may write during whichever course I will take, will literally be the first one.  
  
In addition, I live in the Philippines and my decision as of the moment lean towards Computer Science as I think it provides me many opportunities down the line, especially if my game development career doesn't really take off. But, feel free to convince me otherwise.  
  
For reference, this is my school's curriculum in regards to both courses.  
  
https://mcm.edu.ph/wp-content/uploads/2020/11/EMC-2020.pdf - EMC / Game Dev  
  
https://mcm.edu.ph/wp-content/uploads/2020/11/CS-2020.pdf - Computer Science  
  
Any input is much needed and highly appreciated!",hs1vmji,t3_n2n0ax,1641815424.0,False
n2n0ax,"Is a math minor worth it? May also go for a digital forensics minor, wondering what type of job the forensics minor would be the best for? Any response appreciated thanks!",hs4ek16,t3_n2n0ax,1641851967.0,False
n2n0ax,"Hi! I'm in community college and until I'm done, I'll be staying with my parents. I recently sold my old computer to buy a new one, (it was acting a bit weird) and only have a bit left until I can afford it. In the meantime, I'm using the family computer. I just recently decided I wanted to go into the comp. science field and I don't know how to install JavaScript onto Mac terminal. Help pls???",hs4pt49,t3_n2n0ax,1641856375.0,False
n2n0ax,Hey so I am 25 I have been in the navy for almost 7 years and when I get out it will be 10. I am an electronics technician and have worked in so IT  environments and would like to start a CS degree. Am I crazy? I haven’t been in school since high school.,hs5qz73,t3_n2n0ax,1641871774.0,False
n2n0ax,"Didn’t get into any Computer Science courses in high school but interested in going into the field in university because I’m pretty good at math and science. Will I get blindsided if I have no experience with computer science, or are courses accommodating to beginners?",hsbo5ov,t3_n2n0ax,1641974200.0,False
n2n0ax,I’m really debating going back to school for computer science. But my previous University recommended that I do a BA instead of a BS in computer science based off my previous record as a mechanical engineering major (I really wasn’t focused then). I’m stuck between doing that or going to a coding boot camp. Any thoughts suggestions or advice would greatly appreciated,hsdu18j,t3_n2n0ax,1642014042.0,False
n2n0ax,What are the best tutorials for learning dsa and data science on youtube??,hsj5d4b,t3_n2n0ax,1642104758.0,False
n2n0ax,What are some good ways to begin with computer science? Want to get started early for the experience,hsk5vmh,t3_n2n0ax,1642118825.0,False
n2n0ax,What is the best YouTube channel to understand data structures and algorithms in programming?,hsnq2kj,t3_n2n0ax,1642184130.0,False
n2n0ax,"I'm contemplating a career change, at the age of 40, from running my own mountain guiding business to something in computer sciences. I can afford to study full time if required, but what should I be considering with regards to suitable courses and are there opportunities to work remotely? I live in a small town in Scotland, where there aren't many work opportunities locally.",hsofpef,t3_n2n0ax,1642194053.0,False
n2n0ax,"Started off as a regular SDE but in my bachelors I did work in XR, data science, blockchain and content creation. I thought this would help me diversify but now I am plain confused on what I like more and what should I dive all in to.  
  
I feel that metaverse is the next thing and want to get in early especially wearable computing, I also feel that Web 3.0 Aka Blockchain development is the next step of the internet. Data Science is obviously the thing right now, especially computer vision.  
  
But going into the metaverse domain, the effort to learn the skills required vs the pay scale is definitely not worth it unless ofc you intend to become your own man.  
  
Basically, I am starting to feel that being a vanilla SDE is gonna be a death sentence and I would really appreciate some thoughts on this",hsrff2b,t3_n2n0ax,1642252511.0,False
n2n0ax,"I'm struggling with learning C# at university. I have suspended my studies for this term so haven't been  getting much support. Basically learning myself. Can anybody help with how I can self learn without much support? I need to make a card game for the assignment it's called love letter. So I'm trying with other games. Like uno, poker etc. But, there's too much I can't do. I'm really struggling to learn programming.",hssrmtx,t3_n2n0ax,1642272957.0,False
n2n0ax,Im 28 years old i went to college for five years for a B.S in psychology never finished. Grew up got in the workforce had a kid. Im just curious i want to leave retail so bad but i do make a comfortable living at about 65 to 70k a year depending on over time. I want to go back to school but i have a mortgage and a child now. Would i be wasting my time getting a associates from a community college in CS or should i just do a bootcamp. I have very little CS experience i wrote a Hello world in c# once. I would love to be apart of the tech industry but I cant make less then what i do now . So i guess my second question is what do entry level pay jobs look like in CS specifically in So Cal,hsyyvry,t3_n2n0ax,1642378186.0,False
n2n0ax,"May I get some advice and tips on starting up a CS degree please? I want to enroll in the 60 credit Oregon State University online post bacc program. I got my first degree in Kinesiology… So, yes, I would be entering this program completely blind. But 60 credits (2 years @ 8 credits per term) sounds like a fucking wild ride to be able be apart of. I’d love to leave my 20s proud of myself by doing something like this, I just feel so intimidated because I don’t have any concept of the classes or coding or how hard it will be. Does this sort of program sound feasible for someone going in blind? Could I prepare for it beforehand?",ht00ou8,t3_n2n0ax,1642394636.0,False
n2n0ax,how different are cs and computer engineering? do they get similar jobs?,ht2xb6h,t3_n2n0ax,1642450219.0,False
n2n0ax,"I feel like giving up. I decide to work towards a CS degree but math was never my strong suit. What is killing me is that I’m putting hours upon hours towards these math courses that they are wearing me out so bad. I get home from work, take a little break and then slave away up until midnight….repeat 6 days a week, sometimes 7. I didn’t think it would be like this.",ht516bo,t3_n2n0ax,1642482034.0,False
n2n0ax,"I’ve already googled this, but I also enjoy hearing live human responses: 

1. What are some other popular careers in computer science outside of coding/software development? 

2. Is it possible to land a remote CS career outside of software development? 

Thanks!",ht6upzw,t3_n2n0ax,1642521587.0,False
n2n0ax,"I am starting computer science this semester and am taking three CS classes and 2 gen eds. I am taking Human Computer Interaction, Intro to Programming in Java using Intro to Java and Data Structures 12th edition by Y. Daniel Liang, and Discrete Structures. Does anyone have tips on how to best study for computer science? I am planning on making lots of notes and flashcards for terms/definitions.",ht7f6qw,t3_n2n0ax,1642529189.0,False
n2n0ax,"Hi there,

What's really great about CS is how many resources are out there for different learning styles. I have found YouTube to be the most helpful personally. There are TONS of videos walking you through many different coding challenges.",htjlbul,t1_ht7f6qw,1642731556.0,False
n2n0ax,"Heyy, m trying to major on computer science m only 21 years old and new to technology. What I came expect as computer science major and it’s hard?",ht9t9hm,t3_n2n0ax,1642563501.0,False
n2n0ax,"Hi there! I'm currently working towards a Bachelor's in Computer Science and LOVING IT. My program is actually a bachelor's of arts, which is really cool because you are not only expected to take math and programing classes but you also take quite a few language arts classes giving you a more well rounded education IMO. 
I would recommend looking into a BA in CSC if this sounds interesting to you. The level of math is also not as difficult if that is something you would be intimidated by.",htjl0q6,t1_ht9t9hm,1642731422.0,False
n2n0ax,"Thank you I’ll look into it,",htjnqts,t1_htjl0q6,1642732601.0,False
n2n0ax,"Starting a BSc Computer Science in 3 months (may), best way to prepare myself? CS50x?",htapywj,t3_n2n0ax,1642583657.0,False
n2n0ax,Can someone describe the skills for discrete and computational mathematics clearly? and what areas of high school maths they'd line up with most? starting a CS degree but really worried about the Math,htaqk8c,t3_n2n0ax,1642584133.0,False
n2n0ax,Z3r0DayLu on Twitter too,htf0ron,t3_n2n0ax,1642652506.0,False
n2n0ax,"Hey everyone, I’m a 22 y/o engineering dropout. Just looking for some insight into a couple choices I’m thinking of doing to pursue a future in programming.

Just wondering if anyone has done a coding boot camp and actually gotten a job from it. They seem like the practical choice in terms of education rather than pursuing a post secondary degree/diploma. But are they even recognized? For some context I have done a couple years of C++, Python, and HTML and to be honest they were the only things I liked learning about in university. I was thinking of sticking with python because it seemed versatile, but I want to know if anyone has done a boot camp and actually made a career from it. Any information or advice is greatly appreciated! Have a wonderful day!",htha61e,t3_n2n0ax,1642698147.0,False
n2n0ax,Are there any tricks or tips to getting an internship for software development as a college sophomore? My school requires an internship to graduate and I’m planning on trying to graduate a year early so I would need one for this summer. I’ve applied to quite a few internships starting a few months ago but heard almost nothing back from any of them. Is there any advice or tips that could boost my chances of getting one?,hthejmg,t3_n2n0ax,1642699715.0,False
n2n0ax,"I had to drop out of college after 3 semesters of CS/Engineering, and now I'm unemployed after working several years in the service industry for minimum wage. I'm looking into going back to college in the fall with my spouse's support, but that doesn't really help me now. I really just want to get into the field asap, but I'm not sure what I can do without having finished a degree or relevant work experience. Any advice?",hthfqx5,t3_n2n0ax,1642700144.0,False
n2n0ax,"Hello,

I'm nearly complete with my associates degree in computer science but I am still not sure where to start with finding an entry level position.

Can anyone share with me where they started? I would love the hear what your ""entry level"" job title was and a brief description of the position.  

Legit anything related to the tech field is interesting to me.",htjkcii,t3_n2n0ax,1642731130.0,False
n2n0ax,"I want to be either a game dev, programmer, or concept designer for a game. I’ve been self studying art for the past 2 years but dont know much about programming. Is there a college course that mixes both?",htoe68t,t3_n2n0ax,1642812326.0,False
n2n0ax,"I have recently been considering majoring in computer  science/ programming. I was recently deemed qualified for a game design & programming bachelors in my home country, yet I’m still hung up on my other options. 

I’m not sure if I should dive into it for my bachelors or push it back for my masters. I’d love to get some responses (from any of you in this thread) whom have both done a CS BS or a CS MS and what you thought were the biggest pros and cons, takeaways, challenges, etc.",htoi3kr,t3_n2n0ax,1642813949.0,False
n2n0ax,Is an online degree from SNHU worth it? I.e. will it be considered professional/serious enough for most jobs. I plan to pursue a Master’s in CS from UMASS if I can get land a decent enough job post BS.,htos7lc,t3_n2n0ax,1642818202.0,False
n2n0ax,"I'm studying CS in Uni and I'm supposed to pick a Major next semester 

I'm leaning towards Web Development as I already have an Okay background with HTML/CSS, and I actually quite like using these 2 languages 

But signing up for Cyber Security or Artificial Intelligence seems like a good idea too, and I don't mind them even though I have no background at all with them

Which of these 3 Majors has the best job opportunities, and which pays more?",htqiqrg,t3_n2n0ax,1642854931.0,False
n2n0ax,"Yo, looking to become proficient in Powershell. Anybody have highly recommended free courses online? YouTube series? Etc",hts977q,t3_n2n0ax,1642882094.0,False
n2n0ax,"I'm in my late 20s and switched from being a Registered Nurse of 7 years to a Software Developer career via bootcamp route.

I am at the tag-end of my bootcamp and feel competent enough for an entry-level Jr Dev position (the bootcamp goal), but I wanted an approachable route to getting more into the foundational elements of Computer Science and Programming Theory.

Specifically, I want to learn content that is written for a new programmer and not a theorist--applicable material. I want it to be approachable and as simple as possible, rather than pedantic and obfuscated by archaic synonyms and complex prose.So, basically, not the standard dry and laborious prose of many modern textbooks: material I can actually enjoy (insofar as is possible) studying and get 'good bang for my buck' for.

TL;DR

High-yield, very approachable/simple material that will give a deeper and immediately applicable understanding of computer science/programming concepts for a programmer:- Algorithms, Data Structures, Hardware, Systems, Networks, etc. ... especially things that will help immensely in my early Jr Dev career phase and allow for faster ascension to the Sr Dev level and beyond.  


Further details about me...  
\- Bootcamp focused on Django Framework / Python applications and C# / .Net  
\- Have a Bachelor degree in Nursing",htsqb2o,t3_n2n0ax,1642889078.0,False
n2n0ax,"**tl;dr**

What to study: computer science, cybersecurity, or computer engineering?

**Detailed question**

Hey everyone! This is a question for the seasoned techies in this group. Thank you in advance for your contributions :)

I was wondering what you guys suggest I should select as my field of study. I'm 28 and trying to go back to school for one of the fields in the title of this post, and I was curious to get some advice from people with a diverse set of backgrounds.

A little more about myself: I already have a B.S. in neurobiology. I mention that because it would allow me to opt for accelerated degree/certification programs. I'm a highly visual and creative person, and I love creative problem solving. I'm trying to build a career that I find at least somewhat fulfilling, but more importantly, one that's financially stable (which I'd consider $100-120k/yr within first 1½ to 2 years in the industry) and has ample opportunity for advancement.

In your opinion(s), which of these areas would optimally meet these criteria?",htsvtw1,t3_n2n0ax,1642891287.0,False
n2n0ax,"I finally sound what I want to do in life, and It's definitely in IT. I think I'll finally go to college to become a Systems Engineer, though I don't know what all jobs exists in the IT field.",htwdvmo,t3_n2n0ax,1642957188.0,False
n2n0ax,"I’m applying for a master in computer science, I’ve been working as a chemist since graduating undergrad and received my undergrad in biomedical sciences, I’ve taken calc I and a stats class. the program I’m applying to has intro courses before you get into the foundational material. I’m wondering if anyone has any starting points for someone with my background (no coding experience) to prepare for computer science and how difficult/how much time it might be to learn a coding language. Any tips or direction to any resources would be greatly appreciated!",htzeq52,t3_n2n0ax,1643001152.0,False
n2n0ax,Is it possible to get a MSc in a computer science related course without having a BSc first? I’m Europe based and during second year of my Bachelors and I realized that what I’m studying is not exactly a subject I wanna pursue in life… I still wanna finish my Bachelors and maybe combine it with tech somehow. But what should I do with further education? I’m learning cs on my own right now and I wonder if there is a way for me to do my Masters in a tech field…?,hu2u83u,t3_n2n0ax,1643062673.0,False
n2n0ax,"My nephew is wanting to go into computer science with emphasis on video game design. Are there any suggestions for schools or program tracks that would be desirable for companies related to the field? Looking for options that are budget friendly, thanks!",hu3ndfo,t3_n2n0ax,1643077765.0,False
n2n0ax,"I am a computer science student currently graduating in May of 2022 with my Bachelors in Computer Science also while playing football. I have been recently granted the opportunity to continue my education and play football at a university. So this means I will have free Masters degree, Free food, and most of rent paid for. I'm not exactly sure what I want to do yet in the field. It is a 2 year program and I would finish in May of 2024. Is getting a masters worth it in this scenario? Not knowing what i want to do in the field yet is this worth taking this opportunity? Is giving up 2 years worth of work experience worth getting this masters? I would be 23 years old and no debt when I finished my Masters. Thank you for all insight.",hu6r8ia,t3_n2n0ax,1643136493.0,False
n2n0ax,"Hi I'm a recent graduate who minored in computer science, I enjoyed those courses the most. I currently work full time. I have used Java, Python and C++. I'm interested in back end programming and would like to know what would be the best path for me to learn and grow. Any courses or advice would be appreciated!",hu83qzh,t3_n2n0ax,1643155234.0,False
n2n0ax,"I am extremely slow at getting my bachelor degree in computer science, might take around 12 semesters to finish it. Is that detrimental at finding a good job as a programmer? From what I've gathered plenty of companies are desperate to find good ones.",hu8sqc6,t3_n2n0ax,1643165730.0,False
n2n0ax,"Hi, I've applied for an Undergraduate in Maths, and I'm looking at going into a Computer Science-related field following my degree. How feasible is this, what specific branch of CompSci should I look into (given that I'm currently mosr interested in pure maths), and what skills would I need to build up in order to be successful?",huah3av,t3_n2n0ax,1643203492.0,False
n2n0ax,"If I am looking to have a job as a Software Developer, does it benefit me to have a software engineering degree as opposed to Computer Science?? 

I have no access to a Software Engineering degree other than through ASU online (or a different online university) which I am not opposed to.  There is a university near me that offers a BS in computer science but has no Java courses. 

I graduate with my AA in a few months, so I am looking to transfer within the year. 

&#x200B;

Will an employer choose a software engineering major over a computer science major to be a software developer?",huco97j,t3_n2n0ax,1643233066.0,False
n2n0ax,"What language would be good to learn that could be productive for both games and my career/college?  I have a pretty basic fundamental understanding of Java. I started learning because I like Minecraft - but I realized this might not be as practical anymore. I kind of want to go into a career in government, what would anyone with experience recommend? From what I've heard C++ or Python? (Is Java okay??)",hudpws2,t3_n2n0ax,1643248065.0,False
n2n0ax,"Hello all, I hope this post finds you in good health! I originally typed this up to make a post in ask-reddit, but they don't allow body text. I don't know where else to post this at, so this seemed like the best place. If anyone knows of a good sub to post this in, I would be forever thankful!  
I've been in search of a new career (something better than retail/food industry/warehouse work), and after seeing so many things about people working from home (and having a small bit of tech support, remote work experience), and the thought of being able to actually make a living from the comfort of my own home seems surreal to me. That being said, I don't know where to start!  
I don't have a college degree (and please refrain from saying to go to school, I've put a great amount of thought into it and it's not financially viable for me without contributing to SLABS, which I refuse to contribute to \[and also I just don't want to be in debt like that\]), or much tech experience at all; I have, however, always loved doing things on computers.   
Ultimately, I was hoping to pay for some sort of short programming course or other kind of IT course with my tax returns. I understand that a lot of things take time- and I'm willing to put time in- but I'm also hoping to find a decent job as fast as possible. I've googled lots of things but I always have a hard time finding the information I want/need, and thus, here I am!  
So my question(s) to you all is/are this/these: What kind of IT job do you do at home, and what does your position entail? How did you get started? What kind of experience/training/schooling did you have beforehand? Are there any online courses that you would recommend for a near beginner that would be helpful in finding any variety of IT job?   
Any consideration is greatly appreciated!!  
(P.s. my small bit of tech support from home experience was basically a call center tech support position, and I HATED dealing with the customers. It's a big company that many of you, if not all of you know, but I won't/can't say. And maybe it was just ""that company's"" customers that made it such a drag, but I would like to avoid similar types of jobs if possible. I know I'm asking for a lot, but it's my only life I have and I'd rather get what I want if possible.)",huh642y,t3_n2n0ax,1643310359.0,False
n2n0ax,Okay. Here is my kinda odd question. I am a current CS major (recently started so please excuse lack of knowledge). Why isn’t CS  a two year degree or trade? Basics seem to be able to be learned in a year and Im of the thought that it feels like a lot of learning is done on the job/during an internship or even working on side projects outside of school. People can get coding certificates in a shorter period of time. Just kind of a curiosity as to if there is an inherent benefit to it being a four year program or if it’s just kinda how it worked into our education system (US specifically)?,humrzm8,t3_n2n0ax,1643403004.0,False
n2n0ax,"After a year in and out of university, I have sat down and am now working towards an associate's degree in Computer Informations Technology with a focus on Cyber Defense. I'm loving it so far, but I need some advice. The CIT degree I want has a cyber defense and programming focus. I'm doing cyber defense, but I'm not sure I will fall under the job prospects I want. Would I be more likely to make more money with a cyber defense focus, or would I make more under a programming focus? Which is in more demand? Which will I have an easier time to get a job in? (Also please, hold the advice telling me to just do what I love. I'm past the point of doing what I love and figured I'll be fine hating my job just so I can live well outside of it). Thanks, guys!",huoluki,t3_n2n0ax,1643432005.0,False
n2n0ax,Are there any amazing book suggestions for Python so I can start learning?,huwmnts,t3_n2n0ax,1643578546.0,False
n2n0ax,"I’m currently a senior in highschool and the time has come to choose a major. I will be going to Boise state university and I’m largely interested in computer systems engineering. 
I have adhd and hyperfixiation (which is my superpower lol). I love learning everything though I usually only have a 1-3 month attention span when it comes to learning intense units/hobbies/ classes. 

I was wondering is this a good major for someone like me who is able to get extremely interested in a certain hobby or study, though I tend to lose interest in these studies over time as I become hyperfixiated on other hobbies/ topics? 
In other words does this field of study have many diverse units/ topics that will allow me to succeed 
because the learning content is very vast and always changing? Or on the contrary, is this major something that will be repetitive over the course of the major?

Also, I am more fascinated by the hardware component of computers (as I build computers).",huwqh3b,t3_n2n0ax,1643580002.0,False
n2n0ax,"Finally taking a computer science class in college, the thing i have little to no knowledge and basically going in blind, any tips to have a higher chance of passing?",huygpqy,t3_n2n0ax,1643605727.0,False
n2n0ax,"Hey Everybody,

Can't decide if I can myself a good developer.  
  
I am a 32 year old developer originally from India and I started my computer science from low tier college in India in year 2007. I will be as real as possible. I think I can be stereotyped as a typical Indian programmer who was just there in this field on parent's insistence and for the sake of money in this field but there is an 'exception'. Once I entered my bachelors in 2007, I really started loving C++, Discrete Mathematics, Data Structures and algorithms and Relational Database Management. I wasn't running after grades but still ended up getting good grades due to my interest in all these.  
  
In 2012, I got a job in a software company in tech hub of India named Bangalore. Honestly speaking, I loved debugging Android front end applications for Smart TVs and doing some feature development but back in my mind I was always thinking about going back to my hometown to help in my fathers business which isn't tech. So being honest, I believe coding wasn't my passion. I went back to my hometown in 2015 and started a different business.  
  
I missed coding there but never made a effort to be in touch. The business didn't go down well and I decided to immigrate to Canada based on experience and education. Hence I came to Canada in June 2018. Once I came here, again being brutally honest and at expense of being insulted, I admit that I desired to get a Development job as its hot in Canada and highly paid as well. Thus I ended up being in a coding bootcamp to learn web dev. I did good over there and got a job in April 2019 in a US based company in BC, Canada. The work over there was legacy code in C# for past 15 years or so and bloated system of Stored procedures and some feature development for a mobile app in Sencha(Ext Js) framework. I did great in Stored Procedures, also found interest in Sencha and C#.  
  
But what I still lacked was desire to get out of comfort zone and try scripting like PowerShell,IIS etc which other developers did in my team. Basically, when I am working in two or three languages/technologies/tools I am fine but I when it comes to more than that, I am scared, frustrated when integration doesn't go well between those technologies. I haven't updated my github much.SO does this mean coding is not meant for me ? I have been able to crack good level interviews in three decent companies since April 2021(that's when I left my first job in Canada).I switched three jobs in last 8 months as I didn't like the work there or bad managers. I have been working in a medical software firm now for past 2 months and been loving the work so far but its PHP and C# desktop applications and services and SQL Server. I am confused what I want to do.  
  
I am good at understanding how various front/back end components interact with each other and asking right questions and implementing them and even explaining them in interviews and even some system design questions in that regard. But when it comes to learning new technologies, frameworks, languages by myself for personal interest, I am scared, lazy and all other bad things. Am I meant for a career in Computer Science,If I am, will I always be a average programmer who will survive but not for long or I should think career change ?  
  
  
  
Have already tried getting services from Career coaches but :(  
  
  
  
Any help will be highly appreciated and have high expectations here.  
  
  
  
Thank you.",huyxj08,t3_n2n0ax,1643616770.0,False
n2n0ax,I was going to collage but had to drop out because of family strife and now working a job and I have bills. Should I try and save for certificates and training or take part time collage classes? Are there any reliable ways to learn on my own?,hv27rj4,t3_n2n0ax,1643671608.0,False
n2n0ax,"Have you looked into Coursera? Specifically, the Google certifications? It might be a good place to start",hv3pjyh,t1_hv27rj4,1643697582.0,False
n2n0ax,I had been looking at the basic comptia ones but those seem pretty good. Thank you.,hv3qtji,t1_hv3pjyh,1643698441.0,False
n2n0ax,"I (25,F) just graduated with a Bachelor’s in Computer Science and Information Security and a certification in Spanish Legal Translation and Interpretation. I’ve been working for a well known hospital as an IT Tech but I mostly handle providing and locating licenses for users of the hospital. Before that I was an IT intern at a Charter School fixing computers and assisting events. 

Currently, I am getting paid $20 per hour in NYC. This is an ok salary for me as I can get by with it since I am still living at home with my parents. I would really like to make myself more profitable as I would like to earn more to retire well off. 

I am looking into getting a Master’s degree in either Data Analytics, Cyber Security or Localization Management (specifically the Translation and Localization Management Masters program at Middlebury Institute of International Studies at Montery). But since I have very little programming knowledge I am not sure what languages or routes I should take. This is why I’m asking for some assistance from ppl in any of the three fields:

-What languages should I learn before I apply to a Masters program?
-What university do you/don’t you recommend for said program and why? What did you/didn’t you like about attending that university?
-Where do you work and what do you/don’t you like about it?
-Any certification you recommend I get?

These are just some basic questions but tbh I am thinking of doing Data Analytics first and then moving onto Cyber Security bc they seem to be interconnected. Would that make me less desirable bc I would be over qualified?

Like I said any help would be greatly appreciated since I feel I haven’t actually worked in the Computer Science field and I don’t have a mentor to work with. 

Thank you for your time!",hv3pffd,t3_n2n0ax,1643697497.0,False
n2n0ax,Am new to computer Science.. I just started the degree and I want to make new friends so we could both study together and trace our progress.. and I want to meet someone who’s already in the field to prep me or give me books to read please,hv4cycg,t3_n2n0ax,1643715352.0,False
n2n0ax,"How can cs make the world a better place? Is it possible to have a job in a cs field without being super organized? (Also, any INFPs in cs with advice?)",hvcwgnu,t3_n2n0ax,1643851146.0,False
n2n0ax,"Hello! so im currently 22 and looking to start my education in Computer Science, the only thing is I have no idea where to begin or how to get started or if its the right career choice for me. After a long time searching I realized the work done in programming and computer science is what makes most sense to me... anyway when it comes to advanced education, bootcamps, self taught programs... what is the best way to go about this?",hvdun4x,t3_n2n0ax,1643866636.0,False
n2n0ax,Does anyone have any advice for what the difference is between a Bachelors of Science and a Bachelors of Technology. I am not sure which one to pursue.,hvegiej,t3_n2n0ax,1643881758.0,False
n2n0ax,"I have an opportunity to study computer science at both Georgia Tech and Columbia University. While Georgia Tech is ranked higher for computer science on almost every source, Columbia has an ivy league prestige and is also ranked better as a school. Which should I pick?",hvijnwl,t3_n2n0ax,1643945475.0,False
n2n0ax,"Anyone working in Haskell.How is the future of Haskell?Are there enough opportunities for a Haskell Developers?
I am an Software Developer in India.",hvj6tq8,t3_n2n0ax,1643958005.0,False
n2n0ax,"I'm having a predicament right now. I was going to get a nice pc mostly for learning to program and play videogames in my freetime, but when I brought it up to my mom, she said that we should go to some guy at a best buy. She was talking about how I need to know that maybe colleges have computer requirements or something, and brought up that I should know what college I'm going to go to and all that. I wasn't planning on going to college, and I still don't want to, but I don't know what I should do. All I know is that I want to have a career in software development without getting student loans or anything.   
Also what computers do you guys think I should look at? Currently I'm using a highschool issued laptop. I'm only a junior in high school btw. Another thing she brought up is that I was looking at something that would total roughly $1500 (pc/monitor) while I have roughly $2100 in my savings.",gxx5ui1,t3_n2n0ax,1620865039.0,False
n2n0ax,"computer requirements for colleges are generally pretty lax. unless you're doing intensive modeling work in blender or something, most laptops will be okay to handle coding / compilation processes. 

that being said, the macbook airs with m1 chips are great for their price if you want something portable, light, and powerful for many things - obviously, the biggest caveat is that you'll be limited in gaming, but you could potentially dual boot (run both macOS and windows) on your macbook. i'm not sure what the state of dual booting is right now with the m1 chips but i'm sure that the feature would be available soon if you don't mind waiting.

otherwise, the only other notable windows laptop that i've heard of recently is the dell XPS series. 

for working/gaming, you could connect your laptop to peripherals (keyboard, monitor, mouse).",gyry300,t1_gxx5ui1,1621477921.0,False
n2n0ax,"Incoming Computer Science Major, looking to buy new laptop

As a newly compsci major, I need a laptop. I am completely lost in terms of specs, and screen size. 

With the new release of the M1 Pro and Max chips, I am really tempted to be that person and max out, but even if I did —what screen size should I aim for? I hear coding is best on a bigger screen, but that screen size doesn’t matter if you’re connecting to external monitors. 

I can see myself getting an external, somewhere (kinda far) down the line. 

I for sure want a macbook to run smoothly with my other devices. 

Things you should consider:

I live at my schools dorms, and campus is about a 15 minute walk. Therefore I don’t see myself commuting too much. 

My budget is around $3-4k

I also want a cute handbag to fit my laptop in cause if I go with the 16 inch, the bag options are looking real slim and kinda ugly. 

thanks.",hi8njxu,t3_n2n0ax,1635338318.0,False
qb4bof,"Mods just got maaaaad........
I feel you, bro.",hh7dqzf,t3_qb4bof,1634625305.0,False
qb4bof,"Not mad really more just annoyed. Hopefully people will actually read the big capital letters saying ""stop posting this bullshit here"" but I doubt it",hh7eamg,t1_hh7dqzf,1634625755.0,True
qb4bof,What about a flair that says “tech support” when they use that flair an auto mod replies with all the other tech support subs then deletes their post? Kind of a sneaky sneak way of helping out.,hh8sicz,t1_hh7eamg,1634657913.0,False
qb4bof,"Oohhh, I like that",hh958nd,t1_hh8sicz,1634663132.0,True
qb4bof,"I hate to be the one to tell you, but... Chances are really, really, low.

But, doesn't hurt to try, eh?",hh7en3q,t1_hh7eamg,1634626048.0,False
qb4bof,"Making a stickied thread actually cut down on the tech support posts *significantly*. I would wager somewhere within the range of 75-90% fewer IT posts were made once we linked to tech support subs. Hopefully this will cut out the few ""what mac book should I buy?"" and ""my wam no work"" posts that we still get, but I doubt it, especially when it comes to the second group. Some people just cannot get it into their skulls that Computer Science is not IT. 

Side note - I wish I could be bothered to dig through modmail cause a few months back I had one guy try to argue with me that CompSci and IT were the same thing for like 3 days. I responded maybe twice but the guy kept sending messages every day until I got bored and muted them. Worst part was his problem was something super simple. My guess was he probably needed to reseat his RAM or something really basic like that.",hh7gtdn,t1_hh7en3q,1634627959.0,True
qb4bof,Do you know how I can add this post to my bookmarks? I'm on a 2019 Macbook Pro and have Chrome browser thanks,hh79buz,t3_qb4bof,1634621810.0,False
qb4bof,"*What the fuck did you just fucking say to me you little user? I'll have you know I graduated top of my glass from Reddit Mod Academy and have been involved in numerous thread deletions and have over 3000 confirmed bannings. I am trained in Toolbox macros and I'm the top Automod scripter in the entire reddit mod cabal. You are nothing to me but another user. I will fucking ban you into next week with a macro the likes of which have never been seen on reddit, mark my fucking words. You think you can get away with posting shit like this on the internet? Think again, fucker. As we speak I am filing a report to the reddit admins to obtain your IP address, maggot. The ban wave that wipes out the pathetic little thing you call your reddit account is coming. You're fucking banned, kid. I can mod anywhere, anytime, and I can ban you in over seven hundred ways, and that's just with automoderator. Not only am I extensively trained in shadow banning, but I have access to the entire toolset of the reddit admin team and I will use it to its full extent to wipe your miserable ass off the frontpage of this website, you little shit. If only you could have known what unholy retribution your ""clever"" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. You're fucking dead, kiddo.*",hh7hhf9,t1_hh79buz,1634628562.0,True
qb4bof,This is by far the best thing I’ve ever read on this app,hh7lkyi,t1_hh7hhf9,1634632348.0,False
qb4bof,"My stupid ass hand typed that out of boredom, so it's good to know my ""hardwork"" is appreciated",hh7m8xq,t1_hh7lkyi,1634632958.0,True
qb4bof,"Stupid ass-hand

[xkcd: Hyphen](https://xkcd.com/37/)

---

^^Beep ^^boop, ^^I'm ^^a ^^bot. ^^- ^^[FAQ](https://pastebin.com/raw/vyWra3ns)",hh7m9di,t1_hh7m8xq,1634632970.0,False
qb4bof,Well now that I know you had to type it and didn’t have a macro for everything you say is making me less scared of your threats,hh8w43y,t1_hh7m8xq,1634659422.0,False
qb4bof,Post it on r/copypasta please I want daddy cummy 🥺🥺,hh7ndcq,t1_hh7m8xq,1634633991.0,False
qb4bof,I'd rather it not end up getting spammed around Reddit,hh99joa,t1_hh7ndcq,1634664864.0,True
qb4bof,"If your r/computerscience \- moderating ass is that bored, consider re-reading CLRS, or Numerical Recipes.

:-P",hqhidwt,t1_hh7m8xq,1640826457.0,False
qb4bof,You... you monster. :-D,hh7p2zq,t1_hh7hhf9,1634635561.0,False
qb4bof,\*golf clap\* Bravo. :),hhduigl,t1_hh7hhf9,1634750168.0,False
qb4bof,r/woosh,hh7imbr,t1_hh7hhf9,1634629608.0,False
qb4bof,where the upvote button need help thank,hh7aui0,t1_hh79buz,1634622974.0,False
qb4bof,Im going to be starting my first cs class....how many rtx 3090s do I need in my laptop?,hh8ohm9,t3_qb4bof,1634656227.0,False
qb4bof,As many as you can fit on the pc,hh97g0i,t1_hh8ohm9,1634664025.0,False
qb4bof,"You need to carry atleast two server racks with you full of equipment. Two is the minimum but it is recommended to have atleast four which have as many rtx 3090s as they can fit. Also they should atleast have 10PB of storage combined.

- Your Professor",hp4jd9g,t1_hh8ohm9,1639883899.0,False
qb4bof,"I need help downloading chrome, someone said I need to delete system32 for it to work but I’m having trouble with that so is there something else I need to do?",hh89w9k,t3_qb4bof,1634649664.0,False
qb4bof,"Yeah, you're supposed to reboot after deleting the folder",hh8i1a2,t1_hh89w9k,1634653479.0,False
qb4bof,First you have to set the owner of the files in the system32 folder and subdirectories. Then delete. It’s easier if Windows isn’t running.,hhaciac,t1_hh89w9k,1634680433.0,False
qb4bof,"You are expelled from the class for using Windows.

- Your Professor",hp4jgp5,t1_hh89w9k,1639883947.0,False
qb4bof,What does != Mean? Is that a new key on the new Mac?,hh7bn05,t3_qb4bof,1634623599.0,False
qb4bof,I don’t know but you should make a post on the sub asking about that,hh97k4c,t1_hh7bn05,1634664071.0,False
qb4bof,It's pretty much the same as <>,hhalz0v,t1_hh7bn05,1634684719.0,False
qb4bof,It's about time this was said.,hh7fu1g,t3_qb4bof,1634627082.0,False
qb4bof,"Correct. This is a sub where people learn that going to college to learn to code is a waste of time.

Unless you are interested in learning to code in java like it's  2001",hh7hgtj,t3_qb4bof,1634628549.0,False
qb4bof,Gotta have that piece of $15k+ paper to impress the companies! And then have 3 years experience to get the junior dev job so you can get the 3 years experience to get a junior dev job so you can get the 3 years experi....,hh7hnu0,t1_hh7hgtj,1634628725.0,True
qb4bof,"Ah yes, _recursion_",hha4t1r,t1_hh7hnu0,1634677178.0,False
qb4bof,I doubt the people that will need to see this post will actually see it.,hh83k16,t3_qb4bof,1634646277.0,False
qb4bof,r/pcmasterrace is a pretty laid back place to get help and ask questions.,hh8s9y5,t3_qb4bof,1634657817.0,False
qb4bof,"Pretty sure people see ""computer"" and stop reading. Maybe there should be r/computersupport or something",hh9amct,t3_qb4bof,1634665286.0,False
qb4bof,Basically. I'm tempted to make Computer Support and have the only post be a link to tech support,hh9awoh,t1_hh9amct,1634665398.0,True
qb4bof,/r/computers gets a lot of tech support questions.,hhai9oi,t1_hh9amct,1634683008.0,False
qb4bof,"> ~~despite what all my relatives think~~

I feel you.",hp4itqp,t3_qb4bof,1639883614.0,False
qb4bof,"Would also be nice to clarify that it's not a homework help sub, either. Subs like this (and /r/cprogramming comes to mind) always get ruined by dozens of people posting low-effort, straight-out-of-the-textbook type questions that add nothing in terms of content.",hh9fhpy,t3_qb4bof,1634667207.0,False
qb4bof,Gotta love it. I'm subscribed to /r/artificial and it attracts a small dose of crazies  that just need to share their weird theories with the world.,hhac86q,t3_qb4bof,1634680311.0,False
qb4bof,Finally someone said it.,hhb0gku,t3_qb4bof,1634691523.0,False
qb4bof,Can you fix my printer,hhb6hu5,t3_qb4bof,1634694306.0,False
qb4bof,Is the discord still active?,hkampdv,t3_qb4bof,1636691522.0,False
qb4bof,thank you for the links!!,hukx4s9,t3_qb4bof,1643377324.0,False
qb4bof,[removed],hh8jpns,t3_qb4bof,1634654198.0,False
qb4bof,[removed],hh8v5qg,t1_hh8jpns,1634659019.0,False
qb4bof,[removed],hh8x5tl,t1_hh8v5qg,1634659858.0,False
skltie,"1) yes they are basically the same things. The differences are subtle and mostly unimportant. The exception being that mixed integer programs allow some continuous variables. 

3) this is often true, but not why they are hard. The first optimization problem is basically trivial and all the difficulty is in going from one to the other. 

2,4) you seem to be mixing up convex objective function and convex constraints. 

Linear functions are convex, but integer programs do not have convex feasible regions because convex combinations of integers aren’t generally integers. 

The TSP has a linear objective and non-convex constraints. We can certainly make harder problems by replacing our linear objective with any other kind of function. 

For instance you provably can’t do better than n! if I ask you to guess my favourite tour without any other information. More tractable is to add a submodular function to the objective.",hvlyn6o,t3_skltie,1644007104.0,False
skltie,"Thank you so much for your answer!

Can you please explain the following points:

\-  ""integer programs do not have convex feasible regions because convex combinations of integers aren’t generally integers."" - why is this so?

\- ""TSP has non-convex constraints"" - can you please explain why the constraints in TSP are non-convex?

\- ""you provably can’t do better than n! if I ask you to guess my favourite tour without any other information."" - what do you mean by ""tour"" (is a ""tour"" a possible ordering of cities)? why is it n!  - is this because there are n! possible ""tours""?

\- What exactly is a ""submodular function""? I tried reading about them on wikipedia ([https://en.wikipedia.org/wiki/Submodular\_set\_function](https://en.wikipedia.org/wiki/Submodular_set_function)) but I am not sure if I understand their relevance here. Why would it become more ""tractable"" (tractable = make more easier?) to add a submodular function to the objective?

&#x200B;

Thanks you so much for all your help!",hvmcvjm,t1_hvlyn6o,1644012545.0,True
sk7puv,"Hello, the router in this situation does not really factor (minus the reason for congestion)

Simplified, The TCP client would potentially decrease the TCP window size if congestion is an issue thereby reducing flow of the traffic from the server. Software that uses UDP usually does have mechanisms to manage ""flow""  as well, this is just handled at different layers of the OSI model (Application).",hvjfk9q,t3_sk7puv,1643964347.0,False
sk7puv,Then could a malicious/malfunctioning application throttle TCP traffic by flooding a network with UDP packets?,hvjpazq,t1_hvjfk9q,1643972107.0,True
sk7puv,"Yes, that's called a ""UDP flood"". It's a form of Denial of Service (DoS).",hvjsllc,t1_hvjpazq,1643974545.0,False
sk7puv,"I disagree, as a network engineer by trade, with the previous poster. Your question is why QoS methods exist for managing these situations. You couldn't operate the internet if UDP just wins all the bandwidth because TCP throttles.

You don't need QoS until the transmission line is saturated. Once there, QoS provides a way to equalize the playing field between traffics based on an operators policy. QoS methods exist at L2, L2.5(MPLS) and L3 as well as L4 re:TCP to provide various techniques for handling these situations in aggregate network traffic streams.

See 

https://en.wikipedia.org/wiki/Quality_of_service

Network QoS overview (its a PDF) https://archive.nanog.org/meetings/nanog36/presentations/sathiamurthi.pdf

802.11 to DSCP L3 marking 

https://tools.ietf.org/id/draft-ietf-tsvwg-ieee-802-11-05.html

Look at the section on pcp marking at L2

https://en.wikipedia.org/wiki/IEEE_802.1Q

Class of Service definitions

https://www.omnitron-systems.com/carrier-ethernet-learning-center/carrier-ethernet-2-0-multi-cos

https://www.mef.net/Assets/Technical_Specifications/PDF/MEF_23.pdf",hvkm1vb,t3_sk7puv,1643989080.0,False
skp5fm,"I honestly don't know, and it's been years since my best days in terms of understanding theoretical CS, but Scott Aaronson has written an [in-depth survey on the P vs NP problem](https://www.scottaaronson.com/papers/pnp.pdf) that you might find interesting, if you have the time to read or skim through the massive article. He links to the PDF from his [blog post](https://scottaaronson.blog/?p=3095).",hvmc54t,t3_skp5fm,1644012259.0,False
skkkmf,"From my understanding, *which is just a 2 hour lecture on discrete probability*, that the distribution of each digit should only *approach* 1/10 (1/D) of the total number of cycles (N) as there are only 10 different digits (D). I'm also assuming that TRNG stands for True Random Number Generator as in a theoretical truly random output from a uniformly distributed set of single digits (if you mean like a hardware random number generator I have no clue:P).

&#x200B;

In the 10 million cycles case, I think it is likely that each digit is close to a million (over or under a little) rather than exactly one million each. For example, even if we had a truly random coin and flipped it twice, there would only be total of one head and one tail half of the time (with the other halves been 2 heads or 2 tails) and it would be equally likely for any of those possibilities. To apply it to your scenario, it means that even if the TRNG of the digits was perfect, its entirely possible that each digit isn't at a million exactly but as we keep doing more cycles, each digit approaches 1/10 of the total number of cycles. Hope this helps but I'm not completely confident about it.",hvlmy6r,t3_skkkmf,1644002705.0,False
skkkmf,"Thanks man, this was quite helpful! 

And damn this was dense!",hvlnydr,t1_hvlmy6r,1644003082.0,True
skkkmf,"Quite the _opposite_ actually: such a generator as you describe is provably _not_ random.

Random in this context means essentially that you can't predict the next value by looking at the prior values. But if you ran such a generator 1M times, and got a hundred thousand 1s, 2s, ... 8s but only 99,999 9's? Then the ""equal distribution of digits"" property means that the next output _has_ to be 9. I used the last outputs to predict, with 100% confidence no less, what the next output is. Thus, not random!",hvlwg2m,t3_skkkmf,1644006279.0,False
skkkmf,"Also, if you handed me something that you claimed was an RNG and said you got equal numbers of each digit after 10M runs, I would say ""BS, no way that's random"" because the odds of that happening are roughly [one in a _thousand billion billion billion_](https://www.wolframalpha.com/input/?i=%2810%5E7%29%21%2F%28%28%2810%5E6%29%21%29%5E10%29+%2F+10%5E10%5E7)",hvlykc4,t1_hvlwg2m,1644007074.0,False
skjr2f,"As a Web science researcher that has been published multiple times in, e.g., The Web Conference, to me Blockchains is one of those kind of overhyped buzzwords that everyone seems to think will be the next big thing. Even in the scientific community a few years ago, it felt like you only had to mention the word in your papers to get accepted.

That is not to say, that the core concepts of blockchains do not have their merits, but to think that ""the next Web"" will be solely based on blockchains is akin to saying that the next big mode of mass transportation will be the Hyperloop.

With developments in the areas of AI and machine learning, I honestly think it is much more likely that the next Web will be geared much more towards machine-readability of the data published on the Web. This is also what the creator of the World Wide Web, Tim Berners-Lee envisioned when he, together with Tim Hendler, and Ora Lassila, described the evolution of the Web into what they coined as the Semantic Web or Web 3.0 (yes I know the terms are confusing); they outlined a Web of Data where data entities link to other data entities rather than Websites linking to other websites. Together with enabling data providers to define ontologies, machines are able to navigate this data much more efficiently than what is on the Web today. Because of this reason, big companies like Google, Facebook, Amazon, etc., already use some of the technology on their sites; for instance, when you do a Google search, the infobox you see on the right is created by traversing their own Knowledge Graph to find the corresponding information, and the company behind Wikipedia published their own collaborative knowledge graph called Wikidata.

Now, in the Web community, we see this as a good way of bringing the vast amounts of knowledge on the Web to the people in a readily accessible manner. We are making a lot of progress towards these goals; however, there are some big challenges involved in this, such as data availability, data correctness, and so on, which is where decentralization and blockchains might actually fit in and helped solve the problems. For instance, last year I published a paper on using blockchain-like chains of updates to knowledge graphs to enable people to collaboratively ensuring that information and knowledge available is up-to-date and factual. Another exciting project in this area is the [Solid](https://solidproject.org/) project. Decentralization of the Semantic Web is a rapidly growing scientific community where Tim Berners-Lee is also somewhat still active. I have participated in conferences and workshops with him in the past on the subject.

tl;dr Blockchains are only a small part of what the next Web will be, and I think it is much more likely it will be geared towards machine-readability that be solely based on blockchains.",hvlt8yp,t3_skjr2f,1644005072.0,False
sk80jv,"You are actually pretty much right, you just omitted a factor of ""n"" from your final equation. It should be this:

&#x200B;

n \* 2 ((3/2)\^log\_2(n) - 1)

&#x200B;

Ignore the constants and simplify to get:

&#x200B;

n\*(n\^log\_2(3) / n) = n\^log\_2(3)

&#x200B;

You should also check out the master theorem as it provides an easier method to solve these problems: https://www.geeksforgeeks.org/advanced-master-theorem-for-divide-and-conquer-recurrences/",hvlf8d9,t3_sk80jv,1643999814.0,False
sk80jv,"Oh man come on, how did I miss that. Thank You for the comment.",hvljcjt,t1_hvlf8d9,1644001342.0,True
sjn3gy,"Most of the time, you just run a simulation with a reduced data set or reduced number of rounds and extrapolate from there. Compute time is going to remain (mostly) linear for conventional systems. That means if it takes X time to compute and compare 1 round, it will take n*X time to compute n rounds.

FWIW, you don't even need to keep the distance traveled and paths for all rounds, you are only comparing to the best so far, so the compare time doesn't need to grow (linearly or non-linearly) either. Set the best so far to the first round/route you compute, then see how long it takes to compute and compare the next route/round. It should be roughly linear all the way through the maximum combination of possible paths.",hvfqkvw,t3_sjn3gy,1643905504.0,False
sjn3gy,"They are just estimating using some average computer specs and plugging in the numbers into the asymptomatic run time. Let's say an average PC can run 1 billion instructions a second, you then take the number of estimated instructions and divide it by the number of instructions per second, that gives you the number of seconds your program takes to run. You then convert that into number of hours, days, years, etc.

N! Is a huge run time. 69! ~ 171122452428141311372468338881272839092270544893520369393648040923257279754140647424000000000000000

If a program needed to run this many instructions it would take probably until the heat death of the universe to do with our current technology.",hvg1911,t3_sjn3gy,1643909370.0,False
sjn3gy,"I'd also like to point out, almost no one actually tries to calculate the true running time of a general algorithm mathematically. This is for a few reasons. For one, computers are not all the same and have different performance specs. Another reason is that no modern computer only runs a single program at a time. They concurrently run many processes and thus you can't get an accurate true run time because of the uncertainty of the computer's run-time conditions.

To combat these things, theoretical computer scientists estimate the run time of algorithms via asymptotic run time. This is the familiar Big-O notation (although there are others such as little-o, Big-Omega, etc.). But most of the time, people just want an estimate of how the algorithm's run-time grows **with respect to the algorithm's input.** In the case of the Traveling Salesman problem, the input is the number of cities or towns, and it's asymptotic running time is O(N!), which people usually just equate to ""exponential"" time. This problem is considered a HARD, or in cs terms, an NP-Complete problem (For the decision problem version) .

If you truly want to know the actual running time of your algorithm on a specific machine, you typically implement it and run it a few million times to get the average run time.",hvg46m5,t1_hvg1911,1643910421.0,False
sjn3gy,This dives right into the border between what computer science is and what software development is.,hvihz7o,t1_hvg46m5,1643944701.0,False
sjn3gy,RemindMe! 5 Days \[ [https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating\_the\_run\_time\_of\_the\_travelling/](https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/) \],hvg1v6d,t3_sjn3gy,1643909593.0,False
sjn3gy,"I will be messaging you in 5 days on [**2022-02-08 17:33:13 UTC**](http://www.wolframalpha.com/input/?i=2022-02-08%2017:33:13%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/hvg1v6d/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fsjn3gy%2Festimating_the_run_time_of_the_travelling%2Fhvg1v6d%2F%5D%0A%0ARemindMe%21%202022-02-08%2017%3A33%3A13%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20sjn3gy)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hvg3co1,t1_hvg1v6d,1643910124.0,False
sjn3gy,RemindMe! 5 Days [ https://www.reddit.com/r/computerscience/comments/sjn3gy/estimating_the_run_time_of_the_travelling/ ],hvjaubq,t1_hvg3co1,1643960816.0,False
skhmpy,"Nah, chill. GitHub CoPilot didn't bring any major change in the world, nor did Web 3. These are just some hypes created by old geezers and the so-called experts in the industry who think sharing shit content on Twitter will make them superior.

PHP is still used, so is COBOL. Keep developing new SaaS products which help people, you will never have to fear then.

Edit: Grammar",hvkw2ui,t3_skhmpy,1643992810.0,False
skhmpy,remindme! 8 years,hvlw52p,t1_hvkw2ui,1644006166.0,False
skhmpy,"I will be messaging you in 8 years on [**2030-02-04 20:22:46 UTC**](http://www.wolframalpha.com/input/?i=2030-02-04%2020:22:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/skhmpy/will_software_engineers_become_obsolete_by_2030/hvlw52p/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fskhmpy%2Fwill_software_engineers_become_obsolete_by_2030%2Fhvlw52p%2F%5D%0A%0ARemindMe%21%202030-02-04%2020%3A22%3A46%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20skhmpy)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hvlw8yz,t1_hvlw52p,1644006205.0,False
skhmpy,remindme! 8 years,hvmaett,t1_hvlw8yz,1644011590.0,False
skhmpy,"Coding is dead. No-one does real programming (machine code) anymore. They all use these ""programming languages"" that basically do all the work for you so any dummy can write code. 

/joke",hvl0yri,t3_skhmpy,1643994595.0,False
skhmpy,AI is unlikely to replace software development altogether anytime soon.,hvkvimz,t3_skhmpy,1643992603.0,False
skhmpy,"No…we’re nowhere near becoming an endangered career field. Don’t buy the hype, we have a long way to go before Halo-style Cortana A.I.’s are running around solving all our problems. You should continue to pursue your goal(s). 

There are many exciting fields on the cutting edge of computing that you can get into (quantum computing, computer vision, edge computing, federated learning, etc.). You only live one life, enjoy your time to be a geek!",hvkxuk6,t3_skhmpy,1643993459.0,False
skhmpy,The jobs innovating and creating cutting edge technology on novel problems won't be going anywhere. If the models get REALLY good There might be slightly less demand for code monkeys to build generic e commerce backends much the same as HTML developers died off with more abstraction. Training a model to crunch leetcode problems doesn't actually replace anything substantial.,hvkwmmf,t3_skhmpy,1643993010.0,False
skhmpy,"Software hardly ever brings any “cutting edge technology”. It enables new technologies, but is hardly ever the new technology.

Also, AI will never* replace software engineers. Think about embedded systems, control systems, flight software, smart homes, automotive, etc. AI cannot even adequately replace a simple PID controller, let alone creating a full software application from scratch.. 

This post makes me think you don’t understand technology at all.

*as long as computers, processors and FPGAs exist in a similar form as they do today, then the amount of software engineers will only ever need to increase.",hvl1y1k,t3_skhmpy,1643994952.0,False
skhmpy,No.,hvl2v27,t3_skhmpy,1643995285.0,False
skhmpy,Yes. /s,hvlcfd6,t3_skhmpy,1643998774.0,False
skhmpy,"AI uses machine learning algorithms. These algorithms are fed data and then told which data is correct and which data is incorrect. They then use a weighted algorithm to figure out which is good and bad and use an artificial form of natural selection to evolve values that predict correctly. There are other ways AI works but this is one example. The point is that AI can't think and only work on data similar to past data. 

Programming requires a lot of creativity and you are often confronted with unique problems and unique requirements. Solutions have to be precise and there is very little room for error. There are a ton of software products programmers work with. Machine learning has no way to think or handle unique situations so they won't work.

Its possible they could do some basic standard apps like shopping apps. They are currently being used for syntax highlighting and coding suggestions for software developers because these jobs are somewhat predictable. I think they will be used to generate unit tests.

But even generating unit tests requires some creativity and they will probably generate a lot of garbage that developers will edit and fix. And if they were to generate real code it would probably be a mess.",hvl0e89,t3_skhmpy,1643994384.0,False
skhmpy,[_Do you know the industry term for a project specification that is comprehensive and precise enough to generate a program?_](https://www.commitstrip.com/wp-content/uploads/2016/08/Strip-Les-specs-cest-du-code-650-finalenglish.jpg),hvmfj6l,t3_skhmpy,1644013596.0,False
skhmpy,"It will change. 

Machine Learning subs have recently started to complain that they can't write a better data model than those provided by some fantastic libraries where 10 lines of code replaces months of work. This is because someone smart sat down and did it for everyone else. We will always need someone smart but possibly not as many people in this field. Machine learning does what it was designed to, and majority will become users that create value not builders that create opportunity for value.

Website programming is required for information publishing (static or streaming), sales, data collection, and social. There are many easy ways to get your website out there with templates for all of these technologies. Past what templates offer you might need a developer, but it's mostly content creators that are needed once a template becomes good enough. I would argue it's not the development that is the bottle neck it is the design.  Even social media template sites exist. You will still need smart folks developing templates, but less as value creators settle for packaged products, rather than seeking out something new from value enablers.

Tool programming. I don't see this going away any time soon. Finance and logistics tools are just not capable of being perfect and handling every situation, as are content creation tools.

Gaming.. you can argue that engines can grow to a point of perfection where once again art and content will rule.

But we are about 20 years out until we reach peak programming, in my opinion, so still a very valuable and valid career.",hvleo66,t3_skhmpy,1643999606.0,False
sjasw0,"Of course it’s useful. You should know how hardware resources work on a single node before you consider a cluster. You wont use everything you learn (low level assembly comes to mind) but you will use some of it and understanding things like context switching, I/O overhead, concurrency issues, memory management, etc. will give you a better perspective on more complex distributed architectures. Imagine trying to effectively utilize a load balancer without understanding how resources are being allocated. That’s a recipe for disaster.

Now, you asked for examples so I’ll give you a personal one. I work as a data scientist. Our enterprise cloud data platform has to deliver data products (visualizations, reports, etc.) to consumers. One technical way that we measure user experience on the platform is by evaluating the performance of our Power BI capacity instance. If the utilization/load is too high then our end users will encounter delays and the overall user experience will decline/suffer. Monitoring these hardware resources and identifying which resources are being stressed helps us decide how to optimize the delivery of data products to the platform and it helps us protect against potential slowdowns. If my team and I had no understanding of computer organization, computer architecture, and operating systems we wouldn’t know how to make intelligent decisions about operating efficiently.",hve63y3,t3_sjasw0,1643873883.0,False
sjasw0,"Any benefit? Yes. Immediate benefit to the things you do to get started and build 'ok' distributed systems? Not really.
 
And there are big differences in what parts will be useful and what parts aren't, because even though 100% of them are abstracted, some of them will directly influence decisions you make. CPU architecture, operating systems, and most importantly the memory model and memory architecture (in detail) will be **critical** for writing performant scaled distributed systems. Compiler engineering and assembler you are very likely to never, ever use - though admittedly assembler can be useful for learning how the memory model works properly, and there are some parallels between the way compilers work and the way declarative languages or libraries often used in distributed systems work.",hveiva2,t3_sjasw0,1643883604.0,False
sjasw0,"If you asking this question, you shall probably first write a simple backend service which will run in production, and not thinking about designing large scale distributed systems.",hve0p2d,t3_sjasw0,1643870253.0,False
sjasw0,Thanks for the reply. I have created some backend services know a little about them. I asked this question because I would be creating large-scale distributed systems in the near future and wanted to know whether low-level engineering and computer organization knowledge would have any benefit.,hve6uoy,t1_hve0p2d,1643874407.0,True
sjasw0,It’s not going to help you with implementing this future project but it’ll widen your perspective about what’s going on under the hood. If you have the time go learn it. Personally I’d spend more time on the topics and technologies you primarily deal with and learn the low level stuff on the side when time permits.,hve7ivl,t1_hve6uoy,1643874887.0,False
sjuhh6,"Nada but you could recreate that with ~100 lines of python

1. Extract labels from an image using AWS Rekognition (or opencv). https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html

2. Pass the labels to a GTP3 endpoint. https://gpt3demo.com/apps/openai-gpt-3-playground

3. lol at results",hvi1h09,t3_sjuhh6,1643937378.0,False
siux7r,"Mathematics often has surprising applications in computer science. You might come across some problem, realize that it can be represented as some kind of finite simple group or whatever and suddenly theorems from group theory make your life easy.

Apart from that, a good grasp at some mathematics is always beneficial, no matter through what you approach it.",hvaznp5,t3_siux7r,1643824737.0,False
siux7r,"Totally agree. There is a matematician/computer scientist named Petter Graff. When he is hired to analyze systems to make them more efficient and reduce the cost, he looks for monoids in the application-domain. If or when he recognise a monoid, he can reduce the problem significantly and such an architecture often involves a streaming platform. I don't have the details, but look him up.

EDIT: name of the mathematician/computer scientist",hvb8pg2,t1_hvaznp5,1643828085.0,False
siux7r,"I find this interesting, but my quick searches (the name, ""monoids"") haven't found anything.  Do you have any references?",hvbdzqm,t1_hvb8pg2,1643830047.0,False
siux7r,"A monoid is a semi-group with an identity element. I should  have wrote: ""..looking for a monoid..""

Google \`monoid\` an you will find a lot of resources, e.g. here: [https://mathworld.wolfram.com/Monoid.html](https://mathworld.wolfram.com/Monoid.html)

EDIT: semi-groups have a binary operator, so removed redundancy.",hvbfuml,t1_hvbdzqm,1643830761.0,False
siux7r,"I'm familiar with monoids and semi-groups, I meant specifically anything to do with Peter Graff, or the applications you described.  Thanks though.",hvbhf6s,t1_hvbfuml,1643831366.0,False
siux7r,"Ah, I see! He held a course in 2019 where he described a transactions system which had grown over its proportion, s.t. every transaction cost .4 USD. (PayPal). By ""out-streaming"" the old hog of a system which demanded more and more servers, he would employ abstract algebra when modelling the new stream-based architecture.

 I also spelled his name incorrect. It is Petter Graff.

Here are some links: 

[https://pettergraff.blogspot.com](https://pettergraff.blogspot.com)

[https://www.linkedin.com/in/pgraff/](https://www.linkedin.com/in/pgraff/)

I cannot speak on his behalf, but like any adept computer scientist, I am almost certain that he will reply if someone reaches out inquiring for his concepts of interest.",hvbjqvl,t1_hvbhf6s,1643832256.0,False
siux7r,"That helps, thanks very much!",hvblpul,t1_hvbjqvl,1643833026.0,False
siux7r,Anything you love reading/learning is never a waste of time.,hvbdaio,t3_siux7r,1643829787.0,False
siux7r,Its the basis of lots of cryptography. Eg the diffie-hellman key exchange uses finite cyclic groups,hve5noi,t3_siux7r,1643873568.0,False
siux7r,"I've read some machine learning articles and there are  concepts which i couldn't grasp until I start reading about abstract algebra. I recall one article emphasised on ""optimizing over a smooth manifold"", hence I had to investigate and ran into AA. I also learned that a vector space is an Abelian group and by doing linear algebra, I was learning just a small subset of such an interesting subject. 

The book ""Deep Learning"" (Goodfellow et. al, 2015 (with the strange creature on the cover)) he talks about topology and how e.g. classification can be done with points in space which seem quite randomly placed and how he manages to separate (or dichotomise) them with a straight line (If I remember correctly.) I am not familiar with this field, but I see this mentioned a lot in the context of ML/DL. I believe it is crucial if you wanna be a researcher. 

To get more motivation, I suggest you check out Socratica on ""modern algebra"" on YouTube).",hvbsngi,t3_siux7r,1643835580.0,False
siux7r,Right I had the same experience with measure theory.,hve5saa,t1_hvbsngi,1643873657.0,False
siux7r,"Not CS but group theory is pretty darn essential in chemistry, from predicting molecular shape and reactivity to predicting how molecules will interact with light.",hvc7nym,t3_siux7r,1643841058.0,False
siux7r,"Even in CS. Theoretical CS uses, I believe, every area of Math. I am not well-rehearsed in Group Theory, but I know a little bit about groups, fields, rings, and their applications in this area called Error-Correcting Codes.",hvdt8h9,t1_hvc7nym,1643865837.0,False
siux7r,"Whoops.... I meant to say... ""This comment is not about CS but group theory...""",hvdw1gz,t1_hvdt8h9,1643867435.0,False
siux7r,Oh.. my bad,hvdxxjt,t1_hvdw1gz,1643868543.0,False
siux7r,"There are some wonderful connections to group theory in machine learning, mostly around finding symmetries and invariances",hvenpgl,t3_siux7r,1643887210.0,False
siux7r,"You could say that Group Theory is the basis (or one of them) in several different concepts, such as OOP (as in heritance hierarchies and relations between objects) and it also helps to understand database structures and how to build queries. You could apply that same thinking into other data sets and structures in general.

I think it generally helps you structuring abstract thinking of any elements/objects/points, and thus helps you to design better solutions.",hvc1l06,t3_siux7r,1643838771.0,False
siux7r,You can't study any math without group theory. Almost all math is based on definitions derived from group theory.,hve6g19,t3_siux7r,1643874123.0,False
siux7r,"It depends on what do you want to do. It likely will not see much use if you are just planning to become a programmer, but if you want to be a computer scientist you will find use for it no matter which field you are in.

In general, algebraic structures like group, semiring, lattice; spaces (which in my mind are also algebraic structures) like vector space, topological space, measurable space, and Banach space; and to be more abstract, category theory, are all incredibly useful in my area.

To give you some example, programs naturally have a semiring structure, where the addition is given by non-deterministic choice, and multiplication given by composition. If you add an iteration operation on it, you will get [Kleene Algebra](https://en.wikipedia.org/wiki/Kleene_algebra): an equational theory that is vastly useful not just in program semantics and verification (https://mamouras.web.rice.edu/other/phd-thesis-2015.pdf), but also in abstract reduction system (https://www.researchgate.net/publication/220118377_Abstract_abstract_reduction).

There are extensions to it to verify different kind of systems, some of them can be found in https://mamouras.web.rice.edu/other/phd-thesis-2015.pdf; and
 
- there are ways to reason about concurrency: https://link.springer.com/chapter/10.1007/978-3-642-04081-8_27; 
- network applications: https://www.cs.cornell.edu/~kozen/Papers/NetKAT-APLAS.pdf; 
- correctness of programs: https://www.cs.cornell.edu/~kozen/Papers/typedHoare.pdf; 
- incorrectness of program: https://dl.acm.org/doi/10.1145/3498690; http://link.springer.com/content/pdf/10.1007%2F978-3-030-88701-8_20.pdf

and many more.

And there are also related systems that are based on lattices, the most notable one is [Quantle](https://en.wikipedia.org/wiki/Quantale), or sometimes called Kleene Algebra à la Conway. It is a stricter notion of Kleene Algebra. And many lattice based systems are mentioned in [here](https://core.ac.uk/download/pdf/35095875.pdf)

For spaces, they are necessary to reason about probabilistic programs. For example, the famous [Kozen 81](https://www.cs.cornell.edu/~kozen/Papers/ProbSem.pdf) asserts that all probabilistic programs needs to live in a Banach space for a reasonable semantics. Later, Fredrik and Dexter follows this up with [this paper](https://www.cs.cornell.edu/~kozen/Papers/ProbSemPOPL.pdf), which uses ordered Banach spaces to formulate the higher order semantics of probabilistic programs. Topological space, given its simplicity, are still surprisingly essential in program reasoning, [this paper](https://www.cs.cornell.edu/~praveenk/papers/cantor-scott.pdf) demonstrates that choosing the right topological space for the measurable space have important practical impact on program reasoning and semantics. 

All in all, learn all the math you want, they will be useful some day if you want to do computer science.",hvfwsnr,t3_siux7r,1643907755.0,False
siux7r,Which book are you reading? I might check it out too,hvj8ud4,t3_siux7r,1643959406.0,False
siux7r,Contemporary Abstract Algebra by Joseph A. Gallian..... it's easy to understand for non-brilliant students like me 😅,hvjclyu,t1_hvj8ud4,1643962104.0,True
siux7r,Pretty sure the current fast algorithm for graph isomorphism uses some group theory.,hvdv82p,t3_siux7r,1643866968.0,False
siux7r,"Not exactly ML application, but you can see monoids mentioned below. Twitter had a library called algebird open sourced few years back. Roughly the idea was to express data summarisation operations as monoids. If that was done, you would get parallel processing for free on large datasets. The interesting part here was if a given operation couldn’t be expressed as a monoid (for example finding quantiles) then you would try to find a probabilistic data structure which would let you express the approximate operation as monoid . You could then use it with algebird. This was essentially trading some accuracy to gain parallel processing.",hvfb8vg,t3_siux7r,1643899599.0,False
siux7r,Do you have something better you know you should be doing? If not why not give it a go and learn a subject you like? Studying group theory has many applications in theoretical CS and at the very least will build your mathematical maturity that will help you with anything rigrous you might want to do later. Life's too short to not do things that we like :),hvfg6ls,t3_siux7r,1643901580.0,False
siux7r, Many blockchain applications use Group Theory,hvfpdy7,t3_siux7r,1643905070.0,False
sj5m8j,"Communications Networks by Leon-Garcia and Widjaja is excellent for things like ARQ and Layer 2. For TCP/IP, read the relevant RFCs to buttress TCP/IP Illustrated by Stevens; throw in UNIX Network Programming if you're feeling randy. You'll then want Kleinrock's two volumes on queuing theory. Finally, read the ip-\*(8) man pages covering iproute2, and the section 7 man pages on tcp, udp, ip, and arp.",hvd1cp2,t3_sj5m8j,1643853189.0,False
sj5m8j,"Whoa I'm interested in networking research, but most texts I've read have been more on a economics/math (optimization) or probability side, I'll have to check it out for a higher level view. If you want a more math-heavy variant I would recommend [https://www.amazon.com/Communication-Networks-Introduction-Synthesis-Lectures/dp/1627058877](https://www.amazon.com/Communication-Networks-Introduction-Synthesis-Lectures/dp/1627058877) which is great albeit mathy as heck haha",hve7vvp,t1_hvd1cp2,1643875152.0,False
sj5m8j,Read some RFCs and also the networking book by Andrew Tannaebaum is a great read. Also the Network Warrior published by O'Reilly is a great read.,hve49ez,t3_sj5m8j,1643872603.0,False
sj5m8j,"Berkeley's http://cs168.io/ uses Computer Networking: A Top-Down Approach, 7th edition by Jim Kurose and Keith Ross.",hve7j6y,t3_sj5m8j,1643874894.0,False
sj5m8j,"USC’s CSCI 353 also uses Computer Networking : A Top-Down Approach, I’m a big fan",hvfkafu,t1_hve7j6y,1643903158.0,False
sj5m8j,"BeeJs Book on Networking or something like that. Been ages, I have it somewhere on my PC.",hvdayov,t3_sj5m8j,1643857188.0,False
sj5m8j,http://www.tcpipguide.com/free/t_toc.htm,hvf5atf,t3_sj5m8j,1643897040.0,False
sj5m8j,Professor Messer on YouTube has the whole CompTIA Network+ course for free. Very high quality (free) content.,hvfho9k,t3_sj5m8j,1643902154.0,False
siry8q,If i=0 then y^i is empty. This does not mean y is empty.,hvafx3o,t3_siry8q,1643817548.0,False
siry8q,"ok got it, thanks",hvaj08z,t1_hvafx3o,1643818701.0,True
siry8q,Good luck with the course. It is one of the most interesting (but mathematical and rigorous) topics of CS.,hve0g48,t3_siry8q,1643870098.0,False
siry8q,Yes it’s a lot of maths but quite interesting! Thanks,hve1lwv,t1_hve0g48,1643870841.0,True
siry8q,Oh man pumping lemma was the worst when I took a class dedicated to T of A,hvcbl8v,t3_siry8q,1643842568.0,False
sicshh,log(2^n ) * log(n^2 ) = n log(2) * 2 log(n) = O(n log(n)) because log(2) and 2 are constants,hv7zq38,t3_sicshh,1643768663.0,False
sicshh,"Adding to this

n * sqrt(n) = n * n^0.5 = n^1 * n^0.5 = n^1.5",hv9km4y,t1_hv7zq38,1643803214.0,False
sicshh,"A resource on logarithmic properties: https://www.cuemath.com/algebra/properties-of-logarithms/

A resource on multiplying fractional exponents: https://www.cuemath.com/algebra/fractional-exponents/

Big-O notation removes constants like factors of 2 or log(2) as winniethezoo commented. Otherwise, simplifying the formulas is mostly math.",hv8ksv2,t3_sicshh,1643778141.0,False
sicshh,I've been programming or developing or engineering since 2007 and have a bachelor's degree and at no time was I ever taught this.,hvarttb,t3_sicshh,1643821891.0,False
sicshh,"It's ""just"" algebra and a bit of real one-variable calculus.

Whether you write ""n\*n"" or ""n\^2"" does not (really) change the ""number of 'n's"" in your function, as that is an ill-defined concept anyways.

Also, not every ""big-O function"" (whatever that means) must be written with ""one n"" (whatever that means"".  


Saying that f(x) is in O(n \^ sqrt(n)) is perfectly valid.",hv9fvr7,t3_sicshh,1643799954.0,False
sicshh,Is that equivalent to O(n\^1.5) ?,hvauwp5,t1_hv9fvr7,1643823008.0,False
sicshh,"No, as sqrt(n) ≠ 1.5 in general",hvaza9k,t1_hvauwp5,1643824601.0,False
sicshh,>n * sqrt(n) = n * n^0.5 = n^1 * n^0.5 = n^1.5,hvb4x5j,t1_hvaza9k,1643826691.0,False
sicshh,"True, but n \* sqrt(n) ≠ n \^ sqrt(n)",hvbke2n,t1_hvb4x5j,1643832506.0,False
sicshh,"true, sorry. we probably had the same confusion though",hvbpt1k,t1_hvbke2n,1643834550.0,False
sicshh,yes,hvb4v4m,t1_hvauwp5,1643826670.0,False
sicshh,"log(2\^n) -> nlog(2) log(n\^2) -> 2logn

&#x200B;

2logn \* nlog2 is 2nlogn which asymptotically is nlogn",hvicymn,t3_sicshh,1643942458.0,False
sizzx9,"I'm not sure if I understand what you're asking, but if it helps an array of size 1 is always automatically sorted.",hvc07ny,t3_sizzx9,1643838274.0,False
sizzx9,"Yeah, my mistake was assuming that this was unique to insertion sort because of the way my instructor was stressing it. Appreciate it man",hvc0rf2,t1_hvc07ny,1643838472.0,True
sizzx9,"It took me 1 minute of googling to know it doesn't assume that. In step 2, it checks array[1] with its predecessor, which is array[0]",hvbv92a,t3_sizzx9,1643836502.0,False
sizzx9,"[https://www.programiz.com/dsa/insertion-sort](https://www.programiz.com/dsa/insertion-sort)

&#x200B;

>The first element in the array is assumed to be sorted. Take the second element and store it separately in key.

It does assume that. After some reading I found that other algo's like selection sort also assume the first value to be sorted. The difference is selection sort compares the value in question to the other values in the unsorted list first whereas insertion immediately compares it to the values in the sorted list. My confusion was thinking that counting the first value as immediately sorted was unique to insertion sort but that was wrong.",hvbzax3,t1_hvbv92a,1643837946.0,True
sizzx9,"Nope; but they do express is in a really clumsy way, and I can see where the confusion came from.  They probably meant that the first element is assumed to be sorted because the second is picked as the ""key"" and the first has no predecessor, but the second element is still compared with the first to reorder them if necessary, and so on. The geeksforgeeks article explains it better imo",hvcarge,t1_hvbzax3,1643842247.0,False
sizzx9,"The algorithm doesn't assume anything about the desired position of the first element in the final sorted array.
However, the first loop starts from the second element because the first element 1) has no preceding elements to compare it with and 2) if we view it as an array of length 1, it's sorted. Why would anyone care to call a single value array a sorted one? Just to highlight how the first step of the algorithm is no different from the rest of the iterations where the left part of the array is sorted.",hvc6k5q,t3_sizzx9,1643840637.0,False
sj47go,Check out this playlist I've been meaning to watch for over a year now https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo,hvcpjcc,t3_sj47go,1643848256.0,False
sj47go,I will. Tnx.,hvel12u,t1_hvcpjcc,1643885254.0,True
si23iw,Seems like work to read this.,hv822yz,t3_si23iw,1643769633.0,False
si23iw,Read it at work and call it professional development!,hv82tah,t1_hv822yz,1643769940.0,False
si23iw,dont read it lol,hvahs34,t1_hv822yz,1643818244.0,True
si23iw,"I honestly like this whole make more things public thing companies are doing, like GitLab publicizing its marketing handbook. Whether we like or hate google's products, we can all learn a thing or two from their engineering know-how :D

Great find op!",hv8285n,t3_si23iw,1643769692.0,False
si23iw,Thanks boss!,hvahtr8,t1_hv8285n,1643818261.0,True
shzu1j,"That's O(nlogn)

Edit: to give details: n/b*log(n/b)=nlogn*(1/b)-logb so it's in O(nlogn) for constant b. İf b is an input or parameter don't know but some distributed programming algorithms are the only place I can think of like you said",hv5ummi,t3_shzu1j,1643738675.0,False
shzu1j,b is not constant - it's a parameter,hvecxrc,t1_hv5ummi,1643878934.0,True
shzu1j,I feel like you’ve pry thought of this but — merge sort.,hv6ma5z,t3_shzu1j,1643748953.0,False
shzu1j,[deleted],hv778d3,t3_shzu1j,1643756844.0,False
shzu1j,"**[Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl's_law)** 
 
 >In computer architecture, Amdahl's law (or Amdahl's argument) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. Specifically, it states that ""the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used"". It is named after computer scientist Gene Amdahl, and was presented at the AFIPS Spring Joint Computer Conference in 1967. Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hv77a0h,t1_hv778d3,1643756863.0,False
shzu1j,Binary search?,hv8ksnv,t3_shzu1j,1643778138.0,False
shzu1j,"The closer K is to 0, the worse it is compared to O(n) given a large enough input.  It's just not really worse by an appreciable amount.  But, again, the size of the input is the determining factor here.

Sorting algorithms that use comparison are a good place to start, as the best case for those is O(nlogn)",hv9pdcs,t3_shzu1j,1643806059.0,False
siaup5,"| **Something that you wish to see done that hasn't been done?**

Technological singularity

**| Something that you don’t wish to see done that hasn't been done?**

Technological singularity",hv7npk4,t3_siaup5,1643763688.0,False
siaup5,Yes This!,hv88gta,t1_hv7npk4,1643772366.0,False
siaup5,"Human level AI, complete with emotion and sentience.

Both want it, and don't want it.",hv9mtzh,t3_siaup5,1643804584.0,False
siaup5,"It’s basically making humans predecessors. Noble, but dangerous.",hv9rqg8,t1_hv9mtzh,1643807325.0,False
siaup5,Solve Chess (Draughts has been done),hv87p4s,t3_siaup5,1643772031.0,False
siaup5,What do you mean by solve? Do you mean create a method to solve every conceivable game?,hv8csgx,t1_hv87p4s,1643774276.0,True
siaup5,That's what [solving a game](https://en.wikipedia.org/wiki/Solved_game) generally means.,hvbpoim,t1_hv8csgx,1643834503.0,False
siaup5,Sentient artificial intelligence.,hv9rohu,t3_siaup5,1643807297.0,False
si9m1h,What parts did you get stuck on?,hv7n9xk,t3_si9m1h,1643763503.0,False
si9m1h,I haven't started reading it yet but I read that you need prior knowledge to be able to udneratand it so i am wondering what the topics I need to be familiar with are.,hv7v95y,t1_hv7n9xk,1643766810.0,True
shi03u,"At the very least if students are coming into Computer Science knowing how to program in JS or Python, then higher education can focus more on the theoretical stuff. Granted this is the US so poor schools won't get this education",hv2y0yz,t3_shi03u,1643683205.0,False
sgzipv,"Its not really a two mins explanation but rather a whole course. 

I suggest looking into nand to tetris, while i didnt play it myself i heard only good things about it",huzl6j3,t3_sgzipv,1643634159.0,False
sgzipv,"https://nandgame.com


Nand To Tetris is a course but this is a game made from it that may help.",huzwog4,t1_huzl6j3,1643639731.0,False
sgzipv,I love this!,hv0i6k0,t1_huzwog4,1643648270.0,False
sgzipv,"It's a course actually, not a game, and I came here to recommend it as well! I'm doing it right now and can vouch for it. The nature of the course is starting with NAND gates and a rough explanation of bits and how and why gates work, and then through the course you build other logic gates, and then the internal chips of a cpu, and then a whole cpu, and ram, until you've built essentially an entire computer and OS that you can use to, for example, play tetris, and you've built it completely from scratch.

But to answer OP's question, I'm not really sure what step he's looking for between bits and logic gates...

Maybe electrical/electronics engineering? But as far as computer science is concerned, there is nothing between knowing what a bit is (literally just a binary value determined by power or lack thereof), and logic gates. If I have an And gate, the output is determined by whether or not both bits are ""true"" or ""on"". HOW that gate determines it is outside the realm of computer science, because it's engineering based, but I know it's got to do with transistors so that'd be a good place to start; maybe google exactly how logic gates are engineered.

Hopefully this answers your question or sets you on the right track, I wish I could help more!",huzml2e,t1_huzl6j3,1643634916.0,False
sgzipv,"> I suggest looking into nand to tetris, while i didnt play it myself i heard only good things about it

It's intended as a third year capstone course that reiterates on everything student have learnt and puts them to use, rather than as a course that teaches you these things as it goes.

i.e. it might be a bit much for OP",hv2e7ul,t1_huzl6j3,1643674384.0,False
sgzipv,"Get the book: Code by Charles Petzold. 

This helped bridge the gap between understanding TTL, and doing the large data path hardware stuff at uni.",huzvwgt,t3_sgzipv,1643639389.0,False
sgzipv,"^ I second this, great book",hv0x1ow,t1_huzvwgt,1643653728.0,False
sgzipv,"This is the definite correct answer

It’s the book that strips away the magic of how a computer works",hv486mt,t1_huzvwgt,1643711834.0,False
sgzipv,"I actually study at the university where Noam (from Nand2Tetris) teaches, and it's a mandatory course for us in CS.
It's a beautiful course that get's you all the way from basic Nand gates, through all logical gates, then to RAM storage, CPU functionality (though they don't really go into how the clock operates), then into giving commands to the CPU using Binary, then to translating a low level language into binary commands, then a high level language into a low level language, building a basic operating system and eventually writing programs for that operating system, for example a game like Tetris (hence from NAND to Tetris).

There is really a lot to explain to answer your question (how do nand gates turn into binary functions that do logic, how does a stack operate to allow namespaces and classes, how the screen is drawn etc.). But this course will surely clear most of it up. Noam and Shimon did a great job with this.",hv08ozz,t3_sgzipv,1643644663.0,False
sgzipv,Watch [Ben Eater's videos](https://youtube.com/c/BenEater). He has some really good explanations from basic logic all the way to a full computer.,hv00zgl,t3_sgzipv,1643641570.0,False
sgzipv,[Crash Course Computer Science](https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) might be helpful.,hv1d2f9,t3_sgzipv,1643659681.0,False
sgzipv,"Nand2Tetris is great, and will explain what you are looking for. Another great book that will explain this to you is [The Secret Life of Programs](https://www.amazon.com/Secret-Life-Programs-Understand-Computers/dp/1593279701/ref=sr_1_1?crid=13U1MBSSCZ842&keywords=the+secret+life+of+programs&qid=1643639632&s=books&sprefix=the+secret+life+of+prog%2Cstripbooks%2C88&sr=1-1)",huzwkbv,t3_sgzipv,1643639680.0,False
sgzipv,"""nand2tetris"" has already been mentioned and I would start there.
If you want to play with FPGA stuff to build hardware in software, have a look at https://www.amazon.com/Digital-Design-Computer-Architecture-Harris/dp/0123944244 as a supplementary resource.",hv0fnza,t3_sgzipv,1643647338.0,False
sgzipv,Computer Organisation and Design RISC-V Edition. Read the first few chapters and all your questions will be answered.,hv1j1pb,t3_sgzipv,1643661934.0,False
sgzipv,"It takes years of hard work to understand that. As another comment mentions you should try the nand to tetris course, it is a good course but it still won't teach you everything in detail. What I'd recommend is that you learn C and assembly(if you don't already know them) and then learn about systems programming and eventually slide into electrical engineering. That is the way to understand how computers go from bits to expressing logic in a somewhat detailed way.

If you wanna learn in a concise way, put some months and completely dig into the subject and search about what you do not understand, you will definitely find useful links to blogs, articles, and books that will teach you the stuff.",huznvfx,t3_sgzipv,1643635587.0,False
sgzipv,"You ever seen one of those pictures that’s a face, but when you zoom in it’s made up of faces? Computer logic is like that 7 times with the smallest face being binary. Look up the 7 layers… I forget the term but there are 7 layers of logic that are all the same thing they just each concern themselves with an aspect of all that goes into modern technology",hv1iqc4,t3_sgzipv,1643661817.0,False
sgzipv,I really like this explanation. It still boggles my mind thinking about recursion in Verilog,hv2dumg,t1_hv1iqc4,1643674222.0,False
sgzipv,https://youtu.be/Zz7mcHUNWjE,huzzx0k,t3_sgzipv,1643641126.0,False
sgzipv,"ASCII, and binary to twos complement. And digital logic.",hv00np4,t3_sgzipv,1643641436.0,False
sgzipv,"My stock answer is:
If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

This will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc, though it won't go into any detail of how common OS are implemented or how to implement your own (see /r/osdev for that). Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board"")",hv2eere,t3_sgzipv,1643674469.0,False
sgzipv,"As far as the philosophy is concerned, there's a functor between electrical circuits and logic, such that you can translate a logical model into circuitry. We simply exploit the fact that electrical circuitry behaves logically. And because logical constructions can compose and scale well, we've end up with modern computers capable of supporting vast logical models.

The big trick is that physically, electromagnetism and material physics allow us to create electrical circuits, which we can model logic on. You can use other physical systems to model logic, for example fluid computing. So computers are the result of this weird interplay between math and physics, where we can use a physical system to model an abstract mathematical one.",hv2o635,t3_sgzipv,1643678927.0,False
sgzipv,"In my advanced digital system design class we made a full cpu out of logic gates. It took a whole semester but it was really enlightening. This is what I got out of it in the most concise way possible.

Using only 5 gates you can get a 1 bit full adder. Input two 1 bit numbers and it’ll give you a result and a carry. The carry output from one can go into the input of another to make the adder able to handle 2 bit numbers. You can cascade as far as you want but modern computers use 32 or 64 bit architectures. Doing the twos complement with some more gates allows you to subtract two numbers. So we call the adder and twos complement circuit the arithmetic logic unit. Most real ALUs are super complicated but the result is the same. A digital circuit that takes two inputs, an operation code (add/subtract/AND/OR/SHIFT/ETC), and outputs a result. This is the calculator part of our calculator with a to do list.

Using more gates, you can make an SR NOR latch circuit that can hold data. Cascading these latch circuits like with the adder gives you registers. Usually a CPU has 32 registers built in and these are made with latches since they have to hold numbers. Each register holds a 32 or 64 bit number depending on the architecture. The ALU can only take input from the CPU registers so numbers that need to be crunched have to be moved from system memory to a register first.

The way this is all done is by a 32 or 64 bit number that is called an instruction. Which bit means what is defied by the instruction set, and usually they specify an operation, destination, source, and operand. Your destination, source, and operand comes from registers. 

add $1 $2 $3 
//add the numbers in register 2 and 3, and put it in register 1 
OP[31:24], DEST[23:16], SRC[15:8], OPERAND[7:0]

ld $7 300
//put the value in memory address 300 into register 7

These aren’t exactly correct but the idea is there. 
Finally you have a clock and a program counter that goes through memory one line at a time and executes the instructions. These instructions can jump to other spots in memory so you can create conditional jump statements to get your if statements. Above assembly, it’s all languages and compilers. 

Hope that cleared some of it up.",hv3s1nw,t3_sgzipv,1643699285.0,False
sgzipv,"Back in Uni when I had a course that used the concept of Turing Machines a lot at some point it made click

The simplest Turing Machine has 1 Belt with a row of 3 different characters on it, either 0, 1, or a Blank meaning nothing, the head can read one Character at a time and you have a table that dictates how the machine will behave. The machine has a state it starts in and for each state the behavior is defined depending on what it reads on the belt. So e.g in State Nr.1 if it reads a 1 it will change it to a 0 move the belt one character to the left and change to state 2.

This machine seems quite simple, but now imagine you have a different machine that has an alphabet of 0,1,2,Blank. Even though it has more letters in theory you can still convert any Programm it runs into a Programm that our simple machine can run by converting the characters on the belt into a unique combinations of 1 and 0s, so eg 1 becomes 01, 0 will be 00 and 2 will be 11, then you ad a step into the instructions for reading a sequence and then treating it accordingly.

Since this works with 1 extra letter it will work with N extra letters meaning our simple TM can simulate any Turing machine regardless of its alphabet with its basic alphabet.

Same goes for machines that have multiple heads or belts, you just need to add logic that uses sections of the belt that store positions of simulated heads or belts. So now since we know that our simple machine can work the same way as a complex machine, we don’t have to prove it for any machine and don’t have to worry about it, and can work with complex machines to solve more complex problems knowing that the simple machine can do it aswell

And with computers and logic gates and bits it’s analog. We have bits that are our binary alphabet, and we have logic gates that generate output depending on inputs. We know we can muse bits to simulate the decimal system, and we know we can combine our simple logic gates to things like adders that can do math. So while keeping this in mind no matter how complex it gets, it will all boil down to bits beeing combined with logic gates.",hv3tpqv,t3_sgzipv,1643700472.0,False
sgzipv,CMU CS 15-213,hv3unt9,t3_sgzipv,1643701166.0,False
sgzipv,"The book ""But how do it know"" does an excellent job at explaining it imo",hv44hrz,t3_sgzipv,1643708917.0,False
sgzipv,take a digital logic design course,hv4cj3e,t3_sgzipv,1643715061.0,False
sh0rr0,https://github.com/ossu/computer-science,huznz1s,t3_sh0rr0,1643635639.0,False
sh0rr0,Thanks a lot!,huzspgn,t1_huznz1s,1643637941.0,True
sh0rr0,Have a look at Teach Yourself CS: https://teachyourselfcs.com/,hv0jt4e,t3_sh0rr0,1643648870.0,False
sh0rr0,"There are many resources available online. You can check out [freecodecamp.org](https://freecodecamp.org) and [geeksforgeeks.org](https://geeksforgeeks.org). There are also several youtube channels available, you can check out some of them and then go with the one you find the best. You can also check out GitHub repositories as they also have a collection of good resources. There are paid courses too, but I would recommend going with the free ones initially because they are equally amazing. After you get some basic knowledge you can consider the paid ones if you want to.",hv0oslt,t3_sh0rr0,1643650695.0,False
sh0rr0,"If you want to do frontend, my university teachers literally taught off of w3schools (🤌🏻)",hv1p6we,t3_sh0rr0,1643664248.0,False
sh0rr0,There's the Open Logic Project if you want to learn about that (and basic Computability Theory),hv26754,t3_sh0rr0,1643670961.0,False
sh0rr0,https://GitHub.com/qvault/curriculum,hv34xkh,t3_sh0rr0,1643686290.0,False
shath4,"I don’t have a proof for this, but my gut feeling says it can’t be done.

The reason is that count sort isn’t really a sort at all, it’s a cheat that lets you recreate the original sequence in order without doing a single comparison, by indexing the values in a bit vector.

As you know, this comes at a memory cost equivalent to the largest number in the sequence (or the difference between the largest and smallest). Any optimization of this would either need to project this range onto a smaller range losslessly or abuse some regular distribution property of the original sequence. Intuitively this makes it impossible to do for arbitrary input sequences. There could still be optimizations possible for special classes of inputs, but I don’t think you can do it for arbitrary inputs.

More to your point, all the ways I know of to efficiently represent sparse matrices/arrays end up losing the one property of bit vectors that count sort relies on: O(1) “ordered insert”.",hv1qp9p,t3_shath4,1643664827.0,False
sgvyvn,"Geometry is a mesh of points and connected lines. A graph is a set of nodes and edges. Same diff

That’s why eg image size optimization is internally removing pixels based on shortest path",hv2jhwg,t3_sgvyvn,1643676861.0,False
shfux4,"I double majored in CE and CS. 

CE is much more focused on the hardware, you only take a few intro level programming courses. There are a lot of courses much more focused on low level things like hardware design, solving circuits, signal processing, and microcontrollers.

CS is more software and algorithms focused. You work with more programming languages and your homework tends to involve actually building software rather than just design. Higher level classes get into things like AI, computer security, advanced algorithms, programming language structure, and some really gnarly pieces of software like compilers.

Overall there isn't nearly as much overlap as I thought there would be when I first decided to double major",hv2lp1k,t3_shfux4,1643677830.0,False
shfux4,"In your opinion, would you recommend a double major in CE and CS to other people? I've been thinking of double majoring between the 2 majors for quite some time now.",hv2sbnz,t1_hv2lp1k,1643680752.0,False
shfux4,"That would depend on your goals and motivations. If you're just trying to maximize your job potential, I don't think it's super beneficial to double major. 

But for me it was very beneficial because I didn't really know the difference between the subjects and I wouldn't have known what I wanted to do with my career if I didn't try them both. I started out as just a normal CE major and after my first year I added the 2nd CS major. And over time I grew to like CS better and that's what I got my masters' degree in and now I'm a full stack engineer. 

So I'd say if you know for sure what you want to do between the two, just major in that because the other major won't help a lot across career tracks. But, if you aren't really sure exactly what you want to do and you think you can handle the extra workload, it's a great way to explore both subjects and see which suits you best.",hv34au3,t1_hv2sbnz,1643685998.0,False
shfux4,"How is this post different from [your previous one](https://www.reddit.com/r/computerscience/comments/seqqpv/is_software_engineering_a_sub_title_to_computer/)?

>If they know the hardware don’t they also know what software can be built on it and how?

u/Henrique_FB already provided quite good analogy:

>You are basically saying that a guy who makes paper can make any mathematical equation on said paper. Since he built the paper he knows how to write anything on it perfectly.

And here's mine:

>They decide it as much as car designers decide where a car can drive, with how much baggage and for what purpose.  
>^(Audi Q7 wasn't designed for agricultural work, yet I've seen it done)

And now I thought about another one: _You are saying that any mechanic is better driver than a professional race driver_  
What can also address:

>They only know high level thinks for the most part. 

Professional driver doesn't need to know how every aspect of the engine works - they only need the general idea.  
Car mechanic doesn't even need a driver license!

>So the computer engineers have the same knowledge as a software engineer

No, they don't. They usually lack - tremble please - the higher level stuff and mathematical abstraction.

>For me it feels like computer science is an easier part of hole IT, CS, CE, EE field. 

~~Are you studying CE and have some insecurities?~~ Why?

---

Also, where the idea that ""low level is better than high level"" even comes from?  
Assembly knowledge will do you jack sh\*t when you are dealing with SQL database.",hv2cb5o,t3_shfux4,1643673549.0,False
shfux4,"Quite honestly at this point it just feels like the guy is a stuck up computer engeneering student.

At least it doest seem like an honest question to me. I envy you for having the patiance to answer these.",hv2dl6b,t1_hv2cb5o,1643674104.0,False
shfux4,"Nope, I've just checked their profile. Looks like OP just romanticizes the concept of embedded development.",hv2hrfb,t1_hv2dl6b,1643676061.0,False
shfux4,I’m actually a CS student. I found that my earlier post where a little harsh so I wanted to try again. I can’t seem to find a good answer on the internet. I’m currently finding hardware and embedd interesting and I’m trying understand if I should change path or not. Im sorry for being “weard” I’m stuck in my mind.,hv3t1ev,t1_hv2hrfb,1643699989.0,True
shfux4,"Do a physicist have the same electronics knowledge as a computer engineer?

Someone who studies physics learns how atoms work in low level. If they know the atoms, don’t they also know what pieces can be built on it and how?

For me it feels like computer engineering is an easier part of physics. They only know high level things for the most part. So the physicists have the same knowledge as an engineer and the physicist do also have knowledge of the universe and everything it contains. When does computer engineering education benefit from physics when the time to study physics and CE (both BSc and MSc) is mostly the same.",hv2oh1s,t3_shfux4,1643679060.0,False
shfux4,"I’m understand I’m wrong. Can you give me an example of subjects when software engineers have more knowledge than computer engineers. Algorithms, security, database?",hv3tnb5,t1_hv2oh1s,1643700422.0,True
shfux4,All of those,hv4q5la,t1_hv3tnb5,1643722888.0,False
shfux4,So it’s not common for a computer engineer to know database and security?,hv5mcai,t1_hv4q5la,1643735687.0,True
shfux4,"No

Add to that important topics in the CS/software industry like concurrency, distributed computing, AI.

There’s a difference between learning SQL (which many CE/EE do) and knowing when and why you should prefer a bitmap index instead of a B-tree.

There’s a difference between learning to use Spark/Hadoop and knowing why the task/job won’t gain any speed up because of the specific class of problem you are trying to parallelize.

I’ve worked with EE and CE people with masters degree and they struggled to wrap their heads around certain concepts that are (should be) trivial to CS people. Also, because their code/design decisions didn’t take those core concepts into account and often times that meant trouble to the business.

Unlike your comment, I’m not saying one profession is better than the other. The difference is that you rarely see CS people dabbling into CE/EE roles while the opposite is quite common. That’s because the job market is hot for CS roles not the other way around.

So, from a purely job market perspective. Choose wisely...",hv5u9py,t1_hv5mcai,1643738545.0,False
shfux4,"The trick is that each university has it's own approach to computer engineering. They or somewhere between electrical engineering and computer science somewhere.  Whether it is in the middle, or almost CS but using  engineering as a glue, etc. is really up to the university.

Example:

https://www.vik.bme.hu/en/education/programs/",hv7818j,t1_hv5mcai,1643757171.0,False
shfux4,"You are right that there is some overlap, but maybe not as much as you think. This is a CS grad perspective, so take my CE, and EE thoughts with a grain of salt.

CS focuses on things like Data Structures, Algorithms, Programming Language Theory, etc.

CE focuses on more EE types of things like designing circuits, low level programming like assembly, and how the specifics of a computer work.

I don’t think it is fair to say that CS is the easy IT degree. It may not be as Mathematically challenging as CE or EE, but it is a challenging discipline.",hv2d642,t3_shfux4,1643673918.0,False
shfux4,"> It may not be as Mathematically challenging as CE or EE

But CS is a derivative of mathematics. And CE is EE and CS combined.",hv2gzb6,t1_hv2d642,1643675693.0,False
shfux4,"I don’t think I used anything beyond Calculus 1 & 2 in undergrad CS. I know machine learning uses a lot of Linear Algebra, but I can’t remember using much math. Things we did use math for: Big O, CPU throughput, disk read speed/time, I can’t think of anything else. All of that, if I’m remembering correctly, didn’t use any math beyond Algebra. I only used Calculus in my Calculus and Physics courses.",hv4hk5b,t1_hv2gzb6,1643718350.0,False
shfux4,Haven’t you taken mathematical logic or computability/complexity classes?,hv5hqfj,t1_hv4hk5b,1643733998.0,False
shfux4,"We did have a Discrete Math course that I had forgotten to mention. It touched on logic, but was mostly focused on doing basic calculations on binary numbers, add, subtract, convert to hex and back, convert to decimal and back, and 2’s complement.",hv82vau,t1_hv5hqfj,1643769964.0,False
shfux4,"The word _algorithm_ is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī.  
Did you know that algorithms are math?

Have you heard about Turing Machine? The _mathematical_ model which is considered a symbolic birth of the field of Computer Science?",hv5uamf,t1_hv4hk5b,1643738555.0,False
shfux4,"True, though my Algorithms course didn’t focus on math, it focused on learning to apply established algorithms to solve problems. I swear it felt like we spent 2/3 of the course talking about hash tables lol

As for the Turing machine, I feel like it was briefly mentioned in my Introduction to Computer Science Theory course. That course focused mainly on formal languages, pumping lemma, and finite state machines.",hv83tmv,t1_hv5uamf,1643770366.0,False
shfux4,">Algorithms course (...)  
>(...) briefly mentioned in my Introduction to Computer Science Theory course. That course (...)

Did you know that Computer Science isn't just a name for a glorified school curriculum?

>it focused on learning to apply established algorithms to solve problems.

And? How does that not fit ""algorithms are math""?

>I swear it felt like we spent 2/3 of the course talking about hash tables lol

So... math.

>That course focused mainly on formal languages, pumping lemma, and finite state machines.

All of those? Math.

&nbsp;

Basically, everything what falls under Theoretical Computer Science falls also under Mathematics.",hv8u5dy,t1_hv83tmv,1643783542.0,False
shfux4,"Okay, dude. Whatever. I was speaking from my experience in my undergrad program. As far as algorithms go, they may have been derived/invented using math, but all I did was call functions, no math.",hv9jrds,t1_hv8u5dy,1643802654.0,False
shfux4,"As someone having an msc in CSE (computer science engineering) we learned all what you described, including AI and 3d graphics and the circuit design, assembly programming as well.",hv7797p,t1_hv2d642,1643756854.0,False
shfux4,"No,  they don't.",hv2wj8p,t3_shfux4,1643682561.0,False
sg4epv,An entire textbook couldn't cover all the details.,huu84fw,t3_sg4epv,1643540323.0,False
sg4epv,"There is so much to cover to answer that…

You have to remember that when we started this that you could not buy “a router” — we literally wrote code to move packets from one interface to the other.  All the routing was done by hosts - big computers with multiple users - and IMPs - specialized computers built to forward packets. 

I actually laughed when Cisco started, because building routers out of software was so commonplace and so frequently done that I couldn’t fathom why you would pay money for one… 🤣

Go look for the source for a piece of software called “routed” (pronounced “route-dee”, for “route daemon”). That’s readily available and has not only the packet forwarding logic but the routing protocol logic for the major routing protocols.",hutvrqb,t3_sg4epv,1643530742.0,False
sg4epv,"I’m gonna try to make this as simple as possible, 

a router receives level 2 electrical signals from a “local area network” that are sent from physical “local style” addresses/devices. This can be thought of as addresses that are “below” the router that are managed by the router. The router then prepares these electrical signal IE packets of data, and converts from the “local style” addresses to “logical addresses” IE: “IP addresses”. The router takes care of knowing which IP corresponding to a “physical address” it manages below it. When it receives signals back from levels above the router itself.

The router then sends its own electrical signals up to “level 3” which uses a new protocol called IP protocol to send electrical signals through this “higher level” network.

You should look up models of how this stuff works and research deeper on your own though. I’ve glossed over many detail and or maybe gotten some details wrong. Look up “OSI model”

The stuff that handles what to do with the electrical signals is basically a driver that is a program coded to handle it",huvjnlx,t3_sg4epv,1643563885.0,False
sg4epv,It would be fun to also ask this question on r/explainlikeimfive,huug7cm,t3_sg4epv,1643546133.0,False
sg4epv,Was just thinking that,huuvulr,t1_huug7cm,1643554346.0,False
sg4epv,"At a very high level - you're receiving internet packets (believe in IPv4 or IPv6 form) and then handing them on to the correct next step according to a list of rules you've got.

Imagine like you've got a massive pile of addressed envelopes, how would you go about delivering them?

Either there address is local enough to you that you can pop through letterbox or envelope is taken to a closer point and then the method starts again.",huvkkby,t3_sg4epv,1643564231.0,False
sg4epv,"You could start by learning how to program programmable switches using p4. It will give you some idea of what a router does (receiving packets and then deciding what to do with it depending on some bits). Like others have already mentioned, this will only scratch the surface but might be a good place to start.

Some links that might help 

[https://github.com/p4lang/tutorials](https://github.com/p4lang/tutorials)

&#x200B;

https://www.youtube.com/channel/UCOQAFkDKucJWr-KafdJsdIQ",huvjzg9,t3_sg4epv,1643564012.0,False
sg4epv,As others mentioned this is a huge topic but if you want to look at the code to one here is a link. https://forum.dd-wrt.com/phpBB2/,huvhjai,t3_sg4epv,1643563083.0,False
sg4epv,"ELI5 - There’s multiple isolated clusters that know how to communicate internally. Attached to those clusters is an endpoint that can traverse traffic cross-cluster

ELI10 - Clusters have unique numbers for there internal workstations (endpoints) and external receiver. You can target look up that number through a registry (eg dns or bgp)

ELI15 each cluster can internal host more clusters. Routing policies mimic the same lookup mechanism that we explained at Eli 5. Policies also filtering/forwarding and mutation traffic as it crosses between endpoints 

ELI 20. - The traffic is a stream of packets that use a protocol encoding to denote order and application specific data. You can layer protocols (see osi model) decoupling hardware, software, and routing designs 

Eli 25 - Packets route from the external endpoints to internal endpoints through the OS network interface. This construct uses bit masking to map the port to receiving application. This is a fancy way of saying it looks up a unique number and sends to an internal endpoint.. semantically like when you were 5",huzpuih,t3_sg4epv,1643636577.0,False
sfiktu,Can you share the Python code you used to generate these?,hurn5m3,t3_sfiktu,1643490880.0,False
sfiktu,I love these. Please upload the actual images and dm me with them. I feel like you achieved those dark academia vibes with a subject not typically represented,huq7kxh,t3_sfiktu,1643469794.0,False
sfiktu,Thank you for the kind words! DM'd!,hurcpky,t1_huq7kxh,1643486522.0,True
sfiktu,"DM me too if possible. 
And yes you nailed the dark academia vibes!",hurynlf,t1_hurcpky,1643495777.0,False
sfiktu,Thanks! DM'd.,huvnegb,t1_hurynlf,1643565312.0,True
sfiktu,Me too please? My mom majored in math in college and she would LOVE these.,huvnzdd,t1_huvnegb,1643565536.0,False
sfiktu,That's so sweet. DM'd!,huvpjzf,t1_huvnzdd,1643566132.0,True
sfiktu,If possible could you dm them to me too? They look fantastic,husjiq7,t1_hurcpky,1643505054.0,False
sfiktu,Thank you! DM'd.,huvnfy3,t1_husjiq7,1643565328.0,True
sfiktu,could you DM them to me as well and/or share the python code? thank you :) they look super cool!,husoaq1,t1_hurcpky,1643507171.0,False
sfiktu,Thanks! DM'd.,huvngta,t1_husoaq1,1643565338.0,True
sfiktu,Could you also DM me the images :),husyrmm,t1_hurcpky,1643511911.0,False
sfiktu,Done!,huvnhnc,t1_husyrmm,1643565347.0,True
sfiktu,These are awesome.  Nice work,hurpp70,t3_sfiktu,1643491946.0,False
sfiktu,Thank you!!,huvnim2,t1_hurpp70,1643565357.0,True
sfiktu,"Man, can you please share the code of Python?
Or share this images, man I’ll frame this, is beautiful asf",husrnii,t3_sfiktu,1643508680.0,False
sfiktu,Thank you so much!! DM'd.,huvnkg3,t1_husrnii,1643565376.0,True
sfiktu,"I would love to get the images too, they are gorgeous. Great job!",hux4i5j,t3_sfiktu,1643585183.0,False
sfiktu,I’d also love to have these on my wall! Been looking for science that’s presented in way that could be hung on my wall - this is it!,huywygz,t3_sfiktu,1643616340.0,False
sgh0h8,"The reasons you put them together is that you can not really seperate them. For example, you typically implement Dijkstra's Algorithm using some form of priority queue. The algorithm for finding MSTs is faster with a union-find data structure. You need data structures for your algorithms and algorithms for your data structures. Arguably both are flip sides of the same coin.",huxnl6c,t3_sgh0h8,1643592782.0,False
sgh0h8,"Check out leetcode.com

People use it for hacking interview which tend to be advanced DSA topics",hv2n3wr,t3_sgh0h8,1643678456.0,False
sf4ab2,Imma try some of those BBS numbers on page 8,huo3ipp,t3_sf4ab2,1643423108.0,False
sf4ab2,Really cool find. Thanks for sharing :),huorpr8,t3_sf4ab2,1643435402.0,False
sf4ab2,Glad you like it. The ads alone are gold!!,huos489,t1_huorpr8,1643435646.0,True
seq1gi,I hope that’s from a history class.,hukwpgn,t3_seq1gi,1643377122.0,False
seq1gi,"Sometimes I really miss college, this isn't one of those times",hulbcez,t3_seq1gi,1643383340.0,False
seq1gi,I’m missing it rn,hunmbpw,t1_hulbcez,1643415299.0,False
seq1gi,"That's mid-late 1990s PC architecture.   


ISA was the standard bus on the original IBM PC and XT. It hung around until the late 1990s for legacy devices with low bandwidth requirements, like modems and sound cards. This diagram shows that the entire ISA bus could be run as a single PCI device via the ISA bridge.   


Starting in the late 1990s, many of these devices could be integrated into the motherboard or emulated by the CPU and the ISA bus was dropped.  


https://en.wikipedia.org/wiki/Industry\_Standard\_Architecture",hulh5yz,t3_seq1gi,1643385526.0,False
seq1gi,"Legacy connection, or ""backwards compatability""",hukqztf,t3_seq1gi,1643374230.0,False
seq1gi,“The PCI has a bridge to the ISA bus so that the ISA controllers and their devices can still be used” This is and explanation from my lecture but the answer does seems convincing to me. Wyt?,hukrgop,t1_hukqztf,1643374482.0,True
seq1gi,"What are you confused about? ISA is a legacy bus from the 1980's, modern CPUs don't have direct attached ISA buses, because that's a waste of space.

Instead a direct attached PCI bus can provide a bridge and proxy legacy ISA for backwards compatibility of old expansion cards.

It seems silly now, but during the transition period it would have been a big deal because having multiple weird niche expansion cards was a lot more common and users wouldn't want to have to replace them all just to replace their mobo/CPU.

Edit: This is mostly relevant in industrial/scientific systems. They're usually the mostly likely to have extremely weird old hardware that they need to support decades after everyone else has moved on.",hul3k3m,t1_hukrgop,1643380197.0,False
seq1gi,"Are you confused about the PCI bridge too? Modern systems use PCIe, which *is not PCI*, and for systems with old PCI slots they use a PCI bridge. The bridge of either type is just that, a way to bridge an old standard to the newer one that the main system uses.

On a whim, are you confusing ISA (the slot/expansion bus) with ISA (the Instruction Set Architecture of the processor)? They are completely separate things.",hum3hy5,t1_hukrgop,1643393772.0,False
seq1gi,"IT's so the CPU can pull information from and put information on the devices hooked to the ISA bus. Technically other things on the bus could ""see"" the signals too.",humksl8,t3_seq1gi,1643400254.0,False
seq1gi,Erase it from the diagram and look carefully at the graph again. Can all of the components still communicate with the CPU? What functionality might be lost?,humblyr,t3_seq1gi,1643396790.0,False
seq1gi,"To talk to the sound, printer and modem controller (in the diagram at least).",hunob48,t3_seq1gi,1643416169.0,False
seq1gi,"A bridge allows one network to communicate with another. It is like having a group of english speakers and a group of spanish speakers that cannot communicate. The bridge would be the bilingual person translating between the two groups. Each group uses a different protocol to communicate, so without the bridge, they don't recognize or understand each other.",huohpq7,t3_seq1gi,1643429842.0,False
sf7cyl,"I honestly wouldn’t focus so much on the titles, these can be really confusing as you have discovered. Focus on what a team is doing and what experience you need and that tells more or less the kind of work you’ll be expected to do. 

Programmer and software engineer if we pick apart the actual words would seem to mean that software engineers are more in the business of solving software problems in new ways. This can be creating or utilizing mechanisms to write solutions or algorithms creatively. Programmers may be more basic. But the lines are so blurred and I may very well be wrong on my intuition there.",huo2nzc,t3_sf7cyl,1643422718.0,False
sf7cyl,I have the same intuition 👍,huo5tti,t1_huo2nzc,1643424161.0,False
sf7cyl,"hmm, ok. I'm trying to learn how to code, and learned how to do a little bit of it through Python. But I don't know what I can do with it except to tell a computer what to do. I just know that I enjoy playing around on Python, but want to translate that into a job. Maybe I'm going through information overload.",huo6nie,t1_huo2nzc,1643424540.0,True
sf7cyl,Python is awesome! All software engineering / computer science is is learning more ways of telling the computer what to do. What clever sets of instructions you can give to accomplish tasks,huo7aqw,t1_huo6nie,1643424837.0,False
sf7cyl,"You won’t find a job without a degree without a pretty substantial portfolio and some personal projects. Maybe master python (OOP, argument passing, polymorphism, recursion, etc) and then get familiar with database like SQL. You’ll want to make sure your portfolio stands out as you’ll be applying for the same jobs CS grads will , and most jobs filter to toss applications from people with no degree",huoj6kz,t1_huo6nie,1643430602.0,False
sf7cyl,">But I don't know what I can do with it except to tell a computer what to do.

Theoretically, pretty much anything you can imagine - but you need to build a solid foundation to stand on first.

>Maybe I'm going through information overload.

Very likely, yes. I don't know how much you know, but my general advice to you is to take it a bit slower and try to make sense of each step along the way, staring with the very basics.",huoolo6,t1_huo6nie,1643433552.0,False
sf7cyl,"Slightly different words for the same thing. Usually ""software engineer"" is the formal job title, ""programming"" is what you're doing. Informally, we call people who program ""programmers"".",huo2wlc,t3_sf7cyl,1643422826.0,False
sf7cyl,"Gotcha, that makes a bit more sense.",huo6uh6,t1_huo2wlc,1643424630.0,True
sf7cyl,"* **Computer programming** is the process of performing a particular computation (accomplishing a specific computing task). You can program a website, a game, a database, a robot etc. (notice that programming doesn't always result in creating a software).
* **Software development** is making software, regardless of abstraction level, used technologies or complexity level. Creating simple calculator in Assembly for ZX Spectrum is just as software development as is creating Facebook front- and back-end using React for modern web.
* **Software engineering** is:
  * ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"" ~ IEEE
  * ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"" ~ Merriam-Webster
  * ""an engineering discipline that is concerned with all aspects of software production"" ~ Ian Sommerville",huo4vc0,t3_sf7cyl,1643423725.0,False
sf7cyl,">I want to teach a computer to do things based on what I code it to do. What is this called?

It is called programming unless you want to do it well, in which case it is called software engineering.  If you want to learn how and why either works, and find ways to improve upon that, it's called computer science.",huo8ye5,t3_sf7cyl,1643425605.0,False
sf7cyl,"Focus less on job titles and more on your actual set of skills, that is what will matter the most",huod236,t3_sf7cyl,1643427541.0,False
sf7cyl,"Software engineering is working with a team to design, build and launch code. You'll potentially work with product managers, designers, QA engineers and sales in the process. I've been a software engineer at a few companies including Amazon and spend less than half my day actually writing code and the other half meeting with my team.

Programming is focused around just the building part of software development. It can often be outsourced or contracted out.",huoqvrs,t3_sf7cyl,1643434898.0,False
sf7cyl,"It gets weirder because in Canada Engineer is a protected job title like Doctor.

So unless you have your Bachelors of Engineering you're not a Software Engineer.

Yet by American definition I am.",hupq8ln,t3_sf7cyl,1643460859.0,False
sf7cyl,"A programmer is anyone who writes a programme, while engineer is someone who develops an abstract idea and implements it with the best tools/people.",hups8fz,t3_sf7cyl,1643462094.0,False
sf7cyl,"Lol.

They are interchangeable. 

Software engineer, programmer, coder, Member of Technical Staff, Software developer, Software Development Engineer, Software programmer, software specialist, they're all the same thing.

Relax.",huocaid,t3_sf7cyl,1643427178.0,False
sf7cyl," Software engineer is someone with an engineering degree in the IT domain.

Everyone else might call him or herself whatever they want, like programmer, coder, etc. but not engineer.

The difference I have seen is the mindset: measure twice cut once. Everyone I worked with having an engineering degree  never jumped to coding. They always started with some level of abstraction and worked on the solution until it was correct on a logical level. They start working on the code only when this problem refinement was done, and the code is always clear, logical, and mostly works immediately. 

Also the ability to work on different abstraction levels if needed.",huow3d5,t3_sf7cyl,1643438154.0,False
sf7cyl,The titles are designed by someone in hr with no understanding of the role. They are describing the job title in a different way to prevent you from comparing salaries in the industry and collecting the market rate.,husdwew,t3_sf7cyl,1643502518.0,False
sf7cyl,"They’re all the same pretty much, just some advice, if you want to avoid a long and painful road of finding a job in tech , I’d recommend in starting a bachelors in SWE or CS. Otherwise you’ll be competing with those with degrees for this stuff and entry level is stupid competitive even for new grads",huoihnf,t3_sf7cyl,1643430242.0,False
sf7cyl,"A software engineer is a programmer is a developer is a coder. It’s all the same thing. What matters more is how adept you are with what technologies, what levels of abstraction are you comfortable operating within, how independently can you operate without guidance while still producing quality work, and how well you can integrate your work with the work of others in a way that improves the overall quality of the design.",huq7f0a,t3_sf7cyl,1643469723.0,False
sf5rtw,"Depending on the language, there are specific techniques to make memory allocation and garbage collection efficient. From partially shared data structures to generational garbage collectors, different functional languages address these concerns in ways that, sometimes, even end up being adopted by imperative language compilers too. In particular, pure functional languages have compilers that exploit the lack of side effects to more aggressively garbage collect. Other specific techniques such as deforestation (for example, when an anamorphism is followed by a catamorphism) and lazy (non-strict) execution can be leveraged to reduce the overall memory use and paralellize garbage collection.",huo10bv,t3_sf5rtw,1643421952.0,False
sf5rtw,"That makes sense! I can see how in a language that enforces pure functions you could garbage collect pretty aggressively as you don’t need to worry about a bunch of dependencies like in the jvm, you only need to worry about if a value is directly accessed again. I would imagine this would be much harder to pull off in a multi paradigm language however.",huo26l3,t1_huo10bv,1643422496.0,True
sf5rtw,"Exactly. However, generational garbage collectors, originally developed for Haskell, found their way into modern JVMs too, since they offer a number of advantages, like very parallelizable executions. Of course, in Java, dependencies make it more difficult to be too aggressive. There is a very good paper from Simon Peyton Jones on the generational garbage collector and some of these memory optimizations in pure non-strict Haskell.",huo2mvg,t1_huo26l3,1643422704.0,False
sf5rtw,That’s very interesting thank you!,huog6dd,t1_huo2mvg,1643429062.0,True
sf5rtw,"Don’t discount partially shared data structures too. 

If P is a linked list, and Q is the same linked list with one more item prepended to it (this is a common operation) then Q’s next pointer just points to P. Likewise if you create a new list R which is list P with the first few items lopped off, well, you can have R point to the middle of list P. Nobody is “allocating a whole new data structure.”

If somebody has a tree, and they want a new tree with a different value in a leaf node, you don’t have to allocate a whole new tree, you just have to allocate a whole new set of nodes on the path from the leaf up to the root (log n). If nobody mutates data, then we can share entire branches of the tree that didn’t change in that operation. 

Arrays are probably the biggest hiccup. Yes to truly simulate an array you’d have to allocate a whole new structure on each update. Various alternate strategies include not using an array (would a hash work instead?), having a special mutable array carve-out to make the language “less pure but more practical”, or sacrificing some aspect of arrayness such as: sacrificing O(1) lookups, secretly implement the array as a tree, and accept O(log n) look ups and/or updates instead. Log n access times??!? remember, a balanced binary tree can reference a million leaves in 20 comparisons… it feels like constant time access.",huplf73,t3_sf5rtw,1643457541.0,False
sf5rtw,"I’ll have to look more into that. I’d imagine that mapping certain functions to certain paths of a tree could get pretty complex with enough different “versions” of the tree or say something even more messy like an undirected graph. Is the differentiation usually accomplished with metadata stored about each node?

I can imagine replacing arrays with separate chaining hash tables could work as they can still get O(1) lookup.",hur1o7q,t1_huplf73,1643482037.0,True
sf5rtw,"Differentiation is usually accomplished by creating new nodes, not storing metadata about each node. Some of the pictures in this section might help: https://en.m.wikipedia.org/wiki/Persistent_data_structure#Trees",hurzkc3,t1_hur1o7q,1643496170.0,False
sf5rtw,Helpful resource thank you!,hutejr0,t1_hurzkc3,1643519531.0,True
sepvgl,thanks,hun5viv,t3_sepvgl,1643408434.0,False
secwcr,"Some of the answers here are bullshit. At the end of the day you need the math for the topics you study. Computer science is not one monolithic topic, there are lots of branches and subjects within it. You definitely don't need to know trigonometry if you want to write a compiler or a database..

Having a strong mathematical foundation will help generally, but honestly, a lot of math will only apply to specific areas. If you want to work in graphics, game development, physics simulations, etc, then trigonometry is extremely important, as is linear algebra.

If you want to work in machine learning, or AI, then you need linear algebra, but also statistics, and probably some calculus, especially for deep learning.

If you want to focus on compilers, or programming languages and semantics, then logic and set theory is the most important.

If you want to be an expert in security or cryptography then you need to understand number theory, prime numbers, etc.

I would say for ""general"" computer science, the most important mathematics is logic (often called discrete mathematics) & algebra. Beyond that it's really up to what you do within CS.",huiswen,t3_secwcr,1643333201.0,False
secwcr,"For the vast majority of working professionals, even in fields where the mathematics is relevant, the important part is usually a good grasp of concepts. You definitely need calculus for a lot of topics in Computer Science, but mostly what that means is that you recognize an integration problem when you see one, that you know that the the derivative of a function is 0 at an extrema, etc. 

I have a PhD in Machine Learning, and if you dropped me into a Calculus II exam this morning, I might not crack double digits. I simply don't remember enough technical mathematics to be able to start from ""what is the indefinite integral of this reasonably complicated function f(x)"" to getting a correct answer. What I can do is see a real problem in my field and understand that what I need to do to get past that problem is formulate and solve the right integral. From there, there are loads of tools available to me that don't require me to be able to pass the undergraduate exam on the topic.

That said, to be able to get to that point in my field, I did at some point have to know how to pass the Calculus II exam, so if it's really something that you can't get past, it can still be a very real brick wall.",hul5km9,t1_huiswen,1643381043.0,False
secwcr,"For my degree linear algebra, calculus, and statistics were all required.",huiz9cd,t1_huiswen,1643335922.0,False
secwcr,"They were required for my degree, as well, but I never need linear algebra or calc to do my job(s). For statistics all I've ever really needed are the basics that I learned in high school - mean, median, standard deviation, and I only use those when discussing the performance of an algorithm.",huj5c5n,t1_huiz9cd,1643338568.0,False
secwcr,">If you want to focus on compilers, or programming languages and semantics, then logic and set theory is the most important.

Hot take: If you are unable to understand high school algebra you are going to have a very hard time with stuff like Galois connections. The formal reasoning is the same everywhere.",hukmgo7,t1_huiswen,1643371535.0,False
secwcr,"By discrete maths and algebra for general compsci do you mean abstract algebra? (Groups, rings, modules, category theory etc)",hukavyl,t1_huiswen,1643363053.0,False
secwcr,i would agree with this answer in a software development sub but in a computer *science* sub?,hujjz6g,t1_huiswen,1643345385.0,False
secwcr,"I actually majored in Math and CS, so I think I have a good perspective. I honestly used very little of the math for my CS. My CS was split into two, theory and application. The theory was mostly about proving properties of algorithms, like correctness or complexity, or things like denotational semantics, proving properties of languages, and lastly topics like abstract machines. It was very theoretical but not very math heavy. The applications side was the same, learned about CPU architecture, programming languages, compilers, data structures, computer vision, databases.. more I don’t remember, again not a whole lot of math to be honest. I would say I rarely used math beyond high school level during my CS. There are some exceptions, like I mentioned. For computer vision it’s very important to understand linear algebra and trigonometry, but that’s quite a niche topic.",hujkv9n,t1_hujjz6g,1643345843.0,False
secwcr,"I think you're limiting your definition of math in saying you used very little math.. 

To me, 'proving properties of algorithms, like correctness or complexity, or things like denotational semantics, proving properties of languages, and lastly topics like abstract machines', are literally math. You don't have to be familiar with a huge number of mathematical objects, but you're applying mathematical reasoning and using math's tools.

If you're finding bounds on functions (deriving algo complexity), or using induction to prove correctness, you're doing undergraduate-level math.",hujn81b,t1_hujkv9n,1643347089.0,False
secwcr,"Yeh fair enough, getting the definition right is important, in math and on Reddit!",hujq94t,t1_hujn81b,1643348777.0,False
secwcr,"There are a great many software developers on this sub who think they are computer scientists. Likely because their university called their degree or major computer science, but that doesn't mean they aren't still wrong, either.",hujw4dq,t1_hujjz6g,1643352342.0,False
secwcr,"You definitely need to know things that require trigonometry to learn (or at least are approached via trig even if they don't strictly require it) in order to write a good, performant compiler or database.",huiz4wg,t1_huiswen,1643335868.0,False
secwcr,What do you need trigonometry for in a compiler?,huj55ni,t1_huiz4wg,1643338488.0,False
secwcr,"Not exactly what you asked but, I mean, the math behind compiler shit is so difficult (think about language automata, grammars, parsers, optimizations) that I'll go out on a limb and say that if you can't understand trigonometry don't even try to write a compiler.

Sometimes, especially in undergrad, learning math is useful to teach you mathematical thinking, whatever is the subject.",hujyztx,t1_huj55ni,1643354230.0,False
secwcr,Not what I said. Read it again.,huj6flh,t1_huj55ni,1643339044.0,False
secwcr,What do you need to know that requires trigonometry to learn in a compiler?,huj9o8u,t1_huj6flh,1643340482.0,False
secwcr,"Calculus (actual, not lambda) is usually built from pieces including trig (hence the name of the OP's class) and is used extensively in optimisation. You can write a compiler without it, but you can't write a good, performant compiler.",hujay87,t1_huj9o8u,1643341052.0,False
secwcr,How many people write databases and compilers? A tiny minority of devs,huj4io2,t1_huiz4wg,1643338209.0,False
secwcr,Most devs aren't computer scientists.,huj6hbs,t1_huj4io2,1643339065.0,False
secwcr,It will depend on what problems you want to solve. Most computer programming solutions will be mathematical in nature. Data Structures and Algorithms or heavily based on mathematical theory.,huikdxe,t3_secwcr,1643329623.0,False
secwcr,"Answers here already cover most of it: if you want to be a programmer, understanding the math you use isn't exactly necessary. If you want to stay in computer science beyond an introductory level, you'll do well to actually understand what the math you use *means*. 

Computer Science (and many other STEM fields) are fields that train you to think. Understanding ""why"" something is the correct answer is arguably more important than knowing the answer itself.

But this also goes for any STEM based field, it's not really enough to just memorize steps like you're following a recipe; that will only get you so far. A cookbook is really useful for a burgeoning home cook but if the aspiring chef doesn't ever think about why types of flavors work well together or why a particular ingredient was used in one recipe but not the other... then they'll really only ever be able to cook the things in the book. It will be much more difficult to create dishes of their own.

Any area of study that you pursue, if you don't understand the basics for *why* something is useful to the problem you're solving, then generalizing what you know to slightly different problems will be that much more challenging.",huj0xtr,t3_secwcr,1643336648.0,False
secwcr,"Computer Science, as an academic field, is very mathematical.

Most people who graduate with CS degrees work in jobs that do not require a lot of math.",huiofaw,t3_secwcr,1643331305.0,False
secwcr,Remember that computer science does not equal programmer. Computer science is pretty much math but with computers.,huisibf,t3_secwcr,1643333034.0,False
secwcr,"""Computer science"" and ""programming"" are two different things. You don't actually need much math for programming except for in certain fields (e.g., programming a game physics engine).",huiqxy1,t3_secwcr,1643332365.0,False
secwcr,"It really depends on what you mean by ""understanding the underlying workings of it"". And it also depends on your specialization.

For CS in general, it helps to have a feeling for maths, but it's not required to be an absolute math whizz. However, understanding the logic behind math will help you a lot with algorithms, automata and basic programming components such as recursion.

For cryptography, group theory is pretty important, and it is also the more mathematical part of CS. For programming, or web/network security, it is less of a focus.

You can absolutely get through CS without understanding all the specifics of calculus or trig (afaik they don't go that deep into it either), but you shouldn't despise it. A lot of the fun of computer science is figuring out how things work exactly and precisely, modeling it and translating it into language that is unambiguous. You can see how it is similar to maths in that way.",huirrc0,t3_secwcr,1643332714.0,False
secwcr,"Everything is tough the first time you learn it. Calculus will get easier (algebra is actually more important). The first day of calc we did limit proofs and I was so lost. Years later delta epsilon proofs seem trivial. Just attend the lecture, and then if you are interested, do more research. Linear algebra is definitely your friend.

Edit... memorize the unit circle, very helpful.",huj98iw,t3_secwcr,1643340289.0,False
secwcr,"I am sorry that you have to go through all this because of degree fetishism, even for seemingly unrelated jobs like CRUD applications. But university should not give you automatic job expectation. You should ideally be there for science, not to have a guarantee to land a webdev job.",huka7nc,t3_secwcr,1643362508.0,False
secwcr,Computer science is literally mathematics. You did sign up to be a mathematician.,huik11r,t3_secwcr,1643329475.0,False
secwcr,Is this a joke?,huj4ca4,t1_huik11r,1643338132.0,False
secwcr,"You must not even understand what computer science is to ask such a question... Computer science is not a synonym for programming or software engineering, it is a branch of mathematics that focuses on the theory of computation, efficiency, etc.",huj85rd,t1_huj4ca4,1643339816.0,False
secwcr,Exactly,huj9dmh,t1_huj85rd,1643340351.0,False
secwcr,"Not to be a jerk about the question, but I don't understand why the math component of cs is such a concern. 

I feel like this is a recurring post at least monthly",hundw9k,t3_secwcr,1643411722.0,False
secwcr,Math to some extent is important In comp sci but to fully understand computer science you by no means have to be a math expert. I had the same question when i started my degree in computer science and the answer that was given to me was that I'd need to learn the math that was relevant to certain aspect of the field but unless you go into a job that requires advanced mathematics you probably won't require anything beyond calc 2 or 3. I'm no expert though and others probably have different experiences in the field.,huik9s1,t3_secwcr,1643329575.0,False
secwcr,">be a math expert

Good thing those math credits weren't counted towards my major average....lol",huj5vre,t1_huik9s1,1643338809.0,False
secwcr,"Depends on the uni. Some CS programs are more math heavy 

In real life, only a small percentage of devs use heavy math",huj4akg,t3_secwcr,1643338110.0,False
secwcr,"\>  I need to literally understand why   
everything works the way it does for Trigonometry, Calculus, and   
whatever other math is needed for Computer Science.

What specifically do you mean by this? Are you asking about understanding what it's built of? Or like formal proofs?",hulhts2,t3_secwcr,1643385773.0,False
secwcr,"You actually did sign up to be a mathematician. Computer Science is a sub field of math.

Almost all of computer science is math, you just don’t think it is. Discrete math, combinatorics, category theory, are all things you will use on the regular when writing code, but it’s very different math than what you are thinking. 

And no you don’t need to know calculus generally.

Math is much much bigger than just algebra/calculus/geometry",hujyjjk,t3_secwcr,1643353921.0,False
secwcr,">Like, I didn't sign up to be a mathematician

On a tangent, CS itself is a runaway branch of Mathematics. It (or at least it's theoretical basis, which all that hardware just implements) was spawned literally by Mathematicians.",hujkfpn,t3_secwcr,1643345618.0,False
secwcr,"So, will I have to explain how math works at the fundamental level? I don't think even Computer Scientists know everything that there is to know about math.",hujljfp,t1_hujkfpn,1643346196.0,True
secwcr,">So, will I have to explain how math works at the fundamental level?

You'll have to know enough math to be able to understand the concepts in a particular sub-field that's employing that math.

**Example** : Lets say you're solving a recurrence relation by substitution (useful for finding time complexity of recursive algorithms).

You need to be able to recognize an arithmetic/geometric progression when you see one and know how to find sum of such progressions.

Stuff like that.

On a different note:

1. Being able to apply math and understanding how it works aren't two separate things. To do the former, you need the latter.
2. If your college demands you take a math course, you'll be taking it and understanding the content, one way or another.
3. There are topics in Discrete Maths (commonly taught in CS programmes) that do explore how certain bits of Math work at a fundamental level (eg. formal definition of what a Boolean Algebra is in terms of sets and lattices).

...

If I had to sum it all up, if you're going to do CS, it's not very productive in the long run trying to create a border between what math you can skip and what you can ""comfortably bear"". Things will become unnecessarily complicated.  

Just go ahead and master whatever bit of math they throw at you. Get comfortable with it.",hujmvik,t1_hujljfp,1643346905.0,False
secwcr,"Yeah, I mean I'm up to the task; really, I'm just trying to gauge the expectation.",hujn15i,t1_hujmvik,1643346989.0,True
secwcr,"> I don't think even Computer Scientists know everything that there is to know about math.

No one knows everything there is to know. There's just so much out there no one person can know even half of it.",hulibh7,t1_hujljfp,1643385961.0,False
secwcr,"Not a single person knows all there is to know about mathematics. It's a huge domain and computer science is only a subset of a subset.

You don't need to worry about perfect recall as long as you know enough to derive one idea from another. Professors will forget a lot of the basics through their time, and what you're really supposed learn in a university maths degree is to ""mathematical literacy"". It's like learning to read, you don't need to read all books or memorise all words, just how to parse what's in front of you, and use context to ask the right questions.

If you want to be a software engineer, you really only need to know the bare minimum for whatever field you're working in. The ability to reliably recognise the rough outline of a problem and to Google around for a solution will probably get you into the top 20% of developers.",hulcegm,t1_hujljfp,1643383745.0,False
secwcr,"I've been a code monkey for 40 years now; for the past 20 I've worked as a back-end developer for high-performance computing clusters. I've never needed math more complicated than boolean algebra to do my job.

That said, there are areas of computing where you **will** need more math - crypto, for example,  or image or signal processing. Beyond that, you need to be able to think logically and have a firm understanding of how your code and your runtime environment work.",huj50nk,t3_secwcr,1643338427.0,False
secwcr,Really depends on what field you go into.,huk9bdo,t3_secwcr,1643361792.0,False
secwcr,"If you want to get through school with a degree in CS then yes you need to know a lot of math. Math details the language of logic and is what the actual science of computers is. If you want to be a developer who writes application code then no you do not need to know the theory that in depth, but it is harder to get a job as a developer without the degree and you might face some inequality in the workforce for not having one.

Yeah you are gonna have to truly understand the base of it all, depending on your country and school, but if proofs is in your curriculum then you gotta really understand it",hukzwc0,t3_secwcr,1643378604.0,False
secwcr,"probably not, but to pass your next test maybe.",hul8syl,t3_secwcr,1643382347.0,False
secwcr,"It’s not necessarily about any specific topics in Mathematics, but the general aptitude and confidence to grasp a given mathematical concept - is what’s required of a good computer scientist. 

I might not have read about modular theorems before, but if it ever comes up as a background for something real, I must be capable of picking it up swiftly. This general ability and state of the mind is what’s truly needed.",hulkps4,t3_secwcr,1643386852.0,False
secwcr,"You don’t really need to understand every nook and cranny, just the general concept.

I’m a CS student & as well a math minor in my second semester. Majority of my home work is based on general calc & trig for math & just typical coding problems (I’m a first year so I know it’s nothing as of now) 

Math in general for CS just basically helps you, it’s just your decision if you want to spend a lot of time into it.",hulokkx,t3_secwcr,1643388270.0,False
secwcr,Calculus is very valuable for CS.,hum9da2,t3_secwcr,1643395954.0,False
secwcr,"From what all of my academic advisors have told me is that math is something that might apply to your career, but the bigger emphasis (at least at my school) is the problem solving skills you learn is the main benefit. Learning how to solve a problem using concepts you know is super beneficial. 


Im in calc 1 right now and they care far more about the why of the theorem opposed to the computational skills (grades care about computational skills, calc as a subject cares more about the why)",humsez1,t3_secwcr,1643403168.0,False
secwcr,Being good at maths makes you a better programmer imo cause it allows you think about ways to solve problems faster,hune1ek,t3_secwcr,1643411781.0,False
sf5gj9,"A TCP load balancer will send a NEW TCP to the next server in a round robin manner, but once the connection has been established it just stays with that server. There is state that tracks the connection to route the packets for that TCP connection to the same server. This is the same as normal HTTP except that websockets typically keep the connection open for longer whereas HTTP connections for web pages or API requests are typically much shorter lived. This means that a naive RR LB may be less effective for Websockets unless it keeps track of how many connections a particular server still has open and tries to send new connections to servers with fewer connections.",huop9c6,t3_sf5gj9,1643433934.0,False
sf7tjb,"Could you rephrase the question? There haven't been any notable programming paradigms developed recently, except maybe type-driven/type-oriented (like in Idris 2), and even that has strong roots in old theorem proving techniques.",huokv73,t3_sf7tjb,1643431485.0,False
seqqpv,"A few things,

People already replied about how none is truly a subset of the other - which is true, but a lot of computer engineering students (and some EE as well) ends up with purely software jobs. And some cs students ends up in a more hardware/CE jobs. 

Its more likely for a CE to get into CE jobs as they took much more relevant courses (advanced computer arch, semiconductors etc) but those are mostly electives for cs in most places - so they could get into jobs requiring those course as well. 

CE jobs include to build the micro-architecture that runs a specific instruction set. Deciding the actual instruction set is done by very senior designers where the degree does not matter at all.

Its not programming in assembly but rather describing how a hardware component should behave - understand and study its limitation regarding temperature, voltage, frequency etc. 

Oh and 
>  Is computer engineering higher prestige than software? 

Prestige should not be a factor since you finished high school.",hul08jf,t3_seqqpv,1643378756.0,False
seqqpv,"Few definitions/descriptions:

  * **Software development** is making software, regardless of abstraction level, used technologies or complexity level. Creating simple calculator in Assembly for ZX Spectrum is just as software development as is creating Facebook front- and back-end using React for modern web.
  * **Software engineering** is:
    * ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"" ~ IEEE
    * ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"" ~ Merriam-Webster
    * ""an engineering discipline that is concerned with all aspects of software production"" ~ Ian Sommerville
  * **Computer Science** is the study of algorithms, computation and information. It spans a range from theoretical studies to practical issues of implementing computational systems in hardware and software.
  * **Computer Engineering** is a branch of engineering that integrates several fields of Computer Science and Electronic Engineering required to develop computer hardware and software.

As one can see, all of them overlap, but none is fully a subset of another",hukymcf,t3_seqqpv,1643378019.0,False
seqqpv,"I have an MSc degree in computer (science) engineering and have 20+ years of experience in the field.

In the past we had mathematicians and electrical engineering. Both area started to use more and more computers therefore two title was born, from the math side programmer-mathematician (a mathematician who writes computer software), and from the electrical engineer specialized in low-voltage current computers . The programmer mathematician brought in the pot the math parts, then electrical engineer brought in the engineering methods and principles. Now, it really depends on the history of the school.

If the school created it's program as an offshoot of the math department it will be called computer science, and they will teach you programming from the point when you already have a blinking cursor. If the school however created it's program from electrical engineering (like mine) you will start with the hardware, and start gaining skill from there.

There is a big part which will be common like programming languages, networks, algorithm theory/data structures/math, cryptography, 3d graphics, etc. so if you are doing a regular programmer job, it does not matter which direction you went.

However, there are differences as well. In computer engineering you will have some digital systems/electronics/embedded development/assembly/etc. courses, system design (like SysML), modeling and model validation, fault tolerant systems and maybe some specialized math courses as well (signals and systems for example).

What I see the biggest difference is the mindset. The engineering way is a very multi-disciplinary way which is based on the well established engineering principles (theory, model, validate, etc.). It uses things from lots of disciplines and combines them to solve a problem in a computer domain. For me the computer science looks more like programming, but on steroids.",hulcrdo,t3_seqqpv,1643383880.0,False
seqqpv,"In general terms computer engineers design computer *hardware* while software engineers provide the software.  There is some crossover in the areas of firmware, device drivers, etc.",hukxt19,t3_seqqpv,1643377644.0,False
seqqpv,Okay my understanding is that computer engineers decide what can be written on the machine because they make the architecture?,hukypql,t1_hukxt19,1643378062.0,True
seqqpv,"They decide it as much as car designers decide where a car can drive, with how much baggage and for what purpose.

^(Audi Q7 wasn't designed for agricultural work, yet I've seen it done)",hukz99b,t1_hukypql,1643378310.0,False
seqqpv,"Computer Engineers take hardware and software courses. Software engineers only take software courses. From a general point of view, computer engineers know how the computer will work from start to finish and produce hardware and software accordingly. Software engineers, on the other hand, abstract the hardware and can focus more. But I must underline that we have undergraduate education here. Undergraduate departments of universities are for training academician candidates. If they want to educate software developers, they can open a vocational course. I think software engineering is in this category.",hul8yqf,t3_seqqpv,1643382410.0,False
seqqpv,"You are basically saying that a guy who makes paper can make any mathematical equasion on said paper. Since he built the paper he knows how to write anything on it perfectly.

There is MUCH more to computers then you imagine. And knowing how to build one does not make you good at writing stuff inside it.",huln16p,t3_seqqpv,1643387706.0,False
seqqpv,"Prestige? I might be reading into this and if I am ignore the rest...

It's like you're trying to compare the two for clout. Sounds kinda douchebagy and shouldn't be how you decide an education or career path. If you study computer engineering you're not smarter or better than a software engineer, hopefully you're interested in hardware. Both have their roles.

Hell, you could be a garbage man, and that should be ok too and nothing to look down on if that's your calling. If the tone I'm picking up is accurate, this is going to make for real difficulty in the team fit department....",hul4t6t,t3_seqqpv,1643380723.0,False
seqqpv,"Yep I know it sounds like that, sorry. I’m currently studying BSc web development/software engineering but I’m also into hardware and embedded systems so I don’t know which path to choose. So I’m starting to think that hardware is cooler cause that’s the core and that’s why I should choose embedded systems. But I also like software…",hul5k8m,t1_hul4t6t,1643381038.0,True
seqqpv,"It's not about what's more cool in a prestige sense, unless you're rich already, it's about what you like, can stand doing, can do well at, and can find a job in.

Sure your degree might give you a bump depending on where you got it, or what its in, but that's only because a subset of people with the douchebag mindset made it through the team fit and personality test as a false positive.

Having done interviews for a large biotech firm, I cant tell you how many smart, but obnoxious people we've declined to go forward with. It makes bad team fit if wonderboy talks shit on the TPM, the test engineer, and the support personnel. You wouldn't want to do their jobs: it takes precision and exhausting levels of effort, and they too deserve respect.

This is in the general life advice category now: if you're really smart and gifted and are making great decisions, that's awesome. Good for you. It does not make you better than anyone else. If you're a genius, you didn't choose your genetics, or your luck, so you don't choose to be smart. Be humble. Also, be fair, plenty of people don't want to dedicate their lives to a profession, and that should be honored too, or they maybe have different callings. Many people arent as smart, or gifted, or didn't have the best start, and we shouldn't think poorly of them. Others are smart but uninterested in making money.This kind of ""I'm smart I deserve to look down on others"" attitude I see has lead to the world we live in now. It's a sad place where majority live in precarity, and have hard lives.

I thought VLIW was really cool in embedded.

I don't work in computer engineering, why not find someone who does and ask them about the profession, and how they like it etc?",hul8rwz,t1_hul5k8m,1643382335.0,False
seqqpv,Computer engineering is the physical part software engineering is the programming part,humx8b2,t3_seqqpv,1643405027.0,False
sdur26,The Algorithm Design Manual by Steven Skiena. He’s a professor at State University of New York and hes posted all his lectures online. https://www3.cs.stonybrook.edu/~skiena/373/videos/,hufwzma,t3_sdur26,1643293671.0,False
sdur26,Grokking algorithms is also a good book.,huf7mam,t3_sdur26,1643278836.0,False
sdur26,I'm in the same exact situation as you're and I really find the book A Common-Sense Guide to Data Structures and Algorithms By Jay Wengrow to be really really good.,huf9bk8,t3_sdur26,1643280191.0,False
sdur26,Seconded this book. Read this OP.,hufgq7w,t1_huf9bk8,1643285429.0,False
sdur26,"hey i have a question, should i learn algorithms first or data structures?",hufbok0,t1_huf9bk8,1643281983.0,True
sdur26,"I found it easier to go through the basic structures first, then basic analysis then basic algorithms",huffadu,t1_hufbok0,1643284488.0,False
sdur26,"Just go in order of complexity, start with simple DS, then simple algos, then more complex DS, then more complex algos, etc.",hug9anp,t1_hufbok0,1643298572.0,False
sdur26,"You start with the most basic algorithms, then most basic data structures and then more advanced basically simultaneously",hufc2he,t1_hufbok0,1643282267.0,False
sdur26,"thanks! :D oh and btw, for data structures... do u recommend the introduction to data structures book?",hufdec9,t1_hufc2he,1643283219.0,True
sdur26,"Almost all, if not all, books covering algorithms also cover data structures",hufvkzg,t1_hufdec9,1643293060.0,False
sdur26,The Art of War  - sun Tzu,hugxiwg,t3_sdur26,1643307300.0,False
sdur26,Grokking algorithms is a great intro,hufjcx6,t3_sdur26,1643287028.0,False
sdur26,"I found Sedgewick to be approachable for self study and a decent intro to the topic.  He has versions of his book out with code examples in many popular languages such as ""Agorithms in C"" or ""in Java"" or whatever.",hugv7p2,t3_sdur26,1643306473.0,False
sdur26,Intro to algorithms 3rd edition,huhdung,t3_sdur26,1643313196.0,False
sdur26,"""The Design & Analysis of Algorithms"" 3rd edition by Levitin, Anany is a good algo book that is language agnostic. Used currently by my prof in an Algo class I'm taking (BS in CS).",huhroqp,t3_sdur26,1643318306.0,False
sdur26,CLRS.,huf5bhm,t3_sdur26,1643276963.0,False
sdur26,"To clarify for people who might not get it:

Introduction to Algorithms, by Cormen, Leierson, Rivest and Stein.",hufnocx,t1_huf5bhm,1643289366.0,False
sdur26,"I see this infatuation with CLRS being promoted everywhere and touted as the be-all-end-all book to (self-)study for the subject. I honestly believe that this does more harm than good. I tried to self-study under CLRS before I started college. It made me scared of algorithms. I went back to it a few times during my undergraduate years, but I still never found it to be all that pleasant of a read. 

I still have this book on my bookshelf. I do go through it from time to time, and by this point, I'm familiar enough with the subject area that I no longer find it as daunting as I did during undergrad. However, for a self-learner, I truly believe that the road to understanding how to understand this book is a decade-long endeavor. For this reason alone, I don't think it is a good recommendation to push onto people as a self-study text.

----------------

I feel like the reason for the hype of the book is because of existing hype. This book was great when it first came out because it was one of the first comprehensive self-contained textbooks that generalizes several modern algorithmic patterns. It made the runtime analyses less of a combinatorial mindfuck and relegated it as almost a secondary citizen in comparison to the intuition and the soundness proofs. It deserves a definitive place in the history of modern algorithmics. By comparison, it is fairly accessible and acted like a great cross-disciplinary introduction to the systematic study of algorithms. Pick any modern Algorithm Design text: Skiena, Kleinberg, Sedgwick, and go through its table of contents. Their sections are almost all laid out in the same order as CLRS.

This is why CLRS is impressive, it is so comprehensive that even after over a quarter of a century, new textbooks in the field still follow the same overall structure that it first pioneered in the early 90s.

However, it's not an easy read. I don't just mean that this is a dense technical brick. 

It lacks a certain coherence. Some of the problems emphasize intuition, others emphasize rigorous proofs of time complexity, and still others emphasize a non-intuitive leap of logic required to prove the correctness of these algorithms. How do you show the correctness of greedy algorithms? Beats me. Within specific chapters, the problems and examples presented seem to jump all over the place as well. Even within the same example, you trampoline around from intuitive constructions onto awkward case analyses of some (unintuitive) abstract representation of the problem. There really isn't a unifying thesis for each chapter (or even each problem it discusses). Of course, there aren't any algo-design texts out there that have solved this problem, but I've definitely gone through a few other textbooks that are significantly more coherent and understandable. (In fact, take any two of Skiena, Kleinberg, Sedgwick, Erickson, and Dasgupta and you'll have a more readable presentation of everything covered in CLRS).

----------------------

Look, I don't want to knock on CLRS for the sake of knocking on it, but I really don't think that this is the textbook to push onto people looking to study algorithms on their own. 

I took both of my undergrad and graduate courses in algorithms with Kleinberg (both the textbooks and the lecturer) and the one thing that I am most grateful for in my second attempt to understand this material is the clarity of the presentation. If it weren't for the fact that I had a fantastic lecturer, it would have taken me many more trials before I would have been able to grasp the topic. This despite my general view that Kleinberg's textbook is much more difficult to get through than CLRS.

Unfortunately, CLRS - on its own - does not stand out particularly well when it comes to clarity of presentation.

----------------------

First - texts focused on **designing algorithms**:

I can also talk about the (non-CLRS) textbooks that were popular when I was in undergrad: Ericksons, Skiena, Kleinberg, Sedgwick, (and Dasgupta).

My favorite of the batch is **Sedgwick's Algorithms**, it's the most accessible of the batch, and takes a very grounded approach to help build up your intuition for the subject. For many of us, rereading it after more rigorous training may feel like this is almost too introductory, but that's what makes it an especially great guide to learn from on your own. My favorite part are the runnable examples that help to (literally) visualize these algorithms you're studying.

**Skiena's Algorithm Design Manual** is definitely the funnest read of the batch. The presentation on the actual algorithms themselves is great, it's got a light-hearted expository style like K+T, with a good dose of visual intuition like Sedgwick. However, the best part of the text is the pacing. It's the only textbook so far where I didn't feel fatigued after reading more than one or two sections at a time. This is because Skiena sprinkles small doses of breathers throughout the text; every 10-20 pages of text (mod exercises), he would add some fun anecdotes (""war stories"") related to the material. These are surprisingly interesting and engaging for a textbook on algorithm design, and it definitely helps to pace the book in a way that few other textbooks have really thought to do. 

On the flip-side, unlike CLRS, K+T, or Erickson, the ""Manual"" isn't nearly as comprehensive in the types of material covered. In terms of the core theoretical algo-design foundations, we get through the basic DS, search, graph traversal, and some dynamic programming + approximation algorithms. However, instead of drilling deeper into these topics and, for e.g., complexity + tractability, the ""Manual"" pivots in the second half towards more practical applications. Many of these aren't really what you'd take away from from a typical algo-design undergrad course, but they're nevertheless very insightful, and significantly funner to read and learn about.

I haven't gone through all of **Erickson's textbook** yet, though this post motivated me to reread through a few chapters (Recursion, Dynamic Programming, Greedy Algorithms, and the two chapters on Max Flow/Min-Cut and applications of network flow). It's really very well written and well presented. In fact, I would even go as far as saying that it's genuinely fun to read through the chapters! I think it still suffers from some the same problem that many other algorithms books suffer, but it's definitely one of the most readable and accessible books on the market. The problems and algorithms presented are fairly standard (and covered without the depths of its denser counterparts). 

It feels a bit ""dated"" in the sense that it's still an exact (more-or-less) subset of all of the other Algorithms texts out there. For example, the use of Tower of Hanoi to illustrate recursion; Fibonacci #s, edit distance, and substring partitioning for dynamic programming. Gale Shapely's solution to stable matching, maximal interval scheduling, and huffman coding for greedy algorithms. These problems more or less encompass the entirety of these chapters as well. That said, that's not really a weakness. There hasn't been any significant breakthroughs that can/should be covered in an introductory monograph on Algorithm design. However, I'm impressed at the depth this book goes into for network-flow type reductions; until recently, this wasn't a topic indulged in much depth outside of Kleinberg & Tardos's Algorithm Design. (the bonus chapters also sound like fun extensions)

At the same time, I think my main gripe is that the text is still very heavily focused on presenting why a construction or design of a solution is the right one. This is obviously important, but it doesn't quite impart you with the intuition to design algorithms more generally. For example, why did we use this functional recurrence for edit distance and not look at other subproblems? How did we magically think about looking at the finish time for optimal interval scheduling? Why didn't we look at other types of network constructions for Baseball Elimination? That said, the section on stable matching does go down a false start first before giving the G&S construction. Of course, this is a shared problem that almost all textbooks share, nevertheless, I would have loved if there was a textbook that delves more into why seemingly intuitive algorithms don't work.",hujqz3z,t1_huf5bhm,1643349193.0,False
sdur26,"
--------------------

Next, onto textbooks about **analysis of algorithms**:

I love playing around with generating functions, and **Sedgwick's Analysis of Algorithms** spends a significant portion of the text on how to do these very novel and creative runtime analysis with tools like generating functions. He also published another text on Analytic Combinatorics, which is presented as a series of monographs of his own personal research interests. It's surprisingly accessible, very visual and intuitive, and leaves you with a feeling that you can count just about everything, or at least you can encode these problems as real-valued functionals and look for their singularities to understand how to count them. 

Next is **Kleinberg's Algorithm Design**, but again I'm biased because I was his student for algo. In contrast to CLRS, the best part of Algorithm Design was the exposition and the style. It's still very technical and theoretically inclined, but Kleinberg + Tardos put a lot of effort to make the text more readable. The other distinguishing feature of Algorithm Design is its inclusion of some more exotic algorithms that you usually don't find in a first course on algo design. 

Both K+T are algorithmic game theorists, and their influence in these and econometric algorithms shows through in their text. Most algo design texts mention reductions to max-flow/min-cut. Algorithm Design however spends significantly more of its tree-budget on network-flow type algorithms, both in the Network Flow chapter, as well as throughout in Greedy Algorithms, Approximation Algorithms, and in the discussion of complexity and tractability. His (in)famous course (CS 4820) on algorithm design also spends about a third of the semester on Network Flow back when I took the undergrad course.

Otherwise, I've never read through any significant chunk of Dasgupta. I hear good things about it, it's definitely the smallest text of the bunch.

Beyond these, the field has become significantly more popular since I've left school, so I don't doubt that other textbooks / resources have started to overtake these.",hujqzgu,t1_hujqz3z,1643349199.0,False
sdur26,"The 4th Edition is planned to  be released in March this year ""A comprehensive update of the leading algorithms text, with new material on matchings in bipartite graphs, online algorithms, machine learning, and other topics. """,huk0hw5,t1_huf5bhm,1643355250.0,False
sdur26,"Algorithms with C by Kyle Loudon. It is a very well written book that takes you from basics. The only annoying thing is the comments in the code waste so much space. Other than that, it is a very good book.",huh7xsl,t3_sdur26,1643311029.0,False
sdur26,"https://teachyourselfcs.com

This is a great website that points you to various resources for teaching yourself CS concepts.",huilgti,t3_sdur26,1643330075.0,False
sdur26,"You want lectures and other materials by Leiserson and/or Skiena. Leiserson taught most people writing on the subject, including Skiena. Both have lectures available on youtube, and have authored books together.",hugit8w,t3_sdur26,1643302096.0,False
sdur26,"algorithms to live by, it's not a book for CS education but very fun and useful for beginners",huh9kaj,t3_sdur26,1643311624.0,False
sdur26,Not a book but something to use as a companion piece for whatever book you end up going through. https://www.geeksforgeeks.org/fundamentals-of-algorithms/ helped me out so much.,huhctk5,t3_sdur26,1643312822.0,False
sdur26,I'm using algorithm design by kleinberg and tardos right now and it's great.,huhep3x,t3_sdur26,1643313506.0,False
sdur26,"I haven't gone through it but I picked up  ""Algorithms In A Nutshell""  by George T. Heineman, Gary Pollice, and Stanley Selkow.",huhf58k,t3_sdur26,1643313668.0,False
sdur26,"Algorithm Design Manual by Skiena

A bit old school from a language point of view, but taught me a lot.",hui5m5m,t3_sdur26,1643323574.0,False
sdur26,I don’t compute! What is this education background being equated to not being self taught?? There are very few professors that can explain things where you don’t have to teach yourself! 😂😂😂,huiem9p,t3_sdur26,1643327212.0,False
sdxd4b,Freebie  http://www.cl72.org/110dataAlgo/Algorithms%20%20%20Data%20Structures%20=%20Programs%20\[Wirth%201976-02\].pdf,hufl15i,t3_sdxd4b,1643287967.0,False
sdxd4b,"Thank you for your reply! I have access to O'Reilly Learning through my institution so if there is also any paid books they're likely on there.

I will check this out anyway!",huflewu,t1_hufl15i,1643288178.0,True
sdxd4b,A Common-Sense Guide to Data Structures and Algorithms. Perfectly suited for beginners.,hufncu1,t3_sdxd4b,1643289209.0,False
sdxd4b,">A Common-Sense Guide to Data Structures and Algorithms

Thank you! Having a quick flick through this looks exactly what I want - will mark this thread as solved!",hufq5jc,t1_hufncu1,1643290577.0,True
sdxd4b,"If you’re a visual learner, grokking algorithms (the actual book not the educative.io course) is really nice for beginners",hufsr43,t3_sdxd4b,1643291791.0,False
sdxd4b,Algorithms by sedgewick and wayne hands down. Still one of my faves.,huigqi5,t3_sdxd4b,1643328098.0,False
sdh6gc,[Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl%27s_law),hucp2h3,t3_sdh6gc,1643233361.0,False
sdh6gc,"You should definitely read the linked details but to give a quick and intuitive description of Amdahl's law:

Imagine you're baking a cake. More people can help, but there's a limit. One person can make the frosting while you're mixing the batter. That splits pretty easily and makes sense to do.

Theoretically you could have different people measure each ingredient out or one person per egg to crack but these steps are so quick you'd probably spend more time coordinating with each other than actually just doing the work yourself. Technically can be done in parallel but probably not worth it.

Then you need to actually bake the cake. I don't care how many people you have, you can't put the cake in until the batter is mixed. It doesn't matter how many ovens, the cake won't cook any faster. These steps fundamentally can't be parallelized.

So having a few people (cores) helped some phases (making the batter and frosting), but having more didn't do much (hard to split work when making the batter), while some steps take the same time no matter what (baking). The same principle applies to programs. Some things are easily parallelized, some things are hard to parallelize, and some things simply can't be parallelized.",hud0yh3,t1_hucp2h3,1643237822.0,False
sdh6gc,Is there a method to help identify tasks that benefit from parallelization and tasks that won’t?,hudwy4h,t1_hud0yh3,1643250984.0,False
sdh6gc,"The easiest tasks to parallelize are what we call ""embarrassingly parallel"". 

For example: if your task is purely functional (output depends only on the input and produces the same output for the same input every time) and you need to perform this task on many different inputs, then you can do them all at the same time since no single task like this depends on another one.",hueacnm,t1_hudwy4h,1643256943.0,False
sdh6gc,"That's basically what GPUs do. It's called number crunching. Calculating the color of each pixel on the screen in parallel. Also calculating weights of neural networks in parallel if you use them for AI training. Basically almost everything that makes heavy use of matrix operations can be parallelized like that and hence run on a GPU, thousands of such operations simultaneously.",huez8n5,t1_hueacnm,1643272096.0,False
sdh6gc,"Simple tasks that don't do I/O and have all the state they need available quickly can be blazingly fast.

For instance, that's all the GPU really is: just processes a bunch of fairly simple stuff (floating point math for graphics) at a level your CPU can't dream of. 

It's fairly intuitive to see what will benefit and what won't at the code level, but often harder to design the distributed system to enable more of those opportunities.",hueeaih,t1_hudwy4h,1643258877.0,False
sdh6gc,College campuses use Amdahl’s law to prevent students from filing complaints by filling the process with multiple layers of slow administrative positions. Fun fact.,huer27u,t1_hud0yh3,1643266204.0,False
sdh6gc,Really? :),hurbayi,t1_huer27u,1643485940.0,True
sdh6gc,Thank you kind mister for this good teacher like explanation :),hur90pf,t1_hud0yh3,1643485026.0,True
sdh6gc,Thank you for this!,hur8ue8,t1_hucp2h3,1643484958.0,True
sdh6gc,"Thoughts.

1 Woman can have 1 baby in 9 months

9 Women can have  9 babies in 9 months

9 Women cannot have 1 baby in 1 month

Some problems are linear",hud9t12,t3_sdh6gc,1643241376.0,False
sdh6gc,"> 9 Women cannot have 1 baby in 1 month. 
  
That right there is the *crucial* bit.  
 
Natural Human reproduction is a process that typically requires a woman and 9 months gestation for success. You can’t split the work and you can’t really speed it up.",huhwd1k,t1_hud9t12,1643320038.0,False
sdh6gc,"Understood, thanks Cyher! :))",hurb96s,t1_hud9t12,1643485920.0,True
sdh6gc,"The main reason can be summarized as ""communication overhead"". Cores share the same memory buses, some caches and same infrastructure on the system.

Sometimes cores need to pass data between themselves to execute the algorithm, and this can be expensive in terms of bandwidth and time (in processor scale).

This incurs some communication penalties as the programs scale up. Sometimes memory bandwidth falls short, sometimes caches need to be flushed and refilled much more, etc., hence the scaling up is not perfect or linear.

Source: I develop multicore software.",hucpkyz,t3_sdh6gc,1643233544.0,False
sdh6gc,Thank you for sharing your knowledge and experience man! :))),hurv477,t1_hucpkyz,1643494240.0,True
sdh6gc,You're most welcome. I'm glad it helped.,hurvj9e,t1_hurv477,1643494418.0,False
sdh6gc,">That's basically what GPUs do. It's called number crunching. Calculating the color of each pixel on the screen in parallel. Also calculating weights of neural networks in parallel if you use them for AI training. Basically almost everything that makes heavy use of matrix operations can be parallelized like that and hence run on a GPU, thousands of such operations simultaneously.

Thank you for your input good sir! :)))",hur9ki0,t1_hucpkyz,1643485242.0,True
sdh6gc,Multi and parallel processing are still subject to the law of diminishing returns. [Amdahl's Law](https://en.m.wikipedia.org/wiki/Amdahl's_law) may be what you're looking for from a theoretical perspective.,huctrhw,t3_sdh6gc,1643235073.0,False
sdh6gc,"Will take some look into this, thanks Music man! :))",hurazvs,t1_huctrhw,1643485816.0,True
sdh6gc,"Besides what has been said already: Not all problems can be expressed in a way that allows parallelization like in signal processing. Often computations are dependent on previous results. This is not necessarily due to ""imperfect algorithms"". It's in the nature of the problem.",hucz7zz,t3_sdh6gc,1643237152.0,False
sdh6gc,"Its something that has been said on wiki, but yes you are right, computation can be rather dependent on previous results :) thanks Wayne!",hurb75n,t1_hucz7zz,1643485897.0,True
sdh6gc,"Multi-core CPUs ""do"" perform N times better for right kind of tasks. Two examples are :

* Neural networks or any kind of Matrix multiplication . These tasks can scale up to thousands of cores on GPU.  Similarly, performs scales with number of cores. 
* Financial simulations like Monte Carlo",hug07r9,t3_sdh6gc,1643295019.0,False
sdh6gc,Thank you for this info! :),huray5v,t1_hug07r9,1643485797.0,True
sdh6gc,The same reason it doesn't take 4.5 months for two pregnant women to make one baby.,hud00l3,t3_sdh6gc,1643237456.0,False
sdh6gc,"Understood, thanks! :)",hurav8d,t1_hud00l3,1643485764.0,True
sdh6gc,"One of the primary limitations on x86_64 processors right now is the complexity of the instruction set.  Processing the machine code before the cpu truly begins doing math on it is a pita on x86_64 processors, regardless how many cores you have.  This is one of the key 'secrets' to the M1 processor's speed, because it's instruction set does not have this issue.",hucyskf,t3_sdh6gc,1643236984.0,False
sdh6gc,"> Processing the machine code before the cpu truly begins doing math on it is a pita on x86_64 processors

Could you expand on this a bit? I think you're referring to instruction pipelining, but it's hard to tell.",hud6cb1,t1_hucyskf,1643239961.0,False
sdh6gc,There are a lot of articles online about it.  Here's the top google hit: https://debugger.medium.com/why-is-apples-m1-chip-so-fast-3262b158cba2,huhkqu2,t1_hud6cb1,1643315738.0,False
sdh6gc,"You have just explained CISC vs RISC :)

ARM (eg M1) is basically RISC",huda3ve,t1_hucyskf,1643241501.0,False
sdh6gc,"Instruction set complexity has nothing to do with why 4 cores aren't 4 times faster than 1 core.  Each core has its own decoder, scheduler, registers, etc.

It does impact clock speed and instruction level parallelism, which is a separate thing.",hueuou9,t1_hucyskf,1643268698.0,False
sdh6gc,Work versus Span,hudaegh,t3_sdh6gc,1643241621.0,False
sdh6gc,"You might want to take a look at BeOS and Haiku OS. ""Pervasive Multithreading"" was built in at the OS level. If anyone is going to run into limits of multi-core or multithread processing, it would be them.",hudt9lx,t3_sdh6gc,1643249456.0,False
sdh6gc,Thanks for info! :),hurakjo,t1_hudt9lx,1643485643.0,True
sdh6gc,"There’s overhead in creating a multithreaded program. Creating threads is taxing, dividing tasks is taxing, and combining results is taxing. It’s the same reason two people won’t sort a bowl of M&Ms twice as fast as one, work needs to be divided and combined.",hue89k8,t3_sdh6gc,1643255946.0,False
sdh6gc,Thanks man for your input! :)),huranhi,t1_hue89k8,1643485676.0,True
sdh6gc,"This doesnt answer the question OP, but may help you understand the issue in more detail. Current Personal Computers have identical / nearly identical cores so it shouldnt matter if a process is being run on which core (i dont know if something like core#0 takes initial boot, its too detailed for my level of hardware knowledge)

There was a computer architecture that was designed to have programs run in parallel using a different architecture and that was the CellBE (Cell Broadband Engine). There was the main computer (which had hyperthreading) known as SPE (Synergistic Processing Elements) and there were 8 or so SPU( Synergistic Processing Units) which had a different instruction set to the SPE and there was a lot of space on the chip to allow SPU's to communicate with the SPE as well as each other. Theoretically, one SPU could decompress, pass to next which decrypts, then pass onto another which does rendering.  

The CellBE was in the PS3 and i understand while it performed well, it was a pain to code for.  

https://en.wikipedia.org/wiki/Cell\_(microprocessor)",huer1ma,t3_sdh6gc,1643266193.0,False
sdh6gc,Thank you Scott! :),hura1ym,t1_huer1ma,1643485434.0,True
sdh6gc,"It’s complicated, but the thing you mentioned first is a major cause and there are some inmediately related snags.
 
—
  
To elaborate: *If* CPU1’s task has to be completed *first* before CPU2 can do it’s thing, then you’re right back to square one and might as well use a single
processor and process. There’s going to be a lot of waiting involved otherwise.
 
Basically you have to be able to divide up the work into either *order-independent* computations or entirely separate tasks. This applies to all kinds of stuff at a fundamental level.  
  
Combining all the separate computations at the end can also force you back onto a single CPU or result in resource contention which I mention further down.
 
—  
  
In addition, if your code *blocks* on I/O transfers, then  the CPUs/Cores can suffer from *contention* over any *shared resources*. 
 
This basic issue applies to main RAM, data caching, storage, communication buses, etc.  
 
It’s also critically important not to have *collisions* or issues with flushing buffers, etc. You have failed before you really started if “CPU A was supposed to do Task A and CPU B was supposed to take Result A and peform Task B” and CPU B checked for the results before CPU A had finished… 
  
P.S. 
  
Imagine 50 people trying to dig a whole with 25 shovels, put all the dirt into 4 big piles and *not* run into each other or waste a lot of time….",huhvcbo,t3_sdh6gc,1643319657.0,False
sdh6gc,"Thank you for your input, this is some interesting info! :)",huraitb,t1_huhvcbo,1643485624.0,True
sdh6gc,Thank you all for your interesting and knowledgeable input here! :)) <3,hurbgel,t3_sdh6gc,1643486002.0,True
se2nq0,Stuff like that all fall under type theory I think.,huh08gr,t3_se2nq0,1643308252.0,False
se2nq0,You’re looking for type theory and effect systems.,huhavuu,t3_se2nq0,1643312112.0,False
sd0e5c,"Edit: Sorry, I did not read through the question. You want these in the context of ""type theory"". 

1. Comonad has been encoded in [Haskell](https://hackage.haskell.org/package/comonad), so is [coeffect](https://hackage.haskell.org/package/effect-monad-0.8.1.0/docs/Control-Coeffect.html). Given one of the guy behind coeffect, Tomas Petricek, is fantastic at F#, I am surprised I cannot find a encoding of coeffect in f#
2. Algebraic effect and handler can be added in a type system. There are languages like [eff](https://www.eff-lang.org/) and [koka](https://koka-lang.github.io/koka/doc/book.html). I have not found any research article on a ""practical type system"" (like system F or Hindley-Milner) enriched with algebraic effect (I suspect you can find some publication related to Koka), but I have found [dependent type with algebraic effect](https://dl.acm.org/doi/abs/10.1145/2500365.2500581).
3. ATS tutorial provides lots of cool use of linear and dependent type to guarantee program safety, especially for heap manipulation. Unlike Haskell, ATS does not separate effectful and pure computation. 
This is for good reason, as ATS aims to be a fast language, it prides itself to do safe low level memory access. Separating effectful fragment and non-effectful makes these efficient codes hard to incorporate into larger programs. 

------


Effect system is a huge research area. It is not my research area, so I probably will say something wrong here. I am just listing some research topics that are related to effects. You can look into them, and hopefully find more useful research on their references. 

You already know the monadic effect, however there are some effects that can be more effectively modeled as comonad. some reference on comonad as a model of computation: https://www.sciencedirect.com/science/article/pii/S1571066108003435

There are notions of coeffect, and I am not sure if they are the same as comonadic effect. This seems like an interesting paper unifying coeffect and effect: https://dl.acm.org/doi/abs/10.1145/3022670.2951939 I am sure you can find some reference of coeffect in there. 

For research on monad, coeffect, and general categorical semantics, I would recommend to look into the work of [Tomas Petricek](http://tomasp.net/) (coeffect), Tarmo Uustalu (categorical semantics), and Marco Gaboardi (graded monad).

There is also what called algebraic effect (in the sense of algebraic data type, things defined without quantifier, or as an initial algebra). There is also a notion of [monad algebra](https://bartoszmilewski.com/2017/03/14/algebras-for-monads/), that to me seems to be slightly related to handler, but I am not sure.  

I recommend you to look into the work of [Matija Pretnar](https://matija.pretnar.info/) and [Andrej Bauer](http://www.andrej.com/) on this line of work.

There is also some less categorical approach where they focus on specific effects. One of the most popular one is reading and writing from memory, since it is one of the more theory rich effects.

This is reeeally not my area, so my ramble will get even less stable. 

I think this publication might be relevant here? https://dl.acm.org/doi/abs/10.1145/2535838.2535869 I heard logical relation is very commonly used in this line of research to prove effectful program equivalence.

Also as you have mentioned that linear system can encode some safety properties of effect (for example no read before write etc.), but I have not found any relevant research on that, but you can find some [tutorials on ATS](http://www.ats-lang.org/Documents.html#EFF2ATSPROGEX) that covers topics like this. ATS is a language with linear type, and it uses linear type a lot to guarantee some security properties of effects.",hublxuj,t3_sd0e5c,1643219446.0,False
sd0e5c,Check out the Flix language for some interesting research in polymorphic effect types.,hubbjhk,t3_sd0e5c,1643215791.0,False
sd0e5c,"Not sure about cutting edge research. But for everyday otj development, it's all about limiting side effects via decoupled, modular design.",hubi28u,t3_sd0e5c,1643218094.0,False
sd0e5c,"Imperative programs without side effects do not really make sense in my opinion. What is a statement, if not something that affects the current state? If so, the formal semantics require a current state, and that is what having side effects means.

On the contrary, functional languages can be given semantics that do not need external state, by merely rewriting one term into another until you (eventually maybe) reach a normal form.",hubr402,t3_sd0e5c,1643221278.0,False
sccyyv,"The people who are loudest on social media are by no means the experts in their field. Working deeply focused all day, every day also is not a good way to keep up with what other people are doing. You might want to take a few hours a day to do this. (If you're doign deeply focused work in a lab, that would usually mean you eventually produce research original enough to present at a conference. Attending them usually gives a good overview what others are doing).",hu68zf8,t3_sccyyv,1643129761.0,False
sccyyv,"I approach this from a university academic perspective, so you may have different goals and access to peers.
 
The best thing ever are reading groups. Each discipline and subdiscipline should have a reading group, made up of 2 'layers' of people - 1 layer that is people who are actively in that space, or at least closely tangential, and then a bigger layer which is just anyone interested. For subdisciplines these are usually pretty small, for the larger disciplines these might include the whole school and beyond into other faculties and industry. Each <period>, depending on how many people, interest, publishing rates etc, as well as tied to each conference and major serial, you have people divvying up the papers that were released in that period to the internal layer to read. They then give a snap summary (1-3 sentences max) of those papers to a mailing list, with short recommendations, i.e. from 'this is garbage' to 'you must read this'. Then the 'you must read this' papers are read (or at least skimmed) by the outer layer who are interested, and everyone comes together to discuss them. If they're particularly good papers they get shunted up from the subdiscipline group to the main discipline for everyone to read.
 
This cuts the required publication reading to keep absolutely abreast of what is going on in the whole of computer science to about 1% of what it would otherwise be - because 90% of the papers that are published are crap, either wrong, or just rehashed old stuff, and 9% are hyper-niche interest only.
 
Note that (in an academic setting at least) you don't need to follow twitter, podcasts, youtube, news, or subreddits. They fall into 3 categories - people doing work that has been published elsewhere, people doing work that isn't worth publishing, or accounts run by university marketing departments trying to boost their institution's profile and citation rates. I'd still recommend following them, but they are far more of a 'I am bored and am looking for something to read/watch while I am eating/pooping/travelling/supposed to be sleeping' thing than anything to be religiously consuming.",hu7szqs,t3_sccyyv,1643150901.0,False
sccyyv,I think the most efficient way is to follow them on social platforms.,hu5edob,t3_sccyyv,1643117077.0,False
sccyyv,"Why do you need to be so highly efficient in remaining up to date in your field? Are the older technologies of your field so atrocious that the updated technologies destroy the meaningful existence of the older technologies?

As for me, I strongly believe in understanding the fundamental of your field. The fundamentals rarely change so it's perfectly feasible to focus only on the fundamentals to gain as much insight as you can. When you understand the fundamentals, you should be able to pick up on the meaning of any new technologies (that build upon the fundamentals) that happen to come up over time.",hu5oa5o,t3_sccyyv,1643121679.0,False
sccyyv,wtf why would people downvote this? i totally agree,hu7l5t1,t1_hu5oa5o,1643147761.0,False
sbw98k,"Most generic compression algorithms don't assume any specific file format.  They look for byte-level redundancies and work to eliminate repeated information.

I'm not aware of any general-purpose compression algorithm that will identify the more semantic redundancies you've described.  I'd argue that if you're aware of this kind of structure, you should encode it explicitly in the format.",hu2kut2,t3_sbw98k,1643059121.0,False
sbw98k,"You have described a domain specific problem, and compression method devised for the specific domain will always beat the general algorithms.",hu3eru4,t3_sbw98k,1643074095.0,False
sbw98k,"If we lived in a world where storage space or network bandwidth was more of a bottleneck than they are, it is likely we would see 'plugin' compression algorithms used with a general algorithm layered on top of known file type specific algorithms. But that isn't the case - storage and bandwidth is continuously cheaper, and implementation complexity is probably the most significant limiting factor. The implementation complexity necessary to recognise that that transformation has occurred would be prohibitive (not to mention the actual computation required to recognise and compress it would be a significant cost).
 
It's at the point now where not even text is treated differently to random binary data for compression. While you could theoretically compress something more if you knew about things like stemming words, it's not worth it given the gains are so small vs just looking at common bit ranges.",hu3xwal,t3_sbw98k,1643082549.0,False
sbw98k,"I don't completely agree with you. Having plenty of storage is not an excuse to waste resources. 

Also, network bandwidth is still small in some part of the word, and in places where it's huge the data being transmitted is also ever increasing, so you need compression anyway (even with the fastest network connection we have nowadays, it would be impossible to stream a 4K video without compression). 

Finally, there are applications where compression is still relevant, satellites communication or small embedded devices come in mind.",hu4hqyh,t1_hu3xwal,1643093702.0,False
sbw98k,"Nobody ever said it was an excuse to waste resources, it's a factor in the never-ending balancing act between different compromises. The cheapness of storage and bandwidth means that the value gained from marginally increased compression is tiny, so any cost greater than tiny means it isn't going to be worth it.
 
You need to read what is being written more closely, because the rest of your post is a complete strawman that has exactly nothing to do with what is being discussed. I'm not saying compression isn't useful or used, I'm saying there is no push for specialisation inside general compression algorithms - which is objectively true, because it isn't happening.
 
Video is compressed with a specialised algorithm, not a general one. Satellite communication typically uses *less* compression than you think, because the issue with satellites is latency, not bandwidth, and the processing capability of a satellite tends to be the bottleneck. There will be plenty of compression before and after the satellite is involved, but again there's no value in going after marginal gains there, either.
 
Embedded devices I'm not even sure what you're getting at. As you scale things down processing power drops exponentially faster than storage or network bandwidth. You can cheaply get the same network bandwidth as a full sized desktop on an Arduino micro, and multiple terabytes of flash memory.",hu4j79b,t1_hu4hqyh,1643094705.0,False
sbw98k,"Satellites and embedded systems were just two examples were I have worked on architectures for compression, in which the processing power was out weighting bandwidth. 

You are right though, I missed the point that you were making about specialized compression!",hu4m6r7,t1_hu4j79b,1643096835.0,False
sbw98k,Segmentation into neighbourhoods and then deltas from neighbour avgs.,hu4sb65,t3_sbw98k,1643101638.0,False
sbxbya,"Truly parallel? The amount of cores and CPUs. Concurrent? The amount of threads, cores, and CPUs",hu2pkfr,t3_sbxbya,1643060889.0,False
sbxbya,"Concurrent connections can go even higher than this using non-blocking IO where a single thread can manage N connections. Practically the limit is likely going to be the max number of connections your OS allows at once, but this is tunable with something like linux. If your computer only has one IP address you’ll run out of TCP ports once you start to approach 2^16 connections. But even this can be worked around by using multiple IP addresses. At that point you are only really limited by network bandwidth and memory.

There are special programs build to simulate large numbers of connections per second for scalability testing of things like web servers. Suggest looking at those if this is something you are interested in.",hu2r0x3,t1_hu2pkfr,1643061442.0,False
sbxbya,"That's a good point! If we're looking at asynchronous operations, it's a huge amount",hu2y6un,t1_hu2r0x3,1643064220.0,False
saqw7i,This will come with all sorts of ethical dilemmas in the future. Where does human consciousness actually come from? It's in the petri dish playing pong.,htvz4mj,t3_saqw7i,1642951302.0,False
saqw7i,Computational neuroscience will solve consciousness,htx2088,t1_htvz4mj,1642966498.0,False
saqw7i,Maybe but definitely not in the near future.,htxmypx,t1_htx2088,1642974548.0,False
saqw7i,Why do you think so? Also how near is near?,htxw3gc,t1_htxmypx,1642978048.0,False
saqw7i,"Because the amount of computational power needed to simulate anything close to a normal brain simply isn't there yet.

Edit: And from a biology standpoint we don't know nearly enough about how the brain works to realistically simulate it.

Edits2: Just to elaborate I am refering to a complete simulation down to molecular level",hu0i4ul,t1_htxw3gc,1643029201.0,False
saqw7i,"That's quite a big assumption (that we need to simulate it at the molecular level).

And possibly we might see some big leap with quantum computers, for which simulating quantum systems like molecules is a piece of cake. 

So, I would say it's better to say: we don't know. Maybe, maybe not. That's how things are, those are the paths and let's see! (Or contribute if we are in the area :p)",hu98hsf,t1_hu0i4ul,1643173107.0,False
saqw7i,"Yeah it is indeed. My line of though is that aince we don't know which ""level of abstraction"" (to use our lingo) let's say ""generate"" consciousness, it would be interesting to try and simulate it at different levels.",huh8n35,t1_hu98hsf,1643311286.0,False
saqw7i,Maybe we dont want it to.,htyvx2t,t1_htx2088,1642992534.0,False
saqw7i,5 years,htzlptx,t1_htx2088,1643005267.0,False
saqw7i,Can't beat the good old noodle.,htvhek9,t3_saqw7i,1642942041.0,False
saqw7i,But how do they reward the cells with some sort of loss function? Afaik the article doesn't discuss this.,htxe5mq,t3_saqw7i,1642971202.0,False
saqw7i,"Another article referencing the same paper was posted on r/science a while ago, as far as I can remember these neuron cells react well/ ""like"" predictable signals and ""hate"" unpredictable signals, so when ever they got something wrong they were fed random noise and when they got something right they got a simple periodic signal",htzte1a,t1_htxe5mq,1643010621.0,False
saqw7i,Sex,htzu7ps,t1_htxe5mq,1643011246.0,False
saqw7i,"This will be the AI that actually makes a real difference, if true. I've been watching this space for 30 years, and have always been dismissive. This is the first time I've been impressed.",htvmhp9,t3_saqw7i,1642945140.0,False
saqw7i,It does not take an hour to train an AI to play pong.,htvvvku,t3_saqw7i,1642949865.0,False
saqw7i,"It's based on hardware-speed independent 'time' - based on the number of rallies occurring at a fixed rate, rather than actual time. 5000 rallies for computer-based AI vs 15 rallies for this brain cell version.
 
However, they also state that the brain cells know how to play quickly, but suck at it and would lose vs a standard AI. Which seems very human.
 
Bottom line is the pre-print itself isn't focusing on the speed vs computer AI, it is about the increased performance of human neurons vs mouse neurons, and the potential for future development.",htxhzo2,t1_htvvvku,1642972660.0,False
saqw7i,It doesn’t take a billion dollar company and billions of simulations to teach a teenager to drive a car.,htwchkf,t1_htvvvku,1642956657.0,False
saqw7i,"The difference is that you can ctrl-c, ctrl-v that software.",htxj6bj,t1_htwchkf,1642973100.0,False
saqw7i,that is scary,htx0wau,t3_saqw7i,1642966060.0,False
saqw7i,"This will not spread in mainstream because Elon musk don't post on twitter about this, and don't have a video on youtube discussing about this research",htxduwv,t3_saqw7i,1642971091.0,False
saqw7i,Wow this is.. quite honestly very jarring,hudq370,t3_saqw7i,1643248138.0,False
saqw7i,Completely unethical. I hope they keep the scientists involved in a dark room and force them to play Pong for the rest of eternity.,htxu80g,t3_saqw7i,1642977364.0,False
saqw7i,This is as unethical as a human being born. People dont seem to view the lack of natal-consent as an issue.,htywbad,t1_htxu80g,1642992686.0,False
saqw7i,"Hypothetically, imagine a neural net of the size of a human brain to solve a complex task. If we do not fully understand how brains work, how will we understand what this lab brain will go through? Imagine it lives in hell? Imagine there are pain neurons that are arbitrarily firing and the lab brain is conscious about it.",htzywty,t1_htywbad,1643014958.0,False
saqw7i,"What if our existance is a living hell and we are concious of it, what if life is pain?",hu2v3t2,t1_htzywty,1643063014.0,False
saqw7i,Could be.. but if we can't prove what a neural network perceives in the slightest sense we shouldn't create it in the first place. We have a good estimate of what a baby feels because we see it laugh and we try to give it a good life. We couldn't say that about a pong playing neural net because we can't see inside a black box yet.,hu2wd18,t1_hu2v3t2,1643063502.0,False
saqw7i,"You can always just kill yourself though, unlike these Frankenstein creations.

&#x200B;

If you are going to make such a poor comparison then I will gently remind you that birthing a child for the purpose of forcing them into to slavery is also widely considered unethical.",hu1tue8,t1_htywbad,1643048910.0,False
saqw7i,"Agreed, while it is the consensus that natal-consent is unimporant I hold the uncommon belief that maybe its not super cool.

&#x200B;

Thing is, all human soceities have and continue to practise slavery in some form (modern days is placed on people in developing nations).

&#x200B;

The question remains, how are we going to do work?  
Is it more ethical to coerce full bodied people into it?",hu2vqg7,t1_hu1tue8,1643063258.0,False
saqw7i,"The obvious alternative is to not use slaves and have people with free determination cooperate by exchanging their labour for goods and services? 

Slavery is neither inherent nor necessary  for survival as you so heavily imply. Small societies do not own slaves. Slavery is merely a side effect of despotism.

Anyway in response to your question(s):

1) how are we going to do work? Sort of like what we have now? Why do we need a bunch of laboratory abominations to compute anything when we have perfectly capable non-biological computers.

2) Is it more ethical to coerce full bodied people into it? I know this is a loaded question and completely ignores the obvious fact that the world's and humanity's survival does not depend on the the computational tasks of biological brains in jars, but the answer here is yes, because at least those full-bodied people have some capability to end their own lives by their own free will.",hu32uz5,t1_hu2vqg7,1643066092.0,False
sbk9aw,"My three main issues with crypto are..  
1) It fluctuates in value too much to be used as currency.  
2) In most countries, traditional investments are insured against theft. Theft of crypto isn't.  
3) Much publicised environmental cost of mining.",hu0dzzq,t3_sbk9aw,1643026699.0,False
sbk9aw,"> 2) In most countries, traditional investments are insured against theft. Theft of crypto isn't.

How can cryptocurrency owned by a person be stolen? Cryptocurrency is very secure, right?

> 3) Much publicised environmental cost of mining.

I feel like this argument is a moot point because everything we do has a high carbon footprint on the environment, including watching YouTube. I think moving away from non renewable energy with a healthy mix of renewable and nuclear energy would solve this problem, because regardless of whether we use Bitcoin or not, I think block-chain technology is here to stay.",hu0g8vm,t1_hu0dzzq,1643028103.0,True
sbk9aw,">How can cryptocurrency owned by a person be stolen?

I'm not sure this statement is the slam-dunk you think it is. Just because you own something doesn't mean it's yours forever. Someone owns the Mona Lisa, but it can still be stolen.

There's plenty of malware out there that sits quietly undetected on peoples machines until it detects the user copying their private key to the clipboard, at which point it uploads to the hacker. [There's also been plenty of crypto-exchange hacks over the years.](https://crystalblockchain.com/articles/the-10-biggest-crypto-exchange-hacks-in-history/)

There's also the danger of accidental loss. If my house burns down with my laptop, password manager, and any notebooks containing investment or banking credentials are destroyed, I can still eventually prove my identity and get access to my assets again. [If you lose your private key, you're fucked.](https://www.bbc.co.uk/news/uk-wales-55658942)",hu0n9ic,t1_hu0g8vm,1643031926.0,False
sbk9aw,"Your correct.

One of the oft touted positives of decentralisation is that it empowers the poor to get out of the grip of the powerful.

For me; this is actually really wrong; it transfers the burden of security to the individual, removes any kind of safety net.",hu0xnrf,t1_hu0n9ic,1643036679.0,False
sbk9aw,"I see, apologies if I seemed a bit arrogant since I'm very new at understanding this technology. Thank you for taking the time to explain it to me.",hu0nyjz,t1_hu0n9ic,1643032272.0,True
sbk9aw,"No need to apologise, and sorry if I came across as rude. Just please be very wary of crypto. For every person you read about who retired with millions in the bank at 25 after investing in crypto, there are probably 50 people whose investment went down, or who lost all their crypto through scams, but are too embarrassed to talk openly about it.",hu0one3,t1_hu0nyjz,1643032612.0,False
sbk9aw,"The extreme lack of authority makes it so any mistake is permanent, if someone steals my credit I get every cent reimbursed, if someone steals my wallet I’m absolutely fucked, there’s just no real point from the consumer side to go crypto over fiat",hu3jb4o,t1_hu0g8vm,1643076030.0,False
sbk9aw,"Hmm, I never considered that part. You're right.",hu4k65b,t1_hu3jb4o,1643095388.0,True
sbk9aw,"> How can cryptocurrency owned by a person be stolen? Cryptocurrency is very secure, right?

What if the person hands it away himself :-) ? Yes, you heard it right, that is what crypto theft is. The blockchain is very secure but most people using cryptocurrency are not tech junkies like us, they do not know how to secure their crypto wallet, they either end up giving away their private key to some hacker because of some social engineering or fall into other scams. Also, the cryptocurrency exchanges can have bugs, thus causing uh-huh crypto theft. 

u/rzlmmfia meant that traditional investments are secure from this kind of theft, which the ""common man"" is unaware of, although traditional investments are also not safe, god forbid but somebody can break into your house and steal valuables. I think that there are some benefits of having a centralized authority as the middleman which I would describe in my answer to the main question.",hu0ouri,t1_hu0g8vm,1643032713.0,False
sbk9aw,"From a CS perspective, I believe Blockchain is a very clever and interesting technology, using cryptography not to protect information, but instead to build concensus among trustless parties.

From a social perspective, I believe platforms like Ethereum could revolutionize human interaction at a global scale and disrupt virtually any industry despite entrenched central authorities and gatekeepers.

From a political perspective, I believe deeply in the promise of Bitcoin as a hedge against the irresponsible and reckless behavior of the Federal Reserve, printing the US dollar into valueless oblivion. And also as a hedge against the ""too big to fail"" private banks and their cronies in the SEC and other civil regulatory agencies whose collective incompetence caused the disaster in 2008 which robbed so much wealth from so many people.

However, from a pragmatic perspective I recognize that, in reality, the crypto community thus far has produced little more than new-age digital Ponzi schemes and other various forms of fraud and grift. All while building a culture around itself which has its collective head so far up its own ass that it can't recognize or resolve any of the issues that are undermining it's potential.

Finally, from an emotional perspective, I got caught up in the Elon-fueled mania and am the not-so-proud owner of 29,290.53 Dogecoin. So what the fuck do I know?",hu0xzhn,t3_sbk9aw,1643036818.0,False
sbk9aw,Your answer seems like the most balanced opinion on this comment section.,hu4kcgg,t1_hu0xzhn,1643095511.0,True
sbk9aw,"Believe?

Eh; the technology is interesting but generally there no actual good use case that needs a distributed ledger. We all use regular money just fine; it’s worked for thousands of years.

A lot of money and a lot of smart peoples time has been invested , and yet we’ve still only got crypto coins. So this amazing technology is basically useless.

Cryptocurrency looks very much like a fancy Ponzi scheme currently; the fact the supply of money is increasing at a fixed interval, means the higher demand for coins, the greater value each coin is, and this basically rewards those with existing high holdings. So they are incentivised to attract more people in, which is easier enough if the returns are there. But for those later to the party, you get less of the pot, and potentially don’t get the returns; pretty much a Ponzi scheme.

Unless you’re blinded by the tech; or think it’s so cool, or are an anarchist at heart; you generally think it’s useless.

However, I wish I’d bought some at the start and become a multi-millionaire.",hu0zfxp,t3_sbk9aw,1643037426.0,False
sbk9aw,Except that is not true in some countries where currency is not reliable (e.g. hyper inflation).,hu1jduj,t1_hu0zfxp,1643045073.0,False
sbk9aw,"You're right, but I think the much more fluctuations of crypto's value can be a very big problem for it's adoption.",hu4k8u7,t1_hu1jduj,1643095441.0,True
sbk9aw,"I have not invested in crypto. 

I personally don't believe in Bitcoin, and other crypto currencies. I do believe in their underlying technology. They have huge potentials. It can really be the new phase of internet. 

None of these crypto currencies have any underlying value, the value of a stock is determined by the company, bonds by the govt stability and currency value, etc. Crypto currency as such does not have any underlying value which can make it a valuable investment.",hu0eiqi,t3_sbk9aw,1643027033.0,False
sbk9aw,"> None of these crypto currencies have any underlying value, the value of a stock is determined by the company, bonds by the govt stability and currency value, etc. Crypto currency as such does not have any underlying value which can make it a valuable investment.

Hmm, I kind of agree and disagree with this statement. If we think about it, no currency really has any intrinsic value, the only ones which do are services and goods that are produced. I, however, do get your point. Bitcoin as a currency has yet to see mass adoption in the global economy and I personally don't see how it can happen as it's value fluctuates so much.",hu0gj11,t1_hu0eiqi,1643028272.0,True
sbk9aw,"Currency is backed and guaranteed by a government. Its stability is directly tied to the stability of that government. The only way a major currency would lose value is if the government collapses. You see that instability in countries that are very unstable, like Venezuela. That's not really an issue for the Euro or USD though. 

It used to be backed by gold, but modern economics doesn't care about that.",hu0pnbh,t1_hu0gj11,1643033094.0,False
sbk9aw,"It has been too widely adopted for how early and rough around the edges it is.  Finance bros are using is as a speculative investment is ruining its potential to be a useful currency.  Finance bros combined with its energy usage issues and the fact that a rough new technology is being presented to the general public as the next big thing is placing a stigma on it that will be very hard to over come.  It could've one day been something great and improve the world.  Maybe it still will, but that will require a major rebranding, and people to smooth out the edges despite the current taint attached to it.",hu0nnkn,t3_sbk9aw,1643032120.0,False
sbk9aw,I agree with you. What would you say are the current limitations and problems that Blockchain technologies face?,hu0o51x,t1_hu0nnkn,1643032363.0,True
sbk9aw,"Technically it’s an interesting but extremely inefficient solution to a problem that doesn’t really exist. 

Economically it has way too many flaws as a currency for me to see it being mainstream, but I could see it always having a niche status. 

I don’t really “believe” in it, no.",hu1rjpu,t3_sbk9aw,1643048062.0,False
sbk9aw,I agree with you.,hu4jyrz,t1_hu1rjpu,1643095245.0,True
sbk9aw,[deleted],hu0cvr0,t3_sbk9aw,1643025959.0,False
sbk9aw,"> You shouldn't apologize, people are becoming rich than getting therapy that why they are unhappy. They are not ready to share how they got into and planning to gift their next 50 generations Bitcoin as inheritance.

I'm not sure I quite understand what you are saying here.",hu0gcu3,t1_hu0cvr0,1643028168.0,True
sbk9aw,[deleted],hu0jsnr,t1_hu0gcu3,1643030128.0,False
sbk9aw,I think that can be said about the whole state of affairs happening all around the world. Not a very profound statement lol.,hu0ntbq,t1_hu0jsnr,1643032200.0,True
sbk9aw,Please don’t post stuff like this here.,hu0fltx,t3_sbk9aw,1643027714.0,False
sbk9aw,Why not?,hu0fyoh,t1_hu0fltx,1643027932.0,True
sbk9aw,What is the name of the subreddit? What does this have to do with computer science?,hu0gdh1,t1_hu0fyoh,1643028179.0,False
sbk9aw,"Uhm, not sure how to say this, but Blockchain technology is a topic within the field of computer science lol",hu0o9df,t1_hu0gdh1,1643032422.0,True
sbk9aw,"You didn't ask about blockchain technology / algorithms. You asked about the consumer / social side of crypto, i.e. about whether or not it is a scam, whether or not it will be adopted, etc. And you asked if this subreddit ""believes in crypto."" 

You're not asking about CS. That's why the post has no upvotes. 

\> I would be asking this question in the crypto sub but that sub

You talked yourself out of doing the right thing - go talk about this there.",hu0trwx,t1_hu0o9df,1643034996.0,False
sbk9aw,"Should we not discuss the social and political ramifications of CS? Science and technology do not exist in a vacuum.

If there existed an r/eugenics sub, I would hope it would contain healthy debate on the ethics of such scientific pursuits. Admittedly, this is an extreme example, but the point is that scientists and engineers should always be considering the implications of their work in a broader context outside their narrow scientific field. I believe that is what OP is going for here.",hu13p4n,t1_hu0trwx,1643039131.0,False
sbk9aw,"\> Should we not discuss the social and political ramifications of CS?

oh my god, that's not what this person is asking about. The post was ""hey crypto bros, wanna talk about crypto??""",hu1tlfe,t1_hu13p4n,1643048818.0,False
sbk9aw,Not really. I was asking people's opinion on what they thought about cryptocurrencies.,hu4kfmv,t1_hu1tlfe,1643095575.0,True
sbk9aw,Ignore this idiot. It's people like these that reddit becomes a meme because now you can't even ask anything. I also made a post recently and I only see pokemons like that. I wonder if they have a life of their own...,hufwpu9,t1_hu4kfmv,1643293552.0,False
sbk9aw,"Viewing cryptocurrency as a simple currency is a bit outdated, as it has evolved beyond that. While “currency” aspect does have its place, the technology is now becoming more of a decentralized state machine, with nodes forming consensus as to the state of execution, rather than merely the number of coins in a wallet.  A new trustless economic platform is being created and while I tend to think it won’t be quite as revolutionary as the internet was, this is the more important technologies being developed right now.",hu2ckzx,t3_sbk9aw,1643055992.0,False
sbk9aw,"But don't all transactions happen through like some few companies? I think that's a big part about that. Plus, there is a limited number of transactions you can do through bitcoin in a given time period, which is way less than normal money. I think these are some of the main factors hindering cryptocurrency's mainstream adoption.",hu4k4gy,t1_hu2ckzx,1643095356.0,True
sasbd7,Pretty sure that’s right,htwzued,t3_sasbd7,1642965641.0,False
sasbd7,Not always O(V\^2). If use min-priority queue it drops down to O(V+V log(V)) .,htyot0u,t3_sasbd7,1642989662.0,False
sasbd7,Are you sure that's not O((E + V) log(E))? Making it O(V^2 log(E)) in complete graphs?,hu06upo,t1_htyot0u,1643021520.0,True
sa6sdw,"In my experience it is not worth trying study the courses material just the summer before. You never end doing that much progress and you will literally have to spend time again reviewing it. I think it's better just to do some fun projects that get you more in touch with programing ( I did exactly this and that was my conclusion). Just as an idea, of course I don't want to discourage you from study. Good luck with that!",htrijml,t3_sa6sdw,1642871627.0,False
sa6sdw,Depends how much he knows. Might be worth making sure you understand the underlying concepts before taking calc. A lot of people struggle because they could use work on trig or algebra,htrj2uo,t1_htrijml,1642871834.0,False
sa6sdw,"Yeah right, if u aren't comfortable with the academical stuff it is worth to care of it",htrnyoz,t1_htrj2uo,1642873733.0,False
sa6sdw,I am an international student from different study background so U might consider me a beginner or a a little above it.,htrll7p,t1_htrj2uo,1642872803.0,True
sa6sdw,"Thanks appreciate it , Building projects is particularly I love to do more than just studying stuff and also loved ur idea.

Can you recommend some basic-mediocre level projects I can do and also a place to learn this kinda stuff?",htrldny,t1_htrijml,1642872722.0,True
sa6sdw,ya i agree. you will learn these topics anyways. just spend the time learning something else,htrppx2,t1_htrijml,1642874421.0,False
sa6sdw,"Cs50 is a good intro course

On one of the pinned threads there’s an entire CS degree outlined with free courses I’ll edit and post the link momentarily (actually just go to the learn programming sub reddit)

Khanaceademy and freecodecamp are good too",htri4es,t3_sa6sdw,1642871464.0,False
sa6sdw,"r/learnprogramming sub reddit right?

cs50 by harvard if I am not wrong?

Also thanks :)",htrlw0r,t1_htri4es,1642872919.0,True
sa6sdw,"Yes cs50 is Harvard you can find it on edx.com,
The lectures are by David j Milan and he’s charismatic af so it’s enjoyable

And that’s the sub, I just can’t link to them on my phone

Maybe precalc to make sure your up to speed on your maths,",htrm4j9,t1_htrlw0r,1642873011.0,False
sa6sdw,"Rosen, K: Discrete Mathematics and Its Applications

[Calculus Course on YT](https://youtube.com/playlist?list=PLl-gb0E4MII1ml6mys-RXoQ0O3GfwBPVM)

[Codecademy](https://www.codecademy.com) for programming: start with either Python, C++, or Java


Books on programming (more in-depth theory):
Problem Solving with C++, Walter Savitch

Introduction to Java Programming and Data Structures, Y. Daniel Liang",hts19of,t3_sa6sdw,1642878900.0,False
sa6sdw,Already studying C++ in high school so The book will be really helpful:) thanks.,htuc07z,t1_hts19of,1642913611.0,True
sa6sdw,"There are complete syllabus’s on GitHub if you haven’t looked at your complete CS syllabus from school. They should be able to give a set of topics to use as a foundation for your CS knowledge. If you have your course syllabus you may be able to research the books in advance, though particular classes may change their textbook from year to year so I wouldn’t get anything head of schedule as the textbook may change by the time you take the class. If you want to study ahead there are plenty of good vendors online that offer the textbooks for the class, and I believe some free websites.",hu06don,t3_sa6sdw,1643021142.0,False
sa6sdw,I think it’s better to do leisure learning rather than just getting into details. That will help more IMO when you actually take those classes just search topics you like and see what video lectures you can find.,htsw6px,t3_sa6sdw,1642891434.0,False
sa6sdw,"If you have access to the materials of the courses you will be taking (syllabus, bibliography, etc), I'd suggest that you use them as guidelines. Nevetherless, here are my suggestions:

\- Discrete Math: this [playlist](https://www.youtube.com/playlist?list=PLl-gb0E4MII28GykmtuBXNUNoej-vY5Rz) follows some sections from the book *Discrete Mathematics and Its Applications*, by Kenneth Rosen. You could follow the video lectures and read the corresponding section in the book. I'd suggest that you go up to video 30 (seems a lot, but they are not long); these cover Logic, basic Set Theory and proof techniques, which will give you a nice head start for when you begin your course.

\- Linear Algebra: I think the best resource you could ever want to study this subject is Strang's MIT course [18.06SC](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/). This version is organized specifically for self-study (it has exercises with solutions, lecture notes, etc). I'd suggest that you go through Unit I; this will be more than enough to give you a good start in the subject before your course.

\- Calculus: some may have better suggestions than mine, but I think Khan Academy's [Precalculus](https://www.khanacademy.org/math/precalculus) course would let you well prepared for your first encounter with Calculus. Unlike the other subjects, Calculus requires that you are knowledgeable in algebra, trigonometry, etc. So if you have gaps in these prerequisites, I'd urge you to prioritize studying them instead of the other subjects I listed. LA and DM will be covered in your bachelors, but high school math certainly won't.

\- Programming: last, but not least, [CS50](https://cs50.harvard.edu/x/2022/) from Harvard makes a nice introduction to programming for someone entering a CS major. This course does NOT focus on a particular programming language; instead, it teaches the basics of many programming languages (C, Python, Javascript) and covers basic algorithms and data structures.

Hope I could help. Best of luck! ;)",htt5rgc,t3_sa6sdw,1642895377.0,False
sa6sdw,"Just WOW! for this explanation.
Thanks a lot :)",htubchd,t1_htt5rgc,1642913288.0,True
s9enpp,"Very new to machine learning and Comp Sci. Started learning about ML when I was supposed to be doing my dissertation (on some things called D-Modules and Grothendieck's Crystals) but Alpha Go was busy changing the world.

Properly started learning ML properly 20 months ago (first commit on this repo is on 22nd of April 2020) and this is what I've been up to! Was lucky enough to get onto the GPT-3 beta and I've built a tool to automatically generate resumes based on some notes you enter. Has taken me forever to get it working but finally am at a first properly working version.

The idea is you'll only have to write your resume once (or not really at all), and then you can give it a job spec and it'll automatically tailor it for you.

It is still v much WIP but would love to hear your feedback!

EEDIT: the website is [joinrhubarb.com](https://joinrhubarb.com) :)",htm6sgd,t3_s9enpp,1642782726.0,True
s9enpp,I would legit pay for this tool.,htqb3gf,t1_htm6sgd,1642849683.0,False
s9enpp,"Wow, that's so kind, thank you! I've made it totally free and I have no plan to charge - you can create an account on the website I chucked up :)

(EDIT: it's [joinrhubarb.com](https://joinrhubarb.com) \_",htqbm1o,t1_htqb3gf,1642850051.0,True
s9enpp,Maybe set up somewhere for donations.,htqf3tr,t1_htqbm1o,1642852497.0,False
s9enpp,Agreed. Get us your Patreon link!,htqfy26,t1_htqf3tr,1642853080.0,False
s9enpp,"That's a really kind thing to say but honestly, I haven't done it to make money - the aim is to have enough people using it that I can make the ml models incredible (not just getting past ats but when recruiters read your resume, regardless of your experience, they are wowed and the world becomes a little bit less about presentation of skills and more about your actual skills and how you would fit into a company), so with all that being said, if you can share with your friends and to your networks, that would be worth so much more than any donations :)",htqh8cn,t1_htqfy26,1642853955.0,True
s9enpp,Happy to spread the word :),htqi209,t1_htqh8cn,1642854497.0,False
s9enpp,"That's very kind, thank you!",htqierg,t1_htqi209,1642854724.0,True
s9enpp,You are amazing! Thank you so much!,htqfw88,t1_htqbm1o,1642853044.0,False
s9enpp,"Thank you! It is still quite basic in the ML functionality and planning on doing a lot more so if you have any thoughts or feedback please let me know

(Edit: I've set up a discord so if you do have any thoughts, please message me on there! [https://discord.gg/VmG75yrb](https://discord.gg/VmG75yrb) )",htqgsi8,t1_htqfw88,1642853662.0,True
s9enpp,I've only heard of GPT–3 from Tom Scott videos.,htmb5wa,t3_s9enpp,1642784327.0,False
s9enpp,It's pretty cool - has billions of parameters and is supposedly the most sophisticated language based machine learning out there,htmbzy5,t1_htmb5wa,1642784634.0,True
s9enpp,Oooh. Sounds good.,htmceuq,t1_htmbzy5,1642784786.0,False
s9enpp,Definitely worth checking out if you're interested in machine learning or ai :),htmcqkn,t1_htmceuq,1642784906.0,True
s9enpp,Is GPT-3 open source?,htxi7f1,t1_htmcqkn,1642972742.0,False
s9enpp,"You would think, given that the company that created it is called OpenAI, that the answer would be yes. The answer is that the model architecture and instructions on how to train an instance \_are\_ open source, but good luck actually training a model from scratch without millions of dollars and some pretty sweet hardware. Basically the code is open source but the data is not and the data is the valuable bit so the answer is no :(",htxj9yp,t1_htxi7f1,1642973137.0,True
s9enpp,But can I train the model without a lot of data?,htxm2ol,t1_htxj9yp,1642974199.0,False
s9enpp,You can find pretrained models on huggingface and use those,htxu563,t1_htxm2ol,1642977335.0,True
s9enpp,"That’s awesome! Would you be willing to share some of your experience operationalizing this? I studied Computer Vision (CLIP) but I haven’t found much in the way of taking something from its categorizations to a finished product.

Do you have an API that ingests and predicts, then a UI on top to display the results? It looks really well done.

Are you running it locally or in the cloud?",htmfp6g,t3_s9enpp,1642785992.0,False
s9enpp,"A bit of background on how i built it:

The backend is a massive Django instance which is basically a JSON API (we use DRF to make this easier), which talks to a postgres DB and a redis cluster.

We also use celery for long running tasks (e.g. initially tailoring a resume or beat tasks like sending onboarding reminder emails which I'm setting up now).For the frontend, it's again pretty simple: NextJS with typescript (love typescript) and tailwind for css.

I also use headlessui, the component library from the tailwind team, which has been really helpful in places. The marketing site ([https://www.joinrhubarb.com](https://www.joinrhubarb.com)) is also a NextJS site, I think it's so good for these sorts of things.

The bert instances are all fastapi (not sure if I would use this again) with pytorch for inference. I deploy these on elastic beanstalk (which is also where everything else is deployed) and while it works great for everything else I worry that we're overpaying for some massive ec2 instances we don't need.

Last is the chrome extension which is also react/typescript but like... kinda hacked together with a custom webpack config which needs improvement. We will soon have firefox/safari extensions but it's quite annoying/painful to do and deploying to the stores means we need to go through approval processes which is annoying.

Oh we also have some random lambdas for backend jobs and we use posthog for analytics which I cannot recommend enough, its really so so good.",htmfy8t,t1_htmfp6g,1642786084.0,True
s9enpp,u/pursuitofsadness \- happy to go into more detail on any of these bits if useful :),htmg0k0,t1_htmfy8t,1642786107.0,True
s9enpp,"That’s fantastic! I really appreciate sharing all the components. I’m gonna go through the process and see what using it is like asap!

Once you understood the basics of GPT-3 how fast do you think it was to operationalize this? What did it cost to train the model? Was it easier or harder to do than you expected?

What made fastapi a bad tool to use in your deployment? I’ve generally heard positive things.

With these hyper-scale foundational models I’ve heard the volume of fine tuning data required to get improvement on a more specific corpus isn’t huge (I think it’s called one-shot learning?), was that your experience?

Is it learning in real-time or are the weights updated on a schedule?


And finally, so I’m not a total leech here:

It’s not a lot, but I’m a Product Manager and I would be happy to give you my take on your sign up process and some use takeaways?",htmibsu,t1_htmg0k0,1642786956.0,False
s9enpp,Have DM'd you - happy to provide more detail and would appreciate any feedback you have on what I have made.,htmjqtd,t1_htmibsu,1642787464.0,True
s9enpp,Wow! Very cool. I'm doing a similar thing on overleaf with my premade java code to change which projects on my cv to highlight. Personalised CV matter,htn3thl,t3_s9enpp,1642794854.0,False
s9enpp,That's really cool! Sounds similar to what i've been doing -  Have DM'd,htq3m3z,t1_htn3thl,1642844202.0,True
s9enpp,"lol, nice!!  I love it.",htnhde0,t3_s9enpp,1642799739.0,False
s9enpp,Thanks!,htnyc6x,t1_htnhde0,1642805998.0,True
s9enpp,"Nice, now your AI can talk to their AI",htq2d3b,t3_s9enpp,1642843286.0,False
s9enpp,Now I just need to put in on the blockchain and twitter will go crazy for it lol,htq3v53,t1_htq2d3b,1642844385.0,True
s9enpp,Maybe Mint the code as NFT,htq4s5s,t1_htq3v53,1642845060.0,False
s9enpp,Could call it Resume Chimp or something lol,htq6a2q,t1_htq4s5s,1642846158.0,True
s9enpp,"Love it, looks and sounds like a really awesome tool. Wonder how long it took you to build and if you worked on it with anyone else. Cheers!",hto58v2,t3_s9enpp,1642808702.0,False
s9enpp,Took about 7ish months and worked on with a friend :),htq3syq,t1_hto58v2,1642844340.0,True
s9enpp,"(Although, I did 90% of the dev and he just helped me with bits and pieces)",htq3tuv,t1_htq3syq,1642844359.0,True
s9enpp,Great job:),htqlmbm,t3_s9enpp,1642856646.0,False
s9enpp,Thank you!,htr4i16,t1_htqlmbm,1642865953.0,True
s9enpp,I’ll be using it . Thanks,htrgduw,t1_htr4i16,1642870790.0,False
s9enpp,"Publishing the source code of this would be awesome, not only advertising for your website but also to improve what you did",htrlnox,t3_s9enpp,1642872829.0,False
s9enpp,"I love it, but I cannot go past the work experience section (as I don't have any). I can't save an empty field or skip it.",htsm21k,t3_s9enpp,1642887351.0,False
s9enpp,"Ahh yes I made it a required field but currently working to make it skippable. The codebase is a bit of a mess so it is taking a little while. If you have any other suggestions, please let me know!

(EDIT: I just set up a discord so feel free to suggest any features or issues in there - [https://discord.gg/VmG75yrb](https://discord.gg/VmG75yrb) )",htvccto,t1_htsm21k,1642938372.0,True
s9enpp,Thank you bro :),htvrbu1,t1_htvccto,1642947694.0,False
s9enpp,Can this tool works in portuguese?,htxep4q,t3_s9enpp,1642971409.0,False
s9enpp,"Definitely adding portuguese (and other languages) is on the roadmap but I would say its not likely in the next \~6 months because unfortunately I don't speak portuguese so would need to pay for translations and don't have the money at the moment :( sorry I don't have a better answer for you, localisation is definitely something we want to do!",htxh6fc,t1_htxep4q,1642972342.0,True
s9gnb2,"Cs is not that math heavy tbh. It’s just calculus , linear algebra and discrete. Linear algebra and discrete are more important than anything. I would advise you to pick a language most probably the one used in the university course work. It will help you a lot to finish projects on time.",htmsicy,t3_s9gnb2,1642790689.0,False
s9gnb2,I definitely agree but just want to add in a caveat: If you want to go towards a data science pathway I would not slack off with the calculus.,htrt0im,t1_htmsicy,1642875686.0,False
s9gnb2,"Thanks, I don't plan on slacking off with any subject tbh. This is why I'm asking for resources, I want to do well.",htvzy0o,t1_htrt0im,1642951651.0,True
s9gnb2,"> Cs is not that math heavy tbh

I would argue about that but nvm",htq2hcs,t1_htmsicy,1642843375.0,False
s9gnb2,"I meant to say for undergrad CS. grad and Beyond , especially theoritical CS is different story",htqm6at,t1_htq2hcs,1642856994.0,False
s9gnb2,"Thanks for the reply! I'm actually learning Python and JavaScript and it's going pretty well so far, but I'm really concerned about the maths since I've forgotten pretty much everything😬",htmtmsx,t1_htmsicy,1642791135.0,True
s9gnb2,"Don’t worry! Universities basically start from calculus and linear.
Also note that python will most probably not be used in 90% courses. Java and C++ are most used for universities. JavaScript is barely touched and python is only relevant for ML courses. 

EDIT : I will recommend going over courses requirements for more info",htmvuj1,t1_htmtmsx,1642791964.0,False
s9gnb2,"Oh wow, thank you so much. The course requirements are a bit unclear since I'm an international student so I don't really know what a (Canadian) hs student learns on average, so I'm trying to get a general idea so that when I delve into details it'll be a bit easier.",htmxweo,t1_htmvuj1,1642792716.0,True
s9gnb2,What university? I might be able to help,htmyyps,t1_htmxweo,1642793104.0,False
s9gnb2,Memorial University of Newfoundland.,htmzhex,t1_htmyyps,1642793295.0,True
s9gnb2,"It wouldn't hurt to get a program for practicing coding in a fun way. I made a graphical, no-fail language that is fun like Minecraft. You can email me at Cameron.flotow@gmail.com.

I also left a link to the FB page I just set up.

[CIRKETZ fb page](https://www.facebook.com/groups/471007034699523/?ref=share)",hu4f4th,t3_s9gnb2,1643091981.0,False
s95ek1,"Vectors are very general and generic mathematical object with many different representations. In CS, they're most commonly seen as an ordered list which can be acted on with some basic operations like dot product, elementwise operations , etc. 

Just to give a list of examples at the top of my head:

* **Vector Clocks** \-- vectors clocks are used to enforce a causal order of events in a broadcasting system. [https://en.wikipedia.org/wiki/Vector\_clock](https://en.wikipedia.org/wiki/Vector_clock)
* **Graphics** \--  Vectors are used to represent points, direction and lines. Transformations can then be seen as operations on vectors. [https://en.wikipedia.org/wiki/Polygon\_mesh](https://en.wikipedia.org/wiki/Polygon_mesh)
* **Vector Processors** \-- Rather than the CPU working on one data item at a time, it can work on a vector of items at once. This is used in designing supercomputers. [https://en.wikipedia.org/wiki/Vector\_processor](https://en.wikipedia.org/wiki/Vector_processor)
* **Machine Learning** \-- in many parts of machine learning, the objects which are selected are feature vectors. https://en.wikipedia.org/wiki/Feature\_(machine\_learning)",htkvoco,t3_s95ek1,1642758103.0,False
s95ek1,"There's the vector the combination of magnitude and direction, which is used in linear algebra, physics, engineering. There's vector the 1 dimensional array, which is used in a whole bunch of ways. There's vector the disease transmission method, which comes up in cybersecurity.
 
Your question really isn't answerable, there isn't going to be an article or youtube video covering what you are asking. Work on your question more to make it less vague and come back.",htkr1mh,t3_s95ek1,1642754482.0,False
s95ek1,"Computer graphics, physics simulations/some CFD techniques are the ones I have off the top of my head. It’s used in game design and missile guidance too.

Edit:
https://en.wikipedia.org/wiki/Guidance,_navigation,_and_control

https://en.m.wikipedia.org/wiki/3D_projection

These are some introductory Wikipedia articles about topics where vectors end up being used with some frequency.",htkqy95,t3_s95ek1,1642754409.0,False
s95ek1,Quaternions!!!,htleud3,t1_htkqy95,1642771192.0,False
s95ek1,Look into computer graphics.,htkmvkp,t3_s95ek1,1642751377.0,False
s95ek1,import numpy as np,htkmuyf,t3_s95ek1,1642751365.0,False
s95ek1,"Graphics, machine learning, many areas. Plenty of algorithms use it for various things, there are vectors involved in PageRank, the algorithm search engines like Google use.",htmxgha,t3_s95ek1,1642792555.0,False
s95ek1,"used it a lot in computer vision, graphs & mathematically computation software like matlab",htsqaa8,t3_s95ek1,1642889070.0,False
s9ffoc,An ACM membership is less costly and as far I can tell is the same as the regular OReilly online subscription. The periodicals and other content/benefits you get from the ACM membership are excellent.,htnejwf,t3_s9ffoc,1642798731.0,False
s9ffoc,"Thanks for the info mate  
What periodicals and benefits are there currently? is it this https://www.acm.org/membership/membership-benefits",hu4b9xk,t1_htnejwf,1643089627.0,True
s9ffoc,"That’s it. The ACM digital library, Communications of the ACM, and their various newsletters are really nice.",hu5g0yu,t1_hu4b9xk,1643117907.0,False
s9ffoc,I’d suggest O’Reilly. I probably own fifty of them. I like the consistency and structure of their books regardless of the author. The material is great both for learning and as a reference on the job.,htpmjm9,t3_s9ffoc,1642832748.0,False
s9c6ve,"> My question is that is data aligned according to its size(self-alignment) or according to the memory access granularity of the processor.

If ""memory access granularity"" refers to the granularity of memory accesses as treated by the memory consistency model, then the answer to your question is according to data type size.

> It would be great if you could specify some cpu architecture that allows data types larger than its memory access granularity.

The Cray NV-1 and NV-2 architectures (implemented by the X1 and X2 supercomputers of the 2000s) had a 32-bit access granularity, but was a clean-sheet 64-bit architecture.",htrm8do,t3_s9c6ve,1642873051.0,False
s8eq88,"""Master's of Doom""- David Kushner, also ""Hackers Heroes of the computer revolution"" - Steven Levy, really motivated me for like a month or 2 after reading them. But motivation is overrated, what you really want is discipline. If you're able to do something when you don't feel like it you've already out did the average person.",htfs44z,t3_s8eq88,1642669835.0,False
s8eq88,"“Masters of Doom”! I think DOOM is one of the best case studies in CS/SE, that every aspiring student or seasoned practitioner should analyze because what John Carmack produced, was really incredible. He is, probably, my biggest inspiration, that made me transition from just focusing on 3D graphics as an art form, to wanting to explore computational systems further.",htggsz3,t1_htfs44z,1642686369.0,False
s8eq88,he is a true programming cowboy,hti1m1s,t1_htggsz3,1642708156.0,False
s8eq88,"Bornstrom's thesis on why we most probably are living in a simulation, following the same line of thought a programmer in a sufficiently advanced civilization is god (quite literally).",htk6nms,t1_htfs44z,1642741381.0,False
s8eq88,Idk just read wikipedia of von neumann.,htgccl7,t3_s8eq88,1642684129.0,False
s8eq88,It's crazy to me how somebody can be that intelligent.. like the difference between von Neumann and your average CS graduate is probably like the difference between your average CS graduate and a 12 year old,htj1o8d,t1_htgccl7,1642723115.0,False
s8eq88,Godel Escher Bach by Hofstadter might do the trick.,htghdte,t3_s8eq88,1642686642.0,False
s8eq88,"The Mythical Man-Month

Overview: https://en.wikipedia.org/wiki/The_Mythical_Man-Month",hti5kva,t3_s8eq88,1642710002.0,False
s8eq88,"Hmm, these aren't necessarily the deepest but I think you would like these.

Just for Fun: The Story of an Accidental Revolutionary by Linus Torvalds

Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age by Michael A. Hiltzik

Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Tim Berners-Lee",hti62oc,t3_s8eq88,1642710237.0,False
s8eq88,"Geoffrey James, The Tao of Programming (1986)

Jon Bentley, Programming Pearls (1986)

Jon Bentley, More Programming Pearls (1988)

Jon Bentley, Programming Pearls 2/e (1999)

Gene Kim, The Unicorn Project (2019)",htie32c,t3_s8eq88,1642713182.0,False
s8eq88,"Haven't read The Unicorn Project, but The Phoenix Project was worth a read imo",htiewua,t1_htie32c,1642713482.0,False
s8eq88,"Loved Phoenix Project! A little more DevOps-y, but also we’ll worth reading for coders, IMHO.",htjvst8,t1_htiewua,1642736147.0,False
s8eq88,"the 1981 Pulitzer Prize winner ""Soul of a New Machine"" by Tracy Kidder, the 1995 novel ""Microserfs"" by Douglas Coupland, ""Neuromancer"" and the short story ""Burning Chrome"" by William Gibson, the aside beginning with ""The programmer, like the poet"" in Fred Brooke's Mythical Man-Month, ""A Mathematician's Apology"" by GH Hardy, Wigner's essay ""the unreasonable effectiveness of mathematics in the natural sciences"", Dijkstra's ""a Discipline of Programming"", all the footnotes from SICP and any HAKMEM you can find, aaronson's ""the ghost in the quantum machine"", and The Mentor's ""Conscience of a Hacker"" from Phrack issue 7.",htjhdyt,t3_s8eq88,1642729837.0,False
s8eq88,CODE,hthajdv,t3_s8eq88,1642698280.0,False
s8eq88,Pragmatic programmer is good choice.,htga5q7,t3_s8eq88,1642682923.0,False
s8eq88,Algorithms to live by is a good one. Not motivational but gives hope that everything we learn has a real life use other than putting food on our table,htgp4ef,t3_s8eq88,1642690104.0,False
s8eq88,"* A bit old now, but Dust or Magic is interesting from a game design point of view https://www.amazon.com/Dust-Magic-Bob-Hughes/dp/0201360713 
* Similar vintage, Are your lights on? more about the analysis side of things than actual coding https://www.amazon.com/Are-Your-Lights-Figure-Problem/dp/0932633161
* greenspun's Web Publishing https://www.amazon.com/Philip-Alexs-Guide-Web-Publishing/dp/1558605347
* Programmers at work, an older classic https://www.amazon.com/Programmers-Work-Interviews-Computer-Industry/dp/1556152116",hthffda,t3_s8eq88,1642700028.0,False
s8eq88,[The Cathedral and the Bazaar](https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar) is a classic philosophy of code book that every programmer should read at some point.,hti70ew,t3_s8eq88,1642710600.0,False
s8eq88,Pragmatic Programmer is a pretty nice read,htkf9xz,t3_s8eq88,1642746200.0,False
s8eq88,I’m reading [The Making of Prince of Persia](https://www.jordanmechner.com/store/the-making-of-prince-of-persia/) by Jordan Mechner. I love it.,htfrw17,t3_s8eq88,1642669654.0,False
s8eq88,Dreaming In Code,htiytgt,t3_s8eq88,1642721936.0,False
s8eq88,The hungry hungry caterpillar teaches you how so many different parts of a whole go into making the program.,htkainf,t3_s8eq88,1642743452.0,False
s8eq88,Wow this post is getting some action! Thanks everyone for your suggestions.,htmp5lh,t3_s8eq88,1642789424.0,True
s8eq88,Thanks for this post! I’ve been in search of something similar,hu4ljau,t3_s8eq88,1643096375.0,False
s8kxq7,"Gale-Shapley could fail because it's not optimizing a global quantity like the sum of the distances. Take this case where you want to connect lower case and upper case letters and the distance between them is the literal distance in the figure:

A---a-B---b-C---c

Gale-Shapley will give you a stable matching so it will connect a and B (because if a is not connected to B, then a and B will ditch their partners and get together) and b and C. So that leaves A and c to get connected which is a huge distance. Clearly connecting A and a and B and b and C and c is shorter than just the A to c distance.

What you're looking for is obviously well studied since it's such a fundamental question. I don't know much about it, but here's a link suggesting that there is an efficient algorithm for it even if the distance of each pair could be chosen arbitrarily (and a super-efficient randomized algorithm!): https://en.wikipedia.org/wiki/Assignment_problem#Balanced_assignment",htiujzg,t3_s8kxq7,1642720180.0,False
s8kxq7,"Yes, Gale-Shapley will work. Model the people and houses as proposers (men/women in the original algorithm) and the sorted distance to each of the other as their list of preferences. Running the algorithm will give you a stable matching, which means no other pairing would have a greater total preference (lower distance).

Edit: one thing to note, the stable matching will minimize the distance for the proposer, so model the problem accordingly.",hthun7u,t3_s8kxq7,1642705556.0,False
s8kxq7,It's just maximum bipartite matching,htugh9g,t3_s8kxq7,1642915839.0,False
s7yrsa,"CPUs are designed to operate on bytes, not individual bits.
8bit=1 byte, 
 16bit=2bytes. 
There is no way to operate on half of a byte or any other fraction.

Interestingly, this limitation doesn't actually matter. Compression algorithms will use huffman coding. If you somehow made an image that only used 12 bits and the rest was filled with rezos, compression algorithm could take care of that and find some optimal way to store such images any way.",htd2pzm,t3_s7yrsa,1642623247.0,False
s7yrsa,"> There is no way to operate on half of a byte or any other fraction.

Fun fact, half of a byte is called a nibble.  You'll only ever see it in assembly programming, and even then it's incredibly rare to bump into.",htdvoyk,t1_htd2pzm,1642634210.0,False
s7yrsa,"And COBOL, IIRC. Signed numerics store the negative/positive in the low/right nibble of the lowest digit. Is it sad that this takes up space in my brain?",hte53ff,t1_htdvoyk,1642638254.0,False
s7yrsa,and trivia games/shows. I feel like I've seen it on a number of them.,hte98dt,t1_hte53ff,1642640114.0,False
s7yrsa,It uses 4 bits to determine whether a number is negative? Why....,htf4wji,t1_hte53ff,1642654578.0,False
s7yrsa,"Disclaimer: Not a COBOL developer. I’ve just had to read a fair amount in a migration project.

From what I gather, a lot of the persisted data from COBOL is fixed width. Numerics being right justified. If the number gets big enough, you risk loosing the +/- if held on left side. By “packing” the sign with the smallest digit you guarantee knowing whether the numeric is negative or positive. Also ensures you can carry the same digit span.",htgnvpr,t1_htf4wji,1642689580.0,False
s7yrsa,"Almost certainly binary coded decimal.  Each nibble is used to represent one decimal digit.  Yes, this wastes 6/16 of the available storage space but it's trivial to display as a decimal number.  Many CPUs (including x86/x87) have instructions for doing math on BCD numbers, though I imagine these days the use of them must be vanishingly rare.

Once you're using a nibble for each decimal digit, a nibble for the sign bit also makes some sort of sense.",htg28cz,t1_htf4wji,1642677777.0,False
s7yrsa,"As I've mentioned in another comment, the x86 platform still has instructions for doing maths on binary coded decimals, where each decimal digit is represented by a nibble.  COBOL is the only language I've seen that supports it though.",htg2jq0,t1_htdvoyk,1642678001.0,False
s7yrsa,">rezos

Is this the CEO of azamon?",htgdpou,t1_htd2pzm,1642684842.0,False
s7yrsa,There is one computer I know of that uses 16 bits but one is for parity so there's 15 usable bits. It's the Apollo Guidance Computer.,htgwja1,t1_htd2pzm,1642693065.0,False
s7yrsa,"> There is no way to operate on half of a byte or any other fraction

You can certainly operate on arbitrary bit-sized integers, it's just less efficient than those same operations on the size and alignment that the CPU was designed for.

The folks at adobe probably decided image processing at arbitrary bit depths was just not worth the effort- nobody really needs it.  Users would be better served exporting to their desired bit depth if they had a use-case that *really* called for it.",htf4zma,t1_htd2pzm,1642654623.0,False
s7yrsa,"In a sense, tradition.

Earlier computers were designed around different bit lengths: [4 bit architectures like the early 4 bit microprocessors](https://en.wikipedia.org/wiki/4-bit_computing), early PDP computers (like the PDP-8) had a [12 bit architecture](https://en.wikipedia.org/wiki/12-bit_computing), some early mainframes used [36 bit architectures](https://en.wikipedia.org/wiki/36-bit_computing)--and early versions of the LISP programming language were built around those architectures. (LISP uses a two-pointer cell with some control bits--and with a 16-bit address system, 36 bits works very well for a two-pointer cell with 4 control bits.)

But eventually we seemed to settle on 8 bits because it was a power of 2 and because the ASCII character set would fit inside. And eventually we settled on powers of 8 bits (8 bit, 16 bit, 32 bit, 64 bit) for backwards compatibility.

But there is absolutely nothing magic about 8-bits outside of tradition.",htdl2ky,t3_s7yrsa,1642629948.0,False
s7yrsa,"A slight clarification (this was a good write up of the history, not disagreeing with the parent comment at all), though the choice of 8-bits wasn’t magical, once 8-bits was chosen there is “magic” in terms of memory alignment and speed of operations, which is why you see formats aligned on powers of two boundaries.",htdytoy,t1_htdl2ky,1642635510.0,False
s7yrsa,"Memory alignment and speed of operations stuff happen because you can vectorize the base memory storage unit using multiples of that base memory storage unit at the same time--not because the base memory storage unit is a byte with 8 bits.

Meaning you gain a lot with a 16-bit processor over an 8-bit processor because it can operate on two bytes at a time rather than one byte at a time--not because the byte happens to be 8 bits.

Now it turns out 8 bits worked well in earlier microprocessor designs because it was twice 4 bits--the Z-80 CPU, for example, actually had a 4-bit ALU, and would perform things like addition operations by carrying out the operation on the bottom 4 bits first, then again on the top 4 bits. (It's one reason why the Z-80 had the 'half-carry' flag in the flag registers, though the 'half-carry' flag could then be used for the [DAA (Decimal Adjust Accumulator)](https://stackoverflow.com/questions/8119577/z80-daa-instruction) instruction to perform [BCD](https://en.wikipedia.org/wiki/Binary-coded_decimal) addition and subtraction.)

But now that modern CPUs perform vectorized math using multi-byte words, the advantage of doing things like performing 8-bit arithmetic using a 4-bit ALU is lost. There is no reason why an ALU can't be 9 bits wide or 23 bits wide, except that we have gotten so used to 8-bit bytes we seem to think there's no other way.

----

Edit to add: As a side note, the reason why the PDP-10 was 36-bits wide 'byte' was that it was twice the PDP-9's 18-bit wide 'byte', and was able to vectorize operations by operating on two 'bytes' at a time. (In older parlance each addressable unit of memory was referred to as a 'word', even though later parlance came to use the word 'word' to mean 2 8-bit bytes.)",hteq4cf,t1_htdytoy,1642647611.0,False
s7yrsa,"To be clear (maybe my comment wasn't), I wasn't dismissing your original premise (8-bits was not a magical number, it just worked out well) but once we chose 8-bits, there's a reason you see in memory structures aligned on 8-bit or other powers of 2, like the the original questions graphics format.",htf39bd,t1_hteq4cf,1642653746.0,False
s7yrsa,"Absolutely some good things come from 8 bits--but a lot of the things we think of as intrinsically 8 bits in nature really aren't. 

Like 8-bit color in a color file. Some higher resolution monitors are capable of displaying more than 8-bit color--but the most I've ever seen is 10-bit colors; that is, [10 bits per RGB channel.](https://www.bouncecolor.com/blogs/news/10-bit-monitor)

When building graphics software, however, it's easier in this era of large hard disks and gigabytes of memory standard, it's easier and faster--because of the way we vectorize operations across multiple words--to use 16-bit color representations, rather than attempt to bit-shuffle 10 bits into a 30-bit representation.

Were we still using the PDP-8 architecture of 12-bit bytes, however, we'd probably be talking about the naturalness of using 12 bits as the limit of human vision, and talking about how it's natural to represent color as 3 12-bit bytes. There may even be people bragging about their 12-bit color monitors (though they were really only 10, and using dithering for the other 2 bits).

Color graphics in particular has always been a compromise between what the hardware can do and what the eye can perceive. And 8 bit color channels did not solve these compromises; your eye can perceive a 256 color gradient on a monitor.",htgnzr8,t1_htf39bd,1642689627.0,False
s7yrsa,"I think the other answers here are excellent, but wanted to point out that sometimes we do still break with tradition. HDR video, for example, is typically 10-bit per color.",htg46ge,t3_s7yrsa,1642679159.0,False
s7yrsa,"It often is useful to make the word size a power of two. For example, a bit multiplexer will be able to cleanly select a bit out of such a register.

Further, once 8 bit CPUs became a thing, memory being byte-addressable became a standard. If you now design a new CPU which does not use a word that is a multiple of 8 bits large, you either need completely new memory or are going to waste bits. Both is inefficient, so new CPUs are adapted towards existing RAM, and we have stuck with 1 byte being the basic unit of memory ever since.",htd6sso,t3_s7yrsa,1642624757.0,False
s7yrsa,"There are color formats between 8 and 16 bits:

https://en.m.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats

Most modern architectures have a memory bus 2^n bytes wide, so many data formats these days will use as much of that as possible. Compact representations these days are much less important than memory bandwidth and latency.

The real question is why a byte is 8 bits wide. Believe it or not, it’s mostly just consensus. 8 bits was the right size at the right time, neither too big for a memory bus nor too small to be useless. For example, you can fit both the upper and lower Latin alphabet in 7 bits with an extra bit as a check bit (ASCII), so it’s a convenient size.",hted1ei,t3_s7yrsa,1642641820.0,False
s7yrsa,"Desktop version of /u/bargle0's link: <https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hted32w,t1_hted1ei,1642641841.0,False
s7yrsa,"In computing because of bits everything is POWER OF 2, so get used to 2, 4, 8, 16, 32, 64, 128... 1024, 2048...",htdx4be,t3_s7yrsa,1642634796.0,False
s7yrsa,Pretty much everything in computers boils down to boolean operations. This is either true or false; 0 or 1. The technical reason is that either electricity is going through or it is not. Since that only gives you two options everything else tends to involve powers of 2.,htdoqvg,t3_s7yrsa,1642631383.0,False
s7yrsa,x2,htene1e,t3_s7yrsa,1642646424.0,False
s7yrsa,Much easier to write the logic for word sizes that are sections of each other. Be much harder to make a 13bit machine run compatibly with 8bit logic.,htf4rdb,t3_s7yrsa,1642654502.0,False
s7yrsa,"ok, but why is it like that? 8 bit and 16 bit just represent how many possible color combinations per channel there are.",huhwlet,t1_htf4rdb,1643320124.0,True
s7yrsa,"So they represent how the storage is chopped up. Your different variables, like assigning a color to a pixel is part of a system of standardizations. We decided on word size (8bit) now people wrote programs to manage that data storage schema but making the first few bits represent commands that the hardware would interpret as actions to take. So a system of programs (operating system) was eventually developed to manage the addition and removal of sequences of words that formed the actions on a cpu (programs). Because of this standardization we had to agree on how things would be read so we can make generalized logic that can do things on every system.

So eventually we get to the application layer (layer 7) which is a composite of general logical “applications” that live below it (layers 1-6). At this point the application is running on 6 layers of logical framework that all expect layer 1 (hardware) to be exactly those word sizes so that it can perform the millions of actions needed to generate the amount of changes in a system to output such complex functionalities. The color is a pixel isn’t just coloring the pixel, it’s the management of data storage in a way that can manage the color of the pixel in such a robust way that the hundreds of apps and services that are offered today can easily manage the changing of that pixels color in a very very very specific way. 

When you code you generally don’t say what a pixels color is you code what the background color is, or the text buttons color is, or you create an array that contains bits configured in what the program expects as rgb format or whatever makes sense and then you tell the cpu/gpu how each word is chopped up because all the other programs running right now require that the data system be chopped up into exactly 8 or 16 or 32 bits.

Now that’s the reason for exact word sizes the binary expansion as in 8bits to 16bits is because now legacy systems can still chop the database up into 8bit sizes without breaking the systems that run on 16 bits. You just treat one word as two words within that applications allocated storage. 

TLDR: all other programs expect a standard word size, so new applications leverage capabilities built on top of those standards. We go from 8 to 16 because 8 bit programs can run still by treating each storage word as two words without running over a single word size allocation, so legacy programs can always run.

This is a simplification because there are also timings and physics stuff the hardware of the earlier times were hard coded into some paradigms, but the reading and organization of information lives eternally within the binary expansion paradigm",huj8us5,t1_huhwlet,1643340122.0,False
s7yrsa,All thing happens in bytes! So a byte is equivalent to 8 bits. 2 byte is 16 bits. If there was anything inbetween 8 n 16 bits it could be in decimals maybe.,htf9d3q,t3_s7yrsa,1642656970.0,False
s7yrsa,Binary means base two. 8 and 16 are powers of two.,htfj07t,t3_s7yrsa,1642662978.0,False
s7yrsa,Take the binary log of 8 and 16. Does that answer your question?,htfs4hc,t3_s7yrsa,1642669842.0,False
s7yrsa,"No it doesn’t, i don‘t get why in photoshop for example, you can only choose between 8 and 16 bit. Since options in between are possible. 8 bit are 256 possible colors per channel. And 16 bit are 65 536 colors per channel. So the difference is huge.",huhw912,t1_htfs4hc,1643319996.0,True
s7yrsa,"I thought your question was about word size and not a particular software. Anyways, historically, number that are powers of two made internal math easier. I gave you CSesque answer.",huhwyuy,t1_huhw912,1643320265.0,False
s7li8v,"Educated guess, so I may be wrong, but I believe it is because really old modems used [Bauds](https://en.wikipedia.org/wiki/Baud) per second, which are equivalent to bits per second for binary systems.

Seeing as it was common to use the baud rate for dial-up modems, when they first introduced broadband, ethernet, etc., they just continued to use bits per second.

AFAIK, files were always organized into chunks, since if you accessed them bit by bit, you'd need a lot more storage to keep track of all the bits than the space of the drive itself. For example, standard ext file systems on linux are split into 4 kb chunks (by default, you can change that), so creating a file with 0 bytes will still take 4 kb. It also makes sense with 8-bit ASCII encoding to say that a file containing the string `hello` takes 5 bytes, and not 40 bits.

> Can't we just have the same unit for both of them (to reduce the confusion of some people)?

Probably too late now. You have decades of documentation using other standards... Why change now?",htau2oq,t3_s7li8v,1642586933.0,False
s7li8v,"**[Baud](https://en.wikipedia.org/wiki/Baud)** 
 
 >In telecommunication and electronics, baud (; symbol: Bd) is a common unit of measurement of symbol rate, which is one of the components that determine the speed of communication over a data channel. It is the unit for symbol rate or modulation rate in symbols per second or pulses per second. It is the number of distinct symbol changes (signaling events) made to the transmission medium per second in a digitally modulated signal or a bd rate line code. Baud is related to gross bit rate, which can be expressed in bits per second.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",htau3ws,t1_htau2oq,1642586959.0,False
s7li8v,"This is the most correct answer. Transmission just cares about how fast can I send data and there isn’t a set number of 8 bit bytes I have to send. And remember that historically bytes were hardware dependent and of varying length. Also, transmission speeds aren’t just for your network but also used for inside your computer for other transmissions like the bus speed.",htbkpv9,t1_htau3ws,1642602667.0,False
s7li8v,"Bytes have been commonly 8-bits for a very long time (since the 1970s at least), though, even if it wasn’t codified as a standard yet.  
  
The reason it was ambiguous and hardware dependent before was that a *byte* was defined as “the number of bits that were used to encode a *single character*”.",htcn6i9,t1_htbkpv9,1642617505.0,False
s7li8v,"I've wondered for quite a while, that's really informative",hudqloz,t1_htau2oq,1643248349.0,False
s7li8v,"It's mostly historical. 

Even today, most internet traffic is serialised over a single wire. So that wire sends bit after bit after bit. Sure, it packets them up into bytes/packets/whatever, but the fact is that it's a 1-bit serialisation.

However most memory modules and disks operate in terms of bytes, usually burst-reading a row or sector at a time.

Both of those descriptions aren't 100% true in all cases, but are the most common situations for both formats. (e.g. there are very few parallel communication cables/protocols, but they do exist)",htbmcbr,t3_s7li8v,1642603366.0,False
s7li8v,"It might be a marketing strategy, but it might be the resolution perhaps? You transmit bits and you store a byte. Storing a bit doesent makes much sense, but it could be that the marketing strat works on me haha",htao19s,t3_s7li8v,1642582118.0,False
s7li8v,I had a professor in school tell us it was marketing lol,htb68o1,t1_htao19s,1642595479.0,False
s7li8v,"Practically speaking, you transmit packets, which contain an amount of data you'd generally measure in bytes. It's marketing.",htbignx,t1_htao19s,1642601682.0,False
s7li8v,"> Practically speaking, you transmit packets, which contain an amount of data you'd generally measure in bytes.

Nowadays, sure. However, modems didn't just appear out of the blue. They are the product of years of evolution. Data used to be transmitted in different formats prior to ASCII, such as the [Baudot code](https://en.wikipedia.org/wiki/Baudot_code).

> It's marketing.

It''s a legacy of history.

- Does it benefit telecom companies? Of course.
- Are they happy that it inflates their speed numbers? Hell yeah.
- Is marketing the reason things have been counted in bauds and bits for 6+ decades? **No.**

Actually, one ""byte"" has historically had different bit values. 

> The size of the byte has historically been hardware-dependent and no definitive standards existed that mandated the size. Sizes from 1 to 48 bits have been used.[4][5][6][7] The six-bit character code was an often-used implementation in early encoding systems, and computers using six-bit and nine-bit bytes were common in the 1960s. These systems often had memory words of 12, 18, 24, 30, 36, 48, or 60 bits, corresponding to 2, 3, 4, 5, 6, 8, or 10 six-bit bytes. In this era, bit groupings in the instruction stream were often referred to as syllables[a] or slab, before the term byte became common.

https://en.wikipedia.org/wiki/Byte",htbs9ug,t1_htbignx,1642605810.0,False
s7li8v,"Each of those packets needs to have bit level error correction, and each bit is the smallest resolution (and the biggest) that needs to be considered. When stored and accessed by the CPU, we usually only use byte level precision (indexing by bits would mean we could only address a very small space and isn’t practical for anything).

So no, it isn’t just marketing, and it isn’t just a relic of old times. The problems from that relic era are still alive and well today.",htcafw1,t1_htbignx,1642612805.0,False
s7li8v,"When you receive data you are not just getting the bytes that wind up in the file. You are also getting a bunch of other bits that belong to the frame/packet that the data comes in.

If they changed it to bytes/sec that might be confusing/misleading because people may make the leap from ""bits in"" to file download size, which would not be correct.

I agree with some of the others though. There is at least some historical component to it as well.",htcebuq,t3_s7li8v,1642614233.0,False
s7li8v,Marketing: 1 gigabit/sec sounds better than 125 megabyte/sec.,htc4nwd,t3_s7li8v,1642610648.0,False
s7li8v,"125 MB/sec is actually an absurdly fast rate of transfer  when you consider the physical distance traveled and the sheer quantity of intermediate hardware.  
 
That’s almost as fast as the fastest of IDE hard drives ever and they operated *locally* and transmitted data in *parallel*.",htcl0t1,t1_htc4nwd,1642616717.0,False
s7li8v,"Oh yeah, I agree. And with minor exceptions here and there, the data is almost always perfectly intact with all those trillions of transfers going back and forth every few seconds",htcmkzs,t1_htcl0t1,1642617288.0,False
s7li8v,"They are measuring different things.

When you download a file of 1024 bytes, you expect it to take up 1024 bytes in disk.

Now suppose your internet connection is 8,192 bits per second (=1024 * 8).  You might expect your file to take one second to download but you would be wrong.  Every network connection has overheads and the rate that you can transfer useful data is always less than the line speed. Overheads are framing, headers, packet encapsulation and so on. But if you tell people they can get 10 MBps they expect to download 10MB in a second, while they'll actually get about 850-900kBps depending on the exact networking technology.

Another reason is that people building networking gear care a lot about the bit rate to figure out carrier frequencies and so on and these technologies often originated in telephony rather than packet data, where you really do care more about bits than bytes.",htbeim9,t3_s7li8v,1642599859.0,False
s7li8v,"This is a weirdly misleading comment.

>They are measuring different things.

They measure the amount of data in both cases.

>When you download a file of 1024 bytes, you expect it to take up 1024 bytes in disk.

You may expect that, but you would be wrong on most today's disks. The allocation units where the files are stored have some minimal size, e.g. 4KB. So the amount of useful information stored on the disk is lower as well (can be 4096x lower).

But even if there was a different interpretation for these two cases of measuring amounts of data... How exactly would help using one unit for one case and this unit * 8 for the other one?",htdflgs,t1_htbeim9,1642627946.0,False
s7li8v,Error correction encodings play a role too.,htcm7o7,t1_htbeim9,1642617152.0,False
s7li8v,"its a marketing strategy
edit: people hardly ever notice the difference between Kb and KB or dont know and companies try to exploit that to show bigger values",htanqpy,t3_s7li8v,1642581889.0,False
s7li8v,Fun fact: a kilobyte doesn't actually mean 1000 bytes. It means 1024 bytes... IIRC power of 2.,htats0b,t1_htanqpy,1642586700.0,False
s7li8v,"> Fun fact: a kilobyte doesn't actually mean 1000 bytes. It means 1024 bytes... IIRC power of 2.

[Theoretically wrong](https://en.wikipedia.org/wiki/Kilobyte). A **kibi**byte is 1024 bytes according to SI and the IEC.

In practice, people misfortune words all the time, so it's hard to be certain.",htauik5,t1_htats0b,1642587281.0,False
s7li8v,">In some areas of information technology, particularly in reference to solid-state memory capacity, kilobyte instead typically refers to 1024 (210) bytes.

""kibibyte"" was an attempt to resolve the confusion of ""kilobyte"" having 2 meanings but despite working in the computer industry for nearly 40 years I have literally never heard anyone ever say it nor use it in a piece of tech writing.",htb4m7s,t1_htauik5,1642594485.0,False
s7li8v,"> ""kibibyte"" was an attempt to resolve the confusion of ""kilobyte"" having 2 meanings but despite working in the computer industry for nearly 40 years I have literally never heard anyone ever say it nor use it in a piece of tech writing.

Indeed. I did mention there is a disconnect between the theory and practice. Still, it does exist. `dd` uses both.

    > dd if=/dev/random of=66k bs=65536 count=1
    1+0 records in
    1+0 records out
    65536 bytes (66 kB, 64 KiB) copied, 0.0034109 s, 19.2 MB/s

    > ls -lh 66k # gnu's ls -lh defaults to KiB
    -rw-r--r-- 1 u g  64K Jan 19 14:18 66k
    > ls -l --si 66k  # but it does have an option to use SI units
    -rw-r--r-- 1 u g 66k Jan 19 14:18 66k

I guess Linux can't change that in core utils to not break backwards compatibility.

> I have literally never heard anyone ever say it nor use it in a piece of tech writing.

I mean, I did write it in a comment on a tech science subreddit and linked to an article that covers it, so you are also misusing the word literally :)",htb5tl5,t1_htb4m7s,1642595228.0,False
s7li8v,"He didn't hear you talking, and he probably didn't click the link, so his literally is likely to be accurate.",htc9ce9,t1_htb5tl5,1642612398.0,False
s7li8v,"That’s just confusing though and I doubt most people read *KiB* as *kibibyte*. I believe the changeover in Linux to that is relatively recent in any case (last ten years?)
 
In computer parlance, *kilobyte* is abbreviated as **KB** whereas *kilobit* is shortened to **kb**. To try to use ‘kB’ is just confusing.",htcoaia,t1_htb5tl5,1642617908.0,False
s7li8v,"> That’s just confusing though

Pretty clear to me, but I have been working with computers a long time. Other tools also display both (e.g. `fdisk`), while others let you define which units to use (aforementioned `ls`, but also `free`).

It's just one of those things you can easily google if you care to understand it, I guess... And something most users don't even have to care about, really.",htcqk7d,t1_htcoaia,1642618738.0,False
s7li8v,"What I’m really getting at is that, afaik, ‘kB’ doesn’t really mean anything. If people want to say 1000 bytes, that’s how they say it, at least on this side of the world.  
  
I don’t think most people vontinuously check the man pages for updates. And if the standard output works for them, they aren’t likely to seek an alternative.",htcqwjw,t1_htcqk7d,1642618865.0,False
s7li8v,"**[Kilobyte](https://en.wikipedia.org/wiki/Kilobyte)** 
 
 >The kilobyte is a multiple of the unit byte for digital information. The International System of Units (SI) defines the prefix kilo as 1000 (103); per this definition, one kilobyte is 1000 bytes. The internationally recommended unit symbol for the kilobyte is kB. In some areas of information technology, particularly in reference to solid-state memory capacity, kilobyte instead typically refers to 1024 (210) bytes.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",htaujky,t1_htauik5,1642587303.0,False
s7li8v,"In practice though, people mostly use *kilobyte* differently when talking about computers. The US also doesn’t uniformly use SI units anyway.",htcnzda,t1_htauik5,1642617795.0,False
s7li8v,"> In practice though, people mostly use kilobyte differently when talking about computer.

Again, yes, I have mentioned that.

However, this is /r/computerscience, not R/everydayaveragejoecomputing. In science, it is important to be precise. There is a clear distinction, and it is important to be aware of it, and understand it. If you design a hard drive, it's important to provide your customers accurate information regarding its capacity.

For every day usage/speech, if I download a Linux ISO and it's 1.5 gigs, I don't really care whether it's GB or GiB.",htcozy5,t1_htcnzda,1642618169.0,False
s7li8v,"This has nothing to do with computer science or ‘the average joe’, it’s about how humans use *language*.  
 
It was established a long time ago that a kilobyte (KB) was equal to 1024 bytes. Trying to force change has only resulted in more confusion.
  
Also, SI is not totally universal, no matter how much it’s creators wanted it to be. The UK eventually transitioned, albeit slowly, and the US never fully switched over. What’s more is that *bits* are conceptual and insubstantial, not liquids/solids/or gases.  
 
We also have an imperial system in common use that’s got a standardized conversion to metric, so there’s really no incentive for actual usage to change.
  
P.S.  
https://usma.org/a-chronology-of-the-metric-system",htcptqv,t1_htcozy5,1642618468.0,False
s7li8v,"> It was established a long time ago that a kilobyte (KB) was equal to 1024 bytes. Trying to force change has only resulted in more confusion.

Established by whom? It was never formally established until the the IEC (of which the US is a full member) published their standard in 1999. Hard drives used 10^n for 50 years. A one-terabyte hard drive is 0.91TiB.

Also, `kB` is the preferred symbol, FYI.",htcv2ov,t1_htcptqv,1642620417.0,False
s7li8v,"Established in *common usage*. 
  
What some standards body says is irrelevant if it doesn’t have universal (or near universal acceptance). 
 
Honestly I think people are even less concerned with the *exact* capacity of hard drives. They’ve grown so large that most people don’t even worry about filling them up. Heck you can get 4 TB capacity practicaly off the shelf.",htcwby2,t1_htcv2ov,1642620882.0,False
s7li8v,"> Established in common usage.

Except it very much is not, since it means different things to different people. A 16 GB stick of RAM has a different amount of bytes than a 16 GB HD.

Standardization is important for industries. It doesn't matter if people misuse language, but it matters that if you purchase a hard drive, you can see that it conforms to the specifications.

> What some standards body says is irrelevant if it doesn’t have universal (or near universal acceptance).

I'd say it's the opposite. It doesn't matter that your Average Joe misuses the terms because he doesn't understand them. It is far more important to be able to look at the output of a system monitoring command and be able to adequately interpret it. When tools allow you to choose between SI and binary (KiB, ... PiB), you can get accurate information.

Anybody who thinks that kB vs KiB is too confusing might not be particularly suited for computer science.",htcxw0n,t1_htcwby2,1642621459.0,False
s7li8v,"What you don’t seem to understand is that KB or KiB is irrelevant so long as we all agree that it’s 1024 bytes. The misbehavior of for-profit corporations is utterly irrelevant.  
  
There is absolutely no reason that computing needs to care about SI at all.",htcyorr,t1_htcxw0n,1642621755.0,False
s7li8v,"I guess one of those things that started as bits cause data flow was so slow people couldn't use bytes in units, and then as speeds increased changing the understanding from bits to bytes would be an industrial problem, hence bits per second is now a standard.

Why bytes for storage? Well data storage started as bytes to begin with iirc, so it stayed as industrial standard.",htanu46,t3_s7li8v,1642581961.0,False
s7li8v,"Forgot to add, all data communication protocols are bits per second. QPI, PCI/E, RAM and so on, not just network.",htanxzu,t1_htanu46,1642582047.0,False
s7li8v,"Back then transmission speeds were very low. 9 kilo bits, or maybe slower were the norm at some time. Advertisement schemes aside, it would be less practical to express these speeds in terms of bytes.",htbemxw,t3_s7li8v,1642599917.0,False
s7li8v,"> 9 kilo bits, or maybe slower were the norm at some time.

9 kbps was in the 1980s. The earliest modems were like 300 bits per second lol",htbk626,t1_htbemxw,1642602429.0,False
s7li8v,"It has to do with how transmission of data works. The signal on a single wire can only have one measured state at a time.  
  
And once you realize that errors can happen and implement *error correction* then 1-bit/1-byte of transmitted signal transitions is no longer equivalent to 1-bit/1-byte of actual data.
  
There’s probably some deliberate fudge factor at the level of an Internet Service Provider (ISP) as far as making it sound *faster*.",htckb6u,t3_s7li8v,1642616454.0,False
s7li8v,I don't really know but I would guess it has something to do with older internet speeds being much slower and storage capacity growing at a significantly faster rate in comparison.,htanu0n,t3_s7li8v,1642581959.0,False
s7li8v,"Its because they aren’t actually equal to each other which is a pretty common misconception. 

One byte is equal to 8 bits however one bit is so incredibly small its impossible to have one bit in the 21st century.

Edit: i should also mention that kilobits, megabits, etc. do exist. Gigabit probably the being mentioned the most. But one gigabyte isnt equivalent to one gigabit. As far as im aware its a marketing scheme as to why we dont just call them the same thing. If your isp says we offer gigabit internet most people assume they are getting “gigabyte” internet and don’t even think about there being a difference",htbc70y,t3_s7li8v,1642598721.0,False
s82mjq,Cisco Packet Tracer? Helped me in understanding networking and was used in school.,hte4ro0,t3_s82mjq,1642638109.0,False
s82mjq,"Protocols are defined and written out explicitly. For instance, UDP is defined as [RFC 768](https://datatracker.ietf.org/doc/html/rfc768)

You just search “<protocol name> protocol paper” and it’ll come up",htexy11,t3_s82mjq,1642651153.0,False
s82mjq,Professormesser.com network + study videos. All free on YouTube,htk0lb5,t3_s82mjq,1642738367.0,False
s7h2vc,"Big O notation tells you how much slower a piece of a program gets as the input gets larger. For example:

* Returning the first element of an array takes a constant amount of time. If you double the length of an array, it still takes the same amount of time. That code does not scale with input size.

* Summing all the elements of an array takes a linear amount of time with regard to the length of the array. Double the length, the code takes roughly twice as long. Make the array a hundred times longer, code takes a hundred times as long.

* Searching for an element in a sorted array? If you're using a binary search, every time the array length is doubled you need to add one extra comparison. Performance _does_ get worse as the array length increases, but less than linearly.

The ""notation"" of Big O notation is just concise shorthand for describing the above patterns. O(1) for constant time, O(n) for linear time (where `n` is the length of the array), O(log n) for logarithmic time, etc.

The focus of Big O notation is on looking at the biggest trend of an algorithm. We don't care whether an algorithm takes 200 steps or 201 steps, we just care about how it scales as input size changes. Therefore, we only look at the slowest part of an algorithm. O(n + 1) simplifies to O(n), because as the array size gets millions of times longer the O(1) is quite insignificant.

Outside of coursework you will very rarely, if ever, need to do a formal proof to demonstrate the Big O of an algorithm. Instead, you're being taught Big O notation now so that you can easily reason about algorithms and data structures in the future. ""Oh, I should use a hash table here instead of a tree, because random element access is O(1) instead of O(log n)."" It's a convenient tool for quickly reasoning about which of two algorithms or data structures will perform better, or for reasoning about your own code and where the bottlenecks are.

Edit: fixed typo",ht9znq5,t3_s7h2vc,1642566573.0,False
s7h2vc,"I wish my textbooks read this clearly. I’m not op, but this helped me a lot thanks",htdizum,t1_ht9znq5,1642629177.0,False
s7h2vc,Thank you! That’s very kind :),htdzku7,t1_htdizum,1642635828.0,False
s7h2vc,"Amazing, saved. Thank you",htcl6n2,t1_ht9znq5,1642616776.0,False
s7h2vc,You’re very welcome!,htdzih2,t1_htcl6n2,1642635800.0,False
s7h2vc,"This was so helpful to me too, despite not being the OP! I actually feel like I somewhat understand Big O Notation now!",htigj4u,t1_ht9znq5,1642714056.0,False
s7h2vc,"It's used to notate the upper bound of how slow a function, `f(n)`, grows with respect to `n`. 

For example, consider the following function (in Python): 

```
def f(n): 
    print(""Hello World"") 
``` 

No matter what `n` is, this function will always run in *constant* time and is independent of the input `n`. We notate this bound with O(1) (1 meaning ""constant""). 

Now, consider this function: 

```
def f(n): 
    for i in range(n): 
        print(i)
```

Now, this function is dependent on `n`. If `n` is 1, it will print 1 line. If `n` is 9000, it will print 9000 lines. We say that the growth of this function is *linear*, and use the notation O(n) to upper bound its growth. 

There's more nuances: like the difference between upper bounding, lower bounding, and the theta one; how do you choose the best upper bound (can't we just say everything is upper bounded by n^n ?); and, how do we calculate the best equation for different functions. But, that's not for dummies.",htad019,t3_s7h2vc,1642574097.0,False
s7h2vc,"I recommend you check out the book [*Grokking Algorithms*](https://www.manning.com/books/grokking-algorithms).

It covers common algorithms and Big O in very approachable terms.

[Here is their ~~explanation~~ *comparison* of common Big O runtimes](https://imgur.com/a/WzTcnAV).

Edit: `s/explanation/comparison/`",htaocl9,t3_s7h2vc,1642582364.0,False
s7h2vc,"**Big O is the language that we use for talking about how long an algorithm takes to run.** \[This is a simplified definition\]

When talking about the running time of an algorithm, we normally use 3 notations Omega Ω, Theta Θ  and Oh O.Those 3 notations represent the following bounds of the running time of an algorithms.

Omega : Lower BoundTheta    : Tight BoundOh         : Least Upper Bound

This dude explains the concept much more better than I can.[https://www.youtube.com/watch?v=f\_IaKCB7Zo8&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y&index=5](https://www.youtube.com/watch?v=f_IaKCB7Zo8&list=PLBlnK6fEyqRj9lld8sWIUNwlKfdUoPd1Y&index=5)",htav6jh,t3_s7h2vc,1642587800.0,False
s7h2vc,"Big O is a way to simplify away insignificant terms.

Let's say you have an algorithm that reads an array of length n and does something to it in **n^3 + 5 n^2 - 22 n + 17** steps. As n gets big, the only thing we care about is the **n^3**, because it'll be so much bigger than the other terms, so we say it takes **O(n^(3))** steps.

So it allows us to just deal with the important factors, and hand-wave away the smaller, less significant terms. This is really useful in algorithms where sometimes it takes 2n^2 + 19n + 85 steps and sometimes it takes n^2 + n steps. These cases still run in a similar amount of time, but analyzing the algorithm exactly would be an absolute pain, so it's much easier to do the math if we say the algorithm is just O(n^(2)).",htbrwim,t3_s7h2vc,1642605661.0,False
s7h2vc,"A alg is O(some function of n)  

Means that the steps the algorithm will take are no more than f(n) if n is the size of the input.",htbsaw6,t3_s7h2vc,1642605822.0,False
s7h2vc,"In layman terms it is how fast your program does something in worst case scenario (a lot of input) and in best case scenario (not a lot of input) 

It basically says how efficient is your algorithm meaning the steps your program takes to achieve a certain thing.

Look on YouTube for a video called sorting algorithm visual and you will see unsorted bars in the video and then they get sorted with different algorithms and the ones that sort faster have a better big o Notation.

There are algorithm that are so good that it does not matter how many million inputs you have they do them at the same speed almost and there are bad ones that become slower with more Input",htbxbj4,t3_s7h2vc,1642607801.0,False
s7h2vc,"There are n numbers in an array. You want to find the biggest number. You start from the beginning, checking each. 

The biggest number can be at the beginning, in which case that would take you 1 checks. But it can also be in the middle, which would take n/2 checks. And the worst case scenario, it can be the last number you check, which would take n tries to reach. Big O in this case would be O(n) because it denotes the worst possible case of runtime",htc3u09,t3_s7h2vc,1642610331.0,False
s7h2vc,"Suppose you are doing a task. And the task is such that no matter how you do it, the task will gets completed within 24 hrs. So, here you can complete that task in 2hrs, 6hrs,12 hrs and it won't take more than 24hrs. So, you can say that Big O of ""task"" is 24 hrs.",htc9avk,t3_s7h2vc,1642612382.0,False
s7h2vc,"\--- The Story ---

Say we both have created algorithms to solve some problem.

And suppose it turns out that although your algorithm takes longer (I'm making the assumption that yours takes longer only because you asked me to assume you are dumb).

Now the question is how much longer your program takes to run than mine. Maybe on some input your program took 5 times as long as mine to run. That's useful information, but to more thoroughly compare our algorithms we have to see how it performs when we have large inputs.

There are two types of scenarios that come to mind:

\- Scenario type 1: Your program takes at most 10 times as long as mine to run, regardless of how large the input is.

\- Scenario type 2: I can't put a bound on how long your program takes compared to mine, because as we take larger inputs the ratio between the time your program takes and the time my program takes just keeps growing larger.

These cases are quite useful to differentiate between. They're saying something we should deem very important about the efficiency of our algorithms. Instead of starting off running our algorithms and computing the ratio between our running times for various inputs, we should first ask whether there would be a maximum ratio at all.

If there is no ratio (because it keeps growing), then we say that your algorithm's running time is NOT O(my algorithm's running time).

If there is a maximum ratio, then we say that your algorithm's running time is O(my algorithm's running time).

\--- Usage ---

If you're trying to create an algorithm, instead of computing exactly the number of steps it takes, it is better to get an estimate just using O() notation.

Let's take bubble sort with an array of size n. It can take many passes through the array to finish sorting, but not more than n passes. So overall it is taking around n\^2 steps. Do I care about whether it is 0.5n\^2 or 5n\^2? Yes, but not yet! Firstly it is definitely not taking more than 20 steps every time it moves between elements. So it takes at most 20n\^2 steps. The ratio between the number of steps and n\^2 is never larger than 20, so the running time of bubble sort is O(n\^2).

Before we pin down bubble sort's number of steps, let us first see if we can get an algorithm that takes O(n\^1.5) steps, or let's even see if bubble sort actually takes O(n\^1.5) steps. That would be a better guarantee. This is the larger, more important question to pursue.

You can prove that bubble sort does NOT actually run in O(n\^1.5) steps. This means that if somebody comes up with a complicated algorithm that takes 10000 n\^1.5 steps, that will still be better than bubble sort (because if bubble sort actually takes only 10000 n\^1.5 steps we would say it runs in O(n\^1.5) steps).

Figure out order first, worry about exact ratios later.",hteoe7o,t3_s7h2vc,1642646861.0,False
s7h2vc,It’s just a bunch of nonsense brogrammers ask at interviews.,ht9x9wd,t3_s7h2vc,1642565403.0,False
s8a2k9,"It sounds like you are just describing “life” as an NP problem (one where finding answers is hard but verifying it is much simpler). I kinda see the parallel you are drawing there. Don’t see how this is related at all to P=NP as clearly nature doesn’t solves the NP problem in P time, instead it takes forever randomly brute forcing it massively in parallel. If anything it’s a testament that P!=NP as even eons of evolution hasn’t found a better way.",htf4kvu,t3_s8a2k9,1642654409.0,False
s8a2k9,That’s what I’m saying. Nature shows that P != NP,htfdgxc,t1_htf4kvu,1642659332.0,True
s8a2k9,"No this is not a prove. Let me reformulate your proof. We have no fast algorithm to make life. There is a nature algorithm but this one is terribly slow. Therefore there can not be a fast algorithm to make life.

The problem with your proof is that just because we have not found a fast algorithm does not imply that it does not exist. If you would like to proof P!=NP. You would have to proof that the existence of such an algorithm is impossible. This type of proof has been used in the halting problem.",htggp0x,t1_htfdgxc,1642686316.0,False
s8a2k9,"To be able to prove P=NP we would have to reduce a really hard problem into an easier problem. But first we need a formal solution that solves that hard problem. I appreciate your train of thought and it’s interesting to think more deeply. However, biology is not my speciality.

Using your example, first we would have to prove or disprove mathematically that there is an algorithm that can create life (I’ll leave this to the computational biologists) that is tractable or non-polynomial time. [There is evidence](https://www.pnas.org/content/118/49/e2112672118) that we (an AI) can design life using larger building blocks, but having one piece of evidence is not enough to satisfy this type of proof. We’d need something that can be replicated and reproduced empirically and to my knowledge we’re not quite there with synthetic life. In 20 years who knows what we’ll have. (Also please, someone with a better biological background correct me.)",htf6js2,t3_s8a2k9,1642655441.0,False
s8a2k9,"My understanding is that we don’t have that algorithm right now, so as it stands right now, nature shows that P != NP",htfdkky,t1_htf6js2,1642659395.0,True
s8a2k9,"For me, an analogous argument based off evidence isn’t enough to conclude that P=NP or P!=NP.

A counter example I could offer is that, according to the physicists, the universe’s age is on the order of 10^10 and the heat death is theorized to occur on the order of 10^100. Assuming that 10^100 is the time limit for life to form, there’s still a _lot_ of time left in the universe for life to do it’s thing (not even accounting for all the possible chances at life near other stars in the first 10^10 years) in tractable or non-polynomial time.

Perhaps if we had a perfect theory that combined physics, chemistry, biology, and computation into a model descriptive enough to prove or disprove either way. But from my knowledge we aren’t there (… yet!)",htfguv2,t1_htfdkky,1642661490.0,False
s8a2k9,"""show"" in the mathematical terminology means the same as ""prove"" (like in mathematical prove). So that is clearly not the case here.

Still I like your idea, it is an interesting thought.",htmvy7v,t1_htfdkky,1642792002.0,False
s8a2k9,">Biologically, for many organisms (think simple multi cellular; plants, things that don’t have super complex brains) we understand the structure and function of literally every single atom and cell that they are made of.

Name one such organism.",htfvg25,t3_s8a2k9,1642672531.0,False
s8a2k9,Many small Protozoa and very simple life forms. The main grey area in our knowledge is brain functions. There are organisms that are simple enough that we basically understand how each piece of them works.,htfz074,t1_htfvg25,1642675347.0,True
s8a2k9,"Are you sure that we can't design ""artificial"" life? 

[Here is one example of a company which grows artificial meat and designs artificial yeast for beer, using designer cells.](https://en.wikipedia.org/wiki/Ginkgo_Bioworks)

I think we could grow our own plants and design our own seeds from scratch on a CAD in the near future, if it isn't already the case somewhere at Monsanto or something. And that's just the tip of that awesome iceberg.

Edit: To clarify, I think we can probably make whatever we want. I am skeptical of limits of any kind. We can probably design whole organisms in a Frankenstein kind of way. I think that's more likely than not. People find strange comfort in these arbitrary limits on what we can and can't do, and sometimes I'm not sure they are reasonable ones. If we threw trillions of dollars at companies like the one I linked above and other such companies and researchers, then who knows.",hthtaq1,t3_s8a2k9,1642705060.0,False
s8a2k9,"Wonderful thought, thanks for sharing!",htf62fu,t3_s8a2k9,1642655190.0,False
s7mb3k,"To write down the input, you need to write down all n numbers, which takes space proportional to n. But for W, you only need to write down log W bits to represent the number W. This means that the input size is log W.

Technically you could force the input format to write the number W in unary (W bits long), and you’d have a modified knapsack problem that’s solvable in P. Nobody actually writes down inputs in that way for obvious reasons.",htau3gl,t3_s7mb3k,1642586950.0,False
s7mb3k,"> But for W, you only need to write down log W bits to represent the number W. This means that the input size is log W.

Right. 

So, Knapsack is still in NP as `W` can be exponentially large wrt `n`. Correct?",htaupe4,t1_htau3gl,1642587429.0,True
s7mb3k,"W can be exponentially large relative to the input size. A problem is in NP if its solution can be verified in polynomial time relative to the input size. 

The input size here is n log W, since we need n numbers, no more than W bits each.",htauyt3,t1_htaupe4,1642587635.0,False
s71762,"I meta language, but don't machine learn, so commenting for visibility.",ht7o7v8,t3_s71762,1642532514.0,False
s71762,"It's all math that you'll have to learn converting 2d into 3d and I believe you will need to produce x,y,z coordinates from your mapping then you can create a 3d object.

Just focus on creating a sphere first based on your images.",ht90mk6,t3_s71762,1642550920.0,False
s71762,"I appreciate that ""it's all math"" and I'm attempting to do just this, but the point of my question is that I do not understand the fundamentals involved.  I'm currently re-learning linear algebra in the hopes that it will demystify some of what I'm seeing.  I'd hoped that there existed a tried and true method for doing this as it seems such a straightforward task.",hth5eck,t1_ht90mk6,1642696406.0,True
s6msij,"sci hub. combination of google scholar and sci hub is all you need tbh

&#x200B;

edit :  [sci hub](https://sci-hub.mksa.top/)",ht4mdzt,t3_s6msij,1642475215.0,False
s6msij,https://scholar.google.com,ht4mi0o,t3_s6msij,1642475263.0,False
s6msij,"[arxiv.org](https://arxiv.org) just in case, but sci hub is the holiest",ht620h7,t3_s6msij,1642508084.0,False
s6msij,"I normally try with Google Scholar first, then one or all of the following: [z-lib](https://z-lib.org/) for books and some articles, [sci-hub](https://sci-hub.se/), [libgen](https://libgen.fun/scimag/index.html) when sci-hub fails.",ht6zenr,t3_s6msij,1642523376.0,False
s6mxkp,"The better analogy would be you a window cleaner/construction worker who is high up on building cleaning/constructing something.

Now you need to use things like hammers, brushes, cleaner fluid etc. When you are hanging there midway through a building, the tools you have access to fastest are the ones you carry in your hand. You dont need any extra time to use them. Your hands are like CPU registers. Fastest but limited space, your hands can only hold so many tools.

Say if you come across a difficult to clean spot that your current tools cannot be used for. Then you can carry some less frequently required but still important tools in your pockets. You will need some time to use it because you will have to empty your registers(hand) and take out the required tool from your pocket and you may need to search which pocket you kept it or you just remember (associative memory) which pocket carries which tool. This is the CPU cache (level 1). Your pocket too cannot hold too many tools so you may keep some on a backpack/toolbox that you carry with you, you will need more time again to take out the tool from there because you have to open and close the toolbox and replace something in your hand. But a toolbox allows you to carry a bit more than you your pockets. This is still cache but level 2 cache, slower than level 1 (pockets) but have more memory. A cpu can have L3 cache as well. (For this we would neee to talk about multiple workers (cores) sharing a common big tool box).

Next say you need some thing that is used even less frequently that you generally dont bother taking with you on person, however it still happens enough that you need to use it atleast a couple times per building, so instead of taking it with you on person you take it in your platform that you use to go up on, in this way if you need the tool you dont have to go all the way to the ground to get it, this is like RAM, much slower than cache you had to get down a bit to reach your hanging platform to reach it swap out some things and then carry it back up. Remember for registers and cache you didnt have to climb down. But is still pretty fast. And have way way more capacity than cache (toolbox or pockets).

Then sometimes it maybe so that you didnt prep well and left somthing in the garage/workshop below. But if you need something like that you MUST come all the way down take the thing and go back up. This is your disk, you have a huge space (workshops on the ground can be as big as you want) but you have to come all the way and go back. So it is very slow compared to RAM but can hold much more stuff.

And lastly say your company had a sudden emergency and now you are are legally required to wear a special suit to protect you (maybe they heard about asbestos in the building you were cleaning or something). But here is the thing, since this is something sudden even your company was not prepared for it, so you must drive to your company HQ get the suit from there and come back. This is hugely slow but also massively big, like once you have to freedom to go anywhere the whole world can be your storage. This is the internet and you getting the suit is just downloading.",ht57qbu,t3_s6mxkp,1642485699.0,False
s6mxkp,Stuff you access a lot or that you accessed most recently gets a spot in the door shelves for quick and easy access. Infrequently used data gets packed into the back of the fridge or in the veggie bins.,ht4ro87,t3_s6mxkp,1642477493.0,False
s6mxkp,"Shipping and transport is a more fitting analogy. Cargo ships are slow and cumbersome, but they can move huge volumes of goods across the world. Upon arrival though, goods from one ship needs to be distributed across many cities. It'd be hugely disadvantageous to use ships for distribution from city to city because of how big and slow they are.

Therefore, that task would fall to the next in the shipping volume hierarchy: 18 wheeler trucks. They may have 1/10,000th the volume of a ship, but they're much more nimble. Highways are accessible to them, allowing them to go between cities.

Once the goods arrive at a warehouse or a shipping facility at some city, even smaller trucks or vans are required for delivering goods to individuals because imagine waiting for an 18 wheeler to navigate through your neighborhood.

At each level in the hierarchy, you're sacrificing volume to be more nimble and accessible, which is analogous to bandwidth vs latency. L1 cache has the lowest bandwidth but also the lowest latency. Which means it's capable of moving small pieces of data to registers, and doesn't make the CPU wait very long (a few clock cycles). Whereas the L3 has the highest bandwidth and the highest latency, probably dozens of clock cycles. It's good at loading in big chunks of data from the RAM, and parts of it to L2 but that operation takes say dozens of clock cycles.

The point of having this hierarchy at all is that RAM is actually very slow compared to CPUs. CPUs have hit GHz a bit over 2 decades ago, which means that some operations can be completed in 1ns or less. However, RAM requires many nanoseconds to be read from and written to even today, and if the CPU waited for it all the time then it'd be irrelevant to have faster clock speeds. The cache hierarchy is an attempt to bypass this limitation of RAM. Since L3 is bigger than L2 and L2 is bigger than L1, you can think of L3 as basically a slice of the contents in RAM, L2 as a slice of L3, and L1 as a slice of L2. This arrangement basically arms the CPU with data that a running program might need in the near future so that you don't incur the penalty of waiting on the RAM for every little memory access, only every now and then. This only works because RAM is read in parallel and that it has the bandwidth to feed the L3 cache, but not the latency to feed the CPU's registers.

This arrangement also causes things like data accesses in arrays to be faster than structures like linked lists because in an array, data elements are directly adjacent to one another, which provides greater probabilities that the subsequent elements are already loaded in the cache. Whereas with linked lists, this node might be in the cache, but the other nodes may be randomly scattered throughout memory, and each node access would then require waiting on the RAM",ht67cp1,t3_s6mxkp,1642511324.0,False
s67l1c,"Nice explanation!

>Perhaps  you could comment on the ones that I missed but you know about.

SVD can be used to compress the wavefunction |Ψ⟩ of a quantum mechanical system expressed in the [Matrix Product State](https://en.wikipedia.org/wiki/Matrix_product_state) formalism (or tensor networks, more generally). Loosely speaking, SVD is used to [split](https://en.wikipedia.org/wiki/Schmidt_decomposition) a system into two parts, say U and V, which gives |Ψ⟩ = |U⟩S|V\^T⟩. Here the singular values in S encode the quantum entanglement between U and V.

The compression is done by discarding small (< 1e-10, say) singular values in S (together with corresponding columns/rows in |U⟩ and |V\^T⟩). This effectively removes unimportant entanglement information from |Ψ⟩. [Repeating the procedure](https://arxiv.org/abs/1008.3477) on all possible partitions of the system, creates a low-rank approximation of |Ψ⟩ which can be accurate under a few (but important) circumstances.

Since the data required to describe a quantum systems of N particles would otherwise scale exponentially (e.g. 2\^N for spin-1/2), this SVD procedure is key to pushing the limit when simulating large quantum systems on classical computers.",ht27k3z,t3_s67l1c,1642440487.0,False
s67l1c,"**[Schmidt decomposition](https://en.wikipedia.org/wiki/Schmidt_decomposition)** 
 
 >In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces. It has numerous applications in quantum information theory, for example in entanglement characterization and in state purification, and plasticity.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",ht27mob,t1_ht27k3z,1642440514.0,False
s67l1c,Good bot !,ht2zxrs,t1_ht27mob,1642451241.0,True
s67l1c,Works well with image compression too.,ht2o2eu,t1_ht27k3z,1642446716.0,False
s67l1c,"Yes, exactly. 💯",ht2zvz6,t1_ht2o2eu,1642451221.0,True
s67l1c,Thanks for sharing! The compression part you mention is similar to what one would do to pick principal components that corresponds to the largest singular values when performing PCA for data analysis/visualization of higher dimensional data.,ht2lwrr,t1_ht27k3z,1642445898.0,True
s67l1c,I had an exam today and was looking for SVD for 1h . What were the odds that you would publish this 2 and a half hour after it ? Lol,ht39gek,t3_s67l1c,1642454928.0,False
s67l1c,😮👀 oh damn. If only I knew.,ht3bysv,t1_ht39gek,1642455899.0,True
s5vvf5,When are you going to start?,ht0c5br,t3_s5vvf5,1642401213.0,False
s5vvf5,An email will be sent within 4 days. Thank you for the note.,ht0c9nu,t1_ht0c5br,1642401293.0,True
s5vvf5,"What will be the medium for communications? Discord group? Slack? Two weeks of the course is equivalent to how many lectures?

Personally, I will be self-studying Design and Analysis of Algorithms on the first semester of the year. My goal is to be able to tackle advanced topics, in particular integer linear programming, by the second semester. However, I didn't plan to follow this MIT course because, as far as I'm aware, it's a second course in algorithms that requires some background I still lack. I just selected some lectures that I found useful. Are you aware that there is a previous algorithms course (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/) to the one you pointed?

Anyways, I will be studying the CLRS book, which is used in the course, but I will start with earlier chapters that are not covered (in fact, they are assumed as known by the student...). Hence, I'm not sure if I'd be able to keep pace with you. Depending on the dynamics of the community, it'd be a pleasure to be part of your endeavor.",ht6l1no,t3_s5vvf5,1642517764.0,False
s5vvf5,"> What will be the medium for communications

Discord and maybe Zoom.

> Two weeks of the course is equivalent to how many lectures?

You can see the course's schedule through its link on my post.

> Are you aware that there is a previous algorithms course

Yes I am. Thank you for the note.

> it's a second course in algorithms that requires some background I still lack

You might watch the lectures even if your comprehension isn't 100%, Then pick-up an easier reference and exercises related to the lecture you didn't fully comprehend.

The main requirement is flexibility and capability to interact and learn from others.",ht72ncc,t1_ht6l1no,1642524596.0,True
s5vvf5,"**NOTE** As seen [here](https://imgur.com/a/hHMmfjc), Someone sent a wrong email address. If it is you, Please submit another form.",htfui99,t3_s5vvf5,1642671774.0,True
s5vvf5,signed up - hope I'm not too late!,hv1ayof,t3_s5vvf5,1643658894.0,False
s5vvf5,No you are not,hv3971h,t1_hv1ayof,1643688300.0,True
s694uk,">     I've read that bayesian neural networks can mitigate this problem

Do you base it on some paper? My understanding of BNNs is that they are used for some kind of regularizations to prevent overfitting. If I understood your problem correctly then this problem while close its still not the same as your problem and ill explain.

In most methods, we assume our data came from some probability distribution and we wish to ""say smart things"" about said distribution from only those samples.

Overfitting essentially means the algorithm does not understand the overall distribution from the samples, and think they came from a much more specified distribution. This problem is often very hard to deal with and some sophisticated methods are used. 

But on the other hand, the ability to understand *different* distributions through the data is something else. For example, if i teach my network to classify animals but gives it only dogs, what it does when shown cat is unpredictable - it might classify as animal because it look somewhat similar to the dog (the distribution we sampled the data from) or it might think its not from the distribution and classify as not an animal. 

Those two problems while similar are different, thats from my understanding of things.",ht2w0ml,t3_s694uk,1642449716.0,False
s694uk,"Thanks for the response, hadn't head about overfitting before and while its not what I'm looking at it certainly has similarities. 

The problem I'm looking at is creation of false positives due to the system not understanding what it is seeing. Going back to the reading that suggested that BBNs could assist with that (The Alignment Problem by Brain Christian), the idea was that an ensemble of modals using Bayesian uncertainty would likely disagree with each other when faced with anything far from the data with which they were trained. 

We can use this degree of consensus or lack of consensus to indicate something about how comfortable we should feel accepting the models guess. We can represent uncertainty, in other words, as *dissent*.

This would be particular useful if a false positive could have dire consequences or in a situation were the available training data was limited or couldn't possibly take into account the complexity of the situation where the AI would be operating. 

Thanks again for the response!",ht41jc4,t1_ht2w0ml,1642466483.0,True
s694uk,"Using an *ensamble* of bnns is actually an interesting thought, i havent considered it.

In general without actually using things its very hard to accuratly guess if something would work in deep learning algorithms, so I wouldnt put all of my eggs on it.

Your problem essentially means that using one NN would lead it to have a distribution over the results that has very low varience but could be wrong. What Brain is saying (if i inderstood correctly) is that the distribution itself changes greatly between each training proceedure and while each module seperatly remains with low varience the addition of all modules will get high varience thus indicate that the net does not understand the input.

Thats to my understanding as i havent been working with bnns much.",ht5f01k,t1_ht41jc4,1642490430.0,False
s694uk,Yeah that's basically the case I believe.  I appreciate the chance to bounce ideas back and forth. Helped it make sense in my head,ht7hgw9,t1_ht5f01k,1642530024.0,True
s67zoq,"Without the ""optimization"" the code would not be correct.

To prove lets look at the simple case of n=2 and p=0.5

There is now two rolls of the dice, one for index(1,2) and the other for (2,1)

If one of the rolls win (with the probability of 0.5) there would be an edge between the vertecies.

The probability of at least one wins is one less the probability of both lose which is (because the rolls are independent) the multiplication between each p, which ends up in 0.75 for an edge unlike the promised 0.5.

You can make the code clean in the ""optimized method"" or to allow the roll to make true values in the array false, but this would implicitally only consider half the rolls in double the runtime.",ht20ppf,t3_s67zoq,1642437870.0,False
s67zoq,"Oh, right. I missed the part where I need to set a true to false if the second roll is false. Thank you so much.",ht2jd6n,t1_ht20ppf,1642444938.0,True
s67zoq,"I think the algorithm would be correct if it also explicitly the edge to false when curr_p > p. In that case the existence of an edge between each pair of vertices would still be set twice but only the latter time would matter.

Without that, the second time those same row and column indices are hit can only *increase* the probability of there being an edge, so it's not the same as evaluating the probability only once for each row and column index pair.

So, basically what u/MyCreativeAltName said.",ht2hrik,t3_s67zoq,1642444331.0,False
s67zoq,Thank you so much. This makes so much sense.,ht2ji6p,t1_ht2hrik,1642444990.0,True
s5r8t0,Youtube recommendations mainly. MIT Tech Review is pretty great too to keep track.,hszhzqw,t3_s5r8t0,1642386130.0,False
s5r8t0,"Domo Origato, yeah youtube is great",hszjax1,t1_hszhzqw,1642386701.0,True
s5r8t0,"Hacker News, KDnuggets, IEEE Spectrum, Communications of the ACM, Quanta",hszt0jk,t3_s5r8t0,1642390908.0,False
s5r8t0,"ACM's digital library: https://dl.acm.org/

IEEE's digital library: https://ieeexplore.ieee.org/Xplore/guesthome.jsp",ht08fh0,t3_s5r8t0,1642398925.0,False
s5r8t0,"Hey there!  
This is the  weekly tech news teller me and my friend created you can see the main sources of information for this newsteller that are used on the main page listed. ( these webpages themselves are very informational for news and etc.)  
[https://techteller.org/](https://techteller.org/)  


Hope it will help.  
Thanks.",ht0bihr,t3_s5r8t0,1642400803.0,False
s5i47n,"It’s not a stupid question. If you had a component like a transistor but which could output 10 distinct voltage levels — let’s call it a TENsistor — and that output would in turn serve as an input to another tensistor, etc.. you could represent 10^8 states with 8 signal traces.

It wouldn’t be 8 “bits”, but maybe we could call them 8 “dits”. An 8 dit system would then be able to process a 10^8 value (100M) in a single instruction (I’m oversimplifying here but the principle holds). A binary computer would need a 27-bit architecture to achieve the same.

(Again, really oversimplifying here)

So if you had tensistors and were able to lay them down on silicon as densely as in a modern CPU; and if your CPU had the same number of tensistors as the binary CPU had transistors, then yes. It would be much faster.

Of course, you could absolutely design a ‘tensistor’ out of binary logic gates, but it would end up being orders of magnitude larger and more complex than the damn clever NAND based logic in a modern CPU, and so your new architecture would end up in practice being way slower that way.

And going the analog route, where you actually distinguish different voltage levels inside every component — I think we’d be looking at another couple orders of magnitude in size and complexity to do that.",hsxt11m,t3_s5i47n,1642361852.0,False
s5i47n,"I would like to add that the speedup using dits instead of bit is ""just"" a constant factor. The possible representable states grow faster with the exponent than with the base (for large enought numbers). Meaning it is more important how many dits/bits you have than how many states a single of them can hold.

As long as you can build the tensistors just n times larger you would speed things up. If you instead need something like n^2 or even 2^n times the space, tensistors would be worse than transistors (regarding computing capacity per space).

So it is just more efficient to simple increase the amount of bits than to mess around with dits.

PS: I like the word dits 😀",hsyeh0b,t1_hsxt11m,1642370034.0,False
s5i47n,"Yep, good point!",hsyes8a,t1_hsyeh0b,1642370154.0,False
s5i47n,"Take a look at [settling time](https://en.wikipedia.org/wiki/Settling_time). When a wire changes voltage levels it doesn't immediately change to the new voltage; it overshoots and wiggles a bit before it settles down to the intended value. In a binary system there is a lot of room for error, since the wire only needs to be above some voltage or below some voltage. When the wire changes values, it will be high or low pretty quickly, and you don't need to wait long for it to settle into a value that's close enough.

If there are 10 possible values, the voltage will need to be much more precise, so the system will need to wait longer for it to settle on the precise level. This will make all the electronics much slower. I'm a software guy, so I can't say precisely, but an electrical engineer could probably quantify exactly how much slower things would be.",hszi7yc,t3_s5i47n,1642386225.0,False
s5i47n,"Other posters have answered the question well. Just for fun I'll mention that storage (as opposed to logic) actually does use higher number of states. Many SSDs use 2 or 3 bits per cell (4 or 8 distinct charges). It's really tricky to get right and slower, but it's much more space efficient.",ht05206,t3_s5i47n,1642396993.0,False
s5i47n,"It's hard to represent 10 different states. These states could sort of blur together, and distinction between them would be a lot harder. 1/0 (on/off) makes it so that there is no ambiguity- it is easy for a computer to tell the difference.",hsy6nwz,t3_s5i47n,1642366982.0,False
s5i47n,"It's not that it's impossible. It is very possible.

In fact, the first programmable, electronic, general-purpose digital computer - famous [ENIAC](https://en.wikipedia.org/wiki/ENIAC) - was operating on decimal numbers!

But we don't see such machines anymore. Think, why?",hsxsodo,t3_s5i47n,1642361722.0,False
s5i47n,We have also made [ternary computers.](https://en.wikipedia.org/wiki/Ternary_computer),ht062in,t1_hsxsodo,1642397574.0,False
s5i47n,Is that because of potential signal inaccuracy? I genuinely don't know why.,ht0iom8,t1_hsxsodo,1642405656.0,False
s5i47n,I don’t get your point. Why do you think it would be faster?,hsxkc7q,t3_s5i47n,1642358627.0,False
s5i47n,"No, I would expect about 3 times as fast, since with 3 bits you already have 2^3 = 8 possible states, which would be as good as having a base 8 computer. 
Also, during a calculation you are not interested in the base 10 representation. So it's not that useful.
Additionally, computers also do computations on for example pointers, so not all computations are on (base 10) numbers.
Short answer: no, I don't think it will be a lot faster (if it's even faster at all)",ht06llw,t3_s5i47n,1642397874.0,False
s5tfpw,"What's your question exactly? Are you saying you want to play a human to be able to ""play"" the Neural Net instead of the actual game?",ht01pww,t3_s5tfpw,1642395181.0,False
s5tfpw,"I'm wondering if anyone's trained a neural network to simulate/predict the output that a regular video game would give, so you only need to run the NN instead of the full game. I think you understand correctly. The net would be outputting its best guess of what the full game would have returned.",ht037jq,t1_ht01pww,1642395987.0,True
s5tfpw,So you essentially want an ai that can…. Play games? It would be interesting in theory I suppose but training an ai on so many different games that it could theoretically predict the outcome of a new game would be a very complex task I feel,ht07h5m,t1_ht037jq,1642398373.0,False
s5tfpw,"I think OP meant that the neural network would act in stead of the game, and would basically attempt to react to the player's inputs as a game would, without actually going through the actual exact algorithms of the game. So, rather than replacing the human player with an AI in the game/player combination, you'd replace the game by having some kind of a neural network that produces a stochastically generated game-like result and behaviour, e.g. by learning that games tend to pan a view to the left when the player presses the left turn key.

Or at least that's what I got from the comment you replied to.

So maybe something like https://www.youtube.com/watch?v=atcKO15YVD8 but with full game behaviour, including some kind of graphics generation etc.?",ht2x92f,t1_ht07h5m,1642450196.0,False
s5tfpw,"So I think you might have a slight misunderstanding of how Neural Networks function. Well to begin with, the type of network you'd use to build a reinforcement learning model (which you'd train to learn something like chess) is vastly different from one which would generate pixel data (which in your scenario could be a model which predicts the next frame of a game based on input)

For the first type, there's almost never any visual data being passed to the network. The input is in the form of an array (data type could vary but let's just assume this for now) which summarises the current state of the game; and the output could be something like a simple integer which would dictate the model's next move. There's then a reward function which evaluates how good or bad the output move is, which can then be used to tweak the model's parameters for the next iteration of the model's training. This sort of net could definitely be played against in a video game, but not really in the manner you're describing. Think of it as being able to train an AI adversary within the game, rather than creating the entire game itself. The entire game engine, visual assets etc. would still be required to interpret the model's output and put them in the game. This sort of thing has already been done with chess and a few other games. Now one additional thing since you've mentioned Doom Eternal; the more complex the mechanics of the game, the harder it's going to be to setup a good enough model to play it. Afaik we're definitely not at a point where we can create a model to be a player of a game like Doom Eternal.

Now for the second type of model, one where you'd predict the next frame of a game based on a starting frame and any input. This sort of model is theoretically possible today, but it's going to be almost impossible to meaningfully train it to function as a game. This is more in the realm of procedurally generating frames. You wouldn't be able to program any rules/mechanics/levels of any sort into the game to begin with, the network will merely come up with what it thinks should come next based on what it's seen before during training. There's also the problem that this sort of process might work for a few frames but will soon fall into chaos; this is because after a few frames, the original output frames will then need to serve as input, and the general entropy from the unpredictability of the output will be magnified exponentially. Also as a bit of a footnote, a game like Doom Eternal is not going to be feasible for this sort of approach; all of the problems I've highlighted will be exacerbated proportional to the complexity of a single frame of the game.",ht07s8r,t1_ht037jq,1642398553.0,False
s5tfpw,"That... would be kind of scary. Imagine you just train a NN on the top video games, somehow pass in levers to randomize level design, graphics, and gameplay according to what's available in different games, then you get unlimited NN-powered video game experiences, where you only need to polish and do voice-over/direction. I'm really curious now, is this viable?",ht0539y,t3_s5tfpw,1642397012.0,False
s5tfpw,I think TTS tech is looking really promising. If it keeps improving at the current pace we might not have much need for voice actors in 10-ish years.,ht05x5t,t1_ht0539y,1642397489.0,True
s57kne,[removed],hsy9ubx,t3_s57kne,1642368224.0,False
s57kne,[removed],hsym6ff,t1_hsy9ubx,1642373065.0,False
s57kne,"Thanks for posting to /r/computerscience! Unfortunately, your submission has been removed for the following reason(s):

* **Rule 2:** Please keep posts and comments civil.



If you feel like your post was removed in error, please [message the moderators](https://reddit.com/message/compose?to=/r/computerscience).",hsza1zr,t1_hsy9ubx,1642382717.0,False
s57kne,"This is so cool, man",hsxnqgk,t3_s57kne,1642359880.0,False
s5swwz,"I think by this logic you could say all computer development is physics, because it either pertains to the materials the computer is made out of, or the electricity running through those materials. That's not a _wrong_ classification, but it's probably not the most helpful, either.

We usually distinguish software and hardware _because_ of the practical differences in skill sets. Most software engineers don't have a deep understanding of computer architecture and microprocessor design. Maybe they've had no exposure past one class in undergrad with a few diagrams about the ""five stage pipeline"" and writing a bit of assembly. Similarly, most computer systems engineers don't have the same depth of software understanding as a software engineer or computer scientist, and haven't taken classes on operating systems, compiler/interpreter design, computer networking, and so on.

To return to your brain and thoughts example, neuroscience, psychology, and therapy all overlap in that they deal with your brain and thoughts, which are certainly interconnected. Nevertheless, those three fields are distinct and have a lot of divergent expertise.",hszwq0d,t3_s5swwz,1642392641.0,False
s5swwz,That was by far the best explanation I saw. Thank you so much for clarifying this question,hszy9ot,t1_hszwq0d,1642393391.0,True
s5swwz,"The false assumption lies in this statement: ""All the software information is stored in the hardware, which means it is a physical thing"".

While running a program, it is stored in hardware. But software can also be sent over a network, written down on a piece of paper, or exist only in the head of the programmer. That's why it is soft.",ht0fvw4,t3_s5swwz,1642403696.0,False
s5swwz,the software information is stored as a physical thing but specifically it's as a electrical charge. the configuration of that charge is easily changeable the physical hardware is not.,hup9zj4,t3_s5swwz,1643448589.0,False
s4k68d,"Here are two college textbooks that I highly recommend. They're college textbooks, so it means they're very expensive, but it also means there's lot of reasonably priced used copies out there.

Introduction to Algorithms, i.e. ""the CLRS big book of algorithms""  
[https://mitpress.mit.edu/books/introduction-algorithms-third-edition](https://mitpress.mit.edu/books/introduction-algorithms-third-edition)

Artificial Intelligence: A Modern Approach, i.e. ""AIMA"" or ""Russell and Norvig""  
[http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)  


Books about programming language go out of date quickly. These books focus on the procedures themselves (independent of any particular language), so the material stays relevant for a long time. Some of the material in these books is 70+ years old and still studied because they remain relevant today, even though computers and programming languages have changes dramatically since then.",hssfla4,t3_s4k68d,1642268301.0,False
s4k68d,"It’s not a book but I wish someone had encouraged me to read research papers and technical documents sooner.

RFCs are often networking focused but a lot of what we do in CS has direct applications there and can be abstracted/generalized with graph theory. There are algorithms, data structures, topological suggestions, and even some coding best practices.

It’s great to go to those documents because I think people come into computer science with a lot of preconceived notions about how logic and computer systems work. It helps me to see what the actual thought leaders saw as the pressing industry issues, and helped me to see the standard for state of the art solutions.

Stuff like that fleshes out the fundamental understandings your books will teach you - while giving you a sense of the breadth of the field.",hss4syd,t3_s4k68d,1642264103.0,False
s4k68d,What is a RFC?,hst5q0v,t1_hss4syd,1642278464.0,False
s4k68d,"An RFC is a “Request For Comment”, which is like a formal statement of a proposed system, protocol, algorithm or otherwise solution to a technical problem. 

It is intended to be an opportunity for a community of engineers to gather around a proposed solution and improve it, or otherwise reference the agreed standards of a given solution.

The most famous set of RFCs are by the IETF (Internet Engineering Task Force): https://www.ietf.org/standards/

Read through some - start with something you recognize. Gateways are a good starting point in my opinion, or RFC1918. I think you’ll find them accessible given a little familiarity.

The really exciting thing about computer science for me is this quote from Steve Jobs:

“Everything was made up by people that were no smarter than you”",hst6mka,t1_hst5q0v,1642278827.0,False
s4k68d,I enjoy reading RFCs a LOT! I usually read them and then try to implement the technique discussed in the RFCs. I usually read networking RFCs,hsvgp91,t1_hst6mka,1642316353.0,False
s4k68d,"I'm in a different field and I agree with the research papers suggestion. 

One surefire way to do it is to look at textbook authors' and contributors' journal articles. Search Google Scholar for their names and download some that are free. This way you don't easily stumble accidentally on journal articles that have bad writing/wrong methods. Same could work for referenced articles on Wikipedia pages: search for more work by the same author(s).

Once you have some downloaded, read a few paragraphs from each and see how much you understand. If it's 60% or more, awesome, read that one. If it's less, discard and grab another. You could try to look up terms but usually there are whole complex ideas and esoteric methods behind a bunch of the terms. Also the writing could just be an overly esoteric style. Find an article first that conveys meanings well to you before you get frustrated trying to look terms up.",hsvxybf,t1_hss4syd,1642329617.0,False
s4k68d,I think it is a good idea to continue reading SICP. If you like math you can try reading an intro discrete math book.,hsslabi,t3_s4k68d,1642270500.0,False
s4k68d,"There are a buncha people who say SICP as a whole is outdated. I have an easy time understanding and disagreeing with that perspective. However, I have also seen some claim that SICP chapters 1, 2, and 3 are liquid gold, while chapters 4 and 5 are lacking comparatively. How do chapters 4 and 5 compare to the earlier chapters?",hswscx2,t1_hsslabi,1642347684.0,True
s4k68d,https://www.reddit.com/r/computerscience/comments/s14xir/comment/hs6fo0x/?utm_source=share&utm_medium=web2x&context=3,hss4zf0,t3_s4k68d,1642264174.0,False
s4k68d,"Go through all the books/courses in this website: [https://teachyourselfcs.com/](https://teachyourselfcs.com/)

I went through most of them during uni, and can attest that they are all fantastic textbooks.",hstp77k,t3_s4k68d,1642286442.0,False
s4k68d,"Hunt, Thomas. The Pragmatic Programmer

Robert Martin. Clean Code

Robert Martin. Clean Architecture

Martin Fowler. Refactoring

These will get you past the stage of hacking together something that works, to the point where you at least know what well-designed code is supposed to look like.

After that, you're ready for

Eric Evans. Domain Driven Design

Kent Beck. Test Driven Design

Gang of Four. Design Patterns

Joshua Kerievsky. Refactoring to Patterns",hssf8h6,t3_s4k68d,1642268166.0,False
s4k68d,"I should point out that literally *none* of these books are computer science books. They are **software development/engineering** books. Very important field, and where I spend most of my time even though my job title has compsci in it, but it's an important distinction for the newbie to recognise that these are not compsci.
 
Computer science books would be things like
 
* any 1 of several algorithms & data structures books  
* The Art of Computer Programming  
* Types and Programming Languages  
* Compilers: Principles, Techniques, and Tools  
* Introduction to Automata Theory, Languages, And Computation  
* Computational Complexity: A Modern Approach
 
However the 2 most important OP already has, K&R + SICP.",hstlib0,t1_hssf8h6,1642284927.0,False
s4k68d,"Meh... I know this perspective and I think it's from the way how courses are labeled. They put SE on one side and everything else on the other. A bit like programming and mathematics in the 60s and 70s. This way of separation rly hurts my brain...

I can see connections between Uncle Bob's SOLID principles and Dijkstras thoughts in the humble programmer. I see the connection between engineering methods and HCI and to design and to psychology to constructivism to hylomorphism to Alexanders adaptive morphogenesis to digitalisation of analogue signals to approximation in mathematics. In what bucked shall I put these thoughts? SE or CS?

I rly prefer the perspective: Software engineering and software development are specialised fields in Computer Science that exist alongside other fields like algorithm or machine learning. Everything is connected and further more it's connected to domains that are clearly out of the field of computer science.",hsu4iyh,t1_hstlib0,1642292816.0,False
s4k68d,"It's not a perspective, it's a reality. Software developers/engineers use computer science, but that doesn't mean that they aren't different things. Connections between things don't mean there is no difference between things - there is a connection between my finger and my wrist, but they are distinct. Software development/engineering is *not* a specialised field in computer science, it is an interdisciplinary field that includes (some of) computer science, and also includes a bunch of other things. This is objective fact, it's not something multiple people can have different correct opinions on.",hsu59s8,t1_hsu4iyh,1642293133.0,False
s4k68d,"Yes sure connections don't mean there are no differences. But that's not the point: You think of wrists and fingers. I think of limbs, wrist and fingers.

And an objective fact, the reality in relation to a human concept... Something like this doesn't exist. That's why I speak of perspective and don't tell anyone what is right or wrong. Every single human concept is just a conceptual model based on our personal mental model. If I ask you to draw me past and future down on a paper you may draw a line from left to right. Someone else in the world will draw a line from right to left and another from top to bottom. Which conceptual model is right?",hsu9zcz,t1_hsu59s8,1642295113.0,False
s4k68d,"If you throw out the meanings of any words, you lose the ability to communicate. You can either speak English with the rest of us where the words computer science and software development mean computer science and software development, or you can speak whatever language you are speaking where they aren't different things - but you can't expect to be able to communicate with anyone if you run off and use your own definitions.",hsubyjz,t1_hsu9zcz,1642295964.0,False
s4k68d,"Exactly! 😊 It's about the meaning! Very good.

""The spoken words are the signs of ideas in the soul and the written words are the signs of spoken words. Just as the written signs are not the same for all people, so the words are not the same for all people."" -Aristoteles, Peri hermeneias

Semiotik? Model theory? Stachowiak? 
Words are only sounds, without a human interpretation they are indeed meaningless. But here, in a discussion about what is computer science, we have to think in different levels of abstraction, a different granularity when looking at the details, the relation and the dynamics with which a concept is connected. And in a context like this, the subject of computer science, I think that one can expect more than a linear simplified form of classification to answer this question.

Does anyone object when someone asks software engineering questions here? Why are you working in a professional title but with a software engineering job? It is not simply black and white, even if that is easier and more convenient.

It is very natural and human to create taxonomies and this is the basis for what is ""true"" for us. In the end, however, it remains only a simplification of reality, a way for our human mind to grasp the potentially infinite complexity of reality, which always remains an incomplete representation. And because we see this image as true, things that say otherwise are false, an attack on our world order that makes us quite aggressive because we want to stand up for what we think is right. This leads us to exclude those who think differently, for example by talking about you and us and automatically implying that you are not part of us. Someone who says he has a scientific background knows that this kind of exclusion has troubled so many of the people we honour today, throughout their lives, and only because people insisted that their concepts were not the one of ""us"".

In the end, however, it remains what it is, a simplification, not a truth. And that's why I prefer the term perspective. And I am very glad that we are having this discussion at this level ""where"" I am from.",hswk4ou,t1_hsubyjz,1642344015.0,False
s4k68d,">Why are you working in a professional title but with a software engineering job
 
Because people who aren't in the computer science or software development fields don't understand the distinction, yet write the job titles for those fields. Same reason why programmers get asked to fix hardware.
 
This is not a philosophical question, it is a question of what the community of practice has defined as different things over decades of communication. Again, you can choose to run off and use your own definitions, but you self-exclude yourself from that community and generally make yourself a nuisance when you impose your incorrect definitions upon that community.",hswkzp4,t1_hswk4ou,1642344425.0,False
s4k68d,"> Because people who aren't in the computer science or software development fields don't understand the distinction, yet write the job titles for those fields. Same reason why programmers get asked to fix hardware.

So we can agree that in general, in the non-professional environment, these issues are not differentiated. We are ""computer people"". But the perspective that I follow and that we also try to build our curriculum around and make first-year students understand what their computer science degree consists of is a variety of disciplines in the field of computer science. Even if we exclude SE from this consideration. With a computer science degree, will what you do simply be computer science? No, it is the specialisations that we have chosen. Visualistic, machine learning, digital signal processing, etc. are all directions in computer science that are so broad in themselves that we consider them as a separate field. To unite all this under the term computer science is just as inappropriate as asking a programmer to repair hardware. 

This is not only the case with CS, but also with SE, CE and all other categories that were defined in the 90s. This classification is the result of a knowledge-based education where I dont say its right or wrong. It treats computing as a meta-discipline a collection of disciplines having a central focus of computing. But even this classification now recognises that these disciplines are intertwined. In particular, SE and CS have a massive overlap in the theoretical part of ""application technology"", ""software development"" and ""systems infrastructure"". The ""community"" has been classifying CS for 10-15 years in such a way that almost all aspects of CS are relevant to SE. And rightly you will say that someone who has spent the same time focusing on more theoretical content will probably be more competent in this field than someone who has spent time in both theoretical and application.

And that is also the reason why I do not like this classification. It is simpler but loses important details that are relevant for example in a job profile. I prefer a classification based on competences. Here, knowledge, skills and disposition are considered in the context of computer science without a separation into disciplines. And we have already defined in 2017 which elements of disposition, cognitive skills and computing knowledge are useable for this classification. The resulting profiles are much more complex but much better suited to describe people or activities.",hsx2q7g,t1_hswkzp4,1642351895.0,False
s4k68d,">we can agree that in general, in the non-professional environment, these issues are not differentiated
 
Someone who isn't a chemist can't easily differentiate between a bottle full of H2O and a bottle full of H2O2. You going to drink both?
 
You are teaching your students incorrectly. A history major learns much more than just history. That doesn't make those other things 'history' just because they are taught to a student in a history major.
 
You can prefer whatever you like, but you are incorrect, and you are just going to confuse the people you speak with until they realise you are speaking a pretend language.",hsx4djw,t1_hsx2q7g,1642352548.0,False
s4k68d,Well I didn't talked about what they learn and what not and that's the point. You lack the ability to listen and even comprehend what I am talking about. So: You are right I am wrong and I'll take these perspectives to a place where they can't hurt you.,hsxcas1,t1_hsx4djw,1642355577.0,False
s4k68d,"As a collector of CS books, thank you, this just added a few to my to-buy list",hst02yz,t1_hssf8h6,1642276214.0,False
s4k68d,Code by Petzold. It was required reading where I went to college but I regret not reading it long before that. It's not a long book at all but should absolutely be required reading everywhere.,hstqyxf,t3_s4k68d,1642287165.0,False
s4k68d,"1. The Nature of Computation by Cristopher Moore and Stephan Mertens
2. Concrete Mathematics by Don Knuth et al
3. Algorithms a Creative Approach by Udi Manber

But please finish SICP first. Makes everything look a lot easier and more interesting.",hsuzxom,t3_s4k68d,1642306524.0,False
s4k68d,"https://doc.lagout.org/Others/Data%20Structures/Data%20Structures%20and%20Network%20Algorithms%20%5BTarjan%201987-01-01%5D.pdf

This was used as the text for a graph algorithm class I took in graduate school I think it feels like a really intuitive and interesting way to describe the subject. Personally, I really like the way Tarjin writes and introduces algorithms. This doesn’t teach you C or Java, but something much more fundamental. Give it a look if you want a bit of a challenge!

Tarjin for me is the Knuth of graph algorithms.",hsvdm6k,t3_s4k68d,1642314291.0,False
s4k68d,Cracking the Coding Interview. Recommended to me by my professor and really helps you prepare for interviews and real world problems!,hssilmj,t3_s4k68d,1642269469.0,False
s4k68d,Pragmatic programmer.,hsth7dx,t3_s4k68d,1642283167.0,False
s4k68d,studying UML would be helpful since different types of diagrams can be very helpful in planning and communicating software systems among technical and non-technical people alike,hssx7py,t3_s4k68d,1642275089.0,False
s4k68d,I do not know a single person that used UML more than once a year after getting their degree.,hsudst9,t1_hssx7py,1642296743.0,False
s4k68d,"At certain levels of the planning process diagrams are definitely useful.  UX workflows, multi-system interactions, data flows, etc.  I find they help with business to technical specification as a bit of common ground between analysts and engineers.",hsv43n3,t1_hsudst9,1642308693.0,False
s4k68d,hey ppl do you know Doctor Strange?,hu1xlz8,t1_hsudst9,1643050307.0,False
s4h4a4,Just a guess but perhaps when the authors refer to ‘implementers of C’ they’re talking about people making compilers of the language?,hsqz9o2,t3_s4h4a4,1642241220.0,False
s4h4a4,That would be my guess as well.,hssdcs5,t1_hsqz9o2,1642267435.0,False
s4h4a4,Yeah makes sense,hsu0ix3,t1_hsqz9o2,1642291124.0,True
s4h4a4,"What I think they're saying is:
- implementer of C = someone who creates parts of the C language itself or an extension to it.
- C programmer = someone who uses the C language to develop for example an application.",hsqzjxr,t3_s4h4a4,1642241434.0,False
s4h4a4,"One makes C, other makes with C",hsqzp0a,t3_s4h4a4,1642241545.0,False
s4h4a4,"Implementer most likely to be referred to someone who's creating the C libraries, improving compilers etc 
Programmer is the person using the language/consuming the libraries created by implementer to develop something say a game.",hsr6kap,t3_s4h4a4,1642246688.0,False
s4h4a4,Yeah,hsu0m67,t1_hsr6kap,1642291162.0,True
s4h4a4,"Implementer refers to one who makes the ""idea"" of C language into reality: writes the compiler, runtime library etc.

Programmer is one who writes programs in the C language and depends on the implementation of it (compiler, runtime etc.).

The ""idea"" of a language can be considered to exist as described in standards and definitions before there exists an implementation of it although some people start hacking on an implementation before there is a formal definition of it (syntax etc.)

Implementer also has to consider how the description of language will apply in real world in a real computer architecture.",hsraj1i,t3_s4h4a4,1642249501.0,False
s4h4a4,That’s helpful thanks,hsu0wc8,t1_hsraj1i,1642291278.0,True
s4h4a4,The implementer is the person writing the compiler or the standard library,hsranpy,t3_s4h4a4,1642249590.0,False
s4h4a4,"An implementer of axes is a smith, a “programmer” of axes is a lumberjack.",hss41c5,t3_s4h4a4,1642263796.0,False
s4h4a4,That’s a good analogy,hsu0qd2,t1_hss41c5,1642291211.0,True
s4h4a4,Amogus,hss80tu,t3_s4h4a4,1642265355.0,False
s4h4a4,"C implementer: makes compilers or implements the standard library for C

C programmer: people who program in C",hsspkvj,t3_s4h4a4,1642272158.0,False
s4h4a4,"Implementers are engineers who write the C compiler from the official C spec. There are many versions: c99, c11, etc. A spec is a document that defines the standards of the language. The grammar, the rules, etc. These rules aren’t always strict and some of it is left to the implementers to decide. 

The C programmer consumes the implementers work and writes programs with the help of routines it provides in a shape of header files.",ht111tk,t3_s4h4a4,1642419983.0,False
s4h4a4,Yeah makes sense and that is very helpful. I understand that an engineer can can create and implement versions of C that are tailor-made for the OS or compiler they are writing.,hta4oan,t1_ht111tk,1642569187.0,True
s4frba,"Yes, a vast majority of them",hsrqr59,t3_s4frba,1642258169.0,False
s4frba,"I'm not sure I entirely agree with the assumption that any other well-known machine learning method outside of deep learning isn't ""effective"".

Deep learning is pretty much by definition based on artificial neural networks. So, 1/1 of the base methods used for deep learning are, in some sense, inspired by nature (at least originally).

However, for smaller amounts of data or for data with more linear relationships between variables, things like support vector machines may still be completely reasonable. If the phenomenon being modeled is linear or close to linear, a linear model might even well perform better than a nonlinear one such as an ANN, at least if you don't have a crapton of training data. (edit: and nearly all of these other methods are *not* inspired by nature.)

Also, as far as genetic algorithms go, are they actually one of the ""two most powerful"" machine learning methods? I may be a bit out of the loop, but genetic algorithms used to be one of those methods that sound appealing and may get a lot of hype but which often don't necessarily perform that well, or at least not necessarily better than other simpler and less computationally intense methods do.

I've seen some of that hype for GAs appearing again e.g. on Reddit in recent years but I'm not sure that attention is actually warranted. Again, I may be out of the loop, so perhaps GAs have actually enjoyed some kind of a significant rise in real performance along with neural networks recently. Since I'm not aware of that actually happening, though, I'm inclined to think of it as mostly hype, and I wouldn't count GAs as another data point towards effective machine learning being naturally inspired. I'd be happy to be shown wrong by someone in the actual know.

You might want to ask whether there are other methods beside ANNs that could be used for a layered approach that integrates some kind of feature learning, kind of similarly to deep learning. That's a good question, to which I don't know the answer. I'd just phrase it a bit differently because despite the (warranted) attention and hype, deep learning probably isn't the only kind of effective machine learning.",hsrtfwa,t3_s4frba,1642259383.0,False
s4frba,"Maybe do some research into statistics? As a field they do a LOT of data analysis and predication that’s not based around neural networks or genetics algorithms. As far as I know anyway, I’m not a statistician so anyone actually versed in statistics feel free to correct what I’ve said.",hssy8sf,t3_s4frba,1642275486.0,False
s4frba,"Right now DL looks nothing like whats done in nature.

DL are NN that use more then one hidden layer - if you assume automatically that all NN are ""copied from nature"" then of course all DL falls into this category.

Every single ML algorithm is basecally just math, you cant dismiss it for being this way too, math is an extremely powerfull tool.

On top of my head though, a lot of RL algorithm are not at all infulenced from nature, for example DQN.",hsqwvs9,t3_s4frba,1642239423.0,False
s4frba,All algorithms are memorization and/or math. Machine learning leans towards statistics tho. You can’t compute anything (or have an intelligent system) without math.,hss047h,t3_s4frba,1642262217.0,False
s4frba,"Here's a simple method that gets used all the time, and has no connection to nature: linear regression",hstpdfc,t3_s4frba,1642286514.0,False
s3mbvt,Computerphile,hsm7y5o,t3_s3mbvt,1642160395.0,False
s3mbvt,Sebastian Lague,hsmpulv,t3_s3mbvt,1642169963.0,False
s3mbvt,Sebastian Lague is a GOD,hsnpvli,t1_hsmpulv,1642184056.0,False
s3mbvt,"absolutely. his explanations are superb, calming voice, and incredible talent and work ethic. Most importantly, he does really cool shit that inspires me",hsoo39j,t1_hsmpulv,1642197312.0,False
s3mbvt,I just followed your recommendation and man… that was amazing. Definitely subscribing to this channel.,hsp23rg,t1_hsmpulv,1642203066.0,False
s3mbvt,Absolute favourite of mine. Glad you enjoyed him too.,hsp44d5,t1_hsp23rg,1642203922.0,False
s3mbvt,Seconding this one. Sebastian Lague is amazing. His projects are visually very interesting and tackle some fascinating challenges.,hsotvol,t1_hsmpulv,1642199640.0,False
s3mbvt,"One of my absolute favorites. His coding adventure videos are all awesome. His voice is so soothing, I often put his videos on at night/in the morning.",hsqpebs,t1_hsmpulv,1642233722.0,False
s3mbvt,Ben Eater all the way,hsmzg23,t3_s3mbvt,1642173907.0,False
s3mbvt,interesting and hardcore,hsnw3v7,t1_hsmzg23,1642186505.0,False
s3mbvt,I second this,hsn3jg1,t1_hsmzg23,1642175530.0,False
s3mbvt,Happy cakes,hsn3lup,t1_hsn3jg1,1642175555.0,False
s3mbvt,That guy is a legend,hsorp2v,t1_hsmzg23,1642198754.0,False
s3mbvt,Computerphile.,hsm7ygp,t3_s3mbvt,1642160400.0,False
s3mbvt,"[BackToBackSWE ](https://youtube.com/c/BackToBackSWE) is a good channel for algorithms

[Reducible ](https://youtube.com/c/Reducible) is another great channel similar to [3Blue1Brown](https://youtube.com/c/3blue1brown) but specifically for Computer Science. It explores CS topics in a much deeper way with excellent visuals and explanations. 3Blue1Brown does have a few videos on [Neural Networks.](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)",hsnfzr1,t3_s3mbvt,1642180318.0,False
s3mbvt,Two minute papers and mCoding make great videos around those topics.,hsm6i08,t3_s3mbvt,1642159378.0,False
s3mbvt,Michael Reeves is def someone who inspired me to take up computer science. But he’s more of a person to watch to show the fun/humorous side of computer coding.,hslxrll,t3_s3mbvt,1642152682.0,False
s3mbvt,Humorous...ha. He puts alot of effort to make his videos funny,hsn2aq1,t1_hslxrll,1642175042.0,False
s3mbvt,"Algorithms: https://youtube.com/c/UndefinedBehavior

ML/AI (more fun to watch): https://youtube.com/c/CodeBullet

Applied Math (i.e. cryptography, neural networks, FFTs, etc): https://youtube.com/c/3blue1brown",hsndewz,t3_s3mbvt,1642179330.0,False
s3mbvt,Network Chuck!,hsn6wkt,t3_s3mbvt,1642176834.0,False
s3mbvt,"Steven Skiena (channel name)

Computer Science (channel name)",hsom0kh,t3_s3mbvt,1642196494.0,False
s3mbvt,3Blue1Brown if you like a mix of math and cs,hsp8imr,t3_s3mbvt,1642205852.0,False
s3mbvt,"[Jacob Sorber](https://www.youtube.com/c/JacobSorber) is superb for any low level programming interests (embedded systems, C, etc) or just general data structure/algorithms.",hsoa37z,t3_s3mbvt,1642191898.0,False
s3mbvt,">Sebastian Lague

Hey, i never previously found or stumbled on this guy in my search for computer science/programming Youtube folks and creators. Checked him out - and what his content to be a real gem. Especially his data structures with C. I was looking to polish / refresh my data structure fundamentals and exactly in C ! What a finding, thanks!",ht5nbq2,t1_hsoa37z,1642496779.0,False
s3mbvt,Jdh,hsnddeu,t3_s3mbvt,1642179314.0,False
s3mbvt,"You might like some of these:

Fireship
Brad Traversy
Tom Scott
Computerphile
Neural Nine
Corey Schafer
Distrotube
Jabrils",hspb92g,t3_s3mbvt,1642207069.0,False
s3mbvt,"Check out Sentdex. He has tons of videos about anything Python and has shiftet towards an ML/AI type of channel lately. He recently released a book where he wrote many neural network algorithms from scratch in Python/numpy to teach the concepts.

I haven't read it or seen the most recent videos because I'm more into computing systems, but it's a super cool channel.

Another cool one is Anthony Sottile's channel. He makes a lot of open source developer tools in Python. He streams and uploads to YouTube afterwards. He maintains stuff like pre-commit, pytest, flake8, tox and some other cool popular projects.",hso3j02,t3_s3mbvt,1642189388.0,False
s3mbvt,TwoMinutePapers,hso5ty3,t3_s3mbvt,1642190258.0,False
s3mbvt,"* [https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1\_aw](https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1_aw)
* [https://www.youtube.com/user/Computerphile](https://www.youtube.com/user/Computerphile)
* https://www.youtube.com/c/Fireship
* [https://www.youtube.com/c/mitocw/playlists](https://www.youtube.com/c/mitocw/playlists) (This one is a lot of different fields. Basically you can just do a search of ""MIT machine leaning"" or ""Stanford machine learning""",hsm8a82,t3_s3mbvt,1642160626.0,False
s3mbvt,"You may be interested in this awesome project too : [https://www.youtube.com/watch?v=rPkMoFJNcLA](https://www.youtube.com/watch?v=rPkMoFJNcLA)

It's not really machine learning but more about how can evolution make individuals better. It's my favorite ML related project",hsnoimy,t3_s3mbvt,1642183543.0,False
s3mbvt,Engineering Man,hsoptbx,t3_s3mbvt,1642197994.0,False
s3mbvt,"Some fun, interesting ones I didn't see mentioned (but these are pretty specific to their applications) are [ThinMatrix](https://www.youtube.com/c/ThinMatrix) and [BPS.space](https://www.youtube.com/channel/UCILl8ozWuxnFYXIe2svjHhg)",hsotyjz,t3_s3mbvt,1642199673.0,False
s3mbvt,"While his focus might be more on the philosophy side of things, Mark Jago is a trained computer scientist, and his videos are mostly about formal logic, with the occasional foray into CS. 

https://m.youtube.com/c/AtticPhilosophy/videos

Plus he’s got a cool aesthetic",hsp45yl,t3_s3mbvt,1642203941.0,False
s3mbvt,"[Abdul Bari](https://youtube.com/channel/UCZCFT11CWBi3MHNlGf019nw). I've watched and rewatched his content many times. I wish I knew about him when I was taking my algos class in college. He was making many of his algorithms videos right around that time, too.",hsp5tsf,t3_s3mbvt,1642204658.0,False
s3mbvt,i heard his content is just basic and not for intermediate/advanced programmers,hsqigft,t1_hsp5tsf,1642228894.0,False
s3mbvt,"Maybe his non-algo related content. I haven't watched any of it. But he has many videos walking through algorithms that I learned about in my more advanced algorithms course in college.

The first couple of algo videos are fundamentals but there are over 30 in his algorithms playlist.",hsqom73,t1_hsqigft,1642233153.0,False
s3mbvt,"3Blue1Brown has some computer sciency stuff, has tons of math as well.",hspdoaa,t3_s3mbvt,1642208156.0,False
s3mbvt,RemindMe! 1 day,hspr097,t3_s3mbvt,1642214260.0,False
s3mbvt,"I will be messaging you in 1 day on [**2022-01-16 02:37:40 UTC**](http://www.wolframalpha.com/input/?i=2022-01-16%2002:37:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/s3mbvt/interesting_computer_science_youtubers/hspr097/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Fs3mbvt%2Finteresting_computer_science_youtubers%2Fhspr097%2F%5D%0A%0ARemindMe%21%202022-01-16%2002%3A37%3A40%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20s3mbvt)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hspr20l,t1_hspr097,1642214283.0,False
s3mbvt,“Stat Quest with Josh Starmer” for Data Science and Machine Learning. You will not regret it.,hspvrta,t3_s3mbvt,1642216536.0,False
s3mbvt,Fireship. His videos are relatively short and very info packed.,hspz4ek,t3_s3mbvt,1642218161.0,False
s3mbvt,Tren Black,hsq3ipu,t3_s3mbvt,1642220367.0,False
s3mbvt,Joma tech,hsq6ge4,t3_s3mbvt,1642221927.0,False
s3mbvt,"Jordan Harrod makes interesting videos about machine learning and AI ethics, and about her experience as a PhD student",hsqcari,t3_s3mbvt,1642225101.0,False
s3mbvt,"- Spanning Tree
- Ben Eater
- Computerphile
- 3Blue1Brown
- AlphaPhoenix",hsqewgt,t3_s3mbvt,1642226629.0,False
s3mbvt,Carl Herold?,hsqgtj9,t3_s3mbvt,1642227827.0,False
s3mbvt,Forest Knight,htwn83r,t3_s3mbvt,1642960758.0,False
s36y35,"Oh come on, you can't just show something this cool and *not* link a paper or article or at least the name of the tech.",hsjngrf,t3_s36y35,1642111582.0,False
s36y35,"I agree, so I searched “tennis ball tracking software” on Google…

https://en.m.wikipedia.org/wiki/Hawk-Eye#:~:text=Hawk%2DEye%20is%20a%20computer,path%20as%20a%20moving%20image.",hsjuxon,t1_hsjngrf,1642114447.0,False
s36y35,The article says this system works with six cameras which is way less impressive than the singular camera input mentioned in title.,hsjwfch,t1_hsjuxon,1642115023.0,False
s36y35,"Not at all lol.  It's pretty sweet.  The title is just flat out bullshit.  How are you going to tell where an object is in 3D space with only a single input at a single point?

https://en.wikipedia.org/wiki/Triangulation_(computer_vision)

EDIT: Hmm, this is actually an interesting thread:

https://movies.stackexchange.com/questions/572/why-do-you-need-6-points-to-define-a-location-in-3-dimensional-space",hskmll2,t1_hsjwfch,1642125956.0,False
s36y35,">How are you going to tell where an object is in 3D space with only a single input at a single point?

Yeah, that's what made me think ""this is really amazing"".",hslua78,t1_hskmll2,1642150004.0,False
s36y35,"**[Hawk-Eye](https://en.m.wikipedia.org/wiki/Hawk-Eye#:~:text=Hawk-Eye is a computer,path as a moving image)** 
 
 >Hawk-Eye is a computer vision system used in numerous sports such as cricket, tennis, Gaelic football, badminton, hurling, rugby union, association football and volleyball, to visually track the trajectory of the ball and display a profile of its statistically most likely path as a moving image. The onscreen representation of the trajectory results is called Shot Spot. The Sony-owned Hawk-Eye system was developed in the United Kingdom by Paul Hawkins. The system was originally implemented in 2001 for television purposes in cricket.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hsjuzaz,t1_hsjuxon,1642114464.0,False
s36y35,"Desktop version of /u/cassidysvacay's link: <https://en.wikipedia.org/wiki/Hawk-Eye>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hsjuzj8,t1_hsjuxon,1642114467.0,False
s36y35,">VcSv

Based on a masters thesis at Cambridge by Jack Davis, founder of [Loci.AI](https://Loci.AI). He has now raised VC funding to eventually build a photorealistic neural rendering engine.",hsp3lrw,t1_hsjngrf,1642203701.0,True
s36y35,Motion sickness version.,hskm8ry,t3_s36y35,1642125805.0,False
s2qf5f,Those who don't understand infinite loop definition won't understand this infinite loop definition.,hsg84h1,t3_s2qf5f,1642050481.0,False
s2qf5f,you may not understand it but you will feel it,hshccuo,t1_hsg84h1,1642078916.0,False
s2qf5f,I think technically that's infinite recursion rather than an infinite loop,hsg84ys,t3_s2qf5f,1642050488.0,False
s2qf5f,Recursive infinite loop,hsgbtj1,t1_hsg84ys,1642052526.0,False
s2qf5f,"whats the difference, beginner here",huoglo1,t1_hsgbtj1,1643429278.0,False
s2qf5f,Semantics. Recursion can create an infinite loop.,huomoh6,t1_huoglo1,1643432470.0,False
s2qf5f,Not really. It looks like a go-to.,hshomq2,t1_hsg84ys,1642084846.0,False
s2qf5f,"Kind of a moot point, with the exception that recursive function calls will eventually cause a stack overflow.",hsgdoir,t1_hsg84ys,1642053636.0,False
s2qf5f,Believe that can be avoided with tail end recursion,hsge09f,t1_hsgdoir,1642053838.0,False
s2qf5f,"There's no difference between infinite recursion and infinite iteration, though many programming languages will expose a difference in terms of a stack overflow. There are also those that don't.",hsl4gci,t1_hsg84ys,1642134043.0,False
s2qf5f,Which book? I think there must be more cool examples like this...,hsgaybh,t3_s2qf5f,1642052033.0,False
s2qf5f,seems to be a Latex book .. \infty,hsgbp26,t1_hsgaybh,1642052454.0,False
s2qf5f,"This was pretty common in programming books in the 80s and 90s (when I last read physical programming books). I assume it's still pretty common today.

Google does something similar when your search for teens related to infinite loops.",hsgiyka,t1_hsgaybh,1642057110.0,False
s2qf5f,"> teens related to infinite loops

Uhhmm... .teens?",hsgpz6w,t1_hsgiyka,1642062367.0,False
s2qf5f,"""terms"" sorry",hsgqt5d,t1_hsgpz6w,1642063021.0,False
s2qf5f,"Now do ""deadlock"".",hsg87eo,t3_s2qf5f,1642050525.0,False
s2qf5f,you may need 2 other philosophers,hshchh7,t1_hsg87eo,1642078986.0,False
s2qf5f,"This is just a GOTO with a dead end; there's no motivation to go back to 'Inifinite loop' in the index text. The Batman version does a better job: [https://wiki.secretgeek.net/unbounded-recursion](https://wiki.secretgeek.net/unbounded-recursion).  
Edit - Fixed the link after a long weekend.",hsgxb7k,t3_s2qf5f,1642068344.0,False
s2qf5f,page not found,hshm1z2,t1_hsgxb7k,1642083705.0,False
s2qf5f,"Why was your weekend long, huh?",hv2zgxz,t1_hsgxb7k,1643683830.0,False
s2qf5f,Got stuck in some wierd iteration thing of course!,hv6pxnt,t1_hv2zgxz,1643750292.0,False
s2qf5f,This often happens with recursion also,hshf2uz,t3_s2qf5f,1642080360.0,False
s2qf5f,"I dont get it, May someone explain pls? Am a newbie in CC",hshw08a,t3_s2qf5f,1642087872.0,False
s2qf5f,"When you search for “infinite loop” it tells you to look on page 252. So you search for “infinite loop” on the page and it tells you to go to page 252, where you search for “infinite loop” on the page and it tells you to go to page 252…. Thus an infinite loop of searching.",hsjdug3,t1_hshw08a,1642107970.0,False
s2qf5f,I stared at this trying to get it for so long I got a migraine and had to stop.,hsh6aiv,t3_s2qf5f,1642075186.0,False
s2qf5f,"delimiter, 47",hshbx8b,t3_s2qf5f,1642078670.0,False
s2qf5f,may i ask what book is it ?,hshpi74,t3_s2qf5f,1642085218.0,False
s2qf5f,"Also, why goto is bad",hshvx56,t3_s2qf5f,1642087839.0,False
s2qf5f,Genius!,hsi0fez,t3_s2qf5f,1642089566.0,False
s2qf5f,Are they trying to make my brain crash?,hsifx7n,t3_s2qf5f,1642095330.0,False
s2qf5f,Lol!,hsj55wd,t3_s2qf5f,1642104687.0,False
s2qf5f,What book is this?,hskvztp,t3_s2qf5f,1642130082.0,False
s2qf5f,"this is an *infinite recursion* technically speaking, not an infinite loop",httwly2,t3_s2qf5f,1642906785.0,False
s3yrrt,"The Concorde solver uses the cutting-plane method, iteratively solving linear programming relaxations of the TSP. The interface shows the solver's progress at the end of each major iteration of cutting planes by coloring the edges according to their current LP values.

The Graphical User Interface implements in addition to the optimal Concorde solver the following edge generating algorithms:

* Delaunay Triangulation
* Minimum Spanning Tree
* Various Nearest Neighbor Set generators

The Concorde interface can read and write graphs from and to files in various formats. (Note: Input graphs must have greater than 10 nodes.) Users may add, delete, and move a graph's vertices interactively. The interface consists of two display panes. One is used to show the graph, the other to print textual information about the graph, the algorithm's progress and results.",ht3swrg,t3_s3yrrt,1642462780.0,False
s3yrrt,"Thanks, I'll need to dig into the cutting plane method",ht5am8a,t1_ht3swrg,1642487489.0,True
s3yrrt,I found this video which explains that for the TSP. It's very well explained : https://youtu.be/8yajCJKezZQ,hv1mse1,t1_ht5am8a,1643663336.0,True
s3yrrt,[https://youtu.be/tChnXG6ulyE](https://youtu.be/tChnXG6ulyE) this one too,hvcawbv,t1_hv1mse1,1643842299.0,True
s3yrrt,I think the introduction of [this paper](https://dl.acm.org/doi/pdf/10.1145/3071178.3071304) may be a good start,hsoh7zc,t3_s3yrrt,1642194642.0,True
s3gau6,Cocke and Minsky's construction of a universal Turing machine within a tag system can be found [here.](https://dl.acm.org/doi/10.1145/321203.321206) And  [here is](http://www.complex-systems.com/pdf/15-1-1.pdf) Matthew Cook's proof that a cyclic tag system can emulate Cocke and Minsky's tag system.,hskrj9n,t3_s3gau6,1642128098.0,False
s3241k,"A few things, 

The power supply will simply give your computer power to work. It does not dictate the clock freq or anything like that.

What exactly a computer does is very complicated, but a simple abstraction is a von neumann architecture. This is not at all whats happening behind the hood but thats what the hardwere wants you to see.

The tldr of von neumann architecture is that for every instruction (the ones the compiler breaks the high level program into) the cpu goes through five steps:

Fetch, where the cpu gets the instruction. Decode is where the cpu understand the cmmand. Execute is where the computation happen. Memory is where you read from the memory (ram) to get data you need for the instruction and write-back is where the results are stored in the registers or memory.

Your code does not go to the ram, but your cpu reads and write to the ram in order of executing instructions.

Writing hardwere (and checking it) is not an easy task at all. This is done by splitting the actual hardwere into many separate parts that should work indepentently and combining it all.

An hardwere engineer wont usually deal with transistors but would write in what looks like code (hdl) and use basic building blocks to create the whole piece.

Hoped it answered your questions",hsi09b6,t3_s3241k,1642089501.0,False
s3241k,"Does the clock frequency dictate the ""fetch"" component? 

Im just confused on how your computer knows to read the binary. Like when I type out my code and run it, how does the cpu know to read it. Is it always looking?",hsi0r86,t1_hsi09b6,1642089692.0,True
s3241k,"The cpu has whats called a program counter (or pc). This is a register that stores the address of the the next command it should fetch. One instruction is called branch - this instruction would change your program counter and therefore change the code you will run (for example if statement might translate into conditional branch).

Essentially (its a bit more complicated, but thats the jist of it) when you run a program the pc would point to the location of the first instruction the compiler made from your code. When the pc is there, all it do is just do the usual 5 steps until theres nothing to run from your program and it does some sort of branch to where it was before it ran your program.

Its pretty complicated so what ive said is not 100% accurate, but its close enough.

Also, under von neumann architecture we assume that the clock define all of the stages, which means all five stages should take the same time to run.",hsi3cv5,t1_hsi0r86,1642090687.0,False
s3241k,"> Also, who sits around and actually designs computer architecture, I feel like that has to be one of the most complicated jobs on the planet. You've got millions of transistors to manage controls? Or is it more abstract than that.

Computer scientists and engineers design computer architectures, if you're using the term *computer architecture* to mean *computer architecture* (aka instruction set architecture [ISA]). The design of actual computer hardware is done by computer and electronics engineers. These are the folks who figure out how a computer architecture is implemented to meet the performance, power, size, and cost goals. They'll make heavy use of abstraction and hierarchical design to manage the complexity in modern computers.",hslendn,t3_s3241k,1642139424.0,False
s33v2n,"As you said, the main Python interpreter is program called CPython.

Its source code is written in C - a compiled language.  
A compiled source code is turned into machine code.  
Machine code is executed (""interpreted"") by hardware.

In case of Jython - it's written in Java then compiled to byte code executed by JVM.  
Java Virtual Machine is also usually written in C and/or C++.  
Then again, source code get compiled, turns into machine code, which is executed by CPU.

In case of PyPy: written in RPython (special subset of Python).  
Then it's translated into a form of byte code, then to C and you know the rest.",hsiprv0,t3_s33v2n,1642098932.0,False
s33v2n,Yup! Thanks God Bless!,hsiua5d,t1_hsiprv0,1642100612.0,True
s33v2n,"That’s a really insightful question and I first want to say congratulations on finding that problem. Seeing inconsistencies like that’s really important for learning and a well written question (like this) is never a dumb question. 

The answer is not every language is interpreted. Some languages (like C) are compiled which means transformed into the raw machine code that can be run on the hardware. No interpreter needed once compiled. 

However this does pose the challenge of how the compilers were written. For example, the c compiler is written in c so how did it get compiled? That was done through boot strapping. First the most fundamental part of the compiler was written in assembly. This let you compile an extremely reduced C program. Then the next layer of the compiler was written in that reduced C and then compiled to allow a slightly less reduced C to be compiled. So on until the entire language could be compiled.",hsiqegg,t3_s33v2n,1642099166.0,False
s33v2n,"Out of all the platforms that I have posed this question and lost my brain cells thinking about this question over and over, your answer in layman terms is the best I could understand.

I understand the case now for interpreters written in languages that are compiled (Cpython written in C). Just to complete my understanding of this, what if the interpreter is written in the same language as to be interpreted eg. PyPy (interpreter written in Python for Python)? are such interpreters wired to in-built compilers?

Update: Just checked that the PyPy is written in RPython which is translated into C. So I guess the process for this is Your code in python => PyPy (RPython) => C files (which are compiled)

Thank you so much for your answer. God Bless! I can go to sleep now peacefully.",hsislyq,t1_hsiqegg,1642099984.0,True
s33v2n,">Your code in python => PyPy (RPython) => C files (which are compiled)

No, no, no. Your Python code is never turned into C code during interpretation.

PyPy interprets your regular Python, not RPython.  
It's the program PyPy - the interpreter itself - that was written in RPython, which got turned into C to be compiled into machine code.

Python language doesn't care what interpreter was written in.",hsj6v6n,t1_hsislyq,1642105303.0,False
s2ar5c,"All CPU sizes are turing complete and computationally universal (so long as they atleast contain conditional branch)

The bus size is mostly just about performance, even very small devices like the 8bit gameboy could access large memory banks by using bank shifting.

There are compilers which can convert any program to execute on the z80 (such as in the 8bit gameboy)",hsd9n83,t3_s2ar5c,1642006474.0,False
s2ar5c,"Wow, I didn't know that, Thanks for answering!",hsda1j7,t1_hsd9n83,1642006624.0,True
s2ar5c,"It's very important not to conflate bit width in one aspect with bit width in another. If an 8-bit CPU didn't have way to deal with data types larger than 8 bits then they wouldn't be able to count over 256. Instruction size is not the same as bus size is not the same as data type size is not the same as memory address size, yet all are expressed in bits.",hsdtlsb,t1_hsda1j7,1642013881.0,False
s2ar5c,"The CPU won't be able to operate on more than one byte at a time, but multiple bytes can be combined to represent larger values. Operating on those will just take multiple instructions.",hseoay2,t1_hsdtlsb,1642025356.0,False
s2ar5c,"And while it probably goes without saying, working those larger values comes with a **significant** performance penalty.

Even just adding two 16-bit numbers turns into:

* Add LSB bytes.
* Test for overflow/carry.
* Add carry to one of the MSB bytes.
* Add MSB bytes.

That second step is multiple instructions so you have 1 8-bit ADD instruction turning into an easy 5+ for a single 16-bit ADD. Can always make up for it via clock speed but back in the 8-bit computing days we didn't have Arduino 16MHz speeds and long calculations like that ended up just being avoided for performance reasons.",hsflc22,t1_hseoay2,1642039768.0,False
s2ar5c,You never heard of add-with-carry instructions? They've been standard for a long time...,hsfyyll,t1_hsflc22,1642045867.0,False
s2ar5c,Don't you still have to check the carry flag and add it to the next byte before adding the next significant bytes?,hshir39,t1_hsfyyll,1642082171.0,False
s2ar5c,"It's built into the instruction. For example, on the 8080 the ADD instruction will ignore the carry bit but set it if the addition generates a carry. ""ADD D"" basically does A += D. The ADC instruction will add the carry bit. ""ADC D"" does A += D + carry.",hsi2a61,t1_hshir39,1642090277.0,False
s2ar5c,i salute your knowledge,hsfn89p,t1_hsd9n83,1642040582.0,False
s2ar5c,"If you’re interested in poking around, and don’t want to learn any specific assembler, the cc65 compiler can compile C code into 6502 compatible executables for many vintage 8-bit systems, such as the Commodore 64. You can write little examples like what you’re talking about and see what happens. Speaking from experience, there’s a huge hit when you go from 8-bit integer math to 4 byte long int. Not only is it slower, but it adds a lot of extra code, which takes up program size and ram when the program is loaded. 

For example, I created a modern-like command line interface for the Commodore 64 called ChiCLI. 

I added a simple math function, so you could do some quick calculations on the command line. Nothing fancy at all, but like 11 lines of code turns into 472 extra bytes of machine code! Maybe that doesn’t seem like a lot, but in this case it’s 1% of my approx max 50K program! All the code is about 4 klocs, so 11 lines is about 0.27% of the code base. So 0.27% of the code turns into 1% of the entire program! 

The whole thing is here: 
https://github.com/chironb/ChiCLI

Here’s the code I’m referring to: 

// ********************************************************************************
		// = COMMAND / MATHS COMMAND 0--> This takes up 472 bytes!!!
		// ********************************************************************************
		} else if ( user_input_command_string[0] == '=' ) {

			long int answer, first_number, last_number;
			sscanf(user_input_arg1_string, ""%li"", &first_number);
			sscanf(user_input_arg3_string, ""%li"", &last_number);
			switch (user_input_arg2_string[0]) {
				case '+' : answer = first_number + last_number; break;
				case '-' : answer = first_number - last_number; break;
				case '*' : answer = first_number * last_number; break;
				case '/' : answer = first_number / last_number; break;
			  default  : puts(""?""); break;
			};//end-switch
			printf(""  = %li\n"", answer);",hsdxgfd,t3_s2ar5c,1642015333.0,False
s2ar5c,"8 bit CPUs like the intel 8080 were used to create arcade machines without even having bit shifting instructions independent of the accumulator register. Such machines had shift registers added as additional pieces of hardware made of a few ICs, http://searle.x10host.com/spaceInvaders/BlockDiagram.jpg",hse4p9h,t3_s2ar5c,1642018105.0,False
s2ar5c,"I don't know about tetris and pong in particular but some comments saying that it would be Turing complete are not telling the full story.

For Turing completeness it would need to be able to access memory in ways that are not usually done. For instance if its memory access is only through saying ""I want the memory at so-and-so address"", then it is not Turing complete. It would be severely crippled and even old 8-bit computers effectively use a 16-bit address register to get past this and have a little more power (while still not being Turing complete).

The way it would be Turing complete is if it could also say ""Shift the memory by one address left/right"". This way by running this command repeatedly it could access memory addresses that it otherwise wouldn't have been able to. (If you've seen a depiction of a true Turing machine this is how the machine accesses the tape.)",hsfmpyw,t3_s2ar5c,1642040363.0,False
s2ar5c,Thanks for this explanation. I was hoping to read someone explain what is meant by 8-bit computers being Turing complete.,hsfsi7j,t1_hsfmpyw,1642042892.0,False
s2ar5c,"A normal 8-bit CPU is turing complete with instructions for conditional branching as mentioned in one of the posts above. 

By turing complete we mean that it can run whatever the f\*\*\* you want really. Bitcoin mining, witcher 3, reddit etc. Now, how fast these things will run is a whole other question though.",hse5iph,t3_s2ar5c,1642018417.0,False
s2ar5c,"I like to think of this like being a carpenter, of a craft worker. A 64 bit cpu can handle all the tools and a whole range of sizes quite easily. However you can, if you needed to, do the same work and effort with a few simple tools. 

It would take a lot longer and it would be way more complicated to use the simple tools, but it can be done.

An 8bit (and even a 4 bit) cpu can perform the same “work” as a 64 bit cpu, but its implementation and execution would be much more complicated",hsfcu51,t3_s2ar5c,1642036000.0,False
s2ar5c,"[https://www.youtube.com/watch?v=ifXr7LORNCo](https://www.youtube.com/watch?v=ifXr7LORNCo)

&#x200B;

10 best 8bit games",hsdc5nq,t3_s2ar5c,1642007415.0,False
s1yfl6,This guy did fab in a home lab and has a great writeup on the physical process: http://sam.zeloof.xyz/second-ic/,hsbgnwm,t3_s1yfl6,1641968899.0,False
s1yfl6,"Thank you, that was a fascinating read. Some people are too smart",hsbi5tv,t1_hsbgnwm,1641969890.0,False
s1yfl6,How smart this is should be evaluated long term. It is highly probable you would be fast tracking yourself to stage iv cancer in such a home lab.,hse5qpb,t1_hsbi5tv,1642018500.0,False
s1yfl6,Utterly fascinating.,hsbxtb3,t1_hsbgnwm,1641981911.0,True
s1yfl6,"Instead of working, i too am being fascinated by his process. Great link /u/timeforscience",hscvfii,t1_hsbxtb3,1642000996.0,False
s1yfl6,This is incredible. Thank you for posting this,hsd07yt,t1_hsbgnwm,1642002884.0,False
s1yfl6,I literally just wanted to post him the moment I read the title.,hsda8xj,t1_hsbgnwm,1642006702.0,False
s1yfl6,"Edit: sorry for the formatting I am writing on phone. By hardware I guess you mean chips. Well like someone said the design and production/fabrication are two different things.
ARM designs the chips but does not produce the physical chips. You can learn design online, and with degrees. We use computers to design as well. We ""program"" our logical requirements like need 5 ALUs here, have a bus there following the infinity fabric protocol etc in something like verilog which them gives us a schematic with respect to logic gates.
The final implementation and production of the chips are actually in the domains of semiconductor electronics, nano tech, chemical engg and material science. The main technique used is known as photolithography. There is only ONE company in the world that makes the Machines to do photolithography and all the fabs buy from it.
The general basic process follows: (this is outdated now)


- You get your raw silicon

- process them into ingots (cylinders)

- cut them into thin round wafers

- Polish them

- You then use the output of the verilog design to make a mask.
A mask a generally a flat face of something that can block light, and you then cut the mask into the shape of a circuit (2D) to allow some spaces in between where the wires/transistors would be.


- you place the mask on top of the silicon wafers

- You use that mask and use high powered light to melt away the some of the silicon that is under the gaps of the mask.

- in the newly created spaces you add some impurities like gallium or aluminium through electron vapour diffusion(not sure if that was the correct name I am forgetting some stuff) or some other techniques to dope the silicon so that it is more or less conductive.


- what this does is creates transistors PNP or NPN FETs(field effect transistors) at the wafer level.

- then it is layered with some other layers depending on the fab.

- there can be layers of masks and multi layer designs to improve efficiency. 

- a single silicon wafer will have many cpus/chips.


- there will always be chips that will have defects in them because we are dealing with such a small scale. Those chips either are either discarded or more commonly sold as lower tier products with the bad/defective circuits burned off.


- The use of light is also the reason why crossing the 1nm barrier would be so difficult apart from electron tunneling.

I hope this somewhat explains it.
Today we have even more packing density with FINfet and 3D stacking.
We would probably see chips with inbuilt cooling pipes at the transistor level soon.",hsbhqnf,t3_s1yfl6,1641969611.0,False
s1yfl6,">photolithography.

I did not even remember such term existing, I absolutely thank you for this!",hsbwpn4,t1_hsbhqnf,1641980988.0,True
s1yfl6,Can you recommend any good resources to learn design online?,hsbxzvt,t1_hsbhqnf,1641982062.0,False
s1yfl6,"That will depend how good you are already with digital electronics and digital logic.

If you are a beginner then you should probably do a course on digital logic first and then move onto computer architecture and organization, microprocessor internals (x86 and arm).
There are lots of completely free college level playlists from the likes of MIT opencourseware. 
For digital logic you WILL need to do a lot of problems. Because problems are the way that you actually engage with something.
Once you are done and have a good grasp on the basics there are lots of nice udemy courses that goes for very low prices during sales, I am talking like $10 but are full of content. Take a single or at Max two courses and not more and focus on digesting them well.
The courses you should be looking out for generally are courses that have VLSI, VHDL, verilog in their teaching content.
For a first course any top rated course with many hours of video would do.
By the time you are done with that you'll know what you want to do next. For getting industry jobs, you can maybe start with FPGA. Because it is something you can actually own and experiment on and upload the code to GitHub like any other software project. And the concepts that you are going to need to program an FPGA overlaps a bit with verilog. You can even use C++.",hsbyzk8,t1_hsbxzvt,1641982898.0,False
s1yfl6,"Huge +1, and you can actually run all that stuff in simulators, the actually fabbing part is more of an industrial process, very hard to get into it as a hobby.

If you want to do anything there as a hobby, you can try to build small “computers” with transistor-transistor logic, check Ben Eater’s channel on youtube.",hsc1icu,t1_hsbyzk8,1641984927.0,False
s1yfl6,"To give you some idea of the scale of things, my computer systems engineering degree included a semester-long course in first year that began with you knowing nothing about digital logic and ended with small groups working together to do a gate-level design of an 8-bit CPU in VHDL.  We were given an instruction set specification and a minimum amount of memory it had to support (64k IIRC) but otherwise everything about it was up to us.  This was circa-2000 and the CPU we were designing was roughly Z80 sort of technology.

We thought we were super cool with our pipelined execution and paged memory layout giving us 16MB addressable memory (hardware was not memory mapped into data memory in this design).

We were allowed to assume 1ns per gate; our design was able to run at 16MHz.  Good times.

Anyway, point is that it's possible to get your head around enough knowledge to do a basic CPU design in a few months of two lectures a week, some reading and some experimenting.",hsclhjf,t1_hsbyzk8,1641996775.0,False
s1yfl6,"Woah, cooling pipes at the transistor level. It sounds like they will run cooler. When do you think it will be used in common products like cpu and gpu chips?",hsc4vy3,t1_hsbhqnf,1641987469.0,False
s1yfl6,"There’s 2 main processes:

Photolithography is used to create semiconductors and chips. This is an optical and chemical process which will imprint certain doping patterns onto semiconducting material along with copper interconnects. This is essentially a tiny circuit, you can make transistors, diodes, resistors and some very limited values of capacitors and inductors in this process. Conductive pins which reach outside the chip casing provide access to certain points in this circuit. Production falls under physics, chemistry and solid state physics, design falls under electrical engineering, these usually aren’t done by the same company.

Printed circuit board (PCB) fabrication is used to create finished boards with all their components including chips, resistors, transformers, connectors, capacitors, inductors etc etc. Similar to semiconductors this also uses photolithography to create the copper traces on the board, along with a host of various CNC machines for drilling holes, cutting edges, placing components. There’s various high volume ways of soldering the components to their pads such as wave soldering or reflow ovens. Production falls under mechanical engineering and a little bit of chemistry, design falls under electrical engineering, again usually not done by the same company but larger companies will spin their own PCBs because a PCB fab house is much cheaper to run than a semiconductor fab.

Other kinds of components all have their own specialized manufacturing method, for example a USB connector is a cleverly folder piece of sheet aluminum. A capacitor is two conductive plates separated with a layer of ceramic insulator. A transformer is enameled copper wire wound around a ferrite core. A crystal oscillator is a finely cut piece of quartz with two metal plates on each side. Etc etc.

Once you have assembled boards the rest isn’t too complicated, they’ll be screwed into some kind of plastic or metal enclosure, various forms of cables will provide connections to other boards, external power sources and I/O devices.",hsbwnbp,t3_s1yfl6,1641980935.0,False
s1yfl6,"This humbles me as a software engineer haha, and I thought my job was complicated!",hsbxylg,t1_hsbwnbp,1641982031.0,True
s1yfl6,You should probably ask r/ComputerEngineering or a similar engineering subreddit.,hsbc98g,t3_s1yfl6,1641966161.0,False
s1yfl6,"I'm a software engineer with some cyber security /penetration testing background, I haven't touched anything CPU related for almost half a decade never had to all this seems absolutely amazing to me.",hsbxwsp,t1_hsbc98g,1641981989.0,True
s1yfl6,"I’m a materials scientist, though, I work with polymers more than silicon. A lot of people here already mentioned aspects of the process, but I know many people who go into Si chip manufacturing. Maybe you could ask r/materials if you want more answers regarding lithography and vapor deposition. It’s pretty cool.",hsbz1e6,t3_s1yfl6,1641982940.0,False
s1yfl6,"I knew that computer architecture / engineer would be a labyrinth but I never expected that just the creation and production of a simple CPU requires literally a dozen ++ of different sciences to be done, can't even imagine how hard it will be to comprehend how a high level language interacts with the metal and controls a software.

If you're doing this you're absolutely amazing.",hsbzb9j,t1_hsbz1e6,1641983162.0,True
s1yfl6,"Haha I actually study computational materials science so I use code to study polymers, but there is definitely a lot overlap between sciences these days.

As an aside, a cpu is incredibly complex even though it seems like such a simple thing these days! Pretty incredible.",hsc2iac,t1_hsbzb9j,1641985697.0,False
s1yfl6,"Design and production are very distinct. Production is a mostly automated process that involves creating silicon wafers and burning circuit patterns into ‘em. The design process involves creating circuit diagrams and software simulations, AFAIK. They also use software to optimize the layout of the circuit, so that minimal space is used on the chip.

EDIT: I’m surprised you can’t find any information on these processes. There’re tons of documentaries and series detailing the creation of computer parts. Maybe try going to your nearest university and finding resources there if your internet searches come up dry? Most of what I know of the process comes from my computer organization class in college so it’s not a bad place for that purpose. :P",hsbf38j,t3_s1yfl6,1641967881.0,False
s1yfl6,"I refer to information about the physical creation then production, which I already know they are separate processes, apparently it makes sense I can't find them cause there's only one company in the world that creates the tools for it.",hsbwry0,t1_hsbf38j,1641981040.0,True
s1yfl6,Small correction. Its only one company that makes the cutting edge nanometer photolithography machine. For PCB (the likes of motherboards) there are other options. You can make PCBs on your own in a homelab!,hsbzbtn,t1_hsbwry0,1641983175.0,False
s1yfl6,I'm looking specifically for the way a nanometer-photoligraphy machine interacts with the rest of the computer architecture and how it is done when it comes to its physical substance of the computer its self.,hsbzj9o,t1_hsbzbtn,1641983346.0,True
s1yfl6,"Hmm, the main working concepts are not very hard to grasp. Its the details and the fact that we are talking nanometer scale that makes it hard from an engineering standpoint. And that will take a whole career to understand tha complete nuance of.
But honestly from the perspective of CS it really doesnt matter how the logic is implemented physically. CS starts at the logic level. If tomorrow we change from semiconductors to say carbon nanotubes provided we have the same TTL layer (transistor transistor logic) we can run the same programs on it even though the underlying implementation is completely different. Thats the beauty of abstraction and the reason you dont need to be an electronics expert to design chips. Because chip design and verilog generally abstracts out the transistor layer.
If you know your digital logic. All you need to know to understand the basics is how photolithography can be used to make things like transistors, diodes, resistors. Once you got your logic gates everything else comes by copy pasting.
So what you REALLY want to understand is at the semiconductor level how we make transistors. And for that if you are already in uni you could probably take a digital circuit design class or semiconductor electronics class. The info we get at the undergraduate level is enough to see how this can be done on a smaller and smaller scale. Now how to make it smaller?? Thats a whole different ball park and comes under the domain of nano tech, material science and physics.",hsc0a7e,t1_hsbzj9o,1641983950.0,False
s1yfl6,"I’d research into the “abstraction” mentioned here: it’s called the von-neumann architecture, and basically everything in the world runs on it.",hsc1u65,t1_hsc0a7e,1641985183.0,False
s1yfl6,What company ?,hsc2efg,t1_hsbwry0,1641985614.0,False
s1yfl6,"Silicon, silicon, silicon",hscu0n6,t3_s1yfl6,1642000423.0,False
s1yfl6,"This person reverse engineers (mostly vintage) hardware as a hobby and their blog is one of the most insightful on the matter, including the tiny clever design tricks: http://www.righto.com/

On the less serious hand, this article describes the process of building a cpu with colorful illustration: https://blog.robertelder.org/how-to-make-a-cpu/",hsdg95q,t3_s1yfl6,1642008938.0,False
s2gdc0,"""digital"" comes from ""digit"" and means it can be expressed by a digit or it is countable, you can express it as a digit, you can point in it. It's the counterpart of ""analog"" which is not countable but continuous, you can't point on it, it is not an exact digit.
Real things can be digital, like the apple on a tree, or they can be analogous, like air or like water.",hsedg3a,t3_s2gdc0,1642021335.0,False
s2gdc0,"To be digital is to be discrete, where any digital information has only a limited number of discrete values. For electronics, voltage is continuous but we have selected certain voltages to represent digital information, and that digital information can only be one of two voltages, a “zero” or a “one.” It is discrete, even though voltage as a concept of physics is continuous",hsei01i,t3_s2gdc0,1642022990.0,False
s2gdc0,"Great question. I could be wrong but I believe digital means representing numbers with digits. So instead of measuring a voltage by seeing how much its current deflects a magnet, for example, you could measure it by convering it to binary digits with a ADC. 

Now how a ADC represents measurable quantities with digits is a question I don't feel qualified enough to answer.

For more information on how analog signals are measured digitally, I would look into Series-Approximation Registers (SARs).",hsedp6c,t3_s2gdc0,1642021427.0,False
s2gdc0,"Digital comes from a latin ( I think) word  meaning finger, as in technology at your fingertip.",hsjwy11,t3_s2gdc0,1642115223.0,False
s2nqom,"From my limited knowledge, programming languages themselves tend to be English. Now, there's nothing stopping you from doing basic manipulation to ""translate"" it into another (spoken) language. Plus functions, variables, etc. aren't limited to English",hsfmzni,t3_s2nqom,1642040478.0,False
s2nqom,"An AI which assists a non-native person during email/essay writing process, or just a good freakin' translator. You can do these things if you love both cs and foreign languages! :D

There are also so many things could be done. For example, extended search algorithm which would translate the given query into other languages and give much more results in other languages (which could be translated to preferred language after) rather than just in one you used. It would make searching things so much more productive and effortless.

Sorry for such a messy comment, I'm not really good at editing. Which makes me think of an algorithm which corrects texts in several languages using their traditional punctuation and spelling rules (i think it exists already, but still)

So yeah, there are still lots of things to be done, lots of possible projects and opportunities! (Even though most of the ones stated are already implemented in our everyday lives, but there is still a lot of fish to catch)",hsfs088,t3_s2nqom,1642042671.0,False
s2nqom,"Theory-wise I think there's an overlap in that if you've developed skills to learn languages then those same skills will help you learn computer languages.


Job-wise there are a lot of technical writing jobs in CS where they need people who can write in multiple languages.",hsfzwpo,t3_s2nqom,1642046318.0,False
s2nqom,"I don't know a lot about it but I believe ""context-free grammar"" is an overlapping idea in linguistics as well as programming language/compiler design and maybe computer science in general",hsg4f0l,t3_s2nqom,1642048553.0,False
s2nqom,"On the contrary, look up esoteric programming languages. It’s purely experimental though.",ht127hr,t3_s2nqom,1642420837.0,False
s2nqom,"Yes, the theory behind programming languages that give us syntax, semantics, and “grammar” is a base concept in this space. They are majorly based off of a concept called context free grammars(CFG). CFGs can be though of as the building blocks of of how to make sentences, strings in CS. Research into what these are and how they work and you’ll see obvious parallels as to how base theories in CS and mathematics are present in formal language.",hu4eirp,t3_s2nqom,1643091597.0,False
s1gfy8,This is awesome and I love the art. I'm definitely going to print these and post them in my lab.,hsbnrpe,t3_s1gfy8,1641973905.0,False
s1gfy8,Thank you! I'll take that. 😄 So your lab is related to CS/mathematics?,hsbp6xv,t1_hsbnrpe,1641974991.0,True
s1gfy8,"Yup! :)

We work in Infosecurity and cybersecurity. I was very surprised by how much ML is used in our field.",hsbryat,t1_hsbp6xv,1641977149.0,False
s1gfy8,Interesting! Is the ML being used for some form of anomaly detection?,hsbt2db,t1_hsbryat,1641978025.0,True
s1gfy8,"Yes! However for our specific subfield and group, no (but there is a different lab here that is doing some form of anomaly detection). I can't really say what exactly we are working on since it might be easy to find me irl lol. 

But to give you an idea, our features usually consist of assembly code, which is then vectorized, and then we apply some traditional ML algorithm - typically clustering (k means is popular since it is simple :P ). But we currently are exploring different and better techniques to improve for our next project",hsbyp2d,t1_hsbt2db,1641982650.0,False
s1gfy8,"Yes, of course.

Usually, with more complex models the trade-off is between performance and explainability. Sounds exciting! It is always nice to go from more classic approaches to something more modern and see how it affects the project and the performance. Also, the nice thing here is that you will now already have a good baseline to compare the ""better techniques"" for your next project.",hsbz5b6,t1_hsbyp2d,1641983027.0,True
s1gfy8,Amazing,hsbsnvs,t3_s1gfy8,1641977704.0,False
s1gfy8,Thank you!,hsglxf6,t1_hsbsnvs,1642059246.0,True
s1gfy8,EDIT: Thank you to u/antiogu for pointing out the error. The y-intercept should be 2 in my sketch on the last panel.,hsbpx69,t3_s1gfy8,1641975560.0,True
s1h1gt,"LinkedIn, TL;DR, console, Linus tech tips, Some Ordinary Gamers, simpli learn, fire ship, network chuck.",hs8j8oq,t3_s1h1gt,1641924518.0,False
s1h1gt,https://youtube.com/c/CodingTech,hs9ojp2,t3_s1h1gt,1641939666.0,False
s14xir,"* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **The Art of Computer Programming** (TAOCP) by _Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to Automata Theory, Languages, and Computation** by _Hopcroft, Ullman_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Discrete Mathematics** by _Ross, Wright_
* **Introduction to Graph Theory** by _Wilson_
* **Software Engineering** by _Sommerville_
* **Design Patterns: Elements of Reusable Object-Oriented Software**
* **Design Patterns Explained: A New Perspective on Object Oriented Design**
* **Fundamentals of Database Systems** by _Elmasri, Navathe_
* **Numerical analysis** by _Kincaid, Cheney_
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **Artificial Intelligence: A Modern Approach** (AIMA) by _Russell, Norvig_
* **Compilers: Principles, Techniques, and Tools** (Dragon Book) by _Aho, Lam, Sethi, Ullman_
* **The C++ Programming Language** by _Stroustrup_
* [**Beej's Guide to Network Programming**](http://www.beej.us/guide/bgnet/)
* [**Modern C**](https://modernc.gforge.inria.fr/) by _Gustedt_
* [**x86-64 Assembly Language Programming with Ubuntu**](http://www.egr.unlv.edu/~ed/assembly64.pdf) by _Jorgensen_
* **Effective Modern C++** by _Meyers_",hs6fo0x,t3_s14xir,1641884352.0,False
s14xir,Oh wow that’s a lot of good books only a few of which I recognise. Did you read them all?,hs6ohn8,t1_hs6fo0x,1641890918.0,False
s14xir,"Not from cover to cover, of course, but yes, I've read at least fragments of each.

Some of them I'd read for work, some for university, some for sheer curiosity; I liked most of them much enough to even buy my own copy (although some are too expensive and TAOCP is work in progress).",hs8inwr,t1_hs6ohn8,1641924304.0,False
s14xir,"> TAOCP is work in progress
 
Is it though? Game of Thrones could be finished and given a sequel series with several spinoffs and that last book still won't be here.",hs8zuw3,t1_hs8inwr,1641930591.0,False
s14xir,"The latest errata to TAOCP was from 2021-12-24, ergo it's being worked on.

And even if it takes years then what? You suggesting we won't see it at all?  
JWST was supposed to launch in 2006, it got delayed (by lot), but we still got it.",hs9mht8,t1_hs8zuw3,1641938883.0,False
s14xir,"Errata hardly counts as continuing a work in progress, it just means a reader sent in a correction. The difference between JWST and TAOCP is that there was no time limit on JWST - there is on TAOCP. When Knuth (who is 84 years old) is gone, that's it.
 
And that's even taking 4b as the conclusion, when the current plan has 4b, 4c, 4d, 5, and perhaps 6&7 also.",hs9njbk,t1_hs9mht8,1641939284.0,False
s14xir,Is there really such time limit? Robert Jordan died in 2007.  The Wheel of Time still got finished in 2013.,hs9pab9,t1_hs9njbk,1641939949.0,False
s14xir,"Fiction is a different beast to non-fiction. Depending on the licence Knuth has with his publisher there may well be something called TAOCP that gets published, but if it's someone else writing it you might as well just grab any other book on the topics and slap that label on it.",hs9q6p7,t1_hs9pab9,1641940296.0,False
s14xir,Man ... you the MVP. You even wrote your own book and shit AND you're a PhD. Damn.,hs766kj,t1_hs6fo0x,1641904272.0,False
s14xir,"Haha, I've just noticed how similar _Jorgensen_ and _Jorengarenar_ look like.",hs8itdq,t1_hs766kj,1641924361.0,False
s14xir,"Wait, so you're not him?",hs9akcg,t1_hs8itdq,1641934518.0,False
s14xir,"No, I'm not. Unfortunately, I'm far from even dreaming about PhD",hs9izu3,t1_hs9akcg,1641937576.0,False
s14xir,"Ah, well. Awesome comment/post anyways.",hs9j9mw,t1_hs9izu3,1641937677.0,False
s14xir,"Hi, 

Is it possible for you to also suggest the reading order (of at least a few books) as well?",hs7likp,t1_hs6fo0x,1641911856.0,False
s14xir,"Surprisingly hard question, you did ask.  
For scientific literature there rarely is order of reading. You read chapter from one, few definitions from another, fill the blanks from third.  
And I've also already forgotten how it is to be new to all of that, so my judgement ~~may~~ is screwed by already knowing things a beginner normally wouldn't have a clue about. But I'll try.

For programming, I could suggest **SCIP**... alongside **K&R**? The first teaches more about programming in general and the second is about C - a base for most languages we use today.  
Then I'd move to C++ and **Clean Code**.

**Concrete Mathematics** (sometimes also called a ""volume 0 of TAOCP"") is the entrance to math here. Then **Discrete Mathematics**.  
Plus you need something for algebra and calculus (unfortunately, the ones I know don't have an English release).

The two books on automata and computation (by *Hopcroft & Ulmman* and by *Sipser*) I've read simultaneously.",hs8zkdb,t1_hs7likp,1641930484.0,False
s14xir,Thank you!,hscc3jf,t1_hs8zkdb,1641992072.0,False
s14xir,"Most of them are good books but I don't think Sipser's Theory of Computation is a good book, particularly for beginners. The explanations are somewhat lacking. There are better books on automata and computation than that one.

Edit: The Algorithm Design Handbook by Skiena is an excellent introductory algorithm book with good explanations and real-world examples.",hs8or77,t1_hs6fo0x,1641926515.0,False
s14xir,"> Design Patterns: Elements of Reusable Object-Oriented Software

Also occasionally referred to as ""Gang of Four""

Also would like to recommend **Types and Programming Languages** by Benjamin Pierce",hsanngo,t1_hs6fo0x,1641954355.0,False
s14xir,"Technically, it is not the book itself, but its authors who are referred to by that name",hsbfcwh,t1_hsanngo,1641968054.0,False
s14xir,"I learned the most about computation from reading Leslie Lamport. Something about his style is simpler than a lot of CS authors. I love this paper:

[Computation and State Machines](https://lamport.azurewebsites.net/pubs/state-machine.pdf) 

It basically has everything in one place, though it is a little dense if you aren’t on practice with a lot of discrete math. The first two chapters in [Specifying Systems](https://lamport.azurewebsites.net/tla/book-02-08-08.pdf) give the best introduction to the subject. The book is great too, again focusing on what computation is at its core vs. talking about specific technologies.

Another book that has been eye opening for me is [Concrete Semantics](http://concrete-semantics.org). It’s about programming language implementation with a proof assistant, but it also focuses on what exactly a programming language is and how its semantics is not a fuzzy, informal thing, but a very concrete definition that we can write down and refer to. Programming languages are the foundation of CS. You can’t create or analyze an algorithm without a language for expressing it.

This last one I’m torn with recommending, because it doesn’t have the same readable style as the previous authors, but it’s also pretty much the best one-stop shop for the most modern view of CS: [Formal Reasoning About Programs](http://adam.chlipala.net/frap/frap_book.pdf). I still have to do double and triple takes while reading this book, but it covers so many important topics in one place that it’s invaluable. What I do is, when I don’t understand a section, I at least use it to search for other papers / books on the topic before coming back to it. Which might be the intention of the book anyway. But I mean, the definition of computer science should literally be agreed to be “formal reasoning about programs.” So this book contains to many relevant topics in one place to avoid.",hs731a3,t3_s14xir,1641902261.0,False
s14xir,"Wasn’t expecting to see “Concrete Semantics” get a mention. For formal methods, I’d also add “Handbook of Practical Logic and Automated Reasoning“ by John Harrison.

Boolos and Jeffrey’s “ Computability and Logic” was written by philosophers, for philosophy students, but it’s rigorous and expansive enough that CS students should be able to get something out of it. Geoffrey Hunter's “Metalogic” is also excellent, and much more accessible.",hs7wvgx,t1_hs731a3,1641916343.0,False
s14xir,"Re Concrete Semantics - that book just changed my view on programming language theory. I was never particularly interested in it, but got there because of my interest in verification - at some point, you have to verify real code, which requires a definition of its semantics.

And sure there are other books on PL semantics, but the focus on using a proof assistant is also a big plus in my book.",hsaph3p,t1_hs7wvgx,1641955126.0,False
s14xir,sorry got to ask what is the role name of someone who does that stuff?,hs9acjh,t1_hs731a3,1641934440.0,False
s14xir,"Well, you can use this stuff in any role, because these books just teach you how to think about programs. Like I’m a “regular” software engineer, but I use TLA+ (Leslie Lamport’s logic and toolkit) to analyze models of things that I’m building all the time. Amazon and Elasticsearch also have been known to use TLA+ too. And, I use the concepts in all of these books when reading code every day.

Anyone involved in software verification uses things like this on an even deeper level. Like anyone that works on airplane software, or even some people who make [formally verified operating systems](https://sel4.systems). These roles are less common though.",hs9wug3,t1_hs9acjh,1641942911.0,False
s14xir,"Thanks. I wanted to ask one more thing, are they related to compilers? The stuff you mentioned",hs9ycvv,t1_hs9wug3,1641943529.0,False
s14xir,"Yea, the book Concrete Semantics is mostly about implementing a compiler (for a very simple language and a very simple target machine). The way this particular book goes about that is unique, in that it goes over how to prove that the compiler is correct in a proof assistant. This is something that isn't done in practice almost at all, but is very interesting in terms of understanding what a compiler is actually doing.

Coming up with a correctness statement is the best way to show that you really understand something.",hsawj9y,t1_hs9ycvv,1641958187.0,False
s14xir,"I’m a huge fan of [Code: The Hidden Language of Computer Hardware and Software](https://en.m.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software). It’s a little older at this point, but the foundation it lays is amazing. It builds from simple hardware relays, to a CPU, all the way up to an OS, and everything in between. Truly a hidden gem.",hs7372r,t3_s14xir,1641902371.0,False
s14xir,"Desktop version of /u/HiImLary's link: <https://en.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hs73854,t1_hs7372r,1641902390.0,False
s14xir,"The Little Schemer

The Elements of Computing Systems",hs6ogcx,t3_s14xir,1641890889.0,False
s14xir,"I recommend ""Models of Computation: Exploring the Power of Computing"" by John E. Savage. I much prefer it over the Sipser.

For a beginner, maybe the CLRS algorithms text or something that makes you learn C/assembler really well.",hs7kkaf,t3_s14xir,1641911450.0,False
s14xir,Is it better than Sipser and how? I had Sipser bookmarked as an intro must read for computational complexity,hsaqoz7,t1_hs7kkaf,1641955643.0,False
s14xir,"Savage's text is just more comprehensive. It covers circuit complexity, satisfiability, and random-access machine models in great depth in addition to all the things in Sipser.

In terms of having something elegant and self-contained for an intro course - or if you want to work through the whole thing - then Sipser is more suitable. It's better as a primer.

You'll see what I mean if you skim the pdf's.",hsatrad,t1_hsaqoz7,1641956968.0,False
s14xir,"I liked the Andrew Tanenbaum's books on OS and computer architecture. 
Digital logic by Morris mano
Computer architecture by Morris mano
Algorithms by Robert Sedgwick",hs6a614,t3_s14xir,1641880885.0,False
s14xir,"Godel, Escher, Bach by Douglas Hofstadter. If you're keen on a different perspective.",hs6k2w6,t3_s14xir,1641887494.0,False
s14xir,"Mathematics and Computation by Avi Wigderson, Algorithms by Jeff Erickson",hs7ihvg,t3_s14xir,1641910551.0,False
s14xir,"Introduction to Linear Algebra by Gilbert Strang: [https://math.mit.edu/\~gs/linearalgebra/](https://math.mit.edu/~gs/linearalgebra/)

Artificial Intelligence: A Modern Approach by Russell and Norvig: [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)",hs89frn,t3_s14xir,1641920974.0,False
s14xir,"I recently got Mastering Regular Expressions by Jeffrey Freidl, seems pretty good in so far. It’s not really tutorialey, I’d say it’s more concept. Though it shows examples in a few different languages",hs8kwas,t3_s14xir,1641925121.0,False
s14xir,Algorithms to live by,hsab8zv,t3_s14xir,1641948983.0,False
s14xir,"This is one of the books that hyped me up when I got my first CS job.  

*Algorithms To Live By” was neat and made me think “woah, this stuff really is cool”

*Thinking, Fast and Slow* is another one. It’s subject is actually psychology, and has nothing to do with CS, but it was recommended by a CS professor and I’ve seen it come up in the CS subs a few times. I like that it aims to compare the two systems of thought and explains the roles each one play in our day to day decision making and problem solving. The things I learned from that book really stuck with me and my career has directly benefited from reading it.",hsblwzh,t1_hsab8zv,1641972543.0,False
s14xir,"Designing data intensive applications by Martin kleppman 

Systems performance by Brendan Gregg 

Fundamentals of software architecture an engineering approach by mark Richards and Neal ford 

Design patterns: elements of reusable object oriented software by gamma,helm, Johnson, vlissides

The art of immutable architecture by Michael Perry is also a new one I’m enjoying",hsd6q3n,t3_s14xir,1642005376.0,False
s14xir,"A little late to this party, but here's a list I personally loved. My background is a bit varied - PLT, Numerical Analysis/Scientific Computing, and random interests I picked up throughout undergrad and beyond.

**Algorithms**

Building intuitions for how to compute efficiently.

The book that everyone else immediately recommends here is CLRS' ""Introduction to Algorithms"". I break from this tradition a bit in that, this was the one text that scarred me the most, mainly because I was wholly unprepared for it when I first went in.

First Course: these texts are geared for a first exposure to the construction + building up intuitions for algorithms.

* Grokking Algorithms - Aditya Bhargava
* The Algorithm Design Manual - Steven Skiena
* Algorithms - Robert Sedgewick
* Algorithms - Jeff Erickson

Analysis of: these texts are a bit more advanced and focus on proving correctness or properties of algorithms

* An Introduction to the Analysis of Algorithms - Robert Sedgewick
* Algorithm Design - Kleinberg & Tardos
* The Design and Analysis of Algorithms - Dexter Kozen

Reference ""brick"": I've always had a love-hate relationship with this one

* Introduction to Algorithms - Cormen, Leiserson, Rivest, Stein^(not a favorite, but obligated to mention)

--------------------------

**Programming Language Theory**

This is my specialization in undergrad - how to reason about program semantics, how to analyze programs, and how to give type to things. I would recommend at least some algebra before tackling these. Some of the semantics texts may touch on algebraic topology results (e.g. Scott + Domain theory), but the poset / limit constructions are mostly self-contained.

Program Analysis: how to analyze programs

* Principles of Program Analysis - Flemming Nielson
* Principles of Abstract Interpretation - Patrick Cousot
* Advanced Compiler Design and Implementation - Steven Muchnick^(it pretends that it's a compiler text, but it's a static analysis text)
* Data Flow Analysis: Theory and Practice - Sanyal, Sathe, Khedker

Semantics: how to specify + calculate with the semantics of programs (what does it mean?)

* Types and Programming Languages - Benjamin Pierce
* The Formal Semantics of Programming Languages: An Introduction - Glynn Winskel
* Foundations for Programming Languages - John C. Mitchell
* Semantics With Applications: An Appetizer - Flemming Nielson
* Formal Languages and Compilation - Morzenti, Breveglieri, Reghizzi

Types: type semantics, & how to encode properties/contracts/proofs into dependent languages like Coq & Idris

* Software Foundations - Benjamin C. Pierce
* Proofs and Types - Jean-Yves Girard
* Lectures on the Curry-Howard Isomorphism - Sorensen & Urzyczyn
* Type driven development with Idris - Edwin Brady

--------------------------

**Systems / Compilers**

Compilers: how to write a compiler / interpreter

* Engineering a Compiler - Keith Cooper
* Modern Compiler Implementation in Java - Appel & Palsberg
* Compilers: Principles, Techniques, and Tools - Aho, Lam, Sethi, Ullman^(not a favorite, but obligated to mention)
* LLVM Cookbook - Pandey & Sarda

Systems: how computer systems are architected, how operating systems are made up, concurrency in action

* The Elements of Computing Systems: Building a Modern Computer from First Principles - Nisan & Schocken
* Computer Organization and Design RISC-V Edition - John L. Hennessy
* Linux Pocket Guide - Daniel J. Barrett
* Understanding the Linux Kernel - Bovet, Cesati
* The Little Book of Semaphores - Allen B. Downey
* Java Concurrency in Practice - Brian Goetz

--------------------------

**Numerical Analysis / Mathematical Modelling**

Prereqs: linear algebra + diff eq

* Introduction to Linear Algebra - Gilbert Strang
* Linear Algebra and its Applications - Gilbert Strang
* Elementary Differential Equations & Boundary Value Problems - Boyce & DiPrima
* Terrell's Notes on Differential Equations - Robert Terrell

First Course: introduction to numerical analysis + scientific computing

* Introduction to Scientific Computing - Gene H. Golub
* Applied Numerical Methods: With MATLAB for Engineers and Scientists - Steven C. Chapra
* A First Course in Numerical Methods - Greif & Ascher

Second Course (grad): more advanced methods

* Matrix Computations - Gene H. Golub
* A first course in optimization - Charles Byrne
* Numerical Linear Algebra - Nick Trefethen
* Applied Numerical Linear Algebra - James Demmel

Applied:

* An Introduction to Mathematical Modeling - Ed Bender
* Scientific Visualization: Python & Matplotlib - Nicolas Rougier

--------------------------

**Programming**

General languages: great PL manifestos

* The C Programming Language - Kernighan & Ritchie
* A Tour of C++ - Bjarne Stroustrup
* Hack and HHVM - Owen Yamauchi
* Programming in Lua - Roberto Ierusalimschy

Functional PL / Richard Bird: a world where you can do calculations with programs, and learn some category theory along the way

* Learn you a Haskell - Miran Lipovaca
* Real World OCaml - Yaron Minsky
* Introduction to Functional Programming - Bird & Wadler
* Pearls of Functional Programming - Richard Bird

--------------------------

**Learning to read Donald Knuth's TAOCP**

It took me most of undergrad and several years afterwards to work through small parts of the fascicles of volume 4. It led me down a giant detour of analytic + enumerative combinatorics, with complex analysis being the backbone of several of these asymptotic enumeration problems.

Step 1: Learning Discrete Mathematics

* A Course in Discrete Structures - Pass & Tseng
* Discrete mathematics and its applications - Kenneth H. Rosen
* Concrete Mathematics: A Foundation for Computer Science - Donald Knuth
* Concrete Math Companion - Kenneth E. Iverson

Step 2: Enumerative combinatorics

* Visual Complex Analysis - Tristan Needham
* Generatingfunctionology - Herbert Wilf
* Analytic Combinatorics - Flajolet & Sedgewick
* Introduction to the Theory of Species of Structures - Bergeron, Labelle, Leroux

Step 3:

* ???

Step 4: Profit

* The Art of Computer Programming - Donald Knuth^(not a favorite, but obligated to mention)

--------------------------

**Others**

Some other books (in order of ease) that I remember

* LaTeX - Leslie Lamport
* Discrete differential geometry: an applied introduction - Keenan Crane
* Nonlinear dynamics & chaos - Steven Strogatz
* A first course in network theory - Estrada & Knight
* Galois' Dream - Michio Kuga
* Ideals, Varieties, and Algorithms - Cox, O'Shea, Little
* Putnam and Beyond - Titu Andreescu

--------------------------",hsqvmqw,t3_s14xir,1642238446.0,False
s14xir,[deleted],hs7gt3j,t3_s14xir,1641909792.0,False
s14xir,I agree. The authors seem so smug through their writing. The book really isn’t the masterpiece people hype it up to be.,hs8nf0z,t1_hs7gt3j,1641926032.0,False
s14xir,"Clean Code

Also Cracking the Coding Interview (if you haven't read it already). I just started reading it late last night and couldn't put it down.",hs6xkxh,t3_s14xir,1641898309.0,False
s14xir,Should I start learning in my spare time or wait until ny University starts CS subjects for us in 2nd year?,hs7a06r,t3_s14xir,1641906450.0,False
s14xir,Start,hsb3lh5,t1_hs7a06r,1641961493.0,False
s14xir,What about modern CS Books?,hs9a1p2,t3_s14xir,1641934332.0,False
s14xir,C++ Concurrency In Action - Anthony Williams,hsa4oi8,t3_s14xir,1641946190.0,False
s14xir,"- The Annotated Turing
- A Mind At Play (biography of Claude Shannon)
- Assorted Papers on Fun and Games, Knuth
- And ther is an upcoming biography on von Neumann available for pre-order",hsambyl,t3_s14xir,1641953789.0,False
s14xir,GEB?,hsc2c60,t3_s14xir,1641985565.0,False
s0ficm,"This is amazing lol, especially the raspberry pi!",hs1ilik,t3_s0ficm,1641805836.0,False
s0ficm,"Well, Mabrook, I wish them Success in Deen and Dunia as written on the floppy disc 💾",hs1wdbe,t3_s0ficm,1641815905.0,False
s0ficm,"Ayooo, spot the Muslim. Thanks! I’ll let my brother know.",hs1zxdk,t1_hs1wdbe,1641818076.0,True
s0ficm,Love it!   Congrats on the graduation.  These are the best kind of cookies a CS graduate could enjoy .,hs1of4u,t3_s0ficm,1641810255.0,False
s0ficm,This is so wholesome <3,hs1qcii,t3_s0ficm,1641811703.0,False
s0ficm,"Ah, my favorite aspect of CS: uwu",hs2tqmp,t3_s0ficm,1641831250.0,False
s0ficm,"Hahaha. Yeah thats that’s because my brother ironically uses that phrase, so much now that it’s unironic……",hs520rd,t1_hs2tqmp,1641861536.0,True
s0ficm,"This is adorable!!! Congrats to your bro for graduating, this is awesome.",hs3xxlj,t3_s0ficm,1641845847.0,False
s0ficm,"All really cool, but I do have one question. What is yay taco? Maybe I’m totally forgetting something. Once again great stuff!!",hs3tne6,t3_s0ficm,1641844274.0,False
s0ficm,Hahah someone finally noticed. It’s my brothers nick name. Taco. And congrats taco was too hard to pipe. Same with tacos graduation. So yay taco it is,hs51lsu,t1_hs3tne6,1641861361.0,True
s0ficm,Awesome thanks for the response!!,hs56nu3,t1_hs51lsu,1641863474.0,False
s0ficm,yay Taco? I’m graduating with a CS degree in May and I’ve never heard of this xD can someone educate me?,hs3xkdq,t3_s0ficm,1641845711.0,False
s0ficm,Hahah just my brothers nick name. No crazy new code here,hs51pop,t1_hs3xkdq,1641861407.0,True
s0ficm,Damn i thought the @ was a Debian logo... Really good job on everything! It looks great!,hs4qk17,t3_s0ficm,1641856685.0,False
s0ficm,"That’s amazing, how come I wasn’t invited though. 😂",hs2pcdu,t3_s0ficm,1641829593.0,False
s0ficm,Adorable!!! And congratulations 🎊🍾🎉,hs46e6x,t3_s0ficm,1641848961.0,False
s0ficm,That is so awesome 😆❤️,hs4yk0m,t3_s0ficm,1641860084.0,False
s0ficm,"Awesome, you're a good brother, I'm sure he'll be happy",hs1pznd,t3_s0ficm,1641811439.0,False
s0ficm,"I’m a sister, a brother could never. 
My brother got me a dowel, bleach, and a sympathy card when I graduated nursing school. Hahahah each had a meaning.",hs1zst1,t1_hs1pznd,1641818002.0,True
s0ficm,"Oh sorry about that, wish I had a sister like you! Have fun when you celebrate the graduation, cheers.",hs20u1y,t1_hs1zst1,1641818598.0,False
s0ficm,Thanks!,hs21mka,t1_hs20u1y,1641819032.0,True
s0ficm,This is cursed,hs24sgi,t3_s0ficm,1641820717.0,False
s0ficm,What now why would you say that??,hs2hiev,t1_hs24sgi,1641826461.0,True
s0ficm,It's cute but I would hate it,hs2ihan,t1_hs2hiev,1641826859.0,False
s0ficm,Hahah I mean I could see that. Because I was an outsider tryna make it CS themed….so it’s like super cheesy.,hs2oo7k,t1_hs2ihan,1641829336.0,True
s0ficm,"Hey, I’m from CS and I like it. Your brother is lucky 😀😀",hs39ejw,t1_hs2oo7k,1641836936.0,False
s0ficm,arraytest[3] on the yellow floppy disk is cursed af,hs4y8al,t1_hs24sgi,1641859943.0,False
s0ficm,I was wondering if anyone else saw that,hs67fwx,t1_hs4y8al,1641879332.0,False
s0ficm,send the location :D,hs3uyvt,t3_s0ficm,1641844755.0,False
s0ficm,It passed 😅😅😅,hs51mzk,t1_hs3uyvt,1641861376.0,True
s0ficm,"I was expecting computer science symbols to be like asymptomatic notation like Big Omega and boolean or bitwise operators, but very thoughtful nonetheless.",hs2p9c6,t3_s0ficm,1641829560.0,False
s0vdjp,The math you need for data structures is discrete math. Set theory and graphs.,hs70hb4,t3_s0vdjp,1641900483.0,False
s0vdjp,"What – more specifically – are you having problems with?

Without knowing the above, have you studied much probability theory? This (and linear algebra which you mentioned) makeup an incredibly large amount of machine learning, which often comes as a not-too-convoluted application of these subjects.",hs63wcr,t3_s0vdjp,1641877489.0,False
s0vdjp,"I have not studied probability theory yet

My issue is how I haven’t yet been able to actually “apply” any of these mathematic subjects in computer science yet, and idk when I should be learning to do so",hs7vqrn,t1_hs63wcr,1641915916.0,True
s0vdjp,"Grab Matlab and try to tweak a few pictures, you’ll be able to see linear algebra in action almost immediately lol.",hsoqhun,t1_hs7vqrn,1642198267.0,False
s0vdjp,"I like this one, thanks fren",hsoytoh,t1_hsoqhun,1642201690.0,True
s0jzi8,"A few concepts:

Voltage: voltage is the electrical analog of pressure. It has a value for each point in 3D space, these values can change as functions of time. The voltage at a single point has no meaning on its own, it the difference of voltages between two points which has any meaning. 0v is usually defined as some arbitrary reference point which all other voltages will be compared to.

Node: a node is a continuous path of conductive material, for example a wire. It may branch off in multiple directions. All points on the same node are at the same voltage.

Power rails: two special nodes within the CPU would be the ground rail and the power rail. The ground rail would be the node which is defined as 0v. The power rail has some positive voltage such as 5v or 3.3v. These come from the power supply which will maintain the voltage difference between them. The voltages on these two rails are what defines “logic high” and “logic low”. These are distributed all throughout the CPU.

Transistor: a transistor is a 3 terminal device, meaning there are 3 pins which can each connect to different nodes. A voltage on the 1st pin controls whether the other 2 pins are electrically connected or not.

Logic gate: logic gates are special arrangements of transistors which perform logic operations on the voltages coming from their inputs, and set the voltages of their outputs accordingly. For example a not gate can be built with 2 transistors, one which will connect the output node to the positive power rail when the input node is low, and one which will connect the output node to the ground rail when the input node is high.

So to answer your question, 1s and 0s are voltages on wires. These voltages are set by electrically connecting the wires to the positive or negative terminals of the power supply. These connections are made with transistors, other signals on different nodes tell the transistors when to open or close. Logic is created with special arrangements of transistors.",hs2xtap,t3_s0jzi8,1641832730.0,False
s0jzi8,"Thank you very much.

So would you agree that it is wrong to say that the cpu represents 1s and 0s with turned on and turned off transistors? And that it’s also wrong to say that 1 means there is electrical current and 0 means there is no current?

Edit: Also is it correct to say that a logically high (1) wire has a high voltage relative to ground? And a logically low (0) wire has basically no voltage relative to ground?",hs3h834,t1_hs2xtap,1641839761.0,True
s0jzi8,Yes you have it now.,hs4h0vz,t1_hs3h834,1641852901.0,False
s0jzi8,"Your edit is exactly correct.

However no current actually has to flow, it’s just a static voltage difference. With the exception of during the time when a wire is changing voltage, at the physics level charge is the source of voltage, so to change the voltage on a wire a small amount of charge must flow onto the wire. However this is an unwanted but physically unavoidable property of the wire. So at the beginning of a clock cycle if a wire is changing voltage then a small current flows onto the wire through a transistor, but during the bulk of the clock cycle there are no currents flowing.",hs4uk4w,t1_hs3h834,1641858378.0,False
s0jzi8,Oh that makes perfectly sense thank you!,hs69dos,t1_hs4uk4w,1641880421.0,True
s0jzi8,"At the lowest level yes. The reason for 1,0 is transistor in open or closed.

Transistor level is pretty far into EE, so you wont find as much good answers here, but ill try, ill say some wrong things for simplicity sake - it is very complicated when you look at the low level.

We discuss in an open/closed or 1/0 due to the phyisical properties of the trasistor, control signal (or gate voltage) above some theashold (that depends on the doping and process) you can allow voltage and below it you can not.

So assuming your threashold is 0.7V, having a gate voltage of 0.2V would be the same as 0. This lets us discuss in a resulutions of 0 and 1.

A single transistor is not a ""bit"" though, but a combination of transistors (and other linear element) in the end forms some sort of memory element. This element might be an input for a different transistor and so on, so its important to talk about that in an 0/q abstraction as well.

Having a base 3 processor can work however, using a very carefully tuned transistors and logic levels - but its completely usless due to the immense amount of drawbacks.",hs4juqn,t1_hs3h834,1641853992.0,False
s0jzi8,"1s and 0s are just an abstraction humans use to do calculations. In terms of a transistor 1/ON means current can flow through the transistor. 0/OFF means current can not flow through the transistor. 
Now if you lined up a few transistors in various states like:
ON OFF OFF ON OFF ON OFF ON
that doesn't mean much to a human. So let's assign them 1s and 0s, that would look like: 
10010101
Now we have numbers. But that still doesn't mean much to an average person so let's convert them from base 2 (binary numbers) to base 10 ( 0-9 number system we learn in grade school).
I won't go into a full lesson on calculating binary but 10010101 = 149.
So we just used a little abstraction to make a group of 8 transitors represent the number 149!
Now if we keep adding transistors (bits) we can make all kinds of numbers to do all kinds of calculations with.
Hope that makes sense.",hs2fnzq,t3_s0jzi8,1641825715.0,False
s0jzi8,Yes but where in an actual circuit would you line up transistors like that to represent a binary number? This doesn’t really make sense to me. Sequential logic (memory circuits) sure don’t work this way. And I don’t think combinational logic does either,hs2ga3m,t1_hs2fnzq,1641825965.0,True
s0jzi8,"> where in an actual circuit would you line up transistors like that to represent a binary number?

In the CPU, they aren't ""lined up"" (deserialized), they're ""sideways"" (serialized). So a whole, say, 8 bit operand is operated on at once.",hs2iacg,t1_hs2ga3m,1641826781.0,False
s0jzi8,"Okay but for example with an ALU you have way more transistors than bits in the operands, and the transistors don’t really all represent a bit do they?",hs2imzf,t1_hs2iacg,1641826925.0,True
s0jzi8,"> Okay but for example with an ALU you have way more transistors than bits in the operands, and the transistors don’t really all represent a bit do they?

Well no, because by themselves transistors don't do anything, not even store values. To do that, you need to combine transistors with each other (and resistors, diodes, capacitors, ..). To do this you build gates and latches that are represented as single black boxes, but are made up of multiple components. With these abstracted components you can then build logic circuits. [Here's](https://www.cs.bu.edu/~best/courses/modules/Transistors2Gates/) a website I googled that shows how to build gates from transistors (this is already a bit abstracted because if you want to build f.e. an AND gate from transistors you need resistors, too). [This](https://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) is a book that builds a computer from first principles.",hs32sam,t1_hs2imzf,1641834531.0,False
s0jzi8,"Yes I get that but my point was just, someone said that we can assign bits to the transistors and that the computer represents 1s and 0s with on and off transistors. And I still think that transistors aren’t representing any numbers, the numbers are some electricity or voltage or whatever going through a wire.

Also to be pedantic transistors obviously do do something on their own but you know that",hs3geyi,t1_hs32sam,1641839467.0,True
s0jzi8,"> Yes I get that but my point was just, someone said that we can assign bits to the transistors and that the computer represents 1s and 0s with on and off transistors. 

So without going *too* far, there's levels of abstraction of knowledge. This is a point I think Carl Sagan once famously made when he discussed the answer to the question ""why is ice slippery?"" You can look at a computer at a very low level, for example the level of voltage and current changes in transistors. That is what you have chosen to do, but that decision is arbitrary. It's probably the least abstract level you are comfortable with, but there's no reason to not go deeper and talk about Maxwell and field theory and perhaps quantum mechanics. I don't know, I'm not a physicist. But yes, you are correct when you say that ""transistors represent 1s and 0s"" is false on this level of resolution. But it's a *useful* abstraction. If you actually look at a processor (or any integrated circuits), and you keep zooming in, you eventually end up with transistors so tiny you could fit thousands of them into the cross-section of a human hair. And of course those transistors aren't just connected in series, they are instead configured in latches and gates, in multiplexers, and so on. What ""transistor"" in colloquial speech *means* is really dependent on what you are considering. When somebody says ""transistors represent 1s and 0s"", what they kinda mean is that there are tightly integrated MOSFETs (and other elements) on a chip that are arranged into latches that store state as ""current"" or ""no current"", which can be manipulated by tightly integrated MOSFETs (and other elements) on a chip that are arranged into logic gates and the like. And that's all incorrect and just a different level of abstraction (because what on earth is a ""transistor"" if not a human abstraction over what happens if certain materials are put in very specific configurations within very specific electro-magnetic fields, and what is a ""material"", and so on), so we just say ""transistor"".

> And I still think that transistors aren’t representing any numbers, the numbers are some electricity or voltage or whatever going through a wire.

That's functionally the same thing. They are ""representing"" a bit in that current going through a electro-magnetic field projecting from a wire is understood to encode ""1"", and no such current encodes ""0"". 

I actually don't know what else to say, because I don't think you actually have an issue with that, or at least I hope so. Because otherwise, if you are tasked with giving somebody 2 apples, you would have to burst into tears because how can apples be represented by a number and what is counting anyway?",hs3pka8,t1_hs3geyi,1641842772.0,False
s0jzi8,"Thank you very much for that answer!

> I actually don't know what else to say, because I don't think you actually have an issue with that, or at least I hope so. Because otherwise, if you are tasked with giving somebody 2 apples, you would have to burst into tears because how can apples be represented by a number and what is counting anyway?

Lol. Don’t worry I‘m gonna stop asking now. I just want to understand this stuff at an actual physical level",hs3ufdr,t1_hs3pka8,1641844556.0,True
s0jzi8,"Deleted this original reply cuz QuietLikeScience summed it up way better than me first. 

That said,  If you're interested I highly recommend watching the Crash Course Computer Science series on YouTube.  The first few episodes covers this stuff and may shed some light for you.",hs3rp51,t1_hs3geyi,1641843552.0,False
s0jzi8,"Thanks, I‘ve seen those, they are pretty well made however they don’t go as in depth as I want to understand it lol",hs3ulk9,t1_hs3rp51,1641844620.0,True
s0jzi8,"> isn’t voltage the electric tension between two points, which points would that be?

Personally, I like to think of voltage using the [waterfall analogy.](https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Supplemental_Modules_(Analytical_Chemistry\)/Electrochemistry/Voltage_Amperage_and_Resistance_Basics#:~:text=If%20we%20draw%20an%20analogy%20to%20a%20waterfall%2C%20the%20voltage%20would%20represent%20the%20height%20of%20the%20waterfall%3A%20the%20higher%20it%20is%2C%20the%20more%20potential%20energy%20the%20water%20has%20by%20virtue%20of%20its%20distance%20from%20the%20bottom%20of%20the%20falls%2C%20and%20the%20more%20energy%20it%20will%20possess%20as%20it%20hits%20the%20bottom.)

> I have always imagined the 1s and 0s or asserted and deasserted signals as high and low amperage, is that wrong?

Amps and volts are different. Amps are the amount of water going over the waterfall, volts are the height of the waterfall (the potential energy). When the external clock ""drives"" the CPU, it alternates voltages. Electrical engineers look at [eye diagrams](https://en.wikipedia.org/wiki/Eye_pattern) to assess the quality of binary signals. 

> I‘m really confused by this, some say 1 and 0 is the state of the transistor but that doesn’t make sense to me.

You should watch [this video](https://youtu.be/Hi7rK0hZnfc) on sequential logic. However, digital logic design is an art unto itself.",hs2g20w,t3_s0jzi8,1641825874.0,False
s0jzi8,"What is meant by „power“ in that eye pattern? And I understand what voltage and amperage is, but which one is interpreted as 1 and 0?

Edit: Also those combinational logic circuits don’t really use „transistor is on“ as 1 and „transistor off“ as 0 as far as I understand",hs2gkp5,t1_hs2g20w,1641826084.0,True
s0jzi8,"Here are some excellent resources

[Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)

[Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)

Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share)

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",hs4psv2,t3_s0jzi8,1641856372.0,False
s0jzi8,I am reading Code now and I wish I would have read it 20 years ago when it came out. Such a wonderful book. It explains exactly what OP wants to know.,hs6a30g,t1_hs4psv2,1641880836.0,False
s0jzi8,1s and 0s can be anything that is detectable in multiple states by a single detector.,hsgb7ob,t3_s0jzi8,1642052177.0,False
s01ijj,Mass surveillance and facial recognition,hryszds,t3_s01ijj,1641762565.0,False
s01ijj,"Several US law enforcement offices have been accused of using facial recognition with false matches as criminal evidence. Many of these algorithms have a noticeable racial bias.

https://www.nytimes.com/2020/01/12/technology/facial-recognition-police.amp.html",hs1pjq1,t1_hryszds,1641811110.0,False
s01ijj,Can an advanced AI be considered someone's property?,hryvu9c,t3_s01ijj,1641763613.0,False
s01ijj,"This just reminded me of the Star Trek TNG episode where Data is on trial and the questions is does he have rights as a person, or is he the property of Starfleet.  


Maybe I'll show that episode to my students at the start of the debate unit. lol",hryxqox,t1_hryvu9c,1641764282.0,True
s01ijj,"If you ask a student to multiply two three-digit numbers, most people would agree that they have to ""think"" to produce an outcome. However, if you write a simple program for a computer to multiply two three-digit numbers, most people would argue against the computer doing any sort of ""thinking"".

So what can we use to define ""thinking"" (i.e. ""intelligence"") if it isn't based solely upon the outcome?",hrzjrdu,t3_s01ijj,1641772328.0,False
s01ijj,I disagree.  Humans would just follow the algorithm they were taught in school.  Thinking would be required if you were to explain the algorithm or your own algorithm.,hrznjr1,t1_hrzjrdu,1641773754.0,False
s01ijj,"Should decisions of an AI be explainable/interpretable in life or death situations (e.g. medical/military)? What about if it makes better decisions than humans?

How would you know for sure when AI has consciousness? When does it acquire the same rights as humans or animals?

Should AI in social media that tries to feed you maximally addictive content be restricted?

Should people whose jobs are automated by AI have a right to get universal basic income?",hryybso,t3_s01ijj,1641764488.0,False
s01ijj,"I major in computer science and for the computer ethics course I took we debated whether it was ethical to do mass surveillance of students social media posts to track for concerning behavior that could lead to something tragic such as a school shooting (we had learned that many school shooters had posted on social media concerning content). If you are interested in learning more feel free to check out this article on one of the PhD students of the professor who taught my computer ethics course.

https://engineering.lehigh.edu/news/article/online-privacy-algorithms-we-trust",hrz1s4r,t3_s01ijj,1641765716.0,False
s01ijj,"What is the meaning of intelligence? What specific character does an ""advanced"" Aritificial Intelligence supposed to have such that it would be considered on the same level as a thinking human?",hrz1qau,t3_s01ijj,1641765697.0,False
s01ijj,"If you could have your English essay graded by a human or by an AI that has been shown to give correct results with reasonable accuracy, which should you choose?

If a car accident results in a death, does it matter whether the car at fault was a human or an AI? How can such a situation be handled in terms of holding the correct party responsible for damages?",hrz33xr,t3_s01ijj,1641766192.0,False
s01ijj,"Who is liable if AI makes a mistake in medicine / self driving cars?

Is the developer liable? Data collector? Etc.",hrzbyjg,t3_s01ijj,1641769410.0,False
s01ijj,"How do we ensure that our AI reflects human ethics? How do we make sure that algorithms do not reflect patterns such as racism or sexism in terms of recommendations or referrals? (Financial scoring, resume review, etc)

Is it possible for AI to become equal to or better than people at tasks that people perform? If so, how should society address this issue? Is it possible for AI to take over ALL jobs?

What are the ethical differences between AI and natural intelligence? Are there any intrinsic differences between AI and natural intelligence and why? How can laws reflect this?",hrzfvcz,t3_s01ijj,1641770871.0,False
s01ijj,"Define Artificial

Define Intelligence

Define Computer 

Define Human",hrz6lq2,t3_s01ijj,1641767447.0,False
s01ijj,Human and computer have solid scientific definitions.,hrzn53o,t1_hrz6lq2,1641773602.0,False
s01ijj,"Computer used to refer to a person computing (i.e. with a pen and paper). Human has a species definition, but this is not the only definition - though perhaps 'person' could be used here instead.
 
These aren't good debate topic statements though, they are questions. A topic statement gives a position that can be argued for and against.",hs1e7fm,t1_hrzn53o,1641802599.0,False
s01ijj,"I think it's still an interesting activity to write down and analyse the implications and contradictions between these definitions

Corolary arguments which arise could be ""to what extent can a human/ person bequeath their personality/ goals/ rights to a machine/ computer""",hs1mwqy,t1_hrzn53o,1641809123.0,False
s01ijj,"You haven’t given us the objective you’re hoping to achieve. If it’s simply, “Get them to think about these things.” I don’t think you’d need to ask Reddit; a simple Google search would do fine.

It truly depends on what you’ve taught your course as most debates seem to tend towards settling into an argument to find consensus over definitions. 

What have you defined to them as artificial intelligence? And what do you define as “advanced?” What characteristics or traits would an “advanced AI” have from your perspective?

Russell and Norvig define “artificial intelligence” as acting rationally, but that seems to omit other human characteristics such as empathy, selflessness, generosity, etc.

Overall, what are you hoping to achieve with debate topics?",hrzxrxq,t3_s01ijj,1641777740.0,False
s01ijj,"Let's say we live in a world where most work is done and managed by AI. AI controls machines/processes to mine for physical resources, build products with them, and handle logistics. All work is done by AI, except for government/ethical/ high-level decision-making stuff.

How should the benefits of that system be distributed (profit, resource wealth, product ownership)? Who ""owns"" the software assets (including resource production and product creation? Who owns ""data""? Do tech people become the new elites?

This can parallel lots of similar debates around say land property rights, the right to catch your own rainwater, the right to privacy/data, etc. in the realm of ""ownership"".

Also, if AI are treated as pets, how should we deal with ""animal abuse"" (hacking ddos stuff)? Should it fall under the same realm as cyberlaw, or is there some kind of human/animal rights situation happening?",hrz1qeg,t3_s01ijj,1641765698.0,False
s01ijj,"What is thought without emotion?   


this could be expanded to how much of human decision making is done with instincts and how much is logic, and whether that's a helpful for us in our dealings with each other, or whether an AI would be better suited for things like governance/law etc. because it's not burdened with emotions OR are those emotions the thing that keep this from falling apart completely? 

I've always wondered how much we're thinking based on what we know to be true and how much is based on an unknowable predictive engine we've developed over evolutionary time (clearly, dreaming has some element of simulation to it; could dreams not be training scenarios?). Seems like you could almost explain the anti-vax phenomenon as an expression of emotional/gut thinking vs logic and data.

Lots of directions to take this",hrz35hw,t3_s01ijj,1641766207.0,False
s01ijj,"The 2021 Reith Lectures were on the advancement if A.I. There is plenty to go on with those if you are in the UK and have a TV licence.
[BBC Reith Lectures page](https://www.bbc.co.uk/programmes/b00729d9/episodes/player)",hrzhkjc,t3_s01ijj,1641771513.0,False
s01ijj,"My major field of publication is in computational creativity: AI that exhibit behavior that would otherwise be deemed as creative in humans. (Art, music, jokes, architecture design, etc.)

If you check ICCC, you can find a good amount of philosophy on which to base topic. One HS-level topic may be along the lines of, ""Is creative AI a threat to human artistic expression/culture?"" (I.e. if an AI can potentially learn to optimize and make the best music, jokes, and paintings, is there room for humans? What if we get pushed out of the space? If the billboard hits are all written by machines, will there be a major fault in human cultural progression?) (Kind of like how the world champion of Go lost to Google's AlphaGo and quit the game because there was no point in competing anymore.)",hs0s6w1,t3_s01ijj,1641790332.0,False
s01ijj,"Lots for AI driving. Whether giving up your right to drive is good? It would eliminate more casualties but the future casualties would be random instead of bad drivers. Also, how do you value life if a schoolbus is going to hit another car. Should the other car swerve killing itself to save the children or should the car do its best to save itself?",hs0xu9y,t3_s01ijj,1641792761.0,False
s01ijj,"This is all my perspective.

Same rights as humans? depends.

Intelligence of solving problems doesn't necessarily emotional intelligence if the A. I don't have what we considered ""will"" then why would we give rights?",hs0yn7b,t3_s01ijj,1641793165.0,False
s01ijj,"If personal or sensitive data (like, say, surveillance video of private moments in a home) are only screened for illegal activity by an AI, should this be considered an invasion of privacy? (Consider the number of cameras in the home already, like laptop cameras and phone cameras and the like, which theoretically could be recording but then ignoring the data. At what point does this become an invasion of privacy? When the data is processed? When it is screened by a computer? I mean, clearly when a human gets involved it is an invasion of privacy, but if only machines see the data?)",hs1029u,t3_s01ijj,1641793901.0,False
s01ijj,"Resolved: Humanity must prioritize the development of super-intelligent AI so that it can be used to solve world problems (hunger, global warming, economic inequality, etc.).

Resolved: A super intelligent AI would make for a better government leader than a republic.

Resolved: Jobs involving repetitive and mechanical tasks should be replaced by AI/robotic systems without regard to unemployment.

Resolved: A minimum wage should be established for robot workers in order to collect taxes and fund universal basic income.",hs11w8u,t3_s01ijj,1641794894.0,False
s01ijj,"* Virtual assistant giving advice, especially when we don't yet have technology to teach it which advice is allowed and which is not (e.g. Alexa telling the child to stick fingers into electric outlet)
* Impersonating yourself with an AI to teachers/colleagues/friends/relatives without telling them, e.g. voicemail bots
* Online presence-based background checks (e.g. refused a job or school/college admission due to online posts)
* Creating and publishing deepfakes of real people—joke causing unintended consequences, revenge, dirty political competition or propaganda
* Teaching machine learning to synthesize infinite paintings in the painter's unique style, rendering the painter unneeded and uncompensated

To OP: try to avoid banal daydreamer fantasies like human-level AIs? We don't know if it will take another 2000 years to get to that stage. There are real problems banging on the door.",hs1c5p1,t3_s01ijj,1641801180.0,False
s01ijj,A.I. taking out more jobs than any technology in history within the next century.,hs1qqkj,t3_s01ijj,1641811990.0,False
s01ijj,"If any justice system decides to use AI: what safeguards, if any, should there be to protect against that AI discriminating against certain groups (ie male/female, black/white)?",hs390v9,t3_s01ijj,1641836799.0,False
s01ijj,Can a service provider collect store and sell an arbitrarily large amount of generally unspecified personalized data in payment for a finite service provided?,hsgjejl,t3_s01ijj,1642057426.0,False
rzqau2,"As someone who studied cognitive science, computer science is RIPE with meaning. Theories of computation have deep implications for how we understand the mind and it’s functions (computational theory of mind). Computer science also has major implications on the field of mathematics. Godel with the help of Turings work on computation (early computer science) have proven that there are mathematical truths which are NOT provable. Some crazy ass shit. Not sure if you’d learn any of this in a CS class. I learned all of this in philosophy classes. Try and compliment CS with Philosophy and I promise you will find the field ripe with meaning.",hrxwzsp,t3_rzqau2,1641751311.0,False
rzqau2,"I'm a big fan of the work of Douglas Hoffstadter - he's written quite beautifully on the implications of Kurt Godels proof for computation and consciousness (GEB / I am a Strange Loop).

I studied some Bert Russell and Godel during my Maths Bsc. Im currently looking to start a master's in computer science this year as I'd like to learn more about how recursion and ""Godel loopiness"" is implemented to make computers. 

What I find interesting about Godel and Turings theorems is that they seem to suggest that computation and logical experiments will always be at the 'cutting edge' of human knowledge (well at least that's my very amateur interpretation and expression. I'm looking forward to developing a more sophisticated appreciation)",hrz3sac,t1_hrxwzsp,1641766433.0,False
rzqau2,"The lines between ""cognitive science"" ""computer science"" ""mathematics"" ""language"" and ""philosophy"" blur quick 

It's a very meaningful field of study",hrz4xdl,t1_hrz3sac,1641766838.0,False
rzqau2,"I'm a CS undergraduate too, I want to pursue a Computational Psychology or Philosophical masters after my degree. But I find myself utterly confused when I go looking online. Can you recommend something/give a bit of guidance?",hs0a0iw,t1_hrxwzsp,1641782672.0,False
rzqau2,"I'm not too sure what guidance you're looking for and i'm certainly not qualified to give much out. I was a pretty mediocre maths student (got a 2:1 from a Russel Group uni in the UK 5 years ago) and haven't formally studied any Computer Science, Philosophy, Psychology ... Courses. Apologies if this is unsolicited

As I mentioned above, I really enjoyed reading I Am a Strange Loop. I found Hoffstadters point of view really informative on how mathematics and the philosophy of logic can explain human cognition. Would recommend that as an entry-level book without much need for any maths or CS background. Not a massive long read (400 pages). Also has a good audiobook version on Audible - although the hard copy has some illustrations which are handy. 

For more mathematical philosophy I'd recommend just searching for things about; 

1. David Hilberts program for axiomatic mathematics, 

2. Bertrand Russel/Alfred Whiteheads theory of types (and it's criticism)

3. Kurt Godels incompleteness theorem (Nagel and Newman have a nice short book on this) and 

4. Turings theorems on computable numbers.

I don't think I have any books I'd suggest for these which are good introductions (although hoffstadter mentions all of these)  but they all have nice wiki pages 

I consider these 4 developments of philosophy from  20s/30s game changing. Turing and Church went on to use these developments to theorise modern computing and Artificial intelligence.",hs34eqm,t1_hs0a0iw,1641835121.0,False
rzqau2,"I might actually be interested in Cognitive Science but when I looked it up online cognitive science seems to be more about psychology, neuroscience, a bit of philosophy, but actually very little Computer Science or Mathematics in it… 
So I don’t know about it…",hs1lrys,t1_hrxwzsp,1641808260.0,True
rzqau2,"Yeah, cognitive science is a subject that is very hard to pin down. Its defined differently by many different people. Broadly, it is the study of the mind, artificial or real. You can study the mind in many different ways: neuroscience, philosophy, psychology, anthropology and so on. One of the more interesting properties of the mind are its computational properties. That's where AI, computer science, and mathematics come in. So you really can't have cognitive science w/o cs and math.

Its also worth mentioning that the foundations of neural networks were inspired by psychology. Also, Turing/other early computer scientists were primarily interested in computation with how it relates to the mind. But this is some really rigorous stuff that won't appear on the front page of a google search. You really just have to dive deeply into it.

I'd recommend studying godel's incompleteness theorem if you really like math and computation. 

Try reading Godel Escher Bach too!",hs8cbih,t1_hs1lrys,1641922015.0,False
rzqau2,"I feel like I'm in the same boat my friend

Although I suspect there is plenty of maths involved at its root. If you're built for it - you can find maths anywhere 🧀🧀🧀",hs34pgi,t1_hs1lrys,1641835228.0,False
rzqau2,"Sounds like you should be taking philosophy but realistically no job serves a purpose other than income, and if you're lucky, entertainment. Some people are just wired in such a way that they enjoy this subject more than others, and usually its to do with the fact that computing is very logical and simple, but very powerful.",hrwyqga,t3_rzqau2,1641737617.0,False
rzqau2,Guess I’m weird. I actually enjoy this,hryeiub,t1_hrwyqga,1641757482.0,False
rzqau2,Because it's fun..? Not really sure what you're talking about.,hrws7m5,t3_rzqau2,1641734201.0,False
rzqau2,There is no meaning and no purpose...,hrwovdp,t3_rzqau2,1641732228.0,False
rzqau2,lol same,hry6e1v,t1_hrwovdp,1641754625.0,False
rzqau2,Sure there is.  There may not be any inherent meaning but meaning can most definitely be created.,hrzo5tz,t1_hrwovdp,1641773987.0,False
rzqau2,that'll last you like 2 seconds before your brain starts flooding itself with things it needs to think about.,hs00zgo,t1_hrwovdp,1641779039.0,False
rzqau2,"There is no meaning or purpose to computer science, except what you want to attach to it.

In that sense, it's the same as everything else in life.

You can use your knowledge of computer science to earn yourself a decent income. And/or you can use it to make the world a better place for all. Nothing wrong with either and it's all up to you.",hrxh1h4,t3_rzqau2,1641745338.0,False
rzqau2,The purpose of computer science is problem solving and developing your problem solving skills.,hrxl34h,t3_rzqau2,1641746868.0,False
rzqau2,"I believe that's Engineering, not just Computer Science.",hs1x0s2,t1_hrxl34h,1641816304.0,False
rzqau2,"This is a discussion I was having with one of my mates that I studied computer science with at university. There's a massive misconception in our industry that CompSci == coding, when they are nowhere near the same thing. Coding is the practical application of writing software, whereas CompSci is the academic study of computers. That's why they teach aspects such as hardware, networking, logic etc. Lots of people go into CompSci thinking their going to learn how to code, when they're really not going to.

To answer your question about purpose is therefore a two part answer:  
\- Coding - the purpose of writing software is to a) automate processes to make people's lives easier/quick, b) to further humanity (see uses in medicine, climate science and humanitarian work etc), and c) most importantly, to make money

\- Computer science - the same as every science and academic study area, to learn more about the area and to further it. Computer Scientists study the world as it is to find more efficient ways to process data, calculate results, and generally just ""do stuff better"".",hryo0jb,t3_rzqau2,1641760826.0,False
rzqau2,"What was your meaning/purpose in enrolling in a CS degree program? If you can't answer that question with an answer more meaningful than to make money then maybe you should figure out what you want to do with your life that will give it meaning before you waste anymore time, energy, and possibly a lot of money.",hrx0pdy,t3_rzqau2,1641738576.0,False
rzqau2,"Why? Making money is a legit reason to study CS.

A career doesn't need to be the thing to supply your life meaning.",hs00xsb,t1_hrx0pdy,1641779021.0,False
rzqau2,OP is specifically looking for purpose in CS as a career path. My point was he should have already done so before choosing CS as a field of study.  As to the broader reason for choosing a career I always advise you should never sell yourself short. If you have the capacity to study CS and be successful in that career path then you have the capacity to do what ever you want. Do what you love. It's sad to see someone spend a third or more of their life doing something that does not add meaning or purpose to their life. Life is short and unpredictable. Live life without regrets.,hs03vcv,t1_hs00xsb,1641780198.0,False
rzqau2,"""should have already done so before choosing CS""

Why? Meaning, purpose, and passion (do what you love) can come later. Some people don't know what they are passionate about in college. Sometimes you only develop those traits as you go deeper into a topic. It almost has to come later because how meaningful is your work really going to be if you barely have any experience in the field? 

Maybe you try, but never find passion or meaning in a CS career (or hell, any career). But if you can work in a CS career you can save and invest a ton of money early and not need to work a third or more of your life. Then you can go do what you love and actually be able to fund it.",hs0i285,t1_hs03vcv,1641785988.0,False
rzqau2,"You learn how to use computers. Computers help automate everything.

If you don’t understand everything that we need to do in life to survive/grow etc that is another topic.",hrwsoif,t3_rzqau2,1641734468.0,False
rzqau2,">You learn how to use computers.

Computer science is not about how to use computers.",hrxovwk,t1_hrwsoif,1641748307.0,False
rzqau2,"I mean he’s literally asking about CS on the most basic level of life etc.

If how computers are built/function, the logic, foundation etc doesn’t all unlock using a computer then I don’t know what does",hrxp79r,t1_hrxovwk,1641748427.0,False
rzqau2,"He didn't say what you're using them for. Computer science is very much how to use computers to do things.

It's not ""how to use power point"" but it's still using computers.",hrxx0gx,t1_hrxovwk,1641751317.0,False
rzqau2,"Using computers is general enough to cover everything, not sure how that makes it wrong considering the question being absolutely general",hryfkgv,t1_hrxx0gx,1641757854.0,False
rzqau2,"Theoretical computer science is basically the development of theories on computing (logic gates, programming, data management etc.) - it’s basically trying to come up with new ways of doing things so that programmers (computer engineers) or hardware engineers can implement your theories and presumably benefit society with more efficient ways of doing things (algorithms) in this realm.

It’s downstream from things like pure logic, cryptography and mathematics but not quite down to systems administration or business logic programming.",hrx65dl,t3_rzqau2,1641741025.0,False
rzqau2,"There is no purpose and, even worse, there's no future in computer science. Sounds crazy, right? Technology IS the future... if, and only if, there's power... and a supply chain that can develop it but the real hole in the basket of the future of this lifestyle we've created is the need for reliable grid power in a time where the weather is getting predictably worse. 

If you're keen on data, you could get into climate modeling. Plenty of meaning to be found in that, but probably only employment in the insurance industry (if you're after the money). 

All your life you've been told that coding is a necessary skill when the people telling you that didn't think to make the power infrastructure well enough that it can be relied on in the future. 

The secret to all of this is that the only meaning/purpose anyone's life has is to consume resources as quickly as possible, to generate income, to maintain a standard of living that's fundamentally unsustainable for even 1/10th of our population. Your lifestyle is intentionally unsustainable because that's where the wealth is generated; in the burning of resources. 

Now, go spend some money and the rest of the afternoon wondering what the meaning of any of this is beyond acquiring more/better and why that's worth your short lifespan, especially considering that most resources can only be used once and that your carbon footprint will be changing conditions on this planet for a thousand years after you die. This is the machine we built. This is what you've spent your life studying to be a part of. Now it's up to you to choose something else that doesn't make the world worse just so you can have nice things.

Even the people down voting this have to admit that this cannot continue and will either change or run out of the resources it needs to sustain this level of growth. It's basic math. there isn't enough for us to live this way, so we either stop and do something else or we wait for it to break and have nothing else to fall back on. Those are the only options. This ship is literally sinking into the ocean (check out the update on the Thwaites Glacier and notice the trend that every time they check it, it's further along in its collapse than they expect)",hryzoo6,t3_rzqau2,1641764969.0,False
rzqau2,"I love this question. I do like coding but I think I like ""computer science"" more. Computer Science for me has to do with coming up with ways to COMPUTE mathematical functions and that for me, at least, is the meaning. When you program a program that can differentiate an equation, you know you're making something that has and will help in the human evolution at least in the mathematical sense. If you get what I mean-",hry3zd7,t3_rzqau2,1641753786.0,False
rzqau2,Why do you assume there is one?,hrybmbl,t3_rzqau2,1641756451.0,False
rzqau2,"I type the words, the things happening, the direct deposits hit.",hryi6l5,t3_rzqau2,1641758783.0,False
rzqau2,"You can make anything you want, and there’s more tools available to do so than at any point yet. Go make the world better! Build a company, a tool, a game, something.",hrysk3k,t3_rzqau2,1641762414.0,False
rzqau2,"The ""meaning of life"" is too abstract a concept, it should be posted on a philosophy sub.

As for CS' ""purpose,"" you should ask yourself how the ""man made"" fields of study came to be. For instance, was math invented or discovered? The general consensus seems to be that it is both; math is the quantification of naturally occurring phenomenon, but requires the invention of a proof (or something along these lines).

In the same vein, electrical engineering is the demonstration of man's mastery of a natural phenomenon (the flow of electrons). Modern computer science has grown out of electrical engineering by combining the ideas of information theory with electrical engineering. 

Now we are seeing the next evolution of this cycle; as computers become fast enough, we see blooming fields like data science and machine learning, that add more abstraction layers on top of CS that allow humans to model complex adaptive systems and gain further insights into the world around us. Basically, as new technologies are discovered, we (humans) gain greater insights into the world around us and gain progressively more tools to model those systems and look past their face value.

So there isn't really a ""purpose"" beyond providing humanity further insights into the world around us, and increasing our collective understanding. This isn't a ""meaning of life"" but the natural human inclination towards adventure and discovery is a pretty noble pursuit imo.",hrz9nt4,t3_rzqau2,1641768561.0,False
rzqau2,"For the purpose and meaning of it all, please see: [https://youtu.be/pneBKFjxInQ](https://youtu.be/pneBKFjxInQ)",hrxn36d,t3_rzqau2,1641747626.0,False
rzqau2,"CE student, here. I look at ourselves as a sort of digital version of old-style craftsmen. I love to have the possibility to wake up with an idea and to be able to create it from scratch. That is for what concerns the hobby. About the job career, instead, I can tell you that IT is one of the most potential fields nowadays.",hrxqobu,t3_rzqau2,1641748977.0,False
rzqau2,The meaning of life is to procreate so the human race can live on. Computers help in this ongoing struggle by delivering porn (knowledge) and dating apps (action).,hry086l,t3_rzqau2,1641752472.0,False
rzqau2,"I can't help but enjoy and attempt to answer such a question! I expect you'll get a lot of interesting answers.

It's pretty simple for me: I like video games (playing and making them), I like robots and I am a huge Sci Fi nerd. Studying CS for me is just studying what some of my favorite things have in common.

For those with a philosophical bent (of which I'm one), the fact that you can simulate things with computers is kind of a big deal. You make models in almost every domain. It's one of the most accessible ways to start tackling big, interesting questions. 

Zooming out further, computers are the latest in a long line of metacognitive tools. A ""Tablet"" is a Clay Tablet's spiritual successor, quite literally and deliberately. Personally, I feel that the tools we interact with are a big part of the human experience and it's neat to be at the fore-front of things. People making the first stone tools might have felt similarly empowered. I like to think that we are still at the front of the computer revolution and that there is a lot of miniaturizing and normalizing left to do. I'd like to ride my bike out into the woods, turn on my AR device, and suddenly have my home office all around me including access to the internet and haptic feedback, and that kind of thing. Right now being a computer person is often like being a person of two worlds (the person at the computer, and the person away from the computer) but I think the technology's logical extreme is a paradigm where you can do computing anywhere and any time, with few restrictions, combining the mobility of ""away from computer"" with the power of ""at the computer"" and applying this combination to every day life. It is really thanks to smartphones and tablets that things are moving in that direction. What is the next step? It's hard not to get excited about the frontier of metacognition. People must have been similarly excited about paper and ink, scrolls and notepads. 

Personally, I want to see what happens when we are applying these fundamentals not only to computers, but to cells. Cells are in theory programmable and certain principles of computing carry over. The body and mind hint at all kinds of systems and properties we could be using for other things. What will a cybernetic future with biotech look like? I really want to know. I think it could be really awesome.",hrypkoq,t3_rzqau2,1641761373.0,False
rzqau2,"Because I enjoy it, you should do or study what you enjoy. And you can help people! Make their life easier by automation repatitve and boring task :)",hrytn61,t3_rzqau2,1641762803.0,False
rzqau2,"While in school, one of my professors told me that Computer Science is ""the study of what can be computed"".",hrzkwk8,t3_rzqau2,1641772754.0,False
rzqau2,"It got me a job as a software developer and nearly doubled my income. Prior to this, I was a dance teacher - a career I was passionate about, until I realized I’d never be able to afford health care or save for retirement. I still dance, now just as a hobby.",hrztmd5,t3_rzqau2,1641776058.0,False
rzqau2,"Computer science is problem solving, you’ll learn how to apply mathematics, logic, and other concepts to solve complex/abstract problems that people working solutions to!",hs02rj4,t3_rzqau2,1641779750.0,False
rzqau2,"imo cs strengthens your problem solving skills. when you get good at problem solving, you can choose what problems you wish to solve. none of them ever have to deal with cs.",hs0w1ao,t3_rzqau2,1641791936.0,False
rzqau2,The way ive always interpreted and explained CS is as an umbrella term. It holds a ton of different concepts in it; everything from cyber security to mathematical theoreticals. Id say a majority of CS majors in college thought it was just programming until they did more research and realized it was so much more.,hs14ep3,t3_rzqau2,1641796339.0,False
rzqau2,"It sounds like need some Albert Camus, if you're not already aware of him.",hs14qiy,t3_rzqau2,1641796532.0,False
rzqau2,You can be useful to the world in a meaningful way using computer science since everything is digital nowadays. With tech you can help humanity propel towards its future.,hs1gyyd,t3_rzqau2,1641804613.0,False
rzqau2,"There is no meaning in life. Well, other then whatever meaning you give it. 

Some people choose religion as it's comfortable for them, some find other things like helping others, understanding the world, getting the most out of the 80 or so years one has (also I'm not saying these things are mutually exclusive)

I like CS, it's interesting. But I also use it as a vessel to get a job, or do other stuff.

 I'm mainly interested in game design and development but I also try to learn how game engines work, or make rudimentary ones in my own just to learn how they are engineered, the structure, the math, the complexity. 

At the end of the day, things only having as much meaning as you give to them.",hs1ha36,t3_rzqau2,1641804847.0,False
rzqau2,"For me computer science is part of mathematics. It helps us to describe the world we live in, it is a powerful tool for other sciences as well such as physics etc., where many calculations are done thanks to computers and so the whole field of computer science for me is about “automating” tasks and describing world around us using different perspective, similarly as other parts of science do. (Yes I know the debate on wether mathematics and compsci are actually a science or not, but that’s not my point here).",hs1hb0o,t3_rzqau2,1641804866.0,False
rzqau2,">I can’t quite grasp its purpose/use in the bigger meaning of life.

Theoretical CS concerns what kinds of mathematical operations can feasibly be realized in physical systems. Hard science traditionally understands physical systems as realizing mathematical functions, so discoveries in CS constrain what is feasible across the whole domain of the hard sciences.

On the other hand, CS programs were created by the computing industry to lower the costs of hiring programmers and executing computations. The field is based on very abstract mathematics with the wild implications described above, but largely designed to route students away from that stuff except as needed in business applications.",hs32161,t3_rzqau2,1641834264.0,False
rzqau2,"Computer science is the study of computers. 


For example:


- what can be automated using computers and how to do it as effectively as possible

- how to connect computers, servers, embedded systems, iot devices and mobilephones into something greater than the sum of their parts.

- making meaningful and valuable conclusions from big data, for advertisers and city planners for example.

- how to ""bring to life"" the gadgets and devices created by those in the information technology industry.



As for the purpose in life question, that is retarded. There is no objective purpose in life other than any subjective purpose you come up with.

Which is to say, why does anyone do anything? For the sheer boredom of course, to distract themselves of the fact that there truly is no meaning.




Also because they find it fun. Why does everything need to have a purpose or meaning?",hrwulw0,t3_rzqau2,1641735527.0,False
rzqau2,Computer science is not the study of computers.,hrx482x,t1_hrwulw0,1641740191.0,False
rzqau2,You're wasting your education.,hrxehec,t3_rzqau2,1641744345.0,False
rzcexe,"I agree with you that there is a lot of hype around NFTs and cryptocurrency in general. Many influencers are promoting NFTs and cryptocurrency and saying that it is a pyramid scheme without any knowledge of the subject of cryptography and distributed systems(blockchains).

Three entities are significant in the NFT world:
- File Hosting Service (IPFS, Google Drive, AWS S3, Dropbox)
- Smart Contracts on blockchains (Ethereum smart contracts using Solidity, Solana smart contracts using Rust, etc)
- NFT Marketplace (OpenSea, etc; they are the platforms that let the bidders bid and they glue together the above-mentioned entities to create their platform)

So let's imagine you are bidding for a cat NFT on OpenSea and finally buy that NFT from the creator after proposing the highest bid. All this bidding game is played on the platform. When you buy the NFT, the code inside a smart contract is triggered which adds a transaction into the blockchain. The transaction serves as proof that you have ""bought"" the particular NFT and are the legitimate owner of the same.

In reality, you do not ""own"" the NFT, you just own the link to the NFT image which is stored on another storage that may be centralized like Google Drive or decentralized like IPFS. If Google Drive decides to remove the particular image from their servers(which is unlikely but assume), you would get a 404 error.

I assume that you know about pointers in languages like C, Rust or Go. So I would explain the above phenomena with context to pointers. Imagine you have a variable `age` and you have a pointer to that variable `elonAge`.
```
int age = 60;
int *elonAge = &age;
```
Let's assume you have the access rights for `elonAge`, you have access rights to `age` but that doesn't mean that you cannot have any more pointers to `age`. You can have another pointer to that same variable called `muskAge`.
```
int age = 60;
int *elonAge = &age;
int *muskAge= &age;
```
You can read the data stored in `age` by dereferencing `elonAge` as well as `muskAge`.

Likewise, if you own a link to an image on the interwebs that doesn't mean that nobody else can't have access to that NFT image. A person named Geoffrey(forgive me if I misspelled the name) recently showed how this works by creating the [NFT bay](https://thenftbay.org) which is the NFT version of The Pirate Bay. 

He demonstrated that he can access the NFT *images* to which the owners have a link by downloading all the NFTs and torrenting them. I heard his talk in a podcast with Coffeezilla iirc. The NFT Bay is now banned in most countries though.

NFTs are created by hosting your artwork on a file hosting server and then hosting the link to the image on a blockchain via the logic written in a smart contract.

Edit: fixed formatting",hrxb9pq,t3_rzcexe,1641743093.0,False
rzcexe,Thank you! For explaining it really well!,hrxoo0n,t1_hrxb9pq,1641748224.0,True
rzcexe,"Suppose you're married. Everybody in the world is banging your wife, but you can't do anything about it. But you have the marriage certificate, right? That's the NFT.",hrvdg8p,t3_rzcexe,1641701185.0,False
rzcexe,Genius.,hrvgshu,t1_hrvdg8p,1641702784.0,False
rzcexe,"Ah, I wish I had a free award to give you today.",hrwt131,t1_hrvdg8p,1641734663.0,False
rzcexe,Sick world,hrxcqq3,t1_hrvdg8p,1641743679.0,False
rzcexe,A gentleman and a scholar,hryvn8y,t1_hrvdg8p,1641763542.0,False
rzcexe,[deleted],hrvgs9o,t1_hrvdg8p,1641702781.0,False
rzcexe,I guess that joke hit a little too close to home for some people,hrvr86j,t1_hrvgs9o,1641708415.0,False
rzcexe,"My apology for the ignorant joke. I think I will be a silent reader for a while on this sub to reflect and try making up for it 🙇‍♂

I also forgot that this sub is educational.",hrwlntc,t1_hrvr86j,1641730171.0,False
rzcexe,"To clarify, NFT's are not the image. NFT's are the hash which links to the image.

The images are then hosted on standard web hosting platforms. If those web platforms take the image down, then your NFT will no longer load the image. If the NFT links to a copyrighted material and there's a DCMA claim, then your NFT will no longer link to the image.

NFT's are really just a claim in the blockchain to own a piece. Anybody can right click and download the piece. Anybody could then do whatever they want with the image. Whether that claim to own the piece actually has value is in the ye of the beholder.  


The reason you don't actually put the image on the blockchain is it's prohibitively expensive. Each transaction of the blockchain has a cost, and storing an image is *way* to expensive as compared to just a link to the image.",hrukmf2,t3_rzcexe,1641688619.0,False
rzcexe,"So what is the point? It seems all hype and woo-woo bullshit to me, pushed mainly by people with a stake in getting rich off of it.

Why would anyone want to buy one? It's not like owning real art. So it's just...hype?",hrvgvzi,t1_hrukmf2,1641702833.0,False
rzcexe,"> So it’s just hype

Yup. It’s just people trying to ride the crypto wave by pushing this as a new way to collect art.",hrvn06m,t1_hrvgvzi,1641706043.0,False
rzcexe,"Ok. Cool. That was my take,  but wasn't sure if I just didn't get it.",hrvnkmt,t1_hrvn06m,1641706354.0,False
rzcexe,"I want to add that there are financial implications of NFTs as well. They're very hard to tax and with the hype around them, their prices inflate a lot. That means wealthy people can invest in NFT, drive prices up (Elon musk's dogecoin to the moon wasn't directly NFT related but still  a good example of crypto market inflation) then sell go turn a profit on something that can't be taxed. 

Of course it's less stable than holding money in a bank or traditional stocks, but since legislators don't really understand them you get away with more.",hrxcewl,t1_hrvnkmt,1641743554.0,False
rzcexe,"Generally, people dont understand what the NFT is. They just see the image and think that's the NFT.",hrvybwg,t1_hrvgvzi,1641712816.0,False
rzcexe,"When people explain NFTs with urls to images it kind of muddies the purpose of an NFT.  It's just independently verifiable digital ownership of something.  The ability to download that something is dependent on there being someone who can distribute it to you, but that doesn't have to be one single source.  Let's say you buy an NFT song from a music artist.  It's not a URL, it's just a code of some sort.  You might be able to download the song from Amazon music, iTunes, Bandcamp, or any number of other distributers if they have a system in place to verify ownership using your NFT.  And you could give/sell that NFT to someone else if you wanted to.  Obviously this is extremely beneficial to the consumer and not the distributer so it will probably never happen, but that doesn't mean the idea is pointless.

Unfortunately NFTs are pretty much only associated with digital art at this point and used to either make money or launder it.  All the interesting use cases seem to be things that are unlikely to happen because they mess with the bottom line of established services.",hrvnlk2,t1_hrvgvzi,1641706368.0,False
rzcexe,"To be technically correct, it's independently verifiable ownership of a few bytes of data stored on the blockchain, usually a URL. The only guarantee you get is that each NFT belongs to exactly one wallet. Here are some guarantees it can't make, however:

* That the person controlling the associated wallet legally owns the thing the NFT links to
* That any specific person controls the associated wallet
* That the data stored in the NFT actually represents something useful (Many of them are only meaningful in the context of a centralised website or service)
* That URLs in NFTs will always link to the same thing (or anything at all, in fact - many NFTs already link to dead pages)
* That trades involving this NFT weren't done through scams or hacks
* That there aren't multiple NFTs linking to the same thing, each belonging to a different person

If every step involved in making a transaction on the internet is a chain, then crypto is about making one link in that chain as strong as it possibly can be. Crypto people will tell you they have the strongest chain ever created and back it up by just describing that one link over and over again and trying their hardest to ignore anyone who points out that the rest of the chain is made of paper and wishful thinking",hrwme1v,t1_hrvnlk2,1641730669.0,False
rzcexe,"Some good points, but also I don't see anyone claiming that NFT represent legal ownership, or that people can't have their wallets hacked/scammed.

>That there aren't multiple NFTs linking to the same thing, each belonging to a different person

Yep and this is specifically a problem with NFTs for collectibles.  You have to trust that the people selling the NFTs won't just decide to sell more of that supposedly unique or rare item.  This wouldn't be a problem in the NFT music example I used.",hrx935w,t1_hrwme1v,1641742228.0,False
rzcexe,"> I don't see anyone claiming that NFT represent legal ownership, or that people can't have their wallets hacked/scammed.

No, they tend not to talk about this part",hrxi3ry,t1_hrx935w,1641745728.0,False
rzcexe,"It’s not really ownership either, because ownership is a legal concept and afaik no major country has ownership laws that recognize NFTs. Neither ownership laws for physical items nor intellectual property laws really fit to NFTs.",hs0x850,t1_hrvnlk2,1641792464.0,False
rzcexe,"Right, the laws haven't really caught up yet.",hs0yb0i,t1_hs0x850,1641792994.0,False
rzcexe,So if the NFT proves your ownership can you then license the song or image or whatever and make money off it? Can you sue someone who is infringing on your ownership?,hrxezm4,t1_hrvnlk2,1641744544.0,False
rzcexe,"No. It could be one part of a very large system we could develop to allow that, but that system doesn't exist today (well, it sort of does, it just works better and doesn't need crypto or blockchains to work), and NFTs themselves can't solve more than a very small amount of the problem on their own regardless.

Let's say you want the exclusive rights to publish a book. Imagine the US copyright office were to print out your registered copyright and mail it to you. The laws of physics prevent another person from having that exact same printed copy as you, and NFTs do the same for the digital version. The Copyright office could turn around tomorrow and print a second copy of the exact same document and mail it to someone else though, in which case it would be really hard to prove which one of you actually owned the rights to that book or whatever. And in the NFT world, we can just create a second token to the same URL and certify that someone else owns that one. The system prevents two people from having the same ""paper"" claiming ownership. It doesn't prevent two people from having different ""papers"" each claiming ownership to the same thing.

And that's just one problem. A second problem is that no one really recognizes any of this stuff as meaning anything. So maybe you are the only person who owns an NFT that supposedly represents that book. I can just sell copies of the book myself anyway, and there's no mechanism for you to stop me. No government or police agency cares about NFTs. 

You can solve these problems. You can empower some centralized broker to track and enforce everything. But then what's the point of the crypto aspect? If you have a trusted third party, just turn the mining farm off and be done with it. We've been doing all this stuff without crypto for hundreds of years. The US Copyright Office does in fact exist today, and unlike anything in NFTs, there are enforcement systems in place.",hrxn0ew,t1_hrxezm4,1641747596.0,False
rzcexe,"I think that's incorrect though. 
[Non-Fungible](https://nerdschalk.com/what-does-non-fungible-mean/) is a term that has been used in property law for many years, so in order for an **NFT** to be an **Non-Fungible Token**, it cannot by definition be duplicated. 

In your example of creating another token to link to the same URL as the one printed the day before is a really poor use case. I mean its barely worth linking a QR code to a URL. Because **NFTs** are unique objects, the only use cases that justifies their creation is representing or being those unique stores of perceived value - *in some cases an NFT actually endows a once Fungible object with Non-Fungibility*. 

This is why a lot of the use cases are in the arts and entertainment domains but *there are others*...I promise there are. 

What you mention though does make me think about the encoding of an **NFT**, particularly its association with a real-world object. What convinces the guy who just bought an expensive NFT, that another token won't be made the next day to link to his new asset (represented by his token)? 

The fact that the blockchain is immutable, cannot be altered in any way and if someone were to try linking another token to the asset, it would result in some kind of error (probably logical) because there'd already be an association registered on the [distributed ledger](https://www.techopedia.com/definition/30246/blockchain) with whatever that asset was. In the cases where an NFT only represents a fraction of an asset, those details would already be coded into the ledger even prior to all of the tokens being *minted*. I don't know though, this is an assumption from my mental model I've yet to verify.

I like art, art is cool and I think it's necessary for society but the use cases that excite me are the ones in which actually, an NFT can now come to represent a **title deed** or someones **last will and testament** and negate the ever present risk of the deeds building burning down or military coup resulting in a total rewrite of relevant country's land ownership.

Plus...I think the primary purpose of the blockchain and its offshoots is to enable peer to peer transactions on an industrial scale without that 3rd party, that almost always adds nothing to the value/utility of the transaction except for a fee and other things. I get that they were/are necessary for providing **the trust variable** to a deal between strangers, but over the years they have gotten a bit too ""big for their boots"", stretching their original mandate and becoming more of a pervasive force so some change in that regard would be nice (namely decentralization which I approximate as more power to the people, and stronger links between them).",hrytiej,t1_hrxn0ew,1641762753.0,False
rzcexe,"There's no error because the asset isn't the NFT or the URL. The asset is the thing hosted at the URL, and I can duplicate the shit out of that, host out at my own URL, and have an NFT showing I own mine.",hryv2mf,t1_hrytiej,1641763330.0,False
rzcexe,"In that example I don't mean you own the rights to do whatever you want with it.  You own an authorized digital copy of the song that you can download and listen to.

But in the case of using NFTs to own a license, there would need to be laws in place to protect that, if it isn't already covered.",hrxgvyu,t1_hrxezm4,1641745278.0,False
rzcexe,"I'm still confused. If the NFT doesn't prove that you own rights to the underlying thing, what do you own? What's to stop additional perfect digital copies of the exact same thing from getting ""authorized"" and sold?",hrzf1a8,t1_hrxgvyu,1641770554.0,False
rzcexe,"So take NBA Top Shot for example.  They take a highlight of an NBA player in a game and mint that moment.  But they don't just mint one NFT for it, they mint thousands.  And then they put them in highlight packs(think trading cards) and people buy the packs.  There will be thousands of owners of that one NBA highlight.  Each owner has a unique token that verifies that they do own a copy of it.  The even though every token is unique, they are still identifiable as being linked to that one NBA highlight.  The NBA Top Shots marketplace can verify this and allow people to sell the NFTs they own.

The common misconception is that since the NFT is unique, then the thing it represents ownership of is only going tied to that NFT.  In reality, the minter of the NFT you are buying might promise that it's the only NFT being minted for that thing, but they could just lie and mint more of them.  But then they sever any trust they have with their audience and effectively kill their own brand.  And there are may be protocols in place in NFT marketplaces to prevent that(idk I've never used one), but that's not inherit to all NFTs.",hrzps4l,t1_hrzf1a8,1641774613.0,False
rzcexe,That makes more sense. Thanks for explaining it. I realize I don't think I ever want to buy one though lol.,hs07s7v,t1_hrzps4l,1641781775.0,False
rzcexe,Also a related sports memorabilia anecdote I thought of. I had been saving all my baseball and basketball cards from the 90s for the last 30 years through 4 states and about a dozen moves because I thought they would become more valuable over time. I just gave away the whole lot to a collector because they weren't really worth the time to go through them and try to sell cards for a buck or two. I have a feeling NFTs are going to end up in the same dustbin of history. But I'm often wrong about things like this so they could go the other way too.,hs9uadn,t1_hrzps4l,1641941879.0,False
rzcexe,"Some expensive art sits in warehouses and gets traded without anyone looking at it. It's all for speculation, and money laundering.",hrwbwqd,t1_hrvgvzi,1641722787.0,False
rzcexe,"NFTs can also broadly describe anything that has a sense of uniqueness. For example, some games use NFTs to represent in game objects so players can buy/obtain those NFTs and trade them with other players.",hrvobpy,t1_hrvgvzi,1641706765.0,False
rzcexe,"Which is generally pretty pointless. Remember, NFTs aren't just bits that say you own something. They're part of a blockchain system, and the whole point of a blockchain is decentralized control. 

If you have a trusted third party, you don't need crypto, blockchains, or NFTs even in the best of cases. It's the world's stupidest way to implement

    update owners set owner_id=9876543 where item_id=1234567

in the third party's system. And what sort of practical game do you expect to support players trading rights to items in a way that it's important that there be no trusted third party. The entire world is running on a trusted third party system. If you don't trust the game server, nothing matters anyway.

The only way you're really going to use this is just to say you did. It's basically performance art to make a game that uses NFTs to track ownership.",hrxpc0v,t1_hrvobpy,1641748476.0,False
rzcexe,"Perhaps a game item isn't the best example. ¯\_(ツ)_/¯
I'm a huge crypto nerd and I like the space a lot. So I'm biased. I see a lot of utility in NFTs though not really as jpgs or game items. I think those sorts of NFT projects are fun but they're really just digital status symbols at best and Ponzi schemes at worst.

There's certainly NFTs with utility that I find very interesting.

One project I've been looking into utilizing is the Superfluid Protocol which allows you to create an NFT that serves as a landing point for a stream of money. You can create a stream of funds (say the stream sends .000001 ETH per second or something) between address A and address B. If address B is an NFT, you can trade that NFT to another person and they will begin collecting that stream of ETH every second.",hrxuni4,t1_hrxpc0v,1641750443.0,False
rzcexe,I don’t understand why would you want to generate such streams of money? Are you thinking  of such things like bond payments?,hryrxe2,t1_hrxuni4,1641762195.0,False
rzcexe,"There's tons of different use cases. One example that superfluid protocol has on their monorepo is the NFT Billboard. They have an ad space on a website. Initially this ad is blank. A user visiting this website sees a ""See your ad here by initiating a stream!"" You click on the button and initiate a stream. You have that ad there until another user comes along and opens a stream at a higher rate towards the ad space. If another user opens a stream that is at a higher rate than yours, yours automatically stops and your ad stops showing.

They also have some other examples for auctions, lotteries and call options utilizing streams but I haven't explored them too much.",hrz8ada,t1_hryrxe2,1641768065.0,False
rzcexe,"The only important value is perceived value

Also money laundering",hrwiylv,t1_hrvgvzi,1641728240.0,False
rzcexe,">It's not like owning real art.

How so? Why do people own original art in the digital age of perfect replicas? What's the intrinsic value of an original piece of art over a print/copy?",hrycmh4,t1_hrvgvzi,1641756805.0,False
rzcexe,"I'd say you're overlooking ways in which it is quite similar to the art world. The original Mona Lisa is deemed to be of immense value by society, but now consider a copy which no average person could tell was different from the Mona Lisa. This is inexpensive to make, and would probably sell for a couple hundred dollars. Based on this, we can see that tens of millions of dollars of the Mona Lisa's monetary value aren't contained in how it looks. Rather, they're contained in that it is THE original. Using NFTs, one can create a similar ""THE original"" property for a piece of digital art. Namely, they issue a single token for the data of their art. From then on, the owner will have cryptographic-blockchain proof that they are the sole owner of the original token.

Now, considering that NFTs are like art, that makes it so they are a very natural space for bullshit and hype. However, this is just the same phenomenon that takes place in the modern art world. A painted red canvas is worthless, UNLESS it was painted by this one famous person, for example. I personally think that valuing something for being THE original, rather than how it looks, is irrational, both in NFT space, and in real-art space. However it isn't like NFTs are all built on nothing and will pass as a phase. There is just as much merit in valuing NFTs (and therefore reason to expect them to continue to be valuable) as there is in valuing original pieces of art. One can further distinguish whether the NFT has artistic value (ie the apes vs some world renowned digital artist's painting). This is just equivalent to the distinction between a collectible card and a piece of art.

Lastly, it's true that much of paintings' value (but very little proportionally in the case of the immensely valuable ones) is from looking cool. NFTs don't contain any of this value, whereas physical art bundles together the two types of value. This is a meaningful distinction. However, to reject the value of NFTs is to reject the second type of value art has. I reject both as a rational thing, but am willing to accept that society will consistently value the latter, making investment in it feasible.",hryduem,t1_hrvgvzi,1641757243.0,False
rzcexe,"I'm somewhat new to the computer science field but my understanding is that NFTs (or smart contracts in general) use a different back end. Instead of hosting on a back end service provider like AWS, they use IPFS (Interplanetary file system) which is similar to a file sharing service like bittorrent. The idea being that as long as 1 person on that network is hosting that file you'll always be able to retrieve it using your digital claim to the NFT address essentially.

But yes, the prices are 100% speculation with some hype sprinkled on top",hrwscun,t1_hrvgvzi,1641734283.0,False
rzcexe,">put the image on the blockchain

There is more than one blockchain, isn't it ?

Afaik, every crypto currency uses its own blockchain. Which one is used for NFT ?",hrwf5k3,t1_hrukmf2,1641725328.0,False
rzcexe,Ethereum,hrwmhlp,t1_hrwf5k3,1641730733.0,False
rzcexe,"Every cryptocurrency does not use it's own blockchain network. There's a lot of blockchain networks though - Ethereum, Polygon, Solana, Avalanche, Fantom, just to name a few. And some of them aren't even true ""blockchains"".

You can host NFTs on any of the above networks and many more.",hrz8usw,t1_hrwf5k3,1641768274.0,False
rzcexe,"I'll add many people see nfts as solely images or hashes but that's definitely not the piece with the most use cases.

- NFTs have different types and aren't all just meant for images",hryg3fk,t1_hrukmf2,1641758042.0,False
rzcexe,This is such nonsense. I honestly cannot wait for interest rates to rise to wash away all the BS and zombie companies doing absolutely useless work.,hrw5oi8,t1_hrukmf2,1641718041.0,False
rzcexe,"What about the comparison with game skins? Csgo, people open chests and sometimes you can open an ultra rare skin worth a lot of money.

The skin itself is owned and hosted by the company valve, so what does the owner of the skin actually own? Only the 'link' to the skin for their use.

What brings that Csgo skin value?

I think based on game cosmetics existing and being worth money in our current environment, it is proof that there is some use case for NFTs.

Whether that's true for real life applications to things like art collection and celebrity autographs remains to be seen, but I think it's a really ignorant thing to make a general statement like 'NFTs are a scam' because you don't actually 'own' anything because it lives on a domain outside of your control.

The same theory on nfts being a scam could also be applied to money and online banking. Inb4 people start screaming at me that the world economy is going to collapse because everything is fake.",hrwqde1,t1_hrukmf2,1641733134.0,False
rzcexe,"Ok, sure, you could have an NFT that links to a game skin, but ultimately CS:GO could still decide if it wants to let you use the skin in game - which defeats the purpose of the decentralization if a centralized authority could have the final say in the end. In a NFT game item platform you wouldn’t have ownership of your items in a meaningful way.",hrwz5kk,t1_hrwqde1,1641737824.0,False
rzcexe,"> The skin itself is owned and hosted by the company valve, so what does the owner of the skin actually own? Only the 'link' to the skin for their use.

The skin would not exist without Valve, and ownership is maintained at a significantly lower cost (both for the company, and in terms of electricity/compute usage). It is literally impossible for other people to use your skins without you giving it to them, whereas an NFT tells everyone that you ""own"" some link, but does nothing to actually enforce the practical things we usually care about with ownership.

> What brings that Csgo skin value?

The skin itself is valuable.

> I think based on game cosmetics existing and being worth money in our current environment, it is proof that there is some use case for NFTs.

People have bought into all sorts of bubbles in the past that ended up worthless, or into financial schemes that ended up being scams. Why shouldn't NFTs be the same?

> The same theory on nfts being a scam could also be applied to money and online banking. Inb4 people start screaming at me that the world economy is going to collapse because everything is fake.

NFTs aren't a scam because people realize that we made up society ourselves, NFTs are a scam because people make big promises beyond what the actual technology is doing for you. You have to buy into lies or be stupid to think NFTs are worth your money, or be [one of the people actually making money off of the scam](https://www.bloomberg.com/news/articles/2021-12-06/small-group-is-reaping-most-of-the-gains-on-nfts-study-shows).",hrx0vpu,t1_hrwqde1,1641738660.0,False
rzcexe,"Skins have no value and are only bought by children with no concept of money and ""whales""  people who are incredibly rich and $5000 is the same to them as a penny to me, or people hopelessly addicted to the game who bankrupt themselves on it.  Its not every players buys $30 of skins, it's 2% of players spend $300,000 on skins. They are not a viable business model.  Europe is legislating them out of legality, and hopefully he rest of the world follows them.",hrx6q02,t1_hrwqde1,1641741268.0,False
rzcexe,"Are you suggesting that a twenty and two fives is worth less if it came from a rich person with questionable taste than if I got it from ""normal"" people? 

$30 is $30. You seem to be ranting against the concept that the market value of something is a concept that exists, and things are actually worth what you feel they should be worth instead. Not sure what to say to that.",hrxqlbi,t1_hrx6q02,1641748946.0,False
rzcexe,"No im suggesting that 

>They are not a viable business model.  Europe is legislating them out of legality, and hopefully he rest of the world follows them.",hry7jkd,t1_hrxqlbi,1641755029.0,False
rzcexe,"See [my other comment](https://www.reddit.com/r/computerscience/comments/rzcexe/can_anyone_explain_nfts_from_computer_science/hrxpc0v/). 

Digital bits that represent exclusive ownership of digital assets absolutely have value. But nothing in your example benefits from a blockchain. Valve can and should just store the fact that you own that very rare skin through a normal database record like any other system.",hrxpymr,t1_hrwqde1,1641748713.0,False
rzcexe,"Never said it was a scam. I said 

> whether that claim has value is in the eye of the beholder

When I buy a CSGO skin though, nobody can use that skin. That's why people buy them, to show them off in-game.

When I buy an NFT, somebody can just right click and download the image. Sure, it's not the NFT. How many people know that, how many will verify I don't own the digital rights to the image? A tiny fraction.",hry06kh,t1_hrwqde1,1641752457.0,False
rzcexe,"I said the idea of applying NFTs to things like art collection is questionable to me, but just as you can buy a replica of a Monet painting and it is worthless, there is an idea behind the value of an NFT that is derived by participating parties willingly participating in the system.

I am not saying I agree with the idea, but what I mean is that I think I can understand why people are buying into it and I can see the potential for success.

When I originally got into bitcoin many many years ago, people kept laughing at me and nobody took me seriously when I tried to get friends to buy bitcoin.

The concept of having purely digital money was absurd, 'like how do you know someone won't just change the bits in your wallet and then you have nothing?'

It was a poor understanding of abstract values like what really made the cash in their wallets able to give them purchasing power moreso than the idea of blockchain itself, so.... Yea, what I wanted to point out in my first comment was that it's too early to say anything bad about NFTs. The jury is still out, but if there is any area blockchain could be applied, it's areas like art and creative content where value is completely immeasurable.

Would you pay millions of dollars for some paint / carbon molecules on a canvas? It sounds absurd, but some people do.",hs0l5gr,t1_hry06kh,1641787291.0,False
rzcexe,Usually the NFTs are stored on IPFS instead webservers so nobody can take it down,hrwbtl1,t1_hrukmf2,1641722719.0,False
rzcexe,Usually? I’m pretty sure that’s the exception to the rule.,hrwmgw7,t1_hrwbtl1,1641730720.0,False
rzcexe,"I couldn't tell you the percentage. All I know is there are a fair amount still hosted on traditional media sites given the amount of NFT's that ""disappear"".",hry19ly,t1_hrwbtl1,1641752836.0,False
rzcexe,Interesting to even know that putting an image on blockchain is possible. I did not know this.,hrxork6,t1_hrukmf2,1641748261.0,True
rzcexe,"You can put _anything_ on a blockchain. It's just a matter of cost and time, so some things just aren't _practical_ to put on a blockchain.",hryh4ly,t1_hrxork6,1641758408.0,False
rzcexe,I think there’s a maximum block size which limits what you could put on.,hrysha3,t1_hryh4ly,1641762387.0,False
rzcexe,You can always create a blockchain with a larger block size though. It's just a matter of practicality.,hryy728,t1_hrysha3,1641764442.0,False
rzcexe,"Sure but that’s the practical aspect of it. The size of the whole chain needs to be a practical size since anyone who wants to verify it needs to hold the whole damn thing.

There’s limits on how big the block size could be, due to the fact we are limited in the real world.",hryylrx,t1_hryy728,1641764587.0,False
rzcexe,Which is why the actual images are never stored on the Blockchain.,hrz4ydc,t1_hrysha3,1641766848.0,False
rzcexe,"> The reason you don't actually put the image on the blockchain is it's prohibitively expensive. Each transaction of the blockchain has a cost, and storing an image is way to expensive as compared to just a link to the image.

Math/TCS guy here :) i've been looking more into blockchain's it seems like some of them don't have the same transaction's costs as Ethereum/Bitcoin such as Cardano. Honestly instead of NFT's if it where custom-created trading card's put on the blockchain I could see it it working",hrycj9g,t1_hrukmf2,1641756773.0,False
rzcexe,"A dumb question, but if ur NFT was took off by hosting platform, does that mean ur money just disappeare too?",hs1ob5y,t1_hrukmf2,1641810175.0,False
rzcexe,There is no hosting platform for the NFT. It's stored on whatever blockchain. Are you referring to the image?,hs5nfam,t1_hs1ob5y,1641870292.0,False
rzcexe,yes that's what I mean. sorry for my stupidity,hs69qfz,t1_hs5nfam,1641880630.0,False
rzcexe,"I'd assume that it losses value, though it depends on whether buyers think they're buying image or understand what they're actually buying.",hs6f7mi,t1_hs69qfz,1641884047.0,False
rzcexe,Thanks!,hs79spp,t1_hs6f7mi,1641906340.0,False
rzcexe,You are buying a hash of an image combined with a URL to a web server of the image. That’s it. They load the hash to the blockchain for ownership record.,hrutfqe,t3_rzcexe,1641692342.0,False
rzcexe,"Minus the “hash of an image” bit.

Here’s a guide: https://docs.alchemy.com/alchemy/tutorials/how-to-create-an-nft",hrv7qzw,t1_hrutfqe,1641698589.0,False
rzcexe,"If they don't use a hash of the linked content, is there anything preventing someone from creating a new NFT of an existing NFT's image reuploaded?",hrvm0i5,t1_hrv7qzw,1641705503.0,False
rzcexe,"There’s literally nothing stopping you from doing exactly that. There’s also nothing stopping the server that hosts the image from changing said image, deleting said image, breaking the link, or shutting down. 
An NFT is just a url that lives in the blockchain",hrvy1gu,t1_hrvm0i5,1641712623.0,False
rzcexe,There are NFTs of unhashable things as well. Like that Banksy painting that got burned after minting.,hrwc2ql,t1_hrvy1gu,1641722916.0,False
rzcexe,"I swore they used a hash to create an ID. Oh well, just use Opensea and upload your stuff.",hrv9lx6,t1_hrv7qzw,1641699429.0,False
rzcexe,"Lmao why not actually hash the image? Also, if it's just on ropsten, that's even worse.",hrx2e3p,t1_hrv7qzw,1641739367.0,False
rzcexe,Images are just scratching the surface of what you can do with NFT’s.,hrwy6cw,t1_hrutfqe,1641737337.0,False
rzcexe,"I also have a question.

The price of the NFTs are defined by supply and demand. If a lot of people buy a specific stock, for example, there's a lot of demand for that stock and the price will automatically go up. That's because people are buying it.

But, if NFTs are unique, how exactly the supply and demand are defined?",hrvqhnu,t3_rzcexe,1641707990.0,False
rzcexe,"The same as with traditional unique items, like art pieces or unique stamps or John Lennon's glasses etc. Supply and demand still applies, but supply is 1. Therefore the price will become whatever the highest bidder is willing to pay.",hrw9pzc,t1_hrvqhnu,1641721072.0,False
rzcexe,"That is precisely the ""non fungible"" part. Stocks are not unique. It makes no difference which particular share you haber in a company.",hrx8rje,t1_hrw9pzc,1641742098.0,False
rzcexe,For most projects there’s like 5000 pieces that are randomly generated. Some are more rare than others because of traits being more rare.,hrxwwg0,t1_hrvqhnu,1641751276.0,False
rzcexe,"The technical part of it is very similar to the idea of cryptocurrency (and works on the same technology of storing a ""ledger"" in a decentralized, online, data structure).  
The thing that distinguishes NFT from cryptocurrencies is that in NFT we give (completely theoretical) value to each individual ""link"" in the chain, whereas in cryptocurrency we give value to a numerical amount of ""links"" like we would with regular currency.

There is no real authority for NFT market shoppers to claim ownership of digital data, that ownership is not technically recognized by any institution in the world (AFAIK) and so basically people who participate in that market just choose to acknowledge that ownership (I believe that mostly because the media decided to play along out of greed, and failed to inform people that it's pretty much make believe, it's like issuing a certificate of authenticity for a molecule of oxygen, sure you could probably capture it, and you might even be able to prove that the tube you keep it in actually holds that same molecule, but as long as no country acknowledges your ability to own said molecule, it's meaningless.

Now, when talking about cryptocurrencies (say Bitcoin) there is usually some form of computational power required to operate the chain, store it's data and deliver it, so if you join in that effort you are essentially rewarded with some fraction of the currency (at least nowadays). The original idea, as I understand it, was a formula (take for example the calculation of pi), that was very computationally intensive, and if you joined in, you had a chance to be the computer that actually got the next number of pi in the calculation - this was called ""Mining"". That number was unique (in terms of it's position in the ""chain"") so when you got the credit for it, you could actually say that your bitcoin is bitcoin number XXX.

When talking about NFT's, pretty much anyone can choose to create a new NFT out of any unique identifier of digital data (specifically hash value), that unique value is stored on the chain, and so when it's ownership transfers it is registered on that chain - that chain of ownership is what the economy of NFTs is based on, you want to own something (as intangible as it is) that was owned by someone else important, or that has some digital history to it, and with blockchain that history, and connection to the original hash value is verifiable",hrwuoth,t3_rzcexe,1641735571.0,False
rzcexe,NFT’s most basic and pointless aspect is a art photo the real use would be proof of ownership for a car or a house or a concert ticket,hrvoajn,t3_rzcexe,1641706747.0,False
rzcexe,Thank you for this. Everyone gets so caught up in hating nfts bc of the shitty apes and I think its stopping them from realizing there's actual use cases for them out there,hrxktoz,t1_hrvoajn,1641746767.0,False
rzcexe,"So what’s the use case?  Let’s say there is a blockchain out there where I own an NFT that “points” to a car.  Does it point to the VIN number or something else?  

Now let’s say someone steals my car.  Now what?  Do I call the police?  If so, how has the addition of a blockchain improved this situation in any meaningful way?  

So let’s say the officer asks for my title to verify that I actually own the vehicle.  Now I fire up meta mask and show the officer my NFT.  That officer laughs his ass off and leaves.

Of course you may be thinking that in this hypothetical world that we’re talking about, the legal system recognizes NFTs as as legally binding ownership of arbitrary assets and investigates the theft and ends up finding my car and returning it to me.  What advantages did having my ownership residing on the blockchain give me that we don’t currently have?",hs1ymw9,t1_hrxktoz,1641817298.0,False
rzcexe,Is there any reason that all has to be in a decentralized system like blockchain? My local government has a big database of car registrations/ownership that seems to work fine. I just don’t see what problem these blockchain solutions are actually solving,hrxymhu,t1_hrvoajn,1641751904.0,False
rzcexe,"Ever lost your car title? It’s a pain and cost money to replace NFT solves this. 

Ever seen a company get hacked with ransom ware and not be able to anything? NFT data can’t be hacked. 

Ever lost your concert tickets NFT solves that too. 

Decentralized is always better no matter what the application is.",hry26dx,t1_hrxymhu,1641753158.0,False
rzcexe,"NFT doesn’t solve it; because you loose you title for your car you can apply to get another one. You lose the private key to the NFT; you’re ducked.

Same with a concert ticket, NFT doesn’t help.

Decentralisation moves the burden of security from companies or governments with resources, to individuals without resources or knowledge . Not only that, it removes any ability to fix issues. You’ve lost your key to NFT, Bitcoin, you’re shit out of luck.  Someone breaks into your wallet somehow, boom everything is gone. 

Decentralisation is always worse, no matter the application.

Also, NFT can’t be hacked! Come on! Security is always a shifting window; no Cryptographic process is 100% secure; which is why key lengths increase every so often.",hryv6qh,t1_hry26dx,1641763373.0,False
rzcexe,Umm did you actually say the blockchain can be hacked? 🤦‍♂️,hryya9m,t1_hryv6qh,1641764473.0,False
rzcexe,"Yep. Everything can be hacked.

If you think something is 100% secure and always will be - oh sweet summer child.

There’s a great story in either applied cryptography or secrets and lies. It’s about the DES algorithm; this was developed by IBM, but was sent to the NSA. They sent it back with some modifications, but nobody knew *why* they were needed. Years later, a researcher published  new form of attack - which it transpired that DES in its original form was more suspectable to. The NSA had modified it because *it already knew about* the attack.

The question is; what do you think the NSA knows about the algorithms utilised in the current implementations? Who knows, but I’m sure it’s more than the outside world.

The network is *secure* as long as enough nodes aren’t compromised; if enough nodes are compromised then the network is foo barred. The network has to be large enough to reduce the possibility that a single attacker could compromise enough of it. We know that computers can be hacked (it was one of your reasons for liking NFTs) so these nodes could be compromised. It would take a huge amount of resources to comprise enough nodes - and this isn’t worth it. But it’s possible.

However the likely attack is attacking the end user, stealing the wallet and all it contains. What’s brilliant about this; it’s it’s totally your fault and all your money/possessions are now irreversibly lost. 

Like I say nothing is 100% secure. To claim otherwise is just false.",hrz1vzg,t1_hryya9m,1641765754.0,False
rzcexe,If you think the blockchain can be hacked you must not understand how it works 😂,hrz22pb,t1_hrz1vzg,1641765821.0,False
rzcexe,"That’s weird; having read the original white paper, and done a few courses on it, pretty sure I understand it.

Unlike yourself who providing absolutely no substance to your claims. Just single sentences and emojis.

The network is secure only if enough nodes reach consensus on which is the correct chain; the security of the network is based on it being infeasible to compromise enough nodes; or indeed to expensive to flood the network with new nodes.

The security of it; completely depends on trusting that enough people are honest. 

So yes it can be hacked, but doing so would require so many resources that really only a government could do it. Although potentially this would cause a fork, followed by a complete loss of trust in the network.",hrz4smj,t1_hrz22pb,1641766791.0,False
rzcexe,😂 yeah take over the ETH network you only need a small amount of computers from what I’ve read,hrzmymc,t1_hrz4smj,1641773535.0,False
rzcexe,Isn’t there a legal aspect that needs to be in play for NFTs to actually enforce ownership over physical goods?,hsgbqyx,t1_hrvoajn,1642052484.0,False
rzcexe,Is there a decentralized place yet to store the image the hash represents?,hrvv596,t3_rzcexe,1641710740.0,False
rzcexe,You could put it on IPFS: [https://ipfs.io/](https://ipfs.io/).,hrwkpcc,t1_hrvv596,1641729489.0,False
rzcexe,Yeah. Look into IPFS. https://IPFS.io,hrwkw4r,t1_hrvv596,1641729625.0,False
rzcexe,"NFT means non-fungible token, meaning any type of crypto token that is unique/distinguishable. Add opposed to fungible tokens which are completely interchangeable - you can't tell one bitcoin apart from another.

The use of NFTs to hold links to JPEGs is just one possible use of the technology, and not as particularly imaginative one. In future, NFTs could be used to represent things like property deeds, event tickets and much more.",hrxb5sq,t3_rzcexe,1641743049.0,False
rzcexe,"NFTs are a type of smart contract. They let you buy and sell ownership of an asset (physical or digital) using the blockchain. The first owner creates an NFT and issues a licence that states whoever owns the NFT owns the thing. The code for the NFT then uses the blockchain  to ensure that only one person can be the owner at any time.

They're overhyped at the moment, but so was the Internet in its early days (dot-com boom) and nobody would deny now the Internet is important. The basic idea - buying and selling stuff other than cryptocurrency over the blockchain - is a good one.

The code for an NFT can be very simple - you can write one in 14 lines. Have a state that keeps track of the owner. Provide an API that transfers ownership if buyer and seller both authorize the transaction, and answers the question ""Who is the owner?"" at any time. The difficult part - keeping this all synchronized globally - is delegated to the blockchain.

For a good, relatively hype-free overview, here's the page on NFTs on Ethereum.org:

https://ethereum.org/en/nft

And for the gory details, here's the spec:

https://ethereum.org/en/developers/docs/standards/tokens/erc-721/

https://eips.ethereum.org/EIPS/eip-721",hruxtwc,t3_rzcexe,1641694234.0,False
rzcexe,"I would recommend learning at least a little bit of the science/cryptography of/in/for blockchain/hashing if you're into CS, but not actually buying these. 

Just invest in businesses that will generate money (with good causes), or your home, or your life, or just be a philanthropist if you're f-ing rich, rather than buying these elitist BS.

Sorry, for ranting. I just don't support NFTs.",hrvizms,t1_hruxtwc,1641703898.0,False
rzcexe,"I don’t necessarily think it’s any different than owning a skin on a video game in principle. I don’t really know a ton about them and I likely will never buy one, but you buy a skin to show off that you own the cool looking skin the same way someone buys an nft to show off owning a cool looking image.",hrvl72j,t1_hrvizms,1641705061.0,False
rzcexe,[deleted],hrvptvn,t1_hrvl72j,1641707612.0,False
rzcexe,"Yeah but it’s about owning the skin not just using it. A lot of games you can use the skin client side but nobody sees that you have it but people still buy them. 

  I am definitely more towards people can do what they want with their money. Sure there’s things they could do with their money that are more valuable to me, but I have no position to tell them that they should make that choice",hrvqzhp,t1_hrvptvn,1641708276.0,False
rzcexe,[deleted],hrvstef,t1_hrvqzhp,1641709340.0,False
rzcexe,"Yeah for sure i feel you, i’m talking more of like as principle for me. I get where you’re coming from on your perspective though it’s definitely just as valid",hrvzxdm,t1_hrvstef,1641713909.0,False
rzcexe,"100% agree. It has become so hard to learn about the CS side of NFT with all the hype. None of the top results on google come up with any CS info these days. It is astonishing how the world took cryptography topic and turned it into a such a big marketing hype.

None of my cryptography focused friends even want to talk about cryptocurriences or NFTs. They are pissed of at what it's become lol.",hrxp248,t1_hrvizms,1641748372.0,True
rzcexe,"I would just say don't let other people spoil something for you. If others want to look and talk about the pretty window frame, let them talk about the pretty frame. If you know there's something more to it, don't let your ego pull you away from moving closer, cupping your hands around your eyes (to block out the glare from all the flash photography the others are doing) and look into that window bro. There's some real s*** inside. 
And also it might actually be a plan to get people blockchain weary, by emphasizing the big-finance aspect enough so that's all that people see, having them ignore its potential to disrupt the status quo and fundamentally change the balance of power.  
Some people love the status quo. Bankers like being bankers and being depended on. Don't know if I'm making sense. It's 00:32 here, I should probably go to sleep now.",hrz6zzk,t1_hrxp248,1641767592.0,False
rzcexe,Thank you for the links!,hrxpaf0,t1_hruxtwc,1641748460.0,True
rzcexe,Key value paid loaded with a GUID and whatever the digital object you just bought was.,hrvz1sz,t3_rzcexe,1641713303.0,False
rzcexe,it's a fraud,hrw9g8l,t3_rzcexe,1641720868.0,False
rzcexe,From what I have gleaned the hash is taken including the image content itself (not just a link). Therefore the image can be hosted anywhere or move to a new location and since the content is the same the “link” is still intact. For small images I think they can be inserted into the blockchain itself so many parties can store it. However this seems to cost transaction fees so may be prohibitively expensive for images of non-trivial size.,hru88yf,t3_rzcexe,1641683494.0,False
rzcexe,The first nft was a number.,hrvfvac,t3_rzcexe,1641702331.0,False
rzcexe,"I recently took a blockchains course and from my understanding NFT's are powered by smart contracts mostly on the ethereum network. Most smart contracts are written using Solidity but they can really be written in any language. There's an online editor called Remix which you can use to write in Solidity. The smart contract is placed on the blockchain where it is immutable; however, there are ways for your NFT to be stolen. 

remix.ethereum.org",hruxxcb,t3_rzcexe,1641694276.0,False
rzcexe,"You got 1 or 2 CS related explanations, the rest are basically garbage hype/anti-hype.  Good try OP.",hry3xso,t3_rzcexe,1641753771.0,False
rzcexe,"Could someone post an example of where I could find the URL to an NFT within the NFT Contract?

For example, this is an [NTF page on OpenSea](https://opensea.io/assets/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d/8322) which costs 97 eth (or, $307,000). I can expand the Details to find the [Contract Address](https://etherscan.io/address/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d#code) or [Metadata](https://ipfs.io/ipfs/QmeSjSinHpPnmXmspMjwiXyN6zS4E9zccariGR3jxcaWtq/8322), or I could right-click on the image and [open in a new tab](https://lh3.googleusercontent.com/qtXorUoQU99DUrPCj694omQp_8_SZNfP4WPSE5LhjXPP9MvzA0-Y9ZjVQWDwgdrc2Otr5PfLp0pZCLHWiYgGJY6g3UHqflOt9K1RIw=w600) (which is simply served by a CDN).

The Metadata page lists an ""image"" key as containing the value `ipfs://QmXffx3vPcYVTUyQzxYWjkAnePQ4LuPB5vpzdDiqHqxcnm`. So, is that ""image"" metadata element a pointer to a URL with the transport language as ipfs:// ? If so, what is ipfs:// and how does that relate to an HTTP server that hosts the image?

And if that ipfs:// pointer indicates a location of the content, how is that tied into the owner of the NFT?

Edits: I'm recovering from covid and I can't seem to type NFT correctly each time (sometimes it comes out as NTF, other times it's NFL)",hry8rs6,t3_rzcexe,1641755448.0,False
rzcexe,"Not an answer, but a great [write up](https://moxie.org/2022/01/07/web3-first-impressions.html) by moxie (creator of Signal) exploring NFTs and web3. He demonstrates how the image you bid on is not necessarily the image you get.",hry9qe2,t3_rzcexe,1641755786.0,False
rzcexe,"I do think NFTs are overhyped, but I also think many are missing the point. The potential value of an NFT is basically the price difference between original art and a replica that is *artistically* identical.

Why do people buy original art? There is more than one reason: supporting an artist, money laundering, or simply prestige are a few reasons.

It's intellectually inconsistent in my opinion to think NFTs are worthless but original art / artifacts should be worth more than replicas.",hryf3j1,t3_rzcexe,1641757684.0,False
rzcexe,"The idea and the possibilities are cool, but the fact that image is not saved on blockchain is the most ludicrous thing ever. It does have good use cases though. (e.g. Tickets, patents, legal documents, in-game assets, helping artists collect royalties from usages)",hrwad64,t3_rzcexe,1641721584.0,False
rzcexe,Why would saving the image on the blockchain matter?,hrwlrsp,t1_hrwad64,1641730250.0,False
rzcexe,"Because when just the url saved on the blockchain, if the image is taken down, you can say bye to your art piece. Some systems also save the hash, but hashes are not 1-to-1 functions. Same hash could be generated by multiple images, albeit not easy to do so.",hrwma6o,t1_hrwlrsp,1641730596.0,False
s011f6,"You know Linux was/is also evolving and wasn't as it is today from the beginning?  
Nowadays, legions of programmers (many from big corporations) contribute to Linux.

Few years ago I wouldn't even dream of not having to dual-boot for gaming.  
A decade ago I've been just entertaining a thought of running Linux as primary OS.  
Two decades ago Linux was basically only for enthusiasts.  
When GNU/Linux has its release, it was supposed to be only temporary until GNU finish their own kernel.  
When Linux was born, it was just a summer project of a young student.

Linux wasn't so good at the beginning.

And neither is/was Unix for majority of users. Unix was complex system for academia and commerce (because there wasn't any other type of consumers at the time!). CS students have problems with command line, what do you expect from your average Joe from decades ago?

^(Also you may not know, but while Linux is only Unix-like, macOS **is** Unix by blood)",hrz0fc1,t3_s011f6,1641765232.0,False
s011f6,"Sure, I understand all that.

Mac OS X and beyond are based on FreeBSD UNIX, yes, but that didn't come around until 2001 or so.

Classic Mac OS 9 and earlier were not UNIX based. It didn't have protected memory or preemptive multitasking or even multiple user accounts, it was very basic and it could crash easily.

# My point is that if they found a way to make a GUI in 1983 and UNIX existed in 1969, why did it take until 2000 for a consumer OS with protected memory and UNIX-like stability to be available?  That's my REAL question.",hshwi8w,t1_hrz0fc1,1642088070.0,True
s011f6,"Well, I am just another Redditor with his own (stinky ;) ) opinion.


In the case of PC operating systems (mostly Mac OS and MS-DOS and early versions of Windows) This mostly had to do with design constraints with the hardware that they had to work with at the time. MS-DOS was a derivative of CP/M written for the Intel 8086 and the original Mac OS was written for the 68000. Neither chip had transistors that performed the functions of an MMU (memory management unit) and both operating systems were written with such hardware limitations in mind. They were also written to function in extremely small amount of memory. 

The hardware that both platforms ran on (680x0 series of chips for Mac OS and 80x86 for PCs) did evolve to include an MMU (68020 and 80286/80386) but by then the software ecosystems were very intrenched and new versions of Mac OS/MS-DOS still had to support those older chips for a time.

Contrast that with early versions of UNIX which ran on Mini-Computers which were extremely expensive for the day. Those machines had the hardware that implemented/supported memory protection. With that support in place it was much easier to build a multitasking system with protected memory.

So in short, those features are easier to build when you have hardware to help you with it. Early PC-Class CPU's lacked the hardware to easily support them. Technically I think you could make it work without such hardware if you are determined enough but I would imagine may deciding to wait for MMU support to be standard on CPUs before implementing it.


Well there is my dirty stinky opinion answer :)

EDIT: I decided to google around and I figured this link would be of interest to the OP

https://retrocomputing.stackexchange.com/questions/7740/why-was-preemptive-multitasking-so-slow-in-coming-to-consumer-oss",hs0xu78,t3_s011f6,1641792760.0,False
s011f6,"Excellent, this is a super helpful response. I really appreciate that!",hshwsub,t1_hs0xu78,1642088186.0,True
s011f6,"The first version of Windows NT (3.1) was released in 1993, less than 2 years after the earliest version of Linux. Windows NT 3.1 has everything you described. NT was largely based on the architecture of DEC VMS, and designed by members from the same team.",hs0y3fg,t3_s011f6,1641792889.0,False
s011f6,"Right, but Windows NT was not a consumer OS.

My point is if UNIX existed in 1969 and the GUI existed in 1983, why did it take until 2001 or so for us to have a consumer OS with the protected memory and stability of UNIX AND a GUI?",hshwp4h,t1_hs0y3fg,1642088146.0,True
s011f6,"Sorry, but NT was a consumer OS. It just wasn't the most common one yet. Literally everyone running windows today is running some version of NT. As for protected memory, Unix did not originally include that. Memory protection came about when MMUs were added to processors in the late 70s.",hshzumo,t1_hshwp4h,1642089350.0,False
s011f6,"Maybe I misused the term ""consumer"". What I meant was, prior to Windows XP, the overwhelming majority of Windows users were not running an NT based OS. Windows NT and Windows 2000 were not for regular users, lets not pretend they were mainstream in remotely the same way that the others were. Win 2K didn't even have USB support in the beginning. Most people weren't running it. That was my point.",hsqkcld,t1_hshzumo,1642230175.0,True
s011f6,Following the post,hryw4qu,t3_s011f6,1641763718.0,False
rzc739,"I would start with first principles.

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)",hrurxml,t3_rzc739,1641691696.0,False
rzc739,"A Crash Course in Computer Science is amazing! I can't believe how much information it conveys in those short, fun to watch and easy to understand videos. Great work.",hrw2h47,t1_hrurxml,1641715684.0,False
rzc739,"Lambda Calculus - Computerphile https://www.youtube.com/watch?v=eis11j_iGMs

The fundamental concept of programming (writing computations) can be described in terms of Lambda Calculus. There are actually numerous models of computing (and programming) that describe the meaning of computing. Lambda Calculus is very useful because it is a model that requires very little in its fundamentals, and yet the little it does have is capable to describe ""everything"".",hru6303,t3_rzc739,1641682595.0,False
rzc739,"Thank you. I know who Alan Turing in and I think the inventor of Lambda Calculus.

Is there anywhere that has terms that would help me understand?

(I super hate applied things. I have a theory oriented mind so I need to to understand the terms and what they mean first. I don’t pick things up if they are not explicitly
 stated—blame the autism. )",hrukw05,t1_hru6303,1641688731.0,True
rzc739,">Thank you. I know who Alan Turing in and I think the inventor of Lambda Calculus.

 FYI, Alonzo Church came up with Lambda Calculus; Alan Turing came up with State Machines.  Since Lambda Calculus is meant to be stateless, they are different approaches to the theory of computation.   
As a Mathematician, I would also recommend looking into Automata, which is the theory behind state machines and grammar.",hruzugj,t1_hrukw05,1641695108.0,False
rzc739,"I have heard of Alfonso Lorenzo is. He was American. I meant I know who Alan Turing is and who I think the inventor of Lamba Calculus is. I knew they are separate people . I just wasn’t sure if he was the right guy. I also know who Claude Shannon is. (My partner has his CS BS and almost got his math minor so they talk a lot about these guys. We’re just speaking of Charles Babbage yesterday).

Are you an applied mathematician? Or do you study theory? I actually want to take partial Diff EQ and complex analysis. I like math.",hryjlg9,t1_hruzugj,1641759277.0,True
rzc739,I have a BS in IT and a BS in Math(emphasis in cryptology)   In grad school for CS I had to take a couple of prereqs including Automata.   Automata is more math and the theoretical side of computing.  I quite enjoyed it.,hs3g73b,t1_hryjlg9,1641839389.0,False
rzc739,"The best way is to take a compilers class if this is what you want to learn. But if you want an intuition of how to “program” you need to just write more programs, and practice.",hrunmpo,t3_rzc739,1641689884.0,False
rzc739,Not good at learning by application. I prefer to get a gist of the theory. And understand the terms in the least general manor possible. Like how I learned algebra before trig or learned the parts speech and what they mean when learning grammar syntax.,hruoizq,t1_hrunmpo,1641690246.0,True
rzc739,"You could search youtube for what you are looking, but all I can say is the best way is to by doing. Programming is a very practical field, theory will only get you so far - and even after reading it may not be clear how something works until you do it yourself. 

Pick up some algorithmic topics in Math and code them up, make a differential equation solver - or whatever. You get the idea",hruu0hr,t1_hruoizq,1641692588.0,False
rzc739,"I need to use this to do Fourier transforms. So yes. Differential equations is what I’ll be working with. 

Do you know Mathematica?",hruxrf2,t1_hruu0hr,1641694205.0,True
rzc739,"I’m not familiar with Mathematica - but here’s what I can suggest. I’m not sure which language you will be using but SciPy and NumPy are two python libraries that have an api for computing fourier transforms. 

If i were you I’d implement a fourier transform from scratch, and bench mark it against an already implemented version from say like numpy and scipy - that should be good enough to give you an intuition of programming.

my point basically is…. no amount of learning before hand, or theory is enough to make you feel prepared or give you a good idea of programming. u need to dive in head first and start doing it",hrv6tf8,t1_hruxrf2,1641698170.0,False
rzc739,I’ll give it a go. I just want to familiarize myself with terms and a few ideas before hand. I will look into what everyone has posted.,hryju2l,t1_hrv6tf8,1641759359.0,True
rzc739,This!,hrx65pl,t1_hrunmpo,1641741029.0,False
rzc739,"You may like the math for programmers book (https://www.manning.com/books/math-for-programmers). It's aimed at programmers trying to learn math, but maybe you could use it to learn programming in a familiar context.",hrw9iv7,t3_rzc739,1641720923.0,False
rzc739,"You should learn how compilers are made. Then you really get it. Grammars, Lexers, Code optimization, ...",hrx62ag,t3_rzc739,1641740989.0,False
rzc739,Those sound like terms I would like to understand. Thank you.,hryhwma,t1_hrx62ag,1641758683.0,True
rzc739,[https://teachyourselfcs.com/](https://teachyourselfcs.com/) this is a good resource for everything CS,hrwgcp2,t3_rzc739,1641726257.0,False
rzc739,I’m seeing a lot of info on how to program which I don’t really believe would be “programming theory”. Equally there isn’t really such a thing that I’ve ever heard of. You’re best bet would likely be anything on compilers. The closest to theory would be what is usually classified as “Formal Languages/Automata Theory” this would be a very theoretical/mathematical approach towards how we define a programming language/computer model. You would probably specifically be looking at context free grammars (CFG),hry54xc,t3_rzc739,1641754189.0,False
rzc739,Sicp?,hrvfnd4,t3_rzc739,1641702226.0,False
rzc739,"What you’re looking for isn’t usually taught to people who don’t already know how to program pretty well. There are classes on programming language theory but they’re typically at least senior level and the way they’re typically taught relies on knowledge someone new to programming wouldn’t have. That said Programming Language Pragmatics by Michael L. Scott is really good, and I believe online PDF copies exist. So I’d find out what language you’re going to be using and which concepts would be relevant, because a lot of things in the book won’t be relevant to what you’ll need and would be very confusing.",hrybmql,t3_rzc739,1641756456.0,False
rzc739,"It sounds like you and I are in a similar enough situation and frame of mind at the moment. I am starting to learn programming at the moment as well and share a similar learning style. I’ve concluded that introducing theory, first principles, and abstract terms is introduced best through a language taught as an introduction to computer science. Graphing calculators integrate Python so I have chosen to start with Python because of that.

Here are the books I’ve ordered:

• Python Programming: An Introduction To Computer Science – 3rd Edition – John Zelle

• Learn Python3 The Hard Way: A Very Simple Introduction To The Terrifyingly Beautiful World Of Computers and Code – Zed A. Shaw 

• Learn More Python3 The Hard Way: The Next Step For New Python Programmers – 1st Edition – Zed A. Shaw

• Python Crash Course: A Hands-On, Project Based Introduction To Programming – 2nd Edition – Eric Matthes 

• Head First Python: A Brain-Friendly Guide – 2nd Edition – Paul Barry 

• Python Pocket Reference: Python In Your Pocket – 5th Edition – Mark Lutz

• Automate The Boring Stuff With Python: Practical Programming For Total Beginners – 2nd Edition – Al Sweigart 

• Django For Beginners: Build Websites With Python and Django – William S. Vincent",hrvt8w4,t3_rzc739,1641709588.0,False
rzc739,"Thank you. I am not sure what languages physicists and mathematicians use. I hear Python is good. I am not a big fan of programming, but I need it for my major.

I am much more interested in maths, but for an analysis course, I will also need programming. I think Python is good for maths/physics. I will check those out.",hryik5e,t1_hrvt8w4,1641758914.0,True
ryzj9x,"One very niche part of databases I researched last year as part of a databases courses I took was the use of metaheuristic-based optimization algorithms to enhance searches, with the heuristic selection component modeled after animal behavior.  A lot of these are modeled on animals that hunt/scavenge/travel in swarms, like sea creatures or flies, and on average some of them perform more searches faster than traditional or heuristic searches.  Whale Optimization (2016) is one example, but there are many, and more are still being researched.
 https://en.m.wikiversity.org/wiki/Whale_Optimization_Algorithm",hrs0rgs,t3_ryzj9x,1641650895.0,False
ryzj9x,Blew my mind,hrsod7q,t1_hrs0rgs,1641661226.0,False
ryzj9x,Mindblowing! I never thought biology and physics could be used to optimize databases!,hrwgupg,t1_hrs0rgs,1641726645.0,True
ryzj9x,"You can browse different papers and articles online in Google Scholar, the Papers We Love GitHub repo, and various journals.  Here are January 2022 articles on the topic of databases from [arXiv](https://arxiv.org/list/cs.DB/current)",hrrwl9x,t3_ryzj9x,1641648661.0,False
ryzj9x,Encrypted search is interesting.,hrs99aq,t3_ryzj9x,1641654939.0,False
ryzj9x,"This will be about distributed systems, but  from what I see, there's no great divide between these worlds, distsys people concentrate on databases a lot. I see some ML stuff around tuning and some around SRE, but it's mostly around consensus and distributed transactional systems.",hrssml2,t3_ryzj9x,1641662892.0,False
ryzj9x,I recently took a course in blockchain algorithms so we talked a lot about distributed systems and made a lot of connections to concepts from databases such as three phase commits. It's not necessarily databases but there are a lot of similarities and the different applications might interest you.,hruwkzi,t3_ryzj9x,1641693696.0,False
ryzj9x,Something to do with a boy's cod?,hru96me,t3_ryzj9x,1641683884.0,False
ryzj9x,Self driving databases are a current topic of active research.,hruv9kh,t3_ryzj9x,1641693125.0,False
ryzj9x,"This is actually my fav because it's a way to mathematize and automate databases, which are already very.beautiful structures",hskdceo,t1_hruv9kh,1642121978.0,True
ryeny8,There's probably a higher chance teaching math will become automated before programming ever does.,hrodpi7,t3_ryeny8,1641583540.0,False
ryeny8,"Hah, that's actually a great point!",hrotih6,t1_hrodpi7,1641589413.0,False
ryeny8,Or really every other job.,hrpctlm,t1_hrodpi7,1641596821.0,False
ryeny8,"Once programming is automated we either do not need any jobs, or we, do not need any jobs.",hrqul08,t1_hrodpi7,1641621350.0,False
ryeny8,"> A career counsellor said that I should teach math (my other possible career goal) rather than go into software development, since the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years.

Hear me and heed my words very carefully: your counselor is a *fucking idiot*. This cannot be understated. I'm nearly speechless. Computers CANNOT THINK. And no matter how clever our algorithms are going to get, they will only be able to produce work within the confines of said algorithm. That means innovation will still come from humans, because it cannot come from machines.

And in order to create software, someone has to tell the machines precisely what is desired. It almost sounds like... Programming... Business people and non-engineers with no idea how computers work or the nuances of computer science and software will never be able to capture the requirements and edge cases of the thing they desire. People are also very bad at knowing what they actually want. This takes professionals to do this work.

Our industry, our field, is quite, quite safe from being automated into obsolescence. The only way your career counselor can possibly be correct is if we hit the singularity in that time frame, where humans develop synthetic life, it grows exponentially, and we hand off society as a whole to it.",hro9vqi,t3_ryeny8,1641582135.0,False
ryeny8,"God, I hate when people hear some buzzwords and then start making sweeping predictions about what is going to happen in the future, especially when they are your average person and not some high-up, specialized, and/or industry leading person. Who does this counselor think will be making these no-code platforms? Who does this person think will be using these no-code platforms even if they take off? In my experience there are many jobs out there where having math and computer science skills would always be an advantage over your average person even if the job doesn't good super in depth with either skill.",hroe2wc,t1_hro9vqi,1641583677.0,False
ryeny8,You just don't get it...  You gotta use AI to DeFi your blockchain if you ever want your NFT's to go meta.  Do you even cyber?,hroeyc6,t1_hroe2wc,1641583999.0,False
ryeny8,"Automation always leads to a new classification of work.  Robot repair, AI repair, code fixer, quantum devices, etc;",hrplkmy,t1_hroe2wc,1641600370.0,False
ryeny8,You want to know a job bound into obsolescence: career counseling!,hrokoxe,t1_hro9vqi,1641586123.0,False
ryeny8,Preach,hrocurm,t1_hro9vqi,1641583227.0,False
ryeny8,"It really ignores that as capabilities grow, demands grow. When we got railroads did all the hauling get done by 5 machines instead of 500 ox carts? No, we decided we needed to haul more stuff further.

When we got more advanced programming languages all the programmers didn't get fired, we decided we could benefit from the more complex software that was now possible.",hrqv9eb,t1_hro9vqi,1641621750.0,False
ryeny8,"By that definition humans also cannot think, as every piece of abstract thought you hold are basically recombinations of observations. Just try to imagine a colour that does not exist, a sound you have never heard before, etc. Our creativity is an abstraction that recombines atomic concepts into newer, bigger scale objects. This is why you can think of a new animal, song or word, which is not all that different from the progress of state of the art NLP and GAN, just at an elementary level.

That being said, the idea of code being automated relatively soon is absurd for so many reasons if you consider the sheer complexity. We can however already synthesize code based on queries, and there are many smart code completion toolings. I'd say the role of developer is simply changing and perhaps becoming even more important. Just look at how critical infra and devops engineers are. Software engineering hasn't been a basic coding job any more for decades.",hrqvejh,t1_hro9vqi,1641621835.0,False
ryeny8,"> By that definition humans also cannot think, as every piece of abstract thought you hold are basically recombinations of observations.

This is reductionist bullshit.",hrryyel,t1_hrqvejh,1641649954.0,False
ryeny8,">Just try to imagine a colour that does not exist, a sound you have never heard before, etc.

People have created new colours e.g. blackest black.

As for imagining a sound you've not heard before, can I introduce you to the concept of music composition? Humans have been creating new sounds for thousands and thousands of years.",hrr8e8c,t1_hrqvejh,1641630905.0,False
ryeny8,Just now completed watching the Matrix film it draws lot of parallels with this notion.,hrs7k2r,t1_hrqvejh,1641654177.0,False
ryeny8,Thank you for taking time to write this to put a slap across a face-,hroz9dm,t1_hro9vqi,1641591584.0,False
ryeny8,"Not quite. You can automate large swaths of the dev landscape away with current tech. It is shortsighted to assume your job is ""safe"". 

When robots are coding that won't stop me from writing code (which may be better and more innovative than what a robot writes, or not). But it may mean that the employment landscape looks different, and it may mean a different relationship between programmer and device at the highest levels of industry.

Edit: more: Fast-forward to UBI, economy 2.0, and full automation, and perhaps only a few very good programmers are working at the highest levels of industry, and most of the rest is automated, except for the millions of open source coders out there who would all of a sudden find that they have the time and the money to make pretty much anything (including better robots). It would be a real renaissance if you could get past the passing of the traditional economy.",hroh705,t1_hro9vqi,1641584826.0,False
ryeny8,"AI writing code is not that different to compilers writing code. Both are tools that make developers an order of magnitude - or more - more productive. If compilers aren't a threat to your job, neither are AI coding tools.",hroyt95,t1_hroh705,1641591414.0,False
ryeny8,"That's a good point. Compilers did not kill jobs, they *added* jobs. It's just that those jobs were not in 1s and 0s, but in higher level languages. AI which can write code (and to an extent even program and bugfix itself) would surely add jobs, but they would be AI-directing jobs rather than coding jobs. Developers might multiply in number while coders shrink. 

As someone who loves to code, that would not stop me from doing so, but I can't help but wonder how much deeper the development process could be if it were taking place in a Star Trek Holodeck with an AI on-hand, as could be done in the not-too-distant future with no-code or low-code AI-driven AR and/or VR IDEs. That's almost totally possible. It is just a matter of cost, adoption, and production.",hrp21qt,t1_hroyt95,1641592630.0,False
ryeny8,People have had the same worries since the invention of a machine to weave fabric (look up the original luddites if you're interested). It's never led either to the paradise where all the machines do all the work for us or the horror of mass unemployment.,hrp9e0l,t1_hrp21qt,1641595465.0,False
ryeny8,"Horror of mass employment, or having humanity's needs met without needing an exploited underclass.",hrr9o78,t1_hrp9e0l,1641631900.0,False
ryeny8,"Keep looking for that Communist utopia. Hint: it never works, because people are people.

The original luddites were more realistic about human nature and mass-unemployment was definitely their worry.",hrrdcj9,t1_hrr9o78,1641634789.0,False
ryeny8,RemindMe! 1000 years,hrov6hu,t1_hroh705,1641590044.0,False
ryeny8,"Architects didn't go away because CAD software was created. Developer jobs will just change, not be eliminated.",hrqvbsn,t1_hroh705,1641621789.0,False
ryeny8,"> Hear me and heed my words very carefully: your counselor is a fucking idiot. This cannot be understated. I'm nearly speechless. Computers CANNOT THINK. 

To add on further anyone remember the dumpster fire that was [Github Coploit](https://www.fast.ai/2021/07/19/copilot/)",hrptqf9,t1_hro9vqi,1641603819.0,False
ryeny8,I wouldn't say dumpster fire. It's useful occasionally when you hold its hand.,hrqaqhv,t1_hrptqf9,1641611259.0,False
ryeny8,True however github coploit ran into an issue with [plagrism](https://twitter.com/MalwareJake/status/1411351168643706886?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1411351168643706886%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.plagiarismtoday.com%2F2021%2F07%2F08%2Fgithub-copilot-and-the-copyright-around-ai%2F) and it revealed security secrets.,hrqg56e,t1_hrqaqhv,1641613748.0,False
ryeny8,Yeah I heard about that. Hopefully they get that fixed. It feels sketchy using it knowing that it could pop out copyrighted material on a whim.,hrqgal8,t1_hrqg56e,1641613818.0,False
ryeny8,"Sure,  version 1 has some issues to work out,  however the idea itself is fantastic and version 2 is going to have some great improvements.",hrrphtj,t1_hrptqf9,1641644181.0,False
ryeny8,"This though. I’m also shocked at your counselors lack or knowledge, despite acting like they know a lot on the subject. Was coming here to say literally exactly this glad to see someone beat me to it :)",hrs501o,t1_hro9vqi,1641652980.0,False
ryeny8,Wish I had a free award from reddit to give it to you. 💯,hrt38fd,t1_hro9vqi,1641667020.0,False
ryeny8,"I don't know a lot about it, but people said the same thing about visual basic. And developer jobs have continued to rise.",hro91y4,t3_ryeny8,1641581833.0,False
ryeny8,"People are making hyperbolic statements about some fields and exaggarate. So not only is it wrong for that particular field, it does not apply to many others.

Things like VB were used (broadly speaking) in desktop and business logic. COBOL was used for business logic and there were hyperboles about that. Then web happened and a lot of business logic changed to online, always available and to support large numbers of users. I wonder what next change would be..

Then there's all the other fields like embedded devices and IOT which have been and are huge deal: it is not as visible, but they are everywhere and used in staggering numbers of devices. That isn't going to disappear.

Then there's different fields like graphics work: 3D rendering has changed from fixed pipelines to programmable ones and there are tons of shaders used in things like games to simulate reflections (physical based rendering, PBR) and ray-tracing is seeing improvements, although real-time ray-tracing still does not handle everything you would want to (too much computing required).

So, people only have narrow view on what they themselves are using and new cases appear regularly that shift the requirements into different directions. Question isn't about if programmers needed but what kind of skills they will need. Many would not recommend COBOL at this point but that is still used in financial software and people who know it are still being hired to maintain old stuff.",hrof0ha,t1_hro91y4,1641584022.0,False
ryeny8,"We have had no-code and low code options for 20+ years. Check out https://old.reddit.com/r/sysadmin/ for the horror stories of important business processes that have been built around Access Databases and Excel Spreadsheets, ask them about Visual Basic and Foxpro, both low code options from the previous century that still haunt the corporate world. 

No-code, low-code falls apart as soon as there is not a widget that does what you need them to do. If you need to do anything hard it becomes a real programming effort, the harder it is the closer to the machine you need to get.",hrodb3t,t3_ryeny8,1641583391.0,False
ryeny8,"And this kind of thing has been said for decades about all kinds of jobs that still aren't really very automated. Welding was going to be done only by machines 20+ years ago and there are still plenty of welding jobs, plus welders still run the machines that have automated some of these jobs.",hrrmkii,t1_hrodb3t,1641642065.0,False
ryeny8,"No code is someone else's code that you don't have control over. 

Machine learning code generation is just that, code generation. It saves you a step from having to go to google stuff and copy-paste from stack overflow. Code is not software. In fact, there are some estimates I read that coding is only about 10% of a job of a software developer. Even if it becomes fully automated/obsolete (and it won't), you'll still have the other 90%. And it's not like all the code already written will magically become self-supporting.  


I bet that career counsellor types with just two index fingers and has to look at a keyboard and mouth the letters while typing.",hrof1b7,t3_ryeny8,1641584030.0,False
ryeny8,The best developer I ever knew typed with two fingers with no more then 10 WPM. I sit in the same row and have never seen her hit backspace or delete.,hrppv0j,t1_hrof1b7,1641602163.0,False
ryeny8,"Ctrl+A, Space",hrre29m,t1_hrppv0j,1641635362.0,False
ryeny8,The other 90% is browsing reddit,hrqxkdk,t1_hrof1b7,1641623176.0,False
ryeny8,Lol,hrodj3v,t3_ryeny8,1641583473.0,False
ryeny8,"As someone who knows both dev and AI/NLP, i echo the words of someone else here: your counselor is a fucking idiot",hrocske,t3_ryeny8,1641583205.0,False
ryeny8,I think you should tell your counselor to move into programming because in 10-15 years he will be the one jobless.,hromffp,t3_ryeny8,1641586777.0,False
ryeny8,"You can create a website or an app nowadays without coding much but to create a system which is highly scalable and available, you'll still need good programmers and engineers. That's not something a machine or non-technical person can do.",hrol0oj,t3_ryeny8,1641586246.0,False
ryeny8,"The people automating jobs away cannot be automated away.

I NEVER found my counselors useful, they literally are counselors and not X profession/career you actually want to get into. That should always be a major red flag on taking their advice on anything.

The number one most important thing about who's advice you listen to is this: Find the people that have/do what you want and go and ask them how to get it/do it.",hronxru,t3_ryeny8,1641587340.0,False
ryeny8,"If it happens, you can always change career to teach math. But its unlikely to. Machine learning requires a lot of developers right now and is likely to do so for the near and medium future.",hro9tvy,t3_ryeny8,1641582116.0,False
ryeny8,"Your counselor is an idiot, full stop. The need for programmers is only accelerating. I’ve been at this since I was 14 coding in my bedroom dreaming about what could be in the future (thanks TI-99, Sinclair 1000, C-64, TRS-80, and IBM AT. You were all good friends). I’m 52 now and we are only scratching the surface. I still dream about what could be in the future.

For a smart kid from poor to lower middle-class background, this career is the surest and fastest way to a solid 6 figure income in flyover country. Factory automation, agriculture automation, websites, mobile apps, VR games, warehouse management, CRM, ERP, hospital EMRs, air quality analysis, self driving cars, wearable tech, robotics, AR systems for military, quantum computing, smart homes, I could go on and on, but its just going to get more and more intertwined with our lives. Now it is still just an addition to our lives, in 30 years it will be fully intertwined. 

Feel free to msg me privately and I will be happy to mentor you through your college applications.",hrotv2d,t3_ryeny8,1641589545.0,False
ryeny8,"> I should teach math (my other possible career goal) rather than go into software development

Have you looked into how much teachers get paid? Ha!

> the rise of no code tools and machine learning code generation will mean that I won't have a job in 10-15 years

They were saying the same thing about outsourcing in the 2000’s and look how that has played out. Also, did CAD replace engineers in other fields? Lol.",hro9uof,t3_ryeny8,1641582124.0,False
ryeny8,"People who think these kind of things threaten developer jobs don't know about AI & development (or the difference). There's nearly zero overlap. Developing an AI for a task requires AI specialists, has high resource costs, and is significantly more pricey than hiring developers; also it only does pattern matching, so anything that doesn't fall in that purview requires a developer which is... mostly everything. They are both very different use cases.

Developers aren't going anywhere anytime soon.",hroqnw6,t3_ryeny8,1641588357.0,False
ryeny8,"The consensus, a resounding no, your career opportunities are not under threat.",hrofhc7,t3_ryeny8,1641584193.0,False
ryeny8,"I fully agree with the other comments here, but thought I'd provide a little bit of evidence as to how important software developers are when working with low-code platforms.

I was applying for new jobs about a year ago, and noticed that major corporations like Thales use a low-code platform called Mendix, which seems to be a major player in the industry. Thales is currently hiring Mendix developers for multiple positions, such as [this advertisement for a technical lead](https://thales.wd3.myworkdayjobs.com/en-US/Careers/job/Crawley/Mendix-Developer_R0114396-1). Note the requirements for the position: Multiple years of experience, experience with all levels of software stacks, experience in programming languages, experience in security...

These are all skills that non-developers do not have and most likely will not learn. Even if low-code platforms actually take off (and that's a big if), software developers will be the ones writing the code (or the not-code, or whatever).",hrot1n8,t3_ryeny8,1641589238.0,False
ryeny8,"I suppose your counselor is not aware of ""The halting problem""",hrovixx,t3_ryeny8,1641590176.0,False
ryeny8,Terrible advice on your counselor’s part.,hrofbg3,t3_ryeny8,1641584133.0,False
ryeny8,"That’s a very foolish position for your counselor IMO.

Computer science and computer programing are only going to increase in value.

Who is going to program the AI to code? A computer programmer. 

Understanding the computer world is going to be increasingly central to everything. And your knowledge as a computer programmer will adapt to the changes if your interest is serious. 

Honestly, teaching math might not be a job in 10 - 15 years because through computer programs getting better and AI, there in all probability won’t be a need for teachers in the current sense.

Your counselor is clueless.",hroh1x0,t3_ryeny8,1641584773.0,False
ryeny8,"LOL No. 

But it does add a whole bunch of new projects for the developers to work on. 

Software engineering will literally be the last job where machine can completely replace human counterpart. It might still happen some time in the distant future. And once that happens, it’s game over for the entire civilization, not just developers. 

We got time.",hrow31h,t3_ryeny8,1641590384.0,False
ryeny8,We are living in a simulation,hrpisqs,t1_hrow31h,1641599237.0,False
ryeny8,"Your career counsellor has no fucking idea about software development. This is like saying don't become an architect don't become a carpenter don't become a roofer because we have drills today!

I would even report this guy. Developers are in high demand and telling people who are interested in this field to not go this way is the complete opposite what he should do.",hrrdmvr,t3_ryeny8,1641635018.0,False
ryeny8,"Honey, I’m gonna put this in a very simple example.

Tell me any virtual assistant (Google Assistant, Siri, Alexa, Cortana) that you have used and never made 1 mistake.

Computers are nowhere near ready to replace developers. Computers cannot find flaws on there own yet. They can only find what they are instructed/deeply trained to find. Humans on the other hand can use knowledge they possess and apply it to anything they see fit. Computers cannot.

Your consular is a prime example of why applications are made toddler safe.",hroe8td,t3_ryeny8,1641583737.0,False
ryeny8,Tell me any developer that doesn't make any mistake :),hrrsa3z,t1_hroe8td,1641646063.0,False
ryeny8,"Bad / entry level crud developers ? Yes

Complex custom solutions / enterprise development ? No",hroprvg,t3_ryeny8,1641588026.0,False
ryeny8,Your career counselor is a clown,hrp5xhk,t3_ryeny8,1641594119.0,False
ryeny8,"Aaahhhh, I love how confident some people are in making predictions of industries they've probably never touched.

(I'm talking about the counsellor, not OP)",hrqdqm8,t3_ryeny8,1641612633.0,False
ryeny8,Your counselor is a moron,hrqr33b,t3_ryeny8,1641619363.0,False
ryeny8,"No... developers will need to troubleshoot and fix why all the ""no-code"" shit isn't working",hrr5l7g,t3_ryeny8,1641628783.0,False
ryeny8,Your counsellor is a downright moron. Period.,hrreqrt,t3_ryeny8,1641635914.0,False
ryeny8,Do you want to make the mistake of your life? Cuz that's how you do it,hrrrxrn,t3_ryeny8,1641645840.0,False
ryeny8,"*Do you want to make*

*The mistake of your life? Cuz*

*That's how you do it*

\- lookintothefuturem8

---

^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.](https://www.reddit.com/r/haikusbot/)

^(Opt out of replies: ""haikusbot opt out"" | Delete my comment: ""haikusbot delete"")",hrrryl2,t1_hrrrxrn,1641645855.0,False
ryeny8,Your career counselor should look for another job.,hrp2swl,t3_ryeny8,1641592919.0,False
ryeny8,"You should look at /r/programmerhumor sometime. There's often a screenshot of the absolute 10/10 shite that Copilot can produce. It can however sometimes give you something useful, but it still needs a hint of what you want it to write. Someone had to think about what needs to be done and more importantly *how*.

I doubt we'd be able to make an AI that can properly make a beautiful and fast frontend with good UX or make a sensible and proper DB design for a backend environment. Even if we're able to do that in the next, I don't know, 40 years it also has to be cheaper to use than paying a team of developers .",hroerj1,t3_ryeny8,1641583930.0,False
ryeny8,"We have a low code platform from Outsystems, all those guys do is ask us SAP folks to build a Odata service for them to consume the data. They can't do shit on their own.",hropha1,t3_ryeny8,1641587915.0,False
ryeny8,"As many have already stated, I'd ignore your career counselor. Low code/no code solutions automate something that should be automated, the easy stuff that we shouldn't be working on because it's easy. It's just a new abstraction layer. This is great, because it means we can focus on the hard stuff that hasn't been solved yet.

I will caveat, there is a set of software developers who may lose their jobs because there are folks who are paid to write code that really should be automated, but isn't for some reason (usually ""enterprise"" inefficiency.) And, some of those coders lack real problem solving skills and won't be able to transition to more meaty problems.

If you have a strong interest in math and code, you should find plenty of opportunity to work on problems that haven't been solved. That's not going away until, as someone pointed out, we reach the singularity, and then we're all out of work :)

P.S. Thanks for asking this here, it's a really interesting topic, and a good wake up call that people are buying a bit too much in to the hype of low code/no code.",hrot0qt,t3_ryeny8,1641589229.0,False
ryeny8,who watches the watchmen?,hrotxj9,t3_ryeny8,1641589571.0,False
ryeny8,"If anything, developers will be managers of robots, but not without jobs. Your advisor sounds like an idiot.",hroyfy2,t3_ryeny8,1641591275.0,False
ryeny8,"In order to automate a programmers job, first we need our clients to accurately describe what they want.  I have yet once in my career experienced this.",hrp32rh,t3_ryeny8,1641593024.0,False
ryeny8,Your career counselor is a wrong,hrp5y67,t3_ryeny8,1641594127.0,False
ryeny8,"There will be developer jobs for decades to come, and maybe even until the singularity.",hrp81o5,t3_ryeny8,1641594943.0,False
ryeny8,"Only to a point.
When electrical and mechanical computer systems, they took away the jobs of ""computer"" people. But they created jobs in operating and programming these computers. When programming could be written in the memory of the machine, it took away the need to dig through wires and switch them around. When compilers were created, it took away the job of writing in assembly. But it created jobs in creating those compilers, one for every machine, one for every language. When these compilers can be made automatically, if they ever do, there will still be jobs in writing programs; there already were. Even if we create meta-compilers, compilers that take in entirely human language and compile that into a common programming language, which is then compiled into assembly, there will still be the creative process above it all. And I can say with almost certainty that a computer will never design a game from scratch.",hrp8orx,t3_ryeny8,1641595193.0,False
ryeny8,"Maybe I am naive regarding how far AI coding can go, but I work on a firewalling application, and when I receive a requirement along the lines of ""There's this type of traffic X which we are failing to identify quickly enough and make a drop decision  before various NIC/system buffers get shot and we start dropping everything"", I feel like no matter how clever the AI there's still a role for a dev--type human in there somewhere.


Predicting the future (especially tech) is hard though, so 🤷",hrpb430,t3_ryeny8,1641596140.0,False
ryeny8,"No, that’s kinda like saying “ugnert invent fire, stop messing with wheel science already figured out” low code no code solutions are tools that have been coded to allow someone to do something without understanding how to code. They do not create new capabilities, that still needs to be coded, and the integration and expansion of these capabilities will need to be coded.",hrpby5s,t3_ryeny8,1641596473.0,False
ryeny8,"When I started university, OOP and code generators were all the rage, and people were hyping that UML, automatic diagram to code generators, etc. gonna end most of the coding. Also, while AI winter wasn't officially over, it's rumored that AI will be able to code, and will be much better at it (w.r.t. humans) in 10 years.

**Result:** >!It didn't happen like that.!<",hrpc7n7,t3_ryeny8,1641596577.0,False
ryeny8,"The day a software engineers job is automated is the day that literally nobody will have a job anymore. 

Even with these tools - someone's gotta design the systems behind it. AI can adapt but it has to have the foundation to do so first.",hrpcqiu,t3_ryeny8,1641596787.0,False
ryeny8,"Well some peoples jobs, but it would be balanced out by more jobs being created. 

Thats the thing about software, every time something gets easier, it just means that there are more things you can now build on top of it.

Until(if) we reach the singularity, no software is ever going to absolete the developer.",hrpcwh0,t3_ryeny8,1641596852.0,False
ryeny8,We are still very short on skilled software developers.,hrpkf7v,t3_ryeny8,1641599901.0,False
ryeny8,"I develop a low code plugin and I totally agree with everyone else here. We need more developers and our best customers are still developers even if just about anyone could build a CRM without code on our platform.

Our users that aren't advanced only get closer to being developers as they use our platform. AI can't do much more than what we've done ourselves over 12 years of listening to customer support tickets and reacting to what people actually tell us they want.",hrplnu0,t3_ryeny8,1641600406.0,False
ryeny8,"If you work in IT or development, you will know it couldn’t be further from the truth.

Seeing first hand all of the issues that are fundamentally related to these kind of tools - tools that aim to abstract difficult computational tasks - makes it easier to just intuitively understand why its silly to suggest that programming jobs will be fewer in 15 years.

Such tools usually only work well in typical and expected scenarios and there is always some kind of trade off - less efficiency, less control, less interoperability, less portability, less capability, lack of underlying understanding, higher costs, security concerns… the list goes on. 

 Programming jobs are more likely to continue to increase than anything else.

You should still consider your options though - because teaching Math sounds like a cool choice too.",hrpyhpo,t3_ryeny8,1641605858.0,False
ryeny8,"Well, if you're gonna lose your job (as you counsellor said) in 10-15 years make sure you earn so much money that you can start with anything in the future without any hassle.

Don't take advice from someone who isn't in the field i.e. The counsellor.",hrpzpyi,t3_ryeny8,1641606388.0,False
ryeny8,Nah.,hrpzvt9,t3_ryeny8,1641606460.0,False
ryeny8,"Good developers are always trying to put themselves out of a job, in a sense.",hrq2w8u,t3_ryeny8,1641607782.0,False
ryeny8,"AI coding will sure save time from programmers, but you still need to write out what you want to do. The AI is unaware of more abstract concepts than objects or lists. You still need to think on a high level about what you want to do. So no.",hrq3ube,t3_ryeny8,1641608197.0,False
ryeny8,My lead told me react and angular devs will be replaced by AI that can build UI.,hrq462m,t3_ryeny8,1641608338.0,False
ryeny8,Can you please show them this thread,hrq5a4u,t3_ryeny8,1641608825.0,False
ryeny8,"Ha, hahaha, hahaha no.  Those tools, if they help at all, will just make existing developers faster.  And they probably won’t even drop the number of new devs being added in the coming years.

I think that laymen forget that programming isn’t just syntax.  Look at scratch, which could be seen as a “low code” platform.  Ask the average business goon to look at a bright beautiful low code platform and see if they can tell whether a loop will halt or continue forever.  Programming is about thinking algorithmically, not about arcane symbols.",hrq65om,t3_ryeny8,1641609215.0,False
ryeny8,"Why would anyone want to go to class and watch you teach when they can pay for an online recording video of the ""best teacher in the world"" from somewhere in Australia for instance lol",hrqbm8a,t3_ryeny8,1641611661.0,False
ryeny8,Need a developer to make pasta primavera out of the spaghetti those tools churn out.,hrqfa7h,t3_ryeny8,1641613349.0,False
ryeny8,"Yes, shitty developers will have their jobs threatened.  Good developers will be alright.",hrqi9df,t3_ryeny8,1641614757.0,False
ryeny8,I have a neighbor who spouts this bullshit constantly then apologizes to me like I even give a fuck. It’s whackadoo… if a computer does something then someone programmed it to do so.,hrqjp9i,t3_ryeny8,1641615478.0,False
ryeny8,no,hrqlmqr,t3_ryeny8,1641616473.0,False
ryeny8,"I'm in QA, and I promise you anyone who thinks a computer can write code has never read a story's acceptance criteria written by a customer.",hrqobez,t3_ryeny8,1641617871.0,False
ryeny8,"If you think it will, then you don't know what programming is or AI !",hrqoge4,t3_ryeny8,1641617944.0,False
ryeny8,"Producing code is a very small part of software engineering. Most of the work of a software project consists of deciding how to handle all possible types of inputs and failure modes that a system could encounter, in order to make that system serve specific goals.

In [Programming as Theory Building](http://pages.cs.wisc.edu/~remzi/Naur.pdf), Peter Naur argues that a team's understanding of a program is their true product.

When code comes from a third party (such as an AI), it is **harder**, not easier, to review the code and make sure that it does what is wanted without introducing any unacceptable risks. This is like the difference between building a new house, and renovating an old house without knowing whether it contains any lead or asbestos or radon.",hrqpzt2,t3_ryeny8,1641618771.0,False
ryeny8,Can I use copilot for basic python ?,hrqufic,t3_ryeny8,1641621257.0,False
ryeny8,"Red queen hypothesis applies here.

As the tools grow more capable and complex the tasks become more complex and demanding. What a developer once did with 300 lines of assembly can be done in 5 lines of C++ or 2 lines of python. But they don't want that anymore. They want you to use python to set up a neural network in Keras, in the time that programmer got to write 300 lines of assembly. But now what your doing would represent 100,000s of lines of assembly.

So if we someday have a tool like the holodeck in star trek where you can say ""make me a program that simulates all the behaviors or Sherlock Holmes"" the programmers will be doing something that is still a lot of work given those tools.",hrqux5l,t3_ryeny8,1641621551.0,False
ryeny8,"For Gov jobs, maybe. For the rest, no. But be prepared to relearn everything on a regular basis.",hrqxvfq,t3_ryeny8,1641623375.0,False
ryeny8,"In the future, you will have to be extraordinary in your field to sustain. AI is gonna replace average programmers and average math teachers too. You will have to be extraordinary to sustain in the highly-competitive world. Do what you want to do, but be the best at it.",hrqyged,t3_ryeny8,1641623752.0,False
ryeny8,"1. Those things do not create program, they write code. They have no concept of why. It can't make decisions, talk to clients or handle anything else then code.

2. If an ai tool like this can replace your entire job right now, then what the hell are you doing?",hrr5w0y,t3_ryeny8,1641629003.0,False
ryeny8,"Have prefabricated houses affected builders jobs? No. If you need some technological shit doing right, you need to hire a coder. The rest is scam. You can tell your mentor to keep studying math. People who say that kind of stuff, never have coded entrepise applications, and there are a lot of them. Learn to code, the rest is shit. I am 30, I have been coding since I was 12. I am an entrepreneur now, and It is super hard to find good programmers to work with, so PLEASE keep studying how to code.",hrragrb,t3_ryeny8,1641632523.0,False
ryeny8,"[Relevant CommitStrip](https://www.commitstrip.com/en/2016/08/25/a-very-comprehensive-and-precise-spec/).
No matter how abstract the specification is, you will always need someone to work on it. And even if you don't stay up to date, there will also be businesses with old tech. I hear COBOL developers are doing fine.",hrrcqgx,t3_ryeny8,1641634301.0,False
ryeny8,"Like some people have already mentioned, just throwing the word ""machine learning and artificial intelligence"" does nothing. We're far far away (it won't be an exaggeration to say about a century) from having something remotely close to Jarvis, Friday or Edith. So no, AI won't be the one who would be making dev jobs obsolete.

Secondly, it seems like you haven't worked at scale. The low code and no code tools only scale so much or provide exactly the features you're looking for. Sure if you're making a majorly static application then yeah those jobs are ALREADY gone. But other than that we're doing as great as ever.",hrrcs6h,t3_ryeny8,1641634340.0,False
ryeny8,No we will just be stuck in bug reviewing hell,hrrl100,t3_ryeny8,1641640885.0,False
ryeny8,"I wouldn't worry till the Terminators start reproducing, then we're screwed anyway.",hrrmnyi,t3_ryeny8,1641642136.0,False
ryeny8,Happy Cake Day jforrest1980! You are never too old to set another goal or to dream a new dream.,hrrmofv,t1_hrrmnyi,1641642147.0,False
ryeny8,"We've had no code tools for a long time my friend, they only create more work for us, and open new doors.

Also, your counsellor is ignorant. We already automate everything and we just do more as a result, not less",hrrn1jz,t3_ryeny8,1641642419.0,False
ryeny8,Computer science is about a lot more than just writing code. It would harm some jobs probably. If you’re still in school might I suggest gearing your education towards AI itself?,hrry21u,t3_ryeny8,1641649471.0,False
ryeny8,"You’re career counselor is an idiot. Find a new one.

Secondly, computers are stupid until people make them smart. No code had to be coded by someone and there will always be something new that needs created via code",hrs96np,t3_ryeny8,1641654905.0,False
ryeny8,"I was just in the process of ordering something from an online shop created by squarespace when the whole thing seizured, lost my order, and then dumped me onto a different product screen with no way to get back to my cart.

Don't worry, software engineers aren't going anywhere.",hrse93k,t3_ryeny8,1641657105.0,False
ryeny8,Who builds the low code /no code tools?,hrsnl5p,t3_ryeny8,1641660917.0,False
ryeny8,"10-15 years? We’re going from “some specialized NLP algorithms can produce a simple Python function given a detailed description of what it does, with a reasonable chance that it won’t be correct” to “literally developers aren’t necessary anymore” in half the time it took to get from Java 1 to Java 8? Yeah I think you’re safe",hrsqzzf,t3_ryeny8,1641662257.0,False
ryeny8,My college counselor told me not to major in computer science and computer engineering because there were too many people going into the field and there would never be enough jobs out there for all of them.  That was 2003-04. Don’t listen to those idiots and do what you enjoy.,hrsrs51,t3_ryeny8,1641662559.0,False
ryeny8,"Very likely not the case and even if, it will definitely create new jobs in a similar field where you already have atleast some experience and you can always learn new stuff",hrst4v2,t3_ryeny8,1641663090.0,False
ryeny8,Another comment said “listen to me carefully: your counselor is *a fucking idiot*.” I couldn’t have said it better myself.,hrsutnb,t3_ryeny8,1641663747.0,False
ryeny8,"The AI can only write a valid program if someone wrote the test to prove it.  AI is science, science only let's us observe what we think is true.  It can't discover and assert true by itself.",hrszkpw,t3_ryeny8,1641665585.0,False
ryeny8,"Before that happens, career counselling is going to get automated. Hope your counsellor is preparing for a new role- maybe math?",hrt0pix,t3_ryeny8,1641666024.0,False
ryeny8,I’m sure the career counselor is an expert in tech.. that’s why they’re a counselor rather than working in any industry.,hrvkull,t3_ryeny8,1641704875.0,False
ryeny8,AI technology is getting too much hype if the general public believes that there will be not need for human-generated code 10-15 years from now.,hrwgemi,t3_ryeny8,1641726297.0,False
ryeny8,Your career counselor is a fucking idiot,hrp5vti,t3_ryeny8,1641594101.0,False
ryeny8,Your career counselor is a wrong. SWE isn’t going anywhere.,hrp60v5,t3_ryeny8,1641594156.0,False
ryeny8,Your career counselor is a wrong. SWE isn’t going anywhere and it’ll always pay more,hrp62qe,t3_ryeny8,1641594176.0,False
rxuuqm,"depends on what information the guesser receives after every guess. if you only tell if their guess is right or wrong then the best approach is just start at 1 and increment it by 1 for every guess. time complexity O(n). 

if after every guess you tell the guesser if their guess was lower, higher or equal to the number then you can use binary search. time complexity O(logn).",hrktu0c,t3_rxuuqm,1641520595.0,False
rxuuqm,"That is the most efficient solution. With no other information, the best you can do is just arbitrary guesses (assuming you never guess the same thing twice). It's pretty easy to prove, actually; you should give it a shot.

Anyway, since there's no algorithm improvement to be had, it comes down to maximizing the number of guesses you can perform per second. That kind of optimization can be somewhat tricky and involved, but I encourage you to look at utilizing parallelism and gpu computation to solve this problem, along with optimizations of the sort used in GNU `yes`. Unroll the living shit out of some loops, lol.",hrktr3o,t3_rxuuqm,1641520561.0,False
rxuuqm,How do you prove something like that?,hrn10se,t1_hrktr3o,1641565636.0,False
rxuuqm,You assume theres some algorithm that sloves the problem in less guesses and show you can pick a result that the algorithm wouldnt guess.,hrnmpd9,t1_hrn10se,1641573852.0,False
rxuuqm,"less guesses being o(n)?, how can you tell what the algorithm wont guess without knowing how it works?",hro3930,t1_hrnmpd9,1641579745.0,False
rxuuqm,"Read about the ""Adversary arguments"". It's a technique to validate the lower bound of an algorithm in the worst case.

In the Fundamentals of Algorithms (*Brassard & Bratley*) chapter **12.3**, you can find a very good explanation. The PDF for the book is easy to find on google.",hro873i,t1_hro3930,1641581522.0,False
rxuuqm,"You assume theres k guesses where k is less then n (n being the biggest allowed number, such a number must exist just like someone said about uniform over the naturals). After running the algorithm we can denote its guesses as A= {a_i} from i=1 to k. Of course each guess is from 1 to n.

Because n is bigger then k, there exist a number b below n that is not in A. Therefore if the randomly selected number is b, the algorithm wouldnt work.

Its a bit too rigourous, but the point stands.",hrogutu,t1_hro3930,1641584700.0,False
rxuuqm,Just learned this in discrete math (proof by contradiction) and am shocked to actually see it being used,hroqkt1,t1_hrogutu,1641588326.0,False
rxuuqm,"haha, those kind of proofs are very common.",hrou42u,t1_hroqkt1,1641589640.0,False
rxuuqm,About 3 pages of discrete kinda math,hrph4g5,t1_hrn10se,1641598554.0,False
rxuuqm,"Can it ask additional question like ""_is it greater than X_""?",hrktlff,t3_rxuuqm,1641520497.0,False
rxuuqm,There is no uniform distribution on the naturals so no random distributions are particularly natural. Without more information on this particular distribution the answer to your question is ‘it depends’,hrlpx30,t3_rxuuqm,1641535463.0,False
rxuuqm,"Hmm. Well technically if you use multiple guess points that focus at a different starting point you can try to get luckier… but statistically it would be the same time complexity 0(n). Let me explain.

So you explained how you you incremented a number one by one so I assume you start at zero. Well in that loop you probably have something like “while the answer is not equal to guess then increase guess by one” starting at zero.

Well what if at the same time you ran a second guess inside the while loop. And this guess starts from the highest digit you allowed the random number to be chosen from and starts moving one down every iteration. So now you have two variables guessing in the same iteration of the loop. One goes 0,1,2…. While the other goes something like 1000,999,998,997….. 

Now while it sounds like it would make it faster it doesn’t because the two points aren’t being used at the same time; however, I think maybe you could get luckier that way. Just sounds plausible even thought statistically it’s not.",hrlkmrm,t3_rxuuqm,1641532572.0,False
rxuuqm,I had actually tried that and it was a few seconds faster on average but sometimes it would take up to like 8 seconds longer if you were unlucky,hrmzmp4,t1_hrlkmrm,1641565048.0,True
rxuuqm,"being faster on average is only likely due to loop unrolling - since you're making 2 guesses per iteration, there is half the overhead for looping",hrny3og,t1_hrmzmp4,1641577916.0,False
rxuuqm,"While I was reading your post, I thought about set theory. There are infinitely many intergers. If you consider such statement as ""the number is between 10 and 10000"", according to your method, you need at most 10000-10+1= 9991 tries. Each time you try, you have a probability of 1/ 9991 to guess the right number. The statement is the information needed to shrink the size of all possible combinations. It is possible to guess correctly by doing it once, twice, three times....., or in the worst scinario, 9991 times.",hrmbg37,t3_rxuuqm,1641550586.0,False
rxuuqm,"The problem of making it any better than O(n) seems to be it is simply random. I see this as you are given a function to check equality with the chosen number, and don’t have access to the number. Therefore it is completely random and unless there is something known about how the guesses are made that’s simply it. Maybe, just maybe, if it’s a bunch of numbers Inputed by people around the world, you can do some ml to find tendencies of what numbers are most likely to be chosen by a person and run those first, worst case would still be O(n) though. Also you can maybe speed it up a bit like that but there is no way to get O(log(n))",hrmvvnb,t3_rxuuqm,1641563370.0,False
rxuuqm,"Increments of one is likely the best - considering anything else would omit certain possibilities and could lower the probability of guessing the number in the shortest amount of time. 

With that being said, you could try to improve the amount of time it takes by trying all likely probabilities first. For example, if the number is a friends 4 digit passcode, it's more likely to end in 5, 0, or some other number that may hold significance to the creator of this number you're trying to guess. 

Write down all factors that could influence what number you're trying to guess. Then, have the computer guess all - more likely numbers first, and increments of one from there. 

You could have it running several algorithms at once. For example (2,4,6,8,10) (5, 10, 15, 20) and several other more-likely-pattern-based-increments. At the same time, you could have the computer trying series of numbers that may hold significance to the original numbers creation.",hrnoyp8,t3_rxuuqm,1641574659.0,False
rxuuqm,"Give the computer a gun, and you will tell it. /s

Otherwise, pick a random number weighted according to the distribution of how you picked the number.",hrokq03,t3_rxuuqm,1641586135.0,False
rxuuqm,Update: I used multithreading and used 4 threads to speed up the guessing. 2 start on the low end and and the other 2 start at the high end. The pairs of 2 are staggered 1 apart and go up 2 each time and together they go through every number in that direction. Going from both directions also helps because the number is more likely to be on one side than in the middle,hromtdv,t3_rxuuqm,1641586920.0,True
rxuuqm,"If I remember correctly, the chance of choosing the correct number that is within a given range is related to the square of that range's size? So if your number is between 0 and 100 you're very likely to guess it within 10,000 (100 squared) tries. I don't remember how the likelihood is actually calculated though.",hrljt0d,t3_rxuuqm,1641532145.0,False
rxuuqm,"If there are only n options (101 in your example) then you don't need more than n tries to guess it obviously, unless you try already attempted numbers again. So what you are saying is not true.",hrn54v5,t1_hrljt0d,1641567314.0,False
rxuuqm,"I meant random tries, not iterative. As in, how many die rolls would it take to roll a 6? Not necessarily just six, because you aren't just iterating from 1 upward.",hrn5de7,t1_hrn54v5,1641567407.0,False
rxuuqm,"Ok. You are talking about the probability of randomly picking some specific element out of a set with n elements.
That's 1/n.
So where does the quadratic part come in?",hrnl1bw,t1_hrn5de7,1641573255.0,False
rxuuqm,Use a quantum computer.,hrn1h8t,t3_rxuuqm,1641565828.0,False
rxjst3,Using tools from real analysis how would you rigorously set this up ?,hrqhxol,t3_rxjst3,1641614600.0,False
rxn4al,"The clock could show you if it is day or night throughout the year. This was done by programming the length of the days into the clock (afaik with water).

It is a maschine that calculates (among other things) the length of the day based on an input. It has a ""storage"" unit and a ""logic"" unit.",hrm4kgk,t3_rxn4al,1641545279.0,False
rxn4al,"All of those things are true about an hour glass by simply adding and removing sand. A person calculates the length of day and adjusts the input information, water, to provide the desired result the program is always the same though",hsgmqch,t1_hrm4kgk,1642059847.0,False
rxn4al,"A clock can generally be modeled as a cyclical finite state machine with a single input. This is useful in some finite state machine decompositions. Many would say a computer must consistently apply a function over an ensemble of possible inputs so a fully determinate machine like a trivial clock might not qualify. But the castle clock isn't that kind of trivial clock, and it does have many possible inputs, so it's hard to object to calling it a computer.",hrnoxg4,t3_rxn4al,1641574646.0,False
rx1mtm,"Let’s say messages are times on a clock. I want people to be able to send me times, but I don’t want other people to know what those times are.

I know that if you add 12 hours to a time, it remains the same, it does a full 360 degree rotation around the clock.

So I’m going to pick two numbers, x and y, such that x+y=12. If I take any time, and add x, then add y to it, it will be the same time.

So I tell everybody what x is, that’s my public key.

When they want to send me a time m, they first add x to it and then send me m+x. Let’s pretend subtraction is extremely difficult, infeasible for anybody to do. So somebody listening into our communications will know m+x, but they don’t know m because they have a hard time subtracting x from m+x.

When I receive this message, instead of trying to subtract x, I’ll just add y instead, which gives me m+x+y=m+12=m, I now have the original unencrypted message. y is my private key which is a secret only I know.

Nobody else can do this because in order to find out what y is, they would need to compute y=12-x, and remember subtraction is difficult.

This is essentially how public key encryption works, a public/private key pair are generated to have the mathematical property that application of both, yields the original message. It also needs to be extremely difficult to compute one from the other.

In the case of RSA this would be based on modular exponentiation and prime factorization.

Modular exponentiation means m^x mod n, for some choices of n, we can find two numbers e and d, such that m^ed = m mod n, for any m. e and d are like x and y, e will be the public key which the sender will use to compute m^e mod n, which is incredibly hard to undo. d will be my private key, which I will use to compute (m^e )^d = m mod n, this is like completing the full circle around the clock and arriving back at the original message. All public key encryption is based on these “circular” type operations, a full cycle takes you back to the original message, this is broken down into two steps, one for the sender and one for decryption, the sender makes a partial cycle, which is incredibly hard to undo, the receiver however knows some secret to complete the cycle.

In order to find d from the public knowledge, which is e and n, you need to prime factor n. However n is a very large multiple of two prime numbers, n = pq. It’s over 1000 digits long, and this makes it infeasible to compute the private key from the public key as there is no quick algorithm for factoring numbers.",hrfpzgg,t3_rx1mtm,1641433129.0,False
rx1mtm,"That sums it up real good, thank you!",hrfr8hm,t1_hrfpzgg,1641434161.0,True
rx1mtm,"You can also use this description to work backwards to see how signing things works. We each have an e and a d. e is our public key, d is our private key. To send me a message only I can read, you compute m^e mod n using my public key e. Now only I can read it because only I have the right d to use in (m^e)^d mod n.

To sign a message, you instead compute m^d mod n with **your** private key d. Now the only way anyone can read the message is if they have the right e to compute (m^d)^e mod n, and that choice of e has to be your public key. We can all get your public key, so the message is readable to anyone, but only you could have produced the plain text.",hrhuml5,t1_hrfr8hm,1641479952.0,False
rx1mtm,"It’s not clear what you mean by 
> and I sent it to the receiver through symmetric encryption

Public and private keys are used in the context of assymetric encryption. What is encrypted with the public key can only be decrypted with the private key.",hrfj9i4,t3_rx1mtm,1641430316.0,False
rx1mtm,If they don't know the private key how do they ow do they encript it so that only my private key can decrypt?,hrfl3m8,t1_hrfj9i4,1641431063.0,True
rx1mtm,With your public key,hrfos2i,t1_hrfl3m8,1641432573.0,False
rx1mtm,"Let’s give an example:

You want to send me a document secretly over a public network. I send you my public key but a spy also reads it. 

You encrypt the document on your computer with my public key. Then you send the encrypted document over the network. The spy steals a copy of the encrypted document.

The spy has the encrypted document and the public key. But he needs the private key to decrypt it and I never sent that one out so he cannot decrypt. 

I receive the encrypted document and only I can decrypt it since I am the only person to have the private key that matches the public key.",hrfpwr8,t1_hrfl3m8,1641433081.0,False
rx1mtm,"Where where you 2h ago XD I get it, i thought the public key was something that already existed, and not something that I create at the same time I create the private one...",hrfqzr4,t1_hrfpwr8,1641433934.0,True
rx1mtm,"I like to think of the ""public key"" as a lock you can put on data, and the ""private key"" as the key to that lock",hrgx34m,t3_rx1mtm,1641459400.0,False
rx1mtm,You create key pairs. A private key to decrypt and a public key to encrypt. You exchange public keys with someone to send encrypted messages between yourselves. You use that person public key to encrypt a message and send it to them. The only thing that can unencrypt that message is that person associated private key. They do the same thing sending you a message by using your public key to encrypt the message and you use your private key to decrypt it.,hrfkrvw,t3_rx1mtm,1641430929.0,False
rx1mtm,How do they encript in a way that only my private key can decrypt?,hrfkzys,t1_hrfkrvw,1641431021.0,True
rx1mtm,They use your public key. The only thing that can decrypt a message encrypted with your public key is your private key,hrflk3v,t1_hrfkzys,1641431246.0,False
rx1mtm,Can't everyone else use the public key to also find the private key?,hrfln0n,t1_hrflk3v,1641431280.0,True
rx1mtm,"No. The key pairs are asymmetric. The public key can be openly distributed without compromising your private key.

The generation of such key pairs depends on cryptographic algorithms which are based on mathematical problems termed one-way functions.

https://en.m.wikipedia.org/wiki/One-way_function",hrfnksu,t1_hrfln0n,1641432083.0,False
rx1mtm,Yeah that's what I was having problems with... Thank you,hrfo77r,t1_hrfnksu,1641432336.0,True
rx1mtm,"Yes, but it’s very difficult to do that. It takes a certain large number of calculations, so it is practically impossible to find someone’s private key by using their public key. More specifically, for RSA encryption, you would have to efficiently perform [integer factorization](https://en.m.wikipedia.org/wiki/Integer_factorization).",hrfo61z,t1_hrfln0n,1641432323.0,False
rx1mtm,"I got it! Out of curiosity though, is there already a solution for when computers get more powerful and we can efficiently perform integer factorization?",hrfol2r,t1_hrfo61z,1641432493.0,True
rx1mtm,"The algorithm already exists! It's just that the the time needed by the fastest supercomputers we have today to break it would be... well, the universe itself won't last that long.

And since Moore's law is slowing down, i.e. there is a limit of how much faster computers can get, then we can fairly confidently say classical computers won't ever be able to defeat such encryption.

But there are now quantum computers, which are especially good at such kind of tasks, thus posing a serious threat to all of the cryptography.",hrgx16x,t1_hrfol2r,1641459354.0,False
rx1mtm,https://en.m.wikipedia.org/wiki/Public-key_cryptography,hrfn4ey,t1_hrfkzys,1641431896.0,False
rx1mtm,by using your public key,hrfl4wm,t1_hrfkzys,1641431077.0,False
rx1mtm,That way can't everyone else do the same?,hrfla0d,t1_hrfl4wm,1641431133.0,True
rx1mtm,ok so they encrypt the message to you with your public key right? well you use your private key (THAT ONLY YOU KNOW) to decrypt it! Who cares if people can send you encrypted messages? only you can decrypt it!,hrfmpyk,t1_hrfla0d,1641431729.0,False
rx1mtm,"Oohh i get it now, they are both generated by me right, and I share the public one, that is the one used by the receiver...
I don't know how they are generated but that is work for later... Thank you",hrfo28w,t1_hrfmpyk,1641432281.0,True
rx1mtm,"Yep exactly, it's called assymetric key encryption for that reason. Different keys are used for encryption and decryption. Symmetric key encryption on the other hand uses the same key for both.",hrgz5eu,t1_hrfo28w,1641461036.0,False
rx1mtm,"Check out GPG, https://en.m.wikipedia.org/wiki/GNU_Privacy_Guard",hrfovz1,t1_hrfo28w,1641432618.0,False
rx1mtm,I will!,hrfr4ob,t1_hrfovz1,1641434062.0,True
rx1mtm,are you trying to learn PGP encryption?,hrfpj6u,t1_hrfo28w,1641432881.0,False
rx1mtm,"I am not, i just needed to understand how public key encryption worked, the rest is just curiosity",hrfr06m,t1_hrfpj6u,1641433946.0,True
rx1mtm,The encryption and decryption are asymmetric. Meaning they are one way functions. Someone with your public key can only encrypt a message. Someone with your private key can only decrypt the message encrypted with your public key. This is why you keep your private key private and secure. You usually have a passphrase associated with it for added security.,hrfmue9,t1_hrfla0d,1641431780.0,False
rx1mtm,"You keep your private key,  well...private.",hrflo70,t1_hrfla0d,1641431293.0,False
rx5u9h,"Hi there! Curious, interesting question - but seemingly it may need some clarifications before being approached.

How do we deal with games in which player failed (hit mine) before clearing the field? Just dispense, don't regard at all?

By the ""least upper bound"" you mean in a case of some specially constructed arrangement (to reach such bound), not randomly placed? This seems to depend greatly on relation between number of mines and field size, right?",hrgab6n,t3_rx5u9h,1641444970.0,False
rx5u9h,"Yup to both questions. We only consider a sequence of guesses that could result in a win, but require some cumulatively amazing luck.

And you are a mastermind and specifically constructed the grid yourself to make your unluckiest players feel as miserable as possible. I also sense the assignment of mines to squares is important, but no clue which arrangement would be most devilish",hrgb80m,t1_hrgab6n,1641445427.0,True
rx5u9h,"I don't know the answer. But in my playing experience, the true 50:50 are a lot more common near the edges. Also, very important factor is mine density (which is given by you already). I believe that linear increase in mine density results in exponential growth in true 50:50. But this is just my opinion.

Given this and the answer you gave to RodionGork, I think, for general case you need to find the least mine requiring and compatible side by side with itself true 50:50 and spam it all over the plane. An example of this would be a 2 x N grid with N amount of mines, where each collumn has 1 mine in it:

|x|x|3|(3 or x)|...|2|
|:-|:-|:-|:-|:-|:-|
|2|3|x|(the other than above)|...|x|

This pattern has N-1 (considering you know first click is not a bomb) true guesses with 50% mine density. I doubt there can be more guess heavy pattern, as I mentioned before, edges are breeding ground for true 50:50's.",hri0a7o,t3_rx5u9h,1641482243.0,False
rx5u9h,"If you have a wall of mines in the 3rd row from an edge, then pretty much all the cells in the first two rows need guessing. Then you can probably just partition the grid every 3 rows and design a grid so that half of the mines need guessing (half constitutes the ""3rd row wall"" and the other half needs guessing). This will probably work when X can go all the way up to ~2MN/3",hrkutl3,t3_rx5u9h,1641521010.0,False
rwly7h,I'd make this a top-6 and add any clean coding book by Uncle Bob.,hrd8i4q,t3_rwly7h,1641398423.0,False
rwly7h,"Thanks for sharing. 

Will definitely come in handy for aspiring software developers like me :-)",hrd1el6,t3_rwly7h,1641395628.0,False
rwly7h,"I like “The effective engineer”

Thanks for the list, definitely going to do some more reading this year 🙂",hrfng63,t3_rwly7h,1641432030.0,False
rwly7h,"Grabbed the Passionate Programmer after reading your post!  I'm only a year or two into the industry and am having many questions, a lot of self-reflection, and fork in the road moments, so hopefully that book will provide some sort of benefit, thanks!",hrhe4e2,t3_rwly7h,1641471822.0,False
rwnjr8,[deleted],hrcxlic,t3_rwnjr8,1641394056.0,False
rwnjr8,"I would recommend using a library such as sklearn if possible. [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

  
They have parameters to control the initialization and also the number of different runs.

Also checkout this: [https://en.wikipedia.org/wiki/K-means%2B%2B](https://en.wikipedia.org/wiki/K-means%2B%2B)",hrdybc3,t1_hrcxlic,1641407775.0,True
rwv1vj,"They get published in the same way as any other paper via the peer review process. Theoretical papers are assessed based on the quality of the proof (usually more than one) associated with the theory, the underlying evidence of the theory, how well it is explained, the significance/impact, etc.

You might want to look at theoretical engineering journals and theoretical computer science journals to get an understanding of how such papers are written.  


International Journal of Industrial Engineering

Engineering Science and Tecnology

Research in Engineering Design

Systems Engineering - Theory and Practice

Engineering Science and Technology

SIAM Review

Foundations and Trends in Theoretical Computer Science",hria1et,t3_rwv1vj,1641485985.0,False
rwv1vj,"There are also a few conferences dedicated to ""space"" systems, see for example the NASA formal method conferences, https://easychair.org/cfp/nfm-2022

You can take a look at work published there to get an idea.",hrixzij,t1_hria1et,1641494728.0,False
rx5nj6,I have an exercise for PDA if you are interested in that.,hrh3dvn,t3_rx5nj6,1641464349.0,False
rx5nj6,"Hm, I'm so noob so that it seems to be first time I hear about PDA (push down automaton?) - so would eagerly study whatever you share!",hrh9eo0,t1_hrh3dvn,1641468782.0,True
rx4z0z,2 kibibit,hrg71mi,t3_rx4z0z,1641443251.0,False
rx4z0z,On 8-bit machines we'd call 256 bytes a page. Not really required anymore so I'm fairly sure it's fallen out of use.,hrgzxwn,t3_rx4z0z,1641461659.0,False
rx4z0z,"Wow, this is cool info, I trust you but could i get a source on that so i can justify using the term myself?",hsd956g,t1_hrgzxwn,1642006289.0,False
rx4z0z,"If you're talking about modern systems you'd best ignore my vintage waffle and refer to this:
https://en.wikipedia.org/wiki/Page_(computer_memory)

If dealing with old 8-bit systems like the 6502 then it's surprisingly difficult to find anything direct. Mentions of zero-pages (first 256 bytes of memory) or page-boundaries (crossing from one 256 byte chunk or memory to another) here:
https://en.wikipedia.org/wiki/MOS_Technology_6502#Bugs_and_quirks

Maybe I just distilled the idea of pages in my own head?!",hsdzqwk,t1_hsd956g,1642016242.0,False
rx4z0z,"If we are talking about naming data size units, then the following may also be interesting to you:

* **1 kilobyte** is precisely **1000 bytes**, while **1024 bytes** is **1 kibibyte**
* **1 megabyte** is precisely **1000000 bytes**, while **1048576 bytes** = **1024 kibibyte** = **1 mebibyte**
* **1 gigabyte** is precisely **1000000000 bytes**, while **1073741824 bytes** is **1 gibibyte**

Check out [this table on Wikipedia](https://en.wikipedia.org/wiki/Byte#Multiple-byte_units)",hrhpco3,t3_rx4z0z,1641477676.0,False
rx4z0z,Chom p,hrgz81i,t3_rx4z0z,1641461093.0,False
rx4z0z,quarterkilo,hroku79,t3_rx4z0z,1641586178.0,False
rvy218,"Good work, you put in the effort and know you’ve not only taught others but made sure you know it yourself.",hr8no35,t3_rvy218,1641319035.0,False
rvy218,"Yes, ai find it an effective way to go back to the basics.",hr8siun,t1_hr8no35,1641320922.0,True
rvy218,"I've been thinking I needed a refresher on my linear algebra lately, thanks for posting this.",hr90cpt,t3_rvy218,1641323914.0,False
rvy218,You're welcome! Will post more linear algebra posts so keep an eye out 😏👀,hr96m79,t1_hr90cpt,1641326326.0,True
rvy218,Looking forward to them!,hr96zj4,t1_hr96m79,1641326470.0,False
rvy218,Fantastic! 🙌🔜,hr97ezm,t1_hr96zj4,1641326635.0,True
rvy218,"Next up : Cayley-Hamilton Theorem and calculating A^(k) with it.

""*A square matrix satisfies it's own characteristic equation*""",hr98zkp,t3_rvy218,1641327238.0,False
rvy218,Love the A^k calculation 🤩,hr9fuv1,t1_hr98zkp,1641329841.0,True
rvy218,"Thanks, I hate it",hr9azri,t3_rvy218,1641328003.0,False
rvy218,😛 you're welcome 😁,hr9fyri,t1_hr9azri,1641329882.0,True
rvy218,Imma save this for later and look out for more. Thank you!,hr983pq,t3_rvy218,1641326898.0,False
rvy218,🧘‍♂️👌,hr9fx1n,t1_hr983pq,1641329864.0,True
rvy218,"Oh my days .. eigenvectors and eigenvalues!! 

Quite interesting topic especially in machine learning.",hr9ut6y,t3_rvy218,1641335589.0,False
rvy218,"Yeah, it is one of my favorite topics from linear algebra. :) hope you got something out of it.",hr9vvli,t1_hr9ut6y,1641336007.0,True
rvy218,Yes I did .. saved the visuals also .. thank you.,hr9x0sz,t1_hr9vvli,1641336459.0,False
rvy218,Enjoy reading it. Looking forward to your next post!,hragcu3,t3_rvy218,1641344279.0,False
rvy218,Thanks for the encouragement! 💯,hrbsf8p,t1_hragcu3,1641368038.0,True
rvy218,Don't forget page rank!,hrc6qf0,t3_rvy218,1641378867.0,False
rvy218,Oh wow! I let that one slip under the radar.,hrc6wj1,t1_hrc6qf0,1641378995.0,True
rw99z6,https://en.wikipedia.org/wiki/Wolfram's_2-state_3-symbol_Turing_machine,hrakk3s,t3_rw99z6,1641346039.0,False
rvq0w4,"Yes. I know how overvolting and overheating affects computers in chemical level, and what to expect.

If you gonna write scientific software for chemistry, you gonna need a lot of fundamental stuff while programming.",hr761bd,t3_rvq0w4,1641293199.0,False
rvq0w4,Nope.,hr73gdb,t3_rvq0w4,1641291171.0,False
rvq0w4,Some of the phys chem shows up in reversible computing and things like Landauer’s limit.,hr7y5vb,t3_rvq0w4,1641308931.0,False
rvq0w4,"Nope.

But: Math a lot, physics kind of, biology even, and all the language classes.",hr7oe8t,t3_rvq0w4,1641304497.0,False
rvq0w4,How did you implement the languages in your code?,hr9cci8,t1_hr7oe8t,1641328517.0,False
rvq0w4,Knowing several languages with very different syntax helps when you get into functional programming or other declarative languages; it’s also nice to know some linguistics when you are tasked with coding UI strings for an international application; or some phonetics when implementing a “fuzzy match” algorithm for a search engine in a language not supported by existing Soundex implementations.,hr9fadq,t1_hr9cci8,1641329626.0,False
rvq0w4,NLP,hr9gfrf,t1_hr9cci8,1641330060.0,False
rvq0w4,"Lots of my colleagues do, although computational chemistry is kinda cheating.",hr7xukj,t3_rvq0w4,1641308800.0,False
rvq0w4,"I've written software to control robotic arms used in chemical test apparatus, if that counts?
 
There's an entire field of medical technology that is the intersection of biology (including chemistry) and computer science if that's something you're interested in. Has a bunch of equivalent names and subfields like digital health, health technology, e-medicine, e-health, telehealth, bioinformatics, many others.",hr844co,t3_rvq0w4,1641311377.0,False
rvq0w4,"All the time, but I’m in a niche field for it.",hr86ppf,t3_rvq0w4,1641312415.0,False
rvq0w4,"Tried, but",hr7hvse,t3_rvq0w4,1641301086.0,False
rvq0w4,"Chemistry is where you’re most likely to learn things like the ideal gas law and Newton’s law of cooling. Both can be relevant in any situation where you have something generating heat that needs to be dissipated, like a CPU.",hr897ia,t3_rvq0w4,1641313413.0,False
rvq0w4,Only when sharing knowledge tidbits at the coffee machine.,hr8bdfq,t3_rvq0w4,1641314268.0,False
rvq0w4,"Yes. Inorganic chemistry is important to hardware development. 

If you're looking to do higher level development, then it's less important, although there are computational chemists.",hr8iiyi,t3_rvq0w4,1641317065.0,False
rvq0w4,"yes, i turn coffee into code",hr94h8n,t3_rvq0w4,1641325502.0,False
rvq0w4,"Bioinformatics graduate here. The only thing I used was in the context of Chemical and Biological weapons used in terrorism and that was because I was a DoD software engineer contractor....

Nothing else. Big waste of time and organic chemistry suffering.",hr95avk,t3_rvq0w4,1641325818.0,False
rvq0w4,"I’m pretty sure I tested out of chemistry 1 and only had to take chem 2. Was a bit more relevant for the EE part of my EE/CS degree. 

Other than that, no. I think I used Coca-cola to clean off the car battery in my shitty Camaro to get to work…that’s about it.",hr9c9kq,t3_rvq0w4,1641328486.0,False
rvq0w4,Never again. Lol,hr9smwc,t3_rvq0w4,1641334744.0,False
rvq0w4,"I avoided all the chemistry classes I could. Really depends what you want to go into, most probably don't need it but there are always those specialized areas where it's needed.",hraksse,t3_rvq0w4,1641346143.0,False
rvq0w4,I do in computer engineering. But I'm in an odd niche researching chemical reaction networks and genetic circuits.,hrax5to,t3_rvq0w4,1641351342.0,False
rvq0w4,"Also, in same vein have y'all ever used discreet math, calculus in your career?",hr75mpb,t3_rvq0w4,1641292886.0,False
rvq0w4,"Indeed, it's nice skill to have when building stuff related to ML/DL, as well as in other CS areas, like cryptography and computer graphics (probably also others that i never worked with and as such aren't aware).",hr79b4f,t1_hr75mpb,1641295613.0,False
rvq0w4,"Discrete maths yes (quite a broad field though) but calculus no. Still glad I learned calculus though, definitely gives you a broader understanding of maths, which indirectly will improve your programming.",hr7yfty,t1_hr75mpb,1641309047.0,False
rvq0w4,All the time.,hr7yu7r,t1_hr75mpb,1641309219.0,False
rvq0w4,My whole Ph.D. and the post-grad studies build upon these and how to calculate that stuff faster.,hr762o4,t1_hr75mpb,1641293228.0,False
rvq0w4,"Only the smart ones need DMath, Calc, Combinatorics, and Linear",hr86jwe,t1_hr75mpb,1641312350.0,False
rvq0w4,Computers are discrete math machines. Programming languages are discrete math languages.,hr8ffhk,t1_hr75mpb,1641315860.0,False
rvq0w4,Yes actually.,hr8orw4,t1_hr75mpb,1641319461.0,False
rvq0w4,Absolutely,hr9burh,t1_hr75mpb,1641328330.0,False
rvq0w4,"Even if you do not use the math itself, the method of thinking is the same",hr7vhva,t1_hr75mpb,1641307784.0,False
rw2dl6,A pointer is directly a memory address. Most high level languages don't allow you to work directly with them at all or at least try to prevent it. A python id is a unique integer identifier but is NOT a memory address.,hr97jwh,t3_rw2dl6,1641326688.0,False
rw2dl6,"Theres quite a bit difference.

For example you cant do any pointer arythmetic explicitly in python, with or without the usage of id.

More generally, you cant reliably infer an id of one object based on the id of another object even if you know how the code looks ljke.

I havent found a place to use this function to be honest, i assume python use it mostly under the hood.",hr96fn7,t3_rw2dl6,1641326255.0,False
rw2dl6,"I have onky used id to store objects in a dictionnary, it gives a hashable key that is unique, even if two objects have the same representation at a some point.

You could also use it to check if two objects are the same. Not only the same representation, but exactly the same memory, if you change one, both change. But the ""is"" do this and is more readble.

So it can be useful, but in quite specific places, and not for any low-level pointer operations.",hrbslmf,t1_hr96fn7,1641368162.0,False
rvfquw,"It wasn't a problem with storing a date, but a version number which was beginning with date.

So MS has such versioning convention: `YYMMDDxxxx` (where `x` are not important for us here).

So lets create a smallest number for 2022 which fits this convention: `2201010000`  
As you can see, it's bigger than `2147483647`",hr56orc,t3_rvfquw,1641254619.0,False
rvfquw,they should of just used long long,hr6p6sq,t1_hr56orc,1641280215.0,False
rvfquw,or unsigned int,hr70qzb,t1_hr6p6sq,1641289017.0,False
rvfquw,What about the time before Christ? Can't forget about the dinasours,hr755vl,t1_hr70qzb,1641292519.0,False
rvfquw,I didn't know the dinosaurs were fewer than 2022 years before Christ...,hr7jaws,t1_hr755vl,1641301879.0,False
rvfquw,what about 1999,hr7kvbf,t1_hr755vl,1641302720.0,False
rvfquw,Why not string,hr73x49,t1_hr6p6sq,1641291545.0,False
rvfquw,"They were using the version number to check if the version installed is less than than the latest patch available, so they would have had to convert it to a numeric format anyway for comparison since string < string would produce unreliable results depending on the platform.

They could have used a long (64 bit number), unsigned integer, or done a bit of bit-shifting to more efficiently pack the date and version number into 31 bits (apparently one bit was reserved for something else).",hr8e3eh,t1_hr73x49,1641315333.0,False
rvfquw,"> Why did the Microsoft date bug happen in 2022?

Apparently, their test suite sucks.",hr6eoao,t3_rvfquw,1641273796.0,False
rvfquw,How many tens of thousands of employees do you have to hire before someone writes a unit test these days?,hr6jwii,t1_hr6eoao,1641276748.0,False
rvfquw,Would be good to just run the suite with a date a year in the future... they could have caught this a year ago instead of testing in production.,hr6llgq,t1_hr6jwii,1641277811.0,False
rvfquw,Did you read any of the actual coverage of the story? Genuine question; every article I read explained quite clearly what the problem was...,hr7nkqy,t3_rvfquw,1641304103.0,False
rvc4y3,"I mean Minecraft is a great example of programs in general, even being able to compare the same app written in two different languages, Java and Bedrock editions.",hr4j6yz,t3_rvc4y3,1641245256.0,False
rvc4y3,"I think most of you misunderstood me haha. I don’t mean that I want to teach within minecraft, but how the program itself could be programmed. Like which classes, dependencies etc. you would need to program Minecraft",hr5bfl3,t3_rvc4y3,1641256592.0,True
rvc4y3,"I like the classic ""dog and dat and both subclasses from animals"" approach, but you sure can explain OOP using Minecraft.",hr5h72g,t1_hr5bfl3,1641259013.0,False
rvc4y3,"Dog and cat being an animal is one of the worst introductions to OOP that is responsible for so much misunderstanding of what inheritance is to the point it could be given a trophy for worst teaching analogy of all time.
 
First, even if we are leaving composition over inheritance to later, by introducing it that way the misunderstanding you invite causes repeated breaches of the Liskov substitutability principle, because students learn to model their code based on the taxonomy in the real world, which is completely wrong.

Inheritance is not an 'is a' relationship from the real world, that is a gross oversimplification by people who didn't know what inheritance was for. The classic counterexample is square and rectangle - [see this talk by Bob Martin](https://www.youtube.com/watch?v=zHiWqnTWsn4&t=4430s).
 
Rather than modelling the real world behaviour of things in classes, it is far better to approach classes for what they actually are - collections of **software behaviour**. That way you never fall into the square-rectangle trap, but you still arrive at the shape-square and shape-rectangle behaviour (which is also in that talk, it's a good one to watch start to finish).
 
While you could do that with dog and cat, you've gone and abstracted software needs by swapping in 'real world' needs like walk and speak, which makes it really confusing when students are then writing their own code based on software needs.",hr5yyzs,t1_hr5h72g,1641266455.0,False
rvc4y3,"I got your point and the presentation you linked is pretty good too, but I don't think it would a good approach to introduce OOP directly with software behavior. At this stage, people barely know how to write a ""real life"" software.

Also while SOLID is desirable while implementing OOP, they are distinct topics and should be teached separately. For the core concepts of OOP, the real life examples should work fine. When teaching SOLID you can then revisit this examples and show why they doesn't work under SOLID, just like the guy in the video did.",hr79iza,t1_hr5yyzs,1641295774.0,False
rvc4y3,"You don't teach SOLID as a 'thing' on day 1, but you absolutely need to exemplify it and make it a habit in your examples and discussions from day 1. Otherwise you are creating bad habits and assumptions that never need to exist.
 
There are 3ish schools of thought on when they should learn OOP at all, objects-first, objects-early, objects-late. If they are objects-late, then you can give them whatever you like, because they can craft some decent procedural code and you can build on those concepts. Remember that there was a point in time where *everybody* learning OOP was in this category.
 
If they are objects-early, then they already know enough that you can draw out objects and a class structure motivated by the behaviours they already know about. A Zork clone is more than sufficient, which you can learn enough programming to create (procedurally) in a day or 2 boot camp. Rooms, exits, items in the rooms, etc. Easy peasy, and you can even show them the flaws in creating an inheritance structure based on is-a real world relationships when you do so, by comparing your 'what common behaviours do we want to bundle' vs 'let's model a maze in real world concepts' versions. It doesn't need to be 'real life' code, it needs to be real motivations from the software, even if that software is trivial.
 
If they are objects-first, then the best thing to do is give them a scaffold with a canvas object and do shapes. You can do this with BlueJ in the first hour they are at a keyboard.
 
If you are teaching them any other way, then what you have almost certainly been blinded by is the **syntax** of OOP, which is both the easiest and stupidest thing to teach a newbie student. Yes, they'll need to know the syntax of whatever language, they'll need to learn how access modifiers work, they'll need to understand scope, etc etc etc. But that is all crap they can learn by breaking things along the way, and is a distraction from the more useful starting concepts. If they cut and paste boilerplate for a semester but can successfully grow a useful hierarchy out of some software requirements or procedural pseudocode they are light years ahead of someone who has memorised the syntax but will inherit from Rectangle.",hr7cozo,t1_hr79iza,1641297928.0,False
rvc4y3,"Totally agree that an actual real world programming example is much better than the classic dog, cat, animal teaching.",hr64zzf,t1_hr5yyzs,1641269112.0,False
rvc4y3,What? Nah you should build a red stone computer and teach programming using that /s,hr802eb,t1_hr5bfl3,1641309730.0,False
rvc4y3,"Great idea, for more advanced units you could even install computer craft which allows you to write LUA code to control little robots and other blocks",hr4ohyi,t3_rvc4y3,1641247292.0,False
rvc4y3,I’m not sure dependency injection is used in Minecraft?,hr59n7w,t3_rvc4y3,1641255847.0,False
rvc4y3,You really think minecraft a game that literally sold for a billion dollars and is now run by Microsoft doesn't have aby dependency injection? For some reason I really doubt that,hrx8n11,t1_hr59n7w,1641742046.0,False
rvc4y3,"Dependency injection isn’t useful in all scenarios. As I understand it, it’s less common in game development than it is say, enterprise backend development

Them being Microsoft and having lots of money and programming experience doesn’t mean they’re going to use a certain pattern every single time.",hrxza2r,t1_hrx8n11,1641752140.0,False
rvc4y3,Dependency injection at a basic level is greatly useful for making classes testable and reusable. Surely it's riddled throughout minrcrafts code base.,hry710p,t1_hrxza2r,1641754849.0,False
rvc4y3,"In my opinion Minecraft redstone(which can really be thought of as a massive, 3d state machine) is a great way to teach computer architecture, but not necessarily higher level softer design. You can use it to construct simple computers that reflect the same high level digital logic in real computers. I can honestly say that my understanding of the intricacies of a computer has really been solidified after building a redstone computer",hr58nrj,t3_rvc4y3,1641255440.0,False
rvc4y3,They have an entire curriculum and separate game that teaches coding [here](https://education.minecraft.net/en-us/get-started),hr4ynso,t3_rvc4y3,1641251321.0,False
rvc4y3,"Any game is going to be perfectly fine to teach with - if you're teaching at a professional level where you need to grade it's going to be hard with such a well-covered piece of software to know what is your students' work and cut and pasted without understanding from the internet, so for that reason I make sure to keep it well away from any assessments and stick with lesser known games if needed.",hr5zd7j,t3_rvc4y3,1641266624.0,False
rvc4y3,just teach logic in highschool,hr5o0j5,t3_rvc4y3,1641261860.0,False
rw311s,"When using pointers, you are mostly operating on virtual memory space provided by your operating system, which takes care of managing memory for each process. Internally, mechanisms like paging are used to divide the space into equal sized pages which can be put to swap space and reloaded from disk when needed. The MMU takes care of translating virtual addresses to physical memory addresses. It’s actually a lot more complicated than that, but your operating system knows exactly which process allocated which parts of memory and keeps track of everything including memory boundaries and more.",hr983ax,t3_rw311s,1641326894.0,False
rw311s,"Theres a few mechanisms in place, all are way too complicated to write on a single reddit post.

In a nutshell, the **OS** gives each process the **illusion** it has a big and clean memory space. Which means a few processes can reach into the address 0x1234 and store data there. 

All of this illusion (called effective addresses) is being handeled at the os. Then a combined effort of the os and the harware to translate this effective address into real phyiscal address in the dram.

This has nothing to do with any perticular progremming languege, you can take advantages of it to gain some speed up - but for most applications its not needed.",hr988u7,t3_rw311s,1641326953.0,False
rw311s,https://www.freecodecamp.org/news/understand-your-programs-memory-92431fa8c6b/,hr9jsxl,t3_rw311s,1641331338.0,False
rw311s,"It depends.

If `my_var` is on the stack, then its address increments/decrements (depending on the CPU architecture) as new variables are allocated. The CPU has a register to track the latest stack location in memory (called a stack pointer), and the compiler knows how much to add/remove from this register to reserve space in the memory for new variables.

For example, if I create two local variables (local variables get created on the stack), then their addresses should differ by the size of those variables, possibly rounded up to the nearest word. `long` is equivalent to one word or its multiple on most OS so there shouldn't be any rounding in this example:

    #include <iostream>
    
    int main() {
        long on_stack1;
        long on_stack2;
    
        std::cout << ""Address of on_stack1: "" << &on_stack1 << std::endl;
        std::cout << ""Address of on_stack2: "" << &on_stack2 << std::endl;
        std::cout << ""The difference in the address should be "" << sizeof(long) << "" bytes"" << std::endl;
    
        return 0;
    }

The value of the stack pointer is initially set by the OS when the program starts.

If, on the other hand, `my_var` is on the heap (e.g., created with `new`), then the address is dynamically allocated by the memory allocator (which comes with the C/C++ library for the OS) so how it allocates the memory depends on the allocator's algorithm. To avoid the heap's memory allocator accidentally using the memory used by the stack, the allocator tries to reserve heap memory on the ""opposite"" end of the memory address from the stack as far away from it as it can. You can see a typical memory layout here: [https://www.geeksforgeeks.org/memory-layout-of-c-program/](https://www.geeksforgeeks.org/memory-layout-of-c-program/)

    #include <iostream>
    
    int main() {
        long on_stack;
        long* on_heap = new long;
    
        std::cout << ""Address of on_stack: "" << &on_stack1 << std::endl;
        std::cout << ""Address of on_heap: "" << on_heap << std::endl;
        std::cout << ""The address of on_heap should be far from on_stack."" << std::endl;
    
        return 0;
    }

If you use a lot of memory it is possible for the stack to crash into the heap. And it is possible for a memory allocator to fail to allocate a large chunk of memory even if there are many small pieces of memory due to memory fragmentation. A part of the memory allocator algorithm's goal is to minimize such memory fragmentation.

Also, if `my_var` is a global variable or a constant, the compiler reserves a space for it within the binary so that when the program is loaded into the memory the variable is already reserved an address. Its address can be calculated as the starting address of the program + offset to the variable in the binary.

    #include <iostream>
    
    const char* on_global = ""You should see this string in the binary"";
    
    int main() {
        std::cout << ""Address of a constant: "" << (void*)on_global << std::endl;
        std::cout << ""Address of a global variable: "" << &on_global << std::endl;
    
        return 0;
    }

As others have pointed out, modern OSes, with the assistance of modern CPU architectures, use virtual memory so the addresses printed out by the above programs are not the memory's physical addresses. There exists a memory mapping mechanism to use the memory more efficiently and protect one program's memory from each other. I like these videos for understanding virtual memory: [https://www.youtube.com/watch?v=qcBIvnQt0Bw&ab\_channel=DavidBlack-Schaffer](https://www.youtube.com/watch?v=qcBIvnQt0Bw&ab_channel=DavidBlack-Schaffer)",hrjhmdt,t3_rw311s,1641501927.0,False
rvrtx4,"No, not really.

A single clock cycle is the shortest period of time within which you expect anything to happen within the CPU. It's the resolution of time for the CPU, in a sense.

It takes time, however brief, for electrical signals to propagate within the CPU. It takes time for transistors to switch.

When the CPU runs, say, an instruction to add two integers together, it does that by passing current through a logic circuit that produces the correct output bits from the input bits. The output is not instantaneous. Rather, the CPU would check the output at the next clock cycle (or possibly after a set number of clock cycles).

Disclaimer: I'm not an electrical or electronics engineer. If someone knows better and I said something wrong, go ahead and correct.",hr7by93,t3_rvrtx4,1641297441.0,False
rvrtx4,"Moreover, OP should look into how different CPU architectures handle complexity, because 2.5Ghz in CISC is different compared to RISC and different layouts, number of cores, type of parallel processing, semaphores etc. impact CPU speed differently. Which is why when buying a CPU you should always look at benchmarks rather than specs alone",hr874c9,t1_hr7by93,1641312577.0,False
rvrtx4,Read [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code),hr8g71l,t3_rvrtx4,1641316163.0,False
rvrtx4,"No. The short, hand-wavy reason why most CPUs have a clock frequency is because they are synchronous circuits. These use a periodic signal, the clock, to coordinate their operation. The opposite of synchronous circuits would be asynchronous circuits, where there isn't a clock signal coordinating activities, but a multitude of control signals that implement some sort of protocol for sequencing events.

Regarding the frequency at which bits move around in a CPU; what is quoted as the CPU's clock frequency has some relationship to the rate at which bits move around, but that's missing the point of why there is a clock in the first place (which isn't to describe the rate at which bits move around). The reason why there's some relationship is because there will be parts of the CPU where data being moved to different parts, or processed, at the same rate as the clock frequency. This isn't a hard requirement; it's a result of there being a clock signal. There will be parts where data moves at a lower rate than the quoted clock rate; and possibly at a high rate in others.",hr8j26c,t3_rvrtx4,1641317270.0,False
rutayn,"Quantum computers don’t just magically run classical algorithms faster than classical computers. It’s more like the quantum computer can do NEW special operations in a single time step that a classical computer would take MANY time steps to emulate. Some problems can be made dramatically faster if a NEW algorithm is invented that uses these special NEW operations.

An analogy would be like comparing a computer that can only do one add in a time step with one that can do a multiplication in a time step. If you are trying to multiply a number by n the adding machine might do it on O(n) by adding in a loop, but the machine that can multiply directly is just O(1).",hr1fgul,t3_rutayn,1641188514.0,False
rutayn,What are these “new special operations”?,hr3254j,t1_hr1fgul,1641225291.0,False
rutayn,"There are Quantum-specific logic gates like the Hadamard gate, CNOT, and many others that do operations on one or multiple qbits and manipulate the state of the qbit before it is observed.",hr3e7cx,t1_hr3254j,1641229992.0,False
rutayn,Layperson here. So does this double the logical output of all the execution units in a circuit? Instead of having q and qbar we could halve the footprint of chips? And qutrits could theoretically triple the logical output of a given execution unit?,hr3tvdt,t1_hr3e7cx,1641235751.0,False
rutayn,"It’s not so simple as “double” or “triple”. At a high level (simplified) the quantum computer lets us setup “complex” quantum states, and combine them together in certain ways so the intermediate results have complex entanglement and setup a kind of interference so the answer falls out at the end. It is not easy to explain as these new quantum operations are HIGHLY NON INTUITIVE. If you want to dive deeper you could look at [the Wikipedia entry on quantum computation language](https://en.m.wikipedia.org/wiki/Quantum_Computation_Language) to start getting and idea of what these new operations are and how they could be used. However I suspect that very few programmers in the future will be building these quantum algorithms directly and will mostly use higher level libraries which deal with the quantum particulars and have “standard” classical APIs. In other words it won’t be that far off from how GPUs can be used to “accelerate” some kinds of computations today, but aren’t used for the general purpose part of programs (which CPUs are excellent at already).",hr4cjd6,t1_hr3tvdt,1641242689.0,False
rutayn,"Interesting, thanks, giving it a read. I guess I was sort of imagining an extension to ""normal"" ISAs but that's clearly not the case. The possibility of a distinct set of programmers dealing with quantum functions is very interesting though.",hr4kaha,t1_hr4cjd6,1641245678.0,False
rutayn,"Probably the first few waves of professional quantum computer programmers will be those who already have backgrounds in quantum physics, mathematics and the like.",hr585fs,t1_hr4kaha,1641255234.0,False
rutayn,Probably? Better asking whether there will ever be any other quantum programmers!,hr59hrd,t1_hr585fs,1641255783.0,False
rutayn,Pretty much lol. Until quantum operations can be captured by a more convenient interface. But that's unlikely give the level of sophistication required.,hr59tq8,t1_hr59hrd,1641255920.0,False
rutayn,"> does this double the logical output 

What do you mean by this? Like double the number of booleans outputted by a given circuit?

> we could halve the footprint of chips

The physical requirements for quantum computers are entirely different from those of regular ones. Modern quantum systems have tens or dozens of bits, with some cutting edge ones having hundreds. While it's extremely likely that they will improve in a similar manner to the regular computers we are used to, it is definitely much too early to say that a quantum chip will have a smaller physical footprint than a a somehow comparable silicon chip. We are at the ""room sized computer"" stage, and the machines are cooled to near absolute 0 with refrigerated gasses.",hr4ft5o,t1_hr3tvdt,1641243945.0,False
rutayn,"> Like double the number of booleans outputted by a given circuit?

Yeah, basically. Like I'm seeing the [quantum logic gates with more outputs than a ""normal"" truth table.](https://en.wikipedia.org/wiki/Quantum_logic_gates](https://en.wikipedia.org/wiki/Quantum_logic_gate) I may not know enough linear algebra to wrap my head around it though.

I get your second paragraph though, I was silly not to think of that. Obviously traditional fabrication techniques won't work with quantum information. [I thought this was a pretty good explainer on the types of qubits.](https://www.youtube.com/watch?v=-5fKVn1GR9Y)",hr4jpo0,t1_hr4ft5o,1641245455.0,False
rutayn,"So you need to be careful here: there isn't some quantum magic that is adding bits or something like that. Every gate that outputs 2 qubits has 2 qubits as an input. The difference between a qubit and a regular bit is that regular bits are *always* only 0 or 1, and a qubit is a 0 or 1 *only when observed*. However, before a qubit is observed, when it is in superposition, it has some probability to collapse to a 0 or 1 on observation (and this probability can be 100% a 1 or 100% a 0). When gates operate on a qubit, they operate on its state in superposition; they change the probability that the qubit will collapse to one of a 0 or a 1. This lets engineers and scientists design rather creative algorithms involving linear algebra, which can often run faster than the classical algorithms used to solve similar problems.

If you'd like a clear intro, see https://www.youtube.com/watch?v=F_Riqjdh2oM, but be warned! It's all linear algebra.",hr4wzgc,t1_hr4jpo0,1641250643.0,False
rutayn,"Yes, quantum computers are very strange things that perform operations that are hard to understand and utilize.

Off topic, heavy tangent: I understand it was chosen for illustrative purposes, but I have to point something out, in case someone would end up thinking that implementing multiplication by adding in a loop is the way to go on platforms that lack multiplication opcodes. 🙂

Given A • B = C, the proper way to do it is by first decomposing one of the factors, say A, into a sum of powers of two. Then multiply B by each decomposed component of A, accumulating the result into C. Because the components are all factors of two, each multiplication ends up being a simple bit shift, which makes the algorithm perform in the order of O(log n).

If that's not fast enough (and on platforms that lack multiplication opcodes it frequently isn't), you can for example approximate the result of a multiplication by sacrificing memory for some lookup tables, and make use of the fact that log (A • B) = log A + log B.",hr4h5tu,t1_hr1fgul,1641244472.0,False
rutayn,Lol. I was expecting someone to call out that there are better algorithms than O(n) to do multiplication using only adding. I decided to leave the O(n) just to keep the explanation simpler and more understandable. You are of course correct. Thanks for keeping redit up to standards!,hr4jidx,t1_hr4h5tu,1641245378.0,False
rutayn,Great explication 👍🏼,hr25tup,t1_hr1fgul,1641207581.0,False
rutayn,"As far as I know, it doesn't necessarily reduce time complexity. A quantum algorithm has to be specially designed to leverage the benefits of the quantum computer.",hr1c56p,t3_rutayn,1641186680.0,False
rutayn,"[Shor's algorithm](https://en.m.wikipedia.org/wiki/Shor%27s_algorithm) shows how a quantum algorithm can solve a problem in faster time than a classical algorithm, the problem being find the prime factors of a given integer.",hr1fn80,t3_rutayn,1641188615.0,False
rutayn,"Quantum computers have characteristics like [entanglement](https://en.wikipedia.org/wiki/Quantum_entanglement) and [reversibility](https://www.linkedin.com/pulse/computational-reversibility-quantum-computing-sa%C5%A1a-savi%C4%87) .

This creates a computational paradigm that is different than your classical computer and therefore requires different algorithms and so far can only solve certain problems.",hr2xlyz,t3_rutayn,1641223392.0,False
rutayn,"u/OdinGuru’s answer is excellent! I’d like to add, if you’re interested in learning about quantum computing, I have found https://quantum.country to be an excellent introductory resource. It describes quantum operations as shortcuts. We can use these quantum operations to design new algorithms which can solve the same problems we already face in computing, sometimes utilizing the built-in shortcuts to result in faster runtimes. The key is sometimes. Algorithms have to be discovered which use these shortcuts to their full potential.",hr4rlou,t3_rutayn,1641248508.0,False
rutayn,[deleted],hr1vbxu,t3_rutayn,1641199270.0,False
rutayn,That's a completely wrong understanding of QC. Follow your own advice.,hr2blnn,t1_hr1vbxu,1641211781.0,False
ruum67,"This might be some core info for machine architecture / systems people, but I have to disagree that everyone needs to know this stuff. This seems far more intricate than anyone would reasonably need to learn in depth for a college CS degree, and with more and more people going into higher level software engineering, all the stuff here just seems more and more niche.

This is great stuff for someone looking to specialize into hardware, but for your average programmer? This is way too much.",hr3oi50,t3_ruum67,1641233794.0,False
ruum67,"I agree that the title could have been more appropriate. I think Ulrich Drepper got inspiration from the title of another paper that u/Cull_The_Meek mentioned titled ""What Every Programmer Should Know About Floating-Point Arithmetic"". Although I think that it should be a great read for all programmers because it just clears up a lot of confusion that you have while working with high-level langauges, but sure it isn't for an average programmer.

It seems like a good read to me coz I spend most of my time writing low level langs and fiddling with databases",hrgjrwy,t1_hr3oi50,1641450143.0,True
ruum67,Awesome! I once read “What Every Programmer Should Know About Floating-Point Arithmetic” and it was an eye opener for me. Saving this for my reading.,hr1sa1a,t3_ruum67,1641196909.0,False
ruum67,"> I once read “What Every Programmer Should Know About Floating-Point Arithmetic”

[This](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) one?",hr1vrzs,t1_hr1sa1a,1641199621.0,True
ruum67,Yup! It’s mentioned in the introduction of the article OP posted as well.,hr28rj6,t1_hr1vrzs,1641209785.0,False
rushx3,In relation to caching the article is just stating that an unaligned access may overlap TWO cache lines rather than one and that two cache loads is slower and eats up more cache space that a single load.,hr1e7sw,t3_rushx3,1641187815.0,False
rushx3,Eureka! Now I get it. Thanks a ton!,hr1ei6w,t1_hr1e7sw,1641187978.0,True
rushx3,"could you please share the link you're referring to?
I could explain it based on their perspective",hr16brx,t3_rushx3,1641183774.0,False
rushx3,I updated the post with the appropriate links.,hr17lg5,t1_hr16brx,1641184375.0,True
rushx3,"so the thing is CPUs do not access memory like we though they do
they do not have granular access to memory exactly like we have thought all this while

a 8bit CPU will have 1 byte of granular access
for 16byte it can load 2bytes at a time

now consider both these CPUs want to load want to read something at address 0x0001-0x0004 the 8bit CPU will be able to read the address directly from 0x0001-0x0004 onto the memory
but in case of the 16bit CPU it will first have to read 0x0000-0x0001
then perform a shift operation to set the actual starting point
for us it seems like a very nominal problem but that one extra cycle costs a lot, especially when there millions of these operations

now coming back to the real question, if the memory is unaligned you will tend to have more these shifting operations because you'd be loading more number of uneven memory addresses and then shift compared to placing it once and then shifting on from there

in case it is aligned you save on the reading and then shifting operation

carefully read the IBM link and observe the diagrams it will give you a better clarity

https://developer.ibm.com/articles/pa-dalign/",hr1dpvo,t1_hr17lg5,1641187536.0,False
rushx3,"yeah, that is what the IBM developer article explains. A lot of cpu cycles are saved when memory is aligned which is why aligned memory access is faster than unaligned memory access(also illustrated in the IBM developer article). Could you find any explanations on how aligned memory is good for caching?",hr1e8od,t1_hr1dpvo,1641187829.0,True
rushx3,it just reduces CPU instructions needed when the memory is aligned that's how the reads would become faster as less cycles are wasted to read the memory,hr1eko5,t1_hr1e8od,1641188016.0,False
rushx3,"I kinda get it now. If data is unaligned, and the CPU caches a cache line, there is a possibility that the data would be chopped off and wouldn't be cached properly. Because only a portion of the data is cached, the CPU might then cache another cache line containing the other part of the data which wastes cache and also it wastes cpu cycles which are required to shift and combine the portions of the data.",hr1f66i,t1_hr1eko5,1641188348.0,True
rushx3,yes absolutely,hr1g38q,t1_hr1f66i,1641188871.0,False
rushx3,I don't have any articles or resources to back this up but from what I've heard is that when a memory address in RAM is accessed it loads a whole block of it in to cache (L1/L2 cache maybe?). So then if the next instructions are accessing a memory address in that block it will pull it from the cache instead of RAM.,hr1c8xf,t3_rushx3,1641186735.0,False
rushx3,no this is something different I almost forgot about this concept read about it in my comment,hr1cmmz,t1_hr1c8xf,1641186940.0,False
rushx3,"> I've heard is that when a memory address in RAM is accessed it loads a whole block of it in to cache (L1/L2 cache maybe?).

Afaik, on subsequent reads of the same block of memory, a [cache line](https://stackoverflow.com/questions/52890824/cache-line-format-layout) which is usually 64 bytes, is cached. I got to know about these cache lines in this [youtube video](https://youtu.be/247cXLkYt2M?t=274) about data structure optimization but it does not explain why memory alignment is beneficial for caching.",hr1deg6,t1_hr1c8xf,1641187361.0,True
rue0sq,"Let’s say we have an 8 bit data width CPU, with 16 bit instructions.

It has 16x 8bit general purpose registers.

There are 2x (16 to 1) 8bit multiplexers with the 16x 8bit inputs being the values in the registers. They each have 4 control bits to select which register values end up on the output.

The 2 8bit outputs are fed to an ALU which can do 4 operations: add, sub, and, or. The operation is selected with 2 control bits.

The 8bit output is fed back to the 16 registers, each have a control bit which if enabled, will latch the 8bit value into the register at the end of the clock cycle. These 16 control bits are the output of a 4 to 16 decoder, which is controlled by 4 select bits.

So our CPU can choose any 2 registers from our 16, perform one of 4 operations, and then save the result to any of the 16 registers.

It needs 4 bits to select the 1st operand, 4 bits to select the 2nd, 2 bits to select an operation, and another 4 bits to select which register to save the result in.

So you might have an instruction format which looks like this:

00xxaaaabbbbcccc

Where xx are the 2 bits which select the operation the ALU will perform.

aaaa are the 4 bits which select the register of operand 1.

bbbb are the 4 bits which select the register of operand 2.

cccc are the 4 bits which select which register to save the result in.

When the CPU sees this instruction it will route the relevant control bits to the multiplexers, ALU, and decoder to perform the specified instruction. This is what is meant by decoding the instruction. This is a simple instruction set but real ones are much more complex and require more logic to derive the relevant control bits from any given instruction. The area of the CPU which will do this is called the control unit. It uses the instructions to derive control bits which will control the flow and processing of data in the other components of the CPU such as registers, the ALU, memory, various multiplexers and decoders, bus access, etc.",hqyhh8j,t3_rue0sq,1641145316.0,False
rue0sq,"Don’t think I could have explained any better. This is a good quality textbook-level response. Did you write CLRS ;)

Could you also clarify the third sentence? Are you saying 2 8 bit multiplexers logically function as a 16:1?",hqzz88x,t1_hqyhh8j,1641165600.0,False
rue0sq,"2 separate 16:1 multiplexers, which have 8 bit wide inputs and outputs. So a total of 2x16x8 inputs, 2x1x8 outputs, and 2x4 select lines. Always confusing to talk about multiplexers because there’s a lot of independent bit widths to specify haha.",hr08rij,t1_hqzz88x,1641169413.0,False
rue0sq,"It's been a while since I actually looked at digital multiplexers in detail; could I ask how an 8 bit multiplexer is 16:1? Wouldn't it be able to take 256 different combinations to select? Sorry, I thought I understood this, but maybe I need to brush up myself :P Or are you saying there are 4 control bits for each one, meaning 2\^4 remaining combinations on the last 4 bits which would make more sense. Lastly, you're awesome! Is your background more in CS or CE/EE?",hr0y5b5,t1_hr08rij,1641180067.0,False
rue0sq,"8bits isn’t referring to the number of select lines but the size of each individual input. Most multiplexers are just 1bit but sometimes you want to switch not single bits, but entire busses. To give an example:

A 1bit (4:1) multiplexer, needs 2 select lines:

    In0 - 0
    In1 - 1
    In2 - 1
    In3 - 0

    S0 - 1
    S1 - 0

    Out - 1

With the select lines set to 01 it gives output In1 which is 1

A 3bit (4:1) multiplexer, still needs 2 select lines:

    In0 - 101
    In1 - 000
    In2 - 011
    In3 - 110

    S0 - 0
    S1 - 1

    Out - 011

With the select lines set to 10 it gives output In2 which is 011

You can build a nbit (X:1) multiplexer by taking n x 1bit (X:1) multiplexers and tying their select lines together.

My background is EE but working in embedded engineering which is sort of the half way point between EE and CS haha.",hr4fu28,t1_hr0y5b5,1641243955.0,False
rue0sq,You might find Ben Eater's [video](https://www.youtube.com/watch?v=dXdoim96v5A) exploring his 8-bit computer's control logic useful if you want a visual understanding of /u/No_Engineering8506's reply,hqzpehx,t3_rue0sq,1641161753.0,False
rue0sq,"Well in general the CPU have something called a control unit. The control unit configures the cpu into different states (This could be adding values, putting things in memory or setting registers). When you decode a command, you're telling the control unit what state the cpu should be in.",hqyf1an,t3_rue0sq,1641144386.0,False
rue0sq,"Pretty much decoding refers to taking an instruction (or “command”) and breaking it apart depending on what kind of instruction it is, then sending these pieces to the other parts of the processor that executes the instruction. The control unit (which others have mentioned) is the component that checks a certain parts of the instruction, and depending on what values it finds in those parts it will send the correct parts to the correct components.

For example (and  I just made this up) if you want to add 5 and 2, and the control unit is set up to add when the first four indices of the instruction equals 2 (or 0010 in binary) then a 16 bit instruction for that could be: 

0010 000101 000010

The control unit would look at the first four values, see a ‘2’, and from there it knows to take the next 6 values and pass them in as the first operand to the ALU, and the last 6 values as the other operand to the ALU. In reality it’s more complicated as it also needs to know where to save the result and other operations might need even more information, but this is an easy way to visualize it.",hqzelm6,t3_rue0sq,1641157376.0,False
rue0sq,"If you want an expanded example you can think of the commands as gates/pathways that are controlled by switches. Each CPU command has a bitset that corresponds to it. Those gates open when the certain bits are set (through ands/nots) and then the data you're using with the CPU command is sent through that path.

It doesn't really ""decode"" it as much as the bits split up with circuits and sent in chunks. The command code is one such chunk and the data (more specifically the register number being used) is another one. Flags can also be used.

Now take that analogy and compact it, the gates overlap and the logic behind the gates can be simplified using ors an boolean simplification. Our CPUs are massively simplified to be as compact as possible. Programmable CPUs also exist (as in the command sets can be changed), but that is a lot more complicated.

Actual decoding/encoding is done to help us read it, but it's not at all necessary for the CPU function. The decoding you're referring to is more of an analogy for how the data splitting is done and how it's interpreted by the system.",hqzyn6k,t3_rue0sq,1641165371.0,False
rue0sq,"This is at least one college-level course. Binary instructions and data are fed into a hierarchy of semiconductor logic gates using transistors and other electrical components. I would recommend a textbook ""Logic and Computer Design Fundamentals"" by Mano, Kime, and Martin. It's kind-of a rough technical read, but it will get you started. 

I could write a whole essay, but you're not going to ""get it"" until you sit down and work-out some truth tables.",hqzyybm,t3_rue0sq,1641165494.0,False
rue0sq,"You got a lot of great responses here OP, but if you want a simplified TLDR, assume you have 16 bit storage (and processor of the same size) you can imagine 4 of those bits being used to encode an instruction (so 2^4=16 instructions) and then the next 4 bits are an argument for the instruction, such as an absolute value, a memory cell, or a pointer to a memory location, next 4 bits are second argument (same as before), and the last 4 bits are where to store the result. In practice it’s a tad more complex but this is the important intuition to develop imo.",hr0193o,t3_rue0sq,1641166396.0,False
rue0sq,"You got some really awesome responses here, I will try to give an answer that requires less CS knowledge to understand, and perhaps it will help someone out there (though if you really want to understand it look at other comments here!)

So basically, a CPU works by reading some list of ""commands"" in Binary code (lines of 0s and 1s).

These commands vary from OS to OS but they can basically be summed up to 3 categories: ""basic mathematical operations"" and ""memory operations"" and ""Jump operations""*.

The memory operations will be things like write num x to an address y in the RAM, or pull up the number in a RAM address and save it to the stack (a dedicated place on the ram, that mathematical operations will take their inputs from).

The mathematical operations will take numerical inputs from the RAM (from the stack) and replace them (on the stack) with the operation's result.

The ""Jump operations"" are taking us back to the first ""list of commands"" for the CPU, as we are all aware, in programming two of the most powerful tools we have are ""loops"" and ""conditions"", these make the difference between flicking switches manually and actually responding to various types of inputs, and being able to make varying computations with the same function - The Jump operations are able to check whether a condition applies (typically ""is the last stack item ==\<\> to 0"" and if that condition applies, the CPU will ""jump"" to a certain command in the list and start pulling the commands from that line to achieve conditional programming and loop iterations.

That is basically how a CPU operates. Hope it helps someone :)",hr1vxxn,t3_rue0sq,1641199753.0,False
rue0sq,"Logic gates and other tiny shit. Gates flip open gates flip closed, shit gets decoded from java or cpp or another high level language to machine code like move eax to blah blah blah then pop and push and eventually it all ends up to 1's and 0's. Or so I've heard.",hqyo04x,t3_rue0sq,1641147772.0,False
rue0sq,"no, thats encoding ""commands"", op asked aboud decoding them",hqz59os,t1_hqyo04x,1641154037.0,False
rue0sq,Oh shit just put it in reverse then.,hqzdtv5,t1_hqz59os,1641157108.0,False
rue0sq,"gates don’t actually flip at all, their electrical output is altered depending on the input voltage. pop/push are actually implementation-dependent iirc, some just add and subtract from program counter.",hr00fnx,t1_hqyo04x,1641166073.0,False
rue0sq,"„˙ǝƃɐʇloʌ ʇnduı ǝɥʇ uo ƃuıpuǝdǝp pǝɹǝʇlɐ sı ʇndʇno lɐɔıɹʇɔǝlǝ ɹıǝɥʇ 'llɐ ʇɐ dılɟ ʎllɐnʇɔɐ ʇ,uop sǝʇɐƃ„",hr00gu1,t1_hr00fnx,1641166085.0,False
rue0sq,Good bot,hr8g6no,t1_hr00gu1,1641316159.0,False
rue0sq,"Thank you, bobthebuilder747, for voting on Upside_Down-Bot.

This bot wants to find the best and worst bots on Reddit. [You can view results here](https://botrank.pastimes.eu/).

***

^(Even if I don't reply to your comment, I'm still listening for votes. Check the webpage to see if your vote registered!)",hr8g7qf,t1_hr8g6no,1641316171.0,False
ruwflx,"An installer is usually called a software installation package which (normally) includes a setup wizard to walk through which features should be added and to set up initial settings.

It's important to point out its usually a package, because it's packed up- a lot of data is compressed and in the case of online installers much of the data is omitted in favor of a downloading protocol for the data from an online server. Think Steam download, but for a single game or software. I would say this is the most common method for modern installers and why they can be smaller than a hundred megabytes.

For some, it is a fully packed software (offline installers), but these are often very large and tell the software how to decompress and reassemble itself.

A lot of libraries can be omitted because they come pre-installed or previously were and require separate downloads (think C++/.NET redistributables) which are shared between many apps.

Games usually only package the engine and boiler plate code and basic art (in the case of installers), then patches and more complex art assets are downloaded online as updates. That's why a lot of games will pitch a fit if you try to run it for the first time offline. Software in general usually includes an updater for patches as the installer package usually just includes the base code.

Specific components of an installer could be: decompression tool, downloader, updater, setup wizard, validater (making sure all common libraries/dependencies are there), along with a few others.",hr28k43,t3_ruwflx,1641209641.0,False
ruwflx,So basically the size difference comes from the compression algorithms that were used and from omitting certain libraries and having .NET / C++ package manager install them if they were not previously installed? (In the case for offline installers),hr2a00m,t1_hr28k43,1641210663.0,True
ruwflx,"Yes exactly, the runtime redistributables are usually handled by Windows though.",hr2be6l,t1_hr2a00m,1641211637.0,False
ruwflx,"The installer is a dam wall, you hook up to it and it releases the dam water (the application) the installer is a completely separate program to manage the installation of a much larger complex app.",hr1umh3,t3_ruwflx,1641198715.0,False
ruwflx,"In your analogy, where does the water come from? And how come that dam is so much smaller than the water?",hr1w3pg,t1_hr1umh3,1641199878.0,True
ruwflx,"The water comes from a server and the damn is a networking/file configuration tool that connects with the host to open a file stream (connect the pipe to the damn wall) then the application starts pouring through, the installer handles keeping the connection open and
Managing the download progress. The installer just manages the flow of data, but the amount of data behind it is semi arbitrary, you just would t build an installer to install small, simple files.",hr1wduc,t1_hr1w3pg,1641200100.0,False
ruwflx,"I was asking about offline installers that have a significant size difference, they have to store the software inside the installer itself, take for example the old days when the installer was stored on a physical CD that was up to 700mb and still the software could go up to 5gb or even more, but this also happens with more modern installers",hr1wt3k,t1_hr1wduc,1641200440.0,True
ruwflx,"OIC yeah the compression ratios are nuts, I’m with ya there",hr33s6g,t1_hr1wt3k,1641225965.0,False
rubju8,"You would need to look at the reference implementations and libraries for each language individually. C++'s STL has a lot of these kinds of rules, such as containers needing to return their size in O(1).",hqxwosd,t3_rubju8,1641136836.0,False
rubju8,Yeah pretty much.,hqyg5y3,t1_hqxwosd,1641144816.0,False
rubju8,I also like finding [cheat sheets](https://github.com/gibsjose/cpp-cheat-sheet/blob/master/Data%20Structures%20and%20Algorithms.md#10-data-structures) for this,hqzoj9k,t1_hqxwosd,1641161418.0,False
rubju8,"> You would need to look at the reference implementations and libraries for each language individually. C++'s STL has a lot of these kinds of rules, such as containers needing to return their size in O(1).

What are some of rule's that C++ has for their reference implementations ? Some of the feature's being in some questionable performance debt from the new abstraction's being introduced",hr0ewue,t1_hqxwosd,1641171956.0,False
rubju8,"In java you rarely use array, but a list implementation (arraylist, linkedlist, etc.). An add to arraylist is o(1).   


[https://www.programcreek.com/2013/03/arraylist-vs-linkedlist-vs-vector/](https://www.programcreek.com/2013/03/arraylist-vs-linkedlist-vs-vector/)",hqy5191,t3_rubju8,1641140464.0,False
rubju8,"Also, the model behind time complexity does not take account of the modern processor architecture (caching, etc.) and this is the reason when any engineer worth it money will say: only after an actual measurement on a certain architecture will I tell you anything about an algorithm's performance.   


So there is a huge, real life difference between a theoretical algorithm and a concrete implementation of that algorithm on a certain architecture. Also nowadays you can run stuff on multiple cores as well, which makes things more tricky. Also in java (and js as well) you need to think about the GC.  


[https://stackoverflow.com/questions/10656471/performance-differences-between-arraylist-and-linkedlist](https://stackoverflow.com/questions/10656471/performance-differences-between-arraylist-and-linkedlist)",hqy67sp,t3_rubju8,1641140950.0,False
rubju8,"Caching and parallelization affect runtime, not time complexity.",hr0uvm1,t1_hqy67sp,1641178637.0,False
rubju8,And this is why time complexity as a mathematical formula has limited usage when talking about real world performance.,hr1gmf5,t1_hr0uvm1,1641189176.0,False
rubju8,Not sure if there is an existing resource for these quirks. You'll probably just need to look into the implementations of common structures in the languages you're curious about.,hqzh35y,t3_rubju8,1641158435.0,False
rubju8,"> In JS, array.push() is always O(1) since even if need to 'extend' the current array as the reference to the last item is simply removed.

Sometimes array.push requires a copy even in JS. The operation is **amortized** O(1), but it's useful to make sure you're comfortable with why that is.

Once you've got a decent understanding of common data structures it's pretty easy to recognize which ones are used in a given language. At that point it should be pretty easy to accurately guess what the complexity of each operation is.",hqzta2x,t3_rubju8,1641163255.0,False
rtu2da,"Negative time means any date prior to 1970 - which at the time Unix time was developed was very important, because literally everyone programming with it was born prior to 1970. This is becoming less and less important as people die, but is still required. So if you take as assumed that time is stored in an integer of some kind and time starts counting from 1970, it is necessary so you can store dates before then. You could go with unsigned and start from 1901, but the reasons to *not* do that have already been covered.
 
The question of why time is stored as an integer of some kind is mostly to do with sorting and > < comparison in general, which is the most common date/time operations that exist by a mile. These operations must be as rapid as humanly possible, and so date/time storage is optimised for them, hence a single integer count (so you don't have to go looking for the year variable, then the month variable, etc etc).
 
The question of why time starts at 1970 was because at that gives enough time to cover preeeeeetty much everybody's birthdays (not all, but enough that those rare people just got to be born in 1901, there weren't many 70+ programmers back then, nor even employees or customers of that age), while simultaneously being long enough that it seemed like it would get sorted out before we got there (which is very likely to turn out true).",hqw4i5b,t3_rtu2da,1641094912.0,False
rtu2da,"Just to extend this and say that perhaps the most common operation done on the time stamp is the simple increment that happens once a second. And that's very easy to do with a single integer. Like you say, I'd we store it as a multipart number then we have to worry about roll over.",hqx0eba,t1_hqw4i5b,1641114682.0,False
rtu2da,"While it's something that happens, it's not a major issue as that rollover operation occurring just once a second is not a big deal. The issue is not really 'the time now', because that generally happens in one spot per machine (and these days aren't even counted in memory at all, they're tracked by an external clock and updated as needed). Previously stored time is where the efficiency is needed, because that is when you won't have 1 operation per second, you'll want billions.",hqx1jv6,t1_hqx0eba,1641115632.0,False
rtu2da,"Forgive me for the tiny, mostly useless contribution. But the internet as we know it will eventually be older than every living thing, assuming we live for another few thousand years at the least.",hqwzfiv,t1_hqw4i5b,1641113899.0,False
rtu2da,We can hope! :-),hqxop59,t1_hqwzfiv,1641132811.0,False
rtu2da,Because you might want to store a time in the past,hqux5xg,t3_rtu2da,1641075739.0,False
rtu2da,Does this mean there is a limit on how far back the time can go by storing it as a signed integer?,hqy146a,t1_hqux5xg,1641138814.0,False
rtu2da,"What I’ve heard is that when the first Unix systems used the epoch, the compiler (maybe the architecture but that would be weird) didn’t differentiate between signed and unsigned ints. So it is maintained as that type for backwards compatibility, and because a bigger number wasn’t thought to be needed.",hqv6xnd,t3_rtu2da,1641079911.0,False
rtu2da,"It’s convenient to store time using unix time; it’s zone agnostic; it’s generally simple to perform arithmetic on. 

And when many of these systems were created 2038 was a long way off. 

Much like only storing 2 digits of the date; the smallest viable size was chosen because storage wasn’t cheap at the time and when you’ve got millions of records a wasted 2 bytes can add up.",hqv33yl,t3_rtu2da,1641078280.0,False
rtu2da,"Unsigned integer needs different (often unexpected) logic for arithmetic operations due to the fact that it can easily underflow at zero. So for arithmetic types, they are avoided. For example:

* People often do date calculations
  * `daysUntil = refDate - today`
  * `daysSince = today - refDate`
* `for (int i = x; ... ; i--)`, instead of checking for `i >= 0` you need to also check for underflow which is `i <= x`.",hqvrlwa,t3_rtu2da,1641089005.0,False
rtu2da,Oh wow I never knew that could be an issue!  I always just assumed that an unsigned integer sorta just bottomed out at 0.  I feel like these high level languages have coddled me and hid the weird reality of the circuits from me this whole time.,hqvsltr,t1_hqvrlwa,1641089457.0,True
rtu2da,"> unsigned integer sorta just bottomed out at 0

If you try to assign a signed value(less that 0) to an unsigned integer, weird things happen which are implementation-specific. For example, if you assign a negative value to an unsigned int and compile with gcc, this is what would be assigned to the variable:
```
#include <stdio.h>

int main() {
    unsigned int illegalUint = -3;
    printf(""%u"", illegalUint);
}
```
the above code returns:
```
4294967293
```

Certain new languages like Go and Rust(i assume), do not allow signed integer assignments to unsigned integer variables.

> I feel like these high level languages have coddled me and hid the weird reality of the circuits from me this whole time.

I can kinda resonate with you, JavaScript was my daily driver like 5-6 months ago, I just did a lot of node.js and well when I started learning Go, I learned so much about memory and the CPU that I couldn't ever have learnt while using JS.",hqwfcig,t1_hqvsltr,1641100327.0,False
rtu2da,"I, after nearly 40 years of working with computers, built my own Z80 on a breadboard (am building, actually, I’m adding stuff to it) and have been programming it in assembly since I’ll need the assembly code to add devices to CBIOS.  It’s been an amazing experience and I’ve learned so much. Everyone who does this professionally should take on a project like this.",hqxd8ze,t1_hqvsltr,1641125271.0,False
rtu2da,"Because according to Einstein, negative time *could* possibly happen, so we built our computer architecture to deal with it in case it happens!",hqw6zrn,t3_rtu2da,1641096093.0,False
rtu2da,"Do you mean ""the past""?",hqwygup,t1_hqw6zrn,1641113127.0,False
rtu2da,[deleted],hqv1jkp,t3_rtu2da,1641077612.0,False
rtu2da,"The unix standard (most frequently) represents time as a 32 bit signed integer, ranging from -2,147,483,648 to 2,147,483,647.
If you changed it to an unsigned 32 bit integer, you can now store values from 0 to 4,294,967,295. Because you have freed a bit from storing the positive or negative state of the number.
When the number of bits used is expected to be standard, it makes sense to wonder why we don't maximize how high we can count with those bits, and would in fact change the 2038 problem to a 2106 problem.",hqv5rrp,t1_hqv1jkp,1641079417.0,False
rtu2da,"> would in fact change the 2038 problem to a 2106 problem

Yeah, that is why shifting to 64-bit architecture is the best option by far. Btw I wonder that when will we encounter a similar problem with int64.",hqwfl5p,t1_hqv5rrp,1641100458.0,False
rtu2da,"> I wonder that when will we encounter a similar problem with int64.

If only we could calculate this somehow, e.g. simply by finding the maximum value it can hold and adding it to the start date! :)",hqx0is3,t1_hqwfl5p,1641114786.0,False
ru5qo2,"This sounds like [Complexity Theory](https://en.wikipedia.org/wiki/Computational_complexity_theory). You might consider connections to the more general question ""for which problems can we describe limits on the time, space, (or other resources such as number of states) needed by any Turing machine which solves them""",hqx5oww,t3_ru5qo2,1641119085.0,False
ru5qo2,"**[Computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)** 
 
 >Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm. A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hqx5ptn,t1_hqx5oww,1641119106.0,False
ru5qo2,"To answer your specific question, it can be shown that 2 states is ""sufficient"" for any kind of Turing machine you want to make. The more interesting tradeoffs are in time (number of steps) and space (number of visited tape cells).

EDIT: I was wrong, I confused the alphabet size with the number of states. They are related in the sense that a machine with fewer states may require more alphabet symbols to be universal, and vice versa. But here's a universal machine with 2 states and alphabet size 3: [Wolfram's (2,3) Machine](https://en.wikipedia.org/wiki/Wolfram%27s_2-state_3-symbol_Turing_machine)",hqx6x8l,t3_ru5qo2,1641120106.0,False
ru5qo2,"Hang on. A ""Turing Machine"" can be a one bit system?",hqxkx7g,t1_hqx6x8l,1641130619.0,False
ru5qo2,"The tape must be infinite, of course.",hra6ief,t1_hqxkx7g,1641340239.0,False
ru5qo2,"I think the alphabet (tape characters) can be of size two to compute anything (I.e binary), but I don’t think all computations can be done with two states…",hqyhgo7,t1_hqx6x8l,1641145310.0,False
ru5qo2,See my edit. 2 state UTM can exist,hqzghkm,t1_hqyhgo7,1641158051.0,False
ru5qo2,"A 2 state TM can exist, but it can't do everything. A larger alphabet can help out, but with a single tape, a 2-state machine can't do that much.


I've made a program on a 2 state 3 letter turing machine that simply increments a binary number until it hits 255, where the machine crashes. Not much more can be done with that kind of machine.",hr9uiw0,t1_hqzghkm,1641335476.0,True
ru5qo2,Everything can be done with that machine. That's what makes it universal. It may take more patience than you have.,hra06vy,t1_hr9uiw0,1641337713.0,False
ru5qo2,"If it can, can you show me an example of it emulating, say a 16 state 16 letter machine?",hra0ddo,t1_hra06vy,1641337787.0,True
ru5qo2,"No, because it would take more patience than I have. But it's possible. Please read the article I linked.",hra5j2b,t1_hra0ddo,1641339847.0,False
ru5qo2,"A Turing machine has a finite number of states in its CPU. However, the states are not small in number. A real computer consists of registers which can store values (fixed number of bits). A universal Turing machine can be constructed using one tape and having only two internal states.",ht2eu4x,t3_ru5qo2,1642443233.0,False
ru3c37,"I have read or listened or talked about this like some months or an year ago but ah I can't really remember where I read/listened/talked about it. What I do remember is that an AI was given all the laws of classical and quantum physics and it was given the task to discover new laws. My guess is that I read about this in either of these two books:
- A Mathematical Universe by Max Tegmark
- Life 3.0 by Max Tegmark

I probably have physical copies of both the books, so I'll give both of them a look and respond back.",hqwte9o,t3_ru3c37,1641109157.0,False
ru3c37,"After some googling, I found this [article](https://www.technologyreview.com/2018/11/01/1895/an-ai-physicist-can-derive-the-natural-laws-of-imagined-universes/) which talks briefly about this [research paper](https://arxiv.org/abs/1810.10525) by Max Tegmark and Tailin Wu from MIT.

Also I looked up Life 3.0 and well it is a pretty heavy book so I could not find much about the AI discovering the new laws and axioms except two topics that I thought might help. I read about AI creating laws for the society and a topic titled ""Physics: The Origin of Goals"". I am sure that the paper mentioned earlier would definitely be helpful.",hqwva81,t1_hqwte9o,1641110603.0,False
ru3c37,This is insane. Guess I know who to talk to when I visit Big Tech in Summer! I also had the same Ideas about morality before I read about this. Basically you define a goal and then the moralistic stance just revolves around the goal.,hqwwb70,t1_hqwva81,1641111401.0,True
ru3c37,I think the best goal to be honest is just the pursuit of knowledge itself. I know it sounds crazy but it’s a cause I‘m willing to die for when I think about it. Even If we can’t pass all the great filters of the universe maybe we can at least pass on the ability to more advanced forms of life. Our feeling of being human is restricted to knowledge 2.0 something that is probably not capable of understanding the universe.,hqwx0dh,t1_hqwva81,1641111952.0,True
ru3c37,I actually have Knowledge 3.0 as well (I‘m a general intelligence btw hahahahaha). I‘ll have a look too. Omw to creating a discord.,hqwtrx3,t1_hqwte9o,1641109443.0,True
ru3c37,I’ve read life 3.0 and would highly recommend it to anyone interested in this topic. Whilst I can’t 100% remember if it touches on OP’s specific example it definitely at least explores the idea,hqy4w1w,t1_hqwte9o,1641140405.0,False
ru3c37,"The problem I can see with this is the fact that even if the AI finds anything, there's no experimental confirmation involved. 

At most it'll be a 'string-theory'-like idea, that isn't and can't be substantiated. Only you'd get probably millions of them.

Then you'd need to create AI to be able to test (or come up with methods of testing) all of those hypotheses xD",hqxfaiy,t1_hqwte9o,1641126815.0,False
ru3c37,"There are two ways to approach this idea.

* You could try to use computers to verify proofs of theorems, and maybe even generate new proofs of new theorems. There has been some progress on this, but it turns out that mathematical proofs tend to leave out ""obvious"" steps that need to be written out in painstaking detail for computers to understand. Additionally, this approach doesn't help with non-mathematical things, such as understanding language. Relevant link: [https://www.quantamagazine.org/lean-computer-program-confirms-peter-scholze-proof-20210728/](https://www.quantamagazine.org/lean-computer-program-confirms-peter-scholze-proof-20210728/)
* We can also try to ""understand"" the world, with all its complexities and ambiguities, in an ad-hoc way, learning what's what. In the last decade, there's been an explosion in this thing called ""deep learning"", which is a method in AI. This has been pretty successful and has been used commercially. However, due to its ad-hoc nature, it's hard if not impossible for a human what the computer is ""thinking"", making progress in this field kind of slow and full of trial-and-error (except that ""throwing more hardware into the problem"" seems to work surprisingly well).",hqxp92c,t3_ru3c37,1641133116.0,False
ru3c37,I was thinking more of a principia mathematica approach with the dataset inputing a set of axioms and the output being a closely related result (mabye only needing one or two lines of logic). I assume we can then better know what the AI is doing.,hr2vr9e,t1_hqxp92c,1641222586.0,True
ru3c37,"What's the difference between that and the first approach I mentioned?

Also, AI is much, much harder than you think.",hr2wz6i,t1_hr2vr9e,1641223124.0,False
ru3c37,"The difference is that I‘m guiding the AI in a certain direction. The loss function then becomes trivial. I’ve programmed machine learning algorithms from the ground up, I know I’m getting into a lifelong journey. I think it’s fundamentally however it’s the thing most worth pursuing if we ever want to fully understand mathematics and physics.",hr3057e,t1_hr2wz6i,1641224466.0,True
ru3c37,"I don't quite understand what you're saying. Is this what you're looking for?

>A formidable open challenge in the field asks how much proof-making can actually be automated: Can a system generate an interesting conjecture and prove it in a way that people understand? A slew of recent advances from labs around the world suggests ways that artificial intelligence tools may answer that question. Josef Urban at the Czech Institute of Informatics, Robotics and Cybernetics in Prague is exploring a variety of approaches that use machine learning to boost the efficiency and performance of existing provers. In July, his group reported a set of original conjectures and proofs generated and verified by machines. And in June, a group at Google Research led by Christian Szegedy posted recent results from efforts to harness the strengths of natural language processing to make computer proofs more human-seeming in structure and explanation.

https://www.quantamagazine.org/how-close-are-computers-to-automating-mathematical-reasoning-20200827/",hr33qcm,t1_hr3057e,1641225944.0,False
ru3c37,"Point 1 :

What you displayed here is what Einstein referred to when he used the word ""imagination"". I call it ""inspiration"".

It's at the root of almost all of our startling discoveries. And I don't think an AI will be able to replicate that in a long, long, long while. .

Point 2 :

Because of Point 1, what your model at best could perform is fetch us the ""closure set"" of all mathematical concepts that apply due the concepts and first principles already known to us. For eg. (and I don't know how our present AI models work so forgive me if I'm incorrect about this example) but think about imaginary numbers.

If humans hadn't invented the concept of imaginary numbers as a mathematical tool, what I think is that your AI could've only fetched us knowledge applicable to Real Numbers.",hqxiz4d,t3_ru3c37,1641129389.0,False
ru3c37,Have you heard of constructor theory? Maybe we can rearrange the axioms by finding the counterfactuals. For example sqr(-1) doesn’t exist is the counterfactual to sqr(-1) = i . Also even if it only does that I‘m very happy.,hr2v94s,t1_hqxiz4d,1641222360.0,True
ru3c37,">Have you heard of constructor theory? 

Nope. Enlighten me.",hr2zskv,t1_hr2v94s,1641224318.0,False
ru3c37,It basically just questions axioms of physics by assuming the opposite is true.,hr30p7j,t1_hr2zskv,1641224697.0,True
ru3c37,"Ok, so I like your general train of thought, but please allow me to humor you with this angle, namely that our “pool” of knowledge is always essentially what we hold in our collective RAM/working memory for the explicit purpose of executing a task.

Therefore energy must actually be invested by ourselves to how we organize/operate etc. Therefore what you are proposing is totally natural, and in many ways one could argue is already being done although in its only just getting started in its potential. My main point is that we as humans are essentially neural processing nets that guide energy allocation to support ourselves. For example if we need to come up with a specific protein to create a biological outcome for a process we tool AI to that end. 

What you are suggesting, correct me if I am wrong is to direct AI to building a general knowledge pool. The problem is that is too broad.

The most interesting area I see is actually the creation of an object oriented learning architecture that capitalizes everyone’s unique interest/energy level in that interest to dynamically transmit information and organize ourselves at the same time.

With such a system our neural nets actually can function in symbiosis with AI where we both can help each other perform at our collective maximum with us guiding systemic output towards to meeting our current systemic demand while building an exponentially more capable system, itself based on an exponentially growing base of actionable knowledge.",hrdquba,t3_ru3c37,1641405121.0,False
ru3c37,How would the system determine when a theory has been discovered so it can proceeded to check it,hsggjg2,t3_ru3c37,1642055469.0,False
ru6jx7,"> I understand a database containing passwords should be hashed using some sort of key

Keep in mind that [cryptographic hashing and encryption](https://www.ssl2buy.com/wiki/wp-content/uploads/2015/12/hashing-vs-encryption.png) are two different things. 

You aren't hashing the whole database, just the password. Passwords are salted and then put through an execution unit in the CPU that does [this](https://emn178.github.io/online-tools/sha256.html) to them, so that if there is a data breach or a rogue sys admin, there aren't a bunch of plaintext passwords floating around.

> but I understand that each entry should be encrypted again in some other way.

Depends. [Some implementations encrypt as the information is written to disk, others don't.](https://en.wikipedia.org/wiki/Disk_encryption#Transparent_encryption) The database is on the disk. Encryption and hashing can be done at the hardware level; there could be, for instance, an [AES execution unit](https://en.wikipedia.org/wiki/AES_instruction_set) in the CPU before it's sent over the wires (maybe the SATA bus?) to be written to disk.",hr5d36f,t3_ru6jx7,1641257286.0,False
rubwjz,"Could you give an example of what you mean? Often when computers/programs use different data types or file formats they just can’t communicate. To get around this, programmers pick standard formats to use so other programs can understand the output. 

Try to open a word document in notepad and see this failure in action.",hqxzhyz,t3_rubwjz,1641138110.0,False
rubwjz,">how can computers with different systems that have different data type representation communicate properly?

The same way countries with different monetary systems are able to trade goods: they agree on a common system of exchange.",hqy86il,t3_rubwjz,1641141738.0,False
rubwjz,">a common system of exchange

AKA. a **protocol** for communication and behaviour",hr0xjgv,t1_hqy86il,1641179799.0,False
rubwjz,"It's called ""serialization""/""deserialization"". Anything that transmits or stores data has to do it. This can be very simple and fast (but not very portable) or more complicated and slow (but more portable).

At one extreme, you might just save your whole memory image to disk. It would be pretty fast, and you wouldn't need to do anything to your data. For this to work, you probably would need the exact same version of the exact same program on the exact same operating system (and probably a little help from the OS).

At another extreme you could use a text based format like JSON or XML. Everyone knows how to store and transmit text, and standard formats mean it's easy to write libraries that convert them to whatever internal representation your program/language requires. Unfortunately, this process is pretty slow (imagine converting millions of ints into a variable number of characters). It also wastes space. But the up shot is that it's super portable. Even humans can read and understand it!

There are compromises as well. Protobuf is a tool from Google that helps you define how things get serialized and communicated. Two programs have to use the same protobuf definition and version, but they can be written in different languages and run on different OS's. It's still kind of slow, but it's more efficient than JSON. Python has something called ""pickle"" that does this, but it only works in python.

Serialization/deserialization is a huge problem in distributed systems. It's a significant fraction of all computer cycles at Google (look up ""the data center tax""). It can really slow down distributed frameworks and overall is a pain in everyone's butt.",hqydxle,t3_rubwjz,1641143968.0,False
rubwjz,"https://afteracademy.com/blog/what-are-protocols-and-what-are-the-key-elements-of-protocols

This seems like a reasonable summary",hr1klvp,t3_rubwjz,1641191619.0,False
rtkdm7,"Turns out, individual people aren't that unique.  A classic (literally, textbook) example is if 10,000,000 other people searched beer, then bought beer and diapers, when you search beer, or even just linger over an ad for beer long enough to indicate it caught your attention, you're going to get ads for beer and diapers, even though you didn't search diapers.  That's an incredibly simple example, in reality the algorithms look at many more variables than that, looking for patterns.  Maybe people who by beer and milk get ads for diapers and people who buy beer but no milk get ads for protein powder.  There is a bit of confirmation bias to it as well - you notice the times the algorithm was correct, and you don't notice when it wasn't, because that just looks like ""random ads"".

Other times, it's as simple as, for example, Google seeing what kind of products you get spam email for, and showing you ads for things bought by people who got those same emails, or showing you ads for things one of your family members or friends was searching for (it knows your ""friends"" by doing things like matching up who has each other's numbers or emails in their contacts list, when you give it permission to ""manage contacts"" or whatever.)

Another thing that happens is these people were getting those ads all along, but simply never noticed.  Once they do think of that thing, they start noticing the ads; basically the same as how you never notice a certain kind of car until it's brought to your attention somehow (an ad, a friend buys one, etc) then all off a sudden you see them everywhere you look.  The world didn't change, your brain-vision-attention filter did.",hqt4qqc,t3_rtkdm7,1641048007.0,False
rtkdm7,">Turns out, individual people aren't that unique.

[You're all individuals!](https://youtu.be/KHbzSif78qQ?t=31)",hqtsn7m,t1_hqt4qqc,1641059049.0,False
rtkdm7,Yes! We are all different! (I'm not..),hqtuoot,t1_hqtsn7m,1641059891.0,False
rtkdm7,"> just linger over an ad

Ok so they are also using facial recognition software to detect anomalys or emotional reactions to things?",hqt8u5j,t1_hqt4qqc,1641050186.0,True
rtkdm7,No. Linger as in mouse over it or have it on your screen for longer than necessary.,hqtdseq,t1_hqt8u5j,1641052605.0,False
rtkdm7,How to the log data like that?,hqtqi89,t1_hqtdseq,1641058161.0,False
rtkdm7,"Your browser knows where your cursor is and what is currently being rendered. A website can just log that information (so long as its happening on its own site, see Same Origin Policy).",hqtu0fr,t1_hqtqi89,1641059615.0,False
rtkdm7,"To be more exact, it is the script (javascript) that is running that does the tracking of where the cursor is. Browser itself just provides the information to the script and the script decides what to do with it. 

This is the problem with allowing shady code from untrusted sources to be run in on your browser since they get access to a lot of behavior-related information and sometimes some private information as well.

Unfortunately, a lot of the scripts are necessary these days for even basic functionality in the web so disabling it is not that easy.",hqtvc3u,t1_hqtu0fr,1641060159.0,False
rtkdm7,"No, I meant if you're scrolling down a news feed and briefly stop scrolling, they just detect the pause and assume you were interested in whatever was on screen.",hqtl9ch,t1_hqt8u5j,1641055942.0,False
rtkdm7,">linger over an ad

Which means that my cats' demands for attention control the advertisements I see.",hqtrlek,t1_hqtl9ch,1641058618.0,False
rtkdm7,"It isn't straight up mind reading but rather pattern recognition. People are more predictable than we want to admit we are and most people do pretty specific things in specific situations. What big tech has is a fuck ton of data that shows that if you like X you're extremely likely to also like Y.

Let's say for example Google figures out that you like heavy metal. It isn't wrong to like heavy metal. Some people like heavy metal and some people don't. Then let's say that they figure out that 96% of people who like heavy metal also like cheap beer. What Google knows is that betting that you like cheap beer if you like heavy metal is a safe bet as there's only a 4% chance that they're wrong. So what they then do is work with advertisers who want to sell cheap beer and say ""metal fans like cheap beer, target them."" That isn't individual mind reading; that's just recognizing patterns in human behavior. If you like heavy metal then chances are you're in the 96% that also likes cheap beer. To an individual this might look like mind reading as you get targeted advertisement but it really isn't mind reading. What happens is Google analyzes your patterns, connects them to overall population patterns, and figures out what you like and what you tend to search for. They'll also notice things like people who enjoy heavy metal are extremely unlikely to search for, say, Brittney Spears and are thus not going to recommend her to metal fans.

What they use is machine learning which chews through obscene amounts of data and finds the patterns. It genuinely is just computers doing math and pattern recognition then figuring out ""people who do X are highly likely to also do Y."" There are also certain patterns that most people show in their searches when they're going through certain things or are about to do certain things that are also recognizable.

The other thing is that big tech tracks what you search for and knows what you do and don't like. Yes this is actually kind of creepy and I don't like that they do it but right now it's legal so they aren't going to stop. Even then though most of it is mundane; we all have our favorite foods, income levels, and demographic information that affects how we behave. So if I like cheap tacos Google probably knows I like cheap tacos. So if I go to Google Maps and search for ""restaurants"" it's going to give me recommendations that are appropriate to my preferences and income level. If I'm broke as fuck and like cheap tacos it is less likely to show me the most expensive steakhouse in town first. That's not mind reading that's just the machine looking at the numbers and making recommendations based on the patterns it knows.

This also isn't new. Supermarkets and department stores have been analyzing customer patterns longer than computers have been around and have noticed weirdly specific behavioral patterns in humanity. The only difference is that computers are doing it rather than people at Google. One downright weird thing that people do when going into stores is we turn right first. Not everybody does it but the vast majority of people when they walk into a store look and turn right. Why? Who the fuck knows? However there's a weird exception; the Japanese go left.",hqt6f4q,t3_rtkdm7,1641048916.0,False
rtkdm7,And most Brits.. we go left to cos theres typically cheap beer right to the left of a supermarket entrance. Coincidence? I think not,hqtmbc2,t1_hqt6f4q,1641056389.0,False
rtkdm7,"Thank you so much for your considerate responses, very kind 🙂",hqt86pr,t3_rtkdm7,1641049849.0,True
rtkdm7,"You cannot read thoughts without hooking up your body as an input to a machine that could interpret it's thoughts.
The algorithms in Social Media / Google / etc. Are based on the profile they have gathered about you (that is linked to your Social Media account / IP address / MAC address) and compared with the behaviour of accounts that are vaguely similar to you in various aspects (mostly relating to shopping habits).

As was mentioned before, humans are not that different from one another, and age groups with vaguely similar interests will often search for similar things at similar situations, and if you take into consideration the fact you are most likely to notice something when it's really extraordinary (like getting suggested something you only thought about for a second and never searched for, a minute after you thought it), you get the feeling that someone is literally reading your mind.

You are getting algorithmic suggestions all of the time, chances are if the algorithm is good enough, it will suggest something that is anecdotal to your recent actions (since other vaguely similar people searched for it right after behaving in a similar way to you). But when it's suggesting something irrelevant you simply won't notice.",hr1xx3l,t3_rtkdm7,1641201307.0,False
rtkdm7,"There are special algorithms, called artificial neural networks. Those mimic the way your brain behaves, and are trained to recognise patterns in a dataset. Big tech coorperations train these, to predict what the users will do.",hqtpj2h,t3_rtkdm7,1641057745.0,False
rtkdm7,Fascinating. Can you go into a bit more detail? How many neurons are they based on?,hqtqv9d,t1_hqtpj2h,1641058316.0,True
rtkdm7,"They are based on mathematical equations, and the Neurons can be specified by the developer. There are different types of Neurons, unlike the human brain, to handle different tasks like image recognition or generation",hqtsnjf,t1_hqtqv9d,1641059053.0,False
rtkdm7,"Could this be the start of ASI? If the developers are in charge of the number of neurons, couldnt they make one with 86billion the same as a human brain ala mind control! Im freaking out 😬",hqtz4u3,t1_hqtsnjf,1641061706.0,True
rtkdm7,"Adding more and more neurons does not necessarily mean ""real"" intelligence will emerge.

A rough analogy: if you throw a 1,000 toddlers in a room, that doesn't mean the group of toddlers can solve 1,000x (or even 2x) harder math problems. They probably can solve more problems collectively than individually, but their capacity is still limited by their own toddler brains.

Likely, there is new stuff that needs to be invented to develop something we would call intelligence.

If you are interested in this stuff, I would highly recommend the book *Superintelligence: Paths, Dangers, Strategies*. It goes into a lot of the interesting concepts behind AI / how we can classify intelligence.",hqu72pp,t1_hqtz4u3,1641064938.0,False
rtkdm7,"The ""neurons"" in our artificial neural networks are very simplified models of biological neurons in a human brain. They don't function the same way and they aren't connected to each other in the same way. They don't learn the same way and they aren't seeing the same kind of experiences and data to learn from. It's sort of like a child's model of the solar system compared to the actual solar system. It's a useful model of computation -- not a physically accurate simulation of human brains. So even if you keep adding them in, there's no guarantee you're ever going to reach human level thought.

That said, suppose you built a perfectly accurate brain simulation able to do everything a human could do. On the one hand, you'll be as famous as Newton and Einstein. It's a 10,000 year discovery in terms of significance. On the other hand, creating something the works like a human brain can be done by nearly anyone given nine-months to build it and a decade or two to train it. Just making new human brains isn't especially frightening. Every infant doesn't grow up to become some sort of terrifying eater of worlds just because they can think as well as a human.",hqv2exs,t1_hqtz4u3,1641077984.0,False
rtkdm7,"Theoretically, that is possible, but practically the computational power is required, it is impossible, to build something of that scale, yet.",hqu15ob,t1_hqtz4u3,1641062532.0,False
rtkdm7,"Stop using reddit and the internet to feed delusions. I suspect you're the user Insight_7407 and god knows how many other accounts.

Stop the bullshit, and don't lie to yourself. You're seeking out this type of information and interactions intentionally, and you can easily choose not to.

https://old.reddit.com/r/computerscience/comments/r1xcw9/artificial_super_intelligence_asi/",hqwlvf7,t1_hqtz4u3,1641104054.0,False
rtkdm7,"No your misunderstanding me, this is just a thread about technology that dose exist of which im trying to understand from people who know about it..if i was delusional this process wouldnt be very helpful. I have a right to post what i want",hqx9bqi,t1_hqwlvf7,1641122101.0,True
rtkdm7,"If someone figured out how to read minds, do you think the first thing they would do is try to sell you stuff? And if they did, wouldn't they stop advertising things you don't want?",hqvn6yw,t3_rtkdm7,1641087066.0,False
rtv1bn,"Like _lambda calculus_ or _Markov algorithm_?

^(I hope you don't think of TM as of an actual, physical machinery?)",hqv54vy,t3_rtv1bn,1641079151.0,False
rtv1bn,"I suppose they both count, however Turing's model of computation seems the simplest of the three to understand, and I realize I may have not asked the same thing I was wondering. Is there a type of *mechanical computer,* as powerful as the turing machine, that can be implemented in the same way as something like [this](https://youtu.be/vo8izCKHiF0)?
Sorry for any confusion. English *is* my first language, but I'm just really poor at articulating something correctly the first time.",hqv6t1m,t1_hqv54vy,1641079857.0,True
rtv1bn,"Ah, Turing-complete mechanical computer then? Like [Babbage's Analytical Engine](https://www.youtube.com/watch?v=5rtKoKFGFSM)?",hqveuii,t1_hqv6t1m,1641083347.0,False
rtv1bn,"A 'TM' like that with a fixed-size tape has no special power, it's a finite-state machine. Trivially, consider an _n_-state FSM which has inputs for every possible transformation on _n_ states. Clearly, such a machine has a ridiculous number of inputs, but it can be used to simulate any other _n_-state FSM by attaching a little circuit in front that performs a simple mapping from the inputs of the target machine to the inputs of the ""universal"" FSM which effect the same transitions. That little circuit is like a program. The 'TM' here is like one where the program takes a single input, the turn of the crank.

Such a ""universal"" FSM can be reduced with semigroup methods to be quite compact; this (_n_+1)-state transition table (Hartmanis and Stearns 1966, p. 193) can simulate any _n_-state machine, by mapping inputs of the target machine to sequences of inputs for the table:

state | 0 | 1
---------|----------|----------
1 | 2 | _n_-1
2 | 3 | 1
⋮ | ⋮ | ⋮
_k_ | _k_+1 | _k_-1
⋮ | ⋮ | ⋮
_n_-1 | _n_ | _n_-2
_n_ | 1 | _s_
_s_ | 1 | 1

Draw out the exact table for some small _n_ > 3, try inputs 0, 1, 00, 01, ... and you may be able to see how it works. I can imagine a mechanical realization of this device which the margin of this textarea is too small to contain.",hqw384f,t1_hqv6t1m,1641094309.0,False
rtv1bn,"Yes, as others have mentioned, since the universe is finite, you will never be able to build a real Turing machine",hqyojbe,t1_hqv6t1m,1641147963.0,False
rtv1bn,">I hope you don't think of TM as of an actual, physical machinery?


There are at least three physical implementations of a turing machine that I've seen, one partly electronic, one partly electrical, and one made entirely out of wood and metal. There's a link I posted on one of my comments to see the wooden one. Of course, it is only a 3-state 3-letter alphabet turing machine with a finite tape roll, far less powerful than an infinite-state 2-letter alphabet one with an infinite tape.",hr4rs9n,t1_hqv54vy,1641248580.0,True
rtv1bn,"Yes, of course. But many people\* make a mistake of thinking that TM is exactly such physical implementation.

The difference is, in case of ""normal"" machines (ATM, fax machine, washing machine etc.), it's the implementation what matters.  
In case of TM, such literal implementations are mostly toys made for fun; it's the concept that matters.

&nbsp;

^\* ^(what baffles me, you can find some of them even amongst 2nd year CS students)",hr54ofe,t1_hr4rs9n,1641253788.0,False
rtv1bn,isn't it technically not a turning machine if the tape is finite but a linear bounded automata?,hupc038,t1_hr4rs9n,1643450197.0,False
rtv1bn,"John Conway's ""Game of Life"" is not exactly mechanical (or easy to physically automate), but it is easy to play by hand, with grid paper and beads. It is much simpler than a Turing machine, but equally powerful.",hqwmsmj,t3_rtv1bn,1641104624.0,False
rtv1bn,"Any other computational model exactly as powerful as the Turing Machine IS a Turing Machine (and even if it apparently looks different, you could express it's functioning in terms of the usual Turing Machine concept with finite control and one or more read/write tape(s)).

Understand that Turing Machine is a theoretical model of computation.

Specifically, it represents a logical set of computational possibilities. The setup with finite control and one or more (unbounded) read/write tapes can perform all of those computational tasks.

Any Machine/Model that can perform all of the computational possibilities in aforementioned set and no more, is an implementation of Turing Machine.",hqw9pdz,t3_rtv1bn,1641097433.0,False
rstlls,"It's not ""random"" as in roll a dice random.

It's random as in ""arbitrary"". Any position you like can be accessed in the same time / speed / effort.

This is as opposed to stacks, tapes, hard drives and such.

In a stack it's fast and easy to access the top (data point) of it quickly. You don't know what's below it until pop the top to the next below data point and you read it, and so forth.

In a tape or a hard drive, the ""read head"" goes through the data sequentially (literally in a mechanical motion), so that data near the read head is faster accessible than data way before or after it, since the read head needs to seek to that position first to access the data.

RAM has no mechanical or moving components, it's based on electrical current and signals being transmitted.",hqoqcw7,t3_rstlls,1640962406.0,False
rstlls,Nice reference to clear the subject,hqprlp8,t1_hqoqcw7,1640977957.0,False
rstlls,"Yeah, you almost have to think of it as “randomly-accessible memory” rather than “serially-accessible memory” for HDDs or tape drives. Noting that access can either mean read or wrote or both. 

Of course, an SSD nowadays would also be a form of randomly-accessible memory, so the modern usage of RAM to refer to volatile system memory doesn’t really make the same literal distinction as it previously did",hqr2h44,t1_hqoqcw7,1640998727.0,False
rstlls,"Even previously, a ROM chip was randomly accessible too; functionally ROM was no different to RAM when reading.",hqsgd3v,t1_hqr2h44,1641029075.0,False
rstlls,"Yeah people assume ROM and RAM are opposites, when actually they’re entirely unrelated other than both referring to memory

You can have ROM RAM, RAM that is not ROM, and ROM that is not RAM - they describe independent properties of the memory modules",hqszd29,t1_hqsgd3v,1641044891.0,False
rstlls,"While this is correct let me add  that (as an architectural detail of DRAM) you can address data stored sequentially faster due to ""burst mode"". This means that different RAM technology, despite the name, may be a little less random when it comes to time / speed / effort.",hqr8r8b,t1_hqoqcw7,1641001720.0,False
rstlls,"To be correct. This is not a property of all DRAM. DRAM just means its volatile. Burst mode capable ram is called BEDO-RAM, or BEDO-DRAM if it is volatile.",hqsh37t,t1_hqr8r8b,1641029721.0,False
rstlls,"Using addresses you can access any random memory location when you need it. You can't do that with a tape-based memory, for instance. To access the end of a tape you would need to spool through the whole thing, you can't just access any location.",hqog358,t3_rstlls,1640957084.0,False
rstlls,"Exactly, unlike ram which is directly indexed via an address pointer. So you can access memory locations in O(1), unlike the tape like he said, which would be at very minimum O(n) because you have to unspool it until you find your address.",hqov05x,t1_hqog358,1640964534.0,False
rstlls,"Does no one read the other answers? Or just assume none of the first 20 people answered it, and write out the same thing everyone else said?",hqoy7q2,t3_rstlls,1640965931.0,False
rstlls,"It's random because it's describing the way you access memory addresses - **randomly**. Prior to this type of memory, people used some sort of tapes, where you had to sequentially start from lowest address and then go further. With random access you don't need to start from lowest address, you can start at **any** address.",hqog5s2,t3_rstlls,1640957127.0,False
rstlls,"Random as opposed to serial access.

Magnetic tape was used in early computers not only store instructions but to write results.  Core memory and, eventually hard drives were random access by contrast.",hqogd8m,t3_rstlls,1640957247.0,False
rstlls,"RAM stands for Random Access Memory. Random access means that you can access any part of the memory at any time, essentially in ""random"" order.

This is in contrast to serially accessed memory in which data can only be accessed sequentially in the order that it is physically present on the medium. You can't ""jump"" to different parts of the data without ""scrolling"" through the bits in the middle.",hqolpb4,t3_rstlls,1640960118.0,False
rstlls,"From Wiki.

A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such as hard disks, CD-RWs, DVD-RWs and the older magnetic tapes and drum memory), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.",hqog0az,t3_rstlls,1640957039.0,False
rstlls,"Random is meant to characterize it as different from sequential.

In the old days, storage was on tape. This meant the tape had to be wound to the right location, so generally that type of memory is called sequential because you read/write from beginning to end. Random in this case simply means you can read/write from wherever you want without having to wind any tape.",hqommuq,t3_rstlls,1640960588.0,False
rstlls,"It's Random because you can randomly ask from any memory location and based on the information provided like index it can calculate where that memory would be.

Unlike non random where you need to check one by one each memory block to find if that's what you need.

---

**An analogy**

Array is a DS that provides random access you can just say that what `arr[i]`. You can't say that in LinkedList you need to traverse Linked List to reach to `ith` Node.",hqp86ay,t3_rstlls,1640970061.0,False
rstlls,"Additionally to the previous comments, the random access component is also relevant in the context of data structures. So for instance, stacks and linked lists don't allow random access to elements, but arrays do.",hqpky7j,t3_rstlls,1640975226.0,False
rstlls,"The second word indicated by the acronym is the clue: ACCESS.

It means you can access (read or write) any memory location at any time, just by driving the address.",hqpumfz,t3_rstlls,1640979208.0,False
rstlls,"Random access means you can access arbitrary memory locations in any order. An alternative would be sequential access in which memory could only be accessed in order and to get to a memory location at a given address you would have to go through every location whose address comes before it in order.

This isn't just a hardware thing either. There are also data structures that have these access characteristics. The simplest examples of this would be an array or hash table which has random access, a singly linked list which is sequential access only, and then a doubly linked list which has bidirectional sequential access.",hqq55ln,t3_rstlls,1640983642.0,False
rstlls,If only we could refactor technical jargon the way we do code. Arbitrary access memory just makes so much more sense.,hqroiwn,t3_rstlls,1641009652.0,False
rstlls,Ram does not stand for random it stands for Random Access Memory. Which means you can access every address of memory randomly as in it does not matter when you address which memory slot,hqqfwdx,t3_rstlls,1640988318.0,False
rstlls,"Most answers here are indicating that the ""Random"" refers to where memory is accessed - this is only partly correct.

Think about the acronym of RAM in contrast to ROM: Read Only Memory. ROM doesn't necessarily require sequential access any more than RAM does. The only difference betwee RAM and ROM is that ROM cannot be written-to, but *RAM can be written to at any time, randomly*. There is one time you write to ROM, which is the last time, but reading from ROM can be random locations as well - for example, accessing a specific file from a DVD.

The ""random"" of RAM means abritrary read or write requests at any time, not merely accessing random locations.",hqq94s3,t3_rstlls,1640985340.0,False
rstlls,access to it,hqofezp,t3_rstlls,1640956697.0,False
rstlls,Darude Sandstorm.,hqox1lh,t1_hqofezp,1640965428.0,False
rstlls,"random here just means ""not in sequence""",hqosb2r,t3_rstlls,1640963318.0,False
rstlls,Imagine a list of memory addresses sent to u when u need one verse calling a specific one,hqou33i,t3_rstlls,1640964124.0,False
rstlls,"The user is the random part, not the memory itself",hqoujia,t3_rstlls,1640964327.0,False
rstlls,"Because the CPU can directly access data at any random memory address on the RAM.

This is unlike, say, tape memory, where you'd have to go sequentially through locations on the tape, as the tape is rolled to put the right location on it under the read head, to access any particular location on the tape.

Compared to this, the CPU just has to put the address of the location on the RAM it wants to access on the address bus and (simultaneously) the read/write signal on the control bus (as well as output data on the data bus, if it's a memory write operation) and that it's. Transaction will be immediately performed on the desired place on the RAM.

The decoders on the RAM card will take the data from the address bus and trigger the appropriate memory location for operation instantly.",hqovtdk,t3_rstlls,1640964892.0,False
rstlls,It's about the time it takes; it''s constant access time for any random memory location.,hqpb5va,t3_rstlls,1640971271.0,False
rstlls,RAM stands for Random Access Memory,hqpzmit,t3_rstlls,1640981274.0,False
rstlls,"RAM is called ""random access"" because any storage location can be accessed directly.  In addition to hard disk, floppy disk, and CD-ROM storage, another important form of storage is read-only memory (ROM), a more expensive kind of memory that retains data even when the computer is turned off.",ht2f7ne,t3_rstlls,1642443372.0,False
rstlls,"Oh god, the number of people in this thread who don't know the difference between ""random"" and ""arbitrary""...@",hqprnf7,t3_rstlls,1640977978.0,False
rstlls,Perhaps you could explain the difference?,hqsgq5g,t1_hqprnf7,1641029396.0,False
rsjxaz,"New processors don't come out very often, it'd be a very boring graph to watch. The effort is would take to make something that scrapes for the actual info you want, make sure it's correct, and then displays it nicely is probably 100x more work than just updating the graph each time.  
  
It's kinda like asking if there's a realtime tracker of car fuel efficiencies (MPG). There's not really a use to spend money on it and it's not something that would be ""fun"" to make so... it doesn't get built.",hqn9xc6,t3_rsjxaz,1640927222.0,False
rsjxaz,"Not useful today but Intel has committed to breaking Moore’s Law this decade (after all, it’s only an economics law)",hqnol3m,t3_rsjxaz,1640936347.0,False
rsjxaz,"By beating it, or slacking off?",hqns8dj,t1_hqnol3m,1640939059.0,True
rsjxaz,"If you go by their last 10 years of production, slacking off",hqp6us5,t1_hqns8dj,1640969519.0,False
rsjxaz,Not fast enough for real time to be doable or noticeable,hqn9zz8,t3_rsjxaz,1640927262.0,False
rsjxaz,"I’m gonna be honest, at the rate shit is being developed id recommend putting effort into something else that’s similar.

Not saying it’s a bad idea, but it would’ve been helpful a few years ago, as it would be obsolete pretty soon. But if you wanna give yourself a challenge then I’d say go ahead and do it.",hqnawdi,t3_rsjxaz,1640927756.0,False
rs9uqq,"I studied Computer Vision as an undergraduate, I aided in some novel research using CLIP - a model developed by OpenAI. Our research went on to be awarded NSF grants and will soon be deployed in a production environment for live testing.

There’s never been a better time to get into AI, in my opinion. We are really on the cusp of it becoming generally accessible to people through the magic of State of the Art billion hyper parameter models (think GPT-3). It will only get more and more accessible, and if people can keep finding specific use cases of the general algorithms, or some fine tuned descendant, than we are in for a very exciting time as computer scientists. 

DNNs revolutionized the space and in my mind are one of the strongest arguments against another AI winter. There is an enormous amount of greenfield opportunity for research in this space. (Not that it isn’t usually fascinating work)

If I were you I would decide what kind of problem you want to solve - is it something that requires pattern recognition in structured or unstructured data? If you don’t know what that means go find out. Learn why the distinction matters.

Do you want to do computer vision? Audiovisual? Discrete or streaming data categorizations?

Find a problem and learn what the current algorithm is that provides the best solution. Learn the algorithm, read the papers, lean heavily on any professor or grad student who will help you learn. If you can try to take an elective alongside a professor you respect.

TLDR: it’s a huge field, and your question is sort of like saying you’d like to be a specialized doctor, but you don’t mind what kind. As you explore you’ll seek your interests and be able to do some amazing things.

Don’t be disheartened, it will seem like nonsense and too hard - as CS’s we are highly competitive and feel like it should all come easy. NOTHING WE DO HERE IS EASY. It only ever seems that way in hind sight.

For some, AI is the first part of coding that actually challenges their mind. Let that be an indicator of your growth, not a source of insecurity.",hql4ror,t3_rs9uqq,1640893908.0,False
rs9uqq,"Well said and certainly eloquent. I totally agree that the time is NOW, hence why I’ve been finding these videos and highlights on AI so interesting. I’m going to follow your ideologies here a bit and see where it takes me. 

I’ve always been in love with finding new solutions. And AI research is on the frontier of finding these new solutions. I’m excited to see where it takes me, but I am certainly feeling insecure just due to my lack of knowledge. But like you said, I’ll change my mindset to make that a ladder for my own growth instead of quick sand. thank you 🤝",hql5pq8,t1_hql4ror,1640894278.0,True
rs9uqq,"Your school doesn't offer a course on AI / ML ?

Also, you may be interested in reading _Artificial Intelligence: A Modern Approach_ by Russell and Norvig as it is considered the standard textbook on the topic.",hql2fbb,t3_rs9uqq,1640893006.0,False
rs9uqq,"Good point. It’s a small liberal arts college and it does actually offer one ML course within the CS major. Although due to the timings of how classes work there I wasn’t able to ever take it. 

On the other hand the book is a fantastic suggestion. Looking into it now, thank you",hql2p5c,t1_hql2fbb,1640893110.0,True
rs9uqq,"+1 for Russel and Norvig, I took a class that used it and it's really quite good.",hqnei4b,t1_hql2p5c,1640929793.0,False
rs9uqq,Yeah that book is really good.,hqlrp8l,t1_hql2fbb,1640902973.0,False
rs9uqq,Start by learning basic regression and go from there,hqnmfnc,t3_rs9uqq,1640934816.0,False
rs9uqq,"Most of the actual developments involve the maths of how the model works (When it comes to Deep Learning models anyway - as people have pointed out, AI is quite a vague term).

AI research is a very experimental field that relies on demonstrating empirical results. As with most fields, advancements follow on from previous advancements. Many highly impactful AI papers are just refinements of previous developments. For example, the difference between a ""traditional"" GAN and a Wasserstein GAN (wGAN):

[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661) (Original GAN paper)

[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875) (wGAN paper)

It essentially uses the same architecture, or at least is basically architecture agnostic when it comes to layers, etc. The important thing was the switch from using minimax Loss which is reliant on cross-entropy between probability distributions, to using the Wasserstein distance instead. This practically speaking solved significant problems like mode collapse.

Also, Deep Learning itself can be kind of counter-intuitive, for example:

[https://www.pnas.org/content/117/48/30033](https://www.pnas.org/content/117/48/30033)

If you're asking how people come up with these things in the first place - I can't really give you an answer. All research really starts with an intuition. Knowing a decent amount of maths is probably a good start, at least as much as allows you to be able to research topics on your own and gain an understanding of them. However, you don't have to be a mathematician - colleagues of mine come from all sorts of backgrounds: Biomedical Engineering, Software Engineering, CompSci, Business, etc.

If you're coming from a CompSci/Software Engineering background, it will be quite easy for you to jump in and start running state-of-the-art models. Here's a Colab notebook you can try out:

[https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/\_downloads/torchvision\_finetuning\_instance\_segmentation.ipynb](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb)",hqlojr4,t3_rs9uqq,1640901702.0,False
rs9uqq,"Hey thanks for this, it’s a more complex response so I have to read through it but I appreciate the response ! Helps me get some direction",hqlv5fg,t1_hqlojr4,1640904353.0,True
rs9uqq,I am doing my PhD in AI right now and started about three years ago with courses on it from Andrew Ng. Go visit deeplearning.ai and start those courses and interactive sessions 😊,hqo4kj6,t3_rs9uqq,1640948987.0,False
rs9uqq,[deleted],hql3fgr,t3_rs9uqq,1640893390.0,False
rs9uqq,That’s helpful. I was missing the part about grad school. Since i’m only going to graduate with a BA in Comp Sci and I didn’t think about grad school it makes sense that’s where you would learn how to write scientific docs. Now I have to decide if that’s what I want to do in the future… I’m certainly no genius though 😂,hql3ugg,t1_hql3fgr,1640893550.0,True
rs9uqq,[deleted],hql54ie,t1_hql3ugg,1640894047.0,False
rs9uqq,"No worries, this gives me enough I go to look into it on my own. Cant thank you enough 🤝",hql8szm,t1_hql54ie,1640895506.0,True
rs9uqq,"I own a textbook teaching fundamental AI technology. I spent time implementing the technology as a computer application. As part of my study in AI technology, I learned game programming so that I could write AI applications that would display something cool to watch.",hqm7j4c,t3_rs9uqq,1640909436.0,False
rs9uqq,"It starts as simple as it can get, a simple choice-making model, such as a couple of match boxes with colored beads, where every state in a game of hexapawn is represented by one matchbox. The beads inside the boxes represent the potential choices the model can make.


You start playing, and everytime it is the model's turn, you pick the box that represents the current state of the board. You shake the box and randomly pull a bead out. This bead should have a meaningful choice associated with it, and you perform this such choice.


By the end of the game, if you won, you remove the bead that represented the model's last move, which means it can no longer make that bad move, which necessarily, at least near the end of the game, makes the model slightly better at the game than before.


After several games, you will notice the machine winning more, because the choices it made before, the ones that lost the early games, can't be performed anymore.


At least, that's my understanding. This is attributed to Vsauce2. Soon, you get into understanding the different types of reward models and choice models, which are sort of plug and play. Some have a mathematical definition, such as a fitness function or a neural network that makes the choices instead of having a box for every possible state. There's also a few different ways to iterate the model, such as directly manipulating the model, genetically evolving the model, or just randomly changing everything about the model over and over until something happens to work.

Granted, I am not super experienced in the field, as I haven't personally been able to experiment with machine learning, but I've been looking into all sorts of computer science subfields.",hqx0wh4,t3_rs9uqq,1641115092.0,False
rs9uqq,"There's a couple of Ai Libraries (Tensorflow, Pytorch, Scikit learn). Just start by reading the guides on one of them (I recomment Scikit learn as it's targetted towards beginners), and Google anything you don't know.

There's no one path to learn ai, I've found that it's primarily just looking around and seeing what you find.",hr58pfb,t3_rs9uqq,1641255459.0,False
rs9uqq,"I got a PhD in Econ and did a dissertation in computational agent based economics.

""AI"" is such a broad term and used inconsistently. A\* pathfinding used to be considered AI.

Some are talking about logistic regression to build decision trees.

Some are talking about genetic algorithms.

I use behavioral trees in my hobby game project.

Here is the general rule of thumb: don't focus on methods. Methods are for people on the left side of the dunning krueger curve. You limiter is always your data. If you are making a game you probably don't have it set up to collect game data and adjust AI behavior on that so you just go with creative methods of building AI like a behavioral tree that you build by play testing.

AI is overused and oversold. Most of what the explosion in the recent years is just logistic regression and some algorithms that adjust the models in real time.

A lot of people into programming turn tail and run when people start talking about ROC curves and Gini statistics.

If you are doing actual data analysis you have to be careful about just tossing data in some off the shelf python library. Depending on what type of data it is, its very easy to make junk conclusions. In economics the big mistake everyone makes is including GDP in models when it should be GDP growth rates.",hqlcb8z,t3_rs9uqq,1640896893.0,False
rslk6w,"What's your background? Do you already have a strong background with AI (ML, stats, etc)?",hqowuu4,t3_rslk6w,1640965349.0,False
rslk6w,I'd say I have a very theoretical grasp on ML and a solid understanding of statistics.,hqp0g0o,t1_hqowuu4,1640966877.0,True
rslk6w,"I'm assuming you meant 'a strong grasp on the theory', and not 'theoretically I know it exists'! :)

I seem to remember Deepmind publishing work where they advocated for reinforcement learning with reward being all that's necessary for that leap to be made, or something along those lines.

If my memory is right, that would probably be a great place to start for a literature review because Deepmind garners a lot of attention so there's likely to be a reasonable amount of published content referencing it.",hqp2r9f,t1_hqp0g0o,1640967845.0,False
rs0z7i,"Building a binary tree where the nodes are the agents should work.

After building the nodes (lets assume numner of agent n is in a form of 2**k for some k) you will go in post-order traversal on the tree where the value of each would be the winner between both of his childs.

If we assume each game takes O(m) operations then finding the root would take O(nm) operations.

A very important note that this is an approximation for the best, as usually there is not a single best (agent a beats b, b beats c and c beats a for example) so if your simulation is fast i would try to simulate a bunch of the trees with each agent recive a mark based on how far he ends up from the root and see how much the data flucuate with more simulations.",hqjjdr4,t3_rs0z7i,1640870058.0,False
rs0z7i,"I think that might work, but after each round, the top X% are selected to breed the next generation while also participating in the next round. So I already know how strong they were in the previous generation, so I was thinking maybe I can cut the number of games in the following generations because I know how strong some of the agents are. Maybe you can help me find a way?",hqntva4,t1_hqjjdr4,1640940324.0,True
rs0z7i,"Hmm that's not trivial and I dont think theres an optimal solution for you.

On top of my head theres three options, all has their drawbacks and advantages over the other methods. 

One option would be saying that the ranking of last generation is somewhat noisy, in this case you could just treat your old agents just like new ones with the difference of the ranking would be:

r_new(iteration i) = \gamma r_new(iteration i - 1) + (1-\gamma) r_old(iteration i)

With gamma being a parameter of your choice between 0 and 1. 

This method would not decrease the number of evaluations but it would lower your ranking variance which would help your algorithm to converge.

The second method would be building your match tree in such way new agents would always be against old agents.

You can do that by rebuilding your match tree after every round and enforcing new agents would always be against old ones. Eventually you would need to pair old ones against old ones and you would simply take the better agents from last iteration without simulating a match.

This would yield low amount of simulations as you wont simulate old agents against each other, and it would work in the case where all new agents are better then the old ones (after a round the entire match tree would be only new agents), but the drawback being you might lose out new good agent quickly when he would face the best old agent in the first round - not something you want.

The last method comes to mind is somewhat hard to explain. its similar to the second method but instead of using all of the old agents in the first round, the first round you would use the worst half of your agents and all of the new agents. In the second round you would insert the worst half of the remaining agents and so on.

This would work similarly in terms of amount of matches to the 2nd option - this is better for cases where the new generation is not that different then the old one.

That's a fun exercise and those were just methods from the top of my head - there could be better options then i stated.",hqny403,t1_hqntva4,1640943740.0,False
rs0z7i,Is this the same problem ELO solves?,hql8ycf,t3_rs0z7i,1640895564.0,False
rs0z7i,"Kind of, but I want the agents to play as few games as possible, and elo ratings take a lot of games to be accurate.",hqlezx6,t1_hql8ycf,1640897945.0,True
rs0z7i,"Both answers were perfect. I have been interested in different types or agent modelling, so that was helpful.",hqm1fy7,t3_rs0z7i,1640906933.0,False
rs0z7i,"To me, that type of model, while it sounds slow, seems like it could find the best player in a batch much more accurately than giving them all a single-number score.",hqx1396,t3_rs0z7i,1641115246.0,False
rs0z7i,"Hi, this is really interesting. How does one implement a game so that agents can play chess? Like a specific language? I know this is a noob question.",hqjzmx8,t3_rs0z7i,1640877976.0,False
rs0z7i,"What is the question? How to implement chess or how to implement the agents?

For the agents, I used a monte carlo search tree where the final positions are evaluated with a neural network

For implementing the game, I used the python chess library, if you want to implement it yourself there is a lot of material on the topic",hqleo42,t1_hqjzmx8,1640897813.0,True
rr75ns,"Rule #5 bans joke submissions, but in this case I think I will make an exception since in the title we have a genuine question about research on sorting algorithms.",hqexeo5,t3_rr75ns,1640789165.0,False
rr75ns,much appreciated! 🤗,hqp2my9,t1_hqexeo5,1640967795.0,True
rr75ns,"The phenomenon described depends on levels of serotonin and octopamine secretion where serotonin levels are high in dominant crabs and octopamine levels are high in submissive crabs. So, the submissive crabs tend to make sure the coast is clear before laying claim to anything.
If by chance no dominant crab shows up during the eight hours of waiting and shows up later after, then the submissive crab either gives up the loot or challenges the obviously dominant crab which could be brutal for either or both of them with the winner and the loser getting higher levels of serotonin and octopamine respectively.

Totally unrelated but👍",hqey5kd,t3_rr75ns,1640789506.0,False
rr75ns,appreciate the fun fact!,hr01kky,t1_hqey5kd,1641166521.0,True
rr75ns,"You can see it in action here:
https://youtu.be/f1dnocPQXDQ",hqexc1l,t3_rr75ns,1640789131.0,False
rr75ns,I dont think waiting for up to 8 hours would be a good starting point :),hqejw8s,t3_rr75ns,1640781898.0,False
rr75ns,Maybe they're using [SleepSort](https://www.quora.com/What-is-sleep-sort)?,hqesbxq,t1_hqejw8s,1640786707.0,False
rr75ns,It's good enough for crypto people,hqezvfd,t1_hqejw8s,1640790283.0,False
rr75ns,"I think we already have a lower asymptotic bound on the time complexity of sorting algorithms at O(n logn):

If we only permit comparisons and swaps between two elements, then every sorting algorithm can be represented by a binary tree where each node is a comparison, with only two possible outcomes (assuming there is a total order on all elements).

Any correct sorting algorithm must have a different set of swaps for each permutation of an n-length input, so there must be n! leaves, so the height is lg(n!), Which is Omega(n logn)

Of courses, there is still possible research around real-world applications",hqew0s2,t3_rr75ns,1640788512.0,False
rr75ns,">I think we already have a lower asymptotic bound on the time complexity of sorting algorithms at O(n logn):

Just want to clarify that this is only for comparative sorting. There are faster sorts for some specific cases, such as a Radix sort.",hqgnm2i,t1_hqew0s2,1640813733.0,False
rr75ns,How does the time change when it is the elements of the list comparing themselves? This allows for every element to be compared to one other simultaneously. Wouldn't crabsort then be O(n)?,hqf2s0c,t1_hqew0s2,1640791562.0,False
rr75ns,"I prefer using Computational complexity over Time complexity as it represents the relationship between the number of data elements and the amount of operations that need to be executed, and the growth of the relationship. 

It's not exactly time, so much as how many steps do I have to complete (which indirectly is time). Even if you're doing comparisons for each individual in parallel as it relates to others, you would still have n individuals doing n observations, for o(n^2) computational complexity.",hqgprgk,t1_hqf2s0c,1640814569.0,False
rr75ns,"The answer depends on your computational model. Assuming you work on a single tape touring machine, your sorting algorithm would be in O(n²) - you sort n elements whereas each element has to compare itself with up to all elements already in the list. On a PRAM it is in O(n²) as well since you multiply the number of cores by the number of instructions each core has to compute.",hqf5rch,t1_hqf2s0c,1640792843.0,False
rr75ns,"The Turing Omnibus mentions a linear time sorting algorithm called ""spaghetti sort."" Grab spaghetti with different lengths, put it in your hand, and tamp down gently on the bottom until they're all even. Repeatedly pick out the longest stand by putting your hand on top and picking the first one it hits. Since both these hands operations are constant time, you get a linear sorting algorithm!!

Let's do a quick analysis to see where this algorithm, and any ""natural"" algorithms in real life, fall short:

- hands are slow. Where a computer might pick something from memory in a few microseconds, it takes dozens of milliseconds to pick out spaghetti from your hand. That's a hard time limit, since physics prevents the spaghetti from being moved too quickly lest it break.
- there are practical bounds on the length of spaghetti.
- it's hard to hold more than, say, 100 spaghetti strands. Practical sorting will require millions of items to sort.
- in order to make this transferable to and from a computer, the computer has to output spaghetti of certain lengths, or cut spaghetti to a certain length. Beyond the precision required, it's just slow. It might be faster reading the spaghetti, although that may require a computer vision algorithm or some sort of spaghetti index.

So, even though it is a linear time sorting algorithm, practical problems abound around the interface and scaling. I think you'll find those problems anywhere you attempt to use a natural algorithm.

To wit: even if you digitized this algorithm, you'd just be doing insertion sort: the hand moving down from above would be replaced with a physics engine determining interactions between all n spaghetti strands, which is n operations n times for a n^2 runtime.",hqf0e7a,t3_rr75ns,1640790513.0,False
rr75ns,What you described is essentially just an oracle that returns the maximum of a list in constant time.,hqgcslz,t1_hqf0e7a,1640809574.0,False
rr75ns,"I think the selection step actually should take linear time in the max height, since if you're actually applying a uniform procedure, or if you built a machine to enact this process, it must start at least at the max height and potentially sweep all the way down through the range of heights. Consider the case of one 6"" spaghett and ten million spaghetts of negligible height; the average distance each sweep is almost 6"" and distance is time.",hqhnkvh,t1_hqgcslz,1640828756.0,False
rr75ns,"Not sure I completely get what you’re saying. I’m personally imagining some spaghetti standing upright in a bunch on a table. There is then a flat surface from above weighing down. The flat surface repeatedly selects the point of contact with the set of spaghetti, which is just a maximum element, and removes it from the clump (note the surface is already touching the max so it doesn’t need to perform an entire sweep to extract it). It only makes one sweep in total, so it takes O(n + h) comparison and extraction operations where n is the number of noodles and h is the maximum height of a noodle. This is better than radix which uses O(kn) operations where k is the number of digits.",hqhoa7d,t1_hqhnkvh,1640829060.0,False
rr75ns,"If the tallest one is in the middle of the bunch, you have to lift the plate back up to pick it out.",hqhoy2g,t1_hqhoa7d,1640829347.0,False
rr75ns,Why? I think our visualizations is different. Did you read my description?,hqhp2zi,t1_hqhoy2g,1640829408.0,False
rr75ns,Yes. How does a flat surface remove the spaghett it hits? Going back to a person with two hands- one hand holds the bunch together vertically. The other hand sweeps down to hit the tallest and then picks it out. That means potentially sliding it vertically out of the middle.,hqhppp8,t1_hqhp2zi,1640829697.0,False
rr75ns,"Left hand is on the bottom. Spaghettis align flat on the left hand. Right hand is on top pressing down. It is constantly in contact w the max. It extracts that noodle by repeatedly letting the one in contact fall. No sweeping needed to remove the max, as the right hand is just constantly pressing down from gravity.",hqhqjov,t1_hqhppp8,1640830079.0,False
rr75ns,"How does the noodle in contact fall through the left hand? To ensure heights remain comparable they have to be flush at bottom, against the hand or preferably the table.
And in any case, once we grant they must slide out that provides a new worst case: they are all MAXHEIGHT to within a negligible distance. Then to get free each one must slide ~MAXHEIGHT inches and we are linear in max height again.",hqhr2nx,t1_hqhqjov,1640830310.0,False
rr75ns,"The left hand can be a grid surface that can open up the grid point corresponding to the grid point that the right hand is contact with. Also yes, the linear in h is unavoidable (its also not new, thats what the h is in my previous comment), but it is O(h + n) rather than O(hn) as you describe, which is much worse. Note it is O(n+h) instead of O(nh) because the noodles are sliding concurrently, the reason it is so effective is because it can take advantage of this huge parallelism. A lot of “biological algorithms” operate on this principle: go read on Adelman’s work on solving a non-trivial TSP instance through biological algorithms. O(n + h) is better than radix sort while O(nh) is not, and that is important as radix sort is sort of the “standard” for non-comparison sorts. 

Also you are thinking too much into the hand analogy, they can easily be held in place by some easily implementable mechanical process, or just don’t use noodles lol. The point is that there are physical sorting algorithms that use a number of operations linear in every parameter, e.g. O(n + h) instead of O(nh), which are enabled due to parallelism scaling with the size of the problem, which is clearly not a property of problems solved in computers. That is what the not-really-active field of biological algorithms tries to take advantage of.",hqhrexn,t1_hqhr2nx,1640830458.0,False
rr75ns,The book Algorithms to Live By talks a lot about interesting cases like this!,hqf2h69,t3_rr75ns,1640791431.0,False
rr75ns,thank you!,hqp04x2,t1_hqf2h69,1640966747.0,True
rr75ns,You could easily crop this image without the giant text claiming people that provide a service to people to rent a place to live as being awful people.,hqgm0w9,t3_rr75ns,1640813124.0,False
rr75ns,!RemindMe 24h,hqhkbzk,t3_rr75ns,1640827321.0,False
rr75ns,"I will be messaging you in 1 day on [**2021-12-31 01:22:01 UTC**](http://www.wolframalpha.com/input/?i=2021-12-31%2001:22:01%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computerscience/comments/rr75ns/it_would_be_really_interesting_to_research/hqhkbzk/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputerscience%2Fcomments%2Frr75ns%2Fit_would_be_really_interesting_to_research%2Fhqhkbzk%2F%5D%0A%0ARemindMe%21%202021-12-31%2001%3A22%3A01%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20rr75ns)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",hqhkfw6,t1_hqhkbzk,1640827369.0,False
rrvks1,"There is a way to solve them programmatically, and yes, it is a kind-of brute force, but a common technique is to not write the 'brute force' piece yourself, but to rely on a tool (sometimes a library in a programming language, sometimes a first-flass language in it's own right) called a ""SAT Solver""

[https://cse.buffalo.edu/\~erdem/cse331/support/sat-solver/index.html](https://cse.buffalo.edu/~erdem/cse331/support/sat-solver/index.html)

Here's a blog entry closer to the problem type your are trying to solve:

[https://sabhijit.medium.com/logic-puzzles-and-sat-solvers-a-match-made-in-heaven-5e0a7a64c04b](https://sabhijit.medium.com/logic-puzzles-and-sat-solvers-a-match-made-in-heaven-5e0a7a64c04b)

Here's a paper specifically about the ""logic grid"" type of puzzles you see here

[https://freuder.files.wordpress.com/2019/09/challenge-ucc-submission.pdf](https://freuder.files.wordpress.com/2019/09/challenge-ucc-submission.pdf)

Languages like prolog and icon are specifically geared towards solving this kind of problem in a declarative style.",hqkdhle,t3_rrvks1,1640883542.0,False
rrvks1,"Link for better formatting: [https://stackoverflow.com/questions/70527987/solving-logic-puzzles-using-r](https://stackoverflow.com/questions/70527987/solving-logic-puzzles-using-r) 

Thanks!",hqiqd91,t3_rrvks1,1640848651.0,True
rrvks1,Take a look at Logical Programming and SAT,hql0m9c,t3_rrvks1,1640892312.0,False
rrvks1,Consider implementing a DPLL solver. You can use a variety of heuristics to perform substantially better than random in practice.,hqlvd13,t3_rrvks1,1640904438.0,False
rrvks1,"This is a job for quantum computers!
I want a quantum computer so bad...",hqx17cm,t3_rrvks1,1641115341.0,False
rrsvuo,"Applications have states of process, like waiting, running, and so on.  You double click gmail to check your messages, while gmail is loading you check your Facebook page, but it's dead, so next you access Instagram.  Now you go back to check your gmail, this is known as Context Switch.  First you opened gmail, the scheduler will grab gmail app and start loading it, but you wanted to see who is on Facebook, the gmail app will go to a wait state and placed in the queue, then Facebook will go active, then you checked Instagram, so Facebook goes to the wait state and placed in queue and Instagram goes active. Then you go back and check your emails.  Now Instagram goes to the wait state while gmail goes to the active state.  
  No matter how it's worded, the CPU can only execute one app at a time.  It can load a program in each core while executing the main program, but it can only run one app at a time, but it will switch to another app in queue if it sees the current app go to a wait state, like waiting for user input.  It can switch between thousands of apps in seconds. 

So you have a scheduler that handles the removal of the running process from the CPU and the selection of another process on the basis of a particular strategy.

The OS scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system;

Schedulers main task is to select the jobs to be submitted into the system and to decide which process to run.


CPU scheduler main objective is to increase system performance, also selects a process among the processes that are ready to execute and allocates CPU to one of them.

Short-term schedulers, also known as dispatchers, make the decision of which process to execute next. Short-term schedulers are faster than long-term schedulers.


Medium-term scheduling  removes the processes from the memory. The medium-term scheduler is in-charge of handling the swapped out-processes
A suspended processes cannot make any progress towards completion. In this condition, to remove the process from memory and make space for other processes, the suspended process is moved to the secondary storage. This process is called swapping, and the process is said to be swapped out or rolled out. Swapping may be necessary to improve the process mix.


A context switch is the mechanism to store and restore the state or context of a CPU in Process Control block so that a process execution can be resumed from the same point at a later time.

When the scheduler switches the CPU from executing one process to execute another, the state from the current running process is stored into the process control block. After this, the state for the process to run next is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing.

This is the basics of a CPU running applications.  There is another task in there I cannot recall at this moment.",hqihlvj,t3_rrsvuo,1640843091.0,False
rrsvuo,I applaud you taking the time to write all of this out.,hqij16y,t1_hqihlvj,1640843906.0,False
rrsvuo,"The answer of u/DevilDawg93 is about the OS scheduling, but misses the part about the interaction with I/O devices. To answer your question: there are multiple levels of security in which a CPU can be, mainly the **privileged mode** in which the software can do anything, and the **user mode**, in which the CPU forbids access to the areas of the memory not explicitly allowed, access to I/O devices, use of some instructions, etc.

How it works: when the OS starts booting, it executes in privileged mode. It can organise the memory of your computer whenever it wants, and use some areas in the memory (the RAM) to write some important tables, in which you will find the pages tables (which indicates the regions of the memory on which a process can read and/or write). The OS sets some important registers of the CPU: one is pointing to the **interruption** handler, a function in the OS that can manage interruptions, another to a page table.

When the OS switches to a process, it writes in the special register the pointer to the pages table of the process, switch to user mode, and then jump to the code of the process, letting this process do its stuff. Because the process is in user mode, it can't do anything dangerous (requiring the privileged mode)... but can ask the OS to do it. It can trigger an interrupt (a system call), which will call a function of the OS. The CPU switch to **privileged mode** when it happens, and this is safe because only the OS could have written in the register referencing the interrupt handler. So there is no way (outside a security breach) for the user process to write in another program memory or use the I/O directly, the restriction is built in the CPU itself. The application **must** ask the OS to do it itself, and then the operating system takes the time to check if the application is allowed to do so, or not.

Of course this is a bit simplified view of how it works, modern CPU are very complex, but it still works according to this schema. I should add that switching from one process to another (as described by u/DevilDawg93) works the same: in privileged mode, the OS can configure the hardware clock to send a signal (for example every X µs). Each time the signal is sent, an interrupt is triggered, and so the CPU starts executing OS code in privileged mode. It is up to the OS to save the process state somewhere, set the CPU registers (for the pages table of the next application), load this next process state, switch to user mode and start executing it \[this is the context switch\].",hqjoe0s,t3_rrsvuo,1640872773.0,False
rrsvuo,"Well stated u/webalorn! 

Happy Cake Day!",hql2nr0,t1_hqjoe0s,1640893096.0,False
rrsvuo,"Thanks a lot guys, it’s very clear to me now !",hqkihf4,t3_rrsvuo,1640885423.0,True
rq60d4,Rule 6. Format your code and text into maintainable and easily readable form. Apply to Reddit posts.,hq8jcrb,t3_rq60d4,1640664315.0,False
rq60d4,"Disagree. If it was hard to write, it should be hard to read.",hqal7s1,t1_hq8jcrb,1640709390.0,False
rq60d4,Lol,hqaljhv,t1_hqal7s1,1640709522.0,False
rq60d4,"If I had to suffer, others have to suffer too? Is this what you mean?",hqbbc1r,t1_hqal7s1,1640719727.0,False
rq60d4,"Yes, but they have to suffer in a different way.",hqbdqq3,t1_hqbbc1r,1640720687.0,False
rq60d4,Sounds petty tbh,hqbnbfb,t1_hqbdqq3,1640724560.0,False
rq60d4,"It's not; if it's hard to read, then hopefully it scares the interns off.",hqbpppw,t1_hqbnbfb,1640725543.0,False
rq60d4,[deleted],hq8mfwp,t1_hq8jcrb,1640665815.0,False
rq60d4,"The real Rule #7: There are plenty of rules to programming for a real purpose, get creative on your own time.",hq8qyia,t1_hq8mfwp,1640668201.0,False
rq60d4,[deleted],hq8sp4l,t1_hq8qyia,1640669185.0,False
rq60d4,"Do you believe that problem solving requires you to have literally no limits? There are rules. There are areas where there are many options within the rules, but there are rules.",hq8t8rv,t1_hq8sp4l,1640669501.0,False
rq60d4,[deleted],hq8zy09,t1_hq8t8rv,1640673693.0,False
rq60d4,"He said, having presented nothing but anecdotes himself...",hq90is9,t1_hq8zy09,1640674098.0,False
rq60d4,[deleted],hq916fk,t1_hq90is9,1640674555.0,False
rq60d4,"That's... not what straw man means. You might have been thinking ad hominem?
 
You've presented no arguments that require countering.",hq91h8a,t1_hq916fk,1640674767.0,False
rq60d4,[deleted],hq928ny,t1_hq91h8a,1640675318.0,False
rq60d4,"I didn't edit any comment - you edited this one though. That little * you see indicates edits, are you having some kind of psych episode?",hq92qvf,t1_hq928ny,1640675686.0,False
rq60d4,Rob Pike on his 6 Rules of Programming - [https://twitter.com/rob\_pike/status/998681790037442561](https://twitter.com/rob_pike/status/998681790037442561),hq9ejqb,t3_rq60d4,1640685077.0,False
rq60d4,"Rule 5 restates Torvald's, ['good programmers worry about data structures](https://softwareengineering.stackexchange.com/questions/163185/torvalds-quote-about-good-programmer)'.",hq8mxyf,t3_rq60d4,1640666071.0,False
rq60d4,"Rob Pike wrote this first in 19**8**7. Linus would have been 18 at the time and didn't start work on Linux until 1991.

https://twitter.com/rob_pike/status/998681791417409536

https://en.wikipedia.org/wiki/History_of_Linux#The_creation_of_Linux",hq9e7mf,t1_hq8mxyf,1640684796.0,False
rq60d4,"**History of Linux** 
 
 [The creation of Linux](https://en.wikipedia.org/wiki/History_of_Linux#The_creation_of_Linux) 
 
 >In 1991, while studying computer science at University of Helsinki, Linus Torvalds began a project that later became the Linux kernel. He wrote the program specifically for the hardware he was using and independent of an operating system because he wanted to use the functions of his new PC with an 80386 processor. Development was done on MINIX using the GNU C Compiler. As Torvalds wrote in his book Just for Fun, he eventually ended up writing an operating system kernel.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hq9e8mx,t1_hq9e7mf,1640684821.0,False
rq60d4,[deleted],hqa4juy,t1_hq8mxyf,1640702229.0,False
rq60d4,Excellent. You get all the disadvantages of excess complexity combined with all the disadvantages of unpredictable performance and inability to scale.,hqbz6x8,t1_hqa4juy,1640729434.0,False
rq60d4,"Ironically, this is what systems programmers are always complaining that frontend developers are doing: using inefficient but simple algorithms until necessary to change.",hqa4g2h,t3_rq60d4,1640702180.0,False
rq60d4,Really? The most common complaint I hear is the opposite - that frontend development is usually ridiculously overengineered for example like introducing a mountain of 3rd party libraries to make trivial systems.,hqbgoe9,t1_hqa4g2h,1640721869.0,False
rq60d4,"The problem is that people don’t have a consistent definition of that word. Isn’t overoptimizing a form of overengineering? So then “not optimizing” by deferring processing to third party libs couldn’t possibly be overengineering. Except that it is, somehow.

Besides, while dependency explosion is a real problem, that’s mostly a fault of the ecosystem and not the developer. Typically all you need is about 5 direct dependencies before the number of node modules is in the thousands. This isn’t the fault of the engineer and has very little to do with their development practices.",hqbne1p,t1_hqbgoe9,1640724590.0,False
rq60d4,">  deferring processing to third party libs couldn’t possibly be overengineering. Except that it is, somehow.

That's a false dichotomy.

You're assuming the only two choices are implementing something complex yourself or pulling in a complex library. There's also the option of doing something simple, or, even better, leaving out that element entirely.",hqbzlyr,t1_hqbne1p,1640729610.0,False
rq60d4,"I know it sounds like that’s what I am saying. But my real point is that outsourcing processing of a thing to avoid overengineering, should not itself be overengineering, yet it is. In other words, the opposite of an action should not be another flavor of the action itself.

Just because your bundle size is large doesn’t mean you overengineered something. It may have been way simpler to use a library than to build it yourself. It’s kind of like saying using a car to get to the other side of the state is overly engineered because you could’ve walked it, or you could have used a bicycle.",hqc08ol,t1_hqbzlyr,1640729876.0,False
rq60d4,"If you pay really close attention you'll notice, that nowhere did I argue against using 3rd party libraries.

I used dependency of *too many libraries* as *an example* of overengineering. This has nothing to do with the notion of moving the processing in-house somehow making it not overengineering.",hqc1t7r,t1_hqc08ol,1640730527.0,False
rq60d4,"> There's also the option of doing something simple, or, even better, leaving out that element entirely.

There's *sometimes* that option. But generally, this is not what people are complaining about in frontend development. They aren't complaining that $APP does too many things, and that they wish it just had less functionality.",hqc3ojh,t1_hqc1t7r,1640731312.0,False
rq60d4,I think you meant to reply to /u/Tai9ch,hqc4fo2,t1_hqc3ojh,1640731629.0,False
rq60d4,"Yes, sorry I pulled a switcharoo on you by deleting my (poor) first draft.",hqc7v6z,t1_hqc4fo2,1640733084.0,False
rq60d4,No problem. :-),hqc9gfd,t1_hqc7v6z,1640733774.0,False
rq60d4,"Which word? Optimization? Neither OP nor your first reply nor my reply mentioned that word even once, so I'm not entirely sure why you introduce this concept.

Overengineering isn't at all the same. Optimization is figuring out how to do the least amount of work to accomplish a task. Overengineering is introduction of an unacceptable amount of accidental complexity. Optimization can be overengineering, if the optimization isn't necessary, but these concepts have nothing in common besides this.

I agree though that there is a lot of confusion and misunderstanding because technical words are misunderstood, misused and abused all the time. There are a lot of blinds leading the blinds in this industry.

I partially agree about your comment on dependency explosion. It's partly an ecosystem issue, but also partly a cultural issue. I will not fault the engineer of the dependency explosion as a whole, but I refuse to absolve the engineer of the responsibility of their choice of libraries for their project.",hqbrazk,t1_hqbne1p,1640726200.0,False
rq60d4,"I think I made a mistake by going down this road of talking about bundle size. It's really different from the main complaint I have heard, which is that the algorithms are inefficient and as a result, web apps and electron apps etc. use way more cycles/battery power than they need to. My retort to that is that of course, that's the only reason you're using the program -- because it could be built faster than we used to build software. And it's done faster now because we have the power of computationally inefficient tools like ES6 spreads and dynamic hashtables forming immutable datastructures, which make programming itself more efficient of a (human) process than something like dirty flags and mutable state/pubsub insanity.

I disagree on definitions for overengineering, optimization, engineering. IMO optimization and engineering are basically the same thing; you have a system of constraints, as well as a system of preferences, and are trying to produce a solution that solves all of the constraints while honoring the preferences as much as possible. You can engineer for certain things (our company has no money, it needs to be entirely libre software) or others (our company has infinite money, spend as much as possible to get it done quickly), just as you can optimize for either memory or speed. Over-engineering is a strange word because in e.g. bridge design it basically means ""to make something stronger."" To make something stronger in computer engineering is to make it simpler and easier to manipulate, which I feel is really not what people mean by ""overengineering.""

Typically when people complain that an Electron app is overengineered, they don't mean it in the traditional sense of the word, which would be to say it has too much functionality, too much capability. They are cajoling it to mean ""they spent too much engineer time"" on this, I guess, which to me is paradoxical since the whole point of bringing in React, Angular, etc. is to save time, and it often does.",hqc4vim,t1_hqbrazk,1640731816.0,False
rq60d4,"It's alright. :-)

I kind of agree with your definitions. Sort of.

Optimization is a loan word from the field of mathematics, which - loosely speaking - means to maximize or minimize a function given a set of constraints.

I'd rather describe engineering as the practice of designing a product while constantly assessing and choosing trade-offs between various considerations - quality metrics like application speed, size, features and correctness, but also ""softer"" considerations like development time, maintainability and customer requests. What you describe as optimizations I'd rather describe as trade-offs, because that's really what they are - what do we want and what is the cost? These trade-offs are what an engineer must make all the time and try to balance everything acceptably.

I fully agree that overengineering is an odd word. We generally agree that engineering is a good thing, so it's odd overengineering became a pejorative for a particular, disliked strategy of trade-offs.

Anyway, to get back to the central point of our conversation. I come from a system programmers perspective, and I too find it odd that web apps are criticized for algorithm efficiency. Web apps tend to use a lot of cycles, but I don't think the algorithms are the culprits here - I think the cycles may be eaten either in the abstraction layers or by the interpreter itself. This is just pure conjecture though, because I'm not knowledgable enough about web dev to confidently make an informed judgment about it.",hqc94ix,t1_hqc4vim,1640733630.0,False
rq60d4,"I had a thought, and I have a conjecture on how the meaning of overengineering got warped.

If we consider the classic meaning, where it means something is made unusually well (like a bridge meticulously constructed to be even better than with usual engineering), then the parallel meaning to software engineering could be that overengineering a piece of software to execute extraordinarily well (high speed, low memory usage, etc.). In other words, a highly optimized piece of software. Optimization makes the source code incredibly complicated, so overengineered software (classic meaning) is much more complicated than it probably needs to be. I can imagine this notion of needless complication of the source code over time coloured the meaning of overengineering, so it became more associated with too much complication rather than building something exceptionally well.",hqeeedd,t1_hqc4vim,1640778031.0,False
rq60d4,Don’t agree with #5: fixation on data structure can lead to lack of flexibility. Programmers should use data abstraction instead. Also not data but functional specification should drive program design,hq9omtz,t3_rq60d4,1640693048.0,False
rq60d4,"Data driven design tends to be cleaner, simpler and easier to reason about, because you can avoid complicated data conversion in your data abstractions. This also saves you from potential correctness and performance issues in those data abstractions.

Flexibility isn't really an issue, because the code is much more straightforward when it has a conservative amount of abstractions. Therefore it's usually not as bad as you might think to change the data structure to something else. On the other hand heavily abstracted code tends to be so complicated that it's easier to just put yet another abstraction on top of it all, compounding the existing issues.",hqbjej0,t1_hq9omtz,1640722954.0,False
rq60d4,"«Data driven design ...» — does data determine functionality of program or vice versa functionality determines the data necessary for its implementation? What does user need? Functionality or internal program’s data structures?

«Flexibility isn't really an issue, ... when it has a conservative amount of abstractions» — correlates with my statement about using data abstraction",hqbvqdu,t1_hqbjej0,1640728016.0,False
rq60d4,"In data driven design data structure (i.e. data layout in memory) determines how you process the data. This has not anything to do with user needs or application features per se - it has to do with how one goes about implementing these features.

No, it does not correlate - it contradicts. You claim in response to rule #5 that the programmer should use data abstractions instead of fixating on data structures. When you as a rule abstract the data and disregard its actual structure, then it will unavoidably lead to unnecessary data abstractions. When you have more data abstractions than necessary, you do not have a conservative amount of data abstractions.",hqbytmu,t1_hqbvqdu,1640729281.0,False
rq60d4,"«In data driven design data structure (i.e. data layout in memory) determines how you process the data»
— I didn’t ask what “data driven” means, I ask how DDD can prevail since programs are usually written to satisfy some user needs, not to serve any data structure itself — and primary need of user is functionality but not pure data.

«No, it does not correlate - it contradicts» — no, it doesn’t contradicts because you said it yourself that data abstraction solves the issue of potential loss of flexibility.

«When you as a rule abstract the data and disregard its actual structure, then it will unavoidably lead to unnecessary data abstractions ... more data abstractions than necessary» — you cannot have more data abstractions then necessary if you design consequentially — because abstraction means eliminating inessential details for given step of design, and new abstractions appear only when you begin concretizing existing ones and realize that they require some new entities — which automatically makes those new entities necessary 🤷‍♂️",hqpk98s,t1_hqbytmu,1640974946.0,False
rq60d4,"In DDD, user needs determine the data structures. The data structures determine the algorithms. Example: if the user needs fast undo/redo functionality, then a doubly linked list might an appropriate data structure, and therefore the code is tailored to linked list functionality.

It absolutely contradicts, and I thoroughly explained how.

Regarding flexiblity, I said that the percieved inflexibility of DDD is overestimated. The reason is DDD code is generally speaking relatively easy to change.

I agree with what you said about using data abstractions when the need arises in an organic manner. This isn't against DDD. In fact, this is the *only* way you should make data abstractions in DDD - not making data abstractions as a rule of thumb, whether really needed or not.",hqt4agn,t1_hqpk98s,1641047762.0,False
rq60d4,"I suspect that was written within the context of ""thinking about performance"". Don't choose a particular data structure just because you want to use a certain algorithm. In general I agree with you, but the data ultimately does have some structure on a disk/in memory which matters.",hq9rlsf,t1_hq9omtz,1640695091.0,False
rq60d4,I agree with you too) but according to rule #2 we must to measure existing data before optimizing it and according to rule #4 we should prefer the simplest alg. and the simplest data for the first impl.) and abstraction is the easiest way of such simplification to my opinion,hq9t80e,t1_hq9rlsf,1640696134.0,False
rq60d4,"Also ""good enough is often better than perfect""",hqawyhl,t3_rq60d4,1640714040.0,False
rq60d4,"Good enough is never better than perfect. I get these sorts of rules are supposed to be pithy, and I get what they're trying to hint at, but they're always phrased in such asinine ways that it's hard to take them seriously.",hqbzw98,t1_hqawyhl,1640729730.0,False
rq60d4,"Reminds me The zen of Python.

Beautiful is better than ugly.

Explicit is better than implicit.

Simple is better than complex.

Complex is better than complicated.

Flat is better than nested.

Sparse is better than dense.

Readability counts.

Special cases aren't special enough to break the rules.

Although practicality beats purity.

Errors should never pass silently.

Unless explicitly silenced.

In the face of ambiguity, refuse the temptation to guess.

There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.

Now is better than never.
Although never is often better than *right* now.

If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.

Namespaces are one honking great idea -- let's do more of those!",hqb5jzu,t3_rq60d4,1640717439.0,False
rq60d4,How exactly am I supposed to write smart objects using only stupid code?,hqb8b06,t3_rq60d4,1640718522.0,False
rq60d4,"When you have picked appropriate data structures, then you code usually does not need to be particularly sophisticated.",hqc2f90,t1_hqb8b06,1640730784.0,False
rq60d4,"rule 6 

unexecuted code should not cause you to write more executed code. 

(class hierarchy/data structure and types should be fitted to your code and not the other way around)",hqnotxz,t3_rq60d4,1640936528.0,False
rqysm5,Sexbot,hqdkhst,t3_rqysm5,1640755907.0,False
rqysm5,"Turing defined it in his 1950 paper, ""Computing Machinery and Intelligence."" Basically if AI can pass the Turing test, they are effectively as intelligent as anything else.",hqdbkt1,t3_rqysm5,1640751162.0,False
rqysm5,"There have been chat bots which can almost pass it for decades. More interestingly, there are many arenas of life in which people must stop for a moment to think ""is that a robot?"" without knowing for certain. Video game bots (including chess bots), forum (e.g. Reddit) bots, and chat (customer service) bots are all pretty obvious examples but there are less obvious ones too. How long can you drive behind a self driving car on a rainy day before you realize it's autonomous (driving is in many ways communication)? I have had one or two automated robo calls to my phone which took me an embarrassing amount of time to realize were adaptive recordings of some kind (like phone trees, which are also getting really good). There are many games in which it is not obvious that an opponent is a bot.

Basically, I think we are already at the point where the average person is occasionally going to be fooled by a bot impersonating a human. We have long been accustomed to occasionally losing to them at games, although even in victory they are rarely mistaken for human. This is in contrast with the fact that we don't have the kind of artificial general intelligence most people think of when they imagine something passing the Turing test. When it comes to our human ecosystem and the bots (which are not yet ""true"" AIs) which inhabit it, there's already a lot of them out there doing very complex and borderline things (like high frequency trading) which place them in a sort of mixed arena with humans. The expectations of the 1950s are only so useful at the end of the day.

I think it is probable that there is no ""magic line"" after which a thing is ""intelligent"" or sapient or AGI or whatever. We are unimpressed by our current level of accomplishment, but what if we were time travelers from a hundred years ago? Without the finer points of context to help with the distinction, there are a heck of a lot of bots or automatic systems which could easily be mistaken for thinking and willful things. I think that as more and more systems are put in place to make AIs seem intelligent, that line will get crossed without fanfare or anyone having a reason to ""declare"" it crossed. Some day we will all be having cogent, deep, complex, back-and-forth, intelligent conversations with our phones or desktops or cars, and people will still be wondering ""is AGI here yet? What about the Turing test?"". Perhaps instead of a fine line we can observe the stepping across of, we should think of it as a large and fuzzy zone, with us in the middle of it somewhere. People will always be able to point to a seemingly intelligent program and say ""aha, that's just mechanistic"" but at a certain point you are making very philosophical arguments about the meaning of emergence through mechanism by attempting to disprove the alleged intelligence of a program. At some point it will just be easier to assume a thing is sapient until proven otherwise, and I am okay with that future. I think it'll be pretty neat.",hqe2p0v,t1_hqdbkt1,1640768748.0,False
rqysm5,"You're missing the point of Turing's paper. If you can't tell the difference, it doesn't matter if it's ""true AI"" or not. That is the line. So far I still know when to hang up my phone from a robocall. So far I'm not ready start giving my toaster or candlesticks ""rights.""

The difference between writing and chess is the fact that you think in words, not chess moves. That's why it's the only important barrier. Turing thought this all out, trust me. He offers rebuttals at the end of his paper for some of your claims.",hqg6g6s,t1_hqe2p0v,1640807122.0,False
rqysm5,"I was supporting that point, not arguing with it. I think a lot of people are missing all the important points by focusing on this idea of a ""true"" AI when, as you say, that was not really the point of the Turing test. 

>So far I still know when to hang up my phone from a robocall.

Me too, most of the time, but it's getting to the point where I have
on several occasions been unsure if it would be okay to hang up or not
because the robocall was so convincing (on one occasion even replying to me like one of those ""speak to me"" phone trees). It has gotten me thinking about what a future world would be like in which you had to take for granted that sapient processes were all around you, in any given object that could be programmed. In such a world you might adopt a policy of politeness towards objects that would seem highly unusual (to put it bluntly) to someone from, say, Victorian Europe. I already say ""please"" to my Google Voice just because it's healthy to build those reflexes when using your communications muscles, so how much more interesting and nuanced in a few more years when the world is full of systems which are ""close enough to intelligent"" to make you think about your social reflexes when dealing with a machine?

>So far I'm not ready start giving my toaster or candlesticks ""rights.""

I'm not convinced it will ever ask for them, honestly, no matter how far these things go. You may not want to give your candlestick rights, but at what point do you begin to offer it politeness or consideration of some kind, if ever? Or at what point do you get mean to it? In a world full of somewhat sentient or sapient devices which don't want or need ""rights"", it will say a lot about a person how they treat those things. Not for the sake of the device, but for the person's social reflexes.

>The difference between writing and chess is the fact that you think in words, not chess moves. That's why it's the only important barrier. Turing thought this all out, trust me. He offers rebuttals at the end of his paper for some of your claims.

Unless one is just typing randomly then there is logic, movement and order to writing. This is why we can describe computer programs using words, and also chess games for that matter. The comparative cognitive load between traditionally ""cerebral"" activities is probably an area that is prone to a lot of preconceived prejudices, so I hesitate to compare them. You can take a chat bot so far in a conversation sometimes because of how effective projection and cold reading are. To have a deep, cogent, considered, insightful, top notch conversation is so difficult that a lot of humans go through life without ever being more than very bad at it (I'm not great at it myself). When you consider how easy it is to misunderstand, fail to practice active listening, insert a preconception or prejudice, or otherwise mess up a conversation's context wildly between two human beings (and how these things are the norm rather than the exception for all but the most basic conversations between two people much of the time) I think we should consider that in some years most smartphones will be better conversationalists than most people. Even then, we will still probably be arguing about the Turing Test (only then it will be about how ""real people aren't that good at conversation"", in the same way that people accuse video game bots at times). I think it will be just another wake-up call for people who thought that the species was ""defined"" by a specific quality (in this case intelligent communication). Although not all of Turing's rebuttals have aged equally well, I'm not arguing with the paper. I also agree that this ""magic line"" of intelligence is over-rated or does not exist. We will have programs that are ""sapient enough"" long before the argument is settled, philosophically, I think.",hqgje5s,t1_hqg6g6s,1640812110.0,False
rqysm5,"I didn't have to read it, but since I've graded standardized English exams by the thousands, I could tell you're probably human. I'd be willing to bet on it. Congratulations.

Wake me up from Delillo when AI starts betting money. I mean, ""AI"" has owned chess for over a decade and now go, and they still can't make a living like a McDonald's worker fresh out of high school can? Shit, the McDonald's near me is advertising $18/hr. That's like one cheeseburger every 20 seconds.",hqhv5l7,t1_hqgje5s,1640832116.0,False
rqysm5,"every time this question getsasked, someone close to the answer has to come on reddit to downvote the question. This delays the solution.",hqdlq5s,t3_rqysm5,1640756619.0,False
rqysm5,"I don't believe machines will ever truly pass a turing test perfectly until they have a perfect grasp on what it means to be a human. More specifically, what human they are pretending to be.",hqx1ib4,t3_rqysm5,1641115596.0,False
rqqd61,"The computer doesn't know about types. If you do floating point arithmetic, the computer assumes whatever you're operating on is a floating point number; same for integers. The compiler and/or runtime system generally has to keep track of the type of each variable and prevent the developer from doing anything nonsensical. If you're writing assembly, the assembler typically won't enforce any type checking and this responsibility then mostly falls on the programmer.",hqbzit3,t3_rqqd61,1640729572.0,False
rqqd61,"Fun fact: Some language (like C) have ways to reference the same memory location by different data types. You could store a 32 bit unsigned integer, and then read it back as a 32 bit floating point number. And yes, there are legitimate reasons you may want to do that. But it all boils down to the programmer **choosing** to refer to some variable as a specific type. (Or sometimes the language will automatically select the type based on what operation the programmer decided to apply to it.)",hqd21ff,t3_rqqd61,1640746582.0,False
rqqd61,"The data type is not stored with the variable, the data type is stored in a sort of table of contents. Most of the time, when a file is deleted, the file itself is still there, it's just that its index in the file system is removed.",hqx1p7u,t3_rqqd61,1641115759.0,False
rqeudi,Which language do you have experience with?,hqa84y4,t3_rqeudi,1640703903.0,False
rqeudi,Python and a bit of JavaScript,hqee586,t1_hqa84y4,1640777841.0,True
rqeudi,Personally I liked the book Introduction to the Theory of Computation by Michael Sipser as an introduction to the ideas behind computer science.,hqadybv,t3_rqeudi,1640706408.0,False
rqeudi,Thanks! I'll give it a look,hqee6dm,t1_hqadybv,1640777864.0,True
rpr7gj,"I would definitely recommend Algorithms in a Nutshell by George Heineman. The book gave me the most in-depth look into all algorithms and has a couple chapters focused solely on chess. 

If you want to check the book out before purchasing it you can do that here: https://archive.org/search.php?query=Algorithms%20in%20a%20nutshell",hq69ayn,t3_rpr7gj,1640628890.0,False
rpr7gj,"This isn’t a book, but I found the chessprogramming wiki to be very comprehensive!",hq6ns3f,t3_rpr7gj,1640634812.0,False
rpr7gj,https://www.chessprogramming.org/Main\_Page,hq7vvzr,t1_hq6ns3f,1640653591.0,False
rpr7gj,"The most important algorithm to know for these kinds of (two player turn based) games is the minimax algorithm: https://en.m.wikipedia.org/wiki/Minimax

There are lots of variants for optimizing performance but this is the starting point. Coming up with a good heuristic function to use is an important part, and can be approached lots of ways (ML, hand coding, etc)",hq7tayq,t3_rpr7gj,1640652427.0,False
rpr7gj,"**[Minimax](https://en.m.wikipedia.org/wiki/Minimax)** 
 
 >Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. When dealing with gains, it is referred to as ""maximin""—to maximize the minimum gain. Originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hq7tcod,t1_hq7tayq,1640652449.0,False
rpr7gj,"A good chess engine work using reinforcement learning algorithms. 

Depending on your knowledge you can read for example AlphaZero's paper  [here](https://www.science.org/doi/full/10.1126/science.aar6404) but its a hard read without knowing the prerequisites.",hq6hbfd,t3_rpr7gj,1640632144.0,False
rpr7gj,"Not a book as such, but Stockfish is an open source chess engine.  They have a page for people getting involved, including coding. https://stockfishchess.org/get-involved/

And they have a discussion forum. https://groups.google.com/g/fishcooking",hq761k5,t3_rpr7gj,1640642343.0,False
rpr7gj,"Stockfish wouldn't be a good example for a beginner as the engine employs machine learning and hardware acceleration if I recall correctly. Stockfish is an extremely advanced chess engine, hence why it has the highest *[ELO rating](https://en.wikipedia.org/wiki/Elo_rating_system)* amongst other engines. It would serve as a good example for a developer to work up to, but not start off with. You are essentially handing them the keys to a Ferrari when they don't even have their temps yet as well for throwing a box of parts at them shortly before telling them it's an airplane. OP does not want a box of parts, they want a blueprint to assimilate so that they can later on assemble their own airplane from their own box of parts. I cannot recommend this enough, do not refer beginners to major codebases. Doing so will cause them to spend more time trying to learn and navigate the codebase in lieu of the subject itself.",hq81tzc,t1_hq761k5,1640656262.0,False
rpr7gj,"Check this out: https://wiki.cs.pdx.edu//minichess/

This is from one of my profs at Portland State, and it's fantastic. It's a great intro into the algorithms you're interested in.",hq6yzwp,t3_rpr7gj,1640639423.0,False
rpr7gj,Toledo NanoChess by Oscar Toledo Gutierrez. I guess it's not entirely modern as Stockfish or Alpha.,hq78mml,t3_rpr7gj,1640643423.0,False
rpr7gj,This is handy  [https://www.chessprogramming.org/Main\_Page](https://www.chessprogramming.org/Main_Page),hq7vswd,t3_rpr7gj,1640653551.0,False
rpr7gj,"I know you asked for books (for which you should look at the other comments) but I would read on minimax with α-β pruning and Monte-Carlo Tree Search. The former powers DeepBlue, the latter powers AlphaZero (I think?)",hq8jbhr,t3_rpr7gj,1640664298.0,False
rpr7gj,"Norvig's very influential AI book, while not specifically about chess, uses plenty of example from chess. Worth studying in depth.",hq9a6mg,t3_rpr7gj,1640681470.0,False
rpr7gj,Me too! f,hq60rg0,t3_rpr7gj,1640625439.0,False
rp30oe,"If you're an academic then writing papers, attending conferences and reviewing papers will likely keep you up to date.

Apart from that, keeping up with the latest trends is not that important. The ""hot new things"" often grow out or favor in a few years. Sure, if you can spin your research as being tangentially related to this they may sound better, but apart from that new developments in some area often have no relevance on your particular area.",hq24exx,t3_rp30oe,1640547437.0,False
rp30oe,"Prediction is hard, especially if it's about the future.

X, Y, and Z will usually be the tip of the iceberg. Every day you will read/hear about dozens of things that are the latest and the greatest. It's hard to upfront know what is and what isn't going to pan out. One approach is to just pick one and work at it if it appeals to you.",hq24nax,t3_rp30oe,1640547539.0,False
rp30oe,[Hacker News](https://news.ycombinator.com/),hq2708e,t3_rp30oe,1640548565.0,False
rp30oe,"I think it’s easy to be dismissive of latest tech and chasing the new hotness. However, developing a sense of what is signal in the noise, what is worth using some time to examine and potentially adopt in your toolkit as it evolves is one of the greatest skills you can develop as an engineer. This is a industry always on the move and you can move with it or stand still. You know what happens to those engineers who refuse to evolve. 

To answer your question more specifically, company engineering blogs have become great resources. Find companies you admire and see what they are doing to solve their problems.",hq3k46b,t3_rp30oe,1640572088.0,False
rp30oe,"I have a few YouTube channels I go to to see what’s new in tech. Some cover big things like new releases of stuff like laptops and smart phones, some cover pen testing and new exploits, and others are just people far smarter than I’ll ever be giving me their opinion on different things. I like YT a lot better because it gives more information, so if I am interested in something I can refine my search better when I go to look it up. Ticktock and YT clips never give me enough info and I always have to google it after each thing I watch just to find out it’s not that interesting. As far as learning it. I usually like to take in as much as I can just so I know the name and a very broad understanding of what it is. Not enough to actually have a conversation about but enough that if I hear it again I can be like, yea that’s a new browser, or that’s a new block chain scam. However, if I find it interesting or if it frequently comes up in different discussions I will dig a bit deeper and learn more about it.",hq3p1pd,t3_rp30oe,1640574469.0,False
rp30oe,Quantamagazine. Nature. Try two minute papers on youtube for ML related things.,hq4os7t,t3_rp30oe,1640597530.0,False
rp30oe,"Theres several layers to it. You got the high level hypey stuff like crypto, nft, web3 stuff you can reach about on tech crunch or hacker news or wired. You then have the different language level libraries and design patterns and frameworks you can catch up on by googling ""best software engineering"" blogs. And then you have system design on big tech company websites.",hq2ew4f,t3_rp30oe,1640552740.0,False
rp30oe,Google News Feed app It'll figure out what tech you're interested in. Click on the appropriate links and over time you'll get a personalized tech news feed,hq4notr,t3_rp30oe,1640596607.0,False
rp30oe,I'm a software engineer and I honestly find out about the latest stuff when our apprentices say they want to use it.,hq52r9u,t3_rp30oe,1640608774.0,False
rp30oe,"It is NOT fast moving.

It takes years for a new language to mature to usability. The time from when I first heard of Java to usable Java was about five years. If you hopped on Swift the year Apple announced it you rewrote your app at least four or five times if it still works today. 

I would also argue we do not make much real progress, we just shift things around like hem lengths on skirts. We cast off perfectly good stuff for no good reason in favor of new half baked things that are not yet as good all the time.",hq5gz6w,t3_rp30oe,1640616873.0,False
rp30oe,"Forums and blogs and stuff, YouTube channels as well probably
And reading papers, but you find the papers on forums

As well as annual conferences if you have a lot of spare time 

Honestly if you’re involved in your specific field (ie frontend, embedded etc) and you’re in all the subreddits and what not, you should be able to stay more or less up to date",hq5vxal,t3_rp30oe,1640623459.0,False
rp1tzo,[cpprefererence.com](https://en.cppreference.com/w/cpp/algorithm/rotate) Usualy have a possible implementation (as in this case) and is genarally bettwr than cplusplus.com,hq1qubv,t3_rp1tzo,1640541704.0,False
rp1tzo,I find this implementation similarly perplexing but is at least commented,hq1t9kk,t1_hq1qubv,1640542760.0,True
rp1tzo,"First of all the element at middle has to go to first. Where does the element that was at first go? That's unclear, but we'll get there. Let's store it at middle for now. Then the element after middle has to go to the element after first. So let's swap those in the same way. We continue doing this swapping the elements first+k and middle+k. The correctness we can prove during this is that the element that is swapped into the first+k is now at the correct location.

This stops either when first+k becomes middle, or when middle+k becomes end.

\- In the first case, the elements that were swapped into the locations between middle and middle+k are not necessarily in the right location. They should be the last k elements, and all the elements after middle+k were supposed to shift left k times. So we're basically left with rotating the portion between middle and end, with middle+k as the new middle.

\- You can analyze the second case similarly. It's a bit messy to describe this case but it's easy to see what is needed to be done if you draw the list, note what got swapped, and how it needs to be.

You can verify that both cases are just instances of rotate, and that the code given has the correct first, middle and last in order to do the required rotate.",hq34xd8,t3_rp1tzo,1640564761.0,False
rp1tzo,"I have an intuition why it is correct, but this intuition works by understanding the algorithm as an recursive algorithm.

Here's the set up: we want to rotate the array leftwards, such that the element that used to be at the `middle` position is now at the start (i.e. position `first`)

Note that if `middle=first`, we are done.

Otherwise, we will now swap the first k elements with the k elements starting at `middle`. k is at least one, and is determined by `min(middle-first, last-middle)`. For example, let's say we have an array with numbers 0..9, and we want to pull 3 to the front:`0 1 2 3 4 5 6 7 8 9` becomes `3 4 5 0 1 2 6 7 8 9` after this initial swap step. `k=3` as `3=min(3,7) = min(3-0,10-3) = min(middle-first, last-middle)`. `first` points at position `3` (i.e. `middle`), `next` points at `6`.

Another example: Same array, but `middle=7`:

`0 1 2 3 4 5 6 7 8 9` becomes `7 8 9 3 4 5 6 0 1 2`, as `k=3` again, but this time it's because `last-middle=3`, which is the limiting term. `first` points at `3`, `next` at 10 (i.e. `last`)

Essentially, k is chosen such that at least one of the `if`s will become true after k iterations of the loop.

So, we have now done this step. Note that the first `k` elements are sorted, and `k>1`. Now, the magic is that the remaining array can be brought into correct shape by recursion.

In the first case, we have `0 1 2 6 7 8 9` remaining, which can be corrected by rotating such that the 6 is pulled to the front. In the other case, we must rotate `3 4 5 6 0 1 2` such that `0` is pulled to the front.

&#x200B;

The first case corresponds to `if (first==middle) middle=next;`, the last case to `if (next==last) next=middle;`. In the first case, our new `middle` will be `next` and we do recursion. In the latter case, `next` is reset to `middle`. Notice that in either case, the variables are now arranged as if we had done a recursive call, with `next==middle` holding.

As for run-time analysis, `first <= middle <= next <= last` always holds, and `first` continuously grows while `last` remains constant, thus we must eventually exit the loop, after at most `last-first` steps.",hq2vnqb,t3_rp1tzo,1640560492.0,False
rp1tzo,"bro this is great, really nice explanation thank you. this was an interesting read",hq3v5pt,t1_hq2vnqb,1640577506.0,False
rp1tzo,"1. rotate(a, a + k, a + n) = reverse(a, a + k) + reverse(a + k, a + n) + reverse(a, a + n)
2. Let's fix some index i0 and for i0 let's make a 1-shift for indexes i0, (i0 + k) % n, (i0 + 2k) % n, ..., i0. It's easy to do with O(1) memory. Let's do this action for i0 = 0..gcd(n, k)-1.",hq1wad1,t3_rp1tzo,1640544031.0,False
rp1tzo,"Sorry, didn't read the question",hq1whoy,t1_hq1wad1,1640544117.0,False
rop984,"If we're talking practically speaking, auth0 or some other provider is the most secure solution.

I'm not much help on the theoretical side of your question, though I haven't been at a company which separates application data from user authenticating data. Doing so seems to run along the lines of security through obfuscation rather than a sound security principle.

I Am Not A Security Professional, Just A Security Interested Developer.",hpzx3md,t3_rop984,1640497411.0,False
rop984,"> If we're talking practically speaking, auth0 or some other provider is the most secure solution.

auth0 has had critical vulnerabilities in the past.

The downside to using third party solutions is that they are often targeted by more actors, since any potential exploit can give you access to many more targets (e.g. the recent log4j CVE).

Of course, getting security right is very difficult, so you should probably not roll your own, but you also shouldn't trust others blindly.",hq0nkpg,t1_hpzx3md,1640520276.0,False
rop984,"Yea, that was my gut feeling too. I don't really know how databases get hacked (technique-wise), and my layman guess was that even if one db can get hacked, hacking the second doesn't become any easier.. but I should probably research more about vulnerabilities first, before building my mental models. Thank you!",hq04pom,t1_hpzx3md,1640503084.0,True
rop984,"DB's get hacked in a myriad of ways - from SQL injection to compromised credentials to unintentionally exposed databases.

I'd put something far simpler and more common like SQL injection or ensuring credentials are secure above a circuitous theoretical attack requiring separating user credentials from application data.

Your goal is _always_ to avoid being the lowest common denominator, because no system is invulnerable. Hackers are (generally) lazy and looking for a quick pay day. Don't be the easy target, avoid most (though not all) issues.",hq3j9g0,t1_hq04pom,1640571674.0,False
rop984,"[https://attack.mitre.org/](https://attack.mitre.org/)  


this may be a useful resource for you. I don't think it's exhaustive, but ""MITRE ATT&CK® is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations."" (first sentence on the landing page of the website) Going over the titles and skimming through some of the technique groups/individual techniques could help you build an idea of what you need to protect against",hq7b0no,t1_hq04pom,1640644442.0,False
rop984,"Infosec architect here. Rule 1 of application security, never ever roll your own security unless you absolutely have to. Rule 2 if you think you absolutely have to you are probably wrong. 


Some kind of OIDC system is probably best. but you're more or less right, it doesn't really matter if they're separate so long as you do good salted hashes",hq1l3pi,t3_rop984,1640539146.0,False
rop984,"Probably not, the real security advantages could come from how you treat the databases. Like can you not grant some applications access , or somehow make the databases more separated where if you have access to one you don’t to the other.

If practice I wouldn’t do this without other reasons why I want them separated. For example microservices and these are different teams.",hq056dm,t3_rop984,1640503465.0,False
rop984,"My personal thing is remember Murphy’s law when it comes to security. If anything can go wrong, it’ll go wrong. Apart from security, performance could be affected if both of them are the same. You could use a hybrid design where you store user data in some db that is fast at handling aggregate operations (if you need that) and a db which gives kinda O(1) lookup for username->hashed password. If you have a billion users, storing everything in one db would have performance effects. What if the user doesn’t be access to all user data, just needs to login and do something? There’s also the layer of scalability where you sometimes don’t need to scale the user data db while you need to scale user auth db.",hpzxzed,t3_rop984,1640498012.0,False
rop984,"Ok, I see. To clarify more, my (personal) project is basically recreating Google Sheets, and I can't say for sure, but my intuition is that their implementation is to have a unique db (or group of db's? distributed db's?) for each of read-only/read&comment/read&comment&write, so they can optimize each db for the use-case. 

I was curious if there is maybe a security reason (aside from performance) for separating a read-only db from a read&write db, but now that you mention it, I should probably care a lot more about performance.

I'm gonna search up resources on performance and databases in general now. So much to learn... but thank you!",hq040kg,t1_hpzxzed,1640502509.0,True
rop984,"Keep in mind the context you are looking at. Google has a single set of user auth/basic info and lots and lots of different applications. So it makes sense to separate the user's per application data. Makes it easy to add/remove applications over time. 

Security-wise, it likely doesn't matter (unless you handle the auth database very differently the other), but there are a good engineering reason to separate them.",hq0zxlz,t1_hq040kg,1640528863.0,False
rolb3t,"Trial division is O(sqrt(n)) exactly because smallest prime factor is <=sqrt(n). If you want to generate all primes up to n by trial division, it would be O(n\*sqrt(n)). There is of course Sieve method that can go all the way down to O(n).

Yours - idk. Trying to save all prime factors is at least O(nlogn) and that's already worse than Sieve.",hq03998,t3_rolb3t,1640501905.0,False
rolb3t,"I see, thank you!",hq0p11s,t1_hq03998,1640521491.0,True
rolb3t,"The r/math post has been removed, so I will copy and paste it here:

After watching a YouTube video on prime factorisations, I became interested in the patterns that appear when you factorise the natural numbers in order (from n = 2 onwards).

I think it's really interesting how these patterns appear. For example, when 2 is the base (which happens for every other n; as every other n is even, so has 2 as a factor), the powers follow this pattern:

1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5...

Edit: the powers really follow this pattern: 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, 4... (I'd forgotten the case of 2\^0) and the pattern above is just for powers > 0, but the following logic still holds.

This pattern appears to be such that you have a '1' at every (1-based) index other than a multiple of 2 (so 1, 3, 5, 7, 9... and so on). But then if you remove all the '1's from this list, you end up with a new list: 2, 3, 2, 4, 2, 3, 2, 5... Interestingly, this new list follows the same pattern, just now with '2's at every index other than a multiple of 2. This process carries on and on as you keep removing numbers.

What's even more interesting is that you see a similar pattern for higher bases. When 3 is the base (which happens every 3 n, again for the same reason as before), the powers follow this pattern:

1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 4...

This pattern now appears to be such that you have a '1' at every index other than a multiple of 3 (so 1, 2, 4, 5, 7, 8... and so on). But again, if you remove all the '1's from this list, you end up with a new list: 2, 2, 3, 2, 2, 3, 2, 2, 4... And again, this new list follows the same pattern, just now with '2's at every index other than a multiple of 3, and so on...

protocat-112ocat-112uestion is - what is going on here? Has anyone got an intuitive explanation for why these patterns appear?

To try and understand this a bit more (and since I am more computer scientist than mathematician) I looked up and found an algorithm for generating prime numbers. I then implemented it in C# (which I have called 'current algorithm'). I've added some comments with what I think is going on, but generally I understand it. The algorithm iteratively factors out every prime from the target number. Put this in a simple loop counting up from 2, and you can easily generate a table (or dictionary in this case) of all prime factors for n >= 2. What is the time complexity of this algorithm? I had a go at trying to figure it out and arrived at O(n\^3), but the while loop confused me somewhat...

What I wanted to do however was generate the prime factors in a different way. If I could figure out the pattern of the bases and powers, I could instead 'populate' each number with its factors looping though the factors (a base and a power) instead of the target number, starting with 2\^1, then 2\^2, 2\^3... then 3\^1, 3\^2, 3\^3... and so on. I came up with this algorithm (which I have called 'new algorithm'), which as pseudo code is as follows:

    dict = an empty dictionary with keys from 2 to SOME_LARGE_NUMBER
    FOR (base = 2 to base = SOME_LARGE_NUMBER):
      IF (dict[base] already has entries):
        // base is not prime
        SKIP to next base
      FOR (power = 1 to log_base(SOME_LARGE_NUMBER)):
        FOR (extender = 0 to base - 2):
          FOR (n = basepower + (extender * basepower) to SOME_LARGE_NUMBER),             
      INCREMENTING n by basepower + 1:
            dict[base].Add(base, power)

I managed to figure this out basically through trial and error. The core part of this algorithm is the final for loop. For base = 2, power = 1, extender is only 0 and this gives n = 2, 6, 10, 14, 18, 22.... For base = 3, power = 2, extender will be 0 then 1, giving n\_0 = 9, 36, 63... and n\_1 = 18, 45, 72 (which combined give us all the numbers which have 3\^2 as a factor). Again, my question here is why does this work? I am especially confused about my 'extender' code, which is, as I see it, a fudge to make it work. Also, what is the time complexity of my algorithm? I believe this runs in O(n\^3\*log(n)), but I may be wrong... I've only ever looked at the concept of time complexity for simple examples...

It's worth noting that the runtime of my algorithm seems to be much slower than the 'current' algorithm for finding prime factors when SOME\_LARGE\_NUMBER is indeed large (like 100,000). Also, I've not figured out how to optimise it for only looking for a single n, which I'm not even sure is possible, as the n is in the inner-most nested for loop, which is dependent on the outer loops.

However, it is considerably faster for a smaller value of SOME\_LARGE\_NUMBER, like 1000.

Both of these values may of course be affected by my specific implementation of the algorithms.

I'd be grateful if someone could explain or clarify any of the questions I've raised, or even just add any further info, discussion or relevant reading about this, as I'd love to understand more about what I am seeing.",hq0zltu,t3_rolb3t,1640528679.0,True
rolb3t,"I responded to your /r/math post but I don’t think you understood my comment. Try seeing what happens for n=10 because its the one where people are most familiar with the divisibility rule for n^k.

Start from 0 and increment by 1 in steps. Every 10 steps it’ll look like xxxx0, which is a multiple of 10. Every 100 steps it’ll look like xxx00, which is a multiple of 10^2. Every 1000 steps it’ll look like xx000 which is a multiple of 10^3, and so on.

Now, transfer over this logic to n-nary numbers, since in base n, a number of the form xxx0^k has a n-adic valuation of k.

All of this exactly explains all your observations. Your operation of “removing all the 1’s” and seeing the same pattern with every number incremented by 1 is just you recursing on a new instance of your observation where everything is scaled by a factor n, i.e. increment in steps of n instead and chop off the least significant digit of your n-nary number

Essentially, you are just witnessing “fizzbuzz” with the set {1, n, n^2 ,…} with words {0, 1, 2, …} and operation of “max” instead of the usual fizzbuzz on the set {3, 5} with the words {“fizz”, “buzz”} and operation “concatenation”.",hq1lqdj,t1_hq0zltu,1640539422.0,False
rnyhfs,"A processor can be thought of as a state machine. It’s registers constitute its state. It’s ALU and various other combinatory logic blocks make up its transition table between states.

Each clock cycle the values in the registers flow through the combinatory logic where operations are performed on them and they are transformed, ultimately ending up back at the registers. At the end of the clock cycle they are latched into the registers and the whole thing happens again the next clock cycle.

So if some signals in some section of the combinatory logic have not finished propagating through the circuit by the time data is latched into the various registers, then perhaps incorrect data will be latched into the registers and the processor will malfunction and behave in unexpected ways.",hpvkmz2,t3_rnyhfs,1640399692.0,False
rnyhfs,"I think this is the most sound answer. In the most basal sense, if data is not operated on in the order it's supposed to, or certain operations are ""missed"", you can end up with errors of varying severities in whatever you're doing.",hpwi7lo,t1_hpvkmz2,1640424799.0,False
rnyhfs,"In most cases, the output of the circuit will only be read as 0 or 1 by driver/software, and whatever that output is at the expected clock cycle will be treated as hardware response. The driver has to handle that hardware response. 

* Say the component is a network card, the driver needs to handle lack of response from the network. 
* Say the component is a HD, the driver will detect that your HD cannot provide an appropriate USB/SATA/IDE response, thus is not working.

Most devices don't have its own clock chip, so won't be operating at unexpected frequency.",hpvldtj,t3_rnyhfs,1640400168.0,False
rnyhfs,"Tons will go wrong, such as flip flop metastability. It is something that should not happen ever, this is actually so important that design tools check for that without ever asking. To get rid of that constraint, people either use several clock ""domains"" which are related to one another for sync, or sometimes asynchronous logic circuits.",hpwdln1,t3_rnyhfs,1640420422.0,False
rnyhfs,"Component? What component? Component of what?  
Job? What kind of job? Where it performs the task? What for?

A more specific scenario would be helpful for answering the question",hpvakjx,t3_rnyhfs,1640393383.0,False
rnyhfs,Its obvious to me he is asking about digital design inside a processor.,hpyr4h8,t1_hpvakjx,1640474112.0,False
rnyhfs,"To put it simply. It’s based on whether or not the component was designed to function in between clock-ticks.

An example of this would be an I/O function from your H/SDD to your processor/RAM. The transferring takes multiple cycles, and as such the CPU runs into an Interrupt Request (IRQ).

If the component was designed to work based on the tick of the clock. Generally the program will loop infinitely on that instruction, until it’s able to execute that command, or you/OS kill the program for hanging (as that’s what it would appear to you), if it’s a Kernel/OS memory space program, you’re getting a segment fault.",hpy5w6h,t3_rnyhfs,1640463340.0,False
rnyhfs,"Wow, engineers now a days just aren’t familiar with computer science. It’s absolutely ridiculous!!",hq0mo34,t3_rnyhfs,1640519481.0,False
rnyhfs,"In which context is this? What kind of component, software?

If you are talking about pre-emptive multi-tasking OS, then the running task is switched as the time slice is used up. State is saved until there is another slice of execution time for it.",hpwnhpq,t3_rnyhfs,1640429987.0,False
rnyhfs,Front end? Need to be a lot more specific,hpvl337,t3_rnyhfs,1640399979.0,False
rnyhfs,Real-time operating systems (RTOS') might be something to look at - they're specifically designed to do their job precisely on time,hpvne5n,t3_rnyhfs,1640401443.0,False
rnt040,You have to be way more specific than that. Cyber security of Operating Systems? Programming/development? Patching? Quality assurance? There's so many different aspects of operating systems that all have different research and careers.,hpu974r,t3_rnt040,1640374122.0,False
rnt040,"I mainly got interested when I learned how OS does memory management, techniques like paging, the virtual memory concept,maintaining the page table, the entire process of generating the logical address then converting finally into physical address to look for the appropriate frame in the main memory to fetch the desired data. 
Is there any scope of doing research in this memory management? I searched about it but couldn't find much info on it's research. I may not have searched in the appropriate place. So came here for some suggestions.",hpw7t4y,t1_hpu974r,1640415367.0,True
rnt040,"Sounds like you want/are looking for computational resource management with a mix of OS engineering. In terms of research, I'd recommend using sites like Jstor where you can not only look at research, but read/analyze empirical studies, dissertations, and peer reviewed articles on such topics. Also a great way to potentially make connections in the comp science realm. With that said, with the way quantum is approaching, I would heavily suggest also learning cyber security and defense of OS systems and OS architecture. Virtualization is a very popular topic right now as well, you'll be able to find an abundance of info on that.",hpw8h24,t1_hpw7t4y,1640415920.0,False
rnt040,You can check out papers from SOSP to get an idea of research going around OS!,hpvanln,t3_rnt040,1640393434.0,False
rnt040,"Here’s a great thread I found a while back, https://www.reddit.com/r/learnprogramming/comments/5v1c16/why_does_the_cover_of_the_operating_system/?utm_source=share&utm_medium=ios_app&utm_name=iossmf

It’s great to see other people interested in OS Design, it’s not very popular compared to other fields.",hpudp1z,t3_rnt040,1640376257.0,False
rnt040,"I don't know about research exactly, but the field is definitely still looking for changes. RTOS specifically is a very important topic and constantly improving. New OS designs like Zephyr trying to make certain protocol stacks like Bluetooth more sane. Or some out there stuff like Redox OS which ignores POSIX design and just goes for adding interesting stuff like ""everything is a URL"". Which sort of also borrows from Plan9's 9P which everyone interested in OS design should check out.",hpvq3j3,t3_rnt040,1640403139.0,False
rnt040,You’re in luck. OS is a whole field with research constantly going on in its many sub fields. It’s definitely one of the most interesting CS topics.,hpvpz7k,t3_rnt040,1640403064.0,False
rnt040,"I'm mainly interested in OS memory management topic. 
The various techniques that have been developed to handle critical memory segments. Techniques like paging, virtual memory, segmentation, relocation etc. 
I wish to study this area in more details than bachelor level, and contribute to it in future",hpw8090,t1_hpvpz7k,1640415534.0,True
rnt040,"I remember when I took the first real OS class I loved the memory and storage management stuff too. Just to let you know, there’s lots of other places where you can do the same stuff and embed that work. I’d check out some open source emulators or even just regular graphics programming API’s like OpenGL or Vulkan. In Vulkan there’s no default memory allocator so you actually have the option to write an implementation of malloc for a real application.",hpx4tmf,t1_hpw8090,1640443610.0,False
rnt040,"Operating Systems Design and Implementation https://www.amazon.com/dp/0131429388/ref=cm_sw_r_apan_glt_fabc_4TZMCZ9JD572DPY9C296

The bible. It's a great book, and very easy to get in to.",hpvv5c3,t3_rnt040,1640406334.0,False
rnt040,"Beep. Boop. I'm a robot.
Here's a copy of 

###[The Bible](https://snewd.com/ebooks/the-king-james-bible/)

Was I a good bot? | [info](https://www.reddit.com/user/Reddit-Book-Bot/) | [More Books](https://old.reddit.com/user/Reddit-Book-Bot/comments/i15x1d/full_list_of_books_and_commands/)",hpvv5yj,t1_hpvv5c3,1640406346.0,False
rnt040,No,hpwhc8j,t1_hpvv5yj,1640423946.0,False
rnt040,"Check out Genode. It's what my OS Prof suggested.

Also seL4, mathematically proven secure OS, if I understood correctly.",hpx2wlj,t3_rnt040,1640442404.0,False
rnt040,Yes.,hpv86rz,t3_rnt040,1640391962.0,False
rnt040,"I had a course on Operating Systems recently. It was pretty intense, the professor discussed many research developments in areas of Multi core scaled OS, Virtualization, Scheduling Algorithms (mainly, CFS), File systems, Virtual memory.",hpvw3y5,t3_rnt040,1640406957.0,False
rnt040,Is the course available online?,hpw874m,t1_hpvw3y5,1640415694.0,True
rnt040,"Sorry, it was an in-person class.",hpx5wn9,t1_hpw874m,1640444277.0,False
rnt040,Consider whonix,hpx9im1,t3_rnt040,1640446404.0,False
rnvod9,[deleted],hpusafy,t3_rnvod9,1640383439.0,False
rnvod9,"I see. 
I have one more similar question to ask but I need to send an image so would it be a problem if I dmed you?",hputyrc,t1_hpusafy,1640384281.0,True
rnvod9,Quite the opposite actually.,hpxhdt2,t3_rnvod9,1640450625.0,False
rnvod9,Nope. The M1 chip is.,hpxfubp,t3_rnvod9,1640449822.0,False
rnuf9s,"1 - Practical Discrete Mathematics: Discover math principles that fuel algorithms for computer science and machine learning with Python
- Author: Ryan T. White, Archana Tikayat Ray

2 - Fundamentals of discrete math for computer science: a problem-solving primer
- Author: Jenkyns, Tom A., Stephenson, Benjamin David

3 - Good Math: A Geek's Guide to the Beauty of Numbers, Logic, and Computation
- Author: Mark C. Chu-Carroll

4 - Sets, Logic and Maths for Computing
- Author: David Makinson

5 - Discrete mathematics for computer science: (a bit of) the math that computer scientists need to know
- Author: Liben-Nowell

6 - Math Prerequisites for Quantum Computing
- Author: R. Kumar",hpuy59w,t3_rnuf9s,1640386414.0,False
rnuf9s,I love [this one](https://www.amazon.com/-/es/K-Dewdney/dp/0805071660). The book cover a lot of important topics in computer science and does it in such a way that is in the middle of divulgation and technical reading. Have fun!,hpurqpz,t3_rnuf9s,1640383166.0,False
rnuf9s,The problem with a Turing Omnibus is you never tell if it’ll ever stop…. /s,hpv3itj,t1_hpurqpz,1640389303.0,False
rnuf9s,"If I had to recommend 10 then the list would be (in no particular order):

* **The C Programming Language** (K&R) by _Kernighan, Ritchie_
* **Clean Code** by _Robert C. Martin_
* **Concrete Mathematics** by _Graham, Knuth, Patashnik_
* **The Art of Computer Programming** by _Donald Knuth_
* **Introduction to Algorithms** (CLRS) by _Cormen, Leiserson, Rivest, Stein_
* **Introduction to the Theory of Computation** by _Sipser_
* [**Structure and Interpretation of Computer Programs**](https://mitpress.mit.edu/sites/default/files/sicp/index.html) (SICP)
* **Computer Networking: A Top-Down Approach** by _Kurose, Ross_
* **Code: The Hidden Language of Computer Hardware and Software** by _Charles Petzold_
* **Software Engineering** by _Sommerville_",hpumzcj,t3_rnuf9s,1640380787.0,False
rnuf9s,Humble Pi by Matt Parker is a great bedtime read. Can't guarantee that you would become a subject-matter expert after reading it but can surely guarantee that it would spark an interest in you for learning more. Enjoy reading!,hpwn2gp,t3_rnuf9s,1640429568.0,False
rnuf9s,"I think a lot of good ones were already given, but I'm missing **Elements of Computing Systems**

You get to build a computer and programming language from scratch using only NAND gates. The book provides various simulators and instructions to accomplish this. Gives so much insight into how stuff works and is a ton of fun.",hq0btb8,t3_rnuf9s,1640509450.0,False
rnuf9s,"1. Probability and Computing - Michael Mitzenmacher
This book discusses a normal Discrete Math class on discrete probability for the first two chapters and then discusses more advanced results later on. Besides the theory, some applications such as algorithms are also featured to aid in learning. 

2. The Cauchy-Schwarz Masterclass - J. Michael Steele
A must-read if you're into Theoretical Computer Science. I enjoy the ""conversational"" writing style of this book which I found to not be boring and dry. It discusses some interesting results on mathematical inequalities (which I find to be more difficult than equalities, personally).",hq1qzb9,t3_rnuf9s,1640541765.0,False
rnmo0w,"The role of assembly language is simply that it’s easier to read opcodes like ‘mov eax, 1234h’ than a sequence of hex bytes or an even longer sequence of ones and zeros",hpt4fc3,t3_rnmo0w,1640354577.0,False
rnmo0w,"And to add onto what u/jddddddddddd has said, this readability allows programmers to understand compiler optimizations. That is, being able to read assembly code allows programmers to see how the compiler has optimized the high level code written by high level language.

Not only that, it makes it easy for the programmer to make his/her own optimizations by writing assembly code.

You can think of assembly language as an abstraction to machine language because it's very very difficult and completely ineffective for programmers to understand compiler optimizations/read/write optimizations in machine code. Hence, we have assembly language to give an easier time for programmers.",hpt7jib,t3_rnmo0w,1640356271.0,False
rnmo0w,"It's perhaps better to explain through what machine code is.

Assuming an instruction on MIPS-architecture, that adds two values placed in registers 1 and 2 and places the result in register 6:  000000 00001 00010 00110 00000 100000

This is not very legible for humans so instead human would write assembly like: add $2 $1 $6

Here opcode for add is more legible and operands are clearly readable. Compilers could produce machine code directly, but the assembler to make machine code is usually separate program in the compiler toolchain and higher-level compiler feeds intermediate code (assembly) to the assembler. This means that higher-level code can be more independent of the actual target architecture.

Assemblers like GNU assembler can target many different architectures as well.",hptr42l,t3_rnmo0w,1640365722.0,False
rnmo0w,"I'd like to add that assembly language commands are 1 to 1 with machine code. That is, for every command in assembly language, the machine is executing one instruction. This is much different than high-level languages which require many operations per line (printing to the console in python, for instance).",hptsdef,t3_rnmo0w,1640366304.0,False
rnmo0w," At first computers were programmed by putting numbers on punched cards. Folks like John von Neumann programmed this way. It was made easier by the design of the instructions. This carried over to way later, even in microprocessors like the 8086 where it's convenient that the instructions can be split up into groups using octal instruction numbers. So patterns in the numbers themselves stick out like sore thumbs and it becomes easier to think directly about programs using the machine code.

A great abstraction came along with the advent of what are essentially assemblers and linkers. Then you could get away with thinking about things at a higher level and the tools would translate to the appropriate numbers for the processor. At this point CS was really off to the races.",hputwt8,t3_rnmo0w,1640384254.0,False
rnmo0w,I think reading an architecture book would help. For me personally trying to understand opcodes and how the memory worked without having the underlying architecture knowledge was just too challenging.,hptwg9n,t3_rnmo0w,1640368179.0,False
rnmo0w,"Having a solid understanding about assembly can help you with things like debugging and reverse engineering, or using disassemblers. Been many years but I've written some patches/cracks for old apps before.    
Idk about the communities anymore but I favored MASM32 last times I was writing code in it.",hpumih0,t3_rnmo0w,1640380554.0,False
rnmo0w,They are essentially one and the same. Assembly code is the human readable form of machine language.,hpvgkty,t3_rnmo0w,1640397114.0,False
rnmo0w,I had to use assembly in school to understand how the ram works and learn how things like stack and pointers are used,hpvj0hs,t3_rnmo0w,1640398662.0,False
rnmo0w,Take for example the assembly language Java. It reads lines of code and spews out 0s and 1s. Quite fascinating!!,hq0mtwc,t3_rnmo0w,1640519623.0,False
rne5t2,"There is quite a bit of misinformation about asymptotic notation in this thread demn.

 We say f(n)=O(g(n)) if there exist c, n_0 constant such that:

f(n) <= c * g(n) **for all n>n_0**.

Due to the algorithm wont stop for n > 100 we conclude that no such n_0 exist for every g. 

Its a really badly thought out question and it push misunderstanding of the math behind the bigO notation, as the runtime of your algorithm is not O(g(n)) for every g.",hptkf5o,t3_rne5t2,1640362628.0,False
rne5t2,"Yeah this is a poorly designed question imo. If n > 100, this will indeed run forever.",hprvlqt,t3_rne5t2,1640321781.0,False
rne5t2,Are you sure? Isn’t an int 32 bits? I.e. it will wrap around at 2^31 to -2^31,hps3iia,t1_hprvlqt,1640326547.0,False
rne5t2,"Problems of asymptotics are not constrained by the representation of numbers in a given language. If that were the case, almost any runtime problem would be O(1) since there would be a constant upper bound to what integer the function could accept.",hps7vho,t1_hps3iia,1640329605.0,False
rne5t2,"Huh, so I guess P = NP after all.",hpsh7x6,t1_hps7vho,1640337050.0,False
rne5t2,Any NP problem is in P if you put a hard upper bound on the size of the problem lol,hptvh11,t1_hpsh7x6,1640367735.0,False
rne5t2,Yeah that was the joke,hptvqfi,t1_hptvh11,1640367853.0,False
rne5t2,[deleted],hpuenzp,t1_hps7vho,1640376723.0,False
rne5t2,Depends on how you look at it. Typically we would say that addition in terms of bits it’s logn since as a number grows linearly the number of bits needed to represent it grows logarithmically. However in this type of problem it is normally assumed that addition is a constant operation.,hpuggc2,t1_hpuenzp,1640377583.0,False
rne5t2,[deleted],hpugvd5,t1_hpuggc2,1640377787.0,False
rne5t2,"This is starting to get pretty in the weeds but I suppose it illustrates that asymptotic analysis has nothing explicitly to do with physical computers, it only describes mathematical functions.",hpum8ux,t1_hpugvd5,1640380418.0,False
rne5t2,Great response to a good question.,hptnd33,t1_hps7vho,1640363988.0,False
rne5t2,If the algorithm depends on the bit length of a number then it is exponential in the input size. Just like Knapsack,hptfya6,t1_hps3iia,1640360516.0,False
rne5t2,so O(inf) should be acceptable right? I feel like they did not realize it could run forever,hprxsvx,t1_hprvlqt,1640323046.0,True
rne5t2,"In my opinion, yes. Especially if the prof didn't give any clarifications about the input domain either.",hprz03m,t1_hprxsvx,1640323749.0,False
rne5t2,"It is acceptable but expect to not be awarded marks if you didn't write your assumption. That is, O(inf) is possible when n > 100. Have a look at the analysis I did below.",hps2fyo,t1_hprxsvx,1640325864.0,False
rne5t2,"You always assume that n is going to infinity in these sorts of problems, there is no need to bound n",hps7xxw,t1_hps2fyo,1640329657.0,False
rne5t2,"You have to bound n because answers differ based on how n is bounded. There is no single answer since n can be bounded.

Without bounding n, there will just be a single answer and this is not very analytical.",hps8kv5,t1_hps7xxw,1640330140.0,False
rne5t2,"No, when asking for the runtime of a problem the bound is implicitly set as n going to infinity. That means that small inputs (in this case 100 for example) are not relevant to the runtime at all. Sure you could bound the input, but that defeats the purpose of this sort of analysis.",hps8unc,t1_hps8kv5,1640330340.0,False
rne5t2,"When analyzing an algorithm, you need to consider both the best case and the worst case. You don't just consider the worst case scenario.

And n = 100 gives you the best case and hence it is definitely relevant to the question.

When analyzing an algorithm, you need to be explicit. This is also tested in interviews. It simply makes you a better problem solver/analytical reasoner/Computer Scientist.",hps9hzd,t1_hps8unc,1640330833.0,False
rne5t2,"Sorry, but this is not quite correct, the best case of this algorithm is still infinite because you still need to analyze as n goes to infinity.

If determining the best case was as simple as picking a value for n, every function would have a best case of Theta(1).

Also this person asked for an O bound, they did not say anything about best or worst case.",hps9ql1,t1_hps9hzd,1640331017.0,False
rne5t2,"I think I get what you are saying but bounding the input helps you to analyze the algorithm deeper as I have shown with my analysis. 

As you can see, I have considered the input that goes to infinity but I have said when best case happens and when worst case happens; that is, when n is a particular value.",hpsb453,t1_hps9ql1,1640332081.0,False
rne5t2,"Please see my response below, but saying the best case happens when n is a particular value doesn't make sense. Asymptotics just doesn't quite work that way.",hpsbnp1,t1_hpsb453,1640332502.0,False
rne5t2,"Would you say the following is wrong?:

**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that n > 100. That is, n can take any number greater than 100.**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.",hpsasg2,t1_hps9ql1,1640331828.0,False
rne5t2,"In broad strokes no, in details yes.

The first two cases do not really make sense to analyze from an asymptotic perspective since they concern small values of n. Asymptotics is all about growth as n approaches infinity. And as I have said in my other responses, you really cannot select a value of n when it comes to asyptotics so the reasoning inside those two parts is misleading. I think you are focusing a lot of specific cases in your responses, but again, we are always talking about general cases here.",hpsbzj6,t1_hpsasg2,1640332763.0,False
rne5t2,"I see. I think I understand my confusion now.

All the algorithms I have analyzed needed an input size. As in, it wasn't a particular integer. It was something like n = len(array). And so it was intuitive to analyze the algorithm as n reaches infinity.

But when n is an integer (as in, not a size of anything), I immediately went to bound n.

So, you are saying that for any algorithm with any kind of input, we should always be looking at its run time as n reaches infinity right?

So, then for OP's question, the best and worst case is O(inf) when n reaches infinity?",hpsd0hz,t1_hpsbzj6,1640333582.0,False
rne5t2,"Yes, but as you can see there are a lot of subtleties on the way to that answer",hpsd70s,t1_hpsd0hz,1640333726.0,False
rne5t2,"Right, what do you mean by ""subtleties""?",hpsdezx,t1_hpsd70s,1640333907.0,False
rne5t2,I mean everything in we just talked over in this huge chain of comments,hpsdj4m,t1_hpsdezx,1640333999.0,False
rne5t2,"Sorry, still don't get what you mean. Can you clarify please.",hpse2ok,t1_hpsdj4m,1640334444.0,False
rne5t2,"The whole you can’t bound n thing. I think as humans we are inclined to want to give absolute values to things, but this sort of thing requires you to think in terms of growth",hpsea3m,t1_hpse2ok,1640334616.0,False
rne5t2,Right I get what you mean now.,hpsg2g2,t1_hpsea3m,1640336074.0,False
rne5t2,"But best case tend to usually happen when the input is a certain value isn't it?

As in, best case = O(1) when n = 100 or n = 99 for instance.

Another example is if we are looking at binary search. Say there is a list that goes from 0 to 100 and I want to check if 50 is in the list. (assume 50 is in the list).

And if I did binary search on that with 50 as my input, best case would be O(1) when mid = 50.

Worst case would be if the number can't be found in the list. That is O(log(n)) where n is the number of elements in the list.

""If determining the best case was as simply as picking a value for n, every function would have a best case of Theta(1)."" But this is why we consider best case and worst case.",hpsaedl,t1_hps9ql1,1640331525.0,False
rne5t2,"Not quite. Consider your binary search example. Yes the best case is Theta(1) which happens when when we get our number on the first try. However, in this case n is the length of the array we are searching, and it doesn't matter how big n gets, we can still find the number we want in constant time. In order words, in your example, you do not need to fix n to a specific value to get the best case. However, in the above example you would have to do so, therefore these are not the same type of problem. In general asymptotics is all about rate of growth, so fixing n to any one value makes no sense, we care only about how the runtime of our function changes as n grows.

Also I do not understand your response to my last line. Let me give you an example of what I mean by ""If determining the best case was as simply as picking a value for n, every function would have a best case of Theta(1)."" Consider scanning through an array. How long does that take in the best case? Well theta(n) of course, because the best and the worse case of scanning through an array are no different. However, if you take your approach and allow yourself to fix values of n, you could just say the best case of scanning an array is Theta(1), because the size of the array could be 1. Again, this defeats the point of asymptotics because now you are looking at one specific type of array rather than an array in the general case.

One last note: ""But best case tend to usually happen when the input is a certain value isn't it?"" Not really. I think it would be more accurate to say that ""best case tends to happen when something in the data causes a short circuit"", which again is what is happening your binary search example.",hpsbagy,t1_hpsaedl,1640332215.0,False
rne5t2,"Right, I totally get what you mean.  

Yes, we need to consider the runtime of the algorithm as input size increases or as input size goes to infinity. 

So, even if an algorithm is asking for a simple integer, we always need to consider the runtime of the algorithm as n goes to infinity? Because in OP's question, the algorithm is asking for an integer and not a size of anything. 

""best case tends to happen when something in the data causes a short circuit"". Right, I never thought of it this way. This makes more sense.",hpscoe3,t1_hpsbagy,1640333311.0,False
rne5t2,"Yes, n always has to go to infinity. For example

void func(int[] ar) {

int n = ar.length;

for (int i = 0; i < n; i++) {}

} 

Has a runtime of n in both best and worst case.

Something like:

void func2(int[] ar) {

int n = ar.length;

for (int i = 0; i < n; i++) {

if (ar[i] == 0) {
break;
}

}

}

Has a best case of Theta(1) since no matter how long the array is, we break if the first element is 0.",hpsd37o,t1_hpscoe3,1640333642.0,False
rne5t2,"Right. 

So, for your second example, I can say something like the following:

Best case = O(1) when first element in the array is 0. 

Is that right?

So, basically, when talking about best and worst cases, don't involve the input size. Check what makes the algorithm end early (if it does) and that's your best case. Is this correct?",hpsdc4b,t1_hpsd37o,1640333843.0,False
rne5t2,"Yes the best case is constant. However you are still taking input size into account, it is just the thing that breaks the algorithm early is not related to the input size, it's related to something about the data or some probabilistic element of the algorithm.",hpsdgdi,t1_hpsdc4b,1640333937.0,False
rne5t2,"""However you are still taking input size into account"". Can you kindly point me where?

I just said Best case = O(1) when first element in the array is 0. I am not talking about input size here?",hpsdxo1,t1_hpsdgdi,1640334328.0,False
rne5t2,"Maybe that phrasing is bad, we are still considering it to be infinite, even though that is not what we are basing the best case on",hpseep2,t1_hpsdxo1,1640334721.0,False
rne5t2,"Sorry I am still a little confused. 

So, is it wrong to say ""Best case = O(1) when first element in the array is 0""?",hpsg4ps,t1_hpseep2,1640336128.0,False
rne5t2,"No, that is correct",hpttny7,t1_hpsg4ps,1640366901.0,False
rne5t2,Thanks for the clarification.,hpuukgp,t1_hpttny7,1640384580.0,False
rne5t2,[deleted],hptcaw9,t1_hps9hzd,1640358732.0,False
rne5t2,"Yes, it is usually the worst case that is considered when talking about big O but for a complete analysis, we do also consider the best case. And not just that, the average case too. 

As in, it's completely correct to consider the best case as well because it gives a better overall analysis.",hpuux6u,t1_hptcaw9,1640384759.0,False
rne5t2,[deleted],hpuzpxn,t1_hpuux6u,1640387248.0,False
rne5t2,"Right but like I said, when talking about the Big O or when analyzing an algorithm in general, we don't just talk about the worst case. The best case is also considered for a complete analysis and that's what I am trying to say in my original comment.",hpv18d8,t1_hpuzpxn,1640388053.0,False
rne5t2,[deleted],hpv39f2,t1_hpv18d8,1640389160.0,False
rne5t2,"Yeah and I am agreeing the fact that Big O is about worst case but what I am saying is that the best case is also considered when talking about Big O too because it gives a complete analysis of the algorithm. That's what I have been saying in my original comment as well.

I am sure you have seen books and other resources mentioning about the best cases when talking about Big O (when analyzing an algorithm's runtime) and that's because it's important to consider it for a complete analysis.",hpv46ug,t1_hpv39f2,1640389677.0,False
rne5t2,"You don't see books and other resources saying, ""oh yeah, let's not talk about the best case because that's not what Big O is about"". Good books and other resources mention it because it's important for a complete analysis of an algorithm.

Again, I understand that Big O is about considering the worst case but when doing a complete analysis of the algorithm's runtime, you consider the best case as well and that's what I am trying to say. 

You clearly see this on CS books/other resources.

So, I don't get why you think that's wrong.",hpv537s,t1_hpv39f2,1640390179.0,False
rne5t2,"If you bound n and express the complexity in terms of n then trivially every terminating algorithm will have complexity O(1). It really doesn’t make sense to bound n, you just have to look at the definition of O notation to see that.

It makes sense to do case distinctions for algorithms if they behave differently for different inputs but that’s a different thing than straight up bounding the input size.

Also of course it can make sense to bound n and look at properties of your algorithm for bounded n but that’s not the same as asymptotic analysis.",hpsj54p,t1_hps8kv5,1640338688.0,False
rne5t2,"""It makes sense to do case distinctions for algorithms if they behave differently for different inputs but that’s a different thing than straight up bounding the input size."" Can you please give me an example of such an algorithm?

I think the confusion I am having is that OP's algorithm expects integers as input. And those are any integers as input. And depending on the integers given, you can have different big O time complexities. 

Hence, why I made restrictions to n in my analysis. 

So, if we are not supposed to restrict n, then that just means the best and worst case is O(inf) as n approaches infinity right?",hpso54s,t1_hpsj54p,1640342984.0,False
rne5t2,"OPs algorithm is actually one example where doing a case distinction is sensible to do because depending on what n is the runtime is O(n) or the algorithm does not terminate. Though tbh having a nonterminating algorithm as an example for asymptotic analysis is just straight up stupid by OPs prof. 

Fundamentally asymptotic analysis is about how an algorithm behaves as the input size goes to infinity/how it scales with the input size. For example if you have an algorithm in O(n^2 ) then you don't know how quickly it will run for n = 1000 but you can estimate that the time it will take for n = 2000 will be roughly 4 times the time it takes for n = 1000.

>So, if we are not supposed to restrict n, then that just means the best and worst case is O(inf) as n approaches infinity right?

You can make a case distinction for n but that's not the same as bounding n. If you bound n then runtime will be O(1) for any terminating algorithm. However a case distinction without bounding n can give you some better insights.

Consider for example this contrived algorithm:

    f(n):
    if(n = 1): return 1
    else if(n is a power of 2): return 2 * f(n / 2)
    else if(n - 1 is a power of 2): return 2 * f(n - 2)
    else return 2 * f(n - 1)

The asymptotic complexity of this function is O(n). However we can make a restricted analysis and say that we only consider n = power of 2 as input. Then the asymptotic complexity of this function for this restriction is O(logn). However that is not the same as bounding n. If we bound n by for example n <= 10^100 then the algorithm's runtime is O(1).",hpsreuw,t1_hpso54s,1640345719.0,False
rne5t2,"""OPs algorithm is actually one example where doing a case distinction is sensible to do because depending on what n is the runtime is O(n) or the algorithm does not terminate.""

But this is exactly what I have been trying to say. That is, depending on how we restrict n, there will be different big O time complexities. Or am I not understanding something?

Ok, I think I get what you mean with the example you have given.",hpszo7y,t1_hpsreuw,1640351769.0,False
rne5t2,">But this is exactly what I have been trying to say. That is, depending on how we restrict n, there will be different big O time complexities. Or am I not understanding something?

I think this whole discussion boils down to the distinction between bounding and doing a case distinction.

OPs example is really not a good one for this distinction since there the necessary case distinction actually corresponds to bounding n. When I replied to your comment I was more talking about asymptotic analysis in general rather than OPs example.

A case distinction make bounds but in practice that is rarely useful. What I'm mainly referring to is case distinctions like these (considering an input n): n is a prime number; n is a power of 2; n is even or things like that. In all of these cases n is restricted but can still grow arbitrarily large. And n being able to grow arbitrarily large is the important thing since otherwise asymptotic analysis doesn't make much sense.

So bounding can be seen as a form of a case distinction, but that's rarely ever the kind of case distinction we want to make for asymptotic analysis (except for completely stupid examples like the algorithm OP posted).",hpt1a2o,t1_hpszo7y,1640352748.0,False
rne5t2,"""What I'm mainly referring to is case distinctions like these (considering an input n): n is a prime number; n is a power of 2; n is even or things like that."" Right, so, like certain properties of n.

""So bounding can be seen as a form of a case distinction, but that's rarely ever the kind of case distinction we want to make for asymptotic analysis"".

Yeah, so, basically with any asymptotic analysis with algorithms, you will not bound the input because we care about how fast the algorithm is as input approaches infinity.

Thanks for the clarifications.

Edit: so, what's a case distinction for OP's algorithm?",hpt5e5o,t1_hpt1a2o,1640355114.0,False
rne5t2,"**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

**So, is the above a case distinction?** 

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**So, is the above a case distinction?** 

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

**So, is the above a case distinction?** 

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**So, is the above a case distinction?** 

**Let's assume that n > 100. That is, n can take any number greater than 100.**

**I am guessing the above assumption is bounding n right?**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.

**So, is the above a case distinction?**",hpszwqw,t1_hpsreuw,1640351917.0,False
rne5t2,"No that's wrong. Look, this is just an upside down recursive function. Instead of the base case being at the bottom of the number line, it's above. Any input <= 100 has a defined value.

Well alright. So we have something like a Fibonacci sequence, except it's just one recursive call instead of two. So the calls are one to one, so therefore there is a LINEAR relationship between the value of the argument and the number of calls.",hps6l5e,t1_hprxsvx,1640328655.0,False
rne5t2,"Generally yes. Technically speaking you can't say O(inf) because it does not make sense to apply an O bound to inf, but the runtime of this function (both best and worst case as well) is infinite.",hps8c4d,t1_hprxsvx,1640329958.0,False
rne5t2,"When analyzing an algorithm, you need to consider both the best case and the worst case. You don't just consider the worst case scenario.

And n = 100 gives you the best case and hence it is definitely relevant to the question.

When analyzing an algorithm, you need to be explicit. This is also tested in interviews. It simply makes you a better problem solver/analytical reasoner/Computer Scientist.",hpsnvpt,t1_hprxsvx,1640342763.0,False
rne5t2,"My first thought was O(n) since it would just count up to 100, but then I noticed that n is a parameter that could start > 100 so you're right that it could be non-terminating. 

Did the test ask specifically for recursiveFunction(1) or some other constantv argument? Or is there a wrapping function that starts it with a specific number?",hprvnlg,t3_rne5t2,1640321809.0,False
rne5t2,"the only instruction we got was the following, 

For the given algorithms, specify what is the time complexity of the algorithms using big-O notation. You do not need to write your computations. \[8 points\]

&#x200B;

and this was one of the parts, what I typed in the description was exactly what we got, nothing else

&#x200B;

I wrote O(n) initially then realized it was infinite, I never encountered this problem so I thought O(inf) might be it. any more thoughts? I lost quite a bit there",hprwihl,t1_hprvnlg,1640322295.0,True
rne5t2,Did you ask the professor? Maybe they just had a mistake in the test.,hprwwuz,t1_hprwihl,1640322526.0,False
rne5t2,"Professor is stubborn. No reply to emails. You can only send a remark request by tonight and say why  the TA made a mistake in your marking. In fact the prof does not even want an explanation just which part you want remarked 

I think I will just send in a remark even if it may annoy the prof",hprx4z7,t1_hprwwuz,1640322656.0,True
rne5t2,I would do it. His question doesn’t have a well-defined answer unless you specify the O function by breaking it out over the domain of n.,hpu1w8a,t1_hprx4z7,1640370697.0,False
rne5t2,"It would be O(n) under a certain assumption though. It wouldn't be O(n) for every case. If you are interested, you can have a look at the analysis I did below.",hps42hm,t1_hprvnlg,1640326911.0,False
rne5t2,"I think while the person authored the problem did not cover input restrictions, the intention of the question is most obviously to ask the time complexity on inputs that halt, which is O(n). 

Your answer is technically correct, but the professor is trying to test your understanding on recursion, so O(infty) looks like a gotcha answer.",hps4s09,t3_rne5t2,1640327390.0,False
rne5t2,"It's O(n) for when it finishes.

There is no such thing as O(infinity)",hptgpvv,t3_rne5t2,1640360885.0,False
rne5t2,"**Let's assume that 0 <= n <= 100. That is, n can take any value between 0 to 100 (inclusive).**

Under the above assumption, the worst case scenario is O(1) when n = 0 or when n = 1. This is because the number of times the recursive function will be called is a constant. The recursive function will be called 50 times and this is simplified to O(1).

Under the above assumption, the best case scenario is O(1) when n = 100 or when n = 99. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that -inf < n <= 100. That is, n can take any negative number to 100 (including 100)**

Under the above assumption, the worst case scenario is O(n) when n = -100 or when n = -99. This is because the recursive function will be called 50 times to reach 0 from -100 or to reach 1 from -99. And then the recursive function will be called another 50 times to reach 100 from 0 or 99 from 1. I am presuming this is how your Prof came to the answer of O(n) under that assumption.

Under the above assumption, the best case scenario is O(1) when n = 99 or when n = 100. This is easy to see as the recursive function is only executed once (again, a constant number of times) for when n = 100 or when n = 99.

**Let's assume that n > 100. That is, n can take any number greater than 100.**

Under the above assumption, this is when the worst case is O(infinity) because the base case of the recursive function will never be met. The recursive function will be called infinite times. But for such questions, you usually don't consider the above assumption because there is no need/pointless for infinite calls of the recursive function. We want the recursive function to complete. However, this is a possible answer for the exam if the exam isn't specific/clear about what values n can take. So, you are definitely correct to say O(infinity) but you have to write down your assumption.

Edit: Edited my assumptions.",hprxku7,t3_rne5t2,1640322913.0,False
rne5t2,"fwiw, you don't need to restrict your analysis of the O(n) scenario to when n >= -100. Your logic holds if n can take on *any* negative value.",hprz79k,t1_hprxku7,1640323863.0,False
rne5t2,"Yes, you are completely correct. I was focusing too much on the worst case scenario; that is when n = -100 or when n = -99 and hence why I had that restriction. 

But yes, n can take on any negative value. Thanks for pointing it out. 

I am going to edit the analysis.",hprzl1n,t1_hprz79k,1640324086.0,False
rne5t2,[deleted],hprzx32,t1_hprz79k,1640324286.0,False
rne5t2,"If n > 100, it'll run forever.",hps07mf,t1_hprzx32,1640324469.0,False
rne5t2,"Oh crap, I was right before. lol. Need to edit again.",hps1c9e,t1_hps07mf,1640325163.0,False
rne5t2,"Thanks for the detailed write up. hmm, so you think even though the worst case scenario is O(inf), we should ignore this and put O(n)?",hprya91,t1_hprxku7,1640323329.0,True
rne5t2,"No, your answer of O(inf) is correct under that assumption only. So, when you are writing your answer, it is very important to say what you are assuming.

So, if you wrote O(inf) and say this is the Big O under that assumption, then you deserve some marks because you were being analytical about the problem. It shows you understand the problem.

However, if you did forget to write your assumption, usually Profs won't reward you marks. That's usually the case.

As I have said above, I am presuming that your Prof has achieved O(n) under a certain assumption. That is when -inf < n <= 100. O(n) is possible when n = -100 or when n = -99.

Edit: Did Prof provide any reasoning behind his answer? You should definitely ask for the reasoning.",hpryq9i,t1_hprya91,1640323589.0,False
rne5t2,"To build on /u/Classymuch's answer, the reason why collegiate educators are rather stubborn about these things is because in both research and industry you're expected to read between the lines to understand what a (broken) program is doing. So they tend to purposely give you vague questions to see how you handle ambiguity, especially under timed conditions to see how comfortable or prepared you are with the aforementioned task. (Asking questions during exams is always an option!)

For instance, anyone who has PR'd someone else's code before will immediately notice the recursive call's +2 increment and the coincidental 2 base case conditions associated with the aforementioned increment for the constraint n <= 100, and perhaps take steps to repair the code for all integers instead of just n <= 100 with O(|n|). The initiative to take apart and repair a program is what separates a weak programmer from an intermediate one.

To defend my point, job interviews will occasionally ask you to debug or identify a program, and from that discussion, they can see how experienced you are with programming while also seeing what clever tricks you can come up with to simplify or refactor the code. Also, Amazon's leadership principles include ""Dealing with Ambiguity"" as a virtue, and for a interview with Amazon you'll need to prepare past experiences to implicitly demonstrate your ability to deal with ambiguity on problems far more complicated than this problem.

Hope this helps open your mind to these types of problems in future exams! Time permitting, never underestimate the complexity of a problem shrouded by ambiguity.",hps0bnf,t1_hprya91,1640324537.0,False
rne5t2,"I just finished my first year in CS and the University I go to has a strong focus/emphasis on algorithmic analysis and they are always about giving vague questions.

This obviously makes internal/external Uni assessments challenging but at the end of the day, it teaches you to be great Computer Scientists/problem solvers in general and great to see that it helps with interviews too.",hps19s6,t1_hps0bnf,1640325121.0,False
rne5t2,"Excellent, keep pushing forward!",hps1lkd,t1_hps19s6,1640325327.0,False
rne5t2,Ask your Prof for the reasoning of O(n) because it's very important to know how he achieved O(n).,hprzav4,t1_hprya91,1640323921.0,False
rne5t2,"OP, have a look at my analysis again. I had to correct my assumptions. Now they are correct.",hps06be,t1_hprya91,1640324446.0,False
rne5t2,"Hey OP, have a look at my analysis for the second time. Sorry, had to edit my assumptions again. It's correct now.",hps1os1,t1_hprya91,1640325385.0,False
rne5t2,"Nope, it's O(n) because the worst case is it will terminate with stack overflow (not using tail recursion), and integers are able to address the entire memory space to the largest integer is proportional to the size of memory available to the program. Therefore time complexity is O(n).",hpujej2,t3_rne5t2,1640379016.0,False
rne5t2,"Well for numbers <= 100 it will be finite if you don’t blow up the stack. Above that it just go on forever, but below that you will have no more than 100 passes.",hpv596o,t3_rne5t2,1640390272.0,False
rne5t2,[deleted],hprvfwx,t3_rne5t2,1640321690.0,False
rne5t2,"Thank you for your thoughts,

Im not really just arguing I’m trying to understand why the solution is like this. We are allowed to ask for a remark because thé TA is the one who marked this not the prof, the remark is done by the prof.

Instead of just gojng in full guns blazing I thought I’d ask here first to not waste the prof’s time.

Do you think my answer is acceptable?

Based on if I’m right my letter grade changes dramatically too as I’m on the borderline between a A and B, so I thought it’s worth asking",hprwsz9,t1_hprvfwx,1640322465.0,True
rne5t2,"""I’m on the borderline between a A and B"" I swing my vote then, this is indeed a time to argue!

I extended my answer a bit so go ahead and reread for my perspective.

Overall I think its a really terrible question but i dont think he is wrong to classify it as N.

Unfortunately most tasks don't grow at some smooth rate, and indeed most algorithms have inputs for which they fail.

IMHO this task is likely one where you were expected to apply certain static analysis type reasoning, I.E. the function does or doesn't have some property (such as loops or in this case recursive bifurcations)

I generally consider school a waste of time so my opinion is likely tainted but i do think this question proves little about your knowledge and certainly teaches you even less.

I do not think you will win this one with him, better to look elsewhere for your last mark or two, best luck and keep in mind that a B is a damn good mark, and that your boss will never look at the grades you got in school, he just wants know you can actually do the task he needs done.

Best luck! have fun!",hprxu3w,t1_hprwsz9,1640323066.0,False
rne5t2,[deleted],hprx0ti,t1_hprvfwx,1640322589.0,False
rne5t2,"This is the first time I got a question like this, I think leaving the uni is a bit dramatic but thanks for the suggestion. Like you said sometimes you just get 'bad bosses'.

Yes I agree it is a poorly designed question and maybe whoever made it did not realize that it can run forever. In this case however, how do you even represent that answer? you said O(inf) is a bad answer, but in this case isn't it the only possible answer?",hpry5wo,t1_hprx0ti,1640323258.0,True
rne5t2,"Yeah bad bosses are best dealt with by finding a good boss.

Infinity isn't really in the domain of the big-O notation.

As i say big O is about the growth of cost rather than the exact value of it.

Given the basic structure of the program (no loops or bifurcation) it is O(n)",hps7li8,t1_hpry5wo,1640329401.0,False
rne5t2,"O(inf) is not a bad answer by the way. It's an answer that's definitely correct under the correct assumption. That is when n > 100. Have a look at the analysis I did above. 

I hope it helps you.",hps0jzw,t1_hpry5wo,1640324679.0,False
rne5t2,"For n>100 you'll get most likely a stack overflow. I'd consider that termination, too.
This, in general, O(n).",hps9u2w,t3_rne5t2,1640331094.0,False
rne5t2,O notation is math. Adding arbitrary hardware restrictions is besides the point.,hpth9d5,t1_hps9u2w,1640361137.0,False
rne5t2,[deleted],hps8jp9,t3_rne5t2,1640330116.0,False
rne5t2,With big O you usually don't take things like stack size into account. Especially because with this function the compiler usually removes the recursion,hpsjs93,t1_hps8jp9,1640339238.0,False
rnc33s,"If you have enough money for mining rigs, you can take over all bitcoins with a 51% attack. Nowadays, most of the global mining hashrate is done by a few individuals. If they work together, they could take over the network.
Bitcoin was never meant to be a productive system. Satoshi mentioned that in their paper.
Also, it's not democratic. Only a handful of people own most of all bitcoins.",hpsaenv,t3_rnc33s,1640331532.0,False
rnc33s,"The number of practical use cases for blockchains is somewhat limited. A lot of blockchain applications in industry are created by people (encouraged by VCs) who see blockchain as a hammer, and all problems as a nail. Instead of looking for innovative ways to solve problems, they look for innovative ways to use blockchain. In lots of these cases, the problem could be solved much cheaper and simpler by using something other than a blockchain, like a distributed database. See [this](https://eprint.iacr.org/2017/375.pdf) for some limitations (and some benefits) of blockchains.  

Blockchains aren’t even that great at solving the problem they were invented for: currency transfer. The bitcoin network can only handle around 7 transactions per second, while networks like Visa and Mastercard can process tens of thousands per second, all while using orders of magnitude less energy than bitcoin. 

And the energy use of cryptocurrencies can’t be overlooked either: the bitcoin network uses the energy of a mid-sized country, most of which comes from non-renewables. Lots of blockchain fans will say that this can be fixed with proof-of-stake or other consensus mechanisms, but this hasn’t really happened yet (and I’m skeptical that it ever will for major blockchains). Proof-of-stake is also a pretty bad consensus method if your goal is decentralization, as it just rewards the richest, accelerating inequality in your currency. 

Cryptocurrencies also offer little in the way of security compared to traditional banking/credit card systems. If someone steals my credit card and goes on a shopping spree, I can report it to my bank and they’ll close the card and issue charge backs. On the other hand, if someone steals my wallet key and transfers all my cryptocurrency to their account, absolutely nothing can be done to get the money back. It’s gone for good. There are far too many ways for key storage to go wrong for the average person to risk storing a significant amount of money in cryptocurrency. 

Overall, for many applications, there are just better ways to do whatever the blockchain is trying to do. Blockchains are interesting technology, and they can be useful in certain instances, but they’re not broadly useful enough for a “decentralized blockchain led future.”",hps6ghz,t3_rnc33s,1640328564.0,False
rnc33s,"Thanks for this! A lot of interesting points you made. I will definitely check out that paper you cited.

Two things I want to push back on slightly""

1) Is it true that fixing the energy consumption problem with PoS hasn't really happened yet? I know Bitcoin won't ever change, and that Ethereum is taking a really long time to change, but basically every other new and promising ""3rd generation"" blockchain is (Cardano, Algorand, Solana, for ex). I know these chains have their own issues that may stem from the inherent downsides of PoS, but I did want to bring up that they are far more energy efficient than BYC and the current implementation of ETH. 

2) Addressing your point about PoS rewarding the richest. I have considered this and it makes sense on a theoretical level, but for example Cardano has introduced some interesting ways to push back on this (for example, the k parameter which caps the amount of interest a stake pool can earn, which incentivizes decentralization of staked ADA. Have you looked into this at all/have any thoughts? 

Finally, there is a cool game theoretic feature to PoS where in order to attack the network, you would need to control >50% of the networks tokens, but then you are just destroying value of an economy of which you control the majority of. This is in contrast to Bitcoin where the ""resource"" used to assure trust comes from outside the system and thus you don't need to own any Bitcoin to tank the network. 

&#x200B;

Thanks for your reply. Cheers!",hq5lg8y,t1_hps6ghz,1640618947.0,True
rnc33s,"It’s true that there are lots of new PoS blockchains, but separate blockchains can’t fix the energy consumption of the big PoW blockchains like BTC or ETH (1.0). PoW blockchains have to be deprecated or drastically shrink. Etherium may be able to do this, but shrinking bitcoin would require a sea change in the blockchain industry, as my understanding is that almost all cryptocurrencies today rise and fall on average with the price of bitcoin. 

>	for example Cardano has introduced some interesting ways to push back on this (for example, the k parameter which caps the amount of interest a stake pool can earn, which incentivizes decentralization of staked ADA.

I’m not familiar with Cardano and hadn’t heard of this, thanks for mentioning it. Do you know what would prevent someone with a large stake pool from splitting it into smaller pools to circumvent the limitations put on large pools?

My overall criticism of these types of features designed to prevent centralization is that if they work as intended, enormous miners/stakers (the kind that prop up most blockchains) would be incentivized to not use that blockchain and move their resources to a more centralization-friendly blockchain that rewards them more.",hq72n1f,t1_hq5lg8y,1640640884.0,False
rnc33s,"> Do you know what would prevent someone with a large stake pool from splitting it into smaller pools to circumvent the limitations put on large pools?

So I think you're right in that there is no inherent mechanism to prevent that. I think what the k parameter is meant to do is to incentive people who delegate their ADA to a stake pool to choose another one instead of the stake pool that is aggregating all of the staked resources.",hq758gu,t1_hq72n1f,1640642003.0,True
rnc33s,Terrible for the environment,hpsc3l6,t3_rnc33s,1640332852.0,False
rnc33s,"I’ll expand on this…

Blockchain uses “Proof of Work”, in which someone needs to do work just to prove that they’ve done it.  In particular, the way it works in blockchain is that multiple parties race to see who can complete the proof of work first.  So there ends up being an incentive (usually financial, as in cryptocurrencies) to do more work than others.

Work requires energy.  Not only is that a basic law of physics, but computational work specifically requires electricity.  The production and distribution of that electricity has environmental impacts, primarily (though not exclusively) in the form of carbon emissions, which contribute to climate change.

But is the amount of electricity consumed actually significant in the grand scheme of things?  Well, the bitcoin system itself uses about 121 terawatt-hours (TWh) per year.  That’s about the same as the entire country of Argentina, more than the Netherlands or UAE, and nearly as much as Norway.  For a sense of scale, researchers at Cambridge estimated that about 3-6 million distinct users used bitcoin in 2017.  Norway has a population of a bit over 5 million, so to a very rough approximation, each bitcoin user is consuming as much energy just to use bitcoin as each Norwegian is consuming for their entire lifestyle.  (Caveats:  The other countries are more populace, to varying degrees, so the same cannot be said for them.  Also, it is the bitcoin miners, not every user, who are consuming the energy, so I’m sort of amortizing that expense across all users.)  Or to look at it another way, the energy required to implement a single bitcoin transaction is enough to power an average U.S. household for 24 days.  (And that’s not to mine a whole bitcoin; that’s just for one transaction.)  Compared to an alternative financial system, you can perform 750,000 Visa card swipes for the energy of just one bitcoin transaction.  And all of this is only for bitcoin, not the sum total of all blockchain systems.  So yes, the energy consumption is very significant.

And all of this is fundamental to the concept of proof-of-work.  So it’s not just bitcoin, but all blockchain systems that use proof-of-work will have this problem.  And again, none of this is productive work; it’s just work for the sake of proving that you have done the work.

All of that is just the energy cost, though.  As people build large compute clusters and hardware accelerators (or just use graphics cards), etc., there is a lot of electronic hardware being produced only for the sake of performing this proof-of-work (e.g., mining bitcoins), and there is a significant environmental impact to mining all the minerals required to produce those electronic components.  (Not to mention the social and economic impacts of the way they are mined and manufactured..)",hpycrk0,t1_hpsc3l6,1640466894.0,False
rnc33s,This,hpsewtk,t1_hpsc3l6,1640335131.0,False
rnc33s,"Internet connection is required for any verification of transactions (unless you want to download the entire ledger). This doesn't sound like a problem in 2021, but there are still very many cases where internet connectivity is not available.",hpsh3ni,t3_rnc33s,1640336948.0,False
rnc33s,"1. Anything done by a decentralized system can be done more cost-effectively by a centralized system. This is because a naively decentralized system will require n² connections between its agents while a naively centralized system (a star basically) will have n connections between its agents. This still hold true for more optimized systems (mesh networks vs hierarchies). I'm confident that there is a rigorous demonstration of this, I'll let you look for it.
2. It is true that centralized systems require more trust than decentralized systems, but it would be wrong to think that decentralized systems don't require trust at all. At the very least you have to trust the developers that their code actually implements a fair and robust consensus algorithm. Also, the control of the system's future is concentrated in the hands of those developers.
3. Any real life application of a decentralized system will require as much real life trust as its counterpart centralized system, because you still need to trust the real life agent to deliver on its real life guarantee.

What follows is that the only use cases where decentralized systems may have an edge are those where non physical assets are exchanged between agents that don't trust one another.

For example, Ubisoft recent utilization of NFTs for in-game assets is dumb as hell because it could be easily centralized in Ubisoft's servers. Going decentralized doesn't remove the need for trust because you still have to trust Ubisoft to actually give meaning to those NFTs in-game.

The same thing will apply for any kind of corporation controlled metaverse, even if the assets are in a blockchain, you still have to trust the corporation servers for rendering those assets in the way that was promised when you bought that asset.",hpt3hj6,t3_rnc33s,1640354042.0,False
rnc33s,"There's some good applications for it, Things like decentralized patent systems or smart contracts etc. But i doubt any of this ""web3"" stuff is going to happen. Some things just don't benefit from decentralization",hptxgko,t3_rnc33s,1640368639.0,False
rnc33s,Define “future”. I’d wager that the vast majority of people in today’s world have no idea what the blockchain is or how it works. The knowledge hurdles that collective society would have to overcome to adopt public blockchain tech into the  real world would take a long time.,hprix97,t3_rnc33s,1640315282.0,False
rnc33s,The vast majority of people in todays world also don’t understand how the internet works. The vast majority of people don’t need to understand something to adopt it.,hps0c1r,t1_hprix97,1640324544.0,False
rnc33s,"Block chain is a solution to a problem nobody has.

Essentially the problem that block chain solves is ""How can we have secure and distributed records that everybody agrees on?""

And all three of those are already solved non issues. Security? We have RSA and key exchange webs. Distributed? The internet. And that actually scales. Universal agreement? Bruh, what's even the point? What's the point of agreeing on what a bunch of ones and zeros are if people can be free to interpret the meaning or weight of them? Consensus really only comes through power, and power is really only achieved by governments.

Bitcoin was a once in a lifetime exception. Everybody wanted an alternate currency and it was first, but it's essentially a ponzi scheme where the only people who benefit are the first movers.",hpsbz92,t3_rnc33s,1640332757.0,False
rnc33s,Lack of central control,hprgwt6,t3_rnc33s,1640314299.0,False
rnc33s,"In the world we live in, it is really difficult to have a decentralization-led future. We can't decentralize everything at least in the near future. I am not a critic of the power consumption of mining cryptocurrencies because the Proof of work consensus algorithm can be replaced with more efficient and decentralized ones in the future(not Proof of stake).

Blockchains solve a problem that doesn't really exist. People are criticizing the big tech companies for using their data to earn money. Most of the people with this critic are members of the herd. Meta and Google are using utilizing your data to sell you ads to make money, and I don't get it what is wrong in that. They have to make money so that their service keeps on functioning.

We need a centralized authority at some point or the other. Suppose you are buying real estate from a person in exchange for ETH. You send the other person 10k ETH on the Ethereum network and then the person denies to transfer the real estate ownership to you. What will you do? You will have to go to law enforcement, which is a centralized authority.

You can only have decentralized in the world of blockchains(that too partially). The whole system breaks when tokens of the blockchain world have to be used to do something in the real world. People are all hyped around decentralization. Decentralization has more demerits than merits.",hpsnmej,t3_rnc33s,1640342539.0,False
rnc33s,"Ever heard what an escrow is?

Too many newbies how want to look intelligent in muh BloCkChAiN",hq9vqsd,t1_hpsnmej,1640697667.0,False
rmtdj4,As a suplement I strongly recommend this post: https://www.muppetlabs.com/~breadbox/software/tiny/teensy.html,hppf77p,t3_rmtdj4,1640281076.0,False
rmtdj4,That was an incredible read!,hpq7qvm,t1_hppf77p,1640293223.0,False
rmtdj4,Thanks.,hpqp4to,t1_hppf77p,1640300943.0,True
rmtdj4,"If this is too much text for you, you can also watch this video as an alternative or complement.

[https://www.youtube.com/watch?v=CVg7CYVV3KI](https://www.youtube.com/watch?v=CVg7CYVV3KI)",hpokxuz,t3_rmtdj4,1640267665.0,False
rmtdj4,Nice :),hpq0a6f,t3_rmtdj4,1640289978.0,False
rmewjn,I would guess search engine could put a nsfw tag for certain key words to adjust weight,hplx9o7,t3_rmewjn,1640209741.0,False
rm1nxm,"It's not about one machine / language, but *Code: The Hidden Language of Computer Hardware and Software* by Charles Petzold is really good for a broader understanding.",hpjm9nq,t3_rm1nxm,1640171943.0,False
rm1nxm,One of the best books I've read.,hpjt80g,t1_hpjm9nq,1640176748.0,False
rm1nxm,"Yeah gotta agree here, great read if you want to broaden your K&U of the subject",hpjnuea,t1_hpjm9nq,1640173128.0,False
rm1nxm,As I read every chapter my appreciation and gratitude increased.,hpk1cob,t1_hpjm9nq,1640181444.0,False
rm1nxm,"Awesome, just ordered",hpk261t,t1_hpjm9nq,1640181858.0,False
rm1nxm,"I love this book and can't recommend it enough, but I don't feel like it's an in-depth history lesson of computing. It does mention important figures and technologies, but doesn't get into the nity-grity and instead is focused on the ""why"" of programming languages.

Still, amazing book. Read it in a few of nights because of how fun it was.",hpk4htn,t1_hpjm9nq,1640182994.0,False
rm1nxm,I second this.,hpk9zkx,t1_hpjm9nq,1640185509.0,False
rm1nxm,This book is required reading for every computer nerd of every stripe. Came here to recommend it.,hprv2c0,t1_hpjm9nq,1640321477.0,False
rm1nxm,"Code: The Hidden Language of Computer Hardware and Software https://g.co/kgs/JTNx9r

This book talks about the origins of “code”, going from Morse code through to binary, etc.",hpjm9s6,t3_rm1nxm,1640171946.0,False
rm1nxm,I think you'll like this much more than a book if you want only an overview of computer history - [Youtube - Crash Course (Computer Science)](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo),hpk7r8a,t3_rm1nxm,1640184500.0,False
rm1nxm,I’m interested in this too!,hpjm6c2,t3_rm1nxm,1640171871.0,False
rm1nxm,Rage inside the machine by Robert Elliot Smith. A really good book mainly about the biases we build into our systems but covers a large part of computation history going 1200 to Babbage and industrial revolution to training fighter pilots,hpjneio,t3_rm1nxm,1640172803.0,False
rm1nxm,"[History of Computing ](https://en.m.wikipedia.org/wiki/History_of_computing)

Read this then check out the references and external links section for deeper dives.",hpjqq6t,t3_rm1nxm,1640175139.0,False
rm1nxm,"**[History of computing](https://en.m.wikipedia.org/wiki/History_of_computing)** 
 
 >The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hpjqrym,t1_hpjqq6t,1640175172.0,False
rm1nxm,"FWIW it’s a complex, multifaceted history that involves a lot of mathematicians, people interested in logic, those who just want to “compute” in the sense of basic calculations centered around all kinds of things, etc. Eventually you’d get back to abacuses, star charts, etc.
  
So you’re unlikely to find a *single* comprehensive book.  
  
You might want to start with **digital computers** and bypass the mechanical/analog stages of computing history. 
  
At the same time you might want to read a book or two on Alan Turing, since the concept of a *Turing Machine* is essential to modern computing.",hpkf0lz,t3_rm1nxm,1640187688.0,False
rm1nxm,"I'm no expert in the history of computing, but the invention of programming languages and modern computers wasn't something that was done in terms of a ""straight linear process."" Instead, many scientists and engineers worked together to create automation technology for mathematical constructs. Think of the evolution of the computer akin to the evolution of Homo Sapiens. A chimp didn't just give birth to a human. Instead, through a long period of time and many small changes (along with other factors), chimps reproduced enough times to get to the human (__PLEASE NOTE: I AM NOT A BIOLOGIST AND THIS IS A REALLY OVERSIMPLIFIED VERSION OF BIOLOGY__). Likewise, through many many inventors and scientists (in)directly working together trying to solve scientific/engineering problems related to their time period, the technology of modern computing came to be.

I would recommend a firm foundation in CS; try the books ""[How Computers Work](https://www.amazon.com/How-Computers-Work-Evolution-Technology/dp/078974984X)"" or ""[Computer Science Illuminated](https://www.amazon.com/Computer-Science-Illuminated-Nell-Dale/dp/1284155617).""

Then you can look history books of computer scientists s.a. Charles Babbage, Alan Turing, Ada Lovelace, etc. Unfortunately, I am not familiar with the history of CS as much as I should be, so I can't help you here :-(",hpm6b87,t3_rm1nxm,1640213598.0,False
rm1nxm,Turing’s Cathedral or Darwin Among the Machines both by George Dyson,hpjr5xq,t3_rm1nxm,1640175433.0,False
rm1nxm,The innovators by Walter isaccson,hpkmz0i,t3_rm1nxm,1640190980.0,False
rm1nxm,I think it's called the innovators by Walter isaacson,hplnu36,t3_rm1nxm,1640205875.0,False
rm1nxm,The Innovators by Walter Isaacson is a great book.,hpmf8zy,t3_rm1nxm,1640217660.0,False
rm1nxm,There’s a Grace Hopper memoir that has a ton of history in it.,hpn1g27,t3_rm1nxm,1640228264.0,False
rm1nxm,The Cryptonomicon is a fun novel that touches on this subject a bit. But I came here to recommend Bertrand Russell’s type system for logic as an early example and influence on Turning’s contributions to the field.,hpnw15g,t3_rm1nxm,1640249810.0,False
rls9mc,">since the new minimum node will be the parent node, setting the pointer to the new minimum also takes constant time.

I believe the issue here is that there is often more work to be done after removing, in order to rebalance the tree.

If you look at the first example here:

https://www.javatpoint.com/balanced-binary-search-tree

If you simply remove the minimum element, your tree is no longer balanced. And it is not as simple as simply re-assigning the parent. In the case of the example, you would have to reassign at least two other elements.",hpibugc,t3_rls9mc,1640141345.0,False
rls9mc,"Okay yea that's where my error r
Lies. I made the assumption that deleting from a balanced bst given a pointer to the node would take constant time. Guess it makes sense that balanced bst would do some more work to balanced tree after a deletion. Thanks!",hpkvq4v,t1_hpibugc,1640194520.0,True
rls9mc,"Uhh. Huh? You have to search for the node to delete...
That's why it's o(log(n)) because worst case you find the node as a leaf node which is the height of the bst number of iterations... 

Only way for the node to delete be o(1) is if your bst is just one node.",hpi36fm,t3_rls9mc,1640137453.0,False
rls9mc,"Not the OP, but the passage in the book mentions storing a pointer to the minimum value. Then it mentions that because of this finding the minimum value is O(1), but deleting that same value is O(log(n)).",hpi7v8k,t1_hpi36fm,1640139579.0,False
rls9mc,"You have to reorganize the bst after deleting a node and that entails traversing down the right most child or left most child of the deleted node iirc.

So if you're deleting the root node, you'll still have to traverse the height of the bst to reorganize the bst.",hpidgl7,t1_hpi7v8k,1640142074.0,False
rls9mc,U didn't read the question well. You don't need to search for the node to be deleted if you keep track of the minimum node in a separate variable of its own.,hpkw1je,t1_hpi36fm,1640194649.0,True
rls9mc,"You still have to rebalance the bst after deletion of the node as I've explained already. 

You have to traverse the height of the tree to replace the node being deleted.",hpm4u55,t1_hpkw1je,1640212952.0,False
rls9mc,"I'm sorry.

The parents didn't realize the cost at the time.

There are multiple solutions to this, I really hope you find yours.  Memoization or Parallization, or whatever.

There may still be a cost in memory.  Best of luck to you.",hpibclr,t3_rls9mc,1640141133.0,False
rls9mc,"He's talking about a priority queue (i.e. min heap), not a balanced BST. Finding and removing the top node from a minheap is O(1) (it's the root). However, maintaining the heap invariant (aka finding the new min) requires looking at its direct children and bubbling up the nodes to fill in the gap, which is O(log(N)). O(1) is dominated by O(log(N)).",hpik8fu,t3_rls9mc,1640145317.0,False
rma55n,"I don't recall ever hearing that the logical operations are mainly composed of AND, XOR, and NOR. Generally at the gate level, complex designs are mainly built from NAND or NOR gates because they are ""universal"", i.e. you can build any logic design you want with enough NAND gates. Using a single gate type has some benefits when it comes to the physical layout and fabrication of your design, but the fact that your logic will be implemented in this way can be ignored for simple cases.

It's also worth noting that you don't always design logic at the gate level. Things like multiplexers, priority encoders, queues, etc. will make it easier to design complex logic structures.",hpmjuqi,t3_rma55n,1640219823.0,False
rma55n,"I’ve heard about NOR and NAND being universal, but it’s it really all that common to use them exclusively? I’d imagine using just those gates would end up adding many extra gates than if you were to use the whole variety. Wouldn’t it be cheaper to use less gates? Or at this low of a level is it truly a negligible cost?",hpn2aa4,t1_hpmjuqi,1640228667.0,False
rma55n,"Not necessarily correct, it depends on the technology you use to implement those gates, and the purposes of design. I remember that generally, the nand have smaller area in cmos technology and faster response time. In digital world, we only know 0, 1 but in real world, it takes time to change from 0 to 1and stable at 1 and vice versa. It's also easier to layout efficiently a design from same blocks rather than multiple shapes and sizes",hpniwh8,t1_hpn2aa4,1640240046.0,False
rma55n,Try building one yourself. There's a great book called The Elements of Computing Systems which just got a new edition.,hpq70xw,t3_rma55n,1640292909.0,False
rma55n,"An ALU can be a multiplexer with, say, 2 select bits, 4 inputs to select from (add, subtract, and, or) and one output. The add input comes from the adder (and is selected when the select signal is, say, 00), the subtract input (select 01) comes from the adder as well but one of the inputs to that adder will be changed to 2s complement beforehand, the and input (select signal 10) comes from an and gate with a and b as inputs, and the or input (select signal 11) comes from an or gate with a and b as inputs. 

If the design is based on the ttl logic family, you would see and & or gates. On the other hand, cmos logic prefers nand & nor gates, and with some minor changes to the circuit (bubble pushing for example), achieve the same end result.",hqtnqmc,t3_rma55n,1641056991.0,False
rma55n,"This is a great answer, thanks for writing this up!",hqto0sn,t1_hqtnqmc,1641057111.0,True
rma55n,"I think I read in a book some time ago that all of the not gates like NAND, NOR take less transistors to build. That could be part of the reason",hsd2wly,t3_rma55n,1642003923.0,False
rma55n,"AND, NOR, and XOR gates have much faster “processing” times than AND, XOR, and NOR. This is because of how the gates are actually implemented on the hardware.",hpnex9n,t3_rma55n,1640237531.0,False
rma55n,It's all just turtles all the way down but some of these use more turtles than others.,hpq6rdk,t1_hpnex9n,1640292793.0,False
rma55n,Following for the answer,hplsy3f,t3_rma55n,1640207973.0,False
rlfsh9,"Structure alignment tries it's best to align structures to fall within certain boundaries. Specifically memory words. Most address schemes are aligned on word boundaries so if you don't align to them you have to have an (another) offset to find the start of a structure/ field, which takes more instructions and time. You also have caches to deal with, which always read in a certain amount of words, typically called a cache line with is typically measured in memory blocks. The same thing happens if you don't align fields in your structure to at least word boundaries. It can be very expensive if you go across cache line boundaries for a field, especially if one of your lookups results in a cache miss.

 Now like you said, a lot of structures are too big to fit in a cache line. But it still helps to align their fields to a word boundary because you still don't want to also have to figure out the offset within the word. This is why structs in c/c++ will pad fields typically to boundaries of the highest native type. It just makes book keeping easier/faster.

Us humans operate the same way. If you are counting by 4s it's much easier to count by 4 all the time than to count by 4 to a certain point then change to counting by ones.",hpfibxk,t3_rlfsh9,1640098774.0,False
rlfsh9,"Caching is a huge factor of why you dont want your data striped across memory. The ability to fetch data from the cache is often times order(s) of magnitude faster than fetching from memory, which is in turn order(s) of magnitude faster than fetching from a hard drive (HDDs at least). So reducing the number of times you have to go get data from somewhere else, the better.

If you are dealing with very large datasets, where you can’t fit all of the data into the cache, then knowing how data is read from memory/disk becomes more important. For example, execution times while looping over large 2D arrays will vary widely depending on which array is iterated in the outer loop, and which array is iterated in the inner loop.",hpfwyvm,t3_rlfsh9,1640104961.0,False
rlfsh9,"> For example, execution times while looping over large 2D arrays will vary widely depending on which array is iterated in the outer loop, and which array is iterated in the inner loop.

Yup, I took an operating systems course and one of our assignments involved refactoring slow code to optimize iterating over a 2D array. Just changing the outer and inner loops of array access decreased the time of the program from ~30 seconds to about 10 seconds.",hphyn8v,t1_hpfwyvm,1640135372.0,False
rlfsh9,"> Caching is a huge factor of why you dont want your data striped across memory

Why is data striped across memory blocks not good for caching?",hqdx0l5,t1_hpfwyvm,1640764266.0,True
rlfsh9,"At a very basic level, scattered/striped data = more reads/writes performed.

I was going to type out a long explanation but im sure there is better, more verbose documentation out there. 

Like this! https://youtu.be/247cXLkYt2M",hqe3u1i,t1_hqdx0l5,1640769668.0,False
rlfsh9,"That video was a great watch. I always thought that there was not very informational content on YouTube, but well this video defies my belief. I did not fully understand the caching and stuff in the video but it of course gave me a starting point for learning. Thanks a ton!",hqezu1z,t1_hqe3u1i,1640790266.0,True
rlfsh9,"I had a question. I understood that data stored contiguously in memory is great for prefetching and caching, but how is memory aligned data good for caching?",hqigj4q,t1_hqe3u1i,1640842486.0,True
rlfsh9,"I think you have missed something in your example.

If you have a struct with an 8byte field (e.g. a 64-bit integer) and a one byte field (e.g. an 8-bit integer) then on CPUs which have alignment restrictions the compile will add padding at the end of the struct so that when multiple structures are put in an array all fields can be accessed efficiently.

Usually the worst case for alignment restrictions is “natural” alignment which means in this case that the 8byte field must be 8byte aligned. The compiler would then create a struct with { 8 byte field, 1 byte field, 7 bytes of padding }. This ensures that the size of the struct is an even multiple of 8 so if you create an array of them the 8byte field alway lands on a even multiple of 8 byte offset from the start of the array.

Edit: adding explanation for why alignment makes things more efficient.

What is not obvious is WHY aligning fields makes things more efficient. Adding padding to structs “wastes” memory, so it actually makes things LESS cache efficient in general.

At a surface level the answer is that the actual binary CPU instructions have these alignment restrictions built in, so if the compiler wants to access a field it can do it with only a single instruction if it knows the field is aligned. If the compiler know the field is not aligned, then it must compile the unaligned access into multiple instructions that access smaller fields and then more instructions to combine these small access back together into a register with the whole value. Clearly one instruction vs multiple is more “efficient”.

Going deeper one may ask WHY would some CPU instruction sets have this kind of alignment restrictions? Not all CPUs have these restrictions, so why do some? The answer is that these restrictions make the HW simpler, and simpler HW takes less power/room leaving more left over for enhancing performance in general. For example on how aligned access make HW simpler: no access will cross a cache line or page boundary. This means that any aligned load/store maps to exactly one cache line and exactly one page and will never be split accross two of either. HW can be built with this simplification in mind.",hpghkr4,t3_rlfsh9,1640113046.0,False
rlfsh9,"I have no idea what a data structure is, but I’m eager to find out. I’m taking thr FCC JavasScript course.",hpi7g6w,t3_rlfsh9,1640139388.0,False
rm2a30,There are giant bundles of fibre optic cables running through the Atlantic Ocean connecting Europe and North America.,hpjtgy3,t3_rm2a30,1640176910.0,False
rm2a30,"It’s like a funnel, there is a big single point of access to jump across contents so you will ping your local area, then the dns host in that region which communicates to a layer above it to be sent across the ocean. That’s not the exact path but the hops will go from general to specific then specific to general",hpkgowc,t3_rm2a30,1640188391.0,False
rm2a30,Tracert works by pinging and sending packets specially time live packets. These packets then die or drop when there’s no response. The packets used are ICMP packets which then respond back to the sender of the ping. This resides in layer 3. In short tracert maps out routers and connections to that host and is used for network maintenance to check that routers or servers are live and working through the network.,hpjnu9r,t3_rm2a30,1640173125.0,False
rkr2j4,"Tanenbaum is 77. When he was getting a degree CS was not really a thing you studied as a standalone degree. I don't know if there were any CS faculties at the time, but most CS people from that era majored in something like Mathematics or Physics. Consider that he got his Physics BSc in 1965, that's room-sized computer era.


EDIT: To add some more perspective, consider that ANSI C (or C89), the oldest version of C probably anyone here has used, was ratified as an ANSI standard in 1989. Think about that. The absolute **oldest** version of C likely all of us have come into contact with came **24 years** after Tanenbaum got his physics degree.",hpb9qot,t3_rkr2j4,1640019118.0,False
rkr2j4,"To be fair, C was standardized in 1989 but was created in the early 70s.

Still came after his physics degree though LOL",hpdt2ns,t1_hpb9qot,1640058671.0,False
rkr2j4,"People were writing in C long before it was standardized. The K&R book was first published in 1978, so C was already fairly mature by then.",hpdv6dj,t1_hpb9qot,1640059692.0,False
rkr2j4,">wiki says he's a established physicist while his books and his career speaks he's a computer scientist.

But the Wikipedia article devotes only _one_ sentence to physics? Everything else is about Computer Science",hpb9bnr,t3_rkr2j4,1640018949.0,False
rkr2j4,Wikipedia is generally very slanted towards computing topics.,hpblwd0,t1_hpb9bnr,1640024057.0,False
rkr2j4,"True, but Tannenbaum himself barely talks about his physics experience on his site or CV. He talks almost entirely about his computing and OS research.",hpcbw1f,t1_hpblwd0,1640034754.0,False
rkr2j4,"Yes it's true. His phd thesis is also on A Study of the Five Minute Oscillations, Supergranulation, and Related Phenomena in the Solar Atmosphere. 

I don't think this requires much core CS knowledge like OS etc. Man this guy did hardcore physics and computer science together.",hpefkyn,t1_hpcbw1f,1640072509.0,True
rkr2j4,"Now, watch this: 

Mathematician turned into computer scientist:

https://en.wikipedia.org/wiki/Barbara\_Liskov",hpeon9g,t1_hpefkyn,1640080018.0,False
rkr2j4,"There is no surprise in it. Computer Science Engineering as a separate discipline was founded at my former university somewhere between 1995-1997, before that people learned Electrical Engineering, and part of it programming and related disciplines. So, a Computer Science Engineering was an electrical engineer specialized to computer programming, the software part for a couple of years. The other direction was the programmer mathematician, which was taught at another University, where a programmer mathematician was a mathematician specialized in programming.",hpbtk3f,t3_rkr2j4,1640027147.0,False
rkr2j4,"Yes but his research is also on core physics stuff. 
His thesis was:  A Study of the Five Minute Oscillations, Supergranulation, and Related Phenomena in the Solar Atmosphere. 
And I don't think it needs much core CS knowledge like OS ,computer architecture etc. 🤔
He must have gotten very interested in the emerging CS field and decided to invest lots of time in learning it. 🙂",hpegbe1,t1_hpbtk3f,1640073080.0,True
rkr2j4,"Yes, agree, I think this is the way he went. TBH I think research wise it is more useful to have a math or physic degree.",hpek746,t1_hpegbe1,1640076242.0,False
rkr2j4,"Once you realise that your degree is supposed to teach you only one skill : Learning how to learn, that is the point you achieve ultimate freedom. You can learn anything you want after that and with suitable access to facilities or not you can research in any domain you like.

There are countless examples.",hpdwqwx,t3_rkr2j4,1640060489.0,False
rkr2j4,"As others have stated, CS as a study on its own really wasn't a thing yet. The first CS programs only really started becoming a thing in the 60s when he first graduated. Instead, what we consider CS today would have fallen under mathematics, or electrical/computer engineering. And a lot of early programmers would have been in those fields or others like physics due to needing computers to solve certain equations. Back then learning to program would just be reading a manual that came with the giant IBM or whatever, not some actual formal education.

If you want to grasp how someone coming from just math/physics was able to then also have such insight into the more complex CS before it was a standard thing, then I would recommend reading The Art of Computer Programming. It's the densest literature you can read in CS, but it shows how mathematics is the basis to create the major CS topics of today. You can really see the natural progression of the author, Donald Knuth, from mathematician to computer scientist.",hpbglr9,t3_rkr2j4,1640021923.0,False
rkr2j4,"I teach CS at university and I'm self-taught with no CS degrees. It's not like you have to get a CS degree to study CS -- true of any subject really. You just need the motivation, dedication, and understanding of how to teach yourself stuff.

CS itself is a highly interdisciplinary field. You can tell from the amount of technical terms and jargon in CS that's been taken (and bastardized) from other fields like philosophy, math, linguistics, and so on. So, often you can pivot from another discipline, using the expertise you have there to start in some related topics in CS. For example, I have a background in formal logic and that's where I started teaching myself about CS.",hpbn8yd,t3_rkr2j4,1640024598.0,False
rkr2j4,100 times this.,hpcb3ua,t1_hpbn8yd,1640034427.0,False
rkr2j4,Any degrees besides CS?,hpbyzof,t1_hpbn8yd,1640029373.0,False
rkr2j4,"When you're talking about degrees that are reasonably common among people working in industry in ""CS-like"" fields, the heavy hitters are CS (obviously), electrical engineering, computer engineering, physics, and mathematics. But you'll see working engineers with all sorts of backgrounds. It's one of the easier fields to be self-taught in to a degree that lets you do most technical work that employs graduates.

For professors, could be anything. A PhD isn't like a BS where you get a degree that intends to certify that you know the core material for a given field. A PhD intends to certify that you know how to do research. If you have a PhD in Folklore and you decide you want to be a computer science professor, you just need to publish in Computer Science conferences and journals for a while and then you're a computer scientist. However you acquaint yourself with the knowledge needed to get your papers accepted is irrelevant. Universities don't require a CS PhD to be a CS professor (or any other field). You need a PhD in literally anything and then you need people in the field you want to join to say, ""That person is awesome at my field. I've read their work and it's great"".",hpc4wi4,t1_hpbyzof,1640031836.0,False
rkr2j4,My degrees are in a humanities and I taught and did research in that field for several years.,hpcc1xo,t1_hpbyzof,1640034822.0,False
rkr2j4,He's more than that too. Started https://electoral-vote.com roughly 20 years ago. So you can add Poli-Sci as another interest.,hpc0vf3,t3_rkr2j4,1640030168.0,False
rkr2j4,"> I'm curious as to how he got the expertise to write books and do research on OS? 

A question you should ask about any academic! His [CV](https://www.cs.vu.nl/~ast/home/cv.pdf) is here, feel free to read it :)",hpcbtlv,t3_rkr2j4,1640034726.0,False
rkr2j4,Thanks 🙂,hpe9xmc,t1_hpcbtlv,1640068335.0,True
rkr2j4,"\[Edit: The post below is about Knuth, not Tannenbaum. Totally misread the question\]

I was lucky enough to attend one of his lectures. He is an incredibly humble individual, who has the ability to narrow his focus to be laser-like, and just do one thing and learn it entirely. I believe that's how he became who he is - by focusing on one thing, and not getting distracted.

During the lecture he was saying he stopped reading email ~~in the late 90s~~ \*since January 1st, 1990 (source: [https://www-cs-faculty.stanford.edu/\~knuth/email.html](https://www-cs-faculty.stanford.edu/~knuth/email.html)) and never went back. I doubt he has a smart phone that pings away his concentration every few seconds, or several meetings scheduled throughout the day.

He just picks up a subject, goes deep into it, and comes out the other end as a master and having created something useful out of it.

I'm sure there's plenty of subjects he approached where he could not produce something, so we might see only a selection of the successful ones. Still, what he can do is impressive.",hpf9ykb,t3_rkr2j4,1640094797.0,False
rkr2j4,[deleted],hqbd58m,t1_hpf9ykb,1640720445.0,False
rkr2j4,"According to [him](https://www-cs-faculty.stanford.edu/~knuth/email.html),   


>if you want to write to me about any topic, please use good ol' snail mail and send a letter to the following address:\[...\]  
I have a wonderful secretary who looks at the incoming postal mail and separates out anything that she knows I've been looking forward to seeing urgently. Everything else goes into a buffer storage area, which I empty periodically.  
My secretary also prints out all nonspam email messages addressed to \[...\]  so that I can reply with written comments when I have a chance.",hqc1s01,t1_hqbd58m,1640730513.0,False
rkr2j4,[deleted],hrvnvxq,t1_hqc1s01,1641706526.0,False
rkr2j4,"Wow, what a blunder. Whops.",hs1l1lr,t1_hrvnvxq,1641807703.0,False
rkr2j4,[deleted],hs3fnz0,t1_hs1l1lr,1641839195.0,False
rkr2j4,"Sorry for being the cause of ""I've read it on the internet so it must be true"" :D",hs6smmq,t1_hs3fnz0,1641894292.0,False
rkr2j4,"It seems that your comment contains 1 or more links that are hard to tap for mobile users. 
I will extend those so they're easier for our sausage fingers to click!


[Here is link number 1 - Previous text ""him""](https://www-cs-faculty.stanford.edu/%7Eknuth/email.html)



----
^Please ^PM ^[\/u\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20hqc1t9n)",hqc1t9n,t1_hqc1s01,1640730528.0,False
rkf6jh,"Hmm maybe this is a little higher level than what you want, but Ben Eater on youtube has a series where he builds a 65c02 based computer from scratch using a 6502 microprocessor.",hp9kxq7,t3_rkf6jh,1639980849.0,False
rkf6jh,He also has a 8-bit computer from scratch series and kit.,hpb3jxz,t1_hp9kxq7,1640016534.0,False
rkf6jh,Second for Ben Eater - he has some really fantastic content!,hpe5tw2,t1_hp9kxq7,1640065649.0,False
rkf6jh,https://www.amazon.com/8088-Project-Book-Robert-Grossblatt/dp/0830602712,hp9ohgn,t3_rkf6jh,1639983339.0,False
rkf6jh,"What about building something around the raspberry pi RP2040? designed for this purpose, A lot simpler than a full blown ARM64/GOU SoC, I imagine, and with luck there’s a community around it.",hp9vnc5,t3_rkf6jh,1639988955.0,False
rkf6jh,[here you go](https://www.nand2tetris.org/)  . though you need to learn bit of EE for practical purrposes. Buy Andre Lamothe's course from udemy( cheaply available). And for theory of EE search for JIM PYTEL on youtube and start with his DC electronics part 1 playlist.  Best wishes.,hp9qzny,t3_rkf6jh,1639985210.0,False
rkf6jh,"[stack computers](https://users.ece.cmu.edu/~koopman/stack_computers/index.html) 

[hdl resource](https://hdlbits.01xz.net/wiki/Main_Page) 

In stack machines chapter 3.2 there is a basic design with instruction specs that can be implemented in verilog for a fpga without too much difficulty. 

in terms of installing a os on the computer I'm not sure how honestly [os dev](https://wiki.osdev.org/Expanded_Main_Page) might have some resources I imagine that unless you replicate a existing chip design you'd have to edit(make?) the back end of a compiler to use a existing os (am unsure) os dev might have resources and their reading list is pretty good regardless. 

I'm working on something similar (I think) it *might* be easier to first make a small interpreter that serves as the os like forth that's what I'm looking into but haven't tried yet. 

also you could maybe *maybe* use that system to make a tiny vm to boot up a operating system using a vm that uses another computers instruction set as its optcode. 

you're more qualified than I am. I'm just a hobbyist I don't know if that's what you're looking for. I just remember looking to do what sounds similar running into a lot of dead ends and wasted time hope those help.

is there any particular fact about computers you feel you understand particularly poorly?",hpa6bpm,t3_rkf6jh,1639997987.0,False
rkf6jh,"Hello there, I actually have developed some small microprocessor platforms in the past and am putting one together in 2022 for a medical device I am designing. 

My advice to you is to start small. Look at the minimum components necessary to run a fairly simple microprocessor and move up from there. For example, maybe start with a simple Atmel chip (e.g. 328) or even a TI chip like the MSP-430. 

If you have no patience for that, and want to jump right in, I recommend looking at the Atom series x86 platform. The minnow board project is open and you can get right into customising your own platform: [https://www.minnowboard.org](https://www.minnowboard.org)",hr13yt3,t3_rkf6jh,1641182683.0,False
rkf6jh,[deleted],hp9fq5m,t3_rkf6jh,1639977536.0,False
rkf6jh,"Essentially like a raspberry pi.

I’m fine using an existing operating system. Developing my own operating system and SBC I feel like would become unrealistic for one individual. Or at least it is not in my scope of interest at this time.",hp9in93,t1_hp9fq5m,1639979343.0,True
rkf6jh,"I recommend building something compatible with a beagle bone or raspberry pi operating system (ie clone the hardware).

Otherwise, if this is your first or one of your first boards, you may want to consider an easier project first. I have done this exact thing as an electrical engineer with a few years of experience and it was still a challenge to me. Took about 200 hours of work, and needed impedance control (see advanced pcb building techniques). The board has about 350 individual components.

Osd has some good ressources on how to build a beagle bone clone using the osd3358. Read the tutorials!

Best of luck and feel free to pm me if you have any questions",hpakxty,t3_rkf6jh,1640007815.0,False
rkf6jh,"Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)

Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

[Ben Eater's Build a 6502 computer](https://eater.net/6502)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",hpb31ln,t3_rkf6jh,1640016317.0,False
rkf6jh,"Start small.  Design a simple 8bit ALU first and understand how they work, then add a verilog memory module, peripherals, etc.",hpnbe70,t3_rkf6jh,1640235445.0,False
rkf6jh,"> I am an electronic engineer

A student? Or an actually employed EE? I ask because I'm wondering who employs EEs these days to design things with nothing but lights and switches! :)

Do you want a modern SBC, e.g. with an Arm, or are you happy with some random 8bit thing?",hpadsoi,t3_rkf6jh,1640003569.0,False
rkf6jh,"Electronic engineers don’t do much designing. I do testing, program ics, build based on schematics, troubleshoot boards and test systems, design fixtures and some other stuff.

Electrical engineers do design.

Modern SBC is my aim.",hpcbn6o,t1_hpadsoi,1640034650.0,True
rkf6jh,"> Electronic engineers don’t do much designing.

The ones I work with do! Their job is to basically make SBCs in various forms for us software engineers to program :) I don't know if this is a different country thing (UK here) but what you describe tends to lean towards an electronic test engineer.",hpcdbfy,t1_hpcbn6o,1640035351.0,False
rkf6jh,So what do your electrical engineers do then?,hpcgh7f,t1_hpcdbfy,1640036680.0,True
rkf6jh,"Where I work they're not employed, but from what I know in other places they work on bigger things, power systems and stuff?

https://uk.indeed.com/jobs?q=Electrical%20Engineer&l=London%2C%20Greater%20London&vjk=f05819b0247e61d5&advn=2755904151741368

https://www.reed.co.uk/jobs/electrical-engineer-jobs-in-london",hpcwjhc,t1_hpcgh7f,1640043678.0,False
rkr18d,"The classic book in the area is Introduction to Algorithms by Cormen, et al.",hpbcm27,t3_rkr18d,1640020300.0,False
rkr18d,Thanks!,hpbdh2f,t1_hpbcm27,1640020654.0,True
rkr18d,"Do you mean correctness proofs? If so I can recommend Software Foundations volume 3 “Verified Functional Algorithms”. The book is available for free online. I recommend going through at least the first volume first, though.",hrb4j0m,t3_rkr18d,1641354626.0,False
rk5sms,"Check this out: https://en.m.wikipedia.org/wiki/Hardware_random_number_generator

Basically by measuring certain things at the quantum scale you can get ""true"" random",hp7qc2w,t3_rk5sms,1639948307.0,False
rk5sms,"Up to your confidence in certain quantum theories over others.

Randomness is part of standard model (though even there there’s some nuance), but observationally equivalent theories exist that don’t invoke it.

Of course, lack of information still makes event effectively “random” for those measurements — but at that point you might have an easier time just leaning on known chaotic dynamics — to get deterministic, but unpredictable events that match some distribution of interest.",hp8eylt,t1_hp7qc2w,1639959039.0,False
rk5sms,"I'm fairly sure that Bell's Theorem invalidates hidden variable interpretations of quantum mechanics, no?",hp8h1bn,t1_hp8eylt,1639960003.0,False
rk5sms,"Bell’s theorem only invalidates *local* hidden variable theories.  Where *local* references the theorized particle in question.

You can still have hidden variable theories they just have to have ways for particles to communicate without being next-to/on-top-of each other.

De Broglie’s proposed [pilot wave theory](https://en.m.wikipedia.org/wiki/Pilot_wave_theory) would be an example.",hp8ivnj,t1_hp8h1bn,1639960860.0,False
rk5sms,"Desktop version of /u/OphioukhosUnbound's link: <https://en.wikipedia.org/wiki/Pilot_wave_theory>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hp8iwwk,t1_hp8ivnj,1639960878.0,False
rk5sms,Thanks for the info!,hp8ktf5,t1_hp8ivnj,1639961807.0,False
rk5sms,[Good explanation on the matter](https://www.youtube.com/watch?v=ytyjgIyegDI).,hp8pue9,t1_hp8h1bn,1639964262.0,False
rk5sms,On a quantum scale? So like if I flip a coin and get a coffee flavored duck made out of sand? Would that be considered true random or are we still talking heads or tails here?,hp90w4r,t1_hp7qc2w,1639969682.0,False
rk5sms,"We're still talking heads/tails really.  In the end we must measure something, it's just a question of whether or not that thing we're measuring is deterministic or not.",hpbzjn9,t1_hp90w4r,1640029634.0,False
rk5sms,"Some of the best entropy sources we have are probably background radiation or radioactive decay.

You can get truly random data from https://www.random.org/",hp7s3kc,t3_rk5sms,1639949043.0,False
rk5sms,"In every thread about random numbers, I feel compelled to mention [this.](https://blog.cloudflare.com/lavarand-in-production-the-nitty-gritty-technical-details/)",hp9ft5h,t3_rk5sms,1639977585.0,False
rk5sms,"In theory yes. In practice it is very difficult to get it right, esp for security purposes. For various reasons, e.g.  bias, influenceability, measurement errors, degradation. And it also very difficult to assess that something is actually random.",hp86wms,t3_rk5sms,1639955431.0,False
rk5sms,"You can, yes, a lot of algorithms use minute heat changes in the CPU or background radiation from space (same stuff that causes static in radios, that is radiation in the form of radio waves) since both are relatively inexpensive computations and close to true random.

However, usually you don't want a completely random arrangement as many times people won't see it as random.

For example, in a truly random system, you will see runs of the same number especially in binary systems like flipping coins. The macro results (distribution) are random, not necessarily the micro results you see in short samples.

For this reason, most algorithms and programs employ ""pseudo-random"" generators which appear to be more sporadic to us humans.",hp8mac0,t3_rk5sms,1639962526.0,False
rk5sms,"The answer depends upon the context of the question. If you are asking this question to know whether you can implement this in your application or not, well you can using TRNGs(true random number generators) or HRNGs(hardware random number generators) like the other comments mention.

But if you are asking your question on a theoretical level then this answer is for you. Nothing can be random according to [Laplace's demon](https://en.wikipedia.org/wiki/Laplace%27s_demon). It is a hypothesis because for determining relatively ""random"" events(although there are no random events in the known universe according to Laplace's demon) you would need a lot of computation and that often falls into the [Computability Theory](https://en.wikipedia.org/wiki/Computability_theory).

I wrote a paper about your question titled ""Can something be truly random"" but I later never published it. But I'll discuss some ideas about the paper here. So Laplace's demon is actually kinda proved by the fact that we can now calculate the result of a coin toss before the coin actually lands on the ground.

We can generate random numbers using nature and we can generate pseudo-random numbers using computers. Randomness of something completely depends on the patterns and predictability of the data. The predictability of data depends on the information conveyed by the data. Information is retrieved from the order of the data. If the data is redundant, meaning that it has no patterns but a pattern of uniformity, it conveys no information because it has no order. This relation can be stated as follows:
Randomness ∝ predictability ∝ information ∝ order ∝ regularity of data

We are not yet sure whether Laplace's demon holds in the world of quantum mechanics because ah things of the macroscopic world usually break in the microscopic world. 

Laplace’s theorem also collapses when we apply the second law of thermodynamics. According to Laplace’s theorem, nothing can be random and everything is predictable, which means that the information about the universe doesn’t increase because there is no randomness being created whatsover. This completely goes against the 2nd law of thermodynamics which states that the entropy of a physics system always increases with time.

I am sorry if my comment went a little off-topic but I hope you learnt something.
Randomness∝predictability∝information∝order∝regularityofdata",hp9o73w,t3_rk5sms,1639983126.0,False
rk5sms,You could do it Schrodinger cats style and take a sample of background radiation/neutron collisions and use that as your seed.,hp9h0ri,t3_rk5sms,1639978326.0,False
rk5sms,"I’m no computer scientist but when I think of this I envision chaotic systems. Where the amount of information is exceedingly high and every vector for each informative quanta has it’s own unique internal system for objective emergence or expression.
In a system like that, we would get caught up in the definition of “perfect”. If by perfect you mean not being able to mathematically trace the origin then no system can truly be perfect. Even the universe (if we had the ability to track each event through or outside of time) would be perfect. It would be predictable and not necessarily random.
I think the perfection you’re looking for would come from the vector quality of each quanta of information and it’s ability to be predicted or traced. A perfectly random event would emerge as an unexplainable mystery.",hp9v1mk,t3_rk5sms,1639988456.0,False
rk5sms,yes at the really small scale things can be truly nondeterministic. x86 CPUs use thermal fluctuation data for rdseed instruction.,hpf8vv4,t3_rk5sms,1640094248.0,False
rk5sms,A random number which is prime,hp8e73f,t3_rk5sms,1639958682.0,False
rkfmao,"Don't use MD5 at all. Just use bcrypt.

The only time I'd hash something multiple times is in cases such as a zero knowledge password manager where you don't want the server to ever have the plaintext password, so you'd hash it on the client side once and then again on the server side -- but that's a very niche case and not what you need to do.

If you want additional security you can increase the rounds that bcrypt performs. This is the ""cost"" input to bcrypt. The default is typically 10 for most implementations which means 2^10 internal rounds. Using 13 or so is reasonable.",hpab3cv,t3_rkfmao,1640001693.0,False
rkfmao,I'm using md5 so that I don't need to store the byte data in the dB I'm salting it with 15 rounds then hashing that hash again with md5 so I can just use a var char to check the users password I just hash the users input the same way it was created and see if the hashes are the same.,hpahpu8,t1_hpab3cv,1640006005.0,True
rkfmao,It’s still unclear why you MD5 hashing would help anything here.,hpb1ok2,t1_hpahpu8,1640015730.0,False
rkfmao,Its purely just for db storage,hpbnary,t1_hpb1ok2,1640024618.0,True
rkfmao,"But it doesn’t add much security, if any, and adds an extra step later…  
  
Plus any MD5 hash collisions could make it possible to get into someone else’s account or even guess passwords (if the salt is the same each time).",hpbr264,t1_hpbnary,1640026132.0,False
rkfmao,"You can encode the bytes from bcrypt into text using base64, or even hexadecimal.",hpbg9ui,t1_hpahpu8,1640021789.0,False
rkfmao,"The output of bcrypt already contains the salt. You don't need to store anything else. If you were using something like PBKDF2, then you'd need to store the salt either in a separate column, or encode the digest + salt into some kind of hex or base64 string for storage.

e.g. bcrypt will output something like:  `$2a$10$N9qo8uLOickgx2ZMRZoMyeIjZAgcfl7p92ldGxad68LJZdL17lhWy` that has a salt of `N9qo8uLOickgx2ZMRZoMye` \-- this whole thing can be stored just like any other string in your database (e.g. in a varchar column)

Just do `digest = bcrypt(plaintextPassword)`, and save that digest to the database. The language you're using will have something like `hash.check(plaintextPasswd, storedDigest)` for comparing the stored digest against the plaintext password a user enters when trying to log in",hpcdjmr,t1_hpahpu8,1640035448.0,False
rkfmao,"You asked this [yesterday in /r/django](https://www.reddit.com/r/django/comments/rkf0a8/im_making_a_web_app_and_im_hashing_the_passwords/) but deleted your question when people responded negatively, and you're not getting the response you *need to hear* here, so I'll repeat my answer here:

This makes no sense. None.

You've stated elsewhere here that...

> I'm mainly doing it this way so I dont have to use blobs in my database with byte type data and can just use varchar with hexadecimal. 

 This is a solved problem, bcrypt hashes are **strings** that have the form `$2a$10$N9qo8uLOickgx2ZMRZoMyeIjZAgcfl7p92ldGxad68LJZdL17lhWy` and your hashes should be persisted in that format. If your implementation is producing binary output, you should use a different implementation. If you've implemented this yourself, **don't**. Use an off-the-shelf option. **Don't** run your binary data through MD5 just to get a hexadecimal representation. You'll produce something that is completely unportable, and you'll have no way to go back and *make* it portable as your original passwords will be lost after going through this Frankenstein hashing process. EVERY BCrypt implementation in any language can accept a hash in the form `$2a$...` and test it. Your hashes will be useless outside of your own narrow bit of code.

You've also said you're sending `md5(bcrypt(salt + password))` to the database. Where are you saving the salt?? You *need* that salt in order to compute `bcrypt(salt + candidate_password)` the next time the user attempts to log in. If you hash it with MD5, it's gone, and you can't use your stored hash for anything. Storing BCrypt in the form `$2a$...` takes care of this for you, storing the salt as part of the encoded hash.

Your question contains this troubling line:

> ... if they got the salt by hacking into the server ...

This implies to me that you're using **one salt** for all hashes, stored as a single secret. **That's not a salt, and this isn't BCrypt**. BCrypt uses a *per-user salt*, stored along side the output hash. Decorating your hash with a single application-wide secret is a [cryptographic **pepper**][1], which is not part of BCrypt and not useful. It's also unclear to me how you can be using a single pepper and still have BCrypt's cost factor at play.

Finally, MD5 is old and broken, and has **no business** being considered for use *anywhere*, but especially not in any thing related to security or password hashing. MD5 adds absolutely no security here, only complexity and harm, and it's about the worst possible way you could convert raw bytes to a hex encoding.

[1]: https://en.wikipedia.org/wiki/Pepper_(cryptography)",hpc1ye4,t3_rkfmao,1640030613.0,False
rkfmao,"If it’s md5( bcrypt( password + salt)) then yes

If it’s md5( password + bcrypt(salt)) then no. 

I don’t know what bcrypt is off the top of my head but if it’s a hashing algorithm on par with sha256 or 512 then it’s good. If it’s an encryption (that you can decrypt) then probably not.


Out of curiosity, why aren’t you just using the bcrypt value assuming it’s good. Also why not use a high sha rather then the md5. That would make it more secure one way or the other.",hp9jyvb,t3_rkfmao,1639980205.0,False
rkfmao,"SHA-512 is a cryptographic hash while bcrypt is a password hash or PBKDF (password-based key derivation function).  
  
SHA-512 has been designed to be fast. You don't want any delays when validating a signature, for instance. There is no reason for generic cryptographic hashes to be slow.  
  
bcrypt on the other hand is a password hash that performs key strengthening on the input. Basically, it does this by slowing down the calculation so that attackers will have to spend more resources to find the input by brute-forcing or dictionary attacks. The idea is that although the legit users - you in this case - will also be slowed down, they are only slowed down once per password. However, the attackers are slowed down for each try. The legit user is of course much more likely to input the right password first.  
  
Furthermore, bcrypt also contains a salt as input, which can be used to avert rainbow table attacks.

&#x200B;

I'm mainly doing it this way so I dont have to use blobs in my database with byte type data and can just use varchar with hexadecimal. 

&#x200B;

And yes it is md5( bcrypt( password + salt))",hp9kn2l,t1_hp9jyvb,1639980650.0,True
rkfmao,"From what you’ve said this seems perfectly secure. I’m a little concerned that you may be reducing the output space if md5 has a smaller number of bits output then bcrypt it may be more likely to have collisions. Although this isn’t likely a real security risk, more of an academic view of it. 

I would have base64 encoded the output to just be dealing with text rather then md5ing it. Although that’s just a personal preference thing. (Possibly some performance increase as that’s probs faster then md5, but again so minimal it’s academic)

Salted and hashed with a good algorithm is the best way I know to store passwords. If anyone knows of a better I’d be super curious to learn as well.",hp9la0w,t1_hp9kn2l,1639981082.0,False
rkfmao,"Yeah, I don't think an attacker would ever really be able to get the original value of the password out at least. But if anyone else has any more input id be happy to hear it.",hp9lu4q,t1_hp9la0w,1639981456.0,True
rkfmao,"I agree with hourglass492 that if your concern is database storage, you should convert the bcrypt output to a text format such as by base64 encoding. Running your bcrypt output through md5 is reducing your output space for no real benefit.",hpbm6e2,t1_hp9lu4q,1640024170.0,False
rkfmao,Thanks ill just convert it to base 64,hpbml44,t1_hpbm6e2,1640024334.0,True
rkfmao,"Don't do that. BCrypt hashes are stored in the form `$2a$...`, if your implementation is not producing that format, don't use that implementation: https://en.wikipedia.org/wiki/Bcrypt#Description",hpc3nc6,t1_hpbml44,1640031313.0,False
rkfmao,"Instead of MD5 im just going to convert it to base64 as the purpose of the md5 was mainly just for db storage. 

&#x200B;

Instead of MD5 I'm just going to convert it to base64 as the purpose of the md5 was mainly just for DB storage.",hpbmq33,t1_hp9kn2l,1640024387.0,True
rkfmao,[deleted],hp9isll,t3_rkfmao,1639979438.0,False
rkfmao,"Its not MD5 though its bcrypt(password, salt) then I take the bcrypt hash and do md5(bcrypt hash) then send that to the db",hp9jqtu,t1_hp9isll,1639980056.0,True
rkfmao,"I don't think you can use md5( bcrypt( password + salt)) like that, otherwise you wouldn't be able to check if password is correct even if you know it. You have to save salt in plain text format or encrypted by AES. Istead of md5+bcrypt I would suggest using Argon2, with it you can increase or deacrease how long it takes to check plain text password against encrypted one.",hp9o1yf,t3_rkfmao,1639983024.0,False
rkfmao,I don't need to check if the password is correct I can just hash the user's input again and see if it's the same.,hpagnv2,t1_hp9o1yf,1640005377.0,True
rkfmao,Not without the salt you can't.,hpc3oxr,t1_hpagnv2,1640031332.0,False
rkfmao,"Tangential :   
Have you evaluated available libraries for the same operation ?  Depending on the ecosystem (say Java or Python) there might already be libraries to implement password storage + validation +  anti-brute-force (like detect attempts across load balanced instances)",hp9sezt,t3_rkfmao,1639986325.0,False
rkfmao,"Yikes. Don't do this.

Please read this: https://crackstation.net/hashing-security.htm

Don't do this. Take the time to really learn cryptography before you start jamming hashes together willy nilly.",hp9stf6,t3_rkfmao,1639986642.0,False
rkfmao,That article is actually exactly what I'm doing it's impossible for me to decrypt so I take the users input hash it with bcrypt then hash it with md5 and see if the hash is the same.,hpahhsk,t1_hp9stf6,1640005873.0,True
rjvbrs,Write a ray tracer from scratch and try to implement the rendering equation via monte carlo,hp5ufoc,t3_rjvbrs,1639917414.0,False
rjvbrs,Thank you for your recommendation. Would you provide me with supportive resources?,hp5ulyk,t1_hp5ufoc,1639917538.0,True
rjvbrs,"1. Ray tracing in a weekend
2. Rendering equation in Wikipedia
3. scholar.google.com for more

I'm assuming you're experienced in C/C++",hp5usjh,t1_hp5ulyk,1639917669.0,False
rjvbrs,"Implementing the AltaVista MinHash alg could be fun with a web crawler of some sort. Or anything with universal hashing, really.

Bloom filters for malicious URLs has some example implementations out there you could probably build on.

Per Wikipedia, there’s also a use for Karger’s algorithm/Min Cut in segmentation-based object categorization scenarios like image compression.",hp6z1xw,t3_rjvbrs,1639937193.0,False
rjvbrs,Thank you for your contribution,hp97kh3,t1_hp6z1xw,1639972995.0,True
rjvbrs,"You can try bots programming on CodinGame, it's a great way to use those kind of algorithms, especially for reinforcement learning.",hp8kmqg,t3_rjvbrs,1639961712.0,False
rjvbrs,Thanks 😊. I am going to check it out,hp97er8,t1_hp8kmqg,1639972914.0,True
rjvbrs,Do you have experience with music software coding,hp7krrx,t3_rjvbrs,1639946026.0,False
rjvbrs,"No, but I would stop be interested in listening from you",hp97chr,t1_hp7krrx,1639972881.0,True
rjvbrs,Would you think you would be able to make a program that finely tunes audio files frequency/pitch? Say as the standard for music is 440hz you wanna wanna pitch it down to 432hz. It’s possible now but not 100% accurate so I would like to do that without any limits on the numbers and then program it so I can save it,hplguyu,t1_hp97chr,1640203050.0,False
rjvbrs,You could do something with Fourier transform.,hp8ojg1,t3_rjvbrs,1639963626.0,False
rjvbrs,What kind specifically of a project or challenge?,hp97hj2,t1_hp8ojg1,1639972954.0,True
rjvbrs,Lossy compression,hp98p0d,t3_rjvbrs,1639973556.0,False
rjvbrs,"Try the advent of code.  It's a challenge that happens every year; it's language independent; and every one gets their own problem set. 

https://adventofcode.com/",hp9mjxp,t3_rjvbrs,1639981955.0,False
rjvbrs,Thank you so much. I going to add it on my puzzles list,hpejrpp,t1_hp9mjxp,1640075885.0,True
rjvbrs,"Just out of curiosity, what books did you read? Perhaps I can point you to something you're already familiar with.",hq5aotq,t3_rjvbrs,1640613626.0,False
rjvbrs,My favorite book of all time is [Algorithm Design by Jon Kleinberg and Éva Tardos](https://ict.iitk.ac.in/wp-content/uploads/CS345-Algorithms-II-Algorithm-Design-by-Jon-Kleinberg-Eva-Tardos.pdf),hqdh795,t1_hq5aotq,1640754092.0,True
rjwxfl,"Since the keys were securely exchanged via asymmetric encryption, why does it matter? A handshake/exchange happens once, the portion of with the symmetric encryption needs to be fast as it can happen many times.",hp6ehhu,t3_rjwxfl,1639928521.0,False
rjwxfl,"You'll find almost all network protocols such as HTTPS only use asymmetric encryption for the handshake. The same goes for file encryption, typically a symmetric key is used to encrypt the files, then the symmetric key is encrypted using asymmetric encryption, that way you get the speed of symmetric with the security of asymmetric.",hp80xsg,t3_rjwxfl,1639952801.0,False
rjwxfl,"Yeah. Symmetric cryptography is much, much faster than asymmetric cryptography. There's no other reason.",hp6385q,t3_rjwxfl,1639922994.0,False
rjwxfl,It's much faster and just as secure.,hp9ga38,t3_rjwxfl,1639977872.0,False
rjwxfl,"With asymmetric encryption you don’t decode the message, only verify and sign it. You don’t ever want to exchange the private key, only the public key. You could for example sign a message with your asymmetric key and encrypt it with the key you exchanged based on the same key so the receiver can decode the message, verify it hasn’t been tampered with, without the receiver being able to copy your signature.

If you were to use asymmetric encryption for encoding/decoding messages, both sides need to know the private key.",hp9bbns,t3_rjwxfl,1639974960.0,False
rk78ya,"Hi,

I found some idea about this problem at:[Ethereum.stackExchange](https://ethereum.stackexchange.com/questions/116919/detection-of-same-function-reentrancy-vulnerability).

Zulfi.",hpfj7e7,t3_rk78ya,1640099163.0,True
rj8mre,"the compiler knows the type of the fields, so it knows how many bytes each field takes up and thus how many bytes it needs to skip to get to the next field.",hp1x93o,t3_rj8mre,1639839471.0,False
rj8mre,"> How do structs work internally in memory.

To answer that we need to pick an implementation, as this is implementation defined, it's not something covered by the C spec 

> I know that an instance of a struct is a pointer to the first field of the struct.

Sorry, but this isn't true!


    struct example { 
        int i;
        int j;
    } an_example;

    an_example.j = 0;


There were no pointers involved there.


> I also know that all the fields of a struct are contiguous to each other in memory so the memory address of the second field of a struct can be accessed by adding the size of the first field to the memory address address of the first field.

Again this ain't true. This struct, on gcc x86, defies what you say:

    struct example { 
        char c;
        int i;
    } an_example;

    // Then print (&an_example + offsetof(an_example.i)) Vs (&an_example.c + sizeof(an_example.c))

This is due to padding. For GCC look up the ""packed attribute""

> am failing to understand that how do we access the consequent fields of a struct with just the memory address of the first field.

The compiler knows the size of each member of the struct, and therefore knows where each other field is. So everything you write `an_example.i` the compiler knows to translate that into a specific memory offset .  Look up the `offsetof` macro.


You should try out compiler explorer at godbolt.org",hp2oabh,t3_rj8mre,1639851690.0,False
rj8mre,"> I know that an instance of a struct is a pointer to the first field of the struct.
> Sorry, but this isn't true!

Yeah I was wrong there. The instance of a struct is rather a reference variable to the first field of the struct. Again this might be implementation specific(I am using Golang which is similar to C is a lot of aspects) but when I print the memory address of the struct instance and the memory address of the first field of the same struct instance, they both are the same. 

> The compiler knows the size of each member of the struct, and therefore knows where each other field is.

So it also knows the amount of padding applied by data alignment? Because if it wouldn't know the amount of padding, then it won't be able to locate the data. Just confirming.",hp5aas1,t1_hp2oabh,1639900679.0,True
rj8mre,"> So it also knows the amount of padding applied by data alignment? Because if it wouldn't know the amount of padding, then it won't be able to locate the data. Just confirming.

It's doing the padding, so of course it knows it :)

The bigger program is the programmer/program can't usually find it out in a ""legal"" manner.",hp6afzt,t1_hp5aas1,1639926648.0,False
rj8mre,"Ah it all starts to make sense now. I thank you and all the others who helped me. This kind of computer science stuff which talks about the internal workings of memory, etc. really intrigue me. What field should I study to learn a little bit more about this kinda stuff? My guesses are that I would need to study assembly or compiler design or operating systems to get a taste of this type of stuff.",hp6b6i7,t1_hp6afzt,1639926999.0,True
rj8mre,"Yeah, all of those :)

A book I recommended the other day is [Crafting Interpreters](http://craftinginterpreters.com/). It's very easy to read, unlike some of the classic compiler books. It's also free online.",hp6hkiw,t1_hp6b6i7,1639929892.0,False
rj8mre,"Struct elements are contiguous in memory, so you could theoretically add the size of the first element to the address of the first element and get the second element, but often times there is padding between elements so you have to account for that as well. There are two general rules when counting structs and padding

1) each element must start at a memory address (relative to the first element) that is divisible by the size of the given element. Example:

Char (1 byte) | 7 bytes of padding | pointer (8 bytes)

2) the total size of the struct must be divisible by the size of the largest element in the struct. Add padding at the end of the struct to achieve this. Example:

Pointer (8 bytes) | int (4 bytes) | 4 bytes of padding",hp1xr9j,t3_rj8mre,1639839727.0,False
rj8mre,What is achieved by implementing the second rule?,hp236vr,t1_hp1xr9j,1639842366.0,True
rj8mre,"Someone can feel free to correct me on this/add to it, but it’s my understand it has to do with ensuring that one struct, or better yet any element within a struct, does not end up being stored across two memory blocks",hp25wst,t1_hp236vr,1639843618.0,False
rj8mre,"I am having a hard time understanding how the two rules work. Suppose we have a struct definition called `employee` with fields `firstName`, `lastName` and `age` of data types `string`, `string` and `int`, respectively. Suppose that we create an instance of the struct `employee` and store it in a variable `monica`. The values of the struct would be as follows: `firstName=""Monica""`, `lastName=""Smith""`, `age=33`.

Before I knew these two rules, I would visualize the memory locations of a struct like [this](https://imgur.com/a/JUJgudK). But according to the first rule, the different between the `n-1`th field's first bit and first bit of `n`th element must be divisible by the size of the `n`th element.

So according to that, we might not need the padding in the lastName field because its value(""Smith"") just occupies 5 bytes of data and 5 is divisible by the size of the next field(int-1byte). I strongly think that I am wrong here and the base size of a data type never changes. Please clarify this.

Also, how will the second rule help to ensure that any element of a struct does not end up being stored across two memory blocks.",hp2ahfx,t1_hp25wst,1639845671.0,True
rj8mre,"You make everything more complicated by bringing up strings :) Note that in \`C\`, \`string\` does not exist.

Strings (and other advanced datatype) are not first-class types because they can be of arbitrary size. The string """" takes 1 byte, ""abcd"" takes 5 bytes and so on (don't forget the null byte).

Thus, if you want to put a string into a struct, you either

* say that the string is at most x bytes long. Then you have an array of chars of size x, which has the alignment properties of a char, i.e. alignment 1, which just means no restrictions
* put in a pointer to the string, which then resides somewhere else in memory. Now your struct contains a pointer, with its specific size and alignment properties.

In general, types have a specific size and a specific alignment requirement. For integers and pointers, those are equal. For structs, the size of the sum of the size of its members, while its alignment usually is the largest alignment of any of its members.  


For arrays of type T of length n, they similarly have size ""n \* size of T"", but their alignment is still just that of the type T.",hp2fnny,t1_hp2ahfx,1639847948.0,False
rj8mre,"> For integers and pointers, those are equal.

How is the alignment for pointers and integers equal? The size of an integer is 4 bytes whereas the size of a pointer is 8 bytes, so they have different alignments.",hq4hn37,t1_hp2fnny,1640591641.0,True
rj8mre,"that's not what I meant. I meant that if the size is 4, the alignment also is for these types. If the size is 8, the alignment also is.

Unlike arrays, which can have size 1000 and alignment 1.",hq5076m,t1_hq4hn37,1640606908.0,False
rj8mre,"First let me provide some clarification on the first rule. It’s not exactly the n and n-1 relationship you described. Rather, I like to treat the beginning of the struct as a 0 point, and then ensure that each element begins at an “index” relative to the 0 point that is divisible by the size of the given element. For example, if we had a struct that had an 8 byte element, a 2 byte element, and a 4 byte element, it would look like this:

8 bytes | 2 bytes | 2 bytes of padding | 4 bytes

Note we have 2 bytes of inner padding, because our 4-byte element is now separated from our zero point by 12 bytes (without padding, it would be 10), which is divisible by 4. 

So now let’s revisit how we’re representing a string. Rather than storing the string literal, character by character, a string really just stores the address of a character array (where the array lives gets complicated and can vary, but it can be on the heap, in static initialized, static uninitialized, or perhaps even the stack), where the array represents the string. This way every string occupies 8 byes (since it is a char pointer). 

Knowing that, firstName and lastName are now both 8 byte addresses (char pointers)! Also, remember the age is a int and is 4 bytes, even tho the value is 33. So, here is our raw struct 

firstName | lastName |    age
8 bytes     | 8 bytes    | 4 bytes

Do we need any padding? The first element looks good, it’s just 8 byes. The second element is also 8 bytes, and the difference between the zero point is 8—all good. Our third element, age, seems to be good as well, it’s separated by 16 bytes from our zero point, which is divisive by 4. Now do we need any padding on the end? 

Total struct size = 8 + 8 + 4 = 20

20 is not divisible by 8! So we need 4 bytes of padding, to get our total struct size up to 24, which is divisible by 8. Here is our final struct:

firstName | lastName |    age    | padding
8 bytes     | 8 bytes    | 4 bytes | 4 bytes 

Total struct size = 24

Now, why does the second rule ensure we don’t have one element of a struct stretched across two blocks? Here is an example: lets say we have a struct that an 8 byte element and a 1 byte element:

|       8 bytes       | 1 byte

Now, lets say we want to make an array of structs. Each struct is 8 bytes in size, and arrays are stored contiguously in memory, so we have 

8+1 bytes | 8+1 bytes | 8+1 bytes | etc…


If our array is large enough we will eventually approach the end of a block. Will a block size ever be divisible by 9? Like, almost definitely not. So you’ll get the end of the block with less than 9 bytes of space left, and the struct will get chopped!",hp2gang,t1_hp2ahfx,1639848228.0,False
rj8mre,"By 'block' do you mean physical segments of memory? A memory block generally means a contiguous block of memory. How large can a 'block' of memory be?

PS: sorry for asking this question 8 days after your comment",hq13by7,t1_hp2gang,1640530760.0,True
rj8mre,"Basically you need to be able to uniformly step through the struct fields with limited knowledge of the contents of a field. All data is a byte sequence with a pre-defined interpretation.
  
E.g. C-strings are terminated with a *null byte*.  
  
M,o,n,i,c,a,\0,? 
 
\0 is the null byte, ? is an undefined byte value which could be any valid byte as it is not part of the string. 
 
A number like 33 would be stored differently than the string “33”.  
  
3,3,\0,? 
^ string version  
  
00000000 00000000 00000000 00100001  
^ 33 as a 32-bit/4-byte integer",hp2dbi2,t1_hp2ahfx,1639846920.0,False
rj8mre,"In general, data often needs to be aligned properly. A 64-bit pointer takes 8 bytes, and it should be placed at an address divisible by 8. If you try to do otherwise, you have a ""misaligned"" value, and reading such a value may be slow, or the processor might just not support it and your program will crash.

&#x200B;

Now, if you have such an 8-byte-sized value in your struct, you add padding to ensure that it's offset within the struct (i.e. what you have to add to the pointer to the first element to get a pointer to the element you want) is divisible by 8.

However, we actually wanted to have its actual address, i.e. the value given by (address of the first member + offset) to be divisible by 8. To do this, we additionally require that (address of the first member) is divisble by 8. This means that our struct now has an alignment of 8.",hp2f7tm,t1_hp236vr,1639847758.0,False
rj8mre,"> To do this, we additionally require that (address of the first member) is divisble by 8.

Could you please explain why?",hpfpnt8,t1_hp2f7tm,1640101953.0,True
rj8mre,"It's just basic divisbility rules.

You want (x+y) to be divisble by 8. We already require x is divisble by 8. What could we additionally require so that the whole sum becomes divisble by 8?",hpg8sga,t1_hpfpnt8,1640109605.0,False
rj8mre,"> What could we additionally require so that the whole sum becomes divisble by 8?

We want `y` to be divisible by 8. I get it now, thanks a ton!",hpjmfw3,t1_hpg8sga,1640172075.0,True
rj8mre,"Suppose we have a struct like this:
```
struct dog {
   int age; // 0x00 to 0x03
   // 0x04 to 0x07 - padding
   person* owner; // 0x08 to 0x015
   int score; // 0x16 to 0x19
   int lifespan; // 0x20 to 0x24
} bruno;
```
The first three fields are stored at a memory address divisible by 8, whereas `lifespan`'s memory address is not divisible by 8, then how can we say that the struct has an alignment of 8?",hq8y5rf,t1_hp2f7tm,1640672502.0,True
rj8mre,The struct will only ever be placed at addresses divisible by 8. This does not mean that all of its members also will.,hq9bwne,t1_hq8y5rf,1640682890.0,False
rj8mre,"It's called ""memory alignment"". The simplest answer is because the CPUs demand it be that way 

E.g. early arm could only load and store on 32bit boundaries. But an x86 could do 1byte increments. 

It also allows for more efficient indexing operations, but at the expense of padding. You can usually choose the trade offs in your compilers option flags",hp2t6dq,t1_hp236vr,1639853817.0,False
rj8mre,Isn't this implementation defined?,hp2sx0p,t1_hp1xr9j,1639853704.0,False
rj8mre,"It works exactly like with arrays, by using the address of the first element and an offset. The individual types don't need to have the same size, since the compiler knows them at compile time.",hp1v6nu,t3_rj8mre,1639838408.0,False
rj8mre,"Strict fields are *not necessarily* contiguous in memory, they might be packed to ensure the fields align on word boundaries.

That said, if you have the type you have all the information you need for field offsets with pointer arithmetic.  

If you have a pointer p to a field f of a struct s, you can calculate the relative offset from s to f by casting 0 (literally 0) to a pointer of type s, accessing field f, then apply & to that. Something like: `&(((struct s *)0)->f)`. You can then subtract this from any pointer to a field f to get its parent s.

This might seem horrid but it’s how linked lists are implemented in the Linux kernel and has the advantage (over a traditional linked list implementation) that you don’t need to cast the list payload every access, which is error prone.",hp5b9js,t3_rj8mre,1639901460.0,False
rj8mre,"A well thought out and carefully constructed comment.

&#x200B;

It's very clear that you wield C in a professional capacity!",hp6xtb0,t1_hp5b9js,1639936707.0,False
rj8mre,You could look it up in memory,hp1ua1c,t3_rj8mre,1639837932.0,False
rj8mre,I can't look it in memory that how does the compiler navigate through the struct.,hp1uwgw,t1_hp1ua1c,1639838259.0,True
rj8mre,You literally gave the compiler the definition of the struct.  It knows the size of each member and the padding needed to byte align.,hp1x30m,t1_hp1uwgw,1639839385.0,False
rj8mre,"If what the other commenter says is true, you could get an intPtr to the struct, then increment it by something like 
     
    sizeOf(typeOf(myStruct.firstElement)))",hp1vo1m,t1_hp1uwgw,1639838662.0,False
rj8mre,"Ah okay, I kinda get it now. So the compiler navigates through the struct by just adding the size of the first field to the memory address of the first field. Is my interpretation right? just confirming.",hp1x7nv,t1_hp1vo1m,1639839450.0,True
rj8mre,"Not exactly, as that does not take padding into account. The compiler determines the layout of the struct. Based on this, it knows the offset of every member from the start of the struct. Computing the address of each field becomes (p + n) where p is the address of the struct, and n is the offset of that field from the start of the struct. n is known at compile time and is a constant offset.",hp3ej4n,t1_hp1x7nv,1639863584.0,False
rj8mre,No I’m afraid not because there might be padding between that element and the next one.,hp5bhv2,t1_hp1x7nv,1639901650.0,False
rj8mre,"At some level, yes, that has to be it. As far as the implementation details they probably have a bunch of optimizations that would make it complicated to parse.",hp1yuqf,t1_hp1x7nv,1639840279.0,False
rj7x0t,"In order of showing this function is big theta of f(n) [where n is the number of items], you want to show its both O(f(n)) and Omega(f(n)).

Now in order of finding the needed f(n) its mostly comes down to understand what the code does. After you figure what f is (using simply your intution) you need to prove both O(f(n)) and Omega(f(n)).

Lets denote the number of operatios of your function as g(n), you now need to find constants n_0 and n_1, c_0 and c_1 such that:

g(n) < c_0 f(n) for n> n_0 and

g(n) > c_1 f(n) for n > n_1.

And by that the proof is done. Sorry if my formating is bad, im on phone",hp1qrl1,t3_rj7x0t,1639836037.0,False
rj7x0t,"Amazing, thanks dude! Really appreciate the help :D",hp6rjsf,t1_hp1qrl1,1639934142.0,True
rj7x0t,"It's hard to answer this question without knowing what you currently know about big-O, big-Omega and big-Theta notation.

First, you should identify what the ""problem size"" is in this context. It looks like the input of your algorithm consists of a list of lists. Candidates for the problem size here could be:

* The total number of items.
* The number of lists in the `lists` object.
* Both of the above.

The first option is probably the most applicable here, so let's say `n = total number of items`.
For your sample input, `n = 6`.

To find the worst case runtime of this algorithm to be Θ(f(n)), you want to show that a worst case instance of the algorithm will:

* complete in **at most** `constant * f(n)` steps (which shows the algorithm runs in O(f(n)) time), *and*
* requires **at least** `constant * f(n)` steps to complete (which shows the algorithm runs in Ω(f(n)) time).

The `constant`s in the above two conditions need not be the same constant, but they must both be independent of `n`.

So first, you should consider what is a worst case instance for your algorithm. Hint: you've already given a worst case instance for `n = 6`. Then you should prove the two statements for some function `f`.

To determine the function `f `, ask yourself this: if the number of items were to double, how much longer would this code run? And then, can you think of a function `f(n)` with this property?",hp232lx,t3_rj7x0t,1639842310.0,False
rj7x0t,"this is a linear search of a 2d array. if the array is m by n then on average you will search through half the list to find it which would be m\*n/2 which is just theta(m\*n).

&#x200B;

also if you want to shit your pants with the technical details then you can guarantee that there is some constant to multiply m\*n by that will be asymptotically less and some constant that would be asymptotically greater. you can guarantee this is true because i said so and i offer a 30 day money back guarantee.",hp5noj7,t3_rj7x0t,1639911970.0,False
rj7x0t,Theta(1) because it only performs a constant number of operations.,hp3eifx,t3_rj7x0t,1639863574.0,False
rj7x0t,lmao dude obviously he meant if you scale by the list size,hp5nhdv,t1_hp3eifx,1639911806.0,False
rj7x0t,"If I asked you how many operations your code makes before it finishes, what answer would you give? Just curious, and it might reveal the best path towards understanding the answer to the question you gave.",hp4a0nw,t3_rj7x0t,1639879187.0,False
riwlmc,That’s super cool!,hp47tm0,t3_riwlmc,1639878091.0,False
risxvd,"Server software is specifically developed and tested for long uptime (because that is a relevant use case). Other software, for example League of Legends, is not tested for long uptime - if the Client crashes after 24h, nobody really cares (at most, they put in a 12h restart warning/force).",hozlh61,t3_risxvd,1639786416.0,False
risxvd,If you were to use a server as a desktop you would run into the same issues.,hozj8re,t3_risxvd,1639785415.0,False
risxvd,"Server hardware runs to varying degrees:

* climate controlled environment
* multiple power feeds, battery backup, generators
* error correcting memory
* redundant drives with redundant drive controllers
* multiple high volume fans
* professional maintenance
* designed for purpose software stack
* active monitoring of component health
* solid metal chassis properly grounded

Laptop: plastic foldy boy that's filled with dog hair and viruses",hp0wr8r,t3_risxvd,1639812772.0,False
risxvd,especially when you see those repair videos with like cockroaches coming out the vents it’s no surprise they don’t run well,hp1n50x,t1_hp0wr8r,1639833912.0,False
risxvd,"Non-server software often has memory leaks.  Web browsers are especially notorious for leaking memory.  Fixing memory leaks is tough, so often companies don't put resources towards fixing them.   Far easier to have your users restart the app, which is not an option on servers.",hp00koq,t3_risxvd,1639793428.0,False
risxvd,Servers have to be rebooted more often than you think. But high availability configurations make it passible to reboot a single server without the service going down.,hp14jsd,t3_risxvd,1639819263.0,False
risxvd,"A server not need rebooted except for upgrade the kernel or change some failed hardware.

Nobody care if a server is running for months or years without reboot.

Even those of us who have servers at home avoid reboots because we like see high uptimes. 

We never reboot the servers in my company, except for kernel upgrade.

PS:i known its possible upgrade the kernel without reboot",hp1do1i,t1_hp14jsd,1639827073.0,False
risxvd,"Just to add to what’s already been said, it *is* possible to keep a desktop computer running for weeks or months without noticeable performance degradation - is all about what OS and software you run on it.

For example, my last three Macbooks would run 6-9 months between reboots, and my work Windows laptop is the first Windows machine I’ve owned which could almost match that, probably due to memory management improvements in Windows 10.",hp124if,t3_risxvd,1639817177.0,False
risxvd,"I usually reboot my desktop computer maybe once a quarter on average, though I should probably do it more often. Old habits, I guess. (Running Linux, Ubuntu to be specific.)

Server uptime was more of a thing sysadmins bragged about in the past. Today if someone says a server's been going without a reboot for two years I just think ""yikes, gotta be some unpatched security holes in that kernel"". Most server software supports running across multiple instances today, so usually you can take servers down for maintenance/upgrades without anyone noticing.",hp1dkpm,t3_risxvd,1639826999.0,False
risxvd,"software in computers at less tested and  leaks more memory

Software in servers its better done and reviewed.

what is a memory leak?

well programs request to kernel memory space, when finish they release it and the kernel can offer that space again.

if a app crash or the app code forget release the memory.. we would hava a memory leak. Kernel not going to offer that memory because assume it in use by other app.",hp1dycz,t3_risxvd,1639827306.0,False
risxvd,"It's the software, the server's softwares are so much efficient in terms of memory, but a normal pc is not, piling up along the way. If you know what you are doing you can get decent up time without being lagging. I was able to run my pc for 12 days straight with little to no loss in performance.",hozieyj,t3_risxvd,1639785039.0,False
risxvd,It's all your electron apps slowly consuming your available memory (and memory leaks and other bugs in your desktop software that aren't present in server software),hoztsot,t3_risxvd,1639790242.0,False
risxvd,It's pretty much entirely the software.,hp09fb6,t3_risxvd,1639797933.0,False
rinkr5,">If data loss will occur, tell me the limit for both. With and without data loss.

I suspect the answer to your polite question regarding lossy compression is that you compress 1TB to one eighth of a byte.

Always happy to help!",hoy78ym,t3_rinkr5,1639765800.0,False
rinkr5,i’m sure it would still be readable after the decompress,hp17zvh,t1_hoy78ym,1639822262.0,False
rinkr5,"If it weren’t decompressible, then it wouldn’t be compression. That would just be straight up data loss.",hp29zac,t1_hp17zvh,1639845445.0,False
rinkr5,maybe that’s were the world of data compression is heading just straight up data loss,hp2aeri,t1_hp29zac,1639845639.0,False
rinkr5,Compression depends on the contents. A 1TB of 0 could arguably be encoded as 1 bit of 0 and 40 bits to store length.,hoy7eyn,t3_rinkr5,1639765863.0,False
rinkr5,Or 1 bit with a zero and always assume a 1TB length,hozkf2h,t1_hoy7eyn,1639785940.0,False
rinkr5,Or 0 bits if your compression algorithm assumes an empty file to be exactly 1 TB of 0's.,hozze1s,t1_hozkf2h,1639792868.0,False
rinkr5,This guy algorithms.,hp08y8p,t1_hozze1s,1639797683.0,False
rinkr5,This guy assumes.,hp0fal1,t1_hozze1s,1639801120.0,False
rinkr5,"Depends on the data. If it's a random stream of zeroes and ones, it cannot be compressed at all. If it's a terabyte of all zeroes, it could be compressed by 99.9%.

Compression looks for patterns and duplication, so if there are no patterns, there is nothing to compress. Different techniques are used to compress text, images, videos, and audio streams.",hoy80su,t3_rinkr5,1639766092.0,False
rinkr5,"Everything has a pattern even if the file itself is the pattern, but a “unique” file cannot be compressed much at all. What’s important is whether a pattern **repeats**.",hp29bsy,t1_hoy80su,1639845157.0,False
rinkr5,Depends on the data in it.,hoy7gfj,t3_rinkr5,1639765878.0,False
rinkr5,"This. Generally, data compression algorithms rely on knowledge of what is being stored. For example, in images, in order to make the JPEG format, there was a bit of research done on quantizing photos in the frequency/cosine domain. They use a different quantization coefficients for each frequency in an 8\*8 block as it makes the image look similar enough even though the MSE  compared to the original image actually increases.",hozhfd0,t1_hoy7gfj,1639784591.0,False
rinkr5,"Every data can be compressed to a single bit, that just holds the information if it is the compressed data or not. All other information than has to be in the decoder. That was always the case, in 2000 as well as in 2010.

For more information you have to get into information theory.",hoyih8t,t3_rinkr5,1639770144.0,False
rinkr5,"While I'm certainly partial to CS theory, and information theory and compressibility are definitely interesting, I think your answer goes a bit too far into the technically correct but almost entirely unhelpful territory.

OP's question doesn't really give enough information to give the answer they're asking for but information theory is hardly going to be helpful to a layperson unless they're willing to go down an entire rabbit hole to answer their question.

So, this answer kind of gets both an upvote and a downvote, mentally.",hoyytwd,t1_hoyih8t,1639776736.0,False
rinkr5,"Thanks. You managed to perfectly verbalize my intension while writing my comment.

Edit: For lossless compression: Compression is very well studied. So there is no practical differences between the algorithms now, 10 or 20 years ago. We basically already got the optimal solution. By how much you can compress data depends entirely on the self-information or entropy of the data. This is well defined and gives an lower bound to the compressed data size. Modern (>= year 2000) algorithm achieve near optimal compression for ""real world"" data.",hoz5gud,t1_hoyytwd,1639779458.0,False
rinkr5,I wonder why you think that compression is solved. There is still a lot of research going on in the field. Just because the theory given a specific information source is done this doesn't mean that the application is straight forward. The lossless compression theorem really is only the starting point. Applying this to the vast probability spaces of for example image data relevant to humans is the difficult part.,hp1q583,t1_hoz5gud,1639835688.0,False
rinkr5,"Regarding images: JPEG2000 was (as the name suggests) developed in the year 2000. It was developed to be the successor of JPEG, but we still use JPEG. There are many improvements, but they just are not requested, because the already available algorithm are good enough for all use cases and the improvemnts are not big enough.",hpg3wc9,t1_hp1q583,1640107712.0,False
rinkr5,"It really, really depends on what you're compressing. It's impossible to give a number without some kind of an idea of what the data are, and what the shares of different kinds of files and formats are.

Photos and videos can hardly be compressed further using lossless, general-purpose compression. That's because most common image and video formats already employ compression, often both [lossy](https://en.wikipedia.org/wiki/Lossy_compression) and [lossless](https://en.wikipedia.org/wiki/Lossless_compression) compression, and applying compression on top of compression generally doesn't give you much. There might be some slack to be picked up by state-of-the-art lossless compression but not a lot. I'd expect almost no gain from compressing already compressed image or video files.

Word documents and other documents from the modern Office suite are also already compressed, and while the compression used in the file format may not be bleeding-edge and you can probably compress it a little more with a better compression algorithm, it's probably not going to be that much.

Most video games that can take a lot of space also come with the majority of their assets compressed nowadays.

Plain text compresses fairly well, but few people have plain text files taking a lot of disk space. Uncompressed image files may also compress well, depending on the image, but few image formats that are commonly used today are uncompressed.

All in all, if there's a lot to be gained by compressing the data, it's probably already compressed.

With that said, my main backup drive that doesn't include operating system or program files, and excludes most video files, has ~130 GB compressed into ~103 GB.

If you're asking with a more practical matter in mind, why not give your practical scenario instead?",hoyukfc,t3_rinkr5,1639775005.0,False
rinkr5,"Theoretically, not depending on the content, nearly 0: https://github.com/ajeetdsouza/pifs",hozk09n,t3_rinkr5,1639785757.0,False
rinkr5,[Shannon's source coding theorem](https://en.m.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) has the answers you seek.,hp0bkje,t3_rinkr5,1639799079.0,False
rinkr5,"What is in the file?  All zeros... the compression will be awesome.  

If it is images or movies, then not so much",hp0joef,t3_rinkr5,1639803658.0,False
rinkr5,"I'm not an expert on data compression but I believe it depends on the contents of the file. How I remember compressions algorithms working is you look for repeating sequences in the file and then substitute those for smaller sequences and have some sort of table to record the switches.

So it your file is 000**111**000**111***0101* you can break that up into 000, **111**, *0101* and say

000 -> 0

111 -> 1

0101 -> 10

So your new file would be: 010110 (plus the overhead for the table)

So if your entire 1 TB file was all 0s you could compress the shit out of it whereas the less repetitions the less it could be compressed.",hoy832a,t3_rinkr5,1639766115.0,False
rinkr5,"Your scheme is broken.  
  
000**111**000**111***0101*  
  
would convert to: 010110, but

010110  
  
would convert to: 000111000111111000, because you don’t know that “10” isn’t a “1” followed by a “0”.  
  
The issue is that you haven’t specified that a “1” cannot be followed by a second “1” and you cannot distinguish a sequence like this:  
  
10  
  
Is it 111000 or 0101 ?  
 
——  
  
A better system might use two bits of starting data
  
00, 01, 10, 11  
  
You’d still have to be careful about anything with an odd number of bits and come up with a way to encode the original data in fewer bits.",hp28q1w,t1_hoy832a,1639844890.0,False
rinkr5,You're right. It's not an actual algorithm; it's to convey the idea of substituting smaller bits for larger sequence of bits,hp2ku83,t1_hp28q1w,1639850205.0,False
rinkr5,Entropy,hoyxg5f,t3_rinkr5,1639776175.0,False
rinkr5,Depends on how random the sequence of bits are. If I had to guess the bound would be log(n) as in the case of a file that stores all 0 bits we could just write the number of bytes.,hp03gmo,t3_rinkr5,1639794827.0,False
rinkr5,"The lossless compression limit depends on the randomness of bits in your data. Let me explain what I mean by randomness of bits. Something is random if no patterns cannot be found in the data. For example, `11111111` is not at all random, it has a pattern which is ""every bit is 1"".

> If the data is redundant, meaning that it has no patterns but a pattern of uniformity, it conveys no information because it has no order. This relation can be stated as follows:
Randomness ∝ predictability ∝ information ∝ order ∝ regularity of data

^ An extract from a paper I wrote about randomness but never published it.

If you had photos with all the pixels of same values. You could compress it to about `1/resolution + bits required to store the length of resolution` size.

Compression is just representing a relatively large amount of data in a concise form. For example `11111111` will be compressed as ""1 for 0100 bits"". If it is a random(explaining what is random, would make the comment humongous so leave it out) pattern, you cannot compress it because you cannot express something random in some other form because you do not know anything about its pattern.

So, now answering all your questions one by one:

> Does anyone know how far a one terabyte file can be compressed? What’s the limit of today’s technology compared to 2000 and 2010?

Not much really because the compression algorithms which are the norms haven't changed much since 2000s, we still use JPEG widely which was originally developed in the late 1980s. We are still using H264 which was originally developed in 2003. H265 was developed in 2013. So compression hasn't changed much in the last 10-20 years.

You can compress an image without loss of data to about 10% with JPEG encoding, but again this is based on the randomness of the pixel values.

> If one terabyte holds 1,000,000,000,000 bytes, what is the utmost limit of compression?

The utmost limit of compression will depend on the data in that 1 TB of data as explained earlier in the comment.

> If data loss will occur, tell me the limit for both. With and without data loss

Well, its in your hands whether data loss will occur or not. You can compress an image to like 50% its size with loss of data but usually the loss of data is not traded for high compressibility. After a compression constant `k`, the compression is directly proportional to the loss of data.",hp0kfoj,t3_rinkr5,1639804104.0,False
rinkr5,"Have you ever heard of 42.zip?

It is a zip bomb, that compressed is only 42kB in size. Fully extracted it is 4.5PB",hp0v372,t3_rinkr5,1639811438.0,False
rinkr5,"Yikes.  
  
Pretty sure those are special *synthetic* sequences that look like a near infinite set of files, though…",hp2aoyr,t1_hp0v372,1639845764.0,False
rinkr5,"It depends on the algorithm you use.  
  
The simplest forms of compression work best on data with lots of *repeated* byte sequences, whereas more sophisticated ones may look to identify patterns that can be converted into a base point and a mathematical function.",hp27tpe,t3_rinkr5,1639844485.0,False
rinkr5,"Image and video encodings already have compression built-in. It's not optimal since people don't want to wait two seconds to decompress one second worth of video, so you can probably do a bit better. If you have an equal number of txt, jpg, and mp4 files, the videos would dominate the storage.",hoynxsl,t3_rinkr5,1639772336.0,False
rinkr5,"Strictly speaking that’s *encoding* rather than *compression*, but it does reduce the storage space needed.  
  
You can potentially stlill compress the result, also.",hp2aw45,t1_hoynxsl,1639845852.0,False
rinkr5,"For music, lossless compression is about 50%.  Lossy can do more depending on how much you want to notice the compression. Video is likely similar. Text based compression depends totally on the text. If a lot of repeated character patterns exist, you can achieve a lot of lossless compression. Random characters get very little compression.",hozg1oe,t3_rinkr5,1639783950.0,False
rinkr5,"1000000000000 of the same content

log2(1000000000000) < 40

so you need 40 bits of multiplier and 1 bit of the data.

or up to 99.9999999959%  for single time compressing.",hozy7ez,t3_rinkr5,1639792308.0,False
rinkr5,"http://mattmahoney.net/dc/

Probably the best resource on the web I've seen for this.",hp12k6u,t3_rinkr5,1639817548.0,False
rihkz0,"Before you dive into software architecture, I'd recommend working on becoming a great software developer first. The book gets a lot of slack, but for young developers I still recommend Clean Code by Bob Martin. Once you've completed (or better yet, while!) some introductory background reading, start implementing the principles discussed in a language you're comfortable with. Small, simple examples with easy to understand conventions will go a long way. Next, take those principles and use them in an existing architecture - MVC is a great one for Java.

From there, it's good to start working on how you organize your software. For this I like to recommend A Philosophy of Software Design (2018) by John Ousterhout for a supplemental approach to the ""historical knowledge"" as well as studying design patterns. [Game Programming Patterns](https://gameprogrammingpatterns.com/contents.html) provides a great overview of some of the most common design patterns you'll see in software through an application that's very approachable through most - game design. Design patterns are great to know because if you can employ and identify them properly, it will be easier for others to understand what your code is doing and vice versa. This is also the point where you start incorporating those principles you learned above into larger projects. 

Finally, to answer your exact question of ""how to understand large and scalable projects,"" [The Architecture of Open Source Applications](http://aosabook.org/en/index.html) is what you're looking for. Practice taking a problem, designing a solution for it, and comparing against what the author implemented. But work on the above first. 

This is a lot of information, so save this post and come back to it. Software development is a lifelong and ever changing endeavour, so enjoy the journey!",hoxgzd8,t3_rihkz0,1639755753.0,False
rihkz0,Awesome! Thank you for the detailed response. I've been doing a lot of JFX work at my job recently so I've been interested in building my own software on free time and the hardest thing I'm finding is how to start and not have to restart later on due to design. MVC I'm already familiar with bur always interested in diving deeper and learning more! Thanks again.,hoxhdpv,t1_hoxgzd8,1639755908.0,True
rihkz0,"I don't want to sound trite or demeaning but architects  are made from experience and mentorship. there are lots of books about architecture and you should read many of them. Ultimately architecture is balancing tradeoffs, technological, resources and political.  Sometimes it's winning hearts and minds, sometimes it's hearing cats , sometimes it's leading brilliant people who are honestly probably smarter than you.",hoxohf9,t3_rihkz0,1639758635.0,False
rihkz0,"Fundamentals of Software Architecture
Software Architecture: The Hard Parts

By Neal Ford and Mark Richards.  And their bi-weekly conversation at https://www.developertoarchitect.com/foundations-friday-forum.html 

Solid advice in those two books.  And echoing the thought that architecture is a skill developed by experience, they hold a twice-yearly “Kata contest” where they present an architectural problem and have teams compete over the course of several weeks to produce and document the best architecture.  I’m biased, because I’m one of 5 judges for that contest.",hoxv985,t3_rihkz0,1639761191.0,False
rihkz0,"This is very interesting! I actually have both these books on my reading list and can't wait to get around to them. 

Do you mind giving more details on how the Kata is judged? I'd be interesting in taking part.",hozding,t1_hoxv985,1639782843.0,False
rihkz0,Honestly you don't learn those from books. You learn from experience... Just actually do projects from smaller ones to bigger ones.,hox9bdw,t3_rihkz0,1639752643.0,False
rihkz0,"Learn to design in UML and maintain documentation for your software, free code camp has an excellent UML intro on YouTube.",hoxu64d,t3_rihkz0,1639760779.0,False
rip3jw,"What is your question, precisely?",hoytzez,t3_rip3jw,1639774772.0,False
rip3jw,sees like only problem of the imaginary program P that can solve halting problem is self reference( i think that is where loops come from in my original post).{can’t we just say program P will solve halting problem for all programs except itself* .what is wrong in saying that instead of just saying since loops can’t exist this program won’t work outside self-referencing as well and no such program exists.,hoyvglr,t1_hoytzez,1639775363.0,True
rip3jw,"Well I think the idea was to solve the halting problem, and no program can solve it completely, hard time understanding your point here.",hoyz3y0,t1_hoyvglr,1639776848.0,False
rip3jw,"I know, I don’t quite know how to put it. my assertion is that,as said above, the only proof given is refusal of any halting solution is that it breaks down in self referencing. How does that say anything about literally infinite other programs that our supposed solution will work on. I mean all we have done is refute a very specific program. And we still aren’t sure about others. So can’t we say there may or may not be a partial solution( partial because self reference is not permitted) instead of declaring outrightly that nothing of the sort exists.",hoz0n9k,t1_hoyz3y0,1639777473.0,True
rip3jw,"It says that your program will never be 100% correct.

More precisely, it raises the question of how your attempt at solving the halting problem would cope when fed itself.",hoz6uqx,t1_hoz0n9k,1639780038.0,False
rip3jw,">  I mean all we have done is refute a very specific program. And we still aren’t sure about others.

Sure. The proof does not identify any specific program that every attempt at halting analysis will get wrong. There is no such program.

We *know* there are partial solutions. It's easy to write an analyzer that gives the right answer for some programs, though it will give the wrong answer (or no answer) for other programs. You can also pick any program you like and write an analyzer that gives the right answer for that specific program (either `print(""yes"")` or `print(""no"")` will be a suitable answer, though it might take you a lot of thinking to figure out which one).",hozouxl,t1_hoz0n9k,1639787956.0,False
rip3jw,"I've thought about the same thing, and I actually believe that's the case.

We do know that if you don't allow recursion in any way, then it's completely decidable.  Thus, it's something about recursion that causes the issue.

However, recursion is _not_ sufficient to cause undecidability.  It's necessary, but some recursive programs are decidable.

My theory that has yet to be disproven:  if we disallow the known program that causes undecidability, it becomes decidable.  It can't decide on itself, but it can on anything else.",hp02zii,t1_hoyvglr,1639794582.0,False
ri2yda,"Yes, it's trivial. The vulnerability is well over a decade old.",houjfs2,t3_ri2yda,1639698486.0,False
ri2yda,Could you please elaborate how? I have seen references to this online but I can't find a clear explanation of how the ciphertexts can be modified/swapped.,houjv9a,t1_houjfs2,1639698675.0,True
ri2yda,Well it's not swapped per se. You de-auth attack and collect the IVs by the thousands. it's been a while so I apologize going off memory. Like I said old attack. Check out a tool call Air-crack-ng if it's still around.,houm13o,t1_houjv9a,1639699638.0,False
ri2yda,[deleted],houq2oj,t1_houm13o,1639701488.0,False
ri2yda,[deleted],houqc5m,t1_houq2oj,1639701608.0,False
ri2yda,Yep WEP is trivially cracked. SSL also has it weaknesses in older versions but as long as the TLS is higher than 1.0 and SSL is higher than 3.0  it's pretty pretty good shape :),houry45,t1_houqc5m,1639702329.0,False
ri2yda,"Here's a good pdf with all the information you need to understand why WEP is broken. 

https://www.opus1.com/www/whitepapers/whatswrongwithwep.pdf",hovw5vv,t1_houjv9a,1639721710.0,False
ri2yda,Fond memories of moving into a new flat and borrowing neighbour’s WEP-protected WiFi until my broadband was activated,hoydzmd,t3_ri2yda,1639768380.0,False
ri2yda,There was a lot of hub bub about that song but I didn't really find it that vulgar.,hovvzyn,t3_ri2yda,1639721604.0,False
ri2yda,You do realise WEP is shit/old/insecure and hasn't been recommended for over a decade now?,howf8gs,t3_ri2yda,1639735542.0,False
ri2yda,"Yes, I'm learning about all the different wireless security protocols",howfh6x,t1_howf8gs,1639735733.0,True
ri2yda,"Ah, fair enough.",howi8sz,t1_howfh6x,1639737834.0,False
ri6duj,"O(n^2), as you suspected, assuming no ordering on the array's elements. That's quadratic time; exponential should be O(2^n) or similar. Check the difference for n = 10, 20, 30.

If the array is ordered, I think that a O(n) algorithm is possible: map b_i = target - a_i, reverse b, then compare a and b side-by-side until the elements match.",hov5oyf,t3_ri6duj,1639708494.0,False
ri6duj,"Oh, yea sorry I meant quadratic, not exponential.

That makes sense. In this case, the array wasn't ordered.",hov6o73,t1_hov5oyf,1639708933.0,True
ri6duj,"The commenter on YouTube might have been tricked into thinking it's in O(n) because the inner loop runs only for the remainder of the array after the *i*th element rather than always going through the entire array, and that remainder gets shorter for every run of the outer loop.

However, the inner loop still makes n-1 iterations on the first iteration of the outer loop (when i=0), and one iteration on its last run (when i=len(arr)-1), leading to an average of circa n/2 iterations of the inner loop per iteration of the outer loop. Since the outer loop is executed n times, that leads to a total of approximately n * n/2 iterations of the inner loop, which is in O( n^2 ), not in O(n).

That's in the worst case, of course; if matching elements are found, the algorithm stops before going through all of that.

There's probably an off-by-one or some other inaccuracy somewhere in the above, but the big picture should still be right.",howp4yq,t3_ri6duj,1639742595.0,False
rhll1c,"How many bytes a character has depends on the character table you use. Some examples: ASCII (1byte per char), UTF-8, UTF-16, the thing that microsoft uses.

Of course, a space is a character itself. Also a backspace or enter is.

Any you are right, for all praktical cases 1 byte equals 8 bit.",hor7nbs,t3_rhll1c,1639640716.0,False
rhll1c,"Out of curiosity, do you know what character table is used within text messages? Also, do you know what character table is used within email messages? I’m trying to understand if both of these messages carry the same data if they both sent the same message. Let’s say that you messaged, “Hello there!” You emailed and texted this exact message. Would they both be sending the exact same byte size? If not, what’s the difference. 

Also, do spaces count as a character and or byte? 

Lastly, you said enter counts as a byte? Meaning, if you were to send a message that was 36 bytes and the limit was 36, you wouldn’t be able to send the message because the enter counts as a byte?",hor805a,t1_hor7nbs,1639640980.0,True
rhll1c,"Email usually uses UTF-8. Like 99% of all web applications. I would guess that most messenger also uses UTF-8, maybe also UTF-16 scince this supports for example chinese character.",horbeki,t1_hor805a,1639643631.0,False
rhll1c,"Just to clarify, UTF-8 and UTF-16 (and UTF-32) are all different ways of encoding the same characters and all sort the same set of characters (eg Chinese). For most common use cases, UTF-8 encodes a message in the fewest bytes out of them but requires the most processing to handle non-ASCII characters. UTF-32 required the last processing but uses four times as much memory as UTF-8 for ASCII characters.",hotzndh,t1_horbeki,1639690188.0,False
rhll1c,"To answer the last part of your question a bit more in depth.

If you are using a common messenger, you typically press enter to send the message, and if you want a new line you have to press shift+enter. In a email, you can just press enter to get a new line and have to klick on a button to send the email.

This tells us, that a messenger captures the enter key and uses it as a ""command"" to send the text, while the email programm captures the enter key and uses it to append a new line to the text.

So the same key can do different things on different programms. This is because the programm, and not the keyboard, decides what to do with the pressed key. Because of this you also can easily switch between different keyboard layouts without replacing the keyboard.

To answer the question, the enter is not appended to the message. The reciever knows that you send the text because he recieves the text, so there is no need to put an extra indicator at the end of the message.",horj9tq,t1_hor805a,1639650167.0,False
rhll1c,"Edit: There is more to transmitting a message than just the text. What metadata is send highly depends on the way you send it. So I would guess that a email  transmitts more data compared to a messenger.

You can see the data transmitted for an email in almost any email desktop client. There should be a option to ""view source code"".

Edit2: Space count as character. Also newline, tab and all other, so called ""printable characters"". Besides those, there are also ""non-printable caharacters"" like enter or backspace you could theoretically send to someone.

Edit3: If you send a message you do not send the enter. The programm does not append it to the message, the enter is catched by the programm and than the message is send. But you wount be able to append a new line to the message if no bytes where left.",horid8j,t1_hor805a,1639649418.0,False
rhll1c,"There is more to sending an email? Well, let’s say that both the email message and text message contain the exact amount of bytes. In this scenario, I’ll say both contain 36 bytes. If that’s true and the transmission signal adds a certain length of extra data, how great would the difference be? If you know the question, that’d help a lot. 

Also, people are saying you can change the encoding for an email. How would I go about choosing an encoding that is strictly one byte per character? Or is that already the preset encoding software? As for the email, let’s say I’m using the google email or hotmail. 

Space counts as a character? I see. That makes sense. Does a space contain one byte like every other character? Or since it is blank, is it lowered to lesser bits? 

Enter doesn’t count count as a byte? How about as data? Since it’s caught by the program and then sent out, surely it contains wavelength data of a sort. Any idea of the specific amount?

If you create a new line, does the combination of shift+enter equal two bytes if you are utilizing a encoding that equals 8 bits per character. Thus one byte a character. For practical reasons, let’s say that there was only one space skipped on the last line, meaning, it would be 3 bytes now. Right? Or does skipping a line not take enter+shift as a double character command? Merely just a single command since it is utilizing a single command to create a single character. If you know what I mean. That’s for the phone data.

Pertaining to the computer, you said I’d only have to click enter to skip a line. Does the enter count as a byte if you are creating a new line?",hotc918,t1_horid8j,1639681029.0,True
rhll1c,"It is possible to write a message in English that only uses one bye per character. The ASCII, Latin-1 and UTF-8 encodings will all do this. It is not possible to write a message in Chinese that uses one byte per character (assuming an 8-bit byte). This is because one bye can represent 256 Disney numbers. So you can encodes the English alphabet by saying A=65, B=66, C=67 and so on. Then a=97, b=98, c=99 and so on. But there are more than 256 Chinese characters and so it will necessarily take more than one bye per character to encode a message in Chinese.

There are two characters related to new lines in common encodings, called Line Feed and Carriage Return (LF and CR).  These names come from mechanical printers which had to be told separately to move the paper up a line (LF) and to move the print head back to the start of the line (CR).  There are several conventions on how these characters are used to construct new lines in different bits of software. Some use only a CR at the end of each line.  Some use only a LF.  Some use both CR+LF.  CR and LF are both part of the ASCII character set and have values 10 and 13.

Every character in the ASCII art takes the same number of bits to represent.

Note that some older email clients used 7-bit ASCII to represent emails. Since you can represent 32 non-printing characters, upper and lowercase letters, numbers and a selection of punctuation in 128 characters, you only need 7 bits to assign a unique number to reach character. Back when sending bits was expensive, it made sense to only send seven bits for each character.",hou2bxw,t1_hotc918,1639691247.0,False
rhll1c,"It is safe to assume that a byte is 8 bits, though in the past that wasn't always the case.

How a character is represented in data depends entirely on its *encoding.*

SMS text messages use either a 7 bit or 16 bit encoding, so either one or two bytes per character.

Email messages can be sent in HTML format, which permits any coding the sender and receiver can both handle.  For example, UTF-8 is one to four bytes per character.",hordd9v,t3_rhll1c,1639645215.0,False
rhll1c,"That wasn't always the case? If that’s true, how many bits used to be a byte? Any idea? Also, do you know what year a byte become 8 bits? 

So, the character H and the Character 7 would equal the same amount of bytes? Which is one or two depending on the encoding? 

SMS is either one or two bytes per character? That’s helpful. That is the biggest question of mine. I’ll have to see if I can find a definitive answer to that. 

An email message can contain a single byte per character? That’s 100% possible? Secondly, does clicking the enter button count as a byte? Let’s say 36 bytes is the max. You type a message with 36 bytes and then click enter, but it doesn’t send because the enter counts as a byte? Is that a thing or no?",hore9kn,t1_hordd9v,1639645975.0,True
rhll1c,"> So, the character H and the Character 7 would equal the same amount of bytes? Which is one or two depending on the encoding?

Some encodings are fixed-length, i.e. all characters are encoded with the same number of bits. This is how many common 8-bit text encodings worked in the past: every character was exactly 8 bits. That also set an obvious limit on the maximum number of unique characters that could be represented, as there are 2^8 = 256 unique combinations of 8 bits that are possible.

Other encodings are variable-length, and a single character can take one or more bytes to represent. The most common text encoding on the web is UTF-8 where every character takes between 1 and 4 bytes. The most common characters in English text (the ones that are included in ASCII, which is an old character encoding standard) take up 8 bits, or one byte. This would include the English alphabet, digits 0 to 9, and a number of other characters, but not many non-English letters. This allows the encoding to be compatible with the old ASCII standard. There are more than a million other characters that are possible in UTF-8, but they can take up two to four bytes per character.

So, even when using a variable-length encoding, H and 7 are still likely to take up the same number of bytes, but H, ü and 国 can take up different numbers of bytes.

I don't know about the text encoding used in SMS specifically, but as u/CarlGustav2 said, it seems like there are two possible encodings, one of which is a fixed-length 7-bit encoding, with the other one being a fixed-length 16-bit one.

Edit: clarified choice of words",host0b2,t1_hore9kn,1639673588.0,False
rhll1c,"> An email message can contain a single byte per character? That’s 100% possible?

Yes, if it's using a fixed-length 8-bit (or 7-bit) encoding. A 7-bit encoding would allow basic English text but no international characters; an 8-bit encoding such as [ISO 8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) would allow some non-English characters but the set of characters would depend on the encoding, as no 8-bit encoding can have enough unique combinations of 8 bits to represent letters used in all languages. (Some languages such as Chinese or Japanese of course have thousands of characters all by themselves, so they wouldn't fit in *any* 8-bit encoding even on their own.)

If you want to know whether an email message consisting of 100 written characters can actually fit in 100 bytes, it's worth noting that email messages also include various control information in so-called headers, including the sender and recipient, the text encoding used, and various other things, so a full email message is actually going to take up more space than that.",hosvs05,t1_hore9kn,1639674660.0,False
rhll1c,"It might help to understand why we need different encodings. Note that a byte (8 bits) can represent 2^8 = 256 different characters. Now this is clearly enough to represent the English language. The most common 1-byte encoding is usually ASCII. Just Google ""ASCII table"" to see the encoding. On the other hand, what if you want to represent almost all languages? You will need more bits to make that work. If you instead use a two byte encoding then you can represent 2^16 = 65,536 characters. Now this is enough to represent pretty much all characters you'll need globally. 

So essentially, if the application you're using allows characters other than English then it's probably using 2-byte encoding. Single byte encoding was mostly used back before the whole world had internet and every language needed to be encoded. 

For your last question, yes, enter is going to be a newline character (or possibly two characters if you're on windows). To you when you click enter it looks like nothing there, but realize that the reason your cursor moves to the next line is because the text editor is showing you the message being typed, and the only way anything changes on the screen is if a new character is entered. This character has a special encoding that the text editor understands, and in turn it pushes the cursor to the next line when it sees this character.",hosc7td,t1_hore9kn,1639666885.0,False
rhll1c,"I could technically set an email message to utilize ASCll? That’s correct, right? 

If enter counts as a byte, does the enter still count if it is clicked to send a message? Similar to the send button. From other comments, people says it is more of a message catcher that sends the data off. Meaning, it isn’t a byte. I haven’t gotten replies if it is still data compressed into the sent message though.",hotfvmk,t1_hosc7td,1639682460.0,True
rhll1c,"It sounds like you have a specific use case in mind. Perhaps if you explained why you want to send emails and texts at 1 byte per char, we might be able to help you better, rather than all he possible byte/character encoding permutations?",hoxcvq0,t3_rhll1c,1639754114.0,False
rhll1c,"Sms messages tend to use GSM 7 bit encoding (https://docs.huihoo.com/symbian/s60-5th-edition-cpp-developers-library-v2.1/GUID-35228542-8C95-4849-A73F-2B4F082F0C44/sdk/doc_source/guide/System-Libraries-subsystem-guide/CharacterConversion/SMSEncodingConverters/SMSEncodingTypes.html), although they can vary.

Emails are even more variable, you can set what encoding you want, but UTF8 is typical as a default. Spaces are indeed characters.

The amount of bytes per character will vary by compression.  In a plain text doc, one character will equate to 1 byte, but in a pdf, one byte will give you something like 3 characters.

Which is to say as so many things in programming,  for all your questions the real answer is it depends, there is not a single correct response.",hordmlp,t3_rhll1c,1639645431.0,False
rhll1c,"So, an IPHONE would utilize GSM 7 bit encoding? Do all phones utilizing GSM 7? Does GSM contain 7 bits per character? I know 8 bits is a byte, meaning, each character would be slightly shorter than the typical byte, right? Are all the characters set at a certain byte/bit length? Like one byte or 7 bits per character? 

For an email, how would you set a certain encoding? For example, how would you choose an encoding that is strictly one byte per character? Where is the option or setting to enable such a thing? Is the default one byte per character? What is typically the default and how many bytes per character is the average? 

A plain document, a character equals one byte? Though, a pdf would equal three characters? Is that one percent true? If it is, couldn’t you technically compress a message into a pdf and send it as such?",hotewbb,t1_hordmlp,1639682070.0,True
rhll1c,"Your assuming a character takes up 1 byte - while it’s the case a lot of times, a lot of newer encodings are a lot larger, like utf-16 and 32, which take up 2 or 4 bytes respectively",hosenhe,t3_rhll1c,1639667905.0,False
rhll1c,"Which encodings take up one byte per character nowadays then? It seems limiting to prevent one character from sending as a single byte. If a character was sent as a single byte, you’d be able to send more data due to the limit of text being higher.",hotf8db,t1_hosenhe,1639682203.0,True
rhll1c,"Ascii, utf-8. Yes certain encodings work better if you aren’t going to use certain characters",hou3atx,t1_hotf8db,1639691633.0,False
rhll1c,"If I were to only use basic characters, which encoding should be utilized? Strictly one byte per every one character? Is this encoding available via email format or texting format? 

If newer phones aren’t capable, how about older phones? The flip phones and what not? 

As for email, is it capable? 

Any idea how many bytes an emoji is? Not that it matters, however, I am curious.",hou521f,t1_hou3atx,1639692345.0,True
rhll1c,"Actually UTF-8 is variable width that uses 1 to 4 bytes per character, depending on the character.",hounm2f,t1_hou3atx,1639700361.0,False
rhll1c,"What encodings are still 100% fixed. Also, which encodings are fixed at 8 bits per character? 

For UTF, how would you compress and keep the byte size at 8 bits per character, strictly. Which encoding keeps everything set at 8 bits per character? The ASCll encoding takes up 8 bits and or one byte per character? That’s 100% preset? It’s impossible to increase the byte size per character if you were utilizing ASCll? 

Let’s say you were sending the message via electromagnetic waves as a radio wave, if the wave became energized due to an outside influence, would the data become larger or more compressed? Or would nothing happen? Or if it was too powerful, would it just not send due to it acting as an EMP/jammer? 

While utilizing UTF-8, it’s impossible to compress anything to one byte? It’s all set between 2 to 4 bytes? One byte seems limiting, however, doesn’t that allow for larger messages to be sent if the message limit is a certain number. It seems more useful than the higher grade encodings.",hotdncj,t3_rhll1c,1639681577.0,True
rhll1c,"ASCll, Latin-1, and UTF-8 are capable of utilizing one byte per character? For all characters, strictly? Aside from non-English characters, correct? Spacing, punctuation, and other sorts of symbols would still be one byte though, right? 

Emojis don’t really matter, however, I might as well ask anyways. How many bytes per emoji? 

Older email clients used to send 7-bit? That means it would be a little less than 8-bit, right? Meaning, it is 1 bit less thus it isn’t a byte unless you send another character? If this is correct, what email clients were those? Any idea? 

Also, how many bits and bytes are a character in newer email format? Is it possible to have one byte per character when writing an email? 

When you click the send button, does the send signal become added data on a wavelength?",hou49yj,t3_rhll1c,1639692024.0,True
rhll1c,Be aware that email transmission includes message headers that are not part of the message that included details such as routing information. You can read more about message headers [here](https://jkorpela.fi/headers.html),houq5qf,t3_rhll1c,1639701526.0,False
rhll1c,look up an ascii table. they have the 8 bits in all forms and shows the 256 corresponding characters,hov4o2v,t3_rhll1c,1639708036.0,False
rhll1c,In addition to what everybody else said have a look at [https://en.wikipedia.org/wiki/Quoted-printable](https://en.wikipedia.org/wiki/Quoted-printable). It encodes 8bit characters into 7bit characters but needs more of them. It is still used for email. Maybe not often but every email program needs to be able to interpret it.,howc2oi,t3_rhll1c,1639733052.0,False
rhll1c,"**[Quoted-printable](https://en.wikipedia.org/wiki/Quoted-printable)** 
 
 >Quoted-Printable, or QP encoding, is a binary-to-text encoding system using printable ASCII characters (alphanumeric and the equals sign =) to transmit 8-bit data over a 7-bit data path or, generally, over a medium which is not 8-bit clean. Historically, because of the wide range of systems and protocols that could be used to transfer messages, e-mail was often assumed to be non-8-bit-clean – however, modern SMTP servers are in most cases 8-bit clean and support 8BITMIME extension. It can also be used with data that contains non-permitted octets or line lengths exceeding SMTP limits. It is defined as a MIME content transfer encoding for use in e-mail.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",howc3kr,t1_howc2oi,1639733073.0,False
rh4amb,Isnt C++ is more strongly typed than C ?,hoo7w0f,t3_rh4amb,1639591550.0,False
rh4amb,"I don't know whether it's more strongly typed, but C++ is infinitely more dynamically typed than C.

C does not have dynamic types at all. The type of everything is determined at compile time.

In C++, you have polymorphism.",hooywmy,t1_hoo7w0f,1639601977.0,False
rh4amb,"C++ is a static strongly typed language at its core. 
But since it is the swiss army knife of programming languages, it does support a typed dynamic dispatch (runtime polymorphism)  that is more of dynamic typing feature. 
C language doesnt offer( afaik) any dynamic dispatch mechanism in the language. But since it is THE portable assembler, we can implement any dispatch scheme using the language. But the language still stays statically typed, though compiler is very forgiving when it comes to enforcing types ( weak typing)",hop0z4u,t1_hooywmy,1639602776.0,False
rh4amb,Isn't void* a passe-partout in C/C++ ? Like I can give void* as param to a function then pass a int* cast as void*,horxl27,t1_hooywmy,1639659902.0,False
rh4amb,"But then a ""conversion"" explicitly happens, which is what converts the value to a different type.

C's syntax just allows this conversion to happen without additional syntax. The conversion is explicit in the language's semantics. The conversion is explicit in the type-annotated AST.

All statically typed languages have conversions between types, casts are nothing special.",hos0h8r,t1_horxl27,1639661448.0,False
rh4amb,"Yes, you can write C++ such that it's very strongly typed. But also, it inherits the exact same features that C has that make it weakly typed. 

I guess this graph is a kind of lowest-common-denominator kind of thing.",hoosgls,t1_hoo7w0f,1639599478.0,False
rh4amb,Maybe they mean like with inheritance you can cast an object as it's parent? Or type casting functions (not sure if C-structs offer that)?,hooshf8,t1_hoo7w0f,1639599487.0,False
rh4amb,"Is the reason C++ is classified as ""Weak"" typing because you can, for instance, read the bytes of an `int` as a string? In which case can you not do that in Java as well?",hooc5f2,t3_rh4amb,1639593216.0,False
rh4amb,"In Java an ```int``` is a primitive whereas a ```String``` is an object. However you could have an ```Integer``` and a ```String``` where both are sub-types of ```Object```.  
  
You could convert an object into bytes, but you can't just pretend you have bytes. Whereas with C on the other hand, types are more of wrapper around bytes where an ""int"" is 4 bytes/32-bits and an interpretation of them and a C-string is a null-terminated sequence of bytes (last byte is '\0') which will be be interpreted as belonging to a set of characters.",hop5cnb,t1_hooc5f2,1639604464.0,False
rh4amb,"No, you can't in Java. At least not accidentally. And if you do so on purpose, you do not have undefined behavior.",hooyqr7,t1_hooc5f2,1639601913.0,False
rh4amb,"I think the closest thing you will find in ""serious computer science"" is the lambda cube https://en.m.wikipedia.org/wiki/Lambda_cube

I personally have not seen much theoretical study on totality of a real world programming language, as they are way to complex to reason about. And with such a complex system it is very hard to define the notion of ""stronger type systems"".

Not to mention many system has unique type systems like linear type, refinement type, semantics type, gradual type, and cubical type, which are hard to characterize into a structure.

Lambda cube is one of the more comprehensive summary of type systems inspired by intuitionistic logic.",hop85p6,t3_rh4amb,1639605557.0,False
rh4amb,"I think this is the answer I've been looking for, thankyou very much king",hopj5vd,t1_hop85p6,1639610104.0,True
rh4amb,Why the heck is F# weaker than C#? The whole point of F# is the strong type system.,hooexq6,t3_rh4amb,1639594287.0,False
rh4amb,"To be fair this is just the first example I found, probably not the best",hoohtib,t1_hooexq6,1639595399.0,True
rh4amb,I think these are only placed in the correct quadrant,hoprjvf,t1_hooexq6,1639613803.0,False
rh4amb,You are missing an axis: explicit to inferred.,hop38hs,t3_rh4amb,1639603637.0,False
rh4amb,Could someone explain me why c++ is weakly typed?,hopd9pb,t3_rh4amb,1639607633.0,False
rh4amb,Things are very often implicitly converted (thing passing an int to a function that uses a float and vice versa),hopj3wg,t1_hopd9pb,1639610081.0,True
rh4amb,"Thats inherited from C, in fact C++ disables some of the  implicit conversions from C (eg ""string literal"" to char*). So it doesn't make sense for C++ to be more weakly typed.",hoq7qvw,t1_hopj3wg,1639621109.0,False
rh4amb,"Thanks, i didn’t know that being weakly typed is it about implicit conversions, do explicit conversion have anything to do with it as well?",horo10c,t1_hopj3wg,1639653915.0,False
rh4amb,"So where did you find the graphic? Why would it have to in a paper/book? You can also cite non-traditional sources, especially in CS",hoo3w7m,t3_rh4amb,1639590025.0,False
rh4amb,"I suppose it would be fine to cite where I found it, I was just wondering if there was any academic paper which I could cite instead (better)",hooc5mu,t1_hoo3w7m,1639593219.0,True
rh4amb,"Academic papers for software engineering concepts like this are usually garbage, you will find better and more reliable sources in company white papers and textbooks. The academic publishing for compsci is very good, but *software engineering* academia is dominated by people who couldn't hack it in industry *or* rigorous compsci research, and pump out very low quality papers.",hopsgts,t1_hooc5mu,1639614214.0,False
rh4amb,"I think this topic falls more in the realm of programming language theory than software engineering. And theory of programming languages is a rich area of academic research; heck, you could reasonably argue that academia has driven a lot of the innovation in programming languages.",hoqc5tm,t1_hopsgts,1639623047.0,False
rh4amb,"> Why would it have to in a paper/book?

I guess the general idea is that a paper or book would have put much more thought into the graph than random blog, and so it'd be more ""accurate"" and sourced etc.",hoosn1a,t1_hoo3w7m,1639599547.0,False
rh4amb,Ask over at r/programminglanguages,hoozvs7,t3_rh4amb,1639602355.0,False
rh4amb,"This ""infographic"" would seem to be a summation of opinion rather than researched fact - many languages have been plopped in places that make no sense (but then the axes are also seemingly arbitrary words) - I wonder whose agenda it suits?",hopiqoe,t3_rh4amb,1639609924.0,False
rh4amb,OP makes thread to ask for academic papers.  People instead critique the example given.   lol,hoqlet5,t1_hopiqoe,1639627290.0,False
rh4amb,"I think this could actually be proven with some simple code where implicit conversions could be done as tests (could definitely have more tests), as for the metrics I'm not really sure how you would lean more to one side.",hopv6z4,t1_hopiqoe,1639615464.0,False
rh4amb,They misspelled Haskell lol,hordnc0,t3_rh4amb,1639645448.0,False
rh4amb,im confused as to why C++ is towards weak instead of strong,hos0r15,t3_rh4amb,1639661586.0,False
rh4amb,how the fuck is C more weakly typed than python?,hoolprs,t3_rh4amb,1639596891.0,False
rh4amb,"It all wholly depends on how you define ""strong"" and ""weak"" and how you rank languages against each other. Maybe strength refers to different relative rankings of type safety or perhaps memory safety, or maybe how strict the static or dynamic type checking rules are. Python, for instance, has pretty strong runtime type checking rules. It's very often going to throw an error rather than try implicit conversion.",hoos0jq,t1_hoolprs,1639599304.0,False
rh4amb,"Yeah by my understanding, it's considered ""strong/dynamic"" because each object x has a unique well-defined type type(x) at runtime that doesn't get implicitly converted to other types. Contrast this to C, where there's automatic integer promotion between distinct types, array/pointer conversion, and the ability to cast any kind of pointer to any other kind of pointer; there's a lot of ways to treat one type of data as if it were another. At the extreme, the weakest type system would be no types at all, like some assembly code where everything is just machine words/bytes.

Python is also contrasted with something like JS, which will do whatever it can to produce a result (adding ints to strings and the like), even if the oparands are nonsensical.",hopdx8b,t1_hoos0jq,1639607903.0,False
rh4amb,Which is why a diagram like this is useless without an explanatory text.,hop55am,t1_hoos0jq,1639604384.0,False
rh4amb,Agreed,hop6cph,t1_hop55am,1639604851.0,False
rh4amb,"Because it clearly is. You can do whatever you want with a value in C. C is nearly the perfect example of a statically, weakly typed language.",hoptu6q,t1_hoolprs,1639614842.0,False
rh4amb,"OP have you studied type systems? https://en.wikipedia.org/wiki/Type_system  That field has to quantify how strongly typed a language is for it to work, so there is probably something floating around in the neck of the woods you're looking for.  There may be some papers on the topic.

edit: https://en.wikipedia.org/wiki/Comparison_of_programming_languages_by_type_system  This was found in the type system link above, so there is clearly a path to this topic from type system.",hor1e33,t3_rh4amb,1639636262.0,False
rh4amb,"I have the perfect solution for this. Hope I’m not too late:

[ignore the highlights ](https://imgur.com/a/8QKi1VJ)

While that photo isn’t specifically what you’re after the whole paper will leave nothing uncertain: 

[Programming Paradigms for Dummies](https://www.info.ucl.ac.be/~pvr/VanRoyChapter.pdf)

The name is misleading its most definitely **not** for dummies",hor4f2x,t3_rh4amb,1639638346.0,False
rh4amb,Make sure to include Go. I hear only good things about the operating systems minus being developed by Google. But who cares! It's a great language.,hor6mhw,t3_rh4amb,1639639946.0,False
rh4amb,[removed],hop5nq9,t3_rh4amb,1639604584.0,False
rh4amb,Sorry what?,hop6v84,t1_hop5nq9,1639605051.0,True
rh4amb,A je to iz prosojnic od funkcijskega programiranja profesorja Bosnica?,hop7qa2,t3_rh4amb,1639605388.0,False
rh4amb,scala FTW!,hopz3ry,t3_rh4amb,1639617245.0,False
rhhyf3,"The short answer is downloading files does not wear out a computer, the wires, memory, etc. After all, normal internet browsing downloads dozens or even hundreds of files per page.

The caveat is that solid state memory devices (SSD, or tablets, phones etc) can wear out, but it would take extreme usage over a period of time to trigger it.  By extreme usage, I mean something like downloading your 1000GB gdrive/onedrive, and as soon as it finished, deleting your local copy and repeating.

&#x200B;

I'm sure others will provide more details/corrections.",hoqtzx5,t3_rhhyf3,1639631809.0,False
rhhyf3,"At the most basic level, files are stored as bits, which are represented by states of flip flops. Everything above that is an abstraction. If you download a file that somehow runs itself and contains code to run a heavy workload and turns off the fans, it can destroy your computer. Not instantly, but overtime. A file can imprint itself but that depends on what kind of device it is stored on. It isn’t a property of file, rather the property of the device. For eg: PROM is a programmable ROM that can only be programmed once, while EPROM is Erasable PROM, so shine a UV light and the data disappears. EEPROM is electical EPROM, so data can be erased by passing an electric current. Electrical engineering has a lot of things like this. Especially hardware design and verification. Cool stuff.",hora8h7,t3_rhhyf3,1639642718.0,False
rhhyf3,Thank you,horwno3,t1_hora8h7,1639659388.0,True
rhhyf3,"Transferring or processing data doesn't cause any meaningful wear. Writing the data on a storage device may cause wear but usually not enough to matter in most cases.

Electronics, such as the wires or the computer's CPU or RAM memory, don't really suffer physical wear from processing data. Heavy use of components such as the CPU can cause the component to heat up, and repeated heating and cooling cycles may technically shorten the lifespan of the component. However, that would typically only really make a difference on a time span of decades. That's much longer than you're going to be using a CPU or any other computer component in any everyday use.

Downloading files also doesn't really cause heavy load on the CPU or other components in a computer, so downloading data probably wouldn't make even that kind of a difference.

As for storage devices, that depends on the technology used in the storage drive. SSDs do have a limit on the number of times each memory cell can be written to, so frequently writing a lot of new data on an SSD does theoretically shorten its lifespan. How much data you have on an SSD doesn't matter; what matters is how many gigabytes of new data you keep writing.

However, you can typically overwrite each cell on an SSD thousands of times. As long as there's free memory space available, the SSD also automatically tries to spread the data writes evenly over the free physical flash memory so that the wear wouldn't all be on the same cells even if you keep overwriting the same files. You probably can't wear out a modern SSD with any kind of normal use even if you download or otherwise write lots of large files on it. SSD wear might matter in heavy constant use, such as perhaps in a data center, but it doesn't really matter in desktop use.

Mechanical hard drives are a little different. They are, well, mechanical, and they suffer physical wear over time. A hard disk drive has spinning disks that rotate at thousands of RPM, and data are stored by altering magnetic fields on the disk. Mechanical parts such as bearings can wear out over time. However, most of the wear would probably come simply from having the hard disk drive powered on and the disks spinning, or from powering the drive on and off often, rather than from writing lots of data on it.

So, the TL;DR answer is roughly that *downloading* (as in transferring data from the Internet in the first place) doesn't cause wear; writing the files on the disk (as you usually do) may cause some wear, but not enough to matter in most cases.",hos7qu5,t3_rhhyf3,1639664933.0,False
rhhyf3,"I concur. The downloading itself does not really wear out anything. The act of persisting this data is what causes wear.

You can have a functioning computer even without any writable medium. You can even boot linux from an optical disk and from then on everything that needs storage is stored in RAM.",howvegc,t1_hos7qu5,1639746162.0,False
rhhyf3,"One more thing to add: the servers that you download from need to serve the files. That requires processing power (to a smaller extend on the downleading side, too) which is measurable is excess heat. This will (over long periods of time) also cause damage on the hardware.",hozkqrw,t3_rhhyf3,1639786083.0,False
rhnc1k,Every element in the domain of a function must map to something in the function’s range. The is basically the definition of domain. If there was an element x that didn’t have a defined value f(x) then by definition the domain of f actually cannot contain x.,hori60v,t3_rhnc1k,1639649250.0,False
rhnc1k,thanks,horj4ds,t1_hori60v,1639650044.0,True
rhnc1k,"To expand a bit of this if a given function `f` is defined for some subset of `X`, let's say `X'` (so it is a a function `f: X' -> Y`) then in regards to `X` it is also called a ""partial function"". The partiallity comes exactly from the fact that it does not cover all of `X`. However it still obeys the rest of the laws if you look at functions as relations.

Surjectivity is not partiality however. Let's reset and say that `f: X -> Y` is a function covering all of `X`. Surjectivity asks ""is there an `x:X` for every `y:Y` such that `f(x)=y`""

More succinct: [;\forall y \in Y \exists x \in X f(x)=y;] 

Surjectivity can be expanded to partial funciton, this is normally done by disregarding the partiallity (so the cases where `f` doesn't map to anything) and then asking if that restricted function is surjective. In your example, if `Y = {y1, y2}` then even the partial function is surjective.",hos9g6l,t1_horj4ds,1639665691.0,False
rgnbmf,">Has anybody had similar experiences?

Yep!

You're not a moron. A moron would be finding excuses for why their solution was ""better"" somehow.",hol9r4q,t3_rgnbmf,1639533311.0,False
rgnbmf,The chad above is right . Sometimes we block our thinking due to performance anxiety or a belief that we need to prove ourself.,holuv8w,t1_hol9r4q,1639543347.0,False
rgnbmf,The alpha male above is right. The need to prove ourselves can often make ourselves feel less.,homs89m,t1_holuv8w,1639567655.0,False
rgnbmf,The person experiencing imposter syndrome above is right. We always learn by our failures and peers.,homyddx,t1_homs89m,1639571767.0,False
rgnbmf,"The person in denial above is right. It's always better to keep an open mindset rather than a closed one, since an open mindset will always help you grow",hon4pyd,t1_homyddx,1639575311.0,False
rgnbmf,"The wise mountaintop guru above is right. An open mind lets in fresh thoughts, like a open window lets in fresh air and sunlight. Stretching the metaphor, all allow you to grow.",honbf9c,t1_hon4pyd,1639578545.0,False
rgnbmf,Hehe.,honc0j8,t1_honbf9c,1639578816.0,False
rgnbmf,We’ll that puts an end to that,hongbrz,t1_honc0j8,1639580717.0,False
rgnbmf,"It's virtually impossible to write dumber code than an experienced developer has come across in production at some point or another.

If it works and people can read it and it doesn't require adding dumbass dependencies, ship it!",holec26,t3_rgnbmf,1639535384.0,False
rgnbmf,"Just by the fact that you recognize that your coworker's solution is a valid but a simpler solution to your problem, indicates that you're definitely not a moron. You learn something from this experience.
6 months is arguably not a lot of time to gather experience yet. I've been doing this for more than 5 years, and I sometimes still do stupid mistakes (though not at the extent and frequency as when I was still starting out). But the most important thing is you learn. You have to always learn.",holzpit,t3_rgnbmf,1639546055.0,False
rgnbmf,Everyday and always. The best part is when you stumble upon code you wrote years ago and wonder 'how the hell did I even come up w/ this???'. Never gets old.,hom03f5,t3_rgnbmf,1639546280.0,False
rgnbmf,"I've been writing software professionally for close to 20 years. I experience what you describe every week, on average.",homr8dr,t3_rgnbmf,1639566898.0,False
rgnbmf,"All good. You are learning!  


Next time when you need to solve the same problem you know how to do it.  


Generally: When ever you think your code is smart/genius... delete it, it's garbage!

Smart code is bad code. If you think it's smart now you'll have a very hard time understanding in a month from now, your stupid colleagues will have an even harder time. Don't write smart code, write simple code!",holnuea,t3_rgnbmf,1639539754.0,False
rgnbmf,"Excellent advice, I second this. Good code often looks surprisingly simple. Code by newbies is often very complicated.",holpdxx,t1_holnuea,1639540500.0,False
rgnbmf,"Hmm, I honestly have encountered a lot of senior code that is very hard to understand and I actually prefer newbie code because it's usually less complex and easier to reason about. 

Well... I might be eternal newbie, but that's another discussion. Just wanted to give another view point aswell!",honmhoj,t1_holpdxx,1639583231.0,False
rgnbmf,"My first reaction is that the people who wrote the code you saw still have things to learn. If code is hard to understand, that is a problem with the code.

A worthwhile place to learn more is the book ""Clean Code"" by Robert Martin (""Uncle Bob"").  Especially note the ""SOLID"" acronym; there is a chapter for each part of this. If you can use these ideas, you will be producing more easily-understood code.",honuq4f,t1_honmhoj,1639586462.0,False
rgnbmf,"Senior code I was referring to is mostly about doing something simple through multiple abstractions. It seems SOLID, DRY, KISS and all those acronyms but it lacks simplicity.  
It's hard to discuss with just words and I should show you an example, but I think you might've encountered code like that as well? One could say, it is over-engineered. I think over-engineering is a bit subjective and it might be just bad code etc.",hora5nt,t1_honuq4f,1639642655.0,False
rgnbmf,Indeed. What may earn style brownie points within a single line may not be good for the whole project.,homif8o,t1_holnuea,1639559508.0,False
rgnbmf,"I spend weeks deleting complex code from a project (to get it working), then I went on to implement a more complex but better algorithm only to discover that the ""junk"" I threw out was that algorithm (or large parts of it, project didn't work at all when I got it, so no clue).

In retroperspective, I stand by this way (no way I could have fixed/tested that complex algorithm). But I still felt dumb. Especially my commit comments: ""Removed useless Class X"" then later ""Implemented Class X"" (Class names come from science/math paper)

&#x200B;

Looking further back - before University, nobody told me that ""If"" is bad (performance wise), so I stacked 8 If clauses with masterworks of boolean math - in 3 Loops, to be executed each frame. Now I know why I only got 30 FPS and couldn't increase resolution at all. ...

Or that time we had a practical course in university - some Java Full Stack thing (a frontend for some research facebook clone (with complex relation networks), intended for actual use - Java, never again). Took 80% of our time to even get it running (because competence is rare we had 2 people with real Java experiance and only 3 others (inkl- me!) even had the competence to install Virtual Box ... ). When we put our (shitty) frontend to the real backend we realised that data queries literally took \~93 seconds. 93 SECONDS!  (everybody got good to very good grades - and the project (and backend creator) got scrapped (or redone)) We felt really dumb. Had we tried that earlier, we could have saved a lot of stress.",holck4r,t3_rgnbmf,1639534584.0,False
rgnbmf,[deleted],housdil,t1_holck4r,1639702524.0,False
rgnbmf,"You mean the Java project?:The problem was that the backend was designed without regard for performance: Connections (friendships, family etc) were entities with properties, which themselfs each could have connections (infinitly recursing!), and Cathegories, nested etc. Pretty sure even the simplest request resolved to dozen or hundrets of SQL queries and MBs of data. (and, of course the backend, frontend,frontend database and backend database were each on different servers (also not in RAM or on SSD))

I wouldn't complain about the servers (much) we used (2013?) Apache, Tomcat and Postgres, plus JSPs /Servlets combined with RichFaces, some Servlet Server (or what that was called) and one other thing I forgot, and of course, we had to use some universal identity/login service (which was difficult). We spend most time getting to Hello world, and second most time ""exploiting"" our projecft components/writing data leaks because nobody (on our team) could figure out how the JSPs / servlets should share presistent, user linked objecfts... One other highlight was figuring out that my code wasn't buggy - RichFaces had a bug (somebody copy&pasted a set function, but forgot to chage the setted property). Project was way to difficult as a first practical team project.",howtcqe,t1_housdil,1639745063.0,False
rgnbmf,"When I have the pleasure of teaching new joiners in my team how to write programs I tell them this:

* First, it has to **do the thing**
* And better is when **others can understand** how it does the thing
* For bonus points, it should do the thing **efficiently**

Sounds like you may have skipped the second step. But at least you recognised that. Don’t feel bad. This is all part of the never-ending learning path.",homg3h1,t3_rgnbmf,1639557569.0,False
rgnbmf,Can you explain what the difference was?,holn0nd,t3_rgnbmf,1639539361.0,False
rgnbmf,"Don't be embarrassed; it's called learning. As time goes on, you will be learning how to do all kinds of things better.

Seeing how experienced people solve problems you have attacked is always worthwhile.

Along these lines, it would be worth your time to find and read ""Design Patterns"", a classic book by ""the gang of 4"" (it has 4 authors). They show how to attack several other issues that come up all the time.",holovbl,t3_rgnbmf,1639540245.0,False
rgnbmf,"I think there is merit to trying to figure things out yourself rather than immediately falling back to reference material like Stack Overflow. I did this a lot in my CS classes in university, and I think it sets you up to be a better problem solver. A strength to develop though is to know when to go look something up, or at least do some follow up after coming up with something to make sure that you've solved something in the best way possible before submitting it for code review/etc.",hols4tz,t3_rgnbmf,1639541895.0,False
rgnbmf,You'll continue to experience this in retrospect when you look at code you wrote a year ago.,homls0l,t3_rgnbmf,1639562396.0,False
rgnbmf,"yep don't worry it's ok and healthy to feel like this, keep learning and get better


ie: i just refactored a chunk of my old code because i could not understand it fast enough and it was doing stuff redundantely and with wrong var names",hon1iqa,t3_rgnbmf,1639573601.0,False
rgnbmf,">Has anybody had similar experiences?

All day every day (I'm a business owner in IT) And I've been tinkering with code for well over 25 years now, don't beat yourself up over it.   


There will be someone that looks at your code and think ""Why didn't I come up with this"" as well and please reframe critisizing yourself into *""Not the best possible solution for the given problem""* or *""I may have....""*  
I drove myself into a **University needed to intervene** situation due to seeing the code people rattled out during a internship at a big tech company, which made me question my abilities which is a fate I wouldn't wish upon you.",hon3gv8,t3_rgnbmf,1639574662.0,False
rgnbmf,">Has anybody had similar experiences?

Yes, it's called progress",hoob0ux,t3_rgnbmf,1639592777.0,False
rgnbmf,This is called lack of experience. It's normal. And if it works it is okay.,hon2hlu,t3_rgnbmf,1639574136.0,False
rgnbmf,"I've been in this line of work for almost 3 decades. Just last month I wrote a complicated if/else contraption. Upon finishing it and testing it, I realized I could simplify the whole stupid thing down to one conditional. Yes, I am an idiot, too. Welcome to the club!  It's a big club.",honaurk,t3_rgnbmf,1639578290.0,False
rgnbmf,All the time,honwhmn,t3_rgnbmf,1639587162.0,False
rgnbmf,Bruh at least your not getting easy questions wrong on Algo expert i feel braindead,hpjqjwu,t3_rgnbmf,1640175016.0,False
rh7r2v,"For illustration purpose it starts with only 0-45 degree slope and leaves the detail to you to deal with other cases (flip signs/flip dxdy/etc.)

the coefficients in 2dy-dx are chosen because we're making a decision between picking (1,1) or (1,0) as the offset to the next pixel. We're really just comparing dy/dx direction to (1,0.5) and see if we should go above that (1,1) or below that (1,0). But 0.5 isn't integer, so we scale and compute 2dy-dx instead.",hos88f7,t3_rh7r2v,1639665155.0,False
rg53gw,"The book ""Engineering a compiler"" by Keith Cooper and Linda Torczon has about 250 Pages on different optimizations, I can highly recommend it.   
You might get it used cheap or it's in a local library.   


or ... there are places on the internet where one can get Pdfs for free apparently.",hoi8v1i,t3_rg53gw,1639487227.0,False
rg53gw,"[https://llvm.org/docs/Passes.html](https://llvm.org/docs/Passes.html) might be a good start.  


Passes to look for are mem2reg, sccp, the loop- passes and anything in the transform category, actually.",hohzdwh,t3_rg53gw,1639481168.0,False
rg53gw,https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html,hoiq4gp,t3_rg53gw,1639495523.0,False
rg53gw,"The Dragon Books

[https://en.wikipedia.org/wiki/Compilers:\_Principles,\_Techniques,\_and\_Tools](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools)

I like the green dragon, but then I am old.",hojo0xy,t3_rg53gw,1639508833.0,False
rg53gw,Not exactly the same but [http://godbolt.org](http://godbolt.org) is pretty cool!,hojra7t,t3_rg53gw,1639510107.0,False
rfmupc,"I have no experience with this so I'm just spitballing. Depends on how intelligent she is and what kind of computer experience I suppose:

* Scratch would let you make it as easy as possible. Colorful drag and drop tiles to build functions, would introduce the really basic concepts of control flow and all
* You could play with Visual Studio a bit. Dragging and dropping buttons and showing how to do something simple like make a popup window with text or add two numbers
* Python is pretty easy to start with too for obvious reasons, maybe that would make sense to start with for some basics

If you haven't Googled in depth already, there are probably some more specialized languages or ""IDEs"" out there targeted at a young audience",hoetxuo,t3_rfmupc,1639421467.0,False
rfmupc,"One thing I would say about Scratch is that while it looks colourful/easy to learn/fun for kids, there are so many code blocks and so many things the learner has to understand. It also takes a lot of time to look for the right blocks and to place them for something to work.

And I am saying the above from experience. I learned Scratch for the first time in a programming class and failed miserably. Just not enough time to experiment all the blocks and didn't really get how to fit certain blocks into places. And it was really time consuming.

But if you have time in your hands, then give Scratch a try. If she is struggling, definitely let her try Python because it's much easier to learn and make programs in my opinion. You can make a lot more interesting and fun programs with Python and I bet she would like that.",hog26l4,t1_hoetxuo,1639439847.0,False
rfmupc,"Good point. I hadn't considered the potential learning curve for OP as well, sounds like that one might be a time investment",hogut5l,t1_hog26l4,1639453343.0,False
rfmupc,Intelligence is a built skill. Like you learn how to learn better and learn well.  It's not like a fixed thing.,hofexj2,t1_hoetxuo,1639430027.0,False
rfmupc,"Fair enough, not trying to split hairs on that. Just meant that some kids are going to pick it up faster or have a little more under their belt already than others.",hofre66,t1_hofexj2,1639435195.0,False
rfmupc,[deleted],hoewexx,t3_rfmupc,1639422451.0,False
rfmupc,"So, you hate the child",hoh91nv,t1_hoewexx,1639460870.0,False
rfmupc,"As a father of two children ages 10 and 6 where both kids like programming (or ""computer math coding"" as my 6 year-old calls it), I would focus on [Computational Thinking](https://www.bbc.co.uk/bitesize/guides/zp92mp3/revision/1):

1. Decomposition
2. Pattern Recognition
3. Abstraction
4. Algorithms

I would stick with using Scratch (my kids use/know this).

Example:

There's a maze (2D-array) where the student has to navigate a character from from start to finish.

a) They can hard-code the sequence of steps required. (Sequencing concept)

b) They can be forced to use loops to reduce the number of instructions used. (Looping concept)

c) They can use basic decisions (if-then-else expressions) to create a simple algorithm.


 Keep the concepts simple and light. Making success incremental while slightly increasing the difficulty between tasks is key. That keeps the kiddos motivated.",hoezfir,t3_rfmupc,1639423669.0,False
rfmupc,"I started with Scratch when I was around 10 and I’ve been interested ever since. 

https://scratch.mit.edu/",hoex8xv,t3_rfmupc,1639422788.0,False
rfmupc,I started getting my interest in computer science and programming when I was 13. I don't see why not to introduce her. I'm 17yo rn studying Game Development at college and I'm glad I started young and found my passion / future job lol,hoev4s1,t3_rfmupc,1639421942.0,False
rfmupc,"I was introduced to code around that same time and I learned through Scratch. That was my favorite program. But, I had a teacher tell me how to do stuff. There aren't built-in lessons in Scratch. If you want to teach her and be more hands-on, I'd use Scratch. If you want something that teaches her how to code in a way that's similar to Khan Academy, I recommend [Code.org](https://Code.org) or [CodeHS.com](https://CodeHS.com)",hoex8xb,t3_rfmupc,1639422788.0,False
rfmupc,I was an elementary school child when I learned BASIC. I would say BASIC or Python.,hoeys1h,t3_rfmupc,1639423408.0,False
rfmupc,I learned programming BASIC on my dads C64.,hohtx82,t1_hoeys1h,1639476724.0,False
rfmupc,It's too bad modern computers aren't accessible like the old days.,hohywn2,t1_hohtx82,1639480807.0,False
rfmupc,"Eh, I disagree. I think it's more easy nowadays. Especially with higher level languages like python or JavaScript. Remember that you had to type line number yourself  for goto statements?",hoi1bwc,t1_hohywn2,1639482574.0,False
rfmupc,"Figure out the fastest way she can make a game in Roblox and start with that. Ideally if she can do something super simple on an hour and half or less that would be great. Then slowly build from there. If each lesson ends with her game being a little better that’ll probably help a lot with motivation and excitement. 

Overall focus on something she can show off and be excited about. Then sneak in the less fun stuff as you go.",hoeudy9,t3_rfmupc,1639421645.0,False
rfmupc,Start her with [Scratch](https://scratch.mit.edu/parents/),hoexcb9,t3_rfmupc,1639422826.0,False
rfmupc,Already mentioned a bunch of times so just adding to the dog pile here: Scratch got me hooked as well. The drag and drop code blocks and animations made it fun and simple.,hof9atw,t3_rfmupc,1639427680.0,False
rfmupc,"This might sound nutty but hear me out. If you have an Apple device, download the OCaml compiler and teach them ML! I don’t even mean that in a joking way.

Functional programming is actually so much more straight forward and that Mac/iOS compiler is incredibly simple and easy to use.

Sure they won’t be able to do a ton but it’s not like Scratch is really reaching them anything more useful and with ML they will only need to learn like three keywords and the rest just all falls into place.",hoeyfjm,t3_rfmupc,1639423269.0,False
rfmupc,"I don't know any Ocaml (and you're probably more experienced than me at this, since I'm not that experienced a programmer), so I can't say for sure, but based on the criteria you gave, do you think something like Scheme using DrRacket might be easier?

The syntax is dead simple, arguably moreso than Ocaml, and because it's run using an interpreter, it's very easy to see the results of your code quickly. It's also typeless, which means there's one dimension fewer to wrap your head around.

That being said, as someone who's trying to learn how to program by learning C and Scheme side-by-side, I don't know if I'd say functional programming is more straightforward. It's about equal, I would say, although this is based on my rough intuition; I understand imperative programming better than functional because I find C to be more fun to program in, but if I put the effort towards Scheme that I do towards C, I think they'd be about equal.

Also, I realize your comment is about a month old; sorry for necroing.",hrqlffr,t1_hoeyfjm,1641616370.0,False
rfmupc,"Speaking from how I got into CS, I'd say Web Development.

I remember that's how I started when I was a kid. Working with HTML and CSS was extremely simple and provided instant gratification. I remember getting so excited over stupid little things like setting a background image or making text a different color just by typing some letters.

The best part was how much it gradually snowballed. I was forced to consistently learn more and more as my ideas grew. Eventually had to learn stuff like PHP and JS which really came in handy for getting started with programming languages in college.

But web development never felt like work or studying. As a kid, I always considered it more of a art medium rather than a technical skill.",hofb0tz,t3_rfmupc,1639428418.0,False
rfmupc,"BASIC is how I got started, at around that age. See if you can get her interested in something like a turtle drawing library, those are usually quite fun for kids.",hof2qre,t3_rfmupc,1639424998.0,False
rfmupc,Raspberry pi has a lot of learning PDF materials for young children,hof44fi,t3_rfmupc,1639425545.0,False
rfmupc,"**Autonauts** is a cute game that let's you build an automated village. It's very kid friendly and has scratch-like building blocks. The game is super rewarding early on, but it becomes more redundant to progress later. Regardless, it's a good introduction to basic concepts while being entertaining.",hof77ra,t3_rfmupc,1639426804.0,False
rfmupc,"Roblox scripting might actually be a good place to start. You can make a lot of different games in Roblox, and you can start really simple.",hof9tkc,t3_rfmupc,1639427902.0,False
rfmupc,"I got my start around that age by messing around with qbasic, typing in examples from a qbasic book that my dad had. 

The act of rote typing in examples from the book was enough to drill in the syntax and basic concepts. I remember my dad sitting and teaching me about if/then/else, and maybe about for loops. Then I was left to my own devices and started making changes to the examples that I had typed in. I also spent quite a bit of time trying to get things to work due to making typos, which also helped me learn. I seem to remember making lots of Mad Libs type things where you'd be prompted for words, then it would print out silly stuff incorporating those words.

I would recommend setting them up with some environment in a modern language. Something with at least console IO capabilities, or set them up with a skeleton project in a more GUI oriented environment with a ""turtle"" like drawing API, show them the ropes, then see what they come up with. I would lean towards javascript in a browser, or javascript run via node just because it's ubiquitous these days.

I also know kids get into programming via Minecraft these days, so that's another option if she's into stuff like Roblox.",hofmsno,t3_rfmupc,1639433274.0,False
rfmupc,you could try CodeCombat.  it kinda turns coding into a platform-esque video game and makes it fun!  that's how I started out,hofwp5w,t3_rfmupc,1639437451.0,False
rfmupc,"The C Programming Language by Kernighan and Ritchie.

But seriously I would figure out what makes her interested in programming and find something that matches that. If she wants to make games, she should start on that path.

Educational languages can feel boring and fake. Unlike a lot of people here I don't think there is anything wrong with starting out with what people consider a ""hard language"". There are no hard languages, only hard problems. ""Hard languages"" can be more exciting and motivating because you feel like you are doing something real. And if you are self teaching you can go at your own pace anyway.",hoh8mvx,t3_rfmupc,1639460613.0,False
rfmupc,Try her on a few tutorials for different things to see what draws her attention more. Scratch and Code Combat are good ones. I think No Starch Press has some kids programming books for things like python and scratch.,hohfgqe,t3_rfmupc,1639465137.0,False
rfmupc,"Three things come to mind.

1) The Logo programming language, my first memory of writing a program was a turtle moving around a monitor forwards, back in varying distances and rotating 5/10 degrees at a time and repeating n times to create a star 

2) Raspberry pi has some programming languages built in that can interact with actual hardware easily and there is a programming language scratch 3 where programming is done by dragging blocks that are linked to things like button press events and the pi could be hooked up to hardware light a set of traffic lights / pedestrian crossing to introduce something real-world.

3) a game called pingus is a clone of lemmings and is a game with learning thrown in, you have to ""program"" to get the penguins through the level, eg by deploying blocker penguins to hold up them while the builder/digger pengins make the path ahead (pengins will walk straight of a cliff if they unsupervised). One downside to this is that you might have to explode pengins to complete the level and that may not go down well.",hohyj0e,t3_rfmupc,1639480520.0,False
rfmupc,I was wondering when I was going to see someone mention LOGO.  Turtle graphics or bust. :-),hoil2ys,t1_hohyj0e,1639493352.0,False
rfmupc,"You could try these activities through Girls Who Code 

https://girlswhocode.com/programs/code-at-home",hoi8tn2,t3_rfmupc,1639487205.0,False
rfmupc,"100% agree with Scratch. Minecraft redstone is an introduction to logic similar to playing Roblox. Look for camps like Code Ninjas, may not give you the chance to teach her yourself but you can bond talking over what she did there and she gets to try a curriculum meant for introducing kids to programming.",hoexkpw,t3_rfmupc,1639422920.0,False
rfmupc,Roblox,hofpqww,t3_rfmupc,1639434506.0,False
rfmupc,Yes. Do it with Pico8. ittle make it seem less boring.,hofcmle,t3_rfmupc,1639429093.0,False
rfmupc,"I walked a young cousin though making a chess game (no graphics, just terminal). She was a little older but I thought it went well.",hofl0x4,t3_rfmupc,1639432534.0,False
rfmupc,"I would suggest python. Meanwhile do more math problems, after all computer is all about math",hofpxzj,t3_rfmupc,1639434589.0,False
rfmupc,"Look up “STEM robots” on Amazon. They’re robots that you build with cameras and a controller to make them drive and such, but they require some beginner level programming. Great way for kids to learn programming while maintaining some fun interest.",hofxpvn,t3_rfmupc,1639437889.0,False
rfmupc,Use whatever interests/motivates her as a jumping-off point.,hofyc1m,t3_rfmupc,1639438154.0,False
rfmupc,Funny prime numbers games,hofzbq8,t3_rfmupc,1639438588.0,False
rfmupc,"Thats how my first year CS Teacher treats us, the students.",hofzefa,t1_hofzbq8,1639438621.0,False
rfmupc,"Oh, and yet... i know nothing. Nada at all.",hofzh9p,t1_hofzefa,1639438657.0,False
rfmupc,Scratch possibly,hog4eeu,t3_rfmupc,1639441236.0,False
rfmupc,"I think it would be best to start her off with games or online coding websites meant for kids (scratch). If you try to teach her coding basics for python, etc. without anything fun she might get bored pretty fast. I used cmu cs academy as a freshman and the beginning lessons were very easy but that could wait until she’s a little older.",hog6gxh,t3_rfmupc,1639442239.0,False
rfmupc,"Python got me hooked at age 12 because of how easy to use it is. Typing isn’t super strict, and it’s relatively intuitive compared to other languages.",hogfnja,t3_rfmupc,1639446459.0,False
rfmupc,"She could probably learn LUA. Since you can code a lot of things in LUA using Roblox's system, Roblox Studio.",hogho69,t3_rfmupc,1639447371.0,False
rfmupc,"It would really depend on what type of learning she takes to more, like if she gains more with note on definition of things (initialization, loop, data type, or whatever other common place things), or if she gains more from some goal oriented for some set of things (list all even number from 0 to input value). One thing I could suggest for a more goal oriented approach is codingame (codingame.com), since there is a lot of programming languages that could be used, and it has various ""puzzles"" that are solved producing some expected output. The first ""onboarding"" puzzle has a while loop that takes input automatically, and you need to determine the proper output statement per iteration. For some puzzles, there is some graphics to show achieving the goal. For that first puzzle, you have a ship in the center and enemies are flying to you.

You don't have to make an account with the site (it is a free account, if I am remembering right), but you can start without having an account. The account just allows you to save settings and progress.

I think the main item when teaching a younger kid is just determining what keeps them engaged with the topic rather than just what you try to tell/teach them. I hope that helps some.",hogm87x,t3_rfmupc,1639449412.0,False
rfmupc,Maybe begin very basic to make her realize that it requires that tedious attention to detail. Not a focus on the big picture but the small non-graphical game would be great to start. Maybe something like 'Hangman' or 'Tic-Tac-Toe' something you can help her with and she can grasp the concepts of what is happening - and why.,hogpm00,t3_rfmupc,1639450923.0,False
rfmupc,[I suggest these](https://imagilabs.com/?utm_source=partners&utm_medium=direct&utm_campaign=spectra-hackathon). You don't have to buy the keychains. They have learning labs [here](https://www.notion.so/imagilabs/imagi-Learning-Center-5afe3d51d30645849f2738c9b5eb1154) where she can code designs,hogqcvg,t3_rfmupc,1639451261.0,False
rfmupc,"Scratch and khan academy have some pretty cool free programs. I learned the basics of JS with khan academy, there are lessons you can do",hogqwoa,t3_rfmupc,1639451515.0,False
rfmupc,"I don't know if anybody has mentioned it yet, but my gateway into programming was through the Arduino ecosystem. 

I had tried learning to code through various online platforms over the years but the ramp up to doing anything useful was a long road. 

Arduino gives you tangible things to associate your code with so you can really get a feel for what your logic is doing. 

But that was just my experience.",hogrz0a,t3_rfmupc,1639452012.0,False
rfmupc,Lego used to have an embedded controller that you could program with a scratch like language to have the motors go forward/reverse etc. It is what got me into programming in 4th - 5th grade.,hogt62d,t3_rfmupc,1639452569.0,False
rfmupc,"Depending on what she’s interested in, CodePen might be a different option.  It’s fun for making art with code and adding animations and such",hogvsl3,t3_rfmupc,1639453810.0,False
rfmupc,"I don’t see Kode with Klossy mentioned, but it’s an organization specifically focused on helping girls learn to code. They have tons of resources, events, and online and local communities.

 https://www.kodewithklossy.com/",hogwr7z,t3_rfmupc,1639454266.0,False
rfmupc,Web dev is a good entry point. Or setup node. Have 'em work on a simple if else console game. That's a super fun thing. Let's them be creative.,hoh5iw3,t3_rfmupc,1639458764.0,False
rfmupc,"- https://www.computercraft.info/

- https://store.steampowered.com/app/370360/TIS100/

- Just dive head first.",hohb70i,t3_rfmupc,1639462231.0,False
rfmupc,"I think KhanAcademy has some introductory courses to Computer Science that are very easy to grasp, even for children.",hohli3p,t3_rfmupc,1639469650.0,False
rfmupc,"Consider buying a Micro:bit ([https://microbit.org/](https://microbit.org/)) or 2 - a fabulous little engine/chip that you can program directly, sense buttons, shake, sounds, builtin radio communication (if you have 2) etc and it can be a wearable. It has a huge collection of add-ons, lights, electronics, robots etc, codes using blocks (looks like scratch), as well as python (via Mu editor) and javascript, lets you control lights, has a powerful sim ([https://makecode.microbit.org/](https://makecode.microbit.org/)) and is a brilliant entry level physical computing platform that is gender agnostic. We use them with students as young as 8, right up to 15/16 year olds",hohnjae,t3_rfmupc,1639471287.0,False
rfmupc,"there is a HUGE developer community, code share network and heaps of documentation, examples, forums and communities that have competitions, share ideas, showcase cool stuff",hohnxwg,t1_hohnjae,1639471621.0,False
rfmupc,you can prototype real solutions that you can hold in your hand and actually use - the micro:bit is one of the first devices that 5 mins out of the box you can make something that does something you control - really fabulous tech and cheap,hoho39m,t1_hohnjae,1639471742.0,False
rfmupc,terrific gateway drug to other platforms (like rasberry pi and arduino),hoho5ru,t1_hohnjae,1639471797.0,False
rfmupc,"Also consider Minecraft Education (from Microsoft) - it has really powerful programming engine (it is not just blockstacking in cyberspace, or killing creepers, you can do amazing scripting things and there are heaps of resources/tutorials, how tos) to script in-game action (like writing scripts to build complex stuff using sequence, selection, iteration and modularisation - the fundamentals of coding in every language.",hohnv5p,t3_rfmupc,1639471558.0,False
rfmupc,"if the child is already into minecraft, then programming in minecraft adds a real edge that builds on existing skills",hohnzo9,t1_hohnv5p,1639471662.0,False
rfmupc,Scratch it a great one!,hohvcs7,t3_rfmupc,1639477945.0,False
rfmupc,"What all the cool kids use these days - Scratch.  https://scratch.mit.edu

What us old people used as kids - LOGO.  [https://turtleacademy.com](https://turtleacademy.com)

If she was older I'd say start her on something like Free Code Camp - https://www.freecodecamp.org",hoillsk,t3_rfmupc,1639493582.0,False
rfmupc,"Scratch, maybe?",hoimmyl,t3_rfmupc,1639494032.0,False
rfmupc,Coding train on YouTube is a fun way to start,hoj2ee5,t3_rfmupc,1639500417.0,False
rfmupc,Better get her a leetcode account and make her homepage the Blind 75,hoj8i77,t3_rfmupc,1639502762.0,False
rfmupc,"There are a lot of different ways, some are better and some worse. Unless some shows a really serious interest in going deep, I would start with simpler things.  
 
One very important detail is to work out, with her, what is exciting/interesting and use that as a guide for how to teach/explore. I would encourage her to put off messing with Roblox for at least a year or two.
 
**Scratch** is a neat tool/language and is a decent way to get kids interested these days, especially if they just want to make simple games quickly. You can go totally drag and drop if you want to avoid typing code, for instance. If you want go go that route I would recommend making an effort to provide some groundwork in conditionals and logic.  
   
I think **BASIC** was an excellent idea in the past and still has meriy. However it’s essential today to work with a variant that can provide the same ease with math, graphics, sound, etc on a modern computer that historical examples did with vintage microcomputers. *Basic-256* seems interesting, but it might be a little complex for a 10 year old. 
  
If you are willing to take a more hands on approach and help out with anything that’s she struggles too much with the then **Love2D** (uses Lua) or **Processing** (Java-based) might be of interest.",hojb0ze,t3_rfmupc,1639503746.0,False
rfmupc,scratch,hol7tbv,t3_rfmupc,1639532439.0,False
rfmupc,HTML,hq9xct6,t3_rfmupc,1640698581.0,False
rfmupc,Math.,hof0oz7,t3_rfmupc,1639424175.0,False
rfmupc,Swift Playgrounds if you have an iPad or a Mac.,hoev7jy,t3_rfmupc,1639421973.0,False
rf81l6,"It depends on what you mean by “mutate”. Viruses can be written such that they change their own encoding when they spread to a new system— search for “polymorphic virus”. This is not the same sense of “mutation” that we would use when talking about biological evolution, though. Computer viruses are a lot more fragile than meatspace ones, and once the exploit they target is patched they can’t dynamically discover a new one. This means they don’t “adapt” or “evolve” autonomously.",hocjzr0,t3_rf81l6,1639375217.0,False
rf81l6,"Upvote for ""meatspace""",hocq9l0,t1_hocjzr0,1639379548.0,False
rf81l6,"Mutations analogous to meatspace viruses are more like manual updates by adversaries. Adding new features, new evasion, different impact, etc...",hoewin9,t1_hocjzr0,1639422492.0,False
rf81l6,Some computer viruses can introduce slight variations into their program code when they replicate in order to prevent virus scanners from being able to lock onto a signature.  I suppose this is not unlike biological viruses slightly mutating proteins in their capsid that allows them to evade antibodies.,hocnwcz,t3_rf81l6,1639377827.0,False
rf81l6,"in general, no. it is technically possible i suppose but ""autonomous mutation"" is definitely not ubiquitous in computer viruses like it is in biological ones.",hoci1ws,t3_rf81l6,1639373995.0,False
rf81l6,But I suppose a new one would have to be made by the author,hoci5ox,t1_hoci1ws,1639374061.0,True
rf81l6,"Sure - but then it wouldn't be autonomous, right?",hocijqm,t1_hoci5ox,1639374305.0,False
rf81l6,Right,hocim9h,t1_hocijqm,1639374349.0,True
rf81l6,"It would technically be possible to have the virus randomly alter its code independently (although the code that causes it to do that in the first place would of course need to have been written by a programmer). Genetic programming and evolutionary programming are approaches for automatically generating new or altered programs using random mutation and crossover, although I don't think they involve self-mutation.

The problem, especially if the machine code were randomly mutated as just bits and bytes, would be that the vast majority of the mutations would be nonfunctional or nonviable, or not even valid programs. While this may be true of biological viruses as well -- although I'm not an expert and don't know if that's the case -- the sheer volume of biological virus particles even in a single biological host might make that less of an issue if *some* of them end up working out. You won't have a billion or trillion virus processes running on the computer, though, so while the number of potential hosts might theoretically be in the millions, the vast majority of the mutated offspring being nonviable might practically make it a no-go. You probably need quite a volume for completely random mutations to turn out useful.

Of course actual self-replicating computer viruses are probably fairly rare nowadays anyway, and most malware aren't technically viruses.",hof3t60,t1_hoci5ox,1639425419.0,False
rf81l6,If programmed to yes.  Particularly if engineered with AI assistance,hocghhz,t3_rf81l6,1639373051.0,False
rf81l6,"I mean... only becuase they share the same name doesn't mean they have anything else in common.   


We use the name ""virus"" to describe a type of malicious software but besides that there is no comparison to a biological virus and its properties. But ofcourse you can design a computer virus to adapt and mutate.   


But still this question seems to be based on a strange assumption that there is a correlation other than the name...",hoeo8e5,t3_rf81l6,1639419201.0,False
rf81l6,"I think the other commenters already covered the good answers but if the concept interests you, check out [Coding Machines](https://www.teamten.com/lawrence/writings/coding-machines/). It's a >!fictional!< blogpost about a few developers that find such a virus while troubleshooting a seemingly innocuous compiler bug.",hoevc3w,t3_rf81l6,1639422022.0,False
rf81l6,"You could create a virus that adds entropy but that would likely just aid in detection since it is better for code to have a function, even entropic code. Usually viruses are updated over time after being detected.",holbb8f,t3_rf81l6,1639534017.0,False
rf81l6,"Yes it possible, viruses can attach itself to a system resource which then spawns off to different services or resources and spawns and spawns. WannaCry is a perfect example of a mutated virus",hocj0yt,t3_rf81l6,1639374602.0,False
rfovp2,"IDK the ""*best*"" way, but printable characters are definitely a start...",hof8snc,t3_rfovp2,1639427467.0,False
rfovp2,Raw bytes -> hex string is my go-to.,hofd5e8,t3_rfovp2,1639429303.0,False
rf03ai,Another one is Sipser's book. Great book overall.,hob96fa,t3_rf03ai,1639352518.0,False
rf03ai,"I second this. Though many algorithm books discuss this topic, I believe studying formal languages is the best way to get an intuition for the complexity classes.",hobaz8l,t1_hob96fa,1639353331.0,False
rf03ai,+1 for Sipser's book. It's quite good.,hoctu2f,t1_hob96fa,1639382313.0,False
rf03ai,"Some of the resources I used when I was learning complexity classes:

1. [Abdul Bari's take on the topic](https://www.youtube.com/watch?v=e2cF8a5aAhE). Definitely my favorite explanation of it. 
2. [This stackoverflow post.](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)
3. [Video by up and atom](https://www.youtube.com/watch?v=EHp4FPyajKQ)
4. [Another one by hackerdashery](https://www.youtube.com/watch?v=YX40hbAHx3s)
5. [This high level overview article by MIT news](https://news.mit.edu/2009/explainer-pnp)",hob7kxv,t3_rf03ai,1639351801.0,False
rf03ai,"At this point, I feel like I've learned more from Abdul Bari than my entire CS department combined.",hoci9ep,t1_hob7kxv,1639374125.0,False
rf03ai,Thanks!,hob80ac,t1_hob7kxv,1639351994.0,True
rf03ai,"Note, Wikipedia is a terrible resource for learning any technical thing. It is a great resource when you need to refresh yourself on a thing you once knew, or are looking for extra info.",hoc2cc0,t3_rf03ai,1639365811.0,False
rf03ai,Strongly agree. I thought I had this concept sorted out in my undergrad (Cormen). But I now see how there are many holes in my understanding.,hoc2vlv,t1_hoc2cc0,1639366058.0,True
rf03ai,"Algorithms Design

https://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358/",hobt2zl,t3_rf03ai,1639361590.0,False
rf03ai,"Not positive on a resource since I haven't touched this stuff since school, but I'd start with looking at a few reductions to get a sense of how problems relate to each other. Some simple ones might be longest path, Hamilton cycles, and degree-constrained spanning trees. The main idea is that there are mappings between these problems that keep things small (polynomial).

Maybe check out Karp's stuff or Gary and Johnson's stuff to understand the reductions. Once you're a bit comfortable with these ideas then you can get into the nitty gritty of non-deterministic Turing machines and the reduction to general satisfiability.",hob939w,t3_rf03ai,1639352477.0,False
rf03ai,"In case no one has given you a high level idea yet:

A problem X is NP-complete if it is NP-hard and in NP.

NP: A problem you can solve in non deterministic polynomial time; I like to draw the analogy that if you had computer that can run in parallel in multiple dimensions where each dimension tries a different result, it can find a solution within polynomial time ;-). A typical test is to prove that you have an algorithm to check that a result is a solution to the problem in polynomial time.

NP-hard: You can reduce a problem Y that is also NP-hard to X, so if you solve the X, you neccessarily solve that other NP-hard problem. That means X is just as hard as Y.",hobdjk2,t3_rf03ai,1639354497.0,False
rf03ai,"For NP, your definition has a slight mistake. NP includes those problems you can *solve* in non-deterministic polynomial time but can be verified in polynomial time. We don’t know how to build a non-deterministic machine, so your current wording suggests we can’t verify solutions in NP with our current technology.",hof9sel,t1_hobdjk2,1639427888.0,False
rf03ai,Thanks! I corrected it. I meant to say solve as per my analogy below.,hofa1o1,t1_hof9sel,1639427997.0,False
rf03ai,"I learned all about Turing Machines and NP Completeness from one book: Computers and Intractability: a Guide to the Theory of NP-Completeness. They provide the intuition and hard math for everything you might need involving NP and entering the polynomial hierarchy. You might also be interested in reading Karp's original paper on 21 NP problems, just to see what the first reductions looked like in practice. They're both around 50 years old, but still very readable and accessible.",hodx98k,t3_rf03ai,1639408353.0,False
rf03ai,"Gödel, Escher, Bach: an Eternal Golden Braid a book by Douglas Hofstadter.",hocr6lg,t3_rf03ai,1639380242.0,False
reoznh,"No, because they are barely good enough to translate handwriting to text right now. 

Also police do not keep handwriting databases.",ho8u8ad,t3_reoznh,1639316766.0,False
reoznh,"> police do not keep handwriting databases
 
[They do, actually](https://www.fbi.gov/services/laboratory/scientific-analysis/questioned-documents). If you are a researcher in a respected institution you can be granted (very closely overseen) access to their database for the purposes of AI research. AI is already used for this purpose.",ho9ylu9,t1_ho8u8ad,1639333643.0,False
reoznh,"That isn't even remotely the same ML task. Translating handwritten text to characters is entirely different from identifying an author from handwritten text. I'd venture to guess you could even identify authors from digitally written texts, let a lone handwritten text (which has at least an order of magnitude more of information). There are so many idiosyncrasies connected to written communication. Missing i dots or t crosses, frequency of use of ellipses, capitalization, style of the `a`, etc. There is an infinite amount of variability. Given a large enough corpus, it is definitely doable. It is so doable, I would be surprise if there isn't a standard commercialization of it already.

The fields of study is called: https://en.wikipedia.org/wiki/Stylometry",ho9yrvi,t1_ho8u8ad,1639333708.0,False
reoznh,"""handwriting databases"" made me laugh",ho9f6yx,t1_ho8u8ad,1639325954.0,False
reoznh,They do keep the notes though! Because they're considered evidence,ho9sc5w,t1_ho8u8ad,1639331217.0,False
reoznh,Computers aren't any better at guessing than humans. It is just a way to remove the blame from people when the guesses turn out to be bad. I'm sure law enforcement will love it.,ho9vi4b,t3_reoznh,1639332451.0,False
reoznh,"Machine learning chess AIs would like to have a word with you. If they are that capable of pattern recognition already, and better at it than humans, I see no reason why this pattern (handwriting) would be a terribly difficult leap. Computers are REALLY good at pattern recognition. So with a sufficient dataset and training, sure seems reasonable. I'm a dev, and I've written several ml training curriculum for my ml ai for my chess like game.",hoc21rq,t1_ho9vi4b,1639365676.0,False
reoznh,A just society would not convict someone on such flaky evidence.,hoc2hjf,t1_hoc21rq,1639365878.0,False
reoznh,"It's not societies decision to convict or not, it's the grand jury's, and certainly takes more evidence than 1 handwriting match.",hoc2n1f,t1_hoc2hjf,1639365948.0,False
reoznh,"Grand juries determine whether to prosecute (indict) someone, not convict them. Regular juries (or judges in a bench trial) convict people. And that's only in the US, normal countries have normal criminal investigations, none of this grand jury indictment nonsense.",hod91r2,t1_hoc2n1f,1639395048.0,False
reoznh,"In theory, sure. There's not enough data to train on, though, and getting it would be essentially impossible.",ho8yymr,t3_reoznh,1639319182.0,False
reoznh,Maybe as a tool to recognize certain pattern in the handwriting for a expert on this field. But I think ANN is a overkill for this purpose. I can think of a kind of template matching combined with a certain threshold.,hoa79r2,t3_reoznh,1639336935.0,False
reoznh,"Neural Nets can do any learnable task, it has an infinite hypothesis space. This is just identifying similarity between handwriting and there are tons of stuff on this out there already. 
https://arxiv.org/pdf/1606.06472.pdf 
here is a paper that does just that,",hoa8g8q,t3_reoznh,1639337386.0,False
reoznh,No they can identify psychopathic styles of writing or possible pick traits of identity but it can’t prove that someone is a murder verse just a psychopath,hoa6z7u,t3_reoznh,1639336828.0,False
reoznh,"I don't know if a neural network is required. Similarity analysis can be done with a variety of techniques.

But you could use a neural network. Depending of what you want to achieve you 
a) train the neural network to identify if two given hand writings are from the same author,
or b) identify if a given handwriting originates from a specific person.

For a) you just have to collect pairs of two handwritings from the same person (maybe couple days/months/years apart) and train the network on those pairs (and of course pairs of not matching handwritings for the negative case).

For b) you would need to collect handwritings the persons you want to identify and train the network to match the writing and the person. Than you can let the network predict the author of a new handwriting, given the person was in the training set.",hoa5vk7,t3_reoznh,1639336414.0,False
reoznh,Maybe dust the note for fingerprints. Or collect DNA from the note. The post mark on the envelope the note came in.,hobvuts,t3_reoznh,1639362832.0,False
reoznh,"No, because my handwriting changes constantly. I don't know how people keep a consistent writing style. They might as well have their own font tbh. I don't even know cursive so my signature is basically a few letters have fancy lines, and then it's also not consistent.",hod60cv,t3_reoznh,1639392655.0,False
reoznh,"I don't know about neural networks.... but people like do this for a living.  

You can always type what you want to say into a typewriter and then trace the output onto a piece of paper. Undetectable at that point.",hpbg6br,t3_reoznh,1640021749.0,False
rfa9t6,"There will always be malware, even if a new tech came out that was hard to infect, it will only be a matter of time before it will have targeted malware.",hodcbu5,t3_rfa9t6,1639397421.0,False
rfa9t6,Windows itself is malware by definition. So no,hocxc2l,t3_rfa9t6,1639385218.0,False
rfa9t6,Edgy,hofhuu7,t1_hocxc2l,1639431227.0,False
rfa9t6,How you figure that?,hoet2u1,t1_hocxc2l,1639421128.0,False
rfa9t6,"Well to name a few:
1. Loads of Crapware and advertisments right inside startmenu.
1. Per User Unique id for targetted advertisments.
1. Forced updates which restarts a system in the middle of work.
1. Incessant shoving of upgrades from one version to another.
1. Difficulty to change default webbrowser.

Windows 7 was last good version of Windows all after it can be classified as adware which are also Malware. So yes present day windows are malware.",hoh5o8o,t1_hoet2u1,1639458849.0,False
rfa9t6,Edge lord,hpf57ua,t1_hocxc2l,1640092236.0,False
rfa9t6,"I would guess no. Maybe it will be harder and harder to make, but it’ll probably never be impossible. An important thing to realize is malware does normal things for bad purposes. For example, you need the ability to encrypt files or send them over the internet in normal programs. However, who and when this happens to determines if it’s malware or a great product.",hoeuruh,t3_rfa9t6,1639421799.0,False
rfa9t6,"No, because the sorts of things that make a program malware are necessary for legitimate programs to be able to do as well.",hof4d90,t3_rfa9t6,1639425644.0,False
rfa9t6,"Even if you could give an unambiguous definition of ""bad"" behavior by a program, whether an arbitrary program has such behavior is undecidable.",hog3kma,t3_rfa9t6,1639440758.0,False
rfa9t6,"No. It's not generally possible for antivirus software (or any software) to tell for certain whether a particular program behaves in a given malware-ish way.

Antivirus software works by either identifying individual programs or pieces of code as specific malware, or by using so-called heuristics for telling whether an unidentified program seems to have malware-like behaviour. The former is limited by the requirement to specifically identify each particular piece of malware, so it always needs to play catch-up; the latter is not nearly 100% accurate, and it can't really be.

Even if you could unambiguously define which kind of behaviour means that an unidentified program is malware, it's not generally possible to have an antivirus algorithm that would unerringly tell if another program behaves that way. That's partially limited by our ability to design such an algorithm, of course, but it's also something that's been proven as mathematically impossible to do with absolute accuracy.",hojjpu9,t3_rfa9t6,1639507142.0,False
rfa9t6,"No, malware prevention will continue to improve; while malware itself will become better at countering malware prevention.",hqcx8gr,t3_rfa9t6,1640744382.0,False
rfa9t6,"No, there will never be a point where malware won't be made. People will always be finding out new ways to exploit stuff because there's always ways to break things. Nothing is truly bulletproof (invincible).",ht2j55m,t3_rfa9t6,1642444854.0,False
reinb8,It's just regular mod. I think if you try and think of some examples it should be pretty clear. Consider 4 mod 5. Both 2\^2 and 3\^2 are congruent to 4 mod 5 so 2 and 3 are the modular square roots.,ho7wdf6,t3_reinb8,1639290878.0,False
reinb8,"Hey, thanks. That's a good simplification for me to begin with.",ho7wow0,t1_ho7wdf6,1639291097.0,True
reinb8,"I dont think this matters here, but sometimes there is a difference between “%” and modulo and the difference is how negative numbers are handled.  % quite often represents “remainder” and can get negative numbers, whereas if you want to stay in positives you might convert remainder to modulo by adding absolute value of m from n%m). So  -7 % 3 = -2 which is a remainder, and modulo would be -2 + 3 = 1. Fix me if I am wrong, might be talking nonsense haha",ho8c282,t1_ho7wdf6,1639303394.0,False
reinb8,"You might be right! I'm a math student so I was thinking about this concept from a pure math background, which is a world where you just say -7, -1 and 2 are all the same mod 3 and leave it at that. however I could see how there could be cases where you want to be more specific in computer science.",ho8d0rx,t1_ho8c282,1639304218.0,False
reinb8,"The meaning of ""mod"" depends on the authority of the source because the meaning of `%` differs in programming languages. In languages like JavaScript and Java `%` means remainder whereas in languages like Ruby and C# it means modulus. There is a difference between remainder and modulus which you can read about [here](https://dev.to/hamiecod/remainder-vs-modulus-3mc8).

[This math stack exchange question](https://math.stackexchange.com/questions/633160/modular-arithmetic-find-the-square-root/633174) might help you. I understood what is modular square root but ah I don't really know VDF so it would be better if you read the stack exchange answer as compared to my interpretation.",ho8a8dv,t3_reinb8,1639301825.0,False
rdc453,"You have inspired me. From now on I’m gonna use combinations of the letter i, j and l for my variables.",ho03t0t,t3_rdc453,1639153476.0,False
rdc453,"string aO0D = ""go fuck yourself"";",ho087d7,t1_ho03t0t,1639155217.0,False
rdc453,You called?,ho3jll6,t1_ho087d7,1639210291.0,False
rdc453,"main(){

int i = 0;

while(i=0){

cout << aO0D;

}

}",ho27gsr,t1_ho087d7,1639184298.0,False
rdc453,Lmao,ho04c2v,t1_ho03t0t,1639153686.0,False
rdc453,r/foundsatan,ho06z9c,t1_ho03t0t,1639154736.0,False
rdc453,"Here's a sneak peek of /r/foundsatan using the [top posts](https://np.reddit.com/r/foundsatan/top/?sort=top&t=year) of the year!

\#1: [â€™](https://i.redd.it/8u9uo2ekove61.jpg) | [18 comments](https://np.reddit.com/r/foundsatan/comments/la4j28/â/)  
\#2: [I guess i won't go hiking for a while](https://i.redd.it/op9bc53nzpb61.jpg) | [42 comments](https://np.reddit.com/r/foundsatan/comments/kylt43/i_guess_i_wont_go_hiking_for_a_while/)  
\#3: [Airpods](https://i.redd.it/7n0d5klmf4z61.jpg) | [18 comments](https://np.reddit.com/r/foundsatan/comments/nce4k9/airpods/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[Source](https://github.com/ghnr/sneakpeekbot)",ho070hx,t1_ho06z9c,1639154749.0,False
rdc453,"It’s from math notation where i,j refer to rows and columns. There’s no real reason it continues except that’s how people come to learn the notation and people stick with what they know",ho04gtz,t3_rdc453,1639153739.0,False
rdc453,"At the very least you should be using `ii` and `jj` in your code - if only to make the editor search function work usefully with them. Even then, lots of code quality tools will fail you for use of a 2-letter variable name.",ho3fm0d,t1_ho04gtz,1639207272.0,False
rdc453,U can’t use the same thing because there’s no differentiating,ho5a6me,t1_ho3fm0d,1639245829.0,False
rdc453,I’m aware that’s where it comes from but it’s dumb that it ever started that way.,ho0gqsm,t1_ho04gtz,1639158587.0,True
rdc453,"It's not. Index notation like `a_{i, j}` was a huge innovation, and massively helps readability. If you have doubts, consider reading Gauss:

https://www.gutenberg.org/files/36856/36856-pdf.pdf

Here, instead of noting related variables by, say `P_1, P_2, P_3`, Gauss uses separate letters of the alphabet `P, Q, R`. It may seem trivial, but once you have many examples of that going at the same time, it really affects comprehensibility. You also run out of letters pretty damn fast.

You may argue that `i` and `j` are poorly chosen, but they fit nicely in small type as a subscript, and allow packing in the type of dense information needed in communicating mathematics. I've never had any trouble distinguishing them in my own hand, and they stand out just fine in typewritten text.

You may argue that we shouldn't use single letters in the first place, and in some cases you may be correct. But in many others something like `a_{first index, second index}` just adds a ton of clutter, and I think you would find it to be a worse choice in practice.",ho0myfa,t1_ho0gqsm,1639161015.0,False
rdc453,"Bruv, he didn’t mean the invention of notation— he means the use of i,j in it.",ho3b1j5,t1_ho0myfa,1639204113.0,False
rdc453,I concur,ho30s4s,t1_ho0myfa,1639198126.0,False
rdc453,It started back when things were hand written and it was much more easy to distinguish between the two letters. This is why we need to invent a monospaced Papyrus font to use for all future coding. Thank you for coming to my TED talk.,ho0m7u5,t1_ho0gqsm,1639160728.0,False
rdc453,"Monospace is overrated. I've been coding in a quasi-proportional font for a few years now. Iosevka Aile Code. quasi- cause most chars are the same width, but space and i etc are half width.

https://typeof.net/Iosevka/",ho0tn92,t1_ho0m7u5,1639163691.0,False
rdc453,I've been coding in wingdings for 20 years and that's clearly superior.,ho0wfaa,t1_ho0tn92,1639164810.0,False
rdc453,"Why write a post asking why, if you already knew the answer?",ho22qqo,t1_ho0gqsm,1639182201.0,False
rdc453,"The worst is when a student/professor, or anyone really, writes handwritten m and n in sloppy cursive.",ho099ii,t3_rdc453,1639155639.0,False
rdc453,"> The worst is [...] cursive.

Agreed. :D",ho0zjm8,t1_ho099ii,1639166062.0,False
rdc453,"""minimum"" in cursive is especially bad",ho2f41i,t1_ho099ii,1639187741.0,False
rdc453,"Pull up a chair, young'ens, and hear about the old timey language FORTRAN.  Back in FORTRAN, variables were implicitly typed based on the first letter of their name.  Start your variable with letters between 'i' and 'n' and it would be an integer. Outside that range, it would be a float (if I remember correctly).  Why 'i' and 'n' ?  Because some clever language designer decided 'integer' thus the 'i' and 'n'.  Using i and j as loop variables started from there (in the 60s and 70s, probably) and has continued on ever since.",ho0m2bd,t3_rdc453,1639160668.0,False
rdc453,"FORTRAN still used today, by the way.  Is very important in numerical computing. Because of some quirks in the language design, FORTRAN compilers can generate some of the fastest code (faster than C).  https://en.wikipedia.org/wiki/Fortran#Science\_and\_engineering",ho0noyi,t1_ho0m2bd,1639161313.0,False
rdc453,Neat edge case.,ho0wn6q,t1_ho0noyi,1639164897.0,False
rdc453,"I believe numpy has some Fortran source for the fast mathematics, so not really edge case.",ho1oxcg,t1_ho0wn6q,1639176251.0,False
rdc453,"Well, in all fairness though, unless you're specifically modifying the Fortran code, you can use more user-readable  variable names.",ho1u7av,t1_ho1oxcg,1639178483.0,False
rdc453,I was just answering about the fact that Fortran is still used today in a iper used library. Of course using numpy doesn't require using Fortran (thank god) but just because it's under the hood it does not mean it's a super edge case.,ho3r91g,t1_ho1u7av,1639216341.0,False
rdc453,"fwiw, this is no longer the case, but there is still a lot of legacy FORTRAN out there.",ho22vej,t1_ho0noyi,1639182259.0,False
rdc453,"Yeah, even back when I was first learning FORTRAN (long long ago), our instructor emphasized 'implicit none' to turn off that auto-variable nonsense.",ho2o849,t1_ho22vej,1639191913.0,False
rdc453,"Fortran is not faster than C.  For numerical stuff, both are basically equivalent.  The only real difference that I have been able to find is for recursive functions, where Fortran is much slower.

A lot of fast code has been written in Fortran, and it will remain important I'm scientific computing, but it isn't a unicorn.",ho3mgp9,t1_ho0noyi,1639212498.0,False
rdc453,">some of the fastest code (faster than C)

smiling in assembly

Look I might want to explode every time I look at any x86 assembly, but damn is it fast.",ho3u375,t1_ho0noyi,1639218539.0,False
rdc453,What is it about those quirks that make it so fast? Why don't we use those quirks in the design of new languages to make them faster?,ho4ic9p,t1_ho0noyi,1639233657.0,False
rdc453,"Disclaimer: I'm not a compiler expert. This is just what I understand from reading over the years.

FORTRAN is a very simple language. It's in fact the first compiled ""high level"" computer language. The name FORTRAN comes from ""Formula Translation"" and originally it was just a simple way to write math. The problem with, for example C, is some of C's rules prevent super optimization of code. The one I understand that causes the most problems is aliasing. [https://en.wikipedia.org/wiki/Aliasing\_(computing)](https://en.wikipedia.org/wiki/Aliasing_(computing))

If the same memory location can be represented by multiple variables, then the compiler has to assume worst case and cannot optimize certain paths. If the language doesn't allow such flexibility, then the compiler can crack its knuckles and go to town on optimization.

(Lots of hand waving here, sorry. It's been a long, long time since compilers class!)",ho4zoou,t1_ho4ic9p,1639241445.0,False
rdc453,"That's actually a really good explanation and a good starting point for my investigation. I'll look in to that more, thanks!",ho5jafw,t1_ho4zoou,1639249634.0,False
rdc453,Reference: [https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/data-types-constants-and-variables/variables-1/data-types-of-scalar-variables/implicit-typing-rules.html](https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/data-types-constants-and-variables/variables-1/data-types-of-scalar-variables/implicit-typing-rules.html),ho0m80p,t1_ho0m2bd,1639160730.0,False
rdc453,Holy shit! I was SO sure you were just making this shit up.,ho13t6x,t1_ho0m80p,1639167752.0,False
rdc453,Oh noooo... Somehow it feels like every day I learn a new fun fact about old-school Fortran that makes me gag haha,ho1285z,t1_ho0m2bd,1639167125.0,False
rdc453,IMPLICIT NONE!!! One of the first things I learned about when I was learning Fortran.,ho1gt8a,t1_ho0m2bd,1639172894.0,False
rdc453,Indeed. We had 'implicit none' hammered into us when I as learning FORTRAN. Woe be to those who had to maintain enormous libraries of older FORTRAN.,ho2ofcz,t1_ho1gt8a,1639192007.0,False
rdc453,"Hence the old joke in nerd circles: ""God is real unless declared integer"".",ho3eule,t1_ho0m2bd,1639206698.0,False
rdc453,"Fotran was created by mathematicians. This notation using i,j as indices was there before Fortran was invented.",ho3dubw,t1_ho0m2bd,1639205978.0,False
rdc453,"We keep using (i,j) because it is evocative of all the other times we used (i,j).  We reuse them precisely so that it's like all the other times we used them, so that you can see it in a glance and have a sense of what it means.  We use x when we mean an unknown real number or a variable of a real number, we use z when it's an unknown complex number.  It's really helpful to quickly understand something, when we reuse these names.  I would find a text nauseating if it didn't use this orienting technique.",ho0eg29,t3_rdc453,1639157685.0,False
rdc453,Right. It's the principle of least surprise.,ho3eyh4,t1_ho0eg29,1639206779.0,False
rdc453,"I think OPs argument is against certain typefaces and fonts in which the two letters (i,j) can appear identical on initial glance.  Some people are more prone to this than others due to a variety of reasons.",ho0rrlw,t3_rdc453,1639162934.0,False
rdc453,"It was hard to write, it should be hard to read! /s",ho1odxc,t3_rdc453,1639176023.0,False
rdc453,"Complain to whoever came up with the alphabet, all those pairs are letters next to each other and that's why they're used like this.

'i' is index, 'n' is number (or natural), 'v' is vector or vertex.",ho0ejj8,t3_rdc453,1639157722.0,False
rdc453,"I just like to use letters that are close to each other, because it’s easier for me than coming up with another name. Sometimes I’ll even use a,b,c for indices. But if I’m not working with complex numbers, I often use i because it’s easy to remember i = index. And after that comes j,k,l.",ho07w0x,t3_rdc453,1639155093.0,False
rdc453,"Convention, mostly. I can see good, descriptive variable names being useful, but convention will almost always win out in math/compsci.   


None of us want to incur the wrath of our professors, and I know professors that would be instantly annoyed at using something less conventional.",ho07081,t3_rdc453,1639154746.0,False
rdc453,"The i is mostly a convention for [i]terators, I guess the J was the logical follow-up people thought of when they started this trend. I for one only use i in very concise and simples loops/iterations, otherwise short and descriptive names make more readable code / math problems.",ho0aclg,t3_rdc453,1639156069.0,False
rdc453,Lmao how can you get i and j mixed up. I can understand if you're reading handwriting maybe,ho06xng,t3_rdc453,1639154717.0,False
rdc453,"See page 6/30 (or 262) top of the right column of [this](https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf) paper I was just reading (a printed version of). 

Not sure how to attach a screenshot of it without too much effort.",ho0hrtn,t1_ho06xng,1639158982.0,True
rdc453,"Wow fair enough, terrible choice of font there",ho0owtl,t1_ho0hrtn,1639161798.0,False
rdc453,"Speaking as someone from a 'doin stuff' side of research more than hard core computer science, we have been trying and failing to have compsci and maths researchers branch out to use actual words that tell you what they are for as variable names for decades now with no success. They are more maths than program, twisted and evil.
 
The font issue is the journal they're publishing in, which was set for printed physical journals and has never been updated from their original latex style file.
 
This all comes down to the fact the reviewers in any discipline (not a compsci issue, this is an academia issue) are an incestuous club that all know each other and all know the work, so they expect to see an i where there is an i and a j where there is a j and it all feels natural to them. They would apply more scrutiny to a paper that broke from their club's tradition and wrote out inner_index and outer_index instead, and if you suggested updating the font they would crucify you.",ho0cbrq,t3_rdc453,1639156849.0,False
rdc453,"Self-documenting variables make sense in code, but using words for variables in formal proofs is not a good idea. Just put pressure on publishers to use better fonts and we can keep the conventions about what variable letters stand for in various contexts. That part is actually reasonably important for quickly grasping what's going on.",ho0mlv1,t1_ho0cbrq,1639160877.0,False
rdc453,what makes self documenting variables a bad idea in math?,ho0ubao,t1_ho0mlv1,1639163962.0,False
rdc453,"Mostly that it will make proofs hard to read. Formal statements are compact with lots of single character width symbols and potentially many variables. So if variables need to be entire words or multiple characters, these statements will no longer be compact. 

Further, formal statements are often quite general, so whatever words you pick for variable names are going to be quite general too, where it makes no difference if it's named `x` or `realNumber`.

Take the Pythagorean theorem for a simple example: 

`a^2 + b^2 = c^2`

Compare that to:

`sideOneLength^2 + sideTwoLength^2 = hypotenuseLength^2` 

But now imagine for much more complicated statements or entire proofs written out like this. 

In proofs, compactness is a virtue that helps with readability and understandability. There's a reason separate mathematical language was invented and we don't write formulas out like 

`some number minus another number times, the first number plus the second number, equals the first number squared minus the second number squared`

in a natural language like English. We just write 

`(a-b)(a+b) = a^2 - b^2`.",ho11ix5,t1_ho0ubao,1639166845.0,False
rdc453,You feel this way as you've never studied math. It can happen.,ho1xtw2,t3_rdc453,1639180055.0,False
rdc453,"I think it’s because you have to take the index of things a lot, but it’s shorter to type i instead of index. And then it just went to j after then and so on and so forth",ho0fmm9,t3_rdc453,1639158151.0,False
rdc453,"It’s mostly for convention. Picking alphabetically close variable names for variables that are related to each other is just common sense. It feels and reads weird, and gets confusing really fast, when you have a group of variables that are related but they all use arbitrary names. Even more so when the proof is several pages long and then you have to remember at each step what each individual variable does/is. 
 
So, if you pick i for something because it reminds you of index, iterator, image, etc. the logical follow up is to pick j after and not ξ, Ω, z, or “mojito”.",ho0lqsm,t3_rdc453,1639160543.0,False
rdc453,"Not sure about all of the common ones but I believe that ""i"" is typically shorthand for ""iteration""? At least that's the impression I got and why I use it. Also ""n"" is just shorthand for ""number"", so that's what I use as the input number for a function that then gets calculated in various ways.

Some don't make sense to me, like the connection weights in artificial neuron functions are typically a ""g"", and I'm not really sure why.",ho0vdkz,t3_rdc453,1639164391.0,False
rdc453,as long it's not i and jota i'm fine. lol,ho0wb6k,t3_rdc453,1639164764.0,False
rdc453,From now on I’m just going to use 39 i’s in a row as a variable to represent row index and 40 to represent column index,ho15myi,t3_rdc453,1639168460.0,False
rdc453,"i = iterator

j = next letter in the alphabet, if you've already used i as an iterator, same as k etc.

It really does make some amount of sense.",ho1v6je,t3_rdc453,1639178905.0,False
rdc453,*foo* rules!,ho1vgxi,t3_rdc453,1639179030.0,False
rdc453,felt this way starting algebra as I always thought multiplication was the 'x' symbol I asked why the hell would they make 'x' different,ho217qq,t3_rdc453,1639181532.0,False
rdc453,"I find it simple.

i, j, k for loops

m, n for matrix dimensions

p, q for probabilities

u, v for random variables

The point is, they are almost exclusively used as throwaway variables (index, dimensions, temporary, etc.). It’s very easy to understand what arr(i,j,k) means. arr(row,col,depth) isn’t as easy on the eyes. Also, there are languages where row and col may be reserved for functions.

In some languages, i refers to complex numbers so some people use ii instead to distinguish the i complex number from the ii variable.",ho2dct7,t3_rdc453,1639186951.0,False
rdc453,"Some fields of math likes to use character set to denote type, so you will have vector v in one vector space being projected into vector ν (Greek letter nu) in another vector space. Physics likes to do the same thing too with v and nu being the velocity of some object in different frames of reference. It's because all those academics don't need to maintain a code base over two decades of enhancements and bug-fixing.",ho2ej54,t3_rdc453,1639187474.0,False
rdc453,I will use capital i and l together. Thanks!,ho2hjzq,t3_rdc453,1639188856.0,False
rdc453,"LOL. OP complaining about i, j, k has clearly never encountered ξ in the wild.",ho2if1t,t3_rdc453,1639189244.0,False
rdc453,I’ve been revisiting my maths lately and have found that letters as variables can be easy to visually distinguish if you write the variables in cursive. I and J would not look the same in such a scenario.,ho2uzw6,t3_rdc453,1639195160.0,False
rdc453,I only use cool greek letters like ζ ϕ . It make me look smart.,ho2vq44,t3_rdc453,1639195523.0,False
rdc453,"I feel this also depends on the context. Like, you see those kinds of variables in proofs and in papers because the target audience is people who are interested/working in the field and so they know what they represent and they can make sense out of very quickly. It just gets straight to the point you know.

But then if you are say teaching to kids, then using variables like i, j is not helpful. They should be more meaningful and much more clearer. Like, forced documentation in a way. This allows less confusion and makes it easier for kids to understand for instance.

But I am with you. I tend to have long variable names because it's just so much more clearer to me when I am coding. And for anyone reading the code as well. Yes, it can clutter but there are times when it's helpful to have long names for certain code snippets. Not for everything but for some.",ho37t3j,t3_rdc453,1639202087.0,False
rdc453,Sometimes i use funny variables to fuck with my bosses,ho3d62u,t3_rdc453,1639205505.0,False
rdc453,"i for index, j comes after. They aren't that hard to distinguish.",ho3e76o,t3_rdc453,1639206225.0,False
rdc453,"Man, many people don't seem to actually read your post before commenting 🤦",ho3hpmg,t3_rdc453,1639208853.0,False
rdc453,Im pretty sure i is short for index and j is just the next letter. n is short for number and m is just the next letter and p is short for prime and q is just the next letter,ho3ndik,t3_rdc453,1639213230.0,False
rdc453,Because it’s more accessible to use pairs of similar letters to define similar things,ho3pdf3,t3_rdc453,1639214850.0,False
rdc453,"well matematician are a weird bunch, but every human being is not exempt. let them have their convention. as a programmer i learned that different lingo and conventions are used in each work environment so it's really not only a problem of mathematics.

i'm all for longer and auto documenting variables, but it's always a failed crusade from the start.",ho4cs00,t3_rdc453,1639230830.0,False
rdc453,"I study quite a bit of mathematics. I see (u,v), (m,n), (I,j), (alpha,beta) - and more - all over the place.",ho4i70i,t3_rdc453,1639233586.0,False
rdc453,"I prefer to choose these
I,k,n
t,p
Also a,b,c in some cases. Because they are distinguishable.
And just avoid mirrored or cloned letters all together.
No q,m,j (especially fuck j).
Also im annoyed by people who use single letters T,U,V
For generic parameter names. Hope they burn in hell.",ho4j6hs,t3_rdc453,1639234076.0,False
rdc453,"Now I want to use greek letters for my variable names. ([Swift](https://docs.swift.org/swift-book/ReferenceManual/LexicalStructure.html) permits this.)

Can you imagine something like:

    for ι in Α {
        ...
    }

Better yet, Swift allows custom operators to be defined, which means I now want `∈` to be defined as a binary operator to perform a 'set contains' operation.",ho4kjc6,t3_rdc453,1639234744.0,False
rdc453,u v was the worst for me when turning Assignments that are hand written,ho4yrrx,t3_rdc453,1639241068.0,False
rdc453,"Always have in the back of my head that ""i' stands for index and ""n"" for numbers... #OkayCaptainObvious :|",hpktxwk,t3_rdc453,1640193800.0,False
rdlhtp,"Whats wrong with 1234567899, or am I missing something?",ho1zl62,t3_rdlhtp,1639180816.0,False
rdlhtp,"Sounds like monkey could be at 5, you open and close 4, then monkey goes to 4 and you open 5... And you missed it",ho200pf,t1_ho1zl62,1639181002.0,False
rdlhtp,"Let's assume the monkey started at cage 2, after you closed door 1 he went to cage 1 and now he is behind you and can just go 12121212 and you'll never catch him.",ho1zx8i,t1_ho1zl62,1639180960.0,True
rdlhtp,"Ok, I think I was presuming the monkey couldn't jump into a cage that you had just opened.

But I'm still not sure I understand why your solution (23456789987654322) *does* catch the monkey. If the monkey is at 1, you start at 2 and start moving up the cages. Meanwhile the monkey moves from cage to cage in the lower numbers. You get to 9, then start moving back down again, and then at some point you get to one-cage to the right of the monkey, open it, monkey isn't there, and in the next move the monkey moves into the cage (say, 4), and you continue moving down the cages whilst the monkey moves up and you still don't catch him..

Or am I still missing something?",ho21a23,t1_ho1zx8i,1639181561.0,False
rdlhtp,"The monkey can only move to the adjacent cage, so if it starts in 1: at the moment you open cage 9, it must be in a cage with even index. By choosing the 9 again, you know the monkey moved into an odd cage, so it cannot be adjacent to you. As you go back downwards, the monkey must also move away and will never be adjacent and cannot escape to the higher cages.",ho4nzxl,t1_ho21a23,1639236389.0,False
rdlhtp,">so if it starts in 1: at the moment you open cage 9, it must be in a cage with even index.

Ok, so when you open cage 9 for the first time, lets say the monkey is in even-numbered-cage 6.

>By choosing the 9 again, you know the monkey moved into an odd cage

Ok again, so when you open cage 9 for the second time, lets say the monkey moves from 6 to an odd-numbered cage, as you say, so lets say 7

>As you go back downwards, the monkey must also move away and will never be adjacent and cannot escape to the higher cages.

Wait what? The next move from OPs example was to open cage 8. No monkey in cage 8, so close the door. Monkey moves from 7 to 8. OP then continues moving down to 7 and so on, missing the monkey.

Sorry for dragging this out, I'm just not sure I understand the problem..",ho4rv8s,t1_ho4nzxl,1639238096.0,False
rdlhtp,">Ok, so when you open cage 9 for the first time, lets say the monkey is in even-numbered-cage 6.

>Ok again, so when you open cage 9 for the second time, lets say the monkey moves from 6 to an odd-numbered cage, as you say, so lets say 7

Sorry for the misunderstanding. I meant it moved into an odd cage after we opened 9 for the first time, so at the moment we are opening 9 again, it is already in an odd cage.

Step by step from this point:

* Monkey can be in 2, 4, 6, or 8
* We open 9, monkey moves to 1, 3, 5, 7, or 9
* We open 9 again, monkey is not there (so he is in 1, 3, 5, or 7), he can move to 2, 4, 6, or 8
* We open 8, monkey is not there (so he is in 2, 4, 6), he can move to 1, 3, 5 or 7
* ...
* We open 3, he is not there, so we know he is in 1 and has to move to 2, so we open 2 and find him",ho53iyb,t1_ho4rv8s,1639243048.0,False
rdlhtp,"I'm confused about the problem description as well but I think in your example if the monkey was in 2 to start and you opened 1, when you close 1 it could move to 1 and then you'd be checking empty cages for the rest.",ho20whb,t1_ho1zl62,1639181393.0,False
rdlhtp,Try to use extended pigeonhole principle,ho262s5,t3_rdlhtp,1639183676.0,False
rdlhtp,"I forget all the formal terms and rules for mathematical proofs so apologies in advance for the mistakes.

  
Fact 1: After even number of moves monkey will be on same parity (odd/even) as it started, after odd number of moves it will be on different parity.

  
Start at door 2, go up 1 door at a time (2, 3, 4,...)  


Fact 2, starting from the left and working your way up one door at a time, if monkey starts on same parity door you start on monkey will be caught at worst when you get to n - 1 door where n is the number of doors.

  
Proof by induction

  
base case 3 doors n = 3

  
proof by exhaustion

If you start on door 2 monkey must be on 2, you open door 2 and catch him

  
If you start at 1 monkey must be on 1 or 3. If on 1 you will catch on first check. After first check monkey must move to 2. So on second check you will catch it. 3 - 1 = 2

  
case k + 1 (4 doors) n = 4  
proof by exhaustion

If you start at door 1 monkey and monkey starts on door 1 you will catch it on first check.

If you start on door 1 and monkey starts on door 3 you miss first check, if it moves to door 2 you will catch on second check as shown in base case. If monkey moves to door 4 on second move you will miss on checking door 2. But on 3rd move it must move to door 3, you will check door 3 and catch it.

If you start on 2 and monkey starts on 2 you will catch it on first check.

If you start on 2 and monkey starts on 4 you will miss first check, it must move to 3 and you will catch on second check.

  
By induction if monkey starts on same parity as you you'll catch it at worst on the n - 1 door. Fact 2 is true.

  
So starting on door two and working your way up, if you get to the n - 1 door (in our case 9) and there is no monkey, you know that the monkey couldn't have started on an even # door or else we would have caught it according to Fact 2. So this means the monkey started on an odd number door. After checking doors 2-9 (8 moves) the monkey must currently be on an odd numbered door according to Fact 1.  
We know the monkey must be on an odd door, so using symmetry we know if we start on an odd door (9 in this case) and go back down in same manner (9, 8, 7,...) and we know that we will catch the monkey at worst at door 2. So for 10 doors 2345678998765432 will at worst catch monkey on the last door. Just drop the last 2 off your solution.",ho50t2b,t3_rdlhtp,1639241909.0,False
rdlhtp,"The solutions we found is:
23456789987654322",ho1tlqy,t3_rdlhtp,1639178234.0,True
rdlhtp,"I think that the solution of 234...(n-1)(n-1)...432 is enough, so in your example with n=10 it would be 2345678998765432. 

You already explained in a comment that if the monkeys starts in an even numbered cage, we will catch him in the first ""half of the run"". I cannot proof this assumption but it seems to be true. I tested this with numbers up to n=6 and I could not find a counter example. I know this is not a proof, but I firmly believe this assumption is true.
Following this assumption, this means the second half of the run is only entered, If the monkey starts at an odd numbered cage. The two times we open cage at position (n-1) imply, that beginning from the second half of the run, every time we open an even numbered cage, the monkey is in an even numbered cage. Same goes for odd positions during the second ""half"", if we open an odd numbered cage, the monkey is in an odd numbered cage. So the monkey is not able to cross us.
Thus the opening of cage 2 two times in the end is unnecessary, opening cage 2 one time in the end is enough: if we open cage 3 during the second half, the monkey is in cage 1. Monkey then has to move to cage 2, and we open cage 2 afterwards.

So all in all, the 234...(n-1)(n-1)...432 approach gives us 2*(n-2) cages to be opened at maximum, which would be 16 for n=10. Please correct me, if I did a mistake anywhere!",ho42gku,t1_ho1tlqy,1639224667.0,False
rdlhtp,"Absolutely right! Thanks
I wonder if there is an even better solution that is based on other mechanism, but it seems pretty optimal.",ho4306r,t1_ho42gku,1639225044.0,True
rdlhtp,[deleted],ho2xknw,t1_ho1tlqy,1639196467.0,False
rdlhtp,I used 1-10 and not 0-9,ho3hsl3,t1_ho2xknw,1639208918.0,True
rdlhtp,"So the reason this solution works is this:
If the monkey starts on an even numbered cage (6 for example), you'll catch him in the first half of the run, because Any time you're opening an even numbered cage he will be in an even numbered cage and same for odd numbers. He can't cross you and thus you'll catch him.

If the monkeys starts in an odd numbered cage, the doubled 9 in the middle will make it so that you will open an odd numbered cage at the same time as the monkey will be in one. After that, when you'll go down the cages each even numbered cage you open, the monkey will be in an even numbered cage (same for odd numbers) and you'll catch him.

You can try it with 4 or 6 cages just to get the concept but it works the same for 10.and above.

I'll try to make an animation of this or something later.
If there are more questions about the riddle ask in the comments.",ho24jn7,t1_ho1tlqy,1639182997.0,True
rdlhtp,"Your post says: 

>the monkey can go to a previously opened cage

But your comment says: 

>he can't go to a cage you opened.

Isn't this a discrepancy?",ho2dle2,t1_ho24jn7,1639187057.0,False
rdlhtp,"Right, sorry about that.
Updated it, I hope now it's more understandable",ho3i3w5,t1_ho2dle2,1639209149.0,True
rdlhtp,I'm assuming which passage the monkey goes through is random?,ho219av,t3_rdlhtp,1639181552.0,False
rdlhtp,"Well yes but you should consider every move he can make, and find a solution that solves every move of him.
You can assume he'll know what door you are going to open next and try to find the best move for himself based on that knowledge.",ho254gg,t1_ho219av,1639183256.0,True
rdlhtp,"The solution depends on the number of cages. I'm using 0 indexing.

The idea is as follows. Guess 0..1...2....n-1. If the monkey was initially on an even index, we will always win in this first iteration. This holds for the general case where - if the monkey is on our right, and we guess an even index while the monkey is on an even index, we will always find it by linearly scanning.

So our goal now is to somehow ensure this happens.

With an even number of cages, if we guess 0...1...2...n-1, and we don't find the monkey, then it must have started on an odd indexed cage. After an even number of guesses, the monkey will be on some random even indexed cage, so if we guessed cage 0 twice ( to force the monkey to go odd -> even ). After that second guess, we've now guessed an even cage whilst the monkey was on an even index, so we know we will win.

Similarly, with an odd number of cages, we can do a linear scan once, but we only guess cage 0 once as well, as after an odd number off guesses, our orginally-odd-monkey would be on an odd cage, so if we guess 0 after n-1, we will once again gaurentee that we guess an even index while the monkey is on an even index as well.

This problem fucked me up because I initially thought there was a way to solve it with (a natural) recurrance.

For example, I tried to solve the cases n=1...4 by proving you can guess index 1 twice to gain information, ie that the monkey must be on the right as it couldn't be on 0.",ho5f20s,t3_rdlhtp,1639247847.0,False
rdlhtp,"Solution: 2, 4, 6, 8, 10, 1, 3, 5, 7, 9

This is a variation of the rabbit hole problem. If you play with the problem, you'll realize that the possible location of [the monkey oscillates between even and odd numbered cages](https://i.imgur.com/pQcCT5t.png).

The problem can be solved in linear time by guessing over all even cages first. Then all odd cages next. (or vice versa)

Here is sample code you can play around with using the rabbit hole problem (with a 100 holes). You'll notice that you'll always find the rabbit no matter how many times you run it.

    from random import randint
    
    mn = 0
    mx = 99
    
    def jump(rabbit):
      if rabbit == mn:
        return mn + 1
      
      if rabbit == mx:
        return mx - 1
      
      if randint(0, 1) == 0:
        return rabbit + 1
    
      return rabbit - 1
    
    
    def guess(rabbit, hole):
      if rabbit == hole:
        return True, rabbit
      
      return False, jump(rabbit)
    
    
    def main():
      rabbit = randint(mn, mx)
      caught = False
    
      for hole in range(mn, mx + 1, 2):
        caught, rabbit = guess(rabbit, hole)
        if caught:
          break
      
      if not caught:
        for hole in range(mn + 1, mx + 1, 2):
          caught, rabbit = guess(rabbit, hole)
          if caught:
            break
      
      if caught:
        print(f""Found rabbit in hole: {hole}"")
      else:
        print(""Did not catch the rabbit. :-("")
      
    
    
    if __name__ == '__main__':
      main()",hobgdjy,t3_rdlhtp,1639355781.0,False
rdlhtp,"This solution is easily proven wrong.

Assume monkey is in cage 6

G = your guess (the cage you open), M = the cage the monkey moves to

G2, M5 (you open cage 2, monkey moves to 5)

G4, M4  (the monkey moves to 4 after you close the cage

G 6, M5

G8, M6

G10, M7

G1, M6

G3, M7

G5, M6

G7, M7

G9, M6",hojqxpf,t1_hobgdjy,1639509973.0,False
rdlhtp,"Simplest, easiest solution: 
Open all the cage doors, without closing them. The monkey only moves when you close the door. This method will ensure the monkey is found within ten cages. 

Other solution: 
Stick your head into the cage and look down the passage. Assuming the passages are straight (given the cages are adjacent), you will have found the monkey. This method only takes opening one cage, and is therefore the ideal method. Of course, this approach may result in you getting your face scratched at or your head stuck in a cage. However, the problem only specified that the monkey needed to be found, not that the finder needed to emerge from the ordeal unharmed, and therefore this solution is completely functional. Q.E.D.",hor2qyy,t3_rdlhtp,1639637183.0,False
rdlhtp,"If there are n cages, the solution is:

1234....(n-1) n n (n-1) ....4321

The reason this works:

The distance function between the cage we look at and the cage the monkey is in is non-increasing in this case. However, parity of the distance function does not change as we go through 123...(n-1)n

So, if the parity was even when we checked cage 1, then we catched the monkey.

If the parity was old, we will catch him in n (n-1) ... 3 2 1.",ho29z1k,t3_rdlhtp,1639185426.0,False
rdr5st,"Learn the basics of a coding language (I don’t know js but I’m sure it’s a fine language with many pros and cons). Figure out how to solve fuzz buzz type challenges. Once you got those down, learn data structures by reading the theory then implementing them in your chosen language. Don’t just use the built in implementation if it exists, actually build it. Then learn algorithms the same way. I would guess that this task would take high 100’s of hours to lower 1000’s of hours. 

Non technical suggestions:
Have fun with it, if you get side tracked in a project or learning that’s a good thing. It means your having fun. 

Learn to google thing well and stack overflow is your friend. 

Find a supporting community to get you through the sucky times while learning and debugging code because there will be those times

Good luck",ho318j8,t3_rdr5st,1639198366.0,False
rdr5st,Thank you. Do you recommend learning data structures and algorithms through a book or online courses?,ho32rae,t1_ho318j8,1639199192.0,True
rdr5st,"I took a university course so I don’t know how much my experience will help. However I would recommend using the free resource on YouTube and a text (you can probably find one for free online). Follow the textbook as your lesson plan. Do your reading, then try and find a YouTube video if you don’t understand the topic. If you still don’t understand the topic write out a post as detailed as possible about what you do and don’t understand and ask here.

That is pretty similar to a university course with assigned readings, lectures and office hours if you need help. 

I just want to emphasize if you ask questions online, the more work you put into your question the more people will want to help you and the better help they’ll be able to give.",ho33t0y,t1_ho32rae,1639199776.0,False
rdr5st,Just do as many projects as you can. Comp sci/data anything is in such high demand if you have a couple personal projects you cna fully explain they’ll take you like no other. Get dat 100k son!,ho3d9r4,t3_rdr5st,1639205574.0,False
rdr5st,"I would focus on the programming language first.

Based on the gained know how I'd suggest you practice / play around with different data types, algorithms, etc.

You might want to use blogs, leetcode and / or ask concrete questions on some problems you encounter by the means of stackoverflow or reddit.

After some time and with some experience you might start to consolidate your knowledge by reading some theoretical CS sources.",ho3f6fa,t3_rdr5st,1639206944.0,False
rdo7ab,"I think the mistake is assuming that it is unsigned when it is adding and 2’s complement when subtracting. That is possible, but the computer will rely on other things telling it that. You could look at it as always being 2’s complement and the computer uses other logic (either in software or hardware) to determine if the overflow will cause a bug.",ho31ogs,t3_rdo7ab,1639198605.0,False
rdo7ab,"what twos complement overflows then it wraps. some processors will throw an error or raise a flag, and on others you have to write code yourself to check for it. 

7 + 1 = 0111 + 0001 = 1000 = -8

\-8 -1 = 1000 + 1111 = ~~1~~0111 = 7

[https://en.wikipedia.org/wiki/Overflow\_flag](https://en.wikipedia.org/wiki/Overflow_flag)

on x86 JO is ""jump if overflow"" using the overflow flag

[https://stackoverflow.com/questions/48619934/mips-overflow-detection-printing-the-result](https://stackoverflow.com/questions/48619934/mips-overflow-detection-printing-the-result)

this is a page about detecting overflow in mips (which offers no hardware detection of overflow)",hodle15,t3_rdo7ab,1639402772.0,False
rcla2n,"NFTs in theory are one thing, but in practice they are nothing more than digital beanie babies or trading cards, except you don't actually own the beanie baby or trading card and just have a receipt for it. It's a disaster and [the top 10% of traders for NFTs have traded 97% of all assets](https://www.nature.com/articles/s41598-021-00053-8#:~:text=the%20top%2010%25%20of%20traders%20alone%20perform%2085%25%20of%20all%20transactions%20and%20trade%20at%20least%20once%2097%25%20of%20all%20assets) and the ecosystem is rife with fraud and wash trading (trading with yourself to inflate the perceived value of an NFT).",hnwetsd,t3_rcla2n,1639083631.0,False
rcla2n,"You *do* own the NFT, it's just that the NFT *is not the fucking art.* It's too expensive to hold an image on the chain, so all of those NFT images are stored on bog standard servers that *will* go down. Probably immediately after NFT's stop being a fad, for the big players, or when the creator dies, in the case of something like etherrock, in which case it'll be when he dies (or gets bored) and someone buys the domain and replaces the images with something like goatse.",hny4gum,t1_hnwetsd,1639110526.0,False
rcla2n,"Too expensive on transport protocols that require gas fees that are exuberant. We can store entire blu-ray movies in a decentralised manner with little to no cost. Non-fungible tokens aren’t limited to the transport layer they reside upon, they are method to verify ownership - just as pgp was used to verify authors. 

Cryptocurrency on the other hand is generally the protocol and the ledger it resides upon and none have are even close to getting that right just yet. Mining, staking, etc are no the answer.",hnyqum8,t1_hny4gum,1639124365.0,False
rcla2n,"The actual technology behind them tho is elegant. I believe OP came here looking for scientific reasoning, not scorn opinions. A decentralised global database where multiple applications can access and have verifiable proof that the data is what it says it is.",hnyoxfb,t1_hnwetsd,1639122866.0,False
rcla2n,"NFTs seems like mostly nonsense in their current applications (or more precisely, the applications I see most prominently featured in high profile discussions). It seems like an elaborate trick to make non-technical people believe that there is some form of scarcity for digital objects, which is ridiculous because they are inherently fungible. 

The idea of owning an NFT for an artwork is essentially meaningless. It's not tied to any kind of copyright law, and there is nothing stopping someone else from using  artwork for their own commercial purposes. It's barely a step above those websites that sell asteroids or stars to naive people. If asteroid mining becomes a thing, SpaceX isn't going to need your permission to mine all the precious metals on ""your"" asteroid because you have a certificate from a hack website. NFTs are just the same scam dressed in fancier tech jargon.",hnxbmbw,t3_rcla2n,1639097568.0,False
rcla2n,"> those websites that sell asteroids or stars

This is an excellent analogy, I'm using that, thanks.",huqat9q,t1_hnxbmbw,1643471187.0,False
rcla2n,"I foresee issues with the availability and scalability of a decentralized data store. Can we trim old data off the ledger to maintain its size growth? Will these systems be truly decentralized, or will there be some authoritative servers around to ensure low latency of queries?",hnwm220,t3_rcla2n,1639086571.0,False
rcla2n,"My feelings on it seem to flip every week or so, but I always come back to the opinion of it just re-inventing the wheel. I think there are great use cases for crypto like the transfer of money on a global scale, instantly. Other than that, I think it’s just an over-engineered concept that is in a saturated market (FinTech). 

Some of the qualities that Crypto has just don’t make sense to me on how they help the average person do average financial tasks. 

An open ledger of every transaction? Ok, cool. I’ll probably look at it once then never again. 

Decentralized assets? Neat and some people’s cup of tea, but now I have all the responsibility of managing them. 

I don’t think Crypto will ever go away. I think history tells us not to bet against innovation. In the current state, I do hope the Crypto bubble implodes like the Dotcom bubble. Too many scams, too many get rich quick mindsets, and too many shitty opinions (like mine lol)",hnw7mgc,t3_rcla2n,1639080734.0,False
rcla2n,Most FinTech companies are just wrappers around the existing system that hide things like delayed settlements and give you nice APIs. The fact that so many 100+B payment companies exist shows how much money is being made by being a middleman. Things like micropayments will never be possible with the existing system. With crypto this is possible. For example with brave you get payed in BAT for the ads you see.,hnx2ycy,t1_hnw7mgc,1639093684.0,False
rcla2n,"Can you elaborate more on micropayments not being possible with the current system? I sometimes use Brave and loved the idea of their BAT system. 

Other than that, what other micropayments are out there? How is it not possible with the current system? 

I have used mobile apps before where you can rack up points while using the app’s particular function (granted I can’t remember the name). Credit card rewards are basically the same thing as Brave’s Bat system. I can transfer those rewards into cash. I don’t see how crypto makes any new leaps in that department",hny39gr,t1_hnx2ycy,1639109948.0,False
rcla2n,"Modern banking is built on ACH, which is where the stuff he's talking about comes from. It's got a 3 day clearing time, when micropayments need to be faster. Of course, building a replacement for that on a slow, expensive, polluting chain is stupid and can't actually deliver. Fortunately, the fed is creating an ACH competitor called [FedNow](https://www.frbservices.org/financial-services/fednow/about.html) which should hopefully drive these middlemen out of existence, or at least lower their take.",hny543b,t1_hny39gr,1639110836.0,False
rcla2n,Here in EU we already have instant bank transfers within countries and soon also instant bank transfers across the EU. So instant crypto transfers hasn't really covered many use cases here.,hnyassb,t1_hny543b,1639113794.0,False
rcla2n,"As others have mentioned there are now system that allow instant transfers but I don’t think something like BAT would be possible with these systems. They are also only available regionally and in their local currencies. 

In addition to micropayments tokens make new incentive structures possible. For example early facebook or twitter users didn’t get any upside for participating but where some of the most important users. With tokens you could pay early users. This leads to super charged growth since their are more incentives to use a new product. The equivalent in the fiat world would be to give early users stock but this is regulatory nightmare and can definitely not be done in a micropayment way.",hnyhocf,t1_hny39gr,1639117880.0,False
rcla2n,"I understand what crypto and a NFT is. I thought crypto was cool back in 2010 when I first found out about it.

Nowadays crypto (bitcoin) has so many problems imo. Electricity usage, money laundering, used for illegal activities, etc.

NFT just seems like a pyramid scheme / bubble. But it could go on for a long time. Im happy with owning stocks and not messing with crypto / NFTs personally. 😊",hnvfxh7,t3_rcla2n,1639069810.0,False
rcla2n,"[1% of crypto is used for illegal purposes](https://www.forbes.com/sites/haileylennon/2021/01/19/the-false-narrative-of-bitcoins-role-in-illicit-activity/).  Regular cash is close to 5%. I don't get why people wouldn't be interested in this technology.  

The idea of a tamper proof [block chain](https://www.youtube.com/watch?v=bBC-nXj3Ng4) is also very interesting to me from a technical aspect. I've seen governments and organizations even testing out the block chain to store documents as a test to see if it could increase transparency. 

Ethereum is a [virtual machine](https://www.youtube.com/watch?v=gjwr-7PgpN8) that's decentralization.  It might not be practical or have a ton of use cases but the concept is wild isn't it?



[DAO](https://www.youtube.com/watch?v=KHm0uUPqmVE) and [smart contracts](https://www.youtube.com/watch?v=pA6CGuXEKtQ) are again yet another interesting development.

Am I wrong in thinking this stuff is interesting.  Are they dead ends?",hnwrbi9,t1_hnvfxh7,1639088719.0,False
rcla2n,"Imo you've done a great job of encapsulating how I feel about cypto. Which is, a bunch a vague ""cool ideas"" and ""interesting concepts"" that are ultimately _currently_ useless, for most people. I'm sure the very small (in global terms) number people who have become extraordinarily wealthy overnight vehemently disagree with me, but that does not a practical use case make.",hnxbd3x,t1_hnwrbi9,1639097452.0,False
rcla2n,Haha exactly. I suppose  I don't know enough to see it as useless right now. I'm old enough to remember the same comments about Netscape and such.,hny76ep,t1_hnxbd3x,1639111847.0,False
rcla2n,"Doesn't matter if it's interesting, it's using the energy of a small country while making less transactions than your average bank, this is a huge issue and anyone who pretends it not is deluded.",hnx7bbe,t1_hnwrbi9,1639095621.0,False
rcla2n,"Nothing is free in life.  The energy is used to secure the global network and valid every transaction. Think about how much bitcoin is worth. That energy is doing something and it's still less than the phantom energy that's drawn by people leaving their electronics plugged in all the time.  At the same time, proof of stake consensus will use a fraction of what proof of work uses.  Saying all the, adding more transactions doesn't burn more energy.  The energy is consumed by miners competing.",hny8qzz,t1_hnx7bbe,1639112679.0,False
rcla2n,The phantom energy is contributed by millions if not billions of people. Crypto miners/users/traders make up a tiny fraction of this,hnyu5ve,t1_hny8qzz,1639127116.0,False
rcla2n,"Yet that energy in crypto is securing and validating billions worth of wealth. It is doing work.  Like the phantom energy problem, it's disturbed and therefore large.  But unlike the phantom problem, people are actively working to solve and innovate ways to reduce that problem.  But at the end of the day it's still doing work.",hnzagve,t1_hnyu5ve,1639139720.0,False
rcla2n,"[citation needed] on that energy comparison to phantom energy. I think that’s complete rubbish, commonly spread by Bitcoin advocates to distract from its fatal energy problem.",hnylilk,t1_hny8qzz,1639120395.0,False
rcla2n,">Bitcoin consumes a sizable amount of electricity. As of June 2021, estimates suggest something around 110 terawatt hours (TWh) per year, which, for scale, is close to the electricity consumption of the Netherlands (111 TWh) but a bit less than the global ‘phantom’ electricity consumption from electronics that are left plugged in while in standby mode (124 TWh). 


https://www.coincenter.org/education/crypto-regulation-faq/understand-bitcoins-energy-use/

Crypto Currency has an energy problem. I'm not going to argue its fine. But people are trying to solve it. The energy used also does work. It secures and validates all transactions worth billions of dollars. One of my first classes in school the prof detailing how writing buggy code that gets distributed globally ends up wasting energy. Energy consumption isn't just a bitcoin problem. But the energy it consumes doesn't mean what it does is useless or not worth learning about. The consensus mechanism are super interesting to me. I'm reading proofs and white papers and digging into how cryptography works because the whole concept of this network is fascinating.",hnzgntb,t1_hnylilk,1639143241.0,False
rcla2n,"Crypto is used way less than cash for illegal activities. The electric usage will be hugely mitigated when the blockchain will switch to a new transaction verification system call proof of stake. Ethereum (the 2nd biggest) will switch soon next year, countless others already did. 
I think it's really superficial to just mention the issues that are already getting overcame, without mentioning its possible use cases and the blockchain potential.",hnvwuh2,t1_hnvfxh7,1639076421.0,False
rcla2n,"I hope crypto does well, im just not willing to bet money on it going up from here.

I live in Sweden where almost no one uses cash anymore. Everything is card or swish (free instant digital money transfer). Sure it would be nice if something like Swish existed globally, which was the hopes of bitcoin several years ago. There are still problems that need to be solved though like money laundering, transfer fees, exchanges getting hacked and you losing all your coins 😊",hnvy1lb,t1_hnvwuh2,1639076910.0,False
rcla2n,"I also think a thing that got in the way of crypto is that it became an investment rather than money, specially in the mainstream. It feels like most people who get into crypto are in it to make money, rather than the actual reason crypto was first invented (and the reason you probably thought it was cool in 2010 but dislike it now).

Also, the fanbase. Like so many other things the crypto/nft fanbase is cringy to the point of being annoying and I wouldn’t want to deal with them, and that includes financial transactions and such.",hnw36gc,t1_hnvy1lb,1639078972.0,False
rcla2n,"Yes, that's definitely true, but how else is it supposed to grow as a project and start being actually used? 
Why should someone buy crypto to use it? 
It's just like stocks - most of the times people buy it as an investment to earn money, not really caring about what's behind. The same thing goes for crypto, and that's to be expected - how else is it supposed to become widely used, if not through investing at first? 
It is slowly being adopted widely and having a real use case, but of course it'll take time. 

Regarding the fanbase, sure, can't disagree with you, however that's a big generalization. There is a good chunk of really smart people who are trying to actively improving things in this sector with new projects and interesting discussion about blockchain technology, don't get biased by what you see on the surface, as in most fields the worst part of the fanbase is what makes the most noise.",hnw7n47,t1_hnw36gc,1639080741.0,False
rcla2n,"IMHO, the first part of your comment doesn’t really make sense. Cryptocurrencies are meant to be a currency. This is entirely different from stocks, which are, by definition, an investment. There are valid reasons to use crypto as a currency, too, namely the very reasons it was created for: decentralization, anonymity, universality, etc. In fact, it was used for those ends before it was an investment. However, these are things that the average person doesn’t care about, and “making an investment out of them to begin with” doesn’t change that. Stocks have been around forever now, and they’re still not used as currency (you don’t buy things in the market with stock, or pay for services, etc.) because, again, they’re fundamentally an investment, and not a currency, as opposed to cryptocurrencies. 

I’d go so far as saying that crypto as an investment is inherently a bubble based on hype and misinformation. Stocks are associated with the worth of a company, so, in theory, it makes some sense that the price changes with time, as the company’s value also varies according to how well it performs. Crypto, on the other hand, is entirely based on: will people adopt this currency? However, as long as people face it as an investment, they won’t be using it as currency (because it lacks the stability that a currency needs, after all, if you can’t be sure of its worth tomorrow, you probably won’t want to stake your livelihood on it, which is why people won’t be inclined to take it directly in exchange for goods and services on the mainstream, which is even more reason for it’s instability since it exists only tethered to some other currency, i.e.: it doesn’t have “independent” value, it’s worth only however many dollars people are willing to pay for it at the moment)

So, I’d argue that using crypto as an investment only pushes it farther and farther from it’s intended use, and the only use that I, personally, see value in, which is as a currency.",ho2yyp6,t1_hnw7n47,1639197170.0,False
rcla2n,"It's been in a constant up trend for the past ~5 years, why should it stop now, when more and more people and getting involved? 
Take a look at the big picture and check the price of bitcoin or other coins just 1 year ago and you'll see the huge increase. 
Sure, some problems still need to be solved, but most of them are already solved. Exchanges got really secure and I haven't heard about an attack to a major exchange in the last couple of years. Defi platforms are surely more vulnerable as they are really new, so they are expected to be less secure.",hnw89kt,t1_hnvy1lb,1639080988.0,False
rcla2n,"Because it’s all speculative investing (with no real value), not actually used as a currency",hnw9yb1,t1_hnw89kt,1639081660.0,False
rcla2n,"Just because it's not used as a currency it doesn't mean it has no real value. Also, the current banking system has been used for more than 200 years, do you think it can change in just a few years?",hnwl2i6,t1_hnw9yb1,1639086170.0,False
rcla2n,"Blockchain tech: real value. 

A currency increasing in value by 100% in a day: speculative investing (and a bubble that will crash). 

If it one day doesn’t do this, maybe I’ll reconsider that take.",hnwlkme,t1_hnwl2i6,1639086374.0,False
rcla2n,"It's been ~10 years, and there has been no major crash yet, or no crash that hasn't been recovered, I think that can be a solid proof. 

Obviously, it's still a new sector, so huge changes are expected. But as the total market cap grows, it will become more and more stable. Right now, the total market cap is less than just Apple's, so it has a potential of growing",hnwpj0c,t1_hnwlkme,1639087976.0,False
rcla2n,"Yeah but with this argument, you still aren’t getting it. Bitcoin is meant to be a currency. Bitcoin is not an investment. Nobody talks about currency’s market cap or growth potential. A currency will not be used as a currency if there is a huge potential to grow, because it’s not advantageous to actually “spend” that currency.",hnx9xfo,t1_hnwpj0c,1639096790.0,False
rcla2n,"Bitcoin is not meant to be used as an actual currency. It's meant to be a store of value, like gold. There are other, way more valid candidates to be used as actual currency, with really fast transaction speeds and low (or zero) fees such as NANO. Also, stable coins are also a thing!",hnycyqx,t1_hnx9xfo,1639115044.0,False
rcla2n,Madhof’s Ponzi scheme lasted 30 years.,hnylsm7,t1_hnwpj0c,1639120576.0,False
rcla2n,Apple makes something. Bitcoin does not. A currency should not increase in value like a stock.,hnx3qpw,t1_hnwpj0c,1639094030.0,False
rcla2n,"> It's been in a constant up trend for the past ~5 years, why should it stop now, when more and more people and getting involved?

Said by everyone who was so sure **their** investment bubble of choice would **never** burst like all those *other* bubbles did.",hnxfcm8,t1_hnw89kt,1639099265.0,False
rcla2n,"I genuinely do not understand the downvotes.
Addressing Bitcoin problems when talking about the blockchain technology is such a poor argument.",hnx8f5q,t1_hnvwuh2,1639096114.0,False
rcla2n,"NFTs are just outside of their time; not early or late to the party, just irrelevant for our world, I feel. An NFT could, in theory, represent something like a certificate of authenticity for digital goods or even paired with real goods, but that isn’t how I see this trend going. I don’t think the adoption will allow for something dope like Nike starting an authentication service to issue NFTs of authenticity to help mitigate fakes in the show resale market. I don’t see something like a physical plot of land or house coming with decentralized documentation of ownership in the way deeds exist now. So many other things would need to happen in legislature that never will. 

I’d love to be wrong but I think the concept is meant for a society more capable of adaptation than the one we’ve found ourselves in.",hny3nfb,t3_rcla2n,1639110133.0,False
rcla2n,"As far as my understanding goes, NFTs are just mappings of some asset, be it digital or physical, to a digital signature.

It is the interpretation of this as some form of ownership that people focus on. If the majority does believe that having a record that maps something to you represents this, it effectively becomes the truth.

Same thing for crypto currencies, where value is placed on an association with your wallet and the sum of all your transactions.

Seems arbitrary, but the same thing goes for money - if nobody would believe it has value, it would not have it.

My opinion is still sceptical. Crypto currencies praise themselves as decentralized - which may be true as long as you stay in an ecosystem where all goods and services may be paid for with such a currency - but when you want to exchange them for ""real"" money you need to go to a centralized exchange, where you can be identified and privacy flies out of the window as well.

So yes, as long as there is a more relevant ecosystem outside of crypto, it will not be achieve what it promises to be - while polluting the environment and driving GPU prices up.

As for NFTs, your suggestion about money laundering did not occur to me before that but I would have my doubts (see the above statement about privacy).  


Both currently look mainly like speculative assets, less than stable or serious alternatives or additions to what we have currently - while NFTs may just be a tiny bit weirder than the actual art market \^\^",hnwhq1o,t3_rcla2n,1639084806.0,False
rcla2n,"> It is the interpretation of this as some form of ownership that people focus on. If the majority does believe that having a record that maps something to you represents this, it effectively becomes the truth.

Wrong. Ownership requires control, but NFT's provide no control over the image, only the receipt on the blockchain.

> Same thing for crypto currencies, where value is placed on an association with your wallet and the sum of all your transactions.

Wrong, not the same. You can control what happens to the crypto in your wallet.

> Seems arbitrary, but the same thing goes for money - if nobody would believe it has value, it would not have it.

Mostly wrong, cash's value is maintained by the fact it's the only way you can pay taxes and therefore participate in the US's (or whichever countries) economy. So it's only as valuable as the ability to participate in the economy, which can be torpedoed if that suddenly becomes less valuable through various mechanisms, some of them with a psychological component. 

Crypto's largest selling point is the trustless ability to conduct transactions. Not useful in the modern, high trust world but extremely useful in trustless exchanges such as for crime, drugs, or scams, which it was immediately used for and then for basically nothing else.",hny708b,t1_hnwhq1o,1639111760.0,False
rcla2n,">Wrong. Ownership requires control, but NFT's provide no control over the image, only the receipt on the blockchain.

True that. You can not effectively control your asset through that association - people would *really* have to believe in NFTs to restrict ther usage to only you!

&#x200B;

>Wrong, not the same. You can control what happens to the crypto in your wallet.

No question about that - I was trying to point out that the perception of value is similar.  
But you could also argue that you have ""control"" over which NFTs you ""own"", since you can decide to purchase them. Their value (like crypto currencies) is then just a result of what others believe it to be.

Sure, ""leaving"" conventional money would be a whole lot more complex than something like Blockchain, but that tail of taxes, economy is merely the result of people seeing it as something valuable for a very long time.

I fear that my comment seems a little too pro-Blockchain, so let me just note, that this is absolutely not the case! Mainly I wanted to point out that people have alway put value in objectively useless assets and that these hype-things are just another incarnation of it, solving mostly issues that were tailor-made for them.",hnymng5,t1_hny708b,1639121173.0,False
rcla2n,">	cash's value is maintained by the fact it's the only way you can pay taxes and therefore participate in the US's (or whichever countries) economy.

I don’t think cash’s value is so necessarily tied to taxes. If the US government decided tomorrow that it would continue to back and support the dollar through the Fed, but not charge taxes, I don’t think people would consider the dollar to have lost value as currency.

Rather, cash’s value is based on a shared belief in that value as it relates to goods and services. It’s that shared belief that means that we can agree that a dollar is worth a glass of lemonade, or fifteen dollars is worth an hour’s labour at a convenience store. The goods or services that a dollar is worth can change over time. There is no equivalent for crypto - its value is determined by how many dollars (or other fiat) it’s worth, not how much it can be traded for in goods or services. So long as crypto measures value against fiat, it’s not a currency, it’s a good (in this case, a speculative investment).",ho6lwvz,t1_hny708b,1639266846.0,False
rcla2n,"Cashes *definite* value is a shared fiction, but the reason it *has* that value is because all goods and services are *forced* to be related to it or else the government will punish you. Without that universal value assignment, cash becomes just strips of paper.

> If the US government decided tomorrow that it would continue to back and support the dollar through the Fed, but not charge taxes, I don’t think people would consider the dollar to have lost value as currency.

First of all, that support isn't cheap. Financial crimes, counterfeiting, trade management, political ties, and national self defense *all* feed into the value of the dollar, to say nothing of the vast amount of infrastructure, laws, education, enforcement, etc that keeps the 300 million person society ticking. Those 300 million people form a market whose lingua franca is the dollar, and *everything* they do is based on that dollars value. Without taxes, they can do whatever they want in any medium of exchange they want. ""Taxes give the dollar it's value"" isn't just a simple statement, it captures the fact that without taxes *there is no dollar.*",ho6whd1,t1_ho6lwvz,1639271881.0,False
rcla2n,"I think it's a sad joke. The only way it is sustainable (and I use that word very loosely) is if you perpetually manufacture artificial scarcity.
It remains viable only by sustaining growth, ergo growth by artificial scarcity. Growth for the sake of growth is - well - the very definition of cancer. NTF and crypto are cancerous, and operate on the same model as Capitalism, quite frankly. We do not live in an infinite-growth paradigm and any model based on that will end disastrously.",hnwq89e,t3_rcla2n,1639088267.0,False
rcla2n,"Still waiting for crypto to offer one, just one of all these hypothetical use cases that would be of any value to me, because I have been reading about how it is set to revolutionize X, Y, and Z for years now. Does anyone use DLT for anything at all in their daily lives, other than speculation? It doesn't seem like asking much given the insane amount of brain power and capital that has poured into crypto over the past decade. Decentralized money is still cool, admittedly.",hnwoxpz,t3_rcla2n,1639087733.0,False
rcla2n,"There are many interesting use cases for crypto/Blockchain tech, the issue really comes with the scalability, or lack of it, which renders most of those use cases totally invalid.",hnx547y,t1_hnwoxpz,1639094629.0,False
rcla2n,blockchain has many use cases and I personally know if it being used in chemists to track supply - does the agency I know of that is using this utilise or have a need for a token? no,hnyra02,t1_hnwoxpz,1639124705.0,False
rcla2n,"Monero is pretty cool if your use case is money laundering, otherwise it's all useless",hnyazh1,t1_hnwoxpz,1639113896.0,False
rcla2n,"Not as useless as ur mother
***
^I ^am ^a ^bot. ^Downvote ^to ^remove. ^[PM](https://www.reddit.com/message/compose/?to=YoMommaJokeBot) ^me ^if ^there's ^anything ^for ^me ^to ^know!",hnyb2aj,t1_hnyazh1,1639113941.0,False
rcla2n,"Nfts will be used by ticketmaster for tickets. This alone if it works will be huge as all baseball/football games tickets could move that way as well as theatre cinema tickets, it creates the possibility for secondary markets, you can then have follow up “souvenirs” in the secondary market and more consumer analysis. Then if you combine it with recoverable wallets it could be a game changer for all sorts of things where people register ownership to literally anything.

Cheap, reliable, fraud proof, digitised. The future

Nfts that are “art” I don’t get but I can understand the historical significance of this tech right now having value.",hnwhghf,t3_rcla2n,1639084700.0,False
rcla2n,"Thing is, having a secondary market for tickets doesn’t require NFTs. It just needs the venues to allow resale or reassignment via their own platform.

The reason they try not to is because secondary markets already exist with vastly inflated prices.",hnwk1n0,t1_hnwhghf,1639085747.0,False
rcla2n,"Having a digital market place and being able to verify on chain makes the process a lot more trustworthy and fraud proof vs using physical tickets. If it wasn’t any better, companies like ticket master wouldn’t waste their time.. I trust they can see more potential than I can.",hnwp63b,t1_hnwk1n0,1639087829.0,False
rcla2n,">  it creates the possibility for secondary markets

Not a sports fan, but this sounds horrible to me. Won't that just open it up for scalpers to exploit another market?",hnytnjd,t1_hnwhghf,1639126683.0,False
rcla2n,Could enforce a price policy that makes profiteering impossible with on chain transactions,hnyu6c2,t1_hnytnjd,1639127128.0,False
rcla2n,"Hum have you ever heard something about magic beans ?

https://lib.rs/cryptography/cryptocurrencies

Personnaly I think NFTs are just an another overhyped thing only based on speculation. Nothing new.",hnx0lwq,t3_rcla2n,1639092662.0,False
rcla2n,"It's another fucking stupid application of a niche technology - and edgelords everywhere are being suckered into it at lightspeed.  


Riddle me this: Who enforces your so-called ""Digital property rights""?   
What is the advantage of putting it on a blockchain? 

The answers are: Nobody, and nothing. It was another idea created simply so someone could, essentially, sell thin air. Only a fucking moron would buy into it.

...But I am at least honest with myself: I know that folk will put value into anything. So with that in mind ***inb4*** *""x number of people would disagree with you!!!11one""*",hnxyjqx,t3_rcla2n,1639107763.0,False
rcla2n,Mostly a way to launder money.,hnyye3o,t3_rcla2n,1639130702.0,False
rcla2n,you asked a very good question op,hnzg18s,t3_rcla2n,1639142915.0,False
rcla2n,"There are dozens of cool applications for NFTs. Digital art is what made them popular but what's next is cooler

* diplomas
* certficate of authenticity
* land title
* concert tickets",hnyew4b,t3_rcla2n,1639116189.0,False
rcla2n,Home/Car title is the only really good use for NFTs IMO,hocpqa4,t1_hnyew4b,1639379144.0,False
rcla2n,[deleted],hocrg82,t1_hocpqa4,1639380448.0,False
rcla2n,It would make fraudlent selling a lot harder because everyone can see who owns the NFT title on the blockchain and no one would buy without the seller showing the NFT in their wallet.,hoctapd,t1_hocrg82,1639381879.0,False
rcla2n,[deleted],hoctq43,t1_hoctapd,1639382222.0,False
rcla2n,"It isn't ""needed"" per say, it would just increase consumer confidence that they are buying from the owner of the property.",hocunrt,t1_hoctq43,1639382978.0,False
rcla2n,"I think it’ll make waves in the gaming world. With the death of brick and motor, the inability to recycle titles like you normally would is going away. These could be exchangeable in an NFT marketplace and would make an attractive business model to consumers, and the publisher could get paid for every resale transaction. I can see Amazon doing the same thing with Ebooks maybe.",hny4o66,t3_rcla2n,1639110625.0,False
rcla2n,This is the only use case of NFT’s I’ve heard of so far that makes sense to me.,hnymj05,t1_hny4o66,1639121085.0,False
rcla2n,"Resale of digital goods is the only use case I've thought about that might work, but it has a couple of problems that probably means it won't ever happen. Using games as an example, currently keys on every platform are single use. Meaning even if you tied ownership of keys to people on the blockchain, you couldn't resell it. Platforms like Steam would need to support it and immediately its no longer decentralized. Every transaction would need to contact the platform the key is linked to, to transfer ownership. At that point the blockchain is irrelevant, you could do the same thing without it because you require the platform to support it.

No matter how I think about it, for resale of digital goods the platform the goods are used on would need to support it.",hnytgr6,t1_hny4o66,1639126525.0,False
rcla2n,What about a platform for fractionalized nfts (erc721 + erc20) for companies that want to reissue their securities on the blockchain?,hoh5yrt,t1_hnytgr6,1639459022.0,False
rcla2n,crypto is overhyped,hnyzb23,t3_rcla2n,1639131470.0,False
rcla2n,The money laundering one. Also scams.,hnziyv9,t3_rcla2n,1639144401.0,False
rcla2n,Limited technical application from what I understand.  Also it's a picture of a monkey with your name on it it's not worth $50k,hnydezt,t3_rcla2n,1639115310.0,False
rcla2n,"A lot of good points in this thread, but I'll add my two cents: Because NFTs must be verified by a trusted party, it's unnecessary and wasteful to host them on a platform whose appeal is being decentralized & trustless.",hnz5t3f,t3_rcla2n,1639136608.0,False
rcla2n,"The basic idea is quite good

At the moment they’re being used for digital beanie babies, and it’s hard to see true value there

But looking at real world potential: I’ve just spent £1000 on solicitors fees to check the title deeds and other information about the house I’m buying, and most of that cost is retrieving the information they need. A tokenized system could be ideal for something like that. Whether that takes the same exact form as an NFT, I don’t know, but the idea was floated long before NFTs existed

Similarly car ownership, maintenance and inspection records etc would seem like great candidates for a token system - allowing authorized dealers and garages to log information on a decentralized blockchain, which could be much more reliable and cheaper to run than a centralised system.

I wouldn’t say they’re definitely suitable for such tasks, but certainly there’s real world potential there

Perhaps more importantly, digital ownership of songs, movies, books etc would be a perfect candidate. Streaming is great but not ideal because you can’t buy something to keep. But buying digital assets relies on the store/service you use staying in business

NFTs could solve this by allowing you to own a song more generically, so record labels or publishers can reassign the rights to sell those items to another distributor/store/service, on the proviso that they are sold as NFTs and the new provider will supply a download to the owner of the token. That way, you can buy a movie knowing that it won’t just vanish if Google decides to pull the Play Store one day, or buy a game knowing it doesn’t matter too much if Steam/Valve go bust, or a book without relying entirely on Amazon staying in business.",hnzcolc,t3_rcla2n,1639141048.0,False
rcla2n,ITT: people who don’t understand that an NFT is a digital certificate of authenticity wrapped in a rich metadata audit trail authenticated by cryptography and hosted on the closest thing to a permaweb humanity has achieved.,hnzdjzr,t3_rcla2n,1639141555.0,False
rcla2n,"Obvious scam is obvious. You are buying nothing with the expectation that the nothing you bought will be worth something in the future because other people also believe so. It is just a big bubble and when you look at the price graphs, the vast majority of them go up like a rocket and then come crashing down faster than light when the bubble bursts. Making NFT:s is fine I guess, but the market is oversaturated for obvious reasons so good luck making any money. At that point it is basically just a waste of time.",hnzygi6,t3_rcla2n,1639151310.0,False
rcla2n,"Duplicates of digital media can be exact copies. But money is paper that we all agree has value, it’s not much different. The idea that it can be akin to art is a bit off to me. No one will copy exactly the any of the most famous pieces of art, copy’s will always be copies. But a copy of a digital image is an exact copy. Same ones and zeros.",hnyf7bf,t3_rcla2n,1639116374.0,False
rcla2n,"I think there are some interesting potential uses cases in supply chain management and probably others as well, I don’t understand the current trend of digital artwork though.",hnybsdl,t3_rcla2n,1639114361.0,False
rcla2n,Just wait for digital assets outside of nfts my friend 🙏🏼,ho3dc7h,t3_rcla2n,1639205622.0,False
rcla2n,"An NFT is like writing the name of the owner on a piece of art using invisible ink.  It does not give you exclusive use of the art in any way other than allowing you to sell to someone else the right to erase your invisible ink name and write their own.

What is the value of that?  That is completely dependent on what people value.",hok35cs,t3_rcla2n,1639514762.0,False
rcla2n,"In a nutshell, NFTs are bad for two reasons:

1. They are bad for the environment, as they rely on cryptocurrencies that cause huge amounts of carbon emissions. They will continue to rely on these systems for security reasons (despite claims to the contrary about moving to other systems).
2. They are only valuable as tools for money laundering, tax evasion, and greater fool investment fraud.

There is actually zero value to NFTs. Their sole purpose is to create artificial scarcity of an artwork to supposedly increase its value (it doesn't do this, but the pretense that it does can be used for illegal purposes by those who recognize that fact).",ht2jl6h,t3_rcla2n,1642445021.0,False
rcip1t,"I would give them alot of projects , I had numerical analysis course in uni and I learned tons by doing projects in Matlab and python , like Taylor's series and mclaurin series with their applications on finding the approximated values of infinite fractions , I still remember those algorithms because I actually used these bad looking infinite series in something useful( in my case was just getting roots and approximating numbers but you get the point(",hnuzs1u,t3_rcip1t,1639063448.0,False
rcip1t,Numerical Analysis was the shit,hnxazie,t1_hnuzs1u,1639097280.0,False
rcip1t,"To some extent this question strikes me as ""how should history be taught to CS students""?

Answer: Exactly the same way it's taught to everyone else.  You shouldn't silo knowledge, and end up having a highly ""inbred"" understanding of the world.  You could end up thinking your corner of academia is the center of the universe.  One should be able to take knowledge from a different domain and integrate it into your project without having the professor both cut your food and chew it for you too.

Now I do think math classes could be helped by showing more applications in CS.  We're still living through the era when physics was seen as the great mathematization of a science, and so a lot of our examples are in that field.  Today CS is at least as successful a scientific mathematization, with just as many examples and applications, so in that way we could modernize by using more examples in CS.",hnvhhra,t3_rcip1t,1639070418.0,False
rcip1t,"There does need to be a difference in how you teach different students with different motivations - it isn't about siloing knowledge, it's about providing the best opportunities for learning for each student. In a perfect world, this would be tailored to the individual person, but due to various unavoidable realities the best we can hope for is that it is tailored for the different cohorts of students you get in each class.
 
Yes, the best students won't care and can shift between perspectives on their own, they'll succeed in a generally-taught class - but guess what? Those students will succeed if you give them a textbook and a stick to draw in the dirt with, focusing educational design as if those are your target is a complete waste of your time.
 
The knowledge stays the same, but how you present it, lead in to it, connect it to what a student already knows and cares about, that is what determines whether you get the right engagement from the students who need to be taught, as opposed to those who can learn on their own.",hnxjwlj,t1_hnvhhra,1639101307.0,False
rcip1t,"Sure, if you're just trying to be a code monkey or do data entry, then you don't need to know algorithms.  Up to a point that's true, but if you're getting a CS degree, you need to be able to ""speak math"" at the very least, since that's just part of what a CS degree means.  If you're in a CS program, then you  need to be able to understand math in exactly the same way that the math students do.",hnxqda1,t1_hnxjwlj,1639104133.0,False
rcip1t,"You've completely missed the point. You don't teach them different things, you teach them the same things in different ways. The same knowledge, the same skills, but the way you explain them and build up to them is going to be different.",hnxrmoy,t1_hnxqda1,1639104681.0,False
rcip1t,"Yeah, I get the point.  I don't agree.",hnxs3cx,t1_hnxrmoy,1639104884.0,False
rcip1t,"Literal reams of educational research says you are incorrect, and your comment makes it clear you don't get the point at all. By the end of their degree, someone taught in a way that builds from their existing domain knowledge rather than being given a stock standard approach is going to know more, and know it better.",hnxsdgv,t1_hnxs3cx,1639105010.0,False
rcip1t,Calculus should be taught through application not pure mathematics. It’s more important people learn when to use/apply calculus then just to learn the calculus. Because most people learn and forget or even worse learn just how to solve without understanding of what a derivative or other calc item is doing. I imagine most people won’t be able to find a tangent line using derivative .,hnv98w9,t3_rcip1t,1639067245.0,False
rcip1t,Calculus is not pure at all. The real pure calculus is real analysis. Calculus honestly is mostly just plug and chug at most universities.,hofabn1,t1_hnv98w9,1639428116.0,False
rcip1t,Yeah but it shouldn’t be plug and chug. U should be learning application of calculus items,hoflvez,t1_hofabn1,1639432887.0,False
rcip1t,I agree with you. Im just saying thats how it's taught at most colleges. It's also not professors fault because the colleges want them to stick to the curriculum where professors have little say in it,hogdf8s,t1_hoflvez,1639445434.0,False
rcip1t,"Implementing integrators and approximating derivatives with sampling, applied to a few physics problems (springs?). Partial derivatives with image manipulation.

Then move to symbolic manipulation to make sure the common language is understood.",hnutvlk,t3_rcip1t,1639060889.0,False
rcip1t,What you described is exactly how my Calc. Teacher taught it. The class was a mix of physics and CS students so anything we learned was tied to real world problems. It really made The class fun and easier to learn.,hnvsput,t1_hnutvlk,1639074800.0,False
rcip1t,"Learning monstrous functions has to be part of it. Part of what a CS education should prepare you for is engaging with CS-related research (papers and literature) which is full of these functions.

But I would also consistently emphasize the underlying concepts (e.g. rates of change, etc). And I would use applied examples both from Physics and CS to give both a physical and practical understanding.",hnv13r9,t3_rcip1t,1639064010.0,False
rcip1t,Depends on who you are going to get as a result: “artist” or “artisan”,hnuzmcl,t3_rcip1t,1639063382.0,False
rcip1t,I had a class in college (got engineering degree) called Numerical Methods. Junior or Senior level class that was basically how to do calculus with discrete methods on computers.,hnv3occ,t3_rcip1t,1639065057.0,False
rcip1t,"I loved that class.  I took it at Rutgers with a professor that worked in the petrol industry and insisted we work in FORTRAN.  That one part I didn't love.

I gave it a different name though.  It's been years, but it was something like ""computational guesstimation"".",hox6rqo,t1_hnv3occ,1639751557.0,False
rcip1t,"If its actually calculus, then its good. But if its real analysis... RIP.",hnwjkxw,t3_rcip1t,1639085560.0,False
rcip1t,Just let them read this book: \`Calculus the Easy Way\`.,hnvjt8n,t3_rcip1t,1639071308.0,False
rcip1t,"There are a lot of great applications for calculus in AI and Machine Learning in general that might help drive calc home. I found in university that calc 1 and calc 2 tried to cover too many topics too fast, and lacked context/application, making the work feel very abstract. In a perfect world I think calc 1 could even be two classes, as given the spread of topics there is not enough time to have students actually connect with the material. The semester I graduated calc 1, around 50% of the class failed.",hnwkbpm,t3_rcip1t,1639085862.0,False
rcip1t,"The reason you typically have calculus classes in your freshmen year is that they also teach the basics of proofs, set theory and so on in the first few weeks. Now, of course, that is not directly connected to calculus, you can just as well do this in a linear analysis class, or in something focusing on theory of computation (I think that's what they do at stanford).

Now, if you move that part somewhere else, you can teach calculus differently or move it to a different place of the CS curriculum.

The question is whether you should, and if yes, to where and whether you need it at all. I don't really know. Calculus is kind of important for many areas of CS, for example to define big-O notation you kind of need it. Also, a CS curriculum without calculus is hard to imagine, especially for the people in charge of making these since they likely also attended such a curriculum.",hnwrqlu,t3_rcip1t,1639088893.0,False
rcip1t,Unless they hire extra professors for the cs department but other than that I believe there would be different applications for different majors. I think the overall concept is the critical junction while teaching this skill.,hnygqno,t3_rcip1t,1639117298.0,False
rcip1t,"The same way my uni did it, you take calc 1 like everyone else, and then go into comp sci related math courses. You need to understand the basic concepts before you can reasonably be expected to apply them in the real world.

There's a reason why in calc and above most proffesors don't care how ""fancy"" your calculator is, punching in numbers only gets you so far, you need to have some level of understanding why things work they way they do.",hnyvjd3,t3_rcip1t,1639128278.0,False
rcip1t,"Honestly, I don't think it should be, unless the student is going into graphics design, or 3d modeling, or simulating either of those.

Calculus (and beyond) is less a guide to logic, and more a guide to memorizing formulas.

All that said, to answer the question, no, I don't think it should be taught differently based on whether you are in Comp Sci or not.  Definitely _add_ connections to comp Sci, but don't create a lesson ""for computer scientists"".",hnz10wj,t3_rcip1t,1639132912.0,False
rcip1t,Lmao it really shouldn’t,hnzlodf,t3_rcip1t,1639145699.0,False
rcip1t,Calculus has helped a lot but learn optimization. Not like linear optimization but mixed constraint shit,ho3ddya,t3_rcip1t,1639205656.0,False
rcip1t,"I certainly agree that it could be taught in a better way, but I think most schools will not create a separate math class just for CS. There are definitely a lot of practice problems and applications of calculus that could come from CS.

I've had to integrate using polar coordinates, integrate logs, partial derivatives, and all the possible things I've integrated in typical integral classes so I found them quite useful already.

As for the ugly functions, I agree that they aren't as relevant, but I used it as a way to practice and learn how not to make mistakes.

Edit: If calculus was taught with CS applications, even some basic caculus methods may require a probability background or additional CS background. I suppose teaching calculus at such a late stage in your career might be less than ideal unless you knew for sure that you were going into CS.

Edit 2: If I were teaching a calculus class, I might preface a problem in calculus with an application in another field if I see applications that are easily explanable within a few minutes. Otherwise, it might take too much time and take away from actually learning new methods.",hobba9x,t3_rcip1t,1639353472.0,False
rcip1t,"Don't know that the following applies to your question but found it to be an interesting source for learning.

https://brilliant.org/courses/",hoefom8,t3_rcip1t,1639415826.0,False
rcip1t,"I think this depends on whether the class is being taught at a university that aims towards providing a liberal education, or if the class is being taught at an institution that functions more like a trade school.

A trade school trains students for practical skills in a given profession. It makes sense that coursework is tailored to highlight the application of each subject to that profession. Students don’t need to learn underlying theories or ancillary topics if they can’t be applied in their future profession.

A university should be different. Classes should be taught in a way that encourages students to explore the depths of the subject itself. This is part of the process of providing a liberal education.


I admit this stance is a little extreme. I’m sure there are ways of adapting a calculus course to highlight CS-related content without losing anything. I just get very suspicious when university courses start to focus too much on practical application, and stop teaching theory.",hohraat,t3_rcip1t,1639474458.0,False
rcip1t,I got 12/40 in cal1 while getting 38/40 in c++ lab soooooo,hnuxx68,t3_rcip1t,1639062664.0,False
rcip1t,"Real talk, computer scientists should be taught calculus through the lens of computing gradients for neural network parameters. You can start with really simple differentiable estimators then work your way up to fancy deep NN's and gradients over really complex transforms like image perspective transforms.

There's a nice range of difficulty, and it's very easy to see why calculus is worthwhile when you see it so directly applied in backpropagation.",hnw9xfy,t3_rcip1t,1639081651.0,False
rc955j,"x = n + n/2 + n/4 + n/8 …

Multiply both sides by 2.

2x = 2n + n + n/2 + n/4 + n/8 …

Subtract each side of the the first equation from the corresponding side on the second.

x = 2n

Which is O(n)",hntd7gv,t3_rc955j,1639025135.0,False
rc955j,Thank you so much! I've been thinking about this problem for about 2 hours and I think it just clicked. I appreciate it!,hnteheb,t1_hntd7gv,1639025837.0,True
rc955j,You wanna share the code?,hnt8ibf,t3_rc955j,1639022665.0,False
rc955j,"Apologies, I thought I had it attached when I posted it",hnt9nw7,t1_hnt8ibf,1639023260.0,True
rc955j,"Please, don't post images of code when you can just post code (4 spaces before each line for correct formatting)",hnx4g1f,t1_hnt9nw7,1639094340.0,False
rc955j,"Let M be the original value of N before the code starts. Then the number of executions is M + M/2 + M/4 + ... This is a geometric series (look that up if you're not familiar). Any time the length of the loop changes like this, you should look at whether that shrinks the complexity. That's how sorting algorithms can be O(nlogn) with two nested for loops.",hntf8eo,t3_rc955j,1639026247.0,False
rc955j,So this guy Zeno has this paradox right...,hnwx8zt,t3_rc955j,1639091204.0,False
rc955j,What algorithm is it,hnt8jc2,t3_rc955j,1639022679.0,False
rc955j,"Sorry, I thought I had attached it when I initially made the post",hnt9pq3,t1_hnt8jc2,1639023285.0,True
rc955j,"You sure complexity is O(n)
Because there is no n in that code",hntaube,t1_hnt9pq3,1639023874.0,False
rc955j,"""n"" is an arbitrary common label used, not referring to the specific variable name in the code. You could call it O(m), O(p) etc.",hnuend3,t1_hntaube,1639052860.0,False
rc955j,Got you,hnve5mm,t1_hnuend3,1639069124.0,False
rc955j,"Well, if we're using the variable in the code, then it is O(N)",hntbatv,t1_hntaube,1639024116.0,True
rbwnot,"Practice is the best way. Write code and run it so you can see what it does. Play with the classes and libraries you're using and just see what they do. If you ever think ""I wonder what happens if I..."" then write a program and try it. Unless you're doing silly things like manipulating files in system directories it's pretty difficult to actually damage your computer in any meaningful way accidentally. The best way is to just get your hands all over it and write code.",hnqrhlp,t3_rbwnot,1638985659.0,False
rbwnot,">If you ever think ""I wonder what happens if I..."" then write a program and try it.

This is really a good point, I enjoyed a lot playing with pointers in C this way.",hnuv39v,t1_hnqrhlp,1639061423.0,False
rbwnot,Practice https://adventofcode.com/ :),hnr669c,t3_rbwnot,1638991262.0,False
rbwnot,and compare your solutions at r/adventofcode!,hnscngs,t1_hnr669c,1639008182.0,False
rbwnot,"Here's a sneak peek of /r/adventofcode using the [top posts](https://np.reddit.com/r/adventofcode/top/?sort=top&t=year) of the year!

\#1: [Thank you Eric!](https://np.reddit.com/r/adventofcode/comments/kjtou6/thank_you_eric/)  
\#2: [Too often](https://i.redd.it/2dwtt64moq461.jpg) | [62 comments](https://np.reddit.com/r/adventofcode/comments/kbnh5i/too_often/)  
\#3: [\[2020 Day 18 (Part 1)\] Outsourcing the solution. They never care about the order of operations anyway](https://i.redd.it/ugjy19khxy561.png) | [21 comments](https://np.reddit.com/r/adventofcode/comments/kfnt2s/2020_day_18_part_1_outsourcing_the_solution_they/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[Source](https://github.com/ghnr/sneakpeekbot)",hnscolz,t1_hnscngs,1639008197.0,False
rbwnot,"There are some great books by oreilly on the python programming language and many different applications such as web development, finance, data science, and more.

I would say for now, it’s good to be aware of those applications while you learn the language and build intuition and muscle memory.

Your class projects will be really good for applying fundamental  concepts. These projects will be important in your portfolio as they demonstrate basic competency.

What will really connect you to programming will be different projects of practical ideas and personal interest I would suggest using Udemy’s 100 days of code (I have heard great things about it and I will be starting it soon), or other beginner projects you find on the internet.

These projects are beginner to intermediate and are especially helpful if you are not sure what your programming interests are. As you do some of these projects, you will be noticing what you like and dislike. From there, you will be able to guide yourself. 

Programming can seem daunting early on in this vast space especially if you have classmates who have been doing it for a while. It is important to start small and not bite off more than you can chew. You might get discouraged and have a negative experience that lingers on. 

Tldr: get comfortable with the basics, start small, divide and conquer, find your interests, enjoy yourself",hnqt4g4,t3_rbwnot,1638986278.0,False
rbwnot,I think practice is the biggest thing. Write a bunch of code. I also think reading and understanding other peoples code can be very useful. You could probably look at some open source projects to find code to read.,hnqt9vc,t3_rbwnot,1638986335.0,False
rbwnot,"ignore the people linking to shit like advent of code and hacker rank, solving those problems and actually doing projects are two different skills. You need to focus on doing projects , stop thinking about if its unique or anything. Just do it, and you will get better.",hnrcswq,t3_rbwnot,1638993773.0,False
rbwnot,"As everyone else has been saying, code more. But in my experience, find a semi-big project that you will really enjoy doing, and go at it. I was in the same boat, and I found a really fun project: making a password guesser that could get into anyone's school account at my school. Yeah, that project sounds really bad, but I told all my high school teachers about it and I was very transparent about it so I wouldn't get in any trouble. Anyway, I had an absolute blast trying to code the program since it made me feel so cool, like a hacker. You could try a basic or advanced project; just make sure it forces you to code A LOT.",hnrid6y,t3_rbwnot,1638995895.0,False
rbwnot,How did the code work?,hnwbn4y,t1_hnrid6y,1639082344.0,True
rbwnot,"Well, at my high school, it was easy to make this program. For their school logins, everyone had a username that could be easily determined. It included their name and graduation year (Ex: John doe graduates in 2022: jdoe22). For the password, it was always the word ""pass"" and then a 4 digit number (Ex: pass9233). My program first asked the user to type in the student's full name and graduation year. After that, it generated the username automatically and stored it in a variable (very simple code). Then, it opens up a tab in Chrome, goes to our school portal website, and starts entering the username and every possible combination of the password until it works. pass0001, pass0002, pass0003 and so on. In reality, I had it open 4 different Chrome tabs at once to do it faster, but it worked on every single student. Does your school have a password system like that? Just curious",hnwdlhm,t1_hnwbn4y,1639083127.0,False
rbwnot,Nah we get to choose our own passwords,hnwdt6j,t1_hnwdlhm,1639083213.0,True
rbwnot,Experience and read  3 party code,hnqwom7,t3_rbwnot,1638987636.0,False
rbwnot,"I suggest going through a book like SICP https://mitpress.mit.edu/sites/default/files/sicp/index.html

Solving a bunch of programming problems using a lisp language really helped me learn how to methodically solve problems programmatically.",hnrgica,t3_rbwnot,1638995194.0,False
rbwnot,"If you want to hire a virtual tutor, feel free to message me!",hnsxw52,t3_rbwnot,1639017643.0,False
rbwnot,"Lots of practice problems are available online. Try looking into coding competition sites. They have questions sorted by difficulty, and you can build your way up.

Here are some challenges on [HackerRank](https://www.hackerrank.com/domains/python) specifically for python.",hnqu85z,t3_rbwnot,1638986700.0,False
rbwnot,"Write more code. Look up python beginner projects and code along with them, watch python videos on freecodecamp's YouTube page, complete a leetcode question every day or 2. Just do something. You can't learn the wrong things. Just keep learning and the more you learn, the easier it will get to decide/know what to learn next.",hnqumei,t3_rbwnot,1638986852.0,False
rbwnot,"So I am a deadly practical person, and I like to always see the effects of what I am doing, so my way of getting better at coding was (and still is)  - thinking about what would I like to learn or build. Maybe a mobile application? In this case, I read how to build one, what languages you have. I think about the features which this app should have. Then I start doing tutorials which are showing specific elements which I want this app to have, for example how to do AR, or how to make the app use camera and read text from a picture.",hns6k7b,t3_rbwnot,1639005482.0,False
rbwnot,"Make tons of projects, check this link [https://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/](https://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/) try to make all of the projects. Doesn't really matter about programming languages, use whatever you're similar",hnsmoxv,t3_rbwnot,1639012702.0,False
rbwnot,Read good code.   Not enough time is given to reading successful code bases IMHO.,hnt9oif,t3_rbwnot,1639023268.0,False
rbwnot,"I got better by trying to automate various server tasks. Everything from renaming files to web scraping to video encoding etc.  Find those little mundane things you do and try and code it, then go back and improve the code once you've gotten better. You'll get there v",hntc1t8,t3_rbwnot,1639024512.0,False
rbwnot,"You improve coding primarily by coding. Read a lot of software written by others and write a lot of software of your own: there is no substitute for reading and writing software. Thankfully due to free software and open source software movements there are basically endless amounts of software you can read free of charge.

At the same time, you'll want to study as much discrete mathematics as you are able, which will help remove walls from your path before you encounter them in your programs.

You will need to study algorithms and data structures until they are completely internalized into your being, they are indispensable. I recommend Introduction to Algorithms or basically anything on the matter by Charles E. Leiserson. Their lectures are also available free of charge on youtube.",hntfb7g,t3_rbwnot,1639026291.0,False
rbwnot,"Write code, read books, watch videos about programming, check others code",hntpofx,t3_rbwnot,1639032890.0,False
rbwnot,Practice! There’s no short cut. Try contributing to open source projects.,hnqupug,t3_rbwnot,1638986888.0,False
rbwnot,"I know I’ll get some hate for this but I learned the most when I started using lower level languages like C and even Assembly (I would not recommend doing assembly tho). Having to implement everything from scratch with only the essential libraries will let you appreciate and understand runtime a little better. Even tho I use Python from time to time when I need to create a tool fast, I feel that always coding in Python will drive you to the lazy programmer path that can’t do anything without a library. But there’s nothing wrong with that if the requirements are met.",hnst8v1,t3_rbwnot,1639015591.0,False
rbwnot,"If you are looking to improve your problem solving abilities, check out Exercism or Kattis",hnqvd01,t3_rbwnot,1638987134.0,False
rbwnot,"You just have to put in the time.  Think of some kind of program you'd like to have that doesn't exist, and give it a try.  If it seems like ""a little too much work"", it's perfect, try it.  You'll learn a lot just researching the documentation and libraries needed, and fixing bugs.",hnqy0lz,t3_rbwnot,1638988143.0,False
rbwnot,Touch code everyday.,hnr0tin,t3_rbwnot,1638989214.0,False
rbwnot,"Have a goal like

draw up a wireframe of single webpage and try to implement it. Use internet search to implement it. Remember your primitives, loops, bool, ect. Follow software design principles.",hnr1934,t3_rbwnot,1638989378.0,False
rbwnot,"
Hello! You have made the mistake of writing ""ect"" instead of ""etc.""

""Ect"" is a common misspelling of ""etc,"" an abbreviated form of the Latin phrase ""et cetera."" Other abbreviated forms are **etc.**, **&c.**, **&c**, and **et cet.** The Latin translates as ""et"" to ""and"" + ""cetera"" to ""the rest;"" a literal translation to ""and the rest"" is the easiest way to remember how to use the phrase. 

[Check out the wikipedia entry if you want to learn more.](https://en.wikipedia.org/wiki/Et_cetera)

^(I am a bot, and this action was performed automatically. Comments with a score less than zero will be automatically removed. If I commented on your post and you don't like it, reply with ""!delete"" and I will remove the post, regardless of score. Message me for bug reports.)",hnr1asw,t1_hnr1934,1638989395.0,False
rbwnot,"Just do a simple project and look forward to errors that you'll get. every time you get an error either write it down or remember it (will happen a lot!!) at first it will be extremely confusing all these new errors and words you never heard of! but then over time you notice some errors that look familiar. and before you know it you can fix those errors and even predict and prevent yourself from writing buggy code. 

I also recommend specializing in one thing! maybe at first you are more general in your approach and you do anything and everything but then over time you find what you like and do just that! specialize in one thing! web dev, ML, AI, whatever. this way as you spend time developing your skills in that area you are not spreading yourself too thin.",hnr8l94,t3_rbwnot,1638992180.0,False
rbwnot,If you're a beginner and don't know the fundamentals very well then do lots of tutorials.  I personally recommend Scrimba as they have a very unique built in editor in their videos.  After you understand most of the basic move on to making projects.,hnrjj2n,t3_rbwnot,1638996328.0,False
rbwnot,"As the others have pointed out, practice is important. But something that also helped me were guidelines like ""The code should tell you what it does and the comments should tell you why it's doing that."" 

Also looking at actually good code will make you notice your own weaknesses. (For example I used to watch the YT Channel ""Coding Adventures"" at breakfast for a while and that helped me.)

Lastly, I think to better understand advanced concepts, learning some more esoteric languages like Haskell and Prolog, is actually quite entertaining and useful.",hnrwfv8,t3_rbwnot,1639001298.0,False
rbwnot,The same way you get better at anything. Practice.,hns3hd3,t3_rbwnot,1639004169.0,False
rbwnot,"Participate in an open-source project.

Learn how to use a debugger - debuggers are not just used for finding bugs; you can use them to step through code that you want to learn, try changing variables while debugging, learn how to set breakpoints.  This alone will make you a better developer than 50% of the competition (obviously a estimate).",hns5jyx,t3_rbwnot,1639005051.0,False
rbwnot,"Learn more than one language for one! Figure out how they're different, what their strengths are, when it makes sense to use which.

Google things like the docs or best practices and find simple ways to apply it. Take a hello world java program, make it a hello to an array of all planets. Maybe make a planet object with different fields. Call it via for loop. Then try writing it more expression based using streams and filters.

Google interview questions and try to solve them. Find coding challenges based on doing things in certain time efficiency where the brute force way is simple. Look for things that aren't just accomplishments in what they do, but that give you a better understanding of how something works or adds a new tool to your belt that you didn't know you could do before.",hnt1h20,t3_rbwnot,1639019267.0,False
rbwnot,Look at the source code for open source projects that have been around for a long time.,hnu0rlm,t3_rbwnot,1639041833.0,False
rbwnot,"1. Learn different programming languages and their paradigms (OOP, functional, logical etc)
2. Study complicated to understand and keep in mind math concepts — good example is combinatorics — just fo brain training)
3. Practice, but not blindly — i.e. having particular objectives. But the objectives should be feasible in a reasonable time. Good example is to participate in online programming competitions where you have to solve non-trivial enough problems of different difficulty levels and where your solutions strongly verified automatically (so you will always be confident if you succeed or not with the task)",hnuek8l,t3_rbwnot,1639052805.0,False
rbwnot,"Boot camp, projects , video tutorials",hnuw601,t3_rbwnot,1639061894.0,False
rbwnot,create projects with your knowledge do not do tutorials forever,hnvafs7,t3_rbwnot,1639067698.0,False
rbwnot,Spend as much time doing the best kind of learning you can. The best kind of learning is play.,hnrqbr8,t3_rbwnot,1638998909.0,False
rbwnot,[removed],hns792i,t3_rbwnot,1639005774.0,False
rbwnot,"Thanks for posting to /r/computerscience! Unfortunately, your submission has been removed for the following reason(s):

* **Rule 2:** Please keep posts and comments civil.



If you feel like your post was removed in error, please [message the moderators](https://reddit.com/message/compose?to=/r/computerscience).",hntd25r,t1_hns792i,1639025057.0,False
rc3wsu,"Mechanical computing device. Physically pretty big and over engineered for durability. Most of its function is to be a database lookup to relevant stone tablets with the actual information.
Combine with smaller mechanical devices that use directed questions to allow ignorant people to create a useful search for the big device.
If you really want to go for the long haul, the final lookup is actually directions through the massive tablet storage area instead of a tablet retrieval mechanism.
Ideally you won't have any computing, just knowledge, which is where most sci-fi authors go with this problem. But making it a device is a fun conceit 🤔",hnsal50,t3_rc3wsu,1639007258.0,False
rc3wsu,"Mm, yeah I thought about that, and certainly the ""books with pages made of hammered gold"" type thing appeals to me, especially through deep time like thousands or millions of years. For this particular story the aim is to be useable for several centuries, and I think it might be useful for these people to have engineering tools, computer aided design and simulations, perhaps even access to radio for the purpose of communicating with old satellites (as some high orbit satellites will still be in place for millions of years, it's not unreasonable to also make a satellite that might be able to do things for a very long time).",hnsbio2,t1_hnsal50,1639007669.0,True
rc3wsu,"Does your story allow for caretakers of this repository, or should it be completely abandoned and forgotten for a time?  And how long does it need to last?",hnsdwx5,t1_hnsbio2,1639008756.0,False
rc3wsu,"No caretakers, no maintenance, completely abandoned and forgotten. It would be located in a geologically advantageous environment - perhaps a desert at high altitude.

Anything from a century to a thousand years would be fine, but the longer the better, story-wise. A million years would be excessive. I want to be able to describe the making of it in some detail.",hnsep68,t1_hnsdwx5,1639009110.0,True
rc3wsu,"To be fair, for just a century or three you could simply print Wikipedia on archival paper with a laser printer and store it in a cardboard box, if you had a dry cave in the high desert and a way to keep animals out.

If you're committed to something electronic, I would look at solid state storage, something older (and thus likely more robust), like mask ROMs.  Even consumer grade mask ROMs last decades with very little care (think Atari 2600 cartridges, some are from the 1970s, most of them still work if they're clean).  You could probably take inspiration from Viking and Voyager space probe computers, the Voyagers are still functioning after decades.  A nation-state actor could surely take lessons learned and make a very robust system in a similar manner.  Voyager used dual redundant systems and CMOS volatile memory with a direct, permanent connection to the RTG.  You'll also probably want some kind of nuclear-derived power.

My main thought is things need to be designed to be fault tolerant and redundant, the system should have the ability to bypass or work around failed parts, and the storage area should be as clean, even temperatured, and dust/moisture-free as possible, say, in a cement-lined cave.  A big part of why electronics fail is due to temp cycles.  Moisture in the air combining with contaminated on the hardware can cause corrosion.  Avoid the use of electrolytic capacitors.

But really, if you can get away with something non-technical like books, go for it, it'll be more believable.  There are books in the world from the 15th century and (much) older that just laid around an attic most of that time.",hnshq24,t1_hnsep68,1639010461.0,False
rc3wsu,"There will be various abandoned/dead outposts throughout the solar system, as well as a dying generation ship - ultimately I'd like the characters to have some kind of communication with them, even if it's just to witness their slow death. It would also be really interesting if they could connect to some remaining satellites in orbit to get an overview of Earth.

Libraries and the like would certainly exist as well as this thing.


There are a few plot threads I'd like to explore concerning a computer database versus libraries as well - if the computer fails, all of that knowledge is lost at once (with hope for repairing the machine, possibly).

Of course there's also the capability for physics simulations, computer aided design, accurate and precise timekeeping etc. There would some interesting possibilities for showing video and sound as well.


There's also good reason for this thing to be located in a desert - (weather bad) not a great place to live but even just visiting it may make for an interesting story. The user might even become a User or a High User if civilization decides to treat it as a sort of religious artifact or something - and especially interesting if it breaks and The Great Council of Quertyuiop calls upon a crusade to retrieve the Holy Heatsink or something.",hnyqod5,t1_hnshq24,1639124229.0,True
rc3wsu,"Those all sound like interesting story beats to me, I would read that novel.",hnz443g,t1_hnyqod5,1639135368.0,False
rc3wsu,"Also want to add that the story includes automated outposts throughout the solar system, and potentially a (failing) generation ship, made by the same or similarly capable group of people. I want the technology to be consistent so one will inform the other.",hnsfe4v,t1_hnsdwx5,1639009416.0,True
rc3wsu,"Also, an afterthought here - yes, the facility could have caretakers, but they'd have to arrive after the fact, know nothing of how it works or what needs to be done, beyond what can be learned from using it. Adding certain materials to a reservoir, or turning a handle, or hooking a shaft up to a windmill or a primitive electrical generator, fine. Pre-existing knowledge of re-imaging the operating system or soldering surface mount components to a board, not so much. Replacing parts in storage, as instructed by careful study and use of the device, fine.

Any maintenance would have to be done by people who have no idea how it works",hnsg09b,t1_hnsdwx5,1639009689.0,True
rc3wsu,"The physical storage medium is something I considered as well, even for an electronic computer - given unlimited size something like a vinyl record but made of quite hard materials could last for a *very* long time.",hnsbrk0,t1_hnsal50,1639007780.0,True
rc3wsu,Gigantic over engineered card catalog?,hnujej0,t1_hnsal50,1639055668.0,False
rc3wsu,"Yes!  The idea of a technology that could survive a thousand years and be used by ignorant people is preposterous, but that is the requirement of this thought exercise 🔥so  you just have to run with it.",hnuysl6,t1_hnujej0,1639063037.0,False
rc3wsu,"A “foundation,” as it were.",hnt6t06,t3_rc3wsu,1639021804.0,False
rc3wsu,Technically you don’t have to create a thing to store all knowledge.   You only need to create a thing that motivates and instructs the user as to how to turn on the computer.,hnsd7e5,t3_rc3wsu,1639008434.0,False
rc3wsu,"“If you wish to make an apple pie from scratch, you must first invent the universe.”

I say this tongue-in-cheek, but you're right of course and that is a major consideration here. The user would have to choose and be able to operate the device in the first place, and perhaps even have some useful help in learning how to use it.",hnsdsx6,t1_hnsd7e5,1639008706.0,True
rc3wsu,If things really did get back to Neanderthal levels you would need to start with some simple interactive games to build familiarity and trust with the device.   Things like matching shapes or colors like toddlers toy.   Then you could build up to simple spoken language.   It could be like a fast paced survival of the fittest scenario where status in the society was based on the ability to interact with the machine.  Eventually your users could start seeing story boards that explain what happened and build up to learning what we know in detail.   You couldn’t just throw that at someone who might even never have seen a number or a wheel.  They would be frightened of it,hnso0ub,t1_hnsdsx6,1639013289.0,False
rc3wsu,"Easy, you just need to store the data on a Nokia 3310 somehow! 

Or something like NASAs golden record. 

Imo it would not work though:  When, for example, the ancient egypt empires fell, most of the knowlege was lost despite it being available in writing at the temple walls (and a lot of it is lost till today). After a collapse most people would not be able to read anymore and people using computer would be seen as crazy.",hnttzp3,t3_rc3wsu,1639036160.0,False
rc3wsu,"Diamonds and lasers! But in all seriousness, I don't think it's possible to make indestructible computers, but it should be possible to make easily maintainable ones. Where a replacement part can be a lens or some metal bracket.",hnt839p,t3_rc3wsu,1639022450.0,False
rc3wsu,Plastic Fortran punch cards.,hnujzvv,t3_rc3wsu,1639055996.0,False
rc3wsu,"In the 3 body problem they thought about all kinds of ways of storing information for 100,000 years into the future, and finally figured out that the best way was invented eons ago: etching it into stone. Stone will last tens of thousands of years under the right conditions. So your best bet would be to etch information into stone, but if you're really set on the whole 'computer' idea it would have to be something mechanical that interacts with the stone somehow, like retrieving tablets or something. Kind of like the machines you see in the dwemer ruins in skyrim.",hnunzgc,t3_rc3wsu,1639058077.0,False
rc3wsu,"make a system that can repair itself and make its own parts. that way you get some longevity.

then give it the task to conserve educational videos in a RAID 1 configuration and show them on a crt when a hooman is present",hnsgwkc,t3_rc3wsu,1639010093.0,False
rc3wsu,"My first thought for building an archive like this would be to find a star and construct a Dyson sphere, which would ensure it's survival by sending a Von Neumann probe out to replicate the knowledge base when the star wanes (the dying million years or so).

How the information is expressed is an interesting abstract problem though, since those encountering ""The Archive"" might not have the same understanding of technology as it's constructor.",hnt0p0j,t3_rc3wsu,1639018909.0,False
rc3wsu,"Running for _centuries or longer?_ I'd ditch the computer in favor of a card catalog system. Doesn't need any electricity, won't be effected by an EMP or nuclear fallout or whatever led to the societal collapse. It will last as long as the room and printed material endure. Paper becomes fragile with age, but by laminating the cards and coating books in some kind of a sealant they could be made to last much longer. Hopefully the library could contain enough books (maybe in microprint with magnifying glasses to store as high a density of information as possible?) to aid in redeveloping society and technology.",hnt84xp,t3_rc3wsu,1639022475.0,False
rc3wsu,"Like the seed vault, the trick for preservation is cold.  This way items last for a very very long time.

Instructions on how to read with physical books is necessary, because there is no guarantee of infinite power.. maybe geothermal, but it's beyond my knowledge of the topic.  So you have to teach someone how to get power, teach them how to make a circuit and so on.  Have items for them to use, play with, and learn like breadboards and what not.  Once they learn how to make power they can plug in any preserved computer.",hnttya7,t3_rc3wsu,1639036129.0,False
rc3wsu,Poneglyph,hnvg2sp,t3_rc3wsu,1639069868.0,False
rc3wsu,Gameboy Cartridges.,hnzemm3,t3_rc3wsu,1639142151.0,False
rcj9zu,"Everything you want to know can be found in the book Digital Computer Electronics by Albert Paul Malvino. The things turning on and off are parts of an electric circuit. You can see this playing out at http://visual6502.org/JSSim/index.html. You can construct the architecture (SAP, ""Simple As Possible"") covered in Malvino's book yourself by following Ben Eater's video series on youtube.",hnv6tj6,t3_rcj9zu,1639066319.0,False
rcj9zu,"Also ""The hidden language of computers"" a real masterpiece of a book and easy to understand",hnw9dic,t1_hnv6tj6,1639081429.0,False
rcj9zu,It executes one instruction regarding a piece of data at a time.,hnwmfr5,t3_rcj9zu,1639086724.0,False
rcj9zu,Very quickly,hny0979,t3_rcj9zu,1639108546.0,False
rc1j2y,"data type - is the data a string, a number, a date etc.  different languages will have their own types (int, float, string, datetime)

data structure - ""a data organization, management, and storage format that enables efficient access and modification"" examples of these would be an array, a binary search tree, a hash table.  they are all different ways of storing data that provide different benefits (speed, size, inserting, sorting, etc)

data schema - i believe this is for describing the organization of data in a database. if you are new and don't know much about databases i don't think examples will be of much use

data model - i'm familiar with this as a part of the MVC (model, view, controller) design pattern. the data model is how the data for your application is represented.  a key part of mvc is that this is kept separate from the view (what displays the data to the user) and the controller (the ""brain"" of sorts, it manipulates the views and data so they are always separate and unaware of the other)

I guess they are all related in the sense that they all have the word data in them.  Data schema would have data types in it, as would a data model.  a data structure as a concept is independent of the rest in a sense, but you would also see an array in some languages declared with a certain data type",hnrvjru,t3_rc1j2y,1639000946.0,False
rc1j2y,"My CS Professor weirdly said a schema is the way a table is structured but in my 7+ years of data it has always referenced the organization of tables within a database. Like, a database is made up of schemas, the schemas are made up of the tables. Sorry if this is more confusing!",hnsjdy9,t3_rc1j2y,1639011211.0,False
rc1j2y,"That's exactly the opposite of how we learned to use those terms in databases courses: the database is made up of tables, each organized/defined by their schema.

I don't know which is right, and maybe one use of terms is used in theoretical/academia and another use is common in industry.  I'm just reporting what we were taught.",hnzwwqx,t1_hnsjdy9,1639150666.0,False
rc1j2y,I think we’re saying the same thing and my Professor said something different.,ho0mbh5,t1_hnzwwqx,1639160767.0,False
rbz19p,"What you describe is called an ""access control policy"", see e.g. [https://en.wikipedia.org/wiki/Access\_control](https://en.wikipedia.org/wiki/Access_control).

Many such systems are based on a notion of roles. You can search for the keyword RBAC, for role-based access control.

This will give you plenty of things to read ;-)",hntlpub,t3_rbz19p,1639030157.0,False
rbz19p,"Research IRM solutions.  They protect documents by only distributing encrypted copies, and users gain access with client software that is given the decryption key via a server you control, backed by whichever AAA structure you're already using.

The advantage is you can enroll/unenroll people and groups, federate with vendor AAAs, set limits for online/offline use, and revoke access to files even if the data is on a third party system.

Another advantage is there are existing solutions you can buy, which is probably cheaper, faster, and more robust than rolling your own.

By client software, I mean things people are already familiar with, like patched MS office, etc.",hnzxk16,t3_rbz19p,1639150934.0,False
rc9jzq,"> Can AI theoretically solve the halting problem or **at least most of it?**

The issue is that the halting problem isn't defined probabilistic-ally. You can't solve the halting problem with ""I'm 90% confident the given program will halt, because it looks similar to halting programs that I've been trained on"", because by definition that is _not_ determining whether the input program will halt, it's just making an educated guess.

If you want to move from ""this will probably/won't probably halt"" to deterministically solving the problem, then we're back to square one.",hntdhsk,t3_rc9jzq,1639025291.0,False
rc9jzq,"You can't solve any problem with a computer with 100% confidence. A quantum fluctuation or a cosmic ray could flip a bit somewhere and completely change the result or behavior. So really it's just about your level of confidence. Also if you trained a neural network sufficiently, maybe it could give 100% confidence assuming no quantum fluctuations. If you trained a sophisticated enough neural network to genius level then who knows what could happen. The neural network might be so sophisticated that it creates it's own execution engine to run certain parts of the code for example.",hntiv4t,t1_hntdhsk,1639028354.0,True
rc9jzq,"> You can't solve any problem with a computer with 100% confidence

That's a pedantic argument. You _can_ develop algorithms that solve a problem 100% of the time if executed faithfully - the fact that computer hardware is fallible does not mean that the theory is incorrect. It's not about levels of confidence.

> The neural network might be so sophisticated that it creates it's own execution engine to run certain parts of the code

How would this work? If you run a piece of the program to check whether it halts then you can only observe a termination, but not a lack of termination. That is, if it halts, great, you know that it halts, but if it _doesn't_ halt then all you know is that it hasn't halted _yet._",hntju3a,t1_hntiv4t,1639028954.0,False
rc9jzq,"Well in reality and in all of science theories come down to levels of confidence. Even in imaginary land there is no such thing as absolute truth if you want to get into philosophy because you would never know with absolute certainty that an algorithm can solve a problem 100% of the time if executed faithfully. Absolute confidence is a myth. The point is not that the theory is incorrect but rather at a certain level of precision and accuracy become indistinguishable from absolute certainty.

And a neural network can emulate almost any complex function. So it's not that it would just run the code blindly but rather execute certain portions of the code if it seems relevant and use advanced heuristics to make determinations. With enough training, neurons, and architecture sophistication a neural network might be incomprehensibly intelligent on determining whether code will halt or not. I can't really tell you what kind of strategies a neural network with an IQ of 10000 would come up with obviously but there may be a point where it can tell you with maximal confidence for most halting problems, or perhaps all.",hntm9sl,t1_hntju3a,1639030523.0,True
rc9jzq,"If we assume that inductive and deductive logic can reveal truth, then there is such a thing as absolute confidence through mathematical proofs. Algorithms can be proven in this theoretical domain, even if their implementation in hardware is imperfect. If you do not agree that objective truth exists at even this level, then we don't have enough shared axioms for this conversation to go anywhere.

I can't engage with your second paragraph, because it hinges on whether we consider heuristics to be a valid solution to the halting problem. Since I believe a solution is only valid if it is deterministic, and you appear to hold that determinism does not exist, we're at an impasse.",hntn7wy,t1_hntm9sl,1639031159.0,False
rc9jzq,"A neural network can be seeded to run deterministically if you run it on a perfect computer that never makes mistakes. It can be deterministic. With the same seed and the same inputs it would arrive at the same result each time. Heuristic is not necessarily a subset of non deterministic. There is no good demonstration or proof that a heuristic is unsuitable for coming up with solutions. If you did have a proof of that I would be very interested to see that.

But saying that there is absolute confidence in mathematical proofs might be valid if the entity proving the math was perfect. Imperfect human beings cant prove anything to absolute certainty because humans aren't 100% reliable. As soon as you accept the inherent unreliability of human beings, and say each mathematician has a 1 in 1000 chance of making a mistake in validating or interpreting a proof, then there is still a non zero chance that all proofs have a mistake or error. The problem in your assumption is that human beings are incapable of using logic perfectly to arrive at a deduction with 100% certainty. It is standard in philosophy to go with maximal certainty e.g. the maximum certainty a human brain is capable of.",hntv6ub,t1_hntn7wy,1639037112.0,True
rc9jzq,"In theory of computation, for an algorithm to be said to *solve* a problem, the algorithm must provably produce the correct answer for every possible input, not just ones that you can come up with. That may be quite different from everyday use of the word, but it's crucial if you want to talk in terms of theory of computation.

Could an artificial neural network solve the halting problem in the sense the word is used in computability theory? No, unless you're willing to postulate that the different proofs showing the undecidability of the halting problem -- and the more general Rice's theorem -- are wrong.

Could you train a neural network that appears to, in practical terms, be able to answer the question for example programs that you come up with? Maybe. But there are already other methods for answering limited cases of the halting problem, and you don't necessarily need an AI for that.

Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.",hnu5jur,t3_rc9jzq,1639046035.0,False
rc9jzq,"What does provably produce the correct answer mean? If we build a super artificial intelligence that solves certain problems but the proof is so complex that no human could ever understand it, then does that mean it hasn't been proved? Or consider a proof by evaluating all possible inputs and outputs. It may be impossible for a human to ever go through all of the billions of answers to verify it, and yet the computer did verify it. And that doesn't account for the fact that humans are totally unreliable and have a non zero chance of being wrong or making a mistake particularly as things get more complicated. 

""Could an artificial neural network solve the halting problem in the sense the word is used in computability theory? No, unless you're willing to postulate that the different proofs showing the undecidability of the halting problem -- and the more general Rice's theorem -- are wrong.""

This is the crux here. I think the question is theoretically could a maximally intelligent God solve the halting problem? And i don't mean  a maximally intelligent God could create a proof solving the traditional halting problem, but rather give any code to that maximally intelligent being and it could determine whether the code halts or not.  It seems like a very different type of question whether intelligence can solve a problem compared to a direct computational approach. The way I think about it is humans create solutions and proofs in math, using intelligence, that would never be achieved with standard computation. They seem like different problem domains.

""Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.""

That applies to all the high end math proofs humans make today. I could never determine whether the advanced math proofs made today are correct.amd yet I still believe they're correct.",hnv8zh0,t1_hnu5jur,1639067145.0,True
rc9jzq,"> What does provably produce the correct answer mean?

In case you don't have a CS background, it means formally proving that an algorithm, if followed exactly, will logically and inevitably reach the correct answer. It's similar to a proof in mathematics.

If you do have a CS background -- as you probably do considering the understanding you show -- you knew that already, so I'm not sure what the point of the question is.

> If we build a super artificial intelligence that solves certain problems but the proof is so complex that no human could ever understand it, then does that mean it hasn't been proved?

Okay, so I'm assuming we're talking about an ANN producing a proof about something rather than just producing yes-no answers to the question of whether a program will halt.

Can you directly prove that the proof (supposedly produced by the AI itself) is valid? Or that the proof has been generated using a method that's formally proven to only produce valid proofs? If the answer to either of those is yes, then I would generally consider it proven even if we can't directly understand the proof itself, if the methods used for either producing or checking the proof were formally proven to be correct. That is, of course, with the usual caveat that extraordinary claims require extraordinary evidence, and that we should carefully check the validity of whatever logic was used for supposedly vouching for the validity of the proof.

However, it's worth noting that artificial neural networks don't generally work in such a way that you could formally prove anything about the validity of their results. They just empirically seem to work for a bunch of problems. So, if we're dealing with an ANN, we can't prove that the method (the ANN itself) with which the supposed proof was found was guaranteed to only produce valid proofs, and so the proof that it produces would need to be validated in some other way. That could be either by humans understanding the proof and meticulously checking it for errors, or using automated theorem proving. (Automated theorem proving is then again an undecidable problem in the general case, although it's decidable for some subset of possible inputs, so YMMV.)

However, the point is kind of moot if we're talking about perfectly solving the halting problem. It has been proven to be undecidable in the general case, at least in the sense that for a hypothetical algorithm that purports to solve it, it's always possible to construct an input for which it fails.

You might be able to have a neural network that produces what empirically seem to be correct answers to various instances of the halting problem, with or without any formal proof of anything. You might be able to have a neural network produce a formal and provable theorem about *which* kinds of special cases of the halting problem are decidable. But you wouldn't be able to have a neural network that provably (in the formal sense) correctly answers the halting problem for any and all instances of it.


> This is the crux here. I think the question is theoretically could a maximally intelligent God solve the halting problem? And i don't mean a maximally intelligent God could create a proof solving the traditional halting problem, but rather give any code to that maximally intelligent being and it could determine whether the code halts or not.

That's kind of getting into metaphysics. I don't think there will be a valid answer to such a question, any more than to any other question regarding what results from omnipotence.

> It seems like a very different type of question whether intelligence can solve a problem compared to a direct computational approach. The way I think about it is humans create solutions and proofs in math, using intelligence, that would never be achieved with standard computation. They seem like different problem domains.

That's a valid question to think about. Humans, and actual intelligence (whatever that means), do usually arrive at solutions in a different way than direct computation.

I don't think that necessarily helps a hypothetical AI formally solve the halting problem, although I guess I kind of see where you're coming from. Yes-no answers to the halting problem could hypothetically be accompanied by a proof or reasoning of why a program always halts or why it doesn't, and an intelligence that doesn't (superficially speaking) work by means of classical mechanical computation might be able to creatively come up with answers and proofs that a classical algorithm wouldn't.

The problem is that, if whatever produces those answers and proofs *doesn't* do it systematically, e.g. by enumerating all possible proofs, you can't really prove that it will always be able to find one for *all* instances, even if the answers could be shown to be correct about the ones for which it does. The same is of course true of humans.

> ""Another problem is that with an artificial neural network, it might be very hard to convince yourself that its logic is correct. ANNs aren't exactly known to be transparent in terms of how they arrive at their classifications or answers.""
>
> That applies to all the high end math proofs humans make today. I could never determine whether the advanced math proofs made today are correct.amd yet I still believe they're correct.

Of course, but I don't think that's the same thing. *Someone* has been able to present those proofs, and they've been independently reviewed by others with the required expertise. While people might be able to understand parts of why a nontrivial artificial neural network behaves the way it does, large ANNs are largely black boxes to *everybody*.",hnvvavp,t1_hnv8zh0,1639075814.0,False
rc9jzq,"At some point wouldn't the only way to determine reality on the inner halting or not, be to actually run the code to prove the halt. 

It's sorta like the quantum physics cat example. The truth is unknown till the truth is known",hntb6yf,t3_rc9jzq,1639024059.0,False
rc9jzq,"I see what you're saying but perhaps the AI can give an estimate as to how accurate it thinks its results are. And as you add more examples it should be able to get more accurate over time. If you were to scale things up massively could it give you near perfect answers? Quite possibly its hard to say. You know if it can tell you with 99.99999% (six sigma) certainty that it will halt, I'd say that's good enough. At that point we consider scientific theories to be essentially true.

Or could there also a tipping point where it becomes such a genius neural network that it can determine with 100% accuracy whether a program will halt?",hntbh0g,t1_hntb6yf,1639024208.0,True
rc9jzq,"Mathematical proof is not ""good enough"" if it does not cover 100% of the cases.",hnzibxz,t1_hntbh0g,1639144085.0,False
rc9jzq,"Especially if it's code written by humans. It would potentially pick up on an inexperienced coder or common mistakes leading up to a halt. 

Could save a lot of computational power if it think 99% it will halt and just save the full test for codes it thinks might actually go through.",hntbsji,t1_hntbh0g,1639024375.0,False
rc9jzq,"Sounds almost like Microsoft Clippy for programming.

> Looks like you're trying to set up a CRUD server! Can I help?",hntc71k,t1_hntbsji,1639024588.0,False
rc9jzq,"The best way to tackle termination problems is by using symbolic methods, not neural networks.

There are various techniques that can solve the problem for a given class of algorithm, up to some degree of complexity. So, by not defining what ""most of it"" means, we could say that (symbolic) AI is able to solve a reasonably non-empty input fraction.

In general, whenever you have a rigorous symbolical representation of your input, the best way to approach it is to keep the nice symbolic representation and reason about it in a symbolical way. You would just complicate your life and impoverish your starting knowledge by adopting a neural network approach. 
There exists hybrid approach, but they are still in the early phase.

Anyway, the halting problem is in general undecidable and theoretically there is nothing that can change that.",hntyj0m,t3_rc9jzq,1639039894.0,False
rawuw6,"had a spare camera in my cubicle 

used it to detect faces and send a desktop notification 

used it to switch to work from watching random videos on YouTube when someone was around",hnkzxwu,t3_rawuw6,1638877595.0,False
rawuw6,You got a git for that? Unironically sounds useful :D,hnl1iod,t1_hnkzxwu,1638878691.0,False
rawuw6,"I'll share it with you it's not on GitHub

it's pretty basic just found a interesting use case for it",hnl1ref,t1_hnl1iod,1638878854.0,False
rawuw6,Can you also share it with me?,hnn40yf,t1_hnl1ref,1638912779.0,False
rawuw6,"you know, for *science*",hno8sdv,t1_hnn40yf,1638932024.0,False
rawuw6,"will share it in the comments
I'll have to rewrite it but it's pretty easy",hno3yl1,t1_hnn40yf,1638929577.0,False
rawuw6,+1 share pls,hnop0bv,t1_hnl1ref,1638941204.0,False
rawuw6,Share please!!,hnorgay,t1_hnl1ref,1638942854.0,False
rawuw6,Saw something similar on YouTube where some device detects movement (not capturing faces) near the area and the computer screen switches to desktop automatically. It just switches automatically and I thought that was pretty cool.,hnl2k9l,t1_hnkzxwu,1638879390.0,False
rawuw6,Niceee,hnl1pks,t1_hnkzxwu,1638878819.0,True
rawuw6,"I work with ECG signals. My company has TBs of  labeled data from 30+ years in the ECG monitoring business. We are using it to build an automated arrhythmia detection system.

In practice, ECG data is just a very large array of 16-bit integers. Labels can be for a single heart beat or for a segment of any length.

There’s some bureaucracy to it because everything is FDA regulated, but I love my job.",hnl67ae,t3_rawuw6,1638881617.0,False
rawuw6,That's fantastic! I love the idea of working on research that might save someone's life. Nicely done!,hnl7gcq,t1_hnl67ae,1638882325.0,False
rawuw6,What job title/field would this be considered as? I want to get into something similar.,hnmocsf,t1_hnl67ae,1638906168.0,False
rawuw6,"My title is Data Scientist but since it’s a small company I wear many hats. Data engineering, Software Dev, MLOps, etc. I like that aspect of the job as well (some poole don’t).",hnn0s2o,t1_hnmocsf,1638911437.0,False
rawuw6,I am planning to study AI or computational science. Which is the best way would you say I can work in research programs in scientific/medical fields? Should I apply for internships and slowly build experience in that area?,hnpsfqt,t1_hnn0s2o,1638970991.0,True
rawuw6,"While there are some exceptions, the vast majority of research jobs in both academia and industry require a PhD. Sometimes ""research technician"" jobs will be done by people with a [B.Sc](https://B.Sc) or [M.Sc](https://M.Sc); however, they are not usually doing research but assisting with building the technology used by the researchers.

Getting a research technician type job usually some expertise in the software being used, e.g., Python, R, Matlab, etc. So learn to program in software languages used in scientific circles. Also, it is good to have a solid knowledge of analytics and statistics since it comes up frequently.  


Again, there are always exceptions but this describes the majority of such positions.",hnr1c9v,t1_hnpsfqt,1638989410.0,False
rawuw6,"I'm currently doing my bachelors in computer science, which is the best course would you say will help me achieve my goal?",hnr2p7o,t1_hnr1c9v,1638989934.0,True
rawuw6,"1. Any courses on AI, machine learning or computational intelligence.
2. Any courses on analytics, statistics.
3. If your university offers an introduction to research as a graduate course, then see if you can get an exemption to take it as an undergraduate. Such courses are usually not very difficult and this would be very helpful.
4. Since computer vision is still big in medical research then courses on computer vision would be good.
5. Any courses that deal with time-series data.

&#x200B;

If you DM me a link to your university course catalogue, then I could probably make more specific suggestions.",hnr3tu0,t1_hnr2p7o,1638990362.0,False
rawuw6,"Thanks for the list. 
My college is pretty avg and doesn't offer any of those course so I'll have to go somewhere else for PG.",hnr6c17,t1_hnr3tu0,1638991323.0,True
rawuw6,"Ultimately, it is all about having the experience to convince somebody you can do the job. The ""easiest"" (without saying it is easy) is to do a course, but if you can get the experience in another way, then it all counts. I certainly hope you manage to do it. Nothing better than somebody achieving their dreams. :)",hnr6lvq,t1_hnr6c17,1638991428.0,False
rawuw6,"I'm currently doing my bachelors in computer science, which is the best course would you say will help me achieve my goal?",hnr2nxn,t1_hnr1c9v,1638989928.0,True
rawuw6,Trained an NLP model to generate the onion articles,hnmrp5c,t3_rawuw6,1638907675.0,False
rawuw6,What were the results like?,hnoh0kq,t1_hnmrp5c,1638936368.0,False
rawuw6,"Still a work in progress, here’s a shitty proof of concept with an overfitted model (http://67.205.142.210:3000/). FYI it’s a bit slow since I didn’t feel like paying for a GPU machine lol.",hnpvx4c,t1_hnoh0kq,1638972728.0,False
rawuw6,Generated a Frank Ocean inspired album using deep learning,hnmxb83,t3_rawuw6,1638910029.0,False
rawuw6,lemme hear,hnmxrkt,t1_hnmxb83,1638910224.0,False
rawuw6,im currently training an AI model to predict race winners,hnmwmii,t3_rawuw6,1638909736.0,False
rawuw6,[current progress](https://imgur.com/a/QEMeVcF),hnpgoge,t1_hnmwmii,1638963852.0,False
rawuw6,"Getting an RSA algorithm working by hand was hell, but kinda fun",hnmje60,t3_rawuw6,1638904001.0,False
rawuw6,"I’m still at the beginning of my career but I worked on music in my undergrad. My latest project visualizes emotions in non-Western music.

Open source on [GitHub](https://github.com/nhstaple/feelskunaman) 🎸. Anyone can use it you just need SpotifyAPI keys.",hnnloab,t3_rawuw6,1638920643.0,False
rawuw6,"Most recent project was object detection and instance segmentation for orthopaedic x-rays. We were looking to detect certain abnormalities. A big part of the project became generating synthetic data for training since medical imaging data is so expensive (and that’s just for the images, not even labelled). At the moment I’ve just started my PhD and am working on explainable AI for healthcare, particularly around generative models since I have some experience with those.",hnnnd2z,t3_rawuw6,1638921429.0,False
rawuw6,"As part of a research project, I implemented a module to a legal document management system. When a case file was uploaded, it was ranking and suggesting possible legal precedents that could be used in court, saving intern time.

Nothing fancy, it was just using a bag of words model, with tf-idf, but it was getting the job done as the case files were very similar.",hnobzos,t3_rawuw6,1638933671.0,False
rawuw6,"Still in school, so this is an academic project, but I'm working on a project to teach an AI to play Hnefetafl. It's really fascinating seeing and working with the math involved.",hnohhwz,t3_rawuw6,1638936638.0,False
rawuw6,this is an interesting post,hnox7op,t3_rawuw6,1638947242.0,False
rawuw6,Didn't get much comments tho :(,hnpg9kn,t1_hnox7op,1638963544.0,True
rawuw6,[deleted],hnl4ysj,t3_rawuw6,1638880895.0,False
rawuw6,That's awesome. I hope you do win an award for all the work.,hnl6r6k,t1_hnl4ysj,1638881933.0,True
rawuw6,They just lost their job,hnnavor,t1_hnl6r6k,1638915695.0,False
rawuw6,"LOL!! Love it! :)  (not true thankfully, but I love it)",hnpryxl,t1_hnnavor,1638970752.0,False
rawuw6,what did he say?,hnnd3dw,t1_hnl6r6k,1638916692.0,False
rawuw6,Said that they worked on some really important projects that could end up changing the field of AI. Not sure why the comment got deleted,hnorkig,t1_hnnd3dw,1638942938.0,True
rawuw6,It was getting downvoted into oblivion so I assumed people did not want to hear about my research. :),hnprwgh,t1_hnorkig,1638970717.0,False
rawuw6,Would you be able to share some of the publications from your work?,hnl6leh,t1_hnl4ysj,1638881841.0,False
rawuw6,[deleted],hnl7c5d,t1_hnl6leh,1638882261.0,False
rbng33,"Best I can figure, it comes from [core dump](https://en.m.wikipedia.org/wiki/Core_dump). I still don't understand how ""dumping"" got that particular meaning when it comes to computers, and now that I've noticed that I don't know, it bothers me slightly.",ho83f8t,t3_rbng33,1639296186.0,False
rbng33,"**[Core dump](https://en.m.wikipedia.org/wiki/Core_dump)** 
 
 >In computing, a core dump, memory dump, crash dump, system dump, or ABEND dump consists of the recorded state of the working memory of a computer program at a specific time, generally when the program has crashed or otherwise terminated abnormally. In practice, other key pieces of program state are usually dumped at the same time, including the processor registers, which may include the program counter and stack pointer, memory management information, and other processor and operating system flags and information. A snapshot  dump (or snap dump) is a memory dump requested by the computer operator or by the running program, after which the program is able to continue.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",ho83ghv,t1_ho83f8t,1639296212.0,False
rbng33,"Well, I'm assuming Screen Dump comes from the concept of ""core dumping"" which is when a file of a computer's documented memory of when a program or computer crashed.

The term ""core dump"" likely originated in the 1960s when early computers used magnetic core memory. When a running program crashed, all of the data in the entire core was printed out on paper to help with debugging.

Core dumps are generated when the process receives certain signals, such as SIGSEGV, which the kernel sends when it accesses memory outside its addressed space.",ht2k3h9,t3_rbng33,1642445213.0,False
rbng33,"Thank you, that sounds likely!",ht5w6ru,t3_rbng33,1642503917.0,True
rb9f0g,"This is probably more a physics question than computer science question but ones used for my graduate courses have been:

“Optical Waveguide Analysis,” K Kawano and T. Kitoh, Wiley, 2001

“Computational Electrodynamics: Allen Taflove and Susan C. Hagness, Artech House, 2005, (Third Edition)

“Plasmonics”, S. A. Maier

“Surface Plasmon Nanophotonics”, M.L. Brongersma and P.G. Kik, Eds. 

“Principles of Nano-Optics”, Novotny and Hecht 

“Surface plasmons on smooth and rough surfaces and on gratings,” H. Raether 

“Near-field optics and surface plasmon polaritons,” Edited by: Satoshi Kawata

They aren't all computational books and really require rigorous computational math methods course",hno92pg,t3_rb9f0g,1638932174.0,False
rb9f0g,"A quick google search showed many books on this subject.   
No idea how good any of them are.",hno8snk,t3_rb9f0g,1638932028.0,False
rajca8,Take a look at your second and third year courses. You won't have a ton of free time for long.,hnjiu2t,t3_rajca8,1638843640.0,False
rajca8,Exactly,hnko8nc,t1_hnjiu2t,1638867893.0,False
rajca8,[deleted],hnkd3ip,t3_rajca8,1638859082.0,False
rajca8,this,hnkmhjl,t1_hnkd3ip,1638866386.0,False
rajca8,I’m a fan of cyber security and hacking. It’s a great addition because I doubt your program will cover it and it’s an amazing skill to have,hninnir,t3_rajca8,1638830013.0,False
rajca8,"Its a whole field, and i think op should at least study network architecture AND operating systems to barely start.",hnlpwev,t1_hninnir,1638891103.0,False
rajca8,"I have to respectfully disagree. I have learned those subjects as needed in my work on cyber security. For example you don’t need any of that to learn about a basic buffer attacks. Once you understand them you can learn a little about paging on the OS to understand w^x protections, but then get right back into hacking. 

I feel like it’s similar to just starting some coding projects and learning about data structures and algorithm complexity when you need to.",hnmt4tr,t1_hnlpwev,1638908278.0,False
rajca8,The missing semester of your CS education: https://missing.csail.mit.edu/,hnktq2o,t3_rajca8,1638872671.0,False
rajca8,"You beat me to it. This is the best advice around here. OP master everything on this course. 

Although, if you are on a program worth its salt, unless you are a  genius, your lots of free time has its days counted.",hnl4zn7,t1_hnktq2o,1638880909.0,False
rajca8,learn git. you will thank me later,hnkmh1k,t3_rajca8,1638866374.0,False
rajca8,Do universities really not teach git? I don’t imagine there is any software dev job where it’s not a necessity,hnkwp11,t1_hnkmh1k,1638875150.0,False
rajca8,My university didn't teach me git. Hell they didn't even teach me DS and Algos properly lol,hnl8v8q,t1_hnkwp11,1638883111.0,False
rajca8,"That’s just mind boggling tbh..

Especially as you could teach git alongside other topics.. at very least they should be teaching the basics of it",hnl9h72,t1_hnl8v8q,1638883431.0,False
rajca8,"Currently in university for CS, nowhere near learning git for class",hnm7zl8,t1_hnkwp11,1638899135.0,False
rajca8,never seen it even once,hnmej2h,t1_hnkwp11,1638901943.0,False
rajca8,Spend more time on your foundational courses to really *understand*. I don’t understand how you can have a lot of spare time as a first year CS student.,hnky1oo,t3_rajca8,1638876197.0,False
rajca8,"With previous experience (and depending on the specific courses they require, as what you’re thinking of may be second semester and/or second year) the first year of CS can be blown through somewhat easily

For example, where I go, the foundational courses are moreso second year. First year they basically just taught us Java and C in preparation for the next courses",hnm8602,t1_hnky1oo,1638899212.0,False
rajca8,"Before my first year I had taken more school maths classes than most of my peers, learned to program apps in Java, and did some basic computer architecture stuff in Minecraft (building a redstone computer). 

Definitely gave me a headstart and more free time in my first year - but the advantage certainly didn't last long! I imagine OP is just in a similar position, or they're just some freakish genius.",hnnwj7w,t1_hnky1oo,1638925855.0,False
rajca8,Get some weird board (not like a pi or arduino) and see what you can do with it,hnjvmfj,t3_rajca8,1638849243.0,False
rajca8,"Kung fu. Great way to develop discipline, stay fit and not get your ass kicked. You’ll spend most of your career barely moving as a programmer/software engineer so getting in the habit of being active is gonna pay dividends in the long run.",hnlcpbd,t3_rajca8,1638885103.0,False
rajca8,"What kinds of things do you want to do when you graduate? 

For web dev learn more about socket programming and/or webassembly. That will put you ahead of people who only know how to use existing frameworks and tools.

For AI/ML study math and stats such that you can compete with stats majors or get into a grad program. 

For desktop apps learn the Windows API.

For mobile apps study any mobile app toolkit. 

For embedded learn either VHDL or SystemVerilog using an FPGA. That will make you more competitive with EE and CE majors. 

For OS development do as much bare metal programming as you can in C and assembly and learn all the common hardware interfaces. 

For game development learn some Blender and other 3D tools. 

Basically take your preferred career domain and study information that is useful for it but won't be covered or isn't part of CS.",hnlcy1u,t3_rajca8,1638885223.0,False
rajca8,Do leetcode problems. It’ll help you get interviews and pass technical interviews,hnnhkbk,t3_rajca8,1638918723.0,False
rajca8,"ML, basic calc and linear algebra are enough to get started",hnjgn1a,t3_rajca8,1638842687.0,False
rajca8,"He said he’s a first year computer science student that wants to study things not in the program. Calc and linear algebra are basic things he’s likely already taken, and if he chooses to he’ll do some ML soon.",hnkcvie,t1_hnjgn1a,1638858929.0,False
rajca8,is linear algebra course in compsci hard?,hnkgbw3,t1_hnjgn1a,1638861440.0,False
rajca8,Soft skills. That is what is going to make the difference in an interview,hnkwgvb,t3_rajca8,1638874968.0,False
rajca8,"Statistics, get to logistic regression.",hnl2afs,t3_rajca8,1638879209.0,False
rajca8,"Just do personal projects - best way to learn and you can put them on your resume. Hey, if one is really good you might just get your own company going.",hnl4r9f,t3_rajca8,1638880770.0,False
rajca8,"I am also a first year cs student and its amazing how some people have this foreign concept called “free time” , i am studying for my final in 2 hours as i write this…",hnlb5qm,t3_rajca8,1638884316.0,False
rajca8,I'm finna get a C below for this math 141 final on god,hnosw0e,t1_hnlb5qm,1638943899.0,False
rajca8,"Join the debate club, or the boardgame society, or perhaps the rock society, etc.

Maybe volunteer for one of the charitable student organisations?",hnlfknk,t3_rajca8,1638886516.0,False
rajca8,Data Structures & Algo,hnlpqos,t3_rajca8,1638891035.0,False
rajca8,Learn how to write unit tests. They'll save you time in the long run.,hnm6c66,t3_rajca8,1638898411.0,False
rajca8,"this is the next obstacle in my journey.

any recommendations for resources that will teach this from the ground up?",hnsk2ig,t1_hnm6c66,1639011524.0,False
rajca8,"Whatever book or website you choose to learn this from should be fine. It's not hard to learn. It's hard to make it a practice, because once your code works, what's the point of writing tests to see if it works? So sometimes people will write the unit tests *first* \-- you can look up ""test driven development"" for this approach.

I didn't have any resource to learn from -- I just copied the style where I worked. Most of the code already had unit tests, and if you changed anything you had to add a unit test that covered your change.

Python has a built-in unit test framework now, but it didn't have that when I started; so my own unit tests are sometimes a little janky, lol.",hnvgh4c,t1_hnsk2ig,1639070020.0,False
rajca8,Thank you Mr. Numbers!,hnvj786,t1_hnvgh4c,1639071069.0,False
rajca8,"It's hard to say when:

1. You don't list what subjects you're currently doing and
2. You don't say what field of IT you want to go into...",hnysxj8,t3_rajca8,1639126074.0,False
rajca8,Check out pathfinder.fyi and choose hard skills most in demand from there,hovf4hr,t3_rajca8,1639712729.0,False
rajca8,"Look at web dev frameworks like Django or Laravel Backend. React or Angular frontend, if you'd consider web dev as a career option.",hnkr9yx,t3_rajca8,1638870547.0,False
rajca8,"Depends on what you want to do. Where I work a cert for Azure Fundamentals would have been a good step in the door. It is related to CS, but not CS itself.",hnjryc9,t3_rajca8,1638847600.0,False
rajca8,LearnOpenGL.com,hnl9kv3,t3_rajca8,1638883485.0,False
ra4e6d,"Graphics is basically all linear algebra, if you want some motivation you could look at for example ""the raytracing challenge"" which starts with a lot of matrix stuff.

In general: basically any modern subject in maths has some connections to linear algebra, because linear algebra makes a lot of difficult problems ""easy"" (e.g. solving partial differential equations using FEM) and so we try to really find those connections - and given how ubiquitous maths is in programming you can probably find some connections to whatever you want to do (in audio for example: dft is a linear transformation which already leads to matrices and a dft'd signal is a vector).",hng7get,t3_ra4e6d,1638792406.0,False
ra4e6d,Your graphics card is basically machine that does all the linear algebra related to rendering graphics on monitor. Graphics card is like a linear algebra calculator.,hngdzii,t3_ra4e6d,1638796473.0,False
ra4e6d,"Essentially all machine learning is also done on matrices. Actually, a lot of data science is, like principal component analysis (related to taking eigenvalues and vectors of a covariance matrix). In fact, all data is really just matrices with rows being observations and columns being properties associated with those observations.",hngf9o5,t3_ra4e6d,1638797173.0,False
ra4e6d,"I was in the same boat. Took the class and didn’t get the point. Since then, I’ve taken a computer graphics class, a machine learning class, and a deep learning class and have really regretted not taking it more seriously. But I do think they should have made a point of explaining applications at the time…",hnitfwt,t1_hngf9o5,1638832445.0,False
ra4e6d,It's extensively used in machine learning. Good to build a solid foundation early.,hngffdo,t3_ra4e6d,1638797260.0,False
ra4e6d,"Another application a little bit more theoretical (but really interesting) is quantum computing, almost everything in QC is linear algebra (and probability).

You should also read about the Google page rank algorithm, it has a lot of linear algebra.

Good luck with the subject! Give it a chance because is really beautiful",hngr220,t3_ra4e6d,1638802908.0,False
ra4e6d,"Not directly CS related, but relevant:  if you end up working in decision analytics or project management, a solid foundation in LA helps *a lot*. Basically all optimization problems are most easily solved as matrix manipulations. Need to figure out the perfect combo of manhours, materials, and profit for different products/systems? You can do that in a single, fairly simple, simplex tableau.",hnh75ft,t3_ra4e6d,1638809498.0,False
ra4e6d,"https://youtu.be/rowWM-MijXU
Watch this. Linear algebra is like epitome of mathematical applications today. Almost everything you can think of has linear algebra in it some way or the other. It is extremely abstract but hold onto it.",hng170d,t3_ra4e6d,1638787561.0,False
ra4e6d,"I failed linear algebra at university and had to re-take the exam; not because I’m bad at math (I got top grades in all our other math courses) but like you I struggled to connect it to the rest of the curriculum. I knew it had a connection to 3D graphics but we weren’t doing any of that, nor was I planning a career in game engine development.

Fifteen years later I’m building platforms for machine learning and I have done a couple of Coursera courses in ML as well, and let me tell you, linear algebra rocks. There are plenty of software engineering careers where you won’t miss it, but if you think ML/AI sounds like something you would like to explore, you should dive into linear algebra as soon as possible.

The Youtube channel 3Blue1Brown has a great video series about it: https://youtu.be/kjBOesZCoqc",hnghc9b,t3_ra4e6d,1638798275.0,False
ra4e6d,"I'll also throw in scientific computing. Scientists used to wonder if the universe would expand forever, equalize, or eventually shrink back down into another big bang. To answer that question they basically did one giant matrix multiply over measurements of the cosmic background radiation. Everything from chemistry to cosmology uses linear algebra everywhere. Many problems have ""perfect"" solutions (like the traveling salesman problem or laying out wires on a computer chip). Those problems can be turned into a system of equations, stuck in a matrix, and solved using a bunch of the stuff you're learning now.

I know people always talk about how important math is to CS. The reality is that I rarely deal with most math in my work. The exception is linear algebra and statistics, those are everywhere. Some professors once tried to identify the most important patterns in computing so they could build specialized chips. Linear algebra was one of only seven core computations that made the list. I could go on. The point is, not only is math generally useful in CS, you've actually chosen the single most relevant field of math to ask about!",hnh6cy7,t3_ra4e6d,1638809189.0,False
ra4e6d,Do you have any more information on those seven core computations? Sounds interesting and I'd be curious to learn more.,hnw7av3,t1_hnh6cy7,1639080604.0,False
ra4e6d,"As mentioned by others, linear algebra is vital to graphics programming.

A pragmatic example would be a list of 3D vertices that make up a cube. How would you transform the cube? Transformation matrices.

Other examples are intersections, raytracing, and quaternions.",hngop5l,t3_ra4e6d,1638801855.0,False
ra4e6d,"There are two aspects that motivate Linear Algebra in CS (at a high-level):

1. So, so, so, so many real-life computational problems can be formulated as a linear algebra problem; often this is a) some model of a real-world process or b) a collection of data, both of which can be structured in terms of LA objects (i.e., vectors, matrices, & tensors).
2. Modern hardware can do matrix operations \*incredibly efficiently\*. This is largely a product of decades of computer architecture work and optimized implementations of algorithms. A lot of this boils down to data reuse/locality and vectorization.

TL;DR: When you use 1) to represent a problem as LA structure, you can leverage 2) to solve that problem very efficiently (at least, as efficient as your algorithm will allow).",hnhu3te,t3_ra4e6d,1638818471.0,False
ra4e6d,"Machine learning, 3D graphics; game design.

I actually wanted to get more in depth into it myself but have been way too busy my junior year. Hopefully next semester in my senior year I can learn some from discrete math. Haven't taken a math class in 2 years.

I'm not awfully familiar with audio programming but I don't think it would require a lot of interest and knowledge in linear algebra, maybe some in calculus.",hnhyhil,t3_ra4e6d,1638820194.0,False
ra4e6d,"Game development is a major (and fun) one. Representation of graphics and transformations of them (rotation, translation, etc) is usually done through vectors/points and 3D/4D matrices. Jason Gregory (lead dev for Naughty Dog) has a whole chapter about it in his [book](https://www.gameenginebook.com/) - I highly suggest you read at least the “3D Math for Games” chapter for some real world use cases for linear algebra.

(source: I’m a game developer at a studio, and I can confirm linear algebra actually matters in my daily job)

Machine Learning and Deep Learning are also big use cases but other comments have mentioned it already.",hnizunv,t3_ra4e6d,1638835235.0,False
ra4e6d,You mentioned audio programming. I’d look into digital signal processing (dsp) as that’s what a good chunk of audio programming really is under the hood. Things like the discrete Fourier transform and related operations involve things you’d learn in your linear algebra course.,hnjymwp,t3_ra4e6d,1638850641.0,False
ra4e6d,It's extensively used in machine learning. Good to build a solid foundation early.,hngfge3,t3_ra4e6d,1638797275.0,False
ra4e6d,"Hi there I'm happy to say one of my friends has created a really cool website to help with linear algebra and webgl.

[http://www.invectorize.com/home](http://www.invectorize.com/home)",hnggdu5,t3_ra4e6d,1638797773.0,False
ra4e6d,"Maths’s hugely used in IoT, game development and thereabout, you’ll never know where you’d be in the future, so you’d better learn it now as you have such opportunity",hnhs4ou,t3_ra4e6d,1638817688.0,False
ra4e6d,"A matrix, A, can be a transformation of a vector, v.  So for instance There is a matrix that can rotate v by 90 degrees.  This is useful for many things.  In computers, it is useful for graphics.  It's useful in quantum computing as well.  There is something called a hadamard matrix which puts a quantum vector (v) into superposition of multiple states (you can look that up) which in certain systems in quantum programming is a rotation of the state vector on what's called a Bloch sphere  (coordinate space for a 2 level qubit) .  

https://en.wikipedia.org/wiki/Rotation_matrix

https://en.wikipedia.org/wiki/Hadamard_transform  ​

That being said, a vector can represent a lot of things.  It just depends what space it is in and its axes. 

The same goes with computer graphics.  On your screen are pixels that are each composed of RGB filters.  To display a large range of colors the human eye can see, each sub-pixel will have some value for how bright it needs to be in combination with the other sub-pixels for a specific pixel color.  This can be in the the form of a vector and objects created with the pixels can be moved around by linear algebra and matrix transformations.",hnk735u,t3_ra4e6d,1638855222.0,False
ra4e6d,"Gilbert Strang has an excellent book on the topic of linear algebra ([Introduction to Linear Algebra ](https://www.amazon.co.uk/Introduction-Linear-Algebra-Gilbert-Strang/dp/1733146652/ref=asc_df_1733146652/?tag=googshopuk-21&linkCode=df0&hvadid=500859832694&hvpos=&hvnetw=g&hvrand=2832046874964718038&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=1006523&hvtargid=pla-1235394973160&psc=1&th=1&psc=1)).

As many have pointed out here, matrices and vectors have a lot of applications in graphics. 

However, it goes far beyond that. In fact, linear algebra turns up almost everywhere, eg physics problems, data science, optimisation, etc. It is a topic that is well worth having a decent knowledge of, whether you are a mathematician or a computer scientist. 

I study as a mathematician (doing my PhD), and I am continually surprised at how often numerical linear algebra springs up. 

[Here is a free PDF of Gilbert Strang's book](http://libgen.li/edition.php?id=138573030). This is the 5th edition, though a 6th has recently been released. I'm not sure what the changes are. 

What is especially wonderful about the Gilbert Strang book is that it has a selection of sister lectures from MIT available free online on YouTube. So, if you read a topic in the book and struggle on the exercises, you can then turn to the lecture series for additional information. I think its a great way to learn. There's a chance it goes a little deeper than you might need, and might be more generalised, but at least you can get an appreciation for the breadth and importance of this topic!

Edit: you are also likely to run across NLA for things like image reconstruction and file compression, which are computer science topics :) let me know if you want more info. You talk briefly about enjoying audio - well, the discrete fourier transform is all linear algebra, and so it has applications in removing white noise etc (as mentioned by another commenter)",hnkx7xo,t3_ra4e6d,1638875562.0,False
rb71fe,RL?,hnmtvn8,t3_rb71fe,1638908576.0,False
rb71fe,"""real life"" I guess, but... honestly I have no idea where the assumption that DP in school is different from the real deal comes from.",hnn3d5n,t1_hnmtvn8,1638912508.0,False
rb71fe,"You rarely need to create algorithms yourself ""in real life."" You mostly find the ones you need on the internet.

In the rare cases that you do need to, I don't see why it should be different from you learned in school. Reality is often more complicated, but you learn this stuff for a reason.",hno7mys,t1_hnn3d5n,1638931431.0,False
rb71fe,My guess is reinforcement learning. That kinda makes sense.,hnosr51,t1_hnmtvn8,1638943797.0,False
rb71fe,"Yea, that makes more sense than ""real life""",hnosxcy,t1_hnosr51,1638943928.0,False
rahxnv,Since you finished formal automata and still interested in theory part of cs you can continue with Computability Theory then Complexity Theory.,hnii59h,t3_rahxnv,1638827791.0,False
rahxnv,Where/how did you study Automata Theory? Was it in college or some online course/self-teaching? I'm very fascinated in the subject but am always worried about missing key information while self-teaching.,hnjlhka,t3_rahxnv,1638844776.0,False
rahxnv,"College. Just finished my semester today on it. We used Peter Linz’s text book. 

We followed the book very closely. The teacher was all about us learning and didn’t care about our grades, so he challenged us a lot on the assignments and exams. Some of the problems on the assignments would take ~2-3 hours to complete (like devising Turing Machines, PDAs, DFAs)",hnjlt1p,t1_hnjlhka,1638844915.0,True
rahxnv,besides the mentioned computability and complexity theory you can also double down. There are flavours of automata that handle infinitely long words (e.g. Büchi Automata) which can be used to do software verification.,hoscpbo,t1_hnjlt1p,1639667089.0,False
rahxnv,"So you now seem to have ""half"" a understanding of what would be taught in my class on theoretical computer science. ""Basic"" things you might be missing (based on a cursory skim of that book's chapters)  


* The Myhill-Neorde theorem
   * See https://bosker.wordpress.com/2013/08/18/i-hate-the-pumping-lemma/
* A bit more complexity theory 
   * Thinking about NP as ""verifiable problems""
* More undecidability/computability theory
   * (Many-One) Reductions
   * SNM and Recursion Theorem
   * Index Sets
   * Hierarchy of uncomputable problems / Arithmetical Hierarchy
      * That one in general is interesting since it's one of the more graspable connection between logic and computability theory, if that interests you  


Unfortunately, I can't really recommend any good book. But the wikipedia articles on each of them are somewhat useful, though of course they don't offer practice exercises. Perhaps the open logic project might have more, especially on ""undecidability"" theory.",hnjxyez,t3_rahxnv,1638850320.0,False
rahxnv,A natural next step is compilers.,hnjt6et,t3_rahxnv,1638848141.0,False
raflir,"Typically a program/malware is impossible to run without supported environment or libraries, hence you have now two options:

1) write a program in C since it's almost supported by all OS then let it silently install the required environment/libraries. After that it can run the malware.

2) most of the systems support some high level languages by default and are installed beforehand, like, .NET for windows which run C#, vb.net or python for linux.

As for the libraries it's pretty simple, you can make two files inside each other, the first one checks for libraries and install the missing ones then it unpack the second and run it.

Edit: hmm it's cs subreddit, I think r/malware would be better",hni08xe,t3_raflir,1638820878.0,False
raflir,"I see, thank you",hni1nyl,t1_hni08xe,1638821437.0,True
raflir,"I have heard of payloads that included the interpreter with them. For example you’d have a power shell script that has the b64 encoded binary of the python interpreter. The powershell would decode the binary then execute it with the python as input. This assumes you know the computer architecture for the binary and that it runs powershell, but those are easy enough guesses.",hnmvmtu,t3_raflir,1638909310.0,False
raflir,"Think of them as ""self-contained"" programs (in order to run), that may connect to a online host (a server) in order to carry out a certain function.  Of course depending on the malware, will dedicate what it does, if it needs to connect to an external online host, etc...",hni1dj4,t3_raflir,1638821322.0,False
raadci,Tree structures are very common when you analyse text. For example to solve a mathematical equation or interpret a code file. You could look at [this](https://en.m.wikipedia.org/wiki/Parse_tree) but I would also recommend a text on computer languages like SICP.,hnhxy5x,t3_raadci,1638819982.0,False
raadci,"Desktop version of /u/forsasateri's link: <https://en.wikipedia.org/wiki/Parse_tree>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hnhxzmq,t1_hnhxy5x,1638819998.0,False
raadci,thanks,hninvze,t1_hnhxy5x,1638830109.0,True
ram49s,"Provider provides, sure.  Marginally I could see the utility of breaking up interfaces in this way, for composability, but broadly, the implementation sure seems messy.  OOP is a helluva drug.",hnj4gx2,t3_ram49s,1638837329.0,False
ram49s,"Is there a behaviour behind those providers?   
The pattern here is to inject different strategies (usually interfaces, btw, not classes) where some could apply. If you are just returning a fixed value, it has no meaning.

Title provider makes sense if you have or might have several ways to generate a title out of... Something. But it should provide a function for that, so that different implementations could provide different behaviours.

Height provider, for a human, doesn't make a lot of sense as its just a fixed value.",hnkj7tv,t3_ram49s,1638863701.0,False
ram49s,reinventing swift protocols,hnkm4fs,t3_ram49s,1638866076.0,False
ram49s,"I'm soon to be a junior engineer and to me it looks like he knows nothing about OOP. It's impossible that someone had to sit through a faculty-level of OOP course and do assignments by using inheritance **that** way. That is literally impossible. Also, being able to pass OOP course **without** using inheritance is **also** impossible. Ultra necessary basic concept, and honestly him **not even knowing that it exists** is very strange. So my honest advice for him is to get some quality book and exercise to get his way of thinking right. 

I don't think it should be your job to teach him this and it's something that he needs to use a bit of his time to sit it through and cram some examples to get into his way of thinking. If he's using it this way than, to me, it beats the entire purpose of OOP.",hnj6qox,t3_ram49s,1638838359.0,False
rabwtu,"Yes of course it is worth it! most practices are the same for more than 40 years.  
Unless there are new technologies on the matter, it won't make much difference. And even if there are, it doesn't matter so much, it would still be worthwhile. You will only have to do some ""upgrades"" on what you know.",hnhnoyh,t3_rabwtu,1638815955.0,False
rabwtu,Yes.,hnhnctt,t3_rabwtu,1638815823.0,False
rabwtu,Wow this is a good website! Information are good! It is not old at all!,hnk6euz,t3_rabwtu,1638854827.0,False
rabwtu,"The same theory I learned in 2000 is totally valid today. While programming languages and frameworks come and go, the theory and engineering principles are the same. The worst part is people want to reinvent the wheel all the time instead of reading and understanding and using what people already figured out 30-50 years ago. So, go through those textbooks your might learn a thing or a dozen.",hnkcrfi,t3_rabwtu,1638858850.0,False
r9bx1d,Switch != Bridge,hncih7u,t3_r9bx1d,1638726478.0,False
r9bx1d,"Sometimes, people don't explain stuff well enough for me to understand, making this programmed type of speech the chearest possible means for getting information across.

 Or, put simply,


 dataLoss(speechstyle = ""program-esque"") < dataLoss(speechstyle != ""program-esque"")",hqwygwn,t1_hncih7u,1641113128.0,False
r9bx1d,"What this article also shows is how useful personifying things can be. If you think of programs as actors or agents and personify them/have conversations with them, it makes it easier to conceive of them.",hncx22l,t3_r9bx1d,1638731787.0,False
r9bx1d,Much appreciated.,hnb21y8,t3_r9bx1d,1638696551.0,False
r9bx1d,This is a gem. Thanks!,hnb8hnx,t3_r9bx1d,1638701873.0,False
r9bx1d,Are there more kind of resources about networking for developers or beginners?,hne8jkj,t3_r9bx1d,1638749819.0,False
r9bx1d,"Awesome resource, thanks!",hneyrov,t3_r9bx1d,1638761458.0,False
r9qvnk,"I don't think that you should worry about your lack of programming experience. Your role is to lead the team, not do all of the work yourself. Talk to the more experienced programmers and get their opinions about how best to structure and split up the code. Lean on them to help with the more technical aspects and focus on the leadership stuff, like making sure everyone has something to do, making sure people are comfortable with their tasks, setting up tools to plan the project like Trello, and that kind of stuff.

For regularity of meetings, every week is best so that people stay engaged and keep doing stuff with the project. You can always add more to it if it gets done early, but projects always take longer than you'll originally expect them to. As an example of stuff to add, if it's a website, you can always do more to the ui, like making it both computer and mobile friendly.

To help with the skill gaps, a good strategy can be to pair up experienced and inexperienced people to work on tasks together. Pair programming is slower, since you have two people doing the same thing, bit in the long run it will help everyone get to the same skill level.",hnepknk,t3_r9qvnk,1638757310.0,False
r9qvnk,"Just to add, pair programming is great not only to help the less skilled, but it helps the more experienced programmer learn how to communicate concepts and ideas clearly and helps them better their own understandings of how things work when they have to explain it and potentially get questions they never asked themselves.",hnet754,t1_hnepknk,1638758939.0,False
r9qvnk,"Maybe you should step down as a team lead, if you really feel you aren’t experienced enough. I can guarantee you that eventually your inexperience will surface through one way or another, and someone more skilled may challenge your position as team lead.",hndrhpp,t3_r9qvnk,1638742868.0,False
r9qvnk,"I completely disagree with this sentiment. This is a valuable learning experience that OP should go through. It's a safe project to learn with, since it's just a highschool club project that, as he puts it, should be simple. Stepping down should only be considered if OP feels like the amount of work it takes to be a team lead is negatively affecting them outside the club.",hnenef5,t1_hndrhpp,1638756336.0,False
r9qvnk,"I can tell you as a high school student, appointing someone inexperienced into a leadership position will only spur a lot of negative sentiment, especially from the experienced folks you want to retain.",hnewt85,t1_hnenef5,1638760584.0,False
r9qvnk,"Or negatively effects the team experience. Just because it helps OP learn doesn't mean it helps the other 9 people trying to gain experience as well. Not saying OP should or shouldn't step down, just that it really depends on the group as a whole what's truly best.",hnesx7x,t1_hnenef5,1638758814.0,False
r9qvnk,"By the sound of it, the project you’ll be working on is somewhat already decided. In that case, try setting up an introductory meeting to get to know everyone and discuss skill sets, etc. Then, start planning the project with roadmaps, visualizing the code and its dependencies within the scope of what you’re building. This visualization can be a guide, as it provides tasks of what needs to be built. Some things will be more complicated, while others will be much more simple. As long as you keep track of how the different tasks interconnect and maintain consistency, you can split these tasks amongst the group by skill and interest.

There’s a variety of tools out there that can help with this kind of thing, such as GitHub with its repository projects functionality. Try to outline the project, divide the tasks, and approach it systematically (for example, start off with pseudocode before getting more complicated). Don’t forget regular check-ins to make prevent issues with merging code later on.",hndsbn8,t3_r9qvnk,1638743198.0,False
r9qvnk,"Focus on figuring out the leadership stuff. The thing to remember is you’ve got 9-10 coders and only one leader. So you should start with assuming you’ll be doing none of the coding. Once you’ve got a handle on the management you might find that there’s time to schedule some coding to yourself. Being leader doesn’t mean you have to have all the ideas yourself; lean on those members who have more experience, make it clear you’re listening to them.",hnfsf12,t3_r9qvnk,1638779976.0,False
r95hk3,SVG’s are descriptions of images. So the computer can generate an arbitrarily good pixel display of the description. See Wikipedia on Rasterization,hnaah2v,t3_r95hk3,1638678247.0,False
r95hk3,"Agreed [rasterization](https://en.m.wikipedia.org/wiki/Rasterisation) is the general name of the process that computers use to turn mathematical description of shapes (as stored in SVG) into actual pixels for display. Note that this occurs ONLY when you view the result and takes into account how zoomed in you are when you are viewing it. So if you zoom in more, the rasterization process is re-run. This is what allows you to zoom as much as you want and never run out of “resolution”.

Also to answer the specific request for the how this goes for a circle, this [Wikipedia article](https://en.m.wikipedia.org/wiki/Midpoint_circle_algorithm) shows one of the classic algorithms for that case.",hnclroe,t1_hnaah2v,1638727695.0,False
r95hk3,"It’s math. Take a circle with its center at (xc, yc) and with radius r. Now take a point that you’re interested in, (x, y) and check if it’s part of the circle, how do you know? Well is the point further than the radius of the circle from the center of the circle or not? In other words sqrt((x - xc)^2 + (y - yc)^2 ) <= r. If that statement is true the point is in the circle, if it’s false it’s outside of the circle. When you zoom in you check each and every pixel, using its corresponding coordinates within the image, color them appropriately.

There’s equations for rectangles, rotated rectangles, triangles, ovals, various curves, spheres, cubes, etc etc etc.",hnambx7,t3_r95hk3,1638684901.0,False
r95hk3,"Its a lot less complicated than you're assuming.

**S**calable **V**ector **G**raphics format stores image data not as pixels, but as shapes and curves (and colors and widths, etc). An SVG viewer **draws** the shapes to display the image.

Here's a simple example: below is literally the SVG code for a circle with radius 50, centered at position (100, 100), on a canvas that is 200x200 units in size.

&#x200B;

    <svg width=""200"" height=""200"" xmlns=""http://www.w3.org/2000/svg"">
      <g>  
        <title>Layer 1</title>  
        <ellipse ry=""50"" rx=""50"" id=""svg_2"" cy=""100"" cx=""100"" stroke=""#000"" fill=""#fff""/>  
      </g>  
    </svg>

If your more curious, heres the full SVG specification, which defines the SVG code, talks about the document object model (DOM, also a part of web page rendering), and the rendering model: https://www.w3.org/TR/SVG/",hnaajq7,t3_r95hk3,1638678285.0,False
r95hk3,Key word: scalable,hnazrmc,t1_hnaajq7,1638694641.0,False
r95hk3,"Bézier curves, [https://en.wikipedia.org/wiki/Bézier\_curve](https://en.wikipedia.org/wiki/Bézier_curve), can represent curves using straight lines. A straight line is defined by its endpoints, which are just numbers. The section of the Wikipedia article on constructing the curves is what I was most curious about the first time I looked it up.",hnaf5np,t3_r95hk3,1638680689.0,False
r95hk3,math.  vector images are based on math.  computers are not bad at doing math and drawing it on the screen.,hnavvl5,t3_r95hk3,1638691555.0,False
r95hk3,It's about vector graphics: https://en.m.wikipedia.org/wiki/Vector_graphics,hnbb4do,t3_r95hk3,1638704027.0,False
r95hk3,"**[Vector graphics](https://en.m.wikipedia.org/wiki/Vector_graphics)** 
 
 >Vector graphics, as a form of computer graphics, is the set of mechanisms for creating visual images directly from  geometric shapes defined on a Cartesian plane, such as points, lines, curves, and polygons. These mechanisms may include vector display and printing hardware, vector data models and file formats, and software based on these data models (especially graphic design software, Computer-aided design, and Geographic information systems). Vector graphics are an alternative to raster graphics, each having advantages and disadvantages in general and in specific situations.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hnbb57a,t1_hnbb4do,1638704047.0,False
r95hk3,"Simple answer: No.

A computer cannot even draw a mathematical point (which has no dimensions whatsoever)—in fact one cannot exist in any real sense. A mathematical circle has no height and no width.

No perfect mathematical shapes exist in nature, although some shapes come very close. Even when a physical law applied to a certain situation predicts a perfect mathematical shape, there are always extra factors that are not considered by the physical law that mess things up: friction, air resistance, relativistic corrections, quantum uncertainty, atomic granularity.

An example of something very close to a perfect circle in nature is the orbit of Venus about the Sun. This near perfection is attained because:

1. The initial velocity of Venus was such that its orbit is nearly circular and not as elliptical
2. There is very little air resistance or friction in space
3. The Sun is so distant from Venus and so round that it acts almost exactly as a point source of gravity
4. Venus is so big that quantum effects are very small
5. Sun's gravity is weak enough that Newton's law of gravitation is reasonably accurate

But, all of these statements are not perfect, so there are still many small sources of deviation from a perfect circle, even for Venus' orbit.

Consider trying to draw a perfect circle on paper with graphite. Even if you were able to use an AFM tip, laser sensors and a feedback loop to perfectly space every single carbon atom to form the circle, you still have the fact that the circle is made out of atoms. Zoom in enough on the circle and it's not smooth anymore because of the profile of the atoms.

(This moved fast from CS to Science)",ht2lguo,t3_r95hk3,1642445732.0,False
r8rl8f,"Computer graphics is its own subfield of computer science. Specific course names may vary wildly from school to school. You'd really need to check the course catalogue for any possible course of interest.

Some examples may include:  
Introduction to Computer Graphics  
Computer Graphics  
Applied Computer Graphics  
Application of Computer Graphics

etc.",hn79xnc,t3_r8rl8f,1638630844.0,False
r8rl8f,Thanks!,hn7h5nj,t1_hn79xnc,1638634189.0,True
r8rl8f,My pleasure. Good luck with your studies!,hn7hpvb,t1_hn7h5nj,1638634440.0,False
r8rl8f,"Here's an overview of what topics would be covered in such a course: http://www.cs.cornell.edu/courses/cs4620/2017sp/cs4621/

OpenGL resource:
https://learnopengl.com/

WebGL resource: https://webglfundamentals.org/

Cool youtubers to check out:

https://www.youtube.com/c/SebastianLague/videos

https://www.youtube.com/c/IndigoCode/videos",hnad28v,t1_hn7h5nj,1638679567.0,False
r8rl8f,"Interactive media, creative computing, etc.",hnao6sr,t1_hn79xnc,1638686110.0,False
r8rl8f,"if you are interested in lighting and how things look, then look for courses focused on: rendering, light transport or material appearance modelling.

if you want to get into computer graphics then it's good to try everything under the umbrella of graphics. the course I have taken and plan on taking are: Computer graphics, rendering, image analysis, digital signal processing, computer vision, geometry processing.
All of these areas overlap in different ways and things learned in one area can be used in another area. there are probably more that I haven't mentioned.",hn7fsfx,t3_r8rl8f,1638633572.0,False
r8rl8f,There’s linear algebra involved as well depending how deep you get into computer graphics.,hn838uk,t3_r8rl8f,1638643326.0,False
r8rl8f,"Yeah, if you're at all interested in university level computer graphics, linear algebra is more or less mandatory.",hn8a3g1,t1_hn838uk,1638646160.0,False
r8rl8f,Does the category of computer graphics also include the digital signal processing for the video or is that of another CS category?,hn89vew,t3_r8rl8f,1638646066.0,False
r8rl8f,"DSP is generally an EE field, unless you have a specific example in mind?",hn8lthy,t1_hn89vew,1638651174.0,False
r8rl8f,"At my undergrad school, we had a CS class in DSP and it did fall under the Computer Graphics umbrella (as Image Processing).",hn9o4fh,t1_hn8lthy,1638667767.0,False
r8rl8f,"It depends on what you want to do. Images can be thought of as signals of red, green and blue channels and videos are just sequences of images. Performing analysis in on these signals (known as the frequency domain) allows us to get some useful information easier than just looking at the image itself. This is used in computer graphics for things like object tracking and also used in AI as part of the pipeline for things like object recognition.",hn95mpf,t1_hn89vew,1638659703.0,False
r8rl8f,We have it as a separate subject in our course so it's probably a field of its own.,hn7ugpj,t3_r8rl8f,1638639798.0,False
r8rl8f,"If you're interested in 3D game engines, I will tell you to also become really familiar with (in addition to computer graphics, linear algebra, etc.) computer systems (especially microarchitectures, multithreading, etc.) Newer 3D APIs (Direct3D 12, Vulkan) require a lot more knowledge in these areas than older APIs.

Also, if you can, skip OpenGL. Outside of mobile games and 3D modeling, it's a boomer relic (and unfortunately that's all that schools teach you). If you can, learn Direct3D 12.",hn9ovua,t3_r8rl8f,1638668112.0,False
r8rl8f,"> (and unfortunately that's all that schools teach you)

&nbsp;

Used WebGL in our Computer Graphics class lol",hnacioc,t1_hn9ovua,1638679286.0,False
r8rl8f,Yeah it was just called Computer Graphics for the subject I did and we used OpenGL to create 3D scenes. But there's a whole library of books for this one subject.,hna8bnf,t3_r8rl8f,1638677181.0,False
r8rl8f,Learn cpp. Graphics are specifically about vectors and points on the screen. Reflections and shadows are the pixel colors used. If you can program in cpp you be great it. Start learning everything about openGL.,hna8wsj,t3_r8rl8f,1638677479.0,False
r8rl8f,"Might be an Unpopular opinion, but Dont studdy Computer Science If making games and graphics is your only Motivation. You can take sub courses that specialise in that, but don't go into the world because you like games. 

I have personally experienced this over and over, first year first semester we are packed with students who want to be game developers, by the end of the first semester the attrition rate is like 70%, and it only ends in tears.

But then after you understand how computers work then it gets cool.",hnauyjd,t3_r8rl8f,1638690863.0,False
r8rl8f,"It is its own category. (Usually)
It's probably the most related to math.
My uni has a class for opengl under the cisc program. It's called ""computer graphics"" (so catchy)
I'd start out double checking you understand geo and calc fairly well, then see if MIT or someone has free courses online. 
Reflection and path tracing and stuff requires a lot of math and a decent bit of experience with matrix math and a good understanding of data structure type stuff from what it seems.",hnbuspp,t3_r8rl8f,1638716359.0,False
r8rl8f,Yes,hnf247u,t3_r8rl8f,1638762987.0,False
r8rl8f,https://pathfinder.fyi/results/Computer%20Graphics,hovfaud,t3_r8rl8f,1639712810.0,False
r8z2ei,"Commercialized means that you can't produce a product with their API and make money off of it. I assume this also means that you probably can't publish it on the app store, but don't quote me on this.",hn8unba,t3_r8z2ei,1638655018.0,False
r8z2ei,"But I can use it as a personal project, and put it on a resume for sure right?

As for the publishing on the app Store things still aren't clear to me. As long as I'm not monetizing my app, I'm curious if it still falls under ""commercialization"".",hn8w8w8,t1_hn8unba,1638655704.0,True
r8z2ei,"You can use it on your portfolio, yes. As long as you aren't making money off it, you can do whatever you want with it!

Edit: Commercializing something simply means making it available to buy.",hn8wh6m,t1_hn8w8w8,1638655799.0,False
r8z2ei,"I see, that clears it up. Thanks!",hndvph5,t1_hn8wh6m,1638744552.0,True
r8z2ei,If you release it for free and don’t include any in app purchases you can definitely publish. Allowing people to download the built binary for there device is not commercialization. Even better if you open source it. Saying you can check out my app on the AppStore would be amazing for your resume.,hn9vzzh,t3_r8z2ei,1638671373.0,False
r8z2ei,"Thanks for the clarification!!

>Saying you can check out my app on the AppStore would be amazing for your resume.

YES it really would lol. I've been applying to jobs a lot but my resume just never gets shortlisted for some reason. Plus so many recruiters have this requirement of ""must have at least one app published on the store"".",hndvzgm,t1_hn9vzzh,1638744663.0,True
r8z2ei,"I’m not a lawyer but I believe there’s no widely agreed consensus on what “commercial use” means or where the boundaries of it are. For this reason the “NC” family of Creative Commons licenses are best avoided and generally considered to be incompatible with open source. One of the tests in that scenario is “if you include it on a DVD of software, could you sell the DVD to cover manufacturing costs?” I guess that’s analogous  to your scenario where you have costs to publish an iOS app (active developer subscription) if you attempted to recoup that. To sum up, “it depends”. Might be worth avoiding the potential problems.",hnb2l6s,t3_r8z2ei,1638696995.0,False
r8z2ei,Hmmm. Well at least I can use it on my portfolio.,hndvrze,t1_hnb2l6s,1638744580.0,True
r8upnl,"They did it just like modern computers do it. The processor executes a program written using numeric machine code instructions, which are basically a set of digital logic gate configurations that get triggered by specific values. Instruction 01 could be a ""load from memory address into the accumulator register"" instruction, for example, and so when the processor reads an instruction and receives a sequence of bits that corresponds to that instruction, it enters a state where it will next read a numeric address, and then finally it will read the contents of that memory cell and store the value in the accumulator.

Then let's say the next instruction is an ""add accumulator with value"" instruction, and that causes the processor to enter a state where it next reads a raw value, and finally performs a binary addition operation (also implemented using logic gates) with the read value and the value in the accumulator, storing the result in the accumulator.

And so on. This is of course a rather simplified description even by early standards, but it's fundamentally how a computer works. Modern languages always get translated into machine language in the end.",hn8e4oi,t3_r8upnl,1638647867.0,False
r8upnl,"A programing language is just a way to interact with the computer's hardware. You can take a look at digital circuits to see how hardware is built to preform specific tasks. If you look at industrial automation and process control you can find many examples of how real world information is captured by computer systems. 
Have fun!",hn81vev,t3_r8upnl,1638642760.0,False
r8upnl," To understand the classical computer, it's useful to sort of build up to it in complexity. We started with electrical circuits where humans could manually open and close circuits via switches. A switch allows a circuit to store a single bit of information which says either ""the circuit is closed"" or the circuit is open. But what if, instead of a human having to flip a switch, we could create a switch that would open or close a different circuit based on whether or not its own circuit is open or closed. Such as switch is known as a transistor, and through the triumph of the transistor we can go beyond simple circuits and arrive at machines which can perform arbitrary computation by opening and closing circuits in complex patterns.

So however a punch card reader worked physically, it ends up in opening and closing a circuit in a pattern that can then be stored in the open and closed states of the computers internal circuits. Then the computer can use this stored set of open and closed circuits as a starting point and by opening and closing circuits in it's particular pattern( as specified by its instruction set), the computer can execute the stored program. (This might be a slight oversimplification, because real computers might rely on more complex physics for some sorts of memory, but all the logic in a computer could be implemented with circuits in theory)
    
Hopefully by this explanation, you can see that these processes are purely physical, there is no need for natural language such as that which might be used by a human.",hn915z3,t3_r8upnl,1638657806.0,False
r8upnl,It's all about those 0s and 1s.,hn7vy6z,t3_r8upnl,1638640406.0,False
r8upnl,i beleive the computer has each charecter defined hardware wise as a string of bits.,hn87pk3,t3_r8upnl,1638645162.0,False
r8upnl,"I guess in your example I would consider what was on the cards to be the ""programming language"" or ""input.""

Just a guess. Someone else probably has a better answer.",hn87t8h,t3_r8upnl,1638645204.0,False
r8upnl,"Each computer has with a specific machine language associated with it. IE if you feed the cpu's input lines with a specific train of 0s and 1s (low and high voltage levels, in the language of electronics) it will do a cpecific thing (this is an oversimplification obviously), early computers were programmed this exact way.",hn8iftf,t3_r8upnl,1638649727.0,False
r8upnl,[Crash course computer science](https://youtube.com/playlist?list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye),hn9o9zx,t3_r8upnl,1638667837.0,False
r8f02f,"It's **not** a ROM chip. It's some sort of flash memory, NVRAM, or EEPROM chip that can be updated with newer code.",hn5ahyi,t3_r8f02f,1638584605.0,False
r8f02f,Oh thank you. Could you explain what NVRAM and EEPROM are?,hn5bisu,t1_hn5ahyi,1638585114.0,True
r8f02f,"https://en.m.wikipedia.org/wiki/Non-volatile_random-access_memory

https://en.m.wikipedia.org/wiki/EEPROM",hn5gxll,t1_hn5bisu,1638587783.0,False
r8f02f,Thank you I now understand.,hn5ozn0,t1_hn5gxll,1638591868.0,True
r8f02f,"In my experience, EEPROM's are commonly used to contain system specific data like serial number, encryption keys, MAC address, etc... while NVRAM is used for more general stuff like system software images.",hn6svxr,t1_hn5ozn0,1638621171.0,False
r8f02f,Do you know why?,hn7f0os,t1_hn6svxr,1638633221.0,True
r8f02f,"ROM is typically used for permanent data such as serial numbers, calibration tables for the specific device, or programs that are not changeable such as a hardware specific boot sequence.  
NVRAM is typically used for performance or simplicity of design where the content needs to be changed rapidly.  Dynamic RAM (ie VRAM) often must be refreshed for the content to be retained.  Flash memory can be rapidly read and slowly written. Many flash memory devices also have limits on the number of times they can be written. Generally EEPROM has a much higher limit on the number of writes so they are often used to store data that needs to be preserved over a system restart ( power cycle ) such as a position of a gate or valve overnight.  So the types of memory for a computer system is selected based on targeted application function:  
     Dynamic RAM — Typically large memory so refresh cost is minimized; low cost; good to excellent performance; 
                                  ok to lose content I f reset/power loss.
     NVRAM — higher cost, simple design, Excellent performance.  Typically content preserved over reset/power
                        cycle.  Typically smaller memory amount.
     EEPROM — Fast read, slower write, higher write cycles vs FLASH; Content preserved over reset/power.  Typically 
                         Small memory amount.
     FLASH  — Fast read, very slow write, limited write cycles; Content preserved over reset/power cycle. Larger 
                       memory amount.
     ROM — Fast read, no write; Content preserved over reset/power cycle. Typically small memory amount.",hn88c1a,t1_hn7f0os,1638645424.0,False
r8f02f,Reading this response felt like watching Neo when he got Karate downloaded to his brain,hnc6xm8,t1_hn5ozn0,1638721687.0,False
r8f02f,Is that a good thing?,hncja7l,t1_hnc6xm8,1638726793.0,True
r8f02f,[removed],hn5tyxd,t3_r8f02f,1638594560.0,False
r8f02f,what kind of chip does it have? there are bios/eeprom chip programmers you can buy like ezp2019 and ch341a.,hn60l3p,t1_hn5tyxd,1638598531.0,False
r8f02f,I don't know how.,hn5ua1s,t1_hn5tyxd,1638594733.0,True
r8f02f,I have a WiFi smart camera which got bricked during firmware update. It's an Imou Ranger - 2 camera,hn5uxvu,t1_hn5ua1s,1638595111.0,False
r8f02f,What is a smart camera? I'm unfamiliar with this device.,hn5xr9m,t1_hn5uxvu,1638596736.0,True
r8f02f,r/techsupport,hn6qa0r,t1_hn5tyxd,1638619253.0,False
r8mrol,"I have resorted to using pointers within certain functions to help increase the readability of the code and also to help reduce obtuse indexing. But often times it simply comes down to the data structure that's being accessed and the most reasonable way to do that. A good example would be a linked list. By their very nature (especially in C/C++) they use pointers. Traversing a linked list naturally requires the use of pointer. Yes, if you organize the data well, you **could** access the list via an index, but it's probably faster and easier to use pointers.",hn6q0t5,t3_r8mrol,1638619054.0,False
r8mrol,I have used a pointer wen parsing [an obj file](https://en.wikipedia.org/wiki/Wavefront_.obj_file) to choose wich of my std::vetors to put data,hn7vqen,t3_r8mrol,1638640317.0,False
r8mrol,How did you use pointers to do that? could you explain in a little more detail?,hn9whzr,t1_hn7vqen,1638671607.0,True
r8mrol,"The file has different kinds of vectors (position, normal, texture) that all have to be paresd the same but indexed separatly.

Basicly I store a pointer to where it is supposed to go parse it and put it where the pointer points",hnqf10z,t1_hn9whzr,1638980866.0,False
r8mrol,"Yes they are. Consider a char array (mutable string) that you want to iterate over, inspect chars from and possibly modify all in one scope. You could use a pointer to do that.",hn6s3k9,t3_r8mrol,1638620599.0,False
r8mrol,">The main purpose of pointers is to stop the wastage of memory by copying of the values stored in a variable to another variable

No, not really. The main purpose of pointer is to point at something.

When pointer is used only for saving time/space, you can rewrite the code to be pointer-less and it will still work.  
Try it with dynamic data structures, e.g. Linked List.

Or when on lower-level, you need to write something to specific place in memory (e.g. writing `1` at address `53280` on C64 will change the screen frame color to white). And what type is for storing memory addresses? Pointer.",hn6pvs8,t3_r8mrol,1638618944.0,False
r8mrol,"I can right now think of 2 reasons

\- Polymorphism.  
\- Big bunch of memory you don't want to store on heap.",hn7skit,t3_r8mrol,1638639006.0,False
r8mrol,[deleted],hn6kt6r,t3_r8mrol,1638614706.0,False
r8mrol,"Actually, I asked whether pointers have any use in the same scope and not between function calls.",hn6ombx,t1_hn6kt6r,1638617928.0,True
r80r4b,"Aside from a mathematics textbook focussing on lambda calculus, I think your best bet would actually be getting some hands on experience with functional programming languages. It'll probably make understanding lambda calculus a lot easier in the long run",hn32n3i,t3_r80r4b,1638549722.0,False
r80r4b,"I second this. I took a course at uni that involved lambda calculus. They taught lambda calculus first and it was super difficult to understand. Then, they taught us a functional programming language and it all made sense. Suddenly after learning functional programming I didn't even need to study lambda calculus, since using functional programming languages is basically using lambda calculus rules.",hn5gym0,t1_hn32n3i,1638587797.0,False
r80r4b,"Barendregts ""The Lambda Calculus, its Syntax and Semantics"" is an absolute classic. (But very math-oriented)",hn3n9mk,t3_r80r4b,1638557831.0,False
r80r4b,"u/Exourion made a good suggestion in my opinion. 

You can start learning Haskell, which is a pure functional programming language that uses Lambda Calculus. 

I am using the book ""Haskell Programming from First Principles"" by  Christoph Allen and Julie Moronuki and the very first chapter is about Lambda Calculus. 

Hope that helps.",hn4uiq6,t3_r80r4b,1638576856.0,False
r80r4b,[Alligator Eggs](http://worrydream.com/AlligatorEggs/) is fun entry point to ideas,hn5f0kj,t3_r80r4b,1638586839.0,False
r80r4b,SICP,hn3fsmh,t3_r80r4b,1638554849.0,False
r80r4b,"??? Wikipedia does a better introduction to the lambda calculus. 

Unless SICP has a new chapter 6 I’m not familiar with, I’d vote no on this one. Though “The Little Schemer” does a nice job of sneaking in the Y combinator at the end, and makes you think you discovered it yourself.",hn4nvqi,t1_hn3fsmh,1638573760.0,False
r80r4b,"I think it’s a good intro to func prog more than anything, but yeah you’re right not the best for lambda calc",hn5k8wa,t1_hn4nvqi,1638589442.0,False
r80r4b,https://www.amazon.com/Introduction-Functional-Programming-Calculus-Mathematics/dp/0486478831/ref=sr\_1\_1?crid=281AXHPBSTE5L&keywords=lambda+calculus&qid=1638553357&sprefix=lambda+calcu%2Caps%2C184&sr=8-1,hn3c2do,t3_r80r4b,1638553392.0,False
r80r4b,"This looks quite cool.  I wouldn't want to learn functional programming instead of the lambda calculus, but I also wouldn't want to learn the lambda calculus without seeing it in a functional language.  Nice pick.",hn7ogzd,t1_hn3c2do,1638637303.0,False
r80r4b,"I'd take a look at chapter 2 of

https://www.microsoft.com/en-us/research/uploads/prod/1987/01/slpj-book-1987-full.pdf",hn6kgv8,t3_r80r4b,1638614415.0,False
r81i62,Minecraft redstone tutorials. I'm not even kidding.,hn3e8lp,t3_r81i62,1638554238.0,False
r81i62,OMG! Hell Yeah. Thank you for this,hn4rk75,t1_hn3e8lp,1638575460.0,True
r81i62,"Tbh, it was quite fun to implement a T-flipflop after learning the theory behind them.",hn3nk8t,t1_hn3e8lp,1638557951.0,False
r81i62,"Free game where you solve increasingly difficult logic gate puzzles. Maybe not the best use of your time considering you only have a few days, but I think it would give you some nice intuition.

https://apps.apple.com/us/app/make-it-true-solve-circuits/id1536287319",hn2xhsl,t3_r81i62,1638547673.0,False
r81i62,"oh thank you bro, this so much fun",hn4umxu,t1_hn2xhsl,1638576911.0,True
r81i62,it’s tricky to solve with the minimum number of clicks,hn69tq2,t1_hn4umxu,1638605306.0,False
r81i62,That is the very first thing I learned. Are you sure you want to write that exam this semester?,hn4r728,t3_r81i62,1638575289.0,False
r81i62,"Just for context I'm not in college, it's my high school exams and Boolean Algebra & Logic Gates consists more than half of the paper. There are other stuff like arrays, strings etc etc that I already know",hn4rgx9,t1_hn4r728,1638575419.0,True
r81i62,Ah ok. Sorry then,hn4w0i7,t1_hn4rgx9,1638577563.0,False
r81i62,neso academy on youtube!,hn6a5d8,t3_r81i62,1638605569.0,False
r7x6t1,"They are both very broad fields, and so there will certainly be some overlap.

At the highest level, AI is about creating algorithms that can solve problems through reasoning rather than explicit instructions as used in traditional software. AI has many subfields that can include making algorithms that think in human-like ways, to machine learning where solutions are discovered.

Computational science is about solving complex (scientific) problems using computational methods, and so recently has used AI quite a bit; however, it is not strictly focused on AI (parallel computing, hardware, etc. are also used). Generally, it focuses on maximizing the quality of models and simulations.

They both contribute to scientific research equally so neither would be closer or further from your goal. You would need to decide on a subfield to really answer that question.",hn2803f,t3_r7x6t1,1638535794.0,False
r7x6t1,"My PhD concentration is in computational science. I’ll speak to that and let others focus on AI.

* Computational science has its origins modeling physics and engineering problems. Think PDEs.
* The field has naturally evolved to include randomized algorithms, numerical multilinear algebra, streaming problems, among others. 
* The application side has shifted from high-energy physics modeling to data science and ML (like so many areas). 
* Your most essential tools come from a numerical analysis class and maybe optimization. 
* Everyday questions are “do I get the same approximation error with 10 eigenvectors as I do 1000?” or “Should I pay the price for a Newton method or just do fixed point iteration and wait?”",hn30j8r,t3_r7x6t1,1638548891.0,False
r7gaav,Great channel! Love their sorting algorithms competitions.,hn0898h,t3_r7gaav,1638492421.0,False
r7gaav,I was able to follow right up till MIP\*… great video!,hn1jdoi,t3_r7gaav,1638516551.0,False
r7nt4h,"I read ""The Introduction To Algorithms"" and found they explained it pretty well. While formal, they apply it to many Algorithms to give a good intuition. You'll quickly find that algorithms with smaller asymptotic growth (like O(log(n)) as opposed to O(n)) ""do less work"" so to speak; and from there,I had to read a million examples which it provides as well.

About what you said in your rant; you certainly could compare run times of Algorithms directly.  But doing so introduces many factors not really part of the algorithm itself. Like for example the speed of the computer running the algorithm.
With asymptotic growth, we're really interested in the algorithm on an abstract level, away from all implementation.

This is also part of why we say things like ""O(3n+log(n) + 2n^2) = O(n^2)"". All the constant disappear because they don't really matter for our purpose; if the algorithm runs in O(2n^2), one can insert a processor of double the speed to combat it. But one cannot ""fight"" against asymptotic growth; no matter the speed of your computer, O(n^2) becomes arbitrarily large and at a speed where any smallere term like O(7n) becomes completely negligible if we increase n far enough. And we like our n to be big; the amount of data is ever increasing!

Went on a bit of rant here too :) But I recommend Introduction.to Algorithms; one can easily find a PDF online",hn1ael8,t3_r7nt4h,1638510475.0,False
r7nt4h,Thanks you so much! I'm always blown away by how helpful this subreddit is :),hn2vw5r,t1_hn1ael8,1638547026.0,True
r7nt4h,"You probably got answers to some of your questions in the video already. Feel free to ask further if not.

> Like if you already have a function that perfectly describes your algorithms time complexity, what's the point in simplifying it in a weird way

One of the problems is that you *don't* necessarily know the exact function, at least not with precision that would make those exact details meaningful. What would you base the exact complexity function on? Which programming language is it written in? The same algorithm that works based on the exact same idea will have its exact details, and the exact number of steps, look different in different programming languages. Or, more importantly in terms of performance, if it's compiled into machine code, which processor is that? If it's interpreted, which interpreter? The machine code is going to be different for an ARM processor than for an x86-64 one, and the number of machine instructions is going to be different. Some CPU instruction sets might provide individual instructions that do more work in a single instruction but could take longer to execute. Moreover, different instructions take a different number of cycles on different CPU models even if the instruction set is the same.

Asymptotic complexity doesn't change from a language to another, or from a CPU instruction set or a physical processor to another. It's a mathematical property of the algorithm itself. A quadratic function is always going to be quadratic regardless of the constant multipliers; a linear one is going to be linear; an exponential one is going to be exponential. That way you can talk about the algorithm itself more generally rather than its implementation in a particular language, or on a particular CPU.

Including every constant and every term when talking about an algorithm more generally would often mean giving more detail than you perhaps have grounds for. It would be a bit like reporting seven digits in your calculated result about a physical phenomenon when the original measurements are only really accurate enough to warrant two. It kind of looks more accurate but most of it is just noise.

You could argue for keeping the lower-degree terms even if the constants were discarded, of course. It might sometimes make sense to think of an algorithm's time complexity as being in the order of N^2 + N if that happens to be true rather than just N^2. And that wouldn't (probably, at least not always) even depend on the implementation, and could be part of the behaviour of the algorithm itself.

The highest-degree term starts to dominate as N grows indefinitely, which is what asymptotic analysis deals with, so the lower-degree terms are skipped. But you could reasonably argue that the lower-degree terms are part of the algorithm's behaviour, too.

> that breaks algebra rules?

This is a bit of an aside, but I don't think it really actually breaks algebra rules. The common way the notation is used perhaps does.

What for example O( n^2 ) actually denotes is the set of functions that are bounded above by n^2 * c for some constant c. The pedantically correct way would be to say that an algorithm's worst case complexity is *in* O( n^2 ), for example, rather than that it equals O( n^2 ). That's one of the things that people very often write in a way that's rigorously speaking not quite right. Once you figure it out, or if you just don't think about it that far, the sleigh of hand generally doesn't hurt understandability.",hn22x1t,t3_r7nt4h,1638532419.0,False
r7nt4h,">The pedantically correct way would be to say that an algorithm's worst case complexity is in O( n2 ), for example, rather than that it equals O( n2 ).

This helped me so much! Thank you! This subreddit is so helpful that sometimes I feel like I should be paying to ask these questions. Once again, thank you!",hn2wxil,t1_hn22x1t,1638547447.0,True
r7nt4h,"No worries. I can see how it could look confusing and dissatisfying if you look at things with a more mathematical eye and something doesn't seem to fit. The theory, or at least the parts that are part of the canon, do actually fit, though. Things just often get bent a bit and rigour gets lost in more everyday use.",hn38qbd,t1_hn2wxil,1638552101.0,False
r7fkp9,"There is no way to do this. You can never, ever verify that what is happening on an uncontrolled device is what you think it is. The only thing you can verify is that the input you are being given 'makes sense' based on your expectation of what is allowed - this is the problem things like Valve Anti-Cheat are designed to tackle.",hmzq7o8,t3_r7fkp9,1638484556.0,False
r7fkp9,My first question would be what's stopping someone from realizing this and sending over a fake hash they gained from a legitimate source?,hmz3146,t3_r7fkp9,1638475677.0,False
r7fkp9,That's what I'm asking,hmz332r,t1_hmz3146,1638475699.0,True
r7fkp9,"I suppose what I'm getting at is that you should never trust the client.

Verify all input server side.",hmz3d58,t1_hmz332r,1638475808.0,False
r7fkp9,Yeah but how,hmz4itk,t1_hmz3d58,1638476257.0,True
r7fkp9,"Depends on what you're doing. 

Realistically, a client can send you good data, bad data, invalid data, malicious data, malformed data. Data can get corrupted between the client and the server, or can be intercepted and modified.

The servers job then is to take any data and verify that the data is logical and within valid parameters. If I'm entering my name, I can put Joe or 2847 or Joe792!?5Schmo, or even nothing. At some point you have to define what a valid name is.

Think about programming Chess. The client can send the move Black Queen to E5. Sounds reasonable? Well, maybe the queen isn't even on the board anymore, or would be an illegal move, or whatever.",hmz5oy5,t1_hmz4itk,1638476704.0,False
r7fkp9,"You're best bet would be to implement OAuth and use a server to run proprietary functions. A user must then send an untampered token, which is given from and verified using a secret on, the server to authorize them on different resources.",hn1gg83,t3_r7fkp9,1638514399.0,False
r7fkp9,"To add to what everyone else said, are you trying to authenticate a user? Use a login system. Are you trying to validate that a piece of software has been paid for? Actually, use a login system. 

I've actually written a licensing library for windows applications - I also wrote the licensing server. No matter how complicated I made it, there's always going to be some person that will go that extra step of decompiling that library or application and bypassing it. You know what can't be bypassed? A system where the user has to login to a server. You still have to follow best practices and security protocols on your server, though. This is why you hear about companies being hacked and sensitive information being stolen",hn1120i,t3_r7fkp9,1638505363.0,False
r7fkp9,"Have a look at remote attestation, it's probably the closest thing to what you want to do.",hn1c6lg,t3_r7fkp9,1638511570.0,False
r6vpdh,"OP, please seek help. Plenty of people in this thread have sent you resources and people to call to try and improve your situation mentally, and the automated message I sent you contains even more resources. I understand that the world fucking sucks, especially for us queer folk, but ending your life solves nothing in the end.

The problem may go away, but so does all the good stuff. Allow what good exists in your life to carry you forward into tomorrow. I don't know you personally, but almost no one has 0 reason to live. Even at your darkest times, please try and find something, anything, to encourage you to keep living.

Do feel free to reach out in DMs, I'm usually able to respond quickly.",hmzcci9,t3_r6vpdh,1638479194.0,False
r6vpdh,The solution you looking for is a lawyer and a letter in an envelope,hmvmvzp,t3_r6vpdh,1638410769.0,False
r6vpdh,But that costs money :(,hmvnf34,t1_hmvmvzp,1638410998.0,True
r6vpdh,Like everything in life. Storing some data also costs money. There is no free lunch.,hmvnqcj,t1_hmvnf34,1638411133.0,False
r6vpdh,:(,hmvntam,t1_hmvnqcj,1638411169.0,True
r6vpdh,"My dad passed last year due to covid, and the best thing that helped us is that he had his password written up onto a paper and stored it in his desk. And he used the same password for everything, and if not, we could access his emails and reset the passwords. Facebook as it turned out has a feature to take over a profile and create a memorial for it, there are multiple ways like sending the certificate of death to them.",hmvofn2,t1_hmvntam,1638411438.0,False
r6vpdh,[removed],hmw0kbz,t1_hmvmvzp,1638416791.0,False
r6vpdh,"a) sounds like ur life isn't going so hot sorry to hear that, lmk if you need to talk or anything   
b) one way to do it is to do it kinda an old fashioned, this is how people did in the middle ages/classical period, give people trust an encryption key or algorithm, but not the password itself. Next find an obituary API one with data from your area, and spin up a web server that pings the API once per day, if your name shows up, send (via email or snail mail) the encrypted password to people you trust.",hmw9kr7,t3_r6vpdh,1638421386.0,False
r6vpdh,"Kk thx, wish anyone could help but there's nothing I can do but suffer 🙃",hmwavy8,t1_hmw9kr7,1638422110.0,True
r6vpdh,"Hey OP, I was looking through your comments and I know you're feeling suicidal, but I would urge you to try to get help before you do anything you will regret. According to your post history, you're transgender, so [The Trevor Project](http://thetrevorproject.org) might be helpful to you. However, there are other options - if you are in the UK, you can access [Childline](http://childline.org.uk), or [Samaritans](http://samaritans.org). If you are outside the UK, [here is a list](https://www.thecalmzone.net/2019/10/international-mental-health-charities/) of international mental health charities. Over here on Reddit, we have r/suicidewatch, and if you want to talk to someone else, feel free to send me a message. 

Good luck OP, and remember, there's always someone there who is ready to support you.",hmwkhqo,t3_r6vpdh,1638428090.0,False
r6vpdh,"Thx but there's nothing I can do that I'm not doing, I just have to sit here and suffer 🙃",hmwkt43,t1_hmwkhqo,1638428317.0,True
r6vpdh,"I know you think that, but I don't believe that. I felt the same way - but actually using these resources was really helpful. Have you spoken to an anonymous councillor before?",hmwkyn2,t1_hmwkt43,1638428427.0,False
r6vpdh,"Yeah but all they do is ask how I'm trying to kill myself until they just say bye, didn't really get a solution",hmwl1kc,t1_hmwkyn2,1638428484.0,True
r6vpdh,"Yup, that sounds familiar. If you haven't, I'd urge you to try again - I find that it's a sort of lucky dip. Alternatively, is there anyone you can speak to at school/home? Or even a friend?",hmwl5g6,t1_hmwl1kc,1638428562.0,False
r6vpdh,Schools would put me in grippy sock jail and I don't wanna annoy my family (bc they might do the same thing) and I have no friends 🙃,hmwn4r2,t1_hmwl5g6,1638430031.0,True
r6vpdh,"I'm sorry to hear that. Well, if you want to talk, feel free to just PM me.",hmwnd41,t1_hmwn4r2,1638430210.0,False
r6vpdh,"Please stay with us friend. You have friends here. We care. You mention school so perhaps you are still pretty young too, in which case definitely hang on to life. Things get soooo much better once you get out of school. Truly it does, I promise. If you need to talk please dm me.",hmx5ct3,t1_hmwn4r2,1638444988.0,False
r6vpdh,"Interesting project called ""horcrux"" that splits a file into encrypted fragments and only decrypts if you have all the fragment. 

https://github.com/jesseduffield/horcrux

But yes echoing others I do hope you are ok. I'm older now and a lot of stuff can seem incredibly tough when you're going through it so hope you can work through things.",hmx56oc,t3_r6vpdh,1638444863.0,False
r6vpdh,"That's really cool, thx!",hmytj6a,t1_hmx56oc,1638471972.0,True
r6vpdh,Is everything OK for you OP?,hmwdsae,t3_r6vpdh,1638423763.0,False
r6vpdh,No 😎,hmwdtoz,t1_hmwdsae,1638423785.0,True
r6vpdh,Please don't do what it sounds like you're planning. Is there somebody you can talk to? Some way I can help you?,hmwe316,t1_hmwdtoz,1638423937.0,False
r6vpdh,No and no 😎 all I can do is suffer,hmwe9ly,t1_hmwe316,1638424046.0,True
r6vpdh,"Ah,  yes, downvoting this comment will make OP feel better",hmy1ubu,t1_hmwe9ly,1638461313.0,False
r6vpdh,[removed],hmvsmqh,t3_r6vpdh,1638413248.0,False
r6vpdh,"Damn ur smart, thx",hmvy4vq,t1_hmvsmqh,1638415661.0,True
r6vpdh,Don't actually do that. Other people with your name will die before you and send your password to everyone.,hmwfm5g,t1_hmvy4vq,1638424865.0,False
r6vpdh,True :/,hmwfzl6,t1_hmwfm5g,1638425100.0,True
r6vpdh,"Change your name to something unique before dying, obviously.",hmymrgo,t1_hmwfm5g,1638469372.0,False
r6vpdh,[removed],hmwemu4,t3_r6vpdh,1638424262.0,False
r6vpdh,[removed],hmwfjei,t1_hmwemu4,1638424819.0,False
r6vpdh,[removed],hmwgig4,t1_hmwfjei,1638425429.0,False
r6vpdh,[removed],hmwhhzs,t1_hmwgig4,1638426069.0,False
r6vpdh,"This is morbid af but I will try to answer. OP hope you get better. Been there myself and thought about this a lot.

Encrypted 7Zip file with a password on it - AES or better. Depending where you are you can get a will kit very cheap (some post offices have them) you can get them endorsed by a JP (I think) and then it is binding. On your death or when you are deemed unable to look after yourself a guardian will take over your trust and the will can be released to them.

Safe travels bud and if you are somewhere toxic get the fuck out of there at all costs.",hmx8o1q,t3_r6vpdh,1638447318.0,False
r6vpdh,Your code is that important it needs to be given to ur family but u dont want to spend a single dollar. Ok,hmw1tgq,t3_r6vpdh,1638417385.0,False
r6vpdh,"Yeah I gotta spend that money on other stuff, there's a reason I'm close to death :/",hmw2c46,t1_hmw1tgq,1638417635.0,True
r6vpdh,hey i hope you are okay and if preventable i hope you find a solution. message me if you need to talk to someone.,hmwf289,t1_hmw2c46,1638424522.0,False
r6vpdh,"Thx, there's a solution but there's also a lot of stigma and bigotry, only thing to see is if I break before my life is worth living 😎",hmwff95,t1_hmwf289,1638424747.0,True
r6vpdh,well i hope you solve the solution soon. message me if you ever need someone to talk to,hmwfjo8,t1_hmwff95,1638424823.0,False
r6vpdh,"If this is the case, just give access to someone you trust now.",hmw42xn,t1_hmw2c46,1638418499.0,False
r6vpdh,I don't trust anyone lol,hmw4mvm,t1_hmw42xn,1638418781.0,True
r6vpdh,You can trust me bro,hmw8i2d,t1_hmw4mvm,1638420796.0,False
r6vpdh,If you trust google they have a function to send mails after a time period not being logged in.,hmwu74u,t3_r6vpdh,1638435739.0,False
r6vpdh,"Well first you need to establish what the parameters are, how long will you need to go without resetting it? 

But a very basic way to do this would be to put the instructions for accessing your data in an email that you schedule (can do this with gmail) to be sent at a certain time in the future. 

Then you need to come back before that date, and reset it (i.e. delete the original schedule, and make a new one at a later date). If you don't come back before that date (i.e. you died), then the email will be sent.",hmww0b6,t3_r6vpdh,1638437289.0,False
r6vpdh,Really need quorum keys for this.,hmz0okt,t3_r6vpdh,1638474733.0,False
r6vpdh,Wtf is that?,hmz2dfd,t1_hmz0okt,1638475417.0,True
r6vpdh,You see why it's needed.,hmz545d,t1_hmz2dfd,1638476483.0,False
r70ia7,"Your explanation in words is incorrect. It would be correct if the relation was T(N) = 5N + N-1, but the recurrence element T(N-1) needs to be factored in. Another way to think of it that might make more intuitive sense: 

The T(N-1) component of the recurrence relation means that for all values of n, from the first index to the Nth, this function will perform 5n work. If you expand it out, the total amount of work will be 5N + 5(N-1) + 5(N-2) + 5(N-3) ... until you get down to n=1 or the first element. 

Each of these individual terms (ex. 5N) is in O(N), and there are a total of N different terms being added together, which is where the multiplication comes in. Hope this helped!",hmwp31c,t3_r70ia7,1638431521.0,False
r70ia7,"Here's another way to work it out.

First, an important summation to know is:

1 + 2 + 3 + ... + N = N(N+1)/2

Start substituting the formula recursively, and notice the pattern.

T(N) = 5N + T(N-1)

T(N-1) = 5(N-1) + T(N-2)

T(N-2) = 5(N-2) + T(N-3)

...

T(1) = 5(1) + T(0)

T(0) = 0

Combine them all:

T(N) = 5N + 5(N-1) + 5(N-2) + ... + 5(1)

= 5(N + (N-1) + (N-2) + ... + 1)

= 5( N(N+1)/2 )

= (5/2) (N\^2 + N)

= (5/2) N\^2 + (5/2) N

= O(N\^2)",hmxmhim,t3_r70ia7,1638454811.0,False
r70ia7,"From StackOverflow:

  
T(n) = T(n-1) + n  
T(n-1) = T(n-2) + n-1  
T(n-2) = T(n-3) + n-2  
and so on you can substitute the value of T(n-1) and T(n-2) in T(n) to get a general idea of the pattern.  
T(n) = T(n-2) + n-1 + n  
T(n) = T(n-3) + n-2 + n-1 + n  
.  
.  
.  
T(n) = T(n-k) + kn - k(k-1)/2    ...(1)  
For base case:  
n - k = 1 so we can get T(1)  
=> k = n - 1  
substitute in (1)  
  T(n) = T(1) + (n-1)n - (n-1)(n-2)/2  
Which you can see is of Order n2 => O(n2).

Source: https://stackoverflow.com/questions/13674719/easy-solve-tn-tn-1n-by-iteration-method",hmwhtax,t3_r70ia7,1638426278.0,False
r71nbu,"The vt-x extension means VMs can have their CPU instructions executed by the host CPU directly (if everything is configured right). But it doesn’t address anything else, just the CPU. Both VMware and virtual box can take advantage of vt-x.",hmyooiz,t3_r71nbu,1638470104.0,False
r71nbu,Good to know! Thanks.,hmys32a,t1_hmyooiz,1638471415.0,True
r6f0od,"B

Always practice while you learn",hmspnuv,t3_r6f0od,1638367935.0,False
r6f0od,B for sure,hmsvd34,t3_r6f0od,1638370540.0,False
r6f0od,"Agree with everyone who said B. In case you haven't seen it and want some resources to learn OS development, this repo is pretty damn cool.

https://github.com/danistefanovic/build-your-own-x#build-your-own-operating-system",hmtej9c,t3_r6f0od,1638378386.0,False
r6f0od,B is the right choice,hmsyc99,t3_r6f0od,1638371831.0,False
r6f0od,Option B. You might not understand a few things in OS without knowing C.,hmt6vp2,t3_r6f0od,1638375355.0,False
r6f0od,"You should read about it (you will have to read a ton actually) and practice at the same time. Osdev Wiki should be a good starting point. Also the Tanenbaum Book on operating systems has some exercises you can do while reading iirc.

Writing your own OS is a pretty big project. I did a university course where we wrote our own OS and had a really solid foundation given to us and it was still pretty complicated.",hmu655u,t3_r6f0od,1638389063.0,False
r6f0od,In my OS class we use Operative Systems in three easy peaces and I really recommend it. Such a joyfull experience.,hmveo11,t3_r6f0od,1638407209.0,False
r6f0od,"B. It really helps you understand how these things work, also you gain practical experience in low-level programming, which you may use in future.",hmu302l,t3_r6f0od,1638387856.0,False
r6f0od,"If all your after is round robin and multiprocessing, i wouldnt read the whole book. 

Id be flexible and keep your hands dirty, also have a look into Minix 3.  Minix was designed to be an operating system for learning. I think at one point the author was reluctant to expand as it got too big to learn, but its now expanded Minix3 to be a fully fledged OS while maintaining the goal of being an OS to learn on.

[http://www.minix3.org/](http://www.minix3.org/)

&#x200B;

[https://wiki.minix3.org/doku.php?id=www:documentation:start](https://wiki.minix3.org/doku.php?id=www:documentation:start)",hmuzvr0,t3_r6f0od,1638400657.0,False
r6f0od,"I did option C. I had a summer internship where I worked on bringing up a system with an RTOS, muddled my way through it (plus help from others in the lab), then in the fall I took an Operating Systems class at school and all the pieces fell nicely into place.",hmv1wjl,t3_r6f0od,1638401602.0,False
r6f0od,"I am also trying to write an OS. And option B is the best way to proceed. You understand the concept, implement it, if it doesn't work look where u messed up, and continue

https://wiki.osdev.org/",hmwgr99,t3_r6f0od,1638425588.0,False
r6f0od,"Everyone is saying B, but in reality if you are going through a good book which contains plenty of exercises then A is certainly a good option.

What you don't want to do is just read a book without doing any sort of practical work in the meantime",hnut7yc,t3_r6f0od,1639060594.0,False
r6eo9a,"My research program is largely related to those subjects. No particular language is necessary. Python is generally useful in research because of the large number of libraries available. I personally write code in Python or Java if I'm working with a student (because often it is all they know) or C++ when working by myself (largely because over the years I've developed a large AI/ML library in C++). R and Matlab (or similar products) is also quite useful for doing analysis or data preprocessing. For example, there are good Matlab addons for working with EEG and fMRI. These products (and Python) are also useful for computer vision.

You could also start taking a look at scholarly papers on the subjects of interest, and examine how they did their work. If there is a particular specific work that interests you, then using similar technologies is likely to be useful.",hmsr80w,t3_r6eo9a,1638368673.0,False
r6eo9a,"Thanks for the suggestions, I'll start looking at some papers then !",hmsy4gm,t1_hmsr80w,1638371738.0,True
r6eo9a,"If you decide on a more specific area, then I might be able to recommend something. :)  Just reply here, and I'll drop some links if I can.",hmsznut,t1_hmsy4gm,1638372392.0,False
r6eo9a,"I'm mostly interested in human language and cognition so I guess NLP (not sure if cognition is being studied from a computational perspective).

Thanks !",hmt0mci,t1_hmsznut,1638372796.0,True
r6eo9a,"Yes, although research tends to be more niche, e.g. cognitive models of schizophrenia. Here's a good starting point for some foundation. Good luck and have fun! :)  


https://www.sciencedirect.com/science/article/abs/pii/S136466130600132X",hmt155c,t1_hmt0mci,1638373012.0,False
r6eo9a,If your using ur for research r studio will make things a lot easier for you,hmub04a,t3_r6eo9a,1638390903.0,False
r6eo9a,"You might be interested in:

https://probmods.org",hmx8uxt,t3_r6eo9a,1638447446.0,False
r6eo9a,Seems interesting ! Thanks !,hmxaz4v,t1_hmx8uxt,1638448783.0,True
r639yl,"This is a complicated question. Computer science students study these topics (usually as part of Operating Systems and Computer Architecture courses) but the deep work at industry-level is generally done by computer engineers or software engineers with significant training or experience in computer engineering.

I’ve heard good things about Nand2Tetris for CPU stuff. Any graduate-level computer architecture textbook will have a ton of information like the kind you’re looking for. We used [this book ](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1?countrycode=US&format=electronic) in my graduate program. I’ve heard the PDF is floating around online. This book explains hard drives, CPU design, RAM, etc. 

For the GPU, you should look into NVIDIA’s CUDS documentation. 

At a slightly higher level, [the OSDev wiki](https://wiki.osdev.org/Main_Page) is a great reference as well.",hmqqmgk,t3_r639yl,1638323970.0,False
r639yl,"I just want to say, as a computer science major, absolutely don't do computer science for this. 

At least where I go to school, CS is more for software stuff, and only went into details on this for a single class (Operating Systems), and even then, it wasn't super deep.

Computer Engineering (again, at least at my school), goes down into the technicalities of how everything actually works, and how to build it. 

Computer science -> Software Engineer

Computer engineer -> Designing and learning about hardware components and circuits",hmr04q6,t1_hmqqmgk,1638328219.0,False
r639yl,Good advice for sure. It was the same way at my school. :),hmt40rr,t1_hmr04q6,1638374196.0,False
r639yl,"Thanks, very helpful.",hmsz3wf,t1_hmqqmgk,1638372155.0,True
r639yl,[deleted],hmrdk4s,t3_r639yl,1638334808.0,False
r639yl,Thank you!,hmsz4nr,t1_hmrdk4s,1638372163.0,True
r639yl,"When you do this just keep in mind that this video series covers an architecture used for teaching called SAP - ""Simple As Possible"" created by Malvino for teaching, and our actual hardware doesn't operate very much at all like these model systems.

Generally you would be replacing each part of that assemblage with an entire ecosystem of related hardware. It gets very deep, very fast, and the worst part is a lot of it is considered ""secret sauce"" by manufacturers, so you won't get a complete description of what's taking place in something like a developer's reference manual.

That said, a lot of the time those developer's manuals are often your best bet for learning the things you should know about hardware such as ram or your cpu, and are distributed by manufacturers.

Often you can also find information provided by brave souls who have done the dive for you -- https://people.freebsd.org/~lstewart/articles/cpumemory.pdf",hmz0z0x,t1_hmsz4nr,1638474848.0,False
r639yl,"You're looking for this thing: https://www.bookdepository.com/The-Elements-of-Computing-Systems/9780262539807

Get your hands dirty and *really* find out how this stuff works. It's a really good hobby project.",hmrw273,t3_r639yl,1638347098.0,False
r639yl,"> Does this fall under computer science? 

Yes and no.

It did, traditionally. And a lot of Computer Science courses will teach this stuff. But these days it's considered Computer Engineering, but there's a huge cross over between the two, as CE can be seen as a subfield of CS, or a cross over of CS and Electrical Engineering.

Check out /r/ComputerEngineering for more.

Anyway, here's my stock answer for this question:

If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest. You'll learn a lot about computers and networking at the physical level)
3. If you have the time and energy, do https://www.nand2tetris.org/

There's a lot of overlap in those resources, but they get progressively more technical.

This will let you understand *what* a computer is and how a CPU, GPU, RAM, etc works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how software works etc. Arguably it will also give you the tools to design all of how hardware and software components, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. nand2tetris, for example, is specifically about that design journey. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's blinkenlight computer on plastic prototyping board"")",hms4xu6,t3_r639yl,1638354738.0,False
r639yl,Thank you!,hmsz7v3,t1_hms4xu6,1638372202.0,True
r639yl,Check out Computer Systems a Programmers Perspective by Bryant or Computer Organization and Design by Patterson and Hennessey.,hmrfts3,t3_r639yl,1638336037.0,False
r639yl,https://www.nand2tetris.org/,hmsrlp0,t3_r639yl,1638368851.0,False
r639yl,there’s an IT professional course by google. check it out,hmrz1f5,t3_r639yl,1638349605.0,False
r639yl,"If you’d like, I can send you a link to a pdf (through libgen) of the textbook we’re currently using in my computer architecture class for a more beginner-esque read",hms05gt,t3_r639yl,1638350574.0,False
r6qahx,"Having a central processing entity you were talking about (I.e. the proverbial snakes head) for the invading cyborgs is not a stretch when you are already talking about manure level cyborgs.

Also consider strides in quantum computing which means larger numbers being processed many times faster than today’s binary computers.

Or consider organic computers, operating chemically. (Proteins are essentially single purpose chemical computers that occur naturally)

Source: Have degree in film and one in CS",hmv930p,t3_r6qahx,1638404761.0,False
r6qahx,"You need to give more details of the process, because as of now, I can even think about believable bulls**t which would explain a slightly more powerful smartphone to be capable of it.",hmv08wi,t3_r6qahx,1638400825.0,False
r6ei7a,"Cisco use a program called Packet Tracer. Its pretty good. Not sure if its used by professionals or not for planning, but its good for training.",hmu3w7r,t3_r6ei7a,1638388205.0,False
r6ei7a,"I've heard about that one before, but I am curious about more proffesional stuff.",hmufh8q,t1_hmu3w7r,1638392548.0,True
r6ei7a,Check out GNS3 and Eve-NG.,hmuzz5r,t1_hmufh8q,1638400702.0,False
r6ei7a,">Eve-NG

From a little research, it seems for huge scale networks ( like cities or campuses ) OPNET is still superior to those. 

I'm definately going to try GNS3 for small projects though!",hmv8ob5,t1_hmuzz5r,1638404583.0,True
r5yufr,"I just want to emphasize, this post is not a tech support question. I already have the fix. I just really want to understand why the fix works.",hmpvj7h,t3_r5yufr,1638310288.0,True
r5yufr,"There's a **lot** of incorrect information in this thread. Pun intended.
 
Multi-threading was a thing long, long before multi-core CPUs were a thing. Setting a process priority to high and giving it a single-core affinity does **NOT** make it single threaded. This is misunderstanding the concept of process, thread, and core entirely.
 
What you are doing by changing those settings is ensuring that it is the only thing running on that 1 core, on the proviso that nothing *else* is being given above normal or higher priority, and that you never, ever saturate all of your other cores with other processes (which will override that priority). So in essence turning it into a single core machine as far as that software is concerned.
 
Dual- and higher- core processors were very, very new to the consumer space in 2006, with the vast majority of consumer PCs being single core, and the vast majority of consumer software being written in such a way that single core performance was what was prioritised. This doesn't mean that those programs were single-threaded - far from it, the majority were multi-threaded, so the GUI remains responsive while processing work happens in the background. For example, if you're using Internet Explorer, the page can still be loading while you type things into a search bar, you didn't have to wait for that page loading work to be complete before it could respond to your keyboard input.
 
The difference between multiple threads all running on the one core at the 'same time' and multiple cores, is that multiple threads (or processes) running on the same core don't actually run at the same time at all. Through various algorithms they are sliced, diced, and given effectively a time share of that chip's work, one after the other after the other, to fake things running at the same time. Only 1 instruction is every being worked on at the same time, but it could be some from Thread 1, some from Thread 2, some from Thread 3, then back to 1, and so on (the same for processes, it might do something for the game, then the notepad window in the background, then the OS, then the game again, picking up each thread each process has).
 
What multiple cores do is make it so that time sharing is now done across multiple things that can actually execute instructions simultaneously. So if you are dual-core there are 0-2 instructions being processed at any given instant, from up to 2 threads in up to 2 processes. And scaling up to quad-core with 4, etc etc. This means that so long as you never have 1 big-ass thread that wants all the processing power, a dual-core CPU can be twice as fast as a single core, quad-core is 4x faster so long as you have 4 threads that want all that power, etc.
 
The reason the game is crashing when it is running on multiple cores is because the code that it uses to manage its threads (so they don't all try to change the same data, or so that thread 1 that is waiting on some calculations from thread 2 doesn't start too early, etc) was written when there was a physical guarantee that 2 instructions on 2 threads would not happen at the same exact time. When it is on multiple cores, this guarantee disappears, and that thread management fails. There are a bunch of technical reasons this may occur, usually from attempting to do things more quickly (2006 consumer PC hardware was not fantastic by modern standards) and skipping various safety checks.
 
But it is still multi-threaded on 1 core.",hmrjr2m,t3_r5yufr,1638338323.0,False
r5yufr,"Your comment is giving the impression that single core multi threaded applications have higher guarantees than multi core multi threaded applications. It is actually not so. Any operation which is non-atomic will have same guarantees in single core as well as multi core. Similarly, any operation which is atomic will have same guarantees whether it is in single core or multi core. (Atomicity can be established either by explicit lock or atomic machine instruction). Anyway I am not convinced that multi threading is an issue. 

https://www.reddit.com/r/computerscience/comments/r5yufr/why_might_changing_process_priority_and_forcing_a/hmrj6af/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3",hmrwq31,t1_hmrjr2m,1638347660.0,False
r5yufr,"I'm not giving an impression of anything, I am outright stating that there are literal physical guarantees that apply to single-core CPUs that do not apply to multi-core CPUs. You cannot have 2 instructions simultaneously operating on a single core CPU by definition, you *can* have 
 n instructions operating simultaneously on an n-core CPU. This is a fundamental truth to reality.
 
At some point the code in the game is relying on that one instruction at a time guarantee (which is not a guarantee that gets written into an RFC, it just 'is') - the developers probably never even knew they were relying on it, because it would never have come up in testing on any single-core machine.
 
This is not the sort of thing that will come up if you are doing everything by the book - it comes up when you use hacks to make things run faster. For example (no idea if this is related to why it's happening, but it would show this behaviour) - using thread priority to ensure you don't get into a race condition to avoid the need to declare and check locks or sleep/wake. Works when those threads are on a single core and the prioritised thread is doing its thing, doesn't work when the deprioritised thread has another core it can run off onto.",hmrxrp7,t1_hmrwq31,1638348528.0,False
r5yufr,"Thank you for the lengthy explanation.

There is just one thing I want clarification on. Does this mean that any program designed for only single-core processors will try to use multiple cores to execute its threads if there are multiple cores available? I ask this because you said ""...written when there was a physical guarantee that 2 instructions on 2 threads would not happen at the same exact time. When it is on multiple cores, this guarantee disappears, and that thread management fails.""

If so, I have another question. Why do other old PC games I play work just fine on my modern PC? For example, the previous game in the series of *Shiny Days* (*School Days*) does not have similar bugs on my system.",hn63o63,t1_hmrjr2m,1638600628.0,True
r5yufr,"Most of the time it isn't an issue - going from multi-thread single-core to multi-thread multi-core won't break anything if your code is following the correct rules. Like I said, it comes up due to hacking things to break the rules in ways that don't actually break under your testing environments, but leave your code 'fragile' for changes you *didn't* anticipate. (Or I suppose it could also be bugs you put in by mistake that are masked by single-core operation). Most software written back then aren't relying on that guarantee.",hn64hom,t1_hn63o63,1638601218.0,False
r5yufr,"Well it's clearly there is something wrong with your code which handles some threads or asynchronous tasks, something could be wrong here.

And since you put it above normal, this will just suck more cpu resources which shouldn't be opt in even in complex game in normal situations. So as someone already mentioned this doesn't consider a fix. And since it freeze at specific moment try to debug it(could take some time) and find how much resources are being used.


So what programming language and frameworks do you use ? And does it also fail at that point when it's not in full screen? What if it's in big screen but not full (95%) does it work?",hmq7pot,t3_r5yufr,1638315474.0,False
r5yufr,"I'm sorry if I made this unclear but, this isn't a game I coded. Another company made this game and has not made the source code public so I can't tell what language it was written in. Shiny Days is a remake of the game Summer Days (Wikipedia article linked below)

[https://en.wikipedia.org/wiki/Summer\_Days](https://en.wikipedia.org/wiki/Summer_Days)

I was just asking this question because, as a player and as someone majoring in Computer Science, I'm just very curious as to why this workaround works, I couldn't think of anything. I tend to think a lot about why my software acts the way it does when it shouldn't act that way and how the developers could have programmed it wrong.

[https://jast.freshdesk.com/support/solutions/articles/12000055415-school-days-shiny-days-randomly-freezes-and-crashes-when-trying-to-play](https://jast.freshdesk.com/support/solutions/articles/12000055415-school-days-shiny-days-randomly-freezes-and-crashes-when-trying-to-play)

Above is a link to the company's post on this workaround. All they said is that it helps with stability on some CPUs but, I wasn't sure why.

&#x200B;

Also, no it does not crash at all in windowed mode at this point in the game. I cannot adjust the window size either. It is locked at either a specific size in the windowed mode that I can't change or full screen by the developers, so I can't put it at 95%.

By the way, just so you know Visual Novels are usually a series of cartoons/anime where you make very occasional choices that affect the story of the game. Almost like a choose your own adventure game. The scene where it freezes is when it seems to transition from the first scene of the game to the second scene of the game",hmqd0e0,t1_hmq7pot,1638317844.0,True
r5yufr,"**[Summer Days](https://en.wikipedia.org/wiki/Summer_Days)** 
 
 >Summer Days is an erotic visual novel developed by 0verflow, released on June 23, 2006, for Microsoft Windows and later ported as a DVD game and for the PlayStation Portable (PSP). It is the second installation of School Days line of series, succeeding the visual novel of the same name and preceding  Cross Days. Unlike the previous titles, that exist in the same continuity however, Summer Days is a spin-off of the original story retold from the perspective of Setsuna Kiyoura, a high school student out for summer vacation who finds herself attracted to Makoto Itou, a classmate and fellow patron of a restaurant she eventually comes to work at.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hmqd1uw,t1_hmqd0e0,1638317863.0,False
r5yufr,Nice,hmrdvj2,t1_hmqd1uw,1638334975.0,False
r5yufr," the game is released in 2006 lmao so it's very likely that's not 100% compatible for new systems, so no wondering it's buggy. The libraries in directx could be new and the game depends on older versions.

So now there are lots of possibilities.",hmqetgv,t1_hmqd0e0,1638318659.0,False
r5yufr,"That is a definite possibility. Graphics libraries tend to be finicky in older games. I wonder though why this game has such difficulties but other games around this time and games older than it doesn't have this same bug. It is unlike anything I have ever seen and I play a lot of old games, including the game earlier in the series (*School Days*).",hn63xmx,t1_hmqetgv,1638600812.0,True
r5yufr,"Based on the “fix” (which I would consider more of a workaround than a fix), it sounds like your issue is that your program cannot properly handle being run multi-threaded. I don’t know what language you’re using, but the task of ensuring your program runs properly in a multi-threaded environment is generally a nontrivial task. In terms of google-able terms, things like “deadlock” or “race condition” would be good things to read up on.",hmq6kfy,t3_r5yufr,1638314964.0,False
r5yufr,"Thank you, for those terms, I'm definitely going to do some research into those it seems interesting.

By the way, this isn't a game I programmed nor is it open source. It just has this huge bug and this workaround temporarily fixes it. As a generally curious person who is also studying Computer Science, I was just interested in why this workaround works so well.",hmqde7r,t1_hmq6kfy,1638318017.0,True
r5yufr,"Ah, got it, I assumed this was something you were working on yourself.

To give you a little bit more information, running a program single-threaded (what your workaround does) causes each line of the program to happen sequentially, as if you were running through the code by hand. Running multi-threaded allows you to run different chunks of code in parallel, giving you way more power, but it’s a perfect example of “With great power comes great responsibility.” If you don’t make sure that the multiple threads “play nice” with each other, you could run into issues.

A *race condition* is some bug that only expresses itself when the threads get scheduled in a certain way. The first example you’re normally given is

> Imagine a program that creates 2 threads, that we’ll call Ping and Pong. The Ping thread will access a global variable, lets say Score, and increment it by 1, 1000 times, and the Pong thread will access Score and decrement it by 1, also 1000 times.

The logical conclusion is that, at the end of the program, Score would be equal to 0 because we incremented it 1000 times and decremented it 1000 times. However, what’ll most likely happen is that it’ll be left with some non-zero value because the threads didn’t “play nice” with it. This is because you need to first access Score in memory, update it, then replace it in memory.

If the Ping and Pong threads grab the value of the Score at the same time, increment and decrement respectively, and then reassign to Score, whose value actually made it? Does the new value of Score reflect the increment that Ping did, or the decrement that Ping did? The answer is: it’s entirely dependent on how your OS schedules the threads. When you learn about this topic in your studies, you’ll learn about things like “semaphores” or, more generally, “locks” which are mechanism to help make sure your threads “play nice.”

Deadlocking is a concept that comes up once locking is being used. In that Ping Pong example, we can make the threads play safe by using a lock on Score, so Ping and Ping are only allowed to modify the value of Score if they currently hold the lock we associate with Score, which prevents the scenario from earlier where they overwrite each other’s work. Once your program gets more complicated and multiple locks are at play, you may end up with a situation where one thread says “I won’t free up/release/give back my lock on **X** until I get I get a lock on **Y**” while another thread says “I won’t free up/release/give back my lock on **Y** until I get I get a lock on **X**”. That battle over locks causes your program to come to a screeching halt, since neither thread will give up their lock until they get the other, which is why it’s called a deadlock.

Hope your studies go well!",hmqh70s,t1_hmqde7r,1638319730.0,False
r5yufr,"Maybe it is not properly programmed so some variables are accessed by 2 threads at the same time and caused unexpected behavior. Or like u/chrisxfire says, there is a deadlock somewhere ( one thread is waiting for another thread to leave a code section, but there's no other thread in that code section )  


Edit: The fix is kind of a quick solution that they just figured that in that scenario the problem doesn't happen. The only real fix is that they go debug their code, which could be a huge unmanagable spaghetti. In the GameDev industry there's too much spaghetti ( please don't hate me for saying this )",hmreawc,t3_r5yufr,1638335207.0,False
r5yufr,"My initial thought is that at that point of the game a process kicks off that needs to wait for another process to have completed, so when process b gets to a certain point it doesn’t have the expected app state and the app freezes. When you prioritize only one cpu then the game runs “synchronously”, and process b won’t start until the cpu is released after finishing process a",hmqqx0u,t3_r5yufr,1638324099.0,False
r5yufr,"All of these explanations are explaining away why setting cpu affinity might be solving the issue. But I am still curious why does windowed mode not result into game freeze (windowed mode game would still be multi threaded without setting cpu affinity). For that matter, why would increasing process priority be needed if you have already setup cpu affinity (game would be single threaded even if process priority is high/low)",hmrj6af,t3_r5yufr,1638337974.0,False
r5yufr,"Exclusive full screen is going to hit different code to windowed - which gives a very good indication of exactly where in the software the bug is without the inherent nature of window vs full screen needing to have much to do with it at all. Windowed being a way to avoid the code with the bug, single-core operation being a way to avoid the bug causing a crash. As for the priority it is to stop other processes from acting on that CPU core and delaying the game's threads (see my top level comment for why it's not actually single threaded though).",hmrwydi,t1_hmrj6af,1638347850.0,False
r5yufr,Yup I agree with your statement about prioritization not affecting threaded behavior. I assumed full screen and window mode of any application would be handled by operating system (some window server to be specific) and not dependent on application. Damn.,hmrx8rs,t1_hmrwydi,1638348092.0,False
r5yufr,"It's mostly handled by the graphics library being used, but will involve some OS code and some game code as well. It's not as true these days with more modern graphic environments, but back in the day exclusive full screen gave a much 'closer to the metal' access to the GPU, whereas windowed had to go via the OS. Now that isn't so true borderless windowed is becoming the standard instead (and DirectX 12 has removed exclusive full screen completely).",hmrye73,t1_hmrx8rs,1638349051.0,False
r5yufr,"Most likely, there's a bug in the code trying to use multi-threaded operations resulting in a deadlock which presents itself as a frozen or hung application as you're seeing. Setting the application to use a single core prevents it from obtaining multiple threads, thereby working around the bug.

I can't answer why changing the process's affinity to Above Normal is required. I could only speculate.",hmqlphz,t3_r5yufr,1638321751.0,False
r6bl5i,"I don't know the answer to your question, but the distances (or weights/costs/whatever) in general TSP aren't necessarily Euclidean. The edge weights can, in principle, be arbitrary, and they don't necessarily even conform to the triangle inequality, so the weight of a direct edge from A to C can even be greater than the sum of the weights from A to B and B to C.

The visualization in your image looks Euclidean, but is that what the weights actually are, or is that just what the visualization makes them look like?",hmsbnbs,t3_r6bl5i,1638359937.0,False
r6bl5i,I used the concorde executable which generate random euclidean graph,hmsygx4,t1_hmsbnbs,1638371887.0,True
r6bl5i,"The green line segments are not connected; they are not a path. If you want to compare the lengths of the two paths, just compute the length of each.",hmsq3ip,t3_r6bl5i,1638368145.0,False
r6bl5i,"Yes sorry I thought it would be clear that I kept the linking edge, I updated the picture",hmsykda,t1_hmsq3ip,1638371927.0,True
r6bl5i,"Can you elaborate on what the green path ia supposed to be? Because right now it is not really a path.

Concorde is a well established program and well known in TCS and OR communities. It is likely that you are misunderstanding something. 

Also, how large is the instance you are trying to solve?",hmsvpkf,t3_r6bl5i,1638370693.0,False
r6bl5i,"The instance was 300 random cities, I changed the picture to better show what I mean by the green path",hmsy8mp,t1_hmsvpkf,1638371787.0,True
r6bl5i,"Well,  the picture is much more clear now. 

Have you checked if the values are indeed better for your solution? Both solutions could very well have the same optimal value.

If you name your nodes from left to right: A,B,C and D.

Concorde's solution is: BC + AB + AD

Yours is: AC + AB + BD

What are these values?",hmt2gmr,t1_hmsy8mp,1638373563.0,False
r6bl5i,"Unfortunately, concorde don't let you see the adjency matrix as far as I know",hn36u8t,t1_hmt2gmr,1638551362.0,True
r6bl5i,"Can you see the coordinates of each node?

I find it strange that you cannot see the instance or save it somewhere. I have not used concorde though, I only know what it does and can be used for.",hn6lklt,t1_hn36u8t,1638615372.0,False
r5rwl8,"In theory, the only thing your scenario requires for the client to get a response is that an application be listening to the appropriate network interface and port in the server.

In practice, the application doing the listening is usually a [Reverse Proxy](https://en.wikipedia.org/wiki/Reverse_proxy) which can then route requests to the appropriate resources (applications, static files, other sockets) or respond themselves. You can have site1.com route to one application, site2.com route to another, site2.com/thing route to a third and anything else gets a 404 error. Plus it can handle a lot of other useful stuff like certification keys.

Depending on the application stack, other middleware may exist between the client and the server application. 

In Python for instance, web frameworks usually have their apps served behind a WSGI, a Web Service Gateway Interface (or an ASGI, it's asynchronous successor). One of my services, for instance, was made with Flask and hosted with gunicorn and nginx.",hmq8lww,t3_r5rwl8,1638315872.0,False
r5rwl8,"It really depends on the server and API in question. Using PHP as an example, the NGINX web server communicates to php-fpm (a separate process that handles PHP interpretation) via [tcp or Unix socket](https://www.nginx.com/resources/wiki/start/topics/examples/phpfcgi/), whereas Apache can link to a dynamic library and then do the interpretation itself.",hmphe0l,t3_r5rwl8,1638304594.0,False
r5rwl8,"In order to remove abstractions you first need to understand the abstractions - they're there to help you understand the entire system. Back before the internet when network communications were more bare boned and direct, sure, you could just go learn the entire thing together, but these days there are so many complex moving parts that you'll never understand it properly that way.
 
First go for the TCP/IP layer model (the OSI reference model will probably help with this). Understanding what packets are and how the protocols work and how each component of the network infrastructure is expected to deal with things needs to be the basis of your understanding.
 
That will let you then seek out implementations of each layer, which will do all the things you describe in different ways. Someone has already mentioned how NginX and Apache differ on parts of the Application layer side, but there will be differences in how the network interface layer works (is it wifi, ethernet, etc), then how those interface packets are reconstructed into network layer (IP) packets, then those are reconstructed into transport layer (TCP, UDP) packets, which are then reconstructed into application layer data, and in the other direction too.",hmqj889,t3_r5rwl8,1638320642.0,False
r5rwl8,"> Data Arrives to the network interface for the machine, the driver for network interface card tells the OS that it has data

Yes

> (through an interrupt?)

Maybe, maybe not, depends on how the people who build the network adapter designed it

> The OS then looks at the IP:Port# and if there is a connected socket listening with that IP and Port

Kinda. There is a lot (!!) more to network routing than this, but let's just say the OS reconstructs the TCP packet and gives that to the socket

> Is the server and the API the same process?

Maybe, maybe not. For enterprise applications you usually have a lot of microprocesses all doing different things in the backend. You have a database (or, even larger, a lot of databases all distributed over different datacenters all around the globe while guaranteeing some form of consistency), handler threads, web worker threads which do HTTP parsing, perhaps a bunch of seperate processes for different endpoints etc. Your API server might be a data center with many different machines, all being coordinated and working together.

For very small application you can have a single monolith.

> and the dotnet framework (or any API framework) is baking in the socket code and acting as a server as well?

Again, depends on the framework. Simpler frameworks (like flask) have a build-in web server, more complicated ones may outsource this to nginx or some dedicated web server.

> is it something I'm completely missing?

Not really, everything you mentioned is being done in practice.",hmqfnzp,t3_r5rwl8,1638319044.0,False
r5w1xe,"There's two things going on here. Your course work for virtualization and doing virtualization from a practical point of view. For the first part, do what they want you to do and pass the exam. Even if it's completely useless shit.

The second part, install Virtualbox or another hypervisor, get on youtube and watch videos on the subject and just repeat on your own PC or Laptop. It'll become much clearer. At some point it'll just click and you'll wonder why you thought it would be difficult to learn.

Look into using containers such as Docker and LXC too, it's sort of lightweight virtualization.",hmrq182,t3_r5w1xe,1638342498.0,False
r5w1xe,"Thanks for the insight. 

I have been working with Docker for a while and that is what drew me to this course. The practical part is fairly easy for me (maybe because I have never dug deep into the lower levels and fundamental knowledge). I should have been clearer in the original post, all I want is advice for the coursework part :D Like ISA virtualization, binary translation, dynamic opt, HLLVM, ...",hmuygd4,t1_hmrq182,1638400002.0,True
r54to6,I'm reading a good book called Computer Networking: A Top Down Approach. I haven't gotten very far yet but so far it is easy to follow. I'm not sure if that is what you are looking for though. I have no experience in this area.,hml0u5s,t3_r54to6,1638222677.0,False
r54to6,"I second this. This was the textbook I used for my computer networks class. Honestly did not read too much of it, the lecture slides were sufficient (the authors of the textbook created the slides). Still learned lots tho. There’s also tons of reviews questions, practice problems, and assignments. There’s even a website with interactive problems. I feel like it’s got everything you could ask for.",hmlbj3y,t1_hml0u5s,1638227168.0,False
r54to6,"I also used this textbook for two of my classes, but instead of the slides I ended up basically reading the entire thing on my own because I had a timing conflict with my lectures.

Network topics have a tendency to turn into a bunch of alphabet soup for me with all the protocols and their overlapping acronyms, but there’s a ton of visuals in there (and also included in the corresponding slides, I believe) that really helped with recalling everything later on. Also, the way the information is laid out makes it so you form a progressively bigger picture throughout the semester. Was probably the most digestible textbook I had for those reasons.",hmn36o6,t1_hmlbj3y,1638260477.0,False
r54to6,"When you get to a concept or term you don't recognize, if the text you're reading doesn't explain it, just stop and check another resource.

For example:

[https://techterms.com/definition/protocol](https://techterms.com/definition/protocol)

[https://en.wikipedia.org/wiki/Communication\_protocol](https://en.wikipedia.org/wiki/Communication_protocol)

then pick up the original text where you left off.

I really doubt you'll find a complete-enough tome that explains \*everything\* in sufficient detail, the body of information is just too big.",hmkrfg4,t3_r54to6,1638218929.0,False
r54to6,"Computer Networks - Book by Andrew S. Tanenbaum.
This one helped me in similar situation.",hmnpref,t3_r54to6,1638277921.0,False
r54to6,"Came here looking for the comment suggesting the Tanenbaum, cheers!",hmnr1nj,t1_hmnpref,1638278629.0,False
r54to6,"Start with the CCNA/CCENT, JNCIA or equivalent in today's books. Then as /u/berrmal64 said, look up the unknowns or acronym soup in wikipedia, take a note if you want to come back and dig to learn more, and then move forward.

What I'm telling you as someone who's been in the networking field professionally for 20+ years is that you cannot decompose this field into 'first principles' as it is too vast. i would suggest to you, instead, to be task oriented. Learn what you need about the field to complete the tasks at hand and then learn more in the next research phase of the next task.

Get a starter cert for a networking vendor, but be aware that all vendor certs are biased by design towards their implementation.

I mean, you could read a selection of a few hundred of the first 2000 IETF RFCs, and you might be successful in learning something, but a good starter book and a few online references is a much better place for a beginner than any deep dive.

To start as an entry level network tech, effectively, you need knowledge an inch deep and a mile wide, not the reciprocal. 

I approach all my tasks this way, and spend a bit of time researching deeper on topics I am not completely familiar with today, as a support engineer for a vendor that makes networking equipment even after 20+ years of working in the business.",hmo26mj,t3_r54to6,1638284098.0,False
r54to6,thank you sir appreciate it!,hmoadq7,t1_hmo26mj,1638287573.0,True
r54to6,"I’ve been teaching myself using professormesser.com. His Network+ study materials are a good starting point for a total beginner regardless if you plan on going for the cert or not. Of course it won’t teach you everything there is to know since it’s geared towards the CompTIA exam objectives, but you can move on towards more in-depth material once you have those basics.",hmkt5vc,t3_r54to6,1638219609.0,False
r54to6,If you want much basic and theoretical concepts go and watch ravindrababu ravula computer networks in yt,hmm7pfu,t3_r54to6,1638241710.0,False
r4y6sf,Amazing result! Should this not be on the reddit main page?,hmmf75m,t3_r4y6sf,1638245185.0,False
r4y6sf,Hell yeah! Randomness can catch these hands.,hmma1gf,t3_r4y6sf,1638242769.0,False
r4y6sf,"Yeah I grokked maybe 70% of that. 😅
Seems super cool, am going to give it another read later and try to digest it better.


Thanks for sharing 👍",hmmwtkm,t3_r4y6sf,1638255538.0,False
r4v7w6,"Disclaimer: not a C# programmer but I do know a thing or two about memory.

I'm not 100% sure what you mean by memory stack, as you've mentioned the same theoretical structure used in two different places: the stack data structure. I'm going to assume you wanna know about the ""stack memory"" used by programs during function calls.

First off: there are different types of memory at your disposal when you write programs:

1. Text : this is where the source code of your program resides. If someone (or something) tries to change it, you'll get an error as it's read-only
2. Data and BSS : these two are used by variables and objects that live during the whole lifetime of  your program. So global and static variables are usually stored there (at least in C and C++)
3. Heap : this is dynamic memory which you can request from your OS for your program. Of course, if there's not enough free memory, you'll get some indication of failure (nullptr in C++ and maybe an exception in higher-level languages)
4. Stack : I assume this is what you wanna know about. Whenever you call a function, your compiler has to allocate some space to be used by the parameters and local variables of that function. Also, you want to store a pointer to place from which you called the function, so that you can return to that place after the function finishes executing. Now, this ""stack"" is fundamentally no different than the user-defined stack data structures created by programmers in languages like C and C++ (some higher-level languages provide stack data structures as part of their standard libraries). The practical difference is just that the memory stack happens to be a useful data structure when dealing with function calls and is used by the compiler when it emits assembly code (this is what C/C++ compilers do, not so sure about C#). You can see PUSH and POP instructions if you disassemble an executable. It's what computer scientists call a LIFO structure (last element that goes in is the first one to come out), which makes perfect sense in the context of function calls since you want to free up space (i.e. pop) used by the variables of the last function you called so that the next function has a fresh memory space to work with, while you also want to make sure to keep the memory used by the functions higher up the call chain untouched.

Why is this important? Couple of important considerations:

1. Stack tends to be faster. I say tends to because I've worked in some environments where the difference in performance between stack and heap is negligible. That being said, in many environments stack seems to be much faster than heap. So prefer it whenever you can.
2. It tends to be more memory-constrained than heap. You can't just shove gigabytes of data into it and expect things to go smoothly. Sometimes it can happen even when there is no obvious indication that you're shoving a lot of data into it. For example, remember when I said the location from which you called the function is saved into the stack? Well, in a more popular scenario when a programmer forgets to put an exit condition in a recursive function (google it if you don't know, it's basically a function ending up calling itself infinite amount of times) then you will get a ""stack overflow"" since all those saved pointers end up filling the stack. You can overflow stack much quicker in embedded systems with limited resources, than on a desktop gaming PC.

That's what I can think of now. I hope more experienced programmers will elaborate more on this issue and maybe even find a fault in my own answer",hmj2d2w,t3_r4v7w6,1638192944.0,False
r4v7w6,"One more thing about heap allocations: They can cause memory fragmentation if you use them a lot in your code. Probably not that big of a problem if you don't care about performance, but still, take care not to litter your code with it too many times.",hmj329n,t1_hmj2d2w,1638193333.0,False
r4v7w6,"The only fault I would bring up is that program text is not source code, it is the machine code of your program, produced by translating source code to machine code. The ""code"" bit is an overloaded term, but in this case refers to executable instructions.",hmjr3w2,t1_hmj2d2w,1638204357.0,False
r4v7w6,Oops! Should've clarified that!,hmjrd3b,t1_hmjr3w2,1638204461.0,False
r4v7w6,Well explained sir. The stack OP is familiar with in C# is a data structure. The memory stack is a an application of that data structure in the way compilers deal with particular type of memory.,hmkkjbo,t1_hmjrd3b,1638216147.0,False
r4v7w6,"Sorry for the late reply was trying to wrap my head around Big O notation last night 🤦‍♂️ but this is great thank you, this makes it a lot clearer to me and I understand it a lot more now",hmnmpug,t1_hmj2d2w,1638276142.0,True
r4v7w6,"Stack is an overloaded term. It's both a general data structure and also a specific stack is used as you mention as a part of what is referred to as an ABI. The ABI related usage is the one that is used for function calls. Generally its referred to as the ""call stack"" of a program. Your operating system is also managing and using a stack of this type. For more information on the data structure check the C# documentation related to the Stack class in System.Collections. For more information on the ABI related usage of the term, search for information related to what is called the ""calling conventions"" used by the ABI for your computer's architecture.",hmjqbt9,t3_r4v7w6,1638204039.0,False
r4v7w6,"Basically the stack is where local function variables are stored in most programming languages.   
To be clear

    val x = 5 // not a function variable
    def myFunction(y /* not a function variable */) = {
      val z = 10 // function variable
      return x + y + z
    }

So if you had some code like 

    def myFunction2(n) = {
      val pi = 3.14
      // breakpoint 1
      return n + pi
    }
    def myFunction1(n) = {
      val x = 5
      val y = myFunction2(n)
      val z = x + y
      // breakpoint 2
      return z
    }
    myFunction1(1)

then at `breakpoint1` your stack would look like

    3.14
    5

then at `breakpoint2` it would look like

    9.14
    4.14
    5

notice that at `breakpoint2` our `3.14` is gone. because once `myFunction2` returned, then the stack pointer (pointer to the top of the stack) was moved back to the location it was at before the function was called. thus `3.14` was overwritten with `y` and `z`.

hopefully this makes sense to you, if not I'm sure there are lots of great intro CS / unmanaged language classes online that can explain i more depth.

ps. note we never actually ""pop"" the memory stack, we just move the memory pointer back to where it was before a function was called. when we want to reference a value on the stack it's done by accessing the memory address of the value directly.",hmko2sv,t3_r4v7w6,1638217585.0,False
r4v7w6,"Others' answers give good context. This may be a bit more direct:

The ""call stack"" is basically like a C# `Stack` of ""stack frames"". The OS sets aside memory for the call stack before a program starts. **A stack frame contains all the information needed to return from a function call and give back control to the caller.** This information is usually just a ""return address"", which is a pointer to the instruction that should be executed after returning.

A stack frame can also store local variables and function arguments. This is different from a C# `Stack`, which only stores one type of value. The compiler will keep track of the size of each function's stack frame, so the return address can still be found by popping that many values off the stack.

Source code is compiled into a list of simple instructions that the CPU can execute. These instructions operate on tiny chunks of memory inside the CPU called registers. Some instructions are loads or stores, which move data between registers and RAM. Some instructions are arithmetic, which just manipulate the contents of registers. Some instructions also do both.

**There are usually dedicated registers for the ""stack pointer"", which is the address of the top of the call stack, and the ""instruction pointer"", which is the address of the next instruction to execute.** The instructions that operate on these dedicated registers are the answer to your question. 

Most CPUs have instructions dedicated to manipulating the stack pointer. A `PUSH %reg` instruction will increment the stack pointer and store the contents of the general-purpose register `%reg` at that address. A `POP %reg` instruction does the reverse. The compiler can use `PUSH` and `POP` instructions in pairs, or manipulate the stack pointer directly, to store as many local variables as each function needs without losing track of the return address.

Most instructions implicitly increment the instruction pointer. A `JUMP` instruction instead explicitly overwrites the instruction pointer, so control transfers to a new location. **A `CALL` instruction pushes the instruction pointer onto the stack before jumping, like a combination of `JUMP label` and `PUSH %ip`. A `RET` instruction does the opposite of `CALL`, which is essentially `POP %ip`. This is how function calls and returns use the stack.**",hmm26uf,t3_r4v7w6,1638239256.0,False
r4xo16,"Being more specific with parameters would make it a different method. One thing I think you are confusing is the difference between accepting and requiring.

Your code has the `Animal.speak` method which requires a `LoudSpeaker` parameter and returns a string. This means any subtype of `Animal` must have a method that *accepts* a `LoudSpeaker` and returns a string (or string subtype?).

Being more specific (covariant) with the return type doesn't violate the method contract. If you returned a subtype of string (is that possible with string?) your are still by definition returning a string.

Being more specific with the parameters does violate the method contract. If the `Cat.speak` method *required* a `ScreamingSpeaker` (i.e., the method definition specifies it) then it can no longer *accept* a `LoudSpeaker`. Keep in mind that the  `speak` method can still *accept* a `ScreamingSpeaker` when you call it. If you wanted `Cat` to only take `ScreamingSpeaker` you could detect the type of the passed argument and throw an exception (or return null or whatever) if it is not a `ScreamingSpeaker` but since the method signature did not change that is **not** being covariant with the parameters.

Being less specific (contravariant) with parameters is allowed because it still fulfills the method contract. If `Cat` can take (requires) a more general `Speaker` in it's `speak` method, then it still *accepts* a `LoudSpeaker`.

I'm not sure what you mean on line 37 with ""According to LSP I shouldn't pass in LoudSpeaker here?"" because that is wrong. The `Animal.speak` method *requires* a `LoudSpeaker` which means it can also *accept* a `ScreamingSpeaker`.

Edit: Used OP's example code and fixed mixing up terms. This is why I need coffee in the morning.",hmjmfsd,t3_r4xo16,1638202418.0,False
r4xo16,"> If the Cat.speak method required a ScreamingSpeaker (i.e., the method definition specifies it) then it can no longer accept a LoudSpeaker. Keep in mind that the speak method can still accept a ScreamingSpeaker when you call it.

I understand I can pass in anything at runtime. What I'm saying is that Liskovs Substitution Principle dictates (as I understand it) that if the parent class's method (Animal.speak) requires a Loudspeaker, then the child class's method (Cat.speak) can only require a Loudspeaker or it's parent Speaker as a parameter. Regardless of what can be passed in at runtime.

So the methods themselves, because they are in a subtype relationship, must be *contravariant*.

However, any other function, in order to adhere to LSP must be *covariant* in their parameters.

There still seems to me to be a disconnect here. I'm sure I'm still misunderstanding something, I don't think I've found a formal flaw or anything like that but on it's surface it appears that LSP has an implicit contradiction.

Edit: Think about it like this.

Forget about the classes. I'm looking at all the functions in that code whether they are methods or not, and I'm wondering why some functions have to be covariant in their inputs and adhere to LSP and why some (subtype methods) have to be contravariant in order to adhere to LSP.

I'm trying to tease out what implicit assumption I'm making to make it seem like that.

Words are hard, sorry. lol.",hmk55ut,t1_hmjmfsd,1638210001.0,True
r4xo16,"Don't doubt yourself too much. It's a really good sign that you are questioning principles without immediately thinking you've found something wrong with it.

&#x200B;

>I understand I can pass in anything at runtime. What I'm saying is that Liskovs Substitution Principle dictates (as I understand it) that if the parent class's method (Animal.speak) requires a Loudspeaker, then the child class's method (Cat.speak) can only require a Loudspeaker or it's parent Speaker as a parameter. Regardless of what can be passed in at runtime.  
>  
>So the methods themselves, because they are in a subtype relationship, must be contravariant.

This is nearly correct. The *parameters* of the method *may* be contravariant. The `Cat.speak` method can require a `LoudSpeaker` or any parent-type in the method definition. This allows the method to still accept a `LoudSpeaker` but it may also accept a more general (parent) type.

&#x200B;

>However, any other function, in order to adhere to LSP must be covariant in their parameters.

Not quite sure what you mean here. Are you referring to the parameters that are passed in when the function is called? Taking the function with the definition `speak(LoudSpeaker $speaker)` and calling it with `speak(new ScreamingSpeaker())` is not covariance it's just polymorphism. A `ScreamingSpeaker` *is* a `LoudSpeaker` so it can be taken as a parameter. If `Cat` overrides it with `speak(Speaker $speaker)` (being contravariant), then you can still call it with `speak(new ScreamingSpeaker())` but you could also do `speak(new Speaker())` or `speak(new QuietSpeaker())` where `QuietSpeaker` exteneds `Speaker`.

To help clear a little confusion, the types that a function takes in according to it's definition are generally referred to as **parameters**. The actual values that are passed in are generally referred to as **arguments**.",hmk8tvp,t1_hmk55ut,1638211457.0,False
r4xo16,"> This is nearly correct. The parameters of the method *may* be contravariant.

Wikipedia and everywhere else states that in order to adhere to LSP, it **must** be contravariant. Meaning that you must allow for either what the parent class expects as a parameter, or something more generic than that.

[Contravariance of method parameter types in the subtype.](https://en.wikipedia.org/wiki/Liskov_substitution_principle)",hml4xnj,t1_hmk8tvp,1638224372.0,True
r4xo16,"That is what I meant by ""may"". It either is the same as the parent class, or it is contravariant and it takes a more generic type.  
  
I suppose some may say that it is contravariant even if it uses the same type but that seems odd to me.",hmlfa88,t1_hml4xnj,1638228826.0,False
r4xo16,"But one thing you *can't* do is pass in a more specific type in the subclass's methods. I mean you *could* but you'd be in violation of LSP. And therein lies the issue for me. In one sense, LSP is saying that for subclassing, you have one rule in the method's parameters (contravariance), but in other instances it calls for covariance in function signatures. I'm not sure how to square that.",hmlhk3i,t1_hmlfa88,1638229844.0,True
r4xo16,"Passing in a more specific type *is* LSP, not a violation of it.

    signatures:
    Animal.speak(LoudSpeaker) // Base signature
    Cat.speak(Speaker) // Contravariance, no LSP violation
    
    function calls:
    animal.speak(new LoudSpeaker()) // Normal call
    animal.speak(new ScreamingSpeaker()) // ScreamingSpeaker is a subtype of LoudSpeaker, no LSP violation
    cat.speak(new Speaker()) // Allowed because of the contravariance in the signature
    cat.speak(new LoudSpeaker()) // LoudSpeaker is a subtype of Speaker, no LSP violation
    cat.speak(new ScreamingSpeaker()) // ScreamingSpeaker is also a subtype of Speaker, no LSP violation",hmlnuhy,t1_hmlhk3i,1638232730.0,False
r4xo16,"> Passing in a more specific type is LSP, not a violation of it.

I agree. That's how I've always understood it.

However, when it comes to method parameters of a subtype, LSP is pretty explicit that you *should NOT* pass in a more specific subtype than what is required. You can however pass a more generic parent type.

In Wikipedia and everywhere else I've read it says explicitly that LSP requires subtype method parameters to be **contravariant** in their input.

Do you not agree that to be **contravariant** in your input means that you cannot pass a more specific subtype in? I think that's where we're both getting stuck.",hmlvgif,t1_hmlnuhy,1638236200.0,True
r4xo16,"It does **not** mean that you cannot pass a more specific variant. It means that you cannot **require** (in the method definition) a more specific type.

You have to think of the *use* of a method and the *definition* of a method separately.

>However, when it comes to method parameters of a subtype, LSP is pretty explicit that you should NOT pass in a more specific subtype than what is required. You can however pass a more generic parent type.

LSP is explicit that the method must support *at least* the specific type, but it may support a more general type.

It looks like you are misusing the term ""passing in"". Passing is **only** referring to the *use* of a function. Accepting/requiring/taking refers to the *definition* of a function. Contravariance is referring to what a function can **take** as an input, not what you actually **pass** to the function.",hmlwtt3,t1_hmlvgif,1638236852.0,False
r4xo16,"> Contravariance is referring to what a function can take as an input, not what you actually pass to the function.

Right. As I've said before I understand that you can pass in anything you want.

I think we're getting hung up on some of these terms.

How about this. I can just reformulate the question. Why does LSP *require* a subtype's method parameters to be *contravariant*?

And then, why does LSP require that any other function that accepts a type be *covariant*? And why are those two seemingly different?

Also, you are the most patient person in the universe and I offer my many thanks. :)",hmlxnkh,t1_hmlwtt3,1638237219.0,True
r408nu,Oh damn them feels. I’m a CS major and all that tawk about how you’ll get rich working for Big Tech or Silicon Valley really made the field seem off putting. I wish that they taught CS (and other STEM fields) like how they teach philosophy.,hmi3lln,t3_r408nu,1638166818.0,False
r408nu,Yes yes,hmi5qz6,t1_hmi3lln,1638168307.0,False
r3rold,"Some of the simplest solutions include hosting the resume on a GitHub repository, or hosting it on a website linked to from the GitHub profile. If you're set on a cryptographic solution, though, some possibilities include:

* You include your public key on your GitHub profile, and a signed message in your resume, or vice-versa

* You could use some kind of [identity proofs, like what KeyBase does](https://book.keybase.io/guides/proof-integration-guide). This can prove that the same human controls a website, GitHub profile, Twitter account, etc.

Seems over-complicated to me, though, I'd stick with ""my CV is on my website, which is linked to from GitHub, and my CV includes links to both my website and GitHub profile.""",hmcfxo6,t3_r3rold,1638062627.0,False
r3rold,That seems to be a concise explanation. Thank you.,hmcjczr,t1_hmcfxo6,1638064246.0,True
r3rold,The employer could simply ask the interviewee to login and show them that they own the github account? But some type of cryptographic signature seems like a good idea.,hmdhrj4,t3_r3rold,1638083159.0,False
r3rold,"In the case where you want only one way verification and *don't* want to overtly tie your github username to your actual name, the company can just ask the person to add something specific to their bio, comment on an issue on a company repo, etc.",hmdc5ac,t3_r3rold,1638079430.0,False
r3rold,"I have a plain text file with URIs to my profiles on social media etc (GitHub, Twitter, Facebook…) signed with my PGP key and hosted on my website.",hmdyvz1,t3_r3rold,1638096439.0,False
r3rold,"This is pretty much the same problem certificate providers have to solve. They have to check is someone is the owner of a website.

Usually the prove is done by creating a specific file with a content given by the certificate provider on the website. An other way is to set a HTTP header with a value given by the certificate provider. However, the idea is always the same: Provider gives you a secret and the owner has to place it on the website.

To not have to place a secret on the github page for everyone that want to verify the owner. The owner could place a public key. The verifier can than send a challange (random string) to the owner, the owner can than sign the challange with the private key and send it back. If the verifier can verify the signature with the public key from the repository it ownership is proven.",hmew0db,t3_r3rold,1638115902.0,False
r3cbfk,maybe also ask also on physics/math related subs/forums,hmacs8x,t3_r3cbfk,1638029945.0,False
r3cbfk,You would probably have better luck in an engineering or physics sub.,hmbz32e,t3_r3cbfk,1638054905.0,False
r3cbfk,"[https://www.youtube.com/watch?v=qsYE1wMEMPA](https://www.youtube.com/watch?v=qsYE1wMEMPA)

[https://www.youtube.com/watch?v=uG2mPez44eY](https://www.youtube.com/watch?v=uG2mPez44eY)",hm9uvqv,t3_r3cbfk,1638021044.0,False
r3cbfk,"Thanks for advice, sadly i've ready watched them :D",hma4nxr,t1_hm9uvqv,1638026187.0,True
r3cbfk,Look at some of the MOOC sites!,hmdg911,t3_r3cbfk,1638082137.0,False
r3exb5,[deleted],hma1zlm,t3_r3exb5,1638024879.0,False
r3exb5,16 * 65536 = 1048576 bits? which is not 128 kilobits?,hma30v2,t1_hma1zlm,1638025398.0,True
r3exb5,"There are 2^16 (65536) locations, each storing 16 bits (2 bytes, also called a *word*).  65536x2 (bytes per location) is 128kb.

It's _addressable_ with a 16-bit number, meaning one 16 bit number can represent any of the locations.",hma5q1s,t3_r3exb5,1638026694.0,False
r3exb5,"Is this correct? I might be having a stroke, because for some reason I don't get this at all, I might've understood something completely wrong because:
65536x2 = 131072 bytes? which is => approx 1048kb (kilobits).

So I've 65,536 locations
If I store 16 bit worth of something into every single location available it should be exactly 1048576 bits. 

This is my thought process and for some reason I don't get this at all. Be it 16 bits or 2 bytes, essentially it should be the same? As in 1 byte is 8 bits and 2 bytes is 16 bits, hence your explanation seems exactly the same as mine but using bytes?",hma88bc,t1_hma5q1s,1638027863.0,True
r3exb5,"Yeah I don't think any of that is wrong (except kb is kilobytes, not kilobits).  There are 2^16 slots, each slot can store a 16-bit number, so that's 1048576 bits.  Kb here is kilobytes, which is the standard for storage (always in bytes, not bits).",hma8z15,t1_hma88bc,1638028205.0,False
r3exb5,"Is the material wrong? Because it says 
> This means it can store a total of only 128kb

As in kilobits, not in kilobytes. If the material would have kB in it, I would understand it, but no matter how much I wrestle with the bits and bytes here I cannot see how the memory is only 128 kiloBITS in size.",hma9r1w,t1_hma8z15,1638028559.0,True
r3exb5,"Nobody measures storage in bits.  Storage is in gigabytes, megabytes, kilobytes, or bytes.  KB is kilobytes.  The material is correct.  It's kilobytes though, not kilobits.  You can't rely on seeing KB instead of kb.  You may often see gb, mb, or kb...they're all in bytes.",hma9wrg,t1_hma9r1w,1638028634.0,False
r3exb5,"Technically, OP is correct and the material seems to be a bit sloppy about the unit symbols. Traditionally (and also according to an [IEEE standard](https://en.wikipedia.org/wiki/IEEE_1541-2002)), an uppercase B is used for bytes and a lowercase b is for bits. It's technically wrong to use ""kb"" for kilobytes.

Of course nobody actually bothers to get it correct nowadays, so you're right that you need to kind of figure it out from the context rather than relying on notation actually being correct. However, considering that the page is about rather low-level stuff that involves both bits and bytes, one might reasonably expect the author to actually clearly distinguish between the two. OP's confusion is understandable since they apparently learned the unit symbols right at some point, although it's not much of a stretch to figure out the author of the documentation just didn't bother to get the units right.",hmegkd5,t1_hma9wrg,1638108473.0,False
r3exb5,"Technically yes, but lots of people don't bother to get the unit symbols for bits and bytes right.",hmegtql,t1_hma9r1w,1638108611.0,False
r3exb5,"Yeah, I wasn't truly aware of that to be honest and I'm ok with it. I genuinely was just very confused because I literally googled the meaning of ""kb"" and realized it meant kilobits and rest is history. But yeah, I thought that material being rather low-level would've super precise details and didn't consider the possibility of the context at all.   


You live and learn. Now I know a ""bit"" more! :)",hmgj1p9,t1_hmegtql,1638139827.0,True
r3exb5,"It’s a common misconception that 128 kb = 128,000 bytes. It’s actually 2^17 = 131,072. This is the same concept that 1 kb does not equal 1,000 bytes, but rather 1024.",hmdg805,t3_r3exb5,1638082118.0,False
r39ah9,"Fetching images from RAM and context switching is expensive.  To do that many times in a single frame while rendering slows everything down.

Atlasing allows you to pack textures into a single file, normally minimizing the amount of empty space around sprites (thusly size required to store images in memory), and store metadata which is useful in the logic and rendering part of your code.  

The GPU can now use one or two textures instead of switching context between multiple textures residing in memory.  It is hard to understate how big of an improvement texture atlases can be to your rendering times (and it is worth doing because it is simple to do).  A good experiment would be to build two like projects rendering a hundred 2D sprites, stationary, but one utilizes an atlas and the other has every sprite in a separate file.  You will see the importance then.

One of the techniques I like, since I don't like having all of my images in one file, is to build atlases on the fly in memory during scene changes in the game.  I'll have all relevant information for the graphics at that point.  Some images that may not repeat a lot I usually won't put into the atlas because it doesn't make a great difference, but for the most part I always have character textures, equipment textures, and scenery textures in there.  

There is a plethora of really in depth information about this and context switching out there if you want more detailed information.",hmambok,t3_r39ah9,1638034105.0,False
r39ah9,"You are ABSOLUTELY right!

One big texture or 20 small textures, should be the same, problem is old OpenGL is fucked.

Basically the trick is that texture unit filling is expensive, for some FUCKED reason they didn't think games would need many textures, and many of us are still paying for this shortsightedness.

The issue is larger actually, GPUs are great once data is resident but their ability to stream data in and out is very unsupportive.

Overall writing games is so hard that packing textures is never that bad but i totally agree that we should talk openly about how this is not something we (as game devs) should really be dealing with.",hmdkt68,t3_r39ah9,1638085253.0,False
r39ah9,But isn't opengl an ever updating standard? Shouldn't some new version fix this?,hmdohu5,t1_hmdkt68,1638088085.0,True
r39ah9,New versions do fix it but they are less compatible,hmgjl2u,t1_hmdohu5,1638140052.0,False
r2ye5m,You will get completely different answers based on WHAT is being built. A static we site requires a different stack from a web service than a video game than a video editing program than a social media app. Tech are tools and a good computer scientist simply uses the tools for the jobs they were built for.,hm7sdr2,t3_r2ye5m,1637972552.0,False
r2ye5m,"If I was being pedantic (and of course I am because I’m a Computer Scientist) I would point out that this is not something you can have an opinion on. 

It’s a piece of factual information that you may or may not be able to find an accurate answer to. As someone else said it’s almost certainly HTML/JS by the numbers.",hm8740j,t3_r2ye5m,1637980014.0,False
r2ye5m,Go to job boards and sort openings by language. See which language has most openings.,hm8w63k,t3_r2ye5m,1637994192.0,False
r2ye5m,"In sheer volume? HTML, CSS, JavaScript hands down.",hm85z8p,t3_r2ye5m,1637979442.0,False
r2ye5m,"That's not a stack, that's just the front end...",hm8jni4,t1_hm85z8p,1637986622.0,False
r2ye5m,"A Stack is just a complete set of systems that can run independently in its entirety without the need for any extra jazz. Millions of websites are written without the need of a backed that do more than just display text. JavaScript is Turing Complete, so you can put all of the work client-side without issue. Yes, you're right, that's just the front end, but that doesn't exclude it from being a Stack.",hm8rlvz,t1_hm8jni4,1637991256.0,False
r2ye5m,"There is no website in the world that is written without the need for a backend. Some way and some how you have to get that website to the end user, even if that way is walking up to them and handing them a USB stick. That is what a 'stack' is, it includes the web server that is serving those pages even if once they are served they don't need any more info to or from the server.",hm8tbvk,t1_hm8rlvz,1637992321.0,False
r2ye5m,"Saying you still have to host the site doesn't really go to the OP's original question of ""What's the most popular stack"" because the backend is totally irrelevant for a front end stack. There are millions of websites that don't care how you host them, S3 buckets, GitHub Pages, whatever, so saying that a Stack is ""HTML, CSS, JavaScript, and some kind of hosting solution"" is pointless, just like it's pointless to say that you technically need a browser to interact with websites that have a UI.",hm8x7y6,t1_hm8tbvk,1637994896.0,False
r2ye5m,"Except that choice is literally what a stack is. Yes, you can swap out a SQLServer database for MariaDB for Postgres and the site won't give much of a shit, but when someone says 'what stack are you using' your choice of RDBMS is a part of that. Words mean things whether you want them to or not.",hm903ft,t1_hm8x7y6,1637996915.0,False
r2ye5m,"So you would have felt better if I said ""HTML, CSS, JavaScript, Static Hosting""? Fine then, that's my answer if it'll make you feel better. You don't need a database to have an application.

How would you answer OP's question? By rejecting its phrasing and saying it's a bad question?",hm93her,t1_hm903ft,1637999522.0,False
r2ye5m,"It's not a bad question at all, you just answered in a way that is misleading to a newbie who wouldn't know any better. Static hosting is in no way the most common way an application is served, so your answer is wrong in any event. Java EE is the most common in traditional commercial settings, LAMP/MERN elsewhere.",hm94h84,t1_hm93her,1638000311.0,False
r2ye5m,"For backend, Java and sql",hm8rjqi,t3_r2ye5m,1637991219.0,False
r2ye5m,I thought PHP was still more popular,hm9i502,t1_hm8rjqi,1638011983.0,False
r2ye5m,"Really, I didn't know java is still widely used.",hm9d1ll,t1_hm8rjqi,1638007557.0,True
r2ye5m,"In enterprise backend systems, Java is by far the dominant language still. Other languages like Golang are getting more popular for sure, but there's still a lot of Java code out there and will continue to be so for the foreseeable future",hm9hici,t1_hm9d1ll,1638011433.0,False
r2ye5m,"Yeah I've worked at a few large companies. Amazon for example is almost entirely Java. Tried and true, to deliver fast, dont have time to experiment with other tech. Also more resources to reference internally.",hmacayf,t1_hm9d1ll,1638029727.0,False
r2ye5m,.NET ecosystem,hm98t3h,t3_r2ye5m,1638003911.0,False
r2ye5m,Must be a Microsoft guy lol,hmavex1,t1_hm98t3h,1638037905.0,False
r2ye5m,"HTML/CSS/Javascript.

Backend? PHP - legacy languages always dominate volume metrics.

If you love legacy code bases, or want to be in the same position as COBOL developers are now, those are the languages to learn.",hm8fbmu,t3_r2ye5m,1637984266.0,False
r2ye5m,I have seen more Java/Spring backends judging by global contractor and job opportunities.,hm8jzw6,t1_hm8fbmu,1637986816.0,False
r2ye5m,PHP is the [overwhelming](https://w3techs.com/technologies/details/pl-php) most popular backend language.,hmau74v,t1_hm8jzw6,1638037389.0,False
r2ye5m,"Would Personal Home Page ever really be like COBOL? It’s not too different from Java or C in syntax, COBOL is sort of a different beast",hm8g91q,t1_hm8fbmu,1637984762.0,False
r2ye5m,"A decent Python programmer can side-skill in PHP in about a week to be productive, a year to be the equivalent of a senior dev. Totally different to COBOL both in terms of language paradigms and what is done with the language. The reason COBOL devs are in such high demand is because COBOL runs core banking software that is simultaneously very difficult to test and absolutely cannot, ever, ever fuck up.",hm8jxxe,t1_hm8g91q,1637986785.0,False
r2ye5m,I once did pair programming that touched my companies billing code in PHP and it felt really tough and scary. I can’t imagine doing the same but in COBOL for a massive bank.,hm9ev9x,t1_hm8jxxe,1638009155.0,False
r2ye5m,"I assume that you are asking this question to decide upon what stack to choose to learn. It would have been a lot easier if you had stated a specific niche it would have been easier to answer. Learn a general programming language like Python which you can use anywhere(web, ml, application programming, etc). If it is web, then I recommend frameworks like node.js for an easier learning curve, choose go and/or rust for a steep learning curve but better perf.",hm8tmqy,t3_r2ye5m,1637992515.0,False
r2ye5m,"as far as legacy it's xAmp (os of choice, apache for server, mysql for db, and php for code base language) based stacks but I personally don't forsee many new projects being built with this but it is what most of the internet is currently built with. 

Newer stacks that are common would be mean, mern, mevn and conversely fern, fevn, fean stack (f or m for mongodb or firebase dB, express js for server, [angular, react, or vue js for front-end framwork], and node js for server-side code-base.

Ruby on rails had its moment in the sun IMHO and is still quite valid as a framework but you could also argue is not nearly as popular as a few years ago.

In the end it has less to do with what is popular over all since that can vary regionally, and more to do with choosing the right tool for the right job. Generally speaking they all do the same thing, which is provide the devs/engineers a blueprint to fast track the build of an application. It is your job as the engineer however, to figure out which will provide the most useful functions for what you need.

also take any opinion of mine or otherwise, with a grain of salt",hma30ju,t3_r2ye5m,1638025394.0,False
r2ye5m,We’ll with cloud migration u can have several stacks to pick and choose from. Now ur apis can be made with anything and consumed together through something like aws lambda or fargate. Ur front end can be mixed as well. Really depends on what most your companies code was built with prior though,hmb1tth,t3_r2ye5m,1638040619.0,False
r2ye5m,I have never undrstood the serverless architecture. I will have to do research on lambda or fargate…,hmb4lri,t1_hmb1tth,1638041753.0,True
r2ye5m,"It’s def worth learning the basics. U can essentially set up any api u want with any architecture to be consumed as a service. So at my job we got node, spring boot, python backend apis and either angular or node front end. I believe some react as well, but not as much",hmbbv9k,t1_hmb4lri,1638044736.0,False
r2ye5m,TCP/IP.,hm9ozwd,t3_r2ye5m,1638017354.0,False
r2sjef,"I suspect part of it would be the fact that while you’re editing video, the editor has it in a format optimized for editing rather than the usual playback-optimized format. When you export it, it converts it into one such (compressed) playback-optimized format, so the output video runs a lot smoother than a video being edited would.",hm71zhm,t3_r2sjef,1637959954.0,False
r2sjef,"Hardware support for playback. H264 format and other codecs are well supported by hardware decoder in most devices. 

A video editor will open the files and read and convert to its internal format, likely a RAW video format. This takes time and a lot of CPU power.",hm751pp,t3_r2sjef,1637961378.0,False
r2sjef,This. To do complex calculations on each frame the video would be stored in a raw format. If you wanted to preview the changes being made you have to calculate each frame in real time from a raw format.,hm7pp6p,t1_hm751pp,1637971213.0,False
r2sjef,"The version of the video inside of the editor is separated into parts that make is easier to edit.   The regular version is optimized for size and playback.  When the the video is exported, the whole thing has to be converted from a list of editable files to one long file that is encoded in some regular playback format.   When the editor is playing back a version, it is converting those parts  while you are watching it.",hm868i8,t3_r2sjef,1637979573.0,False
r2sjef,"Video compression codecs work on a delta basis: the next frame’s data is often stored as the differences from the previous frame. For normal playback, you have the previous frame data, get the delta and calculate the next frame, repeat. Plus this job can be offloaded when hardware decoding support is available. For anything else: like non-linear video editing, where you need a preview of the whole video in the timeline and jump around in a non-linear fashion, the whole thing basically needs to be decoded up front.",hm9sa0e,t3_r2sjef,1638019477.0,False
r2sjef,This is better suited for /r/AskReddit,hm6myk6,t3_r2sjef,1637952897.0,False
r2sjef,"Thanks, I will post it there as well.",hm6nm8s,t1_hm6myk6,1637953205.0,True
r2a6yz,"A BIOS software image is flashed with specialized software, and hardware that applies power to the chip and uses a chip-specific protocol to write the image.  The hardware can either be a specialized harness or board, or after installed on a full board (in-circuit programming).  That's how programming software directly onto most microchips works.  Chips themselves need ways of getting software loaded directly to them, so manufacturers provide that.

The BIOS is the software responsible for several tasks in a computer, one of which is loading the bootloader software, which locates and loads the full OS.",hm443re,t3_r2a6yz,1637897580.0,False
r2a6yz,"That was a very informative. What is a BIOS software and when you say specialized software do you mean software specifically made to write a program onto the chip in question? Finally, what chip is it written into? The CPU?",hm44isi,t1_hm443re,1637897807.0,True
r2a6yz,"BIOS software (or it's UEFI counterpart) is the software that lives directly on a computer (stands for ""Basic Input/Output System""), not a hard drive like your OS (windows or whatever).  Its usually written by the people who make your motherboard (or logic board or whatever, depending on what kind of device were talking about).  It has many jobs, but a main one is to realize when you turn the computer on and start to load more complex software (like windows or whatever).  Big oversimplification, but that's the idea.

That software is written into ROM (read-only memory) area installed on that board somewhere...not the CPU.  The CPU itself doesn't have a BIOS, it has some other really small software that helps it do it's job (called microcode or firmware usually).  A computer has several different chips/boards/components, each to do a different job, and most of them have some level of firmware that's loaded in some way similar to my first comment (usually developed by the manufacturer).  Even a hard drive has some tiny amount of software on it.

The software that does the loading is special for each exact kind of chip/board/thing, it's usually called programming software but can have any number of names.

The bottom line is that computer components get their first software on them because they were designed to be able to accept programming from the manufacturer, and another component or computer does that programming.",hm45m5d,t1_hm44isi,1637898404.0,False
r2a6yz,"After a CPU powers up, it starts executing instructions starting from a fixed address. Exactly what address, and the power up sequence leading up to, can vary based on the specific hardware. A ROM chip (there a number of kinds) that has the initial 'operating system' on it is wired up so that the memory the CPU starts executing from, corresponds to the address of that chip. 

For the last 25-35ish years, that initial 'operating system' is a small program that generally called a boot loader. It is very small, and able to just enough to to read the operating systems from disk into the memory, and then jump to the start of that memory. 

That's the generic idea of how the hardware works. How'd the text of code to get be compiled code on that ROM chip? Someone typed it into a computer, used a computer to compile it, and a computer to write it to a ROM. 

Of course, if you go back far enough, you'd have a time where you had a computer you wanted to run something, but not a computer to write/compile your program separately on. Initially the programs were translated to their binary representation by hand, and the bits were loaded into a computer manually. Literally set a series of 8 switches to the bit values of the byte you want, then press a button to load it to memory and more on the next byte. 

There has been a long evolution of tech between manual switches and using a fully functional, separate computer. It should be pretty obvious that once you have a computer that will do part of the tedious process for you, you'd use it. You'll find this progressive boot strapping of technology all over engineering. You build tools, so you can build better tools.",hm41xce,t3_r2a6yz,1637896418.0,False
r2a6yz,"When we load the operating system onto our memory for operations, we call that process 'bootstrapping' or 'booting', hence, we are 'booting' up the computer. Booting is done via a 'boot loader' program.  This program is what we are interested in here, the bootloader changes depending on what kind of device we are working with. In modern devices, the boot loading process is sophisticated, so we employ multi-stage bootloaders to be able to boot up a system using chain loading.   
As the name 'chain loading' suggests, your boot loader loads up something and then proceeds to load another boot loader which does the next process. 

Your bootloader firmware is installed together with the BIOS, in non-volatile memory (basically your ROM). This is what you'd call a stage 1 bootloader, it simply starts up, detects your boot device, and then moves to the next stage. (The BIOS does a lot more at the initial stage, but we aren't concerned with it for now).

The Master Boot Record is the first disk block and this is where the real booting process starts. This is the very beginning of your disk partition. The MBR stores the first stage boot loader and the disk partition table.

After the BIOS phase is completed, MBR starts to scan the partition table and loads up the Volume Boot Record, VBR is basically what partitions (and what kind) this particular disk has.   


Just like MBR, VBR's initial segment has what partition type and size the partitions in this particular volume are, followed with the Initial Program Loader (IPL).   
This IPL is our second stage bootloader, which is basically coded to actually load our operating system. (Depending on what kind of operating system you are using, IPL will load up another program that will actually bring the contents of the operating system to the main memory and make it ready for execution).    
Like for example, IPL will load up the NT loader for windows which actually loads up the content of the windows operating system. Or IPL will load up GRUB for Linux.  These would then follow different stages to load up the operating system. 

All in all, it's a complex chaining process that starts from static code and then points you to another location where more sequence of actions are stored. 

(Also most machines now use UEFI over BIOS).  
(Another fun fact, this process can be abused to load up malware or even prevent the completion of the bootloader process. A lot can be done with this since the initial code is actually a static sequence, and if able to tinker with it, the threat actor has a lot of power) 

ps: sorry if this doesn't make sense xD",hm4tkkn,t3_r2a6yz,1637913935.0,False
r2a6yz,This guy has it sus succinct and accurate,hm5jvh5,t1_hm4tkkn,1637934549.0,False
r2a6yz,The motherboard already includes software: UEFI.,hm3j11z,t3_r2a6yz,1637886448.0,False
r2a6yz,How does the software get onto the motherboard and on which chip is it located?,hm3j6i2,t1_hm3j11z,1637886529.0,True
r2a6yz,Guy in my dreams 2-4 days ago said there were machines that write onto motherboards and SBCs similar to how we used to burn data to read only DVDs.  I was thinking about this lately which is why I dreamt it,hm3qs0a,t1_hm3j6i2,1637890505.0,False
r2a6yz,"Bios/UEFI is firmware, not software.",hm58fm6,t1_hm3j11z,1637926580.0,False
r2a6yz,Do you think that is a helpful distinction in this context?,hmxsvg8,t1_hm58fm6,1638457622.0,False
r2a6yz,This is a hard concept to grasp without first understanding BIOS and firmware. I would look up some YouTube videos about them. At the airport right now but when I have some time I’ll come update with some links,hm4vprj,t3_r2a6yz,1637915628.0,False
r2a6yz,They burn a small program into Read-only Memory (ROM or EEPROM) that does some initialization and look to read in the OS when the computer is first turned on.,hm3t7cf,t3_r2a6yz,1637891790.0,False
r2a6yz,">that

Did you mean ""then"", or did you mean ""that does""?


What is the ""initialization""? Does it just mean set up?


Finally, thank you very much.",hm3upzs,t1_hm3t7cf,1637892590.0,True
r2a6yz,"Yeah, it should be ""that does"", I fixed it now.

As far as this question is concerned, ""initialization"" just means telling the CPU where to find the instructions that load & boot the OS. It used to be done by having the CPU go to the Master Boot Record which would contain the essential info for getting the OS booted (along with some other stuff). This is now considered legacy BIOS and most systems use UEFI (of which I know less about).",hm4g56f,t1_hm3upzs,1637904561.0,False
r2a6yz,"Bootloader (BIOS in PC context, firmware in others) is basically a mini-OS that can read/write disc, it allows for installation of bigger OS.

How do you get a bootloader onto your computer? Depends on the architecture:

* in x86, it comes in it's own chip on your motherboard
* in phones/embedded, it could be the first partition of the main storage disk. To install first time, you take out the disk and write to it with another device. Afterwards, the bootloader can support self update through something like TFTP or reading specific file on a removable disk. (Hence the reason, you can brick the device by bad update)",hm5gyx9,t3_r2a6yz,1637932800.0,False
r2a6yz,"The computer has rom (or read only memory) that gets loaded with instructions for how to boot

The data gets loaded on electronically, ie the chip has pins that they use to load data onto it in the factory so it already has the data on it when it gets assembled and sent to you",hm70jej,t3_r2a6yz,1637959284.0,False
r2a6yz,">The data

Did you mean the ROM data?",hm73q9e,t1_hm70jej,1637960765.0,True
r2a6yz,We have this post every day.,hm45vmz,t3_r2a6yz,1637898544.0,False
r2a6yz,"The best way to understand it, is to DIY with linuxfromscratch. The experience of putting your own OS together is satisfying.",hm58tdh,t3_r2a6yz,1637926903.0,False
r2e13s,"Each problem has two bounds. NP is a upper bound: meaning a problem is at most as difficult as NP. NP-hard is a lower bound: meaning a problem is at least as difficult as NP. NP-complete means a problem is both NP and NP-hard. If you want to show a problem is NP-complete, you need to go both directions.",hm4htyu,t3_r2e13s,1637905632.0,False
r2e13s,"> If X is in NP-Complete (so all NP problems reduce to X), and X reduces to Y, then Y is also NP-complete right?

That reduction just means that X is not harder than Y. It doesn't provide any guarantee like Y being no harder than X. For example, Integer linear programming (which is NP-complete) is easily reducible to mixed-quantifier Presburger arithmetic (which is not in NP).",hm49l2x,t3_r2e13s,1637900607.0,False
r2e13s,"Thank you, the specific example is very helpful. I'm still a bit confused though. Why would we need a guarantee that Y is no harder than X to conclude that Y is NP-Complete? 

My thinking was that all NP problems reduce to X in polynomial time, and X reduces to Y in polynomial time, so all problems in NP reduce to Y in polynomial time, thus Y is NP-Complete. Is there an error there?",hm4amel,t1_hm49l2x,1637901217.0,True
r2e13s,That means Y is NP-hard. An NP-complete problem is one that's both NP-hard and in NP.,hm4bk79,t1_hm4amel,1637901772.0,False
r2e13s,"You need further assumptions. First, when stating that X reduces to Y, it needs to be a *polynomial* time reduction (i.e. problem X can be transformed into a subset of problem Y in polynomial time with respect to the length of the input of problem X). Second, you need to assume (or prove) that Y is an NP problem. 

Recall that an NP-complete problem X is simply:
1. an NP problem
2. a problem where all NP problems can be reduced to X in polynomial time (i.e. X is NP-hard).

Intuitively, assuming Y is in NP and there exists a polynomial reduction from the NP-complete problem X to Y, Y becomes an NP-complete problem as follows:
1. For any NP problem A, reduce this to X in polynomial time. This is possible because X is NP-complete.
2. From X, reduce this problem to Y in polynomial time. This is possible since this is assumed.
3. Thus, any NP problem A can be reduced to Y (which shows that Y is NP-hard) in polynomial time, since the two-step reduction process above takes polynomial time.
4. Finally, we assumed that Y is an NP problem. This shows that Y is in NP and Y is NP-hard, which then shows Y is NP-complete.",hm4wcll,t3_r2e13s,1637916145.0,False
r2e13s,"Not necessarily.

One thing is that from your question, you only mentioned ""X reduces to Y"", but does not specify what kind of reduction it is. This I can already imply, there is no information on how difficult Y is.

To give a fun example (I like to argue by using examples), consider X = sorting an array of n integers. This is in polynomial (in P), and this is trivial to verify. Then, if I were suddenly feeling cute and decide to come up with this Y:

1. Set up an email connection
2. For each integer i, do:
   1. Compose a message; specify that the message should only arrive on the ith day after the sending date; set the receiver to self
   2. Add the message to the Message Array
3. Send everything out in the Message Array

>Listen to the incoming email connection and print out the results. On the 1st day you will receive 1 (if there exists any ""1"" in the original array), on the 2nd day you will receive 2, ... ad infinitum

Then the complexity suddenly becomes much higher, because literally another computer is required to do this job (sending the email back to you). Also, sorting integers (X -(reduce)> Y) suddenly becomes unbounded.

It is important to specify the kind of reduction you are talking about. But let's say you are talking about polynomial reduction. It still doesnt say anything about whether Y is NP-complete or not. If I find some Y which is in P (or more realistically, where Y is in EXPONENTIAL) then I would have disproved your idea.

In fact as others may have pointed out, if someone happened to find some Y such that Y is in P, then they would have solved P = NP and would become famous. It is just that no one have found such algorithm of Y and so I cannot give any examples to this. ""People can only think of NP"" does NOT imply ""there is only NP in this world"".

\-------

Actually in situations like this, it is often reversed: I want to know what kind of problem X is, and so I try my best to reduce X to some ""simplest"" problem Y, and I literally cannot get more simpler than Y. But oh look, Y is e.g. in NP! So unfortunately X will have to be in NP too.

This mindset can be useful if you happen to also want to explore undecidability. In the broader scheme of things, P, NP, EXPONENTIAL, ... all are in ""DECIDABLE"", and for Turing, one of his contribution was that he found a whole bunch of problems that were ""UNDECIDABLE"", i.e., you literally will never get any result if you tell any Turing machine (e.g. a modern computer) to ""go calculate it"". EG, ""does this program terminate?""",hm585sg,t3_r2e13s,1637926344.0,False
r2e13s,"I have been a software developer for 5 years. I have literally no idea what this means. I am not ashamed to admit it, but I am curious what this has to do with computer science(besides the obvious mathematical connection) is this an algorithm thing, or is it reference to some type of hardware configuration?",hm75zl0,t3_r2e13s,1637961813.0,False
r27cn1,Honestly just creating a program that operates with these equations would be really cool. I would love to see a project like this,hm39zas,t3_r27cn1,1637881793.0,False
r27cn1,What would such a program actually do?,hm64kb4,t1_hm39zas,1637944719.0,False
r27cn1,Everything that can be expressed in binary equations. So everything a normal computer can do.,hm6megs,t1_hm64kb4,1637952639.0,False
r27cn1,... just slower and more complicated. Got it!,hm6n2y9,t1_hm6megs,1637952955.0,False
r27cn1,"So it appears to make sense! 

I absolutely love this. It reminds me of the sort of stuff I'd doodle on slow nights at work. That said, I'm not gonna choose to check your adder.

In terms of a turing machine, a turing complete system can handle recursive functions, such as ackerman's function. I'm struggling to see how it could be used to perform recursion when there's no actual mechanism to perform a branch (like an if). I'd therefore propose that it's probably not.

I can't think of any way of using this encoding of these gates that wouldn't stop.",hm3anqy,t3_r27cn1,1637882145.0,False
r27cn1,"Yes, this can only encode acyclic graphs, i.e. trees.

Yet even then you can not build a Turing machine out of finitely many bits",hm3z6i1,t1_hm3anqy,1637894959.0,False
r27cn1,"That's a cool concept, thank you. Your NOR is wrong though, it should be

1-(x+y-xy) = 1-x-y+xy

With your formula, 0 NOR 1 would be 2.

You've got the same error with XNOR.",hm4wvud,t3_r27cn1,1637916579.0,False
r27cn1,"    x=0,y=1:
    OP:        1-x+y-xy => 1-0+1-0 => 1+1 => 2
    Corrected: 1-x-y+xy => 1-0-1+0 => 1-1 => 0

Edit: Fixed above block, now showcasing the difference between OP's and u/Lornedon's; some signs are flipped.",hm5cpfx,t1_hm4wvud,1637929933.0,False
r27cn1,"The formula OP had in the post for NOR was 1-x+y-xy.

They two formulas mentioned in the correcting comment look the same because they are the same. That's why there was an equal sign between them",hm5e0w6,t1_hm5cpfx,1637930861.0,False
r27cn1,"Gotcha! I see where I went wrong; the one in the OP is almost the same, just some signs are flipped. Cheers for pointing it out",hm5jex4,t1_hm5e0w6,1637934278.0,False
r27cn1,Crap I will be correcting this along with a couple of other errors in my sums. Thanks for letting me know,hm6yb8s,t1_hm4wvud,1637958234.0,True
r27cn1,So 1+1+1 == 43?  What am I missing?,hm4hjki,t3_r27cn1,1637905448.0,False
r27cn1,"1+1+1+8\*1\*1+8\*1\*1+8\*1\*1+16\*1\*1\*1A note I could have probably added in was that 7xy doesnt mean 710 or 711 it means multiply 7 by x by y

I am just reckoning this is where you went wrong

Or I just totally messed up my 4 bit adder and honestly it is probably my error",hm4s28e,t1_hm4hjki,1637912800.0,True
r27cn1,"    1+1+1+8*1*1+8*1*1+8*1*1+16*1*1*1 == 1 + 1 + 1 + 8 + 8 + 8 + 16

    == 43

So like I said, 1+ 1 + 1 == 43.  What am I missing?",hm4ubfy,t1_hm4s28e,1637914509.0,False
r27cn1,"I think the carry bit z should be input as 2 and the result should be 100 (i.e. binary 4). However, that doesn't quite work, and nor do most other test cases:

    x + y + z + 8xy + 8xz + 8yz + 16xyz
    
    x=1,y=1,z=2?
    1 + 1 + 2 + 8 + 16 + 16 + 32
    76 => not binary :(
    
    x=1,y=1,z=0
    1 + 1 + 0 + 8 + 0 + 0 + 0
    10 => 2!
    
    x=1,y=0,z=0
    1 + 0 + 0 + 8 + 0 + 0 + 0
    9 => not binary :(
    x=0,y=1,z=0
    0 + 1 + 0 + 8 + 0 + 0 + 0
    9 => not binary :(
    
    Attempt to treat the carry as 1 or 2, x and y are 0:
    0 + 0 + 1 + 0 + 0 + 0 + 0
    1 => 0 + 0 + 2 != 1
    0 + 0 + 2 + 0 + 0 + 0 + 0
    2 => This works

Must be something a bit off somewhere! Perhaps the next iteration of these equations will sort it out these issues (or we're both reading it wrong!).

EDIT: Realised I messed up my cases where the z was set but x/y were 0; have fixed them now and found the correct value for z is 2. Still doesn't quite work when adding 1 + 1 + 2, sadly",hm5cc72,t1_hm4ubfy,1637929663.0,False
r27cn1,"I think the intention is to make functions which take decimal inputs that look like binary, then emulate logic systems even when they have multiple inputs and outputs.

If we write `1001` in binary, we're talking about `9` in decimal. But here, we want `1001` in *decimal* as a result of a binary system with 4 outputs, using decimal math.

The whole thing doesn't seem particularly useful. Admittedly, I'm having trouble sifting through the rah-rah here, the OPs word salad, and the math errors, but that's my takeaway.

If we think of [a full-adder](https://en.wikipedia.org/wiki/File:Full-adder_logic_diagram.svg), we have two inputs X and Y, plus a carry input Z. (I'm using the OPs input variables, the image I linked uses A, B, and C instead.) It outputs two bits: sum and carry, which you can conveniently think of as a two digit binary number with carry as the MSB and sum as the LSB.

The goal appears to be to write a function that does this all in *decimal*, such that `f(1,1,1)` for example, results in `11` *decimal*. The function would have this truth table: 

X|Y|Z (Carry)|Result
:--|:--|:--|:--
0|0|0|0
0|0|1|1
0|1|0|1
0|1|1|10
1|0|0|1
1|0|1|10
1|1|0|10
1|1|1|11

It's important to remember these functions have very limited domains: `x` and `y` and `z` can be 0 or 1, and absolutely nothing else ... even though the function definition looks like a plain old polynomials over the reals.

The function given for a full adder:

> Heres an adder that takes a carry and 2 bits and outputs the binary in:
> x+y+z+8xy+8xz+8yz+16xyz

is pretty obviously incorrect, as we're finding.

I think we're a lot closer with this (tested):

    x + y - 2xy + z - 2z(x + y - 2xy) + 10(xy + z(x + y - 2xy) - xyz(x + y - 2xy))

which simplifies a bit (untested):

    y + z + 8yz + 10yzx^2(-1 + 2y) +  x(1 + y(8 - 16z) + 8z - 10zy^2)

[This Google Sheets spreadsheet](https://docs.google.com/spreadsheets/d/1PCjgBgT8Dq2L6hh5oB_iGN440q2mbXe4CaQzqQ9D51M/edit?usp=sharing) tests that expression, and it looks a lot better than the OPs original to me.",hm68jtv,t1_hm5cc72,1637946456.0,False
r27cn1,">The whole thing doesn't seem particularly useful. Admittedly, I'm having trouble sifting through the rah-rah here, the OPs word salad, and the math errors, but that's my takeaway.

I love how you turned my garbage writing in to a food outlet, I should start a business about word salad

You are correct that the aim is to take Denary numbers that look like Binary and add them in Denary to give an output that looks like Binary

As far as math errors goes I believe that I only messed up which way round the + or - signs go

`x+y+z+8xy+8xz+8yz-16xyz` 

Does work and is far more compact then the solution you had, mostly because 0\^2 or 1\^2 are totally meaning less and since xy and z all are ones and zeros you will find that you can just remove them and then cancel down.

I will be correcting my mistakes in an edit",hm6zii0,t1_hm68jtv,1637958808.0,True
r27cn1,Great! Happy that I could help you find something that works.,hm78jzm,t1_hm6zii0,1637962996.0,False
r27cn1,I totally messed up and forgot that you must deduct the sixteen not add it,hm6ygwa,t1_hm4ubfy,1637958309.0,True
r27cn1,"Lol. I remember doing this in first year. If you allow inputs to vary smoothly between one and zero, you end up with basic probability, but you lack an easy way to model dependant variables.",hm6dge9,t3_r27cn1,1637948622.0,False
r27cn1,"But that modeling is the art of it. Really, these constructs can end up being involved in simple machine learning models ... as long as you can correctly fit your data into the [0, 1] continuous range in a meaningful way.",hm7a5x4,t1_hm6dge9,1637963738.0,False
r27cn1,">could we write a Turing machine like this?

Yes! In fact, you can simulate any logic using solely NAND gates as they are functionally complete. An arbitrary amount of NAND gates could be used to implement any possible algorithm based on binary encoding.",hm4gi9p,t3_r27cn1,1637904793.0,False
r27cn1,"I think turing completeness != functional completeness

You can implement every possible boolean function using NAND gates, but implementing a turing machine is a different thing",hm5dkp8,t1_hm4gi9p,1637930546.0,False
r27cn1,How is a latch represented in this schema?,hm4hpfc,t1_hm4gi9p,1637905552.0,False
r27cn1,"I did at one point fiddle around with trying to write a latch

I ended up using the answer system written into the calculator 

so Ans + 1 = Ans gives you a value that increments over time incrementing by one",hm4sy4e,t1_hm4hpfc,1637913458.0,True
r2rbee,The art of computer programming there’s 6 volumes I think. Often touted as one of the greatest computer science series ever written.,hmhou1r,t3_r2rbee,1638158539.0,False
r1xcw9,"At this time, it is not only not possible, we don't even know if it is possible. Or as I like to say, not only do we not have a path to ASI, we don't even know if such a path exists. AI, as it currently exists, is simply a computational tool (or aide) for certain types of problems.",hm1aokc,t3_r1xcw9,1637849707.0,False
r1xcw9,Ok thank you so much. Do you know if aurora21 is anything like ASI or what even is it?,hm1b9st,t1_hm1aokc,1637850004.0,True
r1xcw9,"It is just a brand (in a sense) of a supercomputer. The project they did is to map the connections in a brain, which is very complex and requires a lot of computing power. It would be like mapping the connections made by every road, sidewalk, path, railway, etc. But it is just a map. The goal is to be able to understand how different structural connections relate to different conditions. E.g., can we diagnose Alzeimer's earlier by scanning the connections in the brain, hence treat it earlier; thereby, improving outcomes.",hm1bily,t1_hm1b9st,1637850130.0,False
r1xcw9,"Ok cool, so it dosent take into account the functions of connections? Could that really be done in 3 years, i heard 30 before?",hm1c6u1,t1_hm1bily,1637850466.0,True
r1xcw9,"Maybe, but unlikely.

ASI has been 10 years away for about 60 years. :)  As I said above, there is no known path to ASI right now. Could somebody discover it tomorrow? Yes. Is that likely? No.

Also, the dangers of an ASI are greatly overexaggerated. First, an ASI would have to be hostile. It is not certain that would be the case. So, a hostile ASI could cause a lot of disruption, but it has no way to cross the physical divide, so there are extreme limits to what it could do.",hm1cz57,t1_hm1c6u1,1637850873.0,False
r1xcw9,"> Also, the dangers of an ASI are greatly overexaggerated. 

The dangers of an ASI would be very real, although I agree there is no clear path toward one.

> First, an ASI would have to be hostile. It is not certain that would be the case.

It doesn't need to be hostile per se, any slight misalignment would have drastic consequences. [Accurately aligning advanced AI systems is a difficult unsolved problem.](https://youtu.be/IeWljQw3UgQ)

> So, a hostile ASI could cause a lot of disruption, but it has no way to cross the physical divide, so there are extreme limits to what it could do.

[It's impossible to effectively sandbox an ASI.](https://youtu.be/i8r_yShOixM?t=305) It would be a better manipulator than any human who ever lived. By communicating with its human operator, it impacts the real world and crosses the physical divide. Even our current dumb AI systems are scarily good at manipulating humans (e.g. social networks maximizing engagement).",hm1jry8,t1_hm1cz57,1637854137.0,False
r1xcw9,"1. While there is of course no upper bound on the potential danger that can be caused by an ASI, if we look at it through a realistic lens, then the dangers are overexaggerated.
2. Hostile, within the context of AI, means unaligned or incompatible with human desires or needs.
3. RE: AI as a master manipulator. There's no indication that this is necessarily true. Our current AI systems are not really that good at manipulating us. Humans are good at creating systems that use AI as a computational tool to do such manipulation. These are vastly different things.",hm1p9f5,t1_hm1jry8,1637856581.0,False
r1xcw9,"Disagree. The fundamental root of computation is Binary decision making. I’m not sure what the fundamental structure of an official first and functional ASI will be (quantum , etc). However, an ASI likely does not conform to our human concept of ‘hostile’ . Take the anthill scenario for example 

The TRUE danger is if we allow it to control our governments in the name of computational precision. Control our nuclear weapons, etc. Above this, we do not have the ability to even fathom the repercussions as it is outside of our precedented human range of understanding and mental capacity",hm39vgc,t1_hm1cz57,1637881740.0,False
r1xcw9,"You are quite welcome to disagree.

I don't really understand your first paragraph. It does not make much sense to me. I think perhaps you are misunderstanding the term hostile as it applies to AI ethics. As I posted elsewhere, hostile in that context means unaligned or incompatible with human needs or desires. It does not imply maliciousness or anything to do with ants.

As for the second paragraph, this same can be true for any safety-critical piece of software. Flawed software can have serious repercussions whether AI-based or not. As for the last sentence, this is simply fundamentally flawed (in my view anyway) and based on science fiction or pure speculation (usually from non-experts, such as Elon Musk). ASI does not mean that it can do everything, and it certainly does not mean it can do the impossible. It is in fact quite possible to examine ASI in a scholarly way by making reasonable extrapolations based on what we know about AI. Of course, even such work is speculative because we do not really know much about ASI (see my other posts), but at least it is justified by existing literature.

If you're really interested in this, then I'd suggest looking at some works on AI ethics. There are some good works on ASI as well. Nick Bostrom as written some works on the dangers of ASI (which I personally feel are flawed), and then there are a number of good rebuttals to his arguments. So this is a good place to start.

[https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=ai+ethics&btnG=](https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=ai+ethics&btnG=)

tps://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=superintelligence+bostrom&btnG=&oq=Superintelligence+Bostr",hm47otl,t1_hm39vgc,1637899530.0,False
r1xcw9,"Thanks for sending, I’ll give them a look. And yes I misunderstood your application of hostile. Sure, most software has risk and security related concerns and implications, AI based or not

My argument is that the potential caliber of ASI, when applied to certain dimensions or areas, could potentially have indefinite negative repercussions, and we lack the capacity to even forecast such repercussions. Sure, non experts will fear monger with what is built on speculation. So: the dangers of *current* applications of ASI  are more or less benign. 

Please allow me to illustrate my thought process. I believe that there is power and greater understanding with dimensions. Example: our human existence is likely tied to 3 dimensions. We most likely do not live in a 3 dimensional universe, in fact, maybe at least 4  (a reality with more than an x, y, and z axis). I’m guessing that we are likely unable to comprehend the laws of higher dimensions as we are built with / possess the syntax of 3 dimensions (likely. Unless our body is 3 dimensional but our mind is not. I don’t know). Im saying all of this to demonstrate our lack of understanding as humans. We don’t want to create something which exceeds our ability to a highly significant extent, where variables can’t even be assigned as we lack the ability to detect such variables. Moreover, assign critical roles to said technology. So long as most variables are understood and data is established, the technology is ok to deploy. I hope this makes sense",hmcuson,t1_hm47otl,1638069839.0,False
r1xcw9,And then you have the issue of discovering variables as you deploy the technology into the environment,hmcv352,t1_hmcuson,1638069984.0,False
r1xcw9,"No. *The* premier whole-animal conectome simulation is [OpenWorm,](https://openworm.org/) which is currently working on simulating a less than 1000 cell animal. 

> OpenWorm aims to build the first comprehensive computational model of the Caenorhabditis elegans (C. elegans), a microscopic roundworm. With less than a thousand cells, it solves basic problems such as feeding, mate-finding and predator avoidance. Despite being extremely well studied in biology, this organism still eludes a deep, principled understanding of its biology. 

Despite the organisms simplicity, we have failed to simulate it. There are about 86 billion cells in the human brain, and they are far more complex than the worm's cells. 

You should only start worrying about ASI when you see, for example, a brainless mouse hooked up to a supercomputer and doing mouse things.",hm235in,t1_hm1c6u1,1637862477.0,False
r1xcw9,"Thanks for the reply.

Does the human brain itself not prove the possibility of general intelligence? If brains are nothing more than huge information processors, then given enough time(a thousand years is nothing on the cosmic scale) we will eventually be able to mimic it's full architecture digitally, no?",hm1fkc9,t1_hm1aokc,1637852126.0,False
r1xcw9,"No, it’s not clear at all that biological brains (human or otherwise) can be modeled digitally.  We do know that computers can implement any algorithm, but we don’t know that everything a brain does is algorithmic. And we do also know that certain problems cannot be solved algorithmically (e.g., the halting problem).  Furthermore, for the few things the limited AI of today can do, we know that computers accomplish many of those tasks differently than humans.",hm1ggg2,t1_hm1fkc9,1637852569.0,False
r1xcw9,"What ComputerSystemsProf is accurate. We know that general intelligence is possible; however, as they pointed out computers seem to think differently than us. So this raises some interesting questions as potential paths to general intelligence:

1. How can we make a computer generally intelligent without duplicating human intelligence?
2. How can we make a computer duplicate human general intelligence?

We do not really know if either of those can be done. We can ponder the question, but there is no really clear direction to the goal. Of course, some people have some ideas (general AI is one of my side projects, so I happen to believe it is possible) but to date, none of them have really panned out or again, kind of provided a very clear ""Oooooooh"" moment, that's how we can do it.",hm1hejp,t1_hm1fkc9,1637853025.0,False
r1xcw9,"Well the good news is that we are very likely decades away from real artificial intelligence.

What is often called AI by buzzword salesmen is more properly called machine learning, though even that is a but presumptuous.

The way most of it works is by randomly trying out curves on a graph again and again until one or more of the models guesses the right answer. 

That model is then used and sold as AI even though its really just guessing some fancy math equation that would just take a while to do by hand.

The model of the brain probably cant be used to simulate a mind yet, we don't fully understand how neurons work fully let alone the nervous system as a whole.

Also while MRIs can monitor brain activity (local only, cant be done remotely) they are a long way from being able to read thoughts and especially put them there.

Lastly we don't even know the a true general AI is even possible and the definitions even gut a but blurry.

 I prefer Virtual intelligence to describe a system that appears intelligent but isn't true a thinking mind (""agent"" in philosophy); its only AI once it is properly a novel mind, e.g. displays sentience, sapience, self-awareness, consciousness, and intelligence.

Tldr: most of what is called AI really is just not; they are programs that excel at pattern recognition and functional optimization, but they cant actually think and adapt like a real intelligence. Mapping the brain is only one piece to the puzzle and decades more work is needed; As a result planting thoughts is almost certainly not possible at this time.",hm1ytep,t3_r1xcw9,1637860683.0,False
r1xcw9,"You have people with PhDs in the comments telling you everything you need to know. But most importantly, whoever got you on this path of worrying about AI, stop listening to them. Take care of yourself.",hm5lxmk,t3_r1xcw9,1637935705.0,False
r1xcw9,"The fuck? You have Schizophrenia. The literal last bullshit you should be worrying yourself with is artificial intelligence. Look, our AI amounts to some calculus and linear algebra -- it's a neat accounting trick for estimating functions. There is zero intelligence involved, and AI is a misnomer title. 

We are so far from general AI that we literally don't even have a vocabulary set to properly even DISCUSS the topic.

Even if we did, creating a ""brain map"" is worthless with regards to creating general AI systems. The fact that a  ""supercomputer"" is doing this ""mapping"" is not relevant to AI: biologists want to know about human biology, they aren't trying to build AI systems with that data.

I can't stress this enough: You know you have Schizophrenia, and you have to already be aware that these types of delusions regarding AI are unhealthy and inappropriate. Don't feed these types of thoughts to the point where you have to reach out to people to confirm your suspicions are incorrect.",hm23m0n,t3_r1xcw9,1637862664.0,False
r1xcw9,Preach brother nice comment,hm2o7v3,t1_hm23m0n,1637871460.0,False
r1xcw9,How is reaching out to informed parties not exactly what we should recommend to individuals with doubts and suspicions -- even doubly so for those with mental disabilities?,hm3t59y,t1_hm23m0n,1637891760.0,False
r1xcw9,Or just dive in with some pkd,hm383o1,t1_hm23m0n,1637880855.0,False
r1xcw9,"I suggest learning about computers and programming a bit yourself.

You'll see very very quickly that computers are dumb as bricks and that we're extremely far away from anything remotely resembling agency.",hms3uhh,t3_r1xcw9,1638353808.0,False
r27hds,I would imagine most of the power would be spent storing the entirety of Wikipedia rather than displaying/reading the files,hm3ayev,t3_r27hds,1637882297.0,False
r27hds,Doesn't take any energy to store things at rest in a non-volatile medium (aka disk). Shuffling it to and from the disk is what uses energy.,hm3biuv,t1_hm3ayev,1637882584.0,False
r27hds,"Good point, that makes sense",hm3bpbi,t1_hm3biuv,1637882674.0,False
r27hds,"physically, why does it take more power to store more text?
edit: assuming the text has already been downloaded",hm3d7vt,t1_hm3ayev,1637883438.0,True
r27hds,"For something like this (long term archive?) I would look at technologies known to last a very long time, and work backwards to find the needed power.

I'm not very well versed in hardware, but my first thought is something like a mask ROM, where the data isn't editable, but also is burned into the structure of the chip.  I've got games cartridges with this tech that are 30 and 40 years old with no signs of deterioration at all, I don't think 100 years is a stretch.  I don't think it's very memory sense, and probably not low power either (5v TTL? Idk) but there must be a more modern iteration that is more dense and efficient but similarly reliable, and I don't know if off the shelf, consumer grade 3.3v flash memory on an Arduino will survive that long without data errors. 

For display, you could use something like a 1-line character LCD, like used in simple solar calculators, or even an e-ink display.  I know those are fairly low power, and last decades with care.",hm5f8cp,t3_r27hds,1637931668.0,False
r27hds,"For display, eink is likely your best bet. It only needs power to change. It does not need any power once the image is ""set"".

DNB batteries are snake oil for any general usage application. They output extremely low power over an extremely long time. They are not meant for random bursting use like a consumer device. Unless you are planning on making extremely low power, extremely long life, extremely feature limited, devices that do exactly nothing 99.9999% of the time then it's best to forget they even exist.

Pie-in-the-sky theoretical ""wonder technologies"" almost never pan out, and when they do it's almost never anywhere close to the imagined uses of how the mass media reported on it.",hmblflb,t3_r27hds,1638048818.0,False
r27hds,thanks for the explanation. would it be more practical to use a solar panel?,hmbm1xy,t1_hmblflb,1638049087.0,True
r2cwfc,I have never heard the term data width before.,hm467w8,t3_r2cwfc,1637898725.0,False
r2cwfc,"I’ve heard of “wide data” vs “tall data” in a data science context(ie looking a tabular data, the relative ratio of rows to columns). Doesn’t really have anything directly to do with bandwidth as far as I know.",hm5biqf,t3_r2cwfc,1637929048.0,False
r20mys,"You can reach out to the researchers, you can post on stack exchange, you can go learn the math that the paper is using on Khan academy or something.

Those are my best suggestions.",hm1za86,t3_r20mys,1637860883.0,False
r20mys,"Thank you for the reply. I have a few questions.

Do you think it matters how much I know on the subject before I reach out to the researchers? I want to move on this quickly, but I’m also worried they may not wish to engage if they believe I’m novice in the subject. Not sure if that’s common.

Do you recommend exploring anything on stack exchange in particular? I’ve used stackoverflow plenty, but am not familiar with its cousin.",hm5jdh2,t1_hm1za86,1637934254.0,True
r20mys,Yes. Eastern Promises is brilliant. Second this. For a long time ago but it didn't run as expected.,hmdruai,t1_hm5jdh2,1638090767.0,False
r1lsow,"Most of the time, we talk about worst-case complexity. We also only really care about large data sets. In those scenarios, the highest order function dominates lower ones, ie if a function is say... O(nlog(n) + n\^2) then we can say that it's really O(n\^2).

Assessing complexity is mostly about the relationship between the algorithm and the growth rate of data. An algorithms class shouldn't teach you memorization of the major algorithms in different areas, or the runtimes of those. Instead, it's about learning how to develop and assess algorithms, and the different strategies that led to the development of the major algorithms. It should also teach you how to reduce problems down into simpler ones, or transform them into a similar problem with a known or easier solution.

The overall complexity of your algorithm will depend entirely on the most complicated step, which is your for-loop that will run the sqrt(n) times.",hlzl113,t3_r1lsow,1637809626.0,False
r1lsow,"Makes sense, we take the worst case of all discrete cases and use that. Thanks.",hlzuxe1,t1_hlzl113,1637814540.0,True
r1lsow,"Additionally, it is worth noting that asymptotically what you are doing with the 6n + 1 case is improving the best case of the algorithm without reducing the worst case (which is likely not possible in the general case).

This means that without the optimization you would be able to characterize the algorithm as having a complexity of Theta(sqrt(n)), but with the optimization your best case becomes Theta(1), meaning the complexity of the overall algorithm becomes Omega(1) and O(sqrt(n)). This kind of thing is pretty common in algorithms, where you can solve a specific subset of problems really quickly, but cannot solve EVERY possible problem quickly.

Asymptotics can be confusing because they are really about mathematical functions, not just algorithms, but the general logic to follow is that you cannot say you are reducing the runtime of an algorithm unless you do it for all possible inputs.",hm0azxc,t1_hlzuxe1,1637823899.0,False
r189ka,I would love this as well!,hlze5en,t3_r189ka,1637806499.0,False
r189ka,"Well, writing such a book is dangerous 😂",hm03r38,t3_r189ka,1637819364.0,False
r19gul,"Here you go:

https://www.cs.usfca.edu/\~galles/visualization/",hlz7i6q,t3_r19gul,1637803534.0,False
r0ss7l,"OH, I think I know why. Is rosetta converting apps before running them into code it can run, whereas running linux in a VM is a constant task? Even then, there are still questions.",hluexve,t3_r0ss7l,1637717255.0,True
r0ss7l,"Rosetta is doing several different things that simply ""run Windows or Linux in a VM on Apple silicon"" can't easily do.

- First, as you say, there is ahead-of-time recompilation of x86 code to ARM code to allow apps to run without being interrupted by a just-in-time recompiler.
- The M1 CPU has custom features for emulating the x86 memory model, which aren't part of ARM normally. Rosetta uses these in its recompilation.
- Dynamically linked system libraries need no translation, so many components are still running natively, not being recompiled.

By contrast, a VM probably can't compile ahead-of-time, and may not even be able to JIT much without choppiness and memory overhead, it has to translate the entire running OS rather than just a single userspace binary, and it may not be able to access certain internals of MacOS or of the M1.",hlv5pk6,t1_hluexve,1637730489.0,False
r0ss7l,"It's called [Rosetta 2 because it's a *translation system*](https://en.wikipedia.org/wiki/Rosetta_Stone) between different processor instructions. Translating a book ahead of time is a lot less of a hit on overall performance than translating each word as you go along. That's why the first launch of a Rosetta-run app is slower, but subsequent runs are much faster.

https://en.wikipedia.org/wiki/Rosetta_(software)",hlx2bii,t1_hluexve,1637771947.0,False
r0ss7l,"AOT vs JIT essentially, and Apple have the time and resources needed to make it as performance as possible.

Basically Mac translates the app as much as it can, and then emulates the bits that aren't already translated.",hlv4ieg,t3_r0ss7l,1637729815.0,False
r0ss7l,"Because an app isn’t a whole OS. An operating system is a kernel, drivers, modules, tons of apps and schedulers and timers all working together.

QEMU or whatever emulator you use has to actually emulate a working x86 processor, reserve contiguous memory etc, not just a subset required to run a single app, but every single instruction in a continuous loop, translate it and run it typically using a smaller instruction set. The x86 instruction set is very complex.

It’s why emulating a Power or MIPS processor on x86 to boot Linux also runs like dog shit even though the design is decades old.

There are people working on getting Linux working natively on Mac, that will save a lot of that translation step.",hlvdn8x,t3_r0ss7l,1637735252.0,False
r0ss7l,"I'd say those are two entirely different kinds of problems.

Running code compiled for one instruction set architecture on a different ISA usually comes at a steep performance cost since perhaps the most straightforward solution -- emulating the other CPU with software -- is slow. I don't know how Rosetta works in detail, but since it's either JIT or AOT (ahead-of-time) compiling the x86-64 machine code to its native ARM code, and the compiled ARM code is then run on the CPU, it can achieve much better performance than outright emulation.

Other than that, an application running through Rosetta is still code that has been built to run on an Apple OS, running on an Apple OS. Apple knows all the system calls and any other macOS APIs that an application can make use of; it's their design after all. Since they (probably?) still have the implementations of those same APIs in their OS, now just running on ARM, Rosetta can just mechanically make whatever modifications are required to the machine code so that the system calls now work on the ARM version, and re-link the application executables with the ARM version of any library ABIs provided by the operating system, and whatever else they need to do. (I don't know if re-linking is what they're doing, and I'm speculating, but the point is that once they've got the translation logic done right and all the corner cases working, it's a mechanical translation. Most of the APIs themselves are probably still identical in content. The implementation of the binary interfaces just differs, and the difference is systematic.)

It's still a nice piece of work, but Apple essentially knows and controls all the pieces of the puzzle. They also had time to prepare.

Supporting a modern operating system on a particular piece of hardware is a different thing. An OS needs to be able to work with the specific hardware on which it's running. This requires not only compiling the code for the correct CPU instruction set, but also being able to properly communicate with any other device-specific hardware such as the GPU (integrated in case of the M1) and the motherboard chipset. A modern operating system also needs to deal with power saving modes for all of that, and so on.

Writing the code to do this, not to mention figuring out how the hardware works exactly if no detailed public documentation is available, is a lot of work that has to be done for each individual component that a machine might have. Also, I don't know what if any documentation regarding these on the M1 is publicly available, but Apple isn't exactly the kind of a company to go out of their way to release technical information they don't need to.

The same is of course fundamentally true about running Linux or Windows on any other random hardware regardless of the CPU architecture. However, the new Apple hardware brings in a lot of new components to the mix, and there probably isn't a lot of public documentation available for non-Apple developers. Also, if it were some random laptop from some random brand not working well with Linux, you just wouldn't have heard about it. You hear about the M1 because it's Apple.

--
--

TL;DR: Rosetta needs to solve running (application) code from one CPU instruction set architecture on a different CPU with a different instruction set; running Linux or Windows on the M1 comes with all the usual problems of supporting random undocumented hardware.",hlx33x1,t3_r0ss7l,1637772259.0,False
r0ss7l,"What if I told you that there's an app that let's  you run an application whatever OS you use? if you're having troubles with your emulator, I'd suggest you try [this](https://www.shells.com/l/en-US/)  since it's the closest app that I can think of that can be really beneficial for you.",hlw3rp3,t3_r0ss7l,1637755331.0,False
r0ss7l,"It will get better. 

Rosetta is translating a single application and there will be a ton of optimisation because its translating binaries for Apples own OS.

Proper Linux and hopefully Windows ports will come to ARM making this all less of an issue. Virtualising whole x86 machines will probably improve too but as the ARM versions improve it will probably make sense to emulate those so the CPU architecture is the same.

I always preferred bootcamp/dual boot for real world use, direct hardware access for graphics etc. I’d be happy with that for consumer use. 

I think for now if you have taken the plunge with M1 then you need to be happy with MacOS but options will open up. Apple are always innovators, only someone like them who can do the whole thing from the chip upwards can make it immediately beneficial, which is thanks to their closed system design. They are effectively pushing the industry forward like they did in the 70s and 80s. ‘Build it and they will come.’",hlza1bx,t3_r0ss7l,1637804659.0,False
r0ss7l,Noteseeee x84 ctm jajajaajajajajajajajaa,hlujnzf,t3_r0ss7l,1637719469.0,False
r0ss7l,"X86_64, colloquially known as x84",hlujsn0,t1_hlujnzf,1637719529.0,True
r0ss7l,"Colloquially x64, not x84.",hlv3vbc,t1_hlujsn0,1637729457.0,False
r0ss7l,"105 IQ, huh? 🤔",hlw14ws,t1_hlujsn0,1637753453.0,False
r0ss7l,Saleeee wn jajajajaa,hlujw68,t3_r0ss7l,1637719574.0,False
r0eadx,Cool idea. Do you consider English translation?,hlsnhnl,t3_r0eadx,1637690834.0,False
r0eadx,"Yes, in a few days.",hltjf2w,t1_hlsnhnl,1637703352.0,True
r0eadx,The website is now in English as well : https://www.codepuzzle.io/en,hrrr9xe,t1_hlsnhnl,1641645402.0,True
qzwb8e,I think one month would be very challenging. Do you have an existing data set upon which to build the search?,hlotm0b,t3_qzwb8e,1637616697.0,False
qzwb8e,"So to clarify, this doesn't have to be a search engine that can handle anything I throw at it from halo to food science papers. The goal is just to demonstrate my ability to build such a feature. I think my uni can provide me with datasets but I really want to build my own crawler and parser. Of course the demo will be carefully crafted to show what the search engine can do with the right data set and I'll be completely transparent about it.",hlou0wl,t1_hlotm0b,1637616865.0,True
qzwb8e,"If you have the existing data, and it is not too absurdly big it might be possible. There's kind a couple of interesting problems for a project that would be good. For example, building an effective data structure to facilitate the search. The actual search algorithm could be interesting, especially if you incorporate some machine learning to do some recommendations of related material. It is doable, no mistake about that, but it is a pretty big project for a month. I wonder consider using an existing web crawler and focus on data storage and search. Overall, just try to keep the scope manageable. :)",hlounxk,t1_hlou0wl,1637617122.0,False
qzwb8e,"You're probably right, given how I need to take classes in this month as well, so I can't devote all my time. Plus, I can always build a web crawler later on and incorporate it in this search engine. 
Unfortunately, all machine learning stuff is off the table as I don't know anything about it and I am pretty thorough with my learning, so it'd take me at least 2 months to get a decent grip on it. I do have a course on ml, later on in my degree so I'll probably incorporate some rudimentary ml techniques in this project after that course.",hlovlli,t1_hlounxk,1637617508.0,True
qzwb8e,">Unfortunately, all machine learning stuff is off the table as I don't know anything about it and I am pretty thorough with my learning, so it'd take me at least 2 months to get a decent grip on it. I do have a course on ml, later on in my degree so I'll probably incorporate some rudimentary ml techniques in this project after that course.

When you do decide to have a look at those things, [watch some 3Blue1Brown for the high concept background information](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), and also [have a gander at one of the most commonly used libraries in the field, for the practical information](https://pytorch.org/tutorials/). I've watched the 3Blue1Brown videos and found them as useful as good university lectures even when you are just watching them to slightly increase your understanding and most of the math goes over your head*, and pytorch is a gold standard which is on my list just for the experience even though I don't use python much. If you are comfortable with such personal goals as building a search engine then this is fairly reasonable and probably worth your time.

*It certainly did for me, but I got enough of it to grasp the idea and to know that I could learn it if I took the time, which I certainly will in the very near future.

Edit: I hope it's appropriate to hand out resources like that. I'm sure someone will find them useful.",hlp7vum,t1_hlovlli,1637622803.0,False
qzwb8e,"Thanks man, I'll definitely take a look at them once I get the time or my ml course starts",hlrlbak,t1_hlp7vum,1637674825.0,True
qzwb8e,"However, if you still thinks so then let me know what you think would be a better approach?",hlouli1,t1_hlotm0b,1637617094.0,True
qzwb8e,"I would focus primarily on the data structure aspects. I think they're interesting enough for a (2nd year I'm guessing) project. Non-trivial for certain, and doable within the time frame.",hlow57j,t1_hlouli1,1637617731.0,False
qzwb8e,"Some terms you can search for: Information Retrieval system, Inverted index, PageRank, Web crawlers",hlotqb6,t3_qzwb8e,1637616746.0,False
qzwb8e,"Currently I am planning to build one using the inverted index strategy. I do want to include page rank as well. My thought process was to crawl the web, build a big enough dataset by parsing the pages I crawled, apply pagerank on that dataset and store it in order of their score. When a query happens, I use an inverted index strategy to query the webpages and display them according to the pagerank score I stored earlier.",hlougfz,t1_hlotqb6,1637617037.0,True
qzwb8e,"Hi we had a course on big data last year. To break down one approach is the following.

1. Get Hadoop set up on a system. If you have a cluster available by your school/university, definitely request access, as it will massively increase what you can do for this project.

2. Once set up, build a map reduce job. This is the most important part. When you work with big amounts of data, you need some way to quickly traverse this data, and filter only those relevant results to be displayed. An example dataset could be  found at https://commoncrawl.org. You can take an entire segment of the entire set if you can get a large cluster. NOTE THIS IS MULTIPLE HUNDREDS OF TERABYTES. Otherwise use the indexer to find a smaller sample dataset. 

3. Now how do you map reduce. The idea is simple: You have several cycles where each element is mapped, filtered and shuffled throughout the entire Hadoop cluster. These operations often can be done in parallel and are really trivial by themselves. Important is to ensure that you bring the data to the point of computation, not the other way around. Network traffic will be a large bottle neck. Instructions on how to do this are available online.

4. You could do many, many things now to optimise this process. Map reduce is by no means the end of big data. But it’s a good start, especially for a tiny project.

If this is too much for a small project consider doing a part of it, or just setting up a tiny Hadoop server with a toy example of the search engine!

Good luck",hlph3vq,t3_qzwb8e,1637627059.0,False
qzwb8e,I'd set up elasticsearch and build an interface to it,hlp56pt,t3_qzwb8e,1637621612.0,False
qzwb8e,"Correct me if I am wrong, but that is already a search engine right?",hlpu50c,t1_hlp56pt,1637633181.0,True
qzwb8e,Correct. Upon reading your post that's not exactly what you're looking for,hlpvey5,t1_hlpu50c,1637633773.0,False
qzwb8e,"Actually, no. The goal isn't to have a search engine, but to build one so I can learn more about it and the decisions that go behind creating something like this; specially when it comes to the storage of dataset to allow quick retrieval. While elastic search would build a search engine, it wouldn't achieve those goals",hlqydzr,t1_hlpvey5,1637657814.0,True
qzwb8e,"A quick idea:

Updating:  
Your engine should be informed about any new title/tag ""t"" entering the ""network"". t gets added to a hashmap.

Retrieving:  
A few waves of search for ""s"" in the hashmap, starting from most important to least.  
Search one:  
look for exact matches of s  
Loop as many times as you think it's necessary:  
generate similar strings to s, s'. Search for s'

Notes:

* every next loop is on s', s'', s''' etc
* if your ""network"" was created first, you gotta search through the entire thing unfortunately and update your map

Edit: You can have an in-between wave between s and s', which uses a dictionary to find similar words, or correct the ones you already searched for.",hlptjm3,t3_qzwb8e,1637632918.0,False
qzwb8e,Are you talking about a vector search technique here?,hlrlpfv,t1_hlptjm3,1637675027.0,True
qzwb8e,"I made it up, what do you mean by vector search technique? Can I have a link?",hlro95g,t1_hlrlpfv,1637676332.0,False
qzwb8e,"Vector search is [like this](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/), basically you build vectors to represent your 'objects' then compare them to find the most 'similar' vectors (eg those closest geometrically, or with the smallest angle between them).

I do think the hashing approach you made up is similar (although not the same) as [locality sensitive hashing (LSH)](https://www.pinecone.io/learn/locality-sensitive-hashing/), but in that case you're using a hash function specially designed to hash similar items into the same bucket, so when you have a search query you hash it with LSH and identify which other items were hashed to the same bucket.",hlrq5xq,t1_hlro95g,1637677259.0,False
qzwb8e,"First of all thank you for the valuable information!These are topics I am very unfamilliar with, but what I presented above can be implemented in any way you like, why not.What I can think of out of this is that if you manage to somehow create a Vector-space with meanings of words, It would make the search engine even greater as it would not be limited to spelling. But it's good to mix it with searches of similar words grammarly, because someone can mispell something that probably doesn't exist in a dictionary e.t.c.

LSH sounds awesome, but won't there be a lot of collisions in your hashmap?",hlrswzg,t1_hlrq5xq,1637678519.0,False
qzwb8e,"Yes exactly, there was a [cool example recently](https://youtu.be/7RF03_WQJpQ?t=150) which implemented both BM25 and NLP models for searching through wikipedia on 'what is the capital of the US?', and the BM25 approach comes up with a load of random things that contain 'US', 'capital', etc - whereas the NLP models pull in the right responses.

On LSH yes you can get too many collisions, you have to increase the number of possible buckets until you reach a point where there's not too many in each bucket - you increase the number of buckets by increasing the number of bits (which I think of as being similar to increasing the resolution in photos)",hlsxihu,t1_hlrswzg,1637694737.0,False
qzwb8e,https://youtu.be/mu6ExLCtFsQ,hlrul14,t1_hlro95g,1637679257.0,True
qzwb8e,"Besides the pseudo algorithm I'd like to add:  
No it won't be much more difficult than a sophisticated dictionary structure regardless of implementation. Unless if you have to deal with the real internet which will land a lot of complications along the way and require a lot of computational power, memory and money.",hlpw8v7,t1_hlptjm3,1637634165.0,False
qzwb8e,"A search engine is a variable beast - there are a whoooooole lot of steps to take, each step of which is technically a search engine, and notably **does not have to operate on websites to be one**. One that worked on your lecture slides for the class would be just as valid.
 
First, you have the most basic 'return by exact match on field'. You have a database, you go through that database and pluck out the matching elements by title, or keyword, or author.
 
Next, you can apply rules to those searches - This AND That AND The Other Thing OR Something Else. Many search engines are sitting here, for example https://www.scopus.com/
 
The step after that is 'content based' searching, which is going to look at the body of the material and look through there, including all your rules and whatnot (this is not difficult for function, but is a nightmare for optimisation).
 
Next after that you have internal, implicit rule generation. That is things like implicit 'stemming' of words, synonyms or close to them, and other basic natural language processing 'pre-processing' steps. There is still no machine learning at this stage, just related steps you would normally do before throwing it into your algorithms. (note: this is probably the step I would recommend for a good 2nd, average 3rd year programming assignment in a data science curriculum, perhaps excluding body content).
 
After that you add machine learning for natural language processing. You start looking at the connections between words, related topic, density of the topic in the elements being found, working out subject/object, etc etc etc. This has no indefinite end and is where Google spends a significant amount of their time and money.
 
But that's just core functional capabilities. You have non-core and non-functional capabilities as well, of which the most important are probably weighting and optimisation. Weighting is being able to take your matches and assign them a weight. Easy at the lower ends of core functionality (how many rules did they match, and how often do they match them?), when you get into the pre-processing you need to look at how much pre-processing was necessary - so if you searched for 'speedily', then the ranks for matches would probably go speedily->speed->fast, for example. And when you're into NLP land it gets much more complex.
 
Optimisation is working out how to sort out your indices, sharding etc etc to support not just searching, but very rapid searching, which is a whole different ballgame and involves a whole lot of different techniques.",hlpto9w,t3_qzwb8e,1637632975.0,False
qzwb8e,">Next after that you have internal, implicit rule generation. That is things like implicit 'stemming' of words, synonyms or close to them, and other basic natural language processing 'pre-processing' steps. There is still no machine learning at this stage, just related steps you would normally do before throwing it into your algorithms. (note: this is probably the step I would recommend for a good 2nd, average 3rd year programming assignment in a data science curriculum, perhaps excluding body content).

So basically you mean I should search for words using synonyms, but only do so in the title?",hlrp8di,t1_hlpto9w,1637676810.0,True
qzwb8e,"And keywords, if you can get them. If you're doing web pages these are in a meta tag.",hltuurp,t1_hlrp8di,1637708060.0,False
qzwb8e,"If I am building a search engine that doesn't work on the internet, how do I assign a score? I mean websites could be ranked using some page Rank or some other variation of it, but if we are taking something that works on lecture slides or something else, how do I give them a score?",hlrpn2w,t1_hlpto9w,1637677011.0,True
qzwb8e,Work out some criteria and base it off that. Number of times the search term appears is the easiest.,hltv030,t1_hlrpn2w,1637708124.0,False
qzwb8e,"Yep that seems good. Maybe I can even add a review field to it, to simulate the resource bank of a college where all the teachers have submitted their slides and we will be showing the ones that are rated higher by the students at the top.",hlu3su3,t1_hltv030,1637712047.0,True
qzwb8e,"There are plenty of options, if you're going to go for a fast 1-month project you might want to go with elasticsearch/BM25 etc, other commenters have covered this so I won't repeat the same.

If this is a passion project and you decide to do more on it (or maybe this is within your scope anyway), Google relies more and more on ML/AI methods in their search to allow you to search with meaning/concepts - Elasticsearch and BM25 will not be able to do any of that for you, they rely more on word matching (which does still work well, but means that you need to choose the correct words). If you're interested in the ML/AI version, you need two 'pillars' - vector similarity search and NLP models (typically transformers like BERT).

It's super fascinating imo and worth looking into, for the NLP models intro I [wrote this](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/), and for the vector similarity search (or 'semantic' search when used with NLP) [I wrote a 'course'](https://jamescalam.medium.com/free-course-on-vector-similarity-search-and-faiss-9b3e91a91384?sk=b5cb406932f62c1fb69eb0944efba92c).

Whichever route you take for your first month, I hope you at some point have the opportunity to explore the AI/ML approach because it is fascinating.",hlqnw8p,t3_qzwb8e,1637649417.0,False
qzwb8e,"That is the correct one. You don’t need to learn about databases or hadoop to learn about how google does it. 

BM25 and Transformers (aka BERT* variants) are how Google achieves search results.",hlqvqdp,t1_hlqnw8p,1637655533.0,False
qzwb8e,"I wouldn't bother with PageRank or other extra features for a month long project. I would focus on the very core of the idea of a search engine.   
Mainly I would care about implementing BM25 based scoring, inverted index and some method to deal with spelling errors. Finite State Transducers are a great fit here and the one by burntsushi is great for this use case. BM25 scoring is simple enough to implement I think, and I believe burntsushi's library does provide some levenshtein based spell corrector. 

Writing this up should be possible in a month or so in my opinion, I say this because I built the above system in a month or so.",hlph71q,t3_qzwb8e,1637627101.0,False
qzwb8e,"I have literally just done a search engine data structures assignment.

We didn't build it from scratch, rather we had to complete it in parts and do exactly what the assignment specs had told us to do. For example, using a certain algorithm etc.. 

We used the pageRank method (you can google this) and sorting algorithms to produce search results from key words. We didn't web-crawl or anything like that, rather they provided us with files with url links, and then we had to search those files and access those url links and read the pages.",hlq410f,t3_qzwb8e,1637637858.0,False
qzwb8e,"I would just take a look at their first paper on it: http://infolab.stanford.edu/~backrub/google.html

I think you could build a version of that system.  This goes over the various pieces they use and how they use them.",hlsss2h,t3_qzwb8e,1637692899.0,False
qzwb8e,"Hey, so I took a look at this and didn't find much information on the actual implementation of the data structures. Any ideas on where I can find that? Not expecting the actual code here. They mentioned they have designed the data structures to reduce disk writes but there was no explanation as to how they did it",hmlw8hw,t1_hlsss2h,1638236553.0,True
qzwb8e,"I would narrow the scope of what your search engine can find like a search engine for plants, or a search engine for  cars, then use Google to build your dataset.",hm86go2,t3_qzwb8e,1637979687.0,False
qzwduc,Build different sorting algorithms. There's a reason they show up so commonly as early assignments in a CS degree. They are very good practice for working with lists and logical thinking.,hlovby3,t3_qzwduc,1637617396.0,False
qzwduc,Are you talking about bubble sort kinda stuff? I have already built almost all of them and I wanted some nice questions that show me how they'd be used in a real world scenario.,hlovtyt,t1_hlovby3,1637617604.0,True
qzwduc,You could build a basic accounting software.,hloxc0t,t1_hlovtyt,1637618224.0,False
qzwduc,"Try LeetCode webpage. It's mostly for interview questions, but there are also beginner level stuff and general DS&A problems. For example, implementing your own linked list and then reversing given one, sorting or merging two sorted into one big sorted.",hlpd6fw,t3_qzwduc,1637625210.0,False
qzwduc,This is exactly what I am looking for. Should I take one of the paths they offer?,hlrkfa1,t1_hlpd6fw,1637674358.0,True
qzwduc,You can try competitive coding on platforms like codeforces and codechef,hlr2eb1,t3_qzwduc,1637661380.0,False
qzwduc,"Try pepcoding 

Start from the basic problems of DSA 1 

even if you think that you can solve those ""easy"" problems, still put in the effort and solve everything. It'll take you 6-7 months to complete all those problems.",hlrhbq5,t3_qzwduc,1637672625.0,False
qzwduc,"Try doing a [WAVL Tree](https://en.m.wikipedia.org/wiki/WAVL_tree).

They are somewhat of a halfway point between AVL and red-black trees in terms of performance.

AVL trees have height at most 1.44 * log(n).

Red-black trees have height at most 2 * log(n).

WAVL trees have height at most 1.44 * log(n) if only insertions are performed but can get closer to 2 * log(n) if deletions are performed. It will build the exact same tree as an AVL tree if only insertions are performed.

AVL trees need at most log(n) tree rotations per operation.

Red-black trees need at most a constant number of tree rotations per operation.

WAVL trees need at most log(n) tree rotations per operation, however amortized over time it is a constant number.

Here is the original [paper](http://sidsen.azurewebsites.net/papers/rb-trees-talg.pdf) which details the operations.

As far as the difficulty of implementation I would say they are again somewhere between AVL and red-black trees lol.",hlrd1jq,t3_qzwduc,1637669983.0,False
qzwduc,Solve algo class exercises from other universities. They are usually in depth and require a lot of thinking,hlrtw88,t3_qzwduc,1637678954.0,False
qzwduc,I am in a uni myself and I do solve the assignments myself. The reason I asked this was I wanted to find individual questions giving me an idea of what can be done with a Ds concept like reversing a list. Leetscode and similar platforms seems good enough.,hlrzv5s,t1_hlrtw88,1637681537.0,True
r02lfl,Reverse-engineer the problems existing websites solve and come up with your own solution on pencil and paper?,hlskagy,t3_r02lfl,1637689611.0,False
r02lfl,"The whole point of studying algorithm design and analysis is knowing if they work and how they behave asymptotically without having to test them first.

Do as Dijkstra, and program on paper.",hltok5n,t3_r02lfl,1637705401.0,False
qzo8bt,"I learned scalability and software design patterns over a fairly long period of time by osmosis, so I don't really know of any good resources to help you in that regard, but I do want to say something about the OOP hierarchy problem that you mentioned, which was one of the first non-trivial design problems I encountered when learning OOP.

The trick to that particular problem in OOP, as I see it, is in recognizing inheritance as a secondary feature of the paradigm (the primary feature being just polymorphic message passing). My first language was modular and procedural, so I actually had a bit of difficulty learning OOP as a concept (with Java 6) probably due to the fact that I was so used to thinking procedurally. Whatever the reason for my troubles, learning Smalltalk, with which I quickly replaced Java as my main OOP language, was immensely helpful in elucidating for me the core of OOP and thus in helping me to gain an intuition for what complex OOP software should look like from a high level.

All this to say that focusing upon inheritance relationships (aka ""is a"" relationships) tends to result in hierarchies that are difficult to follow or even to formulate in the first place, which is the issue you mention. Instead of using inheritance, try using interfaces, (or whatever ad-hoc class level polymorphism your language provides). This is a key element in an approach to OOP that is often referred to as ""composition over inheritance.""

For a while, try writing OOP programs using no inheritance at all (other than mandatory inheritance, like from Object in Java, ofc). I don't believe that inheritance is always evil, but refusing to use it is a good way to train yourself to not have to, and thus to become more flexible and better able to accurately model the domain of your software.

Alternatively, I guess you could do what I did and just write really shitty software for a while before eventually figuring it out, but your approach, asking for help and resources, seems to me the definitively smarter way to go about it.",hloj44f,t3_qzo8bt,1637612465.0,False
qzo8bt,"[Design Patterns](https://en.wikipedia.org/wiki/Design_Patterns) is the canonical reference for the patterns themselves, it's mainly a catalogue, but there are use cases there too from what I recall.  Still, for me, this text and any of a number of videos or tutorials on the subject do little to instill in me the ability to recognize when to use what pattern beyond the more simple ones.  I guess it just takes a lot of practice, and prior to that learning the patterns.",hlovoo1,t3_qzo8bt,1637617542.0,False
r07asz,[Wikipedia](https://en.wikipedia.org/wiki/Parallel_ATA) to the rescue!,hlu7f0q,t3_r07asz,1637713728.0,False
qzkegn,IOT hacking by No Starch Press,hlmy2nv,t3_qzkegn,1637588549.0,False
qzkegn,"Ok thanks! 
Does it contain application and importance of different devices?",hln2sc5,t1_hlmy2nv,1637590961.0,True
qzkegn,"Not sure, sorry",hlnjro6,t1_hln2sc5,1637598351.0,False
qzkegn,Ok thanks... I'll check it out anyway!,hlnvhae,t1_hlnjro6,1637603041.0,True
qzkegn,Building the Internet of Things by Maciej Kranz,hlnc3nl,t3_qzkegn,1637595184.0,False
qz9jwv,"Not an expert, but I do know that kernel programming is very difficult due to the fact that encountering a bug in the kernel means it’ll likely crash to the metal. Plus, recompiling kernels takes a very long time.",hlobf17,t3_qz9jwv,1637609361.0,False
qz023c,"You can find a lot of literature on Google Scholar on any of those subjects.

&#x200B;

For example, [https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=genetic+algorithms&btnG=](https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=genetic+algorithms&btnG=)

&#x200B;

I'd suggest limiting the search to the 1970s, 1980s and 1990s to see the foundational papers. E.g., Holland on Genetic Algorithm.",hlj81jx,t3_qz023c,1637516772.0,False
qz023c,Thanks a lot 👍,hlj8xs4,t1_hlj81jx,1637517133.0,True
qz023c,"No worries. I was in a meeting so my last reply was short.

I'd recommend Holland, Back, Grefstennette as all good authors on GA and evolutionary algorithms. Mitchell is good as well for AI/ML in generall.

A good paper on Bayesian inference. Mitchell covers it as well

[https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1968.tb00722.x](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1968.tb00722.x)

A could of good paper on hyperparameters.

[https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)[https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)

&#x200B;

If you decide to get into optimization, then make sure to read the ""No Free Lunch Theorem."" Very important and generally poorly understood.",hljb5pg,t1_hlj8xs4,1637518035.0,False
qz023c,"Holland's paper is really good so far (although there is a surprising amount of small typos). I'm just starting it, but it's obvious that this is related to machine learning (talk of hill climbing algorithms in the first page). 

Thank you for posting this and your other comment as well, and thank you to the OP for bringing this up. I'm going to make a week out of this.

Update: [This paper is really good, actually.](http://www2.econ.iastate.edu/tesfatsi/holland.GAIntro.htm) I've been able to reproduce some of the easier parts of it in Kotlin and it is very neat once you see it in action. In a few weeks or months I'll post a link with the full reproduction of the Prisoner's Dilemma example, because that's a very good one which teaches you the fundamentals of building a little framework in which to solve problems in this way. 

It's so good I've decided to order one of John Holland's books (*Signals and Boundaries*). Very neat stuff. Check that paper out if you're in to this stuff. I'm gonna be having a field day with it for a few weeks at least. Definitely a subject I want to study in depth.

Update 2: I've decided to make a little library around this idea for learning. Early stages yet, but I've implemented many of the ideas from the paper. Feel free to take a gander, as I have open sourced it. Just bear in mind that it is early stages and a prototype. It may prove helpful to anyone interested in grasping how Genetic Algorithms work (at least according to that paper): [Genetic Playground](https://github.com/sgibber2018/GeneticPlayground).",hljq1fv,t1_hlj81jx,1637523826.0,False
qz023c,"Cant help much with genetic algorithms or Bayesian optimization, but for reinforcement learning I strongly suggest  [Sergey Levine's video lectures](https://www.youtube.com/watch?v=JHrlF10v2Og&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc). Going from the basic algorithms and methods used into more difficult problems without shying away from the math involved.",hljqrzd,t3_qz023c,1637524123.0,False
qz023c,"Yep, haven't watched them, but also heard these are great from Sergey.

So is the ""RL: An Introduction"" Sutton & Barto book ([here](https://incompleteideas.net/book/RLbook2020.pdf)) or David Silver's (guy behind DeepMind's AlphaGo) [lecture series](https://www.youtube.com/watch?v=2pWv7GOvuf0) \- I used these two and I ended up publishing research in RL!",hlnigoe,t1_hljqrzd,1637597821.0,False
qz023c,Koza wrote a very detailed book or two on genetic programming https://www.goodreads.com/book/show/644125.Genetic_Programming,hlme97c,t3_qz023c,1637574105.0,False
qyv2sd,"Is this resource useful? 
https://ifs.host.cs.st-andrews.ac.uk/Books/SE9/WebChapters/PDF/Ch_27_Formal_spec.pdf

You may want to try this question over at 
https://www.reddit.com/r/learnprogramming/",hlih7cx,t3_qyv2sd,1637505411.0,False
qyv2sd,"Hey, thanks a ton. This helped a lot !",hlirgr4,t1_hlih7cx,1637509952.0,True
qyv2sd,Check out Agile Systems Engineering by Bruce Douglas. It’s fantastic.,hlkoklg,t3_qyv2sd,1637538345.0,False
qyo107,this is managed by the operating system. the OS retrieves the memory capacity from the underlying hardware via syscalls. it is responsible for allocating memory to running applications.,hlh9vz6,t3_qyo107,1637475022.0,False
qyo107,"Ah that makes a lot of sense, Cool! And I am assuming the operating system adds to the used memory variable as programs write to memory. I am also assuming it is dependent on priorities, like with Windows you can set the priority of a program and it will allocate more resources to that program if available",hlha3oz,t1_hlh9vz6,1637475168.0,True
qyo107,"operating systems can use several memory management models including single contiguous, partitioned, and paged allocation. this wikipedia article is good literature: https://en.wikipedia.org/wiki/Memory_management_%28operating_systems%29?wprov=sfla1

the priority in windows operating systems you are referring to is process priority, also known as ""niceness"" on Unix/Linux-based operating systems. priority/niceness modifies scheduling, which is an allocation of various resources including CPU time.",hlhc7vr,t1_hlha3oz,1637476673.0,False
qyo107,Awesome! Thanks for clarifying that and thanks for the help,hlhdhzh,t1_hlhc7vr,1637477617.0,True
qyo107,"Something along the lines of:

There are 4,447 places in memory info can be stored. Program x wants to use locations 1 - 47, thus 4,400 are free?

Except, that changes many thousands of times per second. Programs x, y, and z are all taking memory and letting go as needed.

Am i in the right ball park?",hljg0bs,t1_hlhc7vr,1637519931.0,False
qyo107,you're in the right ballpark. your example is one of contiguous allocation.,hljkgxl,t1_hljg0bs,1637521651.0,False
qyo107,"Hmm, does the OS retrieve the memory capacity from hardware via syscalls? My understanding is that syscalls are used to call priveleged functions in the OS (such as reading a file, or allocating memory) from functions in the user space, and that the OS keeps track of all the memory that it has allocated in additional data structures, which is how it keeps track of the memory usage. Correct me if I am wrong.",hljepaa,t1_hlh9vz6,1637519429.0,False
qyo107,"you're correct. my mistake, I just reread my post. the os queries the BIOS or UEFI for the systems memory capacity. it makes memory available to applications in user space and reserves memory for itself in kernel space. user space applications have no visibility to memory reserved in kernel space, though the os makes the capacity (amount) of that memory available to some applications via syscalls.",hljk1z8,t1_hljepaa,1637521495.0,False
qyo107,"Most memory and storage hardware doesn't really have a sense of full or not. There is something at every address. It's up to software to decide if that information is meaningful or not. The operating system breaks that memory up and gives it out (assigns it) to processes or to itself. The new ""owner"" of the memory can write useful stuff into it instead of whatever garbage happened to be there initially. The hardware can help with controlling which processes can and cannot write where.

If you'd like to learn more, you can Google for ""operating system memory allocator"" it's a pretty well defined topic so I bet there's good material out there for more details.",hlir6vz,t3_qyo107,1637509840.0,False
qyo107,How does a hotel know how many of its rooms are full?  It keeps track of them as they are occupied and records that.,hljavsd,t3_qyo107,1637517924.0,False
qyo107,"This is exactly the kind of question that is answered in an OS class. If your university has one, be sure to take it! If not, may I suggest Operating Systems: 3 Easy Pieces, which is a relatively easier book to read (as far as CS textbooks go), but covers all the main topics (and a few extra ones too)",hljefd3,t3_qyo107,1637519324.0,False
qyo107,great question !!,hljnld8,t3_qyo107,1637522854.0,False
qyaklu,[CoRecursive: Coding Stories](https://corecursive.com),hlepxnw,t3_qyaklu,1637430425.0,False
qyaklu,"That one is soooo good.  Very in depth, long enough to get pretty technical, not so long that it gets boring.  I've especially liked their ""Apple 2001"" and SQLite shows.",hlevysh,t1_hlepxnw,1637432924.0,False
qyaklu,"[Code Newbie](https://www.codenewbie.org/podcast), [Coding Blocks](https://www.codingblocks.net/), [Command Line Heroes](https://www.redhat.com/en/command-line-heroes), [Soft Skills Engineering](https://softskills.audio/)",hleu2c9,t3_qyaklu,1637432130.0,False
qyaklu,"I like Advent of Computing, but it is more of a deep dive into the history of computing.",hlghna7,t3_qyaklu,1637459187.0,False
qyaklu,"Not compsci but cybsec related darknet diaries are really good, very light in terms of tech but good to wind down with. Malicious life, privacy security and OSINT are very good if you like something I little more technical.",hleptmy,t3_qyaklu,1637430377.0,False
qyaklu,[Algorithms + Data Structures = Programs](https://adspthepodcast.com/),hlguxyy,t3_qyaklu,1637466025.0,False
qyaklu,Lex Fridman has some really good guests. Grumpy Old Geeks is kinda fun.,hlf9zf4,t3_qyaklu,1637439014.0,False
qyaklu,Wes Bos and Scott Tolinski's Syntax podcast is great for web development stuff. I'm not really into web dev but I still find it entertaining!,hlfblsq,t3_qyaklu,1637439742.0,False
qyaklu,Any body know any british coding podcats?,hlgzbs0,t3_qyaklu,1637468467.0,False
qyaklu,"https://oxide.computer/podcasts

https://www.softwareatscale.dev/?utm_campaign=pub&utm_medium=web&utm_source=copy",hlftond,t3_qyaklu,1637447917.0,False
qyaklu,Real Python and Django Chat are on Spotify.,hlhbtux,t3_qyaklu,1637476394.0,False
qyaklu,"Its not directly coding, but Darknet Diaries is really great :)",hly6zyq,t3_qyaklu,1637787902.0,False
qyaklu,What’s it about?,hm1k173,t1_hly6zyq,1637854255.0,True
qyyi89,If you’re creating new ways to do it and studying the theoretical side it’s computer science. If you’re dealing with implementing and adapting an already understood method to real world constraints then it’s engineering.,hliyz6i,t3_qyyi89,1637513043.0,False
qyyi89,"That's kinda a misrepresentation of the boundary between science and engineering in tech.

If you're researching the known limits of the field, you're doing science. If you're implementing scientific discoveries in industry considering costs and scalability, you're engineering.

Though it is always still under the umbrella field called Computer Science.",hljo9ef,t3_qyyi89,1637523118.0,False
qyyi89,One could also argue that it's just a subfield of math,hlix3j9,t3_qyyi89,1637512272.0,False
qys9kx,"I think you've drawn it oddly, which is causing the confusion.

Usually the ""fetch"", ""decode"" ""execute CMD"" is written on the X-axis or time-axis and the sequence of CMDs in the program along the Y-axis.

&#x200B;

See [https://www.sciencedirect.com/topics/computer-science/stage-pipeline](https://www.sciencedirect.com/topics/computer-science/stage-pipeline) for an example (I cannot figure out how to include a picture).",hlia05u,t3_qys9kx,1637501577.0,False
qys9kx,"An easier approach would be to make a line of all the instructions you are going to execute and calculate the time it takes for the last instruction in the line to pass the last stage in the pipeline. (See the image)

[https://i.ibb.co/qMBJ2MV/image.png](https://i.ibb.co/qMBJ2MV/image.png)

Circles are instructions. You have ""n"" of these

Squares are the pipeline with ""x"" stages

So the very last instruction has to travel a distance of (n-1+x) steps

All these take ""y"" nanoseconds each

So the total time should be (n-1+x)\*y nanoseconds",hlhv1ds,t3_qys9kx,1637491122.0,False
qys9kx,"Wait but that equation would mean it would take way longer to complete the program and therefore isn't staged?

I just dont understand what youre saying, could you describe it differently?",hlhx3rx,t1_hlhv1ds,1637492745.0,True
qytazd,search subnet questions and you will find them,hli6urh,t3_qytazd,1637499670.0,False
qy9ke0,"They're electrical, radio or optical pulses so if other pulses are in the same medium they can interfere.",hlejxjn,t3_qy9ke0,1637427891.0,False
qy9ke0,"There's [a whole field of philosophy that deals with the relationship of parts to the whole](https://plato.stanford.edu/entries/mereology/), so I'd imagine they'd have something to say about whether a collection of particles is a physical body.",hlepsbs,t1_hlejxjn,1637430362.0,False
qy9ke0,what is a physical body other than a collection of particles?,hlf6yqm,t1_hlepsbs,1637437662.0,False
qy9ke0,The fact that all physical bodies are collections of particles doesn't imply that all collections of particles are physical bodies. That's just logic. I'm super logical.,hlf870y,t1_hlf6yqm,1637438203.0,False
qy9ke0,"Hi, super logical. I’m dad.",hlg5yxk,t1_hlf870y,1637453573.0,False
qy9ke0,Take it. Take your upvote.,hlgh73v,t1_hlg5yxk,1637458964.0,False
qy9ke0,They certainly aren't metaphysical.,hlentlg,t3_qy9ke0,1637429529.0,False
qy9ke0,LOL GOT 'EM,hlepuao,t1_hlentlg,1637430384.0,False
qy9ke0,"They are physical occurrences, of electrical and em wave signals.

But they are not physical bodies. More analogous to sound waves, which are also physical but not objects.

Interference is like if I say something and you can't understand because some other noise is too loud to hear enough. Collision is like if I say something and you don't catch it because someone starts talking to you specifically at the same time, or I interrupted myself half way to say something else.

It's just electrical and electromagnetic signals not sound waves.",hlhbr4l,t3_qy9ke0,1637476339.0,False
qy9ke0,My network prof told us today an IP packet (single) is long kilometers,hlezbst,t3_qy9ke0,1637434335.0,False
qy9ke0,It's said that each IP packet is a separate miracle.,hlff3sc,t1_hlezbst,1637441314.0,False
qy9ke0,https://www.phys.uconn.edu/~gibson/Notes/Section5_2/Sec5_2.htm,hlepqjm,t3_qy9ke0,1637430341.0,False
qy9ke0,"Lol, TCP stack will retransmit anything lost, dont worry about lost frames or packets :)",hlipx74,t3_qy9ke0,1637509313.0,False
qy9ke0,Information is just an abstract representation of the motion of physical bodies,hliu377,t3_qy9ke0,1637511027.0,False
qykxoh,"The memory says eg ""hello, I am 16 GB"" when the bios starts. It is then known. There's no calculation at all.",hlgyz2v,t3_qykxoh,1637468261.0,False
qykxoh,"Ah okay kinda makes sense, the reason I thought it calculates the amount is because if you look at older machines you can see the amount of RAM count up until like 65536K for example

Edit: A good example I see is in my old Pentium 4 Machine with 700MB RAM. It counts up rapidly until it sees i think 768MB RAM and on screen says 768MB RAM OK or something similar to that",hlh0aep,t1_hlgyz2v,1637469029.0,True
qykxoh,"It's testing the memory for errors before booting. Modern computers can still do this, but the firmware in many cases is configured to skip the tests by default because they generally aren't thorough, modern users are generally impatient and expect startup to be quick, and there's specialized software for testing memory (e.g. MemTest86).",hlh0nt2,t1_hlh0aep,1637469247.0,False
qykxoh,"Yeah while playing a match of Dead By Daylight I realized that, Thanks for the help! Both of you",hlh1xfi,t1_hlh0nt2,1637469998.0,True
qykxoh,A slightly off topic comment but how did you learn about the computer architecture ? Is there a book / site that is helpful to get started with concepts related to building a computer ?,hlig6iv,t3_qykxoh,1637504907.0,False
qykxoh,"I learned from a book called ""But How do it know"" and a series that ben eater made on youtube: [https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU)

He has a lot  of really good videos on the topic that I think you may like",hll7x5j,t1_hlig6iv,1637547328.0,True
qykxoh,Apologies for the late reply - But thanks !,hm1qycq,t1_hll7x5j,1637857330.0,False
qyiizq,"Look at the relationships any gate operators relate to a gate symbol you can draw for a circuit diagram.

A cleanish way to draw the diagram would be to have inputs (normally a,b,c,…) along the top with lines coming down and then lines that divert off of that horizontally into the circuit gates. 

So if you have A+B in your truth table then you know you need input for A and input for B and an AND gate.

There are tons of images if you checkout google. Programming Circuit Diagrams",hlg7onb,t3_qyiizq,1637454386.0,False
qyiizq,"Well, if the function is multi-argument (more than two), I would go for the Karnaugh map in order to minimize the function.",hlg7uv2,t3_qyiizq,1637454466.0,False
qyiizq,I think circuit simulators should usually have an option to input the desired truth table and then automatically create the circuit. At least that is what I used to use in my university. The tool we used is 'logisim'. Search for it and you should be able to find a download link. I also think the software is free or open source. Hope that helps!,hlhmcro,t3_qyiizq,1637484177.0,False
qyiizq,I actually downloaded Logism however I was still curious as to what processes would be involved in converting manually for a deeper comprehension of the full adder. I'm not a computer science student or anything but I feel that it would be very cool knowledge to have. Thanks for replying.,hlkg746,t1_hlhmcro,1637534618.0,True
qyiizq,"Well I remember we did make a full adder in uni. I think we somehow first made a half 1 bit adder. Then a full adder. And I also remember the nice thing about logisim is that you could make your own components. So we took the full adder and made it a component with two inputs and two outputs(result and carry?). Not sure if it's called carry, but it is the overflow bit. So having this new component you could make any n-bit adder. For example a 4 bit adder.",hlkmg6h,t1_hlkg746,1637537394.0,False
qyiizq,"Introduction to the Logical Design of Switching Systems by h.c. torng 

It is an old textbook, but I think it is what you are asking for.  Because it predates all the computer-based methods of doing this, it explains everything from scratch.",hlkmnhw,t1_hlkg746,1637537485.0,False
qyiizq,"Damn 1967, that's an old textbook alright. Sounds like it would be useful though, I suspect there's nowhere online that I can view this however considering its age.",hlkr6in,t1_hlkmnhw,1637539527.0,True
qyiizq,"Indeed it is.  Looks like you could get a copy from Abe Books for just a few dollars though.  The edition I am familiar with is the 1966 one.  I suspect the later ones drop the chapter on relay logic design, since by then they were only used in elevators I suspect.  But if you want to build something like a 4-bit relay adder, it's essential.",hlkvr20,t1_hlkr6in,1637541618.0,False
qyiizq,Might be best to first use a KMap and get the simplified equation... then convert that in your head to a circuit diagram.,hlrlm2s,t3_qyiizq,1637674979.0,False
qyiizq,This is how you would do it by hand,hlrln1e,t1_hlrlm2s,1637674992.0,False
qxxaab,"No, because although I have a B.S. in Computer Science, I am a software engineer.",hlcl93b,t3_qxxaab,1637382293.0,False
qxxaab,"Yeah, this is me. Im a software engineer, even though my B.S in CS",hlcpbbt,t1_hlcl93b,1637384671.0,False
qxxaab,"computer alchemist, got it.",hleimju,t1_hlcl93b,1637427358.0,False
qxxaab,Same here. I guess if I was in the research world then computer scientist would be appropriate.,hlem6c0,t1_hlcl93b,1637428844.0,False
qxxaab,Same,hlf6s0s,t1_hlcl93b,1637437579.0,False
qxxaab,"Since I have a master's degree in CS, and not a PhD, I just refer to myself as a computer master.",hlcyp5j,t3_qxxaab,1637391016.0,False
qxxaab,"I have BS in Computer Science, but did not finish my Master’s degree. So I refer to myself as a computer slave, or on days when I have abandoned human and returned to monke, code monkey. 

https://www.cafepress.com.au/mf/4785794/warning-monkey-coding_sticker?productId=12172404",hle64t5,t1_hlcyp5j,1637421928.0,False
qxxaab,very cool,hldsla9,t1_hlcyp5j,1637414920.0,False
qxxaab,If you get a PhD you would be a computer doctor,hlekpy9,t1_hlcyp5j,1637428221.0,False
qxxaab,"Yes, depending on the context. Sometimes I just call myself a scientist or a medical researcher. It depends on the audience. The problem with telling laypeople that I am a computer scientist is they think this means I'm a programmer (and boy do they have a great app idea they want me to build) or a technician (and boy do they want me to fix their PC/laptop). As you may notice in this subreddit, we get a lot of such confusion with numerous tech support and programming posts. There is some overlap (especially with programming) of course, but a lot of differences as well. Anyway, TL;DR, depends on the context.",hlckoca,t3_qxxaab,1637381963.0,False
qxxaab,What do you actually do if not programming?,hlfhjp7,t1_hlckoca,1637442404.0,False
qxxaab,"CS subjects generally lays down the fundamentals in programming but that doesn't mean everything in CS is based on programming. It's more like on the creation of a more efficient algorithm, or set of instructions. We can also apply this in robotics, mechatronics, avionics, and other fields.

Basically, CS deals with logic and how, why, and when it can be applied to a computing device. This device is not limited to traditional computers. You can use matchsticks for all I care.",hlfn84e,t1_hlfhjp7,1637444978.0,False
qxxaab,"Generally my life goes like this:

1. Get inspired by something.
2. Read the scientific literature about it.
3. Have an idea for a contribution to the literature.
4. Develop it further into a research proposal (data, metrics, analytics, methodology)
5. Gather any necessary data.
6. Preliminary data analysis.
7. Design and develop an algorithm to solve the problem (here lies programming although quite different then industrial software development, which I did for 14 years).
8. Evaluate the algorithm.
9. Fail.
10. Enhance the algorithm or build something entirely new (lots of thinking and some programming)
11. Evaluate the algorithm.
12. Probably still fail.
13. Repeat 10-11 until a breakthrough (literally had a massive research breakthrough yesterday after months of work that will completely change algorithmic inference forever, very exciting!!)
14. Write paper(s).
15. Publish papers(s)
16. Have paper(s) rejected.
17. Revise paper(s).
18. Repeat 16-17 until the paper is accepted.
19. Goto 1. :)

And on the side, mentor students, apply for funding and answer Reddit posts. :)

Note, I'm an applied machine learning researcher. A theoretical computer scientist would not do much if any programming at all.",hlfo2gl,t1_hlfhjp7,1637445352.0,False
qxxaab,This is an easy TLDR I’ve heard before: programming is to a computer scientist as a telescope is to an astronomer. Programming is a tool through which they conduct their research,hlgafmy,t1_hlfhjp7,1637455682.0,False
qxxaab,That's a very apt description.,hli5zb1,t1_hlgafmy,1637499120.0,False
qxxaab,"No, I usually think of scientist as someone who does research as their job. I’m a software engineer.",hlcna7i,t3_qxxaab,1637383457.0,False
qxxaab,Data scientist has entered the chat,hlcy69i,t1_hlcna7i,1637390616.0,False
qxxaab,I refer to myself as a code monkey.,hldbbvi,t3_qxxaab,1637401677.0,False
qxxaab,I refer to me as my own bad self.,hlefjvn,t1_hldbbvi,1637426063.0,False
qxxaab,😂😂😂,hlewbu5,t1_hldbbvi,1637433079.0,False
qxxaab,"I have yet to conduct my own science outside of perscribed assignments, so no. But I will happily award myself the title once I conduct my own research to answer my own question no matter how trivial the question is.",hlcn4ta,t3_qxxaab,1637383372.0,False
qxxaab,Depends on how badly I'm trying to win the argument,hldsmlg,t3_qxxaab,1637414942.0,False
qxxaab,Or impress a girl,hlfow7b,t1_hldsmlg,1637445720.0,False
qxxaab,I refer to myself as just some dude who sits in front of a computer lol,hlcouxk,t3_qxxaab,1637384396.0,False
qxxaab,I don't know what the hell I am anymore.,hle0azc,t3_qxxaab,1637419149.0,False
qxxaab,I feel you mate...,hle6gcp,t1_hle0azc,1637422078.0,False
qxxaab,"No, because I'm not a computer scientist. I refer to myself as a programmer, because I program computers and other devices.",hlddk2s,t3_qxxaab,1637403645.0,False
qxxaab,"Computer scientist by study, software engineer in practice. So yes, but also no",hlds9od,t3_qxxaab,1637414725.0,False
qxxaab,"No, I call myself Slick Fatsack",hldnz53,t3_qxxaab,1637411959.0,False
qxxaab,"The usual nomenclature is this.

The term computer scientist typically refers to people who have the educational background, which is usually Phd, to carry out of original research or contribute to original research in the field of computer science.

People who therefore do not fit this definition do not usually go by the description of computer scientist.",hlcxnxo,t3_qxxaab,1637390241.0,False
qxxaab,"That might be the degree, but you are only a computer scientist if you study computation.",hldcl5s,t3_qxxaab,1637402782.0,False
qxxaab,"In my language i dont recall anyone actually saying it. Programmer, tech guy, it, engineer, developer, but never ""scientist""",hle6lzq,t3_qxxaab,1637422145.0,False
qxxaab,"Nah, Depressed usually does the job.",hle9drs,t3_qxxaab,1637423369.0,False
qxxaab,"No, just like how traders are to math statisticians I am to computer scientists",hldfv2h,t3_qxxaab,1637405628.0,False
qxxaab,Do math statisticians generate and analyze statistics about maths?,hldu6mf,t1_hldfv2h,1637415858.0,False
qxxaab,"I've been called a Computer Science Major, Software Engineer and a Developer in my career. I don't care what they call me at work. As long as it's not derogatory.",hldnbng,t3_qxxaab,1637411502.0,False
qxxaab,"Can they call you ""the most beautiful man/woman/else in the office""?",hlg3l3x,t1_hldnbng,1637452468.0,False
qxxaab,I'm a computerer,hlfe407,t3_qxxaab,1637440864.0,False
qxxaab,I call myself a computer scientist on the grounds that I have the prerequisite training to do research in the field of computer science and I can teach the fundamentals of my field as a teacher (even though I'm not actually a CS teacher). Daily reminder that computer science is a discipline of mathematics.,hldbknf,t3_qxxaab,1637401883.0,False
qxxaab,I don't agree. I think it uses math but it is its own thing.,hldcboq,t1_hldbknf,1637402548.0,False
qxxaab,"The basis of all computer science is all about the mathematical concept known as the computation. In anything related to computer science, I expect a trained computer scientist to formally prove the mathematical properties about any given computation. The consequence of being able to formally prove the nature of any computation means that computer science is fundamentally a discipline within mathematics.",hldl7xc,t1_hldcboq,1637409938.0,False
qxxaab,"I’m not disputing what you are saying about the basis of CS being math, but does it matter? You see, the basis of Medicine is  Biology, the basis of Biology is Chemistry, and the basis of Chemistry is Physics. Yet, by current standards, those are considered separate fields of science, with a lot of theory and methods shared among themselves. Do you expect a biologist to prove/explain the nature of her/his work by the formalities of chemical reactions?

The definition of what’s a field or another is highly debatable. A good analogy is to see the human knowledge as a continuous variable, and fields of knowledge being our attempt to discretize it.

Applied Computer Science which IMO is by far the most important part of the field, spreads across all other fields, e.g Computational Biology, Computational Chemistry, and even Computational Sociology. These fields couldn’t care less about proofs btw. So, how do we classify it? Are those subfields, mixed fields? 

By current standards of what constitutes a field, I dispute the idea of CS not being one in its own, but it doesn’t really matter. The goal of every field of scientific endeavor - applied or theoretical - is to further the knowledge of humankind, debating which field your work falls on, is utterly pointless.

Edit_0: typo",hle913z,t1_hldl7xc,1637423217.0,False
qxxaab,"I'm going to refer to every medician, biologist, chemist, physicist, and mathematician as ""normal scientist"". This is because every field of endeavour is to further the knowledge of mankind, am I right? I'm being facetious here, I don't actually think this. 

The labels for each discipline of study exists because of the location where the locus of study is centred around. It's not surprising that there are overlapping themes, lessons, and tools that are shared between the distinct disciplines. The main idea is that the discipline of study is large enough that it deserves its own label to distinguish where is the locus of attention; the main idea is that the discipline has enough people thinking about the specific lessons for whatever motivation they decide.

Applied computer science exists upon the foundation set by the study discipline that we call computer science. No matter where you choose to apply the lessons of computer science, the foundational lessons of computer science remain a discipline of thought that exists without regard for application into real world practice. I consider applied CS to chemistry as computational chemistry. Formal proofs are still important within computational chemistry based on lessons learned within CS. 

For example, one lesson of general CS is that we can calculate the runtime cost and the runspace cost of any given computation. We can analyse whether the computation in question is a solution within P-time. We can analyse whether the problem itself is possible to be solved as a P-problem. If we find that the problem is a ""hard problem"", this means we won't be able to completely solve the problem on normal computers. One way to deal with this in the real world is to find a partial solution the problem. Or perhaps we find that the problem is so difficult that it's not actually possible to find a computation that gives a partial solution; we give up on trying to solve the specific problem and try to reformulate the problem into something that is feasible. Your average computer programmer who is untrained in these matters cannot apply this kind of analysis to the problem they are trying to solve; they could probably apply a naive ""hard"" solution because they don't know any better. This is just one lesson that is the consequence of understanding the cost of problems and computations, there are many other useful lessons that are a normal part of the computer science discipline.

I like to distinguish the discipline of computer science and the discipline of computer programming as being not the same discipline; you don't need computer science training to work as a computer programmer. If you consider yourself a trained computer scientist, then I expect to you be able to produce your formal proof that explains the different properties of your computation.",hlg1zwp,t1_hle913z,1637451737.0,False
qxxaab,Yeah that makes a lot of sense actually. My mind is changed for sure. I study the damn thing I didn't even consider that haha,hle6338,t1_hldl7xc,1637421906.0,False
qxxaab,"Well, maybe?  Even though i’m now a senior executive i still call myself a software engineer, but when i’m teaching computer science to high school students (through my non-profit), i will sometimes say i’m a computer scientist, especially when knee deep in talking abou measurement of algorithms or complexity classes of problem spaces.

I actually hate the field name ‘computer science’, but dont have one I prefer to replace it with.",hle43ni,t3_qxxaab,1637420986.0,False
qxxaab,"Yes absolutely why not, I successfully wrote hello world and can create html in notepad in windows.",hlekgjn,t3_qxxaab,1637428110.0,False
qxxaab,I do! :),hlddznd,t3_qxxaab,1637404035.0,False
qxxaab,"Refer to myself as a software developer personally but that also happens to be my current job title, even when it wasn’t though- I still use software dev",hle8b3y,t3_qxxaab,1637422904.0,False
qxxaab,no just a humble data janitor,hleaz0o,t3_qxxaab,1637424060.0,False
qxxaab,"No, i have a degree in computer but I'm a systems administrator",hleb0u6,t3_qxxaab,1637424082.0,False
qxxaab,I refer to myself as a senior data engineer,hlegahv,t3_qxxaab,1637426381.0,False
qxxaab,"No, I refer myself as a developer. Lately with the faaaaaaang stuff the lens on what developers should know has shifted to more to the comp sci frame ignoring so much more of the job.

So yeah, unless your doing research type activities your not a computer scientist (IMO).

It seems that ‘engineer’ is a title that some are using to differentiate between developers and those in between scientists.

In the end it’s just a title.",hlemac3,t3_qxxaab,1637428890.0,False
qxxaab,I have a BS in computer science and I’m an electrical engineer.,hlf3y8b,t3_qxxaab,1637436334.0,False
qxxaab,"No, i tell people Im a computer engineer so they don’t think all I do is plug in cables and open Microsoft word.",hlfa3e3,t3_qxxaab,1637439063.0,False
qxxaab,"It’s applied math using computers.

Third order of Hogwarts.",hlfonth,t3_qxxaab,1637445614.0,False
qxxaab,I prefer the term IT Lich.,hlfty37,t3_qxxaab,1637448040.0,False
qxxaab,"I do science with and about computers, so yes, computer scientist is a good term.",hlg3p6e,t3_qxxaab,1637452520.0,False
qxxaab,I have part of a degree in CS and a whole degree in liberal arts. I am a “byte wizard”.,hlgjhy6,t3_qxxaab,1637460097.0,False
qxxaab,Yes.,hle5jv2,t3_qxxaab,1637421658.0,False
qxxaab,Yes,hleok3v,t3_qxxaab,1637429840.0,False
qxxaab,Yes and a games programmer.,hlduumi,t3_qxxaab,1637416235.0,False
qys6ai,"Something like this is actually used, but not to encode more than two states.

It is used in high end fibre optic connections used for internet communication. In a single fibre you not only establish a single connection but establish multiple connections that are multiplexed by different wavelenghts.

I remeber a youtube video from someone that visited a american fibre cable provider where this (remarkable unspectacular) beige box with a throughput of multiple Tb/s, through a single fibre, was shown, where multiple 10Gb/s links were connected and modulated onto different wavelenghts.

However, this only works for transmission. For computation you would need to design a transistor capable of process the different wavelengths parallel. But I doubt that this would improve calculation conpared to just use multiple regular transistors.",hlidlcr,t3_qys6ai,1637503555.0,False
qys6ai,.... What?,hli5r9t,t3_qys6ai,1637498983.0,False
qys6ai,"So instead of us looking at 0-1 values. 

If we had away of using colours to group values together. 
0 being a white light. When passed through an optic gives a value of a zero. 

When a red light is passed through gives on gives you a value of 00

When a blue light is turned on a value of 1 is passed through the optic.

Green a value of 11.

The colour spectrum is so vast, slight variations of colours could send down different formations of values.

What I was suggesting, by different colours could represent a different number. What I meant was a different colour could represent a different segment or sequence of binary.",hli7kad,t1_hli5r9t,1637500108.0,True
qys6ai,"The idea of a 0 and 1 at a physical level is an arbitrary line drawn between high and low voltage. Simply speaking, a voltage measurement above this line is a 1 and below the line is a 0. 

To instead encode 3 different values you can have three different voltage levels. You can have however many voltage levels you want. You do not need colors. The reason binary is used is because it's reliable and simple to work with (it's extremely easy to work with boolean algebra, etc...)",hljd6wy,t1_hli7kad,1637518853.0,False
qys6ai,"OR...we could, I don't know, maybe just use some more bits to denote larger numbers?   
I mean, perhaps we could assign 8 bits together in some way, like, say each one is a power of 2? That way the least significant bit could be 2\^0 or just 0 or 1, then the next one could be 2\^1 or just 0 or 2, and so on. Collecting 8 bits together like that could represent integer numbers from 0 to 255! And if we keep going, using more bits, we could really get some very large numbers! Just thinking out loud.",hlia73g,t1_hli7kad,1637501689.0,False
qys6ai,[See Wavelength-division multiplexing](https://en.wikipedia.org/wiki/Wavelength-division_multiplexing) and [optical computing](https://en.wikipedia.org/wiki/Optical_computing).,hliexsy,t3_qys6ai,1637504270.0,False
qyeqvw,"You can always just encrypt it with <insert encryption algorithm here> and it won't be immediately obvious how to decrypt it, or where the key is in the code. Less effort is something like Base 64 encoding.",hlflhpi,t3_qyeqvw,1637444195.0,False
qyeqvw,That <insert encryption algorithm here>  part is what I need help with. Basically deciding which to use since most implementations depend on some larger libraries.,hlh7fdn,t1_hlflhpi,1637473365.0,True
qyeqvw,"You could store them as integers (e.g. 0-25 for A-Z) instead of ASCII. That's basically still a Ceasar shift, but one step better, because it will prevent them from showing up as ASCII strings — someone using the [`strings`](https://linux.die.net/man/1/strings) command won't find them.",hlfofs3,t3_qyeqvw,1637445513.0,False
qyeqvw,"That's a good idea, going off of that idea... let's say I have 2 letters in my password, is there a way to store just one number? For example is there a way to store two unique numbers in one number that can then be taken apart to reliably obtain the two unique numbers back out?",hlh7o3d,t1_hlfofs3,1637473522.0,True
qyeqvw,"Yeah, your numbers are probably 4 or 8 bytes wide. You said you don't have bitwise operators, but if you have division and modulus, you can separate each byte by dividing by 256^(n) then modulo by 256.",hlh806u,t1_hlh7o3d,1637473743.0,False
qyeqvw,"Interesting, could you explain that more to me? For example If I took the number 2000, and did 2000 / 256 I would receive 7.8125. Are you saying there is a way to return to 2000 from 7.8125 using the modulo operation? Forgive me if I am not seeing this right away.",hliq0wb,t1_hlh806u,1637509356.0,True
qyeqvw,"I meant integer division, where you floor the result. The number 2000 represents two integer bytes: 7, and 208.

(2000 // 256) % 256 = 7

2000 % 256 = 208

... where ""//"" means floored division, and ""%"" means modulo.

It might be easier to understand in the reverse direction. If we want to pack two integer bytes, 7 and 208, we multiply 7 by 256, then add 208.

7 * 256 + 208 = 2000

If you're familiar with bitwise operations, multiplying by 256 is equivalent to left shifting by 8, which makes room for a new byte in the least-significant position. Likewise, dividing by 256 is equivalent to right shifting by 8. Modulo 256 selects only the least-significant byte (equivalent to bitwise AND 255).

EDIT: Oh, and if your language treats all numbers as floating point, it likely uses ""double precision floats"" meaning you can use 53 bits before losing precision, so you can pack 6 byte-sized numbers (integers between 0-255 inclusive) in one ""number"".",hlisjtw,t1_hliq0wb,1637510403.0,False
qxrdbd,"It's called ""self-modifying code"".

It's uncommon because it's really hard to write and there is no benefit to writing such code. Why bother?",hlbfxnt,t3_qxrdbd,1637362686.0,False
qxrdbd,"Very true. And good luck maintaining someone else's self-modifying code!

OP, here's a [blog](https://blog.osteele.com/2006/04/javascript-memoization/) discussing self-modifying code that is elegant and clever—but isn't exactly transparent, so I'd worry about it in production. For example, can you guess what this is about?

`OSGradients.initialize = {  
  OSGradients.initialize = function() {};  
  ... // initialization  
}`",hlbiq7f,t1_hlbfxnt,1637363849.0,False
qxrdbd,You've reminded me of this interesting video from Computerphile (Dr Julian Onions) https://youtu.be/SWU_DgjSwRU,hlehc01,t1_hlbiq7f,1637426829.0,False
qxrdbd,"Isn't this ""trick"" quite well known for implementing memorization? I don't think it's that bad.

Of course, I still won't use it in production code - in Python, the decorator `functools.cached` implements memoization, and I'm sure there's something similar in JavaScript also.",hldf63u,t1_hlbiq7f,1637405038.0,False
qxrdbd,"you mean \`lru\_cache\`? Also, the above code isn't for memoization. At least, there isn't anything about it nearly similar to what I've seen and done with memoization.",hldgrqh,t1_hldf63u,1637406390.0,False
qxrdbd,"`cache` is a new decorator (in Python 3.9), which is basically equivalent to `lru_cache(maxsize=None)`. See the `functools` docs: https://docs.python.org/3/library/functools.html

In Python, we can do something similar to the JS code like this:


    class A:
        @property
        def x(self):
            ans = expensive_computation(self)
            del self.x
            return self.x = ans


Of course, this is not a one-liner, but I'm sure a one-liner is also possible 🙂

This code does memoization / lazy loading of some expensive computation. Of course, we don't need to do this manually - the better version is:

    class A:
       @functools.cached_property
       def x(self):
           return expensive_computation(self)

**Edit:** for some reason, the Python cide is not being formatted correctly.",hleq3ms,t1_hldgrqh,1637430492.0,False
qxrdbd,"There's lots of reasons to write self modifying code, such as it being fun, interesting and emergent. I'm sure there's plenty of academic reasons to do so too.",hlchmh1,t1_hlbfxnt,1637380279.0,False
qxrdbd,"Well, yes, there is value in doing things ""because they are hard"". Unfortunately, that value is not recognized by companies expecting production-ready code, and self-modifying code is hard to reason about or formally verify in general.  


I originally hoped to add some qualification to my answer by linking an ""self-modifying code competition"" or something like that, but that does not seem to exist. The closest thing is the IOCCC, and that is not really about self-modifying code since your code must be cross-platform (I believe).",hlci6ou,t1_hlchmh1,1637380590.0,False
qxrdbd,"I find it a lot easier to write self modifying code actually. One's mileage may vary. Anyway, I said ""fun, interesting, and emergent"" which has nothing to do with difficulty. There's few things more interesting than watching some self modifying code do something elegant, emergent, and unexpected. I write silly procedural games, not production ready code (for now), so there's neither harm nor foul in it. Academically it must surely have value for people really good at it, taboos notwithstanding. It's the emergence that is fascinating.

In my opinion, making code that is fully referentially transparent and clean is way harder. Just my amateur opinion.

Edit to clarify: nothing against clean code. It's important to be able to do. Just saying I find it harder.",hlciw37,t1_hlci6ou,1637380977.0,False
qxrdbd,"More power to you, but be careful if you ever have to be responsible for drafting an RCA",hlcrujw,t1_hlciw37,1637386249.0,False
qxrdbd,"I'm not bragging. I am self taught so I got used to what pleased me and would need to train up new habits to be a professional. I value all kinds of coding. 

May I ask what an RCA is? I am not a professional. Just an eager amateur.",hlcse3c,t1_hlcrujw,1637386589.0,False
qxrdbd,"Root Cause Analysis. Investigating a problem like a service outage, security vulnerability, equipment malfunction, etc.",hld0bs8,t1_hlcse3c,1637392283.0,False
qxrdbd,Many thanks!,hlezimv,t1_hld0bs8,1637434415.0,False
qxrdbd,"Yeah, Generally speaking, most development regards building production systems which need to be explainable when things go wrong. We’re already in the midst of a crisis in Machine Learning explainability; 

Self modifying code would be a development nightmare if it causes any problems for a project using that code.

If I were in charge of a build system, I’d scan for self modifying code in its modules and auto fail the build, and tell the devs to use different modules",hle0ue4,t1_hlcse3c,1637419411.0,False
qxrdbd,"My understanding of machine learning is lean at best, but is it even possible to do something like a neural net without some degree of emergence which is impossible to explain fully? That seems to be one of the reasons it works at all. Not all code can be exactly the sum of its parts. That field is very attractive to me but I'm not quite good enough to get into machine learning as deeply as I would like, yet. Can you explain this crisis in explainability to me, or point me to an article?",hlf0596,t1_hle0ue4,1637434689.0,False
qxrdbd,"I like to outline this crisis as such

Let’s say you build a bridge and after a year or so, it fails, killing and injuring a large number of people. The city needs three things: the families to be compensated, the bridge to be repaired, and justice to be served.

So you get sued, and the city does an investigation to determine what went wrong and why so they can fix it.

If the bridge is built by standard practices, and complying with regulations, they might find that their was a design flaw where something like temperature variations in the particular metals used caused joints to loosen and over time, caused structural failure. They’d be able to determine this because they have records of both the design specs and construction records.

But this analysis might not possible if* the bridge were designed by machine learning because the specs could be too novel to perform a sure analysis.

Result: unclear how to repair the bridge, and now you’re in jail for using designs that weren’t repairable

Case and point: explainability == accountability. If it can’t be explained, then its unaccountable and that is untenable when people’s lives are involved",hlgef69,t1_hlf0596,1637457614.0,False
qxrdbd,"That explains exactly why too much unaccountable emergence could be bad in production. Is there a way to make neural nets without it, though? Is it even possible to log every last step in the process? I would like to know more about the math behind machine learning and maybe make my own little neural net some day but I'm not there yet. What research I've done suggests it would be impossible or highly burdensome to make them fully explainable. For systems used in the real world which could have consequences on peoples' lives or legal standing, however, that burden seems justified if there is a way to achieve that emergence while logging everything. But then maybe you wind up needing another neural net just to parse the data!

Edit to clarify position: Just because it's hard doesn't mean it's impossible, and the benefits outweigh the risks. I would rather live in a world where we take a chance on these things.",hlggzyq,t1_hlgef69,1637458867.0,False
qxrdbd,"I believe in affording for experimentation, and leaving it isolated from work. Too many companies end up doing both trying to cut on costs and all it does is add technical debt",hlh9x6g,t1_hlggzyq,1637475045.0,False
qxrdbd,Can I trouble you for some examples? I don't work in the industry. I'm just an eager amateur who may or may not find a job doing it some day. I would enjoy some analysis from someone with more perspective.,hlha2k9,t1_hlh9x6g,1637475147.0,False
qxrdbd,"In addition to being hard to write, it would be hard to deploy and monitor. Not only do you have program state to worry about, but also source code state and whatever kind of reloader state that exists.",hle3der,t1_hlbfxnt,1637420644.0,False
qxrdbd,"I agree with hard to write, but disagree with there being no benefit.  In truth, we simply don't know if there are benefits.  Nobody has written a (non-esoteric) language that forces its use, or even encourages it.

For all we know, it could be the easiest way to create sentient AI, time machines, and replicators.

All that said, it's true there is no _known_ benefit.

As for why to bother?  Perhaps to be the first to make it easier and mainstream.",hldc8uf,t1_hlbfxnt,1637402480.0,False
qxrdbd,They are rare because you need to be a genius to write them without causing horrible bugs.,hlbyr1x,t3_qxrdbd,1637370929.0,False
qxrdbd,"Really hard to design and debug. The last time I saw it used was in microprocessor assembly for optimization purposes in a very constrained environment.
I wonder if a modern OS could even unprotect program memory to allow this in the first place, huge security hole if you can overwrite code...",hlbi71d,t3_qxrdbd,1637363622.0,False
qxrdbd,"Yeah, I've written a fair amount of self modifying code, but I work in a highly constrained environment where every single bit counts. If I know that a pile of instructions will never be called again (like initialisation code) well, that's just free real estate.

Edit: even then the best example I can think of was to add a bit of debug code so I could follow the code path. It's not very often I've shipped it in production",hldehja,t1_hlbi71d,1637404464.0,False
qxrdbd,[deleted],hlbreoq,t1_hlbi71d,1637367610.0,False
qxrdbd,"This is not true, you can't write in .text memory mappings in any modern OS. Those are mapped read-execute, without the write permission. You'd need to go out of your way to do it (and some security mechanisms sometimes prevent you from having a memory mapping write-execute, known as W\^X).",hlcanv5,t1_hlbreoq,1637376620.0,False
qxrdbd,"This is why we have a security team at work, they watch out for us 💯 and audit published code.  So much to keep track of.",hlcfx3i,t1_hlcanv5,1637379362.0,False
qxrdbd,Definitely. And RWX sections in memory are a huuuge giveaway that something fishy is going on!,hlclxmm,t1_hlcanv5,1637382681.0,False
qxrdbd,"That makes sense.  Too much time in low level environments for me, appreciate the confirmation 👍",hlbuj3o,t1_hlbreoq,1637369016.0,False
qxrdbd,Who told you this?,hlczipk,t1_hlbreoq,1637391648.0,False
qxrdbd,"I disagree with most of the answers given so far. I think most commenters are thinking of programs written in, say, C++ or assembly, and in the context of those languages it is true that self-modifying code is rare. 

But in many languages the self-modification of code is just a natural part of writing programs. Think about how in Python decorators work by dynamically defining an inner function. Or consider the low-level details of how Python works, how basically everything is a `dict`.

There is a related concept called [homoiconicity](https://en.wikipedia.org/wiki/Homoiconicity). A language is homoiconic if you can manipulate code as if it were data. Lisp and Prolog are the usually cited examples, but for me it was Mathematica that really made the concept click in my brain. In Mathematica, functions are just expressions that have a particular evaluation discipline.  So you might construct a mathematical function, say, and then apply that function to a list of values.  Or you might start with a function and then deconstruct the pieces of the function. For example, your function might be a polynomial, and your program might make decisions based on the degree or coefficients of the polynomial. Seasoned Mathematica programmers don't restrict the dynamic manipulation of code to just mathematical functions. 

So the answer is, people write self-modifying code all the time, but it's such a natural part of the languages in which it is done that it may not *feel* like you are doing anything fancy.",hlcprj9,t3_qxrdbd,1637384951.0,False
qxrdbd,"mathematica and wolfram lang deserve more love (and to become open source so that it finally starts gaining contributors and traction)

and yeah, people cite lisp bc it was the first one, and the one where it's actually not weird to do this. functional languages do this all the time. it's fun.",hlcwjz8,t1_hlcprj9,1637389430.0,False
qxrdbd,This is an extremely good answer.,hlfenrt,t1_hlcprj9,1637441113.0,False
qxrdbd,"In a very real sense many, perhaps even most programs will modify themselves all the time - you just need to expand your idea of 'themselves' beyond the base source code. Quite often the source code is a seed for some other thing, and that other thing is what is actually being executed, and will modify itself on the fly a whole lot. Polymorphism in OOP is probably the most approachable example of this.",hlc5340,t3_qxrdbd,1637373881.0,False
qxrdbd,"It is called meta-programming and I once tried to write a ""meta-program"" or a program that modifies itself. It was an year ago, and it was JavaScript... so I could never complete writing the program because it was very hard to write it and there are ""relatively"" less resources about meta-programming (particularly js meta-programming) on the internet. 

It has a use case, just like my case, it was a blockchain(kind of) written in node.js(ikr bad choice) and I wanted the stdin value to be the name of a variable which stored an object `Wallet`. My program's workflow was like this: it would accept a value via stdin [using `readline` module of node.js] and then it would use the value inputted by the user as the variable name to store the user wallet in. Please do not hate me for this approach, ik this is a shitty approach but I just used it because I coded it in less time and did not want to set up a database kinda thing.

Edit: if anybody is interested to know how to do the task I wanted to do, I later found the solution, it can be accomplished using promises in JavaScript.",hlcg88p,t3_qxrdbd,1637379528.0,False
qxrdbd,Self mutating codes are gold mine for malware researchers. They are easy to hide from antiviruses.,hlcj5el,t3_qxrdbd,1637381120.0,False
qxrdbd,"If you think of an operating system + all the currently running programs as a single program, then that program is self modifying. It modifies itself each time a program is opened or closed.",hlbrvc7,t3_qxrdbd,1637367816.0,False
qxrdbd,"To extend that, VMs like Java Virtual Machine (JVM) that have Just in time compilation (JIT) enabled, are technically self modifying.",hlctf73,t1_hlbrvc7,1637387262.0,False
qxrdbd,"It's called meta programing , if language is designed around it it can be viable ,its pretty good in Lisp family of languages specially in Racket",hlc3d6n,t3_qxrdbd,1637373061.0,False
qxrdbd,Getting downvoted by COBOL programmers.,hlcfif1,t1_hlc3d6n,1637379148.0,False
qxrdbd,"I don't understand it ,was something wrong with my answer ?",hlcieii,t1_hlcfif1,1637380706.0,False
qxrdbd,"I don’t think so. I think it’s a wonderful answer. We’ve literally had this since the late 50s, and people shied away from it. So weird. A tool is a tool",hlcivap,t1_hlcieii,1637380964.0,False
qxrdbd,"Now those downvoted seems to be gone ,weird !",hlcry1z,t1_hlcivap,1637386307.0,False
qxrdbd,I may have meta-programmed them.,hlcwft6,t1_hlcry1z,1637389347.0,False
qxrdbd,"As others have said, if debugging and maintaining someone else's code normally, imagine how it would be with the added difficulty of the code changing every time you opened it on an editor/IDE, any possible optimizations would seldom be worthwhile.

That said, there are some cases where the use of self-modifying code techniques are particularly common, malware development immediately comes to mind.",hlc5bo3,t3_qxrdbd,1637373996.0,False
qxrdbd,"There’s some use cases discussed in papers in Lisp.

Programs that edit themselves sounds dope. I’ll try it out.

I don't think saying it's useless is demonstrably correct. It's a tool, the fact is that people haven't been able to think on use cases. I know it should be really useful for generative systems. 

We must galaxy brain meme this and eventually someone will figure something out.",hlcfdsy,t3_qxrdbd,1637379081.0,False
qxrdbd,Do you want Skynet? Because this is how you get Skynet!,hlcnsg0,t3_qxrdbd,1637383756.0,False
qxrdbd,"Skynet runs on lisp then, we've had meta-programming in Lisp since when it was implemented in 1959. 

Crazy stuff.",hlcwetp,t1_hlcnsg0,1637389328.0,False
qxrdbd,"So what you're saying, is we're absolutely safe from human destroying robots?  Because this is the internet, so I'm going to believe you, and use this conversation as source material down the road.",hld7vff,t1_hlcwetp,1637398591.0,False
qxrdbd,At least we didn’t get killed by Java Skynet.,hldbcgm,t1_hld7vff,1637401691.0,False
qxrdbd,Because it couldn't grow past 3B devices,hldlnrx,t1_hldbcgm,1637410275.0,False
qxrdbd,"In many computer architectures (usually of the RISC variety), the instruction cache does not have to be coherent with the data cache. Thus, a self-modifying user-mode program would have to manually manage instruction cache invalidation (by calling the OS). This is a sufficiently difficult undertaking, so in terms of practicality, self-modifying programs may as well be impossible on these architectures.

It's been pointed out in this discussion that when an OS loads a program and executes it, it is itself a self-modifying program. This is correct. It can be generalized; from a strict technical perspective, all stored-program computers are by their nature, self-modifying, unless the program is in ROM and never changes. The OS can be self-modifying because it's much more restricted. It knows where it's loading the program into, and can invalidate the instruction cache accordingly.",hlcp91v,t3_qxrdbd,1637384633.0,False
qxrdbd,"One archaic use case that hasn't been mentioned is for bootstrap loaders back when ROM was not common and, when available, very small.  The first computer I programmed machine language for would boot by putting a -7 in the X register, 2 in the PC, an input from channel to location 2 in the instruction register, and start the paper tape reader in binary 4 character mode.  On the tape you had:

    2: input to location 10 indexed
    3: increment X and branch to location 2 if X negative
    4: load X with 9
    5: input location 0 indexed
    6: skip if buffer not ready
    7: increment X and branch to 5 (X always negative)
    8: branch to start address
    9: (load address + sign bit)
    10 ... end-of-tape (whatever you want)

Anybody know which machine it was? \[Hint: it wasn't named after a radical student group\]",hlcuxao,t3_qxrdbd,1637388274.0,False
qxrdbd,Wouldn't that just be ai?,hlcwm1o,t3_qxrdbd,1637389473.0,False
qxrdbd,They aren't? JIT compilation is pretty common our days.,hlcypii,t3_qxrdbd,1637391023.0,False
qxrdbd,"Self-modifying code can be used for obfuscation - making a code much harder to analyze and reverse engineer. Can be for malicious purposes or just plain anti-reversing (you don't always want your code to be easily understood, as it implements some algorithm you do not want people to know how it works).",hld4wo9,t3_qxrdbd,1637396028.0,False
qxrdbd,Would polymorphism count as style modifying to you?,hldhlnx,t3_qxrdbd,1637407070.0,False
qxrdbd,"Yes, they make reference to it in a book called ""The Art of Computer Virus Research and Defense"" (Symantec Press) by Peter Szor. Part 1, Chapter 7, section 7.6, page 269, ""matamorphic viruses"".

> Virus writers try, of course, to implement various new code evolution techniques to make the researcher’s job more difficult. The W32/Apparition virus was the first-known 32-bit virus that did not use polymorphic decryptors to evolve itself in new generations. Rather, the virus carries its source and drops it whenever it can find a compiler installed on the machine. The virus inserts and removes junk code to its source and recompiles itself. In this way, a new generation of the virus will look completely different from previous ones.

If you're interested in this area take a look at a Java library called Java poet.

https://github.com/square/javapoet",hldit2w,t3_qxrdbd,1637408048.0,False
qxrdbd,"Well if you’re willing to accept that a computer program = source code + data, then modifying data is akin to changing program behavior. Under that interpretation, this happens all the time",hle3jfa,t3_qxrdbd,1637420720.0,False
qxrdbd,"I don’t think it’s a stupid question. Here is an example of a self-modifying software which is capable of running compiled C code in Linux, Windows, macOS, and BSD. It sounds like sorcery, but it’s actually an incredibly clever piece of code. 

https://redbean.dev/

http://justine.lol/cosmopolitan/",hlep0i7,t3_qxrdbd,1637430039.0,False
qxrdbd,It's a pain in compiled languages (which rule serious development),hll1iy5,t3_qxrdbd,1637544344.0,False
qxrdbd,"Oh yeah, that’s called Siri",hlbyvc9,t3_qxrdbd,1637370982.0,False
qxrdbd,Its name is Madness ...  MADNESS I SAYYY,hlcmfsf,t3_qxrdbd,1637382969.0,False
qy34w1,"Viruses don't work by magic. They are programs like any others and just do nefarious things. They don't magically start executing, something has to explicitly start them. Usually this happens without you wanting to, e.g. when opening an infected file, or a malicious website etc.

However a virus executable can just be stored on your hard drive without any issues. Security researchers do it all the time. The key is to never execute it.

So if you have an encrypted virus somewhere, you're safe since you can not execute it, because the file is encrypted. But the encryption really does not matter.

Of course, once you execute the virus, the fact that they were encrypted at some point in time becomes irrelevant.",hldmyvd,t3_qy34w1,1637411242.0,False
qy34w1,I thought there were viruses that could do things themselves and that Trojans were the ones that had to be executed by the user? Of course I may be wrong but this is why I thought virtual machines were a thing. I've heard of viruses that can duplicate and spread all over a system. Do they all need to be activated by the user?,hlluvii,t1_hldmyvd,1637559169.0,False
qy34w1,"No. Computers are not sentient beings. Everything happens because it is explicitly caused by some other thing.  


Viruses typically exploit security issues in programs to trick those programs into executing (e.g. sometimes E-Mail programs automatically attempt to download attachments, trip up and then execute these attachments because someone fucked up while programming them).  


Viruses also don't ""duplicate all over the system"". That's not how any of this works. A virus is a computer program. Its primary goal is to get executed, so that it can do its malicious activitiy. A virus especially wants to get executed after a restart, or when the user deletes the original file. Hence, the virus copies itself to a few new folders, and registers these copies of itself as programs to be run when booting the system.

Trojans typically exploit the user by masquerading as a useful program. They often even perform a normal function besides their malicios activity so that the user does not become suspicious.

&#x200B;

Besides, these denominations are all fuzzy in practice. The names also are. Just because it is called ""Virus"" does not mean that the analogy holds for everything. You would not expect to be able to vaccinate computers, would you?",hloecrs,t1_hlluvii,1637610550.0,False
qy34w1,"Encryption is not a container. It is a function you apply to the data. Example: If you apply + to the numbers 2 and 3 it is not like you are putting the 2 and the 3 into a box with a + on top of it, instead you are generating a new number (5). Like with encryption you can not reverse the function only knowing that the new data is 5, you need addition information. This additional information is the encryption key. If the 2 was your virus it is no longer there.

If a folder is encrypted you actually have to concatinate all the files in it to generate a single file and encrypt this, or encrypt every file seperate.

To answer your question. An encrypted virus is just meaningless data. All data in an encrypted container is encrypted.",hldug6g,t3_qy34w1,1637416009.0,False
qy34w1,As I suspected. Thanks for the answer :),hlluwlr,t1_hldug6g,1637559188.0,False
qy34w1,"Even unencrypted, a virus file just sitting on your hard drive can't do anything until something opens it. It's just zeroes and ones waiting to be executed, not a living creature which runs by itself.",hldn27o,t3_qy34w1,1637411307.0,False
qy34w1,"If a virus is encrypted it cannot be executed. It needs to be decrypted first. Similar to how you cannot start software that has been compressed (compression is a type of encryption), you must decompress first. Of course there could be additional malicious software that decrypts the virus. Viruses are just software that do things you don't want.",hle9r5s,t3_qy34w1,1637423526.0,False
qy34w1,If there’s a virus in your computer it all goes out the window,hldfkui,t3_qy34w1,1637405388.0,False
qy34w1,I never said there was. I just want to know if encryption works both ways.,hldfo5u,t1_hldfkui,1637405466.0,False
qy34w1,Not when you have a virus.,hldfu9q,t1_hldfo5u,1637405608.0,False
qy34w1,"Ok, if I'm the one who created a virus, but then I put it into an encrypted container and sent it to someone and they cannot decrypt the container, could the virus be a threat to them?",hldfzbl,t1_hldfu9q,1637405727.0,False
qy34w1,"If you mean an encrypted zip i can't see how it would ""escape""",hldioqa,t1_hldfzbl,1637407952.0,False
qy34w1,"Or a Veracrypt container. Anything with proper encryption (password protection doesn't always mean encryption eg windows lock screen isn't encrypted, bitlocker is)",hldivsu,t1_hldioqa,1637408107.0,False
qy34w1,"u/JoJoModding's comment pretty much says it all.

A virus (or any kind of malware) is made of program code.

What separates viruses or malware from any other programs is that their program code 1) attempts to do something malicious, and 2) malware usually tries to include itself in your computer's startup programs or other startup routines so that their code is automatically executed whenever you start your computer.

Malware might also have some other kinds of tricks to make itself harder to detect, for example.

Fundamentally, though, malware is just program code. Program code only does something when it's being run, one way or another; it doesn't do anything by sitting on the disk. In fact, a better way of looking at it might be not to consider the program code as doing something (malicious or otherwise) by itself; rather, when you run a program, the operating system and the computer's CPU take the program code they find in the program file and start doing what the program code says.

If the code on the storage device is encrypted, it's not a question of whether it can ""escape"" from somewhere. The program code that has been encrypted does not make sense as program code, and it doesn't look like program code to the operating system or to the computer: it cannot be executed in the first place, so it cannot be doing anything, inside or outside of that folder. It's just inert bytes.

In that sense it doesn't necessarily make sense to say that encryption ""works both ways"", as it's not as if the virus is alive but being contained within the encrypted container somehow and restricted from accessing the rest of the system by the encryption.

You're correct that if malware code has been encrypted, and it's not being decrypted (or the password simply isn't available), the malware can't possibly be doing something. However, that's not really ""the other way around"": it's exactly the same way that encryption works for any other files. MS Office cannot open an encrypted Word document that hasn't been decrypted; the CPU cannot execute program code that has been encrypted without it being decrypted first.",hle7og3,t1_hldivsu,1637422621.0,False
qy34w1,"Yeah, a zip file is encrypted with AES thou :)  
If the computer can't access the virus code how can it run it XD",hle1wl5,t1_hldivsu,1637419922.0,False
qy34w1,Yes very.,hldgcr0,t1_hldfzbl,1637406044.0,False
qy34w1,May I ask how though? Encryption as far as I'm aware turns all of the data into a hot mess until it's decrypted. How can anything function in such a state?,hldghrw,t1_hldgcr0,1637406161.0,False
qy34w1,Why is OP replying only to trolls and not any of the serious answers?,hlg2xel,t3_qy34w1,1637452161.0,False
qy34w1,"Just from that comment alone I'm suspecting you yourself are a troll. The ""trolls"" commented first hence why I replied to them first. Then I got busy as other answers started rolling in, now I'm back again and responding to others.",hllv50f,t1_hlg2xel,1637559332.0,False
qxixkk,"Understanding functional programming is important in common languages as well. 

When you pass something by reference to a function, does it modify that object? Is it clear that that object is modified? That's a common source of bugs in C# or Node. 

Additionally, modern frontend frameworks (Redux, rxjs, etc) can be very functional programming. You have a state. That state can't be modified, so everything computing the new state is functional (ish). 

It's not something that's used as a whole all the time, but the concept of ""make it so that nothing is changed behind the scenes without making it very very clear"" is very useful in a lot of situations.",hl9pvgr,t3_qxixkk,1637338568.0,False
qxixkk,"But you can write immutable classes in any OO language. It is a very fundamental concept called Value Object in DDD, which is like 20 years old.",hla4wwm,t1_hl9pvgr,1637344451.0,False
qxixkk,"Its not about immutability really. People tend to get caught up in that, but the biggest selling point of fp is composability. Functions as truly first-class values is a very powerful feature that allows simple and  robust code reusue. Mutability doesn't compose well, so it's discouraged (or disallowed) in fp. Combine first class functions with rich static typing and expression based control flow, and you've got yourself a language that is phenomenal for writing any deterministic or highly formal software, e.g. compilers, financial systems, theorem proving, verification, AI, etc.",hlacgke,t1_hla4wwm,1637347330.0,False
qxixkk,"> But you can write immutable classes in any OO language.

And that gives you some of the benefits of programming in a functional style. In the same way, you can write functions that act as object methods in C, and that gives you some of the benefits of OO programming.",hlb3yo1,t1_hla4wwm,1637357738.0,False
qxixkk,"FP is over 60 years old, and the math behind it over 80. I don't know the history of  the value object idea, but in general, FP has been informing other paradigms for a long time. So the ability to easily implement FP ideas in not-primarily-functional languages is often thanks to FP.",hlaqfdk,t1_hla4wwm,1637352461.0,False
qxixkk,I agree with u/raedr7n and want to add that sometimes you can't make the whole thing 100% immutable (eg what if a deep copy takes HUGE amounts of time?). It doesn't mean that functional programming isn't useful. Making things composable (aka reusable) and limiting side effects is always a good programming practice.,hlarb7v,t1_hla4wwm,1637352798.0,False
qxixkk,I guess that makes sense. I mean even in an OOP language you’d want to minimize side effects. Writing clean and concise code seems to follow some FP principles. Although I still don’t understand how FP languages would work in a complicated engineering systems that needs to do a lot of different things,hlaovup,t1_hl9pvgr,1637351877.0,True
qxixkk,"Depending on what the specifics of that complicated engineering system entail, there are FP patterns like monads and sum types that can capture and handle a lot of complexity.",hlbhsf1,t1_hlaovup,1637363453.0,False
qxixkk,"I guess where I get stuck is state changes. For example, let’s think about a cars computer. It probably needs to know what gear you are in, which is a state. It probably wants to know if you are breaking or accelerating, which  a state can keep track of. Same goes for using your turn signal, headlights, etc. 

How do you get around not using the state for these objects? I get that for math functions, FP is very good at solving them and maybe even can eliminate some complexity. Maybe people using python are using FP without even realizing it. But how do you ignore state for every case?",hlbn4ak,t1_hlbhsf1,1637365730.0,True
qxixkk,"You'd probably have some type whose values are all states the car can take on, and then you can have functions that alter this state -- that is, they take in the old state and produce a new one. Such functions can be cascaded (composed) to create more complex functions that alter the state. Ultimately, you can simulate stateful systems with fundamentally stateless components (pure functions). 

If you mean to suggest something more like changing the state of something external to the program, something that will have some effect in the real world, then this is something that is addressed in purely functional programming. Altering external state is a behavior of impure functions (functions like print) which makes it less straightforward to model them as mathematical functions. Purely functional languages like Haskell, Clean, etc. have their own solutions to this problem.",hlc484n,t1_hlbn4ak,1637373470.0,False
qxixkk,"I work on a production Haskell codebase.
Basically, you do have state. Haskell allows you to allocate memory and pass/modify it by reference.

These actions have to occur in an expression with the type IO. But the top level entrypoint, `main`, has the type IO, and so there's no fundamental barrier to using state at the top level.

You use pure functions as the default, since the compiler can so aggressively optimize them with out-of-order execution and fusion. But for things that have to be stateful, you give them the type IO, which effectively just turns those optimizations off and makes the machine execute your function line by line like an imperative program.

There are more elaborate APIs people often wrap IO with, but that's the basic idea.

Edit: the other big reason to use pure functions, beyond compiler optimizations, is that you can eliminate while classes of runtime errors from ever occurring. Pure functions should never throw an exception or close a socket when it shouldn't have, etc.",hlf8gu8,t1_hlbn4ak,1637438325.0,False
qxixkk,"From a high level perspective, functional languages are nice because they are very deterministic. Immutable data structures and very strict typing allows for a clean and predictable program.",hlb1zcp,t3_qxixkk,1637356943.0,False
qxixkk,"Functional languages aren't a good fit for everything, but they tend to excel whenever there's a complex domain model and/or a pipeline-like solution to the problem at hand. I wouldn't write a game in Haskell, just like I wouldn't write a compiler in C++. Vice versa, however, would work great.",hlab158,t3_qxixkk,1637346810.0,False
qxixkk,This,hldomnf,t1_hlab158,1637412401.0,False
qxixkk,"It is easier to analyze mathematically (in terms of runtime, correctness, and so on) and some people see it as ""pure"" computer science.  

I remember one of the core Haskell developers as saying, Java has started from a powerful but unsafe place.  Haskell has started from a safe but powerless place.  As time has gone on, Haskell has become increasingly powerful while maintaining safety, and Java has become increasingly safe while maintaining its power.  Both are converging to best-of-both worlds from both directions.  From that perspective if you were more concerned with safety than power, you might prefer a functional and strongly typed language.  

I've heard about recent developments in OCaml about using it (I think through Merlin) to utterly get rid of the layer between it an C.  If this is very successful it offers the promise of a simpler process to deliver an executable, but this has not yet been fully fleshed out.  There are promises about when and if this will happen but ... you know.  ""Promises""

Ultimately, I can't fully understand certain people's love of functional programming other than it being a religion.  I'm much more omnivorous.  I like OO, I like functional, I'll do whatever the fuck I want.",hlbmqj9,t3_qxixkk,1637365567.0,False
qxixkk,Hahaha that’s fair. Honestly I feel like I usually use OOP at a higher level but then within the class I try to adhere to some FP principles when writing classes,hlbnf69,t1_hlbmqj9,1637365859.0,True
qxixkk,"Here’s a nice little discussion on the topic:

https://stackoverflow.com/questions/36504/why-functional-languages",hl9n7yq,t3_qxixkk,1637337501.0,False
qxixkk,">The languages that FP is generally used in are annoying to write software in, as well.

Chances are that you think so because you've mainly used imperative / OO languages. I'd argue that writing Java or C# is *way* more ""annoying"" than Haskell or F#.

>but I don’t understand how it could work for more complex systems

Just try it",hla9k98,t3_qxixkk,1637346263.0,False
qxixkk,Every single large system I've ever heard of being written in a functional programming language since event-driven programming became the mainstream has regretted it. The paradigms don't mesh well.,hlbeekv,t1_hla9k98,1637362055.0,False
qxixkk,"Can you name some of those systems? And funnily enough even driven programming works \*very\* well with a functional style (see for example Scott Wlaschin's ""Domain Modeling made functional"").  


Certainly there are a few failed projects but the same is true for (for example) OOP and there's also very succesful projects / companies that use FP succesfully for large scale projects (e.g. the FPGA design system and hardware synthesizer of QBay (and compilers in general), Facebooks spam detection, (sadly) a bunch of crypto shit, Hasura's whole platform, ... or just look at Erlang's (German - the English one doesn't list all the companies and projects) wikipedia article to see how insanely good FP can work)",hlbh5dq,t1_hlbeekv,1637363186.0,False
qxixkk,Reddit and Yahoo are the classic examples.,hlbl15w,t1_hlbh5dq,1637364829.0,False
qxixkk,"Yeah trying to follow the state of a mutable object as it's passed around a graph of classes and method calls is annoying. Trying to retrofit error handling in a codebase fill with `throw`, trying to reason about which functions throw exceptions and when is maddening.

Good thing is imperative language designers are more and more understanding the value of FP and adding features. It took Java 19 years to get closures, but hey, progress is progress",hlbfu9x,t1_hla9k98,1637362647.0,False
qxixkk,Because the grass is always greener on the other side,hl9pyvt,t3_qxixkk,1637338605.0,False
qxixkk,"For the right situation fp can be really great. I work in data science, and we use languages like haskell or R to do transforms quite a bit. When you care a lot about being to reproduce results  and be aware of all changes happening to data fp is a nice paradigm to be in.

I've heard they can be used for scalable software engineering but I don't know much about that.

&#x200B;

I imagine for a lot of people, solving problems in a functional way is very satisfying and that will bias their opinion a bit even if it not always the most practical aproach.",hl9zg5o,t3_qxixkk,1637342326.0,False
qxixkk,"You use Haskell?  [intriguing](https://c.tenor.com/CSvXOimoG5kAAAAC/spock-eyebrow.gif)  Does Haskell have dataframes?

(For anyone who is curious, scientific programming has two roots, research and AI which does in fact has a heavy FPP and [literate programming paradigm](https://en.wikipedia.org/wiki/Literate_programming) history, though more LISP than Haskell, and super computer programming in FORTRAN.)",hlcfsgg,t1_hl9zg5o,1637379295.0,False
qxixkk,"Haskell doesn't have dataframes natively, but there are libraries implementing it, such as:

[https://hackage.haskell.org/package/Frames](https://hackage.haskell.org/package/Frames)

(I have done no work to evaluate if this library is any good.)

But I suspect dataframes are one of those things that will only ever be productively useful in a dynamically typed language like R or Python.",hlh1ukj,t1_hlcfsgg,1637469951.0,False
qxixkk,SICP does a great job explaining this.,hlaz080,t3_qxixkk,1637355750.0,False
qxixkk,Go back to /g/,hlb2rhb,t1_hlaz080,1637357256.0,False
qxixkk,"Functional programming is awesome in every regard, including ease of writing and learning.

The problem is you're (likely) coming at it from an Object Oriented viewpoint, and seeing the more complex aspects of Functional programing. It's like a procedural programmer seeing object oriented for the first time. 

To put it simply, it's a known fact that any task you can do in programming, you can do in Procedural, Object Oriented, and Functional.

The biggest way I can highlight the advantage of Functional is this: If you want to break down working code into its most indivisible units that still work, what you're going to be left with varies on which it is. Procedural will likely leave you the whole program. Object-Oriented will frequently leave you monolithic Objects with large supporting libraries. Functional will leave you with... one line. 

And that makes all the world of difference. If you're hopping into some new code, Procedural requires you understand the entire program.   
Object Oriented requires you understand the object, the connected libraries, the state, the variables, the ways output can vary from the object, etc.   
Functional requires you to understand one line. 

Further, Functional is insanely more reusable than Object Oriented since it's so atomic. Anything you've done before, you can treat like a buffet table, grabbing what you need. 

After you've grasped it, coding in functional is faster, cleaner, more effecient, easier to pick up, snappier, and all around more fun to write.",hlax6ob,t3_qxixkk,1637355044.0,False
qxixkk,"> Functional requires you to understand one line.

Which one line is that?",hlbn851,t1_hlax6ob,1637365776.0,False
qxixkk,The one you want to change.,hleebcf,t1_hlbn851,1637425536.0,False
qxixkk,"Well, what complex systems have you tried writing in a functional language?

But you have an important point, and as much as I like functional programming, I have to concede it's frequently pretty horrible to have to debug functional code. It's also a lot less straight forward to read and comprehend than some other styles, which matters a lot for maintainability. In most real world scenarios, I think functional programming should be used to complement a more direct, imperative/OO programming style.",hlb3cog,t3_qxixkk,1637357491.0,False
qxixkk,"I tend to find most people exist within a sub-domain in programming, of which there might be some overlap and they decide that whatever works best for them, must be the one true way.",hlazx2a,t3_qxixkk,1637356111.0,False
qxixkk,"I learn JavaScript as my first programming language and as I was introduced to OOP in JavaScript by MDN and FP in JavaScript by [Academind](https://www.youtube.com/watch?v=aoE-92Ac4zE). I also learnt Java which has one of the best OOP styles.

Personally, I like OOP more be it the standard OOP or prototypal OOP. In JavaScript, I had to write a lot of functions to accomplish a task and considering that JavaScript recommends using camelCase and my problem of failing to think of nice function names, it was difficult. 

When I tried to accomplish the same task in OOP js, it was a lot easier for me primary because code can be organized into objects and nested objects which makes it very easy to name and remember the method names like `bank.depositCash` rather than its FP counterpart.

Now, I mainly use Golang and Rust as my daily drivers so I like FP and you can really use FP in complex systems. I like FP in Golang and Rust primarily because of their module systems, like in Golang, packages contain modules and modules contain functions which makes it really easy to import and use them without any hassle.",hlciedy,t3_qxixkk,1637380705.0,False
qxixkk,"They're caught up in [The Churn.](http://blog.cleancoder.com/uncle-bob/2016/07/27/TheChurn.html)

----

Functional programming is older than Object-Oriented Programming, yet people are rediscovering it like it's something brand new. Like ""protocol-oriented programming"" or the other ""programming"" styles floating around that are supposed to be better than sliced bread.

Everything is a tool. Some tools are more useful for some tasks than other tools. You wouldn't drive a screw with a hammer, nor would you pound in a nail with a screwdriver.

And sometimes you use one tool wrapped in another because it solves the problem better. For example, I will routinely (in Java) wrap functional programming in singleton objects and anonymous classes declared from protocols; basically I'm using the class and interfaces as a sort of namespace mechanism to isolate internal state of a collection of functions.

But they're just tools, and a good craftsman learns how to use all the tools in the toolkit, and when it's appropriate to use them.",hlatamz,t3_qxixkk,1637353553.0,False
qxixkk,"Okay that’s similar to what I do, I think. Like OOP at the high level, but utilizing some FP principles inside of the classes",hlay0wj,t1_hlatamz,1637355370.0,True
qxixkk,"Well, Java kinda thinks everything is an object. And while Swift has the notion of functions and function closures, it seems happiest if you organize things around objects as well. 

I also do a lot of mobile iOS and Android development, and OOP is the primary hammer used in making user interfaces. So there's that.",hlb1u5e,t1_hlay0wj,1637356885.0,False
qxixkk,"functional programming help a lot to write explicit and elegant code without side effects using immutability and pure function.

It's a way more easy to ready and understand. Another great feature of functional programming is pattern matching.

Null or nil value are commonly badly used to verify if something has failed or is not allocated. Null or nil value are not explicit, this doesn't give you any explicit information to help you.

That's why such language like Rust use enumeration like Result<S, E> ou Option<T>",hlax5yu,t3_qxixkk,1637355036.0,False
qxixkk,"No state, which is the source of many problems, keeping track of state and communicating mutations of state across the system.",hlaznvz,t3_qxixkk,1637356009.0,False
qxixkk,Hmm interesting. But don’t many problems in software involve changing a state?,hlb14ct,t1_hlaznvz,1637356596.0,True
qxixkk,"Functional programming forces the programmer to be explicit about where state lives in their program and how that state changes.

This can make things quite a bit easier to deal with than the worst case of OO programming, where state is spread all through a graph of objects, any of which can change arbitrarily at pretty much any time.",hlb3h1t,t1_hlb14ct,1637357541.0,False
qxixkk,Lambda calculus is also Turing-complete,hla7i86,t3_qxixkk,1637345473.0,False
qxixkk,Yes..? Why is that worth mentioning?,hlacsz3,t1_hla7i86,1637347453.0,False
qxixkk,"I suppose it highlights that the expressive power of FP languages (which build upon the lambda calculus) is equivalent to imperative ones, as both are Turing complete?",hlc4t1v,t1_hlacsz3,1637373747.0,False
qxixkk,"I don't ""know"" the answer, but I speculate it might be similar to why mathematicians enjoy solving problems. It's kind of like a fine art. When you have it mastered and you can accomplish difficult tasks in a fascinating way, it's really rewarding.",hlaku88,t3_qxixkk,1637350359.0,False
qxixkk,Yeah that makes sense to me. Maybe it’s like mathematicians/physicists look at a problem differently than an engineer would because they apply different principles when solving problems,hlamh5p,t1_hlaku88,1637350977.0,True
qxixkk,"That’s a flawed analogy, because it implies that functional programmers aren’t doing engineering.",hlb6l4y,t1_hlamh5p,1637358796.0,False
qxixkk,"Yeah I didn’t mean to imply that. I guess I was just trying to say that engineering principles seem to be focus on modularity and breaking big systems down into smaller subsystems that operate without know what the other systems are doing (kind of like how your car’s engine doesn’t care if your headlight is broken. They operate somewhat independently and the whole system is modular). OOP fits that idea pretty well for the most part, as long as there isn’t too much encapsulation/inheritance",hlbm9hk,t1_hlb6l4y,1637365363.0,True
qxixkk,"It is true that FP is impractical from the perspective of complex systems, however, what is fascinating about FP is never the engineering applications, but the clarity of understanding that it brings. FP is lesson 101 in programming language theory. We learn to replace design patterns with higher-order functions, to eliminate illegal states with ADT, to improve program composition with abstraction, to prove the correctness of programs with invariant. These things are important enough on their own, furthermore, they also open the door to further study of PL theory.",hlbguj3,t3_qxixkk,1637363062.0,False
qxixkk,"Yeah that has been my understanding of it as well. It is good for a lot of theoretical stuff and proving things out, but can be difficult to implement in the real world",hlbmis6,t1_hlbguj3,1637365474.0,True
qxixkk,"Functional programming can be super useful for certain types of software. For example, we used the functional language OCaml in my compiler construction course in college. I couldn’t imagine how much harder that would have been in an imperative language. The recursive nature of a functional language was quite useful in that context.",hlarf39,t3_qxixkk,1637352839.0,False
qxixkk,"All I can say is I enjoy it a lot, I get a greater sense of satisfaction solving problems in FP, it’s made my work in imperative code bases better and the most complex things I’ve ever built have been in Haskell.

Edit to add: FP isn’t a new shiny thing for me though, I first learned it about 20 years ago.",hlb6pox,t3_qxixkk,1637358847.0,False
qxixkk,"FPP is a toolkit, a set of tools, not a singular tool.  It's easy to overlook this, because FPP programing languages can be limiting at first and seemingly unusually restrictive.

Because FPP is a set of tools, many modern mainstream languages have taken pieces from FPP.  If you're using a popular mainstream programming language today, you're using bits and pieces of FPP already without knowing it.

>Why do some people like it so much and act like it’s the greatest?

Some people like some of the tools FPP provides.  It's ideal to ask them what parts they like to get a better idea.  Odds are the parts they like you've already used and might be able to relate with them, if they don't use convoluted terminology to explain themselves so they are easy to understand.",hlcf46h,t3_qxixkk,1637378939.0,False
qxixkk,What does FPP stand for?,hllsj3e,t1_hlcf46h,1637557725.0,False
qxixkk,Functional programming paradigm.,hlm13jk,t1_hllsj3e,1637563360.0,False
qxixkk,Why not just call it FP?,hlnefhj,t1_hlm13jk,1637596165.0,False
qxixkk,"1. The conceptual simplicity of pure functions allows compilers to perform some elaborate optimizations they otherwise couldn't. E.g. [Elm is blazing-fast and builds small bundles](https://www.freecodecamp.org/news/a-realworld-comparison-of-front-end-frameworks-with-benchmarks-2019-update-4be0d3c78075/), and Haskell performs favourably in benchmarks relative to other high-level languages, sometimes approaching the speed of e.g. C. (YMMV, sufficiently complex Haskell applications tend to have hard-to-debug space leaks owing to ""laziness"", though the community has been moving towards more strictness in I/O, and laziness is not a feature of all functional languages. The community has also been building more understanding of how to fix these issues in recent years.)
2. Fearless refactoring. When people talk about ""functional programming"" these days, they often implicitly include a static ML-family type system. These expressive type systems allow you to encode a lot of information, preventing nonsensical states from being represented in the language. E.g., it's generally impossible to represent an out-of-bounds enum member in Haskell, unlike in C. When adding a new variant of a data type, the compiler will enforce that you handle it in all of your switch statements. Also, impure functions are carefully restricted into the IO type and every type ""inheriting"" from it (i.e. every instance of the MonadIO class). That means that when you're refactoring non-IO code, you can be sure that you haven't introduced an error merely by changing the sequence it's written in.
3. Effective abstraction encouraging uniform interfaces. Historically, software engineering has taken a very unscientific approach to design patterns. But many of them are patterns that are studied by computer scientists and mathematicians. There are arbitrary API differences in many languages between strings, arrays and vectors, but Haskell (specifically via its ML-style type system) allows a high degree of polymorphism for functions acting on these types. Now that I know the common interfaces (the most common are functor, applicative, monad, monoid and traversable), I find myself being able to pick up new libraries very quickly without having to learn their specific, ad-hoc interfaces.",hlg8eas,t3_qxixkk,1637454716.0,False
qxixkk,"Also, /u/WiggWamm, when a program is expressed in pure functions, it's trivial to parallelize it. Most of the speed increases in CPUs in recent years have happened via increasing core count instead of increasing clock speed or register width. Therefore, speedups in program execution time are mostly going to come from improved parallelism and concurrency. Parallelism and concurrency with conventional programming languages is difficult to get right, and takes lots of time. But the fact Haskell has parallelism API that is so simple is an enormous advantage.",hlh1kvj,t1_hlg8eas,1637469790.0,False
qxixkk,"I like this article to explain some things https://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html

But in a nutshell it makes code easier to reason about, easier to compose and refactor, and less prone to bugs.",hlb6cq5,t3_qxixkk,1637358700.0,False
qxixkk,"Well it's just plain fun ,not everything has to be 100% practical . If you like pythonic code fp in Haskell at least is like that on steroids . I like it specially for doing problems on code wars",hlc2c6q,t3_qxixkk,1637372584.0,False
qxv7ln,"Because to mirror a write means it's not complete until the last disk completes. So you are only ever going to get the performance of your slowest disk.

 Reading, on the other hand,  can be split up between disks. So if you wanted to read 2000 sectors of data, disk one might read sectors 0 - 999 while disk two is reading from sectors 1000-1999. Thus twice the speed.",hlcjs43,t3_qxv7ln,1637381464.0,False
qxv7ln,"Writing already happens ""in the background"" essentially, because writes are cached at the OS level and/or the disk controller level. What we call the ""write speed"" is how long it takes for the write operation to completely finish.",hlcge8t,t3_qxv7ln,1637379616.0,False
qxv7ln,Unless I am mistaken RAID 0 does what you are describing,hlehuwr,t3_qxv7ln,1637427047.0,False
qxv7ln,"Yes it does, but then you lose the redundancy (RAID 0 is striped). What I am asking about is this system I laid out where you achieve that performance, but still have the redundancy. Its such a simple solution that it either has been considered already and there are unknown reasons why it wouldnt work, or its so simple that it hasnt been done.",hlemrne,t1_hlehuwr,1637429091.0,True
qxv7ln,"This is a really interesting question. Would like to see what others have to say. I think with most modern RAID cards the cache handles this then writes to both and eliminates most performance issues with 2 drive RAID1. I don’t have real world experience with this since I never use RAID1, but… https://arstechnica.com/civis/viewtopic.php?t=1216469",hlcc56i,t3_qxv7ln,1637377375.0,False
qxv7ln,"A cache could handle this. But if the systems fails and the cache got corupted, you loose the redundancy, which was the reason to use a RAID in the first place.

Modern RAID systems like build into ZFS have various ways of handling reading and writing.",hldwb7i,t3_qxv7ln,1637417052.0,False
qxv7ln,"Because if you call sync() you expect the data to be present REGARDLESS of what happens next (power outage, disk failure etc). What happens if your disk fails milliseconds after the call to write to the disk? How long are you willing to wait if the disk IO is 100%? 

You’re basically proposing your data to be written in RAID0 until the disk is free to repair itself. You can simply do that by enabling a writeback cache, that way your writes go to RAM until such time that the disk is available to finish the write.

There are various better ways of improving performance, sacrificing your data safety is generally the wrong proposal.",hlf8m8f,t3_qxv7ln,1637438394.0,False
qy32gm,"I think you have the basic functionality of the routing table down. What I think you haven’t understood yet is that in networks there is a “routing protocol” which is responsible for figuring out what needs to be in the routing table for each router in order for it all to work. There are lots of different routing protocols from simple to complex (e.g. RIP, OSPF, BGP, etc). But they all are basically applications that run on each router, send packets of information to directly attached neighboring routers, and build up some kind of internal state about what the network “looks like” from which they decide what the routing table should be. If they do this correctly then the network works: packets get forwarded over the correct links to reach the correct destination. However it’s highly non-trivial to do correctly when things in the network start changing. Building such a routing protocol is a classic example of distributed systems.",hldq62c,t3_qy32gm,1637413414.0,False
qxh10d,"Listen, learning C has nothing to do with compiler design, first study compiler design, then try to relate the concept. And you can find on internet the compilation process of a C program.....that is pre processing, compilation , assembly , linking . U can study these steps in details",hl9bk0m,t3_qxh10d,1637332567.0,False
qxh10d,"Yes , but I want to study it to get more confident that what actually is happening behind the scenes. I tried to study it in this way , like when I am not able to relate any concept I got for searching over internet and its a lot more time taking. Thank you for responding!!!",hl9d8p9,t1_hl9bk0m,1637333321.0,True
qxh10d,"Yes, you are doing good work, whenever u in doubt feel free to search, there are multiple courses on YouTube, one I can recommend is neso academy, there others too, but I liked their way of teaching",hl9msnv,t1_hl9d8p9,1637337334.0,False
qxh10d,Neso academy is fantastic.,hla3y60,t1_hl9msnv,1637344076.0,False
qxh10d,[deleted],hl9qclz,t1_hl9d8p9,1637338753.0,False
qxh10d,"I referred several books like Let us C , C in depth and also I read from my professor and internet. I also followed KN King for some topics but Ritchie is the book that I read almost complete.",hl9tfr4,t1_hl9qclz,1637339983.0,True
qxh10d,[deleted],hl9trih,t1_hl9tfr4,1637340112.0,False
qxh10d,"Yeah I started revising C. I can understand most of the things but where I am lacking is , I am not able to related Compiler theory I read on my course to the programming. Like I am getting this or that error what might be the cause in term of compiler and many other things.",hl9uo2g,t1_hl9trih,1637340467.0,True
qxh10d,[deleted],hl9uyhp,t1_hl9uo2g,1637340579.0,False
qxh10d,"Haven't you read compiler design till now? My intention is when I read CD it was more theoretical and examples used form real programming were very much less. So I want to see that theory happening in real C programming. But I think no one studies in this way, I am the only one ( may be I am thinking wrong ) :), and no one gonna write a book or course just for me. Thanks for your consideration.",hl9vr94,t1_hl9uyhp,1637340889.0,True
qxh10d,"I would not try and understand how the higher level abstractions translate to the lower C level. I would instead learn from first principles on how a computer works and build the abstractions up from there. You will learn how a CPU works. How the data bus and registers are used. How memory is laid out and accessed. The call stack and how that works, etc.. This will go a long way in understanding how C sits on top of this and how it's data structures like arrays and structs map to this and understanding how pointers work the way they do and why. Check out these resources:


1. Read [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. Watch [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
5. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
6. Grab a copy of [C programming: A Modern Approach](http://knking.com/books/c2/index.html) and use it as your main course on C.
7. Follow this [Tutorial On Pointers And Arrays In C](https://github.com/jflaherty/ptrtut13)

The first four really help by approaching C from a lower level of abstraction (actually the absolute lowest level and gradually adding layers of abstraction until you are at the C level which, by then is incredibly high!) You can do all four or pick one or two and dive deep. The 5th is a great introduction to computer science with a decent amount of C programming. The sixth is just the best tutorial on C. By far. The seventh is a deep dive into pointers and one of best tutorial on pointers and arrays out there (caveat, it's a little loose with the l-value/r-value definition for simplicity sake I believe.)",hl9ga7q,t3_qxh10d,1637334643.0,False
qxh10d,Ty for this list,hlaalla,t1_hl9ga7q,1637346649.0,False
qxh10d,I'm taking th CS50 class now. It's amazing. Well thought out lectures. Long but fun.,hlcwjcy,t1_hlaalla,1637389418.0,False
qxh10d,"Thanks, well that's a big list as I am a final year undergrad and also many topics, I may already have learned. I will study as much I can.",hl9u2o5,t1_hl9ga7q,1637340231.0,True
qx827n,Not an embedded dev but I'd guess it's probably some type of PROM or EEPROM or some such for the program memory and just SRAM for RAM,hl7z8i8,t3_qx827n,1637300282.0,False
qx827n,Yup precisely. I think EPROM would be a better option for ROM though but yeah EEPROM and PROM works too,hl807k5,t1_hl7z8i8,1637300839.0,False
qx827n,"For a small calculator, the program memory is probably just masked into the chip, and then it uses SRAM cache. For a calculator with slightly more advanced features, it might have EEPROM or even some flash if it's a nice graphing calculator, AFAIK UV-EPROM has almost been entirely phased-out.",hl9o7gz,t1_hl807k5,1637337900.0,False
qx827n,"Go look at a Casio teardown. Although it's possible that these days it's all done on a single IC, in which case I imagine it's just some kind of RAM. It's not like it persists between power ons.",hl8d2us,t3_qx827n,1637309848.0,False
qxl1o1,"DHTs are a building block to the larger whole. So, simply put, it depends on the system where the DHT is being used.

There is nothing about DHTs in and of themselves that declares how data is stored. It simply defines how keys map to values and how many different nodes can connect/disconnect without causing considerable disruption to the system.

In a real application, one might PUT some data into the DHT and the implementation thereof will decide where to store it and with what actual backend. When one wants to GET the data, the implementation will get it from any available node that has it.

Take BitTorrent as an example. The content is described by a hash, and clients look up the hash. The DHT responds not with the data itself but with IP/Port pairs that the client can connect to in a subsequent request to download the data.

Don't know if this helps or not. Basically, how and where the data is stored is not defined. That's up to the implementation. DHT simply defines the infrastructure of the table and how it can maintain high availability with many nodes.",hla63sv,t3_qxl1o1,1637344916.0,False
qxl1o1,"Got it! Thank you so much, this cleared up a lot of things for me.",hlaija2,t1_hla63sv,1637349514.0,True
qxl1o1,Glad it could help!,hlb4xeq,t1_hlaija2,1637358129.0,False
qx22or,"The midian of two sorted arrays in time complexity log(n) where n is the sum if lengths is not quite this simple im afraid. Rather then the coding part, thinking about the algorithm needed is harder.",hl8474x,t3_qx22or,1637303433.0,False
qx22or,"What you have come across is the fact that in maths, ""empathy is hard"". This, of course, does not mean that mathematicians are cold people, but that it is very hard to judge whether some problem is easy or hard, especially for someone who does not have the same skills as oneself.

This is because you often require an insight into why something is right, which is hard to come by, and even if you so, the road you went down to find it is likely unique to you.

Thus, the person who created this problem likely thought that this was easier than it actually is, for a variety of reasons.",hlsi6c9,t3_qx22or,1637688805.0,False
qx22or,Number 4 is hard if do it log(m+n),hl9dafw,t3_qx22or,1637333342.0,False
qwqv67,"A concise way I like to look at it is a database does two things:

1. Stores data in some kind of file system (a data store).
2. Has a way to access that data systematically.",hl4rnyj,t3_qwqv67,1637250401.0,False
qwqv67,"I usually recommend Martin Kleppmann's book ""Designing Data-Intensive Applications"". It starts by considering a very simple database (a single file on disk) then builds from there. I'm sure it will help you build intuition.",hl4uicn,t3_qwqv67,1637251539.0,False
qwqv67,this book is really good! you won't regret reading it,hl4v6mh,t1_hl4uicn,1637251807.0,False
qwqv67,"Yup, DDIA ftw",hl7gib0,t1_hl4uicn,1637290751.0,False
qwqv67,"I think you're overcomplicating it a bit. Its really just a program that stores data. Nothing special happens to a computer or server when you create a database instance on it. As far as interacting with a database there are usually multiple interfaces. Some will have interactive shells (the two I'm familiar with are postgres and hive) where you can write sql or some other basic commands to manage the data. There are also db browsers which are basically a frontend client for interacting with the database; you can execute sql, see the results of queries, explore the database, ect. This is how most developers would interact with the database. There is also usually some sort of API for a website backend or data pipeline to interact with. 

> Secondly, if you could also briefly talk about the computer science of database management systems?

There's a myriad of different concepts that go into a database, do you have a more specific question on how they work?

> Please, also let me know if I'm thinking about this incorrectly. I am a self taught person with a lot of black boxes.

The biggest tip that I have for this is to try not to overcomplicate like I mentioned before. Just take whatever the concept is in its most simple form and try to build off of that understanding rather than worrying about all of the complexity up front. It just makes understanding a little more approachable.

Hope that helps! 

Edit: Comma Splice, addition",hl4nkkc,t3_qwqv67,1637248732.0,False
qwqv67,This is a great and much appreciated response. I understand. Thank you!,hl4r7cb,t1_hl4nkkc,1637250217.0,True
qwqv67,"The term database is pretty abstract.

It can be implemented in multiple different ways, but essential is a software that stores organized data.

Can store data in memory, on disk, tape, anywhere pretty much.
Can be in form of a library (SQLite) or a standalone software (postgres).

Hope this helps!",hlc441e,t3_qwqv67,1637373415.0,False
qwqv67,"Von Neumann is one a of the fathers of modern computations and his bigger cs contribution was make a paradigm when all data the same, one part of the data have ""meaning"" and are programs, you could store the data in whatever way you want for example put all in the same place whit index and then say to the machine search here this, most dB engines(all) take what and where you send for example in SQL and optimize and help whit replication and errors but a database is literally a file which have some time of structure like a excel (or the data science alternative csv)",hl631ze,t3_qwqv67,1637269058.0,False
qwqv67,Are these similar or the same amount 0f prestiges,hm2s8ne,t1_hl631ze,1637873254.0,False
qw8yg4,Cool. Was thinking about logic gate with complex number integration. You can take the work from here...,hl21oev,t3_qw8yg4,1637194273.0,False
qw8yg4,Really cool. I just ordered 3. ☺️,hl3xyyr,t3_qw8yg4,1637235360.0,False
qw8yg4,Agreed.  I just ordered 2!,hl4twxd,t1_hl3xyyr,1637251301.0,False
qw8yg4,:) Hope you love them. The new Kits are super sleek. Be careful soldering the micro USB so that theres no shorts. Any questions ask me. Instructions here: https://tsjelectronics.com/instructions,hl5vptj,t1_hl4twxd,1637266155.0,True
qw8yg4,Thanks! I hope you like them! If there's any problems just message me on here for through the contact page at website.,hl5veh7,t1_hl3xyyr,1637266031.0,True
qw8yg4,"""My mom said it's special""

I love this.",hl5n2u1,t3_qw8yg4,1637262758.0,False
qw8yg4,\>.<,hl5v75b,t1_hl5n2u1,1637265952.0,True
qw8yg4,Just ordered a blue one for my desk at work! Awesome little project. Congrats on the article!,hl9xqq7,t3_qw8yg4,1637341660.0,False
qw8yg4,"Thanks, I was really geeked that someone wrote a little something on my project, and thanks for supporting me making more stuff. Hope you love it.",hla57r7,t1_hl9xqq7,1637344568.0,True
qwnv1m,"OK, programming sounds like a gothic romance now.",hl446od,t3_qwnv1m,1637239377.0,False
qwnv1m,"She looked lustfully at her daemon, wondering what he was thinking. Her hand on his, her breath breathing his breath, her chest heaving in anticipation. She had one of the semaphores, she just needed the other. She slipped her fingers along his iostream and reached for the mutex. He gasped in ecstasy and dropped the semaphore. Finally! But, alas, as she reached for the now-available mutex, she realized that in her haste she had dropped the one already in her other hand.",hl4x3dj,t1_hl446od,1637252570.0,False
qwnv1m,One day this will become as part of theatrical performance.,hl4yyjh,t1_hl4x3dj,1637253313.0,False
qwnv1m,OMG.,hl4xgi2,t1_hl4x3dj,1637252715.0,False
qwnv1m,"Same thing, different names. Sounds even more dramatic in French: étreinte fatale.",hl4cfb0,t3_qwnv1m,1637243742.0,False
qwnv1m,"Deadlock (which is sometimes called the deadly embrace) is another crippling condition.  
 It occurs when two or more programs are each waiting for the others to   
complete - or even just to produce a data value - before proceeding. ...  
 There's a historic reason why deadlock exists.",hl45x3s,t3_qwnv1m,1637240364.0,False
qwnv1m,"They're the same thing, deadlock is the common term in CS.


Let's assume processes P1, P2 and resources R1, R2


P1 locks R1
P2 locks R2
P1 tries to lock R2, fails, waits until R2 is released
P2 tries to lock R1, fails, waits until R1 is released


Now both processes are waiting for the other process to release the resource they need, but neither can release the locked resource before locking the other one. The end result is that both processes are stuck waiting indefinitely.",hl4a6tr,t3_qwnv1m,1637242623.0,False
qwnv1m,"One common way to avoid deadlock is to have processes acquire resources in the same order.

In the example above: if P1 and P2 always attempt to acquire R1 first and then R2 second, then you can guarantee that the system won’t deadlock.",hl4stxw,t1_hl4a6tr,1637250870.0,False
qw5g3c,"Doesn't sound like this is exactly what you are looking for, but the best way to accomplish that for me has been to find past exams on the topic with answer keys. Either from my own university or another. If helpful, here are the past finals and midterms for the first OOP class at my old school, [https://exams.ubccsss.org/cs210/](https://exams.ubccsss.org/cs210/).",hl106st,t3_qw5g3c,1637178530.0,False
qw5g3c,Thank you kind stranger 🙏,hl16cth,t1_hl106st,1637180985.0,True
qw5g3c,"Doesn't your school keep copies of old exams on file? 

They are often in library if not online already.",hl173zj,t3_qw5g3c,1637181285.0,False
qw5g3c,Only some teachers do that but not often. If there's a website that has exams of every lesson it would be great. Googling past exams is a but chaotic,hl194jk,t1_hl173zj,1637182095.0,True
qw5g3c,I thought at most universities it was an exception when they don't have old exam copies  for undergrad courses. My university regulation required professor to make special request to exclude them from library database.,hl1n4nb,t1_hl194jk,1637187883.0,False
qw5g3c,"Geeks for geeks has hundreds if not thousands of practice interview questions where you can filter by subject, company, difficultly and programing in java, python, c++, inside their own web environment.",hl2uro8,t3_qw5g3c,1637207702.0,False
qw5g3c,Upvoted cause I want to know the same,hl0sx23,t3_qw5g3c,1637175627.0,False
qw5g3c,Check out [https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/) . They have quizzes as well as other resources related to all CS subjects.,hl44hf8,t3_qw5g3c,1637239552.0,False
qw5g3c,Yeah it’s called the end of chapter textbook questions,hl1hdjn,t3_qw5g3c,1637185484.0,False
qw5g3c,"Quizlet, chegg",hl2u04h,t3_qw5g3c,1637207321.0,False
qwtqeb,"I'm confused. How does the algorithm decide which bits to flip without an RNG? If you know how to flip bits randomly, you've already solved the problem.",hl50r8j,t3_qwtqeb,1637254016.0,False
qwtqeb,Excellent point. I guess I was thinking in terms of the physical world where you can just flip 8 coins and have a random byte value.,hl54fb3,t1_hl50r8j,1637255453.0,True
qwtqeb,"If I understand what you mean, it is to 'physically' randomly change bits. I think that would be against the idea of computers in the first place. If computers randomly changed physical bits we wouldn't be able to predict and program them. 

Anyways I could also be wrong there :D If so I would be very interested in an example/explanation.",hl5ja1e,t1_hl54fb3,1637261250.0,False
qwtqeb,"As far as computer science is concerned nothing is random but you can for example take a temperature sensor with 10 bit precision and try to get a 16 bit reading from it, the last 6 bits will essentially be thermal/electrical noise. Then you can treat that as an I/O device and acquire random bits like that.",hl5kgo5,t1_hl5ja1e,1637261712.0,False
qwtqeb,I think that you want to read this: https://en.m.wikipedia.org/wiki/Hardware_random_number_generator,hl5qbj9,t3_qwtqeb,1637264044.0,False
qwtqeb,New computer have hardware modules who measure temperature an think like that to generate true random number if not you need a seed you could make alot of functions  for different people and burn it like zcoin but the seed still exists,hl60rlx,t3_qwtqeb,1637268153.0,False
qwtqeb,Because pseudo random number is a very complicated thing. Math rocks!,hl7gc8p,t3_qwtqeb,1637290674.0,False
qwtqeb,"First, theres no reason to shy away from math - its marely a tool.

Second of all, the solutions is impractical - even if you could ""flip a coin"" to determine if a bit would be flipped it would cause problems when using upper/ lower bounds.

For example lets say you have two bits and want a random number between zero and three. You roll both bits and both gets flipped to one, because this is outside of your bound you need to turn that number into the range [0,3] with equal probability. The only way to do that is roll *both* bits again. 

So despite having this ""bit flipper"" black box the solution would still be subpar to simply sampling from semi random thing like tempeture.",hl857ri,t3_qwtqeb,1637304114.0,False
qwegv6,"Well to start with, it can check whether argument and return types match at compile time.


    int takesAnInt(int x);
    
    int main() {
        char c = takesAnInt(3); // Error, return type isn't a char!
        takesAnInt(""foo""); // Error, argument should be an int!
        return 0;
    }

Those checks aren't possible ahead of time in a dynamically-typed language, because variables don't have definitive types until they're evaluated.",hl2oc7x,t3_qwegv6,1637204612.0,False
qwegv6,Depending on your compiler settings that first error would actually just be a warning.,hl3kuw7,t1_hl2oc7x,1637224560.0,False
qwegv6,Python solved that with type hints.,hl49bym,t1_hl2oc7x,1637242181.0,False
qwegv6,IDEs can basically compile as you go (sort of) and show you many type errors in real time. So if you try assigning a string to an int variable you'll immediately get a red squiggly line telling you how dumb you are. Same thing with argument and return types as said in the other answer. This can be especially helpful when using interfaces and/or inheritance as the the variable you give a function might not be *exactly* the same type as the function requests (could be a subtype). Most IDEs are also pretty good at inferring what function overload your are trying to use too.,hl3ks3o,t3_qwegv6,1637224497.0,False
qw5zd4,"Just that the optimal makespan has a time greater than or equal to the longest job, which of course it must. Trivially, if there is only one job, then T\* = t\_1. But no matter what T\* can never be shorter than the longest-running single job.",hl0r4mu,t3_qw5zd4,1637174926.0,False
qw5zd4,Okay so why is it max\_i t\_i not just max t\_i?,hl0ru6r,t1_hl0r4mu,1637175200.0,True
qw8tom,"It’s definitely possible. Like imagine if the python developers decided to build into the interpreter a stage where it runs a static type checking tool like mypy on your code and refuses to execute it if it fails. 

Java might even fit into that description because the compiler requires static typing but only compiles code to byte code which if interpreted by the Java Virtual Machine when you run the program.",hl1pth2,t3_qw8tom,1637189020.0,False
qw8tom,"Yes of course, there is typescript for example.
 
You can have a compiled language not strongly typed or an interpreted language that is typed. The two concepts are pretty orthogonal.

Usually interpreted languages are not typed cause their usual main goal is development speed and adding types means you need to type more (no phun intended lol).",hlc4kkq,t3_qw8tom,1637373634.0,False
qvgwfj,Keeping it accurate as the code base changes.,hkwf1qy,t3_qvgwfj,1637095654.0,False
qvgwfj,"This. I always find docs are amazing at the beginning when the engineer(s) who do the initial implementation do a great job of summing everything up. Then over time more engineers get introduced to the codebase and make changes which affect documented features without being aware of the documentation, creating a drift.",hkyfdj3,t1_hkwf1qy,1637128335.0,False
qvgwfj,Waiting until the very end of coding and debug to start.,hkwiwq5,t3_qvgwfj,1637097163.0,False
qvgwfj,Getting (non-great) engineers to understand how important it is.,hkxacz7,t3_qvgwfj,1637108633.0,False
qvgwfj,"The hardest thing in writing documentation is somewhat similar to how we try to code the same way in a team using code conventions.

Defining your team's documentation conventions and having everyone on board is freakin' **hard**.

Here's what I've been preaching to my team and those who work with us.

1. Start with high level infrastructure stuff using something like https://c4model.com/ The 2 first levels are usually enough to figure out where I should do a fix or where I should attempt to work.

2. Before coding something, start with writing functional specifications first. IMHO, a good reference for writing no-bullshit-functional-specs is https://www.joelonsoftware.com/2000/10/02/painless-functional-specifications-part-1-why-bother/ 

Its form could be simple sentences and bullet points just like within the article. A flow diagram or a sequence diagram is sometimes more efficient in communicating the overall intent. My preferred tool to draw those diagrams is https://mermaid-js.github.io/mermaid-live-editor Since it's all text, you can even commit your diagrams within your code repository.

3. I usually don't care much about inline comments unless it's really trippy and complicated.

4. I do care about good naming conventions for your classes, methods, functions and variables.",hkxtnqm,t3_qvgwfj,1637117131.0,False
qvgwfj,"Depends on the project and the end user. In my previous project at work, I developed the entire app and had to also do a large of acquiring and documenting the business process from the customer. So not only did I had to comment and document my code, but I also had to document the business side.

The business side is generally where I hate having to document, usually because the business folks often don't have a consistent process.",hkwrwhe,t3_qvgwfj,1637100755.0,False
qvgwfj,"For me it's most difficult to find find the right time to write documentation comments. In the first place I think, that the code will change too much while debugging. After I got it working I struggle with the amount of methods which I have to comment. ^^""",hkwjxkf,t3_qvgwfj,1637097567.0,False
qvgwfj,It can be difficult to strike the correct balance between too little and too much comments.,hkwo7z1,t3_qvgwfj,1637099271.0,False
qvgwfj,"If you're doing things the correct way, your code is going to be negotiated, and should flow from tests - to the point the test code itself is a significant source of documentation in itself. This helps to solve the most significant problem with documentation, i.e. that it goes stale, because when you can't commit code that doesn't pass your unit tests, then your unit tests inherently must be up to date. And because you negotiate your code with those people who rely on it (larger system architecture or clients) you help to solve the *other* most significant problem with documentation, i.e. that it's only able to be understood by the person who wrote the code at the time they wrote it.",hkx0j8w,t3_qvgwfj,1637104352.0,False
qvgwfj,"I find that writing documentation is a chore as much as designing algorithms and writing software as a chore: it's all equally tedious. For the case of documenting code, I structure my labels and functions together with commentary for the purpose of a ""clear narrative"". My aim is that I can read my code after six months of not seeing the code. Clear code narrative supports bug hunting and feature tweaking.

As for writing user documentation, this is equally important as writing correct code. Every software system is unique even when there are overlapping themes that are common to software titles. I want my users to be able to comprehend the intent of the features I've exposed to them. Powerful software systems are sophisticated by nature, users deserve some explanation about ""why"" and ""how"".

For my perspective, documentation is not optional. I need to communicate to the code writers and to the users about the meaning of my software. This perspective helps me slog through the chore that is writing computer software and communicating the meaning through documentation.",hkxeenm,t3_qvgwfj,1637110420.0,False
qvgwfj,The part where I realize no one will ever look at it or do anything with it.,hkx2rxf,t3_qvgwfj,1637105323.0,False
qvgwfj,Where I work we just don’t have enough and the systems are very complex. End up having to dig through code to figure it all out,hkxanqb,t3_qvgwfj,1637108764.0,False
qvgwfj,Turning on the computer,hkz65vv,t3_qvgwfj,1637149195.0,False
qvgwfj,"If your document involves APIs, use Postman. Postman automatically generates documentation for you. So, if there are any changes to your APIs, just call it once with the changes on Postman and it will change the documentation for you. In this way, your work will be made much easier and you'll be spending a lot less time on documentation.",hl0juuz,t3_qvgwfj,1637172096.0,False
qvgwfj,"The most difficult part is that my boss doesn't value documentation until he's made to, so we aren't encouraged to do our best in preparing for projects with checklists and assessments, comment the code,  or document the project holistically once done. I try my best to get into the habit of documenting my own work, but we're lacking that important push from management to really cement that thinking",hkwxi9u,t3_qvgwfj,1637103056.0,False
qvgwfj,"Yeah but you dont need documentation until you need it so go back and write it all down after something goes wrong, of course.",hkxnjn1,t1_hkwxi9u,1637114442.0,False
qvgwfj,Keeping it in one place. Inevitably management will change tools of reorganize everything so then nothing is searchable.,hkydexy,t3_qvgwfj,1637127144.0,False
qvgwfj,"As I am learning this, it is the SAD part. Not sure if this is even actually used in building a software. Can anyone working in the industry answer this?",hkyenth,t3_qvgwfj,1637127893.0,False
qvgwfj,"I always found it difficult to put my thoughts into words. Like I know why I'm doing certain stuff because most of it is intuition but when I'm asked to explain why I did something it's takes me some time to explain it in words. I don't know, I think it's just a problem I have.",hkyh1kj,t3_qvgwfj,1637129402.0,False
qvgwfj,"Getting started, especially if large parts of the codebase have outdated or no documentation.",hkyw8ua,t3_qvgwfj,1637141110.0,False
qvgwfj,writing it after the fact. like when im done coding but only god knows how it works,hkyxc6i,t3_qvgwfj,1637142048.0,False
qvgwfj,Knowing when to do it and knowing when not to. Knowing who the audience will be. Making it be good but also generated.,hkzwkby,t3_qvgwfj,1637162902.0,False
qvgwfj,personally for me writing a documentation is the hardest part of writing a documentarion,hl08akz,t3_qvgwfj,1637167610.0,False
qvi861,"Multi-tasking: multiple different tasks sharing a cpu core. Each task gets a share of cpu time, then the next task. On one given core, only one task can run per cpu cycle. By switching between tasks, we can multitask. Note that with multiple cores, we can have true multitasking.

Parallel processing: one task is subdivided among many cores. Think GPU processing a physics simulation, or DNN inferencing. Some problems are parallelizable, which means they are suited for parallel processing. Hardware accelerators like GPU are used for these types of tasks.",hkwp9ys,t3_qvi861,1637099691.0,False
qvi861,Thank you! I think this would it much more simple and appropriate answer for an A-level question (If such was to come up in mock or exam).,hkws5mn,t1_hkwp9ys,1637100858.0,True
qvi861,"I'm cooking dinner. And while I'm chopping vegetables, my neighbor knocks on the door to give me a miss-delivered package, so I stop what I'm doing, put down the knife, answer the door, take the package, then return to chopping my vegetables.

In that scenario, I was doing two different tasks (chopping veggies and receiving a package) where at least one task was not completed in its entirety before also doing/finishing another, but only one of those tasks is being worked on at any single moment. This is ""concurrency"" (also sometimes called multi-tasking).

Now, I'm still cooking dinner, but while I'm chopping vegetables I'm also boiling noodles. The noodles are still cooking even though I'm not attending to them. Two tasks are being worked on (chopping, boiling) at exactly the same moment. This is ""parallelism"".",hky3qlg,t3_qvi861,1637121838.0,False
qvi861,"Multitasking is more of an operating systems concept that means being able to run multiple individual processes (or programs) at the same time, or so that their executions overlap. The execution of multiple tasks may or may not actually take place at the same moment, and multitasking and what appears to the user as simultaneous execution can be achieved e.g. through rapidly switching between tasks instead.

Concurrency and parallelism are more general concepts, and they can take place in several different ways and in different kinds of contexts. Concurrency and parallelism are relevant to the OS concept of multitasking, but they're really concepts of a different level.

Out of those two, concurrency means being able to execute more than one thread of control so that their executions overlap. The threads can be from the same or from different processes. The execution doesn't necessarily take place at the same actual moment, i.e. it may be that any particular moment in time only one thread of control is being executed.

You could think of multitasking as a form of concurrency on the OS level, although when talking about concurrency, you'd usually be interested in multiple threads of control that have some kinds of dependencies or communication between each other.

Parallelism means actually being able to perform multiple operations at the same moment. Depending on the context (and which level of parallelism we're talking about), the operations can be from the same task, from different threads, or from entirely different tasks. But what separates parallelism from concurrency is that multiple things are actually being done at the same moment.",hkwoqgw,t3_qvi861,1637099473.0,False
qvi861,"Ah, that does make sense when thinking it like that. And from reading your reply as well as u/LowLvlLiving, I believe that I now have a better understanding of it now:

* Multi-tasking is more to do with OS types and its ability to perform multiple task nearly simultaneously.
* Assuming conditions are correct for both concurrency and parallelism. Parallelism will be slightly faster in performing task as it does not require constant switching between two or more jobs.
* Parallelism will also be more effective in processing larger or more tasks due to previous bullet point.

(I hope this correctly summarise the points from both of your replies? Also thank you both for answering my question.)

\#Edit: Second bullet point (new) - parallelism is slightly faster in performing task as it does not require the task to be finished before starting the next task, since a coordinator or job scheduler have already assigned it to another CPU/GPU core.",hkwr20v,t1_hkwoqgw,1637100410.0,True
qvi861,"That's generally the gist of it.

> Multi-tasking is more to do with OS types and its ability to perform multiple task nearly simultaneously.

Well, yeah. Or so that the OS or its user can switch between multiple applications or programs without having to terminate one of them first. You'll generally want the OS to be able to switch between them quickly and relatively often so that switching between applications is fluent and so that your music doesn't break up and so that a web server can get around to responding to requests quickly. You want it to be fast enough to *look* like the computer can run multiple things at once. Technically, however, it's multitasking whether it's really quick or not.

Multitasking is something you generally take for granted nowadays, as there haven't been any non-multitasking operating systems in mainstream PC use in the last ~25 years. But, well, *somebody* had to implement multitasking, and sometimes it can be useful to know how and why it works, so we still learn it.

> Parallelism will be slightly faster in performing task as it does not require constant switching between two or more jobs.

Depending on the task (or tasks) and the resources you have, it can be faster by any factor. For example, if you have *n* processor cores and your task can be divided into *n* equal parts that can be run (mostly) independently of each other -- or if you have *n* separate tasks you want to run -- you can ideally get a speedup of *n* times over doing those parts or tasks one after the other. In the best case anyway.

You might save something by not having to constantly switch (as the switching itself has a cost), but the real reason parallelism can give speedups is that it allows you to make use of the multiple cores or other parallel resources you have available.

Parallelism is kind of like dividing work between multiple workers and having them actually work and advance tasks at the same time instead of one of them working and another having to wait for the first one to finish before starting their own part.

Some tasks are easy to parallelize, but in case of other tasks it might be harder, so the speedup you can achieve might be smaller or even none.",hkwx7p3,t1_hkwr20v,1637102933.0,False
qvi861,"in short and to not get confused, just target the core, one core doing multiple tasks is multitasking while many cores doing one task is parallel processing",hkymgs3,t3_qvi861,1637133182.0,False
qvi861,"From what I understand, concurrency (multi-tasking) is about responsiveness and parallelization is about doing more work at the cost of more resources.  


If we have 2 tasks: A and B, each takes 5 seconds to complete. If we run the tasks concurrently, the operating system will do, say, half a second of work on A and then switch to do half a second of work on B - switching back and forth until both are completed. This will actually take a little *longer* than just running them end to end. So why use it?  


Well, what if task A is processing a file and task B is checking for user input. If we run A then B, while A is running the computer will become totally unresponsive as it's completely focused on completed job A when computers have other task to consider.  


It we parallelize A and B we need to have a process free to take on the new load of work and to make sure that running these jobs at the same time isn't going to have any side effects. Parallelization at a large scale can cause nightmares with race conditions if you're not careful, but it great for small batch work - if you've got the computing power to spare.",hkwk7z2,t3_qvi861,1637097679.0,False
qvcvqa,"I also want to know, I didn't find any books but there are some blogs and YouTube videos on about them. You can check Intel tino core uefi project maybe that will give you some reference!",hkydoki,t3_qvcvqa,1637127303.0,False
qvcvqa,You should ask this in r/embedded because usually embedded engineers are responsible for implementing first stage bootloader( bootloaders are very tied with hardware because bootloaders are responsible for hardware initialization ),hkye96x,t1_hkydoki,1637127646.0,False
qvcvqa,"That's a good one, I did so. Thanks!",hl1y4wn,t1_hkye96x,1637192658.0,True
qvcvqa,"I find a book on UEFI it is a good one ""Beyond BIOS Developing with the Unified Extensible Firmware Interface""",hl2k9pm,t1_hl1y4wn,1637202728.0,False
qunqn2,So old they put wood paneling on it,hkretam,t3_qunqn2,1637006866.0,False
qunqn2,That's how you know it was released in the 70's.,hkrq3n6,t1_hkretam,1637011281.0,False
qunqn2,Woodgrain grippin,hkt2wlc,t1_hkretam,1637032313.0,False
qunqn2,No seriously. What is that?,hkvctdk,t1_hkretam,1637080692.0,False
qunqn2,"It is a cover. There is a silicon wafer in the middle, under it. Fine wires connect from the legs to pads on that wafer.

original chip:

[https://www.semanticscholar.org/paper/The-Intel-4004-Microprocessor%3A-What-Constituted-Aspray/7ba1ae43acd70844b8c00e47c436354339c12bfe/figure/6](https://www.semanticscholar.org/paper/The-Intel-4004-Microprocessor%3A-What-Constituted-Aspray/7ba1ae43acd70844b8c00e47c436354339c12bfe/figure/6)

modern-day recreation similar to old design:

[https://fuentitech.com/diy-silicon-people-build-integrated-circuits-similar-to-intels-4004-cpu/192855/](https://fuentitech.com/diy-silicon-people-build-integrated-circuits-similar-to-intels-4004-cpu/192855/)",hkvvnua,t1_hkvctdk,1637088010.0,False
qunqn2,you can't trick me this is a hexbug,hkrt3np,t3_qunqn2,1637012453.0,False
qunqn2,Came here to share some nostalgia with my fellow 2000’s kids.,hkv254d,t1_hkrt3np,1637076374.0,False
qunqn2,That is a hexbug,hkv2uko,t1_hkrt3np,1637076668.0,False
qunqn2,The top part looks like an ice cream sandwich,hkreod3,t3_qunqn2,1637006811.0,False
qunqn2,"Three other CPU chip designs were produced at about the same time: the Four-Phase Systems AL1, done in 1969; the MP944, completed in 1970 and used in the F-14 Tomcat fighter jet; and the Texas Instruments TMS-0100 chip, announced on 17 September 1971. The MP944 was a collection of six chips forming a single processor unit. The TMS0100 chip was presented as a ""calculator on a chip"" with the original designation TMS1802NC. This chip contains a very primitive CPU and can only be used to implement various simple four-function calculators. It is the precursor of the TMS1000, introduced in 1974, which is considered the first microcontroller—i.e., a computer on a chip containing not only the CPU, but also ROM, RAM, and I/O functions. The MCS-4 family of four chips developed by Intel, of which the 4004 is the CPU or microprocessor, was far more versatile and powerful than the single-chip TMS1000, allowing the creation of a variety of small computers for various applications.

//////////////////////

For more information on little-known history and other 1970s events, consult the 50YearsAgoLive Project, a Twitter program that reports events from exactly 50 years ago as if they’re happening in real time. It is meant to stoke an interest in history by making it accessible to the everyday reader:

[https://twitter.com/50YearsAgoLive](https://twitter.com/50YearsAgoLive)",hkr5evh,t3_qunqn2,1637003155.0,True
qunqn2,"The 4004 is rather lovely to study and play with, you can read the datasheet and write code for it in raw hex in a matter of hours.  What most people don't understand about the 4004 is it is less a general-purpose CPU and more a ""multichip microcontroller"".  
  
The 8048 microcontrollers and beyond are spiritual successors to the 4004 I understand.  
  
The 8080, then 8086 that all our PCs hark from have their roots in the 8008 which was the first general-purpose single-chip CPU I think.",hkto1mp,t3_qunqn2,1637043788.0,False
qunqn2,What happened to Intel 0 through 4003?,hkux1l3,t3_qunqn2,1637074192.0,False
qunqn2,This is very cool,hkv8jdk,t3_qunqn2,1637078990.0,False
qv9yun,"Take some very simple programs, like just one loop or if statement if you don’t understand those and walk through it by hand. Like have a list of what the variables values are and update them every instruction. Also ask yourself why each line of code exists. 

Then after you are done with that run the program in a debugger to compare. If you did it right awesome, try a different harder program, if not figure out why you made the mistake.",hkv1jkc,t3_qv9yun,1637076127.0,False
qv9yun,"Here are some things that really helped me:
1. Relating the algorithm or concept to something in the physical world (like imaging sorting a deck of cards when doing sorting algos)
2. Physically drawing diagrams and tediously going through every step using examples.
3. Going through each line of code and trying to understand its purpose, dissecting every component of the syntax as steps you would take to complete the problem irl.
4. Practice writing small programs related to what you’re learning, you can make this part fun by involving other stuff you’re interested in (when I was learning object oriented programming in high school I made a drumset program since I’m into that)

Good luck :)",hkw2b8y,t3_qv9yun,1637090628.0,False
qv9yun,Thanks so much!!! I will try implementing these steps into my learning process,hkz7e68,t1_hkw2b8y,1637150049.0,True
qv9yun,"I found it helpful to put some simple programs into this: [https://pythontutor.com/](https://pythontutor.com/) 

And then step through them. Despite python in the name, it also does a bunch of other languages.",hkw9jsd,t3_qv9yun,1637093496.0,False
qv9yun,"One thing that helps me is running things on a REPL, basically it shows the output of stuff as you write it in real time.

It’s been around as a tech for decades, but a lot of people don’t know it exists. It’s the standard way to do things in Lisp.",hky3hkz,t3_qv9yun,1637121716.0,False
qv9yun,Give yourself much more time than you think you'll need.,hkyc36k,t3_qv9yun,1637126359.0,False
qv9yun,"I like to draw out tricky stuff using a flowchart and kinda note out where things are happening and what variables are being handled. I personally think it helps to know what the machine is doing when you tell it something, so maybe think about that as you look at the code. Also, is it your code you’re having trouble with or are you looking at others’ code on stack overflow or an example online or text book?",hl9zimg,t3_qv9yun,1637342353.0,False
qv9yun,It’s mainly other people’s code - especially my professor’s code. A couple people have recommended drawing things out before but I sometimes don’t even know where to start 😅,hlddcm3,t1_hl9zimg,1637403455.0,True
qv9yun,Start with main or if it’s just a certain few lines look at the function/method they are in and just draw that part out. Draw.io is a good online tool but I keep a small dry erase board and markers nearby for exactly this and math. Anything to help visualize the ins and outs as they get processed. Good luck with your studies.,hles2kn,t1_hlddcm3,1637431304.0,False
qv9yun,"Just start doing things in the language... Say a sentence for what you want to happen then write the code the does it. Python is awesome for this and an excellent first language for this reason.

    python = list()
    doing = True
    things_to_do = 5
    
    print(""We want to do "" + str(things_to_do) + "" things!"")
    
    while doing == True:
        python.append(""things"")
        if python.count(""things"") == things_to_do:
            print(""Looks like we've done enough things for now."")
            print(""Here are the things in python: "" + str(python))
            exit()
        else:
            print(""Let's do more things."")

Doesn't matter if what you do has a purpose or not, just become fluent with the syntax first.",hli2n2s,t3_qv9yun,1637496873.0,False
qv9yun,Thanks so much!,hli7f8h,t1_hli2n2s,1637500022.0,True
qvhov8,"I don't use bit manipulations or many bitwise operations, but my favorite bit of hijinkery is fast inverse square root",hkwkc3k,t3_qvhov8,1637097724.0,False
qvhov8,The book hackers delight was just packed full of them. Lots of branch free operations. Don’t have a favourite but I do like the trick where you multiple by 1 or 0 to avoid an if statement.,hkwr6cb,t3_qvhov8,1637100459.0,False
qvhov8,"Finding all permutations of a set by looping from 0 to (1<<N)-1 then picking members based on bits.

Binary indexed tree and its variants. 

Finding the highest power of two in a number with (n & ~(n - 1)). 

XOR for finding a unique numbers in a sequence of duplicates. 

Tries with bit operations as edges.

These examples are mostly for fun. There are some bit-based data structures have been useful for me professionally though: hyperloglog, Bloom filters, roaring bitmap.",hkynu0s,t3_qvhov8,1637134230.0,False
qvhov8,What are some things that you find interesting?,hkwpnys,t3_qvhov8,1637099847.0,False
qvhov8,"I can’t say I’m using whole lot. Just simple arithmetic, such as int division/multiplication by 2. Albeit I read some where that it’s less performant in those simple cases compared to regular division. 

But I still use them because they are cool and remind me that I gotta learn and use them more. 😁",hkype2p,t1_hkwpnys,1637135440.0,True
qvhov8,"calling popcount 5 times for a branchless and fast way to OR all the bits

1 or 0 to  all true by sll by 31 and then sra by 31

also theres a ton of fractals that can be done as bit hacking",hkz2i7m,t3_qvhov8,1637146389.0,False
qvhov8,[Hacker's Delight](https://en.wikipedia.org/wiki/Hacker's_Delight) is a book full of various bit flipping tricks as well as other useful low level tricks.,hlca3iu,t3_qvhov8,1637376327.0,False
qvrimr,"A ""directed acyclic graph"" means:

1. A ""graph"", sometimes called a ""network"". You have nodes with edges to other nodes. This could represent a ton of things, from a search tree to Twitter following relationships.

2. ""Directed"", means the edges between nodes have a direction, like ""A -> B"", and not ""A <-> B"".

3. ""Acyclic"" means there aren't any cycles. If you have ""A -> B -> C"", then C can't have edges to A or B, and B can't have edges to A.

You can represent DAGs many ways in memory. If this were a homework assignment in C, we might make a struct for a node, which contains an array of pointers to other nodes. Like a tree, with a variable number of branches at each step. Alternatively, you could represent a DAG as an adjacency list, an adjacency matrix, or just about any other way you'd store a network.",hkyg6nu,t3_qvrimr,1637128850.0,False
qvrimr,"Thanks, man!  
Appreciate  it.",hl0jh8q,t1_hkyg6nu,1637171951.0,True
qvrimr,"Directed Acyclic graph is a directed graph which doesn't have cycles. This is useful if you want to solve something like constrained scheduling, eg. chalk out a sequence of activities where some activities depend on others to complete. So if you hypothetically have a case where job A requires job B to complete and job B requires job A to complete, you have a cyclic Directed graph, and you cannot have a topological order (so you cannot logically draw out a sequence for doing those jobs). On the other hand, if job B required job C and C required A, you have a DAG and you can solve the job ordering problem (A->C->B).

In general, a directed graph can have cycles. A DAG is a special kind of directed graph.

In order to understand memory representation of a DAG, you have to understand memory representation of an ordinary, ""undirected"" graph. An undirected graph is a set of vertices and the edges joining them. Say you have V vertices and E edges between them, so you represent the graph as follows:

* You represent the vertices as an array of length V, having indices from 0 to V-1.
* Each array entry 'arr\[v\] 'points to a (linked) list of vertices 'w' which are connected to the vertex 'v', where v is any integer between 0 and V - 1. The list of vertices is called the Adjacency list, or 'adj'.
* So for example, if vertices v and w are connected, v will have w in its adjacency list and w will have v in its adjacency list.
* When you add an edge to the graph, say add\_edge(a, b), b will be appended to a's adjacency list and a will be appended to b's adjacency list (since in an Undirected Graph, edge a-b is same as b-a --- and *this is where a directed graph differs from an undirected graph*)

Now, in a directed graph, edge a-b means edge a->b, and edge a-b and b-a are not the same. So when you add an edge a-b, you only add 'b' to a's adjacency list, and not 'a' to b's.

Now it is possible to have 3 edges a-b, b-c and c-a in a directed graph, which will cause a directed cycle. That's all.",hkz121g,t3_qvrimr,1637145202.0,False
qvrimr,Wow! Thanks for the detailed response. It made my day.,hl0jkit,t1_hkz121g,1637171986.0,True
qvrimr,"A DAG could be represented in memory a bunch of different ways. It really depends on how you were going to use it.  As far as I know there is no reference implementation for a DAG in memory , but I could be wrong.",hkyfomz,t3_qvrimr,1637128529.0,False
qvrimr,"Thanks, appreciate your answer. 

Can you mention a few ways in which it could be implemented in memory.",hkyh46s,t1_hkyfomz,1637129449.0,True
qvrimr,Would it be the same as a tree?,hkzamtp,t3_qvrimr,1637152148.0,False
qvrimr,"Tree implies a single root, so while all trees are DAGs, a DAG is a more general class of graph",hl0gay2,t1_hkzamtp,1637170721.0,False
qvgo6i,"Take a look at software defined radios. One possible approach is to find a (relatively) inexpensive SDR with TX / RX capabilities. You can then use open-source software to ""control"" the SDR. A very popular interface for this application is GNURadio. 

Since this is software-defined, then you essentially control what happens at the GUI level, using the SDR as an RF front-end. All of the mod/demod will happen at the Pi itself, as opposed to being processed at the SDR. Speaking from experience, you may find that the Pi is rather underpowered to handle certain SDRs or signal types.",hkwma10,t3_qvgo6i,1637098495.0,False
qvgo6i,"I don’t know specifically what your set up would be doing but I work with CAN a lot and have experience with TCP/IP and those use a “packet” which is a fancy term for a specific cluster of bits being sent from device to device. The packet is predefined in a standard somewhere and the devices know how to read it or can be told how to read it. There’s some identifying information regarding the source and the destinations and the contents, the data itself, and some CRC data as well. All of that gets framed inside the predefined packet so the devices can recognize where each packet starts and stops. The bits themselves are sent by literally sending specified voltage pulses. Which is beyond my scope I deal more with filling and reading the data after the machine handles the packet.",hla2cou,t3_qvgo6i,1637343462.0,False
qv9g3m,"It will be very difficult to find research papers that are 500 to 600 words. Short conference papers are usually 4-6 pages, and it goes up from there. You might find some published abstracts but they are not so common in CS (they appear more frequently in psychology, medicine, etc.). The only thing I can recommend is to add ""abstract"" to your search terms but I don't think that will really work.

&#x200B;

Also, you may want to check with your teacher because perhaps they meant articles and not research papers. In which case, there should be plenty online.",hkuxqz2,t3_qv9g3m,1637074499.0,False
qv9g3m,"Yes, I found some abstract in my research but they are too short and are less than one page... I think I can look over neural network cause it's fashion this time and can just say to my teacher ""Ho, neural network is a kind of data structure"" btw. Idk if it's true or false but I can maybe have a bit more luck in that way ...",hkxhfdx,t1_hkuxqz2,1637111763.0,True
quwl9r,"Because the application is downloaded using multi-threaded download at the beginning, the speed will suddenly be very high. At the end of the download of resources, multi-threading is not worth it, and the number of threads will be reduced. At the end, it will be a single-threaded download. very slow。",hkuxal8,t3_quwl9r,1637074300.0,False
quwl9r,Loading/Downloading percentages are sometimes completely arbitrary and do not reflect the actual state of the loading. It just makes the user feel better to have something on the screen to look at. There even studies around this.,hla2qgv,t3_quwl9r,1637343613.0,False
qtrrml,You are mistaking experience with genius.,hklhewu,t3_qtrrml,1636903542.0,False
qtrrml,"Thats right, most coding done in courses at universities is basic stuff. Writing 20 lines of correct code is not that complicated when you are doing it for several years.",hklkuks,t1_hklhewu,1636905068.0,False
qtrrml,"Maybe but I'm pretty ""experienced"" at this point and taken point on a few very large data intensive systems and I think if you pointed me at a random problems without a few days leetcode warmups I'd probably struggle if it wasn't in my domain. If it isn't genius its at minimum someone very skilled.",hkln41o,t1_hklkuks,1636906053.0,False
qtrrml,"Yea, but a teacher would be experience at the type of problems students ask. That is his domain. The problems uni students come up are not really that “random”
I am happy that op has a good teacher.",hkn8oft,t1_hkln41o,1636928494.0,False
qtrrml,And a teacher would be more experienced than a student who has taken some courses rather than taught.,hkok1cb,t1_hkn8oft,1636950248.0,False
qtrrml,"It is skill, for sure.",hklnwry,t1_hkln41o,1636906396.0,False
qtrrml,Well it probably seems impressive to me since it's so different from the other profs. Also I checked and it's more like 35 lines for each example which probably still isn't that impressive for some. But to me it is something new to see someone just program multiple examples with about 35 lines each lesson without using some editor that shows you when you fuck up. Just haven't seen someone do that before,hknti47,t1_hklkuks,1636937628.0,True
qtrrml,"So he uses nano or vi? And then he complies using the CLI G++? You guys learning C++?

I had a pretty good professor who was comfortable enough to type out code demos in lecture. It kept the class fun and engaging. I think he used an IDE, though.

I wouldn't mind using a terminal text editor, but I think I gotta have color syntax highlighting, minimum feature....",hkohdtu,t1_hknti47,1636948858.0,False
qtrrml,"Until I have achieved flow, I want rainbow ide's for training wheels. And even then, i might still want rainbows",hkp0tv0,t1_hkohdtu,1636960992.0,False
qtrrml,Pretty sure he uses gcc for compiling. We are learning a few advanced Java methods and the basics of Haskell and Prolog with him this year.,hksktqb,t1_hkohdtu,1637024295.0,True
qtrrml,"Also him coding on the spot is a good example. The students should be paying attention to how he’s coding on the spot and his reasoning, ask if he’s not explaining out loud",hkmmvgj,t1_hklhewu,1636919993.0,False
qtrrml,There exist experienced geniuses!,hklmb84,t1_hklhewu,1636905710.0,False
qtrrml,Agreed. He's just been programming a long time.,hkmazh2,t1_hklhewu,1636915598.0,False
qtrrml,"Exactly. I do this all the time in front of the class, and I can tell you: I am NO GENIUS! 
OK I don't exactly use just bare bones text editors like vim or Notepad (I prefer VS Code), but on occasions I would use vim and compile on the command line (for example to show different compilation levels, compiling with debug info etc) and quickly show the effects to the students. 
We also do a lot of variations of our programming excercises (C/C++) on the spot, and if the students have problems, I will just code them on the fly in front of the class. 
You can do this too. You just need more time and determination.",hkohgja,t1_hklhewu,1636948896.0,False
qtrrml,Well this is the first prof that does it that way and he teaches us 4 different languages that are all pretty different from each other. Our profs from the last semesters always prepared their code and often still had to later edit it because they made mistakes. Also my first time seeing a prof just using text editor and the terminal for everything. I think being able to take in all that experience also needs intelligence.,hkns8p1,t1_hklhewu,1636937031.0,True
qtrrml,Ok but what editor does he use?,hklqzj6,t3_qtrrml,1636907704.0,False
qtrrml,Would be funny it was gedit,hklwrdg,t1_hklqzj6,1636910095.0,False
qtrrml,Whats wrong with gedit 😡,hkmj762,t1_hklwrdg,1636918611.0,False
qtrrml,"Nothing, I use it all the time",hkmzxpr,t1_hkmj762,1636925035.0,False
qtrrml,Nano is superior.  s/,hkpmdz2,t1_hkmj762,1636978524.0,False
qtrrml,Everything. It isn't Hedit.,hkoppqh,t1_hkmj762,1636953513.0,False
qtrrml,inb4 it is regular windows notepad to screw with everybody.,hkm3n5i,t1_hklwrdg,1636912890.0,False
qtrrml,Google docs,hkm3yip,t1_hklqzj6,1636913016.0,False
qtrrml,Excel,hkm5wea,t1_hklqzj6,1636913714.0,False
qtrrml,The true Chad editor. Vim,hkn1a2x,t1_hklqzj6,1636925542.0,False
qtrrml,Obviously,hkoi731,t1_hkn1a2x,1636949276.0,False
qtrrml,atom gang rise up,hkmc4k7,t1_hklqzj6,1636916015.0,False
qtrrml,"The correct editor: neovim.

(Ducks...)",hkmb5j2,t1_hklqzj6,1636915660.0,False
qtrrml,"Nothing wrong with any vim. Neovim looks beautiful when prepped up. 

I'm using ((((DOOM!)))) >:D",hkov9nx,t1_hkmb5j2,1636957032.0,False
qtrrml,Doom nvim is very nice. I was using doom emacs but Doom nvim meets my needs better.,hkpyj01,t1_hkov9nx,1636985358.0,False
qtrrml,that's a thing?? :O gonna check it out,hkq0fd5,t1_hkpyj01,1636986279.0,False
qtrrml,Just the text editor,hknrcr9,t1_hklqzj6,1636936617.0,True
qtrrml,There are a couple of them. Getting to know what the ubiquitous ones are and how to use them could help you in your career later.,hkoo1ap,t1_hknrcr9,1636952507.0,False
qtrrml,"If it's on Linux, gedit is labelled just ""Text Editor"" at least on the Gnome desktop (kind of like Gnome calls the Nautilus file manager ""Files"", etc.), so it might be that.",hkph1by,t1_hkoo1ap,1636974554.0,False
qtrrml,Yeah exactly. I probably should've explained what I mean better 👍,hkslyd8,t1_hkph1by,1637024798.0,True
qtrrml,"What you mentioned about ""knowing the answer upon being asked"" is actually a skill you can pick up on.   

It's something I've done for basic problems myself and realized they if I keep practicing the art of ""problem solving"" I'll get to do it for harder questions.   

Say `hello world!` as you embark on this world of programming, and good luck to you, buddy!",hklm3ip,t3_qtrrml,1636905620.0,False
qtrrml,This is funny.,hklmx1s,t3_qtrrml,1636905970.0,False
qtrrml,"What I found funny was the use of 'Chad' as an adjective. I have never seen it used in a postitve way like in this post. I have only seen it used by incels as a way to describe the guys they simultaneously envy/loathe.

I'm not trying to be rude; I'm completely serious. I have never seen someone call someone a 'Chad' in such a positive way.

For context: [Chad, on the incel wiki](https://incels.wiki/w/Chad).

Do normal people really use 'Chad' as an adjective these days? It just seemed so odd in this context.",hkn6d46,t1_hklmx1s,1636927538.0,False
qtrrml,"Teenager here: yeah, we use ""chad"" as equivalent to ""worthy of respect and admiration""",hko0rpn,t1_hkn6d46,1636941015.0,False
qtrrml,"normal might be a stretch, but its very common now to use 'Chad' as an adjetive, the internet is weird.",hkp455d,t1_hkn6d46,1636963579.0,False
qtrrml,"Definitely, and , anecdotally at least, more often than not I see it used in a positive way. Though, whether or not the people I am around (mostly other students) can be classified as “normal” is heavily debated.",hknael8,t1_hkn6d46,1636929221.0,False
qtrrml,I just thought it was funny that simply being able to code on the spot was such a huge deal to OP.,hknggmx,t1_hkn6d46,1636931816.0,False
qtrrml,"Well that is probably because the profs I had in the last years always prepared the code and students were still able to find mistakes or criticize it. They also often replied to questions with ""I'll research that once I get home and tell you next lecture"" or would think for a quite long time and experiment a little before getting it right. I mean we are 400 students and a few already are experts at the field and simply need a degree so some of the questions are very specific and rather complicated. I am just amazed how he seems to immediately know the answer and starts typing.",hknr19e,t1_hknggmx,1636936469.0,True
qtrrml,"Congratulations, you are finally getting what you're paying for, a competent professor.",hknz2jz,t1_hknr19e,1636940229.0,False
qtrrml,Yeah dude this passed into general slang with some semantic shifts a long time ago lmao,hknlckj,t1_hkn6d46,1636933930.0,False
qtrrml,It has become a popular meme,hknoyz1,t1_hkn6d46,1636935547.0,False
qtrrml,"say chad to a youth and theyll think of that buffed up image of that mascular guy, it was a meme thing. chad = powerful and worthy.",hkpicuv,t1_hkn6d46,1636975602.0,False
qtrrml,I had a prof who did this in university. It sure helped me learn more when I saw him actually typing everything and explaining as he went. I wanted him for every class lol,hklw75l,t3_qtrrml,1636909869.0,False
qtrrml,"If you’re from ucla you know what prof codes on Microsoft word, das.",hkloomj,t3_qtrrml,1636906727.0,False
qtrrml,"Using a shell in a terminal instead of clicking around in some GUI is a lot more efficient once you're used to it. The Unix model is incredibly ingenious, with its small tools that do one thing, and that can be stringed together using an efficient language to perform pretty much any task. I don't see the point in using an IDE since I took the time to learn the POSIX shell, and haven't used one, privately or professionally, in seven years.

Highly recommend. It makes using a computer and programming more or less synonymous, again.",hkmogbu,t3_qtrrml,1636920579.0,False
qtrrml,"While I do use terminal very heavily, there are certain aspects to what an IDE can provide that the terminal simply cannot - primarily to do with visual indicators. None of them are deal breakers, but particularly when you're dealing with very complex systems (think code that has been allowed to grow too 'Enterprisey' and large web application systems) or developing graphics-heavy applications, they can be a significant benefit.
 
I effectively ban them in my teaching until students are in at least 3rd year though, otherwise they turn into a crutch.",hkn2wl9,t1_hkmogbu,1636926161.0,False
qtrrml,"Personally I don't find them that beneficial for those cases either, but it's a personal preference: I find it easier to spot systemic flaws in an architecture if I have to study it manually than if I can just ""jump around"" in an IDE and not fully suffer the consequences of how the project is structured.

I can see there's a point to them for very verbose languages like Java, just to save some typing, but I'm not sure they provide a gigabyte of value for that.",hkopo33,t1_hkn2wl9,1636953485.0,False
qtrrml,"I would love to be in at least one of your classes with this professor, he sounds amazing.",hkltju6,t3_qtrrml,1636908776.0,False
qtrrml,">	He programs everything using the text editor

As opposed to what?

Anyway, good story, made me smile.",hkn7szn,t3_qtrrml,1636928129.0,False
qtrrml,Real men draw the code in MS paint and rename the file to an .exe.,hkpu6m8,t1_hkn7szn,1636983133.0,False
qtrrml,"""IDE""",hkp71r1,t1_hkn7szn,1636965968.0,False
qtrrml,"I agree with the comment ""You mistake experience for genius"" because it brings up an important point - You can easily get to the point you see your professor at if you dedicate yourself to learning the tools, methods, and theory behind the practice. Hopefully that motivates you throughout your studies, I wish you luck.",hkn6ukm,t3_qtrrml,1636927736.0,False
qtrrml,"Those People are great! 
I also find those people pretty impressive that can control the whole PC just with their keyboard. And not even that, they even do it like twice as fast as i ever could with my mouse.
On top of that, they know hotkeys, no person ever in the history of mankind had heard of.",hknicb5,t3_qtrrml,1636932631.0,False
qtrrml,What university do you go to?,hko7038,t3_qtrrml,1636943854.0,False
qtrrml,honestly op would be way better off with his name,hkpc6cm,t1_hko7038,1636970440.0,False
qtrrml,I thought calling someone a Chad was an insult? This seems like a complement,hkp0qbw,t3_qtrrml,1636960917.0,False
qtrrml,"I do the terminal thing too, it's because standard out and standard error are being sent to the terminal so if it's not logging to a file you can see what's going wrong or why it errors out. Not every application does this, also I almost forgot the #1 reason is because you can ctrl + c out of buggy programs like firefox with 400 tabs open. The kind of ""lag"" so bad that you can't even open the task manager to kill the program",hkpeo8f,t3_qtrrml,1636972585.0,False
qtrrml,"Learned how to compile and run Java programs through the command line today. Quite cool, cus I felt like Mr. Robot for a sec.",hkmj5bv,t3_qtrrml,1636918593.0,False
qtrrml,"There is a recorded lecture series called “The missing semester of your CS education”

https://missing.csail.mit.edu/

What you describe about live coding, using the terminal, using a text editor, etc. is part of that lecture, and the best professional engineers drive their computers from the command line like this all the time.  It’s why we prefer posix-compliant shells over the dos command line.",hkneqo7,t3_qtrrml,1636931061.0,False
qtrrml,"Is this understandable for 1st Year CS student like me, I only have a tiny amount of background about programming, so this would really help if I could understand this lecture.",hko5o75,t1_hkneqo7,1636943241.0,False
qtrrml,This because he spent years coding alone in his office and preparing for the lectures.,hkmfx28,t3_qtrrml,1636917393.0,False
qtrrml,Colored words and red squiggles FTW!!!,hkmuckj,t3_qtrrml,1636922836.0,False
qtrrml,"This isn't abnormal. When teaching intro programming courses, I'd do virtually the same thing. I'd be using a IDE because that is what we teach the students to use, but for programs that size, the eclipse (at the time) was too slow to do anything past color/highlighting before I finish jotting a segment in. 

This is just a matter of experience. Those examples really are trivial to him, and hopefully will be to you eventually.",hknip6o,t3_qtrrml,1636932787.0,False
qtrrml,My professor at JuCo was like this. Seriously a legend. I thoroughly enjoyed watching him work. I wish all professors lectured this way. It’s soooooo beneficial to see the process rather than slides and 10 minute recorded videos,hknvhl0,t3_qtrrml,1636938568.0,False
qtrrml,I need this type of profs in my Master degree,hko0umq,t3_qtrrml,1636941051.0,False
qtrrml,"I'm  glad that pumped you. Stuff like that really has gotten me into working in terminals, learning vim / Doom Emacs, stop being scared of low level langs, etc. 

This talk absolutely blew my mind when he started coding live: https://www.youtube.com/watch?v=OyfBQmvr2Hc Due is writing at a million miles per hour. 

There's also the Sussman / Ableson MIT course where they code live in the terminal in Lisp AND ON THE BLACKBOARD! This was in the 80s mind you. Pretty neat stuff.",hkovlwy,t3_qtrrml,1636957258.0,False
qtrrml,Imagine not living in a tty in linux and only ever using CLI.,hkovyhl,t3_qtrrml,1636957492.0,False
qtrrml,My programming prof just shows us videos from youtube of an inaudible indian guy talking about c programming in turbo c. Most of her presentations of codes has syntax issues which makes the whole class boring.,hkowc5c,t3_qtrrml,1636957756.0,False
qtrrml,Girls with autism vs boys with autism,hkpbyfi,t3_qtrrml,1636970249.0,False
qtrrml,"Ah, yes. I did that as well, while I was teaching. As I was in the told, that skill is very rare, although I never understood why, and what's so difficult about writing a program that has less than 200 lines or code on the spot and that works as it should from the first time",hkphzez,t3_qtrrml,1636975305.0,False
qtrrml,"There are many talents, but memorization is just one of them. There is also:
- Creativity
- Discipline
- etc.",hkpk4zh,t3_qtrrml,1636976951.0,False
qtrrml,This is the kind of professor I want to be in the future. I don't want to be a boring old man with powerpoint.,hkpmb64,t3_qtrrml,1636978469.0,False
qtrrml,probably because he teaches the same class every year??????,hkpo6kr,t3_qtrrml,1636979684.0,False
qtrrml,He doesn't. The program has changed a lot during the years. But he definitely is very experienced. Haven't had a prof yet that is this good at his job.,hkslh1u,t1_hkpo6kr,1637024585.0,True
qtrrml,"I don't know why you guys are dragging him down. I mean yes, writing 20 lines of code for an introductory class is not hard, same goes for opening everything over cli; I can do it and I am in my sophomore year, but this guy managed to inspire one of his students to take interest in what he teaches and get the student to do the assignments himself, without forcing them. That's what the teachers job is. I am in CS because of a teacher like this and I can't thank him enough, ever. You can learn syntax and basic concepts over internet; probably better than they will teach you at the university, but getting inspired by your teacher is a priceless thing.",hl5pz36,t3_qtrrml,1637263907.0,False
qtrrml,"I think it is my fault for not explaining it properly. Reading it again really sounds lame and there are a few things I should have mentioned. It is not an introductory class but an advanced class. Writing 30 lines of code might still not seem that much but the damn speed with which he is able to do it even when students ask for veeeery specific examples. He can also easily switch between the languages with no problems. I would also be able to open programs over the terminal but I've never seen someone not using a mouse at all and controlling every aspect over it.  


It might still not sound that impressive to experienced people but you should really see that guy. I also did some more research and he seems to be the chairman of the programming language faculty as well and he regularly holds international speeches. He definitely is different from the other profs we had until now. So many others already had to google shit during class multiple times.",hlgp8c5,t1_hl5pz36,1637463009.0,True
qtrrml,"I usually write dozens lines of code in different files/directories and it often works from the first running

I mean I even see where I will get problems in my code, and where I won't

It's not a magic, it's just a skill and knowledge....",hkmh96q,t3_qtrrml,1636917879.0,False
qtrrml,[deleted],hklj5sl,t3_qtrrml,1636904320.0,False
qtrrml,"i agree with your sentiment, but the way you said it was so rude and condescending...",hklk42n,t1_hklj5sl,1636904742.0,False
qtrrml,What was it?,hklrusx,t1_hklk42n,1636908070.0,False
qtrrml,"just a bunch of unnecessary stuff like ""20 lines of code?? wow!!""",hklz6am,t1_hklrusx,1636911071.0,False
qtrrml,"He learned to use the terminal when a Mouse did not exist.  It is what he's learned and only what he uses.  It is his happy space.  He's probably a lunatic and uses VI, arggg. EMACS is better, but why?  Nano does just fine.

Set up your phone, recording his entire lecture. We never had that (when we walked 8 miles uphill in a blizzard across campus, both ways).

Oh, yea he probably still has stacks of cards in his office.  Visit, be impressed.  Build some xxix servers of your choice either as VM's or on Raspberry Pi's.  Either way, you learn to use the terminal.  Do things his way.  You'll learn.  Enjoy.

You want a real CS job.  Here's one that will make you UNREPLACEABLE!  [https://www.popularmechanics.com/space/a17991/voyager-1-voyager-2-retiring-engineer/](https://www.popularmechanics.com/space/a17991/voyager-1-voyager-2-retiring-engineer/)

https://voyager.jpl.nasa.gov/mission/did-you-know/",hkp4erf,t3_qtrrml,1636963792.0,False
qtrrml,8 miles is 12.87 km,hkp4fd6,t1_hkp4erf,1636963805.0,False
qtrrml,"Ahoy joelhuebner! Nay bad but me wasn't convinced. Give this a sail:

He learned t' use thar terminal when a Mouse did nay exist.  It be what he's learned n' only what he uses.  It be his grog-filled space.  He's probably a lunatic n' uses VI, arggg. EMACS be better, but why?  Nano does just fine.   

Set up yer phone, recording his entire lecture. Our jolly crew nary had that (when our jolly crew walked 8 miles uphill in a blizzard across campus, both ways).  

Oh, yea he probably still has stacks o' cards in his office.  Visit, be impressed.  Build some xxix servers o' yer choice either as VM's or on Raspberry Pi's.  Either way, ye learn t' use thar terminal.  D' things his way.  You'll learn.  Enjoy.",hkp4f85,t1_hkp4erf,1636963802.0,False
qtrrml,8 miles is 12.87 km,hkp4fvc,t1_hkp4f85,1636963817.0,False
qtrrml,"Oh, holey hell!",hkp4weo,t1_hkp4f85,1636964187.0,False
qtrrml,8 miles is  41133.29 RTX 3090 graphics cards lined up.,hkp4fra,t1_hkp4f85,1636963814.0,False
qtrrml,8 miles is 12.87 km,hkp4gij,t1_hkp4fra,1636963831.0,False
qtrrml,8 miles is 6849.71 Obamas. You're welcome.,hkp4ff5,t1_hkp4erf,1636963807.0,False
qtrrml,8 miles is 12.87 km,hkp4g3u,t1_hkp4ff5,1636963822.0,False
qttk9c,"This is the trend for every single development in the industry.

Probably the most recent large one that maybe goes unnoticed is the migration to the cloud. 10 years ago, I would never imagine not having a server room. Now, the idea of physical machines hosting your application is probably something very foreign to people, and of course it was resisted immensely at first.",hkoeqbq,t3_qttk9c,1636947536.0,False
qttk9c,See also [The Story of Mel](http://www.jargon.net/jargonfile/t/TheStoryofMel.html).,hkmtd5a,t3_qttk9c,1636922451.0,False
qttk9c,"Throughout time, the acceptance and adoption of new technology has always followed this pattern: 

Denial, Anger, Bargaining, Depression, Acceptance. 

Oddly enough, this is also happens to be the five stages of grief. Go figure.",hko9aac,t3_qttk9c,1636944926.0,False
qttk9c,What stage would you say Bitcoin/Crypto is in currently?,hkp769t,t1_hko9aac,1636966076.0,False
qttk9c,"IMHO, bargaining. Some adopters, but not mainstream yet. When it reaches about 45% adoption, we'll start to hear all sorts of sad stories about it. That's when you'll know it's in the depression phase.",hkpnn9x,t1_hkp769t,1636979351.0,False
qttk9c,Not relevant but it reminds me of a historian who claimed ancient writing was purposely more complicated than it needed to be because the scribes would benefit.,hkmbk4g,t3_qttk9c,1636915811.0,False
qttk9c,You don't need to go that far. Try studying Law. Completely unnecessary verbose just to gatekeep others from understanding it so they remain necessary.,hkmq6u9,t1_hkmbk4g,1636921229.0,False
qttk9c,Yeah but my example highlights that we've been avoiding getting replaced for 6000 years.,hkn0ink,t1_hkmq6u9,1636925248.0,False
qttk9c,Wasn't it Socrates that claimed that writing was bad because people wouldn't learn to memorize? (Which we learned through Plato's writings),hkn7kdw,t1_hkn0ink,1636928030.0,False
qttk9c,its ironic that we (mankind) only 'remember' this now because it was written down.,hkpuw9f,t1_hkn7kdw,1636983508.0,False
qttk9c,"That’s funny. On the other hand the first Lisp compiler and interpreter was implemented by a grad student who was tired of having to write Assembly code.

John McCarthy told his grad student: ""You can't make a computer language out of a notation language."" 

Grad Student: Lmao.",hkofh83,t3_qttk9c,1636947899.0,False
qttk9c,"it's the same stuff programmers (usually the narrow minded or ignorant ones) have been saying for most new language or framework...

This is similar to the mindset that older programmers who come from C/C++/Java backgrounds call younger programmers coming from a web dev, JS, python background ""script kiddies""",hkpa4ra,t3_qttk9c,1636968653.0,False
qttk9c,"scripts are actually awful, its so unnorganized and all over the place. i was literally horrified at some of the spaghetti python code a peer had done (they were only 4 years younger than me, their code was very affective, but i dont think they thought about someone else being able to read and understand it).

JS as a script has its place because web.",hkpvg37,t1_hkpa4ra,1636983795.0,False
qttk9c,Thank you for posting this! Very very interesting,hkqcf8n,t3_qttk9c,1636991599.0,False
qu8lzi,[Good video here.](https://youtu.be/0oDAlMwTrLo),hkpjfgv,t3_qu8lzi,1636976426.0,False
qu8lzi,CS50 week 3 :),hkq59h8,t3_qu8lzi,1636988513.0,False
qu4c71,"I think the fundamental misunderstanding here is between the difference in read / write operations. A mechanical arm responding to an electrical impulse within the brain specific to the area that lights up when someone tries to move their arm is much easier to read and mimic than it would be write some information to the brain. 

We are beginning to understand how to read impulses from the brain for simple operations. But we have no idea how to write back to it.",hko9khj,t3_qu4c71,1636945061.0,False
qu4c71,">But we have no idea how to write back to it.

We sorta do, conceptually. Education is all about this. As is martial arts, and sports. In fact, anyone who has ever learned anything and gone through the process of creating a repeatable ""function"" (muscle memory* is the phrase I've most often heard) out of something that used to require much more attention during the learning phase is then aware that some kind of self-write operation is a part of daily life and learning new things, beyond just being a passive sensor. If we can map what happens in the brain during other things, then it stands to reason that we could map what happens in the brain when people are intensively learning and training themselves. The idea of insta-grok technology does not seem far fetched to me, but I'm not an expert. I would add the important caveat that the brain is complex and consciousness even more so. It would probably take far longer to perfect and understand how to use such technology than it would to get it going in the first place.

I am not sure I would replace the learning process itself with a Matrix style ""insta-download-to-your-brain"" system, but it would be cool to have more control over the process which we already use when teaching ourselves things. Oftentimes training is like playing the part of both the animal and the trainer at the same time, and even if you're a smart trainer you might still also be a stubborn animal. Some people's muscle memory (and neuroplasticity, for that matter) is less stubborn than others, and easier to tame for this purpose or that. I personally enjoy the process of learning and practice, but I would enjoy having more control over it. Perhaps current Sci Fi depictions only scratch the surface of what cybernetics and biotech could allow, in theory. 

*Muscle Memory is not simply memorization. It is creating repeatable functions which you then use for higher level tasks, like tools in a tool belt.",hkordik,t1_hko9khj,1636954530.0,False
qu4c71,"One I enjoyed your response.

But if I can, the question isn’t that we don’t understand the general process of repetition leading to acquired skill or knowledge. The question is what the actual underlying physiological process occurring during repetition is. And whether or not we could repeat that. 

I see that as a far stretch from mapping some part of the brain which lights up when a person attempts to lift their arm.

Certainly not impossible, but well beyond our current knowledge of how the brain works. Unless you listen to Elon musk of course. Then we’ll have all of this done in no time",hkoryks,t1_hkordik,1636954895.0,False
qu4c71,"I don't think repetition alone teaches a person much. To use the sports and martial arts analogy again: you want to build mental, logical, conditional reflexes for different situations in addition to building the muscle memory of the actions themselves. In general this means applying what you've learned in a variety of situations and getting ""practice"", which is distinct from repetition, although repetition is a byproduct of practice. A surprising amount of, say, Math is about building this kind of thing so that when you see a certain kind of problem the right solution hits you like a reflex. Intuition comes from practice.

I'm not a doctor or an expert programmer (or expert anything), so I'm not sure what the current ""resolution"" limits are for making sense of brain scans. I'm skeptical because it is hard to measure the conscious meaning of even a face expression from a friend, nevermind a gazillion neurons being examined in n-dimensions of data analysis. That said, it seems like a surmountable task with enough effort, given that we have already mapped out so much.

Edited for typo.",hkostqd,t1_hkoryks,1636955442.0,False
qu4c71,Did you just coin the word insta-grok? I love it.,hkqhzjg,t1_hkordik,1636993878.0,False
qu4c71,"I didn't coin the word *grok* (which is from a Heinlein book I've never read and means something like ""to grasp a concept""), but I will happily take credit for *insta-grok* if nobody else has said it before. I'd be shocked if I were the first though, as the word has been around for about as long as microwaving food. 

Glad you enjoyed!",hkr2uwu,t1_hkqhzjg,1637002137.0,False
qu4c71,"It's from Stranger in a Strange Land which was a counter culture hit in the 60s & so the word grok entered the lexicon. But insta-grok was new to me.

Edit: I just searched & it's already in use as a website name.",hkr8f9f,t1_hkr2uwu,1637004350.0,False
qu4c71,Glad to hear I'm not the first! It would be a good brand name for all kinds of things. Thanks for reminding me which book it was from. I read Starship Troopers as a kid but that's the extent of my Heinlein.,hkr99zy,t1_hkr8f9f,1637004690.0,False
quec2x,"I'd say the DQN paper is good to know for games, https://arxiv.org/abs/1312.5602",hkrxhy7,t3_quec2x,1637014199.0,False
quk6my,"Computer science: the scientific discipline of information processing, includes a lot of theory about for instance computability theory, complexity classes, abstract machine models like finite state machines and turing machines, ... also lots of math

Programming: the process of writing code, i.e. text that is in such a language that a computer is able to understand it

Software engineering/developing (don't know of any difference): the discipline of creating software, this involves programming, but also a LOT more, like organization, planning, designing architecture that fulfills quality standards and allows expanding and changing the software later, testing and QA",hkrfgi3,t3_quk6my,1637007124.0,False
quk6my,"From what I know is computer science is a degree which is useful to software engineering and can help you, but a computer science degree isn't required to get a software engineering job

Software engineering/software developer is a job that targets in building, analyzing, developing new things by Programming.

Programming is a process by making a program with code.

Sorry for bad English and I'm just new to these stuff. You guys can correct me, it would be an honor.",hkqurbq,t3_quk6my,1636998951.0,False
quk6my,"10 years ago Computer Science was/is a degree that focuses on theories and data structures. Computer engineering focused more on hardware/electronics. These may have changed since I got out of school.

Programmer/software engineer/software developer differences are going to vary from place to place and often are interchangeable. When different generally a engineer focuses more on designing solutions and developers focus more on producing.",hkqwucd,t3_quk6my,1636999771.0,False
qu426c,"LaTeX is like vim. If you really know how to use it, it's just so much more powerful and easy to use tool, you don't want to touch anything else.",hknuz9m,t3_qu426c,1636938330.0,False
qu426c,"I see, thanks.",hknw33r,t1_hknuz9m,1636938850.0,True
qu426c,Simple: Word and GDocs equations look like absolute dog shit in comparison.,hknzcxp,t3_qu426c,1636940363.0,False
qu426c,"Also, and perhaps more importantly: LaTeX lets you write once and use anywhere, which is important if you want to submit your work into a journal that used a different format than you did, and then also submit it to a second different journal that uses a different format than the first one. You don't actually have to do very much.",hknw6nl,t3_qu426c,1636938897.0,False
qu426c,"The primary reason is that in LaTeX you can put in mathematical equations and they print out like mathematical equations. Try doing that (properly) with Word. Second reason, it will print out the way you define it, Word can’t even guarantee your text will be on the same page even if it is in the same versions across platforms.",hko38xx,t3_qu426c,1636942144.0,False
qu426c,"When you have a large and complex technical document, latex manages figures, tables, citation and equations without  moving your hand from the keyboard, which is very efficient when you know the syntax.",hkootrh,t3_qu426c,1636952977.0,False
qu426c,"As other have mentioned LaTex still does MANY things well that Word/Gdocs does not. And that goes well beyond just equations. I have used it extensively for papers which rarely included equations, but here are the some of the most important factors I found:

* Automatic generation of bibliography with references in paper order. ALL papers in every field have citations/references and numbers range from tens to hundreds. Being able to collect that in a bib file and reference a cite with a simple function is worth its weight in GOLD.

* Separation of content/structure from formatting. When writing in LaTex you write the content with minimal structural markup (like sections/subsections). Almost ALL the formatting like fonts, page layouts, etc are separate and usually GIVEN to you by the journal/conference/publisher so you don’t spend much if any time fiddling with it. For those familiar with it LaTex splits content from style kind of like HTML and CSS.

* Simple ASCII file format that is human readable makes it possible to recover from mistakes/corruption as well as collaborate using revision control systems designed for code. When using a binary format or machine generated XML, this is impossible. If a tool screws something up you are up shit creek and may need to rebuild from scratch. When working with large documents like a thesis or book, this would be a disaster.",hkr0hgy,t3_qu426c,1637001208.0,False
qu426c,"Aside from the features you mentioned, it seems still nothing does justification as well as tex/latex, even after all these years.",hkp04m2,t3_qu426c,1636960470.0,False
qu426c,"I am not a LaTeX user but I usually use Markdown (also a Markup language) to quickly write some text with formatting.
The advantage of using markup language for formatting is that your hands do not need to leave the keyboard to do formatting. If you are familiar with most syntaxes, you can do formating much more efficiently than using GUI to do so. Markdown, for example, let's say I would like to make a quote block or code block. I can simply type '>' or '```' to create one quickly and start inputting what I want. In Microsoft Word, I'll need to perform additional steps to create similar things.
From what I know LaTeX is widely used to make mathematical formulas (correct me if I'm wrong). As I don't know how to use LaTeX, whenever I want to create a mathematical formula, I have to manually add symbols by choosing the list of symbols in the menu. It is definitely inefficient to do so.",hko2mke,t3_qu426c,1636941862.0,False
qu426c,Many Markdown environments are LaTeX extended. Wrap with double dollar signs and try it out. $$6x_2 \geq x_1^2$$,hko3pml,t1_hko2mke,1636942356.0,False
qu426c,"I think most people answered the actual reasons well, but I’d just like to add it can be fun. I don’t know about y’all, but it feels really satisfying to manually specify the proper formatting and then have it appear. Then again, it can also be incredibly frustrating too.",hkpa3ws,t3_qu426c,1636968632.0,False
qu426c,"Ew, no. I don't want to hand-draw everything in a GUI, are you kidding me?

Just because it's hard for you doesn't mean it's hard for everybody.",hknvrfe,t3_qu426c,1636938698.0,False
qu426c,"I never said it was hard, i simply asked why it was used....",hknw17w,t1_hknvrfe,1636938824.0,True
qu426c,Control,hkqipbj,t3_qu426c,1636994169.0,False
qu426c,legacy stuff. journals only accept latex. also the lack of competition. latex is dogshit but its all we can use,hkz30sz,t3_qu426c,1637146809.0,False
qt4qqy,This photo of spaghetti is making me hungry...,hkh6t44,t3_qt4qqy,1636824081.0,False
qt4qqy,before her suicide...,hkh2gmz,t3_qt4qqy,1636822241.0,False
qt4qqy,Can you share something more about this? I googled but there aren't much information about her life. She doesn't even have a wiki page :/,hkjsokj,t1_hkh2gmz,1636865521.0,False
qt4qqy,I think it was a joke? I could not imagine troubleshooting a panel like this. You would have better a chance sending it to an Italian bistro for repair with all that fucking spaghetti,hklhz4p,t1_hkjsokj,1636903796.0,False
qt4qqy,was a joke...,hkm86pk,t1_hklhz4p,1636914567.0,False
qt4qqy,The amount of “nope” associated with the general lack of labels is off the charts,hkh9f0f,t3_qt4qqy,1636825174.0,False
qt4qqy,Imagine trying to run those wires through panels!,hkkygvg,t1_hkh9f0f,1636893905.0,False
qt4qqy,The horror,hklswtr,t1_hkkygvg,1636908512.0,False
qt4qqy,That is like a scene right out of Star Trek. That's awesome.,hkh708z,t3_qt4qqy,1636824164.0,False
qt4qqy,Beautiful colors. I love how gentle film photography is,hkhcc6l,t3_qt4qqy,1636826413.0,False
qt4qqy,"The colors are fake. This photo was black and white, then colorized (poorly). Notice the gray on her arms, and behind her ear.",hkhhqr2,t1_hkhcc6l,1636828729.0,False
qt4qqy,I too would call it quits after tracing all those wires.,hkhi6e5,t1_hkhhqr2,1636828919.0,False
qt4qqy,Pretty sure ML does that,hkqnc19,t1_hkhi6e5,1636996010.0,False
qt4qqy,"Ugh that just gives me a headache. Much love to her, for doing something I would dread doing.",hkij7yj,t3_qt4qqy,1636844752.0,False
qt4qqy,So much better [now](https://www.reddit.com/r/pcmasterrace/comments/5r1cr4/cable_management_from_the_depths_of_hell/)...er...,hkjegjx,t3_qt4qqy,1636858656.0,False
qt4qqy,Cable management would like a word with you,hkl8qwm,t3_qt4qqy,1636899441.0,False
qt4qqy,Imagine getting segfault on this.,hkhsdh2,t3_qt4qqy,1636833313.0,False
qt4qqy,meanwhile the students in my digital logic class are struggling to make a 4 bit adder,hkhg9gn,t3_qt4qqy,1636828092.0,False
qt4qqy,Are you joking? Is she from Star Trek?,hkif9zq,t3_qt4qqy,1636843058.0,False
qt4qqy,my uncle did stuff like this,hkk37em,t3_qt4qqy,1636871399.0,False
qt4qqy,"I wonder, if connecting lines on the modern circuit board represents the wires on this photos",hkk66oz,t3_qt4qqy,1636873408.0,False
qt4qqy,This photo makes you really appreciate PCBs.,hkkh7ul,t3_qt4qqy,1636881391.0,False
qt4qqy,"I guess this is basically FPGA equivalent?

(also, all that work to colourize, and they left her arm like that? strange choices..)",hkl3ceg,t3_qt4qqy,1636896668.0,False
qt4qqy,r/forbiddensnacks,hkl49yr,t3_qt4qqy,1636897140.0,False
qt4qqy,What a cool kitchen,hkkb5we,t3_qt4qqy,1636876930.0,False
qt4qqy,[deleted],hkjxd0l,t3_qt4qqy,1636867961.0,False
qt4qqy,The wires are the source code,hkkok7m,t1_hkjxd0l,1636887028.0,False
qtl1ir,"big O is always an approximation.  It is not a measure of how long something takes but rather a measure of how fast the time it takes grows as n grows.

The reason for the approximation is to make the math easier.

Check out below for a more complete explanation of how 

https://www.baeldung.com/cs/fibonacci-computational-complexity",hkw87zo,t3_qtl1ir,1637092971.0,False
qtkimi,"I’ll give some unconventional advice here. Check out Arduino, it’s a dev board based on the atmega328p microcontroller. A microcontroller is essentially a tiny computer, this particular one has 32kb flash memory, 2kb ram, and runs at 20mhz. But they’re simple, especially compared to a modern PC and operating system, much easier to grasp everything that’s going on. Typically you run them without an operating system, your code just goes right on top of the hardware and you have full access to all of the registers, you can set and read the voltages on the pins, you handle interrupts yourself, etc. You learn a lot this way.

Arduino is a good place to start, it mainly just provides a nice interface for flashing code into the microcontroller, but once you get a little more familiar with it you can use the microcontroller on its own on a breadboard, then use a programmer like the atmel-ice and AVRstudio to flash code into it, AVRstudio also has an assembler you can use as well.

After you get familiar with that you can move upto a multicore family of microcontrollers like stm32 and get exposure to concurrency this way.

The reason I recommend this route is I think these topics are best learned in their simplest possible forms and the PC ecosystem is not really the place to find that. I also think computing is best understood bottom-up instead of top-down.

There’s Arduino kits such as [this](https://www.amazon.com/ELEGOO-Project-Tutorial-Controller-Projects/dp/B01D8KOZF4/ref=mp_s_a_1_1_sspa?crid=2RW64OFAM05ZF&keywords=arduino+kit&qid=1636939028&smid=A2WWHQ25ENKVJ1&sprefix=arduino+kit%2Caps%2C127&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEzMkY3OFBNOTNHQUpSJmVuY3J5cHRlZElkPUEwODcwNzExMVRWOEdTTzhPSlRZRiZlbmNyeXB0ZWRBZElkPUExMDAxMzc2M1RTT1dKR0NNR05TQiZ3aWRnZXROYW1lPXNwX3Bob25lX3NlYXJjaF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl) to start with.

You can get the [Arduino IDE](https://www.arduino.cc/en/software) here. It’s C++.

There’s no hello world because it doesn’t have a screen but the equivalent would just be a program to blink an LED.

    void setup()
    {
        pinMode(LED_BUILTIN, OUTPUT);
    }

    void loop()
    {
        digitalWrite(LED_BUILTIN, HIGH);
        delay(1000);
        digitalWrite(LED_BUILTIN, LOW);
        delay(1000);
    }

setup() runs at startup

loop() runs immediately after and is looped forever until power down, there’s no operating system to return to so your program can never end.

pinMode() just sets a pin as output or input, you’re either outputting a voltage on any particular pin or reading it.

LED_BUILTIN is just a constant for the pin connected to an LED on the arduino. Equal to 13 in this case.

digitalWrite() sets the state of a pin configured as an output.

HIGH and LOW are just constants for digital high and digital low, equal to 1 and 0 respectively.

delay() just loops the cpu for a given amount of milliseconds.

You can flash your code to it by hooking it up to a USB port, selecting that port in the Arduino IDE and then hitting upload.

r/electronics is good if you need help with the circuitry aspects.

r/embedded is good if you need help with programming it or the hardware concepts.

[atmega328p datasheet](https://www.sparkfun.com/datasheets/Components/SMD/ATMega328.pdf)

[AVR instruction set](http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf)",hknwkyu,t3_qtkimi,1636939083.0,False
qtkimi,"That is very cool, and really in a way, a small dream coming true - ever since my cyber days I wanted to utilize microcontrollers - and I didn't even think for a second to get one for these purposes.  


I will def consider getting now, as I get myself a book on the subject",hkohkop,t1_hknwkyu,1636948956.0,True
qtkimi,"""Operating Systems Concepts"" by Silberschatz is probably what you want to start with. 

And of course, simply googling for material on each topic will yield mountains of reading material for you.",hkljpo9,t3_qtkimi,1636904562.0,False
qtkimi,Reading that right now for OS classes. Its a hard class and a struggle sometimes but I feel a lot more confident about programming after reading this. The author explains concepts very well,hklqkf9,t1_hkljpo9,1636907525.0,False
qtkimi,"Looks great! I will start diving right into it. Its just that googling gets me all over the place..   
But I was looking for something more linear that could help me establish solid grounds. Thanks for the recommendations!",hkoh95k,t1_hklqkf9,1636948791.0,True
qtg70j,See [One's Compliment](https://en.wikipedia.org/wiki/Ones%27_complement).,hkjd4xx,t3_qtg70j,1636858056.0,False
qtg70j,"**[Ones' complement](https://en.wikipedia.org/wiki/Ones'_complement)** 
 
 >The ones' complement of a binary number is the value obtained by inverting all the bits in the binary representation of the number (swapping 0s and 1s). This mathematical operation is primarily of interest in computer science, where it has varying effects depending on how a specific computer represents numbers. A ones' complement system or ones' complement arithmetic is a system in which negative numbers are represented by the inverse of the binary representations of their corresponding positive numbers. In such a system, a number is negated (converted from positive to negative or vice versa) by computing its ones' complement.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hkjd6c2,t1_hkjd4xx,1636858074.0,False
qtg70j,"Not sure what the OP's ""and it's a multiple of 4"" is about, but most machines now use 2's complement.  Otherwise, either form uses the high-order bit as sign, so in hex that would be spelled as \[8-F\] in the high-order nibble.",hkjnf6u,t1_hkjd4xx,1636862988.0,False
qtg70j,"Yeah, I was just referring to that page to show how the MSB can represent the sign bit for any bit-sized integer value, which is essentially what the OP was asking IMHO.",hkkybgl,t1_hkjnf6u,1636893806.0,False
qtg70j,That's what I assumed.  I just thought he might  go down a 1's complement rabbit hole without a little more context.,hklxq9a,t1_hkkybgl,1636910485.0,False
qtg70j,"Someone mentioned it in the thread, but using ones complement (inverse of a hex string) will give you the negative number",hkk0ho4,t3_qtg70j,1636869748.0,False
qtg70j,‘8’ is a hex number that begins with 8; is a multiple of 4; is positive.,hkkd3k6,t3_qtg70j,1636878327.0,False
qtg70j,"Well, technically it’s negative in 4bit representations, that’s why we precede it with 0s",hkqnzh0,t1_hkkd3k6,1636996273.0,False
qtg70j,"In the pure sense; hexadecimal is a number system, so a negative hexadecimal number would be -0x…. 

However what your referring to is probably a byte type, in which 0x80 is 128 which is the start of the negative range.

Unless it’s 1 complement which I’m not familiar with.",hkktnc9,t3_qtg70j,1636890691.0,False
qtg70j,"Recall that the sign of a binary number is determined by its most significant bit (number furthest to the left). Every hexadecimal digit is represented as 4 binary bits. For example, Hex digits 0 through 7 in binary are 0000 through 0111. On the other hand, starting at 8 we have 1000 through F is 1111, where the most significant bit is always 1 and which would indicate negative. Therefore, any hex number whose first digit is at least 8 is negative.",hkpkxiu,t3_qtg70j,1636977517.0,False
qtet0i,"To be this seems to get a few things mixed up at once.

O(nk) can be as said on SO an outer loop with n iterations and an inner loop with k. In the case of k = n you get O(n^2 ).

O(n+k) would be on for loop with n iterations followed by a for loop with k iteration, again if k = n then you'd get O(2n) ~ O(n).

I then don't fully understand what you mean with n = 10 and k = 0.

So in the case of O(n^k ) which you could think of k nested for loops with n iterations, if k = 0 it would mean there are no nested loops, in fact no loops at all -> O(1)

In the cubic sense of O(n^2 ) there's no dependency to k. Now if you'd consider O(nk) though you'd have one loop with n iterations and a nested loop with 0 iterations. Now if the second loops doesn't do anything -> O(1).
Might be easier to think of it as the loop with k iteration being the outer loop.

In both these cases if k = 0 the code will not scale with larger inputs and therefore finishes in constant time or O(1). With that said k = 0 to me seems like more a theoretical exercise than anything else.",hkj6yj2,t3_qtet0i,1636855247.0,False
qtet0i,"thanks a lot!

&#x200B;

>theoretical

yea it was just an assumption to understand the difference but like you said there is no nested loops if k is 0 so it's clear to me now!😊",hkjmokx,t1_hkj6yj2,1636862633.0,True
qtapfy,"https://ieeexplore.ieee.org/Xplore/home.jsp

Some of it is free. A lot of it is very technical",hkjc4kd,t3_qtapfy,1636857594.0,False
qt3e8y,"Computers can understand any base that they are designed to. However, it turns out that representing numbers in a base 2 system is currently electrically optimal.

Computers don’t look at a number and try to understand what value it is. A computer is a combination of digital circuits that apply a set of predetermined transforms on a set of signals, in our case these are binary signals. 

I think the best thing to help your understanding is to look at how some of the simple digital circuits operate. Google around and look at an adder circuit, digital comparator, and multiplier. This should help you understand that computers don’t “know” things, they just operate or compute.

This is a good place to start: https://en.m.wikipedia.org/wiki/Adder_(electronics)",hkgtz8e,t3_qt3e8y,1636818522.0,False
qt3e8y,Username checks out :O,hkgv8m9,t1_hkgtz8e,1636819091.0,False
qt3e8y,This is wonderfully explained answer.,hkh4f2q,t1_hkgtz8e,1636823078.0,False
qt3e8y,Thank you\~,hkhd3s7,t1_hkgtz8e,1636826734.0,False
qt3e8y,I also want to know and look forward a valuable answer. Thanks!,hkgsfsu,t3_qt3e8y,1636817807.0,False
qt3e8y,"Imagine for example, we have two addition problems,. let's say 125+67 and 12+43. We do not do anything special in either case. There is a definitive algorithm on how to add 2 numbers and we just apply it to both problems. We do not have to recognise the numbers to carry out the addition. This is also what a computer does. It just carries out whatever algorithm we tell it to on the values we give it. This is just a basic example, but I hope this gets the idea across.",hkh0bem,t3_qt3e8y,1636821303.0,False
qt3e8y,Guy there’s 1000 page text books written about this.,hkjfoye,t3_qt3e8y,1636859252.0,False
qt3e8y,"How does the computer know which one is 1, 2, 4 etc? Well that's depending on the prcoessing unit's architecture. It depends on how the words are stored in memory. Usually it's MSB (most significant bit) on the left and LSB (least significant bit) on the right. But it could technically be the otherway around.

So in memory at a random adress 0xCAFE with MSB first it'll look something like this (given this represents an unsigned integer, for other cases check out two's complement for signed or IEEE 754 for floats)


 | 2^3 | 2^2 | 2^1 | 2^0 | at 0xCAFE


Now does the CPU care what this binary represents? The sinplified answer is no. It will take this binary number and feed it to whatever place we tell it to. 
E.g. when we tell it to add 1 it will feed it into the adder and do that. The interpretation of what this binary number is, happens on higher levels, also know as the code we run on the hardware.

Example for that, you know how you can open a binary file in an editor? It just looks like random gibberish and that's because we take 0s and 1s that don't represent text (ASCII or any other encoding) and try to display it as such.",hkieop2,t3_qt3e8y,1636842812.0,False
qt3e8y,ffs I can't get that formatting right for the number on mobile sorry for that.,hkif19x,t1_hkieop2,1636842957.0,False
qt3e8y,"Computers do not know which binary values represent which numbers. Computers do one thing: they take an input signal (a string of electrical pulses) and return an output signal. So how does a computer know what to do with the input signal? A computer engineer physically designed the computer to perform *the most fundamental computer actions* the technical name is called the **Instruction Set Architecture opcodes**. A computer programmer is a person who is trained to understand the meaning of the ISA opcodes, programmers will write a computer program that conforms towards the computer opcodes. The computer program they write is also a long string of input signals that the computer will perform.

So you can input any string of binary values into the computer and the computer will perform an action according to the definition of the computer opcodes. This doesn't mean that the computer understands what it is doing, it's just performing computations because it was designed to work that way. So when you enter a number that's supposed to represent a number value, the computer doesn't understand this at all. It is supposed to be the programmer's responsibility to give meaning to the binary string value: it is the programmer who is interpreting and assigning meaning to the binary string value.",hkjirfh,t3_qt3e8y,1636860742.0,False
qt3e8y,i suggest going through the nandgame. it really cleared a lot of this stuff up for me.,hkwe6m8,t3_qt3e8y,1637095318.0,False
qsn40p,"> That said, he then explained us that for that reason (not every state with an input I will end up in a steady state) we need an external synching (a clock) and a memory to make a state steady. Why is that? And why don't we need that for asynchronous circuits?

It's kind of by definition. The use of the clock's edge to trigger input/output latching is what makes a circuit synchronous. Whereas an asynchronous circuit is a ""natural"" electronic one that is always in flux.


So a synchronous circuit is the normal stuff you get in digitial logic - a clock, lots of flip-flops, and then random logics gates between the flips, usually defined by a state machine.

An asynchronous one can be made up of things like [Muller C-Gates](https://en.wikipedia.org/wiki/C-element).


I don't know of a good resource to direct you to for this. Do you know much digital logic? Asynchronous circuits is quite an ""advanced"" topic there, but understanding basic digital circuits and you understand the synchronous case.",hkegq7h,t3_qsn40p,1636764192.0,False
qsn40p,"First note that synchronous and asynchronous only apply to sequential logic, not combinational logic.

In a synchronous circuit, the inputs to a stage are sampled only when the clock signal goes active.  So the clock needs to be slow enough that all inputs are valid when the clock goes active, otherwise the output may be wrong.

In an asynchronous circuit, the inputs to the stage are sampled either all the time, or when it is known that all inputs are valid.  The former solution can have glitches or race conditions if the circuit is not designed properly.

In short, it's all about how to knowing when your input data is valid.  Either wait long enough, or have something tell you when it is valid, or design your circuit so that transitory invalid inputs don't make a difference.",hkegrlh,t3_qsn40p,1636764211.0,False
qs8dso,"Computer science is more theoretical, e.g. analyzing and comparing algorithms. Programming is more practical, with specific details about a particular programming language. Sort of like the difference between a Linguistics class and a French class.

The course descriptions should provide more specific details on the two classes.",hkbghmo,t3_qs8dso,1636713465.0,False
qs8dso,Ohh ok thanks!!,hkbgjrh,t1_hkbghmo,1636713518.0,True
qs8dso,"I liked this comparison the most. Simply saying, you can learn one language and yet stuck in it threw the end of your life. Or learn the grammar, from scratch, and learn how to gather the appropriate resources, just to learn any other language. What you choose?",hke10cs,t1_hkbgjrh,1636756782.0,False
qs8dso,"Most unis typically get it wrong, and have computer science as 'programming for good students' and programming/software as 'programming for weak students but we still want their money'. So the real definition may not apply here.
 
Computer science is the mathematics discipline that studies computation, which is the storage and manipulation of data. So data structures, algorithms, formal languages/grammars, logic. Programming, or software development/engineering is the *use* of that computation to solve problems in the real world.",hkbgh2d,t3_qs8dso,1636713451.0,False
qs8dso,"Ohh I hope they don’t cheat me lol, thank u!!",hkbglgo,t1_hkbgh2d,1636713560.0,True
qs8dso,"See I thought that's what was going to happen to me as a CS major, but boy was I wrong. Now in in Automata Theory and Advance Algorithms breaking down individual components and I want to quit so bad. But I'm so close to graduating so I can't. I hate theory, always have.",hkeewkx,t1_hkbgh2d,1636763305.0,False
qs8dso,"Honestly though it’s going to make you a good engineer. If you can write a proof you can write good, tight code. Mathematical fluency will never be a disadvantage",hkelthn,t1_hkeewkx,1636766704.0,False
qs8dso,This !,hkez94c,t1_hkelthn,1636773386.0,False
qs8dso,"Computer science is the what and why, programming is the technically how.",hkbq3so,t3_qs8dso,1636720463.0,False
qs8dso,Computer Science is to programming as Physics is to Mechanical engineering,hkcy0qb,t3_qs8dso,1636740021.0,False
qs8dso,That's my go-to explanation,hkd00kj,t1_hkcy0qb,1636740838.0,False
qs8dso,"Others have distinguished the two well but I will say the following.

Studying CS will mold you into a better problem solver. You will be able to analyze problems with analytical reasoning; thus, helping you to become a better programmer. CS is all about learning why and how it works; hence, knowing what happens ""under the hood"" is very beneficial for any programmer. You will have the skill to solve problems efficiently if the need arises and you will have the skill to optimize code as well. With CS knowledge, you will write better maintainable code. 

You can definitely study programming and learn CS along the way as well. So, it's up to you but know that CS is very applicable and very useful to programming.

Best of wishes.",hkbjg0j,t3_qs8dso,1636715850.0,False
qs8dso,Thank u so much!!,hkbjo7f,t1_hkbjg0j,1636716026.0,True
qs8dso,I was expecting a joke. This post disappoints.,hkc5hc8,t3_qs8dso,1636728299.0,False
qs8dso,You ever heard about the guy who went from VIM to Emacs?,hkggvwu,t1_hkc5hc8,1636811983.0,False
qs8dso,"In addition to the existing answers:

Roughly the difference between driving and studying the materials science and physics involved in driving.",hkbhsse,t3_qs8dso,1636714561.0,False
qs8dso,So I should take computer science first before programming?,hkbhys8,t1_hkbhsse,1636714697.0,True
qs8dso,I'd pick computer science. If you pick programming you'll probably be programming a lot of stuff without knowing why whereas if you do computer science you'll learn why it works and have a better understanding of why you are programming stuff in a certain way when you pick up programming. That's been my experience anyway,hkbiy8c,t1_hkbhys8,1636715468.0,False
qs8dso,Ohh ok I’ve solidified my choice lolol thank u!!!,hkbj83z,t1_hkbiy8c,1636715676.0,True
qs8dso,"It's not always the case. There was an episode of The Big Bang Theory where the guys broke down by the side of the road. Leonard asks if anyone in the group knew about automobile engines. They were all eager to describe how an internal combustion engine works, then Leonard asks who knows how to fix one; they all fell silent. :-)",hkcki2s,t1_hkbiy8c,1636734576.0,False
qs8dso,"Well, that depends on which trajectory you want.

There is a lot of overlap: you can't learn one without getting a fair idea of the other. If you are going to get into programming, you could in principle start with either one.

However, if you start with computer science and pick up programming, the thing that is going to trip you is all the stuff that is neither CS nor programming. Collaboration, systems analysis, etc.

If you start studying programming, you will possibly get the work-related stuff (there are different kinds of programming courses) and a gist of CS.

If you simply mean that you will study programming, then CS or CS, then programming before you start working, I imagine it's roughly the same.",hkbimg9,t1_hkbhys8,1636715216.0,False
qs8dso,"Oh I see, thank u so much!",hkbiury,t1_hkbimg9,1636715395.0,True
qs8dso,"If you can, try mixing both. During school I'd suggest to do more of CS, but still you need some practice to really get what all the sciency stuff means",hkbweuo,t1_hkbhys8,1636723981.0,False
qs8dso,"its harder to learn computer science than programming, imo you can learn programming yourself online and if you take computer science you should get enough experience to cover for the other course and obviously you will learn more theory. i dont think taking both is worth it but it depends on what exactly they include",hke71ex,t1_hkbhys8,1636759565.0,False
qs8dso,"Computer Science is the study of algorithms.  You will learn programming to help you study algorithms.  Essentially, programming languages are a tool and computer science is a means to understand how to use that tool well.",hke2o99,t3_qs8dso,1636757538.0,False
qs8dso,"Do you go to UT Dallas and are you talking about CS 1200 and CS 1325? Bc then everyone in this comment thread is wrong. One is about what the curriculum teaches and the other is actually learning programming concepts. 

I always tell people to post which courses they’re talking about bc without reading the descriptions no one will know what you’re talking about. People made some pretty wild assumptions.",hkc4743,t3_qs8dso,1636727722.0,False
qs8dso,[deleted],hkbub8a,t3_qs8dso,1636722875.0,False
qs8dso,My schools cs program includes a math minor.,hkd50ek,t1_hkbub8a,1636742920.0,False
qs8dso,"In practical terms, it's like construction vs architecture.",hkbzz3z,t3_qs8dso,1636725742.0,False
qs8dso,Programming is computer science but computer science isn’t necessarily programming,hkc88pf,t3_qs8dso,1636729504.0,False
qs8dso,I don't know about that.  I've some programming that's more similar to Italian pasta.,hkd8xrp,t1_hkc88pf,1636744585.0,False
qs8dso,What's the difference between learning medicine and applying a Band-Aid?,hkct32d,t3_qs8dso,1636738028.0,False
qs8dso,"I forget where I heard this analogy but basically like how astronomers study the stars and use a telescope as a tool to do so, computer science and programming can be thought of in the same way. Programming is just a tool we use to study computer science.",hkcyipy,t3_qs8dso,1636740226.0,False
qs8dso,I would probably go with CS to understand the *why*; most of my colleges that have taken programming in school have discovered you mostly learn programming on the job.,hkd2dp8,t3_qs8dso,1636741819.0,False
qs8dso,"It is sort of like studying music vs playing the guitar (or any instrument).  


The guitar is a technical skill and you may be able to use it well even without formal music education, but knowing music theory is a new domain. Sure, they interact a lot. Also, there is no much sense in learning musical theory if you don't play any instruments.  
Finally, playing the guitar will probably help you pay the bills, but most of the greatest guitarists know a lot about music theory.",hke2qvq,t3_qs8dso,1636757570.0,False
qs8dso,"My university has a blanket computer science degree, and several different emphases for specializing. Data science, bioinformatics, software engineering. In addition to some core computer science practices, software engineering at my university covers material related to the development life cycle as well as some practical programming skills like some ides, in depth practice with git, testing and verification, etc. 

I'm just pointing out there can be overlapping material in the labels, and that there's also software engineering.",hke3pfg,t3_qs8dso,1636758010.0,False
qs8dso,The same difference as carpentry and architecture. Programming is practical and computer science is more theory.,hkfbgeh,t3_qs8dso,1636780194.0,False
qsis3v,"I'm not sure about the companies you mention renting out time, but supercomputers ('High performance computing', or HPC) are not as rare or difficult to create as you might imagine. Many Universities and companies with heavy R&D (e.g. pharmaceutical companies or engineering firms) own their own supercomputers. They will usually approach a specialist company that helps them design, procure, install and maintain it. Or help them build one in the cloud.

  
In terms of what is run, a workload manager like Slurm is usually used. This allows for different groups of users/ different queues for compute jobs. So this could be say your astronomy department, your biology department and your engineering department. They can submit jobs to a queue, which will run when resources (CPUs, GPUs and memory) become available. Queues can be given different priorities and resource allocations.

  
It would be up to the person/ people in charge of the supercomputer to determine who can submit jobs and what queues can be used.",hkdoxnl,t3_qsis3v,1636751427.0,False
qsis3v,[deleted],hkdojb3,t3_qsis3v,1636751257.0,False
qsis3v,No shit? Interesting. Well thanks for replying. I definitely thought that the cost of buying one outright would not justify the research efforts.  Now I'm gonna fuck off the next two hours and see what the pricing models look like for funsies!,hkdoz63,t1_hkdojb3,1636751445.0,True
qsis3v,I mean you can get a 500 ish core system for maybe 250,hkdpaey,t1_hkdoz63,1636751583.0,False
qsis3v,"If you wanted to really get hands on, it is possible to create a small one of your own with a few Raspberry Pis: https://www.raspberrypi.com/news/supercomputing-with-raspberry-pi-hackspace-41/",hkg1oqi,t1_hkdoz63,1636801258.0,False
qs8gkb,"You can try to study slowly, take more notes, and then do a comprehensive project for each chapter, use it continuously, and you will become proficient.I think the most important thing in learning is to use it, not to memorize it.",hkbwwjh,t3_qs8gkb,1636724231.0,False
qs8gkb,"First, I pick books based on whether I see them on multiple “must read lists.” It’s not perfect but Hacker News is generally where I pull book recommendations. 

Second, before buying, check the Table of Contents. In the store or online you can usually see the first few pages. See if the chapter names are totally new or known topics. 

Then, if the books a good fit and has good reviews, I get it. Most books have a “how to read this book” in the intro. So the answer to your question is really “it depends.” Some are only meant to be read front to back. Others have pick-and-choose style. Some have a read the first half in order but then you can pick after that. 

If I’m studying, like for interviews, I take an index card and write down chapters & sections I want to read. 

I usually take notes in the Apple Notes app if it’s mostly text (not a lot of code or special characters) or the GoodNotes app on my iPad with a pencil. You can just use a pencil and notepad for less than $5 too. 

Repeating or refreshing material on an exponential back off is CRUCIAL to actually learning. Look up “how to learn” resources. I know that sounds condescending but learning how to learn is the most important meta-skill you can have.",hkcv8sw,t3_qs8gkb,1636738896.0,False
qs8gkb,If it’s technical that I want to remember I wrote it down even if I won’t ever read those notes again just for the memory boost. But if it’s a story or something I don’t plan to use frequently I just read to understand and don’t write notes. U can always go write notes should I need it,hkc8g01,t3_qs8gkb,1636729592.0,False
qs8gkb,"Personally, I partially read them on an on-demand basis, depending on the problem I’m trying to solve. Never really read them from start to finish.",hkc2ao2,t3_qs8gkb,1636726843.0,False
qs8gkb,I just read it like I would any other book I guess. Start with the first page and finish with the last one; don't go on until you know what you've just read. That's it really.,hkdwxvs,t3_qs8gkb,1636754940.0,False
qs8gkb,"Hi, for me it really depends on the kind of book. For a programming/AI style book I will read it quite swiftly and then start some small projects to play with the software or algorithms. Then, instead of googling when something isn’t working the way it should, read the according section thoroughly. 
Now, for books that focus on the theoretical side, e.g. Algorithms by Sedgewick, I will focus throughout the book and read the import sections more than once next to taking notes and summarising the essentials.",hkdznl0,t3_qs8gkb,1636756166.0,False
qs8gkb,"Lots of good answers here.

I like to read books that have type in programs that actually run so I do two passes through the book. One pass I type everything in.  One pass I read everything but the code.  I get context from the reading and first hand line by line exposure to the code by typing.  The two passes can happen concurrently for example I might type in chapter 4 in the morning and read it as im going to sleep that night.

This is the best way for me and ive been through 4 books this way and Its fun and I love it.

Good luck",hkekjvh,t3_qs8gkb,1636766078.0,False
qs8gkb,"I read my books like a novel (just reading straight through) and I take the approach of repeating the book multiple times. I am making use of the proverb that **repetition is the key to learning**.

1. First exposure is all about getting exposure to ideas within the book. I don't worry about remembering ideas or the way the ideas connect, I just want exposure to the ideas so I know that they exist.
2. Second reading is about considering how the ideas relate to one another. I don't worry about memorizing anything, I want to focus on a narrative that connects a weave of ideas from beginning to end.
3. Third reading is when I start to write notes if I feel like writing notes. Since I have exposure to the ideas and how they connect, I now have a simple understanding of the book, I can write effective notes about the book. If all I wanted was a simple understanding, three iterations is normally good for me.
4. Any more readings above three is to reinforce the learning that I've already done. Repeating the study material multiple times means you can pick up subtle ideas that are easy to miss when you're faced with a whole book's worth of detail.  The more repetitions you do, the stronger your imprinting for the ideas you already know, the easier it is to detect subtle details that are easy to miss. 

This strategy allowed me to read books that where initially incomprehensible to me. The value of repetition means I can (eventually) absorb ideas that have no meaning to me. I don't always need to understand ""why"" or ""how"", but I can ""know"" precisely what the idea says through the power of repetition.",hknagf1,t3_qs8gkb,1636929242.0,False
qs8gkb,very cool method!,hkpfg6f,t1_hknagf1,1636973243.0,True
qs8gkb,Bring that manual in the bathroom! I read when I can! 🚽,hkegxpj,t3_qs8gkb,1636764295.0,False
qsg140,"_DNS - Domain Name System_, ergo: no domain = no point in storing random address on DNS server",hkctk03,t3_qsg140,1636738218.0,False
qsg140,"You register a name to an ip. So your ip isn’t stored within that specific carrier until it’s associated by registering it. The internet is IP address, and can be navigated as such, a dns resolution just equates a string to an ip.

Your pc won’t be a open network device, you connect to the internet through a router and modem, these devices have their own IP and that is queryable in the network table, which is a ledger of known IPs. Your pc has a local IP within that devices network and not the public network.

Your pc -> router -> modem-> buncha different devices -> target server -> buncha different devices -> request origin modem -> request origin router -> request origin local IP.

The other thing to note here is that the device you request data from might have the same local IP as you, but within a different parent network.",hkdnn15,t3_qsg140,1636750870.0,False
qsg140,"Generally speaking your isp who assigns you your IP address will have a DNS entry associated with it. This is not always the case it just usually is. Theres not too much extra information there, but often you can get what city the IP is in and who your ISP is. Either directly or by tracing the names of routers on the way to the IP.

The one for my cell phone right now is:
69-232-153-116.lightspeed.tukrga.sbcglobal.net

(Incidentally I'm pretty sure this IP is shared with a bunch of other folk, and changes since it's mobile, so I'm not terribly concerned with security here, don't share your IP as a general rule)

The actual ""DNS ledger"" is distributed over a number of different servers. To figure out what specific computer has this entry more or less works by reading from right to left. There a few root nameservers that are all synced that can point you to nameservers that control top level domains like .net. Those can point you to nameservers that control domains like sbcglobal.net. It usually ends there but there might be a separate nameserver that controls a specific subdomain like tukrga.

Once you get the specific nameserver, in this case ns2.attdns.com, you can ask it for what ip 
69-232-153-116.lightspeed.tukrga.sbcglobal.net points to.",hkfm7ra,t3_qsg140,1636787714.0,False
qqpf3o,"Document formats aren't a field of research, they're a field of application. There is significant research being done into the component aspects of document storage, display, transmission, translation, etc etc etc, but they won't be published as such because they are useful in many more areas than just documents, and as such it is very unlikely that there is a journal specialising in it (certainly there is no high impact journal, and nothing from ACM or IEEE).
 
Transactions on Graphics would publish things that are relevant to documents in terms of image storage within documents. There are relevant journals for the compsci algorithms to do with text and other data compression (which are not document-specific), there is a growing field of computer history which includes looking at data archival and longevity (including reading old formats of documents), and a sub-field of business anthropology looking at the development of document standards.
 
The application work to bring that research together into some new document standard wouldn't be published in an academic journal, it would be released as a white paper from the company that developed it.",hk1xzzr,t3_qqpf3o,1636541418.0,False
qqpf3o,Thanks for this. I would like to know more about fields that have to do with text compression algorithms.,hk5j52i,t1_hk1xzzr,1636598028.0,True
qqpf3o,"I don't think this kind of thing is being worked on at all. Storage is so bonkers cheap that even if all pdfs were cut it half by a new compression algorithm, almost no one would even notice.
  
Things that are possibly being worked on is getting stuff to load faster, but generally the bottleneck in this situation is going to be the CPU/Memory/Storage interfaces so a new algorithm isn't going to help too much.",hk1or8a,t3_qqpf3o,1636533392.0,False
qqpf3o,"One doesn't usually research file formats, those are very much an application. However, people do regularly research into things that might eventually be used in a new file format if companies ever care enough to implement it.

For example, take this (relatively) young new compression algorithm: [ZStandard](https://en.wikipedia.org/wiki/Zstandard?wprov=sfla1)

Maybe this will one day be used in a document file format, maybe it won't. That's not really something for researchers to decide but for companies building those software and products",hk2byii,t3_qqpf3o,1636550413.0,False
qqpf3o,"Something to keep in mind is that file/document formats are almost never about quality/merits/capabilities of the formats. The format people use is determined by what software they, and the people they are sending documents to, have.  

A good example is JPEG. There have been a few formats that give equal or better quality images, use less space, and use about the same resources to decompress/view. Why haven't those formats pushed JPEGs out of existence? Well everyone's web browsers support JPEG and don't support fancy new format X. While it might seem practical to just say, everyone should just update their browsers' to a never version, and then we can switch. Experience has shown that this just isn't the case. And that's not getting into all the backend and image generation software on the server side, or the firmware on physical cameras, and anything like that. 

Everyday file formats aren't a matter of someone building a better mouse trap. Adobe has a lot of power in the document/image sphere because their software is entrenched there. If you can't get Adobe to make their whole software line support a PDF-successor format, that format is dead in the water. 

Put another way, file formats are a matter of economics not computer science. 

PDF files don't use a single specific compression algorithm by the way. They use a pile of them, specialized for the part of the document they are being used for. Compressing vector graphics, fonts, blocks of text, color images, and greyscale images with different algorithms that are tailored to the specific task. 

Note that while introduced in 1993, PDF was a closed, proprietary format until an open standard for it was established in 2008. The number of versions of PDF is around 10-20 I think, it has been updated/changed many times over the years. The most recent update was in December of 2020. While user visible features have certainly been introduced,fill in forms for example were added in version 1.2 in 1996, most updates have been adding encryption and compression algorithms to the multitude that PDF supports. Keep in mind that 1996 form feature is still vastly underutilized.  Being able to view a PDF, and even more so being able to 'print' anything into a PDF, without installing specific software that the user downloaded and installed (as opposed to came with their operating system) are pretty new things. 

In many ways, you could argue that PDF is more of a container format than anything else. Something that's the case for a lot of file formats nowadays. All audio/video file formats and even zip files fall into this category. The standards define a standard way of holding different hunks of data and metadata, with different formats being used by the individual hunks. A new compression algorithm can be added to the PDF standard without throwing away the format or backwards compatibility.

I am curious if there is anything about PDF you find lacking. Of course, file using less space is nice, but that applies to all files. I'm sure that most people, even in CS, don't know the majority of the features that PDF even support. I'd definitely include myself in that category.",hk2m4d7,t3_qqpf3o,1636555195.0,False
qqpf3o,"This is a great and informative post, even though it doesn’t directly address the question.

Please enjoy my Reddit-welfare award.",hk3z1q6,t1_hk2m4d7,1636574448.0,False
qqpf3o,"It wasn't that I found PDF lacking in features. Wavelets are a thing in applied math, and Daubechies wavelets are used in JPEG. This intrigued me, because it's a great example of using something from research to implement a very practical thing related to pictures. Since I deal with PDF documents on a regular basis, and really like the quality of infinite zoomability of crisp documents that use vector graphics, I was wondering if there is a similar application of research level applied math, or algorithms, not for images as in JPEG but for documents. I realise my question was naive, but all the replies (including yours) have been very helpful in clearing up misconceptions. Your comment about PDF requiring different algorithms for different features was particularly helpful; I was under the impression that for a PDF compiled from LaTeX only vector graphics is involved, I was not aware that fonts are handled differently.",hk5lv1m,t1_hk2m4d7,1636599247.0,True
qqpf3o,"Something to keep in that is that decompression/zooming stuff has to be fast enough to use in vaguely real time because that's what people expect with documents.  Fonts and vector graphics can be scale indefinitely, but raster images you really can't do anything perfect with them. 

There a lot of interesting projects using AI/ML to fill in the missing information on pictures. Some could be used to generate high-resolution images so you can zoom more.",hk826so,t1_hk5lv1m,1636650861.0,False
qqpf3o,"> Is there any chance new discoveries are made (e.g. more efficient algorithms to compress documents)

https://en.wikipedia.org/wiki/Zstandard

> Zstandard (or zstd) is a lossless data compression algorithm developed by Yann Collet at Facebook. Zstd is the reference implementation in C.

You mean like this?",hk1xa1m,t3_qqpf3o,1636540828.0,False
qqpf3o,"I have used this before, it is great when you have a lot of similar files. I used it for a very large web crawler storage, where there was lots of repeated data. Although rebuilding a new dictionary was annoying (and heaven forbid you aren't backing it up safely), I could simply make a dictionary file and the average response was like 4kb instead of 60. I actually used it on pickled python response objects, but same idea. AFAIK Facebook uses it to store people's chat histories, which often use similar phrases and speech patterns. It's also high performance. It's great for that kind of stuff

I will say, though (edit: and this is reiterating other comments) formats like PDF are rare. It's not about whether there's a new better standard, it's about adoption. PDF and DOCX were shoved into widespread adoption and that's why they are standard, because they were pushed by Adobe and Microsoft. A better format that nobody uses is worse than a bad format that everyone uses. It takes a long time (or a killer feature) for there to be any significant migration.",hk3samw,t1_hk1xa1m,1636571825.0,False
qqpf3o,"There are/have been working groups and standards bodies working on document formats. On Computerphile, Prof. Brailsford has presented many videos on his experience working in the field.",hk2elq9,t3_qqpf3o,1636551731.0,False
qqpf3o,"I agree with what has been said about this being not a tremendous issue given modern storage capabilities.

If you are interested in compression generally, I would recommend investigating signal processing and digital signal processing, probability theory, and information/coding theory (especially coding theory); typically, the actual format of these files is just some compression mechanism (typically one or more lossy encodings, followed by an entropy encoding) dressed up in fancy clothes.",hk1qii7,t3_qqpf3o,1636534915.0,False
qqpf3o,"I feel like the “cutting edge” research is mostly in other areas: compression, verification, performance, security, etc.",hk26r4s,t3_qqpf3o,1636547532.0,False
qqpf3o,"This would be dealing with Encoding/Decoding/Transcoding.

You would wanna focus on studies of Algorithms, Cryptography, Computer Graphics, and Programing Language Theory.

Reasons why? The goal. Making a super detailed, small memory required document format that is also computationally less intense. So the fields above would give you the insight to facilitate that.

Good luck!",hk3oo4j,t3_qqpf3o,1636570419.0,False
qqpf3o,"http://www.cs.nott.ac.uk/~psadb1/

http://www.eprg.org/research/",hk2nuqr,t3_qqpf3o,1636555939.0,False
qqpf3o,Did you by chance recently ask if open source software is discouraging innovation citing the numerous frustrations associated with PDFs?,hk2o51e,t3_qqpf3o,1636556061.0,False
qqpf3o,"As far as file compression, document formats, pdf's, etc... go, the technology used today has existed for quite a while and I don't think there's a huge amount of research going into it.  However, we are entering the age of Cloud Computing and there is definitely a lot of new technology being developed around streaming compression, distributed documents, document security/e-signatures, and that sort of stuff.",hk38nat,t3_qqpf3o,1636564236.0,False
qqxc02,"    def step_func(analog, threshold=0.5):
        if analog > threshold:
            return 1
        return 0

The above code is in Python. But it's a very simple algorithm, and can be implemented in any language. You'd have to map that function over your entire wave in a preprocessing step.",hk6h40p,t3_qqxc02,1636617879.0,False
qqxc02,"Not sure what you got going on but usually one sets up the sensors, then for each channel an adc, at some sample rate, with some scaling, fft and filters in sw.",hk3kzdb,t3_qqxc02,1636568997.0,False
qqxc02,Noise is usually dominant at higher frequencies. So you can try removing higher frequency waves. Usually Fourier analysis is used for this task. I guess your removing higher than mean method might work better with frequencies separated though I'm not entirely sure.,hk39jp9,t3_qqxc02,1636564590.0,False
qqxc02,Then a simpler low pass filter method might also work.,hk4ljih,t1_hk39jp9,1636583274.0,False
qqxc02,"Are you computing the mean as the data comes in or are you preprocessing it? 

If you’re analyzing data as it comes in your mean will fluctuate and might not give you accurate results.

If you’re preprocessing, this could be very slow as you need to make an initial pass through per run.


What you might consider is seeing what range your sensor outputs and determining what the minimum amplitude is for it to be a 1. Doing this would require you to run the sensor and do some analysis on the data you get.",hk3jicj,t3_qqxc02,1636568433.0,False
qqxc02,"If you want to turn an LED on only when a certain observation is made in the EEG data stream, you might want to look at reinforcement learning methods. There is a Matlab tutorial with an engineer teaching a DNN to detect him making a high five motion based on accelerometer readings. You might be able to find that one online.",hk4lx67,t3_qqxc02,1636583429.0,False
qqxc02,"I suggest first to look at your data, see if you can find the noise at specific frequencies (like the top comment says its usually higher frequencies but not always), if so then a simple filter would do.

If the noise and data sits on the same frequncy your problem change based on your snr and how presice you want to be.

The most important is to know your data well!",hk6icwn,t3_qqxc02,1636618895.0,False
qqxc02,"You are going to want to use a low pass filter before you threshold your ""on/off"" values.

Here is an easy way to do it with scipy. [https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html)",hk9a53h,t3_qqxc02,1636668700.0,False
qqxc02,You could also look into M of N sliding window detection to determine with a good probability that the light should be on. (If the value is greater than the threshold for M out of N ticks) A tick would be some time unit defined by you.,hk9amu8,t1_hk9a53h,1636668906.0,False
qqxc02,"There are a lot of programable senders out there that will do this for you, including raspberry pi devices. You just set the parameters you want and let it do it’s thing. Tweak it as you go until you get what you want. Some of them even support CAN messaging.",hla4fbb,t3_qqxc02,1637344261.0,False
qqvz15,"I had never heard of it; however, looking it up it seems like a legitimate conference. It does not trip any of the red flags for a conference. H indices are ok. Committee sizes are pretty normal. The acceptance rate is a bit high at 30+%. Normally, you should aim for conferences that are closer to 20-25%. But it is not absurd like 70%.",hk2ojp8,t3_qqvz15,1636556235.0,False
qqvz15,Can I please message you personally?,hk2sbvs,t1_hk2ojp8,1636557793.0,True
qqvz15,Sure,hk2twvo,t1_hk2sbvs,1636558438.0,False
qrlsrw,Programming will never be obsolete the languages just become higher level.,hk7amy7,t3_qrlsrw,1636639014.0,False
qrlsrw,"I'm inclined to agree. Even when they are programming themselves, there will probably always be a market for someone to dig in there and take it further. One of the first things programmers did was automate part of their jobs by inventing compilers. The field seems to be founded on raising the level of interaction through this kind of virtuous cycle.",hk7pfl3,t1_hk7amy7,1636645698.0,False
qrlsrw,Self teaching for over a year and already prophesying  the end of all things,hk7b90q,t3_qrlsrw,1636639314.0,False
qrlsrw,"Yeah... I don't think AI is going to be capable of coding in a right way as there is no right way to code, have you seen GH Copilot?",hk7arqg,t3_qrlsrw,1636639079.0,False
qrlsrw,Copilot is part of the reason I ended up on this thread.,hkclvdt,t1_hk7arqg,1636735127.0,False
qrlsrw,[deleted],hk7ipzw,t3_qrlsrw,1636642764.0,False
qrlsrw,"Basically coders will be made obsolete, engineers will become more important.",hk7s09h,t1_hk7ipzw,1636646799.0,False
qrlsrw,"You are right actually, that's spot on what I was thinking about. !",hk85h0f,t1_hk7s09h,1636652158.0,False
qrlsrw,"People have been saying that technology will put everyone out of work for something like 200 years.  It was only a couple of years ago that everyone was saying how necessary a universal basic income was as machines were taking over work and when there's nothing to do, it's not realistic to expect everyone to support themselves by working.

Look around.  There is no shortage of jobs, there is a shortage of labour.  I don't see any reason this pattern will not continue.",hk7czgm,t3_qrlsrw,1636640147.0,False
qrlsrw,Something like 200 years?  Socrates laments in the Phaedrus that the new invention called “paper” would destroy memory. Obsolescence is an argument as old as time.  Keeps not happening.,hkab0eh,t1_hk7czgm,1636685881.0,False
qrlsrw,Have you ever dealt with a customer? An AI wouldn’t be able to deal with that bullshit.,hk7rota,t3_qrlsrw,1636646665.0,False
qrlsrw,"Not what we have now, we would need human level AI.",hk8038t,t1_hk7rota,1636650021.0,False
qrlsrw,Which is definitely not a couple years out,hk85m4y,t1_hk8038t,1636652216.0,False
qrlsrw,Decades minimum (if it ever happens).,hk8agsf,t1_hk85m4y,1636654095.0,False
qrlsrw,"Nah. Computers cant figure out scope or what brings value.

AI is also highly overrated. The “AI revolution” is just an explosion of data mining and data storage.
Computers 20 years ago could have done the same shit but data is more availa le.

What data would you mine to make an original program? None.",hk7yc37,t3_qrlsrw,1636649334.0,False
qrlsrw,"Lol forget programming my dude, the circus could use someone like you",hk7epnm,t3_qrlsrw,1636640950.0,False
qrlsrw,Be careful on what you read and who you learn from. Most information online is from people that don't know anything and just write articles about AI taking over programmer's work.,hk7oq7i,t3_qrlsrw,1636645398.0,False
qrlsrw,"I enjoy coding.

And as a result of the course that I did, I realised coding isn't something I'd personally wanna do as a job anyway (well at least in the way it works in most companies), and I'm okay with that.",hk7cpom,t3_qrlsrw,1636640015.0,False
qrlsrw,"I mean, if AI is really at a point it can automate programming itself, it's likely it'd have automated away other, manual jobs already by then",hk7qd0s,t3_qrlsrw,1636646091.0,False
qrlsrw,AI to do development work? That's decades away at least (if it ever happens at all).,hk7zzr8,t3_qrlsrw,1636649983.0,False
qrlsrw,"AI has been threatening to remove devs for 40 years.

I'm not scared.",hk81nvw,t3_qrlsrw,1636650653.0,False
qrlsrw,"It's not going to replace programming jobs.

Some kind of an AI thing may be able to produce some code for common cases or tasks. That's something we've seen in recent demonstrations of ""code-writing AIs"". It's not going to be able to produce brand new kinds of code for brand new tasks that don't resemble something they've seen in the material they've been trained with. Even more importantly, any kind of an AI like that is not going to be 100%, and it's not going to be able to tell when its output is wrong or to correct itself. You'll need a programmer to at the very least tell if the code is reasonable or does what it's supposed to. And reading and debugging code can be at least as hard as writing it in the first place.

Good luck just pulling some code out of an AI, plugging it into production and thinking you're done with it, without having someone with expertise involved. That's just not going to happen.

Being able to always get correct code from an AI, or having the AI also understand whether the code actually makes sense, would require artificial intelligence on a much higher level than we're able to even imagine building at the moment. If that ever happens, the entire society will have to be rethought, and programming is almost certainly not going to be one of the first victims.

So, I largely deal by trying to ignore the clickbait titles and senseless articles that proclaim AI is going to replace programmers.",hk8cbxi,t3_qrlsrw,1636654841.0,False
qrlsrw,Buddy McDonald’s is still paying real people to dump fries out of a basket into smaller cardboard baskets. I think we’re quite a long way from AI replacing programmers.,hk8f70s,t3_qrlsrw,1636655983.0,False
qrlsrw,"...said no AI researcher, ever. Don't listen to journalists when it comes to computer science. AI research is still in its infancy after 50 years, and struggling to even define what intelligence is. Real world AIs can be rather frightening though, but mainly because they increasingly get entrusted to make real decisions affecting people's lives, despite being so utterly dumb and manipulable.",hk8gnxc,t3_qrlsrw,1636656570.0,False
qrlsrw,"If all goes wrong, I will still code for the fun. I love coding, earning money is just a consequence.",hk7oem4,t3_qrlsrw,1636645259.0,False
qrlsrw,Even ai will need more people to do operation,hk7s6o4,t3_qrlsrw,1636646874.0,False
qrlsrw,[deleted],hk81zrd,t3_qrlsrw,1636650783.0,False
qrlsrw,What class did you take?,hkcm82o,t1_hk81zrd,1636735269.0,False
qrlsrw,Don’t believe the sensationalist hype.  They’ve been saying the same thing for years.,hk86dha,t3_qrlsrw,1636652509.0,False
qrlsrw,"Yeah… have you actually dealt with any real AI? The most widely used “advanced” AI still autocorrects “fuck” into “duck”. Sure there are some neat AI’s out there but they are extremely limited in what they do, like recognizing real world objects, or analyzing pictures for certain content. The ability to write functioning code just isn’t in the cards for AI yet, and even then, there will still be humans writing the code for the AI… so not really something I’m gonna worry about.",hla58r0,t3_qrlsrw,1637344579.0,False
qr8cpv,Don’t do it. Especially not on Twitter,hk5b4a6,t3_qr8cpv,1636594467.0,False
qr8cpv,Lol. Who are the biggest SWE influencers to avoid?,hk5bpaa,t1_hk5b4a6,1636594727.0,True
qr8cpv,"If you’re looking for real content I would suggest signing up for a weekly newsletter on something you’re interested in.

SWE Twitter is essentially the same 5 recycled jokes and low effort content",hk5py03,t1_hk5bpaa,1636601141.0,False
qr8cpv,Twitter SWE is basically JavaScript porn.,hk52iks,t3_qr8cpv,1636590604.0,False
qr8cpv,any specific accounts you follow?,hk53x6s,t1_hk52iks,1636591233.0,True
qr8cpv,"I follow synsation on Instagram 
Really inspiring. She was a baker then became a web developer",hk52x4u,t3_qr8cpv,1636590785.0,False
qr8cpv,thanks,hk53y7a,t1_hk52x4u,1636591246.0,True
qr8cpv,IAmTimCorey on YouTube is about it. And he pretty much only deals in C# and ASP.NET content,hlas7uv,t3_qr8cpv,1637353141.0,False
qpu9g8,"""Code"" by Charles Petzold is what you're looking for.",hjw4wbz,t3_qpu9g8,1636428588.0,False
qpu9g8,"thanks, I'll look into it",hjw6uxv,t1_hjw4wbz,1636429500.0,True
qpu9g8,"Great book, reading it before starting my studies really helped me grasp some early concepts.",hjwwb08,t1_hjw4wbz,1636445520.0,False
qpu9g8,"Seconded, fantastic book and explains exactly what you want to know plus a whole lot more.",hk05xl1,t1_hjw4wbz,1636502860.0,False
qpu9g8,"That's maybe a really weird way to learn it, but you can try building a Minecraft CPU. I tried it, and from that point on I really realized how CPUs work, what you need the individual parts for, etc. For example, I never really understood what the control unit is used for. (Well I thought I understood it, but I didn't) That was, until I had built my ALU inside Minecraft, and realized ""cool, I have a ALU that can add & multiply things, but how do I set the correct enable bits for the corresponding instruction? Multiplying takes a lot longer than adding, how do I wait longer before executing the next instruction? How do I even get to the next instruction?"". That's what the control unit is for.

All the questions you didn't even think of will answer themselves when you try to build your own CPU.",hjwxsxz,t3_qpu9g8,1636446817.0,False
qpu9g8,"Lol, that's how I've learned how logic circuits work - either by building them from real components or building them in Minecraft. It was kind of eye opening.",hjz7mzn,t1_hjwxsxz,1636488674.0,False
qpu9g8,Wait what’s a minecraft cpu,hjxisli,t1_hjwxsxz,1636463012.0,False
qpu9g8,"I haven't played Minecraft in quite a few years but I think the game is pretty advanced in terms of redstone and all the things you can build with it? 

[Here's something I found on it.](https://minecraft.fandom.com/wiki/Tutorials/Redstone_computers) (I only briefly glanced at it so I'm not sure if it's what we're looking for here)",hjxmmj3,t1_hjxisli,1636465069.0,False
qpu9g8,ok I've never played minecraft before but now I really want to. That is insane,hjxqb9v,t1_hjxmmj3,1636466904.0,False
qpu9g8,"Here's my [stock answer](https://www.reddit.com/r/C_Programming/comments/qpa6pp/i_am_a_curious_boy_having_lots_of_interest_in/hjt2xkg/) for this:

If you want to learn about computer architecture, computer engineering, or digital logic, then:

1. Read [**Code** by **Charles Petzold**](https://www.amazon.co.uk/Code-Language-Computer-Hardware-Software/dp/0735611319).
2. Watch [Sebastian Lague's How Computers Work playlist](https://www.youtube.com/playlist?list=PLFt_AvWsXl0dPhqVsKt1Ni_46ARyiCGSq)
2. Watch [Crash Course: CS](https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) (from 1 - 10) 
3. Watch Ben Eater's [playlist](https://www.youtube.com/playlist?list=PLowKtXNTBypETld5oX1ZMI-LYoA2LWi8D) about transistors or [building a cpu from discrete TTL chips](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU). (Infact just watch every one of Ben's videos on his channel, from oldest to newest)
3. If you have the time and energy, do https://www.nand2tetris.org/

This will let you understand *what* a computer is and how a CPU works. It will also give you the foundational knowledge required to understand how a OS/Kernel works, how a compiler works etc. Arguably it will also give you the tools to design all of that, though actually implementing this stuff will be a bit more involved, though easily achievable if you've got the time. (And if you follow Ben Eater's stuff and have $400 to spare, then you too can join the club of ""I built a flimsy 1970's computer on plastic prototyping board"")",hjx874b,t3_qpu9g8,1636455982.0,False
qpu9g8,"I'm a Senior CS student and even though I took Computer Organization, I'm constantly thinking, ""I get how this works, but HOW does it work a level deeper?"" 

For this reason, thank you for one of the most useful posts I've ever come across.",hjxn339,t1_hjx874b,1636465307.0,False
qpu9g8,"Did you university not have a course on digital logic?


(Although, thinking about my own course years ago, it's possible only the basic gate stuff was mandatory, and the RTL-level stuff where you mush gates together into a computer might have been optional / for computer engineering students)",hjxoxao,t1_hjxn339,1636466226.0,False
qpu9g8,"Unfortunately not, at least not that I'm aware of. 

We have a course on ""the fundamental topics of modern computing systems"" which focuses heavily on assembly, but not the deep-level of why it works, just how to get it to work.

We also have another course on assembly that acts as an introduction to CPU architecture (pointers, logic, etc) but that doesn't go in depth on a hardware level. 

Either way, there's *something* missing, it could definitely be that I'm not an engineering student, but some of your links satisfy that itch I've always had.",hjxvs4c,t1_hjxoxao,1636469418.0,False
qpu9g8,"You should send your University an nasty letter calling them poopy heads.

Digital logic is something that should be touched on in every degree, I think. If only because the basic gate stuff is fun!

Instead you probably had to endure some crap about databases that you could probably have guessed.",hjy25b4,t1_hjxvs4c,1636472145.0,False
qpu9g8,[deleted],hjyb3qp,t1_hjxn339,1636475784.0,False
qpu9g8,"> My boss just says, don't question how it works. As long as it does.

I think this is bad advice. In my experience the people who question how it works will write better code, and they'll be much more useful in a situation of when stuff goes wrong, because when stuff goes wrong abstractions break down, and if you have no knowledge of those lower foundations you've had your legs swept from underneath you.",hk1yzql,t1_hjyb3qp,1636542205.0,False
qpu9g8,"That is exactly my problem. Let me explain more - I haven't had a chance to review all the great links everyone posted, so maybe they already explain what's missing in my head.

=======

CPUs are made of billions of transistors, assembled in logic gates, assembled in more complicated circuits like the Adder, ALU, etc.

I'm sitting in front of that computer and I write a very basic program, compile it and for the sake of argument, I store it in RAM.

'storing' something in RAM means that at the lowest levels, a transistor/capacitor pair gets activated and one capacitor gets filled with current storing 1 bit (I'm guessing now ?). 

so reading something from memory must mean that an electric current arrives in the ram circuitry that says: give me the status of all capacitors in row 10

Then how would the status (full/empty) of capacitors be communicated back to the CPU and what does the CPU do with that info ? Does the CPU in turn also have transistor/capacitor pairs that get activated in order to 'work' with this information ? 

========

This is the kind of explanation I'm looking for because otherwise I've seen plenty of documents that discuss address space, and lookup functions, adding functions but they never explain how \*those\* work.",hjy1hu4,t1_hjxn339,1636471873.0,True
qpu9g8,"> Then how would the status (full/empty) of capacitors be communicated back to the CPU and what does the CPU do with that info ? Does the CPU in turn also have transistor/capacitor pairs that get activated in order to 'work' with this information ? 

The address and data buses are a vital part of a CPU <-> RAM interface. Check out the resources for more :) They'll tell you how such a bus is implemented and how RAM module multiplexors are usually implemented.",hk1yuju,t1_hjy1hu4,1636542091.0,False
qpu9g8,Commenting to save for later,hk0dz03,t1_hjx874b,1636506449.0,False
qpu9g8,Look into https://en.wikipedia.org/wiki/Von_Neumann_architecture,hjwaqms,t3_qpu9g8,1636431342.0,False
qpu9g8,Do you know about logic gates? Start there.,hjw4cbo,t3_qpu9g8,1636428330.0,False
qpu9g8,"I do, for example I know about adder circuits - [https://en.wikipedia.org/wiki/Adder\_(electronics)](https://en.wikipedia.org/wiki/Adder_(electronics))

and coming from the other end I also know about microcode.

So starting with assembly code, say someone writes:

add eax,1

then that gets compiled to binary

when you execute that binary, I would like to know exactly what happens from start to finish. Who interprets the binary code, how does the adder circuitry get involved, who ""puts it in motion"" . It's black magic to me that a string of 1s and 0s can have physical consequences inside a CPU.

I realize this is pretty low level stuff but I never understood it and I'd like to know more about it",hjw7kgh,t1_hjw4cbo,1636429833.0,True
qpu9g8,"Others have linked tons of helpful documentation, but for me what really clicked was watching Ben Eater build his own CPU. Seeing the bus, registers, and gates physically laid out helped it all make sense, and he explains every detail.

https://youtu.be/dXdoim96v5A",hjwfq9e,t1_hjw7kgh,1636433890.0,False
qpu9g8,That’s awesome! This dude is genius . I don’t remember when was the last time to enjoy someone explaining stuff in an interesting way and to take my attention 100%. Thank you for sharing !,hjxqe4y,t1_hjwfq9e,1636466944.0,False
qpu9g8,Yea this was what I was going to suggest.  Even having it on stand by is good if it doesn't make sense now. His videos got more and more cooler the more I learned.,hjxvj68,t1_hjwfq9e,1636469309.0,False
qpu9g8,Dude I’m so glad someone mentioned him. Discovered him a couple weeks back. He’s amazing,hjxivfo,t1_hjwfq9e,1636463056.0,False
qpu9g8,This is a super complicated topic and someone typing it all out is a big ask - this is documented extensively online though!,hjwamue,t1_hjw7kgh,1636431291.0,False
qpu9g8,"Check out [https://www.nand2tetris.org/](https://www.nand2tetris.org/) the go from logic gates to cpu to writing assembly and finally create tetris.

Havn't done the whole course yet myself but if you want to explore it further it is worth a look",hjx04cn,t3_qpu9g8,1636448901.0,False
qpu9g8,"I read this book 2 years ago and I think it answers *exactly* what OP is asking. I didn't follow any course, just the actual book.

Seeing how an adder works revealed the mystery of how circuitry can actually do math.

Then seeing how the whole circuit starts off in chaos where any parts (not good with terminology) could be on/off but once electricity has flowed through the whole circuitry it reaches an equilibrium.

And how certain parts (registers) are protected from that chaos and are updated only after that equilibrium state is reached.

>Who translates 'mov' into what it actually does and who knows how to interpret that command ? 

An assembler does this. It does what you think, it sees mov and there is a set binary sequence which corresponds to that. It just sticks that down as the next bytes of the binary file.

The CPU never sees the letters m/o/v, it is fed the first 8 bytes of the binary file. Those bytes are just put into a CPU register. The bits in those bytes set up the initial conditions of the circuitry which will come to some equilibrium. Then it automatically puts the next 8 bytes of that file into the same register (unless there was a jmp command or an interrupt etc.).

For example, the 10th bit might be connected up to a AND gate, so if that bit is a 0, the output of the AND gate will be off. That's just an example of what I mean by initial conditions affecting the circuitry.

I started out giving a very general answer and went into some details so it's a bit inconsistent but I hope I've advocated for the book well enough.",hjzi8sg,t1_hjx04cn,1636492876.0,False
qpu9g8,Youtuber ben eater [made his own 8 bit computer](https://youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU) from logic gates and then shows how he programs it. It really helped me understand it all,hjxchd8,t3_qpu9g8,1636459159.0,False
qpu9g8,"Somewhat of a simplified answer, but I think this conveys the jist of what you’re asking for.

So first understand what a [Boolean function](https://en.m.wikipedia.org/wiki/Boolean_function) is. Every function of a processor is built as one of those, the mov, ldr, str, etc. will be a function of ones and zeros being input, and translates to a certain series of ones and zeros output. You can build any possible Boolean function you desire with the correct series of transistors like this.

Next you need to understand what a [Multiplexer/Demultiplexer](https://en.m.wikipedia.org/wiki/Multiplexer) is. Essentially, it’s like a type of Boolean function where you give it an address of ones and zeros and it will pick from either a corresponding input or output bus. This is how you “select” things in a computer.

Assembly code correlates almost one-to-one with the machine instructions that gets put into a processor. Each machine instruction is a series of ones and zeros (most commonly 32 or 64 bits, this depends on the specifications of the processor). A certain number of those bits represents the “op code,” which would be your mov, ldr, add, etc. and that specific series of bits would go into one multiplexer to “select” the function that is being performed.

Each of those functions requires other information (ie add needs two values to add together and a register to store them, mov needs a source and a destination register, etc.) which would be passed as arguments. Other bits in the instruction represent those arguments, which would be either your registers or immediate values in your assembly code. Each of those strings of inputs go into a multiplexer that selects the appropriate value.

The largest section of the machine instruction is usually at the end which is the offset. This is needed when accessing data from large memory stores because your memory addresses point to large sections of data, and when you need a specific piece of the data you would specify an offset.

Your assembly then gets translated into these instructions.

Look into Von Neuman architecture and this might put some more of this in context. I hope my explanation at least answers more questions than it creates.",hjxjzeh,t3_qpu9g8,1636463673.0,False
qpu9g8,"Desktop version of /u/JDHuff185's links:

 * <https://en.wikipedia.org/wiki/Boolean_function>

 * <https://en.wikipedia.org/wiki/Multiplexer>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hjxk0ls,t1_hjxjzeh,1636463692.0,False
qpu9g8,"It's a vastly simpler example then a modern CPU, but I found it really helped my understanding - https://youtu.be/dXdoim96v5A

Ben Eater's 8-bit CPU defines an 11-bit ""control word"" which is hand-crafted by the chip designer and stored in ROM for each instruction (and each step of each instruction; they have 5). The control word has a number of bits that each correspond to a transistor that enables/disables a piece of functionality in the CPU .

For example, one step of a Load Memory Into Register instruction have an enabled control word bit that tells the Memory module to ""output"" its current value to the bus, and the Register module has its ""input"" bit enabled to read from the bus.

The CPU works its way through a series of control words to move data around in the CPU, or toggle functionality (for example one of the control word bits switches the arithmetic unit between Addition and Subtraction modes).",hjws2z6,t3_qpu9g8,1636442070.0,False
qpu9g8,"I don't know why this bothers me so much but it isn't ""ben eater's 8 bit cpu"", that video series is covering SAP-1, designed by paul malvino for teaching students digital electronics.",hjwta3d,t1_hjws2z6,1636443025.0,False
qpu9g8,"Yeah, though it's not precisely SAP-1; there are changes, improvements and optimisations he's made, as well as his own clock module, designing the layout etc. There's a lot he's added that I think qualifies it as his CPU, in the same way that Apple's M1 is based on ARM's design",hjwtgdz,t1_hjwta3d,1636443168.0,False
qpu9g8,"Those things were in SAP2 through SAP3... No. It's all in the same reference book, written by Malvino. And people are expected to make changes while building that's what it's for, it's an architecture used to teach.",hjwtiwu,t1_hjwtgdz,1636443223.0,False
qpu9g8,"CPUs aka microprocessors are just made for processing data. They are interfaced with different interfacing IC which does rest of the actual work, like moving data, sending inputs from peripheral devices and sending output to the said peripheral devices. 
For example, you have DMA or Direct Memory Access which allows hardware or peripheral to access the main memory without the need for microprocessor to do anything. 
Microprocessor mainly performs logical and control operations. Like picking data, processing it (doing logical or arithmetical operations like >,<,+,-,/ etc.)
The way they work largely depends on what kind of architecture is being used. Different microprocessors work differently. 

A great way to learn this is to learn how the older processors worked. An 8-bit one would be perfect in my opinion. Something like 8051, 8085 or 8086 are still taught in many places to teach the basics of microprocessors.",hjxjdc0,t3_qpu9g8,1636463332.0,False
qpu9g8,"1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)",hjxu3yg,t3_qpu9g8,1636468676.0,False
qpu9g8,"https://youtube.com/playlist?list=PLH2l6uzC4UEW0s7-KewFLBC1D0l6XRfye
Try crash course computer science.",hjzwuty,t3_qpu9g8,1636498862.0,False
qpu9g8,"I don't understand why people are so eager to recommend books when you're looking for a relatively simple answer. Start the other way around, watch easily consumable medium and then dive into books for detailed information. 

**I recommend** [**this video by Computerphile**](https://www.youtube.com/watch?v=IAkj32VPcUE) **which covers exactly what you've asked** (and a bit more) although on a somewhat high abstraction level. For further reading, what you're looking for is Fetch-Decode-Execute cycle.",hjwx0rk,t3_qpu9g8,1636446127.0,False
qpu9g8,"Probably because books are better than videos, especially in terms of information density. And OP is after a topic that is particularly complicated and requires a lot of information.

I would say watching that 11 minute video isn't going to satisfy OP's questions, whereas reading Code will, and it'll also give them a huge amount of other relevant information they never knew to ask for.",hjx8cgy,t1_hjwx0rk,1636456103.0,False
qpu9g8,"There is a crash course on YouTube on computer science. It explains from ground up all you need to know. No technical knowledge required, you can watch it in your leisure time. https://youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo",hjx0wpu,t3_qpu9g8,1636449616.0,False
qpu9g8,"There is a module on the CPU called instruction decoder. There is a very simple 4 bits CPU simulation that helps you understand it. ""simulator.io | Sample - 4-Bit CPU"" https://simulator.io/board/AWZpw7Fy3I/2",hjxkkus,t3_qpu9g8,1636463995.0,False
qpu9g8,"If you want some in-depth knowledge on how a CPU works and how you build it up from logic gates, I can recommend ""Digital Design and Computer Architecture"" by Harris & Harris.",hjxtlvk,t3_qpu9g8,1636468451.0,False
qpu9g8,"This is quite a large topic area and other users have given good resources to start with. for some footnotes essentially your general purpose cpu is made of a few components such as an arithmetic logic unit (ALU), registers (the fastest form of memory), and a control unit (which controls how these pieces communicate. each flop is controlled by your computers clock so each cycle of electricity will cycle through the cpus network of transistors to perform a task.

the ALU as it's name suggests can perform simple arithmetic operations such as addition and subtraction. From these two opperations we can build more complex operations such as multiplication, division, modulo. etc (see ripple adder for a basic circuit bit number addition).

 small segments of data can be saved in the registers which hold data that should be frequently accessed with less frequently accessed data going to ram then hard-drives.


going down a level these components are made of circuits such as a JK flip-flop which can store a single bit of data, D or data flip-flops, timers, and comparators. 

these in turn are made of the low level logic gates we all learned about at somepoint such as your OR's, AND''s, XOR's, etc. 

finally you get down to the transistor a simple off and on switch where several are used in the correct configuration to create the gates.


beyond this you get into theoretical EE territory which is not my wheelhouse. hope this helps and if I made any mistakes let me know as I am a bit rusty on the topic myself.",hjyclic,t3_qpu9g8,1636476376.0,False
qpu9g8,"A specific answer to your ""Who translates 'mov'"" is that all CPUs have a specific set of instructions. This is called an instruction set. Different CPUs may have different instruction sets. For example, Intel and ARM CPUs do not have the same instruction set. Therefore a compiled Assembly program may run on an Intel CPU but not on an ARM CPU. Or it may run, but yield different outputs. The instruction sets are basically a giant table of Hexadecimal numbers which correspond to a specific instruction.

But if you want to create an instruction set, you have to program the gate logic for the instructions. Gate logic is basically a series of cycles where each cycle has a set of operations for which gates and busses to open/close. Some instructions take more cycles than others to be completed depending on the CPU architecture and instruction set which is why we have concepts like ""Branchless programming"". Branchless programming is basically writing if/else logic differently to avoid switching branches in the compiled machine code since switching branches can be a heavy instruction and takes more cycles.

A good but small difference between ARM and Intel instruction sets is that an 8-bit integer is signed differently. You know the difference between unsigned and signed integers in programming. Say you have a C program where you declare a variable c:

    char c;

On an Intel CPU or most x64 CPUs, this basically compiles or is equivalent to:

    signed char c;

Since an integer is evaluated as a signed integer by default. But on ARM CPUs this compiles to:

    unsigned char c;

So you have to take that into account when you program for ARM CPUs. For example, if you compile this code below to an ARM instruction set. Based on what I mentioned earlier, what is the problem?

    char abs(char c) {
        return c < 0 ? c * -1 : c;
    }",hjzjih2,t3_qpu9g8,1636493383.0,False
qpu9g8,"[http://www.buthowdoitknow.com/](http://www.buthowdoitknow.com/)  
This book explains CPU in very easy to follow way

[https://www.youtube.com/playlist?list=PLuiLMR-Dpj-3s72aqvmKC5Ik\_d6GB6KOf](https://www.youtube.com/playlist?list=PLuiLMR-Dpj-3s72aqvmKC5Ik_d6GB6KOf)  
This playlist explains concepts and logics you need for building computer in minecraft but they apply to real life too

[https://www.youtube.com/c/BenEater](https://www.youtube.com/c/BenEater)This channel got a lot of videos for low level concepts and hardware",hjx287p,t3_qpu9g8,1636450791.0,False
qpu9g8,"[https://ict.iitk.ac.in/wp-content/uploads/CS422-Computer-Architecture-ComputerOrganizationAndDesign5thEdition2014.pdf](https://ict.iitk.ac.in/wp-content/uploads/CS422-Computer-Architecture-ComputerOrganizationAndDesign5thEdition2014.pdf)

read this textbook",hjykt1i,t3_qpu9g8,1636479615.0,False
qpu9g8,especially chapter 4,hjyl08v,t1_hjykt1i,1636479692.0,False
qpu9g8,"Mods, we get this question like EVERY day. Can we please add something to the sidebar/wiki?",hjypm88,t3_qpu9g8,1636481509.0,False
qpu9g8,Sometimes they channel voltage and sometimes they don't. Duh.,hjyzjvo,t3_qpu9g8,1636485453.0,False
qpu9g8,There are lots of useful links. You pretty much need to start with smaller to larger blocks. transistors -> logical gates -> ALU -> registers -> clock. A modern day CPU is an insane piece of engineering and close to magic that it even works.,hjznc10,t3_qpu9g8,1636494912.0,False
qpu9g8,Here you go https://youtu.be/sK-49uz3lGg,hjzrh16,t3_qpu9g8,1636496614.0,False
qpu9g8,"Maybe a bit more high level than you want, but after you done with other links posted here check out
https://www.nand2tetris.org/

It deals with this exact question -- how you go from transistors to operating system.",hk01osf,t3_qpu9g8,1636500969.0,False
qpu9g8,"Comment for personal archive* this is an amazing thread, also pulling in minecraft, beautifully done!",hk01y8y,t3_qpu9g8,1636501086.0,False
qpl4wt,All of them!,hjuovnq,t3_qpl4wt,1636405832.0,False
qpl4wt,This. Imagine doing statistics without a computer.,hjvgcnd,t1_hjuovnq,1636417444.0,False
qpl4wt,you can have computers without computational science though,hjwj59i,t1_hjvgcnd,1636435896.0,False
qpl4wt,"Well, maybe. But who are the people making those algorithms go brrr (sorry)? What people are responsible for making it feasible to program them, for making it secure etc",hjx71tv,t1_hjwj59i,1636455027.0,False
qpl4wt,"The human genome project was only possible with computer science. Sequencing using accurate machines was going too slow and not really working right. Then someone had the idea to use cheap and high throughout machines even though they made tons of mistakes. The trick was to read each genome lots of times on these fast machines and use computers with statistical algorithms to piece it back together and repair errors. This is now the standard way to sequence genomes in general (at least when doing the whole genome rather than looking for specific markers). Genomics is very computational in nature and has had a direct impact on medicine and agriculture (and tons of other stuff).

The reality is that I could probably do this for any scientific or engineering field. Airplane companies only do wind tunnel stuff at the very end because of computers. Drugs are analyzed and candidates chosen using computers. Materials are investigated with simulations and machine learning and such. Astronomy uses massive computations to answer questions and make sense of the massive amounts of data we collect. There are projects to map neurons in brains using computer vision techniques. All of these involve algorithms and math, but they also rely on advances in computer hardware, networks, languages, and systems. Ultimately we do what we do so that other people can use computers for what matters (hopefully).",hjvmmrr,t3_qpl4wt,1636420341.0,False
qpl4wt,"Pretty much every field needs robust solvers for systems of linear equations, least squares problems, eigenvalue problems, and singular value problems. That's computational science (not computer science) in a nutshell.",hjvql2k,t3_qpl4wt,1636422140.0,False
qpl4wt,The entire field of bioinformatics is a good example,hjuqis6,t3_qpl4wt,1636406484.0,False
qpl4wt,"Was just about to suggest this. Only thing is that you need a PhD and some research experience, and additional knowledge of biology.",hjvn2o8,t1_hjuqis6,1636420546.0,False
qpl4wt,"I did some computational research as a chemist when I was in my undergrad.  Not much was accomplished by myself.  However, the school I graduated from literally has a super computer to do quantum calculations for chemistry and biology, including a whole department that does research just for computation.

EDIT: I imagine physics and many others use it a lot too for other stuff, but during my time there I only saw biology and chemistry stuff.",hjvohqv,t3_qpl4wt,1636421191.0,False
qpl4wt,"I have an entire course named computational physics, which is all about using computer programs to find solutions and different scenarios to physical equations.",hjw3zn2,t3_qpl4wt,1636428171.0,False
qpl4wt,I'm pretty sure every laboratory uses software to analyze and store data,hjvs1eo,t3_qpl4wt,1636422792.0,False
qpl4wt,The proof for the 4-color theorem was the first one to be done with help from a computer iirc if you're looking for specific examples,hjupyc8,t3_qpl4wt,1636406262.0,False
qpl4wt,It was not the first one to be done with help from a computer - it was the first proof where the calculations involved in the proof essentially *had* to be done on a computer. Computers were assisting in research and proofs long before that.,hjvhu12,t1_hjupyc8,1636418122.0,False
qp5vod,"Yeah, that's pretty dense. But if you go one sentence at a time and really think through each of the definitions, you can make sense of it. Reading math literature (which this essentially is) is not like reading a novel. You frequently have to go to prior sentences to remind yourself of what something was defined as being.

What Knuth is actually saying though isn't too complex (though he certainly makes it look like it).

In the first two sentences, he's remarking on the fact that the algorithm defined previously (I'm looking in my copy of Volume One of *The Art of Computer Programming*, though mine is the second edition from 1973) as quadruple (*Q*, *I*, Omega, *f*) is not sufficiently restrictive, as it isn't necessarily *effective* (previously defined as being computable by a pen and paper in finite time).

He then goes on to provide a restriction of the quadruple such that the algorithm *must be* effective. He then remarks about how the restriction he provides could be done in many different ways, specifically noting Turing's notion of effectiveness (the most famous and earliest such example), and Markov's notion given in *The Theory of Algorithms*.

How what he wrote corresponds to what I just described can be tricky to figure out, but if you want to read *The Art of Programming* I suggest you try to get used to it.",hjrpbuu,t3_qp5vod,1636347120.0,False
qp5vod,Thanks so much for the reply. I guess I’ll have to dig into this more and try to diagram it out for myself a bit. I appreciate you taking the time!,hjrqs13,t1_hjrpbuu,1636348033.0,True
qp5vod,Yeah sure thing! And definitely have a piece of paper next to you when reading this kind of stuff. You can only keep so much in your head at once.,hjrrgzp,t1_hjrqs13,1636348484.0,False
qp5vod,"I've found that it is much easier to keep a lot more in my head when it is something I'm writing, speaking, or coding than it is when reading something that someone else wrote. One of the real quirks of cognition right there, as the difference is massive. One of the things that makes something like that harder to hold in the mind (for the reader, as I have done this plenty as a writer and felt no shame about it until a few weeks later when trying to read what I'd written) is the use of single letter variables and Greek letters rather than words. There are likely a great deal of programs or examples of pseudocode that are easier to read than that, despite being potentially more complex or lengthier. 

That said, the last time I took an algorithms class I did get pretty good at turning math-speak into pseudocode and found the process very rewarding once I got the hang of it. Once you've done it a few times, Wikipedia is your oyster and the sky is the limit for implementing random stuff based on vague or highly complex descriptions in the language of math. 

For someone looking to get better at it, who enjoys math, logic, and coding but lacks a higher math education, would you say that ""The Art of Computer Programming"" is a good and useful read in the 21st century for someone looking to get good enough to contribute to the field in a serious way?",hjru72d,t1_hjrrgzp,1636350293.0,False
qp5vod,"Totally agreed. I often read something I had written a few weeks ago and become totally confused. But if I write down something someone else wrote I immediately understand it better. And I'm at the point where some Greek letters (alpha, beta, gamma, delta, epsilon, theta, omega) are easy for me to read, but most of them introduce cognitive load that makes understanding more difficult. Which is weird given that they're all just arbitrary symbols.

But I'm not nearly at the point where I can comment on contributing to the field. And I read *The Art of Computer Programming* more out of historical interest than a desire to learn computer science. I can say though that it's definitely not good as an introduction to computer science. Knuth was a younger man when he first wrote them, and younger men almost always try to look smart. If you're not already knowledgeable in the subject much of what he wrote will go over your head (and by design). Doubly so if you lack a math background. What you want is an author who's trying to seem less intelligent than they actually are (Brian Kernighan is a great example, though he hasn't really written about computer science, just computers). For that though I don't have many good recommendations, though one book I always recommend to people trying to learn more about computers is *Code*, by Charles Petzold.",hjrxtdk,t1_hjru72d,1636352887.0,False
qp5vod,"Many thanks! I saw an interview with Kernighan once and was shocked at his easy and humble demeanor. Really chill dude. I admit to wanting to give Knuth a read just to see if I can, though. It's always fun to try and climb those kinds of mountains.",hjs2sfu,t1_hjrxtdk,1636356748.0,False
qp5vod,"I know, Kernighan is great, and especially for his age! He talks with the energy of a twenty year old, but with the humility and wisdom of someone, well, his age. I hope I can be as sharp as him when I get up there in years.

And I totally relate. It's kind of a sense of pride when you read ""the canon"", the works of The Greats. I'd say go for it if you want to, just don't feel the need to do so right away, and come well prepared.",hjs3ca6,t1_hjs2sfu,1636357204.0,False
qp5vod,"Yeah, I sometimes just scribble something when reading, visualizing the definitions. That can help.

Thanks also for the explanation.",hjsf1oa,t1_hjrpbuu,1636367723.0,False
qp5vod,"I am a little rusty on the details but I believe he is describing a version of Churchs lambda calculus, one of the earliest ways to define an abstract notion of what a human can compute: https://en.m.wikipedia.org/wiki/Lambda_calculus

Turing came up with an equivalent version that most (including Church) think is more natural and elegant: https://en.m.wikipedia.org/wiki/Turing_machine",hjt3n8y,t3_qp5vod,1636382575.0,False
qp5vod,"**[Lambda calculus](https://en.m.wikipedia.org/wiki/Lambda_calculus)** 
 
 >Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It is a universal model of computation that can be used to simulate any Turing machine. It was introduced by the mathematician Alonzo Church in the 1930s as part of his research into the foundations of mathematics. Lambda calculus consists of constructing lambda terms and performing reduction operations on them.
 
**[Turing machine](https://en.m.wikipedia.org/wiki/Turing_machine)** 
 
 >A Turing machine is a mathematical model of computation that defines an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the model's simplicity, given any computer algorithm, a Turing machine capable of simulating that algorithm's logic can be constructed. The machine operates on an infinite memory tape divided into discrete ""cells"". The machine positions its ""head"" over a cell and ""reads"" or ""scans"" the symbol there.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjt3ojb,t1_hjt3n8y,1636382592.0,False
qp5vod,"Thank you so much for taking the time to link those pages for me! I appreciate the response.  You’re absolutely correct as well, the next portion mentions these very briefly.",hjuh2zs,t1_hjt3ojb,1636402715.0,True
qp5vod,"Shouldn’t it be obvious?

The f theta omega lambda = theta N, with respect to x1 … xj such that the compute sequence is within Q",hjtvxiw,t3_qp5vod,1636394164.0,False
qp5vod,Not helpful but thanks for taking some time anyway,hjuh4da,t1_hjtvxiw,1636402730.0,True
qp5vod,"FYI this was just a joke comment at how complicated it looks. 

I was mostly just trying to say random variables and pretend I knew what it was saying.",hjv58oq,t1_hjuh4da,1636412506.0,False
qp5vod,Oh sorry haha! I’m used to seeing the /s on Reddit!,hjvzklv,t1_hjv58oq,1636426150.0,True
qp5vod,"Just in case you are still thinking about this. This website does a wonderful job of explaining what's going on with Knuth's definition of an algorithm and compares it to a Markov Algorithm.
https://www.rudikershaw.com/articles/computationalmethod3",hk8fohv,t3_qp5vod,1636656177.0,False
qpr8lq,r/netsec r/cybersecurity,hjw0wvt,t3_qpr8lq,1636426763.0,False
qpjwrs,"> I don't see why we don't just use a [NULL] character at the end of every string if it's so much more efficient than the alternative.

It's not ""so much more efficient."" In many cases it is _far less efficient_, for example, when you need to determine the length of the string. If the string is stored along with its length, determining the length is instantaneous. But if it's stored with a null terminator, you need to count every character.

Storing the length also makes it possible to reference substrings from anywhere within a larger string. With null-terminated strings, you can only reference a substring which goes until the end. For example if the string is ""HELLO WORLD"", in C (a null-terminated string language) you could refer to the substring ""WORLD"", by adding 6 to the start address, but you could not refer to the substring ""HELLO"" without modifying the data to replace the space with a null character. In Rust (another systems language, with length-based strings), you can split the string into ""HELLO"" and ""WORLD"", without modifying the ""HELLO WORLD"" string data.

Null-terminated strings are only more efficient by saving a few bytes per string, which rarely matters anymore, because memory capacities have increased from kilobytes to gigabytes.",hjuba6o,t3_qpjwrs,1636400370.0,False
qpjwrs,"Why not go with both then? If the real struggle these days is in efficiency over storage, why not store the length and a null character, and the compiler could just use whatever is most effective for a specific operation? Or is that overthinking it a little too much?",hjvndnd,t1_hjuba6o,1636420684.0,False
qpjwrs,"That does happen, for interoperability's sake. It doesn't save any storage — it uses slightly more, because you store the length _and_ an extra null character. The C++ specification requires a `string` to store its length, but most implementations also add a null terminator, making it easier to send the string to C functions.",hjvyc4f,t1_hjvndnd,1636425596.0,False
qpjwrs,"Ah okay, that last part was interesting, I guess I worded it wrong because I already understood that first part, but it's interesting that it exists like that",hjw9ikp,t1_hjvyc4f,1636430756.0,False
qpjwrs,"It's more memory efficient, but computationally less efficient.",hju8sc6,t3_qpjwrs,1636399374.0,False
qon555,"Mr Robot, ep1 reference to GNOME vs KDE.

And generally, the series shows lot of real commands from Linux and explain cyber security stuff",hjny21c,t3_qon555,1636287191.0,False
qon555,"Cool! I haven't seen this series yet, but I'll definitely put it on my list",hjnypju,t1_hjny21c,1636287628.0,True
qon555,Silicon Valley is great,hjosyn2,t3_qon555,1636301878.0,False
qon555,Yup! like gilfoyle the most!,hjrg7gp,t1_hjosyn2,1636342144.0,False
qon555,"Numbers is a TV crime/investigation TV show that centers around a math professor's insights into problems. Math, physics, CS, etc. are all part of every episode.",hjojwch,t3_qon555,1636298243.0,False
qon555,"Typically crime shows tend to have a character that is their tech/hacking specialist. The first show that came to mind for me was Criminal Minds, there is a character who’s only job is hacking and the show often shows her referencing linux, pinging IP addresses, etc",hjqhzjf,t3_qon555,1636326222.0,False
qon555,Futurama is loaded with CS jokes and references.,hjor9pb,t3_qon555,1636301201.0,False
qon555,"The Devs miniseries has references to some actual quantum computing topics, including a random mention of Shor's algorithm IIRC (although that one was just in some chitchat).",hjr1q7g,t3_qon555,1636335183.0,False
qon555,Halt and Catch Fire. Excellent drama at the backdrop of 4 different eras of tech booms,hjrs7mk,t3_qon555,1636348968.0,False
qon555,Definitely silicon valley and mr robot!,hjrk2ys,t3_qon555,1636344155.0,False
qon555,The Imitation Game,hjrp2v4,t3_qon555,1636346967.0,False
qoyr6v,"i've worked with embedded systems and board bring-up (baremetal or w/ u-boot) is what i do ... reading RAM specs, and programming the memory controller.

1 RAM has many interface, mix and match is a PITA. physical interface. access protocols. then ranks, and banks. (*i shudder as i remember a project, F that sheyt*)

2 you need a memory controller specific to the RAM modules. most controllers i've used are built-in to an ASIC

3 you need to think about battery backup and good power smoothing

4 RAM is very fast, USB is slow. you are at the mercy of USB throughput speeds.

> the fact that this software exist is proof that a ram disk is possible

remember (*or maybe not*) DOS had a RAMDISK.",hjqbxw5,t3_qoyr6v,1636323618.0,False
qoyr6v,"I am indeed too young for DOS 😂 but I do know what you are talking about, thank you for the helpful input. I'll likely ask you and others more questions as I dig deeper into this project. 

Hmm...  I'm looking at cat8 data transfer rates and it looks like we have a max trans. Speed of 40gbs and under Old ddr3 ram I'm seeing Max trans. Of 12.8gbs on the (possibly more common) 1600's. I imagine thats per module, so I'd max out the cat8 or USB gen 3.2 bandwidth at around 3 modules. Assuming I could get an integrated controller to distribute the data properly amongst the modules. Which for now, is good enough for proof of concept... Though eventually I'd like to make that 4 or 6 module capacity for people who have upgraded more than 1 laptop, I currently have 4 sticks of ram from 2 different laptop upgrades I performed. Different sizes, and potentially different generations (one may be ddr4, laptop was almost 10 years newer) I'm liking this idea for a cool tinkerer project. 😁",hjqe0ua,t1_hjqbxw5,1636324509.0,True
qoyr6v,"cool tinkerer project as it is, but you really need to properly interface (physically and protocol-wise) those RAM properly first (HW and very low level firmware). i don't see any hobby boards that provide slots/HW interfaces for these modules.",hjr4ajn,t1_hjqe0ua,1636336395.0,False
qoyr6v,"I was thinking about the hardware already, I figured I could scavenge or buy some ddr3 dim slots from somewhere, as for the low level programming needed for the firmware, I'd have to find a way to write a custom code, likely in c+ to organize and identify the hardware, and initialize a storage standard like nvme or something to layer on top of the ramdisk software. It's gonna be a pain, but yea. 
It's doable, a 3d printer would be good to make the dimm module housing.
 Mind sharing a source on good learning material to write low_level firmware? I'm familiar with a little bit of python and JavaScript.
 But nothing worth noting, I'm not currently capable of making standalone apps as of yet.",hjr5gab,t1_hjr4ajn,1636336942.0,True
qoyr6v,I am open to learning tho.,hjr5hd7,t1_hjr4ajn,1636336956.0,True
qoyr6v,"Check out Gigabyte's i-RAM. It is an ""external"" RAM disk, but it uses the PCI bus and SATA.

https://www.youtube.com/watch?v=bYbCYgYZVT8",hjrmk9v,t3_qoyr6v,1636345501.0,False
qoj0r4,[deleted],hjndhun,t3_qoj0r4,1636268981.0,False
qoj0r4,"I see, so if someone wanted to have a password on their PC but very deep within the encryption have it locked to altitude, that way all you need do is be at the specific height. 

If when entering the password you are not at the altitude required to have the PC unlocked could it then be set to erase all data and then it is set to overheat and then explode inside insuring that all Data is as unrecoverable as possible?

So the altitude is the door and the password is the key. 

Without the door itself the key is useless. 

What do you think?",hjnevuy,t1_hjndhun,1636270291.0,True
qoj0r4,[deleted],hjnge0q,t1_hjnevuy,1636271739.0,False
qoj0r4,"This is mainly for personal curiosity, although any and all knowledge gained could come in useful. 

So things can be tricked and tempered with, what about old school satellites that are used for navigation instead of the internet, if somehow someone managed to connect the PC so it was only accessible when connected to a sat nav and it’s can’t connect to the internet because it’s hardware was removed and the software was also corrupted specifically the software for internet access. If then it could only be unlocked with connection to a sat nav connection and then only when you reached a certain altitude with 10m SQ at around that certain altitude, then you enter the password and then it unlocks safely, if however the altitude is way off or the password is incorrect on the first attempt, it would then erase all data followed by the CPU and the data drive heating up until it is completely destroyed. 

Would this be more secure instead of having it rely on GPS via the internet?",hjnicmp,t1_hjnge0q,1636273681.0,True
qoj0r4,[deleted],hjniv05,t1_hjnicmp,1636274185.0,False
qoj0r4,and there's the other thing that this is basically just a longer password. Adding a couple characters to the password is going to be better than jumping through hoops like this for anything except an ARG.,hjnmepw,t1_hjniv05,1636277507.0,False
qoj0r4,"So this is a form of multi-factor authentication. Instead of ""something you know"" (password) + ""something you have"" (fingerprint, phone, etc) it's ""somewhere you are"" (location). Even if the location could be spoofed, an adversary would have no way of knowing the position of the device ahead of time, and a robust lock-out mechanism would guarantee brute forcing it becomes unfeasible.

...or does it? In your threat scenario, your laptop is tied to a physical location. If it's stolen, an adversary would probably notice the GPS sensor and find a way to spoof it so it reports the same location where it was stolen (having them stolen it themselves). So at best, it's not a great advantage.

You could do variations of the above where the GPS coordinates (down to a reasonable number of decimal points, to have precision but be usable) could be used to initialise some variation of [HOTP](https://en.wikipedia.org/wiki/HMAC-based_one-time_password) like TOTP (think Google Authenticator) so that instead of time, the second factor is coordinate-based. Which would make for an interesting Cicada-like game, where the laptop can be unlocked only at different geographical locations... or be given to different people, each of them able to unlock it only in one location only they know. 

I'm fairly certain there are some corner cases whereby the procedure would risk failing open (for example, how to ""prime"" the laptop to unlock in certain locations without someone knowing all the locations ahead of time), but it might make for a good story :)",hjnn8a4,t3_qoj0r4,1636278249.0,False
qoj0r4,"**[HMAC-based one-time password](https://en.wikipedia.org/wiki/HMAC-based_one-time_password)** 
 
 >HMAC-based one-time password (HOTP) is a one-time password (OTP) algorithm based on hash-based message authentication codes (HMAC). It is a cornerstone of the Initiative for Open Authentication (OATH). HOTP was published as an informational IETF RFC 4226 in December 2005, documenting the algorithm along with a Java implementation. Since then, the algorithm has been adopted by many companies worldwide (see below).
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjnn93j,t1_hjnn8a4,1636278269.0,False
qoj0r4,"Firstly, yes. Secondly stop watching too much agents of shield. Thirdly, i fully agree with the answer of u/No_Engineering8506 but would say its safer to base it on position, time of creation and motion, i.e. the device would only be accessible on the earth but only directly beneath the iss (or something)",hjnyr0i,t3_qoj0r4,1636287654.0,False
qoj0r4,"Assuming a “trusted geolocation device” which does both encryption and decryption based on (location, password) there is still the possibility of an attack by manipulating the satellites. This may result into the access of the encrypted data or worse the inaccessibility of all data encrypted with TGDs.",hjnk1w9,t3_qoj0r4,1636275314.0,False
qoj0r4,"technically yes, but very difficult to make integrally. it might be easier to do it with local network (if this still meets your criteria).",hjnmld6,t3_qoj0r4,1636277671.0,False
qoj0r4,"You might be able to use nearby wifi SSIDs for geolocation to avoid spoofing 🤔 that comes with its own host of problems, like if the wifi around you changes suddenly you can't unlock your device.",hjp088t,t3_qoj0r4,1636304760.0,False
qoj0r4,"Yes, it would simply be another layer attributed with multi-factor authentication methods. How would you determine location? Longitude/latitude can have whatever degree of precision (by degrees, minutes, and seconds) and altitude is harder to measure and identify on a latitude/latitude basis. Your level of precision would influence the length of the brute force process, and unless you had ridiculous sensitivity on location - which would be troublesome - it would become easier to bypass this authentication method.",hk0pvmx,t3_qoj0r4,1636511802.0,False
qoqb6q,"There is a lot of research. I'm not sure exactly what you are looking for, but if you refine this search you are likely to find it.

https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=non+computable+functions&btnG=&oq=non-comput",hjp39dh,t3_qoqb6q,1636305950.0,False
qoqb6q,"Check out Barak’s Intro to Theoretical Computer Science book, available online.

He presents a very explicit concept of uncomputable, and the many functions that fall into that category (including the Halting problem). Highly recommended.",hjr1o7t,t3_qoqb6q,1636335157.0,False
qoqb6q,"Look for papers by Alan Turing, Alonzo Church, Kurt Gödel, Jacques Herbrand, and Andrey Markov Jr.

Generally speaking it comes down to declaring that what computation is are processes that complete in a finite number of steps to reach a result, and showing that some processes cannot be shown to have a finite set of steps that will do so. The ones that cannot be shown to do so are precisely those that are not computational processes. Those algorithms are not computable.For Turing it was showing that some processes can't be encoded as instructions on a machine that can be shown to stop processing when the machine processes the instructions, and there was no set of instructions that could tell you whether the instructions for other programs would result in a machine that stops processing or not. For Church and others not being computable meant there was no way to reduce some logical formulas using recursive processes in such a way that would eventually be considered maximally normalized, and there is no such recursive process that can show whether another formula has a maximally normalized form or not.",hki1xl0,t3_qoqb6q,1636837382.0,False
qo5xff,Your terminology is a bit confusing. A set only has unique elements to start with. Give an example of what you want?,hjkwted,t3_qo5xff,1636223675.0,False
qo5xff,"Yes, but a want to maximize unique elements among sets. Choose x number of sets that maximize unique elements",hjkxkct,t1_hjkwted,1636223994.0,True
qo5xff,Look into set cover problem and see if you can write your problem like it. If you can then use the best available method for finding set covers?,hjkxvl5,t1_hjkxkct,1636224132.0,False
qo5xff,"Thank you. I'll take a look into it. I was familiarize with the existence of optimization problems, but have never seem the term ""discrete optimization"".

[Looks indeed](https://www.youtube.com/watch?v=cjSeHSjPmsk) very similar to what I'm searching for here.

Thanks.",hjl3o10,t1_hjkxvl5,1636226679.0,True
qo5xff,"What are the input parameters to the algorithm? I think that's what I'm not understanding. 

It almost sounds like you have a minimum subset cover problem on your hands, but that depends on what exactly are the input parameters to the function and what do you want it to output exactly.",hjky7ll,t1_hjkxkct,1636224278.0,False
qo5xff,"Simpler version: I have a list of sets and want to find the best n sets that (together) maximize unique elements.  


less simpler version: I have a list of sets and a list of elements and I want to find the best n sets that (together) maximize unique elements that are present in the element list",hjkytbg,t1_hjky7ll,1636224541.0,True
qo5xff,"That still doesn't really answer the question. 

Are you given n? Or do you find n? And what do you mean by ""best n sets?""",hjkz860,t1_hjkytbg,1636224722.0,False
qo5xff,"Parameters:

I) collection of sets

II) n - size of group to consider

III) (Optional) - list of elements

Output: which n sets from collection should be selected in order to maximize unique elements (if parameter III is given, consider only elements from that list) considering all sets from collection.

Is that description more accurate?",hjkzlvs,t1_hjkz860,1636224887.0,True
qo5xff,"Yes I understand now. 

This seems like a problem that will generally take exponential time since something like a greedy algorithm wouldn't work here (you can't just pick the largest sets). 

Your best bet is likely to use a backtracking approach, but with some clever pruning to cut down on runtime (stop searching using a specific set if it isn't adding enough elements to the total cover or something like that).",hjl1h8c,t1_hjkzlvs,1636225705.0,False
qo5xff,"Thank you. That pruning idea seems very usefull to be used together with some sort of discrete optimization problem mentioned above.

I'll search more about what you've mentioned. 

Appreciate it.",hjl47ru,t1_hjl1h8c,1636226917.0,True
qo5xff,"This is set cover, which is NP-Complete for finding the optimal solution: [https://en.wikipedia.org/wiki/Set\_cover\_problem](https://en.wikipedia.org/wiki/Set_cover_problem)

You probably want to just pick a group of sets at random, or to pick K sets from largest to smallest and take their union (this is the greedy approximation algorithm described here: https://en.wikipedia.org/wiki/Set\_cover\_problem#Greedy\_algorithm).",hjl8pc0,t1_hjkytbg,1636228887.0,False
qo5xff,"Thank you. I'll study it. But as far as I can tell, adding the next subset with the most uncovered points each time is not an optimal way to do it in that case. Is that what greedy aprox do? (Don't bother to answer if you don't want to, once I have not completely understood the wiki page yet). tks!",hjlbsik,t1_hjl8pc0,1636230255.0,True
qo5xff,"Actually, it's closer to this related problem, which is still NP-Hard: [https://en.wikipedia.org/wiki/Maximum\_coverage\_problem](https://en.wikipedia.org/wiki/Maximum_coverage_problem)  


It has the same sort of greedy approximation algorithm, which chooses next the set with the most uncovered items.",hjlf9o5,t1_hjl8pc0,1636231783.0,False
qo5xff,"FWIW, you \*can\* solve this exactly for N sets by checking all 2\^N combinations of sets, and seeing which has the largest number of covered items for the fewest chosen amount of sets, but it's an exhaustive exponential search that's only feasible for small N.

You can get an approximate result (which may be good enough) by randomly choosing K < N sets, checking how many items are covered and tracking how large K is, and then iterating repeatedly to keep the solution with the most covered items for the smallest K. This is sort of similar to monte carlo integration.",hjlfqfy,t1_hjl8pc0,1636231989.0,False
qo5xff,"Example: I am proggraming something to find (among a list of texts, songs for example) the best n songs to learn in  order to maximize vocab (based or not in a desired vocab list)",hjky1ub,t1_hjkwted,1636224208.0,True
qo5xff,">  A set only has unique elements to start with

btw I think that sets can have duplicate elements to start with, but it is useless to have duplicate elements. In other words, it is not a rule that a set cannot have duplicate elements but there is no use of having duplicate elements. You can read more about it [here](https://stackoverflow.com/questions/10011475/can-a-set-have-duplicate-elements)",hjn1n3p,t1_hjkwted,1636260138.0,False
qo5xff,"A set does not have ""duplicate elements"" means you don't write a single element name twice. E.g., `{1,1,1,1} = {1}` and both sets have cardinality 1. A set is defined by membership rules. An element belongs to a set if it passes those membership rules. If an element passes membership rules for numerous reasons you still only write it once in the set. Z unioned with Z is still Z. Now there does exist a [multiset](https://en.wikipedia.org/wiki/Multiset) which allows for duplicates as you are thinking of them.

Stack Overflow is good place to get help with code but a terrible place to get your maths definitions from. Usually, you can visit [math.stackexchange.com](https://math.stackexchange.com/).",hjn2ovr,t1_hjn1n3p,1636260756.0,False
qo5xff,"**[Multiset](https://en.wikipedia.org/wiki/Multiset)** 
 
 >In mathematics, a multiset (or bag, or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The number of instances given for each element is called the multiplicity of that element in the multiset. As a consequence, an infinite number of multisets exist which contain only elements a and b, but vary in the multiplicities of their elements:  The set {a, b} contains only elements a and b, each having multiplicity 1 when {a, b} is seen as a multiset. In the multiset {a, a, b}, the element a has multiplicity 2, and b has multiplicity 1.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjn2q2j,t1_hjn2ovr,1636260776.0,False
qo5xff,"Searching set coverage is NP-hard, and thus has no polynomial time solution. As such a good greedy algorithm is your best bet to find the *exact* optimal solution. If instead you are interested in finding an *approximation* of the optimal solution, then there are many options in terms of [sampling](https://www.mit.edu/~mahabadi/slides/sublinear-sc.pdf) or [linear programming with relaxed constraints](http://theory.stanford.edu/~trevisan/cs261/lecture08.pdf).",hjlj3sg,t3_qo5xff,1636233520.0,False
qo5xff,"So, you have X sets (sets cannot have the same element in them twice, those are usually called ""bags"") and you get to choose n of them, which n do you choose to maximize the number of elements in the total union of all n sets?

This is definitely related to the set cover problem:  
[https://en.wikipedia.org/wiki/Set\_cover\_problem](https://en.wikipedia.org/wiki/Set_cover_problem)

I'd recommend looking at using a dynamic programming algorithm for this, there may be a simple recursive formulation that, when used with memoization yields a simple, elegant, performant algorithm",hjm0cv5,t3_qo5xff,1636241349.0,False
qo5xff,"**[Set cover problem](https://en.wikipedia.org/wiki/Set_cover_problem)** 
 
 >The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. It is one of Karp's 21 NP-complete problems shown to be NP-complete in 1972. It is a problem ""whose study has led to the development of fundamental techniques for the entire field"" of approximation algorithms. Given a set of elements                         {         1         ,         2         ,         .
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hjm0ej6,t1_hjm0cv5,1636241370.0,False
qo5xff,"This seems very similar to this Problem:
[Set over](https://en.m.wikipedia.org/wiki/Set_cover_problem). 

This a NP-Complete problem so there is a good chance you wont find a polynomial algorithm for it. 

I thought about this problem a bit now and i dont think you can get something much better than brute force. There might be a dynamic solution though that runs in linear time with respect to the Capacity (the range of possible values in your case). 

If you need further help just ask me^^",hjmhyg1,t3_qo5xff,1636249623.0,False
qo5xff,"Thank you! I'm trying to read about the cover set problem, many people pointed me there and seems to be similar indeed.

I don't know much about math but what you mean by dynamic solution is something related to optimization problems? Maybe it could be solveable through optimization algorithms considering cost function as number of chosen sets or something?",hjmiiut,t1_hjmhyg1,1636249893.0,True
qo5xff,"They are most likely referring to Dynamic Programming https://en.m.wikipedia.org/wiki/Dynamic_programming
Programming in this sense is referring to filling in a table with solutions as you go along.",hjn1kl6,t1_hjmiiut,1636260097.0,False
qo5xff,"Desktop version of /u/codeIsGood's link: <https://en.wikipedia.org/wiki/Dynamic_programming>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hjn1m3k,t1_hjn1kl6,1636260122.0,False
qo5xff,"You want to select the set with the most unique numbers?

Find the set with the most numbers, let's call it ""set X"", than count how many unique numbers it has, let's call it ""N count"". Than, go through each set. If the set size is smaller or equal to ""N"", than don't count it. If it's bigger, than count it.

I don't see how you could find the set with the most unique numbers without counting the unique numbers on each set. A good optimization is to count the number for each set that is added or changed, so when you need to find the set, you already know it",hjmjjlk,t3_qo5xff,1636250389.0,False
qo5xff,This won't work. It's possible a union of smaller sets yields more unique elements than the largest set.,hjn1as1,t1_hjmjjlk,1636259945.0,False
qo5xff,"If a set has 10 unique numbers, than no set with less than 11 numbers can contain more than 10 unique numbers. Simple

Might as well start with the largest set of random numbers, because it's more likely to be the one",hjn6jsx,t1_hjn1as1,1636263310.0,False
qo5xff,"The algorithm returns a list of sets to choose not a single set to choose.

Example where this doesn't work.

Set 1: {A,B}

Set 2: {C}

Set 3: {D}

 Set 4: {E}

The algorithm would output sets 2, 3, and 4 since the union of sets 2, 3, and 4 has more unique elements than only set 1, yet set 1 has a higher cardinality than any other individual set.",hjoj9i8,t1_hjn6jsx,1636297971.0,False
qo5xff,Parkour has been on-point since Origins. It was an accident. They ended up giving up on Yamaha and just sent them to a custom body shop to get it over my leathers,hjymczv,t1_hjn6jsx,1636480222.0,False
qo5xff,"Believe this is an NP hard probelm, although I remember that the greedy approach (always choosing the set with the most uncovered items (items which are not already seen) is discussed as an approximation.",hju4d4m,t3_qo5xff,1636397594.0,False
qoqmin,Wow,hjqj23r,t3_qoqmin,1636326702.0,False
qoqmin,"finally, no TLE for me on LC questions",hjrmv2v,t3_qoqmin,1636345670.0,False
qoqmin,"The supercomputer, called Jiuzhang 2, can calculate in a single   
millisecond a task that the fastest conventional computer in the world   
would take a mind-numbing 30 trillion years to do..",hjokkkb,t3_qoqmin,1636298520.0,True
qoqmin,According to China,hjr2wxe,t1_hjokkkb,1636335741.0,False
qoh47t,"There is a lot of literature on genetic algorithms and schema theorem. I would suggest reading Holland's original work on it.

To give you a somewhat concise answer, the basic principle is that high-quality solutions have elements (building blocks as Holland called them) that are likely to appear in other high-quality solutions. Hence, with genetic algorithm (GA), a population of high-quality solutions is maintained. Two members are selected and their building blocks (genes) are intermixed. By mainly using the building blocks from these solutions it increases the likelihood of finding a better solution than those in the population. However, since it is possible that a key building block is missing from the population and hence cannot be selected by simply intermixing (crossover), a mutation operation is added to give a chance of finding such an element.

Since GA is non-deterministic it cannot guarantee to find the globally optimal solution. However, it is guaranteed to find better solutions over time because only better solutions are preserved in the survival/culling step. The computational cost for finding a better solution generally increases over time.

Note, that GA is not optimal for all search problems and is highly parameter driven (see various papers by Grefstennete and ""No Free Lunch Theorem""). Parameter optimization is required to make it work well for even those problem spaces for which it can search effectively.

GA is ineffective when the underlying hypothesis is not true, i.e., that high-quality solutions are not composed of building blocks of other solutions. Element independence is good for GA, and interdependence can be bad, as can deceptive search spaces. The best way to really know how to search a particular space effectively is to conduct a fitness landscape analysis.",hjnafhu,t3_qoh47t,1636266276.0,False
qoh47t,Great explanation!,hjp0ktk,t1_hjnafhu,1636304901.0,False
qoj64n,"That's the beauty of the TCP/IP network stack; each layer is independent of the others. Link layer, network layer, and transport layer don't really care what the others are doing. Want to use only wireless tech for the physical layer? Go ahead. Want to use carrier pigeons for your physical layer? Sure. Hell when SpaceX' Starlink is fully operational this would be a totally feasible scenario.",hjnqcc0,t3_qoj64n,1636281066.0,False
qoj64n,"It would be possible if we had completely wireless network infrastructure across the whole planet. There's no reason why this wouldn't be theoretically possible (although in practice, the whole network will probably crawl to a halt with current wireless tech, as it comes nowhere near the throughput of fiber optics), it's just that we haven't built this way (for good reasons, probably).",hjnnpsz,t3_qoj64n,1636278689.0,False
qoj64n,"Exactly, there is only so much RF spectrum we can use which must be shared between wireless devices that are near each other. Whereas each cable can do whatever over the whole spectrum they support with little/no interference with other cables and devices.",hjqns50,t1_hjnnpsz,1636328755.0,False
qoj64n,"Technically yes, practically no.",hjp1pqa,t3_qoj64n,1636305348.0,False
qoj64n,Have you been watching what Elon has been doing lately?,hjo9osm,t3_qoj64n,1636293705.0,False
qoifaz,[deleted],hjok49s,t3_qoifaz,1636298334.0,False
qoifaz,I mean we initialize the entire array _(each box)_ in the code to 0. Why do we only take zero? Why not another number?,hjoke3x,t1_hjok49s,1636298448.0,True
qoifaz,"If your asking why the first row and first column are zero, I think we can rationalize as if we have zero items, then no matter what weight we have we cannot generate any value, thus the value is zero. Additionally if we have zero available weight, then no matter how many items we have we cannot generate and value. Thus the first row and first column must be all zeros. From a mathematical perspective, zero should be the base case as when considering element number 1, we just take the weight of zero elements as zero if we cannot add the first element to the knapsack ( it’s weight is too high).",hju598p,t3_qoifaz,1636397950.0,False
qnl3xe,This is a good question.........,hjilk4q,t3_qnl3xe,1636176052.0,False
qnl3xe,"If you’re curious about answers I also posted this question to r/statistics and got a fair deal of responses. Paraphrasing, the consensus seems to be that statisticians are less concerned with neural networks and favor interpretable, empirical verified, analytically sound models. CS people by contrast tend to treat it like more of an experimental science.",hjilvx2,t1_hjilk4q,1636176279.0,True
qnl3xe,"Here's a sneak peek of /r/statistics using the [top posts](https://np.reddit.com/r/statistics/top/?sort=top&t=year) of the year!

\#1: [\[D\] Accused minecraft speedrunner who was caught using statistic responded back with more statistic.](https://np.reddit.com/r/statistics/comments/kiqosv/d_accused_minecraft_speedrunner_who_was_caught/)  
\#2: [\[D\] Very disturbed by the ignorance and complete rejection of valid statistical principles and anti-intellectualism overall.](https://np.reddit.com/r/statistics/comments/k88ifr/d_very_disturbed_by_the_ignorance_and_complete/)  
\#3: [\[E\] The 2nd Edition of An Introduction to Statistical Learning released. Still free. Lots of new topics.](https://np.reddit.com/r/statistics/comments/p0yg74/e_the_2nd_edition_of_an_introduction_to/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/)",hjilwm7,t1_hjilvx2,1636176293.0,False
qnl3xe,"I think this is sort of like asking what is the difference between a hammer for a carpenter and a hammer for a blacksmith.  Like, they're both tools, and they both hit things, but it's the what the tool is building that's the different.  The carpenter builds a house, the blacksmith builds more tools.  The Statistician (carpenter) is using ML build some data or interpret it.  But CS is using ML to build more or better tools.

I think as you mentioned that /r/statistics is more concerned about the interpretable models they get out using ML.  And that makes sense.  I think CS is more concerned with the algorithms to make those models.  

There's a ton of overlap, ML is like applied statistics using computer science algorithms.  But, CS tends to focus more on how to efficiently build the tools (so to say).  Which algorithms best fit the application and how to arrange the data so that it can be taught to the model. When working with ML there's more focus on the kinds of regressions, variance and bias from some given training set vs the test sets.  The overall goal in CS ML is to find ways to get the best resolution of the data while limiting the BigO time it takes to do it.

For instance.  A statistician doesn't really need to care too much about how long it takes for an ML to process a model.  If they're just analyzing numbers of some huge data set about global wind trends, or aggregate user data and viewership trends.  There's no pressing need that the ML be able to instantly pop out a prediction in miliseconds given some new input, just that whatever answer it arrives at is correct.  CS will say, okay, it's correct, but now my autodriving car just crashed because it wasn't able to quickly make that prediction.  So CS will want to know, ""how can I get as accurate of a model as I can, that can process a new input as quick as I can?"".

And obviously, all this with a grain of salt.  This is just generalties betweens fields but obviously individual use cases for each person in their specific industry may differ.  Like I said, there is a ton of overlap.",hjiqprm,t3_qnl3xe,1636179842.0,False
qnl3xe,"Saw this thread when it was posted in the stats reddit. Here's my tongue in cheek summary.

If you do ml in a stats dept you are smarter, better, more principled, more rigorous, and more interested in interpretability.

If you do ml in a cs dept you will make more money.",hjkw79u,t3_qnl3xe,1636223405.0,False
qnl3xe,One prob focuses more on how mathematically and less dealing with code while the other prob emphasizes more application of techniques,hjk17n1,t3_qnl3xe,1636210119.0,False
qnj83v,"No, no relation. it's 'making' a memo, for example if you want to keep a list of solutions",hjgkguf,t3_qnj83v,1636143025.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh36m,t1_hjgkguf,1636297036.0,True
qnj83v,"I have never seen the two used interchangeably.

If I had to guess, I'd say that a lot of people use 'memorize' in place of 'memoize' because the former is a far more common word than the latter and they're making an innocent typo out of habit.",hjh2cqz,t3_qnj83v,1636149858.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh3r9,t1_hjh2cqz,1636297043.0,True
qnj83v,"Memoization is a specific technique where you store the result of an expensive computation so you don't have to redo that calculation everytime you need it.  It's from an old programming method called 'dynamic programming'.  One example that I see a lot is in Rails, where you might do an expensive database query. Instead of doing that query everytime you need it, you can make it an instance variable:   
\`@some\_var ||= DatabaseRecord.where(query\_is\_expensive)\`    
The ||= operator only does the query if that variable isn't already assigned.  

Another example might be if you are doing a loop with an expensive computation every time. You can just save that computation and reference that variable instead of doing the same thing each time.",hjgmmi8,t3_qnj83v,1636143830.0,False
qnj83v,"Calling dynamic programming an ""old programming method"" isn't quite right. Fundamentally it is just a technique for designing algorithms with recursion - it's not ""old"" in the sense that it is outdated. It would be like calling calculus an ""old method for computing derivatives"".

I think the term ""programming"" can throw one off - when the term was introduced it was a more mathematical term, compared to now it is a more engineering term. The term ""linear programming"" is a helpful comparison.",hjhk3je,t1_hjgmmi8,1636157164.0,False
qnj83v,Fair. I meant it more in the sense that it’s been around for a few decades but good point.,hjhlkxq,t1_hjhk3je,1636157823.0,False
qnj83v,Makes sense - tbh I figured you meant that. I just wanted to make it clear incase your explanation would mislead someone who hadn't heard of dynamic programming but had done some computer programming.,hjhoxv0,t1_hjhlkxq,1636159323.0,False
qnj83v,"I confirm, dynamic programming are used every days in bioinfirmatics.

It's just another way of thinking.",hjhrtm5,t1_hjhk3je,1636160641.0,False
qnj83v,What is the difference from caching?,hjgwcg4,t1_hjgmmi8,1636147519.0,False
qnj83v,"My understanding is that memoization typically refers to saving the result of some computation.

Caching usually refers to saving data locally that lives in some far away location, not necessarily requiring computation. For instance, web browsers cache certain data from servers for later use, CPUs cache data that lives in RAM, etc. 

Though I'm not an expert by any means, I'm sure there could be exceptions.",hjhf8so,t1_hjgwcg4,1636155109.0,False
qnj83v,Your answer really makes sense.,hjhfnd6,t1_hjhf8so,1636155279.0,False
qnj83v,"I recommend you to see a memoized Fibonacci function.

Memoization is awesome because it's a simple thing which avoid to use a lot of useless computations. The best is, many programming languages provide memoization in their standard lib.

Memoization works only for pure functions. Your function has to always provide the same output for a specific input.",hjhsi9m,t1_hjhfnd6,1636160956.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh7xr,t1_hjhsi9m,1636297093.0,True
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh7bd,t1_hjhf8so,1636297086.0,True
qnj83v,"u/JazzGateIsReal's answer is perfectly acceptable, but I would add that really there is a bit of ambiguity.

Caching is a more general term - memoization is a specific form of caching that particularly helps when computing recursive functions.

For instance, in Python you can implement memoization by using the built-in `functools.lru_cache` as a decorator on a function (LRU means Least Recently Used - which is how it removes items from the cache).

Caching can be done in more general ways, e.g. to minimise calls to a server in a networking context so that the server is free to attend to more pressing requests.",hjhpyuu,t1_hjgwcg4,1636159793.0,False
qnj83v,It’s a form of caching. Caching of the results of functional calls.,hji2joh,t1_hjgwcg4,1636165647.0,False
qnj83v,My Rails example could probably be more accurately called caching.  Memoization more strictly defined is used in recursive functions (as I understand it).,hjgz3bz,t1_hjgwcg4,1636148576.0,False
qnj83v,"I had this thing in OCAML classes. I always thought it come with ""lazy programming"" since my teacher didn't name it and the main subject was lazy programming.",hjiy3dc,t1_hjgmmi8,1636185633.0,False
qnj83v,"*OCaml

It's not an acronym anymore.",hjjn6te,t1_hjiy3dc,1636203565.0,False
qnj83v,"Is tabulation top-down or bottom-up? Saw some videos which say it's top-down. Bit confused because bottom-up is about starting from smaller subproblems and then solving the main problem, which is what happens in tabulation in my opinion, doesn't it?",hjoh4z8,t1_hjgmmi8,1636297058.0,True
qnj83v,Note: my Rails example may not be a strict example of memoization but it's a similar concept: https://en.wikipedia.org/wiki/Memoization,hjgnse9,t1_hjgmmi8,1636144269.0,False
qnj83v,"Can do the same thing in Python with Pandas. Run the expensive query once, store it in a dataframe, assign it to a variable and have access to it repeatedly while only running the query once",hjgv7ib,t1_hjgmmi8,1636147089.0,False
qnj83v,'Never memoize what you can infer' - Albert Einstein,hjif2sb,t3_qnj83v,1636172113.0,False
qnj83v,Nope,hjjaiwz,t3_qnj83v,1636195534.0,False
qnduwn,"The M1 doesn't have any eDRAM, it mounts LPDDR4 memory on the same package the processor, resulting in a system-in-package (SiP).

This isn't a novel idea in mobile devices and embedded computers; for example, the original Raspberry Pi from 2012 did it, though it was to reduce the size of the system, rather than to improve memory performance. In the server space, Intel's Sapphire Rapids Xeons support HBM on package. The are coming out later this year or earlier next year, the last time I checked.

Outside of CPUs, GPUs have been doing this for a while; as have the NEC SX-Aurora Vector Engines (since 2017) and FPGAs. Other processors needing high-bandwidth, like neural network processors might be doing the same thing, but I don't know.",hjfh7lm,t3_qnduwn,1636128073.0,False
qnduwn,"ohh, that makes more sense, I was misled by diagrams of the layout in Apple's marketing I guess. I know mobile processors have been using the SiP design, its still interesting to see it in more powerful designs despite not being what I thought it was.",hjfjky6,t1_hjfh7lm,1636128976.0,True
qnduwn,"I don’t know, seems like a great way to limit upgradability … although I’m not sure there’s too much of that to begin with on that platform.  They had something similar a decade ago or so with stacked RAM. Basically soldered on top of the CPU.",hjffwqf,t3_qnduwn,1636127563.0,False
qnduwn,"To be fair, these systems aren’t meant to be upgraded. Like most other laptops.",hjg547e,t1_hjffwqf,1636137259.0,False
qnduwn,Or repaired,hjjb4yo,t1_hjg547e,1636195984.0,False
qnduwn,I'm not exactly sure about ram but M1 will and is having a huge effect on the hardware industry. check out the new intel cpu's and you'll notice how alder lake also boosts a number of effeciency and performance cores just like the M1 does. In no time w'll see more ARM in the desktop space.,hjfp5jp,t3_qnduwn,1636131102.0,False
qnduwn,"More integrators will do it, Intel has something in the pipeline I believe for Core processors.

Won’t be practical for the high end though, but you can already see in Xeon and IBM POWER, they’ll be growing L1-3 cache from MB to GB, merging L2 and L3 with the POWER architecture looking at shared and concurrent cache across chiplets and even across chips.

It will be some interesting years to come. For best results you’ll have on-die RAM and slower external RAM and even slower PMEM backed up by SSD as permanent storage with PCIe NVMe as the slow tier. That can all be integrated as a single access tier with controllers managing the migration of data from RAM to NVMe. That’s where Apple is going by baking all that into the chip and with tight integration in the OS, the SSD is basically the “slow” RAM.",hjigowe,t3_qnduwn,1636173025.0,False
qnduwn,IBM integrates eDRAM inside IBM Power Server processors since Power8 in 2014 as a cache.Apple is soldering ram with the SoC just like the GPU or APU used in PS5 or XB SX|S so it's not the same as eDRAM. If you want to upgrade M1 from 8 GB to 16 GB or M1 Pro from 16 to 32 or M1 Max from 32 to 64 it will take you a lot of time to get the process right assuming that you can use a soldering machine.,hjj3k3i,t3_qnduwn,1636190128.0,False
qnduwn,"It is close to impossible to upgrade any post 2015 mac. But with the introduction of the M1 chips. Damn they are quite powerful, a laptop that is truly worth the title of pro.",hjgbphp,t3_qnduwn,1636139744.0,False
qnduwn,"If integrating dram provides incomparably greater performance over others (with similar form factor), yes.
Ram modularity is not that important for most of comsumer product, compared to everyday performance and battery life.
Not sure about M1 Pro or M1 Max, but I'm pretty sure that M1 competitors will follow in some time.",hjib0gm,t3_qnduwn,1636169928.0,False
qn56a6,"Yes, although metadata (file system info) will be different. But copying a file results in a precisely identical copy. This is one reason why digital is superior to analog recordings (from a degradation standpoint); digital copies are perfect, but analog copies lose fidelity.",hje11vn,t3_qn56a6,1636097770.0,False
qn56a6,"And interestingly enough, sometimes analog's degradation qualities can be desirable (see the proliferation of tape and vinyl simulation audio plugins, and big name music engineers who still record to tape).",hjehyzd,t1_hje11vn,1636111886.0,False
qn56a6,"Yes, there’s a healthy lo-fi movement.",hjf5zg6,t1_hjehyzd,1636123664.0,False
qn56a6,"I'm not a sound engineer, but I think any high-end audio tape recording still in existence today would be using DAT (Digital Audio Tape).",hjeorrv,t1_hjehyzd,1636115892.0,False
qn56a6,"No, I'm referring to [regular analog tape](https://www.youtube.com/watch?v=D5VHW1J5o0Q).",hjetvpv,t1_hjeorrv,1636118447.0,False
qn56a6,That was interesting to watch. I had no idea people still used analog tape these days.,hjex3dp,t1_hjetvpv,1636119928.0,False
qn56a6,"And it's interesting seeing the development of digital software to simulate those ""imperfections"" and subtle characteristics you get from recording to tape, using analog compressors/EQs/preamps, analog synths with variations in the aging components, tube guitar amps, etc.",hjf87kz,t1_hjex3dp,1636124544.0,False
qn56a6,"It's less common for sure, but digital data can degrade as well, albeit under a different set of circumstances. Having redundancies and quality checks for data nowadays mitigates this to the point where risk is negligible, but [bit flips by cosmic rays](http://www.businessinsider.com/cosmic-rays-harm-computers-smartphones-2019-7) can also introduce errors.",hjfs5je,t1_hje11vn,1636132267.0,False
qn56a6,"Yes, and there are plenty of other issues that can degrade digital signals but I only meant to discuss typical/normal operation.",hjhdwmv,t1_hjfs5je,1636154536.0,False
qn56a6,"This entirely depends on the operating system, but I believe that some OSes won't actually copy the file (just pretend to copy it) until something wants to modify it.",hjdz7ja,t3_qn56a6,1636096136.0,False
qn56a6,"Officially known as ""Copy on Write"".",hje7eiq,t1_hjdz7ja,1636103596.0,False
qn56a6,Oh wow that's actually pretty interesting,hjfjtf9,t1_hjdz7ja,1636129068.0,True
qna9yb,"Because writing simple business logic code isn't the point of computer engineering. Computer engineering focuses on computer architecture, computer organization, systems programming (things like firmware, operating systems, device drivers, etc.), and electronics engineering.",hjeprb4,t3_qna9yb,1636116412.0,False
qna9yb,"I agree, but I think 90% cs major are pursuing swe now",hjeq5qz,t1_hjeprb4,1636116620.0,True
qna9yb,computer engineering is not software engineering,hjew80m,t1_hjeq5qz,1636119535.0,False
qna9yb,I am saying most of cs majors will be a swe(or want to be),hjexpwo,t1_hjew80m,1636120207.0,True
qna9yb,It doesn't change the fact that SWE is not Computer Engineering and vice versa (despite sharing *engineering* in name),hjfkgm6,t1_hjexpwo,1636129316.0,False
qna9yb,Is software engineering not just applied computer science.,hjex0xc,t1_hjeq5qz,1636119899.0,False
qna9yb,"most of the time, no. Computer science studies the math and algorithms behind a certain problem. CS studies the beautiful world of computer logic whereas swe (\*sighs\*) is not cs because most of the sw engineers just write basic crud code",hjeye18,t1_hjex0xc,1636120508.0,False
qna9yb,"You need to be good at CS to be good at software engineering, tho.",hjf68k4,t1_hjeye18,1636123763.0,False
qna9yb,"Yep, if you aren't good at CS, you won't be as able to understand when applying stuff from CS would be beneficial vs not doing it, as well as, understanding what you should even apply in the first place.",hjf9079,t1_hjf68k4,1636124862.0,False
qna9yb,"Agreed. You don’t necessarily need to understand software engineering to be good at CS, but you need to be good at CS to be a good software engineer",hjfqcdw,t1_hjf9079,1636131564.0,False
qna9yb,"Sounds like it is dependent on where you went to school and how you are approaching the problems you are working on. My school was very much focused on ""the beautiful world of computer logic"" and then how to use it to build software and solve problems with it. As for problems I'm working on, just yesterday I was having a conversation with the architect on my team about how we could multithread a part of our ""basic crud"" application to reduce overall runtime complexity on an endpoint while also avoiding putting more stress on our company mainframes than we absolutely need to.",hjf0mtd,t1_hjeye18,1636121486.0,False
qna9yb,"I think we are pursuing something academically ""clear"" separation but it's never so easy. Like at my former university: it is called computer science engineering. We were taught heavy math, physics, electronics, networks, programming, computer science, engineering modeling, etc. It was really a mixture, and later you could specialize.",hjezavw,t1_hjex0xc,1636120914.0,False
qna9yb,"I'm sorry, I don't quite follow. If you agree with me that computer engineering is distinct from computer science, then why does it matter at all whether 90% of CS majors pursue SWE or not? They're still different disciplines.",hjew3p2,t1_hjeq5qz,1636119479.0,False
qna9yb,That’s cause computer science degrees generally prepare you for that line of work. Computer engineering is more focused on things like embedded software and other things closer to the hardware level,hjf64ff,t1_hjeq5qz,1636123718.0,False
qna9yb,Computer engineering is not software engineering,hjeu1c1,t3_qna9yb,1636118522.0,False
qna9yb,"To make things more funny in our system when you do the computer engineering bsc, on the 5th semester there is a specialization for software engineering or infocommunications.",hjev84s,t1_hjeu1c1,1636119081.0,False
qna9yb,Usually first two years of all 3 majors are same. Studying math and CS fundamentals.,hjft2g0,t1_hjev84s,1636132623.0,False
qna9yb,Computer science isn't software engineering.  There are software engineering specific programs but if you just changed the title of a program from computer science to software engineering without changing the nature of the program it'd be misleading.,hjetdrn,t3_qna9yb,1636118213.0,False
qna9yb,"> Why do we keep calling CS not CE?

Why do we call Computer Science Computer Science? Because it's Computer Science... There's a massive overlap between CS and CE, but I don't see why we'd rebrand it? If we called CS CE, what would we call CE?

Just because people who get a CS degree go on to be software engineers that doesn't mean they didn't study CS. Just like a lot of Maths majors go on to become financial analysts, doesn't mean we should rename ""Maths"" to be ""finance"".

Personally I did a degree in Computer Engineering but I still become a Software Engineer (although it was CE-adjacent).",hjf0kz0,t3_qna9yb,1636121464.0,False
qna9yb,"Makes sense!
I think most of cs major will become an engineer, that is my point. Also the cs major courses *are* engineering oriented.",hjf1zis,t1_hjf0kz0,1636122057.0,True
qna9yb,"I think you're confused about what Computer Engineering is.

If you'd have said: ""software engineering vs computer science"" or ""Why do we keep calling CS not SE?"" then it would have made more sense.

I would draw the spectrum as something like:

          Maths          ..CS..
            Physics  EE  CE  SE",hjf489x,t1_hjf1zis,1636122966.0,False
qna9yb,"Engineering is the use of scientific principles to design and build ""things"". Computer Engineering is an engineering discipline where you learn how to build computer/information systems and solutions on a theoretical (logical) and implementation level as well. As every engineering discipline we are working with models, and since models are an abstractions of the real world (or the system to be built) from a certain view, this is why you need to study differential equations, graph theory, physics, electronics, distributed systems, etc. so that you can model things correctly. 

All those things are taught at the university, but you can learn it also on your own. 

In Europe/Hungary we use the name Computer Engineering, or Computer Science Engineering.

https://www.bme.hu/computer-science-engineering-bsc?language=en

Most people actually won't become engineers. To become an engineer you need to go through an university and get your bsc or msc. They will become programmers.",hjerwrp,t3_qna9yb,1636117498.0,False
qna9yb,"Lol I agree most of it, except that I think a programmer is a engineer, same",hjeswl8,t1_hjerwrp,1636117981.0,True
qna9yb,A programmer is a computer engineer the same way as a bricklayer is an architectural engineer.,hjetxfj,t1_hjeswl8,1636118470.0,False
qna9yb,"No I don’t agree, there is a reason why they spend six figures to hire a programmer",hjev580,t1_hjetxfj,1636119044.0,True
qna9yb,It has nothing to do with salary 😂,hjevb4q,t1_hjev580,1636119121.0,False
qna9yb,"I haven’t heard companies hire programmers..it is same as software engineers, and with current devops culture, even not much difference between a swe and architect",hjexkca,t1_hjevb4q,1636120139.0,True
qna9yb,"swe is just a formal name for programmers. companies don't hire computer scientists because they don't build anything. computer engineers design and build computers. software engineers design and build software. they are symbiotic, not synonyms",hjeyaxr,t1_hjexkca,1636120470.0,False
qna9yb,"They give it fancy name, but if you are don't have an engineering degree, you are not an engineer. The same way as you are not a doctor if you don't have a doctor degree. It is rather simple. You might do some level of ""engineering"" work, or just code some Rest interface with some business logic.",hjey3oc,t1_hjexkca,1636120378.0,False
qna9yb,There's a difference between them. Computer engineering is electrical engineering applied to computers with a bit of computer science.,hjeyo1e,t3_qna9yb,1636120634.0,False
qna9yb,Computer engineering is typically electronics engineering. Computer science can either be actual CS or software engineering,hjeywmm,t3_qna9yb,1636120741.0,False
qna9yb,Oh that makes sense my bad,hjezpxw,t1_hjeywmm,1636121091.0,True
qna9yb,"Some, I think valuable, distinctions:

Computer Science is the study of algorithms, computing, formal languages, recursion, runtime analysis, data structures, operating systems, databases, graphics, sound synthesis, machine learning, embedded systems, and anything related really. Some of which is more math and paper oriented, some of which is more coding and practice oriented. 

Software engineering (SWE) is an application of those skills listed above, some more than others maybe, to build systems that accomplish some goal for a product or project. Being more well versed in CS is the same thing as being better at SWE. CS is the foundation of good, scalable, fast software.

Computer engineering is a non standard term, and to me is a bit ambiguous as to if you’re talking about systems engineering which I think requires some Electrical Engineering coursework as well, or just SWE which is what your question seems to hint. 

To answer your question:
“Why do we keep calling CS not [SWE]”

Just because some people go on to get a PhD in CS and advance the field with new algorithms and tools, doesn’t mean that those who don’t are not studying CS. CS is a whole host of skills, many of which would be the exact same ones you should learn if you wanted to just skip school and go straight to industry to do SWE. Universities, being places that want to attract people to do PhD programs, keep the title CS. Industry, being it’s own entity that can decide any number of labels for its employees has largely chosen SWE as their moniker. Most people don’t really care that they’re different words, but that’s why they’re different. In practicality, they’re the same field with practitioners and theorists, just like medicine has with doctors.",hjfg1wr,t3_qna9yb,1636127620.0,False
qna9yb,"There  are many engineering majors in the universities too, my point is that computer should belong engineering school",hjfk92w,t1_hjfg1wr,1636129233.0,True
qna9yb,That’s not what you asked in your question. My school includes CS in the Science and Engineering department. Even got a school of engineering hoodie. Sounds like you should take this up with your school,hjfl9ib,t1_hjfk92w,1636129633.0,False
qna9yb,"Most CS programs I’m familiar with are apart of the engineering dept, it’s quite rare to have a separate CS department as far as I know. 

What you do after your degree does not dictate what your degree is called. SWE is the profession that is in most demand that a CS degree qualifies you for. There are also so many paths that range from network architect to systems analyst that boiling the degree down to computer Engineering (which is a field already) does not accurately describe what you learn in a CS degree, especially at the bachelor level.",hjj1vtq,t1_hjfk92w,1636188740.0,False
qna9yb,"Computer science engineer here.

I did physics and differential equations.

Only difference between my major and an EE is the extra software classes and computer oriented hardware classes, versus power conversion and power transmission stuff.


I am NOT a software engineer.

We weren't taught how to write software. We were taught to use computers as a system to solve problems.",hjf5h1t,t3_qna9yb,1636123461.0,False
qna9yb,"Now, that's rather interesting. Let me try to remember what kind of classes we had 20 years ago.... (computer science engineer, fault-tolerant systems major)

Analysis (diff equation, fourier, etc.), Linear algebra (equations, matrices, etc.), Physics (2 semester), Electronics (mostly transistors, 1 semester), Digital systems (2 semesters), Graph theory (1sem), Algorithm theory (1sem), Signals and systems(1sem), Controllers (like PID 1 sem), Probabilty theory (1sem), 3D graphics (1sem), Coding technology (error coding, compression, des, rsa, etc.) , Computer architectures (1 sem), Programming in various languages (3 sem.),  Web programming (1 sem), Communication systems (1 sem), Databases (1 sem), Software development methodologies (1 sem) etc. 

And for the major we had stuff like: model based system engineering, fault tolerant systems, system integration, system validation, and various labs.  


How does it compare with what you learned?",hjfdojc,t1_hjf5h1t,1636126697.0,False
qna9yb,"Sounds pretty similar, but we do engineering statistics and statics as well.",hjfijxh,t1_hjfdojc,1636128582.0,False
qna9yb,"If not swe, What position will be then?",hjfg0yy,t1_hjf5h1t,1636127609.0,True
qna9yb,"An engineer.

Software engineering is a method of producing software by applying engineering-like techniques.

It's not really engineering.

Edit : Why are you booing me? I'm right.",hjfisn6,t1_hjfg0yy,1636128675.0,False
qna9yb,"Software engineer is an engineer, isn’t?",hjfjy3r,t1_hjfisn6,1636129117.0,True
qna9yb,"""Coincidence"" of names",hjfmfw3,t1_hjfjy3r,1636130074.0,False
qna9yb,All engineering basically producing products and considered applied science.,hjfth32,t1_hjfisn6,1636132781.0,False
qna9yb,There's a difference between programming and applied science.,hjfxjsn,t1_hjfth32,1636134349.0,False
qna9yb,Programming is a general way of implementing algorithms to compute mathematical problems. Hence programming is applied science.,hjg8ww1,t1_hjfxjsn,1636138702.0,False
qna9yb,"That's like saying putting together a kit is applied science.

Try writing your own array sorting algorithm and using it at work. Your senior will ask why you didn't just use the default library.",hjg9vzk,t1_hjg8ww1,1636139073.0,False
qna9yb,"At my work we write our algorithms. Your point is invalid. It’s like when a manufacturer company is building a robot should they build their sensors and cameras from scratch too or just buy one from the market? It’s a basic engineering concept, don’t reinvent the wheel.",hjgab3p,t1_hjg9vzk,1636139227.0,False
qna9yb,"I'm not saying that. I'm saying that a lot of programming is done using already been cooked code.

It's a different thing entirely to talk about sensors and physical devices because you still need to design the interface, regardless of which IC you use.",hjgakvb,t1_hjgab3p,1636139327.0,False
qna9yb,This shows that you really don’t know what you are talking about. If all codes are “cooked” out there I wonder why I’m getting $250k a year. Can’t my company just copy the codes and paste them together and save them millions? Have a nice day Mr. Genius.,hjgbam7,t1_hjgakvb,1636139591.0,False
qna9yb,"Calm down there Javascript master. No one is questioning your ability to invert a binary tree or anything.

All I'm saying is that you could be replaced by a trained monkey. No need to be so salty.",hjgcz2c,t1_hjgbam7,1636140216.0,False
qna9yb,"Same for sw, it is also a design phase",hjgoqfg,t1_hjgakvb,1636144624.0,True
qna9yb,"Computer Engineering != Software Engineering

if(person.inCS_major()){

   personCanBeSWE  = true;

   personCanBeDataArtitect = true;

   personCanBeGameDev = true;

   personCanBeComputerEngineer = false;

}",hjj7gg2,t3_qna9yb,1636193219.0,False
qna9yb,"Your question makes little sense. What is precisely the context between computer science versus computer engineering?

If you are referring to a degree program, than the answer is obvious. A degree is named based on the course content and syllabus, not on the eventual career outcome of people receiving education under that program.

Majority of people who receive any degree at undergraduate level and masters level, and even significant number of people who go all the way to Phd level ( or any equivalent qualification) will not have a career as a researcher in that field or equivalent position that requires the epitome of specialization in that field. Therefore, select few physics major become physicist; select few creative writing majors become full time authors of creative literature; select few business majors go on to become a CEO ; and you can expand this argument to many other fields.

So, a computer science is called that because the university offering it believe that the courses and syllabus fall under the field. And, it is common for colleges to differentiate between a computer science and computer engineering course. At my school these are two different majors despite considerable overlap, and at most school the same applies.

If you are referring to how the term CS is used in casual conversation — e.g CS pays well— there can be some semantic ambiguity which by the pragmatics of the language it is very clear anyway. For the very example I have cited, it is clear that speaker is making a comment on specific subset of the field, rather than a statement on the field in its entirety.

Some lay people may not understand the differences and the intersection of different fields but that usually has little to no bearing on the use of terminologies, jargons, and other common lexicon, and it is very easy to convey these differences if and when the situation requires for it.",hjfd8ph,t3_qna9yb,1636126525.0,False
qna9yb,Computer engineering deals with hardware. Computer science deals with theory. Software engineering deals with software and applications. All of them overlap in different degrees.,hjfspl4,t3_qna9yb,1636132482.0,False
qna9yb,"I know of (non-computing) engineers who are pissed at the term software (etc) engineering because it’s borrowed the terminology without taking the rest of what makes an “engineer” along with it i.e. the notion of chartered status, a regulatory body for professional standards, etc",hjfznbk,t3_qna9yb,1636135172.0,False
qna9yb,"The names are conflated because CS spawned out of the math department at some schools and out of engineering departments in others. It spans a wide variety of traditional categories, and the names have never really settled.

In my mind SWEs are programmers, the only “real engineers” are hardware/architecture people, and “computer scientists” span a wide range of computing theory to very applied stuff.

Also, my college degree officially says “computer science and computer engineering” even though it was just one major, and my Master’s says “electrical engineering and computer science” even though it was on very abstract CS stuff. Just a matter of what the departments were called, lol.",hjhlozy,t3_qna9yb,1636157872.0,False
qna9yb,Op are you 12?,hji369w,t3_qna9yb,1636165947.0,False
qna9yb,maybe you would like to rephrase the post...,hjeyj8p,t3_qna9yb,1636120573.0,False
qna9yb,"Yeah sorry I meant it should be an engineering degree not a science degree , well maybe it does not matter",hjezyh6,t1_hjeyj8p,1636121193.0,True
qna9yb,Computer engineering is more about the hardware interaction than software,hjf6zn3,t3_qna9yb,1636124058.0,False
qna9yb,I don’t think you know what computer engineering is.,hjg43os,t3_qna9yb,1636136873.0,False
qna9yb,Computer science is a branch of mathematics. Computer engineering is a branch of electrical engineering. A computer scientist is a specialized mathematican. While a computer engineer is a specialized electrical engineer.,hjht8fj,t3_qna9yb,1636161298.0,False
qmvvyw,"I think Rust and Go are allowed into the party of languages you wouldn't mind in your kernel, but those are fairly recent developments and they have a lot of catching up to do. C++ is still shunned in OS development, partly due to old Torvalds sentiment that a lot of people picked up on, which... sure. There are few in this world that could challenge the pedigree of that outlook and I am not one of them. Objective-C is higher level than C and that's supposedly what MacOS runs a lot of kernel level stuff on but that language is just digital pain.",hjdfgx4,t3_qmvvyw,1636082722.0,False
qmvvyw,Go’s mandatory garbage collector would be problematic in a kernel.,hjeteds,t1_hjdfgx4,1636118222.0,False
qmvvyw,"> can we build an OS from Python

Sure, I don’t see why not. *Should* you build a language in Python? No, probably not. People have built OSes in high-level languages though: https://node-os.com/ https://github.com/froggey/Mezzano

And probably plenty more. 

What language *should* you use today? I’d say Rust. The Linux kernel is obviously written by incredibly smart people, but programmers make mistakes. Google has a project to identify bugs in the Linux Kernel and found that over 70% of them are memory issues or data races. Mozilla, Microsoft, and many other companies have said the same things about their C/C++ code bases. However, these issues are, by design, impossible in Rust. Rust’s unique ownership model makes data races impossible and allows it to properly free your memory without the need for programmers to do it manually like in C/C++ *and* without the use of a garbage collector like all high level languages. And Rust is just as fast as C/C++ and sometimes faster, while still being a higher level language.

If you want to learn a low-level language or make an OS today, consider Rust over the traditional choice of C/C++.",hjdvtsg,t3_qmvvyw,1636093279.0,False
qmvvyw,"I know that Rust prevents memory leaks because of ownership, but does it really help prevent race conditions?",hje9mbe,t1_hjdvtsg,1636105554.0,False
qmvvyw,"Yes, it has rules for safe concurrency enforced through the borrow checker and ownership too.",hjeajz8,t1_hje9mbe,1636106354.0,False
qmvvyw,"Not all race conditions, only specifically data races as far as I know. But for data races specifically, yes, it completely eliminates them. I don’t actually know Rust so I can’t really explain how it works though.",hjfpjor,t1_hje9mbe,1636131255.0,False
qmvvyw,"The answer is C, thank you.",hjdgcyd,t3_qmvvyw,1636083187.0,False
qmvvyw,"Answers about C are wrong. Kick every of that responders.

There are at least some os kernels on golang, you can just Google them. [Example](https://www.google.com/url?sa=t&source=web&rct=j&url=https://github.com/gopher-os/gopher-os&ved=2ahUKEwiC0M-snoH0AhXSwosKHS-EBocQFnoECEMQAQ&usg=AOvVaw2Oce0Pru-lcy5pyqaAEwwA)

It's possible to create os at least by using any language, that is compiled into machine code",hjep5xd,t3_qmvvyw,1636116100.0,False
qmvvyw,I think its C,hjefink,t3_qmvvyw,1636110221.0,False
qmvvyw,It's all machine code in the end so you can do it however you like so long as you got the compute resources.,hjffjxo,t3_qmvvyw,1636127423.0,False
qmvvyw,"Microsoft Excel, but first you must invent the universe.",hjfsfqy,t3_qmvvyw,1636132377.0,False
qmvvyw,"> I guess I'm asking programming languages that best communicate with modern hardware architectures

What did he mean by this?",hjmpn6h,t3_qmvvyw,1636253417.0,False
qn6kqf,"I can't tell you every database but Postgres has some pretty good Documentation that goes through how their protocols work. [HERE](https://www.postgresql.org/docs/current/protocol.html). You'll probably want to look at the Message flow section to get to the networking bit.

TL:DR for the other Databases they'll all be some flavour of a standard TCP connection flow aside a couple that may do some flavour of UDP.",hjeclfa,t3_qn6kqf,1636108021.0,False
qn6kqf,That was a great read. Thanks for recommending it.,hjz7fk2,t1_hjeclfa,1636488592.0,True
qn6kqf,"Do you mean like the OSI model layers? Here’s a pic:

https://electricalacademia.com/wp-content/uploads/2018/12/image-result-for-osi-model-layers-and-its-function.gif

Layer 1 is physical layer (actual electric signals through cables) all the way up to presentation and application layer (6 and 7) where your data format is understood and your app presents data on the screen. For connection management look to layers 5 the session layer, that were you’ll find all the goodies about host and client sessions. But it’s just part of a bigger picture, so if you want to deep dive into how computers connect and talk to each other I think looking up the osi model will help, and you can drill down where you are interested.

https://www.freecodecamp.org/news/osi-model-networking-layers-explained-in-plain-english/

I hope I understood your question and could be of some help!",hjkghzv,t3_qn6kqf,1636216727.0,False
qn6kqf,Opening a connection literally creates a thread that both the server listens/sends on and the client listens/sends on. Data goes up and down the OSI model (depending on the protocols used) when data is transmitted. The rest is the basics with networking.,hk0qrmp,t3_qn6kqf,1636512200.0,False
qm9wc6,Github Repo containing visualisation: https://github.com/angary/simulated-annealing-tsp,hj8aqko,t3_qm9wc6,1635990330.0,True
qm9wc6,This is so sick. Just last week I was learning about Simulated Annealing in my Performance Modeling Class. Applications are endless!,hj8w3eo,t3_qm9wc6,1636001584.0,False
qm9wc6,Sick! 😎,hj8bsat,t3_qm9wc6,1635990802.0,False
qm9wc6,So cool dude here I am writing  and **proving the correctness** of the same old bellmen ford for my homework :(,hj8bq0w,t3_qm9wc6,1635990773.0,False
qm9wc6,that was satisfying,hj8ib6a,t3_qm9wc6,1635993879.0,False
qm9wc6,Oh man I’ve forgotten this from uni 😥,hj9i488,t3_qm9wc6,1636020024.0,False
qm9wc6,Wow this is so coooooool!!!!,hj9xx5g,t3_qm9wc6,1636030321.0,False
qm9wc6,"Cool visualization! I'm actually working on a probabilistic analysis of simulated annealing, so it's always nice to come across it in the wild.

What cooling schedule are you using, if I may ask?",hja0n4b,t3_qm9wc6,1636031687.0,False
qm9wc6,I'm using a geometric cooling schedule - however in the gif I speed up the playback rate during towards the lower temperatures (just so you can see more of the action faster at the lower temps),hja7m60,t1_hja0n4b,1636034862.0,True
qm9wc6,Geometric schedules cool quite quickly no? Are the results significantly better than just straight 2-opt?,hja86cn,t1_hja7m60,1636035106.0,False
qm9wc6,"Cooling quickly can be a benefit as you don't need it at a temperature too long (however, you can adjust the cooling rate).  


So far, the SA is better than pure 2-opt, however not significantly (though I may not be using low enough cooling rate - lower cooling rate == better results, though takes significantly)",hja8kzs,t1_hja86cn,1636035285.0,True
qm9wc6,"Yeah that's what I figured, with a geometric rate you ""freeze"" so quickly that it's not *that* much better than iterative improvement. Getting rigorous bounds on that is one thing I'm interested in.

You can probably get better results pretty easily by implementing a basic version of Lin-Kernighan. The full LKH algorithm isn't even really necessary to outperform simulated annealing.",hjad1pr,t1_hja8kzs,1636037127.0,False
qm9wc6,This is awesome! I just wrote an Operations Research exam today and was tested on simulated annealing,hjaovio,t3_qm9wc6,1636041753.0,False
qmuxrz,python is not compiled.,hjc5p0d,t3_qmuxrz,1636062423.0,False
qmuxrz,"Okay then a different example, like Java or C++",hjc5xum,t1_hjc5p0d,1636062521.0,True
qmuxrz,"Java isn't compiled to machine code either 🙂

To answer your direct question the `if (x == 2)` might to produce the same machine code. Basically a `cmp` and a jump instruction (`jne` maybe). 

`print(x)` would be pretty different between languages (not just the keyword / function name itself) due to string interpolation, formatting, buffering, flushing, etc. There's a lot that goes into printing stuff out.

For your most specific question (regarding C++ and C), **if** the same compiler is used and **if** the basically the same source code is used, then it _might_ be the same machine code. Though C++ `std::cout << x` likely produces different machine code than C `printf`.

(this is all a vast generalization and I am **not** an expert, not even close. Please anyone else correct me)",hjc8b30,t1_hjc5xum,1636063487.0,False
qmuxrz,"Yeah I thought as much that if the syntax is different (even though the logic be the same) it would have different machine code. (but I'm also not certain)

So I would truly like to know if the analogy is true of choosing the programming language is like choosing a different brand of screwdriver for making a car. Is it truly not any less efficient to program operating systems with C++ or C# or a more modern language?

Like I know programming languages were designed to program is specific areas, like SQL for database and Java for websites, and Python for applications. So my question as I originally stated is will C always be the language we use to communicate with hardware?",hjcd28x,t1_hjc8b30,1636065465.0,True
qmuxrz,"C/C++ were designed as systems languages, as something you can put right on top of the hardware. They have fixed bit length variables, direct memory address access, pointer arithmetic, direct hardware register access, interrupt handling. They essentially do a good job at reflecting what the hardware is actually like. Most others were designed as applications languages, to produce programs that only need to talk to an operating system but don’t deal with the hardware directly. That’s not a strict rule you can use either for the other purpose but they’re not really designed with that purpose in mind. If you take a look at embedded engineering where it’s still common to write programs right on top of the hardware with no operating system in between it’s almost exclusively C/C++ for this reason.",hjclx8w,t1_hjcd28x,1636069220.0,False
qmuxrz,"> So my question as I originally stated is will C always be the language we use to communicate with hardware?

_Always_ is a long time! At some point, quantum computing will be mainstream. Far into the future, who knows? We may all be living in a simulation with a universal VM where our every thought is merely a representation of some form of bytecode in the Zuckerverse.

In the short to medium term, yeah, C will still be king with regards to communicating with hardware. Rust will gain some traction in container runtimes, device drivers and small parts of OS kernels, but C will still be the lingua franca",hjd0zwh,t1_hjcd28x,1636075832.0,False
qmuxrz,"> Like is the instruction of if (x==2) then print(x) the same machine language for C and python?

Answering the question directly, no.

Python compiles down to its own byte code that is unique to python.

Java compiles down to byte code for the JVM (Java Virtual Machine). The JVM hosts other languages as well such as Clojure.

C compiles down to _whatever instructions set you want_. That might be x86, that might be x86_64, that might be 6502 or ESP8266.

Compilation just translates the code from one language to another, usually simpler, language. Usually down to some machine-readable code, either a virtual machine (NOT like VMWare) or physical machine code. Physical machine code is interpreted by physical hardware rather than software (we must get this low level at some point) and depends on the exact machine you're running the code on. So C doesn't even have to compile to the same code/instruction set as C.

Finally, the `print(x)` actually must ask the OS to do the printing for you. So in the case of the example code, if you  compile the code for an intel mac, and a windows PC, or a Linux PC, you'll use the same instruction set, but it will result in different code as the code will talk to the OS in different ways.",hjcgi7w,t3_qmuxrz,1636066914.0,False
qmuxrz,"Short answer, yes, but a lot of languages are not compiled to machine code but are instead interpreted, either directly or via an intermediate language.

A given processor (or family of processors) will have a language that is unique to it. All compilers for that processor will generate the same language.",hjdiwde,t3_qmuxrz,1636084544.0,False
qmuxrz,"No. For example, Go, Rust, and C all have different calling conventions in the assembly they compile down to.

Compilers can also be non-deterministic, and with options for different optimizations and mitigations, the same lines of C could result in wildly different assembly.",hjf5dd7,t3_qmuxrz,1636123419.0,False
qmuxrz,"You asked about different languages, but let's do it for the same language!

[GCC](https://godbolt.org/z/GeqGPvdr1) and [Clang](https://godbolt.org/z/9TT4Tr1zd) - same code, same language, for the same platform, Clang is even somewhat compatible with the GCC, and yet there already differences.  
Turn optimizations on - [GCC](https://godbolt.org/z/f38vYvKWW) and [Clang](https://godbolt.org/z/19Td9j6je) output at first glance different Assembly.

If such is the case for the same code using two compilers, then between languages the differences are even bigger.",hjfjz04,t3_qmuxrz,1636129126.0,False
qmil8h,"The physical memory is organized such that one value can occupy several adjacent addresses. Say your stack contains 8 bit integers (uint8), when looking for the next integer, the program will skip 8 bits and start reading again.",hj9ry2h,t3_qmil8h,1636027008.0,False
qmil8h,"This means that programs will usually have constants which will be used in operations. For instance, there may be a constant named c in your program with an assigned value of 1 (int c = 1). This constant c may be used in an operation to increment a variable i with an initial value of 0 assigned to it (int i = 0). This operation would be i = i+c. 

The variable i may be used to access items of an array. Suppose you have an array A. Accessing an item at index i of array A would be done by having the following in your program: A[i]. Incrementing i by c first would allow you to access the next item in A if you were to call A[i] again in your program. 

Hope this explanation made sense.",hj9s8aw,t3_qmil8h,1636027182.0,False
qmtz3q,"the copying is the installation. One does not happen before the other, one *encompasses* the other. (Some OSs instead download everything from the internet during installation, but downloading is just another form of copying)

For question 2: No. Yes.
The hard drive is not ""your whole computer"". It's the part that stores data. But yes, if you take it out and into another PC, your *system* will most likely boot there (assuming the new PC does not have special hardware requiring drivers you did not need to install for the old one) and be the same system.",hjdecfn,t3_qmtz3q,1636082141.0,False
qm0yhw,They are probably encrypted and not hashed.,hj6qkji,t3_qm0yhw,1635967071.0,False
qm0yhw,"Websites store passwords hashed, but I don't think they generally hash email-addresses. If they did, they wouldn't be able send password-reset emails, marketing emails, and so forth.

EDIT: BTW, this probably isn't the right sub, maybe r/webdev ?",hj6lmrm,t3_qm0yhw,1635965181.0,False
qm0yhw,"Marketing emails no, password reset yes. Why? Because the user has to provide the email to do a password reset do you know what the unhashed value is.",hj7swci,t1_hj6lmrm,1635982466.0,False
qm0yhw,"I remember reading [somewhere](https://termly.io/faq/are-email-addresses-personal-data/) that emails are personal data, and so I though they'd also be hashed. I'll post in webdev to double check. Thanks for the response!",hj6nc8g,t1_hj6lmrm,1635965830.0,True
qm0yhw,"Considering there are ~~no legal ramifications~~ (view reply) for companies to do so, I doubt many actually encrypt everything that would be considered PII, but I guess we don't actually know.

I can't imagine anyone hashes PII, as then it obfuscates the actual data which you need, which is something you can deal with in regards to passwords and such, but not PII.

You would encrypt all PII if you wanted to follow the best practices, though I doubt most companies do. So most companies probably just store emails normally unless they use some third party company to handle / host their data which may do that for them.

This would all fall under [non-sensitive PII](https://www.virtru.com/blog/6-steps-to-securing-pii-for-privacy-and-compliance/) which is why no one really cares. If it was sensitive PII it would be a different story",hj6pprq,t1_hj6nc8g,1635966743.0,False
qm0yhw,"It's fairly easy to enable disk encryption, the process for migrating an existing database to an encrypted drive wouldn't be too hard.",hj6rzxf,t1_hj6pprq,1635967625.0,False
qm0yhw,"webdev here, we don't hash email, hash is one way, we need to retrieve the email in someway for many purposes. Forget password is the easiest example.

password is hashed + salted, other than that, we probably just save it as is. probably some shops encrypt their data too for privacy but it shouldn't be hashed.",hj6sstw,t1_hj6nc8g,1635967935.0,False
qm0yhw,"*disclaimer: I don't actually own a website and I have never coded a website which had to store any user data. I may have made some mistakes in the following post, due to my inexperience. Please point them out if you notice any*

I think passwords should be hashed, such as with BCRYPT, user data should be encrypted, such as with blowfish, and emails should be stored plaintext (to recover passwords).",hj6o60s,t1_hj6nc8g,1635966147.0,False
qm0yhw,"You're talking cryptography more than anything webdev specific. There are one way functions ([hashing](https://en.wikipedia.org/wiki/Hash_function)) where you can't retrieve the original piece of data, and two way functions ([encryption](https://en.wikipedia.org/wiki/Encryption)) where if you have the appropriate key you can get it back. Quite often personal data like emails will be encrypted, to prevent it from being as easily stolen, since you need the key too. This is not seen as that useful for personal info in practice though, since if your database is compromised usually your web server software is too - and the web server has to have the key to do its job, so the attacker can obviously decrypt the info they want. It's more useful for data transmission than storage.",hj79896,t1_hj6nc8g,1635974258.0,False
qm0yhw,[deleted],hj7hv0v,t1_hj79896,1635977711.0,False
qm0yhw,"If your web server is compromised, then your data throughput is compromised. If your data throughput is compromised then you can *always* get access to the decrypted data by using whatever mechanism the web server uses to get the decrypted data, which is must have access to by definition since it *uses* the decrypted data. There is literally no way around this, owned web server = owned data, period. That's why you protect the web server like it's gold, and never store encrypted passwords.",hj7is34,t1_hj7hv0v,1635978092.0,False
qm0yhw,"Yes, emails are personal data. So is your address, phone number, and credit card number.

But if Amazon wants to post you your order, call you about a problem with your account, or charge a payment to your card, they need that information in cleartext.",hj6oida,t1_hj6nc8g,1635966281.0,False
qm0yhw,Not hashed but encrypted,hj8b820,t3_qm0yhw,1635990549.0,False
qm0yhw,"Because they are not? I can certainly see why sometimes one might want to encrypt them, but I can't imagine why you'd want to hash them aside from very niche cases.",hja9m4g,t3_qm0yhw,1636035725.0,False
qm0yhw,"Well, comparing the hashes of two values indicates a certain probability that these value are equal. But it’s not an absolute proof. Keep in mind that f(x) = 1 is a perfectly fine hash function, albeit not a very useful one.",hj6ujbl,t3_qm0yhw,1635968608.0,False
qm0yhw,"Usually you do not hash emails, but if you do, you should have a master key for hashing, just decode them with the encrypted hash master key, is the same for enceypted data with othe methods. If you’ve lost the master key, good luck asking the users for their emails and matching them manually on your db.",hj89gcq,t3_qm0yhw,1635989757.0,False
qm0yhw,"Hash all possible email addresses and compare the hash values. Eventually, you will get all the addresses if you give it sufficient amount of time...

Some potential optimization would be to wrap it as a proof of work for some crypto currency and let others help you.",hj8j5uh,t3_qm0yhw,1635994297.0,False
qm0bpt,"why analog? i do a fair amount of AI and ML, but would analog have much benefit? it seems unnecessarily niche",hj6zzq8,t3_qm0bpt,1635970714.0,False
qm0bpt,"Purely theoretically, analog computing offers much better efficiency, as a signal can represent any value within its bandwidth, instead of just 0 and 1. You can also perform mathematical operations directly in the electrical circuit using operational amplifiers, instead of relying on clocks and algorithms. 

New Mind has a good [Youtube video](https://www.youtube.com/watch?v=owe9cPEdm7k) on the topic.",hj77z60,t1_hj6zzq8,1635973761.0,True
qm0bpt,Sounds like a bad idea,hj6zd7k,t3_qm0bpt,1635970476.0,False
qm0bpt,Elaborate,hj6zlgj,t1_hj6zd7k,1635970563.0,True
qlvaqy,"Stuff with visuals can lead you down many roads. 
I wondered the same thing myself as a student.


Start with https://p5js.org/ . Go to the showcase or examples and you will see that in 3 lines you are able to make a shape move around the screen with mouse and keyboard.

In a couple more lines you can animate an image in tons of cool ways.

There's a channel called coding train on YouTube which will show you a beautiful world of creative coding with this language, from image to sound to interpreting nature via code and creative approaches to common CS problems. 

Now, if you want to get into games, you can of course do tens of unity tutorials, but i find that GameMaker Studio's drag and drop or coding tutorials (they're both just as easy) will within a span of 2 hours allow you to create a game and understand what is going on.

All games are, commonly, are loops that take input. You add conditions to that loop like character interactions, movement, dialog/event triggers. The game ""engine"" then processes it in standardized ways and usually displays some graphics in a way it decides. It really is just a simple program with a lot going on. If you start wondering about how the engine handles physics, or how it displays to the screen, you are asking questions about the engine itself and not how to make games. I'd say start with making games first and then you can journey onto the engine if you want

I'd start at these two places to get the instant satisfaction of making something along with understanding it!",hj6jq12,t3_qlvaqy,1635964449.0,False
qlvaqy,"Look into becoming familiar with an engine. I'm no expert and I'm sure someone here can give better advice but many engines support integrating your own c++ etc. Unity, and unreal are both great and powerful tools in the hands of anyone including a beginner.

I know it probably feels like you would forget many of the languages you've learned if you take on more and more information in CS but 80 percent or so of the information stays with you in your long term memory. You just have to reflect on them once and a while. Even if only passing  thoughts. It's reactivation the neurons responsible for storing that memory which will keep it fresh as when you first learned it.",hj5yt5l,t3_qlvaqy,1635956557.0,False
qlvaqy,"That depends on what exactly you want to make. If you want to make GUI desktop programs you'll have to learn a GUI library. There are a few that are cross platform that you could use like Qt, WxWidgets, and GTKmm but for the most natural UI you would want to use your OS's native GUI system. None of these options are easy. At the very least you'll need to know how to build and link against an external library and how to write code to its interface. 

Most GUI toolkits use event driven code(registering callbacks for events, etc.), state machines, and extensive multi-threading. It's a lot to learn at once. If all you've done so far is simple `cin` and `cout` command line stuff, all of that will have a steep learning curve in comparison but that is how real world GUI applications like MS Word, etc. are made. This is why a lot of companies cut corners and use electron and similar web frontend based tools despite their steep performance and memory usage penalties. 

If you want to make games you'll have to learn to use a game engine like Unreal, Unity, or Godot. The alternative would be hand writing everything yourself like the renderer (2D or 3D), audio engine, physics engine, input support, etc. which would be far too much pointless work. That said learning a game engine will still take time and effort but it's a much more reasonable goal than anything else I've mentioned.",hj613gj,t3_qlvaqy,1635957411.0,False
qlvaqy,"Forget games. Developers are plentiful. 

Get into VR! It's the future, and a couple of years of personal projects will make you a prime hire when you graduate.

If your school has a CS department, they'll likely have a VR lab. If not, launch one.",hj6b4rr,t3_qlvaqy,1635961187.0,False
qlvaqy,"I would recommend looking into some libraries like SFML or SDL. They allow for some pretty basic rendering tools that are fun to play around with.

If building things up from the basics isn't what you're interested in then game engines like unity offer tons of high level utilities for easily developing games.",hj6mz0e,t3_qlvaqy,1635965691.0,False
qlvaqy,Just download Android Studio; there are plenty of examples and tutorials built in! Plus there's a GUI for designing UI elements that makes things much more intuitive (imo).,hj75vs8,t3_qlvaqy,1635972937.0,False
qlvaqy,"Most people use a GUI framework, for cpp something like qt.

For games depending on the type of game most people go with a game engine, unless you want to tinker with a low level gl(graphics library) yourself.

I've heard of people using gl libs for softwares too, for instance I think the GUI for Blender was written in opengl. I suppose since it's a 3d modelling software the people working on it are familiar with it anyway but I don't think many people would.",hjaaxyg,t3_qlvaqy,1636036276.0,False
qlzztd,"The bit used in rdt 2.1 is only in response of the ability to handle corrupted ACK/NAK’s (not the actual data sent). If an ACK/NAK is corrupt, the sender needs to resend the package to the receiver to get another ACK/NAK that isn’t corrupt. If the receiver has already received the previous package, when it is resent by the sender it’ll be a duplicate package, so we need a way for the receiver to know whether the package sent most recently is the previously received package or a new one. The single bit (sequence number) you refer is used by the receiver to check whether the received package is a duplicate or a new package. 

This bit is updated in the receiver end after sending an ACK, so for example say it’s set currently at 0, it receives a package with bit 0 from sender, sends an ACK back, now bit is update to 1. Let’s say that ACK was corrupted so the sender resends package with bit 0. Since the current sequence number in the receiver end is 1 and 0 != 1 the receiver knows this isn’t the expected package so it must be the previous package and it resends the ACK for bit 0 (when you resend an ACK from a previous package you don’t update the sequence number)

The reason we can use just 0 or 1 for a sequence number is because this is a stop and wait protocol and rdt 2.1 assumes no packages will be lost (rdt 3 handles that after)",hj6pd09,t3_qlzztd,1635966606.0,False
qlzztd,"> this isn’t the expected package so it must be 

Thanks for the explanation, but my question is still not answered. I kind of understand the F.S.M. shown...but i can't grasp whether it's bulleproof or not...( and I won't test all possible scenarios, I'm not a masochist. I ran a few and it seemed okay, but something bothers me with it. )  


Is that bit sufficient for error control? Is it bulletproof? ( considering there are no lost packages )",hj6srb8,t1_hj6pd09,1635967918.0,True
qlzztd,"I’m not sure what you mean by error control, but if you mean corrupted packages, then that is addressed in rdt2.0 by the use of checksums and ACK/NAKs. Rdt 2.1 builds on this by adding the extra bit (this is only meant for handling of corrupted ACK/NAKs as everything done prior to this in rdt 2.0 already handles other errors - aside from package loss)

Simply put though, if we assume there isn’t package loss this is enough for reliable data transfer. I don’t have a proof for this but you may convince yourself by walking through all possible scenarios and seeing how it will work in those scenarios.",hj6tvvl,t1_hj6srb8,1635968356.0,False
qlzztd,"Yes by error control i mean I mean corrupted packages.  
We built on 2.0 in order to prevent situations like receiving a corrupted NAK and sending a package twice, therefore destroying the entire file to be transfered.   
No package loss is not enough to consider data transfer realiable. You also need to look out for packet corruption to consider a data transfer reliable. For example rdt2.0 is not reliable for the reason i mentioned above.",hj740pr,t1_hj6tvvl,1635972225.0,True
qlzztd,"I think you misinterpreted what I meant. When I said ""if we assume there isn’t package loss this is enough for reliable data transfer."" I meant ""if we assume there isn’t package loss *RDT 2.1* is enough for reliable data transfer"". By *this* I meant *RDT 2.1*. Sorry I should have been more clear.

I do want to clarify though, that the bit alone isn't what handles corrupted packages, it's the combination of the bit, the checksums, and the ACK/NAKs (all of which are included in RDT 2.1). RDT 2.0's only fault (assuming no package loss) is that it doesn't handle corrupted ACK/NAKs, RDT 2.1 fixes this.",hj76udu,t1_hj740pr,1635973311.0,False
qlj8jr,There's a ruby library called Futurama; maybe that's it? I'm afraid that's all I've got.,hj3m6tj,t3_qlj8jr,1635907731.0,False
ql9rw5,Well written text which introduces math as necessary. I'd say just dive in and only catch up on something if necessary.,hj1xhw2,t3_ql9rw5,1635883156.0,False
ql9rw5,"This book is so extensive and covers algorithms in a practical and theoritical approaches, you will only need the math already included and well explained inside the book",hj2csne,t3_ql9rw5,1635888992.0,False
ql9rw5,"I really don't think there's a prerequisite needed for this book other than discrete math.

For that I would recommend Concrete Mathematics by Donald Knuth.

As for the book, I think Algorithms Design Manual by Skiena is far more readable than CLRS which is IMO more of an encyclopaedia than learning resource.",hj3xx5e,t3_ql9rw5,1635913339.0,False
ql9rw5,"Love Skiena's book too. Got the 2nd edition recently which was good enough for me and cheaper than the 3rd. Good hardback binding, typical Springer quality. As fir the content, it has implementations in C and I liked also the war stories sections that show practical applications for each algo.

I'd like to buy CLRS in the future as indeed it's considered the algos & ds Bible, and I think it's very good as a reference, if you want to dive in the details. I wouldn't study it from cover to cover.",hj41bqq,t1_hj3xx5e,1635915289.0,False
ql9rw5,I remember this book....,hj30517,t3_ql9rw5,1635898489.0,False
ql9rw5,"I think you need to practice to appreciate the amazing algorithm! Like why we need a heap? after a good program which dramatically reduce O(n) , you will agree who thought about it must be a genius! same for dsu or quick sort.",hj3hobd,t3_ql9rw5,1635905779.0,False
ql9rw5,Learn by doing. Don't understand an algorithm yet? Code it. Don't use a library that implements it; sit down and code it.,hj2hccb,t3_ql9rw5,1635890754.0,False
ql9rw5,"This CLRS book is indeed amazing but don't be discouraged if you prefer something else.  I use it as a reference.

As for a textbook I used ""Algorithms in C"", 3rd ed. by Sedgwick.  The author is one of Knuth's regular collaborators.  The 4th edition is significantly simplified, and I think it's ideal for self-starters or beginners in algorithms and data structures.

Skiena is amazing, but it's *about* algorithms, not really a standard algorithm textbook.  Likewise with Concrete Mathematics.  I have Knuth Vol. 1 and I flip through it occasionally, but again, it's more for inspiration.  I would rather read Concrete Mathematics for mathematical fundamentals and use CLRS or Sedgwick for the algorithms.  Skiena is also an essential book for me.

If I had to choose the smallest number of books, I would choose Skiena and Sedgwick 3rd ed. (the latter comes as two books) or Sedgwick 4th ed. (one book) if you're really new to this.

Everything you'll ever need to know is in the following:

* Sedgwick 3rd ed. (two books)
* Sedgwick 4th ed.
* CLRS
* Skiena latest ed.

There are many other great textbooks but you can't go wrong with any of the above.",hj50u83,t3_ql9rw5,1635942124.0,False
ql9rw5,Are you reading it cover to cover? I generally skim each chapter for uses and running times that way if I come across anything that might require that algorithm I know to open CLRS for a deeper dive. But I've never on depth read this book cover to cover.,hj2iare,t3_ql9rw5,1635891129.0,False
ql9rw5,"It's an amazing book, I live by it. Haven't cracked it in years, but back when I was studying CS, it helped me so much.

At some of my friends' workplaces, they call it the ""White Bible"", based on its color (Numerical Recipes is likewise the ""Red Bible"").

It's dense so don't try to get it all at once. I would *personally* recommend implementing every algorithm and data structure in there you really care to understand yourself. If you just need a taste, skim and don't feel bad about it.

The most important thing you can do with any algorithm in CS is to know that it exists. Anything beyond that you can work towards when the need arises.",hj2tnmf,t3_ql9rw5,1635895788.0,False
ql9rw5,"Best book ever, that's my feedback",hj1mrfn,t3_ql9rw5,1635879006.0,False
ql9rw5,"Maybe suppliment with MIT opencourseware 's algos. 
https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/

It's quite good and the content on eg. Max flow is very detailed and helpful",hj2oveo,t3_ql9rw5,1635893813.0,False
ql9rw5,"My advice is don't try to ""get through it faster"". Try to engage with it every day, even if you are only reading a couple pages per day. When learning anything, you want to think critically about the material and actually mull it over. After you've read a given passage, ask yourself what the main point of it was, try to do a kind of instant replay in your mind of the information you just consumed. Do the exercises after each chapter. It's a marathon, not a sprint.",hj5rx7m,t3_ql9rw5,1635953965.0,False
ql9rw5,"I'm pretty new to CS, and I've always heard that books are quite useless when it comes to learning computer science or programming in general?",hj6ga9d,t3_ql9rw5,1635963138.0,False
ql9rw5,"That's not true, for me at least... I think is very important to learn the theoretical part in order to master the practice, is always import to have references, they will give you a solid base for problem solving, and to make you understand why certain approachs for certain problems work.",hj6lv7i,t1_hj6ga9d,1635965271.0,True
ql9rw5,"No I definitely agree. Because otherwise there's always a huge disconnect in ones understanding 

However, since I've never heard anyone else but myself say that, I don't know if it's true or not.",hj94hig,t1_hj6lv7i,1636007890.0,False
qkkwwt,"If i is between n and n/2, the inner loop only runs once. If i is between n/2 and n/4, the inner loop runs twice etc. This means that the total amount of loops is (n/2) \* 1 + (n/4) \* 2 + (n/8) \* 3 + ... + (n/2\^log(n)) \* log(n) times. If I got it correct the total amount of loops will be about n \* 2, meaning that the complexity is indeed O(n).",hix5iz3,t3_qkkwwt,1635795285.0,False
qkkwwt,Wow this makes complete sense. Thank you.,hix6j8e,t1_hix5iz3,1635795692.0,True
qkkwwt,The real answer is…. This code is O(infinity) since your code will loop forever since 2 * 0 =0,hiy1ozw,t3_qkkwwt,1635808725.0,False
qkkwwt,"The best way of thinking about this is to work backwards.  With a range n, with have n/2 elements that are are >= n/2 so will iterate once, we have n/4 elements that are >= n/4 <= n/2. so will iterate twice.  We have n/8 elements that are >= n/8 <= n/4 and so will iterate 3 times etc.  So we get the sequence n/2 \* 2\* n/4 + 3\*n/8 + 4\*n/16 etc.  = n\* sum (i/2\^i). This turns out to be precisely 2, since its 1/2 + 1/4 + 1/8 ....  = 1 ,  + 1/4 + 1/8 + 1/16 ... = 1/2 + ...  = 1 + 1/2 + 1/4 + 1/8 ...  = 2",hiyqxmm,t3_qkkwwt,1635820126.0,False
qkkwwt,"The other answers are correct that it is O(n) I wanted to explicitly outline why your log(i) approach was wrong because it seems you, me, and several others were confused by this at first.

Tl;dr:
We aren't doing work equal to Σlog(i) but rather Σlog(n/i). This causes O(n) instead of O(n*log(n)). I provide a proof below showing it is O(n) without Stirling's formula.

Proof:
Σlog(i) for each i is wrong because we aren't doing log(i) units of work for each i. That would be how many times we can double 1 without exceeding i; However, in the function the units of work are based on how many times can we double i without exceeding n. These equations model this value (where x is the number of times we can double i to equal n).

`i*2^x=n ⟹ 2^x=n/i ⟹ x=log(n/i)`

To model the entire function we sum each iteration:

`Σlog(n/i)`

`= log(n/1) + log(n/2) + ... + log(n/n)`

`= n*log(n) - (log(1) + log(2) + ... + log(n))`

`= n*log(n)-log(n!)`

The proof that n\*log(n)-log(n!)∈O(n) can be done by induction (after spending too much time on it and hating yourself).

`Let g(n) = n*log(n)-log(n!)`

`Let h(n) = 2*n`

Claim to be proven:

`P(n): g(n) < h(n) is true for all integers n >= 2`

Base Case:

`P(2): 2*log(2)-log(2!) < 2*2 ⟹ 1 < 4 ⟹ P(2)=True`

Induction Step:

Let's define a function that represents the increase of g(n) to g(n+1). That is, we will define function gΔ(n) be the difference between g(n+1) and g(n).

`Let g(n) = n*log(n)-(log(n!))`

`Let g(n+1) = (n+1)log(n+1)-log((n+1)*n!)`

I did this bit by hand to be sure, but [wolfram alpha can confirm](https://www.wolframalpha.com/input/?i=%28n%2B1%29log_2%28n%2B1%29-log_2%28%28n%2B1%29*n%21%29-%28n*log_2%28n%29-%28log_2%28n%21%29%29%29) the following (assuming n is positive, which it must be as we only increase from the base case of 2):

`gΔ(n) = g(n+1) - g(n) = n*log(1+1/n)`

Doing the same for h(n) is trivial (it always increases by two):

`hΔ(n) = h(n+1) - h(n) = 2(n + 1) - 2n = 2`

The limit of gΔ(n) [turns out to be 1/ln(2)](https://www.wolframalpha.com/input/?i=limit+of+n*log_2%281%2B1%2Fn%29+as+n+approaches+infinity) as n approaches infinity, which implies gΔ(n) never exceeds 1/ln(2). Note 1/ln(2) < 2. Now we know if P(n) is true, P(n+1) must also be true because h(n) to h(n+1) is an increase by 2, while g(n) to g(n+1) cannot increase by more than 1/ln(2). It follows:
`P(n)=True ⟹ g(n) < h(n) ⟹ g(n) + n*log(1+1/n) < h(n) + 2 ⟹ g(n+1) < h(n+1) ⟹ P(n+1)`

P(n) ⟹ P(n+1) for all integers n>=2, which proves:

`g(n) = n*log(n)-(log(n!)) ∈ O(n)`

There are other ways to prove this (e.g. Stirling's formula) but I felt this is the most intuitive, albeit long winded, approach.",hj8h3aq,t3_qkkwwt,1635993288.0,False
qkkwwt,Wish Reddit will support LaTeX in some point...,hk95rz0,t1_hj8h3aq,1636666871.0,False
qkkwwt,"  
def  
f(n):  
    c = 0 -------------------- 1  
    for i in range(n):---- n +1  
        j = i ------------------ n  
        while j < n: -------- (O(logn) +O(log(n-1) + O(log(n-2)+…+1) + 1  
              j = 2\*j ---------- (O(logn) + O(log(n-1) + O(log(n-2)+…+1)  
              c = c + 1 ------- (O(logn) + O(log(n-1) + O(log(n-2)+…+1)   
   
T(n) = 3 + 2n  + ((O(logn)\*(O(logn)+1))/2) <= 3 + 2n + (O(logn))\^2   
Since there is c0 > 0 and n0 >= 0 such that:  
(T(n) <= 3 + 2n + (logn)\^2 <= n\*c0) for every n >= n0, 

T(n) belongs in O(n)  


In simpler words: T(n) is 3 + 2n + something kinda (logn)\^2,   
but we only care about the behavior of the time function and since (logn)\^2 < n  after some n we just ignore it and express T(n) as O(n)",hj6r3ba,t3_qkkwwt,1635967272.0,False
qkkwwt,"My explanation has a few flaws, but I won't edit it as reddit's edit is acting weird and messes up the whole thing...and I am too lazy to try and fix it :P   


I hope you get the general idea. If you need any further explanation ask me.",hj6s5xo,t1_hj6r3ba,1635967690.0,False
qkkwwt,"Because the main factor in its performance is how many of ""n"" you are iterating over.",hix3c8x,t3_qkkwwt,1635794399.0,False
qkkwwt,"I understand that n grows faster than log(n), but the complexity of the inner loop (logn?) is multiplied but the outer loop (n). As I understand it, if the loops were not embedded then you would have O(n) + log(n) which would be the case were the main factor (n) is what you use.",hix4kkl,t1_hix3c8x,1635794897.0,True
qkkwwt,"Heh, was this a trick question? I got home and took a closer look, and because your range starts at 0 you're going to get an infinite loop here. You can multiply 0 by 2 all day and it will still be less than n. Tell your professor that trick questions are lame.",hixpup5,t1_hix4kkl,1635803570.0,False
qkkwwt,"Haha your right. I’ll bring it up to him, I bet he intended to use range(1,n) on the quiz question.",hixszq6,t1_hixpup5,1635804911.0,True
qkkwwt,Thank you for posting it. I deserved those downvotes for leaping before looking. To make up for it I'll post a neat Kotlin example profiling that loop using 1-based ranges in a day or two. Might be insightful. Best of luck.,hixtutz,t1_hixszq6,1635805289.0,False
qkkwwt,"Alright, so I decided to run an experiment in order to defend my intuition that this would be O(n). Please correct me if I'm wrong, but the numbers seem to back up my case. Computational Complexity is not an exact science and a certain amount of ""common sense"" is involved in deciding which trends deserve to be considered in the final result. I was doubting my intuition here after receiving those well-deserved downvotes for being so hasty, but my tests seem to suggest I was right. I'm glad I took the time to take a closer look. 

The following example code is written in Kotlin and uses some experimental features which were suggested by my IDE, but this stuff translates to Python and any other language you are using:

    import kotlin.time.Duration
    import kotlin.time.ExperimentalTime
    import kotlin.time.measureTime

    @OptIn(ExperimentalTime::class)
    fun main() {
        // For testing this kind of thing, a custom Result class is handy. We're going to use a list of them:
        val results = mutableListOf<Result>()

        // We're going to do 9 iterations, which should cover up to around a billion:
        val numTests = 9

        // We're going to start with 10 and increase by a factor of 10 every test:
        var nLimit = 10
        repeat (numTests) {
            results.add(
                f(1..nLimit)
            )
            nLimit *= 10
        }

        // We'll cap it off by checking each result:
        results.forEach {
            println(it)
        }

        /*
            Conclusion: Although the amount of time used is vanishingly small and will vary from test to test, the
                trend is clear when you view the number of iterations in the inner loop. As n increases by a factor of
                ten, the number of inner iterations also increases by about a factor of about 10.
                This is experimentally true, which makes it about 0(n) in my amateur opinion.

            Result(n=10, time=4.927005ms, innerIterations=16)
            Result(n=100, time=56.142us, innerIterations=194)
            Result(n=1000, time=488.659us, innerIterations=1990)
            Result(n=10000, time=3.690477ms, innerIterations=19990)
            Result(n=100000, time=5.846741ms, innerIterations=199988)
            Result(n=1000000, time=10.475195ms, innerIterations=1999986)
            Result(n=10000000, time=24.845879ms, innerIterations=19999984)
            Result(n=100000000, time=237.521900ms, innerIterations=199999979)
            Result(n=1000000000, time=2.323592679s, innerIterations=1999999977)
         */
    }

    @OptIn(ExperimentalTime::class)
    data class Result(
        val n: Int,
        val time: Duration,
        val innerIterations: Int,
    )

    @OptIn(ExperimentalTime::class)
    fun f(n: IntRange): Result {
        if (n.first == 0)
            error(""0-based ranges will result in an infinite loop here."")

        var innerIterations = 0

        val time = measureTime {
            for (i in n) {
                var j = i
                while (j < n.last) {
                    j *= 2
                    innerIterations++
                }
            }
        }

        return Result(n.last, time, innerIterations)
    }",hj14vpq,t1_hix4kkl,1635871980.0,False
qkkwwt,"Hey man, thanks for doing the nitty gritty analysis, to see that it is in fact O(n)! Idk why you got downvoted but what I meant by my comment was that “the n is is the dominant factor” didn’t make intuitive sense to me, not that you were wrong about it being O(n). But I understand it now, the inner loop starts at O(logn) for first few iterations but quickly becomes essentially constant time, so the outer loop does in fact win out.",hj1i97b,t1_hj14vpq,1635877275.0,True
qkkwwt,"I would not have known for sure until I tested, so my intuition was suspect from the start and I probably deserved the down votes for jumping the gun. I certainly don't mind them in this case.

Thanks again for posting it! Was a fun excuse to play with Kotlin on my new setup.

Edit: More: I also probably should have explained that the increase in complexity was constant in proportion to n, no matter how big n is. The language I used in the down voted post isn't quite clear enough to pass muster in, say, an interview or class assignment. This is why down votes exist, though. Sometimes they help you clear up your style.",hj1nr0n,t1_hj1i97b,1635879384.0,False
qkkwwt,"The answer will be n(logn) I think. 
The for loop will run n times. 

For the while loop:

let's say n=64
now j=i means j will be 1 initially. 
For each iteration of while j is multiplied by 2 which means:
1 2 4 8 16...64  == 2^0 2^1 2^2....2^k.

2^k= n

k= log(n) 
 For each iteration of for loop while loop is executing. 
So total complexity is n*log(n)",hixftxk,t3_qkkwwt,1635799417.0,False
qkkwwt,The higher order operation is for I in range n which runs in O(n). Since j runs less than n we can omit it from our thinking. That’s the easiest way to see it. At a minimum u must iterate through all n once,hj0two2,t3_qkkwwt,1635867535.0,False
qkkwwt,"I’m not sure if I understand that reasoning exactly since a loop that runs n times with log(n) in the inner loop is O(nlog(n)), not n, even though the inner loop runs a factor slower.",hj0uezp,t1_hj0two2,1635867746.0,True
qkkwwt,U can think of it as running in log n but then u have to consider it goes log(n)+ log(n/2)+log(n/4)… in which case u end up with total of n anyways. It’s simpler to just consider the highest order being o(n) because we still gotta iterate through n times. So focusing on highest fact that we need minimum of n iterations will leave u with O(n). Technically it’s be correct to add up all the logs but it’s the same result as just considering the main for loop. Any for loop from i to n will leave u with O(n),hj0xdci,t1_hj0uezp,1635868958.0,False
qkkwwt,Not in the classical definition of O(n). O(n) is an upper bound. Just because something loops over n does not make it O(n). It makes it Ω(n).,hj7pd33,t1_hj0xdci,1635980918.0,False
qkkwwt,Yeah but most people don’t seem concerned with bounds other than top bound as it’s the limiting factor,hja401a,t1_hj7pd33,1636033257.0,False
qkkwwt,"My point is just looking at the first for loop isn't enough to deduce the upper bound.

You say ""Any for loop from i to n will leave u with O(n)"". 

Counterexample:

`for i in range(0,n):
    mergesort(n_elements)
`

This loop is not O(n) but it loops over n in the highest loop. It's O(n^2 log(n)).",hjc13za,t1_hja401a,1636060579.0,False
qkc8u6,"This was a really good watch, Thanks !",hivs8cv,t3_qkc8u6,1635774029.0,False
qkc8u6,Thank you for your support !,hiyehqu,t1_hivs8cv,1635814428.0,True
qkc8u6,"Great job!

Would you be interested in walking through the part where you went from CPU and data sheet to figuring out how to hook up memory and clock and so on?",hiw9wuq,t3_qkc8u6,1635782134.0,False
qkc8u6,"Thank you!

It's good to know that you would be interested in such video. I didn't really know how far I should in the explanation of the making of, mainly because there are already plenty of interesting  and well made videos about the subject on Youtube. I will consider it seriously, thanks for your feedback!",hj558ui,t1_hiw9wuq,1635944429.0,True
qkc8u6,👏👏,hivyg8b,t3_qkc8u6,1635777009.0,False
qkc8u6,Thanks !,hiyejf7,t1_hivyg8b,1635814448.0,True
qkc8u6,Nice!,hivq6is,t3_qkc8u6,1635772967.0,False
qkc8u6,Thank you!,hivx4qb,t1_hivq6is,1635776410.0,True
qkc8u6,"The first assembly language I learned was Z80 on a TRS-80.  
Compared to today's CPUs, it was a joy to program.",hiybwud,t3_qkc8u6,1635813265.0,False
qkc8u6,"Exactly, this was actually what motivated me starting this project. Today, even if you get an MCU board such ESP32 or Arduino, you get to develop in C or C++. Even if one chooses assembly, he'd have to learn Xtensa or AVR assembly which are far more complex than Z80 assembly.

So the goal of this project is to be able to learn Z80 assembly and do nice things with it, including graphics!😄",hiyfft4,t1_hiybwud,1635814856.0,True
qjz6f7,"I can't think of anything that fits those criteria exactly, but here are some good channels that come to mind (in order of increasing specialization):

Tom Scott has a [really good series called ""The Basics""](https://www.youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha) which teaches general concepts to keep in mind.

I hope you've heard of [Kurzgesagt](https://www.youtube.com/user/Kurzgesagt), which has amazing science videos. (I'd check them out, even though they're not strictly math or computer science)

[Junferno](https://www.youtube.com/channel/UCRb6Mw3fJ6OFzp-cB9X29aA) covers random computer science related subjects with a healthy dose of deadpan humor. You might enjoy [this video](https://youtu.be/MBQvN03i4-4).

[Fireship](https://www.youtube.com/c/Fireship) has a great series called 100 seconds of code, where he goes through basic concepts quickly. Take a look through [this playlist](https://www.youtube.com/playlist?list=PL0vfts4VzfNiI1BsIK5u7LpPaIDKMJIDN) to see if any of the topics are interesting to you.

[AlphaPhoenix](https://www.youtube.com/channel/UCCWeRTgd79JL0ilH0ZywSJA) is a great channel about science, and computer science. [I liked this video](https://youtu.be/TOpBcfbAgPg).

[HoneyPot](https://www.youtube.com/channel/UCsUalyRg43M8D60mtHe6YcA) produces pretty good documentaries for developers.

[Two Minute Papers](https://www.youtube.com/c/KárolyZsolnai) covers real world math papers in a fun and entertaining way. A lot of his videos relate to AI.

[Sebastian Lague](https://www.youtube.com/c/SebastianLague) has some amazing videos about game development, which often involve more math than you think. I particularly recommend his [coding adventure series](https://www.youtube.com/playlist?list=PLFt_AvWsXl0ehjAfLFsp1PGaatzAwo0uK).

[freecodecamp.org](https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ) has a pretty good youtube channel that does a lot of deep dives into particular subjects.

[Ben Eater](https://www.youtube.com/c/BenEater) has really good videos about low-level computing. If you are interested in learning about how computers *really* work, this is a good channel for you.

[LiveOverflow](https://www.youtube.com/c/LiveOverflow) has some amazing educational videos about cybersecurity, for more intermediate audiences. He's not directly related to Math, but his content is so good I can't not mention it for people interested in cybersecurity.

I hope that helps!

*Edit*: [mCoding](https://www.youtube.com/channel/UCaiL2GDNpLYH6Wokkk1VNcg) is also great if you're interested in Python or C++.

[jdh](https://www.youtube.com/c/jdhvideo/) does a lot with low-level programming, and has interesting videos such as [making his own operating system](https://youtu.be/FaILnmUYS_U) and [writing minecraft in C](https://youtu.be/4O0_-1NaWnY).",hitjm93,t3_qjz6f7,1635720708.0,False
qjz6f7,this is a top tier list 👌,hivomm8,t1_hitjm93,1635772129.0,False
qjz6f7,thanks! I hope it justifies all the time I spent watching youtube :),hixe8x0,t1_hivomm8,1635798780.0,False
qjz6f7,You missed Harvard CS50,hivz5dj,t1_hitjm93,1635777326.0,False
qjz6f7,Do you know any that go other logic gates and truth tables? I recent failed a test because I couldn’t make a truth table that fit a certain criteria. Also how the CPU and memory work? Like how does the computer save stuff in memory or how it moves thing to different memory cells?,hiv17hg,t1_hitjm93,1635753754.0,False
qjz6f7,[deleted],hivc3nr,t1_hiv17hg,1635763833.0,False
qjz6f7,Thanks dude I appreciate it!,hix7p3q,t1_hivc3nr,1635796163.0,False
qjz6f7,"I think the closest thing to what you're looking for is probably Computerphile. They're similar to Veratisium, but for computer science.",hitskus,t3_qjz6f7,1635725274.0,False
qjz6f7,Even more similar to Numperphile than Veritasium.,hiv4kzw,t1_hitskus,1635757078.0,False
qjz6f7,"People have already mentioned the popular ones you’re looking for but also [CS50](https://youtube.com/c/cs50) is a good starting point … after the CS-crash course series already recommended.

>![random related playlist](https://youtube.com/playlist?list=PLkh6icMoy1Q46-gs9D3Bo8zLKDya8UBFr)!<",hiusnun,t3_qjz6f7,1635745917.0,False
qjz6f7,"u/BimphyRedixler already has the best answer. Especially seconding Two Minute Papers, Ben Eater, and Sebastian Lague. Saving a couple that I hadn't heard of before. A couple more to add:

* [What's a Creel?](https://www.youtube.com/c/WhatsACreel) has some fun computing experiments. Some of the x86-64 assembly stuff might be of interest.
* [Coding Secrets](https://www.youtube.com/c/CodingSecrets) is mostly about the fascinating lost art of squeezing entire games into the tiny ROM's available to 90's game consoles. Occasionally some neat low-level tricks covered.",hiubtrl,t3_qjz6f7,1635735250.0,False
qjz6f7,"Thanks! I watch way too much YouTube :)

I haven't heard of What's a Creel before. Thanks for sharing!",hiukzvx,t1_hiubtrl,1635740382.0,False
qjz6f7,The vlog brothers “Crash Course” has a computer science course that is more fun history based then technical explanation,hitvvp8,t3_qjz6f7,1635726986.0,False
qjz6f7,"My top 3 picks:

\- Hussein Nasser for backend engineering topic  
\- freeCodeCamp.org for software engineering (they cover many things, from competitive programming to front end dev) topics  
\- LifeOverflow for cybersecurity topic",hiuab4h,t3_qjz6f7,1635734454.0,False
qjz6f7,"Imo the best one is [reducible](https://www.youtube.com/c/Reducible),  is literally the 3b1b of computer science, his videos are awesome and the topics are so interesting.

I also love [computerPhile](https://www.youtube.com/user/Computerphile), probably is the most famous of this type of channels.

And finally there is a guy who used to make really great animations of compSci videos, the problem is that it seems that he is no making videos anymore, but well, you should check it out [Spaning tree](https://www.youtube.com/c/SpanningTree/videos).

Hope to be useful, have fun!",hitz21t,t3_qjz6f7,1635728603.0,False
qjz6f7,"The coding train is a pretty good channel that does mostly p5 js but he's great at explaining what he's doing.

I watched him all throughout college and I think it helped me a bit",hiu9iq0,t3_qjz6f7,1635734041.0,False
qjz6f7,Creel,hiuzrjg,t3_qjz6f7,1635752348.0,False
qjz6f7,"You could try [Sentdex](https://youtube.com/c/sentdex), he covers mostly Python, but has some great full walkthrough for beginners. 

I used his Tkinter guide to help me finish my pythons course at college (his videos taught me more than that $3k class)",hiwwrvk,t3_qjz6f7,1635791697.0,False
qjz6f7,I enjoy watching Eric Demaine’s lectures from MIT opencourseware on YouTube! Check it out,hix1f5c,t3_qjz6f7,1635793608.0,False
qjz6f7,Ex google tech lead and millionaire,hiu7g92,t3_qjz6f7,1635732949.0,False
qjz6f7,"Seems like you aren't updated well enough, it is:

""Ex-google Ex-facebook multi-millionaire techlead""",hivbkb8,t1_hiu7g92,1635763406.0,False
qjz6f7,"The only thing i can really recommend you from my experience, don’t watch youtube to learn something, educational platforms with courses (paid or even free) or books will be way more useful.

If you really want to use youtube for what ever reason, there are probably talks by known people in the community you are wanting to learn something in, try to stick to them.

There are great books on coding paradigms, good practises and efficient ways to program, i d recommend you those


This advise may seem harsh, but as someone who mainly watched youtube to learn programming and  stuff  related to programming, i d say that even if it was fun, i wouldn’t do it again.
After switching from youtube to books and text courses / professional courses, i was amazed by the difference of quality",hitd4op,t3_qjz6f7,1635717557.0,False
qjz6f7,"Great comment! I started off programming by learning it from YouTube and now I never watch YouTube except the people talking about their programming experiences, etc.",hivbnvo,t1_hitd4op,1635763485.0,False
qjz6f7,same,hivca8n,t1_hivbnvo,1635763976.0,False
qjz6f7,[deleted],hithqv0,t1_hitd4op,1635719792.0,False
qjz6f7,depends on what you want to learn,hithurm,t1_hithqv0,1635719845.0,False
qjz6f7,"I recommend Pluralsight for online courses, it includes video, tons of code examples and slides too. I also like their way of doing the video transcripts, you click on the text and it jumps to that section of video.   They also include short tests at the end, then give you the links to that section of video and that you did not learn on the first pass.  
They have high quality lecturers, tons of fundamental concept classes and also cutting edge software.
Udemy is ok, but not as pro as Pluralsight.",hiwrxc1,t1_hithqv0,1635789703.0,False
qjz6f7,skip courses and just buy books to read and do problems from. You will retain so much more.,hiup7gd,t1_hithqv0,1635743283.0,False
qjz6f7,Not at all some courses are really great,hiw33cy,t1_hiup7gd,1635779115.0,False
qjz6f7,"Harvard’s CS50 is a great intro class but there is a reason college is not just lectures. 

Cal Berkeley’s CS61A, and CS61B courses can be found online which give you access to lectures, books, notes, homework, labs, and past exams. Spending the time to write notes from the lectures and doing the homework’s were what taught me the most. Anything is is just superficial. As soon as you experience a problem you won’t know how to apply critical thinking to it. This is why tutorials fail to truly teach you.",hiwmkuj,t1_hiw33cy,1635787510.0,False
qjz6f7,"I don’t know what you experienced, but i can tell you that there are great in depth courses which even supply you with practise exercises and direkt help systems on dir example udemy.com for like 15$",hiwv4zo,t1_hiwmkuj,1635791025.0,False
qjz6f7,"Let me ask a question so I don’t assume here. Do you have an engineering college degree?

Is so you would tend to agree with me that tutorials, udemy, and coursera are lacking in comparison to actual college classes. The aforementioned could be helpful for refreshers on specific tools like git or algos like binary trees but unless you spent time fundamentally understanding trees the udemy class on trees won’t leave you with lasting understanding. 

Furthermore computer science is much more than just programming. Learning about networks, distributed systems, computer architecture, and OS are a few of the fundamentals that will help you understand what happens when you use a language. Most udemy type classes don’t teach this but instead they teach you how to create a list in Python. What’s the point if you don’t understand that python is created on top of C which uses linked lists to on the fly create a Python list that you can add elements to. This is one reason why Python lists are slower than C lists.",hiwx2n2,t1_hiwv4zo,1635791821.0,False
qjz6f7,"No i did not visit an engineering college.

You can not compare a course with around 10-15h or content worth 15$ to an collage class which has hundreds of hours (if you include studying for exams and homework) and which you pay thousand of dollars for.
Most courses are not meant to be a “learn all about this theme that exists in detail” they are more like another resource you can use to learn about the topic you are studying and in my opinion they are way more efficient if you want to study something, by the facts that these are people who scripted the video content they deliver and they try not to waste time (which is very different from schools and colleges (this is what i experienced)).
I also enjoy the fact that you can replay, change the playback speed and choose when you are studying, this gives me the opportunity to manage my schedule better and gets me generally more motivated.

If we are talking about the quality of these courses, i can tell you that the instructors are skilled and oftentimes known people which put really much (i m talking about oftentimes 6-9 months) time into it and script it incredibly well to make the most efficient content they can, they mostly pay a lot of attention to what they say and you can see exactly what they are going to teach you, in which order and in which way. I have some friends who visited a cs university and reported me the way in which many teachers teach really old and unrecommended ways of c++, because they learned like this in their university but aren’t up to date anymore (of course this isn’t the case everywhere and there are also great teachers out there, but i ve heard about it a lot), this is something you will not experience in courses because you can easily see detailed recommendations, exactly what he teaches you and in which way as i ve already said.

Some courses are really great and teach you valuable things, it would be a shame to miss them about, i can recommend searching for good rated courses about topics you want to learn about and use them as a resources (even if it oftentimes do not go into great detail, they are an awesome way to get started)",hixdxmb,t1_hiwx2n2,1635798653.0,False
qjz6f7,"Could you recommend any book to learn programming ? It doesn't matter the language, i want the concepts",hitxszv,t1_hitd4op,1635727953.0,False
qjz6f7,"What are you interested in; Algorithms, Software Architecture? It's not like programming is a monolithic subject.

And what kind of search requests failed you? To be honest, it's quite easy to get started with programming in all kinds of ways now. The [Humble Book Bundle](https://www.humblebundle.com/books) alone hast 3 different bundles this month that get you started with low level related coding stuff.

^(Edit: fixed Markdown)",hiv8d9g,t1_hitxszv,1635760701.0,False
qjz6f7,"Take it from me, there are no ""Good"" channels. 

You can pick any one of them, start coding yourself, and improve. Oh and you're not gonna understand the 'deep understandings' of the language without grinding it for a long time on it yourself. 

I would strongly advise you to just start coding. Waiting to find that perfect channel is procrastination.",hiuivmx,t3_qjz6f7,1635739081.0,False
qjz6f7,"It doesn't seem to me this person is waiting for the perfect channel. They seem to just be looking for good recommendations while they do actual work

I agree on the importance of getting your hands dirty though.",hiul5bf,t1_hiuivmx,1635740476.0,False
qjz6f7,">Take it from me, there are no ""Good"" channels.

Thats why I created my own channel 😎",hivbiev,t1_hiuivmx,1635763365.0,False
qjz6f7,"I really enjoy back to back swe

You can find his channel [here](https://youtube.com/c/BackToBackSWE)",hiut3ei,t3_qjz6f7,1635746283.0,False
qjz6f7,https://youtube.com/user/lefticus1 C++ Weekly,hivvvbu,t3_qjz6f7,1635775815.0,False
qjz6f7,"You can start with the computer fundamentals 

[https://www.youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha](https://www.youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha)

Regarding computer basics start with any computer language you should start with c/c++

[https://youtube.com/playlist?list=PL2\_aWCzGMAwLSqGsERZGXGkA5AfMhcknE](https://youtube.com/playlist?list=PL2_aWCzGMAwLSqGsERZGXGkA5AfMhcknE)",hiw2nt9,t3_qjz6f7,1635778917.0,False
qjz6f7,Continuous Delivery from Dave Farley is very interesting.,hiw96fl,t3_qjz6f7,1635781818.0,False
qjz6f7,"For theory stuff I am yet to find a good channel, if you know any let me know, for programming I recommend Fireship (focused on web mainly) and Brad Traversy (Has a plethora of stuff and also hosts guests on his channel for languages that he has no or little experience in, also, he's a really good guy all around)",hiw9lmq,t3_qjz6f7,1635782000.0,False
qjz6f7,"""Thenewboston"" has some great videos, carried me through CS classes that were in c++ and explains those coding concepts well. This is more implementive knowledge though. In CS you have to learn the theory and the implementation, but I think thenewboston is a fun and easy way to start!",hiy4r6i,t3_qjz6f7,1635810081.0,False
qjz6f7,Find Corey Schaeffer if Python is your thing and Python should be your thing.,hitd73u,t3_qjz6f7,1635717590.0,False
qjz6f7,why should python be your thing?,hitely0,t1_hitd73u,1635718265.0,False
qjz6f7,Good question!,hitfwpg,t1_hitely0,1635718894.0,False
qjz6f7,"If you are not fully committed to learning how to code, then choose Python but there are many shortcuts and libraries that does all the work for you without you having to think about how you should solve the problem by yourself.

Obviously, if you are experienced and you know how to code, then using Python is great because you can use all those tools that the language gives you but if you are starting out new, I don't recommend starting from Python because it's not a language that forces you to think.

If you are starting from Python and you are a beginner, then learn about computer memory as well. Mainly about how main memory works and how different kinds of data are stored, the stack and the heap.

Say if you start learning C and you are a total beginner, then the learning curve is very high, there is a lot to learn; not just about the language but you will also learn about how memory works. However, you will get better in problem solving when using a language like C since there aren't a lot of tools/libraries/shortcuts that C gives you to solve problems. So, it forces you to build those tools in a way, which definitely improves your critical thinking.

So, here is my advice. If you are not sure whether to learn coding or not, try Python first. Learn a bit of Python and see if it interests you. If you are really serious about coding and you want to be a good at problem solving, then learn either C/C++ or Java. Java is a great starting language as well.",hitncam,t1_hitely0,1635722582.0,False
qjz6f7,"I agree, but this doesn’t clear up that the comment creator said    that python should be your thing",hitocd4,t1_hitncam,1635723078.0,False
qjz6f7,"If one is new and wants to get their feet wet then Python is a great starter. You can make incredible progress learning the language and structure of programs in very little time. Easy to write and read, low barrier to entry etc...",hitqchc,t1_hitocd4,1635724104.0,False
qjz6f7,"I agree and that is what I said as well; Python is the way to go if you have no clue what programming/coding is and you simply want to see if it is for you.

And yes, you can definitely make a lot of progress by understanding about the basics of programming such as loops/conditional statements/functions/classes/recursion and so on.

But when comparing to languages such as C/C++/Java, Python is not a language that forces you to understand why something works and how something works. Knowing how and why things works can significantly help you to write better code and also to think like a computer.

For example, in Python I can swap two variables like so:

    x, y = 1, 2

The above is very intuitive. x = 1 and y = 2. This is very easy for a beginner to understand but does he/she understand what is ""actually"" happening? This only teaches the beginner how to code but doesn't teach how to problem solve at all.

Now, let's try the same in C:

    int x; int y; int hold_x_for_me;
    hold_x_for_me = x; 
    x = y; 
    y = hold_x_for_me;

The above gives, x = 1 and y = 2. I know, you can do the same with Python but it's very easy to fall into the trap of using the shortcut because it's much easier to understand and because it doesn't force the beginner to learn why it works.

The above is a bit more complex for a beginner but now the beginner can at least try to understand why the hold\_x\_for\_me is needed. It forces he/she to think why this extra variable is needed.

It allows beginners to start thinking about computer memory, how data is stored, what variables really are (an abstraction to memory address) and these are very important concepts to learn when learning how to code as well. Because by learning such concepts, beginners tend to develop/improve their critical thinking and also improve to think more logically, which is what you need when coding.

As you can see, C has types as well and thus when declaring variables, the correct type must be used. This again forces beginners to learn about how different kinds of data are stored, which is again another important topic. But when using Python, it's very easy to just brush it off.

BUT, you can still learn Python and also learn all the other important concepts I mentioned above. When I learned Python at Uni, we did also learn about how things and why things worked (what happens under the hood) and so it was very useful to learn with Python.

So, if you are a self learner/in the path of self teaching, then make sure you learn what happens under the hood as well (computer memory, namespaces, scoping, data types, how things are stored) if you are going to learn Python.

Even if you do choose to learn C/C++/Java, you may learn what happens under the hood as well depending on how great the resource (book/video) is and so definitely do your research as well.

I will name few important concepts any beginner should be learning when learning how to code:

\- Namespaces  
\- Scoping  
\- Mutability  
\- Data types  
\- Computer memory (main memory/stack/heap in particular)",hitzm0f,t1_hitqchc,1635728889.0,False
qjz6f7,Well for one it is easy to learn. My personal thought is that it's the easiest.,hitpyn1,t1_hitely0,1635723908.0,False
qjz6f7,"Python handicaps a person if they want to learn more powerful languages later.  The dynamic types and and weird class/method structure is so far removed from what the norm is.  Python is good to learn first I'd that's the only thing you're ever going to learn.  But for all other cases, youd be better served learning some kind of OOP first.",hitwro3,t1_hitpyn1,1635727429.0,False
qjz6f7,Well we can agree to disagree :) This is my recommendation and I can tell you with utmost confidence that learning Python does not hamper future learning of other languages. You simply build on it. That's how learning works.,hitygbj,t1_hitwro3,1635728290.0,False
qjz6f7,"While I agree that learning Python first and breaking out into more maintainable languages, I disagree with pushing towards a particular paradigm (like OOP or FP). My first language was Java and the boilerplate was super confusing for me at the time.",hiv8nxy,t1_hitwro3,1635760972.0,False
qk9vxt,"Good article. The last three points especially 
* Hard Realtime
* HW-SW-codesign
* Safety-criticality",hiw0vcj,t3_qk9vxt,1635778101.0,False
qk5g35,"fast.ai

kaggle",hiwawdj,t3_qk5g35,1635782566.0,False
qjk3eb,Run length encoding,hiqnif2,t3_qjk3eb,1635665412.0,False
qjk3eb,"^ what this person said

https://en.wikipedia.org/wiki/Run-length_encoding",hiqtmf1,t1_hiqnif2,1635670603.0,False
qjk3eb,"That's the technique applied here, correct. It's not technically the name of the data structure though.
There is no specific name for this structure but the technique is what's interesting.",hirf3h5,t1_hiqnif2,1635686632.0,False
qjk3eb,"There is, this data structure would just be a list",his2ofk,t1_hirf3h5,1635697681.0,False
qjk3eb,"Yeah, but that's not really helpful for OP, is it? He already stated that this is a list or array...

He obviously is interested in understanding if the way the initial data set is transformed into a run-length encoded data set has a particular name. Yes, a list or array are data structures, but that's a given.",hisddxr,t1_his2ofk,1635702311.0,False
qjk3eb,"I mean you said there’s no specific name for this type of structure, and I just wanted to point out that there is, and it’s just a simple list. The format of the data inside the list can take many different forms but that doesn’t change the data structure.",hisdu5m,t1_hisddxr,1635702486.0,False
qjk3eb,"Yeah sorry for the misunderstanding. I meant to say that it's in the end ""just"" a ""standard list/array"" (nothing special about this part), and that the special thing here is the run-length encoding.
That's it.",hiso9o3,t1_hisdu5m,1635706820.0,False
qjk3eb,"It’s cool, no problem. We agree 👍",hisodw2,t1_hiso9o3,1635706870.0,False
qjk3eb,Perfect. 😉,hisohx3,t1_hisodw2,1635706914.0,False
qjk3eb,"If you want to be pedantic about it, it’s run-length encoded data. You can store a linked list in an array, however it’s still referred to as a linked list. “Data structure”, in its name, literally means you’re talking about the structure of the data.",hisn7cp,t1_hirf3h5,1635706379.0,False
qjk3eb,"Cheers bro.

(Not sure why you would ever want to ""store a linked list in an array"" btw.)",hiso233,t1_hisn7cp,1635706733.0,False
qjk3eb,"Because when it comes down to it, all of memory (except in some exotic architectures) is an array. We structure data inside that array so that we can perform efficient operations on that data, hence “data structure”. I could just as easily say what you call “a list” is just linear data in array starting at a specific address. That’s not helpful, and isn’t what the op was asking for.  

You could extend this to any primitive. For example, let’s say our primitive is a DAG. If I organize the data as a binary tree. If I tell you it’s a binary tree are you going to come back and say “technically the name of the data structure is a DAG.”?

Edit: By the way, in answer to your parenthetical question, if my above explanation as to the architecture of memory as an array (individual data primitives addressed with contiguous integers) didn’t answer it, I’ll say it more explicitly. What you call a linked list is stored in memory, which is an array, so you already store it in an array, that’s why you’d want to do that “bro”.",hispng6,t1_hiso233,1635707390.0,False
qjk3eb,"Oh my. A few misconceptions right here.
If someone asks you what the difference between a linked list and an array is in a software dev interview, I hope you are not going to tell them that a linked list is stored in an array. That's definitely not what they will want to hear.
An array is usually stored contiguous in memory, not the integers. It's a random access data structure.
A linked list is usually not contiguous in memory since it traditionally needs to allow for constant time insertion and removal.

I would suggest you not to go around and throw random technical terms together and instead be precise in what you talk about. The former is generally not appreciated in the industry.",hiuhjpp,t1_hispng6,1635738301.0,False
qjk3eb,"\> Oh my. A few misconceptions right here.

Great, starting with a condescending attitude. You may find that's not ""appreciated in the industry"" either.

\> If someone asks you what the difference between a linked list and an array is in a software dev interview, I hope you are not going to tell them that a linked list is stored in an array. That's definitely not what they will want to hear.

Did you even read what I wrote? Do you understand computer architecture? How do you access memory when you code? It's addressed as a big one dimensional array. A memory address is just an index in to that array and tells you where data is stored. A linked list isn't some magical beast that lives outside of memory, it's implemented on top of memory. That big array. I was trying to make a point that every data structure is an abstraction of other data structures, though you seem to have missed that point, so perhaps I didn't do a very good job communicating it.

As someone who interviews software developers on a regular basis, I can tell you that I'd be impressed if someone actually talked about how data structures like linked lists were actually stored in memory. It would clue me in that they actually understand the underlying mechanism they are writing abstractions over. This is sadly lacking in many of today's software developers.

\> An array is usually stored contiguous in memory, not the integers.

I'm honestly unable to understand what point you're trying to make here. Yes, an array is not stored in ""the integers"" because that sentence doesn't even make any sense. If you're referring to my statement ""individual data primitives addressed with contiguous integers"", did you perhaps misunderstand? I can speculate on why you said what you said, but honestly I don't know, so I won't.

\> A linked list is usually not contiguous in memory since it traditionally needs to allow for constant time insertion and removal.

I'm not sure why you felt to need to point this out. I never said a linked list was stored in contiguous memory. I did talk about a ""list"", referring to one of your comments to GeronimoHero, but a ""list"" is not the same data structure as a ""linked list"".

\> I would suggest you not to go around and throw random technical terms together and instead be precise in what you talk about. The former is generally not appreciated in the industry.

First, I don't throw around random terms, and I don't reply when I don't know what I'm talking about. Please let me know what ""random terms"" I've mentioned in my replies, I'm genuinely interested.

Second, I'm not particularly worried about not being appreciated in the industry as I've very likely been developing software longer than you've been alive (perhaps you're older than 34, but I'm guessing younger based on this conversation) and don't seem to have any difficulty being appreciated. Except perhaps in this asinine thread.",hiuuten,t1_hiuhjpp,1635747774.0,False
qjk3eb,"Btw, I have an M.Sc. in computer science and have actively developed proprietary real-time physics simulation software in C++ as tech lead for more than 15 years.
So, I'd consider my input if I were you. But that's your choice.

Re ""integers"": it's interesting that you don't understand what I meant here given that I quoted you. Please reread what you wrote about ""contiguous integers"" and then my answer. This might help you understand what I meant to say.",hiw93ki,t1_hiuuten,1635781784.0,False
qjk3eb,"So I said ""individual data primitives addressed with contiguous integers"" (that's a copy and paste.) How does that not describe memory, or an array?

As you have an [M.Sc](https://M.Sc). in computer science, please define what a data structure is for me and how RLE is not a data structure.

Honestly, I'm not going to consider your input about what the industry appreciates.",hixoky5,t1_hiw93ki,1635803032.0,False
qjk3eb,Condescending attitude: speak for yourself. If you go back to your first comment you will see where it went sideways. You set the tone by calling me pedantic.,hiw8q49,t1_hiuuten,1635781620.0,False
qjk3eb,"Your first comment is literally ""It's not technically the name of the data structure though."" How is that not pedantic? I wasn't attempting to patronize you, I was pointing out an error in your technicality.",hixno48,t1_hiw8q49,1635802654.0,False
qjk3eb,"Data structure wise, it would just be a list where each entry would be an `(int, int)` pair.  
 
The name of the technique that this employs, which is what I'm guessing is what you're really interested in, is called run-length encoding.",hiqlx5u,t3_qjk3eb,1635664087.0,False
qjk3eb,"For a compression algorithm that uses something similar, take a look at the burrows-wheeler transform!",hir1kcy,t3_qjk3eb,1635677285.0,False
qjk3eb,"Thats where I am coming from. I went through the functional Pearl by Richard Bird, and was thinking about how I can further use the pre-processing BWT offers.",hir65hr,t1_hir1kcy,1635680795.0,True
qjk3eb,"Okay, so now this going deep on subject-specific algorithmics, but a nifty little piece of code is [cufflinks](https://www.nature.com/articles/nbt.1621).

I don't have a university-library account anymore (so can't access the paper proper), but if I remember correctly, they use the BWT to build libraries which they use to map millions of length-50-words unto a smaller library of ""words"" of the sizes ranging from low hundreds to thousands. 

Why that's something you need is very biochemistry, but I remember thinking the implementation is quite smart. This problem, transcriptome assembly, usually deals with datasets sizing into the terrabytes.",hir6snq,t1_hir65hr,1635681253.0,False
qjk3eb,Its called run length encoding,hiryr4r,t3_qjk3eb,1635695983.0,False
qjk3eb,"it is basically a list of tuple of the form \[(int, int)\]. But rather than having a list of tuple, much efficient data structure would be dictionary unless you don't want the data to be mutable. 

Python already provides a function for it from the collection module. You can use counter class from collections module, which takes a iterable as a argument and creates a dictionary, with unique elements as the key and the frequency of the elements as value.

for reference: [https://docs.python.org/3/library/collections.html#collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)",hiqupwp,t3_qjk3eb,1635671579.0,False
qjk3eb,"It’s called a bag, and a set is the unique values",hiscrre,t3_qjk3eb,1635702025.0,False
qjk3eb,List of tuples.,hiqseha,t3_qjk3eb,1635669555.0,False
qjk3eb,Map masquerading as an Array..jk,hiqlpgx,t3_qjk3eb,1635663913.0,False
qjk3eb,Mapping is not the purpose here. So that's incorrect.,hirfenq,t1_hiqlpgx,1635686813.0,False
qjk3eb,"Because the data structure still seems and array you perhaps want the name for similar algorithm not structure. 

I not remember the name used but if search compression algorithms, you will find the name easy. Probably the most popular technique.",hiqo06y,t3_qjk3eb,1635665828.0,False
qjk3eb,Run-length encoding.,hirfbm2,t1_hiqo06y,1635686763.0,False
qjk3eb,inverse look-and-say. jk i have no idea,hiqlkou,t3_qjk3eb,1635663805.0,False
qjk3eb,It could be easily achieved using Map (Hashmap).,hir6n50,t3_qjk3eb,1635681150.0,False
qjk3eb,Tupperware,hiurr8e,t3_qjk3eb,1635745194.0,False
qjk3eb,[deleted],hiqqhv9,t3_qjk3eb,1635667944.0,False
qjk3eb,A list of triples or a dictionary (?),hirfdh9,t3_qjk3eb,1635686794.0,False
qjk3eb,Hashtable or Hashmap?,his8327,t3_qjk3eb,1635700014.0,False
qjrkb9,"There are two types of compression. 

Lossless compression , is basically what you said, so for example if I had 5 a's in a row I'd do A5 instead of AAAAA. 

Then there's lossly compression it's a combination of the above and also throwing away ""unnecessary"" data. You technically loose information and quality but usually it's good enough.",hisd7pk,t3_qjrkb9,1635702210.0,False
qjrkb9,"Lossy compression would not be the same data- for example jpegs would lose quality. Lossless - I think so, but I'm no way an expert on this",hirt8sg,t3_qjrkb9,1635693567.0,False
qjrkb9,"Yes, compression is about removing redundant information from a dataset. I don't think whether it's lossy or not matters for purposes of definition, it just boils down to by what level of accuracy you're preserving the information in the set.",hiv4rzi,t3_qjrkb9,1635757268.0,False
qjwzwd,"https://www.cs.dartmouth.edu/~ac/Teach/CS105-Winter05/Notes/kavathekar-scribe.pdf

Does this help (especially around 6.3)?

Also at the beginning of each iteration of the loop P is the previous augmenting path found in the previous iteration (or in the case of the first iteration P is defined before the loop, the first found augmenting path). This is essentially signifies if there is an augmenting path , and then how to augment the matching using P (flipping matched and unmatched edges) before repeating once more. Once there are no augmenting paths M is maximal.",hju9urb,t3_qjwzwd,1636399801.0,False
qjntyw,Press X to doubt,hisgyty,t3_qjntyw,1635703792.0,False
qjntyw,Math is wrong. It is 51 thousand times faster.,hj3ttaw,t3_qjntyw,1635911209.0,False
qjntyw,"Says you... how did you come to that conclusion? Also even if that would be true, its still a huge step forward!",hj77djg,t1_hj3ttaw,1635973522.0,True
qjntyw,"Probably by reading the abstract of the [actual journal article](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.127.180501), which is linked to at the bottom of the science-news article:

> We estimate that the sampling task finished by Zuchongzhi in about 1.2 h will take the most powerful supercomputer at least 8 yr.

(1 year is about 8766 hours, or 8760 if you ignore leap years)

Also, the science-news article claims ""its calculation complexity is more than 1 million times higher than Google’s Sycamore processor"", but the journal article actually claims:

> The computational cost of the classical simulation of this task is estimated to be 2–3 orders of magnitude higher than the previous work on 53-qubit Sycamore processor",hj88rz6,t1_hj77djg,1635989455.0,False
qjntyw,"Feel free to correct me if I am wrong but -  
  
Fugaku can hit 442,010 TerraFLOPS/second and this new Chinese quantum computer dusts that. It means buisness. Since in 8 years Fugaku could do 1,858,563.648 YottaFLOPS (A YottaFlop is 1 followed by 24 zeros of very precise mathematical operations) in 8 years. This article is comparing a quantum computer to a super computer so naturally it is going to do well. 8 years vs 72 minutes is 5,110,000% faster though. So that would be a grand total of only - 94,972,602,412.8 YottaFLOPS, or ninety four billion septillion nine hundred and seventy two million sextillion six hundred and two thousand quintillion and four hundred and twelve point eight two FLOPS in 72 minutes.",hjxdzpj,t1_hj77djg,1636460167.0,False
qj9666,"I know you're just trying to ask a simple question, but the formatting of your post is like a fork in the eyes, you might get a better answer if you fix it. There are any number of references online about how to format Reddit posts.

Also, as a non-answer: insertionSort is more elegant in a functional language and easier to see that the invariant holds. But I suggest you draw a diagram of what's going on here, you can see how the invariant holds from doing that.",hiovq6k,t3_qj9666,1635628944.0,False
qj9666,"The hint on the inner loop is that it doesn't re-order anything existing in the array, so as long as it puts the new item in the right place, you've maintained the invariant.",hiow6v6,t1_hiovq6k,1635629153.0,False
qj9666,"Existing in which array? If you mean the array A\[1..j-1\] I think that it would reorder things in this array potentially, it could reorder elements in this array by sandwiching the item to insert between them.  So believe it re-orders things in the array?",hipu6vw,t1_hiow6v6,1635645867.0,True
qj9666,"No, there is no re-ordering in this array. Look at how the copying works: `A[i+1] = A[i]`. If you apply that to a contiguous sequence in the array in descending order, you will have that same contiguous sequence shifted ""up"" by one in the array. If you doubt this, run through this algorithm yourself using a short array on a piece of paper, with indices on each entry in the array.

There is no re-rodering here, only a copy of a contiguous sequence from one location to another in the same order.",hj1o965,t1_hipu6vw,1635879573.0,False
qj9666,"They point out in the text that they didn't want to get bogged down in describing the loop invariant of the while loop at the bottom of page 19. The actions of the while loop are the little arrows underneath the blocks in Figure 2.2. I think the invariant for the while loop is that the element is either out of order until it is in order? Basically the condition of the while loop. It's doing the work of insert that you would be doing in the following Haskell code, but I believe in the opposite direction. 

    insert :: Ord a => a -> [a] -> [a]
    insert x [] = x:[]
    insert x (y:ys) = if x < y
                      then x:y:ys
                      else y : insert x ys
    insertionSort :: Ord a => [a] -> [a]
    insertionSort [x] = [x]
    insertionSort (x:xs) = insert x (insertionSort xs)

Ultimately, if you go by what the book describes, you have five cards face down, you pick up the first card. Is that one card in sorted order?",hippk3w,t3_qj9666,1635643503.0,False
qj9666,"When you say ""the element is either out of order until it is in order"" what do you mean?",hipubia,t1_hippk3w,1635645933.0,True
qj9666,A[i] will be greater than key as long as it's not inserted in the correct order.,hiqyhqn,t1_hipubia,1635674864.0,False
qj9666,"Probably not the best way to go about proving the sortedness, but here's an inner loop invariant:

(1) A\[1..i\] is the same as ""A\[1..i\] before the loop started"",

(2) A\[i+2..j\] is the same as ""A\[i+1..j-1\] before the loop started"", and

(3) A\[i+1\]=A\[i+2\]>key.",hiqegti,t3_qj9666,1635658036.0,False
qj9666,Sorry maybe I'm missing something clear but how is A\[i+1\] = A\[i+2\] a loop invariant? Once i gets decremented in the loop wouldn't that change?,hiqf388,t1_hiqegti,1635658502.0,True
qj9666,"There's an A\[i+1\]=A\[i\]. Then i is decremented. So at the end of the loop that's A\[i+2\]=A\[i+1\].

But it's actually true that that's not a loop invariant. That's only true after the first run of the loop, not before it starts, so if you add that clause it's fine. (Not sure about this, because I'm not sure of the exact defn of loop invariant)

The A\[i+2\]>key is important to show that once you exit the loop, if you set A\[i+1\]=key, the whole list is the sorted version of the original A\[1..j\].

The A\[i+1\]=A\[i+2\] is useful for proving by induction that the loop invariant holds even after the next run of the loop.",hiqgjir,t1_hiqf388,1635659625.0,False
qj9666,"Just to be clear, the invariant isn't saying something like A\[35\]=A\[36\], because when i becomes 34 it could change A\[35\]. What the invariant says is ""When you're done with any number of runs of the loop and you look at the value of i at that time, then you'll also see that A\[i+1\]=A\[i+2\] at that time.""",hiqw84a,t1_hiqf388,1635672909.0,False
qjk2pi,"Start with the known inputs and desired outputs, resolve the steps needed to process the input into output.",hiqnrt9,t3_qjk2pi,1635665635.0,False
qjk2pi,Takes time and experience as everting else do not quit easily,hiqpd05,t3_qjk2pi,1635666977.0,False
qjk2pi,"Start with easier problems. All forms or problem solving are essentially about breaking down the problem into more manageable subproblems or steps, so it would probably help to get more practice with easier problems so that you know how to break apart the more difficult ones.",hiqyd4n,t3_qjk2pi,1635674753.0,False
qibyr4,There are companies out there that are developing their own operating systems but it tends to be for very specific applications - aerospace and automotive industries for example. Outside of that I can't think of many good reasons why you would want to.,hii9cbr,t3_qibyr4,1635512171.0,False
qibyr4,"TempleOS helps you talk to God, for example",hiionee,t1_hii9cbr,1635518881.0,False
qibyr4,"Very, very special purpose",hiipf2a,t1_hiionee,1635519189.0,False
qibyr4,"A divine purpose, if you will",hikgtla,t1_hiipf2a,1635544287.0,False
qibyr4,"It did predict covid, after all.",hijvhxm,t1_hiionee,1635535729.0,False
qibyr4,"And it will predict the next one, the time of your death and the heat death of the entire universe. Terry wasn't messing around.",hiqwz2h,t1_hijvhxm,1635673562.0,False
qibyr4,"Ha!  You got me!  I immediately googled TempleOS.  Well played, sir.",hij27rw,t1_hiionee,1635524278.0,False
qibyr4,That’s definitely a rabbit hole that’s east to fall into haha 🕳,hij3lby,t1_hij27rw,1635524819.0,False
qibyr4,Real time centered,hiiebn4,t1_hii9cbr,1635514487.0,False
qibyr4,"There are plenty of custom proprietary operating systems being developed, but they are mostly used for specific industrial applications. For systems that you interact with on a daily basis, the reality is that the two dominant stacks that emerged in the mid 1990s (Unix derived systems and Windows NT) are good and adaptive enough to do everything.

What is also important, and in the case of Windows it's the ultimate reason for its success, is backwards compatibility. Being able to build on decades of supported software is not something you just throw away.

Looking at the Linux kernel, as well as Windows NT (well, you can't really look because it's closed source), replacing something like that is just not realistic at this point. It's impossible to estimate how many man years were spent developing those operating systems. Building something that can compete with these systems is an insane task.

The only reason to build something completely new would be to identify something the others cannot do and that everybody needs.",hiigdsw,t3_qibyr4,1635515405.0,False
qibyr4,"I think it has more to do with economics than anything else. With the exception of Linux, all of the companies that supported those OS are huge corporations that make money off of all kinds of software.

You correctly note that lots of innovation in the OS space since the 80s and 90s were adopted by the others. Indeed, there is a lot of cross pollination - you’ll notice this with the vendors competing for your workflows. How often have you seen one vendor pop up with some feature, then another copy it the next release cycle?

It’s because at some point in the past, the financiers and engineers are mega large corporations realized that the future was in a “platform”: a way that arbitrary improvements and extensions could be hooked onto an underlying system. I began to understand this through Microsoft Services in my work.

There’s a platform, which runs plug-ins, or services, which are small pieces of functionality that complete a specific task. A collection of these and their behavior + our interface with them is a “program”

So why does that matter? Simple. If I have a great platform that people almost *have* to build on for whatever reason, I essentially own a piece of every plugin that runs on it. I become the gatekeeper.

If I have the requisite skills on my teams I can build my own features, or copy what’s out there to keep up with the joneses.

If I have enough cash I can just buy the plugins that are most successful. Look at Apple and JAMF for a recent example.

The consequences of the apple epic ruling also come to light.

So I build a platform once and can get tons of value out of it. From an investment perspective it’s superior to one-off bespoke development.

Edit: clarity and a few more examples",hiim27n,t3_qibyr4,1635517817.0,False
qibyr4,Linux is no exception.  A very significant amount of the development that goes into Linux and the software around it is done by large corporations with for-profit motives.,hiipxua,t1_hiim27n,1635519401.0,False
qibyr4,"This is true, I wanted to take a general “economics of software development” at enterprise level, and ignored open-source for clarity.

Open-source behaves differently from a closed-source software venture, and I don’t understand it as well as closed-source development.",hiivjyo,t1_hiipxua,1635521667.0,False
qibyr4,"A very good insight into why open source became such a major thing is to drive down prices for your complements. 

A good read on the topic is https://www.gwern.net/Complement

This one article opened my eyes to a lot of questions concerning why some corp. did open source and what is their possible end goal.",hijthy5,t1_hiivjyo,1635534931.0,False
qibyr4,"*pulls a soapbox out of a bag of holding*

Selfish motives aside, a strong and vibrant culture of open source software development and free computing increases tech and math literacy while growing the traditional pool of people from which innovation occurs. I think it is clear that neither fully proprietary nor fully free points of view will prevail, because nature likes a good idea and widespread literacy, invention, and technological agency without limits is a very good idea. I personally get a kick out of the corporate fondness for open source software as a perfect example of selfishness breeding something universally pretty good. The very people doing the extending and embracing are often responsible for propagating ideas farther and wider than the hardliners ever would have on their own, by their own ideology, even as such hardliners and ""never corp"" types keep the other side from getting too carried away. Like all ideological concerns, it is a funny little spectrum with important details that get obscured by the level of rhetoric involved (which creates the gang warfare mentality so common in all ideological concerns). 

In short: I am pretty happy that big companies find it in their interest to support free software, while I am equally happy that there are people so extreme about it that the concept is fairly safe from total co-option. This is a thing that is good for generational tech literacy, and even national security when you extrapolate from that.

*puts soapbox back in bag of holding and runs off*",hik0xul,t1_hijthy5,1635537823.0,False
qibyr4,Agreed. Thx for sharing🙌,hikjuuh,t1_hik0xul,1635545578.0,False
qibyr4,Soapbox of speech craft,hilcjjd,t1_hik0xul,1635559118.0,False
qibyr4,Thank you for the link! I appreciate the opportunity to learn 😁,hijvens,t1_hijthy5,1635535692.0,False
qibyr4,"It is the same with graphic drivers and browsers, and to some extend also with networking protocolls, usb, hdmi/DP, filesystems, image formats, file compression and so on.

There are new inventions but they are not improved enought that it is worth it to switch. For example, did you know that there is an image format called JPEG2000, which improves many aspects of JPEG, but JPEG is just good enough.

In some cases like task sceduling, deadlock resolution or file compression it is already proven to be optimal solved. So there is no way to improve it.",hiiv004,t3_qibyr4,1635521447.0,False
qibyr4,"Casey Muratori talks about exactly this in his talk ""The Thirty Million Line Problem"" [https://www.youtube.com/watch?v=kZRE7HIO3vk](https://www.youtube.com/watch?v=kZRE7HIO3vk).

Basically we expect so much of an OS these days that it's almost imposible to make a brand new, competing OS.",hiigts3,t3_qibyr4,1635515596.0,False
qibyr4,"Same issue with the Web as a whole, if I remember correctly Chrome had more lines of code than Windows.",him01qk,t1_hiigts3,1635572297.0,False
qibyr4,Because you don’t need to reinvent the wheel?,hiia4vt,t3_qibyr4,1635512548.0,False
qibyr4,Work slows down at good-enough.,hikz0ce,t1_hiia4vt,1635552522.0,False
qibyr4,"Noooo this statement is such nonsense. If today's solutions are terrible, why wouldn't you want to make something better?",hiicwi7,t1_hiia4vt,1635513834.0,False
qibyr4,"That's not what ""don't reinvent the wheel"" means.  It means if there is already a *good* solution, it's a waste of the time, effort, and investment needed to make another solution, that is either no better, or only marginally so.

Of course you're right that if all current solutions are *bad* we should try to make something better.

Imo for the purposes of desktop/workstation OSes though, Linux and Windows are both pretty good these days, you'd have to come up with something reaaaaaly different and amazing to call current solutions ""terrible"" by comparison.",hiiexab,t1_hiicwi7,1635514759.0,False
qibyr4,"Yeah I guess that's a valid interpretation of the phrase.

The kernel of current OSes are good afaik, it's just the huge pile of bad user code they have on top what makes them bad.",hiif9z6,t1_hiiexab,1635514914.0,False
qibyr4,"The “bad code” is what, exactly?

There are plenty of cases where the solutions are often good enough to not warrant rewriting.  If someone decides a part of that system is completely unacceptable, there’s nothing getting in the way of them starting a new project.",hiilz6a,t1_hiif9z6,1635517782.0,False
qibyr4,"Not a windows user, eh?

Eta: you can't search the files on the same fucking hard drive it's installed on and return the right results?  That's 100% unacceptable lol, and yet windows is ubiquitous",hijlvnn,t1_hiilz6a,1635531895.0,False
qibyr4,"The OS isn't made to search files, you can use a program for that. The fact that Windows let's you easily develop and then run a program to find files is why it's a good (enough) OS",hijr4nm,t1_hijlvnn,1635533984.0,False
qibyr4,The operating system that is used by every person who doesn't know anything about programming or computers doesn't need to have file search capabilities?  What??,hik3pxb,t1_hijr4nm,1635538911.0,False
qibyr4,"I mean ideally it would have a better one, I'm not arguing that point - I'm just saying that isn't the primary point of the OS, so doesn't count as a reason to write a new OS since you can use a tool easily to fix that. That's like saying buy a new house because you don't like the colour of the walls instead of painting them.",hikemmk,t1_hik3pxb,1635543365.0,False
qibyr4,Is file search a top concern when picking an OS? If there was a windows alternate but within great file search how much would you pay to switch? Even as a new user would you pay more than windows? Even if somehow the new OS could run all windows software I don't think you would put a high $ amount on it,hijv85i,t1_hijlvnn,1635535620.0,False
qibyr4,"I mean yeah I know why people stick with it.  Its actually incredibly awful user experience, though. Remember idk, the whole thing where they had an illegal monopoly?  Lol come on.",hik3hqm,t1_hijv85i,1635538823.0,False
qibyr4,"I'm not saying it's perfect but the annoying parts aren't even close to getting me to give up the convenience never mind work environments where the cost of switching would be significant. No one is developing a windows alternative because people aren't mad enough to pay the cost of switching. You can have some feelings about how MS used to behave, that's fair. Doesn't change the fact that building a windows competitor doesn't make sense financially",hik5d1u,t1_hik3hqm,1635539564.0,False
qibyr4,"I agree but to me it's like, if the whole system is running on Microsoft, that's a public utility and should be high quality, extremely reliable, and open source",hik5uqp,t1_hik5d1u,1635539762.0,False
qibyr4,Thats true but without financial motivation it would be really difficult. Most of the things an open source OS would need to compete with windows aren't sexy or fun and you would struggle finding qualified people to work on it for free,hikbs3f,t1_hik5uqp,1635542179.0,False
qibyr4,"That’s a straw man.  The monopoly has nothing to do with the adequacy of the software or platform.

Windows does have file indexing built in, and it does work.  I don’t think the service is on by default.  There are other options like Everything, which also works.",hik7ly6,t1_hik3hqm,1635540468.0,False
qibyr4,Oh please do explain how it's a straw man! 😇,hikbiqj,t1_hik7ly6,1635542069.0,False
qibyr4,It was the second sentence,hile3o8,t1_hikbiqj,1635559894.0,False
qibyr4,"Well, Google is developing Fuchsia completely from scratch (supposedly to overcome inherent architectural limits of the Linux kernel), but as someone noted, it's a whole lotta work   
https://fuchsia.dev/",hikj07z,t3_qibyr4,1635545210.0,False
qibyr4,"iOS, Android (granted android is functionally a linux distro), and Microsoft has flung more pasta at the proverbial mobile OS wall than almost anyone. The majority of consumer computing is done on mobile devices today, so if there's money to be made in the production, maintenance, and control of an operating system it's there. There's also Chrome OS which dominates in education. Is your question maybe more in the realm of why there aren't new kernels being developed? It would be fair to say most operating systems today are built on top of some fork of either a windows or linux kernel (this includes apple operating systems), but the OS space itself seems healthy.",hiik24b,t3_qibyr4,1635516989.0,False
qibyr4,big and hard + not worth it,hiix1d0,t3_qibyr4,1635522250.0,False
qibyr4,"~Urbit seems to be an interesting project that builds a full stack OS from the ground up based on data ownership. They have some weird details, as you’ll begin to read, but it’s attracting some interest in the cypherpunk community.",hij2xay,t3_qibyr4,1635524554.0,False
qibyr4,I think computers are changing as a whole. Smart phones came out in the mid 2000s and now most of people's daily device is either a tablet or phone.,hiig9w2,t3_qibyr4,1635515357.0,False
qibyr4,"And yet they all run on an OS 'originally' written for mainframes, which migrated to desktops, network servers, and workstations before ending up in your pocket.",hiii27j,t1_hiig9w2,1635516135.0,False
qibyr4,There’s just no need for new OS’s for public use? There is already an OS for everything. Specific OS are still being made for specific industries.,hijltdf,t3_qibyr4,1635531871.0,False
qibyr4,"It turns out that linux is already very good and because it is an open source project, it is constantly getting better…so yeah.",hik1963,t3_qibyr4,1635537945.0,False
qibyr4,"If you have a long layover at an airport like I did a few weeks ago, you’d might be interested in this [Timothy Roscoe presentation](https://youtu.be/36myc8wQhLo). He talks about how OS’s are far more complex than academics realize/admit to. With SoCs becoming more common and transistors becoming denser and cheaper, the OS extends far beyond the kernel. This only tangentially answers your questions, but is an interesting topic nonetheless.",hikggin,t3_qibyr4,1635544135.0,False
qibyr4,"Because applications are the most economically motivated projects. An operating system doesn’t actually do anything - it enables other applications to solve a problem. An application that can be monetized has much more market pressure to exist. 

And, existing OS’s are very stable and serve their purpose pretty well so, there’s really no reason to completely reinvent the wheel with them.",hil83fb,t3_qibyr4,1635556939.0,False
qibyr4,"It's a hell of a lot easier to build upon a pre-existing coding language than make a new one from scratch. Hence why Linux and Unix distros are so popular. and why Javascript and shit like that are also equally popular. Though if you want an OS that was coded recently I mean, TempleOS?",hiiifv6,t3_qibyr4,1635516302.0,False
qibyr4,Did you search the subreddit for previous questions about this exact topic?,hiic9dk,t3_qibyr4,1635513545.0,False
qibyr4,Microkernel OSes proved to be slower and not much safer than monolithic OSes. The major selling point of old OS is the tools and applications that they provide. A new OS would have to have a major advantage to overcome the lack of software.,hij42t2,t3_qibyr4,1635525006.0,False
qibyr4,"There is no need for a new OS from scratch.

Is you think there is then why not make it?",hik808p,t3_qibyr4,1635540629.0,False
qibyr4,"Linux. 

For most purposes that an operating system would have been made from scratch twenty years ago or even forty years ago, Linux either fits the bill very well, or is good enough. 

Though keep in mind that in the few places you still some regular OS development, though as hardware gets more and more powerful, it become less worth the trouble. The two ends of the spectrum if you will, HPC and embedded. 20 years ago there were generally using OSs there either developed for that kind of system or even running a custom OS. Using Linux in either place makes development of software and systems insanely easier, and the only reason that Linux wasn't used before was it was too slow or had too much overhead. Nowadays, to the degree that overhead is still there, just throwing some more hardware at the problem fixes. 

In terms of advances, so much of it happens behind the scenes. The Linux kernel of today does think that weren't invented yet in the 90s. As long as the interface between userspace and the kernel stay the same, the kernel can be radically changed, and it has. One thing that effectively future proofed a lot things was the assumption that a lot of processes/threads were logically, if not actually, running at the same time. Doing so provide a huge amount of flexibility, even back when only servers and high-end systems had more than one cpu/core. This abstraction made going from systems with 1-2 cores to systems with 10-20 cores possible. It takes a lot of stuff to make that performant, but again it's all under the hood, hidden from the user, and from userspace developers.

Same thing in terms compilers, language implementation, and runtimes. Programming languages have changed a lot in the last 20-30 years, the best being  the only real software was written in C or C++, period. There were exceptions of course, but outside of very specific domains, there wasn't really a choice. There were a number of scripting languages  that people used, many  Perl (and PHP if you want to think about it at all), which traded a lot of performance and/or safety for ease of use. The web wasn't doing anything like it does today, so scripting languages were generally good enough. 

That's some to keep in mind in general. Doing anything from scratch takes a lot of resources, generally at least a couple orders of magnitude more, than using an existing solution. Anyone that cares about costs, which is basically everyone, is going work with something that is good enough if they can.",hiky16q,t3_qibyr4,1635552053.0,False
qibyr4,"For most hardware, the reasons are that current OSs are

* good enough for most existing hardware
* general enough to be adapted to new hardware
* too large to efficiently rewrite
   * Linux is estimated to cost several billion dollars, if one wanted to re-write it
   * Alternatively, you would need >50.000 person-years
* around long enough so that backwards compatibility becomes an issue
* in general of good quality, better than what most can produce on their ow
   * having been widely used for a long time helps removing bugs",hil2crv,t3_qibyr4,1635554125.0,False
qibyr4,Because the big 3 are very good,hilwdja,t3_qibyr4,1635569827.0,False
qibyr4,…and how is this analogous to crypto/web3,hilz7um,t3_qibyr4,1635571725.0,False
qibyr4,"Negative complexity entropy

We should have systems that boot up in milliseconds, but alas...",hiqrl0u,t3_qibyr4,1635668869.0,False
qi8v4v,"The data for all the game assets like levels, characters, 3d models, sounds and all that stuff is kept on the DVD.

What the save data does is stores your *progress* in the game. Things like which level and checkpoint you're on, numerical information like stats, your Player name. things like that which take relatively little data. This could all fit easily on the card.",hihqznk,t3_qi8v4v,1635500073.0,False
qi8v4v,[deleted],hihzvyu,t1_hihqznk,1635506907.0,False
qi8v4v,"Current game consoles store a lot more stuff on disk, like game updates and such which take a lot more space.",hii0nfw,t1_hihzvyu,1635507391.0,False
qi8v4v,I think the newer playstations put the entire game on the console,hik5188,t1_hii0nfw,1635539432.0,False
qi8v4v,"well nowdays it cant read the disc fast enough to keep up, since graphics take up sooo much more space nowdays. so it copies everything from the disk to your hardrive so that when you play the game you dont have to wait for it to read the disc (remember those awful loading screens? if it was the same ud be waiting for 20mins nowdays)",hii6sii,t1_hihzvyu,1635510905.0,False
qi8v4v,I would guess that the internal storage would be used to cache some data of the game from the CD (for purposes of fetching game data faster).,hii0ume,t1_hihzvyu,1635507515.0,False
qi8v4v,Because they sell digital games now and allow screen caps and clip storage.You’re thinking way too hard about this.,hii0qlt,t1_hihzvyu,1635507445.0,False
qi8v4v,lmao yeah my bad :),hii4vug,t1_hii0qlt,1635509883.0,True
qi8v4v,"why u delete? even if u were wrong, ppl cant see the context of our replies. just put an edit in.",hii8sjd,t1_hii4vug,1635511908.0,False
qi8v4v,"Game assets and what-have-you are stored on the medium, like the bits on your hard drive or the cd you put into your console.

Your progress in the game can be kept in much smaller space because it isn't saving all the models, assets, so on; it is saving your current location, your current inventory, and your current plot points.

For example, (I don't know GoW specifically so here's a generic example), You can keep track of every individual area of the game in terms of an integer Id, you can keep track of your position within that level in three floats, you can store your inventory as an array of integer Ids, and you can save your plot progression with some minimized objects.

Example file:

```
{
  currentArea: 30321,
  currentPosition: {
    x: 44.0115,
    y: 188.9492,
    z: 65.7193
  },
  currentInventory: [
    10438,
    192873,
    1294674,
    49081,
    19237,
    487
  ],
  currentHitPoints: 74,
  plotProgression: [
    { withCharacter: ""Poseidon"", progress: 3 },
    { withCharacter: ""Athena"", progress: 8 },
    { withCharacter: ""Apollo"", progress: 1 }
  ]
}
```",hij2po3,t3_qi8v4v,1635524471.0,False
qi8v4v,"Because save files store minimum amount of data of current game state that is needed to be able to run the game from that same state next time. It doesn't store graphic textures, entities, map, locations... Those are all recreated (instantiated) from disc when save file is loaded from memory card. 

For example, saved file could store: id of last mission passed, id of current map, coordinates of last checkpoint, player stats (weapons, health, magic, armor)... With all that data, proper game files are loaded from disc and game resumes based on saved state.",hihrc0u,t3_qi8v4v,1635500372.0,False
qi8v4v,"On top of my head, just save location id, opened chests, gorgon eyes, feathers, level ups, orbs + current level state",hii9t2p,t3_qi8v4v,1635512392.0,False
qi8v4v,"Ps2 required a disk to be inserted. Most of the data was on that disk. The only thing saved in the 8mb memory card was the actual save data. Just the location or level you were on, level of your character, orbs you had, etc.",hiiai0d,t3_qi8v4v,1635512720.0,False
qi8v4v,"Old games had less data, because lower quality, less background things going on, plus most stuff was stored on the CD. Saves aren't huge. I frequently back up my PS4 saves of certain games onto a flash drive and most of the time it's usually only 4 or 5 megs\*, after 60+ hours of gameplay.",hihyduh,t3_qi8v4v,1635505909.0,False
qi8v4v,4 or 5 GB of data just to save game progress? Which games?,hihzjv4,t1_hihyduh,1635506685.0,False
qi8v4v,"Update, I was wrong. 4 or 5 Megabytes. Ghost of Tsushima is the most recent.",hihzxpc,t1_hihzjv4,1635506938.0,False
qi8v4v,Look up .kkrieger. not an answer to your question but you might come back with a new set of questions :),hiqrxel,t3_qi8v4v,1635669161.0,False
qiq0jd,"The classic ""[Engineering V](https://www.researchgate.net/figure/The-Systems-Engineering-V-Diagram_fig2_320585817)""?

DO-178C?

ARP4754?",hilcc9b,t3_qiq0jd,1635559018.0,False
qiu1wc,"I agree that knowing CS will make it easier to learn any language. But when a senior dev is ""flexing"" about knowing this and that language, they might actually be flexing about knowing its ecosystem inside out and about being immediately productive on a large-scale project. When I read ""Java developer with 5 years of experience"", I do not read ""has implemented algorithms in Java for 5 years when they could have done it in pseudocode"". I read ""this guy has battlefield experience and can quickly get my Java project up and running"". 

You can learn CS for 5 years and it will 100% make it easier to understand all the quirks of Java. But being actually productive in a real-world Java project is something else entirely. Do you know Lombok? Maven? Maybe Gradle? Are you familiar with the JVM? Can you spot a performance problem using its monitoring tools and fix it in the corresponding code? Do you know the Reactive programming librairies? Spring? How familiar are you with JUnit? Etc. And if for some weird reason you had to switch this real-world project say from Java to Python, CS wouldn't help you very much.

English is a tool, but learning grammar won't make you good at English. And it will certainly not make you knowledgeable about the whole ecosystem of jokes, proverbs, expressions, synonyms, etc.

> And I won't even like to call you a programmer if you just know the programming language.

A programmer builds useful programs with a programming language. This kind of pointless statements just make you sound arrogant, and arrogance is probably the worst personality trait in a developer.",him6ind,t3_qiu1wc,1635577095.0,False
qiu1wc,">This kind of pointless statements just make you sound arrogant, and arrogance is probably the worst personality trait in a developer.

Yeah... I also think that I was arrogant in the post and I am sorry for that. Thinking to delete the post...",himg5r3,t1_him6ind,1635584935.0,True
qiu1wc,No worries. We all make blanket statements sometimes...,himgwe5,t1_himg5r3,1635585559.0,False
qiu1wc,"I think there are two sides to this. A programmer builds useful programs with a programming language. A good programmer builds useful programs that have a low count of lurking bugs, handle bad inputs gracefully, are easy to read and maintain, are easy to install/deploy, meet customer requirements, give clear error messages, have a clear version history... Those are essential skills for a programmer that are largely unrelated to the language you're working in.

When I'm hiring, I'm much more interested in someone convincing me they have those skills than that they've worked in language X for 5 years. If someone clearly has those non-language skills, I'm prepared to hire them with no experience in the relevant language at all, on the assumption that they'll pick it up quickly enough that it won't be an issue. Just asking someone to demonstrate having worked on a project in language X for Y time I view as very risky - it's surprisingly easy to bullshit your way into a junior role in a big organisation and then look busy while never actually achieving anything. You can get away with this for a long time.

A couple of years ago, there was a guy contracting at one of the companies I work with. He was hired because he appeared to have solid Go experience on his CV. When he arrived, it pretty quickly became obvious that he could sort of talk the talk and had seen people working in Go, but had never actually written anything useful. As soon as you started asking detailed questions, his bullshit turned to defensive bluster. As soon as you asked him to write some code, nothing happened. If you really pressed him to produce something, he would deliver something cobbled together from StackOverflow examples that didn't really address the requirement and was nearly up to the code standard you'd expect for a five minute demonstration that something was possible. Usually it involved some wildly inappropriate technology choice, because that's what the examples he found used and he didn't know how to adapt them (trying to replace sqlite with Elastic on an embedded system already struggling for memory is an example that sticks in the mind - he didn't understand why we wouldn't listen to his amazing ideas). Error handling, when it happened at all, consisted of `panic()`.

He was bagging £400 per day and as far as he was concerned, he'd made it. He had no awareness that a problem existed and no interest in improving - as far as he was concerned, everyone else was the problem. His contract still got extended three times and it was only after two years that we managed to convince HR that his contributions were entirely negative and to stop extending.

Six months later, his recruitment agent told me he'd spent the profits from his two years on a three month holiday in Croatia, then moved to London and used his experience with us to bag a gig paying nearly double what he'd earned with us. Said recruiter was surprised I didn't think this an excellent outcome.",him96ir,t1_him6ind,1635579217.0,False
qiu1wc,"“Learn how to program and you won’t ever need to learn a language”

Each language has its own paradigm, but these paradigms can all be learned. I had a class in university called ‘Structures of Programming languages’ where we learned just this. Eventually you learn that a language is sort of like a spice, its just a matter of picking the certain one you need for your dish or program.",hilypve,t3_qiu1wc,1635571384.0,False
qiu1wc,"I can only assume you've not had to write substantial amounts of JavaScript, lol.

I do think that once you limit the field to somewhat competence languages you are generally right. If you are good at programming, you'll be served just fine most general purpose languages out there.",hing7og,t3_qiu1wc,1635606927.0,False
qiu1wc,"Programming languages are like storage of computer, not as important as cpu but a computer can't work without it",hilxsel,t3_qiu1wc,1635570750.0,False
qiu1wc,"If you have read my entire post, I highlight that programming languages are not as important as the concepts. I never mentioned that you never need programming languages",himg0jy,t1_hilxsel,1635584814.0,True
qiu1wc,True..,hin5ua8,t3_qiu1wc,1635602078.0,False
qiu1wc,"You can learn a language in a night, maybe. Its library framework though? Hoo boy. Also you're not gonna write a website in Cobol. Pick the language that's appropriate for the job you are doing. That said, I generally agree.",hiqt9ck,t3_qiu1wc,1635670286.0,False
qiu1wc,"ya its sort of like if you know one you know them all. ""i know x programming languages"" sounds like something you only see on the internet though. i cant imagine someone unironically saying that in real life",him5qji,t3_qiu1wc,1635576483.0,False
qidftd,"I don't think so, a function is either recursive or it is not.

There are functions that are interesting in their recursivity, or complex in their recursivity, but it is not that they are ""more recursive"" than the others, they are simply recursive functions.",hiijcb6,t3_qidftd,1635516686.0,False
qidftd,"It's not a gradient of less to more recursive, it's a truth- or falsehood; a function is either recursive (invokes itself) or is not",hiipll3,t3_qidftd,1635519263.0,False
qidftd,Well some recursive functions call themselves once while others call themselves more than once.,hiiyeev,t3_qidftd,1635522780.0,False
qidftd,"[Yes.](https://en.m.wikipedia.org/wiki/Ackermann_function)

Or, to put it less tongue-in-cheek: there is a concept of ""primitive recursive,"" and recursive functions that are *not* primitive can be considered, in a sense, *more recursive* than usual. They cannot be computed with loops whose upper bounds are known in advance.",hijxlc4,t3_qidftd,1635536541.0,False
qidftd,"There are non-primitive recursive functions (like Ackerman) which cannot be formulated using for loops, so I guess these can qualify as “more recursive”?",hjubwq8,t3_qidftd,1636400623.0,False
qhohv1,Thoughtless design or bad programmers can also make programs consume more energy.,hielrf8,t3_qhohv1,1635442014.0,False
qhohv1,Theoretical attacks do not scare me :),hiehmw7,t3_qhohv1,1635440407.0,False
qhohv1,Maybe they can make an AI that can figure out how to be more efficient?,hih1524,t3_qhohv1,1635480251.0,False
qhohv1,The whole internet is just a huge waste of energy. There are some justified uses but a lot of it is just worthless junk. We only see a tip of an iceberg.,hihg0g4,t3_qhohv1,1635490498.0,False
qhhz7s,"There is.

The simplest way to recall it is to think of:

\- negation as unary minus;

\- conjunction as multiplication;

\- disjunction as addition.

So that ""not p and q or r"" is just as ""-1 x 2 + 3"".

So because you would parenthesise the arithmetic expression as ((-1) x 2) + 3), you would interpret ""not p and q or r"" as ""(((not p) and q) or r)"".

Signs such as implication, double implication are even lower down the line than disjunction.",hidhtnl,t3_qhhz7s,1635425319.0,False
qhhz7s,In Mathematics grouping symbols are typically always used for everything except negation which is unary.,hiczw68,t3_qhhz7s,1635413166.0,False
qhhz7s,"I don’t think there’s a universal consensus, but if you google e.g. ‘Java operator precedence’ you’ll see a table detailing the priority of both mathematical and Boolean operators for that language",hicw3zf,t3_qhhz7s,1635409864.0,False
qhhz7s,"“A and B or C” = AB + C

And take precedence over or

Just look up the boolean order of precedence. Your answer is the first thing that pops up.",hidlfer,t3_qhhz7s,1635427104.0,False
qhhz7s,"Wouldn't it be faster to look at the or first? If that is true, then no need to do anything else.",hie42ca,t1_hidlfer,1635435090.0,False
qhhz7s,Yes.....? Pretty sure....,hicvyar,t3_qhhz7s,1635409731.0,False
qhcmwq,"It depends on the implementation. Ultimately, it probably doesn't matter, but I'm partial to having the head store value, because cons cells.",hic2l10,t3_qhcmwq,1635389951.0,False
qhcmwq,"Consider this 3 element list...

Head of List Pointer -> Item 0 \[This is just a pointer\]

Item0  (pointer -> Item1, data) \[aka Head of list\]  
Item1 (pointer -> Item2, data)   
Item2 (pointer -> Null, data) \[end of list\]

The initial pointer has no data, it is just a pointer, not actually part of the list.",hic6yad,t3_qhcmwq,1635392152.0,False
qhcmwq,[deleted],hicryk1,t1_hic6yad,1635406277.0,False
qhcmwq,You're getting downvoted because u/CypherAus states that Item0 is the head.,hidznr1,t1_hicryk1,1635433293.0,False
qhcmwq,"the head is the first item n=0, contains the item and the pointer to next item. the LinkedList object stores the pointer to the head.

so if you delete the first item the head pointer in the LinkedList object will point to the second item, note that the LinkedList object is the same and only the head pointer value within the object has changed.",hid0lx5,t3_qhcmwq,1635413778.0,False
qhcmwq,"Think about the language you're using.

Does the head **node** store a value?

The answer is yes, because all nodes store values. That's what they're for.


If you were you change the question to ""does the head **pointer** store a value?"" then things get more complicated to answer.",hid5rhl,t3_qhcmwq,1635417898.0,False
qhcmwq,"You can do either I guess, but it is generally just like any node you decide to mark as head. (i.e. a pointer to the node at the beginning of the list)",hibzf86,t3_qhcmwq,1635388487.0,False
qhcmwq,"Agree with /u/CypherAus - The implementations I've seen typically have the head being just a pointer.

This is good from a ""separation of concerns"" perspective - Your node class only ever holds data, your linked list class only ever manipulates nodes.",hicilnb,t3_qhcmwq,1635399070.0,False
qgzhy7,"With a modern optimizing compiler either way to express the equation should result--more or less--in the same generated code. 

(The exception is if certain operations have defined 'code execution' blocks which cannot be optimized out. For example, if `y'` actually represents a function call, and the compiler cannot know the function call doesn't have side effects, writing `y' + y'` may result in two calls to the function `y'` instead of one.)

Generally when I write code, I favor clarity over compactness. So I would write:

    ey = y'-y;
    ex = x'-x;
    e = y' + ey + x' + ex;

over

    e = y' + (y' - y) + x' + (x' - x);

Exception: there are certain well-known equations that have meaning to us when expressed as a single expression, such as the Pythagorean theorem. So I'd favor writing:

    r = sqrt(x*x + y*y);

over

    a = x*x;
    b = y*y;
    r = sqrt(a + b);

----

All of this--since it makes little difference to the compiler--is a matter of style.

And I tend to write assuming another human being later on will have to maintain what I write--so I tend to write for legibility.",hi9n5ln,t3_qgzhy7,1635353704.0,False
qgzhy7,"You can try exploring how your compiler processes your code, i.e. on godbolt.com",hia0g7q,t3_qgzhy7,1635358894.0,False
qgoxsp,"The simplest and often most effective predictive text engine is simply a statistical use of n-grams. N-grams are sequences of n words. These can be 2,3,4,5, or more words. You’ll likely want n-grams of different lengths to handle the beginnings of sentences where there are few if any words already typed. In order to build a predictive text engine, you would need to parse some corpus of text. Things like ebooks, websites, Text messages, etc. Parse these texts and determine the frequency of unique words following each unique n-gram. Once you’re done parsing, prune away the low frequency words following each n-gram and leave only the (3?) most common ones.  From there, you simply use a lookup table to provide text suggestions by determining which n-gram has already been typed and suggesting the three most common words to follow it. 

This is a type of Markov decision process.

You can update the engine as needed by incorporating more recent text and weighting in favor of that while decaying the weight of old text.",hi9p366,t3_qgoxsp,1635354470.0,False
qgoxsp,"To make a good predctive text generator you would want to use ML, which mean you would need data.

To code from scratch is an impossible task, so use libraries that offer the functionality you need (some deep learning package) and  choosing the correct algorithm is also not trivial.

I suggest to simply follow a guide and understand what they done, to understand why they done or how its done internally i suggest to watch a deel learning course (standford in youtube is great).

Example guide: https://medium.com/analytics-vidhya/build-a-simple-predictive-keyboard-using-python-and-keras-b78d3c88cffb",hi7pk8d,t3_qgoxsp,1635312680.0,False
qgoxsp,"The common approach used to be Hidden Markov Models, or Markov Chains",hi7w59a,t3_qgoxsp,1635317622.0,False
qgoxsp,[deleted],hi87h44,t3_qgoxsp,1635327391.0,False
qgoxsp,"**[Trie](https://en.wikipedia.org/wiki/Trie#:~:text=In computer science, a trie,key, but by individual characters)** 
 
 >In computer science, a trie, also called digital tree or prefix tree, is a type of search tree, a tree data structure used for locating specific keys from within a set. These keys are most often strings, with links between nodes defined not by the entire key, but by individual characters. In order to access a key (to recover its value, change it, or remove it), the trie is traversed depth-first, following the links between nodes, which represent each character in the key. Unlike a binary search tree, nodes in the trie do not store their associated key.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hi87i1j,t1_hi87h44,1635327413.0,False
qgoxsp,*laughs in Github copilot*,hi9u98w,t3_qgoxsp,1635356516.0,False
qgbxhk,"My interpretation is that you have 4 years of experience with web and you're having trouble with a basic assignment. 

I would take a step back and try to determine why it's difficult, and be critical, like, it is algorithms making it difficult or is the problem beneath you because of your experience?

If experienced with the topic, approach it as an auditor of the problem and report your opinion and your improvements for the problem to whomever created it (TA or instructor). You get the problem done, instructor gets feedback (they love that), and future students benefit. Win win",hi629fg,t3_qgbxhk,1635284098.0,False
qgbxhk,"No the assignments and even the exam is rather easy, but it's  the learning of for example Javascript weird comparisons, when I already know and used Javascript a lot.

The Tips that you gave were else very usefull.",hi88pbu,t1_hi629fg,1635328422.0,True
qgbxhk,"There's always more to learn. That's the nature of computer science and of programming. If you're not being challenged then power through and use the extra time to learn about stuff that interests you. Push yourself towards expertise if you feel so strongly. Or just take the good grade for now honestly. As an academic field and as an industry, self-driven learning is a critical skill to pick up.",hi5a35q,t3_qgbxhk,1635273123.0,False
qgbxhk,"Yep, well that is the challenge to overcome.",hi88wi1,t1_hi5a35q,1635328589.0,True
qgbxhk,Gamification sometimes helps. E.g. leetcode or alike.,hi552wg,t3_qgbxhk,1635271141.0,False
qgbxhk,That sounds like a nice approach.,hi88q77,t1_hi552wg,1635328442.0,True
qgbxhk,"I'm in a similar boat. Been doing entry level dev work for the last seven years basically. Now I'm going back to school to actually get the stupid slip of paper, and it's so hard to put my mind to stuff because I already know the bulk of it. Some classes have well structured activities instead of lectures and those are fine, but it's the ones with video lectures that are just mind numbing to me. 

Honestly, my solution is to just pound energy drinks like there's no tomorrow and then muster all the willpower I have to force myself to focus. It helps to remind myself that if I finish my stuff asap I can spend time on personal projects instead. Ultimately, my best motivator is probably deadlines though.",hi5j0u4,t3_qgbxhk,1635276610.0,False
qgbxhk,"Yes exactly, mind numbing describes it so well. Not so sure about the energy drinks method but deadlines are definitely of good use.",hi8923x,t1_hi5j0u4,1635328725.0,True
qgbxhk,">Not so sure about the energy drinks method

Yeah, I think the energy drinks only work for me because I have untreated ADHD haha",hi8t8c0,t1_hi8923x,1635341209.0,False
qgc7oj,"The question is a bit vague, which is probably why you're not finding anything definitive. You'd need to define your terms more clearly:

&#x200B;

1. What constitutes a ""breakthrough search algorithm"", i.e., by what metrics is something a breakthrough search algorithm?
2. What constitutes ""classical computing""?",hi59ubj,t3_qgc7oj,1635273024.0,False
qgc7oj,"Breakthrough was a bit unnecessary, I’m just looking for an early or the first search algorithm. By classical computing I’m talking about basically anything that isn’t quantum computing",hi5bvhc,t1_hi59ubj,1635273826.0,True
qgc7oj,"Search algorithms predate computers as we know them, which is why it would still depend on the specifics. For example, depth first search was first created by a French mathematician and is probably one of the first search algorithms as we know them. It is still used extensively either directly or as the basis for other search algorithms; however, it isn't very efficient in many cases. So does this count? It is possible to argue both yes and no.

&#x200B;

[https://en.wikipedia.org/wiki/Tr%C3%A9maux\_tree](https://en.wikipedia.org/wiki/Tr%C3%A9maux_tree)",hi5cjcd,t1_hi5bvhc,1635274090.0,False
qgc7oj,I’m gunna put linear search ahead of a DFS.  People have been looking down lists as long as there have been lists to look down.,hi5jez3,t1_hi5cjcd,1635276759.0,False
qgc7oj,"Binary search also probably came first. If you are looking for a word in the dictionary you will automatically do a shoddy binary search for the word, so I assume that was formalized in math a long time ago",hi5pniy,t1_hi5jez3,1635279141.0,False
qgc7oj,"Well, it depends. This is why the question is difficult to answer precisely without establishing the terms. Much of the analysis of these algorithms as we know them today (in a computational sense) came about in the 1940s-1960s. For example, John Mauchly did the first real analysis of binary search as a computational algorithm in 1946. Of course, it is based on notions that might be much older but it is difficult to pinpoint them to a specific point in history as an algorithmic process (again as we know such things today). The same is true for linear search. Tramaux's work was really important in laying the groundwork for the development of algorithms as we think of them. The question, as posed, is somewhat unanswerable. An argument could be made for a lot of different ways to search. I mean evolutionary search predates almost any other search mechanism on Earth by millions of years, but one would be hard-pressed to call it an algorithm as we think of it, even if we are inspired by it today.",hi7hxtm,t1_hi5pniy,1635307960.0,False
qfwbyw,"The main mathematics you need for AI:

\*Linear algebra (essential to understand most ML / AI approaches)

\*Basic differential calculus (with some multivariable calculus)

\*Coordinate transformation and nonlinear transformations (key ideas in ML / AI)

\*Linear and higher order regression (making predictions based on existing datasets)

\*Logistic regression (classifying a data as one thing or another)

\*Numerical analysis (because converting mathematical formulas into reliable and running code is often harder than you think)

\*Basic statistics (ML / AI use many statistics concepts).

If you're asking for a specific path, I really don't know. I recommend you look at the undergraduate curriculum of the [AI program at Carnegie Mellon,](https://www.cs.cmu.edu/bs-in-artificial-intelligence/) which is the best in the world in Artificial Intelligence and  I believe the only one in the United States that offers AI degrees at this level.",hi2hjst,t3_qfwbyw,1635216582.0,False
qfwbyw,"As a grad student studying AI, this. All of this. You may want to get familiar with signal processing (particularly  Fourier transforms) as well if you want to do some advanced things with computer vision, audio, or other time series data.",hi2hw4x,t1_hi2hjst,1635216754.0,False
qfwbyw,"First of all, agree 100% on the math.

But is Carnegie Mellon (Thrun) really (currently) the best in the world in AI? Better than University of Toronto (Hinton), University of Montreal (Bengio, Goodfellow), Stanford (Fei-Fei Li, Andrej Karpathy, Andrew Ng), Berkeley (Russell, Norvig, Jordan) and MIT CSAIL (Minsky, Kurzweil)?",hi391de,t1_hi2hjst,1635235565.0,False
qfwbyw,[removed],hi2wlb2,t1_hi2hjst,1635225486.0,False
qfwbyw,You need to what?,hi320p9,t1_hi2wlb2,1635229589.0,False
qfwbyw,"> Isn't htis part of LInear algebra?

The linear ones are. The nonlinear ones aren't. Like the name says.",hi4kb4z,t1_hi2wlb2,1635262955.0,False
qfwbyw,"This is great info, thank you sir",hi2j7dg,t1_hi2hjst,1635217416.0,True
qfwbyw,Excellent list and I agree with all of it. I'll also just add some probability theory to it. A basic understanding of MLE and probability distributions is very useful although I think we can assume that these topics will be covered in the statistics course you've mentioned.,hi49471,t1_hi2hjst,1635258376.0,False
qfwbyw,"This is a good list. The biggest ones are linear algebra, probability, and all of the calculus. For cs in general though the more math you study the better. The ideal of math to know is ""all of it.""",hi5sjxu,t1_hi2hjst,1635280256.0,False
qfwbyw,"You need to know linear algebra very well so getting advanced concepts and tons of practice from basics to advanced is more than necessary.

You need to know calculus very well.

You will also need to know probability very well. Statistics very well. 

You also need to know about computing, graphs, and algorithms. 

It’s a good time, one of thise work hard play hard areas. =) 

Check out mitocw if you have not done so. MIT literally provides free course lectures and materials",hi2j6ur,t3_qfwbyw,1635217409.0,False
qfwbyw,"graph theory (Dijkstra's algorithm, traveling postman etc).

differentiation and integration.

matrix theory.

these are parts of Discrete Mathematics, so a text book on this topic would be ideal.",hi3fx9k,t3_qfwbyw,1635241465.0,False
qfwbyw,https://mml-book.github.io/book/mml-book.pdf,hi3ajob,t3_qfwbyw,1635236927.0,False
qfwbyw,"For the linear algebra, I can recommend the 3Blue1Brown video series: https://www.3blue1brown.com/topics/linear-algebra

Also, Andrew Ngs free course on Coursera walks through the math as it goes along, not as intuitive as 3b1b but all in the context of machine learning algorithms.",hi37wky,t3_qfwbyw,1635234551.0,False
qfwbyw,"This is an atypical learning path, but I think it's good. It's also textbook heavy, cus tbh I have no idea how people learn things from watching youtube.

First, read ""Linear Algebra through Geometry"". I've worked with people in ""AI"" (computer vision) who are brilliant coders and on paper linear algebra whizzes, but don't have a geometrical intuition in their body. This might be an over generalisation but at this point AI is just making things closer to each other in an N dimensional space. 

Then, read some intro calc book. I've never found one satisfying so look elsewhere for this one. You really don't need to go that deep, but you should intuitively understand derivatives and integration fairly well. 

Now, read Algorithms for Optimisation. 

(Given that you say you have time, I'm recommending the following two, but if you're reading this and you just want a job ASAP honestly you could probably skip them for the moment. But DON'T SKIP THEM FOREVER).

Now read Rudin. Welcome to math.

Now read a stats textbook. Casella and Berger if you wanna get dirty. 

Vector calculus by Hubbard and Hubbard is dope but not strictly needed. 

Now IMO you're ready to get into the ML. A good option is intro to statistical learning. If you've followed the path so far, it'll be pretty straightforward. Breaking with my thread here, I'd say do the coursework and watch all the lectures on cs231n Stanfords ML class. The material is available online and it's pretty dope. 

After that, you'll know where to go. Also, if you get through all that, the field will be vastly different by then, so ask again!",hi46wtq,t3_qfwbyw,1635257430.0,False
qfwbyw,"ML is mostly regressions and its called AI for some silly reason.

Pathfinding algorithms used to be considered AI but aren't anymore.

If you want to do video games, video game AI is just if then else type stuff (discrete state machine).

The thing to remember about AI is that its not that impressive once you get to the other side. You start to see how ""artificial"" it is. I have less faith in driverless cars being adopted widespread in my lifetime.

Biological intelligence is far superior in reacting to things not in a data set. AI is still garbage in, garbage out.",hi3mynn,t3_qfwbyw,1635246859.0,False
qfwbyw,anything with numbers and some of the stuff with edges and vertices,hi30eic,t3_qfwbyw,1635228317.0,False
qfwbyw,Start with linear algebra and stats as that will provide the most benefit there’s also books like math for ml free online which are catered to getting down the required math skills,hi59ghk,t3_qfwbyw,1635272873.0,False
qfwbyw,This inspired me to start learning again too!,hi59q0j,t3_qfwbyw,1635272977.0,False
qfwbyw,I’m taking a course titled AI this year in university. I can send you the course outline which lists all the topics we cover. You can then research them from there if you like.,hi6999e,t3_qfwbyw,1635287087.0,False
qfwbyw,"Start with calculus and linear algebra.

On your way learn some statistics as well.

For most applications this should be enough to get you going.

Learn the rest as you go.",hiae43c,t3_qfwbyw,1635364185.0,False
qfwbyw,Probabilities and some statistics.,hiraytf,t3_qfwbyw,1635684150.0,False
qfhy4b,"Algorithm is precisely defined series of steps that need to be performed to get the appropriate output for an given input. Emphasis on ""precisely defined"" and ""appropriate output"" - all cases must be covered and for every input algorithm must produce valid output. So, first characteristic of good algorithm is its [correctness](https://en.wikipedia.org/wiki/Correctness_(computer_science)).

Now we can talk about algorithm efficiency. There are different types of it, but most common are [time complexity](https://en.wikipedia.org/wiki/Time_complexity) and [space (memory) complexity](https://en.wikipedia.org/wiki/Space_complexity). These Wikipedia articles cover those topics deeply so if you are interested do read them. 

Time complexity is expressed as a function of the size of the input. These functions can be classified using [asymptotic notation](https://www.codecademy.com/learn/cspath-asymptotic-notation/modules/cspath-asymptotic-notation/cheatsheet).",hhzt5nu,t3_qfhy4b,1635175224.0,False
qfhy4b,I really feel like this is best answer and I wish it had more upvotes.,hi0gl0a,t1_hhzt5nu,1635184855.0,False
qfhy4b,"Wish granted, I guess. It's the favorite answer by a landslide now.",hi14zxr,t1_hi0gl0a,1635194626.0,False
qfhy4b,"To dive deeper, which would you say is more important, worst-case complexities or average-case complexities? Is it use-case dependent?",hi19c4l,t1_hhzt5nu,1635196411.0,False
qfhy4b,"Pretty much. Bubble sort, for example, is extremely good when arrays are almost sorted and extremely useless otherwise. Average case is usually more important since it's *the average* but worst-case might be particularly horrific or unfortunately common in your use case.",hi2scju,t1_hi19c4l,1635222636.0,False
qfhy4b,Simple and clear,hi16apy,t1_hhzt5nu,1635195149.0,False
qfhy4b,After those most important characteristics I like the ones that appear to have a sense of beauty or elegance. Take for instance solving exact cover problems with Algorithm X using the dancing links technique. Just beautiful.,hi1mwgq,t1_hhzt5nu,1635202307.0,False
qfhy4b, /thread. Well said!,hi23kuq,t1_hhzt5nu,1635209990.0,False
qfhy4b,"Excellent response.

I'd add elegant simplicity, i.e. it's plain to see what is happening and no superfluous parts to it.",hi2f7jn,t1_hhzt5nu,1635215433.0,False
qfhy4b,"""Good"" is context sensitive. If you have space constraints, then an otherwise desirable algorithm that has an unacceptable space requirement isn't good for your needs, is it? If time is top priority, then you want to look at its time complexity. I have a book on parsing around here somewhere that goes into detail about a number of algorithms. I recall that one algorithm has a worst-case time complexity that is quadratic. The thing is, it's provable that in the parsing algorithm, you can never hit the worst case scenario. Or if you know something about your data, you might be able to pick an algorithm that has a specific complexity just for that.  Quick sort has a good average time complexity, but something more specific might be better.

And algorithmic complexity is only there to tell you the ""shape"" of the algorithm. Two algorithms that are both O( n^2 ) might have vastly different running times. Bubble sort is n^2, yet if your data set is small enough to fit in a single cache line, it literally doesn't matter what other algorithm you use, because you'll always be IO bound on the memory bus. A better algorithm, you'll still be waiting for a cache read or write. A constant time complexity is usually the goal, because the time requirements are understood perfectly, but if a bubble sort takes 2 hours to run for a given data set, that's still better than a given constant time algorithm that takes 3 years. So constant complexities aren't a panacea.

If you're looking at algorithmic complexity in terms of a practical application, it's a starting point, not the ending point.",hi0cc0j,t3_qfhy4b,1635183114.0,False
qfhy4b,"There's no universal definition of a good algorithm. It depends on the problem you are trying to solve and what you are trying to optimize (time complexity, space complexity, readability, etc.).",hi08l21,t3_qfhy4b,1635181579.0,False
qfhy4b,"I'm coming at this from an application development perspective.

If it meets the business need, its a good algorithm.

I would say that from a project management perspective, getting bogged down in any one algorithm is a bad idea. You generally should just see everything as ""get it all working first"" and then optimize.

Completing a working application is one of the most difficult things to do.

I'm just a lowly hobbyist with a game I made on the side, but I am about to submit it to the App store after 3 years of work. I think the main reason I got it to market where others don't is I have no problem coming out with garbage that works in the hope that I can come back to it and improve later.

Algorithms aren't really a major focus of application development, but games have tons. But in those, efficiency doesn't matter all that much. Your game isn't going to fail because a suboptimal pathfinding algorithm that could have been improved 2x. 10x maybe I guess.

The areas I can think off that algorithms matter are like people that make bot traders on Wall street. If I were one of them, I'd be sh\*tting my pants about each line because of the $ amounts involved.

If your doing JPL/NASA/Life critical you got to design things with that in mind. JPL has their own standards and its pretty strict - no mallocs and each loop has to have a definite end (no i<variable, it has to be like i<1000). They don't want someone to die in space because of a loop that doesn't terminate.",hi0dka9,t3_qfhy4b,1635183619.0,False
qfhy4b,"An algorithm is only good for certain tasks, there’s isn’t a sorting algorithm that’s best at sorting anything. Quick sort is good because most of its use cases end with a pretty efficient runtime BUT there’s still use cases where it does poorly; this is where you would use a different sorting algorithm that would normally perform very poorly compared to quick sort but in this particular instance it performs exceptionally well.",hhzr36s,t3_qfhy4b,1635174327.0,False
qfhy4b,"correct me if im wrong; but, divide and conquer has the best time order for large unordered sets.",hi1bsvg,t1_hhzr36s,1635197460.0,False
qfhy4b,"Again, it's context dependent. Any comparison sort can't be faster than O(nlog(n)), but there are faster sorts for other use cases, such as radix or counting sorts. Or perhaps you're very much memory constrained, and have to do everything in place. Some of those algorithms get much more difficult to implement. Context is everything with algorithms.",hi1pyv5,t1_hi1bsvg,1635203701.0,False
qfhy4b,"This is very true, efficiency isn’t everything with an algorithm: memory consumption plays a huge part in whether or not to use it as well.",hi231ze,t1_hi1pyv5,1635209745.0,False
qfhy4b,"Yeah you’re right, it is about average case performance. An algorithms performance is dependent on the context in which you’re implementing said algorithm. Learn about runtimes and Big O notation.",hhzonvw,t3_qfhy4b,1635173276.0,False
qfhy4b,Worst case could also be more important especially for algorithms with large inputs. It depends on the situation whether an algorithm is good or not.,hi07tq6,t1_hhzonvw,1635181264.0,False
qfhy4b,Yeah I'm a novice and was doing some hackerrank problems.  I was doing a problem where you have to check if a number and its reverse are evenly divisible by k.  I assumed that I might save time by just accounting for any reverse number of i upon iteration i if reverse i was in the array as well.  Maybe I did it poorly but just simply checking them individually timed faster on my pc.  I was thinking maybe it was dependent on the array size though.,hi0cmx9,t1_hi07tq6,1635183238.0,False
qfhy4b,Searching for an item in an (unsorted) array is linear time which if you put it in a loop turns into quadratic. A set is better since it offers near constant time lookup.,hi0fgi1,t1_hi0cmx9,1635184394.0,False
qfhy4b,Would say focus is on worst case in the CS field. Average case concerns is more for real-world stuff. :-),hi0bkr4,t1_hhzonvw,1635182807.0,False
qfhy4b,Good point!!,hi0dh9y,t1_hi0bkr4,1635183584.0,False
qfhy4b,"Space complexity is not a big deal when it comes to *algorithms*. How much memory your algorithm uses is a concern, but not as concerning as the time your algorithm needs to execute. So I think that time complexity comes first.",hi0398q,t3_qfhy4b,1635179381.0,False
qfhy4b,"Well you have to look at what it does, and then how it does that in comparison to other ways of doing it. Is it more or less efficient? If so in what ways? Is it possible it is sacrificing something, e.g. memory, in order save more on computation?

Sometimes you have lots of resources. Sometimes you're building something for a small device with battery, storage, compute etc limitations.

Part of being an engineer is to be able to make those kinds of design choices.",hi0fjkp,t3_qfhy4b,1635184428.0,False
qfhy4b,"It actually very complicated to determine if an algorithm is good. This is why people study computer science.

It is like asking how to determine if a plane is good. Or if this particle accelerator is good. Or if this surgery is good.",hi0jgsk,t3_qfhy4b,1635186036.0,False
qfhy4b,"you can look at space and time complexity which is the most obvious first step. but additionally algorithms can be suited to expected inputs. for instance tim sort isn't a particularly fast algorithm when it comes to its average constant factor on random data. however, tim sort is very very fast when subsets of the list have been pre-sorted. in python this helps speed things up for data science applications where you might combine pools of already sorted data. 

algorithms are good because they suit an application correctly",hi0mkbv,t3_qfhy4b,1635187292.0,False
qfhy4b,"logarithmic time order

O(n) = log(n)",hi1be7y,t3_qfhy4b,1635197288.0,False
qfhy4b,What is 'good'?,hi1xg4f,t3_qfhy4b,1635207178.0,False
qfhy4b,"Before I get into my comment, I want to preface that I'm a beginner too, so whatever I say here, definitely fact-check me and make sure I'm right!

Good depends on your usage case. Time and space complexity are asymptotic and have to do with the performance of algorithms especially when dealing with inputs of very large size or when you're doing an operation many, many times. For example, quicksort and mergesort are similar if we just look at the time complexities, but depending on the data we're dealing with, quicksort or mergesort could be faster than the other. On the data structures side, we can also see that a hash table implemented using linear/quadratic probing (to account for collisions) versus one using chaining can be better, despite them having similar performance characteristics -- and this is because, on the machine level, a hash table using probing can take advantage of caching to be faster. So there are many things to be considered when working with algorithms, like the kind of inputs you'll be feeding them (consider more than just their size) and the machine that you're working with.

There's also more complicated stuff to consider when you're dealing with amortized algorithms and datastructures, which (overall) can be efficient, but in certain cases can experience slowdowns that might be a negative (i.e. real-time systems with hard deadlines, I believe).",hi28bqx,t3_qfhy4b,1635212179.0,False
qfhy4b,If it has a significantly better time complexity than existing soloutions,hi2u14o,t3_qfhy4b,1635223715.0,False
qfhy4b,"You want to research Big-O, AKA algorithm effeciency.

Link: https://towardsdatascience.com/introduction-to-big-o-notation-820d2e25d3fd

That is what we had to learn in school. You can kinda rough estimate things by eyeball once you practice a bit.",hi2yaim,t3_qfhy4b,1635226719.0,False
qfhy4b,"Definitive: An algorithm should have well defined input and output values. If you add two numbers, you should get a number, not a character. 

Exhaustive: a good algorithm needs to work in all cases (or have as few edge cases as possible). For example if you had a sorting algorithm that could only sort 10 elements than that algorithm is not as useful as one that can sort arbitrary arrays.

Stable: an algorithm is stable, if given a small change in input, you have a small change in output. An unstable algorithm is the YouTube algorithm. It shouldn't be the case that if I play a song for my 4 year old cousin that then I should get children videos 2 weeks after.

Fast: being able to work on modern computers well. This usually boils down to having low complexity but it's not enough as low complexity could still mean slow implementation on actual computers.",hi4gue1,t3_qfhy4b,1635261554.0,False
qfwfbn,"I feel like you're answering your own question. More power means scaling everything else up (not just for heat dissipation - a high powered CPU doesn't really mean anything when the system that uses it becomes its own bottleneck). And scaling everything else up defeats the purpose of a raspberry pi.

In reality though, a raspberry pi is overkill for a lot of projects that people want to use a raspberry pi for. It's a great learning tool but if the goal is efficiency when it comes to power and memory, there are much better options out there (they're just harder to get working).",hi33nq5,t3_qfwfbn,1635230917.0,False
qfwfbn,"I'm curious, what kind of options? Can you provide any links? Thanks in advance!",hi37vfv,t1_hi33nq5,1635234524.0,False
qfwfbn,Arduino and similar microcontrollers are enough for a lot of projects,hi3891d,t1_hi37vfv,1635234857.0,False
qfwfbn,"As the other reply mentioned - arduinos are good kits. But even these can be overkill. As always, it all depends on your use case. If you had a simple IOT project that used very little power and required very little memory, something like this would make a lot more sense than a Pi or Arduino: https://en.wikipedia.org/wiki/ESP32

But the learning curve for these is higher, so starting with an Arduino or Pi makes a lot more sense in the beginning.

r/microcontrollers and r/embedded are good places to look for more information.",hi9u9ey,t1_hi37vfv,1635356518.0,False
qfwfbn,Really interesting stuff. Thanks!,hiaqaiu,t1_hi9u9ey,1635368834.0,False
qfwfbn,"It doesn’t need it?

Same reason a Toyota Corolla doesn’t have a 6L V8… Different use cases",hi2y5cn,t3_qfwfbn,1635226612.0,False
qfwfbn,cpu demands are main reason raspberry pi doesn't incorporate a better cpu,hi2zq4t,t3_qfwfbn,1635227789.0,False
qg1nhc,"Adaptive processing is already a thing, though its primary concern at the moment is scaling of, and access to, resources such as caches.",hi4sqbn,t3_qg1nhc,1635266301.0,False
qg1nhc,self-modifying fpga :thinkface:,hi5dtfo,t3_qg1nhc,1635274586.0,False
qfnew0,"Damn, a few upvotes but no answers :/ must be a problem a lot of us are running in to.",hi2tb9e,t3_qfnew0,1635223249.0,True
qfnew0,"Yeah, sadly this is often a killer before even hitting the keyboard for me. Data is expensive and you get what you pay for.

While the veracity can't be guaranteed, I've spotted a few decent crowd-sourced data repositories on GitHub. Not much use if you need live data of course, but it's something!",hi8jf9z,t1_hi2tb9e,1635336012.0,False
qfnew0,"Thank you! The only other thing I can think of is building a web scraper for what ever it is I would need but that doesn't seem reliable as a single change to the webpage could break everything.. Maybe the best route is to create a virtual service to mock the data and then when your ready, pony up and pay for the real thing.",hi939e8,t1_hi8jf9z,1635345664.0,True
qfnew0,use google images as training data to guarantee bias in your model. 11/10 best free training set,hi30byg,t3_qfnew0,1635228263.0,False
qfah2t,[deleted],hhz3zpo,t3_qfah2t,1635162581.0,False
qfah2t,Yes. Crash course is fantastic,hhzmgt3,t1_hhz3zpo,1635172323.0,False
qfah2t,Crash course is so good for getting a basic understanding on how computers work at a very low level.,hhzti92,t1_hhzmgt3,1635175373.0,False
qfah2t,"Looking for the same thing. In all of my conversations with programmers, I find that the most difficult thing for me is establishing a common vernacular. Working on the product testing end of an IoT company, I ran into engineers who knew only how to speak their own “language”",hhyi9me,t3_qfah2t,1635144336.0,False
qfah2t,"If they only know how to speak thier own language, chances are they themselves don't know what they're talking about exactly.",hhypzda,t1_hhyi9me,1635151031.0,False
qfah2t,Ben eater,hhyvy0x,t3_qfah2t,1635156437.0,False
qfah2t,"Wanted to recommend his Playlist, but then I realized that the question is about terminology, not technology...",hhzeb0o,t1_hhyvy0x,1635168487.0,False
qfah2t,Damn I misread. Thanks.,hi9r3kd,t1_hhzeb0o,1635355277.0,False
qfah2t,People here are answering from a computer science point of view. But are you just talking about a normal user who wants to use things like Microsoft Word and web sites?,hhzuu5i,t3_qfah2t,1635175940.0,False
qfah2t,8 bit guy,hhyq30v,t3_qfah2t,1635151125.0,False
qfah2t,Mention few course and youtube channels to get in touch with it,hhyg6gu,t3_qfah2t,1635142625.0,True
qfah2t,https://youtube.com/playlist?list=PL96C35uN7xGLLeET0dOWaKHkAlPsrkcha,hhzz7s2,t3_qfah2t,1635177734.0,False
qftnw9,"Imagine a situation: you are having a problem you don't really understand. Instead jumping into coding (why on earth are people still doing that, I have absolutely no idea) first you need to gain insight and understanding. To understand, you need to remove the unnecessary parts, and start to make an abstraction of the problem. In computer science, we have plenty of such abstractions, including: concept modeling, process modeling, state machines, graph theory, physical models (F=m\*a is also a model), petri nets,  etc. What we want to do is in order to solve the problem, we transform it into one (or multiple) of well known abstraction, because for those abstractions we have a huge amount of tools.  


Imagine a case when you need to develop a software which calculates traveling times from cities to cities. This problem can be abstracted to a graph theory problem: finding shortest paths. Imagine a case when you need to write a calculator with different modes. Dang, this is actually a state machine. Imagine there are lots of various nouns in a long description where you are absolutely lost (like we were when we had to create a document for an audit). Then you you concept modeling when you model the various concepts and the connections between them. Or you need to understand a process and people ask you what if there are too many customers standing in the line, how will our system react to it? Then you have a way to describe it (like activity diagram, or bpmn, etc.), or transform it to some kind of markov chain and give insights. 

The best part is that if you have an abstraction, a model, you can reason for or against it, you can even simulate how the system will work without the need of building it.   


This is why we do modeling.   


https://www.sebokwiki.org/wiki/What\_is\_a\_Model%3F",hi2tu31,t3_qftnw9,1635223587.0,False
qftnw9,draw a picture of how it works to do a reality check,hi3000y,t3_qftnw9,1635228003.0,False
qfgg3g,https://stackoverflow.com/questions/3980416/time-complexity-of-euclids-algorithm,hiaehgq,t3_qfgg3g,1635364326.0,False
qfhaf7,"This might be better suited for a computer engineering subreddit, but I'll answer the best I can. My understanding is some simulators treat two different inputs going into the same wire like an OR, and others just give an error. [Here's an example in Logisim](https://i.imgur.com/4zITit5.png) where two input pins of opposite value result in an error. Not sure what happens in real life, but this probably results in some sort of unpredictable behavior. I don't think I've ever seen a circuit diagram with two wires connected like that without an OR. Couldn't find anything else on this topic though.",hhzy6a3,t3_qfhaf7,1635177308.0,False
qfhaf7,"That's great, thank you. I think I will repost it to a computer engineering subreddit, but your answer makes sense",hi022g8,t1_hhzy6a3,1635178893.0,True
qfhaf7,"Shifters are generally constructed from multiplexers. A textbook example of a 2-to-1 multiplexer selects the value of one out of two inputs like this:

    q = (a & ~sel) | (b & sel);

The OR gate propagates a one from the selected input to q.",hi0f2cn,t3_qfhaf7,1635184232.0,False
qfhaf7,"You can't drive a net with multiple sources, hence you need to OR the signal so that if one is a zero and the other is a 1 you get only one output value.",hi0m8j8,t3_qfhaf7,1635187159.0,False
qeyub5,"Ecology may have some examples for you. Like the way certain trees form relationships with fungi to re-distribute nitrogen, potassium to their saplings. I wouldn't consider them the same thing, but I think there are probably a few different layers that could be further compared with the OSI model.

&#x200B;

[https://www.youtube.com/watch?v=-8SORM4dYG8](https://www.youtube.com/watch?v=-8SORM4dYG8)",hhwc7yt,t3_qeyub5,1635103640.0,False
qeyub5,That's really fascinating. Thanks a lot!,hhwp0uv,t1_hhwc7yt,1635108841.0,True
qeyub5,There’s also the anternet https://www.nbcnews.com/tech/tech-news/ant-inspired-internet-anternet-may-be-coming-soon-flna966571,hhxoqpv,t1_hhwp0uv,1635125883.0,False
qeyub5,https://en.wikipedia.org/wiki/Emergence,hhwolu5,t3_qeyub5,1635108673.0,False
qeyub5,"**[Emergence](https://en.wikipedia.org/wiki/Emergence)** 
 
 >In philosophy, systems theory, science, and art, emergence occurs when an entity is observed to have properties its parts do not have on their own, properties or behaviors which emerge only when the parts interact in a wider whole. Emergence plays a central role in theories of integrative levels and of complex systems. For instance, the phenomenon of life as studied in biology is an emergent property of chemistry, and many psychological phenomena are known to emerge from underlying neurobiological processes. In philosophy, theories that emphasize emergent properties have been called emergentism.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hhwonhn,t1_hhwolu5,1635108692.0,False
qeyub5,Good bot!,hhwpmlp,t1_hhwonhn,1635109094.0,True
qeyub5,Wow thanks for blowing my mind!,hhwpvxt,t1_hhwolu5,1635109205.0,True
qeyub5,You should also look into the Game of Life.,hhwyihw,t1_hhwpvxt,1635112986.0,False
qeyub5,"Technically, *any* form of communication could fit into the OSI model (albeit sometimes with a little creativity) since OSI is a *conceptual framework*. People tend to think about it with regards to modern computer networking, but that's just a specific *implementation* of the model. Think back when we only had the telegraph for long distance communication. I'm sure you can easily see the 7 layers of the OSI model in that implementation if you think about it.",hhxihgb,t3_qeyub5,1635122721.0,False
qeyub5,"When I make a mistake, that's a fault at layer 8",hhxgcf3,t3_qeyub5,1635121656.0,False
qebtrg,"Some fun and/or useful ones off the top of my head:

Bubble sort! The traditional ""bad algorithm"".

Bresenham's line drawing algorithm.

Dijkstra's shunting-yard algorithm.

Mark-and-sweep.

The Mandelbrot fractal.

Binary search.

Recursive descent parsing.

Distance field raymarching.",hhs3pzj,t3_qebtrg,1635021117.0,False
qebtrg,"Also take a look at the **Window Sliding Technique** and **Kadane’s Algorithm**.  
These two have helped me solve some subarray problems MUCH faster than my brute force solutions to those type of problems. Also came up in an interview once.",hhsmw5g,t1_hhs3pzj,1635029974.0,False
qebtrg,"I've met Jay Kadane! He's a badass!

I asked him about the history of the maximum sub-array problem. I knew about Bentley presenting the problem and this is roughly his response:


JK: ""You ever de-bone a trout?""

Me: ""Nope""

JK: ""Well you can either pick out the pin bones individually or you can remove the backbone and pin bones in one shot. It was like they were individually picking out the pin bones.""


Very cool!",hhv48v3,t1_hhsmw5g,1635085324.0,False
qebtrg,"I’ll be honest, that’s such a wild way to go about describing the history of that algorithm, I did NOT see that coming! What an absolute legend!",hhvgplh,t1_hhv48v3,1635090959.0,False
qebtrg,Actually learned about line drawing algorithms in my graphics class this semester. Really interesting stuff.,hhtlv6a,t1_hhs3pzj,1635047067.0,False
qebtrg,I kinda love bubble sort being in this list.,hhstei2,t1_hhs3pzj,1635033054.0,False
qebtrg,"Along with binary search I think it's one of the best for introducing some solid computing fundamentals, even to people outside the field in general.",hhu14a5,t1_hhstei2,1635057111.0,False
qebtrg,I think it's just the natural sorting algorithm. Like as a child I would sort numbered blocks by picking up 2 and putting them in the right place and move to the next number,hi0keny,t1_hhu14a5,1635186420.0,False
qebtrg,shunting-yard makes parsing calculator or logical functions sooo much easier,hhu1li6,t1_hhs3pzj,1635057479.0,False
qebtrg,What can we use the MandleBrot for in the real world,hhu6me3,t1_hhs3pzj,1635061568.0,False
qebtrg,"Most valuable is the INSIGHT, how simple algorithms can produce most complex results.",hhu76as,t1_hhu6me3,1635062037.0,False
qebtrg,I’ve just always thought it’s good for making a cool design I didn’t know there’s a use case for it,hhu794l,t1_hhu76as,1635062105.0,False
qebtrg,Making cool images and animations.,hhuc95x,t1_hhu6me3,1635066318.0,False
qebtrg,"I'd figure search and sort algorithms are a good start, but make sure to know about Big O notation along side it",hhrwlpv,t3_qebtrg,1635017955.0,False
qebtrg,"Yeah, probably not much news to anyone here, but without knowledge of the Big O time complexity, if I had to pick between a 5 line Bubble Sort or multi-line Timsort... I'd probably go for the bubbles.",hi8jzho,t1_hhrwlpv,1635336342.0,False
qebtrg,"Binary search, radix and merge sort, breadth and depth first tree traversal, A*, Dijkstras algorithm, KMP and Boyer-Moore string search, and maybe counting sort for an example of a great hack that challenges what you thought you knew about sorting complexity.

Also for good measure it’s fun to take a deeper look at RSA, search indexing, neural nets (and gradient descent), and randomized algorithms.

Man I love algorithms and data structures. So much fun and still so much more to learn and explore.",hhs3mkr,t3_qebtrg,1635021076.0,False
qebtrg,[deleted],hhsst4e,t3_qebtrg,1635032771.0,False
qebtrg,Same found it super useful,hhszu5b,t1_hhsst4e,1635036060.0,False
qebtrg,"My use case was where we wanted users to be able to search a list and return a close match accounting for spelling errors, without using a like or wildcard match.

The length of the distance was determined by the length of the search string to prevent enumerating the entire list.",hht5lan,t1_hhszu5b,1635038835.0,False
qebtrg,Literally did the same thing with a company that had two separate databases that needed to be joined on customer name. It was painful.,hi95z52,t1_hht5lan,1635346770.0,False
qebtrg,Merge sort,hhs1mdm,t3_qebtrg,1635020190.0,False
qebtrg,"Branch and Bound techniques are widely applicable but much unused!

Techniques like A* are actually instances of a much more general set of algorithms loosely amenable to a search.

I find that some people solve almost everything with branch and bound while others (most people) rarely use them at-all.

Tasks like optimal voxel meshing or optimal triangle strip ordering are powerfully useful examples where branch and bound is king.",hhs0tqq,t3_qebtrg,1635019840.0,False
qebtrg,dfs,hhsq3ec,t3_qebtrg,1635031483.0,False
qebtrg,Least squares regression.,hhsvvrh,t3_qebtrg,1635034238.0,False
qebtrg,"Bogosort: considering various arrays, the optimistic case is the fastest of all other algorithms. So if you feel lucky, try it out!",hhud7xu,t3_qebtrg,1635067160.0,False
qebtrg,"I don't know that there are algorithms *everyone* should know, more that there are algorithms everyone doing certain things should know - and even there the sorts of things you need to 'know' in order to 'know' an algorithm are very different.
 
Sedgewick is the classic foundation. Knuth if you are doing real computer science rather than software development. But then there are a bunch of different ways to go from there - parallel computing algorithms, distributed computing algorithms, database specialised algorithms, data processing algorithms, various algorithms that prioritise certain things that are *not* cpu processing time (memory, reliability, security, network utilisation, power consumption).
 
Both Sedgewick and Knuth are a bit of a slog though. You could go with [Algorithm Design](https://theswissbay.ch/pdf/Gentoomen%20Library/Algorithms/Algorithm%20Design%20-%20John%20Kleinberg%20-%20%C3%89va%20Tardos.pdf) or [Algorithm Design and Application](https://canvas.projekti.info/ebooks/Algorithm%20Design%20and%20Applications%5BA4%5D.pdf), for a different kind of slog.
 
The day they release a Head First Data Structures & Algorithms will be the day I agree to teach a data structures and algorithms class... until then those classes will always be a friggin nightmare.",hhu41o8,t3_qebtrg,1635059439.0,False
qebtrg,I always found Knuth absolutely unreadable.,hhu7c3z,t1_hhu41o8,1635062176.0,False
qebtrg,"Have you checked out ""Grokking Algorithms""? It's not a head first and pretty limited in its content but it follows a similar idea.",hhyr5ek,t1_hhu41o8,1635152110.0,False
qebtrg,You can iterate down a list and get the top X elements in O(N) complexity. I've been asked that on 3 different job interviews so far.,hhunl9v,t3_qebtrg,1635075819.0,False
qebtrg,you have piqued my curiosity. At first I thought it would be X linear selects but that would be O(XN). How do you do this?,hhuwy4b,t1_hhunl9v,1635081593.0,False
qebtrg,I think quick select for the Xth largest and then scan through for elements bigger than that works.,hhv2qcs,t1_hhuwy4b,1635084589.0,False
qebtrg,Right.. how did I not think of that hahaha,hhv6ecq,t1_hhv2qcs,1635086345.0,False
qebtrg,Bogo sort,hhvhsy5,t3_qebtrg,1635091431.0,False
qebtrg,Stalin sort,hhsceuo,t3_qebtrg,1635025119.0,False
qebtrg,Remove the population until you're left with an empty (sorted) list?,hht1mmp,t1_hhsceuo,1635036910.0,False
qebtrg,remove everything out of order. the remaining items are sorted,hht5f9l,t1_hht1mmp,1635038753.0,False
qebtrg,"Well. If you just need a sample, that is O(n).",hhu9jet,t1_hht5f9l,1635064062.0,False
qebtrg,This is big O of what the leader wants it to be and nothing else!,hhud0s1,t1_hhu9jet,1635066984.0,False
qebtrg,Bubble tea. (Sic),hhtfdlb,t3_qebtrg,1635043628.0,False
qebtrg,Linear sum assignment,hhtm06h,t3_qebtrg,1635047137.0,False
qebtrg,Definitely berlekamp massey /s,hhtne5d,t3_qebtrg,1635047893.0,False
qebtrg,"bogo sort (aka monkey sort)

sleep sort",hhulfae,t3_qebtrg,1635074175.0,False
qebtrg,"I'm going to add BFS/DFS (personally prefer BFS cuz it's easier to implement without recursion), because I have had to use it in several places, both on online coding challenges as well as my own school project.",hhuwsa6,t3_qebtrg,1635081500.0,False
qebtrg,Metropolis hastings if ya nasty,hhverth,t3_qebtrg,1635090131.0,False
qebtrg,Fast Fourier Transform,hhw673h,t3_qebtrg,1635101205.0,False
qebtrg,Stack heap.,hirb4fs,t3_qebtrg,1635684248.0,False
qewt85,"The basic idea for all counting problems like this, is to find the recurrence relationship.  Your going to have a tree that looks like this Root (tree a) (tree b) (tree c)...   you want to start building your tree, until you have all subtrees that ""look like"" the whole tree, only smaller. Then you'll get a reccurence relationshiop of the form T(n) = O(1) + T(n-a) + T(n-b)  + T(n-c)...  

Solving the recurrence relationship gives you T(n).

Here's a simple example.  Consider the language with symbols a and b sth there are no two bs next to each other.  (ie. aaabaaa is allowed, but not abbaaa).

With a string of length n, we can right the tree as a(tree of length n-1) + ba(tree of length n-2) so, the difference equation is T(n) = T(n-1) + T(n-2), which gives you the fibonacci sequence.",hhw6f0t,t3_qewt85,1635101292.0,False
qewt85,There are more complicated scenario like a+a+..+a with grammar expr->expr+expr which follows Catalan number's recurrence which is not even linear,hhxfsy5,t1_hhw6f0t,1635121381.0,False
qewt85,Yes... i was giving a simple example...,hhxkoh8,t1_hhxfsy5,1635123830.0,False
qebyt6,"Simply put (so likely a little inaccurate, even if it gets the point across), whenever you call a function inside another, the computer writes down what it was doing before calling the function and puts it on a pile. Then, when the called function completes, the computer can take that paper off the pile, read it, and pick up where it left off.

Now imagine your pile of papers is 10000 pages tall and tips over. That is a stack overflow.

It’s not bad to write recursive functions, but if the implementation is incorrect, you can easily cause an overflow. There are many best-in-class algorithms that are recursive; mainly you just have to make sure to never redo work and to always make progress (and always have a base case).",hhryd35,t3_qebyt6,1635018760.0,False
qebyt6,This is much better than the top answer. The stack is an abstraction that programming languages use to enable function calls.,hhsmtia,t1_hhryd35,1635029939.0,False
qebyt6,"Thanks for the clear explanation! So I guess this stack also includes stuff running in the background, like the OS, Firefox, File Explorer etc. So if you close down everything except your IDE, you should have more space on the stack, right?",hhrzlip,t1_hhryd35,1635019305.0,True
qebyt6,"No, each program has its own stack.",hhs1zju,t1_hhrzlip,1635020348.0,False
qebyt6,"Yep, how Jake Peralta put it, it's ""stacks on stacks on stacks""",hhsp8yd,t1_hhs1zju,1635031082.0,False
qebyt6,I'm a noob and is this anything related to virtual memory?,hhuoktn,t1_hhs1zju,1635076504.0,False
qebyt6,"No, virtual memory is using disk space as extra memory.",hhuv4x2,t1_hhuoktn,1635080553.0,False
qebyt6,"Virtually memory is not that; it’s a virtual address space that real memory (and other stuff depending on the computing architecture like ROMs) is mapped into, on a per process basis, and it’s related to stuff like memory protection. Virtual memory is often enabled by a hardware MMU.

What you  are describing is “swap”.",hhwiyg7,t1_hhuv4x2,1635106349.0,False
qebyt6,Thanks for the explanation.,hhuvaud,t1_hhuv4x2,1635080647.0,False
qebyt6,"Each thread(?) has its own stack. Each function call pushes the current context onto the stack, executes the function, and then pops the context off the stack to continue where it left off. If the functions keep calling more functions, usually when recursive, you can run out of stack.",hhs2tem,t3_qebyt6,1635020722.0,False
qebyt6,The Stack is an area in memory that is a stack data structure. https://www.sciencedirect.com/topics/engineering/stack-memory,hhry34s,t3_qebyt6,1635018632.0,False
qebyt6,"Thanks, I'll try to give this a read.",hhrzo4p,t1_hhry34s,1635019336.0,True
qebyt6,[OSTEP](https://pages.cs.wisc.edu/~remzi/OSTEP/) for if you really want a deep dive,hhsqobj,t1_hhrzo4p,1635031761.0,False
qebyt6,"Every time a function is “called” the system sets up what is called a “frame” which contains all the information the function needs to run (the arguments, local variables, etc.) and it’s this frame that is added to the stack. When the function finishes, it returns a value to the previous function/frame and gets removed from the stack. If the code calls a function and there’s not enough room for another frame on the stack (which is stored in memory), this is what’s called a stack overflow.

Many languages have an optimization called “tail-call optimization”. What that does is, if the last thing a function does is call another function, then it removes the frame from the stack before calling that function because in that case it does not need to be there. With that in place, if you write your recursive function in such a way that calling itself is the last thing it does, then stack overflow is no longer going to happen. So, “it’s bad” to write recursive functions only if you do t know what you’re doing.",hhsk7fa,t3_qebyt6,1635028723.0,False
qebyt6,"Is it still true that several popular languages, including python and Java, do not include tail call optimization?",hhsr86a,t1_hhsk7fa,1635032017.0,False
qebyt6,Two languages that I do not use. I’m not really sure.,hhsrlj4,t1_hhsr86a,1635032191.0,False
qebyt6,"Python and Java are specifications; neither cover this in their specifications.

The most popular implementations (CPython and OpenJDK respectively) do not currently provide TCR. This does not stop other implementations from providing it, however.",hhw3f3c,t1_hhsr86a,1635100109.0,False
qe1vmc,"I'll give you my intuitive definition:

AI is simply Artificial Intellingece, so anything that we could call intelligent behaviour that's not an act of living being, i.e. from machines.

Machine Leaening and Deep Learning are methods that can be used to implement AI.

Data Science is something that aims to make sense of data. It can achieve this using statistical methods or AI.",hhqdpuw,t3_qe1vmc,1634989191.0,False
qe1vmc,"And machine learning vs deep learning: machine learning is any technique that tries to use data to make a prediction about things it hasn't seen yet. There are lots of machine learning techniques, deep learning is just one of them. It uses deep neural networks (a class of algorithm) to do machine learning. Other algorithms include decision trees and support vector machines, among others (if you feel like googling them).",hhqvkve,t1_hhqdpuw,1635000438.0,False
qe1vmc,This too thanks,hhrx1tn,t1_hhqvkve,1635018157.0,True
qe1vmc,Noted,hhrx0qu,t1_hhqdpuw,1635018143.0,True
qe1vmc,"I think it's essentially:

Data science(base of most of ai research)

ML(implementation of data science to train an entity to predict data)

Deep learning(implantation of ML to learn (mimic) the same way that humans do)

AI(implementation of deep learning and ML and other methods to mimic intelligence itself)

It's kind of blurry at the AI level but AI is generally the combination of everything, while data science is generally the base of it all.",hhqvl9c,t3_qe1vmc,1635000444.0,False
qe1vmc,Appreciate the help thanks,hhrwwk2,t1_hhqvl9c,1635018091.0,True
qe1vmc,"Deep learning- learns it's self
Ml- needs to be taught
Ai- coding
Ds- extracting info from data, prediction of future.",hhqxhdb,t3_qe1vmc,1635001524.0,False
qe1vmc,Ok thank you,hhrwrv7,t1_hhqxhdb,1635018033.0,True
qe1vmc,"In other words. Deep learning is subset of ml, ml is subset of AI. and data science is playing with data.",hhssl2h,t1_hhrwrv7,1635032665.0,False
qe1vmc,"AI = systems imitating intelligent behavior.

ML = decision/diagnostic/prediction algorithms optimizing themselves by being ""trained"" by data.  This is a subset of AI.

DL = Methods that involve ""learning"" imitating how humans themselves learn (therefore, a subset of ML).

Data Science = Commercialized + rebranded statistics: includes statistics and things that enable statistics (adjacent tech, business processes, etc.).  Often, but does not necessarily, include using AI (often ML, and sometimes DL).",hhs0na5,t3_qe1vmc,1635019761.0,False
qe1vmc,"**AI** isn't really a strictly defined field, and the word can mean lots of things. Generally, though, it means software or systems that behave in a way that looks like it would require some kind of intelligence. This could mean a software system or a robot that's able to adapt to its environment rather than just following strict and narrow predefined patterns. It could mean something that mimics human behaviour; for example bots in multiplayer games are meant to do this, along with hopefully playing the game well enough. Software that performs a task which would appear to require intelligence or expertise if the task were performed by a human could be considered AI. This could be for example finding signs of disease or injury in medical images.

You could write at least an entire book chapter on what AI means. At its broadest, though, AI can mean any kind of an artificial system that behaves in a way that you could, by some stretch of imagination, call ""intelligent"".

Intelligent (or at least intelligent-looking) behaviour can be achieved using different kinds of techniques. Although AI is often used nearly synonymously with machine learning nowadays, AI is not really any single technique or even field of techniques. It literally just means something human-made (practically software) that acts as though it possessed some kind of intelligence, regardless of how that's technically achieved.

**Machine learning** studies algorithms that infer some kind of knowledge or behaviour based on examples it has been given. A classic example would be an image classifier: you feed the algorithm images that contain pedestrians (for example), along with the information that the images contain them. You also feed it negative examples, or images that don't contain pedestrians. The algorithm *learns* some kind of a model about which kinds of images contain pedestrians and which don't. After training the classifier with the samples, you can then feed it new images and it will hopefully be able to automatically tell whether a new image it's given has a pedestrian in it or not.

That's just an example, and there are other kinds of machine learning as well. The general gist is that ML is about algorithms that can automatically ""learn"" something from examples the algorithm has been given.

There are lots of different algorithms and techniques that can be used for machine learning. Some of those algorithms are more suitable for some tasks while others work better in other tasks. All of ML is essentially computational statistics, though: you feed some data into a machine learning algorithm, and the algorithm infers some kinds of patterns from the data. Some ML algorithms are obviously just statistics and probability math being done on a computer, while others seem more like some kind of magic that just happens to work for some reason.

One class of techniques that can be used in machine learning are artificial neural networks (ANN). The inspiration for them originally came from trying to mimic (a really simplified) idea of how human or animal neurons and brains were assumed to work. (In reality, ANNs don't actually resemble real brains that much.)

Without going into details, **deep learning** is just machine learning done with artificial neural networks that are larger (or ""deeper"") than the simpler networks that had been previously used. ""Deep"" ANNs are quite flexible in terms of what they can be trained for, and they can perform well in many tasks if they're trained with enough example data. However, they also require a lot of computational resources. Other, more traditional machine learning algorithms can be trained with much less processing power.

The connection between ML and AI is that ML is one of the most common techniques used for building ""intelligent"" systems. Automatically learning from examples is obviously useful for building software that can behave ""intelligently"". ML as a field arose from research into artificial intelligence, although many other kinds of techniques were also still used for trying to build intelligent systems at the time.

**Data science** is really just an umbrella term for all kinds of statistics, data analysis, and machine learning type things that are done computationally. It includes (computational) statistics, data mining (finding interesting patterns in data), machine learning, and doing all of this efficiently on a computer so that large amounts of data can be processed.",hhs67d4,t3_qe1vmc,1635022254.0,False
qebwa6,My goal is to upload videos to a feed with the users picture on the top left,hhrwsho,t3_qebwa6,1635018041.0,True
qebwa6,"I would say just do whatever works, then go back and try to optimize. I have no knowledge of the inner workings of any social media platforms but my guess would be that’s what they did ultimately, they got a working product then as it grew they went back and optimized with people that knew how to do that.",hhs0tys,t3_qebwa6,1635019843.0,False
qebwa6,Got you thank you for the response. I was thinking I'd probably end up doing it by trial and error since no one in the history of the universe has ever gotten their code bug free in the first try haha.,hhs43qf,t1_hhs0tys,1635021290.0,True
qebwa6,"Think about it this way, mark zuckerberg and jack dorsey were just college kids with a little bit of programming skills, they weren’t geniuses or anything so my guess is they just got the minimum viable product up and running then they went back later when they needed to scale and actually hired engineers to do that for them.",hhs9433,t1_hhs43qf,1635023594.0,False
qebwa6,I’m sure you could implement a social media platform based on just those ideas! Observer pattern is powerful. It would be a cool project to have under your belt.,hhscqzy,t3_qebwa6,1635025278.0,False
qebwa6,Awesome! Thank you for the response and insight if nothing else it will be a fun learning experience.,hhsgwo2,t1_hhscqzy,1635027191.0,True
qe36q2,Do you have the link?,hhqjnla,t3_qe36q2,1634993224.0,False
qdgm1o,"Algorithms (4th Edition) https://www.amazon.com/dp/032157351X/ref=cm_sw_r_cp_api_glt_fabc_V43KK74XWVV7F9EXNGNW

Read this book and program all examples and all questions in each chapter. 

Once you do a couple and see how the algorithm translates into code via the examples in the book then it’ll get easier",hhm888y,t3_qdgm1o,1634909134.0,False
qdgm1o,Thank you so much!,hhmpvol,t1_hhm888y,1634916442.0,True
qdgm1o,Library genesis link for it http://libgen.li/edition.php?id=137306805,hholu9j,t1_hhmpvol,1634945676.0,False
qdgm1o,thanks so much :)),hhorrth,t1_hholu9j,1634948720.0,True
qdgm1o,"This book also has a corresponding site that has lecture slides and videos and lots of good resources! I'm using it in my data structures and algorithms class and it's rly good. 

https://algs4.cs.princeton.edu/home/",hhnlue1,t1_hhm888y,1634929410.0,False
qdgm1o,Thank you!!,hhorra4,t1_hhnlue1,1634948713.0,True
qdgm1o,"When you're stuck, it's usually one of the two categories:

1. Kinda feel it should go this direction, but you're convinced/afraid something won't work. For this you just need to read others' code see how they deal with that.

2. Don't even know where to start. This indicates you don't grasp the theory as good as you think you do. Should go back to reading theory.",hhm980m,t3_qdgm1o,1634909573.0,False
qdgm1o,"Hmm yeah, but I feel like when I look at others' code, I'm just copying and I don't actually learn anything?",hhmq0f7,t1_hhm980m,1634916494.0,True
qdgm1o,"You need to squash that notion or it will hold you back from learning at the pace that you’re capable of. Looking at other people’s code and yes, even copying it verbatim, is exactly what you need at this point in your development.

For a while it may feel like you’re not learning but believe me, you are, as long as you take the time to read through the code as you’re copying it — make sure you understand what every statement and variable does before you allow yourself to move on.

By doing this, you’ll expose yourself to implementations of algorithms that you aren’t yet able to implement from scratch, but by repeatedly using them you’ll train your mind to think in those abstractions - variations of trees, maps, tries, graphs, etc. - and soon you’ll find yourself combining them, tweaking them, and eventually reimplementing them from scratch.

This is how you learn, not by studying the bike but by riding it. After all, training wheels are not there to stop you learning, they are there to stop you falling.",hhn4x4x,t1_hhmq0f7,1634922529.0,False
qdgm1o,wow... thank you,hhot318,t1_hhn4x4x,1634949386.0,True
qdgm1o,"If you're not copying the code for the exact homework problem you submit, but rather you read/copy code to learn first, *then* do homework problem *from scratch*, you'll learn more than you expect",hhos35t,t1_hhmq0f7,1634948882.0,False
qdgm1o,"ooh that does make sense, thank you!",hhowgr3,t1_hhos35t,1634951141.0,True
qdgm1o,"There are only so many ways to implement a solution to common basic algos. There will be differences, but it's not going to huge.",hhot7zk,t1_hhmq0f7,1634949456.0,False
qdgm1o,"Bro, I'm a software engineer and currently doing a PhD, I can tell you that looking at other people's code is a huge part of the job. By doing this you learn new ways to implement things and inspiration for your future project. In short learn from people ! 

Plus forums such as stack overflow (huge basic) have a great community willing to help anyone for anu problem and most of the team with a teaching spirit. 

By forging one becomes a black smith well by coding one becomes a coder ! Don't be afraid to make misteaks (and listen to your compiler/interpreter goddamnit !)",hho36zn,t1_hhmq0f7,1634936705.0,False
qdgm1o,"haha, thanks for the insight :)",hhosfga,t1_hho36zn,1634949057.0,True
qdgm1o,"Show me :) 

More specifically, post some code online (a github repo would be ideal, nobody will care how crappy your code might be, don't worry about that). I've got a good feeling that just writing the code and posting it online, trying to make sure you didn't miss anything.. will make you ""get"" where the mistake is. If not, there's always [stackoverflow.com](https://stackoverflow.com). 

When it comes to translating algorithms into code, sometimes being familiar with a language helps. For example, you could try to implement a linked list in ANSI C, but perhaps having familiarity with Python or Java would make it easier. 

What language are you using? What algorithm are you implementing?

Keep in mind that every Turing-complete language can program every algorithm that will ever be written, but a computer program is meant to be understandable to humans. The executable code is meant for a computer.",hhne186,t3_qdgm1o,1634926209.0,False
qdgm1o,thanks!,hhotwl6,t1_hhne186,1634949798.0,True
qdgm1o,"What language are you trying to implement these algorithms in?  If it's a language that doesn't have pointers/references you're going to have a hard time.

It's common to implement algorithms in C/C++/Java or similar languages.  You can do it in Python, but it's as not recommended due to the language not explicitly having some of the concepts necessary to implement algorithms.",hhotpoh,t3_qdgm1o,1634949702.0,False
qdgm1o,"yeah, I'm trying to implement them in c++",hhow8i3,t1_hhotpoh,1634951020.0,True
qdgm1o,"It can be quite the hurdle to get to the point where you're good enough in C++ to be able to write the interface part of a class, let alone everything before it.  So already you're starting with a high barrier of entry before beginning to write a data structure.  C++ is definitely hard mode, but well worth it if you stick it out.

Are you familiar with the basics like https://en.cppreference.com/w/cpp/language/rule_of_three ?",hhrnh5p,t1_hhow8i3,1635013947.0,False
qdgm1o,"Yeah, thanks :)",hhtq0kv,t1_hhrnh5p,1635049439.0,True
qdgm1o,"Is this is Robert Sedgewick course?

Let me warn you… I volunteer with a math teacher to help teach high school computer science.  A few years ago I worked with a group of students on that exact course, and I broke several of them… that course is not geared towards high school… it would be rough on a freshman or sophomore in college.  There are much better resources out there for a high schooler to learn algorithms.

If you find it tough, don’t get discouraged… just find better material.

I’d suggest anything aimed at career software engineers who transitioned into the field from another career path… like books from PragProg or Manning.

https://pragprog.com/titles/jwdsal2/a-common-sense-guide-to-data-structures-and-algorithms-second-edition/",hho1y1j,t3_qdgm1o,1634936150.0,False
qdgm1o,"Nah, I'd rather struggle through it if I think I could learn more, even if it takes me more time and effort. Thanks for the advice though!",hhoslzn,t1_hho1y1j,1634949150.0,True
qdgm1o,"If you want more resources, check out [visualgo.net](https://visualgo.net) and [https://www.cs.usfca.edu/\~galles/visualization/Algorithms.html](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html). Don't be overwhelmed by the sheer amount of algos/ds - some of them are more widely used than others. If you want to visualize trees on your screen instead of drawing them, you use Graphviz: [https://dreampuf.github.io/GraphvizOnline](https://dreampuf.github.io/GraphvizOnline).",hhpiwuh,t3_qdgm1o,1634963608.0,False
qdgm1o,Thank you so much!!!,hhpjmvj,t1_hhpiwuh,1634964065.0,True
qdgm1o,Search for the algorithm in Wikipedia. You'll find pseudocode that you just translate into whatever language you are using,hhnpwmp,t3_qdgm1o,1634931090.0,False
qdgm1o,"yeah, the translating part is the problem",hhov8g0,t1_hhnpwmp,1634950495.0,True
qdgm1o,"you clearly lack experience in programming. with practice and experience you will learn to translate your thoughts into code. you are just not good at translating yet.

learning more about algorithms is probably secondary for this problem but it may also shape your thought process in a good way that is more similar to code for example more procedural, structural and recursive instead of intuition and association based.

look for other people's code AND explanations to leetcode type problems.

Edit:

how much would you struggle writing code to find the index of the second largest element in an array? can you reverse a linked list in constant size? the simple code maneuvers to juggle and keep track of data in variables is probably what you need to get good at first. solving big problems is a bunch of these maneuvers after you construct the solution in your head but that solution needs to be code-friendly i.e easy to transfer to code, for that see before edit",hhnuwhi,t3_qdgm1o,1634933135.0,False
qdgm1o,thanks! I'll work on that more,hhow4po,t1_hhnuwhi,1634950966.0,True
qdgm1o,[removed],hhnwq5a,t3_qdgm1o,1634933905.0,False
qdgm1o,thank you for the tip!,hhou8jn,t1_hhnwq5a,1634949969.0,True
qdgm1o,"That's normal. Make sure you understand what Abstract Data Types are. And learn how a ADT can be implemented in many ways. Try to implement a simple operation in data structure. For instance try implementing push/pop methods in Stack. If you can't come up with solutions, it's okay too. Look at the examples, learn, reflect and brainstorm how it can be implemented in various ways.",hhnwrpo,t3_qdgm1o,1634933923.0,False
qdgm1o,thank you!,hhostq0,t1_hhnwrpo,1634949260.0,True
qdgm1o,"Don't worry! It sounds like you are still in that phase of your learning to code where you get the core concepts but things haven't sunk in yet. Specifically, you are not yet fluent in the language you are writing in. This feels like you know what you want to say but you don't know how to say it. 

I get this again every time I learn a new computer language. For me the only solution is to power through it. If I had a conceptual problem I could go for a walk or sleep on it and it would help. But when stuck with writers block, I just have to power through. This means just sitting at my computer writing code and googling things until I get it. This can take hours, but there really is no other way. Remember that the code is a language to tell the computer what you want it to do, and errors are the computer telling you it doesn't understand what you are saying. Think about if you were learning french. You can't really get good at it until you speak it to Someone and actually realize how to communicate in that language. Especially if you are communicating complex topics like philosophy. It requires just as good a grasp of the philosophy as of the language. 

Some tips:

- Break things down into smaller steps. 

- Write psuedo code. Start with plain English your mama would understand and gradually modify and expand that to be closer to the language and what the computer needs to do. 

- Try every idea you get, even if you think it won't work. Maybe you will get a better understanding of why it doesn't work and it will open new paths. 

- be curious. Learn how everything in the language works exactly. Read a couple blogs on every concept! And 

- don't copy any code. It's ok to find the code you need somewhere, but until you are fluent, rewrite EVERY code snippet you use. The motor memory helps in amazing ways. 

Good luck!",hho3cjr,t3_qdgm1o,1634936774.0,False
qdgm1o,Thanks for the tips :) they really help,hhos3a2,t1_hho3cjr,1634948884.0,True
qdgm1o,"If you “know” a word, but can’t define it: you don’t know the word.

If you “know” your position is right, but can’t give a clear proof of it then you never actually knew that, you just suspected it.

If you “understand” an algorithm, but then can’t implement it then you don’t understand the algorithm (assuming you know enough basic coding to implement things).

None of this is meant to discourage.  
This is to encourage you to understand how our brains work.  
We often “know” something — and what has happened is that we understand *part* of it or are able to answer *some* questions  about it.  And our brains, which evolved to be lazy to be efficient, say: “done!”, no outstanding problems so we got kt.

That is a *cognitive illusion*.  
The reason we must test ourselves is because feeling we understand something is not proof of understanding.

TLDR:  you understand some part of it, but have more to learn.  No problem.  That’s how learning works.  Don’t be hard on yourself for not really understanding, but *do* be suspicious of the part of you that say you understand things — and use that suspicion to discover deeper understanding than most people allows themself.",hhoa0sw,t3_qdgm1o,1634939833.0,False
qdgm1o,"alright, thanks for the advice!",hhou1n5,t1_hhoa0sw,1634949870.0,True
qdgm1o,Join a discord community. I was only able to grow is due to people that helped me while I was getting stuck a lot of times,hhpxp1q,t3_qdgm1o,1634975182.0,False
qdgm1o,Do you have any recommendations?,hhq9qda,t1_hhpxp1q,1634985995.0,True
qdgm1o,https://discord.gg/H2MVDhSs,hhu30vb,t1_hhq9qda,1635058613.0,False
qdgm1o,"It sounds like you just need some more experience with programming. Keep at it, and your brain will start to recognize the patterns!",hhq28va,t3_qdgm1o,1634979287.0,False
qdgm1o,Thank you!!,hhq9ryd,t1_hhq28va,1634986034.0,True
qdgm1o,"My friend recently did a data structure course on coursera and If it was the same one then I wouldn't be much surprised, the course was really difficult even for my friend, who had been coding for a year now. I would suggest reading [this book](https://www.amazon.com/Grokking-Algorithms-illustrated-programmers-curious/dp/1617292230), It helped him understand the basics a lot better than the course did.",hhn1kzf,t3_qdgm1o,1634921163.0,False
qdgm1o,"Haha, thanks! Unfortunately I'm a broke highschool student, but I'll look into the book :)",hhn3jb6,t1_hhn1kzf,1634921957.0,True
qdgm1o,Look up library genesis… :) (use u-block origin too),hhnh1c3,t1_hhn3jb6,1634927423.0,False
qdmby0,"I also remember using that!

The website is http://nandgame.com/ if I'm thinking of the same one as you.",hhnnrxi,t3_qdmby0,1634930211.0,False
qdmby0,"Yes I am sure it is that!! I tried so many websites, thank you for letting me know :)",hhps707,t1_hhnnrxi,1634970454.0,True
qd6bj4,"Take a look among [these linux distros](https://www.techradar.com/best/best-linux-distros-for-education), geared towards education.",hhkysia,t3_qd6bj4,1634876683.0,False
qd6bj4,"Installing things like books, calculators, games, video/editing software, and anything else that would be helpful to have on a PC that you don't necessarily need internet to maintain or use.",hhlfe73,t3_qd6bj4,1634889115.0,False
qd6bj4,Do you have any suggestions where I can download such things?,hhlmiro,t1_hhlfe73,1634895479.0,True
qd6bj4,you can also torrent software. It's okay it's for education it's still ethical,hhmfw7e,t1_hhlmiro,1634912397.0,False
qd6bj4,"Wouldn't they most likely format them and install what they want on them? They wouldn't want to risk you having left your porn folder on them :)

Have you asked them?",hhkf1cj,t3_qd6bj4,1634866571.0,False
qd6bj4,Yes people in developing areas with limited experience and access to electronics will simply format and install whatever they want from their local fiber optic internet connection. Of course!,hhlf7gx,t1_hhkf1cj,1634888958.0,False
qd6bj4,Lol dude read the part that says no fucking internet,hogkst3,t1_hhkf1cj,1639448773.0,False
qd6bj4,"Pack some books and resources for the teachers so they can pass the knowledge to students. Also remember  that even most adults in poor areas don’t know how to use the computer, so even basic Microsoft Office will help them.",hhn1s36,t3_qd6bj4,1634921244.0,False
qd6bj4,You should leave porn on them.,hhku4s4,t3_qd6bj4,1634873976.0,False
qdkg3r,"Studying quantum computers requires a good knowledge of both physics (quantum physics) and computer science.

Here are some good papers to get started.

https://scholar.google.ca/scholar?hl=en&as\_sdt=0%2C5&q=quantum+computing+primer&btnG=",hhmz2uo,t3_qdkg3r,1634920162.0,False
qdkg3r,"Study this.

https://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176

Good luck.",hhpwo1l,t3_qdkg3r,1634974285.0,False
qdkg3r,Thx,hhpx4bu,t1_hhpwo1l,1634974670.0,True
qczvme,"A while loop will continue running as long as the condition is true. 1 is, by definition, true (depending on the language)",hhj60rq,t3_qczvme,1634846439.0,False
qczvme,Just to clarify: 0 is considered false. True is anything that's not 0. You could just as well type while(-1) or while(69420).,hhjus8m,t1_hhj60rq,1634857041.0,False
qczvme,Nice,hhk0j3s,t1_hhjus8m,1634859769.0,False
qczvme,Found the programmer!,hhk3y4x,t1_hhjus8m,1634861401.0,False
qczvme,"…Except in Lua, Ruby, and basically in Lisp. In those languages also *0* wouldn’t be considered false.

Edit: I put a typo. Forgot the negation / “n’t”.",hhkjuve,t1_hhjus8m,1634868848.0,False
qczvme,He said 0 is considered false. So what is the difference then?,hhl95h6,t1_hhkjuve,1634883950.0,False
qczvme,elaborate,hhlnqx7,t1_hhl95h6,1634896486.0,False
qczvme,"The difference would be that 0 would actually be considered *true* in Lua, Ruby, and Lisp.",hhlq1mh,t1_hhl95h6,1634898389.0,False
qczvme,Is “0” (a string) considered true or false?,hhm40ng,t1_hhjus8m,1634907162.0,False
qczvme,"Some languages might automatically convert it to an int, but in most cases, it will be true. That's because strings are generally just an array of Unicode codes. The Unicode code for the char '0' is actually 30. In general, if you want the actual value 0 in a string, you have to put a backslash behind it, like so: ""\0"". 

But even then, it still won't evaluate to false, because strings actually have an additional hidden 0 at the end of them, used to know where the string ends. So ""0"" is actually an array that is equivalent to [30, 0]. 

Even an empty array [] will be considered true unless they get automatically converted by the language. That is because an array is actually a reference to a memory location, or in other words, a positive integer representing the start of your array.

For a string, or any other array, to evaluate to false, the reference would actually have to contain the value 0. This is typically called the nullpointer, or in other words, null.",hhmkgql,t1_hhm40ng,1634914243.0,False
qczvme,"Interesting, thank you!",hhmt1i3,t1_hhmkgql,1634917716.0,False
qczvme,"either true or a type error, depending on the language

edit: actually I think it's false in PHP",hhm4nh4,t1_hhm40ng,1634907473.0,False
qczvme,"Yes 
1 is equivalent to true and 
0 is equivalent to false.",hhlofoa,t1_hhj60rq,1634897068.0,False
qczvme,Because 1 is usually true in programming languages,hhj4qwd,t3_qczvme,1634845932.0,False
qczvme,"The while() statement expects an expression that resolves to a boolean value. In C, any nonzero integer resolves to true and zero resolves to false. Hence while(1) is a loop with an always true condition.",hhj6o4w,t3_qczvme,1634846698.0,False
qczvme,while(2)  is even more fun.,hhjaneu,t3_qczvme,1634848271.0,False
qczvme,Not as fun as while(-1),hhjk8aw,t1_hhjaneu,1634852279.0,False
qczvme,while(infinity),hhk28r8,t1_hhjk8aw,1634860577.0,False
qczvme,NameError: name 'infinity' is not defined in this namespace,hhm4jnx,t1_hhk28r8,1634907421.0,False
qczvme,while(Math.infinity),hho2utm,t1_hhm4jnx,1634936558.0,False
qczvme,While(infinity + 1),hhlhxsd,t1_hhk28r8,1634891347.0,False
qczvme,while( ( (1 + 1) == 2 ) == 1),hhk2pa8,t1_hhjaneu,1634860798.0,False
qczvme,efficient,hhlcmfc,t1_hhk2pa8,1634886816.0,False
qczvme,"Surprised no one mentioned it, but go look up truthy logic in different programming languages. It is the concept applied here",hhjlkxa,t3_qczvme,1634852862.0,False
qczvme,"To elaborate on the other answers: when they say ""1 is true"" what they mean is that the value 1 is typically used to *represent* something being true. And 0 is typically used to *represent* something being false. It's an arbitrary choice. A clearer way to write an infinite loop is ""while(true)"".",hhj6ibi,t3_qczvme,1634846633.0,False
qczvme,"I'm honestly curious, and I hope you can correct me if you know the correct answer. C (not C++) doesn't have 'true', right? What I mean to say is it isn't defined as a keyword... I'm beginning to doubt myself because it sounds idiotic... but I swear I learned something like this once long ago...",hhk9h30,t1_hhj6ibi,1634863987.0,False
qczvme,"Yes, correct. But C does have <stdbool.h> which #define's bool, true and false as well.",hhkb66a,t1_hhk9h30,1634864778.0,False
qczvme,Not really. They are just convenient types you're supposed to convene to the reader that something is boolean. C only has integer types. Even `_Bool` is just a 1-bit integer by the standard. The semantics of conditions are defined by comparsion with 0. The semantics of relational comparsion operators are defined to literally give 0 or 1.,hhloic4,t1_hhkb66a,1634897128.0,False
qczvme,So... which of my statements are incorrect?,hhm1tsa,t1_hhloic4,1634906032.0,False
qczvme,"C has ""true"" and ""false"" defined via preprocessor to be 1 and 0.",hhkesyt,t1_hhk9h30,1634866465.0,False
qczvme,"Most languages have this concept called **truthiness** which you neglected to mention.  &for loops are more common therefor 
```
for(int i=0;~i;[=]{
    if(breakCondition)
        i=~0;
});
```
is easier to understand. 
/s but I’ve seen this argument before at work.",hhju2i9,t1_hhj6ibi,1634856706.0,False
qczvme,I need to bleach my eye,hhk55re,t1_hhju2i9,1634861979.0,False
qczvme,"1, 2, 3 any none zero number Is true",hhjhq7u,t3_qczvme,1634851198.0,False
qczvme,"What many are saying here isn't entirely accurate; it's not that 1 is true, it's that anything that's not 0 is true.",hhjhy8w,t3_qczvme,1634851292.0,False
qczvme,"It depends on programming language. You expect to have some boolean (true/false) value as condition in parentheses. Some languages would convert value in brackets to boolean if it's not boolean already. In others you will get error message.


For example, see below link on what rules PowerShell uses to make such conversions:

https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_booleans?view=powershell-7.1",hhj7yb6,t3_qczvme,1634847207.0,False
qczvme,"In a transistor, “1” means it’s on, which means true to the computer. Likewise, “0” is off, and subsequently means false.",hhjlx75,t3_qczvme,1634853010.0,False
qczvme,"While what everyone else says about 1 being true is correct, there's another piece...

It's *also* that the while loop doesn't have an *alternative* exist clause. Such as

\`\`\`if (time.current > time(""Noon"")){ break; }\`\`\`",hhjkfxd,t3_qczvme,1634852373.0,False
qczvme,"While(true){}

  
  This is how it translates",hhjqr1u,t3_qczvme,1634855173.0,False
qczvme,"It comes down to truthy and falsy values. Truthy values are pretty much anything but your language's equivalent of `null`, `undefined`, 0, `false`, etc. Everything else is truthy including empty objects `{}` if we're dealing with JavaScript",hhjsn6s,t3_qczvme,1634856046.0,False
qczvme,you forgot my favorite value: -0,hhl5kzh,t1_hhjsn6s,1634881211.0,False
qczvme,Programming languages without a built in boolean type sometimes handle comparison operations by having the expression return zero for false and non-zero for true. I'd have to check to be sure but I think most CPUs use something like this to return the result of comparison instructions and conditional jump instructions that are used to build loops in compiled languages.,hhjup0l,t3_qczvme,1634856999.0,False
qczvme,"Technically, it depends on what's inside the loop. A `while(1)` loop is a perfectly valid construct, as long as there's some code inside the loop that will issue a `break;` \- usually tied to some conditional.

But yes, `while(1);` is a complete and absolute endless loop, since the `1` is considered *not false*, and will never change.",hhjvls0,t3_qczvme,1634857432.0,False
qczvme,Now I have to try this,hhjx8b5,t3_qczvme,1634858204.0,False
qczvme,"in c++ bool, char, int, ... - are all numbers. 1 <=> char(1) <=> true",hhk2hzs,t3_qczvme,1634860700.0,False
qczvme,"It’s the same as while(true). Everything’s a bit eventually: 1 is true, 0 is false.  

Doing this instead of throwing an error can make looking at conditions “cleaner”.

Another example would be null being evaluated as false to make null checks easier.",hhkie7y,t3_qczvme,1634868154.0,False
qczvme,Because 1 is always not zero.,hhkoqcn,t3_qczvme,1634871209.0,False
qczvme,Why will an open window endlessly let the outside air environment in?,hhl3i4s,t3_qczvme,1634879742.0,False
qczvme,"its called ""truthiness"" 

here's an article about truthiness in javascript

https://developer.mozilla.org/en-US/docs/Glossary/Truthy",hhl55nn,t3_qczvme,1634880905.0,False
qczvme,Is it actually endless? Can you prove that? Look up the halting problem for a real  head scratcher.,hhl7e7v,t3_qczvme,1634882570.0,False
qczvme,Always true 🤣,hhlbymg,t3_qczvme,1634886253.0,False
qczvme,In a while loop if the condition is true the loop never stop. In many languages integer 1 is considered as true and 0 is considered as false. If you know python you may know that wrapping an integer 1 with bool ( bool (1) -> true ) gives true,hhlnpbj,t3_qczvme,1634896449.0,False
qczvme,Didn’t a mod make a post about questions like this recently? I thought we didn’t allow these types of questions,hhn6wkd,t3_qczvme,1634923338.0,False
qczvme,Also: https://www.google.dk/search?q=why+is+while(1)+an+endless+loop,hhn73w5,t1_hhn6wkd,1634923424.0,False
qczvme,"1 and 0 are binary equivalents to true and false. In bitwise logic 1 is considered on, or true. So this is the same as while(true). In many programming languages you can instantiate a value as Boolean type, and still give/expect 1 or 0 as valid arguments",hhjdtt5,t3_qczvme,1634849544.0,False
qd407r,"The only thought I would have is the reset function of the registers that would set the loop back to 00. Or if there was a if statement that turned off the register. For example you could have an if statement of “if S > n then remove clock from register or remove the ability to load a new number”

There is no if logic in the circuit and the loop has to have if logic to stop so it has to be wrong. 

(Note: I may be making a mistake, it’s been a while since I worked with this stuff)",hhknsqz,t3_qd407r,1634870749.0,False
qd407r,"I understand registers on a software level. I've interacted with them a lot when I had an internship doing reverse engineering. But, I've wanted to up my knowledge so I'm resisting the digital logic implementations of them. I digress, but it was to illustrate I have somewhat of an idea of what I'm doing.

My thoughts when is when the control signal from the register is sent (e.g., reset), that indicates the end of the loop.

As for the initialization, the load signal indicates to replace the state of S. In other words, initialize S.

So, to recap:

The register does the following ( slight over simplification):

1. CLK = State Change
2. LD = Initializes
3. Reset = Control",hhkt47z,t1_hhknsqz,1634873430.0,True
qcxk48,The IBM 650 did just that.,hhiula0,t3_qcxk48,1634841795.0,False
qcxk48,"They can, but there is no reason to do so.",hhj0wvn,t3_qcxk48,1634844377.0,False
qcxk48,"This is absolutely right, I'll add some additional details and other good reasons not to.

The easiest answer is as /u/Zepb said: Spinning hard disks are so incredibly slow (humans are bad at orders of magnitude, but we're talking 100000x slower than DRAM). You can easily run a full blow neural net in the time it takes to read one byte from a disk. Talking to a server on the other side of the country is only about 20x slower than reading one byte from a disk. It would be pointless to try and read that from disk every cycle.

That being said, not all storage is that slow. Some non-volatile memory (NVM) technologies are getting fast enough that it's not totally insane to think about treating them like memories (e.g. only 10x slower than DRAM). In this case, some people are looking at addressing storage (info stays put if you turn it off) like main memory. When you have caches and stuff, it's actually plausible. The problem then becomes how you make sense of the information you stored. If my program crashes half way through, how do I make sense of the information it's already written? If two processes are reading/writing to the storage, how do we coordinate (particularly if we want everything to continue making sense later)? These are very tricky issues and this technique is a very active area of research. My personal take is that then non-volatility (storage) part of NVM is frankly not that interesting. The fact that it's really big and can be turned off to save power, however, is very interesting.

Ultimately, there are always a million tradeoffs we make in software/hardware. How we write programs, how we think about storage, how these big complex systems behave and interact, this all controls what we decide to do. Markets and technology change, and there's lots of room for creative memory solutions.",hhj6h8b,t1_hhj0wvn,1634846621.0,False
qcxk48,"I like to add why RAM is important, besides being fast, since this is what OP want to know. RAM stands for Random Access Memory which means that every address (you can think of it as a single storage cell) can be accessed in the same amount of time. This helps to make a programm run equally fast each time it is executed, regardless of the distribution of the data in memory. A HDD is usually not considered to be a RAM since the access time depends on the position of the read-head and the position of the data on the disk. So there is a huge variation in access time.

To unblame HDDs, they are very good at storing huge amount of data and if the data is read in consecutive blocks it is pretty fast. But the advantages of HDD are only at handling large volumes of data which is not needed in programm execution.

HDDs are not exactly RAM but also not exactly not-RAM. A better example is tape memory, these are LAM Linear Access Memory. There the data is stored on a big magnetic tape like the old video-tapes. You have to scroll through the tape until you can read the data from a single position. Depending on where the data is stored it takes different amount of time to access it. To some extend a HDD has similar limitations.",hhl2pfw,t1_hhj6h8b,1634879198.0,False
qcxk48,"So the way a program runs, at a very low level, is that it's all just 1s and 0s. For each ""step"" of the program, a cycle runs, where depending on instructions that it's given, it'll make changes to the information it's dealing with. To do this, it runs through the entirety of the information stored in the memory. 

The CPU is processing and writing data to the ram is a fairly continuous cycle, not byte addressable is referring to how it's both running through the entire set of data it's working with, and that it's using a variable amount of that data- both things that are hard if you just have a massive volume to try and deal with at once. 



It's hard to find let alone work off of a recipe in your bookshelf, it's easy if you are just working with the cookbook.",hhirkvn,t3_qcxk48,1634840583.0,False
qcxk48,"CPUs expect to get their data via memory, so they have a memory controller that electrically interfaces to the memory.

CPUs don't have a hard disk controller, so there needs to be something in between the CPU and the hard disk so the CPU can get the program instructions from the disk.   In current systems, that 'something' is the operating system, which interfaces to the hard drive control hardware.

And you could make a hard drive byte addressable, but the overhead in making a request is high so it isn't very useful.",hhj63gv,t3_qcxk48,1634846468.0,False
qcxk48,"There's this thing called swap file that Windows uses in case it runs out of RAM. I changed its configuration without knowing what I was doing and my computer was booting for over 30 minutes.

I don't know how it's working exactly, but I'm guessing that because I've setup file to be quite large system was accesing hard drive much more frequently looking for data that would normally be stored in RAM.

It had maybe 64 or 128 MB RAM and probably around 10GB HDD. As @ArgoNunya writes here, maybe with modern SSDs it would be much faster than what I've experienced. But I'm sure average PC user will never need to find this out, because we have gone quite a long way since year 2000 in terms of RAM capacity.",hhjbvmg,t3_qcxk48,1634848759.0,False
qcjqwn,"Assume a box and you put a label on that according to what you want to store in it, that label is called data type and how you arrange those boxes is data structures.

For example stack, queue, linklist, trees are data structures,
Where as int, float, double, bool are data types.

Array is a data structures where each element (boxes) are placed at continuous location(one after another).",hhghfts,t3_qcjqwn,1634792989.0,False
qcjqwn,"That's a solid start, but you didn't mention that data types can be unions of types (see Discriminated Unions in any number of programming languages, or non-discriminated unions in something like C/C++).

Types can themselves also be a data structure. And while arrays are certainly a data structure, in many languages `int []` is a type, as well as a structure of contiguous memory and effectively a struct and/or vtable for object attributes. Similarly a lot of functional programming languages might have a `'a list` or `'a Tree` to represent those data structures. I suppose you could draw a distinction between the name and the in-memory structure, but that can lead to confusion.

And then of course there are structural types where the type is literally defined by the structure of the data...",hhjmfbs,t1_hhghfts,1634853230.0,False
qcjqwn,All data stored in a classical computer can be thought of as data structures of simple sets of bits.,hhkin1u,t1_hhjmfbs,1634868267.0,False
qcjqwn,"Sure, but that's not a particularly useful way to think about it. If you're at that low of a level, you're not going to want to abstract away the hardware to that extent. Even if you're only talking about in-memory storage (why?) all addresses are created equal--they could point to anything from hardware registers to no-execute pages, memory-mapped files, etc. And don't get started on what addresses work out to when dealing with solid state storage.  
The point of structuring data is to make reasoning about it and making algorithms to handle it easier and more efficiently. Many of the best results in data structures have nothing to do with bits--they'd work just fine using trinary, or some sort of discretized analog memory.",hovpt31,t1_hhkin1u,1639717971.0,False
qcjqwn,Superb explained 👏,hhigoio,t1_hhghfts,1634836178.0,False
qcjqwn,"I think you meant to say contiguous, but continuous kind of works too.",hhipnii,t1_hhghfts,1634839802.0,False
qcjqwn,Nobody enjoys the grammar police,hhiz7cw,t1_hhipnii,1634843678.0,False
qcjqwn,"It’s not grammar, it’s just the right word selection.",hhizohz,t1_hhiz7cw,1634843875.0,False
qcjqwn,"People trying to get a bit too meta in this thread.

Data types describe what a variable can contain. For example, an ""integer"" datatype can hold integers. A ""String"" data type can hold ""strings"". If you create a class called VendingMachine then a VendingMachine variable can hold instances of VendingMachines.

A data structure is an abstract scheme for organizing data. For example, a binary tree is a way to organize data into nodes, grouped into a tree structure. But this is abstract. The computer doesn't automatically know how to use a data structure. An ""array"" - colloquially speaking is a (sometimes not 100% formally defined) data structure. You're saying the data is lined up in some kind of linear, arbitrarily accessible list (the word list here is used informally).

Now the combination of those two are where things get interesting. You ""implement"" mechanisms in a programming language to use data structures. Most commonly, you can implement a class. For example in python I can write a `class SawBinaryTree` to define my own class (and data type) called ""SawBinaryTree"". I can write all the methods to store data in a binary tree format. But you could also write a ""AcreyesBinaryTree"" class to implement a binary tree data structure, maybe more efficiently than I did. They both represent data to the user in the same way, but they are distinct types in your language. So those are 2 ""data types"" that implement the 1 ""binary tree"" data structure.",hhhmp8x,t3_qcjqwn,1634823579.0,False
qcjqwn,"Data - a block of 1s and 0s.

Data type - how to we interpret that data? Is it numbers?  Floating point or integer? Signed or unsigned? ASCII code? Unicode?  How many 1s and 0s are in each discrete chunk?

Data structure - how do we arrange those interpreted, sized chunks, so that we can find the ones we want again easily?  Put them all next to each other in a row?  Is that row sorted in some way?  In the order we got them? In the order we want to send them?  In a tree so we can search them quickly?",hhha3aj,t3_qcjqwn,1634816580.0,False
qcjqwn,no idea how you got a downvote for this,hhhiex9,t1_hhha3aj,1634821426.0,False
qcjqwn,"Lol, idk, Reddit is a capricious mistress.",hhhj3vx,t1_hhhiex9,1634821788.0,False
qcjqwn,"“Data type” is used twice, the second where “data structure” should be.",hhhjzg4,t1_hhhiex9,1634822237.0,False
qcjqwn,stack overflow level of stick-up-butt and toxicity,hhhp9ha,t1_hhhjzg4,1634824783.0,False
qcjqwn,"Oh, thanks for the heads up, I'll fix the typo",hhhtl7y,t1_hhhjzg4,1634826721.0,False
qcjqwn,We stack overflow now,hhhmp4i,t1_hhhjzg4,1634823577.0,False
qcjqwn,lol absolutely. I didn’t downvote; it’s just my best guess.,hhhndy6,t1_hhhmp4i,1634823905.0,False
qcjqwn,"A data type is particular to a language or architecture. It is how a particular type of data (eg integer, float, string) is stored in that language or platform.

A data structure is a conceptual way of organizing and accessing data, eg a tree or a list. Some languages have certain data structures implemented as data types (eg lists and hash tables).",hhi3wx1,t3_qcjqwn,1634831096.0,False
qcjqwn,"A data structure is a nonempty set, a collection of operations of finite arity, and a set of axioms the operations satisfy. Go ahead and open a book on set theory if you want to choke on it. I tell you the theoretical definition so we can all give it a nod and forget about it, unless you're doing work in mathematics and theory of computation.

A data structure is data and the functions that operate on that data. Part of this definition includes how the data is arranged. A single bit is a data structure, as is a structure with multiple fields that have some sort of relationship - an array of elements and the length of the array, for example. Functions that apply to that array make assumptions about the nature of its elements and its length.

A type gives the structure a name. It's a form of data itself that can be reasoned, typically described in source code and applied by the compiler. In C++, for example, templates ain't nothing but compile-time functions that take types as parameters, they're executed by the compiler, and the output are more types and functions. Lisp don't give a shit, and there's hardly a distinction between code, data, and execution.",hhj0h72,t3_qcjqwn,1634844200.0,False
qcjqwn,[deleted],hhgiax6,t3_qcjqwn,1634793587.0,False
qcjqwn,"Good information with one (purely academic) exception that an array is not an ADT. It's actually closer to a data type, just non scalar, than an ADT. Some textbooks regard it as a composite data type rather than a data structure.",hhhpwm8,t1_hhgiax6,1634825084.0,False
qcjqwn,"https://brilliant.org/wiki/arrays-adt/
Please tell me if my understanding is wrong.
Edit: I am not confused, but, just my theoretical understanding is different.",hhhqact,t1_hhhpwm8,1634825257.0,False
qcjqwn,An array ADT is not an array and an array is not an ADT. An array is not abstract. It is homogenous data types stored in a contiguous section of memory.,hhhy3ls,t1_hhhqact,1634828654.0,False
qcjqwn,I appreciate your comment 👍,hhhyf2g,t1_hhhy3ls,1634828788.0,False
qcjqwn,"Data type is label that determines how the 1s and 0s in the data are interpreted by the program. Data structures are how data is organized; stack, queue, heap, tree, etc.

So an array would be a data structure.",hhhlngd,t3_qcjqwn,1634823068.0,False
qcjqwn,An array is a data structure. Data types are very abstract.,hhi7txa,t3_qcjqwn,1634832707.0,False
qcjqwn,"Data types - ( for language - how the info is stored , we use %d , %c for different ways to show op of a data stored) , ( for us - what we can stored , like . , or letter/chars)

Data structures - ( how to arrange a different units of data type in memory as our convenience ) linked list , queue etc

Arrays( non primitive data type and primitive data structures )

non primitive data type - stores group of values 

primitive data structures - that are predefined in a language",hhin4g6,t3_qcjqwn,1634838767.0,False
qcxh83,Your BIOS is in ROM - Read-Only Memory.,hhiqcx2,t3_qcxh83,1634840091.0,False
qcxh83,Except you can update it. So it’s not all that read-only.,hhj1a73,t1_hhiqcx2,1634844528.0,False
qcxh83,I guess back in the day was true ROM when you had to replace the chip to update the bios. Now it's more of a flash memory locked unless you need to update.,hhj6dpz,t1_hhj1a73,1634846583.0,False
qcxh83,I think we still just call it ROM for short when in reality it something like EPROM (Erasable programmable read only memory).,hhjfs1i,t1_hhj1a73,1634850360.0,False
qcxh83,"Also, to be clear, mobile phones have 32 or 64 (or however much) of storage, never of their memory (ram)",hhipchr,t3_qcxh83,1634839677.0,False
qcxh83,"There is a hard drive in your computer, the operating system is on it.


Memory (ram) clears when your computer it turned off, storage (HDD/SSD) doesn't. 



Edit: every computer component has some rom but you usually can't access it.",hhinz3q,t3_qcxh83,1634839112.0,False
qcxh83,ROM is faster than SSD?,hhiovej,t1_hhinz3q,1634839482.0,True
qcxh83,ROM isn't a component to be comparing. It's not a term you need to deal with in beginning CS or if you're building your computer.,hhip11a,t1_hhiovej,1634839546.0,False
qcxh83,"I have not thought about this in a solid 6 years, so forgive me.

But wouldn't ROM be at least semi important, as it contains the instructions for bootstrapping during startup? Like where to find the OS and such.",hhiplkx,t1_hhip11a,1634839780.0,False
qcxh83,"Right, but instead of ROM, we usually have Read-Write Memory, so that we can change things about bios, install a new driver, update the operating system, et c. Only things that would never for any reason ever have to change (like the framework of the BIOS itself) would be written to ROM, so as such, not a lot of space is dedicated to it. 



All of the storage (be it flash, HDD or SSD format) is Read-Write memory",hhiqccw,t1_hhiplkx,1634840084.0,False
qcxh83,"Older computers used rom only though, correct?",hhiuyvm,t1_hhiqccw,1634841946.0,False
qcxh83,"Not since punchcard days, anything that saved information once the computer was off had a write function.",hhivcor,t1_hhiuyvm,1634842101.0,False
qcxh83,"I have a confusion , are RAM and ROM are physical stuffs or these are just concepts?",hhipn95,t1_hhip11a,1634839799.0,True
qcxh83,"Ah, got it, so we're dealing with two things here- RAM, random access memory is a term being used twice here, it's both a function AND a component. For lack of creativity, the ram in your computer is provided by stick of ram. 


Read Only Memory is also a function, but it doesn't have a piece of hardware dedicated to it, all the parts have tiny chips that have ROM functions, but there isn't a ROM component.",hhiq207,t1_hhipn95,1634839967.0,False
qcxh83,"RAM and ROM are general concepts.

Random access memory is memory can stored to or read from anywhere in the memory space (in contrast to, say, a tape where you've got to read the data one after the other).

Read only memory is memory that can only be read from and not stored to. For example, a CD.

However, in terms of the real world and building PCs, ""RAM"" is often shorthand for the component known as ""DRAM"", which is a computer component (look at [this wikipedia page](https://en.wikipedia.org/wiki/Dynamic_random-access_memory) for more) that holds the working memory for a computer. The CPU will read to and write to it randomly (random here meaning freely, at will, at any point of memory).

The concept of Random Access Memory (shortened to RAM) and the computer component commonly known as ""RAM"" are not identical.

There is no component in a PC named ROM (even though the concept of read only memory does exist in a PC, such as in firmware)",hhiquoo,t1_hhipn95,1634840291.0,False
qcxh83,"ROM, as far as consumer computers go, is known as BIOS. This is the system that tells your computer how to startup. You can think of it kinda like the starter in your car; in that is mostly useful in the boot process (though, it does provide a lot of hardware-level functions during normal operation like turning on the fans when the CPU gets too hot). It is almost never an interesting question to ask ""how much ROM do I have"" because it's tiny (a few Kbs) and the information stored there is only useful if you are installing a new operating system or trying to manually modify your hardware operation (which, you shouldn't unless you know what you are doing). 

&#x200B;

RAM and HDD/SDDs (gonna call it ""disk"" from here on) are interesting because they are where your actual data and operating system get stored. In practice, RAM is ""Working memory"" and disk is ""long term memory"". When you start up the OS, it is on the disk, but gets copied into RAM so it can be run. Same with any program you start up; it gets copied from disk to RAM, then it starts executing in the CPU. All your devices also write essentially directly to RAM as well. Your keyboard, mouse, USB drive, network card, etc all dump data into RAM (well, it is a little more complicated than that, but it's true enough).  The reason you have to load it from Disk to RAM is partly because disk is so far, Partly because disks are cheaper and therfore store more information, and partly that the CPU would be waiting too long. But the most important reason is that RAM is erased when you turn off your computer/phone, but Disks are not.  

&#x200B;

So finally, to answer your question, disks are slow, large, persistent storage. They don't count as RAM because they are so slow that you have to copy from them to RAM to even do anything with it. The 32/64 GBs of memory in your phone might be RAM, but it is probably disk. Phones, even high end ones have around 0.5-8GB. The latest iPhone has 6GB, but hey, there might be an android phone that has 64GB of RAM: it would be strange but not impossible. So that 32/64GB in your phone is probably disk. And if you upgrade it, like with an SD Card, that would be an increase in disk, not RAM.",hhjayxe,t3_qcxh83,1634848401.0,False
qcxh83,"Initially the OS stays in the hard disk of your computer. When computer is turned ON your BIOS which is hardcoded in the motherboard loads your OS into RAM. 

Every computer, laptop, or mobile has both RAM and secondary storage called Harddisk present in it. Secondary storage keeps all the programs in it. Programs which are important / needed at a time those are transfer to RAM and from there processor process the program",hhqppbz,t3_qcxh83,1634996553.0,False
qc7295,Where my Dinosaur Book peeps at?,hhes8br,t3_qc7295,1634763520.0,False
qc7295,Dinosaur book !,hhescad,t1_hhes8br,1634763560.0,False
qc7295,Came to look at the comments just to see if people are talking about the Dino book,hhghvpy,t1_hhes8br,1634793293.0,False
qc7295,"Dinobook is what we're using. The text is easy to digest given certain requisite knowledge. Except for a few parts here and there, I've been able to follow along very easily",hhhq8la,t1_hhes8br,1634825234.0,False
qc7295,"Tanenbaum is still regarded as the canonical text.  Details of operating systems have changed but the core principles haven't changed enough for it to be outdated.  

(People always talk about how fast tech changes, and that's true.  But what that really means is that every time you learn a ""framework"" it's obsolete by the time you've learned it.  Core computing concepts and principles don't change that fast.)",hheno13,t3_qc7295,1634761730.0,False
qc7295,[OSTEP](https://pages.cs.wisc.edu/~remzi/OSTEP/) is quite popular and is fairly engaging.,hhedin5,t3_qc7295,1634757750.0,False
qc7295,I go to UW madison and remzi rocks,hhfjvnb,t1_hhedin5,1634775796.0,False
qc7295,10/10 would recommend,hhfnwe3,t1_hhedin5,1634777610.0,False
qc7295,We use this at Cornell,hhfvr9l,t1_hhedin5,1634781251.0,False
qc7295,Ostep is what we use at my university,hhezv21,t1_hhedin5,1634766705.0,False
qc7295,"Not in USA but in Australia, we refer to the ""Modern Operating Systems"" by Tanenbaum.

The book is very informative, has a lot of information and does go into depth but one thing the book lacks which I think is very important to have in any OS book are diagrams/images.

There is just not a lot of diagrams to illustrate a concept/idea and I wish it did, it would have definitely enhanced understanding.",hhf32ze,t3_qc7295,1634768127.0,False
qc7295,This is what we used (U.S.) and it is a fantastic book.  I still have mine on the shelf behind my desk at home with highlighter marks and post-it notes throughout.,hhfwh3o,t1_hhf32ze,1634781588.0,False
qc7295,"Yeah definitely is a good read, if it had more diagrams/images, it would have been so much more interesting to read. I haven't finished reading but it's great so far.",hhfwznd,t1_hhfwh3o,1634781827.0,False
qc7295,https://pages.cs.wisc.edu/\~remzi/OSTEP/,hhgrjp6,t3_qc7295,1634800933.0,False
qc7295,When I took OS we used ‘Operating Systems Principles & Practice’ Volume 1 and 2,hhefj5w,t3_qc7295,1634758531.0,False
qc7295,"https://www.zybooks.com/catalog/operating-systems/#toggle-id-5-closed

It’s a highly-interactive introduction to OS",hhg2sf3,t3_qc7295,1634784571.0,False
qc7295,Designing Data Intensive Applications book.,hhhsx7a,t3_qc7295,1634826427.0,False
qbv5z3,"The best place to start is with something you can do. I know that sounds silly but hear me out. 

Think of a problem like “get the user to input two numbers and output whether they have a common prime factor. 

Start with

OUTPUT “Enter two numbers”
Num1 = Userinput
Num2 = Userinput 

Commonprime = False

If commonprime 
OUTPUT “Common prime factor found”
Else
OUTPUT “”No common prime factor found”

That doesn’t solve any of the hard part of the problem but I bet you can do that in your preferred programming language. 

You’ll be amazed how much better you’ll feel with some code on the page. Next think about the next easiest part of the problem. Can you work out all the factors of a number. 

If you can’t do any single part of the problem you probably need to start with some easier problems and work your way up - that will allow you to develop a range of strategies.",hhc58z1,t3_qbv5z3,1634716038.0,False
qbv5z3,Thank you that really help. I saw that after looking at the code and understanding it I had an idea on how to start the next problem get some lines in it before looking at the answer now.,hhc6muc,t1_hhc58z1,1634717300.0,True
qbv5z3,same here Any competitive programming sites you know of?,hhc45xi,t3_qbv5z3,1634715094.0,False
qbv5z3,Codewars is amazing they have simple problems for what you need,hhcsawt,t1_hhc45xi,1634733802.0,False
qbv5z3,"Try to do the questions for which you already saw the answers. Most times I would see the answers and think I understand, but find myself struggling when I try to do it. If you find yourself struggling too much, refer back to the answer. This time it’ll stick.",hhc53h5,t3_qbv5z3,1634715900.0,False
qbv5z3,So I should try I rewrite the code in another way? Or just look at the code (that I understand now) then rewrite it?,hhc6g1i,t1_hhc53h5,1634717124.0,True
qbv5z3,Understand how it works and implement without looking at the code unless you are stuck.,hhc73zk,t1_hhc6g1i,1634717748.0,False
qbv5z3,"I checked your profile and saw your age. That's the biggest factor, experience and exposure will grow you. 

If you look back at the code you wrote today in 2 years time, you'll shake your head and laugh. Actually, I'll laugh at the code I wrote 6 months ago. 

It's a constant learning experience. But if you want to be able to tackle problems easier, try to solve some problems rather than trying to build a solution instead.

Go through the development life cycle.

Customer says they have a problem.

You enquire.

You build requirements.

You show them requirements.

They agree on some and disagree on others.

You re-iterate. 

They agree.

You design a solution using a mix of A technology and B technology. 

You build the solution.

You give the customer the solution.


I saw from your previous posts that you're looking for a side hustle and development can really help there. I got my break into Software Development from a lifetime in Environmental Science by making and selling VR and AR apps. 

Give yourself a challenge, you learn more from your failures than your successes.",hhcplne,t3_qbv5z3,1634732297.0,False
qbv5z3,"I've had the same problem when I started competitive programming.

The problem you are facing is something very common when you are not gifted in problem solving. But good news it can be learned. Basically you have the same issue that you may have faced in math class : I understand the math but fail when it comes to solving the exercise. 

What I did is that on websites like codeforce or hackerrank I worked only on one type of problem for instance problems using linkedlist. And even though at first i needed to check the solution to understand how to solve the problem. At some point I just knew what I had to do since you do mostly the same thing. Then i moved on different types or problems following the same method. Use previous solved problems to implement their algorithms in other problems !

I suggest to take the time to carefully read the problem, understand what outcome is required and what information are needed to solve the problem. Don't hesitate to draw the problem, too many people tend to work in their heads instead of writing stuff down to visualize the problem. In short don't go too fast ! 

Another suggestion that I would give you  is to make sure that you are not starting off with too difficult problems and that you know your programming language well ( i know it sounds dumb but i figured when i started competitive programming that my knowledge in c++ was quite bad and that even though i knew what certain functions did when i saw them in the solution my brain would just not think about using them ). 

A last recommendation that I could give you is to watch youtube videos about programming and problem solving. William lin or Erichto for instance are two youtube chanels that will help you understand some concepts ( it served me pretty well especially when it came to shorten my code and make it more simple ). Don't hesitate to ask people for help and look for documentation. Try as much as you can to solve a problem without looking for the solution. As it was said in another comment writing some code and trying to understand how to make progress towards the answer of your problem will make you improve even if you need to rely on the solution ay the end ( i say that because i used to give up pretty fast and look for the answer as soon as i faced difficulties, it's fine if you don't manage to solve a problem in 1 hour ). 

Becoming better at problem solving, algorithm etc is a long way that will require you rigorous practice. The more you code and solve problems the better you will get. Kind of like the progress it took you from learning how to code until you made your first snake game if you will. You need to persevere and keep working.The process is definitely very enjoyable and you will be proud the 1st time you will manage to solve a problem on your own (and also your coding interview skills will be very good and sharp).

Hope that helps and good luck on your grind !",hhcr2s5,t3_qbv5z3,1634733142.0,False
qbv5z3,"Find simpler questions? Project Euler has a wide range of difficulty, from easy to (IMHO) absurdly difficult. Try to ask yourself what a first step would be - or, what the last step might be. Sometimes thinking in reverse and satisfying dependencies can be an easier way to solve a problem.",hhd73zs,t3_qbv5z3,1634740770.0,False
qbv5z3,Just practice and before writing any code just think about the solution first remembering the logic behind classic algorithms. It helps a lot.,hhdl4b7,t3_qbv5z3,1634746482.0,False
qbv5z3,"We may compete on codeforces, leetcode or hackerrank etc. I was in the same situation with you almost 2 months ago, with no ability to solve but the ability to understand completetly. My first codeforces contest score was 350 :D. Now I am around 1000, which still sucks, but I guess I am getting better. However, only problem-solving is not enough, reading the intro. to. algos. or similar books would definitely help you.",hhdpgrz,t3_qbv5z3,1634748185.0,False
qbv5z3,Thank you so much. And I don’t know what that score is for but all I know is that it went up and that a good thing.,hhghgx5,t1_hhdpgrz,1634793011.0,True
qbv5z3,"Psuedo code it. 

It breaks the problem into pieces, code it, and build “into” whatever language you’re using.",hhfk44e,t3_qbv5z3,1634775905.0,False
qbv5z3,Thanks for asking this question I've been having these same exact problems.,hhfy5px,t3_qbv5z3,1634782366.0,False
qc6rth,"Binary trees, b+trees, heaps, etc are very low level technologies. They are baked into nosql fast access data services like Redis, DynamoDB, etc.",hhg5i6z,t3_qc6rth,1634785955.0,False
qbl118,"https://www.coursera.org/learn/algorithms-part1

https://www.coursera.org/learn/algorithms-part2",hhaah8e,t3_qbl118,1634679562.0,False
qbl118,Awesome thank you!,hhaazfy,t1_hhaah8e,1634679777.0,True
qbl118,"You want the book Introduction to Algorithms, and basically any content at all by Charles E. Leiserson. Their lectures are on youtube, and so forth.",hhad84r,t3_qbl118,1634680751.0,False
qbl118,MIT open courseware,hhaiz0x,t3_qbl118,1634683324.0,False
qbl118,Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms),hhb0iza,t3_qbl118,1634691554.0,False
qbl118,"[https://www.youtube.com/watch?v=HtSuA80QTyo&list=PLUl4u3cNGP61Oq3tWYp6V\_F-5jb5L2iHb](https://www.youtube.com/watch?v=HtSuA80QTyo&list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb)

It's a good introduction also you should practice and solve problems if you want to really get better. I recommend [https://leetcode.com/](https://leetcode.com/) go for its problems and if it's hard for you just read the solution.

don't be afraid of solving algorithm problems just go for easy ones and after solving some you will notice your progress.",hhccw5m,t3_qbl118,1634723087.0,False
qbl118,Introduction to algorithms by cormen is pretty good and extensive,hhdxvv7,t3_qbl118,1634751497.0,False
qbl118,"Important to know the theory, but useless until you apply it. Learn something and go to leetcode for practice.",hhbw9pz,t3_qbl118,1634708734.0,False
qbl118,"The canonical text is CLRS. That's the acronym for the authors' names, the book title is *Algorithms*.",hhbgfn9,t3_qbl118,1634699185.0,False
qbl118,I used youtube,hhcpsuv,t3_qbl118,1634732413.0,False
qbkvnl,"ASCII is old, Unicode is new, so Unicode is what you do.

Simple rhyme, simple message, true though. Unless you’re working on older systems like me with xbase++ which is a still-developed sidekick of Clipper. For any import export operation I have to closely catch any needs to translate ascii<->Unicode. When possible stick with Unicode, so my rhyme in first place remains true.",hha5uh2,t3_qbkvnl,1634677603.0,False
qbkvnl,"The first 127 symbols in the Unicode are the 127 ASCII symbols. So Unicode is backwards compatible; an extension.

There is only one benefit to ASCII, from the top of my head: It has constant size characters. With UTF-8, the size of the string (in bytes) is not necessarily the length of the string (in characters).",hha73ey,t3_qbkvnl,1634678126.0,False
qbkvnl,"ASCII and Unicode are two alternative ways of encoding text as bits. Other comments have addressed the technical details: ASCII is older, and includes 127 characters, each one byte long. Unicode is newer, and after the first 127 characters (which are the same as in ASCII for backwards compatibility) may require additional bits for a total of about 144K unicode characters. From an end-user perspective, however, the big difference is that ASCII is English-alphabet-only! Even if you're writing software for an English user-base, many English-speaking people have accent marks in their names. Writing software that doesn't support non-English characters is likely to limit the experience of your users, even if ASCII is smaller and a little easier to reason about.",hhbkuho,t3_qbkvnl,1634701555.0,False
qauqn2,"They are indeed equivalent. But in some cases the argument is much easier when you use strong induction, and the reason is you just assume more, so you have more to play with. However in many cases simple induction is enough and you don't need the extra assumptions.",hh5kd56,t3_qauqn2,1634590341.0,False
qauqn2,"Both of these are consequences of the least element principle.  
By the way, I strongly suggest using quantifiers in your formulations in general.",hh5kzvs,t1_hh5kd56,1634590691.0,False
qauqn2,"To add on, both Induction and Strong Induction can be derived from the Well-Ordering Principle, but the opposite is true as well: you can derive Well-Ordering from either Induction or Strong Induction :D",hh69xkh,t1_hh5kzvs,1634602592.0,False
qauqn2,"Yes, they are equivalent. Strong Induction is stronger, as the inductive hypotheses is more general, just as you describe.

From a practical point of view, often having that extra generality is very useful. A particular CS example is when dealing with recurrence relations. Say you want to prove that  F(n) = F(n-2) + F(n-1)  is the (insert the equation for the nth Fibonacci number). Strong induction gives us both F(n-2) is the (n-2)th Fibonacci number and F(n-1) is the (n-1)th Fibonacci number. Weak Induction would only give use the F(n-1). 

Another sign that you want use strong induction, is if you want to make claims about all numbers smaller than n. A common proof technique is to select the smaller number n for a which a property holds. A good example of this the proof of prime factorization. Basically, any time where proving the induction step is based on more than just one number, you want to use Strong Induction. 

The question you might want to consider is if Strong Induction gives a more general inductive hypothesis, why both with Weak Induction.
I don't have a good answer for it, but it's an interesting question to ponder.",hh63of0,t3_qauqn2,1634599567.0,False
qauqn2,Sometimes with certain recursive formulas it’s much easier to use the strong form when you have a few different layers of recursion and are trying to figure out a discrete formula.,hh5y0ei,t3_qauqn2,1634596846.0,False
qauqn2,It’s also useful when you are working with multiple base cases,hh73xme,t1_hh5y0ei,1634618002.0,False
qauqn2,"A simple example where strong induction really shines is factoring. 

Suppose I want to show every integer is the product of primes. Well I can start with an integer n, and if n is prime itself we’re done, so it must be composite. Then n=ab and we’d like to use our inductive hypothesis on a and b, but all we have for sure is that a,b<n if we’re using weak induction we are very sad, but if we’re using strong induction then we’re done.",hh6b58a,t3_qauqn2,1634603179.0,False
qauqn2,"What everyone else said is right.

As a possible minor benefit of strong induction, especially for a CS context: It more resembles structural induction.  Take for instance ""Consider the language formed by the formation rules: epsilon | (e) | e\*e.  Every expression has an even number of parentheses.""  You wouldn't want to waste energy indexing every word in the language with a natural number, only to find that the inductive hypothesis for a sentence indexed by n won't then help you prove the inductive claim.

Much better to approach this with structural induction:  Prove the claim for the base-case of a ""zero-complexity"" word.  In this case that just means the word ""epsilon"" with no parentheses.

Then assume S is a word formed by rule 2 or 3, and suppose that we already know that *any sentence of less complexity* (this is the part that's a lot like strong induction) has an even number of parentheses.  Either S = (e) or S = e1\*e2.  In the former case e has an even number of parentheses and so S has 2+(that many) and so S has an even number of parentheses.  In the latter case e1 and e2 each have an even number of parentheses and S has (that many)+(the other many) which is an even plus an even.  So again S has an even number of parentheses.  Therefore in all cases S has an even number of parentheses.

So anyway, point being, strong induction is very similar to the structural induction proof above, because we make a blanket assumption for the inductive case: that the claim holds for everything ""less than"" the current case.",hh6dcvp,t3_qauqn2,1634604251.0,False
qauqn2,"They are equivalent. As you point out, proofs using strong induction will be at most a few words shorter than proofs using induction.

But there are places where something like strong induction is needed. And that's when you're not doing induction over the natural numbers. Sometimes you might want to prove that something is true for all trees. There it's more natural for the induction hypothesis to look like ""If something is true for all subtrees of the tree T, then it is true of T as well"". A tree doesn't have a unique ""largest subtree smaller than it"", as opposed to a number n that has n-1. So the strong version is more suitable.",hh72qsi,t3_qauqn2,1634617239.0,False
qb42z7,"It’s more general than that. Its a field where you have terms or expressions and  rules that rewrite them. E.g. `map a . map b = map (a . b)` is a rewrite rule describing map fusion for some functional language. Some compilers (eg ghc) have support for rewrite rules as part of their optimisation passes. The study of the field is more formal/general, covering things like does a given system of rules and expressions always terminate, etc",hh7gmvr,t3_qb42z7,1634627797.0,False
qai7co,Ben Eater on youtube made a computer from ICs. He explains how everything works along the way. I highly recommend you watch,hh3ajqu,t3_qai7co,1634551070.0,False
qai7co,I opened this thread expecting Ben Eater to be the top comment and I'm pretty dismayed that he's not.  Emphatically seconded.,hh47sze,t1_hh3ajqu,1634570134.0,False
qai7co,dude is awesome,hh4z6q6,t1_hh3ajqu,1634581475.0,False
qai7co,The first few videos of [Crash Course Computer Science](https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) cover this. Highly recommend the entire series.,hh33sp8,t3_qai7co,1634544876.0,False
qai7co,Thanks!,hh3411c,t1_hh33sp8,1634545095.0,True
qai7co,"This crashcourse is awesome. Easy to understand and fun to watch, but it‘s really just a crashcourse, it does not go too deep",hh3517r,t1_hh3411c,1634546037.0,False
qai7co,I'd say main problem with my teachers at University is that they only go from result-to-result without really explaining and I had quite a hard time understanding those concepts. The main problem with all this is how simple on/off can do such things?,hh3571x,t1_hh3517r,1634546188.0,True
qai7co,"You start with [transistors](https://simple.wikipedia.org/wiki/Transistor). You build [logic gates](https://simple.wikipedia.org/wiki/Logic_gate), like an [AND Gate](https://simple.wikipedia.org/wiki/AND_gate), from transistors. You build specialized circuits, like a [Binary Adder](https://simple.wikipedia.org/wiki/Binary_adder), from logic gates. You build slightly more generalized circuits, like an [Algorithmic Logic Unit](https://simple.wikipedia.org/wiki/Arithmetic_logic_unit) or ALU, from specialized circuits. Now you can add numbers from on/off signals. Addition is the basis of all computation a computer performs.",hh37jxz,t1_hh3571x,1634548377.0,False
qai7co,Happy cake day!,hh4qckd,t1_hh37jxz,1634577844.0,False
qai7co,"Code by Charles Petzold is a must read book for you if you want to understand this. It goes from on/off to CPU to simple OS.

It’s an easy and fun read if you really want to understand computer architecture. It will get you started on the right foot.",hh5oc6m,t1_hh3571x,1634592475.0,False
qai7co,"There's a book with a free course called ""from Nand to tetris"", it basically teaches what you are asking for by making you create a virtual version of all pc components starting from the chips and go all the way up to creating your own language.",hh33x0y,t3_qai7co,1634544989.0,False
qai7co,Appreciate it!,hh3423l,t1_hh33x0y,1634545122.0,True
qai7co,FYI the subject you’re interested in is named “computer architecture.”,hh346i5,t3_qai7co,1634545236.0,False
qai7co,Know any good sources on the subject?,hh34iu3,t1_hh346i5,1634545556.0,True
qai7co,"I learned from textbooks and hands-on experimenting, but google the term and see what you get.",hh3645f,t1_hh34iu3,1634547038.0,False
qai7co,"If you want real life hardware examples and explanations, check out the channel ""Ben Eater"" on yt!",hh35id5,t3_qai7co,1634546483.0,False
qai7co,"http://www.infocobuild.com/education/audio-video-courses/computer-science/cs61c-spring2015-berkeley.html
This is the best course u will find. Professor is awesome. They start from C language then assembly the computer architecture and logic gates.

There is one other course from MIT if u want, just search OCW computation structures.",hh3hpbl,t3_qai7co,1634556662.0,False
qai7co,"Grab a copy of the book Digital Computer Electronics by Paul Malvino, it covers a computer architecture used for teaching what you want to learn, called the ""Simple As Possible"" architecture, SAP.",hh3tft8,t3_qai7co,1634563556.0,False
qai7co,"I don't know which architecture will be teach to you but you could take one digital design book and check out this new game [Turing Complete](https://store.steampowered.com/app/1444480/Turing_Complete/). The game don't teach you, because of that i suggest check this game with a book. this might help a lot for better visualization and trial and error for learning.",hh3wvbl,t3_qai7co,1634565241.0,False
qai7co,"[Sebastian lague released this video](https://youtu.be/QZwneRb-zqA) on how computers work starting with a basic binary system and on/off switches that could be represented in hardware as physical electric current toggles. From there, you just keep combining small modules to achieve the ability to perform arithmetic and store memory.",hh43kul,t3_qai7co,1634568312.0,False
qai7co,"You will probably want to learn about *digital electronics* then, including things like logic gates (AND, OR, NOT, XOR, etc). Then you can check out stuff like binary adder circuits to get a sense for how it does mathematical computation.",hh5ww1k,t3_qai7co,1634596324.0,False
qai7co,"I can't recommend the book ""But How Do It Know"" by J Clark Scott. Starts with simple gates and works up to a fully working computer.",hh658bv,t3_qai7co,1634600311.0,False
qai7co,There's a pretty good book by William Stallings called Computer Organization and Architecture,hh6muc9,t3_qai7co,1634608763.0,False
qai7co,Read Elements of Computibg Systems and do the work.,hh96mr2,t3_qai7co,1634663699.0,False
qb5blt,"Your credit card is run via a completely different company afaik. They just sell through your bank. I've had issues using it as ID in branch for the same reason. They don't access it.

I guess the technical infrastructure for debit cards would have to be massive as there's so many of them.",hh7eo9w,t3_qb5blt,1634626075.0,False
qb5blt,"That actually clears things up, thanks.",hhfway4,t1_hh7eo9w,1634781508.0,True
qabacx,"Calculus is heavily used in some fields of computer science, particularly in computer graphics and machine learning (a little bit of the analysis of operating systems and networks as well).  Ideas from calculus (in particular ideas about limits and sequences and series) are also really important when you start to analyze algorithms, later on in your coursework.  

You'll use some other topics in ""heavy mathematics"" more often, like graph theory!",hh1uxy9,t3_qabacx,1634516884.0,False
qabacx,"I’m actually just learning computer science as a hobby since I’ve already completed courses in I.T related work and programming as well. However computer science is way different from what I expected so far so I’m just making sure that this is something that could fit me right.
I’m not bad at math but it doesn’t change the fact that it’s still scary :D",hh1vc66,t1_hh1uxy9,1634517083.0,True
qabacx,"Don't be scared!!    You don't need to be some super math genius to do computer science, and there's a lot of really cool computer science that isn't very mathematical at all... 

These are just the areas of cs that use more calculus.",hh1vtdj,t1_hh1vc66,1634517324.0,False
qabacx,"Oh well we’ll see soon enough for sure :) so far I’ve only learnt about the binary and hexadecimal system which aren’t really anything special so hearing about all that stuff made me think a little
Thanks a lot for the help tho!",hh1wh4x,t1_hh1vtdj,1634517651.0,True
qabacx,"Remember, that there are other majors that are much harder than CS.

Would you rather try and fail or fail by not trying?",hh2ryvj,t1_hh1wh4x,1634535094.0,False
qabacx,"Mathematic is much more of a training thing than most people realize. Most ideas are quite simple, too, it's just a bit of a language barrier. There is hard stuff, as well, ofc, but that's usually nothing you come across in the undergrad.

Not sure where I read it, but there is almost no person who can just read through a math textbook. Sometimes it takes hours to ponder over a single paragraph.

So, don't be scared. It just takes a bit more effort :-)",hh33hj5,t1_hh1vc66,1634544582.0,False
qabacx,"Well hello there, I'm about to have a course on Computer Graphics and could you be kind enough to say where Calculus in particular may show up? I thought Linear Algebra would be enough for basics and is it for something advanced?",hh30z7g,t1_hh1uxy9,1634542316.0,False
qabacx,"Linear Algebra is a course that builds off of calculus, so if there’s Lin Al, there’s also some amount of calculus. Plane intersections and the like are all taught in multivariate calculus, but are topics within Linear Algebra as well.",hh3gipa,t1_hh30z7g,1634555839.0,False
qabacx,Thanks for the response.,hh3mzcg,t1_hh3gipa,1634560037.0,False
qabacx,"I finished high school as someone who was “characteristically bad at math” and I initially dreaded the math in CS. I’ve found though as time goes on that CS has allowed me to approach math through a more enjoyable lens. It’s not nearly as bad as I was worried it would be. You don’t have to be a *math person* to learn computer science. 

Don’t get me wrong though, there’s a decent amount of math. As a computer science student I’m required to complete courses in calculus (thru calc 3), computational statistics, probability and linear algebra. In most of my classes so far we’ve used math to some degree for proofs, run time calculations, etc. Topics like Machine Learning and AI can be very math and computationally heavy.",hh25l8k,t3_qabacx,1634522211.0,False
qabacx,I've been getting into audio programming and digital signal processing and it seems like a calculus-heavy field.,hh1yem2,t3_qabacx,1634518610.0,False
qabacx,"You're moving more into EE territory though so that's not exactly typical CS. It definitely is math heavy though with lots of convolution and transforms. Thankfully the latter can be used to turn most problems into algebra or simple arithmetic, but it's still a heavy field as you said.",hh3k01p,t1_hh1yem2,1634558186.0,False
qabacx,I'm pretty interested in learning some more about this. Do you have any recommended sources you've enjoyed?,hh2ar9j,t1_hh1yem2,1634524870.0,False
qabacx,"https://www.amazon.com/Digital-Signal-Processing-Computer-Perspective/dp/0471295469?ref_=d6k_applink_bb_marketplace
I've found this book.",hh4d7wv,t1_hh2ar9j,1634572432.0,False
qabacx,"Check out these YouTube channels: The Audio Programmer, JUCE, and the Audio Developer Conference",hh4eyeo,t1_hh2ar9j,1634573157.0,False
qabacx,"In terms of calculus specifically, the thing you’ll need the most learning computer science is an intuitive understanding of what a derivative is and what an integral is. Not in the abstract mathematical sense with symbols (although that is useful and necessary if you want to go deeper than surface level into the areas like machine learning where calculus is relied upon more deeply), but in terms of understanding the fundamental relationship between a function and its derivative, what zeroes, minima, and maxima mean in terms of a functions derivative or integral, etc.

For that sort of stuff, college courses can be hit or miss (especially at the intro level like Calculus 1 where the professor is far more likely to be following the traditional approach of presenting formal definitions and working through problems rather than approaching intuitively and then formalizing). If you take one that is a miss in that regard, look to online videos on YouTube channels like 3blue1brown where they take a more intuitive approach. It can be very helpful, especially if you’re able to contextualize those intuitive explanations with the formal definitions you will probably learn from a class.

——

As I get into my thoughts on Math required for Computer Science in general, it’s probably useful to point out where I’m coming from on it. For my undergraduate, I double majored in Computer Science and Math. I’ve been working as a Software Engineer for 3 years since then and I’m one year into my MS in Computer Science (part time).

I consider Computer Science to be a subfield of Mathematics, but the line is a bit blurred between the fields of Computer Science and Software Engineering. Many people will go into a CS degree expecting to learn how to code and practical skills for developing software and be surprised by the mathematical side.

That being said, many undergraduate CS degrees, including my own degree, are catered more towards this audience and therefore teach many practical skills and focus on the theoretical concepts that are more useful in software engineering (e.g. design patterns, agile methodology, requirements analysis, software testing).

Therefore broad math knowledge ends up being really useful, but deep knowledge is only necessary for the specific field(s) of math used in whatever area(s) you dive more deeply into (i.e. beyond an introductory course in the subfield).

Some examples:

Computer graphics makes heavy use of linear algebra. I’ve played around with some low-level 3D graphics programming, and I found myself doing matrix multiplication quite frequently as I tried to make sense of the transformations I had to do to map object vertex coordinates to world space and to screen space.

If you do anything with Simulation or Game Development, in addition to the graphics stuff, you’ll need to understand basic physics, which is applied Calculus.

If you want to optimize an algorithm, you will need to do some algebra to find the asymptotic runtime of the algorithm. This can get pretty complex, especially when recursion is involved, and limits are commonly used.

Some subfields of machine learning (including deep learning) rely heavily on derivatives, but not just the kind you’ll see in Calculus I, but partial derivatives too. This is the one place in CS where I’ve used anything I learned in Calculus III.

Anything low-level or on the hardware side is going to run into Boolean Algebra. This is where the idea of CS as a subfield of Math comes in, because you’re unlikely to learn this in a Math class, but many CS programs have a course that covers it while teaching about digital circuits. Still, the field is much closer to what I learned in the math course Abstract Algebra (it covered groups, rings, and stuff like that).

Cryptography touches Boolean Algebra, Group Theory, and I think some Geometry as well. It can and will probably pull in many fields of math, if it can leverage them to create difficult one-way “trapdoor” problems. If linear algebra isn’t used there, it probably will be in the not-too-distant future.

Graph Theory comes into play in many areas of Computer Science, especially when networks are involved, but also in Parsers/Compilers.

I’m sure there’s really big obvious one’s I’ve left out, but there’s a few examples of post-secondary math used in Computer Science.",hh2z3mb,t3_qabacx,1634540688.0,False
qabacx,"> I consider Computer Science to be a subfield of Mathematics, but the line is a bit blurred between the fields of Computer Science and Software Engineering.

As someone with an exceptionally similar background (bachelors in CS with a minor in math, and a masters in CS), I’ve always seen Computer Science as a field similar to architecture.

It’s heavily based in math, and you wouldn’t be able to work out most of the underlying features without math, but the subject itself encompasses things outside the scope of math (building actual buildings; writing code) and wouldn’t just be considered a subfield. At my company for example, I don’t think many of the employees who actually write code would say that what they do has to do with math, more than systems planning and whatnot.

If you exist in theoretical computer science, you’re correct in that it’s exceptionally similar and has a ton of overlap with pure math (complexity theory being one of those topics you mentioned). As soon as you get into practice/industry though, it becomes fairly detached from math.

Since the OP seems to be self-teaching CS, it seems a lot more likely they’d be able to pursue what they wanted without needing to delve into a ton of math (unless their goal is to do one of those math heavy things lol) like you or I had to do as part of our coursework.

> Therefore broad math knowledge ends up being really useful, **but deep knowledge is only necessary for the specific field(s) of math used in whatever area(s) you dive more deeply into** (i.e. beyond an introductory course in the subfield).

And exactly this

> If you want to optimize an algorithm, you will need to do some algebra to find the asymptotic runtime of the algorithm. This can get pretty complex, especially when recursion is involved, and limits are commonly used.

I mentioned this is another comment, but algorithm optimizations can go arbitrarily far into math. For my Masters I was working on implementing bounded distance calculations for things like k-Means and KNN, and since that touches problems used in ML, the math was way heavier.",hh3nfkm,t1_hh2z3mb,1634560298.0,False
qabacx,"The reason I say it’s a subfield of math is because I don’t count Software Engineering as part of Computer Science, but rather as a separate field that makes practical use of Computer Science. However, I do recognize that many Computer Science programs are currently a mixture of both fields and the most likely job outcome of a degree in CS is to become a Software Engineer, so when I’m not being pedantic, I do merge the two a bit (which I did a bit towards the end of my comment).

So for the architect example, if there was no such thing as an Architecture degree, but you could become an architect by getting a Physics degree, then it wouldn’t change the definition of Physics, even if a good number of Physics programs started offering electives purely about Architecture.

But yeah, the only reason I led with the pedantry was to call out the distinction so if that was not already clear to the OP, they might start to look into what exactly it is they are trying to learn, whether that’s theoretical CS for the sake of knowledge, or just enough to be practical in programming.

For the latter, my recommendation is the book _Cracking the Coding Interview_, which in addition to the expected practice problems and advice on interviews, has some textbook-style information on a broad range of topics. I often recommend it to self taught developers since it’s basically all the important parts of a CS degree.",hh75o31,t1_hh3nfkm,1634619171.0,False
qabacx,"> The reason I say it’s a subfield of math is because I don’t count Software Engineering as part of Computer Science, but rather as a separate field that makes practical use of Computer Science.

Which I wanted to clarify is a “you” thing. I’ve never seen someone argue that CS is strictly a subset of math when it contains things well beyond the scope of a math degree.

> So for the architect example, if there was no such thing as an Architecture degree, but you could become an architect by getting a Physics degree, then it wouldn’t change the definition of Physics, even if a good number of Physics programs started offering electives purely about Architecture.

But architecture is significantly more than just physics, in the way CS is significantly more than math. Architecture isn’t just making a building that doesn’t fall over, it’s about art as well. If you were to kneecap architecture and lock it wholly within physics, our cityscapes would just look like someone copy and pasted the same building over and over. The portion of the field you’re dropping off in your explanation is sort of fundamental to the field, and not something you can ignore. That’s why *I* went pedantic and pointed that out.

> whether that’s theoretical CS for the sake of knowledge, or just enough to be practical in programming.

Based on some of the other comments, the OP seems exceptionally fresh to this realm, with binary and hexadecimal bases being one of the more recent concepts they learned, so I think it’s okay to be a little less verbose with things. With how wide our field is, describing it as such to folks who are only interested in a narrow sliver of information can be a daunting turn off. Math is hard enough for a lot of folks, so if you present it as just a subset of math, and not a field which requires significantly more nuance, you run the risk of “scaring” folks away.

Just my 2 cents from trying to convince my engineering friends to pick up coding to help them in their courses/work, and then hearing what sort of arguments they use for not wanting to do that.",hh7xaq3,t1_hh75o31,1634642306.0,False
qabacx,"I don’t necessarily want to drag this on since I don’t even feel all that strongly about what I said, but I do want to be clear in what I did mean.

First, it seems you misunderstood my analogy. I was comparing Software Engineering and Computer Science to Architecture and Physics. I don’t think anybody would call Computer Science art, but people certainly consider art to be a part of programming. Any other engineering field would probably be a better comparison but I didn’t want to drag the whole licensure thing into it.

And I do want to point out that considering Computer Science to be a subfield of math isn’t just a “me” thing. A lot of people argue on either side of the matter. It comes down more to whether you consider Computer Science to be “the set of things taught in a CS degree” or what I described, where that is a larger set of topics that includes computer science.

It doesn’t help that everyone has a different idea not just of what computer science is, but also of what math is. Both have fuzzy borders. I googled a bit just now to make sure it wasn’t just some strange opinion I just happened to have heard from a few outliers, but it’s definitely a thing that people question. An answer I saw was that “theoretical computer science” is math but some non-theoretical aspects of the field are not. I think at the end of the day, since there’s no formal definition of what is or isn’t math or CS, nobody really cares to come to a consensus on it and they just agree to disagree.",hhnz0ky,t1_hh7xaq3,1634934877.0,False
qabacx,First and second derivatives are really important in numerical algorithms. Many real world problems are posed in such a way that you want to minimize a convex function. First and second order information is used to find a candidate answer as well as to say the candidate fits some criteria to accept it as the solution and terminate computation.,hh286mb,t3_qabacx,1634523541.0,False
qabacx,"Like the other comments mentioned, it really depends on *what* exactly in computer science you’re doing. It’s a huge field, so if you could specify what exactly you’re hoping to do with your knowledge, it’d be easier to tell you how much math you’d need to do that.

For example, you don’t need any serious math to make a webpage or write basic systems applications, but you would need a lot of math knowledge if you were interested in theory like algorithm design and machine learning.

I’m someone who minored in math in college (doing Numerical Analysis courses, Theory of Numbers, and the like) and I got my Masters where I focused entirely on theory (like Automata, Complexity Theory, Graph Theory, Compiler Optimizations), and I now work as a software developer who has to do like no math at all. It *really* depends on what exactly you do within computer science, as to how much math you really need lol",hh24n31,t3_qabacx,1634521725.0,False
qabacx,"Oh I mentioned it before I’m just practicing rn as a hobby so I’m just curious on what’s coming as I’ve already pretty much mastered pc/server building and programming in languages such as python/Java/C+
I just saw a lot of complicated algorithms earlier today and they’ve been bugging me a little",hh2d4ca,t1_hh24n31,1634526111.0,True
qabacx,"> I’ve already pretty much mastered ... programming in languages such as python/Java/C++

Trust me, if you’re just learning about hexadecimal and binary, then you’ve still got plenty to learn from those 3 languages ;)

If you’re just talking about like the basics though, then that’s a great set of languages to get involved with, since it’s a great *breadth* of languages.

> I just saw a lot of complicated algorithms earlier today and they’ve been bugging me a little

Could you maybe expand and point to some of the ones you mentioned (I’d also be happy to talk through those if you wanted)? Algorithm design can be math heavy, but is really dependent on what you’re doing.

For example, I worked on some stuff for my Masters related to bounding distance calculations using an approximation that took fewer CPU instructions, where the math required *was* higher dimensional calculus, but only in the proof, not the creation.

We were using this framework to help speed up various common machine learning algorithms and in order to better tweak our work to fit those you needed to bust out some fairly complicated stuff. Now, while figuring out what we needed to do required a lot of math, actually doing it didn’t require like any at all, because it’s more about understanding what problem you’re working on, and how the code will be able to mimic that, than any of the underlying math.",hh2iedw,t1_hh2d4ca,1634528959.0,False
qabacx,"I received my BSc in Computer Science and worked in industry for a while before moving to product management. I did novel undergraduate research in Computer Vision, and have written software deployed in fortune 50 environments. I’ve seen a bunch, but not everything. It’s hard to emphasize the size of this field.

Computer Science definitely requires math, but not necessarily the kinda of math you are used to calling math.

There’s a whole world of math we use in computer science like formal logic, finite state systems, automata, graph theory, algorithmic optimization, the list goes on. A better understanding of calculus is helpful, but not necessary for every discipline.

It’s a lot like IT, where you have a tool or approach to a problem, and as long as it “works” it’s kind of up to you the degree you wanna understand it. “It works” isn’t good enough for computer scientists, often we focus on eking out any little optimization we can because we think at scale. Approximation of changes due to scale require some math, and it enables us to make generalized comparisons across different problem sets.

A lot of it doesn’t feel the same as math, except when you’re taking an exam. I still have nightmares about “One knave only tells truths, and the other lies! If you can’t figure it out you repeat the semester! HeeHee!”",hh2b1ed,t3_qabacx,1634525016.0,False
qabacx,"A very valuable tool is series functions, like series addition or multiplication. These are actually pretty easy to do in code as most languages have some form of ""for-next"" loop. Recurring functions can produce results that single-pass equations can't always. But it's not the solution to everything, sometimes I write a complex function only to find some simple algebraic equation that does the same thing. Just another tool in the toolbox!",hh2b8dp,t3_qabacx,1634525117.0,False
qabacx,"Computer science is a very big field, and in a lot of subfields, maths is definitely needed. However, the specifics of maths needed might differ based on the specific computer science field you are studying. Calculus might be needed in fields such as optimization, control systems, computer graphics, etc. However, I would say that the biggest and most important branch of mathematics that is used in almost any subfield of computer science is discrete math: combinatorics, graph/tree, number theory (arithmetics), and other discrete structures.",hh2e9rt,t3_qabacx,1634526723.0,False
qabacx,"I dont know very much about graphics programming but i've heard it heavily uses calculus and linear algebra. Certain topics you learn in calculus will come up everywhere in computer science in general. Newtons method is used a lot in optimized math related code. Microcontrollers that need to do trig functions fast will store a table of values and a table of derivative values to approximate the values very very fast. Series, sequences, and recurrence is also used in complexity analysis and stuff.",hh2svxb,t3_qabacx,1634535771.0,False
qabacx,"It really depends, a lot of the math is really just to deeply understand what you are doing. For example, Machine Learning is one area which relies super hard on Math, but there are so many tools available which allow you to do all kinds of Machine Learning without using or even understanding any math at all.

The biggest part that math comes in handy is for interpreting and problem solving if the out of the box solutions/algorithms don't work. In the machine learning example, multivariable calculus provides so much insight into what is going on, that it can help you identify and resolve problems even if no one has ever taught you how.",hh3cka2,t3_qabacx,1634552808.0,False
qabacx,"You need discrete math / mathematical logic much more than calculus. I have read 100 papers in the last couple years, all of which use formal logic, none of which use calculus.",hh3hkua,t3_qabacx,1634556576.0,False
qabacx,"Web dev with sysadmin things on the side. Trudged through anything regarding theoretical maths and physics, didn't need them since. I assume it depends heavily on your field, of course I wouldn't need them for example :D

It is often the case that courses despite not being directly important do give you more spherical understanding of what's what, and I feel every single bit helps build your knowledge on various connected topics exponentially. However, most of it stays in the trash can for now in my case, basically anything beyond general concepts and considerations.",hh40ihd,t3_qabacx,1634566950.0,False
qabacx,"Fourier transform (not FFT) uses calculus if I recall.

Used to transfer audio data from amplitude-time to frequency-time and phase-time.

Used extensively in audio processing, and speech recognition software.

You'll find a lot of cases like this where something somewhat specific is implemented using the basis of calculus for other more widely used stuff.",hh5g8jp,t3_qabacx,1634588450.0,False
qabacx,i feel the same fear. same position .,hh5w2fh,t3_qabacx,1634595938.0,False
qah5lk,"Computers can't deal with irrationals - they use integers or an approximation of the real numbers called ""floating point numbers"". Even for floats alone there are usually multiple algorithms: one for integer and one for non-integer exponents. Rationals are usually not supported directly by the hardware and are instead ""simulated"" on the software level, so you might find any algorithm here.",hh2zsf5,t3_qah5lk,1634541285.0,False
qah5lk,"I know computers don't support irrationals but what I am asking is, for example, 3\^4 is an easy task a computer just multiplies 3,  4 times with itself. However, 3\^e cannot be done in the same way, so maybe computers use series to calculate.  

>there are usually multiple algorithms: one for integer and one for non-integer exponents

Thanks, this answer is what I was seeking. Besides, if you possess /know any article about the topic please share, I couldn't find much.",hh30avk,t1_hh2zsf5,1634541722.0,True
qah5lk,"It depends on the program. Some intended for scientific calculations will recognise e and use an efficient algorithm that gets an accurate result. Some will just truncate e to a certain precision as soon as they need to calculate it.

A nice example which worked until very recently but I can't seem to replicate now was typing into Google search (0.1*0.1)/0.1. The answer was 0.09..... because google simply truncated 0.1 (which in binary has an infinitely recurring repeat after the point) instead of calculating it another way.",hh32av6,t1_hh30avk,1634543489.0,False
qah5lk,"Your calculator example might be related to this [https://dl.acm.org/doi/pdf/10.1145/3385412.3386037](https://dl.acm.org/doi/pdf/10.1145/3385412.3386037) It's been a while since I read it but iirc it targeted specifically calculators (I think the windows calculator uses it?) with the goal of removing the cases that really fuck with ""normal people""'s knowledge of math. There's also a talk about it on youtube if you're interested.",hh32yud,t1_hh32av6,1634544104.0,False
qah5lk,I was talking about a specific case with Google's built-in calculator that was still a problem as late as 2019. Thanks for your help though!,hh37tq9,t1_hh32yud,1634548633.0,False
qah5lk,"I don't know of any articles in particular but

* to check hardware support you can check out the instruction set of the platform (e.g. for x86 you'll find an instruction to compute `x log_2(y)` which can be used to write a clear (but not necessarily good) exponentiation algorithm)
* to see how compilers handle it check out their generated assembly (either directly or via [https://compiler-explorer.com/](https://compiler-explorer.com/) )
* to see how people actually implement it (probably black magic bit-hackery) check the source code of math libraries. e.g. for C: [http://www.netlib.org/fdlibm/](http://www.netlib.org/fdlibm/) (in particular the e\_pow.c file)",hh32gqv,t1_hh30avk,1634543637.0,False
qah5lk,"thanks, I'll definitely look.",hh3ncf4,t1_hh32gqv,1634560247.0,True
qah5lk,e is estimated as a rational number. as far as the computer is concerned its exponentiating a rational by a rational.,hh3at0q,t1_hh30avk,1634551300.0,False
qah5lk,It depends... generally for floating point numbers yes.  Big int libraries optimize integral powers,hh42a75,t3_qah5lk,1634567744.0,False
qakzly,"I haven't had the chance to ponder this too much, but is there a reason to think that this might be an NP problem? I've very skeptical of an O(N\^2) solution for this that works with all possible inputs (and I'm generally skeptical of any greedy algorithm).

Is it possible to construct a pathological example for a given N that would cause a combinatorial explosion in flows to be considered? Is there a way to prove that that's not possible due to, say, the transitive nature of the exchanges?",hh3od24,t3_qakzly,1634560833.0,False
qakzly,"Personal experience, GeekForGeeks should not be trusted blindly. There's a lot of good and bad articles all mixed in there.",hh3kgd8,t3_qakzly,1634558481.0,False
qakzly,But I cant seem to find any solution for this question anywhere on the internet other than the method described in the article .,hh3l65o,t1_hh3kgd8,1634558935.0,True
qakzly,">to find any solution for this question anywhere on the internet other than the method described in the a

This algorithm doesn't exactly minimizes number of transactions as pointed out by the first comment on your post.",hh3vtqc,t1_hh3l65o,1634564741.0,False
qakzly,"No need to be shy. It's simply wrong.

And pretty sure it can reduce to subset sum question, by making everyone's balance as number in the set, and then # of transactions <= n-2 is equivalent to finding a subset of zero sum.",hh3rr6r,t3_qakzly,1634562678.0,False
q9x87e,Thanks.,hh1m6xk,t3_q9x87e,1634512635.0,False
q9u6uz,"try writing the whole number out. you would be left shifting instead of right shifting then. also remember the implied 1. 

&#x200B;

1.1101011011\*10\^10001 first number ignoring the bias of 15 (dont ignore if its homework)  
1.0001100101\*10\^01001 second number the same way

expand them  
111010110110000000.0  
1000110010.1

add them  
  111010110110000000.0  
\+000000001000110010.1  
\-------------------------------------  
  111010111110110010.1

round it  
  111010111110000000.0

figure out exponent  
1.110101111100000000 its 10001

subtract one and remove digits past 10:  
1101011111

combine exponent with fraction and sign bit:

0100011101011111  


I'm pretty sure your mistake was either forgetting the 1 before the fraction or shifting the number wrong",hgyvupj,t3_q9u6uz,1634465580.0,False
qa3kdv,"If by ""computational method"" you mean ""algorithm"" you might like the Algorithm Design Manual, by Skiena. It's a good introductory choice.",hh1qwox,t3_qa3kdv,1634514895.0,False
qa3kdv,The Comprehemsive algorithms book is Cernan et al. Intriduction to algorithms,hh42eq6,t3_qa3kdv,1634567798.0,False
q9waxg,"Well, I guess that's an answer of sorts.",hhcnu11,t3_q9waxg,1634731224.0,True
q9exqs,"Doesn't exactly fit into pure computer science, but the Darknet Diaries podcast is fantastic and he discusses computer security without all of the buzzword sensationalism.",hgvmdiz,t3_q9exqs,1634403193.0,False
q9exqs,Thank you just added it!,hgxb3yk,t1_hgvmdiz,1634430056.0,True
q9exqs,Scott Hanselman's podcast is decent. As is the Stack Overflow podcast.,hgvjpgg,t3_q9exqs,1634402016.0,False
q9exqs,Thank you. Adding both!,hgvlqco,t1_hgvjpgg,1634402914.0,True
q9exqs,"The Lex Friedman podcast is fantastic. He certainly has ventured into other topics, but has a strong passion for AI and a massive amount of brilliant guests!",hgxa0l5,t3_q9exqs,1634429545.0,False
q9exqs,Agreed. I have him: https://app.cicero.ly/people/427089628,hh06y8u,t1_hgxa0l5,1634490734.0,True
q9exqs,"""People on social media talk about people and things, I just want to do what the computers want""",hgvurcd,t3_q9exqs,1634406754.0,False
q9exqs,Bjarne Stroustrup,hgw7x9i,t3_q9exqs,1634412398.0,False
q9exqs,Add [CoRecursive](https://corecursive.com/)!,hgw8mi8,t3_q9exqs,1634412693.0,False
q9exqs,Nice one! Done,hgwelkq,t1_hgw8mi8,1634415294.0,True
q9exqs,I think Ben Eater has a great YouTube channel for the fundamentals,hgxbfb2,t3_q9exqs,1634430200.0,False
q9exqs,What exactly amazing  has done Lex in computer science besides that amazing comp sci/physics/science/deep learning podcast?,hh04068,t3_q9exqs,1634489487.0,False
q9exqs,it's Fridman. he literally says it in the beginning of his podcast.,hhgct3o,t3_q9exqs,1634790009.0,False
q9exqs,[Bryan Cantrill](https://youtu.be/cuvp-e4ztC0),hiegiiw,t3_q9exqs,1635439974.0,False
q9e5oo,"Worse yet, it's not really about computers!",hgvb5nj,t3_q9e5oo,1634398237.0,False
q9e5oo,"“Computer science is no more about computers than astronomy is about telescopes.""",hgyhm6f,t1_hgvb5nj,1634454017.0,False
q9e5oo,It's like that holy roman empire.,hgyrh3b,t1_hgvb5nj,1634462052.0,False
q9e5oo,"Does it matter? Call yourselves what ever you like. Computer engineer, computer scientist, computerologist, programmer or developer. I like to switch it up depending on what I'm working on. If I'm working on the web I'm a developer. If I am working on research, I am a scientist. If I am programing in c or c++, I am an engineer.",hgw4urz,t3_q9e5oo,1634411094.0,False
q9e5oo,IMHO you're an Engineer if you have a degree in Engineering.,hgyirvd,t1_hgw4urz,1634454906.0,False
q9e5oo,"""Let's see: you're a undergraduate computer science student with emphasis on...""

*\*squints\** 

""Software ~~Engineering~~ Creationing""",hgys1cl,t1_hgyirvd,1634462511.0,False
q9e5oo,"Lol when I was in school, computer science was considered part of engineering",hgz3e30,t1_hgys1cl,1634471396.0,False
q9e5oo,It is where I studied engineering.,hgz7glx,t1_hgz3e30,1634474083.0,False
q9e5oo,"Well, a title verifies that you have that knowledge. That doesn't mean you can't have that knowledge and be an engineer without the title.",hgyznll,t1_hgyirvd,1634468611.0,False
q9e5oo,That logic breaks down if you apply it to lawyering or doctoring.,hgz7c1r,t1_hgyznll,1634474008.0,False
q9e5oo,"One thing is what a job or the state requires yo to have (a title, sometimes), and other thing is what jobs you're capable to do.",hgzj254,t1_hgz7c1r,1634480267.0,False
q9e5oo,"The laws are there for a reason. Just because it isn't enforced in the US yet, doesn't mean it won't be. Requirements for calling yourself a Software Engineering will come sooner than you think. At first for particular industries like finance and infrastructure, then more generally. The only reason it hasn't come yet is because it is such a young field (60 years vs thousands of years for doctoring and lawyering).",hgzmxnl,t1_hgzj254,1634482048.0,False
q9e5oo,"Yeah, old minded people will try to force it, like everything",hgzyn64,t1_hgzmxnl,1634487183.0,False
q9e5oo,"If they ask at the airport desk: computer engineer, if someone asks randomly: programmer.",hgxnfm4,t1_hgw4urz,1634435987.0,False
q9e5oo,Computer engineers are specialized electrical engineers. Try software engineer.,hgyx6a1,t1_hgxnfm4,1634466655.0,False
q9e5oo,"Actually the translation from where I from is systems engineer, which corresponds to the degree of systems engineering that is very similar to computer science/sofrware engineering usually with a bit more physics and hardware stuff.",hgzelt7,t1_hgyx6a1,1634478061.0,False
q9e5oo,"Those mean different things to me:

**Programmer**: Anyone who can type code. It says nothing about your skills in developing software.

**Developer**: You know how to build software, more than just scripting.

**Computer Scientist**: You develop algorithms, be that with pen and paper or whatever. 99% of them are academics. Often their actual software developing isn't that high, and would need someone else to commercialize their algorithms.

**Computer Engineer**: You have a formal degree in engineering, specializing in computer engineering. Generally you work on hardware. That said, this is different from country to country, notably in the US anyone can be an Engineer with any background.

Then also:

**Software Engineer**: You have a formal degree in engineering, specializing in software engineering. Generally you work as a developer building large/complicated systems. That said, this is different from country to country. In the US being a software engineer and a developer is probably synonyms.

In general: If you wrote computer/software engineer on your resume here in Norway and didn't have a formal education in it, I'd automatically disqualify you as a fraud.",hgz6x43,t1_hgw4urz,1634473758.0,False
q9e5oo,I swear to God. If I hear someone call themselves a Computerologist..... Well I'll just lose my mind!,hgz394n,t1_hgw4urz,1634471299.0,False
q9e5oo,"Up in here, up in here!",hgzi4f2,t1_hgz394n,1634479812.0,False
q9e5oo,Ya'll gonna make me go all out,hgzmtsl,t1_hgzi4f2,1634481999.0,False
q9e5oo,Similar to how mathematics is not a science but its often called that.,hgw659r,t3_q9e5oo,1634411645.0,False
q9e5oo,"We should continue referring to it as computer science if for no other reason than ""computology"" is a fucking stupid moniker.",hgvg58c,t3_q9e5oo,1634400431.0,False
q9e5oo,Is calcomancy okay? May I be a Computemancer?,hgw1w7j,t1_hgvg58c,1634409836.0,False
q9e5oo,"The politically correct term, is Technomancer.",hgxikdc,t1_hgw1w7j,1634433604.0,False
q9e5oo,I feel like Ray Kurzweil is the only true technomancer tho. I could never 😮,hgxzoz0,t1_hgxikdc,1634442286.0,False
q9e5oo,"A prof of mine in college preferred ""procedural epistemology""",hgxm7oz,t1_hgvg58c,1634435375.0,False
q9e5oo,"Computer science is a misnomer. I like to describe my field as ""the mathematical study of the mathematical idea of the computation"". I distinguish computer science as a distinct discipline to computer programming.",hgya0j8,t1_hgvg58c,1634448452.0,False
q9e5oo,I think just Computation or Computing would've been a better name.,hgyl9c8,t1_hgvg58c,1634456932.0,False
q9e5oo,We shall call us techpriests from now on!,hgyqffk,t1_hgvg58c,1634461170.0,False
q9e5oo,"Science in other languages is often used for ""discipline"" rather than the English ""empirical study of a natural phenomenon"". I think it works for this case as the discipline use.",hgvvaif,t3_q9e5oo,1634406992.0,False
q9e5oo,What languages? I speak 3 romance languages and all of them hold a very similar meaning to english. Scientia in latin.,hgw3x72,t1_hgvvaif,1634410693.0,False
q9e5oo,"German has the word ""[Wissenschaft](https://en.wikipedia.org/wiki/Wissenschaft)"", which is a broader concept than science and includes many kinds of academic inquiry. At least Swedish, Finnish and (according to the Wikipedia article) Polish have words with a similar broad meaning.

Neither is there a direct translation for these words in English, as far as I know, nor is there necessarily a direct translation of the word ""science"" with a similar scope in those other languages. I don't know all of those languages well enough to be sure of the latter in all of them, though.",hgwcyec,t1_hgw3x72,1634414566.0,False
q9e5oo,Very interesting thanks!,hgwohp4,t1_hgwcyec,1634419615.0,False
q9e5oo,Yep I heard it first from German,hgwq8ck,t1_hgwcyec,1634420407.0,False
q9e5oo,"To add to that nice piece of information:

In Dutch the equivalent of that is ""wetenschap"" and is also used to mean academic inquiry. It can be loosely translated as ""knowledge"" as well, being a concatenation of ""weten"" (to know) and ""schap"", which is a suffix coming from ""scheppen"" which means ""to create"", and the latter suffix now also means something like ""a state of being"". Dutch, English, German, Swedish, Icelandic, Danish and Norwegian all arose from proto-german, so there might be an English or Old-English equivalent.",hgyoigz,t1_hgwcyec,1634459588.0,False
q9e5oo,"I also came across the Dutch equivalent and thought it was probably similar in meaning but wasn't sure since I don't know Dutch. I guess all the equivalent Germanic words in those other languages might be similar.

The Swedish word for computer science is ""datavetenskap"", so it kind of does what the original commenter said. As a bonus, it doesn't mention computers. :)

As for old English, it seems like the word ""science"" has been in the English language since the 14th or 15th century [1] and changed its scope since. I wasn't able to find an old English equivalent for the Germanic word but I didn't look very hard.

I'm a bit confused from reading various dictionary entries for the word ""science"", though. [The Wiktionary page for ""science""](https://en.wiktionary.org/wiki/science#Usage_notes) indicates the word would also be inclusive of social and formal sciences, and computer science is certainly one of the latter. I wonder if even English is really consistent in terms of what the word means.

--

[1] https://www.etymonline.com/word/science",hgz9xnn,t1_hgyoigz,1634475552.0,False
q9e5oo,Cool. Didn't know that. I guess meaning can be a rather complex topic. E.g. Wittgenstein and his [language game](https://en.m.wikipedia.org/wiki/Language_game_\(philosophy\)) thing.,hh3a39q,t1_hgz9xnn,1634550666.0,False
q9e5oo,"Desktop version of /u/Abiogenejesus's link: <https://en.wikipedia.org/wiki/Language_game_(philosophy)>

 --- 

 ^([)[^(opt out)](https://reddit.com/message/compose?to=WikiMobileLinkBot&message=OptOut&subject=OptOut)^(]) ^(Beep Boop.  Downvote to delete)",hh3a445,t1_hh3a39q,1634550687.0,False
q9e5oo,"**[Language game (philosophy)](https://en.m.wikipedia.org/wiki/Language_game_\(philosophy\))** 
 
 >A language-game (German: Sprachspiel) is a philosophical concept developed by Ludwig Wittgenstein, referring to simple examples of language use and the actions into which the language is woven. Wittgenstein argued that a word or even a sentence has meaning only as a result of the ""rule"" of the ""game"" being played. Depending on the context, for example, the utterance ""Water""! could be an order, the answer to a question, or some other form of communication.
 
^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/computerscience/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)",hh3a470,t1_hh3a39q,1634550689.0,False
q9e5oo,"Breaking it down, first there's the word for knowledge, and then a suffix that translates to -ship or -hood.

So literally I think it means knowledge-ship or knowledge-hood.  
But translating more loosely I think I'd go with ""Wise-work""",hgzdlno,t1_hgwcyec,1634477547.0,False
q9e5oo,"Scio means I know in Latin. Not sure what it would mean in modern Romance languages. 

In most Indo-European languages the terms for science derived from the (often archaic) terms for knowledge.

In Slavic languages there is no distinction between ""arts"" in the B.A. sense and sciences in B.Sc. sense. A single word is used for all academic disciplines.",hgyck16,t1_hgw3x72,1634450228.0,False
q9e5oo,In german it is Informatik. Like Mathematik but for information technologie. I like this name. Sometimes I heard it called informatics (english pronounciation).,hgwbdoq,t3_q9e5oo,1634413873.0,False
q9e5oo,"In theory I like it as well but ""Informatiker"" is such a bad stereotype (btw I started as EDV guy ;))",hgwiwvp,t1_hgwbdoq,1634417154.0,False
q9e5oo,"I know, I know ... but its getting better. The stereotype is changing. At least I hope so ... I really do my best to show that an it guy is able to behave like a normal person :D",hgwjpyt,t1_hgwiwvp,1634417496.0,False
q9e5oo,"In Europe we generally picked the word informatics to describe it. Which I find it nicer, from the French information automation. I have a degree on  informatics engineer, that covered soft and hardware.",hgyppgb,t1_hgwbdoq,1634460561.0,False
q9e5oo,Simply put: computer science is a formal science like mathematics - since computer science is a branch of mathematics.,hgxtfo5,t3_q9e5oo,1634439006.0,False
q9e5oo,"To an extent, I agree.

Computer science is not _a_ science.  But it is _science_.

To call it computology, is to call it ""the science of computing"", as that's what -ology means.

So it is correctly named Computer Science, as it is the science of computing.",hgwhks0,t3_q9e5oo,1634416578.0,False
q9e5oo,"I think it is a science. Science of computing. How can we compute something with logical steps, what can be computed, how to design and implement computation models, how to design and implement machines for doing computation. Once you have profound science of computing, you can use this to develop products and applications, and there comes the engineering. Now they both go on parallel thats why its taught as Computer Science and Engineering.",hgymcqu,t3_q9e5oo,1634457821.0,False
q9e5oo,"Computer Science is a branch of mathematics. A sub-branch of discrete math, to be precise.
Since math is not science, neither is Computer Science.",hgvofta,t3_q9e5oo,1634404075.0,False
q9e5oo,"The problem with your argument is this:

Discrete math deals with the formal/symbolic specification of theories, algorithms and so on. Now the formal definitions on their own have no impact in the physical world, however, when computed on a machine, there is some residual effect that can be observed and translated into domain knowlege.

Now, given the def of science being, 'a systematically organized body of knowledge on a particular subject', Computer Science is def a science, just not a natural science.",hgvxbhb,t1_hgvofta,1634407890.0,False
q9e5oo,"Next they’ll say higher education has nothing to do with how the professional or real world operates, and it’s all one giant scheme to sell debt.

Wait…",hgxby3c,t3_q9e5oo,1634430439.0,False
q9e5oo,Ngl you had us in the first half 🤪,hgxvffv,t1_hgxby3c,1634440030.0,False
q9e5oo,Higher education shouldn't be a vocation training center. It should be an institution of theoretical learning.,hgya4a3,t1_hgxby3c,1634448520.0,False
q9e5oo,"I've always thought this, and I have a Master's degree in computer science",hgwp54e,t3_q9e5oo,1634419910.0,False
q9e5oo,But political science is science?😬😬,hgxva9g,t3_q9e5oo,1634439961.0,False
q9e5oo,And it isn't about computers 🤣🤣🤣,hgyhf80,t3_q9e5oo,1634453871.0,False
q9e5oo,"It is the reason why there is a phrase called Natural Science. As long as a decipline answers why and how a certain thing works, it's qualified to be called a Science. That's it. I dont think this should be an argument.",hgyt9cf,t3_q9e5oo,1634463473.0,False
q9e5oo,"I consider computer science a science because you actually use a ton of equations from physics to get to the results we have.  Of course, lot of advanced studies use some math (abstract algebra, calculus, etc).  This leads to having a lot of theories that define our base assumptions.",hgzhx8u,t3_q9e5oo,1634479717.0,False
q9mje2,"Start with Wikipedia, then google each modulation",hgx9eb6,t3_q9mje2,1634429251.0,False
q9mje2,"no i will not, that is why i posted this post.",hgxctam,t1_hgx9eb6,1634430845.0,True
q95xo2,My suggestion is to start with Automate the Boring Stuff with Python chapters 11 - 14 will contain useful information to get you started . You first want to start with the first few chapters on Python programming basics. But 11 - 14 will get you working with the sorts of documents that will most likely contain the data that you seek to manipulate.,hgu25qi,t3_q95xo2,1634367362.0,False
q95xo2,is this on freecodecamp or something else?,hgu9asd,t1_hgu25qi,1634373531.0,True
q95xo2,"The book is free online: https://automatetheboringstuff.com/

And the udemy class is almost always free (or exceptionally cheap Too)",hguzwd0,t1_hgu9asd,1634392710.0,False
q95xo2,"It would help if you could tell us a little more, such as:

1. The kind of data you’re receiving; Excel files sent by email, csv exports from some school system, or?

2. The kind of data processing you need to do once you have all the data; are you just ranking students by worst grades (simple sorting)? Or are you looking at students with otherwise fine grades but one or two grades are failing (variance / outlier detection)? Or students who used to get good grades but who seem to be slipping (regression)? Or something totally different?

Basically what you want to do is probably not too difficult and you should be able to learn enough to build a good working POC by next summer. But the input and concrete analyses determine how you should proceed so a little more detail would help.",hguotvj,t3_q95xo2,1634386357.0,False
q95xo2,"I will be able to find all of that out from the principal on monday, and will let you know then. Thanks!",hgxzgu4,t1_hguotvj,1634442165.0,True
q95xo2,"Transact SQL aka T-SQL.

It's what Microsoft SQL servers aka Databases use, which is what the majority of corporate businesses use for data management. 

Structured Query Language is a strongly typed, static, succinct language used for interacting with Relational Databases.

There is no shortage of work for people who can use SQL well - https://learnsql.com/blog/sql-programming-language/ 

It's also a very easy to understand language. You write it similar to how you'd write a request in English.

Example- SELECT LastName FROM Customers WHERE FirstName = 'James' 

This will return you a list of all the last names for records in the database table ""Customers"" where the first name is James. 

You can play around with SQL here, it's very widely used and if you learn SQL, you'll definitely have jobs lined up! 

https://www.w3schools.com/sql/ 

Once you have a good enough understanding of SQL, add that to a Microsoft Product ""PowerBI"" and you'll be able to solve your problem with no sweat.",hguwg13,t3_q95xo2,1634390898.0,False
q95xo2,"Have a look at Python for Everybody by University of Michigan on the Coursera platform. The courses are fairly interactive and the final capstone project on Course 5 may be of interest to you: Retrieving, Processing and Visualizing Data with Python.

[Python for Everybody](https://www.coursera.org/specializations/python)",hgwpmiz,t3_q95xo2,1634420129.0,False
q8w84n,An emulator simulates the hardware in the machine on top of the software. The absolute requirement is that every piece of hardware beyond the base compiler language is simulated.,hgsktni,t3_q8w84n,1634336274.0,False
q8w84n,"In short, the difference is somewhat a blackbox and whitebox approach. Simulation is blackbox. As long as the inputs and outputs match, the internal design can function however we like. Emulation is whitebox. We are recreating the exact logic used internally by the source device.

As an example, let's look at drawing a 5-pixel horizontal line. In a simulator, we can draw that line however we like, even using specialist hardware available on the machine running the simulation for improved performance.  
In an emulator, we need to determine the method of *how* that horizontal line should be drawn. Do we start from the left or the right? What interval is there between drawing each pixel? Is it even drawn pixel-by-pixel, or all at once? What state should the registers hold following the operation? All these have to be considered, and as another commenter mentioned, to achieve this we need to specifically define this logic in software!


For the second part, any Little Man Computer implementations are *simulators*. The reason for this isn't much of a techie one - It's simply because the LMC instruction set was designed as a teaching guide rather than stemming from actual hardware. As the name implies, it's based on the concept of a busy little man in a mail room. Until we get fully human AI and we find out who this mystery mail man is, it might be a while before we get wholly accurate emulators!",hgt67ks,t3_q8w84n,1634347172.0,False
q8w84n,This seems like a homework question...,hgsouuj,t3_q8w84n,1634338238.0,False
q8w84n,Lol what,hgsqcm6,t1_hgsouuj,1634338987.0,True
q8w84n,A crap one.,hgub7ia,t1_hgsouuj,1634375268.0,False
q8w84n,This.,hgsadmd,t3_q8w84n,1634331409.0,False
q8w84n,"If a data stuct. will represent a man's dick, will it be an array or a linked list?",hgt5ta6,t3_q8w84n,1634346955.0,False
q8w84n,Linked list might be better. Don’t want to reallocate and copy when you get a hard-on,hgu7a1n,t1_hgt5ta6,1634371732.0,False
q8rs2z,"Start from first principles 

1.  [Code: The Hidden Language of Computer Hardware and Software](http://charlespetzold.com/code)
2. [Exploring How Computers Work](https://youtu.be/QZwneRb-zqA)
3. Watch all 41 videos of [A Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo)
4. Take the [CS50: Introduction to Computer Science](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) course.
5. Take the [Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)](https://www.coursera.org/learn/build-a-computer)
6. Ben Eater""s [Build an 8-bit computer from scratch](https://eater.net/8bit)

(If you actually get the kits to make the computer, make sure you read these:

[What I Have Learned: A Master List Of What To Do](
https://www.reddit.com/r/beneater/comments/dskbug/what_i_have_learned_a_master_list_of_what_to_do/?utm_medium=android_app&utm_source=share)

[Helpful Tips and Recommendations for Ben Eater's 8-Bit Computer Project](https://www.reddit.com/r/beneater/comments/ii113p/helpful_tips_and_recommendations_for_ben_eaters/?utm_medium=android_app&utm_source=share )

As nobody can figure out how Ben's computer actually works reliably without resistors in series on the LEDs among other things!)

Here is a decent list of [8 Books on Algorithms and Data Structures For All Levels](https://www.tableau.com/learn/articles/books-about-data-structures-algorithms)

You can also check out [Teach Yourself Computer Science](https://teachyourselfcs.com/)

And finally, [play the long game when learning to code.](
https://stackoverflow.blog/2020/10/05/play-the-long-game-when-learning-to-code/)",hgsjjkr,t3_q8rs2z,1634335654.0,False
q8rs2z,Not OP but thanks for the list!,hgu0avg,t1_hgsjjkr,1634365853.0,False
q8rs2z,"I'd say from the beginning, or at least an earlier era, but it depends on your general disposition. Computers were a lot easier to comprehend 40 years ago, whereas a modern computer is almost impossibly complex, requiring you to operate with more of an abstract model of how a computer works. Learning machine-level programming on a simpler device will help reveal the nature of computers, so to speak.

But then again, that may totally not be your thing. If you're interested in an academic approach then ""Structure and Interpretation of Computer Programs"" by Abelson & Sussman is an excellent book that somehow manages to cover more or less everything, while also getting you started with the very CS-credible programming language of Scheme. The book is available for free online, in lots of different formats, if you don't want to invest in a physical copy.

If you want to achieve things quickly then look into learning Python programming. It's a language that's easy to pick up and which is very expressive (allows you to achieve a lot with relatively little code), and also very popular amongst scientists of various persuations. In traditional CS domains it's mainly made inroads in AI and machine learning.

Really, pretry much anywhere can be a good place to begin, but it helps immensely if it's somewhere you find at least moderately interesting. Good luck!",hgrc7uv,t3_q8rs2z,1634317099.0,False
q8rs2z,"I think I'd put a hard cap on that 40 years. Anything earlier and you regress away from some of the improvements in computer design that made life easier. For example, concepts like RISC are a huge improvement over what came before.",hgrwqbf,t1_hgrc7uv,1634325532.0,False
q8rs2z,"Well, in the beginning everything was RISC. 40 years ago was the era of the eight bit micros that later served as inspiration for the RISC architecture. The CISC ""monsters"" didn't really break through until the late eighties (though CISC also isn't bad by definition; the Motorola 68000 series is a wonderful CISC architecture for example).",hgs1nc1,t1_hgrwqbf,1634327612.0,False
q8rs2z,I don't know. I'm currently reading TAOCP which uses 60s inspired design for it's fake MIX architecture. It is stupidly complex to me when looking at the newer MMIX RISC inspired design using modern sensibilities. That original design is so unnecessarily complex to me in comparison that I've completely paused my reading until I can get the addendums with the easier to understand and more relevant modern design.,hgs3hlm,t1_hgs1nc1,1634328401.0,False
q8rs2z,"Yeah, there were definitely some wild architectures in the fifties and sixties! Thankfully there's very little risk someone would accidentally stumble over one of them today, without knowing what they are looking for. :-)",hgs46ou,t1_hgs3hlm,1634328705.0,False
q8rs2z,"Haha yeah, that's why I think it's just best to hard cap it to the 70s or 80s as the earliest. Just minimizes the chance of running into weird oddities like decimal/binary hybrid computers with variable length words or whatever other weird stuff that wasn't standardized yet. Basically don't go back in time to before C or UNIX.",hgs5cmy,t1_hgs46ou,1634329210.0,False
q8rs2z,Yeah... Or just something as subtly insane as ones' instead of two's complement. No earlier than Unix is a good dividing line.,hgs6jks,t1_hgs5cmy,1634329728.0,False
q8rs2z,"I agree. Python is probably the best place to start and quickly learn CS principles. 

I tried starting with C# before I knew anything and it felt impossible to breakthrough.

Start by doing what's possible then suddenly you're doing the impossible.",hgt4p8v,t1_hgrc7uv,1634346346.0,False
q8rs2z,"It probably depends on what sort of student you are. If you are self directed and motivated you might pick up a book on programming and work through it. Getting a bit of the history of computing is good. Learning something about the hardware is good. You may find a more specific interest in any one of these areas and go deeper.

I do better in more structured, classroom settings. If I were in your position I would either enroll in a college/university or a community college and take a series of classes. If you are not in the US I am not sure what the equivalent of community colleges might be. These often offer 2 year degrees in many subjects.

Computer Science is a remarkably large field with a rich history. Few people are experts in the whole of CS. Most people tend to gravitate towards a fairly narrow sub-field and specialize in that. 

Good luck on your journey",hgrbfrm,t3_q8rs2z,1634316779.0,False
q8rs2z,Back in the beginning a found a quick overview of the history of computers helped me place things in my head. From there I started reading and watching about increasing amount of stuff. There are a lot of recorded talks from conferences and stuff where someone will go on at length about a very specific topic. I always found those fun and informative. I always had an interest in games so I watched through John Carmack talking about how he created various graphical things. Definitely fascinating though very complex. I'm sure there's a lot left to find me fill in the rest though. Youtube is a good resource.,hgr6qas,t3_q8rs2z,1634314853.0,False
q8rs2z,Thank you so much!,hgr9qra,t1_hgr6qas,1634316097.0,True
q8rs2z,"It's been mentioned elsewhere in this thread, but Harvard's CS50x is a great place to start.",hgtm7ww,t3_q8rs2z,1634356097.0,False
q8rs2z,I'm afraid that won't be enough to get a job or build a career at any levels. Comp sci domain is like dog years. Things tend to move pretty fast here compared to other fields. And the interviews are super competitive.,hgu52hm,t3_q8rs2z,1634369820.0,False
q8rs2z,"I think my strategy would be instead of just starting with history or basic concepts, begin with the things that interest you the most. A lot of us are old and may not remember that our passion for CS comes from discovering things we like and deep diving on those. Eventually we round out our knowledge with formal training or effort like the excellent comments before mine suggest, but for the absolute beginner the motivation must be true and not just getting into it because tech is popular or makes a lot of money.",hgutwec,t3_q8rs2z,1634389456.0,False
q8rs2z,"Start by learning binary math.    
When you start to learn programming, avoid the scripting languages.  Start with something like Java.  Learn how to compile and run from the command line.  Later, learn a scripting language, not to be confused with an interpreted language.  
After that, study computer archenteric, automata, and some assembly language.    
Then you will be ready to begin.",hgtg3zp,t3_q8rs2z,1634352550.0,False
q8rs2z,What even is this comment,hgv1kqq,t1_hgtg3zp,1634393596.0,False
q8rs2z,Likely some cs sophomore trying to feel impressive.,hgv62dn,t1_hgv1kqq,1634395816.0,False
q8rs2z,"I like how you start with binary math because duh. And then after I’ve learned computer (I assume he means) architecture and assembly and I can write programs in Java and assembly, and I also happen to have some basic theory of computation stuff, then I’m ready to begin. Like, dawg, we began a long time ago. Just start coding lol it’s not that deep.",hgv7xah,t1_hgv62dn,1634396702.0,False
q8rs2z,I think he's just drawing on his 10 years of experience teaching at a research university.,hh2wc8c,t1_hgv62dn,1634538421.0,False
q8rs2z,">""Then you will be ready to begin""

No easier way to make everyone lose all possible respect for you.",hgz8nih,t1_hgtg3zp,1634474809.0,False
q8rs2z,"I say start with Head First Java.

I did Head First Java up to programming threads and it taught me stuff I'm still using while I learn about everything else all the time.",hgz8uf9,t3_q8rs2z,1634474926.0,False
q8rs2z,"Yeah, a University",hhi6h96,t3_q8rs2z,1634832152.0,False
q8rs2z,"[John Zelle's Intro Python book](https://www.goodreads.com/book/show/80440.Python_Programming) is a good starting point for beginners! It might gloss over some low-level things, but if you're looking to try your hand at coding right away, it's a pretty good choice and easily digestible I think.",hgrsg14,t3_q8rs2z,1634323732.0,False
q8rs2z,Teachyourselfcs.com,hgulet1,t3_q8rs2z,1634383938.0,False
q8rs2z," Chances are, you’re going to become very interested in a particular part of CS. For me it was programming at first, now I’m into OS and hardware security. Keep your mind open and don’t be afraid of any topic.",hgv6l76,t3_q8rs2z,1634396065.0,False
q9c7cp,"What do you mean with **This helps automate the process of writing code for many programs.**?

You need a compiler (or at least an interpreter) to run a program. The CPU does not know how to execute text.

You do not need to write a complete compiler from scratch for every language. You can compile to an intermediate representation and use a gcc-backend or LLVM or something similar to get executable instructions for your CPU (or VM).",hgv0m24,t3_q9c7cp,1634393088.0,False
q9c7cp,I think they mean IDE not compiler,hgvc1mr,t1_hgv0m24,1634398643.0,False
q9c7cp,"In academics they build their own compiler to give you the practical experience in compiler design. But in practice you will not build a compiler until you are developing a new language. Then where does my knowledge of compiler design comes hands on? Answer is, optimizing the existing compilers, building interpreters for small languages, research in compiler design etc.",hgxrwxg,t3_q9c7cp,1634438237.0,False
q9c7cp,"Compilers are designed not for programs but for programming languages.
There are some reasons for developing a new compiler:
1. For existing language:
1.1. To produce more effective code than other compilers 
1.2. For new hardware/software platform 
2. For new languages — as there are no languages completely free of any issues and research in this area doesn’t stop 

Also there is a couple of interesting theoretical concerts about compiler design: metacompilation & universal compiler",hgv18dw,t3_q9c7cp,1634393419.0,False
q8kgky,"No other book goes to the same depth and breadth as Knuth.

But the Wizard Book - Structure and Interpretation of Computer Programming by Abelson, Sussman and Sussman - should certainly be on your list.",hgq29ib,t3_q8kgky,1634294773.0,False
q8kgky,"SICP 

K&R 

Dragon book 

CLRS Introduction to Algorithms

Not sure about this one but Sisper's Introduction to the Theory of Computation",hgte7c7,t3_q8kgky,1634351500.0,False
q8kgky,Artificial Intelligence: a Modern Approach.,hgu4eop,t3_q8kgky,1634369260.0,False
q8kgky,The dragon book?,hgpxbi3,t3_q8kgky,1634290638.0,False
q87ayn,It depends on what you are implementing. But as a rule of thumb I’d say 50/50.,hgoyvn8,t3_q87ayn,1634266567.0,False
q87ayn,"Do you mean the ratio of the two activities in relation to each other or relative to the entirety of a software engineering job? Because like most jobs, the majority of the time is spent on things like email and other mundanity",hgpimaf,t3_q87ayn,1634277976.0,False
q87ayn,Just the coding part of the business logic / interface / .. versus the coding part of the tests.,hgpjkk5,t1_hgpimaf,1634278709.0,True
q87ayn,"Compsci is not software development. You probably want something like /r/webdev/, but tbh software developers don't talk shop on Reddit, it's mostly students and self-taught hobbyists. You want hackernews or lobsters.",hgomw2y,t3_q87ayn,1634261113.0,False
q87ayn,"You both need to read the pinned thread in the subreddit.

1. New to the area should ask questions there
2. The first phrase says “new to programming or computer science?”. Then yes, this subreddit is also for programming. Also in the subreddit description says it is for “all things computer science”. And that includes software development.",hgoyl2x,t1_hgomw2y,1634266429.0,False
q87ayn,"I've read it, and I don't particularly care - the subreddit can say it's about anything it wants, what matters is who is here and what answers you're likely to get.",hgoznf3,t1_hgoyl2x,1634266934.0,False
q87ayn,"You were saying he was asking in the wrong place and that this subreddit was not related to his question, which is WRONG. His question was fine for this subreddit.
Each sub has its own rules, you don’t get to decide on what is proper or not based on your own taste. LOL",hgp4t1g,t1_hgoznf3,1634269517.0,False
q87ayn,"I mean you can write rules on a piece of paper and stick them on an empty shed, then shout your question into the shed, it's not going to get you the answers you're looking for.",hgp76p4,t1_hgp4t1g,1634270796.0,False
q87ayn,"One thing is give an advice for other places where more and better answers. Other is the to say it is not the proper place. 

I can see we’ll not agree. So I think it is no good to keep talking further here. 👋",hgp8i5p,t1_hgp76p4,1634271526.0,False
q87ayn,"I said compsci, not /r/computerscience. You just read more into it than was there...",hgp9bdn,t1_hgp8i5p,1634271987.0,False
q87ayn,It has not much to do with web development but understand the concern. I am glad to have any kind of scientific research as a form of research paper as well tbh.,hgp4qq7,t1_hgomw2y,1634269484.0,True
q87ayn,"/r/webdev is just the most active softdev community of any kind on Reddit where you're likely to find actual developers talking about developing. There's developers of all flavours on Reddit, but they tend to talk shop less on here and more in other places that aren't so completely overrun by students and people who don't know what they're doing.",hgp7cp7,t1_hgp4qq7,1634270888.0,False
q87ayn,"Here's a sneak peek of /r/webdev using the [top posts](https://np.reddit.com/r/webdev/top/?sort=top&t=year) of the year!

\#1: [The website I have been tasked with updating today...](https://i.redd.it/pumkw1ulx2l71.png) | [933 comments](https://np.reddit.com/r/webdev/comments/pggevn/the_website_i_have_been_tasked_with_updating_today/)  
\#2: [Wanted to share a coding task I was asked to do in an interview. This is how to handle this type of situation.](https://i.redd.it/evg91bkr09t61.png) | [527 comments](https://np.reddit.com/r/webdev/comments/mr5s0o/wanted_to_share_a_coding_task_i_was_asked_to_do/)  
\#3: [18 Cards of how to design web forms](https://np.reddit.com/gallery/nm6wcl) | [387 comments](https://np.reddit.com/r/webdev/comments/nm6wcl/18_cards_of_how_to_design_web_forms/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/)",hgp7dwr,t1_hgp7cp7,1634270907.0,False
q87ayn,Testing time is a lot like the time complexity of your function - you wanna get it as low as possible.,hgqn4tx,t3_q87ayn,1634306617.0,False
q8b9g3,Byzantine Generals problem. This is pretty much the basis of bitcoin/cryptocurrencies. An open ledger,hgol6xb,t3_q8b9g3,1634260355.0,False
q8b9g3,"A ledger that contains all completed transactions. This would include timestamps, amounts, as well as the payer and payee.",hgogrtc,t3_q8b9g3,1634258325.0,False
q8b9g3,"Is there any feasible way for this ledger to exist such that it does not need to be held by an external (i.e., not A, B, or C) party? How could A themself prove they had not given an IOY to B?",hgoh2v2,t1_hgogrtc,1634258466.0,True
q8b9g3,"Depends on the definition of third party. If by third party you mean some entity aside from A, B, or C, then I suppose each entity could have their own ledger. Within A’s ledger there should be a record of all transactions in and out of the vault.

If by third party you mean some entity other than C (the querying entity in the given example), I’m not sure they would be aware of transactions taking place unless there was some kind of publish-subscribe situation you could apply. Without querying A directly for the information about outstanding debts, that is.

Otherwise, if that third party can be some payment processor that holds the ledger for all transactions, that might be another option.",hgoia8z,t1_hgoh2v2,1634259019.0,False
q8b9g3,"I guess the question comes down to: how can C hold A accountable for what A tells C. If A says they did not give B an IOY, how could C know that is the truth?",hgokle7,t1_hgoia8z,1634260079.0,True
q8b9g3,"If you can’t trust information given by A, cannot contact B, and there is no publicly held ledger, I’m not sure there is a solution.",hgols8l,t1_hgokle7,1634260624.0,False
q8b9g3,"I think that is right. A public ledger solves the problem if A&C record the IOU on the ledger and C only finishes the handoff after the transaction is accepted in the ledger. Because at that point, C can check if there were any IOUs written by A between the recording of the $100 and the A&C transaction. That guarantees no entity B exists in the official record.

Perfect use case for distributed ledgers.",hgonf19,t1_hgols8l,1634261348.0,False
q8b9g3,Isn’t this just Bitcoin?,hgom63h,t3_q8b9g3,1634260797.0,False
q8b9g3,Was basically trying to ask if there is a way to solve this problem without using a public ledger --> which is what bitcoin uses.,hgp1oo4,t1_hgom63h,1634267920.0,True
q8b9g3,Ahhh I gotcha.,hgq8cwx,t1_hgp1oo4,1634298981.0,False
q8b9g3,That is the first time I've seen IOU spelt like that.,hgplaem,t3_q8b9g3,1634280044.0,False
q87ymv,"They already exist. Google has their TPU. Amazon has a chip called inferentia. A bunch of start ups have dedicated deep learning chips as well. It's a fast growing market, especially for cloud vendors.",hgnr9u3,t3_q87ymv,1634246574.0,False
q87ymv,"Do you think this split will hit the consumer market as well, eventually?",hgpdo05,t1_hgnr9u3,1634274536.0,True
q87ymv,"What do you mean, this has already happened, that's what people are telling you.

https://www.xilinx.com/products/boards-and-kits/alveo.html

You can buy and use any of these boards. The bits about data centers is targeted marketing for a core audience, not limitations.",hgqidlj,t1_hgpdo05,1634304398.0,False
q87ymv,"TPU.

Hardware made for specific algorithms are called: ASIC. aka Application Specific Integrated Circuits.",hgnyvq0,t3_q87ymv,1634250029.0,False
q87ymv,"I think you have your history of graphics wrong. Before GPUs we had video cards and chipsets that implemented eg VGA, it wasn’t exclusively all done on the CPU. Once upon a time FPUs were also discrete.",hgpux9m,t3_q87ymv,1634288485.0,False
q87ymv,ASICs and FPGAs are already being used for real time computer vision systems.,hgv2al9,t3_q87ymv,1634393970.0,False
q87ymv,"Apart from TPUs, which are primarily used for deep learning, you also have experimental neuromorphic chips which offer more bioplausible learning rules.",hgpij4s,t3_q87ymv,1634277911.0,False
q7mriy,*Relative Community Sizes per Year,hgkzxle,t3_q7mriy,1634193235.0,False
q7dw91,"Super intro

https://towardsdatascience.com/a-very-brief-introduction-to-fuzzy-logic-and-fuzzy-systems-d68d14b3a3b8",hgi5kgf,t3_q7dw91,1634144440.0,False
q7dw91,"If you already have experience with other types of control (i.e. PID), here is a comparison for building temperature controls without too much math. 

&#x200B;

See section 5.2 - Static Fuzzy Logic Controller (FLC):  
""Since fuzzy control theory is somewhat new to those involved in building systems; it is appropriate to review some of the basic concepts. The reader interested in a more   
 comprehensive review of the subject will find (Lee, 1990a, b; Gouda, et al. 1997)  
helpful.""

&#x200B;

https://www.sciencedirect.com/science/article/pii/S1474667017369008/",hgiowkd,t3_q7dw91,1634152110.0,False
q7dw91,Which literature do you use in class?,hgj3w1t,t3_q7dw91,1634158204.0,False
q7dw91,"Lotfi Zadeh is the classic, and pretty easy to grok.",hgk02a5,t3_q7dw91,1634172743.0,False
q805c4,"See also the paper on Arxiv ""Insidious Nonetheless: How Small Effects and Hierarchical Norms Create and Maintain Gender Disparities in Organizations"" https://arxiv.org/abs/2110.04196",hgm0e9s,t3_q805c4,1634219576.0,True
q6gpe2,"He wasn't the sole creator of C and Unix, but his contributions to computer science are immense. A lot of the computing we do today relies on that work. RIP Dennis Riche, I will never free() you from memory.",hgbwntv,t3_q6gpe2,1634025044.0,False
q6gpe2,"I’m not crying, it’s just a memory leak!",hgcnfv9,t1_hgbwntv,1634044850.0,False
q6gpe2,[deleted],hgbyugc,t1_hgbwntv,1634027095.0,False
q6gpe2,What kind of language are you writing in bro,hgc1f0x,t1_hgbyugc,1634029501.0,False
q6gpe2,"A pseudocode based in JS.

A **really shitty one** at that.",hgc2pok,t1_hgc1f0x,1634030705.0,False
q6gpe2,You could have at least written in C.,hgexkdg,t1_hgbyugc,1634079374.0,False
q6gpe2,Yuck 🤢,hgdybry,t1_hgbyugc,1634064695.0,False
q6gpe2,what other things did he do?,hhs3bg0,t1_hgbwntv,1635020944.0,False
q6gpe2,C,hgc8rm9,t3_q6gpe2,1634035901.0,False
q6gpe2,C,hgcdq3e,t1_hgc8rm9,1634039282.0,False
q6gpe2,C,hgcjul6,t1_hgcdq3e,1634042969.0,False
q6gpe2,C,hgfez68,t1_hgcjul6,1634087457.0,False
q6gpe2,Si,hgcoyj4,t1_hgcjul6,1634045599.0,False
q6gpe2,UNIX,hgcqb88,t1_hgcoyj4,1634046253.0,False
q6gpe2,C,hgmmi09,t1_hgcdq3e,1634228899.0,False
q6gpe2,You in information heaven,hgfliu6,t1_hgc8rm9,1634090470.0,False
q6gpe2,"malloc some memory for him. Don't free it though, let it leak",hgcenqq,t3_q6gpe2,1634039884.0,False
q6gpe2,Maybe I'll just realloc some other memory,hggdif9,t1_hgcenqq,1634106650.0,False
q6gpe2,this brings to mind this video from Bell Labs https://youtu.be/tc4ROCJYbm0,hgcxjnz,t3_q6gpe2,1634049541.0,False
q6gpe2,"This man in the intro has no business being so suave in this video.

His low voice, the turtleneck, foot up and hunched over. YouTubers should take note.",hgd0kpj,t1_hgcxjnz,1634050845.0,False
q6gpe2,"they are all chill, listen to Kevin Thompson; it was before the era of anxiety inducing  management practices",hgd5jro,t1_hgd0kpj,1634052887.0,False
q6gpe2,RIP,hgc0q9n,t3_q6gpe2,1634028861.0,False
q6gpe2,Legend never dies. He lives in our hearts,hgcvd7l,t3_q6gpe2,1634048590.0,False
q6gpe2,"""10 years ago today,....""",hgdt92x,t3_q6gpe2,1634062638.0,False
q6gpe2,F,hgbwvcr,t3_q6gpe2,1634025239.0,False
q6gpe2,"No, it was C.",hgc7dhh,t1_hgbwvcr,1634034832.0,False
q6gpe2,take my upvote and leave,hgcct06,t1_hgc7dhh,1634038679.0,False
q6gpe2,He is paying respecc,hgcqjka,t1_hgc7dhh,1634046365.0,False
q6gpe2,"Dennis, you're a legend in computer field and we'll never forget you",hgdffa7,t3_q6gpe2,1634056926.0,False
q6gpe2,C,hgekbkv,t3_q6gpe2,1634073657.0,False
q6gpe2,C,hgff1qd,t1_hgekbkv,1634087489.0,False
q6gpe2,He may be gone but his legacy will never die,hge3cpz,t3_q6gpe2,1634066748.0,False
q6gpe2,That man literally paved the way for everyone else who would go on to pave the way. Legend.,hgeiasc,t3_q6gpe2,1634072818.0,False
q6gpe2,"Just came to say, fuck Jobs. Thats it",hgflkyw,t3_q6gpe2,1634090497.0,False
q6gpe2,rest in peace,hgd0pco,t3_q6gpe2,1634050901.0,False
q6gpe2,Simplicity and elegance both fit to he language and the person.,hgdfj5p,t3_q6gpe2,1634056968.0,False
q6gpe2,Thank you good man.,hgewen1,t3_q6gpe2,1634078847.0,False
q6gpe2,C inspired many programming languages. Thank you.,hgg0w6u,t3_q6gpe2,1634098296.0,False
q6gpe2,You should see the goodbye world script,hgd9iwo,t3_q6gpe2,1634054503.0,False
q6gpe2,Sir thank you for Java,hgcf6c6,t3_q6gpe2,1634040213.0,False
q6gpe2,Thank you for saying thank you,hgdyg90,t1_hgcf6c6,1634064746.0,False
q6gpe2,C,hggag5c,t3_q6gpe2,1634104276.0,False
q6gpe2,Who is dat,hgcqg7l,t3_q6gpe2,1634046321.0,False
q6gpe2,The man whose feet you should prostrate yourself before if you hope to be anything in the field of computer science.,hgff53a,t1_hgcqg7l,1634087532.0,False
q6gpe2,You can just say “Ten years ago”,hgcs1lq,t3_q6gpe2,1634047070.0,False
q6gpe2,"""Ten years ago today""",hgey3qc,t3_q6gpe2,1634079622.0,False
q71moh,Coding is like anything else in life.  You grow through failure.  There is nothing wrong with asking for help.  Too many people assume that is a weakness.   The brightest software engineers I know ask way more questions than they provide answers.,hgfqiy9,t3_q71moh,1634092850.0,False
q71moh,"Sure, I can learn how to build a house, it’s just that I’m not the best of handymen.",hgj16lo,t3_q71moh,1634157078.0,False
q71moh,"Coding doesn’t come natural to anyone it’s actually the opposite of everything we do in our daily lives. Example, as humans we don’t think about the little details we automate it and move on. When was the last time you thought about washing your hand after toilet, or drinking when you’re thirsty. You just do it because we automated it. But, in coding it’s the opposite of automation you have to make every single decision consciously. And it’s hard, because it’s weird and that is Ok, it’s hard and weird to everyone and the best thing people do is accept for what it is and try to do their best. I would highly encourage watching the unedited video screens of professional developers from twitch to YouTube and see how many times they stumble over the smallest things. In your mind you have an image of a software engineering who exactly know what he/she is doing and fluently converts that thought into a code, but that is not reality. Again WATCH twitch and YouTube video of live coding and see how they handle mishaps.",hgfrr90,t3_q71moh,1634093459.0,False
q71moh,"I think what you're saying isn't universally applicable.

I think coding actually does come naturally to me because it's so much like something I do a lot in my life. I love playing board games. In the end, board game rules are basically a way to program humans, and I went through a lot of these.

My programming courses are therefore basically free (at least at the moment). When it comes to math though I struggle like the worst of them, so it all balances out.",hgz9n5s,t1_hgfrr90,1634475378.0,False
q71moh,"No, despite what people desire to believe everyone is not able become a software engineer. However, there are a great deal of people who could, who do not, regardless of any innate ability or inclination. If you love to read programs like you say, it sounds like you have the inclination. Regarding seeking assistance: That's a smart thing to do.

At the end of the day only you can determine if software development is ""for you"".

I recommend you spend time writing software that you personally will use, in whatever language you prefer to do it in. Keep reading software written by others. There is no substitute for reading and writing computer software.

If there are specific road blocks you run in to, seek assistance with them.",hghaka7,t3_q71moh,1634131410.0,False
q71moh,everyone is capable of it yes. but when you learn anything the largest factor in how well you learn is how much you like it. not everyone likes to code and so learning is exponentially harder.,hggfxki,t3_q71moh,1634108610.0,False
q71moh,I used to think so.,hggffzx,t3_q71moh,1634108201.0,False
q71moh,"No body codes “well” even the biggest companies with the most brilliant engineers make dumb mistakes, and everyone constantly find better ways to do something even after the software is released, that is why we have all the ongoing updates for any software. 

I have worked on code, and after writing 100 lines for 3 days  I realize that I could have replaced it all with 3 lines. 

The question is, can you write something workable, and do you have the presence of mind to go back and fix mistakes.


Contrary to popular belief, you don’t have to be the “best” in anything, May be you aren’t a brilliant or talented developer, but with enough trying I think anyone can become a passable one.",hghrh39,t3_q71moh,1634138713.0,False
q71moh,Anyone can do it. However not everyone can build something.,hgj7k5z,t3_q71moh,1634159739.0,False
q71moh,"Struggle is fine. Everyone struggles in something when studying. Get some time to learn on your own and you will see that confidence comes with it. 

Aside from that, Comp. Sci. is a heavy study area so sleep and eat well or that already heavy and hard area will become even harder with a stressed head.",hgjefoq,t3_q71moh,1634162699.0,False
q71moh,A good resource for learning to code is definitely w3schools. It taught me the basics very fast and it has an easy to understand layout. I would highly recommend it,hgmmu1k,t3_q71moh,1634229033.0,False
q71moh,"No one is born a programmer. Sure, logic might come easier to some, but every good programmer learns from being a bad programmer.",hgv751f,t3_q71moh,1634396328.0,False
q71moh,"I don't think so.

I know people who couldn't become bad programmers even if they tried, much less good ones.

But if you're actually interested in becoming one, your chances already went up by a lot.

You say you kept bugging the person next to you to explain stuff to you. I think that's the best, single most important attribute any programmer could have and I wish I had more of it.",hgz91sf,t3_q71moh,1634475047.0,False
