{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13511177",
   "metadata": {},
   "source": [
    "**Subreddit level**\n",
    "- Sentiment score\n",
    "- Topics in posts\n",
    "- Followers/Members\n",
    "\n",
    "**Post level**\n",
    "- Sentiment score\n",
    "- Relevance score\n",
    "- Topics in comments\n",
    "- Total Num Comments\n",
    "- Num Comments per submission\n",
    "- Upvote ratio\n",
    "- Score\n",
    "\n",
    "**Comment level**\n",
    "- Sentiment score\n",
    "- Relevance score\n",
    "- Up-vote count\n",
    "- Down-vote count\n",
    "- Contraversality\n",
    "- Total Awards Received\n",
    "- Score\n",
    "- Is_locked, collapsed, submitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0848e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.models.subreddit_analysis import SubredditAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7451927a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Preprocessing DataFrame for Topic Modeling*********\n",
      "Fill NaNs\n",
      "Remove URLs\n",
      "Expand Contractions\n",
      "Make Lowercase\n",
      "Tokenize\n",
      "Filter Stopwords\n",
      "Lemmatization\n",
      "********DONE: Preprocessing for Topic Modeling*********\n",
      "********Topic Modeling for Posts*********\n",
      "Number of entries being modeled: 0\n",
      "Intiailizing model and training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Make sure that the iterable only contains strings.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bt/h872r7px5mq918xhhvvdsb7m0000gn/T/ipykernel_52005/481926587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubredditAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'science'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/UW/Capstone/reddit_capstone_repos/Reddit/src/models/subreddit_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, subreddit, sort_order, set_num_posts, set_num_comments)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# 2. topic modeling for posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'********Topic Modeling for Posts*********'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mbertmodels_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_prep_posts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body_word_token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtopic_prep_posts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topics'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertmodels_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbertmodels_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_prep_posts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UW/Capstone/reddit_capstone_repos/Reddit/src/models/bertmodels.py\u001b[0m in \u001b[0;36mtopic_modeling\u001b[0;34m(self, df, col, calculate_probabilities, verbose, visualize)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Intiailizing model and training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_probabilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.7/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mcheck_documents_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mcheck_embeddings_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.7/site-packages/bertopic/_utils.py\u001b[0m in \u001b[0;36mcheck_documents_type\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Make sure that the iterable only contains strings.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Make sure that the iterable only contains strings."
     ]
    }
   ],
   "source": [
    "sub = SubredditAnalysis('science', 'hot', 1000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a91975",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.get_processed_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.get_processed_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.get_result_df().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7985c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sub.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece586e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['index_x', 'post_id', 'title', 'score_x', 'upvote_ratio', 'subreddit', \n",
    "                               ' url', 'num_comments', 'body', 'created', 'body_word_token',\n",
    "                               'body_tag', 'body_string_x', 'topics_x', 'sentiment_x', 'index_y',\n",
    "                               'comment_id', 'parent_id', 'comment', 'up_vote_count',\n",
    "                               'down_vote_count', 'controversiality', 'total_awards_received',\n",
    "                               'score_y', 'is_locked', 'is_collapsed', 'is_submitter', 'created_utc',\n",
    "                               'comment_word_token', 'comment_tag', 'body_string_y', 'topics_y',\n",
    "                               'sentiment_y', 'relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59efb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1852680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['post_index', 'post_id', 'post_title', 'post_score', 'post_upvote_ratio', 'subreddit', \n",
    "                               'post_url', 'num_comments', 'post_body', 'post_created', 'post_body_word_token',\n",
    "                               'post_body_tag', 'post_body_string', 'post_topics', 'post_sentiment', 'comment_index',\n",
    "                               'comment_id', 'parent_id', 'comment', 'comment_up_vote_count',\n",
    "                               'comment_down_vote_count', 'comment_controversiality', 'comment_total_awards_received',\n",
    "                               'comment_score', 'comment_is_locked', 'comment_is_collapsed', 'comment_is_submitter', 'comment_created_utc',\n",
    "                               'comment_word_token', 'comment_tag', 'comment_body_string', 'comment_topics',\n",
    "                               'comment_sentiment', 'comment_relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45e5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
