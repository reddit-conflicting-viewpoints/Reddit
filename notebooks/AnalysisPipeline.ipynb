{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d72e4ee",
   "metadata": {},
   "source": [
    "Subreddit level\n",
    "Sentiment score\n",
    "Topics in posts\n",
    "Followers/Members\n",
    "\n",
    "Post level\n",
    "Sentiment score\n",
    "Relevance score\n",
    "Topics in comments\n",
    "Total Num Comments\n",
    "Num Comments per submission\n",
    "Upvote ratio\n",
    "Score\n",
    "\n",
    "Comment level\n",
    "Sentiment score\n",
    "Relevance score\n",
    "Up-vote count\n",
    "Down-vote count\n",
    "Contraversality\n",
    "Total Awards Received\n",
    "Score\n",
    "Is_locked, collapsed, submitter?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd988574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.features.preprocess import PreProcess\n",
    "from src.models.relevance import Relevance\n",
    "from src.models.bertmodels import BertModels\n",
    "\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e773559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditAnalysis:\n",
    "    \n",
    "    def __init__(self, subreddit='computerscience', sort_order='hot', set_num_posts=500, set_num_comments=500):\n",
    "        \"\"\"\n",
    "        Subreddit Analysis constructor\n",
    "        \n",
    "        3 DataFrames:\n",
    "            - Subreddit Data\n",
    "            - Post Data\n",
    "            - Comment Data\n",
    "        \n",
    "        Running static functions(?):\n",
    "            Preprocess Posts data\n",
    "            Preprocess Comments data\n",
    "            Retrieve Topics for posts and comments\n",
    "                Add Topics list to respective dataframes\n",
    "            Get Sentiment Score for posts and comments\n",
    "            Get Relevance Score for posts->subreddit and comments->posts\n",
    "                Add scores to respective dataframes\n",
    "            \n",
    "        :param subreddit: name of subreddit for analysis\n",
    "        :param sort_order: order of submissions retrieved from reddit\n",
    "        :param set_num_posts: set max number of posts for analysis\n",
    "        :param set_num_comments: set max number of comments for analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        bertmodels_obj = BertModels(subreddit=subreddit, sort_type=sort_order)\n",
    "        \n",
    "        def topic_extractor(topics_tag_list, topics):\n",
    "            topic_mapping = topics_tag_list.set_index('Topic').to_dict()['Name']\n",
    "            topics_list = list((pd.Series(topics)).map(topic_mapping))\n",
    "            return topics_list\n",
    "        \n",
    "        #1. posts preprocessing\n",
    "        self.posts_df = bertmodels_obj.posts_df[:set_num_posts].copy() \n",
    "        topic_prep_posts = bertmodels_obj.topic_preprocess(self.posts_df, 'body')\n",
    "        \n",
    "        #2. topic modeling for posts\n",
    "        print('********Topic Modeling for Posts*********')\n",
    "        bertmodels_obj.topic_modeling(topic_prep_posts, 'body_word_token', visualize=False)\n",
    "        topic_prep_posts['topics'] = topic_extractor(bertmodels_obj.model.get_topic_info(), bertmodels_obj.topics)\n",
    "        self.bert_posts = topic_prep_posts.copy()\n",
    "        print('********DONE: Topic Modeling for Posts*********')\n",
    "        \n",
    "        #3. sentiment analysis for posts\n",
    "        print('********Sentiment Analysis for Posts*********')\n",
    "        bertmodels_obj.sentiment_preprocess(topic_prep_posts, \"body\")\n",
    "        print('********DONE: Sentiment Analysis for Posts*********')\n",
    "        \n",
    "        #4. comments preprocessing\n",
    "        self.comments_df = bertmodels_obj.comments_df[:set_num_comments].copy()\n",
    "        topic_prep_comments = bertmodels_obj.topic_preprocess(self.comments_df, 'comment')\n",
    "        \n",
    "        #5. topic modeling for comments\n",
    "        print('********Topic Modeling for Comments*********')\n",
    "        bertmodels_obj.topic_modeling(topic_prep_comments, 'comment_word_token', visualize=False)\n",
    "        topic_prep_comments['topics'] = topic_extractor(bertmodels_obj.model.get_topic_info(), bertmodels_obj.topics)\n",
    "        self.bert_comments = topic_prep_comments.copy()\n",
    "        print('********DONE: Topic Modeling for Comments*********')\n",
    "        \n",
    "        #6. sentiment analysis for comments\n",
    "        print('********Sentiment Analysis for Comments*********')\n",
    "        bertmodels_obj.sentiment_preprocess(topic_prep_comments, \"comment\")\n",
    "        print('********DONE: Sentiment Analysis for Comments*********')\n",
    "        \n",
    "        #7. getting relevance score\n",
    "        print('********Relevance Scores*********')\n",
    "        relevance = Relevance()\n",
    "        self.data = topic_prep_posts.merge(topic_prep_comments, left_on='post_id', right_on='post_id', how='left')\n",
    "        relevance.generate_relevance(self.data)\n",
    "        self.res_df = relevance.df\n",
    "        print('********DONE: Relevance Scores*********')\n",
    "#         relevance.save_to_file(file_name='relevance_1.csv')\n",
    "\n",
    "    def get_posts_data(self):\n",
    "        return self.posts_df\n",
    "    \n",
    "    def get_comments_data(self):\n",
    "        return self.comments_df\n",
    "    \n",
    "    def get_processed_posts(self):\n",
    "        return self.bert_posts\n",
    "    \n",
    "    def get_processed_comments(self):\n",
    "        return self.bert_comments\n",
    "    \n",
    "    def get_result_df(self):\n",
    "        return self.res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a47ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = models.model.get_topic_info()\n",
    "# di = temp.set_index('Topic').to_dict()['Name']\n",
    "# topics = list((pd.Series(models.topics)).map(di))\n",
    "# topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28a129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Preprocessing DataFrame for Topic Modeling*********\n",
      "Fill NaNs\n",
      "Remove URLs\n",
      "Expand Contractions\n",
      "Make Lowercase\n",
      "Tokenize\n",
      "Filter Stopwords\n",
      "Lemmatization\n",
      "********DONE: Preprocessing for Topic Modeling*********\n",
      "********Topic Modeling for Posts*********\n",
      "Number of entries being modeled: 417\n",
      "Intiailizing model and training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1727a9ce26234e5dbf9bc1e25f8b56dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:14:00,200 - BERTopic - Transformed documents to Embeddings\n",
      "2022-02-21 11:14:04,431 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-02-21 11:14:04,452 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********DONE: Topic Modeling for Posts*********\n",
      "********Sentiment Analysis for Posts*********\n",
      "********Preprocessing DataFrame for Sentiment Analysis*********\n",
      "Fill NaNs\n",
      "Remove URLs\n",
      "Expand Contractions\n",
      "Remove escape characters.\n",
      "********DONE: Sentiment Analysis for Posts*********\n",
      "********Preprocessing DataFrame for Topic Modeling*********\n",
      "Fill NaNs\n",
      "Remove URLs\n",
      "Expand Contractions\n",
      "Make Lowercase\n",
      "Tokenize\n",
      "Filter Stopwords\n",
      "Lemmatization\n",
      "********DONE: Preprocessing for Topic Modeling*********\n",
      "********Topic Modeling for Comments*********\n",
      "Number of entries being modeled: 500\n",
      "Intiailizing model and training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197af42b9226423595162451f2e9a223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:14:42,752 - BERTopic - Transformed documents to Embeddings\n",
      "2022-02-21 11:14:44,953 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-02-21 11:14:44,985 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********DONE: Topic Modeling for Comments*********\n",
      "********Sentiment Analysis for Comments*********\n",
      "********Preprocessing DataFrame for Sentiment Analysis*********\n",
      "Fill NaNs\n",
      "Remove URLs\n",
      "Expand Contractions\n",
      "Remove escape characters.\n",
      "********DONE: Sentiment Analysis for Comments*********\n",
      "********Relevance Scores*********\n"
     ]
    }
   ],
   "source": [
    "sub = SubredditAnalysis('computerscience', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b649db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
