{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saimuktevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from src.features.preprocess import PreProcess\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>New to programming or computer science? Want a...</td>\n",
       "      <td>380</td>\n",
       "      <td>1.00</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>1055</td>\n",
       "      <td>The previous thread was finally archived with ...</td>\n",
       "      <td>1.619890e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qb4bof</td>\n",
       "      <td>THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER R...</td>\n",
       "      <td>410</td>\n",
       "      <td>0.97</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>54</td>\n",
       "      <td>Tech Support: /r/techsupport\\n\\nComputer Recom...</td>\n",
       "      <td>1.634619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbw98k</td>\n",
       "      <td>How do general compression algorithms approach...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>7</td>\n",
       "      <td>For example, if one had a source file containi...</td>\n",
       "      <td>1.643058e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbxbya</td>\n",
       "      <td>Max number of parallel http requests</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>3</td>\n",
       "      <td>Hi all, is there a way to know/calculate how m...</td>\n",
       "      <td>1.643060e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saqw7i</td>\n",
       "      <td>Human Brain Cells From Petri Dishes Learn to P...</td>\n",
       "      <td>212</td>\n",
       "      <td>0.99</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://science-news.co/human-brain-cells-from...</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.642934e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                              title  score  \\\n",
       "0  n2n0ax  New to programming or computer science? Want a...    380   \n",
       "1  qb4bof  THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER R...    410   \n",
       "2  sbw98k  How do general compression algorithms approach...     17   \n",
       "3  sbxbya               Max number of parallel http requests      0   \n",
       "4  saqw7i  Human Brain Cells From Petri Dishes Learn to P...    212   \n",
       "\n",
       "   upvote_ratio        subreddit  \\\n",
       "0          1.00  computerscience   \n",
       "1          0.97  computerscience   \n",
       "2          0.88  computerscience   \n",
       "3          0.44  computerscience   \n",
       "4          0.99  computerscience   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/computerscience/comme...          1055   \n",
       "1  https://www.reddit.com/r/computerscience/comme...            54   \n",
       "2  https://www.reddit.com/r/computerscience/comme...             7   \n",
       "3  https://www.reddit.com/r/computerscience/comme...             3   \n",
       "4  https://science-news.co/human-brain-cells-from...            26   \n",
       "\n",
       "                                                body       created  \n",
       "0  The previous thread was finally archived with ...  1.619890e+09  \n",
       "1  Tech Support: /r/techsupport\\n\\nComputer Recom...  1.634619e+09  \n",
       "2  For example, if one had a source file containi...  1.643058e+09  \n",
       "3  Hi all, is there a way to know/calculate how m...  1.643060e+09  \n",
       "4                                                NaN  1.642934e+09  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read posts\n",
    "df_posts = pd.read_csv(\"../data/raw/computerscience_posts.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>title_token</th>\n",
       "      <th>body_token</th>\n",
       "      <th>title_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>New to programming or computer science? Want a...</td>\n",
       "      <td>380</td>\n",
       "      <td>1.00</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>1055</td>\n",
       "      <td>The previous thread was finally archived with ...</td>\n",
       "      <td>1.619890e+09</td>\n",
       "      <td>[New to programming or computer science?, Want...</td>\n",
       "      <td>[The previous thread was finally archived with...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qb4bof</td>\n",
       "      <td>THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER R...</td>\n",
       "      <td>410</td>\n",
       "      <td>0.97</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>54</td>\n",
       "      <td>Tech Support: /r/techsupport\\n\\nComputer Recom...</td>\n",
       "      <td>1.634619e+09</td>\n",
       "      <td>[THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER ...</td>\n",
       "      <td>[Tech Support: /r/techsupport\\n\\nComputer Reco...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbw98k</td>\n",
       "      <td>How do general compression algorithms approach...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>7</td>\n",
       "      <td>For example, if one had a source file containi...</td>\n",
       "      <td>1.643058e+09</td>\n",
       "      <td>[How do general compression algorithms approac...</td>\n",
       "      <td>[For example, if one had a source file contain...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbxbya</td>\n",
       "      <td>Max number of parallel http requests</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>3</td>\n",
       "      <td>Hi all, is there a way to know/calculate how m...</td>\n",
       "      <td>1.643060e+09</td>\n",
       "      <td>[Max number of parallel http requests]</td>\n",
       "      <td>[Hi all, is there a way to know/calculate how ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saqw7i</td>\n",
       "      <td>Human Brain Cells From Petri Dishes Learn to P...</td>\n",
       "      <td>212</td>\n",
       "      <td>0.99</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://science-news.co/human-brain-cells-from...</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>1.642934e+09</td>\n",
       "      <td>[Human Brain Cells From Petri Dishes Learn to ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                              title  score  \\\n",
       "0  n2n0ax  New to programming or computer science? Want a...    380   \n",
       "1  qb4bof  THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER R...    410   \n",
       "2  sbw98k  How do general compression algorithms approach...     17   \n",
       "3  sbxbya               Max number of parallel http requests      0   \n",
       "4  saqw7i  Human Brain Cells From Petri Dishes Learn to P...    212   \n",
       "\n",
       "   upvote_ratio        subreddit  \\\n",
       "0          1.00  computerscience   \n",
       "1          0.97  computerscience   \n",
       "2          0.88  computerscience   \n",
       "3          0.44  computerscience   \n",
       "4          0.99  computerscience   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/computerscience/comme...          1055   \n",
       "1  https://www.reddit.com/r/computerscience/comme...            54   \n",
       "2  https://www.reddit.com/r/computerscience/comme...             7   \n",
       "3  https://www.reddit.com/r/computerscience/comme...             3   \n",
       "4  https://science-news.co/human-brain-cells-from...            26   \n",
       "\n",
       "                                                body       created  \\\n",
       "0  The previous thread was finally archived with ...  1.619890e+09   \n",
       "1  Tech Support: /r/techsupport\\n\\nComputer Recom...  1.634619e+09   \n",
       "2  For example, if one had a source file containi...  1.643058e+09   \n",
       "3  Hi all, is there a way to know/calculate how m...  1.643060e+09   \n",
       "4                                                     1.642934e+09   \n",
       "\n",
       "                                         title_token  \\\n",
       "0  [New to programming or computer science?, Want...   \n",
       "1  [THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER ...   \n",
       "2  [How do general compression algorithms approac...   \n",
       "3             [Max number of parallel http requests]   \n",
       "4  [Human Brain Cells From Petri Dishes Learn to ...   \n",
       "\n",
       "                                          body_token title_filtered  \n",
       "0  [The previous thread was finally archived with...                 \n",
       "1  [Tech Support: /r/techsupport\\n\\nComputer Reco...                 \n",
       "2  [For example, if one had a source file contain...                 \n",
       "3  [Hi all, is there a way to know/calculate how ...                 \n",
       "4                                                 []                 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the title\n",
    "df_posts['title'] = df_posts['title'].fillna('')\n",
    "df_posts['title_token'] = df_posts['title'].apply(sent_tokenize)\n",
    "df_posts['body'] = df_posts['body'].fillna('')\n",
    "df_posts['body_token'] = df_posts['body'].apply(sent_tokenize)\n",
    "df_posts['title_filtered'] = \" \" #introducing new column\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>How late is too late to start a career in prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>I am a freshman at a university and haven't be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>I'm still in highschool but really interested ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>This is probably a common question, but how we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>I am planning on starting a CS major this fall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                            comment\n",
       "0  n2n0ax  How late is too late to start a career in prog...\n",
       "1  n2n0ax  I am a freshman at a university and haven't be...\n",
       "2  n2n0ax  I'm still in highschool but really interested ...\n",
       "3  n2n0ax  This is probably a common question, but how we...\n",
       "4  n2n0ax  I am planning on starting a CS major this fall..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = pd.read_csv(\"../data/raw/computerscience_comments.csv\")\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>title_token</th>\n",
       "      <th>body_token</th>\n",
       "      <th>title_filtered</th>\n",
       "      <th>title_word_token</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>body_word_token</th>\n",
       "      <th>body_filtered</th>\n",
       "      <th>body_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>New to programming or computer science? Want a...</td>\n",
       "      <td>380</td>\n",
       "      <td>1.00</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>1055</td>\n",
       "      <td>The previous thread was finally archived with ...</td>\n",
       "      <td>1.619890e+09</td>\n",
       "      <td>[New to programming or computer science?, Want...</td>\n",
       "      <td>[The previous thread was finally archived with...</td>\n",
       "      <td>[new, programming, computer, science]</td>\n",
       "      <td>[New, to, programming, or, computer, science, ?]</td>\n",
       "      <td>[new, program, comput, scienc]</td>\n",
       "      <td>[The, previous, thread, was, finally, archived...</td>\n",
       "      <td>[previous, thread, finally, archived, 500, com...</td>\n",
       "      <td>[previou, thread, final, archiv, 500, comment,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qb4bof</td>\n",
       "      <td>THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER R...</td>\n",
       "      <td>410</td>\n",
       "      <td>0.97</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>54</td>\n",
       "      <td>Tech Support: /r/techsupport\\n\\nComputer Recom...</td>\n",
       "      <td>1.634619e+09</td>\n",
       "      <td>[THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER ...</td>\n",
       "      <td>[Tech Support: /r/techsupport\\n\\nComputer Reco...</td>\n",
       "      <td>[tech, support, sub, computer, recommendation,...</td>\n",
       "      <td>[THIS, IS, NOT, A, TECH, SUPPORT, SUB, OR, A, ...</td>\n",
       "      <td>[tech, support, sub, comput, recommend, sub]</td>\n",
       "      <td>[Tech, Support, :, /r/techsupport, Computer, R...</td>\n",
       "      <td>[tech, support, rtechsupport, computer, recomm...</td>\n",
       "      <td>[tech, support, rtechsupport, comput, recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbw98k</td>\n",
       "      <td>How do general compression algorithms approach...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>7</td>\n",
       "      <td>For example, if one had a source file containi...</td>\n",
       "      <td>1.643058e+09</td>\n",
       "      <td>[How do general compression algorithms approac...</td>\n",
       "      <td>[For example, if one had a source file contain...</td>\n",
       "      <td>[general, compression, algorithms, approach, d...</td>\n",
       "      <td>[How, do, general, compression, algorithms, ap...</td>\n",
       "      <td>[gener, compress, algorithm, approach, data, r...</td>\n",
       "      <td>[For, example, ,, if, one, had, a, source, fil...</td>\n",
       "      <td>[example, one, source, file, containing, 100, ...</td>\n",
       "      <td>[exampl, one, sourc, file, contain, 100, trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbxbya</td>\n",
       "      <td>Max number of parallel http requests</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://www.reddit.com/r/computerscience/comme...</td>\n",
       "      <td>3</td>\n",
       "      <td>Hi all, is there a way to know/calculate how m...</td>\n",
       "      <td>1.643060e+09</td>\n",
       "      <td>[Max number of parallel http requests]</td>\n",
       "      <td>[Hi all, is there a way to know/calculate how ...</td>\n",
       "      <td>[max, number, parallel, http, requests]</td>\n",
       "      <td>[Max, number, of, parallel, http, requests]</td>\n",
       "      <td>[max, number, parallel, http, request]</td>\n",
       "      <td>[Hi, all, ,, is, there, a, way, to, know/calcu...</td>\n",
       "      <td>[hi, way, knowcalculate, many, parallel, http,...</td>\n",
       "      <td>[hi, way, knowcalcul, mani, parallel, http, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saqw7i</td>\n",
       "      <td>Human Brain Cells From Petri Dishes Learn to P...</td>\n",
       "      <td>212</td>\n",
       "      <td>0.99</td>\n",
       "      <td>computerscience</td>\n",
       "      <td>https://science-news.co/human-brain-cells-from...</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>1.642934e+09</td>\n",
       "      <td>[Human Brain Cells From Petri Dishes Learn to ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[human, brain, cells, petri, dishes, learn, pl...</td>\n",
       "      <td>[Human, Brain, Cells, From, Petri, Dishes, Lea...</td>\n",
       "      <td>[human, brain, cell, petri, dish, learn, play,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                              title  score  \\\n",
       "0  n2n0ax  New to programming or computer science? Want a...    380   \n",
       "1  qb4bof  THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER R...    410   \n",
       "2  sbw98k  How do general compression algorithms approach...     17   \n",
       "3  sbxbya               Max number of parallel http requests      0   \n",
       "4  saqw7i  Human Brain Cells From Petri Dishes Learn to P...    212   \n",
       "\n",
       "   upvote_ratio        subreddit  \\\n",
       "0          1.00  computerscience   \n",
       "1          0.97  computerscience   \n",
       "2          0.88  computerscience   \n",
       "3          0.44  computerscience   \n",
       "4          0.99  computerscience   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/computerscience/comme...          1055   \n",
       "1  https://www.reddit.com/r/computerscience/comme...            54   \n",
       "2  https://www.reddit.com/r/computerscience/comme...             7   \n",
       "3  https://www.reddit.com/r/computerscience/comme...             3   \n",
       "4  https://science-news.co/human-brain-cells-from...            26   \n",
       "\n",
       "                                                body       created  \\\n",
       "0  The previous thread was finally archived with ...  1.619890e+09   \n",
       "1  Tech Support: /r/techsupport\\n\\nComputer Recom...  1.634619e+09   \n",
       "2  For example, if one had a source file containi...  1.643058e+09   \n",
       "3  Hi all, is there a way to know/calculate how m...  1.643060e+09   \n",
       "4                                                     1.642934e+09   \n",
       "\n",
       "                                         title_token  \\\n",
       "0  [New to programming or computer science?, Want...   \n",
       "1  [THIS IS NOT A TECH SUPPORT SUB OR A COMPUTER ...   \n",
       "2  [How do general compression algorithms approac...   \n",
       "3             [Max number of parallel http requests]   \n",
       "4  [Human Brain Cells From Petri Dishes Learn to ...   \n",
       "\n",
       "                                          body_token  \\\n",
       "0  [The previous thread was finally archived with...   \n",
       "1  [Tech Support: /r/techsupport\\n\\nComputer Reco...   \n",
       "2  [For example, if one had a source file contain...   \n",
       "3  [Hi all, is there a way to know/calculate how ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                      title_filtered  \\\n",
       "0              [new, programming, computer, science]   \n",
       "1  [tech, support, sub, computer, recommendation,...   \n",
       "2  [general, compression, algorithms, approach, d...   \n",
       "3            [max, number, parallel, http, requests]   \n",
       "4  [human, brain, cells, petri, dishes, learn, pl...   \n",
       "\n",
       "                                    title_word_token  \\\n",
       "0   [New, to, programming, or, computer, science, ?]   \n",
       "1  [THIS, IS, NOT, A, TECH, SUPPORT, SUB, OR, A, ...   \n",
       "2  [How, do, general, compression, algorithms, ap...   \n",
       "3        [Max, number, of, parallel, http, requests]   \n",
       "4  [Human, Brain, Cells, From, Petri, Dishes, Lea...   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0                     [new, program, comput, scienc]   \n",
       "1       [tech, support, sub, comput, recommend, sub]   \n",
       "2  [gener, compress, algorithm, approach, data, r...   \n",
       "3             [max, number, parallel, http, request]   \n",
       "4  [human, brain, cell, petri, dish, learn, play,...   \n",
       "\n",
       "                                     body_word_token  \\\n",
       "0  [The, previous, thread, was, finally, archived...   \n",
       "1  [Tech, Support, :, /r/techsupport, Computer, R...   \n",
       "2  [For, example, ,, if, one, had, a, source, fil...   \n",
       "3  [Hi, all, ,, is, there, a, way, to, know/calcu...   \n",
       "4                                                 []   \n",
       "\n",
       "                                       body_filtered  \\\n",
       "0  [previous, thread, finally, archived, 500, com...   \n",
       "1  [tech, support, rtechsupport, computer, recomm...   \n",
       "2  [example, one, source, file, containing, 100, ...   \n",
       "3  [hi, way, knowcalculate, many, parallel, http,...   \n",
       "4                                                 []   \n",
       "\n",
       "                                           body_stem  \n",
       "0  [previou, thread, final, archiv, 500, comment,...  \n",
       "1  [tech, support, rtechsupport, comput, recommen...  \n",
       "2  [exampl, one, sourc, file, contain, 100, trans...  \n",
       "3  [hi, way, knowcalcul, mani, parallel, http, re...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PreProcess.preprocess(df_posts, 'title')\n",
    "PreProcess.preprocess(df_posts, 'body')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4419 words total, with a vocabulary size of 1946\n",
      "Max tweet length is 72\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in df_posts['body_filtered'] for word in tokens]\n",
    "tweet_lengths = [len(tokens) for tokens in df_posts['body_filtered']]\n",
    "vocab = sorted(list(set(all_words)))\n",
    "\n",
    "print('{} words total, with a vocabulary size of {}'.format(len(all_words), len(vocab)))\n",
    "print('Max tweet length is {}'.format(max(tweet_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer', 46),\n",
       " ('like', 37),\n",
       " ('know', 35),\n",
       " ('hello', 32),\n",
       " ('nt', 30),\n",
       " ('science', 29),\n",
       " ('hi', 28),\n",
       " ('n', 28),\n",
       " ('time', 25),\n",
       " ('learning', 23),\n",
       " ('would', 22),\n",
       " ('question', 22),\n",
       " ('algorithm', 22),\n",
       " ('software', 20),\n",
       " ('2', 20),\n",
       " ('data', 18),\n",
       " ('recently', 18),\n",
       " ('book', 17),\n",
       " ('want', 17),\n",
       " ('programming', 17),\n",
       " ('understand', 17),\n",
       " ('code', 17),\n",
       " ('bit', 16),\n",
       " ('could', 16),\n",
       " ('people', 15),\n",
       " ('memory', 15),\n",
       " ('get', 15),\n",
       " ('years', 15),\n",
       " ('algorithms', 15),\n",
       " ('one', 14)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_words = [item for sublist in df_posts['body_filtered'] for item in sublist]\n",
    "word_freq = FreqDist(flat_words)\n",
    "word_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_word_token</th>\n",
       "      <th>comment_filtered</th>\n",
       "      <th>comment_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>How late is too late to start a career in prog...</td>\n",
       "      <td>[How, late, is, too, late, to, start, a, caree...</td>\n",
       "      <td>[late, late, start, career, programming]</td>\n",
       "      <td>[late, late, start, career, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>I am a freshman at a university and haven't be...</td>\n",
       "      <td>[I, am, a, freshman, at, a, university, and, h...</td>\n",
       "      <td>[freshman, university, nt, able, work, side, p...</td>\n",
       "      <td>[freshman, univers, nt, abl, work, side, proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>I'm still in highschool but really interested ...</td>\n",
       "      <td>[I, 'm, still, in, highschool, but, really, in...</td>\n",
       "      <td>[still, highschool, really, interested, comput...</td>\n",
       "      <td>[still, highschool, realli, interest, comput, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>This is probably a common question, but how we...</td>\n",
       "      <td>[This, is, probably, a, common, question, ,, b...</td>\n",
       "      <td>[probably, common, question, well, coding, boo...</td>\n",
       "      <td>[probabl, common, question, well, code, bootca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n2n0ax</td>\n",
       "      <td>I am planning on starting a CS major this fall...</td>\n",
       "      <td>[I, am, planning, on, starting, a, CS, major, ...</td>\n",
       "      <td>[planning, starting, cs, major, fall]</td>\n",
       "      <td>[plan, start, cs, major, fall]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                            comment  \\\n",
       "0  n2n0ax  How late is too late to start a career in prog...   \n",
       "1  n2n0ax  I am a freshman at a university and haven't be...   \n",
       "2  n2n0ax  I'm still in highschool but really interested ...   \n",
       "3  n2n0ax  This is probably a common question, but how we...   \n",
       "4  n2n0ax  I am planning on starting a CS major this fall...   \n",
       "\n",
       "                                  comment_word_token  \\\n",
       "0  [How, late, is, too, late, to, start, a, caree...   \n",
       "1  [I, am, a, freshman, at, a, university, and, h...   \n",
       "2  [I, 'm, still, in, highschool, but, really, in...   \n",
       "3  [This, is, probably, a, common, question, ,, b...   \n",
       "4  [I, am, planning, on, starting, a, CS, major, ...   \n",
       "\n",
       "                                    comment_filtered  \\\n",
       "0           [late, late, start, career, programming]   \n",
       "1  [freshman, university, nt, able, work, side, p...   \n",
       "2  [still, highschool, really, interested, comput...   \n",
       "3  [probably, common, question, well, coding, boo...   \n",
       "4              [planning, starting, cs, major, fall]   \n",
       "\n",
       "                                        comment_stem  \n",
       "0               [late, late, start, career, program]  \n",
       "1  [freshman, univers, nt, abl, work, side, proje...  \n",
       "2  [still, highschool, realli, interest, comput, ...  \n",
       "3  [probabl, common, question, well, code, bootca...  \n",
       "4                     [plan, start, cs, major, fall]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PreProcess.preprocess(df_comments, 'title')\n",
    "PreProcess.preprocess(df_comments, 'comment')\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24142 words total, with a vocabulary size of 5828\n",
      "Max tweet length is 124\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in df_comments['comment_filtered'] for word in tokens]\n",
    "tweet_lengths = [len(tokens) for tokens in df_comments['comment_filtered']]\n",
    "vocab = sorted(list(set(all_words)))\n",
    "\n",
    "print('{} words total, with a vocabulary size of {}'.format(len(all_words), len(vocab)))\n",
    "print('Max tweet length is {}'.format(max(tweet_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer', 317),\n",
       " ('like', 205),\n",
       " ('nt', 183),\n",
       " ('science', 181),\n",
       " ('would', 147),\n",
       " ('programming', 134),\n",
       " ('data', 127),\n",
       " ('code', 125),\n",
       " ('think', 125),\n",
       " ('good', 112),\n",
       " ('one', 108),\n",
       " ('want', 97),\n",
       " ('use', 97),\n",
       " ('know', 96),\n",
       " ('get', 94),\n",
       " ('really', 91),\n",
       " ('time', 91),\n",
       " ('software', 90),\n",
       " ('cs', 84),\n",
       " ('need', 80),\n",
       " ('1', 80),\n",
       " ('math', 78),\n",
       " ('first', 76),\n",
       " ('n', 76),\n",
       " ('book', 74),\n",
       " ('language', 74),\n",
       " ('way', 73),\n",
       " ('learning', 72),\n",
       " ('c', 71),\n",
       " ('learn', 69)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_words = [item for sublist in df_comments['comment_filtered'] for item in sublist]\n",
    "word_freq = FreqDist(flat_words)\n",
    "word_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.9.4-py2.py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from bertopic) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: pyyaml<6.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from bertopic) (5.4.1)\n",
      "Collecting hdbscan>=0.8.27\n",
      "  Using cached hdbscan-0.8.27.tar.gz (6.4 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting umap-learn>=0.5.0\n",
      "  Using cached umap-learn-0.5.2.tar.gz (86 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from bertopic) (4.62.3)\n",
      "Collecting plotly>=4.7.0\n",
      "  Using cached plotly-5.5.0-py2.py3-none-any.whl (26.5 MB)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from bertopic) (1.22.1)\n",
      "Collecting cython>=0.27\n",
      "  Using cached Cython-0.29.27-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: six in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from hdbscan>=0.8.27->bertopic) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from hdbscan>=0.8.27->bertopic) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from hdbscan>=0.8.27->bertopic) (1.7.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.0.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
      "Collecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.11.4-cp39-cp39-macosx_10_11_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.10.2-cp39-none-macosx_10_9_x86_64.whl (147.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 147.2 MB 147 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.11.3-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.6.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.1.18)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.2-py3-none-any.whl (9.9 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: requests in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.7)\n",
      "Collecting numba>=0.49\n",
      "  Downloading numba-0.55.1-cp39-cp39-macosx_10_14_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.6.tar.gz (1.1 MB)\n",
      "Requirement already satisfied: setuptools in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (58.0.4)\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-1.21.5-cp39-cp39-macosx_10_9_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0 MB 10.9 MB/s eta 0:00:01     |█████████████████████████████▍  | 15.6 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.0-cp39-cp39-macosx_10_9_x86_64.whl (25.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/saimuktevi/opt/anaconda3/envs/reddit/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.0)\n",
      "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.27-cp39-cp39-macosx_10_9_x86_64.whl size=646856 sha256=e4d456f8b22d9a5f857b02733b7fcf3e1382d2892e0f86c5f92142d96d5ca06c\n",
      "  Stored in directory: /Users/saimuktevi/Library/Caches/pip/wheels/0e/91/d1/488d513c322dc8d0a536aeaed47292b0431758edfda92292d0\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=861bcf746f2e1440c1cd3b22137b9eb187254c2b09d43bc2adb725c8041057d6\n",
      "  Stored in directory: /Users/saimuktevi/Library/Caches/pip/wheels/17/1d/fd/a16123b417c527e0452c3e10ae5139cd2ab2f6fee93e892441\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=1f6a754c97d400387c98c1f3d06d8c9dbf471ffd389b87dde1574fed70233e72\n",
      "  Stored in directory: /Users/saimuktevi/Library/Caches/pip/wheels/4a/66/81/205d77bd58539a1d07fb8b795fcea0b17f4d59912c0f4202b7\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=43fbaeeae2db8e372ba75ca80189001fc35f387ea4905424dd1bae4d1d7c5bae\n",
      "  Stored in directory: /Users/saimuktevi/Library/Caches/pip/wheels/c0/f0/1d/20aa0dcee54ef40c179a4a4f57fad413437864a67757013a99\n",
      "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
      "Installing collected packages: numpy, llvmlite, filelock, torch, tokenizers, sacremoses, numba, huggingface-hub, transformers, torchvision, tenacity, sentencepiece, pynndescent, cython, umap-learn, sentence-transformers, plotly, hdbscan, bertopic\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.1\n",
      "    Uninstalling numpy-1.22.1:\n",
      "      Successfully uninstalled numpy-1.22.1\n",
      "Successfully installed bertopic-0.9.4 cython-0.29.27 filelock-3.4.2 hdbscan-0.8.27 huggingface-hub-0.4.0 llvmlite-0.38.0 numba-0.55.1 numpy-1.21.5 plotly-5.5.0 pynndescent-0.5.6 sacremoses-0.0.47 sentence-transformers-2.1.0 sentencepiece-0.1.96 tenacity-8.0.1 tokenizers-0.11.4 torch-1.10.2 torchvision-0.11.3 transformers-4.16.1 umap-learn-0.5.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.21 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.9/site-packages/bertopic/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[1;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.9.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.9/site-packages/bertopic/_bertopic.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Models\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhdbscan\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.9/site-packages/umap/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.9/site-packages/umap/umap_.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tril \u001b[38;5;28;01mas\u001b[39;00m sparse_tril, triu \u001b[38;5;28;01mas\u001b[39;00m sparse_triu\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msparse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.9/site-packages/numba/__init__.py:200\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    199\u001b[0m _ensure_llvm()\n\u001b[0;32m--> 200\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# we know llvmlite is working as the above tests passed, import it now as SVML\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# needs to mutate runtime options (sets the `-vector-library`).\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllvmlite\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit/lib/python3.9/site-packages/numba/__init__.py:140\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 1.18 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 1.21 or less\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 1.21 or less"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(preprocessed_df['title'])\n",
    "topic_model = BERTopic(language=\"english\", embedding_model=\"paraphrase-multilingual-mpnet-base-v2\", min_topic_size=3, calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
